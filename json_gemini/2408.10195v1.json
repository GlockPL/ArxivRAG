{"title": "SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views", "authors": ["Chao Xu", "Ang Li", "Linghao Chen", "Yulin Liu", "Ruoxi Shi", "Hao Su", "Minghua Liu"], "abstract": "Open-world 3D generation has recently attracted considerable attention. While many single-image-to-3D methods have yielded visually appealing outcomes, they often lack sufficient controllability and tend to produce hallucinated regions that may not align with users' expectations. In this paper, we explore an important scenario in which the input consists of one or a few unposed 2D images of a single object, with little or no overlap. We propose a novel method, SpaRP, to reconstruct a 3D textured mesh and estimate the relative camera poses for these sparse-view images. SpaRP distills knowledge from 2D diffusion models and finetunes them to implicitly deduce the 3D spatial relationships between the sparse views. The diffusion model is trained to jointly predict surrogate representations for camera poses and multi-view images of the object under known poses, integrating all information from the input sparse views. These predictions are then leveraged to accomplish 3D reconstruction and pose estimation, and the reconstructed 3D model can be used to further refine the camera poses of input views. Through extensive experiments on three datasets, we demonstrate that our method not only significantly outperforms baseline methods in terms of 3D reconstruction quality and pose prediction accuracy but also exhibits strong efficiency. It requires only about 20 seconds to produce a textured mesh and camera poses for the input views.", "sections": [{"title": "1 Introduction", "content": "3D object reconstruction is a long-standing problem with applications spanning 3D content creation, augmented reality, virtual reality, and robotics, among others. Although traditional photogrammetry and recent neural field methods have made significant strides in reconstructing high-fidelity geometry and appearance, they typically require dense view inputs. However, in many practical scenarios, such as in e-commerce and consumer capture situations, acquiring a comprehensive set of high-resolution images along with precise camera data is not always feasible.\nOn the other end of the spectrum, the tasks of converting a single image to 3D and text to 3D have recently seen substantial progress thanks to the rich priors embedded in 2D diffusion models and pre-training on extensive 3D datasets. These methods may achieve high-quality geometry and texture that matches the input view, but they also introduce ambiguities in the regions not visible in the input image (such as the back view). Although these methods attempt to hallucinate reasonable interpretations of these invisible areas, the generated regions may not always align with users' expectations, and users often lack sufficient control over these ambiguous regions.\nIn this paper, we explore a critical scenario where the input consists of one or a few unposed 2D images of a single object. The images are captured from arbitrarily distributed camera poses, often with little or no overlap. We tackle both the 3D reconstruction and pose estimation of input images under this sparse view setting. Note that, in dense view setting, traditional Structure-from-Motion (SfM) solvers (e.g., COLMAP [58]) are typically employed for pose estimation. However, with sparse view inputs, these solvers often become unreliable and tend to fail due to insufficient overlapping visual cues. This issue is the main reason why existing sparse view reconstruction methods generally require known camera poses as input. While some recent methods have attempted pose-free reconstruction and pose estimation for sparse views, they are usually trained on a predefined small set of object categories and exhibit poor generalization to unseen object categories."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Sparse-View 3D Reconstruction", "content": "Reconstructing 3D objects from sparse-view images is challenging due to the lack of visual correspondence and clues. When a small baseline between images is assumed, several methods have pretrained generalizable models to infer surface positions by establishing pixel correspondences and learning generalizable priors across scenes. However, these methods often fail to produce satisfactory results when the sparse-view images"}, {"title": "2.2 Pose-Free Reconstruction", "content": "Unlike the methods mentioned above, which assume known camera poses, many studies have aimed to solve the pose-free reconstruction challenge. When provided with dense images, some approaches jointly optimize the NeRF representation along with camera parameters. However, due to the highly non-convex nature of this optimization problem, such methods are susceptible to initial pose guesses and can become trapped in local minima. This issue worsens when input images are sparse, with increasing ambiguity and reduced constraint availability. In response, numerous proposals have attempted to enhance optimization robustness. For example, SpaRF uses dense image matches as explicit optimization constraints, while FvOR starts with coarse predictions of camera poses and alternated updates between shape and pose.\nIn contrast to the optimization-based methods, there is a body of research proposing generalizable solutions for this problem. VideoAE infers scene geometry from the first frame in a video series and estimates camera poses relative to that frame, which allows for warping scene geometry to decode new viewpoints. SparsePose first regresses and then iteratively refines camera poses. FORGE designs neural networks to infer initial camera poses, fuse multi-view features, and decode spatial densities and colors. GRNN offers a GRU-based reconstruction method estimating the relative pose for each input view against a global feature volume. The RelPose series use probabilistic modeling for relative rotation estimation between images. Other works eschew explicit camera pose estimations, instead employing transformers to encode input views into latent scene representations for novel view synthesis.\nMore recently, leveraging large vision models and diffusion models, which have shown significant promise, new efforts have emerged for camera pose estimation. PoseDiffusion implements a diffusion model guided by 2D keypoint matches to estimate poses. PF-LRM adapts the LRM model to predict a point cloud for each input image, then utilizes differentiable PnP for pose estima-tion. iFusion employs an optimization pipeline to assess relative elevations"}, {"title": "2.3 Open-World 3D Generation", "content": "Open-world single-image-to-3D and text-to-3D tasks have recently undergone significant advancements. Recent 2D generative models and vision-language models have supplied valuable priors about the 3D world, sparking a surge in research on 3D generation. Notably, models such as DreamFusion, Magic3D, and ProlificDreamer have pioneered a line of approach to per-shape optimization. These models optimize a 3D representation (e.g., NeRF) for each unique text or image input, utilizing the 2D prior models for gradient guidance. Although they produce impressive results, these methods are hampered by prolonged optimization times, often extending to several hours, and \u201cmulti-face issue\u201d problems.\nMoreover, beyond optimization-based methods, exemplified by Zero123, numerous recent studies have investigated the employment of pre-trained 2D diffusion models for synthesizing novel views from single images or text. They have introduced varied strategies to foster 3D-consistent multi-view generation. The resulting multi-view images can then serve for 3D reconstruction, utilizing either optimization-based methods or feed-forward models.\nWhile most existing works focus on single-image-to-3D or text-to-3D, they often hallucinate regions that are invisible in the input image, which provides users with limited control over those areas. In this paper, we seek to broaden the input to encompass unposed sparse views and address both the 3D reconstruction and pose estimation challenges in a time-efficient way within tens of seconds."}, {"title": "3 Method", "content": "Given n unposed input images $\\{I_i | i = 1,...,n; 1 \\leq n \\leq 6\\}$, which illustrate a single object from arbitrary categories, we predict their relative camera poses $\\xi_{ii'}$ and reconstruct the 3D model $M$ of the object. As illustrated in Fig. 2, we first finetune a 2D diffusion model to process the unposed sparse input images (Sec. 3.1). The 2D diffusion model is responsible for jointly generating grid images for both the NOCS maps of the input views, as well as multi-view images with known camera poses. We use the predicted NOCS maps to estimate the camera poses for the input views (Sec. 3.2). The resulting multi-view images are fed into a two-stage 3D diffusion model for a coarse-to-fine generation of a 3D textured mesh (Sec. 3.3). This joint training strategy allows the two branches to complement each other. It enhances the understanding of both the input"}, {"title": "3.1 Tiling Sparse View Images as Input Condition", "content": "Recently, numerous studies have shown that 2D diffusion models not only possess robust open-world capabilities but also learn rich 3D geometric priors. For instance, Stable Diffusion, can be finetuned to include camera view control enabling it to predict novel views of objects\u2014a task that necessitates significant 3D spatial reasoning. Consequently, we are inspired to utilize the rich priors inherent in 2D diffusion models for the tasks of sparse view 3D reconstruction and pose estimation.\nUnlike most existing approaches that use a single RGB image as the condition and focus on synthesizing multi-view images, our goal is to take a sparse set of input images and stimulate Stable Diffusion to infer the spatial relationships among the input views implicitly. To accomplish this, given 1 ~ 6 sparse views from arbitrary camera poses, we tile them into a 3 \u00d7 2 multi-view grid, as illustrated in Fig. 3 (c). The image in the first grid cell determines a canonical frame (to be discussed later), while the order of the other views is inconsequential. When there are fewer than 6 sparse views, we use empty padding for the remaining grid cells. This composite image then serves as the condition for Stable Diffusion, which is expected to assimilate all information from the input sparse views during the diffusion process.\nWe employ Stable Diffusion 2.1 as our base model. To adapt the original text-conditioning to our tiled multi-view image condition, we follow to apply both local and global conditioning strategies. For local conditioning, we use the reference-only attention mechanism where we process the reference tiled image with the denoising UNet model and append the attention keys and values"}, {"title": "3.2 Image-to-NOCS Diffusion as a Pose Estimator", "content": "Conventional Structure-from-Motion (SfM) solvers, such as COLMAP, rely on feature matching for pose estimation. However, in scenarios with sparse views, there may be little to no overlap between input views. The lack of sufficient visual correspondence cues often renders the solvers unreliable and prone to failure. Consequently, instead of relying on local correspondences, we leverage the rich semantic priors embedded in 2D diffusion models for pose estimation.\nOne of the primary challenges is to enable 2D diffusion models to output camera poses. While camera poses can be represented in various scalar formats (e.g., 6-dimensional vector, four-by-four matrix, etc.), they are not native repre-sentations for a 2D diffusion model to generate. Inspired by recent works demon-strating that 2D diffusion models can be used to predict normal maps [38]-a"}, {"title": "3.3 Multi-View Prediction for 3D Reconstruction", "content": "We follow the paradigm of recent single-image-to-3D methods by initially generating multi-view images and subsequently using a feed-forward 3D recon-struction module to convert these images into a 3D representation. It is note-worthy that the input sparse views might not encompass the entire 3D objects, nor provide adequate information for 3D reconstruction. Therefore, we propose to predict multi-view images at uniformly distributed camera poses first, and then use these predicted images for 3D reconstruction."}, {"title": "3.4 Pose Refinement with Reconstructed 3D Model", "content": "In Section 3.2, we finetune diffusion models for NOCS map prediction and cam-era pose estimation. However, due to the hallucinatory and stochastic nature of diffusion models, unavoidable errors may exist. The generated 3D mesh $M$, though not perfect, provides a multi-view consistent and explicit 3D structure. We can further refine the coarse poses predicted from the NOCS maps by lever-aging the reconstructed 3D shape.\nPnP \\\\ Pose Refinement via Differentiable Rendering. Starting with initial poses $\\{\\xi_i\\}$ extracted from the predicted NOCS maps, we refine them through dif-ferentiable rendering [26]. Specifically, we render the generated mesh $M$ at op-timizing camera poses $\\xi_i$. We minimize the rendering loss between the rendered image $I = R(M,\\xi_i)$ and the input image $I_i$ to obtain the optimally fitted camera pose $\\xi_i^*$. The optimization process can be formulated as:\n$\\xi_i^* = \\text{arg}\\underset{\\xi_i \\in SE(3)}{ \\text{min}} (\\lambda \\cdot L_{\\text{mask}}(I, I_i) + \\mu \\cdot L_{\\text{rgb}}(I, I_i))$"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Evaluation Settings", "content": "Training Datasets and Details. We train our models on a curated subset of 100k shapes from the Objaverse dataset. Considering the variable qual-ity of the original Objaverse dataset, we opted to filter out higher-quality data by initially manually annotating 8,000 3D objects based on overall geometry quality and texture preferences. Subsequently, we train MLP models for quality rating classification and texture score regression, utilizing their multimodal fea-tures. Based on the predictions of these models, we select shapes that are rated as high-quality and have top texture scores. Further details about the data filtering process are included in the Appendix."}, {"title": "4.2 Experiment Results", "content": "Pose Prediction. We report the pose estimation results in Tab. 1, where it is evident that SpaRP outperforms all baseline methods by a significant margin. It is worth noting that RelPose++ [29] and FORGE [17] struggle to yield satisfac-tory results for our open-world evaluation images. iFusion, an optimization-based approach, is prone to becoming trapped in local minima. With only one initial pose (ninit = 1), it also fails to produce adequate results. In contrast, our method leverages priors from 2D diffusion models and can generate acceptable results in a single forward pass. Even without any additional refinement (w/o refine), our method can already produce results similar to iFusion with four initial poses (Ninit = 4), while being far more efficient, requiring just 1/25 of the runtime. With the integration of further refinement through a mixture of experts, our method achieves even better performance.\n3D Reconstruction. We present the qualitative results in Fig. 4. With only a single-view input, single-image-to-3D methods fail to produce meshes that faithfully match the entire structure and details of the ground truth mesh. For instance, most single-view baseline methods are unable to reconstruct the stems of the artichoke, the back of the firetruck, the red saddle on Yoshi, and the two separate legs of Kirby standing on the ground. In contrast, sparse-view methods yield results that are much closer to the ground truth by incorporating information from multiple sparse views. Compared to iFusion, EscherNet, our method generates meshes with higher-quality geometry and textures that more"}, {"title": "4.3 Analysis", "content": "Single View vs. Sparse Views. In Fig. 5, we present the results obtained by our method when provided with single-view and sparse-view inputs. With a single-view input, our method can still generate reasonable results, yet it may not accurately capture the structures and details of the regions that are not visible. Our method demonstrates the capability to effectively integrate information from all sparse-view inputs provided.\nNumber of Views. In Tab. 3, we quantitatively showcase the impact of the number of views on both 3D reconstruction and pose estimation. We observe that incorporating more input views enables the 2D diffusion network to better grasp their spatial relationships and underlying 3D objects, boosting both tasks.\nPnP \\\\ Pose Refinement. While the predicted NOCS maps can be directly converted into camera poses, we have found that these poses can be further refined through alignment with the generated 3D meshes. Fig. 6 showcases the predicted poses before and after refinement. Although both are generally very close to the ground truth poses, refinement can further reduce the error.\nNumber of Experts. We employ a mixture-of-experts strategy to address the ambiguity issues related to NOCS prediction for symmetric objects. By using this strategy and increasing the number of experts, there is a substantial increase in pose estimation accuracy. Please refer to the Appendix for more details and quantitative ablation studies.\nJoint Training. We finetune 2D diffusion models to jointly predict NOCS maps and multi-view images from sparse, unposed views by leveraging a do-main switcher. As shown in Tab. 4, this joint training strategy enables the two branches to implicitly interact and complement each other, enhancing the inter-pretation of both the input sparse views and the intrinsic properties of the 3D objects, which in turn improves the performance of each task."}, {"title": "5 Conclusion", "content": "We present SpaRP, a novel method for 3D reconstruction and pose estimation using unposed sparse-view images. Our method leverages rich priors embedded in 2D diffusion models and exhibits strong open-world generalizability. Without the need for per-shape optimization, it can deliver high-quality textured meshes, along with accurate camera poses, in approximately 20 seconds."}, {"title": "A Additional Real-World Examples", "content": "In Fig. 7, we demonstrate that SpaRP can be applied to real-world sparse-view images without camera poses. This includes images captured by users with consumer devices (e.g., with an iPhone) or e-commerce product images (e.g., from amazon.com). SpaRP is capable of achieving commendable results in both pose estimation and 3D reconstruction."}, {"title": "B Ablation Studies on Number of Experts", "content": "Due to the inherently stochastic nature of diffusion models, our multi-view diffu-sion model may sometimes fail to accurately understand the spatial relationship between input images of objects and estimate their relative poses in a single"}, {"title": "C Robustness to Varying Camera Intrinsics", "content": "Our multi-view diffusion model demonstrates robust performance across varying input image camera intrinsics. During its training, we randomize both the focal length and optical center of input images. The input image field of view (FOV) follows a normal distribution $N(36^\\circ, 9^\\circ)$, centered at 36 degrees. The optical center also follows a normal distribution centered at the image center. As shown in Fig. 8, we tested the model's performance across input FOVs ranging from 5 to 65 degrees, covering common photographic focal lengths. Using 20 different objects, we calculated the average PSNR and LPIPS for predictions at various FOVs. Our model demonstrated consistently high performance across the tested range. This showcases its robustness to intrinsic variations in input images."}, {"title": "D Sparse-View Reconstruction using the Estimated Poses", "content": "Our estimated poses can benefit numerous downstream applications, including many existing sparse-view 3D reconstruction approaches that require camera poses. Here, we demonstrate how our estimated poses can be utilized with Ze-roRF [61], a sparse-view 3D reconstruction method. ZeroRF is an optimization-based method that does not rely on pretrained priors and requires camera poses"}, {"title": "F Details of Dataset Curation", "content": "The Objaverse dataset [9] contains about 800,000 shapes. However, this dataset includes numerous partial scans, scenes, and basic, textureless geometries that are unsuitable for our task of generating single objects. To optimize the train-"}]}