{"title": "SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views", "authors": ["Chao Xu", "Ang Li", "Linghao Chen", "Yulin Liu", "Ruoxi Shi", "Hao Su", "Minghua Liu"], "abstract": "Open-world 3D generation has recently attracted considerable attention. While many single-image-to-3D methods have yielded visually appealing outcomes, they often lack sufficient controllability and tend to produce hallucinated regions that may not align with users' expectations. In this paper, we explore an important scenario in which the input consists of one or a few unposed 2D images of a single object, with little or no overlap. We propose a novel method, SpaRP, to reconstruct a 3D textured mesh and estimate the relative camera poses for these sparse-view images. SpaRP distills knowledge from 2D diffusion models and finetunes them to implicitly deduce the 3D spatial relationships between the sparse views. The diffusion model is trained to jointly predict surrogate representations for camera poses and multi-view images of the object under known poses, integrating all information from the input sparse views. These predictions are then leveraged to accomplish 3D reconstruction and pose estimation, and the reconstructed 3D model can be used to further refine the camera poses of input views. Through extensive experiments on three datasets, we demonstrate that our method not only significantly outperforms baseline methods in terms of 3D reconstruction quality and pose prediction accuracy but also exhibits strong efficiency. It requires only about 20 seconds to produce a textured mesh and camera poses for the input views.", "sections": [{"title": "Introduction", "content": "3D object reconstruction is a long-standing problem with applications spanning 3D content creation, augmented reality, virtual reality, and robotics, among others. Although traditional photogrammetry and recent neural field methods have made significant strides in reconstructing high-fidelity geometry and appearance, they typically require dense view inputs. However, in many practical scenarios, such as in e-commerce and consumer capture situations, acquiring a comprehensive set of high-resolution images along with precise camera data is not always feasible.\nOn the other end of the spectrum, the tasks of converting a single image to 3D and text to 3D have recently seen substantial progress thanks to the rich priors embedded in 2D diffusion models and pre-training on extensive 3D datasets. These methods may achieve high-quality geometry and texture that matches the input view, but they also introduce ambiguities in the regions not visible in the input image (such as the back view). Although these methods attempt to hallucinate reasonable interpretations of these invisible areas, the generated regions may not always align with users' expectations, and users often lack sufficient control over these ambiguous regions.\nIn this paper, we explore a critical scenario where the input consists of one or a few unposed 2D images of a single object. The images are captured from arbitrarily distributed camera poses, often with little to no overlap. We tackle both the 3D reconstruction and pose estimation of input images under this sparse view setting. Note that, in dense view setting, traditional Structure-from-Motion (SfM) solvers (e.g., COLMAP) are typically employed for pose estimation. However, with sparse view inputs, these solvers often become unreliable and tend to fail due to insufficient overlapping visual cues. This issue is the main reason why existing sparse view reconstruction methods generally require known camera poses as input. While some recent methods have attempted pose-free reconstruction and pose estimation for sparse views , they are usually trained on a predefined small set of object categories and exhibit poor generalization to unseen object categories."}, {"title": "Related Work", "content": "Reconstructing 3D objects from sparse-view images is challenging due to the lack of visual correspondence and clues. When a small baseline between images is assumed, several methods have pretrained generalizable models to infer surface positions by establishing pixel correspondences and learning generalizable priors across scenes. However, these methods often fail to produce satisfactory results when the sparse-view images have a large baseline. Some studies have attempted to alleviate the dependence on dense views by incorporating priors or adding regularization into the NeRF optimization process. Others have employed 2D diffusion priors to generate novel-view images as additional input for the NeRF model. For example, ReconFusion trains a NeRF from sparse-view images and uses a denoising UNet to infer some novel view images as support for the NeRF model. EscherNet utilizes Stable Diffusion for novel view synthesis and designs a camera positional encoding module to yield more consistent images. Furthermore, some recent works have integrated specialized loss functions and additional modalities as inputs into NeRF-based per-scene optimization.\nIn contrast to these methods, our approach does not require camera poses for the input sparse views. It is not limited to small baselines and is capable of generating 360-degree meshes. Furthermore, without the need for per-shape optimization, our method can quickly produce both textured meshes and camera poses in about 20 seconds."}, {"title": "Pose-Free Reconstruction", "content": "Unlike the methods mentioned above, which assume known camera poses, many studies have aimed to solve the pose-free reconstruction challenge. When provided with dense images, some approaches jointly optimize the NeRF representation along with camera parameters. However, due to the highly non-convex nature of this optimization problem, such methods are susceptible to initial pose guesses and can become trapped in local minima. This issue worsens when input images are sparse, with increasing ambiguity and reduced constraint availability. In response, numerous proposals have attempted to enhance optimization robustness. For example, SpaRF uses dense image matches as explicit optimization constraints, while FvOR starts with coarse predictions of camera poses and alternated updates between shape and pose.\nIn contrast to the optimization-based methods, there is a body of research proposing generalizable solutions for this problem. VideoAE infers scene geometry from the first frame in a video series and estimates camera poses relative to that frame, which allows for warping scene geometry to decode new viewpoints. SparsePose first regresses and then iteratively refines camera poses. FORGE designs neural networks to infer initial camera poses, fuse multi-view features, and decode spatial densities and colors. GRNN offers a GRU-based reconstruction method estimating the relative pose for each input view against a global feature volume. The RelPose series use probabilistic modeling for relative rotation estimation between images. Other works eschew explicit camera pose estimations, instead employing transformers to encode input views into latent scene representations for novel view synthesis.\nMore recently, leveraging large vision models and diffusion models, which have shown significant promise, new efforts have emerged for camera pose estimation. PoseDiffusion implements a diffusion model guided by 2D keypoint matches to estimate poses. PF-LRM adapts the LRM model to predict a point cloud for each input image, then utilizes differentiable PnP for pose estimation. iFusion employs an optimization pipeline to assess relative elevations and azimuths. It utilizes Zero123 predictions as a basis and optimizes the relative pose between two images by minimizing the reconstruction loss between the predicted and target images.\nIn contrast to these existing approaches, our proposal capitalizes on the extensive priors inherent in pre-trained 2D diffusion models, thereby providing exceptional generalizability to handle a diverse range of open-world categories. Our method predicts camera poses and 3D mesh geometry in a single feedforward pass, negating the need for per-shape optimization."}, {"title": "Open-World 3D Generation", "content": "Open-world single-image-to-3D and text-to-3D tasks have recently undergone significant advancements. Recent 2D generative models and vision-language models have supplied valuable priors about the 3D world, sparking a surge in research on 3D generation. Notably, models such as DreamFusion, Magic3D, and ProlificDreamer have pioneered a line of approach to per-shape optimization. These models optimize a 3D representation (e.g., NeRF) for each unique text or image input, utilizing the 2D prior models for gradient guidance. Although they produce impressive results, these methods are hampered by prolonged optimization times, often extending to several hours, and \u201cmulti-face issue\u201d problems.\nMoreover, beyond optimization-based methods, exemplified by Zero123, numerous recent studies have investigated the employment of pre-trained 2D diffusion models for synthesizing novel views from single images or text. They have introduced varied strategies to foster 3D-consistent multi-view generation. The resulting multi-view images can then serve for 3D reconstruction, utilizing either optimization-based methods or feed-forward models.\nWhile most existing works focus on single-image-to-3D or text-to-3D, they often hallucinate regions that are invisible in the input image, which provides users with limited control over those areas. In this paper, we seek to broaden the input to encompass unposed sparse views and address both the 3D reconstruction and pose estimation challenges in a time-efficient way within tens of seconds."}, {"title": "Method", "content": "Given n unposed input images {$I_i | i = 1,...,n; 1 \u2264 n \u2264 6$}, which illustrate a single object from arbitrary categories, we predict their relative camera poses $\\xi_{ii'}$ and reconstruct the 3D model M of the object. As illustrated in Fig. 2, we first finetune a 2D diffusion model to process the unposed sparse input images. The 2D diffusion model is responsible for jointly generating grid images for both the NOCS maps of the input views, as well as multi-view images with known camera poses. We use the predicted NOCS maps to estimate the camera poses for the input views. The resulting multi-view images are fed into a two-stage 3D diffusion model for a coarse-to-fine generation of a 3D textured mesh. This joint training strategy allows the two branches to complement each other. It enhances the understanding of both the input sparse views and the intrinsic properties of the 3D objects, thereby improving the performance of both pose estimation and 3D reconstruction. Optionally, the generated 3D mesh can also be used to further refine the camera poses."}, {"title": "Tiling Sparse View Images as Input Condition", "content": "Recently, numerous studies have shown that 2D diffusion models not only possess robust open-world capabilities but also learn rich 3D geometric priors. For instance, Stable Diffusion , can be finetuned to include camera view control , enabling it to predict novel views of objects\u2014a task that necessitates significant 3D spatial reasoning. Consequently, we are inspired to utilize the rich priors inherent in 2D diffusion models for the tasks of sparse-view 3D reconstruction and pose estimation.\nUnlike most existing approaches that use a single RGB image as the condition and focus on synthesizing multi-view images, our goal is to take a sparse set of input images and stimulate Stable Diffusion to infer the spatial relationships among the input views implicitly. To accomplish this, given 1 ~ 6 sparse views from arbitrary camera poses, we tile them into a 3 \u00d7 2 multi-view grid, as illustrated in Fig. 3 (c). The image in the first grid cell determines a canonical frame (to be discussed later), while the order of the other views is inconsequential. When there are fewer than 6 sparse views, we use empty padding for the remaining grid cells. This composite image then serves as the condition for Stable Diffusion, which is expected to assimilate all information from the input sparse views during the diffusion process.\nWe employ Stable Diffusion 2.1 as our base model. To adapt the original text-conditioning to our tiled multi-view image condition, we follow [60] to apply both local and global conditioning strategies. For local conditioning, we use the reference-only attention mechanism , where we process the reference tiled image with the denoising UNet model and append the attention keys and values from this image to corresponding layers in the denoising model for the target images. This mechanism facilitates implicit yet effective interactions between the diffusion model and the sparse views. For global conditioning, we integrate the mean-pooled CLIP embedding of all input images modulated by learnable token weights into the diffusion process, enhancing the model's ability to grasp the overarching semantics and structure of the sparse views.\nAs depicted in Figs. 2 and 3, our objective is to concurrently generate grid images for both NOCS maps of the input views and multi-view images from known camera poses. To achieve this, we utilize a domain switcher that enables flexible toggling between the two domains. The switcher consists of two learnable embeddings, one for each domain, which are then injected into the UNet of the stable diffusion models by being added to its time embedding."}, {"title": "Image-to-NOCS Diffusion as a Pose Estimator", "content": "Conventional Structure-from-Motion (SfM) solvers, such as COLMAP , rely on feature matching for pose estimation. However, in scenarios with sparse views, there may be little to no overlap between input views. The lack of sufficient visual correspondence cues often renders the solvers unreliable and prone to failure. Consequently, instead of relying on local correspondences, we leverage the rich semantic priors embedded in 2D diffusion models for pose estimation.\nOne of the primary challenges is to enable 2D diffusion models to output camera poses. While camera poses can be represented in various scalar formats (e.g., 6-dimensional vector, four-by-four matrix, etc.), they are not native representations for a 2D diffusion model to generate. Inspired by recent works demonstrating that 2D diffusion models can be used to predict normal maps a domain different from natural images we propose using a surrogate representation: the Normalized Object Coordinate Space (NOCS) . We finetune Stable Diffusion to predict NOCS maps for each input view.\nAs depicted in Fig. 3(b), a NOCS frame is determined for each set of input sparse view images and the underlying 3D object. Specifically, the 3D shape is normalized into a unit cube, i.e., x, y, z \u2208 [0, 1]. The shape's upward axis aligns with the dataset's inherent upward axis of the 3D object, typically the gravity axis. Predicting the object's forward-facing direction may be ambiguous, so we rotate the 3D shape in the NOCS frame to align its forward direction (zero azimuth) with that of the first input view, thus unambiguously establishing the NOCS frame. For each input view, we then render a NOCS map, where each 2D pixel (r,g,b) represents the corresponding 3D point's position (x,y,z) in the defined NOCS frame, as shown in Fig. 3(c). These NOCS maps align with the operational domain of 2D diffusion models, similar to the normal maps in previous work [38].\nTo facilitate interactions between NOCS maps from different views and generate more 3D-consistent NOCS maps, we tile all NOCS maps into a 3 \u00d7 2 grid image as the input condition following the same tiling order and the empty padding convention. We finetune Stable Diffusion to generate these multi-view tiled NOCS maps, so the 2D diffusion model can attend to both the input sparse views and their NOCS maps during the diffusion process.\nAfter generating the NOCS maps for the input sparse views, we employ a traditional Perspective-n-Point (PnP) solver to compute the poses {$\\xi_i$}\nfrom the NOCS frame to the camera frames of each input view by minimizing the reprojection error:\n$\\xi_{i}^{pnp} = arg\\min_{\\xi_i \\in SE(3)} \\sum_{j=1}^{m_i} ||p_{i,j} - proj(q_{i,j}, \\xi_i) ||^2 , $"}, {"content": "where $p_{i,j}$ represents the jth pixel's location in the ith NOCS map; $q_{i,j}$ is the corresponding 3D point location in the NOCS frame; $m_i$ is the number of pixels for the ith view, and proj is the perspective projection operation. Note that the PnP algorithm assumes known camera intrinsics and optimizes only for the camera extrinsics. A RANSAC scheme is applied during the PnP computation for outlier removal, enhancing the robustness of the pose prediction to boundary noises and errors from the 2D diffusion model. As all NOCS maps share a common NOCS frame, we can thus determine the relative camera poses between views i and i' through $\\xi_{i'}\\xi_i^{-1}$."}, {"title": "Multi-View Prediction for 3D Reconstruction", "content": "We follow the paradigm of recent single-image-to-3D methods by initially generating multi-view images and subsequently using a feed-forward 3D reconstruction module to convert these images into a 3D representation. It is noteworthy that the input sparse views might not encompass the entire 3D objects, nor provide adequate information for 3D reconstruction. Therefore, we propose to predict multi-view images at uniformly distributed camera poses first, and then use these predicted images for 3D reconstruction.\nUnlike traditional novel view synthesis our approach employs a fixed camera configuration for target multi-views. As depicted in Fig. 3, our target multi-view images consist of six views with alternating 20\u00b0 and -10\u00b0 elevations, and 60\u00b0-spaced azimuths relative to the first input view. Although the elevation angles are set absolutely, the azimuth angles are relative to the azimuth of the first input sparse view to resolve the ambiguity in face-forwarding directions. Furthermore, We maintain consistent camera intrinsics across target views, independent of input views. These strategies mitigate challenges in predicting camera intrinsics and elevation during the 3D reconstruction process. Existing methods hindered by this issue may be sensitive to intrinsic variations and often depend on predicting or requiring user-specified input image elevations.\nSimilar to NOCS map prediction, we tile all six views into a 3 \u00d7 2 grid image and finetune Stable Diffusion to generate this tiled image. The 2D diffusion model, conditioned on the input sparse views, aims to incorporate all information from input views, deduce the underlying 3D objects, and predict the multi-view images at the predetermined camera poses. Although the predicted poses of input sparse views are not directly employed in the 3D reconstruction, the joint training of NOCS prediction and multi-view prediction branches implicitly complement each other and boost the performance of both tasks.\nUpon generating the multi-view images at known camera poses, we utilize the multi-view to 3D reconstruction module proposed in to lift these images to 3D. The reconstruction module adopts a two-stage coarse-to-fine approach, which involves initially extracting the 2D features of the generated multi-view images, aggregating them with the known camera poses, and constructing a 3D cost volume. This 3D cost volume acts as the condition for the 3D diffusion networks. In the coarse stage, a low-resolution $64^3$ 3D occupancy volume is produced. This is subsequently refined to yield a high-resolution $128^3$ SDF (Signed Distance Field) volume with colors. Finally, a textured mesh is derived from the SDF volume employing the marching cubes algorithm."}, {"title": "Pose Refinement with Reconstructed 3D Model", "content": "In Section 3.2, we finetune diffusion models for NOCS map prediction and camera pose estimation. However, due to the hallucinatory and stochastic nature of diffusion models, unavoidable errors may exist. The generated 3D mesh M, though not perfect, provides a multi-view consistent and explicit 3D structure. We can further refine the coarse poses predicted from the NOCS maps by leveraging the reconstructed 3D shape.\nStarting with initial poses {$\\xi_i$} extracted from the predicted NOCS maps, we refine them through differentiable rendering. Specifically, we render the generated mesh M at optimizing camera poses $\\xi_i$. We minimize the rendering loss between the rendered image $I = R(M,\\xi_i)$ and the input image $I_i$ to obtain the optimally fitted camera pose $\\xi_i^*$. The optimization process can be formulated as:\n$\\xi_{i}^{*} = arg \\min_{\\xi_i \\in SE(3)} (\\lambda \\cdot L_{mask}(I, I_i) + \\mu \\cdot L_{rgb}(I, I_i)),$"}, {"title": "Experiment Results", "content": "We report the pose estimation results , where it is evident that SpaRP outperforms all baseline methods by a significant margin. It is worth noting that RelPose++  and FORGE struggle to yield satisfactory results for our open-world evaluation images. iFusion, an optimization-based approach, is prone to becoming trapped in local minima. With only one initial pose (ninit = 1), it also fails to produce adequate results. In contrast, our method leverages priors from 2D diffusion models and can generate acceptable results in a single forward pass. Even without any additional refinement (w/o refine), our method can already produce results similar to iFusion with four initial poses (ninit = 4), while being far more efficient, requiring just 1/25 of the runtime. With the integration of further refinement through a mixture of experts, our method achieves even better performance.\nWe present the qualitative results . With only a single-view input, single-image-to-3D methods fail to produce meshes that faithfully match the entire structure and details of the ground truth mesh. For instance, most single-view baseline methods are unable to reconstruct the stems of the artichoke, the back of the firetruck, the red saddle on Yoshi, and the two separate legs of Kirby standing on the ground. In contrast, sparse-view methods yield results that are much closer to the ground truth by incorporating information from multiple sparse views. Compared to iFusion, EscherNet, our method generates meshes with higher-quality geometry and textures that more accurately match the input sparse views. We report the quantitative results , where our method significantly outperforms both single-view-to-3D and sparse-view approaches in terms of both 2D and 3D metrics. Moreover, our method exhibits superior efficiency, being much faster than the baseline methods."}, {"title": "Analysis", "content": "In Fig. 5, we present the results obtained by our method when provided with single-view and sparse-view inputs. With a single-view input, our method can still generate reasonable results, yet it may not accurately capture the structures and details of the regions that are not visible. Our method demonstrates the capability to effectively integrate information from all sparse-view inputs provided.\nIn Tab. 3, we quantitatively showcase the impact of the number of views on both 3D reconstruction and pose estimation. We observe that incorporating more input views enables the 2D diffusion network to better grasp their spatial relationships and underlying 3D objects, boosting both tasks. Pose Refinement. While the predicted NOCS maps can be directly converted into camera poses, we have found that these poses can be further refined through alignment with the generated 3D meshes. Fig. 6 showcases the predicted poses before and after refinement. Although both are generally very close to the ground truth poses, refinement can further reduce the error.\nWe employ a mixture-of-experts strategy to address the ambiguity issues related to NOCS prediction for symmetric objects. By using this strategy and increasing the number of experts, there is a substantial increase in pose estimation accuracy. Please refer to the Appendix for more details and quantitative ablation studies.\nWe finetune 2D diffusion models to jointly predict NOCS maps and multi-view images from sparse, unposed views by leveraging a domain switcher. As shown in Tab. 4, this joint training strategy enables the two branches to implicitly interact and complement each other, enhancing the interpretation of both the input sparse views and the intrinsic properties of the 3D objects, which in turn improves the performance of each task."}, {"title": "Conclusion", "content": "We present SpaRP, a novel method for 3D reconstruction and pose estimation using unposed sparse-view images. Our method leverages rich priors embedded in 2D diffusion models and exhibits strong open-world generalizability. Without the need for per-shape optimization, it can deliver high-quality textured meshes, along with accurate camera poses, in approximately 20 seconds."}, {"title": "Additional Real-World Examples", "content": "In Fig. 7, we demonstrate that SpaRP can be applied to real-world sparse-view images without camera poses. This includes images captured by users with consumer devices (e.g., with an iPhone) or e-commerce product images (e.g., from amazon.com). SpaRP is capable of achieving commendable results in both pose estimation and 3D reconstruction."}, {"title": "Ablation Studies on Number of Experts", "content": "Due to the inherently stochastic nature of diffusion models, our multi-view diffusion model may sometimes fail to accurately understand the spatial relationship between input images of objects and estimate their relative poses in a single diffusion pass, especially with objects that have some symmetry. We found that employing a Mixture of Experts (MoE) strategy effectively mitigates this issue. Specifically, we run the diffusion models $N_{init}$ times with different random seeds to generate multiple sets of NOCS maps for pose prediction, selecting the optimal one based on the minimum rendering loss from the pose refinement stage. As shown in Tab. 5, increasing the number of experts ($n_{init}$) from 1 to 8 led to a significant improvement in the accuracy of relative pose predictions across both the OmniObject3D [85] and GSO [12] datasets. This demonstrates that the MoE strategy is simple yet effective in improving the robustness of our pose prediction approach."}, {"title": "Robustness to Varying Camera Intrinsics", "content": "Our multi-view diffusion model demonstrates robust performance across varying input image camera intrinsics. During its training, we randomize both the focal length and optical center of input images. The input image field of view (FOV) follows a normal distribution $\\mathcal{N}(36^\\circ, 9^\\circ)$, centered at 36 degrees. The optical center also follows a normal distribution centered at the image center. As shown in Fig. 8, we tested the model's performance across input FOVs ranging from 5 to 65 degrees, covering common photographic focal lengths. Using 20 different objects, we calculated the average PSNR and LPIPS for predictions at various FOVs. Our model demonstrated consistently high performance across the tested range. This showcases its robustness to intrinsic variations in input images."}, {"title": "Sparse-View Reconstruction using the Estimated Poses", "content": "Our estimated poses can benefit numerous downstream applications, including many existing sparse-view 3D reconstruction approaches that require camera poses. Here, we demonstrate how our estimated poses can be utilized with ZeroRF [61], a sparse-view 3D reconstruction method. ZeroRF is an optimization-based method that does not rely on pretrained priors and requires camera poses as input. As depicted in Fig. 9, by using only five images along with the corresponding predicted poses as input, ZeroRF is capable of generating a NeRF that synthesizes reasonable novel views. The resulting mesh also shows commendable global geometry, considering the challenging nature of the task."}, {"title": "Evaluation Details", "content": "To account for the scale and pose ambiguity of the generated mesh, we align the predicted mesh with the ground truth mesh prior to metric calculation. During the alignment process, we sample 12 rotations (30\u00b0 apart) as initial positions and 10 scales from 0.6 to 1.4, which is dense enough in practice. We enumerate the combinations of these rotations and scales for initialization and subsequently refine the alignment with the Iterative Closest Point (ICP) algorithm. We select the alignment that yields the highest inlier ratio. Both the ground truth and predicted meshes are then scaled to fit within a unit bounding box.\nWe adopt the evaluation metrics from [32] to assess the reconstruction quality from two perspectives: (1) geometric quality and (2) texture quality. For geometric quality, we apply the F-score to quantify the discrepancy between the reconstructed and ground truth meshes, setting the F-score threshold at 0.05. To evaluate texture quality, we compute the CLIP-Similarity, PSNR, and LPIPS between images rendered from the reconstructed mesh and those of the ground truth. The meshes undergo rendering from 24 distinct viewpoints, encompassing a full 360-degree view around the object. The rendered images have a resolution of 512x 512 pixels."}, {"title": "Pose Estimation", "content": "To evaluate pose estimation, we render five sparse views for each shape and assess the relative poses between all ten pairs of views. We convert the predicted poses to the OpenCV convention and report the median rotation error, rotation accuracy, and translation error across all pairs. The rotation error is the minimum angular deviation between the predicted and the ground truth poses. In contrast, the translation error is the absolute difference between the corresponding translation vectors. We present accuracies as the percentage of pose pairs with rotation errors below the thresholds of 15\u00b0 and 30\u00b0. It should be noted that iFusion infers only the relative elevation, azimuth, and distance, and cannot provide the 4x4 camera matrix without the absolute camera pose of the reference image. For iFusion, we supplement the elevation angle of the reference image using an external elevation estimation method , which has a median prediction error of 5\u00b0 on the GSO dataset. Additionally, many baseline methods do not require camera intrinsics as input, resulting in predicted poses with varying distances from the camera to the shape, as reflected by the magnitude of the translation vectors. To address this intrinsic ambiguity, we normalize the predicted translation vectors for each method by using a scale factor that aligns the first view's predicted camera translation with the ground truth translation. After normalization, we report the absolute translation errors. Furthermore, in our ablation studies, we investigate the impact of the number of input views and the number of experts on pose estimation performance using subsets of 100 shapes."}, {"title": "Details of Dataset Curation", "content": "The Objaverse dataset [9] contains about 800,000 shapes. However, this dataset includes numerous partial scans, scenes, and basic, textureless geometries that are unsuitable for our task of generating single objects. To optimize the training process in terms of efficacy and efficiency, we curate a high-quality subset consisting of single objects with high-fidelity geometry and vivid textural appearance. We begin by randomly selecting a subset of 3D models and then task annotators with assessing the overall geometry quality and evaluating texture aesthetic preferences. Subsequently, we train a simple network to predict such annotations.\nFor assessing overall geometry quality, annotators are required to assign one of three possible levels to each 3D model:\nHigh quality: Objects that represent a single entity with a clear semantic meaning, such as avatars and animals.\nMedium quality: Simple geometric shapes (e.g., cubes, spheres); geometries that are abstract or have unclear semantic meaning; and repetitive structures found in the Objaverse, such as skeletal frames of houses and staircases.\nLow quality: Point clouds; scenes with multiple elements; incomplete, low-quality, or unidentifiable 3D scans.\nFor texture preference, given the difficulty in defining absolute standards due to aesthetic subjectivity, we adopt a binary choice approach for annotation. This method presents annotators with pairs of 3D models, prompting them to select the one with superior texture quality or visual appeal.\nOverall, we have recruited 10 annotators and collected labels for 4,000 pairs of shapes in total. Based on these annotations, we trained MLP networks to predict overall geometry quality ratings and texture scores, respectively. Both networks take the multimodal features of each shape as input, which include image, text, and 3D features, as encoded in OpenShape [33]. The rating classification MLP predicts a one-hot encoded label across three levels, and is trained using the cross-entropy loss. Meanwhile, the texture scoring MLP regresses a score for each shape and is trained using a relative margin loss.\nDuring the training of SpaRP, we utilized the trained MLPs to curate a subset of approximately 100,000 objects. These objects are rated as high-quality and possess texture scores within the top 20%."}]}