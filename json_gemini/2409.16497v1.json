{"title": "Unsupervised Text Representation Learning via Instruction-Tuning for Zero-Shot Dense Retrieval", "authors": ["Qiuhai Zeng", "Zimeng Qiu", "Dae Yon Hwang", "Xin He", "William M. Campbell"], "abstract": "Dense retrieval systems are commonly used for information retrieval (IR). They rely on learning text representations through an encoder and usually require supervised modeling via labelled data which can be costly to obtain or simply unavailable. In this study, we introduce a novel unsupervised text representation learning tech- nique via instruction-tuning the pre-trained encoder-decoder large language models (LLM) under the dual-encoder retrieval framework. We demonstrate the corpus representation can be augmented by the representations of relevant synthetic queries generated by the instruct-tuned LLM founded on the Rao-Blackwell theorem. Further- more, we effectively align the query and corpus text representation with self-instructed-tuning. Specifically, we first prompt an open-box pre-trained LLM to follow defined instructions (i.e. question genera- tion and keyword summarization) to generate synthetic queries. Next, we fine-tune the pre-trained LLM with defined instructions and the generated queries that passed quality check. Finally, we generate syn- thetic queries with the instruction-tuned LLM for each corpora and represent each corpora by weighted averaging the synthetic queries and original corpora embeddings. We evaluate our proposed method under low-resource settings on three English and one German re- trieval datasets measuring NDCG@10, MRR@100, Recall@100. We significantly improve the average zero-shot retrieval performance on all metrics, increasing open-box FLAN-T5 model variations by [3.34%, 3.50%] in absolute and exceeding three competitive dense re- trievers (i.e. mDPR, T-Systems, mBART-Large), with model of size at least 38% smaller, by 1.96%, 4.62%, 9.52% absolute on NDCG@10.", "sections": [{"title": "1 INTRODUCTION", "content": "Dense retrieval systems commonly employ dual-encoder retrieval models which use two separate encoders, either symmetric or asym- metric, to represent the query and corpus [8, 9, 13, 33]. The corpora are indexed with representation and will be retrieved in response to each query based on the relevance scores. The scores are usually calculated based on embedding similarity, such as dot product or cosine similarity. Although dense retrieval systems have developed rapidly, the model performance largely depends supervised text rep- resentation learning and relevancy capturing between the query and corpus [36]. Yet, it remains to be a major challenge to properly re- trieve when lacking labeled modeling data. Existing work [21, 22] leveraged pre-trained large encoders (specifically T5 models, Raffel et al. [26]) to alleviate the data thirst. However, their proposals still required annotated datasets either by web mining or manual anno- tation for fine-tuning in order to improve the generalization ability of dual-encoder retrieval models, for example, dealing with out-of- domain data. An alternative solution is to train a dense retrieval on synthetic query-corpus relevance pairs. [18] trains a question gener- ation system on general domain data and applies it to the targeted domain to construct synthetic question-passage data. To save the effort of training a task-specific generation model on general data, like Natural Questions [15] or MSMARCO [20], Promptagator [5] proposes to use pre-trained LLMs, like FLAN [32], as a few-shot query generator to build the data for training the dual-encoder. How- ever, the synthetic queries are not directly leveraged at inference, potentially causing gaps between training and inference of dense retrievers [2]. Earlier work, e.g., doc2query [24], concatenates the generated queries with the corresponding corpus, aiming to enrich the corpus representation with questions that the corpus can poten- tially answer. An improved version, docTTTTTquery [23] leverages pre-trained T5 models as the expansion model, leading to more relevant synthetic queries and better retrieval performance."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Instruction-tuning", "content": "Tuning pre-trained LLMs with (natural language instruction, re- sponse) pairs to enhance models' ability to follow instructions and understand user intention. It is a rising paradigm in NLP to strengthen model's generalizability on unseen tasks. FLAN [32] significantly improves a 137B LLM's zero-shot performance via in- struction learning on various NLP datasets with multiple instruction templates. InstructDial [10] also shows significant zero-shot perfor- mance boost in unseen dialogues when applying instruction-tuning to dialogue domain. InstructGPT [25] enhances GPT-3's performance by fine-tuning it on instructions and human feedback collected from OpenAI API. Self-Instruct [31] fine-tunes the open-box GPT-3 on its own generated instructions and instances which achieved on par performance of InstructGPT."}, {"title": "2.2 Dense Retrieval Text Representation", "content": "The foundational component of dense retrieval is the text represen- tation. Under dual-encoder framework, it has been a long standing practice such as Sentence-BERT [27], ColBERT [14] to represent query and corpus with encoder only models, e.g., BERT [7] and RoBERTa [16]. Recently Sentence-T5 [21] demonstrate that encoder- decoder pretrained LLM like T5 can achieve superior performance. Further, representing corpus with single representation may not well model the fine-grained semantic interaction between the queries and corpus. Poly-encoder [11] and ME-BERT [17] learn multiple representations to better capture the corpus semantics and show sig- nificant improvement. Doc2query [24] and docTTTTTquery [23] append generated synthetic queries to the corpus and thus enrich the semantic information."}, {"title": "3 METHOD", "content": "We propose an unsupervised text representation learning approach through self-instructed-tuning a pre-trained encoder-decoder LLM. The first step is to generate instruction following responses from an LLM and instruction-tune the LLM itself with filtered quality (natural language instruction, response) pairs. The second step is to compute the augmented corpus embedding weighing in synthetic queries' (e.g. questions, keywords) embeddings. Figure 2 presents the overall flow of our approach."}, {"title": "3.1 Problem Scenario", "content": "Denote corpora as C1,C2,...,Cn, and their relevant queries as Q11, Q12,...,Q21,..., where queries Qi1, Qi2.... are relevant to the same corpora Ci. For example, Q11 can be Harry Potter 1 and Q12"}, {"title": "Corpus Embedding as an Expectation Estimator", "content": "The group cen- ter is a comprehensive depiction of the entire group and is indicative to distinguish from other groups. With the pre-trained Encoder(\u00b7), the group center is essentially the expected value of the embedding of each group's queries, denoted by E(Encoder(Qi)). When we use the embedding of the corpus, i.e. Encoder(Ci), as its representa- tion, we are using it to estimate the group center E(Encoder (Qi)). This is effective when we don't have any information from the query group."}, {"title": "Application of the Rao-Blackwell theorem", "content": "Assume we have 1 m relevant queries Qi1,Qi2, ..., Qim for corpus C\u2081. Then \u2211 Encoder(Qij) is a sufficient statistics to estimate E(Encoder(Qi)). mj=1 According to Rao-Blackwell Theorem: If g(X) is any kind of estimator of a parameter 0, then the conditional expectation of g(X) given T(X), namely E(g(x)|T(x)), where T is a sufficient statistic, is typically a better estimator of 0, and is never worse. Plug in Equation (2), we get an improved estimator for E(Encoder(Qi)), which is E(Encoder(Ci) | \u2211 Encoder(Qij))."}, {"title": "3.4 Instruction-Tuning the LLM", "content": "While LLM demonstrates reasonable text generation capabilities, its ability to follow specific instructions can be honed. Instruction- tuning focuses on training a model to precisely follow the provided instructions.\nAs we don't have the query-corpora labeled data, we propose to self-instructed-tuning the LLM on its self-generated quality (i.e. gated) responses following given instructions to enhance synthetic queries generation relevance. This approach has demonstrated its effectiveness [31]. The instruct-tuned LLM is then used to prepare the synthetic queries for the corpus representation augmentation as in Equation (6)."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Datasets", "content": "In this work, we tested four IR datasets where the summary of the database is shown in Table 1. English: (1) NFCorpus [1] has au- tomatically extracted relevance judgments for medical documents. (2) SciFact [30] consists of expert-annotated scientific claims with abstracts and rationales. (3) SCIDOCS [3] has seven document- level tasks from citation prediction, document classification, and recommendation. German: (4) GermanQuAD [19] has the rele- vant information for high complex German QA with a large size of corpora. Due to computation resource limits, we downsample the corpus in SCIDOCS and GermanQuAD datasets, where we en- sure the downsampled corpus include all relevant corpus for test queries. Note that such downsampling does not prevent us from fairly comparing the zero-shot retrieval efficacy of our approach with open-box LLMs because all experiments are performed under the same data setting. To help the encoder capture the fine-grained semantic interaction between queries and corpus, we divide each corpora into multiple sentences using the PunktSentenceTokenizer 1 from nltk package and use the sentence level multi representation for the corpora, meaning that if any of the sentence is retrieved, the passage is retrieved."}, {"title": "4.2 Baseline", "content": "We compare between the corpus-only representation and our pro- posed augmented corpus representation in zero-shot experiments under the dual-encoder framework. To obtain the representation of a sequence from the encoder, we perform mean aggregation over the last hidden state of each token following [21]. We measure the relevance between query and corpus using cosine similarity."}, {"title": "4.3 Encoder-Decoder Models", "content": "T5 is an encoder-decoder model pre-trained on a combination of unsupervised and supervised tasks, where each task is transformed into a text-to-text format [26]. FLAN-T5 is an enhanced version of T5 fine-tuned on a mixture of tasks [32]. Considering that these types of models are open source, offer various sizes, support English and German, and have an encoder-decoder architecture, we leverage the FLAN-T5-Base and Large models in our experiments."}, {"title": "4.4 Instruction Query Generation", "content": "For instruction query generation and instruction-tuning, we consider two types of instructions (i.e. keyword summarization and question generation) as shown in Figure 2. We also develop a filter to im- prove the quality of generated instructions. If the task is keyword summarization, the number of keywords should be smaller than the half number of sentences in corpus. If it's question generation, the generated sequence should end with a question mark. The filter is simple, leaving room for further improvement. The numbers of the filtered synthetic queries are shown in Table 1."}, {"title": "4.5 Hyperparameter Setting", "content": "When performing instruction-tuning, we use the same hyperparam- eter setting for all the models. Specifically, we use the AdaFactor optimizer with learning rate 0.0001, batch size 16, and the number of epochs 30. Early stopping is performed when the validation loss shows no improvement for five consecutive epochs.\nWhen generating queries using FLAN-T5 models, we only con- sider one returned sequence for each instruction and assume they are equally important. We denote the generated question and keywords as question, and keywords. We tested the multiple weighting meth- ods for corpus representation where the best approach is giving the weight on the original corpus as w0 = 0.6, so that each of questioni and keywords, has the weight 0.2. Thus, the corpus representation is:"}, {"title": "5 RESULTS AND DISCUSSION", "content": ""}, {"title": "5.1 Corpora vs Sentence Indexing", "content": "We evaluate whether the sentence level multi-representation can capture the semantic interaction between the corpora and the query. Results for FLAN-T5 models using encoder-only representation are shown in Table 2. The sentence level multi-representation embed- ding technique outperforms the corpora level single representation by a large margin across all datasets. As the model size increases, the performance also gets better. Note that our approach uses no labeled data to achieve on par performance as SOTA models, and sentence level indexing is a way we do for chunking. According to the promising empirical results, we will apply the sentence level multi-representation technique to all the following experiments."}, {"title": "5.2 Overall Results", "content": "Table 3 describes the performance of FLAN-T5 models regarding instruction-tuning. Overall, we can mostly find the improvements of performances in all metrics after instruction-tuning, except for SCIDOCS. This is mainly because the quality of generated queries after instruction-tuning are proper and detailed (Table 5), and also each synthetic query is less overlapped which makes the corpora distinguishable. The influence of instruction-tuning is greater in larger model since it can have better generation capability and be more affected by fine-tuning with instructions.\nTable 4 compares our approach and SOTA models in zero-shot scenarios. To clarify, FLAN-T5-Base has similar size as other SOTA models which can be considered as a fair comparison. First of all, instruct-tuned FLAN-T5-Base shows the boosted averaged results than other SOTA models which reveals the prowess of our approach. Considering a larger model (i.e. Tuned-FLAN-T5-Large) enhances the performance further. Thus, our suggested approach is consis- tently applicable in different size of models where the larger one promises the better performances."}, {"title": "5.3 Ablation Study", "content": "Optimal Corpus Representation From our findings, new corpus representation based on synthetic queries from instructions is useful to improve the retrieval performances. To define the optimal weights"}, {"title": "6 CONCLUSION", "content": "In our research, we propose the unsupervised text representation learning technique through self-instructed-tuning encoder-decoder LLMs. Based on the Rao-Blackwell theorem, we leverage the em- beddings of synthetically generated queries (i.e. questions and key- words) to augment the corpus representation for the dual-encoder retrieval framework. In zero-shot experiments, our proposed corpus representation consistently improves the performance over encoder- only corpus representation. Even if the open-box LLM was not pre-trained on retrieval task and there is no labeled modeling data, after fine-tuning with our approach it exceeds the SOTA models across different datasets, presenting the high effectiveness and data efficiency of our method in retrieval tasks.\nIn future work, we plan to explore our proposed method on sepa- rate encoder and decoder models."}]}