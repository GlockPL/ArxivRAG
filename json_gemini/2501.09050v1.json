{"title": "Generating Realistic Synthetic Head Rotation Data for Extended Reality using Deep Learning", "authors": ["Jakob Struye", "Filip Lemic", "Jeroen Famaey"], "abstract": "Extended Reality is a revolutionary method of delivering multimedia content to users. A large contributor to its popularity is the sense of immersion and interactivity enabled by having real-world motion reflected in the virtual experience accurately and immediately. This user motion, mainly caused by head rotations, induces several technical challenges. For instance, which content is generated and transmitted depends heavily on where the user is looking. Seamless systems, taking user motion into account proactively, will therefore require accurate predictions of upcoming rotations. Training and evaluating such predictors requires vast amounts of orientational input data, which is expensive to gather, as it requires human test subjects. A more feasible approach is to gather a modest dataset through test subjects, and then extend it to a more sizeable set using synthetic data generation methods. In this work, we present a head rotation time series generator based on TimeGAN, an extension of the well-known Generative Adversarial Network, designed specifically for generating time series. This approach is able to extend a dataset of head rotations with new samples closely matching the distribution of the measured time series.", "sections": [{"title": "INTRODUCTION", "content": "Extended Reality (XR), encompassing Virtual, Mixed and Augmented Reality, has proven to be a major revolution in media consumption. In addition to its widespread use for recreational purposes [30], XR has enabled novel approaches for other tasks, including training [21], remote operation [28], and architecture and construction [2, 11]. A key enabler of the XR experience is how it can reflect the user's real-world motion accurately and immediately within the experience [47]. This enables the user to seamlessly and intuitively change their gaze direction, and can also serve as a source of input for virtual experiences.\nThe user freedom in XR induces a number of challenging demands on the system. When the user rotates their head, the displayed content must adapt to this at a moment's notice. More specifically, the motion-to-photon latency dictates that the effect of any user motion must be visible on-screen within 20 ms as to avoid nauseating the user [15]. Several algorithms aid in fulfilling this latency requirement. Generated content is often warped right before display through algorithms such as Asynchronous Time-Warp, using the most recent measurements of user pose [38]. When displaying pre-recorded 360\u00b0 content, viewport-dependent encoding ensures that only the content expected to be within the user's field of view is transmitted to reduce transmission latency [9, 20]. Furthermore, video away from the user's expected centre of gaze may be encoded at lower quality, further reducing data size [25]. Overall, algorithms aiming at satisfying the motion-to-photon latency often include Deep Learning components converting users' orientational data to useful outputs, such as how to compress visual data. Training and testing these Deep Learning algorithms is a notoriously data-hungry process [6, 14]. Furthermore, extensive evaluation of full algorithms again requires massive amounts of orientational data.\nWhile the aforementioned algorithms are the most ubiquitous consumers of orientational data in the field of XR, needs for substantial orientational data sources arise in other situations as well. For truly wireless interactive XR, where content is generated off-device and streamed in real-time over the air, frequencies in the millimetre-wave band (30 GHz to 300 GHz) or higher are needed to stream content in extremely high quality [15, 34]. To guarantee sufficient signal strength in these frequency ranges, communication must be beamformed between the sender and receiver, rather than being sent and received omnidirectionally [39]. In addition, in the field of Redirected Walking, non-deterministic mappings between real-world motion and virtual motion are applied to avoid real-world collisions without restricting virtual freedom [33]. High-performance solutions in both of these cases again require enormous amounts of orientational data for training and evaluation."}, {"title": "RELATED WORK", "content": "While the intersection of orientational data collection and synthetic data generation is still in its infancy, the two fields separately are well-developed. This section provides an overview of the two."}, {"title": "XR pose datasets", "content": "Over the years, a wide array of datasets containing orientations or poses (i.e., locations plus orientations) of XR users have been made available to the community. In this overview, we only consider works presenting novel datasets (i.e., not compiled from previous works) at a reasonably high sampling rate, which are, at the time of writing, readily available online.\nCorbillon et al. present an orientational dataset sampled at 45 Hz gathered from 59 subjects shown 6 minutes of 360\u00b0 video [12]. Lo et al. collected a 30 Hz dataset containing orientations along with saliency maps, identifying objects that attract attention, using 10 minutes of 360\u00b0 video shown to 50 subjects [24]. The set of videos contains both recorded and pre-generated content, further subdivided into slow-paced and fast-paced content. Li et al. showed 221 test subjects a 1 to 2 minute sequence of videos, repeated 10 times, gathering orientation and gaze direction at 48 to 60 Hz [23]. They then used this as input to a Deep Learning model predicting perceived visual quality. Next, a dataset by Wu et al. contains the full pose at 100 Hz for 48 users exposed to nearly 90 minutes of video [40]. For the AVtrack orientational dataset, Fremerey et al. showed 10 minutes of video to 48 subjects [17]. With only 10 Hz and an angular precision of 1\u00b0, this dataset is tailored more towards investigating longer-term behaviour of subjects. In Nasrabadi et al.'s dataset, orientation was measured at 60 Hz from 14 minutes of video shown to 60 subjects [29]. Xu et al. showed 35 minutes of video to 58 subjects, recording orientation and gaze at 60 Hz [41]. Next, Hu et al. recorded orientation and gaze at 100 Hz with 30"}, {"title": "Time Series Generation", "content": "Once a reasonable amount of data is gathered using test subjects, generating synthetic, but realistic data may be necessary to obtain a sufficiently large dataset for some application. For this, some approaches have been proposed. One such approach is the classical Smith's algorithm, designed for generating instantiations of a wireless channel model [32, 44]. In essence, it considers the time series to be generated as a set of signals. Known samples are converted to frequency space using the Fourier transform. Then, random noise sequences are weighted using filter coefficients, resulting in frequency coefficients similar to those of the known samples. Converting back to the time domain using the Inverse Fourier transform results in realistic synthetic signals. Recently, Blandino et al. generated synthetic head rotation traces using this approach [5]. However, they only considered the mean power spectral density in the model, meaning the distribution between samples is lost. We will compare our solution to this Fourier-based solution using the authors' publicly available code.\nData generation has also seen significant attention from the Deep Learning community, where the GAN is generally considered to be the prime candidate [18]. Recently, Martin et al. applied this approach to scanpath (i.e., a sequence of gaze directions) generation, a field adjacent to head rotation generation [26]. As a regular GAN is not time series-aware, the authors added compatibility by using Dynamic Time Warping (DTW), a measure for similarity between time series, as a loss function. Other approaches for inserting general time series compatibility into a GAN model have been proposed [16, 27], with current state of the art being TimeGAN [43]. These approaches are covered extensively in the next section."}, {"title": "METHODOLOGY", "content": "This section outlines our methodology for generating realistic synthetic orientational time series using TimeGAN. We will illustrate"}, {"title": "TimeGAN", "content": "Classical approaches to data generation are often model-based, meaning that transferring the generator from one source dataset to another requires expert input, and may necessitate significant changes in design. As such, more general, model-free approaches are desirable. GANs are generally considered to be the prime candidate for this [18]. A GAN is a general design of a Deep Learning agent, where two sub-systems interact adversarially, to eventually generate samples matching the distribution of some source dataset. The generator receives random noise as inputs, and converts these to synthetic samples of the desired dimensions. The discriminator is a classifier, which, through supervised training, learns to classify samples as real (i.e., from the source dataset) or fake (i.e., from the generator). While the generator, which cannot access the source dataset, will initially output essentially random noise, it is given access to the loss function of the discriminator, meaning it can adapt its output to maximise that loss (i.e., make it more difficult for the discriminator to distinguish between fake and real). Interactively, the discriminator pushes the generator to produce more realistic samples, in turn encouraging the discriminator to discover more subtle differences between real and fake. The exact implementation of the two sub-systems depends on the type of data to generate.\nIn arguably the most well-known application of GANs, generating fake images, these are constructed using Convolutional Neural Networks [22]. When samples are time series however, extra care must be taken to ensure that the time-correlation between different time points within a time series is maintained. To this end, TimeGAN introduces significant augmentations to the GAN system [43]. Specifically, TimeGAN's discriminator and generator consist of Gated Recurrent Units (GRUs), a component capable of considering time-dependencies. Furthermore, embedder and recovery sub-systems are added, which respectively encode a time series to a latent space of lower dimension, and decode this latent space back to the original space. These two are trained first, and then the generator produces samples in this latent space, to be converted to time series through recovery. This reduces the complexity of the generator's task to a more practically feasible level. Finally, a supervised learning step is introduced. In this step, the generator is made to complete (latent representations of) incomplete time series from the source dataset. A loss function, measuring the distance between the generated time steps and the actual time steps from the source dataset, further encourages the generator to learn the time-correlation within time series. Regular adversarial learning and this supervised learning are performed alternatingly, and their loss functions are implemented as cross-entropy loss and mean squared error, respectively. The system is summarised in Figure 1.\nTimeGAN was shown to outperform earlier GAN-based approaches on financial and energy datasets in its initial presentation, and has since been applied successfully to medical data [13]. In this paper, we use the TimeGAN approach based on the initial authors' code\u00b2."}, {"title": "Data Inspection and Preparation", "content": "We mainly evaluate our approach using the dataset in [10]. We select this dataset to enable direct comparison with [5], based on the same dataset. As this iteration of our work focuses on orientational data, we disregard the positional data in the dataset entirely. Remember that the dataset contains 18 orientational traces of 2 minutes each, performed by 3 test subjects, sampled at 250 Hz, provided in yaw-pitch-roll format. We maintain this representation as it is easily interpretable. Figure 2 shows the Probability Density Function (PDF) of the yaw, pitch and roll, quantized into buckets 10\u00b0 wide, with the middle bucket centered around 0\u00b0. Clearly, yaw motion is significantly more pronounced. This is unsurprising, as points of interest in a virtual world are usually distributed in roughly a horizontal plane around the user. Furthermore, turning around one's axis or looking to the side is more comfortable than looking up/down or tilting one's head. Because the virtual experience was an indoor environment, users' gaze was generally aimed at one of the walls, explaining the local maxima around -90\u00b0, 0\u00b0 and 90\u00b0. While the normal distributions of the pitch and roll are easily generated by neural networks, the yaw's distribution may be more challenging on two fronts: its multiple peaks, along with discontinuities through time, whenever the representation rolls over between 180\u00b0 and 180\u00b0. To combat the data's non-normality, we transform the data non-linearly using a quantile transformer, forcing a normal distribution [1, 3]. Experimentation showed this to be more effective than power transforms such as Box-Cox [7] and Yeo-Johnson [42] for this application. To resolve the discontinuities, we simply shift the remainder of a time series by 360\u00b0 whenever a discontinuity occurs. While this means we can no longer ascertain the exact data range a priori, data does remain well within reasonable bounds in practice. We emphasise that all transformations are reversible, and that synthetic data is transformed to the original representation before comparison with the source dataset.\nNext, we discuss how to arrange the time series into an appropriate format for synthetic generation. Originally, the data is divided into eighteen time series of 2 minutes at 250 Hz, resulting in 30 000"}, {"title": "TimeGAN Tuning", "content": "samples per time series. As the GAN's training time and difficulty scale with the input size, and Deep Learning traditionally requires many distinct samples, subdividing the samples is inevitable. In line with experiments in the original TimeGAN paper, we propose to subdivide each time series into smaller instances of 25 samples each, using a sliding window. In addition, we downsample the data. We argue that downsampling does not significantly reduce the utility of the dataset. Intuitively, a person can only perform a few distinct head rotations per second. Furthermore, the law of inertia implies that these motions will be relatively smooth, and therefore easily recreated accurately through simple interpolation techniques. For a more rigorous justification, we first refer to the power spectral density estimations of the data, presented in [5]. Energy is focused around the lowest frequencies, with over 90% at 5 Hz and lower. As per the Shannon-Nyquist sampling theorem, this information will be maintained when downsampling to 10 Hz. Additionally, we investigate empirically how well the original data can be recreated after downsampling. We downsample the full dataset for different downsampling factors, then attempt to upsample back to the original frequency using a simple cubic spline interpolator.\nBased on the analysis above, we decide to downsample the dataset to 16.67 Hz, a downsampling factor of 15. This results in separate time series of 1.5 seconds each. Figure 4 shows one such sample, arbitrarily selected. We expect samples of this length to be sufficient for most applications, as prediction horizons for dynamic encoding and beamforming are usually in the order of 100 ms [4]. For cases where longer samples are desirable, one could fuse several samples together. We leave this for future work.\nThe dataset, subdivided into 23 700 1.5 second samples of 25 time points each, is provided to the TimeGAN, where the quantile transformer is fit. For proper operation, the TimeGAN's hyperparameters"}, {"title": "Evaluation Metrics", "content": "Once a synthetic dataset is generated, some way of evaluating how well its distribution matches the original dataset's is needed. Determining this both accurately and interpretably is notoriously difficult. Some commonly used approaches are dimensionality reduction through t-SNE or PCA, enabling visual comparison. Another is TSTR, where some neural network taking input data of the type under consideration is trained using only synthetic data, then evaluated using real data. With these approaches it is however difficult to determine what qualifies as \"good enough\", making them primarily useful when comparing data generators. For this case specifically, the dataset itself is fortunately easily interpretable. Therefore, we determine a number of metrics which together interpretably characterise the relevant features of the dataset. We propose the following metrics:\n\u2022 Orientation distribution: separate PDFs of the yaw, pitch and roll will show whether the distribution between viewing directions is maintained. We consider every time point within one sample as a separate data point, rather than taking the mean of each sample, as to avoid masking information for discontinuous samples.\n\u2022 Per-sample motion distribution: the distribution of the range (i.e., difference between maximum and minimum value) of yaw, pitch and roll shows whether time-correlation is maintained properly. The distribution of slower-moving and more rapid samples should ideally be maintained.\n\u2022 Autocorrelation of velocity: The autocorrelation of the velocity (i.e., first derivative) of yaw, pitch or roll reveals whether the \"smoothness\" of the motion is maintained.\n\u2022 Cross-correlation of velocities: The cross-correlation of velocities quantifies time-correlation of motion between different axes. Intuitively, \u201cdiagonal\" motion (i.e., not on only one axis) should cause such time-correlation. Ideally, this should be maintained in synthetic data, but can only be expected to be maintained if the three features are not generated independently.\nWe will evaluate our approach using these metrics. In addition, we also generate PCA and t-SNE plots, to investigate whether differences in synthetic dataset quality revealed by the above metrics are also visible in these plots."}, {"title": "EVALUATION", "content": "To obtain a synthetic dataset using TimeGAN, we trained the system for the full 1250 epochs, generating a dataset every tenth epoch. We manually inspected each result using the described metrics and selected the best-performing option. We stress that this should not be considered as overfitting. When training a network for, say, prediction, such an approach is undesirable. This would overfit the predictor to the evaluation dataset, which does not necessarily lead to a well-performing predictor for new data once the system is deployed. In this case however, the generated dataset is the final result of the system, and the system will not be presented with"}, {"title": "Head rotation metrics", "content": "We now evaluate each of the novel metrics. Note that, when plotting distributions, values are quantized into buckets 10\u00b0 wide.\nOrientation distribution. We first analyse the distribution of the raw yaw, pitch and roll values. We consider every time step of every sample as a separate data point and plot their distribution in Figure 5. The distribution of roll values is rather limited, and both synthetic datasets match it closely. Roll motion requires tilting one's head, which is rather uncomfortable. The pitch's distribution is slightly wider, and here the FFT fails to match the distribution closely, while the TimeGAN again closely matches the distribution. With the complicated yaw distribution, this occurs to an even greater extent. The three peaks are closely matched by the TimeGAN, while they are significantly less pronounced for the FFT, despite only the latter being hand-crafted to match this distribution.\nMotion distribution. Figure 6 illustrates how well the motion of the synthetic dataset matches that of the original dataset. The FFT shows a major peak at very low motion, while the TimeGAN again closely matches the original dataset. We do note that, when"}, {"title": "Autocorrelation of velocity", "content": "considering yaw, very-low-motion samples are underrepresented in the TimeGAN dataset. We hypothesise that, when the motion distribution is broad, generating these very-low-motion time series is inherently challenging with the TimeGAN. The time series is generated per-step, and to form a very-low-motion time series, every next step's value must be very close to the previous. A noteworthy deviation for even a single time step increases the overall motion in the entire time series. Even if the probability for generating values close to the previous ones is reasonably high, the probability of this happening for every time step in the 25-step sequence will be low. Further evidence for this hypothesis is that when motion is consistently low, the TimeGAN easily generates this motion distribution accurately. Overall, we argue that this phenomenon is inevitable with the current approach, and leave potential solutions for future work. We do note that very-low-motion samples are less valuable. Applications such as viewport-dependent encoding or beamforming are mainly challenging under higher motion. We argue that failure to generate higher-motion samples, as occurs with FFT, is significantly more damaging to the data's utility. This failure occurs at least partially due to the FFT being designed to consistently match the distribution of the mean time series, rather than the distribution of all time series.\nIn a realistic time series, velocity changes gradually, largely due to the law of inertia. Intuitively, the autocorrelation, a measure of similarity between a time series and a time-lagged copy of itself, should decrease gradually as the time lag increases. This has been observed in previous work [4], and Figure 7 also clearly shows this behaviour for the original dataset. With the"}, {"title": "Cross-correlation of velocity", "content": "TimeGAN, the autocorrelation for pitch and roll is matched closely. With yaw, it decreases somewhat more rapidly, which is likely a side-effect of the lack of very-low-motion samples discussed above. For the FFT, the autocorrelation decreases significantly more rapidly, becoming near-zero earlier. One may expect autocorrelation to be high with the FFT, as the generated signals are of low frequency, making them inherently relatively smooth. We hypothesise that this result is at least in part due to the lower overall motion in that dataset, meaning the effect of minor perturbations is more noticeable in the (normalised) autocorrelation.\nIntuitively, one would expect the yaw, pitch and roll to display at least some correlation, as humans do not naturally only rotate their heads along one axis at a time. Any \"diagonal\" motion results in motion on multiple axes, while holding one's head still results in no motion on each axis. Indeed, Figure 8 confirms this intuition: the high correlation at 0 time lags shows that high motion along one axis implies a high likelihood of high motion along another. As the number of time lags increases, the correlation subsides: motion along one axis has little effect on the motion along another axis half a second later. This holds for both the original dataset and the TimeGAN, where the cross-correlation is even slightly higher. With the FFT however, this behaviour does not occur at all. The cross-correlation is near-zero for every time lag, meaning that there is no correlation between the different axes. This is unsurprising, as the FFT generates data for the three axes entirely independently from each other. Clearly, this results in a significantly less realistic dataset."}, {"title": "Generality", "content": "While we have provided some intuition for the generality (i.e., dataset-independence) of this approach, we provide further evidence by repeating the evaluation with another dataset. We selected the dataset by Lo et al., discussed in Section 2.1, for its broad range of content. This dataset contains 50 30 Hz traces of 10 minutes each, each gathered by another user. Without further hyperparameter tuning, we applied the same process to generate a synthetic dataset as above. These results again indicate a close match between the source and synthetic datasets. The under-representation of very-low-motion samples again occurs, but no other issues appear. We consider this as a strong indication of the generality of this approach. Intuitively, this is unsurprising, as the TimeGAN had no knowledge of any of our metrics. The only feedback available to the generator is the discriminator's loss. Without any additional steering, the generator learned from only this information source to match the original dataset very well."}, {"title": "Discussion", "content": "Based on the analysis above, we discuss the utility of the TimeGAN, compared to the FFT on several fronts."}, {"title": "Dataset-specific manual design", "content": "with the FFT, an expert needs to analyse the dataset, determining models for the distributions within the time series. In [5], it is claimed that the model is general, as it can be adapted to another dataset by simply changing the parameters of the distributions. We note that this may not necessarily be the case. As the dataset was recorded in an indoor virtual experience, the yaw is approximated with a multi-modal Gaussian distribution, with their means representing the directions towards the room's walls. We do not expect this distribution to apply well to an outdoor virtual environment, where the user would likely look in any direction with similar probability. In applying TimeGAN however, no such dataset-specific design was applied. The model converts the source dataset to have a normal distribution, meaning it is insensitive to the distributions within the source dataset. The only type of tuning needed was hyperparameter tuning. During this process, we observed that the quality of the synthetic dataset was not particularly sensitive to the hyperparameters. Furthermore, no additional tuning was needed for the second dataset in Section 4.2. Determining the optimal dataset from the different snapshots does require manual intervention, however this essentially comes down to comparing the plots of the metrics presented above, which can be performed rapidly by an individual with no expert knowledge, and could even be automated to an extent."}, {"title": "Runtime.", "content": "the FFT is relatively fast, requiring only minutes of computation on a regular workstation computer. GANs however are notoriously computationally inefficient. Training our model on a workstation computer may take over 10 hours. We do note that this high runtime is not prohibitive for this application. Training needs to occur only once, after which the model can output new sequences in a matter of seconds."}, {"title": "CONCLUSIONS", "content": "Overall, we are convinced that this approach is capable of producing a large array of highly usable head rotation samples regardless of the specific head rotation distribution, for applications such as proactive viewport-dependent streaming and XR beamforming.\nIn this paper, we presented a novel approach for generating synthetic head rotation data for Extended Reality applications. We showed that, unlike the only other approach currently described in the literature, our TimeGAN-based approach is able to generate realistic data according to a range of metrics which together characterise the head rotation data. Our metrics incorporate where the users look and how they turn their heads. We expect this approach to be valuable to researchers in several XR-related fields, including dynamic multimedia encoding and millimetre-wave beamforming. As such, we commit to releasing an open-source implementation of the system by this paper's publication date. In future work, we intend to reduce TimeGAN's tendency to generate datasets where very-low-motion samples are underrepresented in case of a wide motion distribution."}]}