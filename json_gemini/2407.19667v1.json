{"title": "Smart Language Agents in Real-World Planning", "authors": ["Annabelle Miin", "Timothy Wei"], "abstract": "Comprehensive planning agents have been a long term goal in the field of artificial intelligence. Recent innovations in Natural Language Processing have yielded success through the advent of Large Language Models (LLMs). We seek to improve the travel-planning capability of such LLMs by extending upon the work of the previous paper TravelPlanner (Xie et al., (1)). Our objective is to explore a new method of using LLMs to improve the travel planning experience. We focus specifically on the \u201csole-planning\" mode of travel planning; that is, the agent is given necessary reference information, and its goal is to create a comprehensive plan from the reference information. While this does not simulate the real-world we feel that an optimization of the sole-planning capability of a travel planning agent will still be able to enhance the overall user experience. We propose a semi-automated prompt generation framework which combines the LLM-automated prompt and \"human-in-the-loop\" to iteratively refine the prompt to improve the LLM performance. Our result shows that LLM automated prompt has its limitations and \"human-in-the-loop\" greatly improves the performance by 139% with one single iteration.", "sections": [{"title": "Introduction", "content": "The history of artificial intelligence (AI) and large language models (LLMs) in constraint-based planning dates back to the early days of AI research. Constraint satisfaction problems (CSPs) have been utilized for various applications including scheduling, resource allocation, and problem-solving. Early works by researchers such as Dechter and Pearl (2) laid the foundation for understanding the theoretical underpinnings of CSPs and developed algorithms for efficient problem-solving within these frameworks. More recently LLMs have significantly enhanced the capability and flexibility of constraint-based systems.\nLarge language models, such as GPT-3 and its successors, have revolutionized the field by enabling more natural and flexible interactions with constraint-based planning systems. Recent studies by Brown et al. (3) and Bommasani et al. (4) have demonstrated the potential of these models ranging from natural language understanding to decision-making support systems. The integration of LLMs with constraint-based planning can better accommodate complex user requirements and preferences.\nOur work focuses on enhancing the ability of LLMs to achieve goals under constraints. This includes systematically analyzing current benchmarks, proposing a general framework for improving LLMs, automating constraint understanding based on system resources and implicit human feedback, and improving the travel planner benchmark. Our advancements demonstrate the potential for achieving human-level performance with LLM automated prompt plus minimal human intervention."}, {"title": "Related work", "content": "A multi-constraint problem can be challenging for humans to solve, let alone a large language model. Developing a framework around the types of constraints and categorizing them into key basic types dramatically simplifies the process of decomposing the problem and the data needed to train, validate and test the model. It is useful to bucket the constraints into 1) environment constraints or constantly changing inputs due to real world conditions such as flight availability on certain dates 2) common sense constraints or what a reasonable person might deem to make sense such as limiting driving to 8 or 10 hours a day and 3) hard constraints or well defined requirements such as a maximum budget (Xie et al.,(1)).\nIt also makes it easier to set and evaluate pass and fail criteria by bucketing. While this evaluation (and prompt improvement) may also be automated, getting to the correct or acceptable solution purely through automated iterative experimentation can be very time consuming and inefficient. Using a \"human-in-the-loop\" system (Xin et al., (5)) has the potential to dramatically speed things up, sometimes up to 10x, versus typical iterative workflows. Whereas Xin et al., does intermediate results reuse and introspection between iterations, automates background search during think time, and automates cues for iterative changes, we take a different approach by using human failure analysis to identify model weaknesses and use that to directly improve the automatically generated prompt."}, {"title": "Methods: Framework for improving LLMs", "content": "In this paper, we propose a general framework for creating an effective prompt for LLMs and apply it to the travel planning use case (1) where a travel plan is generated with the help of LLMs. The framework consists of two main steps as depicted in Figure 1. The 1st step is creating an initial prompt by LLMs through its automatic summarization of external resources such as python evaluation script and reference data in our case. This automatic step reduces labor intensity when creating the initial prompt for use. The 2nd step is further improving the prompt through prompt-tuning with human-in-the-loop where manual failure analysis identifies failed plans in the training data and addresses agent faults in the prompt such as adding a corrected plan for the failed plan to the prompt. This 2nd step is an iterative process to optimize the prompt using the training data to generate prompt \\(R_i\\) at iteration i. The iteration can stop when the performance converges between consecutive iterations.\nThe initial automated prompt mainly covers the extracted rule and the improved prompts focuses on picking up the tricky failure cases, but it is still an open question whether just simply concatenating them is the right way to make LLM yield better result. Humans can analyze the extracted rules and failure examples and re-organize or condense the prompt to avoid some side effects, like lengthy content, etc. For that reason, we also manually created a prompt and use its performance to serve as a gold standard for comparison. It gives us the opportunity to measure the gap in performance between"}, {"title": "Experiments", "content": "The set-up for our experiments consisted of selecting an LLM to use to automatically generate the constraints in step 1 and selecting an LLM to use for the travel-planning agent. We decided to use GPT-4 Turbo as the base travel-planning agent model due to its high performance as the state-of-the-art language model as well as its easy accessibility through its API. We chose to utilize GPT-40 to automate the generation of the constraints due to it being a cheaper option than GPT-4 Turbo given that there is a free quota every day. Additionally, by utilizing the GPT models, we used the OpenAI API that outsources the computing hardware required."}, {"title": "Structured Reference Information", "content": "Previous papers' trials of the travel-planning agent utilized reference information in the form of raw JSON (1). While the JSON itself is structured, the information contained within each JSON key was actually in CSV form, so we felt that restructuring the reference information as a series of CSV files would allow the travel-planning agent to perform better. To this end, we created a script that converted each raw JSON reference information into a list of CSV files. Then, we appended this structured reference information to each prompt to achieve better results."}, {"title": "GPT-4 Turbo Baseline", "content": "To obtain a baseline to compare our results to, we utilized the TravelPlanner (1) paper's Github repository, utilizing its evaluation scripts and plan generation datasets. We call these results the GPT-4 Turbo Baseline. Additionally, we used their zero-shot prompt, with unstructured reference data, as a baseline to compare to."}, {"title": "Automation Module", "content": "To automate the prompt rule generation, we used GPT-40 to summarize the evaluation script's rules and created a prompt without examples to avoid mismatches with the evaluation criteria, which we call the Initial GPT-40 Generated Prompt. To further enhance the prompt, we added a manually selected example from the TravelPlanner training dataset's most challenging plans, resulting in a prompt combining automated rules and a curated example. This result is called the Improved GPT-40 Prompt after 1st iteration. While we only ran this automation module loop once, we believe that further loops would help increase performance further."}, {"title": "Manual Prompt Creation", "content": "After analyzing the GPT-4 Turbo Baseline results, we identified and included poorly performing constraints in the prompt, leading to significant performance improvements. This resulted in the \"gold standard\" for our automation module to aspire to, which we call the GPT-4 Turbo Manually Modified Prompt."}, {"title": "Evaluation", "content": "To evaluate our travel-planning agents, we utilize the TravelPlanner paper's evaluation algorithm. This algorithm evaluates the plan generated by the TravelPlanner using a number of constraints, split between Commonsense Constraints and Hard Constraints. Commonsense Constraints consist of those constraints that a traveler is not required to make, but will naturally make. For example, one commonsense constraint could be to require Diverse Attractions, as it would not make sense for multiple days to visit the same attraction. Hard Constraints, on the other hand, are things that human travelers are required to follow. These include constraints such as the budget for the trip as well as abiding by all the room rules of each accommodation that is booked."}, {"title": "Data Splits", "content": "We utilize the data splits from TravelPlanner (1), and these splits can be obtained in our GitHub. Specifically, to evaluate our results, we utilize the validation split with 180 queries, and to manually fine-tune as well as fine-tune the automated prompt, we utilize the 45-query training split."}, {"title": "Results", "content": "Table 1 below shows the passing rates. Row \"GPT-4 Turbo Baseline\" shows the result from Trav-elPlanner (1). The next three rows contain the results from manually modified prompt, initial GPT-40 automated prompt and the improved prompt after 1st iteration with \"human-in-the-loop\"."}, {"title": "Limitations", "content": "Our framework relies mostly on automation to generate prompts and evaluate the results. It also uses human-in-the-loop as the feedback mechanism. There are a few key limitations to our framework. First, the reference data in (1) is limited in terms of the selection of travel information, accommodations, restaurants and attractions. We didn't have sufficient time to include a more diverse set of reference data for the agent to use. Second, we used our own evaluation script to assess the efficacy of the resultant LLM generated travel plans; however, LLMs don't inherently have this capability built-in. Third, while we have shown that this framework is applicable to travel planning, we have not tested it with other use cases (i.e. such as planning a high school or college student's multi-year course load given budgetary, degree requirements and other constraints). Finally, human-in-the-loop is inherently not scalable due to the manual nature of the analysis, and another LLM agent should be designed for alleviating this drawback."}, {"title": "Conclusion", "content": "We developed a framework of using LLMs with a \"human-in-the-loop\" feedback mechanism to automate and solve constraint-based problems. We evaluated this framework with the travel planning problem (1). The LLM-automated prompt has poor performance. We are able to improve the pass rate by 139% by one single \"human-in-the-loop\" iteration which produces result on-par with the baseline and closer to the human manually created prompt. While there are limitations with this framework, we believe that this framework has great potential to achieve more accurate results and has broader applicability to solve constraint-based problems which make it a worthwhile endeavor. Aside from travel planning, the \"human-in-the-loop\" approach has far-reaching potentials such as enhancing image recognition for wildfire detection and optimizing wearable monitoring devices for more precise heart rate and SpO2 readings. The possibilities are truly endless; this novel approach to improve LLM promises to simplify decision-making across various industries, leading to a more efficient and equitable future where data-driven insights benefit everyone."}]}