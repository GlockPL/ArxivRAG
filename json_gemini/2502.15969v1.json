{"title": "Forgotten Polygons: Multimodal Large Language Models are Shape-Blind", "authors": ["William Rudman", "Michal Golovanesky", "Amir Bar", "Vedant Palit", "Yann LeCun", "Carsten Eickhoff", "Ritambhara Singh"], "abstract": "Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of \"sides\" nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-40's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMS remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning.", "sections": [{"title": "1 Introduction", "content": "As Multimodal Large Language Models (MLLMs) demonstrate success in vision-language tasks (Deitke et al., 2024; Chen et al., 2025), researchers seek to analyze their underlying mechanisms (Golovanevsky et al., 2025; Jiang et al., 2024; Luo et al., 2024) and evaluate their ability to perform generalized problem-solving compared to human reasoning (Cherian et al., 2024). A key aspect of human reasoning is the interplay between intuitive, reflexive thinking (known as System 1) and deliberate, logical reasoning (known as System 2) (Evans, 2008; Evans and Stanovich, 2013; Kahneman, 2011). While humans engage in both types of reasoning, a key question arises: Can MLLMs move beyond memorized responses of System 1 to engage in the analytical reasoning of System 2?\nMathematics provides a powerful framework for this investigation, as fundamental reasoning skills are crucial for complex problem-solving. The combination of logic, abstract concepts, and specialized symbolic language makes mathematical reasoning problems a strong test of the extent to which models engage in System 2 reasoning rather than relying solely on memorized concepts (Satpute et al., 2024). Studying System 2 reasoning abilities is especially relevant given recent progress in natural language processing, where Large Language Models (LLMs) like GPT-4 (Achiam et al., 2023), Qwen (Bai et al., 2023), and LLaMA (Touvron et al., 2023) have demonstrated notable success on mathematical benchmarks like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). Despite these advancements, MLLMs remain limited in visual-mathematical reasoning. While closed-source models like GPT-40 demonstrate potential, their performance on benchmarks such as MathVerse (Zhang et al., 2024a) remains far below human levels, with open-source models typically performing even worse (Zhang et al., 2024a). Even MLLMs specially trained for visual, mathematical reasoning (G-LLaVA (Gao et al., 2023), Math-PUMA (Zhuang et al., 2024), Math-LLaVa (Shi et al., 2024) cannot close the gap between open-source and closed-source models. Many benchmarks feature complex geometric diagrams that require multi-step reasoning, with human performance reaching no more than 70% accuracy (Wang et al., 2024a; Zhang et al., 2024a). Cherian et al. (2024) show that MLLMs tend to perform better on more complex mathematical tasks but struggle with simple tasks designed for young children. This counterintuitive result raises concerns about whether MLLM performance on visual-mathematical benchmarks reflects genuine mathematical reasoning (System 2) or merely the retrieval of familiar concepts from training data (System 1).\nWe design a two-part task to evaluate MLLMs\u2019 reasoning abilities: (1) shape recognition, which relies on visual recall (System 1), and (2) side counting, which engages visual reasoning (System 2). Starting with common geometric shapes and properties allows us to pinpoint specific failures in how MLLMs process mathematical diagrams. Our results show that while the underlying LLMs demonstrate perfect accuracy on non-visual questions about polygon names and side counts, their multimodal counterparts fail when models have to rely on images to answer questions correctly. Unlike humans, who can count a shape's sides and infer its identity (e.g., \u201c1, 2, . . ., 7\u201d \u2192 \u201cheptagon\"), our results suggest that MLLMs do not attempt such reasoning. Instead, they rely on System 1 heuristics like shape memorization and spurious correlations between visual features and labels, ignoring vital information in the image.\nTo understand why MLLMs struggle with polygon identification from visual inputs, we analyze vision encoder embeddings. Our results show that vision encoders are \u201cshape-blind\u201d: common shapes form distinct clusters, while less frequent ones like pentagons, heptagons, and octagons overlap (Section 3.1.2). Even models specifically trained for geometric understanding, such as G-LLaVA, Math-LLaVA, and Math-PUMA, exhibit shape-blindness where distinct polygons are embedded in the same region in vector space. We further demonstrate vision encoders' shape-blindness with a two-shape multi-step task: identifying shapes, counting the sides, and computing the total sum of sides, adding complexity by incorporating arithmetic into the reasoning process. On average, models correctly identify both shapes in an image only 27.58% of the time, as shape recognition depends entirely on visual input. However, MLLMs compute the sum of the identified shapes' sides correctly in 70.75% of cases, indicating that despite errors in recognition, the summation operation remains highly accurate. While open-source models struggle with less common polygons, GPT-40 identifies shapes with at least 49% accuracy. To test whether models use System 2 reasoning, we introduce a dataset of abstract shapes and irregular polygons unlikely to have appeared in training (see Figure 1). No model accurately counts the sides of these novel shapes, reinforcing their reliance on memorization rather than genuine geometric understanding.\nAs MLLMs continue to advance, our findings urge the community to re-examine the complexity of visual-mathematical benchmarks. If state-of-the-art open-source models fail to recognize simple shapes, should we be evaluating them on complex geometric tasks? Our results suggest that current models rely heavily on factual recall rather than true geometric reasoning, limiting their ability to generalize beyond familiar shapes. To explore ways of bridging this gap, we take a step forward with Visually-Cued Chain-of-Thought (VC-CoT) prompting, leveraging the fact that many geometric datasets already contain annotated shapes (e.g., labeled triangle vertices A, B, and C). By explicitly guiding the model to reference image annotations (letters or numbers) and reason about their relationships, VC-CoT significantly improves GPT-40's ability to count the sides of novel shapes, boosting accuracy from 7% to 93%. Across Molmo, Janus-Pro, GPT-4-Turbo, and GPT-40, our VC-CoT prompts achieve an average accuracy improvement of approximately 5% compared to standard CoT prompts on MathVerse. These findings highlight the importance of structured prompting in strengthening the connection between visual perception and mathematical reasoning in MLLMs.\""}, {"title": "2 Related Works", "content": "Despite the success of MLLMs, emerging research highlights their struggles with reasoning. Alhamoud et al. (2025) find that vision-language models struggle to negate in retrieval and multiple-choice tasks. Beyond general reasoning, open-source, closed-source, and math-specific MLLMS perform poorly on visual math benchmarks (Wang et al., 2024a; Zhang et al., 2024a).\nA key limitation in MLLMs is the vision encoder. Tong et al. (2024) show CLIP-based vision encoders fail to capture fine-grained details \u2014a detrimental trait for tasks requiring precise visual reasoning. Similarly, in the visual-mathematics domain, Gao et al. (2023); Shi et al. (2024); Zhang et al. (2024b); Zhuang et al. (2024) demonstrate that vision encoders produce inadequate representations of mathematical diagrams and vision-text misalignments hinder multimodal reasoning.\nOne way to address these challenges is fine-tuning. Recent works adopt a multi-step approach: (1) fine-tuning vision encoders to enhance visual representation, (2) training modality projectors for better alignment, and (3) instruction-tuning with CoT datasets (Gao et al., 2023; Shi et al., 2024; Zhang et al., 2024b; Zhuang et al., 2024). While this may increase performance on math benchmarks, studies show that fine-tuning foundation models often reduces generalization ability and does not address overarching reasoning capability (Yang et al., 2024; Chu et al., 2025).\nIn the language domain, Chain-of-Thought (CoT) prompting has proven highly effective in encouraging System 2 reasoning in LLMs (Wei et al., 2023). Xiang et al. (2025) further reinforces this, demonstrating that structuring reasoning through CoT can significantly improve logical inference and problem-solving. However, applying CoT to MLLMs has been far less successful. Zhang et al. (2024c) show that open-source MLLMs struggle with CoT, largely due to limitations in existing visual-instruction tuning datasets, which prioritize short, simplistic responses over structured reasoning. Thus, recent works explicitly fine-tune MLLMs for CoT reasoning, particularly in object counting and mathematical reasoning tasks (Zhang et al., 2024c,b; Zhuang et al., 2024; Deitke et al., 2024). However, even these fine-tuned models continue to struggle on complex visual-mathematical benchmarks such as MathVista (Lu et al., 2024), and MathVerse (Zhang et al., 2024a). These challenges highlight that despite recent efforts, System 2 reasoning in MLLMs remains an open problem, with current approaches failing to achieve generalizable reasoning abilities."}, {"title": "3 Experiments", "content": "Models Evaluated: In our experiments, we evaluate 13 diverse MLLMs. We consider (1) general open-source models, including LLaVA-1.5-7B (Liu et al., 2024), LLaVA-Next-7B (Li et al., 2024b), LLaVA-OneVision-7B (Li et al., 2024a), Qwen2-VL-7B (Wang et al., 2024b), InternVL-8B (Wang et al., 2024b), Molmo-7B (Deitke et al., 2024), and DeepSeek's Janus Pro-7B (Chen et al., 2025); (2) math-specialized open-source models, including Math-LLaVA-13B (Shi et al., 2024), G-LLaVA-7B (Gao et al., 2023), and Math-PUMA-7B (Zhuang et al., 2024); and (3) closed-source models, including GPT-4-Turbo and GPT-40. See Table 6 in Appendix A for full evaluation details.\n3.1 Probing Geometric Knowledge\nGeometric reasoning provides a natural testbed for distinguishing between System 1 heuristics, such as memorization of common shape patterns, and System 2 reasoning, which requires deliberate reasoning, such as counting sides of a shape. To examine this distinction, we create a dataset comprising six common regular polygons: triangle, square, pentagon, hexagon, heptagon, and octagon (see examples in Figure 6 in Appendix B.1). For each shape, we generate images with different colors, rotations, and sizes, creating a dataset of 2000 images. All images are 400\u00d7400 pixels, and we ensure that even the smallest shapes occupy at least 15% of the image (more details in Appendix Section B). Our analysis follows a three-stage approach to disentangle the contributions of vision, language, and their integration in MLLMs. First, we assess the MLLM as a whole, evaluating its ability to connect visual information to geometric properties (Section 3.1). Next, we prompt the underlying LLMs in a text-only setting to determine their knowledge independent of vision (Section 3.1.1). Finally, we analyze the vision encoder by examining the vision embeddings, shedding light on what the model \"sees\u201d without language guidance (Section 3.1.2).\nFor each image, we use two prompts to test for geometric knowledge: \"What shape is in the image?\" and \"How many sides does the shape in the image have?\". Table 1 shows that MLLMs excel at identifying common shapes like triangles and squares, with many models achieving perfect accuracy. However, performance drops sharply for less familiar polygons such as pentagons, heptagons, and octagons. Most models fail on heptagons, with only GPT-40 and Molmo exceeding 1% accuracy. Notably, Molmo is trained with a point-then-count approach, identifying objects before quantifying them. Despite this targeted training, Molmo correctly counts pentagon sides only 7% of the time and heptagons 51% of the time (more examples of Molmo behavior in Appendix F). Moreover, GPT-40 correctly identifies heptagons 92% of the time but counts their sides accurately only 58% of the time, revealing its tendency to recognize shapes without using visual information to reason about their properties. These findings prompt a key question: is the breakdown in shape recognition and side-counting due to flawed visual processing or a failure to apply reasoning? We explore this by analyzing the vision encoder and LLM backbone separately.\n3.1.1 What does the text-decoder know?\nTo assess LLMs' knowledge of the geometric properties MLLMs struggle with, we construct the following prompts: (1) \u201cWhat is the name of a  n -sided polygon?\u201d where  n  ranges from 3 to 8, and (2) \u201cHow many sides does a  shape  have?\u201d where  shape  is a regular polygon from triangle to octagon. The results, shown in Appendix Table 7, reveal that the underlying language models know geometric properties. All models get 100% accuracy on \"How many sides does a  shape  have?", "What is the name of a  n -sided polygon?\u201d. The success of LLM backbones in these tasks aligns with prior findings that state-of-the-art models perform well on lower-level math problems (Xiang et al., 2025). Note that LLaVA 1.5 and G-LLaVA's LMs struggle with naming certain shapes, suggesting some failures stem from gaps in pre-training data rather than vision-related issues. In Appendix E, we analyze Google N-Grams data and confirm that the shapes these models struggle with are also the least frequently mentioned in text.\n3.1.2 What does the vision-encoder see?\nTo evaluate how vision encoders process shapes, we use t-SNE to visualize their embeddings. Figure 2 shows results for LLaVA-OneVision, which exhibits the same embedding pattern as all other models (Appendix D). Common shapes like triangles (blue) and squares (red) form well-defined clusters, reflecting the models' strong performance on these shapes (Table 1). In contrast, vision encoders appear \u201cshape-blind\u201d to less common shapes, often embedding them into the same cluster. In particular, hexagons (pink), heptagons (green), and octagons (orange) exhibit dispersed and overlapping embeddings indicating poor differentiation.\nAppendix D provides nearest neighbor analysis for all models, confirming that triangles and squares form distinct clusters in vision-encoder representations. In LLaVA-OneVision, 99.1% of a triangle's 20 nearest neighbors are also triangles, while only 41.1% of a heptagon's neighbors are heptagons, with most being hexagons or octagons. This trend is consistent across all open-source models.\nAlthough LLMs excel at answering basic shape property questions, vision encoders in MLLMS struggle to differentiate shapes, causing the entire MLLM system to misidentify shapes. The contrast between Table 1 and Table 7 suggests vision encoder limitations as a key failure, aligning with prior findings on their lack of fine-grained detail (Nayak et al., 2022). The interplay between weak vision encoders and strong LLM backbones is evident in GPT-40 and GPT-4 Turbo. GPT-40 outperforms GPT-4-Turbo in shape identification, raising heptagon identification from 0% to 92%, yet struggles with side counting (58% accuracy). While a human would engage in System 2 reasoning and count the sides (e.g., \"1, 2, . . ., 7\u201d \u2192 heptagon) when faced with an unfamiliar shape, models instead default to predicting 6 or 8 sides, suggesting reliance on memorized patterns (System 1) rather than genuine visual reasoning. These findings highlight a fundamental issue: recognizing shapes in pretraining data does not generalize to an understanding of geometric properties such as having \"sides\u201d. This disconnect prompts an examination of how the lack of visual understanding of \u201csides\u201d impacts performance on more complex, multi-step reasoning tasks, which more closely resemble real-world math datasets.\n3.2 Evaluating Multi-step Math Reasoning\nWe use a three-step mathematical reasoning pipeline to pinpoint bottlenecks in MLLM geometric reasoning: Step 1: identifying shapes in an image; Step 2: retrieving their side counts; and Step 3: summing the sides of two shapes. Combining multi-shape recognition with arithmetic operations helps separate vision challenges (e.g., shape recognition) from reasoning limitations (e.g., arithmetic errors or mapping mistakes). Following the single-shape generation procedure (Appendix B.2), we extend the dataset to include two-shape images with variations in color, background, size, and rotation, ensuring no overlap or collision. Table 2 reveals that most models struggle in the two-shape setting, with the shape identification step (Step 1) emerging as the primary bottleneck. While models like GPT-4o and GPT-4 Turbo achieve near-perfect accuracy in recalling shape properties (Step 2), their failure to correctly recognize shapes in Step 1 limits overall performance. For instance, GPT-40 identifies shapes with 73.40% accuracy but achieves only 56.23% accuracy in Step 3. Similarly, LLaVA-Next, InternVL, LLaVA-OneVision, and Molmo perform well in mapping and arithmetic reasoning but frequently misidentify shapes, leading to errors seen in Figure 3, panel B. In addition to the ground-truth sum of sides, we evaluate accuracy for Step 3 based on the output of Step 2, allowing us to isolate errors in shape identification from arithmetic errors (see Figure 3). Consider an image containing a pentagon and a square. If the model misidentifies the pentagon as a hexagon, Step 3 accuracy would still be correct if the sum aligns with the model's incorrect mapping (e.g., 6 + 4 = 10 instead of the correct 5 + 4 = 9). Table 2 demonstrates that models execute sums correctly based on the side mapping from Step 2. Namely, the low performance in Step 3 accuracies is primarily due to an inability to retrieve the correct shapes in the image, not an inability to execute sums.\nExamining reasoning through distinct steps highlights a fundamental limitation in multimodal integration. MLLMs succeed in basic arithmetic but fail when vision and language must work together. Their tendency to rigidly map less common shapes to familiar ones (e.g., misclassifying heptagons as hexagons) highlights the lack of reliable visual grounding. Even with strong LLM backbones, this limitation persists, reflecting broader challenges in vision-language mathematical benchmarks (Zhang et al., 2024a). To investigate this further, we evaluate performance on entirely unseen shapes devoid of familiar visual priors and biases.\n3.3 Counting Sides of Abstract Shapes\nWe introduce a dataset of novel shapes to test the hypothesis that MLLMs rely on System 1 reasoning-memorizing shape-side relationships rather than actively counting and using visual information. These shapes are unlikely to appear in training, allowing us to evaluate whether models can generalize beyond familiar regular polygons. We consider three distinct categories of abstract shapes:\nMerged Shapes: We combine two regular polygons, such as a triangle and a square, where the shapes share a side. A simple shared-side algorithm can be applied to obtain the number of the sides:  s_1 + s_2 - 2  where  s_1  and  s_2  are the number of sides of shape 1 and shape 2, respectively. Applying this algorithm to a triangle and a square gives: 3 + 4 \u2212 2 = 5 total sides. Table 3 shows that models generally fail to implement this algorithm, with most achieving 0% accuracy on merged shapes (Tri-Sq through Sq-Hep). LLaVA-OneVision and LLaMA-3.2 each achieve over 90% accuracy on exactly one merged shape. This is because most open-source models (except for Molmo, which is trained on counting) tend to guess a single value from 3 to 8 on every shape, thus getting \u201cguessing correctly": "n a specific category while scoring 0% on all others. These results demonstrate that models do not implement the  s_1 + s_2 - 2  algorithm.\nIrregular Polygons: Extending the experiment from Section 3.1, we evaluate side-counting on irregular polygons with 3 to 8 sides. Performance remains low across all models, with GPT-40, Molmo, and Math-LLaVA achieving the highest accuracy (42%). Most correct predictions come from irregular triangles, while shapes with 4\u20138 sides yield near 0% accuracy, suggesting poor generalization from regular to irregular polygons.\nAbstract Shapes: Arrows, stars, and plus signs are common in documents and presentations, thus more likely to appear in pretraining data. GPT-4o and GPT-4 Turbo achieve 84-92% accuracy on these shapes, while open-source models score close to 0%. When these shapes are combined into novel configurations (an arrow on top of a plus sign seen in Figure 1), all models yield 0% accuracy.\nThe results in Table 3 highlight the influence of pretraining exposure, as models fail to generalize side counting beyond familiar shapes. Instead, their performance confirms our hypothesis that MLLMs rely on System 1 thinking, memorizing shape-side relationships rather than engaging System 2 reasoning to count and interpret visual information. The reliance on memorization over reasoning suggests a need for strategies that can shift MLLMs toward more deliberate, step-by-step problem-solving.\n3.4 Visually-Cued CoT\nOur results demonstrate that MLLMs heavily rely on System 1 thinking when completing visual-geometry tasks. We explore a potential solution to encourage System 2 reasoning in MLLMs by introducing Visually-Cued Chain-of-Thought (VC-CoT) prompting. CoT prompting is widely recognized for improving logical reasoning in LLMs by guiding models to break down complex tasks into intermediate steps (Wei et al., 2023). However, prior work suggests that MLLMs struggle with CoT reasoning (Zhang et al., 2024c,b; Wang et al., 2024a). Existing CoT frameworks for MLLMs focus primarily on the language model without leveraging information available in the image. VC-CoT incorporates explicit \u201cvisual cues\u201d into COT prompts to help bridge the gap between CoT success in LLMs and its effectiveness in MLLMs.\nWe conduct a case study on two shapes from our previous experiments: a regular heptagon and an arrow on top of a plus sign. For both shapes, models faced significant difficulty in accurately counting their sides, even when the shapes were correctly identified as a \"heptagon\u201d or a \"triangular shape on a cross", "conditions": 1, "How many sides does the shape in the image have?\") with CoT prompts that explicitly reference visual annotations (see Figure 5). We use random and ordered letters/numbers when annotating images to ensure that performance is not solely measured under conditions that provide explicit references to the number of sides. Even for plain images with no labels, we included a CoT prompt (seen in Figure 5) to test whether reasoning prompts alone could enhance performance.\nTable 4 shows the performance of Molmo, Janus-Pro, GPT-4-Turbo, and GPT-40, and Appendix Section G shows results for all other models. Molmo and Janus-Pro were explicitly designed for CoT reasoning through dedicated training, while GPT models are well-documented to excel with CoT prompting (Wei et al., 2023), making them the most relevant candidates for VC-CoT. In Table 4, we see that these models exhibit substantial improvements when visually cued CoT prompts are paired with explicit visual annotations, such as numbering or labeling each side. This is especially evident in the GPT models, where GPT-40 increases from 7% to 93%, in the random letters annotation.\nAcross all models, note that annotations alone or CoT without annotations fail to override visual biases. A striking failure emerges even when models are explicitly given the correct answer through labeled sides (e.g., numbering a heptagon's sides from 1-7). Non-GPT models still fail without CoT, exposing a fundamental weakness in visual perception. Without VC-CoT prompts, MLLMs fail to extract fine-grained details and default to System 1 memorized associations, hindering structured reasoning. However, linking reasoning steps to visual cues helps mitigate visual biases in these models.\n3.5 Does VC-CoT generalize beyond side-counting?\nGiven the success of VC-CoT prompts in improving side counting, we evaluate VC-CoT on a complex mathematical reasoning task. Many multimodal geometry datasets already include diagrams annotated with letters, numbers, and angles, providing a natural opportunity to leverage these visual cues. In particular, the vision-dominant split of the MathVerse dataset contains sufficient visual annotations to answer the question. While MathVerse includes CoT prompting, it only slightly modifies the question, changing \u201cplease directly answer\" to \"please first conduct reasoning": "without guiding models to extract the available visual information. In contrast, our visually-cued CoT explicitly prompts models to first identify all present shapes, numbers, and letters and establish their spatial and numerical relationships before answering the question. This structured approach aims to engage System 2 reasoning, forcing models to process the visual structure, retrieve relevant information from the image, and then reason about their relationship. As shown in Table 5, specifically addressing the visual annotations through VC-CoT improves accuracy across all models. For Molmo and Janus-Pro, MathVerse CoT, does not increase performance, and even decreases compared to the direct prompt. As seen in Appendix Section H, these results generalize to other models as well. While these experiments serve as a case study, they highlight a promising direction for enhancing the reasoning capabilities of MLLMs. We believe incorporating visual cues in CoT prompts can enhance MLLMs\u2019 ability to bridge the gap between vision and language reasoning."}, {"title": "4 Conclusion", "content": "Our study highlights fundamental limitations in MLLMs' ability to integrate visual information for reasoning. While LLM backbones possess strong geometric knowledge, vision encoders remain the primary bottleneck, forcing MLLMs to rely on System 1 thinking instead of System 2 reasoning. Even in simple tasks like shape identification and side counting, MLLMs default to memorized patterns rather than systematically analyzing visual inputs. As an initial step to improving MLLM reasoning, we introduce Visually-Cued Chain-of-Thought (VC-CoT) prompting to engage System 2 reasoning. Our results show that explicitly guiding models to extract and reason about visual cues improves performance, boosting GPT-40's accuracy by 86% on our shape dataset as well as enhancing results on MathVerse.\nOur findings serve as a broader call to action for the MLLM research community. To effectively pinpoint and address limitations in vision-language integration, we emphasize the need to explore simple, controlled scenarios before engaging with complex multimodal benchmarks. By improving fine-grained visual perception and utilizing visual cues for prompting, researchers can move toward MLLMs that engage in true vision-based System 2 reasoning rather than defaulting to LLM-driven deduction."}, {"title": "5 Limitations", "content": "Although we evaluated a diverse range of 13 models, including open-sourced, closed-sourced, and fine-tuned models specialized in mathematical reasoning, there are two recent models we were unable to assess. At the time of writing, the weights for MAVIS (Zhang et al., 2024b) are not publicly available. Additionally, SVE-Math (Zhang et al., 2025), which adapts the vision encoder with a GeOGLIP encoder trained on mathematical diagrams, shows promise for mathematical reasoning. However, there are unresolved issues in the code base, including missing files in the official GitHub repository, which have yet to be addressed. We will add these models to our evaluation suite as soon as these issues are addressed.\nFurthermore, our study is designed around a controlled synthetic environment, allowing us to systematically analyze the components of visual geometry datasets. Thus, our method, Visually-Cued Chain-of-Thought (VC-CoT), is particularly geared for datasets that include image annotations, such as Mathverse (Zhang et al., 2024a). Future work could explore how VC-CoT adapts to real-world images, potentially leveraging other types of annotations, such as pointing data from Pixmo (Deitke et al., 2024) for example, to bridge the gap between synthetic and real-world settings."}]}