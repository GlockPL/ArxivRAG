{"title": "Numerical Pruning for Efficient Autoregressive Models", "authors": ["Xuan Shen", "Zhao Song", "Yufa Zhou", "Bo Chen", "Jing Liu", "Ruiyi Zhang", "Ryan A. Rossi", "Hao Tan", "Tong Yu", "Xiang Chen", "Yufan Zhou", "Tong Sun", "Pu Zhao", "Yanzhi Wang", "Jiuxiang Gu"], "abstract": "Transformers have emerged as the leading architecture in deep learning, proving to be versatile and highly effective across diverse domains beyond language and image processing. However, their impressive performance often incurs high computational costs due to their substantial model size. This paper focuses on compressing decoder-only transformer-based autoregressive models through structural weight pruning to improve the model efficiency while preserving performance for both language and image generation tasks. Specifically, we propose a training-free pruning method that calculates a numerical score with Newton's method for the Attention and MLP modules, respectively. Besides, we further propose another compensation algorithm to recover the pruned model for better performance. To verify the effectiveness of our method, we provide both theoretical support and extensive experiments. Our experiments show that our method achieves state-of-the-art performance with reduced memory usage and faster generation speeds on GPUs.", "sections": [{"title": "1 Introduction", "content": "Transformers have been dominant in generative models. This includes Large Language Models (LLMs) (Vaswani et al. 2017; Touvron et al. 2023b) for language generation, as well as recent autoregressive image generation models (Van Den Oord, Vinyals et al. 2017; Esser, Rombach, and Ommer 2021; Ramesh et al. 2021; Yu et al. 2022). Notably, models such as LlamaGen (Sun et al. 2024), which use image tokenizers to convert continuous images into discrete tokens, have demonstrated the ability to surpass diffusion models (Ho, Jain, and Abbeel 2020; Rombach et al. 2022) in image generation tasks. The \u201cnext-token prediction\" paradigm demonstrates significant capabilities in addressing both language and image generation tasks, enabling solutions that mimic human-like conversational interactions (Achiam, Adler et al. 2023; Li et al. 2024a).\nRecognizing the capabilities of large autoregressive models pioneering works (Frantar and Alistarh 2023; Sun et al. 2023; Ma, Fang, and Wang 2023; Ashkboos et al. 2024; Zhan et al. 2021; Zhao et al. 2024; Zhan et al. 2024b) have sought to compress these models to enhance their execution efficiency. Compared to irregular pruning methods, structural pruning offers a more efficient reduction in both computational and memory overhead (Jian et al. 2021; Gong et al. 2022, 2023). By maintaining a consistent and regular structure, it simplifies implementation, accelerates processing, and leads to more predictable resource savings (Kong et al. 2022, 2023). However, most of these efforts focus solely on language models and language-related research areas. Consequently, their methods are not readily applicable to image generation tasks because of the fundamental differences in data structure and computational requirements between language and image processing (Reed et al. 2016; Parmar et al. 2018; Lee et al. 2022; Shen et al. 2024a,c,b, 2023b,a; Li et al. 2023b, 2024e,d). Therefore, it is crucial to explore the transformer architecture itself, rather than focusing on specific application models. This motivates us to develop a general method for compressing autoregressive models applicable to multiple kinds of generative tasks.\nAdditionally, the recovery of pruned models are crucial. Full-parameter retraining of large autoregressive models after pruning is often computationally prohibitive, making calibrations with a few samples a preferred approach. Previous work (Frantar and Alistarh 2023) employs the Optimal Brain Surgeon (OBS) technique (Hassibi, Stork, and Wolff 1993; LeCun, Denker, and Solla 1989) for weight updates during pruning. However, its heavy reliance on the approximation information increases sensitivity to noise and reduces robustness across different datasets. SliceGPT (Ashkboos et al. 2024) relies on a large number of samples for pruning and calibration, leading to overfitting on calibration data and limiting the generalization to other different datasets.\nIn this work, we present a novel structural pruning approach that leverages our proposed numerical score, combined with compensation techniques for performance recovery. We first calculate the numerical score for each layer through solving the optimal pruning mask for the minimization of pruning errors using the Newton's method. By ranking these numerical scores of all layers, we generate the globally pruning mask with the specified pruning ratio. Additionally, we introduce a compensation algorithm to recover pruned models by updating the remaining weights to account for the loss caused by the pruned weights. We empirically evaluate our method using the LLaMA model family including LLaMA, LLaMA-2, and LLaMA-3 as representatives."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Compression for LLMs", "content": "The large number of parameters in LLMs motivates the need for pruning (Gong et al. 2020; Wu et al. 2022; Zhan et al. 2024a; Li et al. 2022; Zhang et al. 2022; Zhan et al. 2024c; Shen et al. 2024d) to improve efficiency. The work (Frantar and Alistarh 2023) introduces the Optimal Brain Surgeon (OBS) method (Hassibi, Stork, and Wolff 1993; LeCun, Denker, and Solla 1989) to compress the LLMs, which removes weights with minimal impact on the loss function. It then updates the remaining weights by utilizing the inverse of the Hessian matrix to mitigate errors caused by the pruning process. Unfortunately, this kind of pruning method is still irregular, meaning it does not lead to significant reductions in memory and computational requirements. Subsequent works, such as LLM-Pruner (Ma, Fang, and Wang 2023), SliceGPT (Ashkboos et al. 2024), and FLAP (An et al. 2023), propose structural pruning methods that effectively reduce memory usage and accelerate inference on GPUs. These methods offer significant advantages over irregular pruning by directly enhancing the utility and efficiency of the models. While autoregressive models excel in sequential data processing, such as text, the distinct nature of image data, where spatial relationships and pixel-level details are critical, demands different approaches. As a result, adapting these models to image generation introduces complexities that limit their scalability and effectiveness."}, {"title": "2.2 Autoregressive Models in Image Generation", "content": "Autoregressive models, initially renowned for their success with LLMs, have recently gained popularity in the image generation research area. Pioneering works (Van Den Oord, Vinyals et al. 2017; Esser, Rombach, and Ommer 2021) introduced image tokenizers that convert continuous images into discrete tokens. These tokenizers, which have been demonstrated to be effective by the following works (Ramesh et al. 2021; Yu et al. 2021, 2022), enable autoregressive models to generate image tokens using the next-token prediction approach. Recent work (Sun et al. 2024) delivers a series of image generation models with a new constructed image tokenizer. This research demonstrates the effectiveness of LLM frameworks in image generation tasks, validating their potential beyond traditional language applications. Additionally, the work (Li et al. 2024a) delves deeper into the continuous-valued domains of autoregressive models and removes the image tokenizers for image generation tasks. This work achieves stronger results while leveraging the speed advantage of sequence modeling, which further enhances the utilization and demonstrates the potential of autoregressive models in image generation tasks."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Preliminary", "content": "Notations. We use $\\mathbb{E}[\\cdot]$ to denote the expectation. For two vectors $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}$, we use $\\langle x,y \\rangle$ to denote the inner product between $x, y$, i.e., $\\langle x, y \\rangle = \\sum_{i=1}^n X_i y_i$. We use $\\mathbb{1}_n$ to denote a length-n vector where all the entries are ones. We use $X_{i,j}$ to denote the $j$-th coordinate of $x_i \\in \\mathbb{R}^n$. We use $||x||_p$ to denote the $l_p$ norm of a vector $x \\in \\mathbb{R}^n$. For each $a, b \\in \\mathbb{R}$, we use $a \\circ b \\in \\mathbb{R}^n$ to denote the vector where i-th entry is $(a \\circ b)_i = a_i b_i$ for all $i \\in [n]$. We use $||A||$ to denote the spectral norm for matrix $A$. For a square matrix $A$, we use $\\text{tr}[A]$ to denote the trace of $A$, i.e., $\\text{tr}[A] = \\sum_{i=1} A_{i,i}$\nInternal Computation Alignment. To maintain model interpretability and mitigate the risk of model drift, we ensure consistency in the internal computations of the Attention module. This approach is aligned with established methodologies in the literature (Vaswani et al. 2017; Yang et al. 2020). Considering the definition in this paper mainly focuses on $X \\cdot W$ where $X \\in \\mathbb{R}^{N \\times D}$ and $W \\in \\mathbb{R}^{D \\times D'}$, we visualize the pruning strategy in Figure 1. In detail, we utilize the identical pruning mask (i.e., pruning strategy) for the columns of weights associated with the query, key, and value, as well as for the rows of weights in the output projection. Meanwhile, we apply the same strategy to the MLP module, using column pruning for the up and gate weights, and row pruning for the down projection weights. In this paper, we construct structural pruning metrics focusing on the output projection layers of Attention module and the down projection layers of MLP module."}, {"title": "3.2 Numerical Score", "content": "We define the weight as $W \\in \\mathbb{R}^{D \\times D'}$, input as $X \\in \\mathbb{R}^{N \\times D}$, and we denote the mask as $M \\in \\{0,1\\}^{D}$. Additionally, we define the pruning ratio $\\rho \\in [0, 1]$ as the ratio of the number of zeros to the total number of entries in pruning mask $M$. Note that, when we apply the mask column by column, the mask $M$ is a $D$-dimensional vector. Specifically, if $M_j = 0$ for $j \\in [D]$, we prune the entire row for $W$, i.e., $W_j = 0$, and if $M_j = 1$ we keep the original $W_j$.\nTo compute the numerical score, we explore the bound of the error (i.e., difference) between the original weights and pruned weights. For the bound of the error, we first formulate the error for $i \\in [D']$ as\n$$\n||XW_{\\ast,i} - X(M \\circ W_{\\ast,i})||_2.\n$$\nSimplify further, for $i \\in [D']$, Eq. (1) can be transformed into the following,\n$$\n||X((1_D-M) \\circ W_{\\ast,i}) ||_2.\n$$\nIn the above equation, the $||1_D - M||_2$ denotes the number of zero entries in $M$, which is corresponding to the simply $\\sqrt{D}$. Furthermore, assuming $||X|| \\leq R$, we demonstrate that the following Lemma 1 holds,\nLemma 1 (informal version of Lemma 9 at Appendix D.2). We show that for $i \\in [D']$ we have\n$$\n||XW_{\\ast,i} - X(M \\circ W_{\\ast,i})||_2 \\leq \\sqrt{\\rho}R||W_{\\ast,i}||_2.\n$$\nIt is intuitive for us to minimize the error after establishing the error bound in Lemma 1. Thus, we examine each term. In the error bound of Lemma 1, the pruning ratio $\\rho$ is manually specified. We adopt the normalization for the input $X$, then the norm of normalized $X$ is upper bounded by 1. Meanwhile, for $||W_{\\ast,i}||_2$ term, it is the $l_2$ norm for $i$-th column of weight $W$.\nIn order to minimize the error, we regulate both $\\rho$ and $||W_{\\ast,i}||$. Then, we generalize the mask $M$ from binary value to real value for the calculation of the numerical score. Meanwhile, we set one threshold which converts the real-valued mask back into a binary mask. For mask $M \\in [0, 1]^D$ and pruning ratio $\\rho \\in [0, 1]$, the calculation of the numerical score is formulated as follows,\n$$\n\\mathop{\\arg \\min}_{M} \\sum_{i \\in [D']} ||XW_{\\ast,i} - X(M \\circ W_{\\ast,i})||_2,\n$$\ns.t.\n$$\n\\langle 1_D, M \\rangle = (1 - \\rho)D.\n$$\nTo better solve Eq. (2), we define the numerical score $z \\in [0, 1]^D$ and $r := (1 - \\rho)D \\in [0, D]$. The equality constraint in Eq. (2) is then equivalent to $\\langle 1_D, z \\rangle - r = 0$.\nThen, Eq. (2) becomes the minimization problem with the equality constraint. To efficiently solve such problem, we adopt the Newton' method (Bubeck et al. 2015). By turning the equality constraint into a penalty term for regularization, we further generate the following equivalent problem,\n$$\n\\mathop{\\arg \\min}_{z\\in [0,1]^D} \\sum_{i \\in [D']} \\frac{1}{2}||XW_{\\ast} - X(z \\circ W_{\\ast,i})||_2 + \\frac{\\lambda}{2}(\\langle 1, z \\rangle - r)^2,\n$$\nwhere $\\lambda \\in \\mathbb{R}_+$ is the regularization parameter.\nTo explain how we solve this, we define the loss function for $i \\in [D']$ as follows,\n$$\nL(z)_i = \\frac{1}{2}||XW_{\\ast,i} - X(z \\circ W_{\\ast,i})||_2.\n$$\nMeanwhile, for regularization term, we define as follows,\n$$\nL_{reg}(z) = \\frac{\\lambda}{2}(\\langle 1, z \\rangle - r)^2.\n$$\nCombining Lemma 12 and Lemma 13 at Appendix D.3, we compute the gradient of Eq. (4) and Eq. (5) as follows,\n$$\ng = ((W W^\\top) \\circ (X^\\top X))(z - \\mathbb{1}_D) + \\lambda((1,z) - r) \\cdot \\mathbb{1}_D.\n$$\nCombining Lemma 14 and Lemma 15 at Appendix D.3, we compute the Hessian of Eq. (4) and Eq. (5) as follows,\n$$\nH = (WW^\\top) \\circ (X^\\top X) + \\lambda \\cdot \\mathbb{1}_{D \\times D}\n$$\nSubsequently, using Algorithm 1, we efficiently compute the optimal numerical z in O(TD\u00b3), where T represents the number of iterations for Newton's Method, typically around 50 in practice. Besides, we derive the following Theorem 2.\nTheorem 2 (Mask optimization, informal version of Theorem 10 at Appendix D.3). If the following conditions hold:\n*   Let $W \\in \\mathbb{R}^{D \\times D'}$, $X \\in \\mathbb{R}^{N \\times D}$\n*   Let $z \\in [0, 1]^D$.\n*   Let $r \\in [0, D]$ denote the number of ones (it can be a fractional number).\n*   Let $\\lambda > 0$ denote a regularization co-efficients.\n*   Assume $||X|| \\leq R$.\nThere exists an algorithm (Algorithm 1) that can get the optimal $z$ in $O(TD^3)$ for Eq. (3)."}, {"title": "3.3 Global Pruning", "content": "To ensure consistent head-level computation in the Attention module, given numerical scores $z_{attn} \\in \\mathbb{R}^D$, we group the scores of channels for $h$-th head to determine the importance score $z^{head}_h$ of individual heads as follows,\n$$\nz^{head}_h = \\frac{1}{D_h} \\sum_{i = h \\cdot D_h}^{(h+1) \\cdot D_h} z^{attn}_i\n$$\nwhere $D_h$ denotes the dimension of each head, $h \\in [H]$ is the head index.\nUnlike the Attention module, where heads work in parallel to capture various aspects of the input and their outputs are interdependent, the MLP module has a simpler structure with minimal interdependencies between its components. Thus, we retain the channel scores $z_{mlp} \\in \\mathbb{R}^D$ to guide the pruning process for MLP module.\nTo ensure a balanced pruning process that reflects the relative importance of each layer, we simultaneously sort the numerical scores across all layers to derive the globally pruning mask. Since a single head in the Attention module is evaluated with one score but contains significantly more weights than a single channel in the MLP module, we apply scaling factors based on the model design to balance number of pruned parameters between the Attention heads and MLP channels. For a specified global pruning ratio $\\rho$, hidden state dimension $D$, head dimension $D_h$ in the Attention module, and intermediate size $D_{inter}$ in the MLP module, we define $\\Psi$ as the set that stores the scores for the whole model, then apply the scaling factor $\\alpha$ when generating the threshold $\\eta$ for all the scores as follows,\n$$\n\\Psi := \\alpha \\cdot (\\{\\{z^{head}_h\\}_{h=1}^H\\}_{l=1}^L \\cup \\{\\{z_{mlp}^{D_{inter}}\\}\\}_{l=1}^L),\n$$\n$$\n\\eta = sort(\\Psi)[(\\rho (L \\cdot H + L \\cdot D_{inter}))], \\alpha = \\frac{4D_h}{3},\n$$\nwhere $H = \\text{index}(D/D_h)$ denotes the number of head in Attention module, $L$ denotes the number of layers for the whole model. Since the Attention module involves pruning 4 linear projections (query, key, value, and output) in head level, while the MLP module prunes only 3 (up, gate, and down) in channel level, the scaling factor $\\alpha$ is given by $\\frac{4D_h}{3}$. When the threshold $\\eta$ is determined, we prune the heads in the Attention module and the channels in the MLP module across all layers based on the strategy that removes heads or channels with numerical scores below the threshold."}, {"title": "3.4 Compensation for Pruning", "content": "With the above discussion, we obtain the pruning mask with Newton's method. To further improve the model performance, we modify the remaining weights in the model to compensate the loss of the pruned weights.\nProblem Formulation. Note that to align the internal computations in the attention and MLP modules, we prune the rows of the output layers in the modules and the columns in other layers of the modules. If the columns of a layer with $W$ is pruned in $XW$, the corresponding columns of the output also become zero and we are not able to compensate its loss, since modifying other unpruned columns can not change the zero output for the pruned columns. Thus, we only update the weights of the output layers with row pruning in the Attention and MLP modules. We modify the remaining rows based on pruned rows in $W$. For layers with column pruning, we do not modify their unpruned weights.\nFor the original weights $W$, after pruning, there are $k$ pruned rows which are all zeros and their row indexes are denoted by $p_i$, $\\forall i \\in [k]$. We modify the weights with the weight perturbations $\\delta W$, so that the layer output difference (before and after pruning) measured with $l_2$ norm is minimized. The weight optimization problem can formulated as the following,\n$$\n\\mathop{\\min}_{\\delta W} L(\\delta W) = ||X(W + \\delta W) - XW ||_2 = ||X\\delta W ||_2,\n$$\ns.t.\n$$\n(\\mathbb{1}_{p_i})^T(\\delta W)_{p_i,\\ast} + W_{p_i,\\ast} = 0, \\text{ for } i = 1, 2, ..., k.\n$$\nwhere $e_{p_i} \\in \\mathbb{R}^{D \\times 1}$ is the one-hot vector with the $p_i$th element as 1 and all others as 0. Thus, $e^\\top W$ denotes selecting the $p_i$th row of $W$. (W)_{i,j} represents the element in the $i$th row and $j$th column of the matrix. Then, (W)_{p_i,\\ast} represents the $p_i$th row of $W$. We can see that the constraint in Eq. (9) ensures that the corresponding pruned rows in the modified weights are all zeros, and the remaining weights are optimized to minimize the loss incurred by pruned rows.\nIt can be further transformed to the following,\n$$\n\\mathop{\\min}_{\\delta W} L(\\delta W) = ||X\\delta W ||_2,\n$$\ns.t.\n$$\nM_p \\delta W + W_p = 0,\n$$\nwhere $M_p \\in \\mathbb{R}^{D \\times k}$ is the collection of all $e_{p_i}$, i.e., $(M_p)_{\\ast,i} = e_{p_i}$, or $(M_p)_{\\ast,i} = e_{p_i}, \\forall i \\in [k]$. Similarly, $W_p$ is a collection of all pruned rows in $W$ with $(W_p)_{i,\\ast} = (W)_{p_i,\\ast}$, $\\forall i \\in [k]$. We have $W_p = M_pW$.\nOptimal Solution. Eq. (10) can be solved analytically with the following Theorem 3. The detailed proof is shown in Appendix B.\nTheorem 3. The optimal solution for Eq. (10) can be derived as the following,\n$$\n\\delta W^* = -(2X^\\top X)^{-1}M_p(M_p^\\top (2X^\\top X)^{-1}M_p)^{-1}M_p^\\top W.\n$$\nRemark 4. The optimal loss of Problem (10) corresponding to the optimal weight perturbation can be expressed as\n$$\nL^* = \\sum_i (W^\\top M_p(M_p^\\top (2X^\\top X)^{-1}M_p)^{-1}M_p^\\top W)_{i,i}\n$$\nThe sum in Eq. (12) is computed over D' (the number of columns in W), i.e., $i \\in [D']$.\nRemark 5. If the rank of $2X^\\top X$ is not full so that the inversion $(2X^\\top X)^{-1}$ is unavailable, we apply the dampening method to compute $(2X^\\top X + \\gamma \\cdot I)^{-1}$ instead of $(2X^\\top X)^{-1}$, with $\\gamma$ as the dampening ratio."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Experiment Setup", "content": "We conduct the experiments on LLaMA model families including LLaMA-1 (Touvron et al. 2023a), LLaMA-2 (Touvron et al. 2023b), and LLaMA-3 (Meta 2024) for the language generation tasks. For evaluations, we compare the perplexity of the models on the WikiText2 (Merity et al. 2016), PTB (Marcus, Santorini, and Marcinkiewicz 1993), and C4 (Raffel et al. 2020) datasets with the 2048 sequence length. We also follow LLM-Pruner to evaluate the zero-shot accuracy on common sense reasoning zero-shot classification datasets including BoolQ (Clark et al. 2019a), PIQA (Bisk et al. 2020), HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi et al. 2021), ARC-easy (Clark et al. 2018), ARC-challenge (Clark et al. 2018), and OpenbookQA (Mihaylov et al. 2018). For experiments, we adopt 128 samples from training dataset of WikiText2 to compute the numerical score and compensate the pruned models. For fairness, we also adopt 128 samples for other methods."}, {"title": "4.2 Results of LLMs", "content": "For the LLaMA models, we present the results with different pruning ratios varying from 10% to 70% in Table 1. Based on the perplexity results evaluated with 2048 sequence length on three datasets, our method consistently outperforms other methods across all pruning ratios, demonstrating the effectiveness of our proposed approach. Full results with more sparse ratios are included in Table 5 of Appendix A.1. Results show that for the larger model LLaMA-65B with pruning ratio of 70%, both LLM-Pruner and FLAP fail to produce an effective pruned model with their respective methods. In contrast, our method successfully maintains the most of the model's capabilities.\nWe further evaluate the zero-shot capabilities of the pruned model across seven downstream tasks. The results of LLaMA-7B model are shown in Table 2. Full results, including additional pruning ratios and the LLaMA-13B model, are detailed in Table 9 in Appendix A.5. Our method demonstrates superior performance compared to the other three methods on those common sense reasoning zero-shot classification datasets. Besides, we show the results with LLaMA and LLaMA-2 models of our method on MMLU (Hendrycks et al. 2021) and GSM8K (Cobbe et al. 2021) datasets in Table 8 of Appendix A.4, which demonstrates that our method retains both generative and mathematical capabilities.\nWe show the results for LLaMA-2 and LLaMA-3 models with 2048 sequence length on WikiText2 dataset in Figure 2. The detailed perplexity results for both model families on three datasets are shown in Table 6 and Table 7 of Appendix A.2 and A.3. The blue line representing our method's results consistently appears at the lowest position on the graphs, indicating its superior performance compared to the other methods with all model families."}, {"title": "4.3 Results of Image Generation", "content": "We implement the FLAP pruning method on LlamaGen model and compare this method on image generation task. We show the sparse results with LlamaGen-XXL (1.4B) and LlamaGen-3B models on ImageNet with 384\u00d7384 resolution in Table 3. We observe that for the smaller model LlamaGen-XXL (1.4B), our method shows a distinct advantage at higher pruning ratios. For the larger model LlamaGen-3B, our method consistently outperforms across all pruning ratios, effectively preserving most of the original model's capabilities. We further visualize the images generated by 10% sparsity models in Figure 3 with additional visualizations provided in Figure 6 of Appendix A.6. We observe that our method generates better image results compared to FLAP method in most cases."}, {"title": "4.4 Ablation Study", "content": "Results with 128 Sequence Length. To demonstrate the effectiveness of our method for short sequence lengths, we present the results generated with a sequence length of 128 in Table 4 using the LLaMA-7B model and the WikiText2 dataset. Comprehensive results, including additional pruning ratios and datasets, are provided in Table 10 of Appendix A.7. As observed, our method consistently performs the best across all pruning ratios.\nNumber of Samples for Compensation. To verify the efficiency of the compensation process for our method, we conducted experiments using different numbers of samples. The results of these experiments are shown in Figure 4. The results demonstrate that the performance difference between compensation with 128 samples versus 512 or even 1024 samples is minimal across all pruning ratios. This indicates that 128 samples are sufficient for our compensation method, highlighting its efficiency."}, {"title": "5 Conclusion and Limitation", "content": "In this paper, we propose the numerical score which is calculated through Newton's Method for the minimization of pruning errors. We further sort numerical scores across all model layers for global pruning. Additionally, we introduce a compensation algorithm to reconstruct weights in pruned models. Experimental results show that our method achieves the state-of-the-art performance, which demonstrates the effectiveness of our method. Meanwhile, our method reduces memory usage and accelerates generation on GPUs without requiring additional implementations. One limitation of our method is its reduced effectiveness with smaller Llama-Gen models for image generation tasks, primarily due to the usage of the discrete image tokenizer, which tends to lose important details as the sparsity increases."}, {"title": "A Appendix", "content": "A More Experimental Results"}, {"title": "A.1 Full LLAMA Results", "content": "We show the full results of LLaMA family models on three different datasets in Table 5 varying sparsity ratio from 10% to 70% with all kinds of model size. Our method consistently achieves better performance than all other three state-of-the-art methods."}, {"title": "A.2 LLaMA-2 Results", "content": "We show the detailed perplexity results of LLaMA-2 family models on three different datasets in Table 6. Our methods achieves better performance and shows better generalization on different datasets than all the other methods."}, {"title": "A.3 LLaMA-3 Results", "content": "We further show the detailed perplexity results of LLaMA-3 family models on three different datasets in Table 7. LLM-Pruner codebase does not support LLaMA-3 models. We achieve consistent better performance than all the other models on three datasets with all kinds of model sizes."}, {"title": "A.4 Results on Generation and Math Datasets", "content": "We provide the results of our method on generation and math datasets in Table 8. The results demonstrate that our pruning method effectively preserves both the model's generation capabilities and its mathematical performance."}, {"title": "A.5 Full Results for Common Sense Reasoning Datasets", "content": "We provide the task performance on common sense reasoning dataset with LLaMA-7B and LLaMA-13B models with 10% and 20% sparsity in Table 9. The results show that our method can perform better than other three methods."}, {"title": "B Derivation of Theorem 3", "content": "The Lagrange function for Problem (10) is\n$$\n\\mathcal{L}(\\delta W, \\lambda) =||X\\delta W||^2 + \\sum_i (M_p \\delta W + W_p)^T \\cdot e_i\n$$\n$$\n= \\text{tr}[\\delta W^\\top X^\\top X \\delta W", "lambda_{ik}": "T$ and each $\\lambda_{ij}$ corresponds to the $j$-th row and $i$-th column of the constraint. And their sums are computed in the Lagrange function. The trace function $\\text{tr}[\\cdot", "bigg": "n$$\n$$\n= - \\sum_i \\lambda_i^T M_p^T (2 X^T X)^{-1} \\bigg( \\sum_i M_p \\lambda_i e_i^T + W_p e_i^T \\bigg)\n$$\n$$\n= - \\sum_i \\lambda_i^T M_p^T (2 X^T X)^{-1} \\bigg( \\sum_i M_p \\lambda_i e_i^T \\bigg) \\bigg( \\sum_i e_i e_i^T (2 X^T X)^{-1} X^T \\"}]}