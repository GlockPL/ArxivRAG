{"title": "Modeling and Discovering Direct Causes for Predictive Models", "authors": ["Yizuo Chen", "Amit Bhatia"], "abstract": "We introduce a causal modeling framework that captures the input-output behavior of predictive models (e.g., machine learning models) by representing it using causal graphs. The framework enables us to define and identify features that directly cause the predictions, which has broad implications for data collection and model evaluation. We show two assumptions under which the direct causes can be discovered from data, one of which further simplifies the discovery process. In addition to providing sound and complete algorithms, we propose an optimization technique based on an independence rule that can be integrated with the algorithms to speed up the discovery process both theoretically and empirically.", "sections": [{"title": "Introduction", "content": "Predictive models have become increasingly prevalent in decision-making over the past few decades. In essence, a predictive model is a function that maps a set of features (often available from data) to a set of outcomes; see, e.g., (MacKenzie 2013; Neilson et al. 2019; Ellis 2012). For instance, a predictive model can be used to forecast the future weather based on data from the past ten days. Machine learning models are a common class of predictive models whose parameters are learned from data, e.g., support vector machines (Cortes and Vapnik 1995), decision trees (Breiman et al. 1984), and more recently, neural networks (Bishop 1995; Goodfellow, Bengio, and Courville 2016). Other types of predictive models that do not involve machine learning include statistical models such as linear regression (Freedman 2005), rule-based expert systems (Buchanan and Shortliffe 1984), and probabilistic models constructed from domain knowledge (Pearl 1988; Darwiche 2009).\nIn this work, we consider a setup (in Figure 1a) where the predictive models are treated as \"black boxes\", meaning that their behaviors are not interpretable by humans. This happens, for instance, when the model parameters are not publicly available or when the models (e.g., deep neural networks) are too complex to be transparent; see, e.g., (Lipton 2018; Caruana et al. 2015; Lada Kohoutov\u00e1 et al. 2020). To model the input-output behavior of predictive models under this setup, we introduce a class of causal graphs that represent the predictive models as causal mechanisms. This type of modeling appears to be different from the conventional approach yet effectively captures the data-generating process of the predictions. To illustrate the subtlety, consider an example where a predictive model is used to predict a patient's Disease (D) based on their Age (A), Symptom (S), and Medication (M). Without bearing in mind that D is a prediction from {A, S, M}, one may model the interactions among variables using the graph G shown in Figure 1b. However, G does not capture the data generating process of D, and it becomes erroneous if we use G to answer causal queries. For example, we would falsely conclude that an intervention on Symptom (S) does not affect the predictions on Disease (D). Instead, Figure 1c depicts the true causal graph in which the predictive model is converted into a causal mechanism for D. As we will see later, this conversion technique can be applied to model all predictive models.\nOnce we represented the predictive models as graphs, the direct causes for predictions on the outcome Y become exactly the parents of Y in the causal graph. Understanding the direct causes for model predictions has a wide range of applications. First, it provides insights into which features contribute to the predictions, which has vast implications for model explainability and fairness; see, e.g., (Ali"}, {"title": "Technical Preliminaries", "content": "We assume all variables are discrete, though all the results can be extended to continuous domains. Single variables are denoted by uppercase letters (e.g., X) and their states are denoted by lowercase letters (e.g., x). Sets of variables are denoted by bold, uppercase letters (e.g., X) and their instantiations are denoted by bold, lowercase letters (e.g., x)."}, {"title": "Causal Models and Interventions", "content": "In this work, we consider causal graphs in the form of acyclic directed mixed graphs (ADMGs) (Richardson 2003).\nDefinition 1. An acyclic directed mixed graph (ADMG) is a graph that contains directed edges (\u2192) and bidirected edges (\u2194) and in which directed edges do not form any cycles.\nLet X,\nY be two variables in an ADMG, we say that X is a parent of Y, and Y a child of X if X \u2192 Y. Moreover, we say that X is an ancestor of Y, and Y a descendant of X if there is a directed path from X to Y. We say that X is a sibling of Y if X \u2194 Y, and a spouse of Y if X and Y share a same child. We say that X is a neighbor of Y if it is a parent, child, or sibling of Y. A variable V is called a collider on a path if \u2192 V \u2190, \u2194 V \u2194, \u2192 V \u2194, or \u2194 V \u2190 appears on the path and is called a non-collider otherwise.\nIntervention is a standard technique for studying the causal relations among events. By definition, an intervention fixes a variable to a specific state, which is different from naturally observing the state of a variable. For example, instructing (intervening) a patient to take a drug yields a different effect than seeing (observing) a patient taking a drug. We write do(X = x), or simply do(x), if an intervention fixes a variable X to the state x. A variable X has a causal effect on variable Y if an intervention on X changes the distribution of Y. The causal effect can only happen if X is an ancestor of Y in the causal graph (Pearl 2009)."}, {"title": "Independences in Graphs and Distributions", "content": "(Conditional) independence is a central notion in the domain of causal inference and discovery. In fact, the goal of discovery is to identify causal graphs consistent with the independencies present in a given data distribution. We next review the definitions of independence for both causal graphs and distributions and discuss the interplay between the two.\nThe independence relations in a causal graph (ADMG) are characterized by the notion of m-separation (Richardson 2003). By definition, let X, Y, Z be three disjoint variables sets in an ADMG G, X and Y are said to be m-separated by Z, denoted msep(X, Z, Y), iff every path between X and Y satisfy the following property: (1) all the non-colliders on the path are in Z; and (2) none of the colliders on the path is an ancestor of Z. \nLet Pr be a distribution and X, Y, Z be three disjoint variable sets, we say that X and Y are independent conditioned on Z if Pr(x|y,z) = Pr(x|z) for all instantiations x, y, z. We adopt the notations in (Darwiche 2009) and write $I_{Pr}(X, Z, Y)$ if the independence relation holds and $\\neg I_{Pr}(X, Z, Y)$ otherwise. In practice, the distribution Pr is often provided in the form of data as shown in Figure 2b. Popular methods for testing independences from data include $\\chi^2$-test (Pearson 1900) and G-test (Sokal and Rohlf 2013). These independence tests, however, suffer from two bottlenecks as pointed out in (Spirtes, Glymour, and Scheines 2000, Ch. 5). The first is computational inefficiency, which occurs when independence tests are overused"}, {"title": "Markov Boundary", "content": "As we will see later, the discovery of direct causes for model predictions can be reduced to the discovery of Markov boundary in some scenarios. Therefore, we also review the notion of Markov boundary along with some discovery algorithms in this section. We start with the definition of Markov boundary in (Pearl 1988) with a slight rephrasing.\nDefinition 3. Let Pr be a distribution over variables X, Y. The Markov boundary for Y, denoted MB(Y), is the minimal subset of X such that $I_{Pr}(Y, MB(Y), X \\setminus MB(Y))$.\nThat is, Y is independent of other features when conditioned on its Markov boundary. Suppose a causal graph G is a P-MAP of the distribution Pr, then the Markov boundary of Y is unique and is equivalent to the Markov blanket of Y in G (Pearl 1988); see Appendix A for a review of the formal definitions and discovery algorithms for Markov blankets.\nOne key subroutine (procedure) used extensively by most Markov blanket discovery algorithms is the adjacency search, which identifies the neighbors of Y in the causal graph G. The procedure is based on the following observation: variables X, Y are adjacent to each other in G iff they are always dependent in Pr regardless of the conditioned variables. To check whether two variables are adjacent, the adjacency search algorithm enumerates all possible conditioned sets Z \u2286 X with an increasing size and removes a variable X from the neighbors of Y if $I_{Pr}(X, Z, Y)$. Consider the causal graph G in Figure 2a that is a P-MAP of"}, {"title": "Causal Modeling for Predictive Models", "content": "We present a class of causal graphs called predictive graphs to represent the input-output behavior of predictive models. Given a predictive model that takes a set of input features X and predicts an outcome Y, we construct a predictive graph that satisfies the following constraints: (1) Y cannot be a cause of any X \u2208 X; and (2) there is no hidden confounder between a feature X and Y. These constraints follow naturally from the data generating process of Y: intervening on predictions can never modify the input features, and the only possible causal factors for the predictions are the input features. We formally define the notion of predictive graphs.\nDefinition 4. Let X be a set of features and Y be an outcome. A predictive graph is an ADMG over X, Y where the only possible edge between X \u2208 X and Y is X \u2192 Y.\nWe will use G(X, Y) to denote a predictive graph wrt features X and outcome Y. Figure 1 depicts a predictive graph G({A, S, M}, D). One key observation is that the predictive model is translated into the causal mechanism for Y in the predictive graph; that is, the causal mechanism (which involves Y and its parents) captures the input-output behavior of the predictive model. From now on, we shall assume that the data distribution Pr(X, Y) from a predictive model is always induced by some true predictive graph G(X, Y).\nIn a predictive graph, the parents of outcome Y are exactly the direct causes of the mode predictions. In practice, however, the predictive graph is often not available and we do not know the direct causes. Hence, our goal is to discover the direct causes from data. This leads to two key questions: (1) when are the direct causes discoverable (uniquely determined)? (2) how can we find these direct causes efficiently if they are indeed discoverable? Before addressing these questions, we consider the formal definition of direct causes from (Woodward 2004) with a rephrasing.\nDefinition 5. A variable X is a direct cause of Y if $Pr(Y|do(x), do(x')) \\neq Pr(Y|do(x'))$ for some state x of X and instantiation x' of X \\{X}.\nThat is, variable X is a direct cause of Y iff an intervention on X affects the distribution of Y while fixing the states of other variables. The definition suggests that discovering"}, {"title": "Assumptions for Discovering Direct Causes", "content": "We propose two assumptions under which the direct causes of the predictions are discoverable. In both cases, we show that the direct causes become equivalent to the Markov boundary (Definition 3) so we can leverage methods for discovering Markov boundaries for discovering direct causes."}, {"title": "Canonicalness", "content": "We start with the first assumption called canonicalness, which is commonly assumed by existing algorithms for discovering Markov blankets.\nDefinition 7. A distribution Pr is said to be canonical if it is a P-MAP of some causal graph G.\nNote that the causal graph G in Definition 7 may not be the true predictive graph; in fact, G can be any ADMG, which makes the assumption quite general. The following result shows that the direct causes are discoverable when the data distribution is canonical.\nTheorem 8. Let G(X, Y) be a predictive graph that induces a canonical distribution Pr. Then the direct causes of Y in G form a unique Markov boundary of Y in Pr.\nLet G be the causal graph that is a P-MAP of the distribution Pr, then the Markov boundary for Y in Pr is exactly the Markov blanket of Y in G. That is, the problem of discovering direct causes in a predictive graph can be reduced to to the problem of discovering the Markov blanket when the given distribution is canonical. To illustrate, suppose a distribution Pr induced by a predictive graph G is a P-MAP of the causal graph G' in Figure 3a, then the set of direct causes of Y in G is {A, B, C, D, E, F}, which is exactly the Markov blanket of Y in G'. This result provides a method for discovering direct causes that is based on adopting the existing"}, {"title": "Weak Faithfulness", "content": "Our second assumption is a weaker type of faithfulness that imposes constraints on the distributions induced by the true predictive graph. As we will show later, the assumption not only makes the direct causes discoverable but also leads to an improvement on the computational efficiency.\nDefinition 9. Consider a predictive graph G(X, Y). A distribution Pr is weakly faithful wrt G if X \u2208 X is a parent of Y in G only if $I_{Pr}(X, Z, Y)$ for all Z \u2286 X \\{X}.\nIn practice, the weak faithfulness requires that the model predictions always depend on the direct causes regardless of the conditioned set on other features. This assumption is likely to hold, for instance, when the predictive model is a linear or polynomial regressor. To see an example where the assumption is violated, consider a canonical distribution Pr that is a P-MAP of the causal graph in Figure 3b. Pr is not weakly faithful since $I_{Pr}(Y, A, B)$ even though B is a direct cause of Y by Theorem 8. The following result states that the direct causes are discoverable under weak faithfulness.\nTheorem 10. Let G(X, Y) be a predictive graph and Pr be a distribution that is weakly faithful wrt G. The direct causes of Y in G form a unique Markov boundary of Y in Pr.\nAnother advantage brought by the weak faithfulness assumption is that it enables a faster discovery of direct causes when compared to the existing Markov blanket discovery algorithms. First, the direct causes of Y coincide with the definition of neighbors of Y under the weak faithfulness. Hence, all the direct causes can be found through a single adjacency search for Y, which avoids the additional independence tests for discovering non-neighbor variables (e.g., spouses) as required by Markov blanket discovery. Second, since Pr is induced by a predictive graph, Y is independent of all other features conditioned on its parents by Markov assumption. This allows us skip the \u201csymmetry correction\u201d step (Tsamardinos, Brown, and Aliferis 2006) in adjacency search, which further simplifies the discovery process."}, {"title": "Optimization with an Independence Rule", "content": "We next introduce a novel independence rule that can be integrated with the adjacency search to accelerate the discovery process when Pr is canonical. This result can be combined with the optimization technique mentioned in the previous section if Pr is also weakly faithful.\nWe start with the following main theorem which introduces an important independence rule.\nTheorem 11. Let Pr be a distribution over disjoint variable sets X, Y, Z, W. If $I_{Pr}(X, Z\\cup W, Y)$ and $I_{Pr}(X\\cup Z, \\emptyset, W)$, then $I_{Pr}(X, Z, Y)$.\nThis result allows us to skip the independence test on $I_{Pr}(X, Z\\cup W, Y)$, which involves a larger conditioned set, if we know that $I_{Pr}(X, Z, Y)$ and $I_{Pr}(X\\cup Z, (\\emptyset), W)$, which involve smaller conditioned sets. This method can be applied widely to skip independence tests in adjacency search, where the independence tests are conducted with increasingly larger conditioned sets. Again, skipping independence tests speeds up the adjacency search, therefore the discovery of direct causes, since the time complexity of discovery algorithms is dominated by the number independence tests; see Section 2.2 for our earlier discussion on this.\nWe next define a notion that can be used to characterize scenarios in which an independence test can be skipped.\nDefinition 12. A variable set V is said to be I-decomposable wrt distribution Pr if V can be partitioned into non-empty sets $V_1$ and $V_2$ where $I_{Pr}(V_1, \\emptyset, V_2)$.\nThe notion of I-decomposability can be used as follows. Suppose we want to test $I_{Pr}(X, Z, Y)$, the classical way is to apply $\\chi^2$-test (or G-test), which can be quite time consuming if the sample size is large. However, if we further know that Z' = (Z\u222a {X}) is I-decomposable, we can immediately conclude that the independence does not hold and skip the independence test for the following reason. Given that Z' is I-decomposable, we can partition Z' into independent sets $Z_1, Z_2$ where X \u2208 $Z_1$. Since we test independence with an increasing size of conditioned set, we must"}, {"title": "Experiments", "content": "We conduct experiments to further demonstrate the effectiveness of the I-decomposability rule. We compare the computational efficiency and sample efficiency of discovery algorithms with and without I-decomposability rule under the cases of (i) canonicalness and weak faithfulness; and (ii) canonicalness only. For case (i), we compare the performance of six different algorithms: Algorithm 1 without line 7 (ADJ), Algorithm 1 with line 7 (ALG1), Interleaved HITON-PC (Aliferis, Tsamardinos, and Statnikov 2003; Aliferis et al. 2010) (I-HITON), interleaved HITON-PC with the I-decomposability rule (I-HITON-DEC), Semi-Interleaved HITON-PC (Aliferis et al. 2010) (SI-HITON) and Semi-Interleaved HITON-PC with the I-decomposability rule (SI-HITON-DEC). For case (ii), we compare the performance of two algorithms: the M3B algorithm (Yu et al. 2018) (M3B), and M3B algorithm with the I-decomposability rule (M3B-DEC).\nFor all algorithms, we employ $\\chi^2$-tests to test independences from data. In particular, we set a threshold of 0.2 on the p-value for all pairwise independence tests required by I-decomposability (line 7 in Algorithm 1), a threshold of 0.1 for ADJ, ALG1, I-HITON, I-HITON-DEC, SI-HITON, SI-HITON-DEC, and a threshold of 0.05 for M3B, M3B-DEC. When a discovery algorithm returns more direct causes than there actually are, we keep the direct causes that attain the lowest p-value among all independence tests conducted by the algorithm. In Algorithm 1, this can be implemented by recording the p-values for all independence tests in line 8.\nFor all experiments, we consider random causal models (causal Bayesian networks) that contains 100 variables. The causal graphs for these models are generated using the Erd\u0151s-R\u00e9nyi method (Erd\u0151s, Paul and R\u00e9nyi, Alfr\u00e9d 1959) as follows. In case (i), we first generate a random ADMG over 99 features where each directed edge is added with probability 0.5 and each bidirected edge is added with probability 0.1. We then randomly pick c features to be the parents the outcome Y. In case (ii), we generate a random ADMG over 100 variables where each directed edges is added with probability 0.5 and each bidirected edge is added with probability 0.01. We bound the maximal degree of variables by d. In both cases, every variable has 2 or 3 states.\nOur first set of experiments compares the computational efficiency of the algorithms. We consider causal graphs with different complexity by varying the number of direct causes \u0441\u2208 {7,8,9,10} in case (i) and vary the maximal degree with d\u2208 {7,8,9,10} in case (ii). In both cases, the algorithms need to discover the direct causes from 100,000 random samples generated from the true causal model. Tables 1 and 2 (in Appendix) record the average accuracy, time (in seconds), and number of independence tests of algorithms over 20 runs. It is evident that algorithms with the I-decomposability rule require fewer independence tests and hence are faster than the algorithms without the rule, with extreme cases when the integration of I-decomposability rule halves the time required by discovery, e.g., c = 10 in Table 1. This demonstrates that the I-decomposability rule can significantly speed up the computational efficiency of the discovery algoirthms. In general, the improvement is more significant in Table 1 than Table 2, indicating that the rule is more effective in case (i) than case (ii).\nOur second set of experiments compares the sample efficiency of the algorithms. We vary the sample size from N \u2208 {1000, 5000, 10000, 20000, 50000, 100000, 150000, 200000}"}, {"title": "Conclusion", "content": "We studied the problem of discovering features that directly cause the predictions made by predictive models, empowered by a causal modeling framework that represents the prediction process using causal graphs. We presented two conditions under which the direct causes are guaranteed to be discoverable and become equivalent to the notion of Markov boundary. In these cases, existing methods for discovering Markov boundaries can be leveraged to discover the direct causes. We further proposed a novel independence rule that can be integrated with existing algorithms to improve the computational efficiency. This work opens the door to modeling predictive models with causal tools, even when these models are non-transparent like neural networks. Potential future works include identifying more conditions under which the direct causes can be (efficiently) discovered, studying the discovery of indirect causes for model predictions, and exploring the applications of the independence rule in broader contexts of causal discovery."}, {"title": "A More Details on Markov Blankets", "content": "When the distribution Pr is a P-MAP of some directed directed acyclic graph (DAG) G over variables X, Y, the Markov boundary of Y in Pr is exactly the Markov blanket of Y in G, which contains the parents, children, and spouses of Y in G (Pearl 1988).\nMore generally, when Pr is a P-MAP of some ADMG G, the Markov boundary of Y is also unique determined but its conversion to the Markov blanket is more subtle. First observe that Pr must also a P-MAP of some maximal ancestral graph (MAG) since classes of ADMGs and MAGs are Markov equivalent as shown in (Richardson 2003). That is, every ADMG can be converted to a MAG with equivalent m-separations. Instead of studying the Markov blanket in ADMGs, we shall consider the notion of Markov blankets in the equivalent MAGs, which has been defined in (Yu et al. 2018). In particular, let the district set of Y to be all the variables that are connected to Y with a bidirected path, the Markov blanket of Y in a MAG contains the following variables (Yu et al. 2018):\n\u2022 pa(Y): the parents of Y\n\u2022 ch(Y): the children of Y\n\u2022 sp(Y): the spouses of Y\n\u2022 dis(Y): the district set of Y\n\u2022 pa(dis(Y)): all the parents of variables in dis(Y)\n\u2022 dis(ch(Y)): all the district sets of ch(Y)\n\u2022 pa(dis(ch(Y))): all the parents of variables dis(ch(Y))\nWe summarize some popular methods for discovering Markov blankets (MB) from data, which has been studied extensively in the past. We start with algorithms that discover MBs in DAGs. IAMB (Tsamardinos, Aliferis, and Statnikov 2003) was one of the earliest methods that uses greedy search for discovering MBs. Later, the divide-and-conquer approach was employed to improve the efficiency by finding parents and children (PC) and spouses separately; see, e.g., HITON (Aliferis, Tsamardinos, and Statnikov 2003), PCMB (Pe\u00f1a et al. 2007), IPCMB (Fu and Desmarais 2008), GLL (Aliferis et al. 2010), CFS (Ling et al. 2023). More recently, it was shown that the performance can be further improved by finding PC and spouses simultaneously; see, e.g., BAMB (Ling et al. 2019), EEMB (Wang et al. 2020), FSMB (Liu et al. 2024).\nThese algorithms can be classified into two approaches based on how the PC set is discovered. First is the adjacency-based approach, which initializes the PC set with all features and then removes variables from the PC set if they become independent of the target Y under some conditioned set. Algorithms that fit into this category include IPCMB and FSMB. Second is the grow-and-shrink"}, {"title": "Proofs", "content": "We first show the only-if direction. By contradiction, suppose $I_{Pr}(Y, X', X)$, then Pr(y|x,x') = C for all x where C is a constant. We can then compute Pr(y|do(x')) as follows.\n$Pr(y|do(x')) = \\sum_{X}Pr(y|x, do(x')) Pr(x|do(x'))$\n$= \\sum_{X}Pr(y|x, x') Pr(x|do(x'))$ (Rule 2 of do-calculus)\n$= C\\sum_{X}Pr(x|do(x')) = C$\nSince $Pr(y|x, x') = Pr(y|do(x), do(x'))$ by Rule 2 of do-calculus (Pearl 2009), we conclude $Pr(y|do(x')) = Pr(y|do(x), do(x')) = C$ for all x, contradiction.\nNow consider the if direction. Suppose $\\neg I_{Pr}(Y, X', X)$, we can always find an instantiation y, x' such that $Pr(y|x_1,x') \\neq Pr(y|x_2, x')$. Moreover, there must exists some state x* that attains the largest $Pr(y|x^*, x')$. Again, we can write out the Pr(y|do(x')) as follows\n$Pr(y|do(x')) = \\sum_{X} Pr(y|x, do(x')) Pr(x|do(x'))$\n$= \\sum_{X} Pr(y|x, x') Pr(x|do(x'))$ (Rule 2 of do-calculus)\n$< \\sum_{X}Pr(y|x^*, x') Pr(x|do(x'))$\n$= Pr(y|x^*, x') \\sum_{X} Pr(x|do(x'))$\n$= Pr(y|x^*, x')$\n$= Pr(y|do(x^*), do(x'))$ (Rule 2 of do-calculus)\nWe conclude $Pr(y|do(x')) \\neq Pr(y|do(x^*), do(x'))$."}]}