{"title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models", "authors": ["Yu Meng", "Kaiyuan Li", "Chenran Huang", "Chen Gao", "Xinlei Chen", "Yong Li", "Xiaoping Zhang"], "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a range of multimodal tasks. However, their inference efficiency is constrained by the large number of visual tokens processed during decoding. To address this challenge, we propose Per-Layer Per-Head Vision Token Pruning (PLPHP), a two-level fine-grained pruning method including Layer-Level Retention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the Vision Token Re-attention phenomenon across decoder layers, we dynamically adjust token retention rates layer by layer. Layers that exhibit stronger attention to visual information preserve more vision tokens, while layers with lower vision attention are aggressively pruned. Furthermore, PLPHP applies pruning at the attention head level, enabling different heads within the same layer to independently retain critical context. Experiments on multiple benchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and reduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of 0.46% average performance drop, while also achieving notable performance improvements in multi-image tasks. These results highlight the effectiveness of fine-grained token pruning and contribute to advancing the efficiency and scalability of LVLMs. Our source code will be made publicly available.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Vision-Language Models (LVLMs) have established them as a prominent research focus in multimodal learning. Numerous open-source implementations have demonstrated remarkable capabilities across various tasks, including multimodal understanding and reasoning.\nNevertheless, LVLMs face computational inefficiency challenges, mainly due to converting visual inputs into lengthy vision token sequences, ranging from thousands to tens of thousands. Previous studies (Chen et al., 2024; Lin et al., 2024b) find that LVLMs exhibit lower attentions to vision tokens in deeper layers compared to shallower layers, thus a certain amount of vision tokens are pruned at specific shallow layers, and the same tokens are pruned in all subsequent layers. However, such coarse-grained pruning strategies often lead to a significant performance decline in complex tasks that require comprehensive visual information, including open-ended VQA and image captioning.\nTo address this challenge, in this work, we propose Per-Layer Per-Head Vision Token Pruning (PLPHP), a plug-and-play adaptive fine-grained vision token pruning method that includes two levels: 1) Layer-Level Retention Rate Allocation and"}, {"title": "and 2) Head-Level Vision Token Pruning, significantly reducing the performance loss associated with pruning.", "content": "The first level of our proposed method stems from our analysis of the attention to visual information in the deeper layers of LVLMs. As shown in Figure 1, we observe the phenomenon of Vision Token Re-attention across LVLMs with different architectures where attention scores of vision tokens are initially high and decrease in intermediate layers, but rise again in certain deeper layers. This indicates that LVLMs do not always disregard vision tokens in deep layers, thus we need to dynamically adjust the pruning rate to accommodate the unique attention patterns of different decoder layers.\nThe second level of our method is motivated by an in-depth investigation on the variations in vision token attention across different decoder layers. As shown in Figure 2, we divide the vision tokens into five groups based on their spatial relationships and plot the proportions of attention scores for each group across different layers. We observe that different parts of the same input image receive varying proportions of attention across different decoder layers, suggesting that each decoder layer specializes in processing distinct contexts. Furthermore,\nwe conduct a more granular analysis at the level of attention heads. As illustrated in Figure 3, different attention heads within the same decoder layer exhibit distinct patterns of attention, demonstrating that the focus on different contexts occurs at the attention head level. This observation suggests that the unique contextual information processed by each attention head should be independently preserved during the pruning process to maintain model performance.\nBuilt on these motivations, by dynamically adjusting retention rates according to layer-specific attention patterns layer by layer, PLPHP retains more vision tokens in layers where image attention scores are high, while aggressively pruning layers with low attention scores. Additionally, through head-level independent context pruning, PLPHP preserves the most critical contextual information for each attention head, leading to performance improvements. Comprehensive evaluations across multiple model architectures and various benchmarks demonstrate the effectiveness of PLPHP. Our method achieves over 50% compression of the KV cache, over 18% decoding acceleration, and only a 0.46% average performance degradation with notable improvements on multi-image tasks.\nThe contributions of our work can be summarized into the following three points:\n\u2022 We uncover the widespread phenomenon of Vision Token Re-attention through investigations on various LVLMs, which could be a significant factor leading to the performance degradation of existing pruning methods.\n\u2022 We propose PLPHP, a plug-and-play adaptive fine-grained vision token pruning method that improves the performance of pruned models significantly while maintaining high computational efficiency.\n\u2022 We conduct extensive experiments across multiple benchmarks and model architectures, validating the superiority of our proposed method."}, {"title": "2 Related Work", "content": "Recent advancements in LVLMs significantly enhanced multimodal content understanding. Liu et al. (2023) developed LLaVA, an early general-purpose multimodal model integrating CLIP (Radford et al., 2021) with language models. Subsequent innovations include Qwen-VL (Bai et al.,"}, {"title": "2.1 Large Vision-Language Models", "content": "Recent advancements in LVLMs significantly enhanced multimodal content understanding. Liu et al. (2023) developed LLaVA, an early general-purpose multimodal model integrating CLIP (Radford et al., 2021) with language models. Subsequent innovations include Qwen-VL (Bai et al.,"}, {"title": "2.2 Efficient Multimodal Large Language Models", "content": "To optimize the computational efficiency of LVLMs during inference, works such as MobileVLM (Chu et al., 2023), Tinygpt-V (Yuan et al., 2023), MoE LLaVA (Lin et al., 2024a), and LLaVA-Phi (Zhu et al., 2024) proposed more efficient model architectures. Meanwhile, Li et al. (2023) introduced a model-distillation approach that transfers knowledge from large vision-language models (VLMs) to smaller, lighter counterparts. Q-VLM (Wang et al., 2024a) provided a post-training quantization framework for LVLMs by mining cross-layer dependencies to improve quantization efficiency. From the perspective of token pruning, TokenPacker (Li et al., 2024c), Dynamic-LLaVA (Huang et al., 2024), and AVG-LLaVA (Lan et al., 2024) investigated training LVLMs with fewer vision tokens to boost computational efficiency. However, these methods typically require additional model training, which imposes further computational overhead.\nTraining-free token pruning has also been widely employed in prior research to alleviate token redundancy in vision transformers (ViTs) and large language models (LLMs). For example, PruMerge (Shang et al., 2024) and VisionZip (Yang et al., 2024) suggested strategies to reduce vision tokens generated by vision encoders, thereby lowering vision token volume. FastV (Chen et al., 2024) and SparseVLM (Zhang et al., 2024b) observed that visual tokens become less significant in deeper layers, thus proposing to eliminate redundant vision tokens during inference. VTW (Lin et al., 2024b) introduced a strategy to remove all vision tokens at a specific layer based on KL Divergence. Although these methods have demonstrated effectiveness,"}, {"title": "3 Method", "content": "Our method is a plug-and-play module during the inference process of LVLMs. Therefore, we first outline the inference process of LVLMs as preliminary, followed by the design of our proposed PLPHP."}, {"title": "3.1 Preliminary", "content": "LVLMs typically employ an autoregressive generation paradigm during inference, which comprises two stages: the Prefilling Stage and the Decoding Stage.\nPrefilling Stage. In the Prefilling Stage, different modalities are mapped into a sequence of embedding vectors (tokens), which serves as the input to the LLM backbone. We denote the interleaved multimodal input token sequence of m text segments and n images $X^1 \\in \\mathbb{R}^{S \\times D}$ as:\n$X^1 = \\left[\\begin{array}{cc}X^{(T)_1} & X^{(I)_1} \\\\ : & : \\\\ X^{(T)_m} & X^{(I)_n}\\end{array}\\right],$\nwhere $X^{(T)_i} \\in \\mathbb{R}^{S^{(T)_i} \\times D}$ represents the token sequence of the i-th text segment, and $X^{(I)_j} \\in \\mathbb{R}^{S^{(I)_j}}$ represents the token sequence of the j-th image. $S^{(T)_i}$ and $S^{(I)_j}$ represent the number of tokens for the i-th text segments and the j-th image, respectively, while $S = \\sum_{i=1}^m S^{(T)_i} + \\sum_{j=1}^n S^{(I)_j}$ represents the total length of the input token sequence. $\\mathcal{I}^{(T)} = \\{i | 1 \\le i \\le \\sum_{i=1}^m S^{(T)_i}\\}$ and $\\mathcal{I} = \\{i | 1 \\le i \\le \\sum_{i=1}^n S^{(I)_i}\\}$ denote the corresponding token index sets of $X^{T_i}$ and $X^{(I)_j}$ within $X^1$.\n$X^1$ is then fed into an LLM composed of N decoder layers. Since the output and input shapes of each decoder layer are the same, we can denote the input of the l-th decoder layer as $X^l \\in \\mathbb{R}^{S \\times D}$. For the h-th attention head in the l-th layer:\n$Q^{l,h} = X^lW_Q^{l,h},$"}, {"title": "3.2 PLPHP", "content": "PLPHP is a two-level adaptive fine-grained pruning method with Layer-Level Retention Rate Allocation and Head-Level Vision Token Pruning. The architecture is illustrated in Figure 4."}, {"title": "3.2.1 Overview", "content": "PLPHP is a two-level adaptive fine-grained pruning method with Layer-Level Retention Rate Allocation and Head-Level Vision Token Pruning. The architecture is illustrated in Figure 4."}, {"title": "3.2.2 Layer-Level Retention Rate Allocation", "content": "To measure the extent of a decoder layer's attention to visual information, thereby determining the number of vision tokens to retain, we define the Vision Attention Score $\\gamma^l$ of the l-th layer as:\n$\\gamma^l = \\sum_{k \\in \\bigcup_{i=1}^m \\mathcal{I}^{(T)_i}} \\frac{1}{H} \\sum_{h=1}^H A_{s,k}^{l,h},$\nwhere H represents the number of attention heads in each decoder layer. Note that the value of $\\gamma^l$ is between 0 and 1. The larger the value of $\\gamma^l$, the higher the l-th layer's attention to visual information.\nIn order to properly allocate the vision token retention rate based on the Vision Attention Score, given two thresholds $\\alpha$ and $\\beta$ ($0 \\le \\beta \\le \\alpha \\le 1$), the l-th decoder layer is categorized as a vision-attentive layer when $\\gamma^l > \\alpha$, as a vision-indifferent layer if $\\gamma^l < \\beta$, and as a vision-balanced layer otherwise. The token retention rate"}, {"title": "3.2.3 Head-Level Vision Token Pruning", "content": "Given the retention rate $r^l$ calculated in the first level, we proceed to perform fine-grained pruning. According to FastV and Zhang et al. (2025), LVLMs typically exhibit a global focus on images in the first two layers and the last layer. Therefore, for a model composed of N decoder layers, we select the third layer and the penultimate layer as the starting and ending layers for pruning.\nTo extract the most important vision tokens to preserve, for the h-th ($1 \\le h \\le H$) attention head in the l-th layer ($3 < l < N - 1$), we calculate the indices of vision tokens with the highest attention scores within the j-th image input, accounting for the proportion $r^l$:\n$\\mathcal{T}_{j}^{(IR), h} = \\text{argtop}K_j \\left(A^{l,h}[:, j]\\right),$\nwhere $K_j = r^lS^{(I)_j}$ and the argtopK operation identifies the indices of the top K elements with the highest values in the given sequence.\nWe then prune vision tokens by updating the key cache and value cache of the attention head by:\n$K^{l,h} \\leftarrow K^{l,h} \\left[\\bigcup_{i=1}^m \\mathcal{T}_{j}^{(T)} \\bigcup_{j=1}^n \\mathcal{T}_{j}^{(IR),h}\\right],$\n$V^{l,h} \\leftarrow V^{l,h} \\left[\\bigcup_{i=1}^m \\mathcal{T}_{j}^{(T)} \\bigcup_{j=1}^n \\mathcal{T}_{j}^{(IR),h}\\right],$\nwhere [\u00b7] represents the indexing operation, which retrieves elements from a sequence according to the given indices.\nTo provide an intuitive explanation, for every attention head of the l-th decoder layer, we retain only the top $r^l$ proportion of the most attended tokens for each image, and remove the remaining $1 - r^l$ proportion from the context. Since the number of text tokens is typically negligible compared to vision tokens, we retain all text tokens.\nOur method allows different attention heads within the same decoder layer to selectively drop different contexts, thereby better utilizing the property of multi-head attention mechanisms where distinct heads can focus on various parts of the contextual information."}, {"title": "4 Experiments", "content": "We first conduct experiments with our method based on LLaVA-OneVision across different benchmarks. The main results are shown in Table 1."}, {"title": "4.1 Experimental Setting", "content": "Benchmarks. In terms of multi-image benchmarks, we select four subsets from LLaVA-NeXT-Interleave-Bench (Li et al., 2024b): Spot-the-Diff (SD), Image-Edit (IE), Visual-Story-Telling (VST), and Multi-View (MV). We also select three single-image benchmarks: Flickr30k (Plummer et al., 2015), COCO 2017 Caption(Lin et al., 2014), and DetailCaps4870 (Dong et al., 2024).\nMetrics. Open-ended VQA tasks are evaluated using the ROUGE-L (Lin, 2004) (R) metric. CIDEr (Vedantam et al., 2015) (C) and METEOR (Banerjee and Lavie, 2005) (M) are employed to assess image captioning tasks. Overall Score is used to evaluate the performance on Multi-View benchmark. Regarding efficiency analysis, we utilize Vision Token Retention Rate (RR), KV Cache Size (KV), and Decoding Latency as our metrics for evaluation.\nBaselines. We choose FastV and VTW as our baselines. FastV discards image tokens with low attention scores in the shallow layers, while VTW retains all image tokens in the shallow layers and discards them in the deeper layers.\nImplementation Details. We implement PLPHP and all baselines on an NVIDIA A100 (80GB) GPU. All methods are evaluated using LMMs-Eval (Li* et al., 2024; Zhang et al., 2024a). More discussions regarding our benchmark selection, baseline"}, {"title": "4.2 Main Results", "content": "We first conduct experiments with our method based on LLaVA-OneVision across different benchmarks. The main results are shown in Table 1."}, {"title": "4.3 Generality of PLPHP on Various LVLMS", "content": "To further demonstrate the generality of PLPHP on various model architectures, we implement PLPHP on common LVLMs with different LLM backbones, and directly compared them with uncompressed models to highlight our effectiveness, with results shown in Table 3. Since IDEFICS2 and Mantis are unable to follow instructions in Detail-Caps4870, we evaluate PLPHP on the other six benchmarks. Remarkably, Qwen2-VL equipped with PLPHP surpasses the uncompressed model across all benchmarks, achieving an average improvement rate of 1.5%, while saving an average of 58.1% KV Cache storage space. For the other two models, our method also achieves an average of 57% KV Cache compression while surpassing the original models across multiple benchmarks."}, {"title": "4.4 Efficiency Analysis", "content": "To analyze the efficiency of PLPHP, we conduct experiments on DetailCaps4870 since it includes long generation contents. We can observe from Figure 7a that PLPHP achieves a comparable total decoding latency to both baselines. The latency introduced by the unpruned Prefilling Stage is minimal (less than 0.5 tokens of delay). Figure 7b shows that PLPHP maintains a lower KV cache size during the evaluation process compared to all baselines, leading to a shorter decoding latency. Table 4 shows that PLPHP attains performance closest to the uncompressed model. The nearly consistent evaluation time also indicates that the additional"}, {"title": "4.5 Ablation Study", "content": "To explore the impact of r and Ar, we conduct ablation experiments on four benchmarks, with the results illustrated in Figure 6. It can be observed that setting Ar > 0 consistently outperforms the cases where Ar = 0, indicating that adaptive pruning rates are superior to a fixed pruning rate. This finding demonstrates that our proposed layer-level pruning rate allocation has a positive impact on model performance.\nSince r is the most direct parameter reflecting the average pruning rate, we test the impact of r on efficiency, with the results presented in Table 5. PLPHP achieves an 18.1% decoding speedup and a 53.8% KV Cache compression under the default settings where r = 0.4, and further reaches a 20.2% acceleration and a 62.4% compression at a lower retention rate, enhancing the computational efficiency of LVLM decoding remarkably.\na and \u1e9e also indirectly influence pruning rates, thus we also conduct ablation studies with the results shown in Table 2. Intuitively, increasing a and \u1e9e elevates the criteria for vision-attentive layers and vision-balanced layers more stringent, leading to higher pruning rates at the cost of performance loss. Conversely, decreasing them relaxes the criteria, enhancing the performance but at greater computational expense."}, {"title": "5 Conclusion", "content": "In this work, we introduce PLPHP, a two-level pruning method designed to improve the efficiency of LVLMs with Layer-Level Retention Rate Allocation and Head-Level Vision Token Pruning. Comprehensive experiments demonstrate that PLPHP outperforms existing pruning methods, achieving a 18% decoding acceleration, over 50% KV Cache compression and only 0.46% performance degradation, with improvements on multi-image tasks. We believe our work contributes to efficient LVLMs, further promotes their applications, and improves the user experience."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Details of Evaluation Settings", "content": null}, {"title": "A.1.1 Benchmarks", "content": "Since PLPHP maintains the computational integrity of the LVLMs' Prefilling Stage, its efficiency advantage is primarily reflected in the low decoding latency during the subsequent Decoding Stage. Therefore, we mainly choose benchmarks composed of open-ended VQA and image captioning tasks. The benchmarks we select encompasses both multi-image task benchmarks and single-image task benchmarks."}, {"title": "\u2022 Multi-Image benchmarks:", "content": "The LLaVA-Interleave Bench is a comprehensive benchmark dataset designed to evaluate the performance of LVLMs in multi-image scenarios. It consists of 13 challenging tasks with a total of 17,000 instances. We curated four subsets consisting of open-ended VQA tasks from LLaVA-NeXT-Interleave-Bench: Spot-the-Diff, Image-Edit, Visual-Story-Telling, and Multi-View."}, {"title": "\u2022 Single-Image benchmarks:", "content": "The Flickr30k dataset is a widely used benchmark in the field of image captioning and visual understanding. It consists of 31,783 images collected from the Flickr platform, each paired with five human-annotated captions. The COCO2017 Caption subset contains more than 45,000 images, each annotated with five captions written by human annotators, describing the visual content of the images in detail, including objects, their attributes, and the relationships between them. DetailCaps4870 provides more fine-grained and specific image content descriptions than standard captioning datasets, which is more useful for efficiency analysis."}, {"title": "A.1.2 Baselines", "content": "We select FastV and VTW as our baselines in our experiments. Notably, FastV offers two versions of implementation: one that supports KV cache and one that does not. Since the non-KV-cache implementation introduces substantial computational overhead, we use the version that supports KV cache to ensure a fair comparison. For both of the baselines, we refer to the official open source code 1 2 and implement them on the models we evaluate."}, {"title": "A.1.3 Models", "content": "For Qwen2-VL, we set max_pixels to 1280 \u00d7 28 \u00d7 28 and min_pixels to 256 \u00d7 28 \u00d7 28 according to the official recommendation. The Mantis model that we choose is Mantis-8B-SigLIP-LLaMA3. For LLaVA-OneVision and Mantis, we use the official original versions 3 4, while using the versions provided by the transformers library (Wolf et al., 2020) for all other models."}, {"title": "A.2 Case Study", "content": "To showcase the effectiveness of our proposed method, we present a series of case studies in the form of multimodal chatbots, as shown in Figure 8."}]}