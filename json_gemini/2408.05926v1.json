{"title": "BI-MDRG: Bridging Image History in Multimodal Dialogue Response Generation", "authors": ["Hee Suk Yoon", "Eunseop Yoon", "Joshua Tian Jin Tee", "Kang Zhang", "Yu-Jung Heo", "Du-Seong Chang", "Chang D. Yoo"], "abstract": "Multimodal Dialogue Response Generation (MDRG) is a recently proposed task where the model needs to generate responses in texts, images, or a blend of both based on the dialogue context. Due to the lack of a large-scale dataset specifically for this task and the benefits of leveraging powerful pre-trained models, previous work relies on the text modality as an intermediary step for both the image input and output of the model rather than adopting an end-to-end approach. However, this approach can overlook crucial information about the image, hindering 1) image-grounded text response and 2) consistency of objects in the image response. In this paper, we propose BI-MDRG that bridges the response generation path such that the image history information is utilized for enhanced relevance of text responses to the image content and the consistency of objects in sequential image responses. Through extensive experiments on the multimodal dialogue benchmark dataset, we show that BI-MDRG can effectively increase the quality of multimodal dialogue. Additionally, recognizing the gap in benchmark datasets for evaluating the image consistency in multimodal dialogue, we have created a curated set of 300 dialogues annotated to track object consistency across conversations. The code and the dataset is publicly available at https://github.com/hee-suk-yoon/BI-MDRG.", "sections": [{"title": "1 Introduction", "content": "With the development of instant messaging technology, visual modalities are increasingly used alongside text in online communication. To enhance user interaction with intelligent agents, a new task, Multimodal Dialogue Response Generation (MDRG) [43], has been proposed. This task requires models to generate both text and image responses based on dialogue history containing texts and images. Since learning an effective multimodal generation model with a sin-"}, {"title": "3 BI-MDRG: Bridging the Image History in Multimodal Dialogue Response Generation", "content": "We introduce BI-MDRG, a conversational agent designed to produce both textual and visual responses with enhanced awareness of image history. Sections 3.1 and 3.2 detail the training procedure, effectively integrating image history information into text responses and textual image descriptions. Section 3.3 outlines the inference process, wherein the image history informs the image responses by leveraging the captured details from enhanced textual image descriptions."}, {"title": "3.1 Bridging the Image History for Image-Grounded Text Response", "content": "The example shown in Figure 1-(c) (left) highlighted one of the crucial limitations in the previous MDRG system: their reliance on textual descriptions for understanding image history, which hinders the image-grounded textual responses. To overcome this, we have adopted an architectural change along with a multimodal causal attention mask modulation to effectively bridge the image history information $Image_{1:t-1}$ to the text response $rText$ (as depicted in Figure 2)."}, {"title": "Architecture", "content": "Our Textual Dialogue Response Generator G (Figure 3-(a)), consists of a decoder-only language model with added visual cross-attention layers. These layers directly engage with image features provided by the Visual Encoder V, drawing inspiration from Flamingo [2], to reduce dependence on textual image descriptions for perceiving images. In our framework, a dialogue context $D = {(rText, rimage)}_1^n$ comprises multiple turns, each with an associated text response $rText$ and an image response $rimage$. An image captioning model produces textual descriptions $ui$ for each image $rimage$, which are then transformed into citation-augmented descriptions $u'_t$ as shown in Figure 3-(c) and further detailed in Section 3.2 (if there is no image response for a turn, $rimage = \u00d8$ and $u = 0$). These descriptions and the text responses {$rText, u_1,...,r Text, u_n$} are fed into G, while the images {$rimage,....rImage$} are processed by V to extract image features which are fed to the cross-attention layers in G. Although our model directly cross-attends to the inputted image features, we retain the textual descriptions $u_{1:n}$ as essential inputs since we require the generation of textual description by our G model, which subsequently gets used by a text-to-image model for constructing the image response. Keeping the textual description inputs allows efficient teacher-forced next token prediction training."}, {"title": "Multimodal Causal Attention Mask Modulation", "content": "For the input sequence {$rText, u_1,...,r Text, u_n$}, we use a specialized mask along with the standard causal mask (Figure 3-(b)). The traditional causal mask allows each text response $rText$ to access previous textual image descriptions $u_{1:t\u22121}$, leading to reliance on textual information over visual context. Our masking strategy prevents $rText$ from accessing $u_{1:t\u22121}$, redirecting focus to the actual image features of $rimage_{1:t-1}$, ensuring text responses are grounded on raw image features."}, {"title": "3.2 Citation Module: Bridging the Image History to the Textual Image Description", "content": "The example shown in Figure 1-(c) (right) highlighted another critical limitation of the previous MDRG system: its inability to ensure consistency in image responses. To address this, we propose the Citation Module (Figure 3-(c)) to bridge the image history $image$ and the textual image description ut (as depicted in Figure 5) by ensuring that the textual image description accurately relays which objects should persist in subsequent images."}, {"title": "Citation Module", "content": "Citation Module plays a pivotal role in tracking recurring objects in the dialogue using textual image descriptions. For instance, descriptions like \"a dog is in front of a fireplace\" and \"a dog running in the snow\" are augmented to \"a dog[cite]0[/cite] is in front of a fireplace\" and \"a dog[cite]0[/cite] running in the snow,\" respectively if they reference the same dog. Motivated by [30], Figure 4 details the citation process for textual image descriptions {$U_1, ..., U_n$}. For each textual image description $u_i$, a Part of Speech (POS) tagging processor Pis employed to tag words and pinpoint the key object word $o_i$ in the description. The word $o_i$, along with its corresponding image $Image_i$, is processed through an open-set object detector M to obtain the bounding box of the detected object, which is then input to a segmentation model S for generating object segmentation mask $s_i$. These masks are applied to isolate the objects from their backgrounds in $rimage_i$ since background removal has been proven helpful for better extraction of object features [6]. These isolated objects are then analyzed by a visual feature extractor E to extract features $f_i$. The resulting feature set {$f_1, ..., f_n$} undergoes clustering based on cosine similarity to identify identical objects across images, as outlined in Algorithm 1; this involves assigning each element a cluster id $c_i$ based on the similarity of their features. For each $o_i$ in $u_i$, we augment the word so that it is followed by its corresponding cluster id (i.e., citation tag) $c_i$ of $f_i$, resulting in the citation augmented textual description $u'_i$. This Citation Module operates with off-the-shelf components for citation tags, requiring no training on the target dataset."}, {"title": "Generative Training Objective", "content": "We use the next token prediction training via teacher forcing, which is used in standard auto-regressive language models [37]. Specifically, given the token sequence $w = {wj}_{j=1}^N$ of the input sequence {$rText, u_1,...,rText, u_n$} and the images {$rImage, ..., rImage$}, we minimize the negative log-likelihood:\n$$L(w) = \\sum_{j=1}^N \\log P(w_j|w_{<j}, {rImage}_{n=1}^N;G, V)$$.\nWith such training, our model can generate textual image descriptions during inference with citation tags that reflect the objects needing consistency."}, {"title": "3.3 Inference Procedure: Bridging the Image History for Consistent Image Response", "content": "This section outlines our inference procedure, which employs Customized Text-to-Image Model [23] in conjunction with citation-augmented textual image descriptions (Figure 7). This bridges the image history $rImage_{1:t-1}$ and image response $rimage$ (as depicted in Figure 6) allowing for consistent generation of the image response. For an incoming dialogue context $D = {(rText, rImage)}_{i=1}^{t-1}$, we initially construct"}, {"title": "4 Experiments", "content": "This section presents the implementation details, evaluation benchmarks, and the experimental results of our approach. Section 4.1 outlines the implementation specifics. In Section 4.2, we assess the overall quality of our model against the standard benchmarks established in previous works [9,47]. Section 4.3 is dedicated to evaluating the image grounding effectiveness of our model. Finally, Section 4.4 examines the consistency of the image responses generated by our system."}, {"title": "4.1 Experimental Setup", "content": "Dataset As mentioned in Section 2.1, our benchmark datasets include ImageChat [42], PhotoChat [47], and MMDialog [9]. Specifically, for the overall"}, {"title": "Implementation Details", "content": "For the Textual Dialogue Response Generator G and the Visual Encoder V, we initialize with the pre-trained OpenFlamingo 4B model\u00b3 [4]. During the fine-tuning phase, we employ special tokens to structure our inputs and outputs: [IMG] and [/IMG] encapsulate textual image descriptions, while [EOT] signifies the end of conversation turns. The BLIP2-flan-t5-xl model [24] is used for converting image responses to corresponding textual image descriptions. Additionally, our Citation Module C uses [CITE] and [/CITE] tokens to mark the beginning and end of citations linked to key objects within the textual image descriptions. As noted in Section 3.2, the Citation Module C is composed of four key components: the POS tagging processor (P), the open-set object detector (M), the segmentation model (S), and the visual feature extractor (E). Specifically, we employ spaCy [13] for P, GroundingDino [28] for M, the Segment Anything Model (SAM) [15] for S, and DINOv2 [33] for E. The similarity threshold T is set to 0.6. No further learning is done for these modules; they are utilized as pre-trained components within our system for citation tagging. For our customized text-to-image generation model, F, we used BLIP-Diffusion [23] when conditioning on the input image and the standard Stable Diffusion 2.1 [38] when there is no input image conditioning."}, {"title": "Learning Details", "content": "Let us denote {\u03b8\u03bd,\u03b8\u03c2, OG\u2081} as the parameters of the perceiver resampler of the Visual Encoder V, the visual cross-attention layers of the Textual Dialogue Response Generator G, and the language model layers of G, respectively. In the first stage of training, we train Og\u2081. The batch size is set to 256 with a maximum token length set to 256. In the second stage of training, we jointly train \u03b8y and \u03b8g. The batch size is set to 128 with a maximum token length set to 512. Both the first and second stage is trained by minimizing the next token prediction loss (Eq. 1) using the AdamW optimizer [29] with a learning rate set to 1e-4. The trainings were conducted using 16 x NVIDIA A100 80GB PCIe."}, {"title": "4.2 Multimodal Dialogue Evaluation", "content": "Evaluation Dataset We evaluate the overall performance of our BI-MDRG system on the test set of PhotoChat [47] and MMDialog [9] dataset. PhotoChat contains a single image per dialogue, while the MMDialog includes dialogues with multiple images across turns, offering a more complex context for assessing"}, {"title": "4.3 Image Grounding Evaluation", "content": "Evaluation Dataset To evaluate the ability to generate image-grounded textual responses, we utilize the ImageChat dataset [42], consisting of dialogues centered around a single image (an example is shown in Appendix C.4). Since the conversations are grounded in an image, the dataset is suitable for the evaluation of the image grounding capability of the model. We use the same model trained on the MMDialog dataset in Section 4.2 without further tuning on the ImageChat train set. Evaluation Metric In order to evaluate the image grounding capability, we use BLEU and ROUGE as the metrics. We perform the evaluation only on the last turn of each dialogue and consider all previous turns as the input context."}, {"title": "4.4 Image Response Consistency Evaluation", "content": "We integrated the Citation Module into our BI-MDRG system to improve consistency in image responses within the dialogue. This module assigns citation tags to primary objects in textual image descriptions, tracking their presence across"}, {"title": "5 Importance of Citation Tags for Image Consistency", "content": "Due to powerful pre-trained models for text-to-text and text-to-image, adopting text as an intermediary for image responses is a practical solution for the MDRG [43] task. However, due to the inherent information loss of images during this process, achieving image consistency is infeasible without a targeted framework for consistency maintenance. Table 4 shows image consistency and dialogue response performance across various settings. Not using citations shows similar dialogue response performance (Intent, TID, TR) compared to LLMCite and our Citation Module. However, LLMCite improves image consistency (DINOv2) from 0.25 to 0.34 (4B) and 0.26 to 0.33 (9B). Our Citation Module further boosts this from 0.34 to 0.53, indicating its importance for image consistency without affecting dialogue response performance. Notably, scaling the model size (4B to 9B) improves textual re-sponse but fails to maintain image consistency without our citation framework, as also evident by ChatGPT's shortcomings in Appendix E."}, {"title": "6 Conclusion", "content": "This paper presents BI-MDRG, a novel framework for Multimodal Dialogue Response Generation (MDRG) aimed at bridging the image history for enhanced text and image response. Our model's innovative use of image history to inform both text and image responses addresses fundamental limitations in previous methodologies, particularly in maintaining consistency in multimodal interac-tions. The effectiveness of BI-MDRG has been demonstrated through rigorous evaluations using multiple benchmark datasets and a custom-annotated dataset."}, {"title": "A Limitations", "content": "Our framework relies on customized text-to-image models to ensure image con-sistency in multimodal dialogues. While these models generally offer better con-sistency than standard text-to-image models without conditioning, they are not infallible and may sometimes fail to accurately capture the conditioned input image. This represents a current limitation of our work. However, with the rapid advancements in customized text-to-image generation, we expect these short-comings to decrease over time."}, {"title": "B Broader Impact", "content": "It is crucial to emphasize that the main contribution of our work is not the cus-tomized text-to-image model itself but the overall framework that facilitates its effective use in multimodal dialogue scenarios. By focusing on enhancing image consistency, our framework opens up new avenues for more coherent and en-gaging multimodal interactions. This underscores the potential of our approach in revolutionizing how conversational agents handle multimodal inputs and re-sponses, paving the way for more sophisticated and human-like dialogue systems."}, {"title": "C Benchmark Datasets", "content": "As stated in Section 2.1, Multimodal dialogue datasets generally fall into three categories: question and answering (Q&A), in-scene, and conversation-based."}, {"title": "C.1 Categorization of Existing Multimodal Dialogue Datasets.", "content": "In Table 5, we summarize the datasets for each category."}, {"title": "C.2 PhotoChat Dataset", "content": "PhotoChat [47] features dialogues collected from social media, where a single image is shared in one of the conversation turns, which mirrors everyday human interaction. An example of PhotoChat dialogue is shown in Figure 10-(a)."}, {"title": "C.3 MMDialog Dataset", "content": "The limited scale and domain diversity of the PhotoChat dataset restricts its applicability. Overcoming these limitations, MMDialog [9] features over a million diverse dialogues from social media, where multiple images are shared across numerous conversation turns, providing a more realistic representation of open-domain multimodal conversations. An example of MMDialog dialogue is shown in Figure 10-(b)."}, {"title": "C.4 ImageChat Dataset", "content": "To evaluate the image-grounding advantage of our BI-MDRG to the previous system, we use the ImageChat Dataset [42]. This dataset has three turns of conversation about a given image. An example of ImageChat Dialogue is shown in Figure 10-(c)."}, {"title": "C.5 Multimodal Dialogue Image Consistency (MDIC) Dataset", "content": "The challenge of ensuring consistent image generation in multimodal dialogue systems is amplified by the absence of datasets annotated for entity consistency across conversational images. We developed the Citation Module for our BI-MDRG system to address this gap. This module is designed to pseudo-label the"}, {"title": "D Details on LLMCite", "content": "In Sections 4.4 and 5, we employ a baseline citation approach, LLMCite, illustrated in Figure 13, which leverages an instruction-tuned large language model (LLM) to assign citation tags (specifically, we use OpenChat 3.5 (7B)4). From the MDIC dataset, we frame citation tag prediction as a multiclass classification task. Given a dialogue history D = {(rText, image)}_{i=1}^{t-1}, we first convert images into textual descriptions to form {rText, u1,...,rText, ut}. For the last turn t, we preprocess ut to include only up to the principal object ot, denoted as ut. For preceding turns u1:t\u22121, we append classification tags C1:t-1 (sequentially labeled as (a), (b), (c), ...) to principal objects 01:t-1, resulting in augmented descrip-tions u'1:t-1. This modified sequence {rText, u'1,...,rText, ut} is then provided to the LLM with instructions, as illustrated in Figure 14, to choose the most appropriate ci:t-1 matching ot within the dialogue context."}, {"title": "E Additional Examples", "content": "In Section 5, we demonstrated that merely increasing the model size does not enhance image consistency. This limitation arises because the framework relies on text as an intermediary step for generating image responses, leading to an in-herent loss of image information. ChatGPT also operates within this framework, utilizing text as an intermediary due to the challenges and infeasibility of imple-menting an end-to-end model, a point underscored in Section 5. Consequently, our proposed framework, specifically designed to maintain image consistency, becomes critical. Figure 15 illustrates that ChatGPT also struggles to maintain image consistency, reinforcing the need for our targeted framework."}]}