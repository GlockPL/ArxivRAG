{"title": "BI-MDRG: Bridging Image History in Multimodal Dialogue Response Generation", "authors": ["Hee Suk Yoon", "Eunseop Yoon", "Joshua Tian Jin Tee", "Kang Zhang", "Yu-Jung Heo", "Du-Seong Chang", "Chang D. Yoo"], "abstract": "Multimodal Dialogue Response Generation (MDRG) is a re- cently proposed task where the model needs to generate responses in texts, images, or a blend of both based on the dialogue context. Due to the lack of a large-scale dataset specifically for this task and the benefits of leveraging powerful pre-trained models, previous work relies on the text modality as an intermediary step for both the image input and output of the model rather than adopting an end-to-end approach. However, this approach can overlook crucial information about the image, hindering 1) image-grounded text response and 2) consistency of objects in the image response. In this paper, we propose BI-MDRG that bridges the response generation path such that the image history information is uti- lized for enhanced relevance of text responses to the image content and the consistency of objects in sequential image responses. Through ex- tensive experiments on the multimodal dialogue benchmark dataset, we show that BI-MDRG can effectively increase the quality of multimodal dialogue. Additionally, recognizing the gap in benchmark datasets for evaluating the image consistency in multimodal dialogue, we have cre- ated a curated set of 300 dialogues annotated to track object consistency across conversations. The code and the dataset is publicly available at https://github.com/hee-suk-yoon/BI-MDRG.", "sections": [{"title": "1 Introduction", "content": "With the development of instant messaging technology, visual modalities are increasingly used alongside text in online communication. To enhance user in- teraction with intelligent agents, a new task, Multimodal Dialogue Response Generation (MDRG) [43], has been proposed. This task requires models to gen- erate both text and image responses based on dialogue history containing texts and images. Since learning an effective multimodal generation model with a sin-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Multimodal Dialogue Datasets", "content": "Multimodal dialogue datasets generally fall into three categories: question and answering [1,8] (where the task involves asking and answering questions about a specific image), in-scene [12,31,35,44,49] (where each dialogue turn corresponds to a scene from a movie or video), and conversation-based [9, 20, 21, 27, 42, 47] (which engage in natural dialogue about a given image or involves image sharing within natural conversations). Further details are in Appendix C.1. This paper primarily explores the model's capability for natural dialogue within the conversation-based category. Notable datasets in this segment include ImageChat [42], PhotoChat [47], MMDD [20], DialogCC [21], MMChat [51], TikTalk [27], and MMDialog [9]. Given our focus on English-language scenarios, we exclude MMChat and Tiktalk from our evaluation, as it is a dataset primarily in Chinese. Additionally, DialogCC (not publicly available) and MMDD, which are synthesized by algorithmically pairing images with text-only dialogues for random turns, are also excluded from our analysis. Therefore, our evaluation is centered on ImageChat, PhotoChat, and MMDialog. ImageChat [42] consists of image-centered dialogues, where each dialogue is centered around a single given image. PhotoChat [47] features dialogues collected from social media, where a single image is shared in one of the conversation turns, which better mirrors everyday human interaction. Still, their limited scale and domain diversity restrict their applicability. Overcoming these limitations, MMDialog [9] features over a million diverse dialogues from social media, where multiple images are shared across numerous conversation turns, providing a more realistic representation of open-domain multimodal conversations."}, {"title": "2.2 Multimodal Dialogue Modeling", "content": "Pioneering studies [11, 32, 36] have delved into improving the performance of image-grounded conversational agents. While [14,45,46,51] introduced a Seq2Seq based model focusing on multimodal dialogues, it primarily generated textual responses, not fully embracing the multimodal response scenario. In a notable advancement, [43] presented Divter, which not only produces informative text but also generates high-resolution images, marking a significant leap forward in multimodal dialogue response generation (MDRG). It is important to note that our focus in this paper is generation-based models, contrasting with retrieval- based [3, 16, 22], which output image responses by retrieving existing images from a corpus instead of generating new ones."}, {"title": "2.3 Customized Text-to-Image", "content": "Recent studies on text-to-image diffusion models [5, 17, 38, 40] focus on cus- tomization [7, 10, 18, 39], learning specific concepts from a few images. Following this, users can flexibly generate the learned concepts into new scenes. Text inver- sion [10] generates varied contexts for a concept by updating the text embedding without altering the model. Dreambooth [39] and Custom Diffusion [18] fine-tune the U-Net architecture using an identifier, class label, and images. In a notable enhancement, BLIP-Diffusion [23] enables zero-shot subject-driven generation, allowing fast customized text-to-image generation."}, {"title": "3 BI-MDRG: Bridging the Image History in Multimodal Dialogue Response Generation", "content": "We introduce BI-MDRG, a conversational agent designed to produce both tex- tual and visual responses with enhanced awareness of image history. Sections 3.1 and 3.2 detail the training procedure, effectively integrating image history infor- mation into text responses and textual image descriptions. Section 3.3 outlines the inference process, wherein the image history informs the image responses by leveraging the captured details from enhanced textual image descriptions."}, {"title": "3.1 Bridging the Image History for Image-Grounded Text Response", "content": "The example shown in Figure 1-(c) (left) high- lighted one of the crucial limitations in the previ- ous MDRG system: their reliance on textual de- scriptions for understanding image history, which hinders the image-grounded textual responses. To overcome this, we have adopted an architectural change along with a multimodal causal attention mask modulation to effectively bridge the image history information $\\text{Image}_{1:t-1}$ to the text response $r^{\\text{Text}}$ (as depicted in Figure 2)."}, {"title": "Architecture", "content": "Our Textual Dialogue Response Generator G (Figure 3-(a)), con- sists of a decoder-only language model with added visual cross-attention layers. These layers directly engage with image features provided by the Visual Encoder V, drawing inspiration from Flamingo [2], to reduce dependence on textual im- age descriptions for perceiving images. In our framework, a dialogue context $D = {(\\text{r}^{\\text{Text}}, \\text{r}^{\\text{Image}})}_{1}^{n}$ comprises multiple turns, each with an associated text response $\\text{r}^{\\text{Text}}$ and an image response $\\text{r}^{\\text{Image}}$. An image captioning model pro- duces textual descriptions $u_i$ for each image $\\text{r}^{\\text{Image}}$, which are then transformed into citation-augmented descriptions $u'_i$ as shown in Figure 3-(c) and further detailed in Section 3.2 (if there is no image response for a turn, $\\text{r}^{\\text{Image}} = \\O$ and $u'_i = \\O$). These descriptions and the text responses {$\\text{r}^{\\text{Text}}_i$, $u'_{1}$...,$\\text{r}^{\\text{Text}}_i$, $u'_{n}$} are fed into G, while the images {$\\text{r}^{\\text{Image}}_i$..., $\\text{r}^{\\text{Image}}_n$} are processed by V to extract image features which are fed to the cross-attention layers in G. Although our model directly cross-attends to the inputted image features, we retain the textual descriptions $u_{1:n}$ as essential inputs since we require the generation of textual description by our G model, which subsequently gets used by a text-to-image model for constructing the image response. Keeping the textual description in- puts allows efficient teacher-forced next token prediction training."}, {"title": "Multimodal Causal Attention Mask Modulation", "content": "For the input sequence {$\\text{r}^{\\text{Text}}_i$, $u'_{1}$...,$\\text{r}^{\\text{Text}}_i$, $u'_{n}$}, we use a specialized mask along with the standard causal mask (Figure 3-(b)). The traditional causal mask allows each text response $\\text{r}^{\\text{Text}}_i$ to access previous textual image descriptions $u_{1:i-1}$, leading to reliance on textual information over visual context. Our masking strategy prevents $\\text{r}^{\\text{Text}}_i$ from accessing $u_{1:i-1}$, redirecting focus to the actual image features of $\\text{r}^{\\text{Image}}_{1:i-1}$, ensuring text responses are grounded on raw image features."}, {"title": "3.2 Citation Module: Bridging the Image History to the Textual Image Description", "content": "The example shown in Figure 1-(c) (right) highlighted another critical limitation of the previous MDRG system: its inability to ensure consistency in image responses. To address this, we propose the Citation Module (Figure 3-(c)) to bridge the image history $\\text{image}$ and the textual image description $u_t$ (as depicted in Figure 5) by ensuring that the textual im- age description accurately relays which objects should persist in subsequent images."}, {"title": "Citation Module", "content": "Citation Module plays a pivotal role in tracking recurring ob- jects in the dialogue using textual image descriptions. For instance, descriptions like \"a dog is in front of a fireplace\" and \"a dog running in the snow\" are aug- mented to \"a dog[cite]0[/cite] is in front of a fireplace\" and \"a dog[cite]0[/cite] running in the snow,\" respectively if they reference the same dog. Motivated by [30], Figure 4 details the citation process for textual image descriptions {$U_1$, ..., $U_n$}. For each textual image description $u_i$, a Part of Speech (POS) tagging processor $P$ is employed to tag words and pinpoint the key ob- ject word $o_i$ in the description. The word $o_i$, along with its corresponding image $Image$, is processed through an open-set object detector $M$ to obtain the bound- ing box of the detected object, which is then input to a segmentation model $S$ for generating object segmentation mask $s_i$. These masks are applied to isolate the objects from their backgrounds in $Image$ since background removal has been proven helpful for better extraction of object features [6]. These isolated objects are then analyzed by a visual feature extractor $E$ to extract features $f_i$. The re- sulting feature set {$f_1$, ..., $f_n$} undergoes clustering based on cosine similarity to identify identical objects across images, as outlined in Algorithm 1; this involves assigning each element a cluster id $c_i$ based on the similarity of their features. For each $o_i$ in $u_i$, we augment the word so that it is followed by its corresponding cluster id (i.e., citation tag) $c_i$ of $f_i$, resulting in the citation augmented textual description $u'_i$. This Citation Module operates with off-the-shelf components for citation tags, requiring no training on the target dataset."}, {"title": "Generative Training Objective", "content": "We use the next token prediction training via teacher forcing, which is used in standard auto-regressive language mod- els [37]. Specifically, given the token sequence $w = {w_j}_{j=1}^{N}$ of the input se- quence {$\\text{r}^{\\text{Text}}$, $u'_{1}$...,$\\text{r}^{\\text{Text}}$, $u'_{n}$} and the images {$\\text{r}^{\\text{Image}}_{1}$, ...,$\\text{r}^{\\text{Image}}_{n}$}, we minimize the negative log-likelihood: $L(w) = \\sum_{j=1}^{N} log P(w_j|w_{<j}, {\\text{r}^{\\text{Image}}}_{n=1};G, V).$ (1) With such training, our model can generate textual image descriptions during inference with citation tags that reflect the objects needing consistency."}, {"title": "3.3 Inference Procedure: Bridging the Image History for Consistent Image Response", "content": "This section outlines our inference proce- dure, which employs Customized Text-to- Image Model [23] in conjunction with citation- augmented textual image descriptions (Figure 7). This bridges the image history $\\text{r}^{\\text{image}}_{1:t-1}$ and image response $\\text{r}^{\\text{image}}_{t}$ (as depicted in Figure 6) allowing for consistent generation of the image response."}, {"title": "Inference", "content": "For an incoming dialogue context $D = {(\\text{r}^{\\text{Text}}_i, \\text{r}^{\\text{Image}}_i)}_{i=1}^{t-1}$, we initially construct"}, {"title": "4 Experiments", "content": "This section presents the implementation details, evaluation benchmarks, and the experimental results of our approach. Section 4.1 outlines the implementa- tion specifics. In Section 4.2, we assess the overall quality of our model against the standard benchmarks established in previous works [9,47]. Section 4.3 is dedicated to evaluating the image grounding effectiveness of our model. Finally, Section 4.4 examines the consistency of the image responses generated by our system."}, {"title": "4.1 Experimental Setup", "content": "Dataset As mentioned in Section 2.1, our benchmark datasets include Im- ageChat [42], PhotoChat [47], and MMDialog [9]. Specifically, for the overall"}, {"title": "4.2 Multimodal Dialogue Evaluation", "content": "Evaluation Dataset We evaluate the overall performance of our BI-MDRG system on the test set of PhotoChat [47] and MMDialog [9] dataset. PhotoChat contains a single image per dialogue, while the MMDialog includes dialogues with multiple images across turns, offering a more complex context for assessing"}, {"title": "4.3 Image Grounding Evaluation", "content": "Evaluation Dataset To evaluate the ability to generate image-grounded tex- tual responses, we utilize the ImageChat dataset [42], consisting of dialogues centered around a single image (an example is shown in Appendix C.4). Since the conversations are grounded in an image, the dataset is suitable for the eval- uation of the image grounding capability of the model. We use the same model trained on the MMDialog dataset in Section 4.2 without further tuning on the ImageChat train set."}, {"title": "Evaluation Metric", "content": "In order to evaluate the image grounding capability, we use BLEU and ROUGE as the metrics. We perform the evaluation only on the last turn of each dialogue and consider all previous turns as the input context."}, {"title": "4.4 Image Response Consistency Evaluation", "content": "We integrated the Citation Module into our BI-MDRG system to improve consis- tency in image responses within the dialogue. This module assigns citation tags to primary objects in textual image descriptions, tracking their presence across"}, {"title": "5 Importance of Citation Tags for Image Consistency", "content": "Due to powerful pre-trained models for text-to-text and text-to-image, adopting text as an intermediary for image responses is a practical solution for the MDRG [43] task. However, due to the inherent information loss of images during this process, achieving image consistency is infeasible without a targeted framework for consistency maintenance."}, {"title": "6 Conclusion", "content": "This paper presents BI-MDRG, a novel framework for Multimodal Dialogue Re- sponse Generation (MDRG) aimed at bridging the image history for enhanced text and image response. Our model's innovative use of image history to inform both text and image responses addresses fundamental limitations in previous methodologies, particularly in maintaining consistency in multimodal interac- tions. The effectiveness of BI-MDRG has been demonstrated through rigorous evaluations using multiple benchmark datasets and a custom-annotated dataset."}, {"title": "A Limitations", "content": "Our framework relies on customized text-to-image models to ensure image con- sistency in multimodal dialogues. While these models generally offer better con- sistency than standard text-to-image models without conditioning, they are not infallible and may sometimes fail to accurately capture the conditioned input image. This represents a current limitation of our work. However, with the rapid advancements in customized text-to-image generation, we expect these short- comings to decrease over time."}, {"title": "B Broader Impact", "content": "It is crucial to emphasize that the main contribution of our work is not the cus- tomized text-to-image model itself but the overall framework that facilitates its effective use in multimodal dialogue scenarios. By focusing on enhancing image consistency, our framework opens up new avenues for more coherent and en- gaging multimodal interactions. This underscores the potential of our approach in revolutionizing how conversational agents handle multimodal inputs and re- sponses, paving the way for more sophisticated and human-like dialogue systems."}, {"title": "C Benchmark Datasets", "content": ""}, {"title": "C.1 Categorization of Existing Multimodal Dialogue Datasets.", "content": "As stated in Section 2.1, Multimodal dialogue datasets generally fall into three categories: question and answering (Q&A), in-scene, and conversation-based. In Table 5, we summarize the datasets for each category."}, {"title": "C.2 PhotoChat Dataset", "content": "PhotoChat [47] features dialogues collected from social media, where a single image is shared in one of the conversation turns, which mirrors everyday human interaction. An example of PhotoChat dialogue is shown in Figure 10-(a)."}, {"title": "C.3 MMDialog Dataset", "content": "The limited scale and domain diversity of the PhotoChat dataset restricts its applicability. Overcoming these limitations, MMDialog [9] features over a million diverse dialogues from social media, where multiple images are shared across numerous conversation turns, providing a more realistic representation of open- domain multimodal conversations. An example of MMDialog dialogue is shown in Figure 10-(b)."}, {"title": "C.4 ImageChat Dataset", "content": "To evaluate the image-grounding advantage of our BI-MDRG to the previous system, we use the ImageChat Dataset [42]. This dataset has three turns of conversation about a given image. An example of ImageChat Dialogue is shown in Figure 10-(c)."}, {"title": "C.5 Multimodal Dialogue Image Consistency (MDIC) Dataset", "content": "The challenge of ensuring consistent image generation in multimodal dialogue systems is amplified by the absence of datasets annotated for entity consistency across conversational images. We developed the Citation Module for our BI- MDRG system to address this gap. This module is designed to pseudo-label the"}, {"title": "D Details on LLMCite", "content": "In Sections 4.4 and 5, we employ a baseline citation approach, LLMCite, illus- trated in Figure 13, which leverages an instruction-tuned large language model (LLM) to assign citation tags (specifically, we use OpenChat 3.5 (7B)4). From the MDIC dataset, we frame citation tag prediction as a multiclass classification task. Given a dialogue history $D = {(\\text{r}^{\\text{Text}}_i, \\text{r}^{\\text{image}}_i)}_{i=1}^{t-1}$, we first convert images into textual descriptions to form {$\\text{r}^{\\text{Text}}$, $u'_{1}$...,$\\text{r}^{\\text{Text}}$, $u'_{t}$}. For the last turn t, we preprocess ut to include only up to the principal object ot, denoted as ut. For preceding turns $u_{1:t-1}$, we append classification tags $C_{1:t-1}$ (sequentially labeled as (a), (b), (c), ...) to principal objects $o_{1:t-1}$, resulting in augmented descrip- tions $u'_{1:t-1}$. This modified sequence {$\\text{r}^{\\text{Text}}$, $u'_{1}$...,$\\text{r}^{\\text{Text}}$, $\\hat{u}_{t}$} is then provided to the LLM with instructions, as illustrated in Figure 14, to choose the most appropriate $c_{i:t-1}$ matching $\\hat{o}_t$ within the dialogue context."}, {"title": "E Additional Examples", "content": "In Section 5, we demonstrated that merely increasing the model size does not enhance image consistency. This limitation arises because the framework relies on text as an intermediary step for generating image responses, leading to an in- herent loss of image information. ChatGPT also operates within this framework, utilizing text as an intermediary due to the challenges and infeasibility of imple- menting an end-to-end model, a point underscored in Section 5. Consequently, our proposed framework, specifically designed to maintain image consistency, becomes critical. Figure 15 illustrates that ChatGPT also struggles to maintain image consistency, reinforcing the need for our targeted framework."}]}