{"title": "CLUSTER AND SEPARATE: A GNN APPROACH T\u03a4\u039f VOICE AND STAFF PREDICTION FOR SCORE ENGRAVING", "authors": ["Francesco Foscarin", "Emmanouil Karystinaios", "Eita Nakamura", "Gerhard Widmer"], "abstract": "This paper approaches the problem of separating the notes from a quantised symbolic music piece (e.g., a MIDI file) into multiple voices and staves. This is a fundamental part of the larger task of music score engraving (or score type-setting), which aims to produce readable musical scores for human performers. We focus on piano music and support homophonic voices, i.e., voices that can contain chords, and cross-staff voices, which are notably difficult tasks that have often been overlooked in previous research. We propose an end-to-end system based on graph neural networks that clusters notes that belong to the same chord and connects them with edges if they are part of a voice. Our results show clear and consistent improvements over a previous approach on two datasets of different styles. To aid the qualitative analysis of our results, we support the export in symbolic music formats and provide a direct visualisation of our outputs graph over the musical score. All code and pre-trained models are available at https://github.com/CPJKU/piano_svsep.", "sections": [{"title": "1. INTRODUCTION", "content": "The musical score is an important tool for musicians due to its ability to convey musical information in a compact graphical form. Compared to other music representations that may be easier to define and process for machines, for example, MIDI files, the musical score is characterized by how efficiently trained musicians can read it.\nAn important factor that affects the readability of a musical score for instruments that can produce more than one note simultaneously, is the separation of notes into different voices (see Figure 1). This division may follow what a listener perceives as independent auditory streams [1], which can also be reflected in a clearer visual rendition of a musical score [2]. A similar point can be made for the division into multiple staves (generally 2) for instruments with a large pitch range, such as piano, organ, harp, or marimba. We will consider in this paper piano music.\nThe term voice is frequently used to describe a sequence of musical notes that do not overlap, which we call a monophonic voice. However, this definition may be insufficient when considering polyphonic instruments. Voices could contain chords, which are groups of synchronous notes (i.e., notes with the same onset and offset) and are perceived as a single entity [3]. We name a voice that can contain chords a homophonic voice. Note that partially overlapping notes cannot be part of a homophonic voice.\nMusic encoded in MIDI (or similar) formats, even when containing quantized notes, time signature, or bar information, often does not contain voice and staff information. The same can be said for the output of music generation [4], transcription [5], or arranging [6] systems. Therefore, such music cannot be effectively converted into a musical score, to be efficiently read and played by human musicians.\nThe tasks of producing voice and staff information from unstructured symbolic music input are called voice separation (or voice segregation in some papers [3]) and staff separation, respectively.\nMost of the existing approaches to voice separation have focused only on music with monophonic voices [7-12], which is not sufficient for our goal of engraving2 piano music. The task of homophonic voice separation is much harder to solve: the presence of chords within voices makes the space of solutions grow much bigger; and the choice of the \"true voice separation\" can be ambiguous, with multiple valid alternatives among which experts may disagree.\nThe existing approaches to homophonic voice separation can be divided into two groups: the first [1, 3, 5, 13] use dynamic programming algorithms based on a set of heuristics, which makes for systems that are controllable and interpretable, but also hard to develop and tune. Such systems are often prone to fail on exceptions and corner cases that are present in musical pieces. The second group of approaches [14-17] applies deep learning models to predict a voice label for each note. Such an approach creates"}, {"title": "2. RELATED WORK", "content": "The most influential work for this paper is the monophonic voice separation system by Karystinaios et al. [12]. Similarly, we consider voice separation a edge prediction task and use a similar score-to-graph routine and the same GNN encoder. Differently from that work, we consider homophonic voices and staves and, therefore, we extend the model formulation, the deep learning architecture, and the postprocessing routine to deal with this information.\nShibata et al. [5] developed a voice and staff separation technique applied after music transcription to quantized MIDI files. It works in two stages: first, an HMM separates the notes of the two hands (which will then be used as staff), and then a dynamic programming algorithm that maximizes the adherence to a set of heuristics is applied to separate voices in the two hands independently. We compare against this method since it is the most recent approach focusing specifically on homophonic voice separation.\nThere are some approaches based on neural networks [14-17], but they never perform this task in isolation, but rather in combination with other tasks such as symbolic music transcription, full scorification, and automatic arrangement. This means that they can only train on a much smaller dataset and a comparison would not be fair.\nAll the approaches mentioned before, except [12], perform voice separation as a label prediction task, which is problematic, as discussed in the introduction, due to the label imbalance and choice of the maximum number of voices. The former is particularly problematic for the neural network approaches."}, {"title": "3. METHODOLOGY", "content": "Our system inputs data in the form of a set of quantized notes (e.g., coming from a quantized MIDI or a digitized musical score), each characterized by pitch, onset, and offset. This information is modeled as a graph, which we call input graph, and then passed through a GNN model to predict an output graph containing information about voices, staves, and chord groupings. We remind the reader that in our 'homophonic voice' scenario, chords are groups of synchronous notes that belong to the same voice.\n3.1 Input Graph\nFrom the set of quantized notes representing a musical piece we create a directed heterogeneous graph [18] $G_{in} = (V, E_{in}, R_{in})$ where each node $v \\in V$ corresponds to one and only one note, and the edges $e \\in E_{in}$ of type $r \\in R_{in}$ model temporal relations between notes [12]. $R_{in}$ includes 4 types of relations: onset, during, follow, and silence, corresponding, respectively, to two notes starting at the same time, a note starting while the other is sounding, a note starting when the other ends, and a note starting after a time when no note is sounding. We also create the inverse edges for during, follows, and silence relations. Each node corresponds to a vector of features: one of the 12 note pitch classes 3 (C, C#, D, etc.), the octave in [1, . . ., 7], the note duration, encoded as a float value $d \\in [0, 1]$ computed as the ratio of the note and bar durations, passed through a tanh function to limit its value and boost resolution for shorter"}, {"title": "3.2 Output Graph", "content": "The output graph $G_{out} = (V, E_{out}, R_{out})$ has the same set $V$ of nodes as the input graph, but a staff number in {0, 1} is assigned to every node. There are two edge types in $E_{out}$: chord and voice, i.e. $R_{out}$ = {\"chord\", \"voice\"}.\nVoice edges [8, 12] are an alternative in the literature to the more straightforward approach of predicting a voice number for every note; the usage of voice edges has the advantage of enabling a system to work with a non-specified number of voices, and avoiding the label imbalance problem for high voice numbers. Voice edges are directed edges that connect consecutive notes (without considering rests) in the same voice. Formally, let $u_1, u_2 \\in V$ be two notes in the same voice then $(u_1, \\text{\"voice\"}, u_2) \\in E_{out}$ if and only if offset($u_1$) < onset($u_2$) and $ \\nexists u_3 \\in V$ within the same voice such that offset($u_1$) < onset($u_3$) < onset($u_2$).\nThe previous definition also holds in our setting with homophonic voices. Let us extend the definition of chord (a set of synchronous notes) to include the limit case of a single note. Two chords are consecutive if any two notes, respectively, from the first and second chords are consecutive. In the case of two consecutive chords with m and n notes in the same voice, there will be m * n voice edges.\nChord edges are undirected and connect all notes that belong to the same chord without self-loops, so for a n-note chord, there are $ \\frac{n(n - 1)}{2}$ edges. They serve to unambiguously identify which notes together form a single chord.\nThe same output graph can be created from an already properly engraved score. To obtain the graph we only need to draw the true voice edges between consecutive notes in the same voice within a bar and for chord edges we draw the chord ground truth between synchronous notes with the same voice number assignment. This graph can subsequently serve as the ground truth during training."}, {"title": "3.3 Problem Simplification", "content": "In this section, we apply some obvious musical constraints to reduce computation and memory usage in our pipeline, without impacting the results. Let us first focus on chord edge prediction. Given the simple constraint that all notes of a chord must start and end simultaneously, we can restrict the chord edge prediction process to only consider pairs of sychronous notes (same onset and offset values) as candidates. We do this by creating a set of chord edge candidates $A_{chord}$ which are calculated automatically and associated with our input graph. Only notes connected by such candidate edges will be considered in the chord prediction part of the model (see next section).\nThe same reasoning can be applied to the voice edges, by creating a set of voice edge candidates $A_{voice}$ such that $ \\forall u_1, u_2 \\in V, (u_1, \\text{\"voice\"}, u_2) \\in A_{voice}$ only when offset($u_1$) > onset($u_2$). Another step can be taken towards reducing the number of candidates in the set $A_{voice}$ by incorporating some musical engraving considerations.\nThe separation of notes in multiple voices does not have to be consistent in the whole score, but only within each bar, to produce the intended visual representation. There are no graphical elements that show if two notes in different bars are or are not in the same voice4. Music engraving software does not force users to use consistent voices across bars. This can be often observed in digitized musical scores where music motives that belong to the same voice, are assigned different voices in different bars. Such observations have motivated projects such as the Symbolic Multitrack Contrapuntal Music Archive [20] that explicitly encode a \"global\" voice number.\nSince cross-bar consistency is not necessary for our goal of engraving (and is often wrongly annotated in our data) we limit the voice edge candidates $A_{voice}$ to contain only pairs of notes that occur within the same bar. This design choice is also reflected in our evaluation, i.e. we do not evaluate how the voices propagate across bars, but only within each bar. Note that this process is different from processing each bar independently since our network (detailed in the next section) considers music content across bars."}, {"title": "3.4 Model", "content": "We design an end-to-end model (see Figure 2) that receives an input graph as described in Section 3.1 and produces an output graph as in Section 3.2. The model is organized as an encoder-decoder architecture.\nThe encoder receives an input graph created from a quantized MIDI score and passes it through three stacked Graph Convolutional Network (GCN) blocks to produce a node embedding for each note. We use the heterogeneous version of the Sage convolutional block [18] with a hidden size of 256; the update function for each node u is described by:\n$h_v^{(l+1)} = \\sum_{v \\in N(u)} ({h_v^{(l)} + h_u^{(l)}})$\n$h_u^{(l+1)} = \\sigma (W \\cdot concat(h_u^{(l)}, \\bar{h_u^{(l)}}))$\nwhere $N(u)$ are the neighbors of node u, $\\sigma$ is a non-linear activation function, $W$ is a learnable weight matrix.\nThe decoder consists of three parts that all use the same node embedding as input: i) a staff predictor; ii) a voice edge predictor; and iii) a chord clustering (i.e., a chord edge predictor). The staff predictor is a 2-layer Multi-Layer Perceptron (MLP) classifier that produces probabilities for each graph node (i.e., each note) to belong to the first or second staff. The voice edge predictor receives the embeddings of pairs of notes connected by edge candidates and produces a probability for each pair to be in the same voice. It works by concatenating the pairs of note embeddings and applying a 2-layer MLP. The final decoder part, chord clustering, receives the embeddings of pairs of notes connected by chord edge candidates (i.e., pairs of synchronous notes) and produces the probability for a pair to be merged into a chord. This is achieved by computing the cosine similarity between the elements of the pair. This process forces the"}, {"title": "3.5 Postprocessing", "content": "A straightforward approach to deciding whether to connect two notes with a voice edge would be to threshold the predicted voice edge probabilities. However, even when using edge and chord candidates, we could still produce three kinds of invalid output: (1) multiple voices merging into one voice, (2) one voice splitting into multiple voices, and (3) notes in the same chord that are not in the same voice. To eliminate these issues, we add a postprocessing phase that accompanies our model and guarantees a valid output according to music engraving rules.\nThe first step, which we call chord pooling, merges all nodes that belong to the same chord to a single new \"virtual node\". This is done by looking for the connected components considering only chord edges in the output graph, then pooling in a single node all original nodes in each connected component, creating a new node which has as incoming and outgoing voice edges all edges entering and exiting the original nodes, respectively. If multiple edges collapse in one edge (e.g. in the case of two consecutive chords in the same voice), the new edge has a probability that is the average of the corresponding edge probabilities.\nAfter the first step, we are left with monophonic streams, which could still exhibit problems (1) and (2). We can solve this with the technique proposed in [12] for monophonic voices, i.e. by framing the voice assignment problem as a linear assignment problem [21] over the adjacency matrix obtained by the updated edge candidates $A_{voice}$. We follow the linear assignment step by unpooling or unmerging the nodes that were previously pooled, in this way, obtaining the original nodes again. During unpooling, the incoming edges and outgoing edges of the \"virtual nodes\" are reassigned to each original node, thus resolving problem (3)."}, {"title": "3.6 Evaluation", "content": "We evaluate the predicted voice assignments with the metric proposed by Hiramatsu et al. [15], which formalizes the metric of McLeod and Steedman [22]. This is a version of the F1-score for voice separation [8] which is adapted to work on homophonic voices, by reducing the importance of notes if they are part of a chord. This is important since chords create many voice edges (e.g., two 4-note chords in the same voice are connected by 16 edges), which could potentially overshadow the importance of edges in monophonic voices (or voices with fewer/smaller chords).\nFormally, the homophonic voice F1-score F1 is calculated as:\n$P = \\frac{\\sum_{i<j} a_{ij} \\hat{a_{ij}} / W_i}{\\sum_{i<j} a_{ij} / W_i}$, $R = \\frac{\\sum_{i<j} a_{ij} \\hat{a_{ij}} / W_i}{\\sum_{i<j} \\hat{a_{ij}} / W_i}$\n$F1 = \\frac{2PR}{P+R}$\nwhere i < j, in the sum, considers all pair of notes i, j such that offset(i) < onset(j); $a_{ij}, \\hat{a_{ij}}$ are equal to 1 or 0 if a voice edge exists or not in the ground truth and predictions, respectively; and $w_i$ and $w_r$ are the number of notes that are chorded together with the note i in the ground truth and predictions, respectively. Unlike [15], we consider"}, {"title": "3.7 From Network Prediction to Readable Output", "content": "The computation of voice and staff numbers is sufficient for the system evaluation, but not for producing a usable tool, which we are interested in in this paper. The missing step, to be described in this section, is the integration of the network predictions into a readable musical score. To achieve this integration we need to undertake two essential steps: beam together notes within the same voice, and infill rests to \"fill holes\" within each voice.\nFor the first step, we proceed according to the rules of engraving [2]. We beam two consecutive notes (or chords) in the same voices if their duration is less than a quarter note (excluding ties) unless they belong to different beats. Following the music notation convention we consider the compound time signatures $\\frac{6}{8}, \\frac{9}{8}, \\frac{12}{8}$ to have, respectively, 2, 3, and 4 beats. When confronted with tied notes, the algorithm prioritizes producing notations with the fewest number of notes, an heuristic with promotes easier-to-read notation [23].\nThe second step consists of introducing rests so that each voice fills the entire bar and can be correctly displayed. Some rests could be set as invisible to improve the graphical output when their presence and duration are easy to assume from other score elements, but we display all of them for simplicity. As for the notes, we choose the rest types (with eventual dots) to minimize the number of rests in the score.\nThe two steps described above cover common cases and produce a complete score in MEI format [24]. However, the score export is still a prototype, since developing one that is robust against all corner cases is an extremely complex task, and is outside the scope of this paper. Since score output problems may obscure the output of our system, we also develop a graph visualization tool. Both the input and output graphs (including the candidate edges) can be displayed on top of the musical score in an interactive web-based interface based on Verovio [25]. Some examples of the output graph visualization are in Figure 4."}, {"title": "4. EXPERIMENTS", "content": "We train our model with the ADAM optimizer with a learning rate of 0.001 and a weight decay of 5 * 10-4 for 100 epochs. For a quantitative evaluation, we compare our results with those of a baseline algorithm and the method proposed by Shibata et al. [5], on two rather diverse datasets.\nOur baseline algorithm assigns all notes under C4 (middle C) to the second staff and the rest to the first. Then it groups all synchronous notes (per staff) as chords. Finally, it uses the time and pitch distances between the candidate pairs of notes as weights to be minimized during the linear assignment process (the same as we use in our postprocessing) which creates the voice edges."}, {"title": "4.1 Datasets", "content": "We use two piano datasets of different styles and difficulties to evaluate our system under diverse conditions. The ability to handle complex corner cases should not reduce the performance on easier (and more common) pieces.\nThe J-Pop dataset contains pop piano scores introduced by [5]. Most of the scores consist of accompaniment chords on the lower staff and some simple melodic lines on the upper staff. The dataset contains 811 scores; we randomly sampled 159 (roughly 20%) of these for testing and used the rest for training and validation.\nThe DCML Romantic Corpus is more challenging. It was created by [26] and contains piano pieces from the 17th to 20th centuries with some virtuosic quality. It includes characteristics such as cross-staff beaming, a high number of voices, challenging voicing, etc. Similarly to the pop dataset we randomly sample 77 out of the 393 scores (approx. 20%) and use the rest for training and validation.\nThe J-Pop dataset is available in MusicXML format, while the DCML Romantic Corpus is in Musescore file format. We use Musescore to convert DCML files to MusicXML and load them with the Python library Partitura [27] to extract the note list."}, {"title": "4.2 Results", "content": "Our model aims to be generic across a variety of music, therefore we train a single model on the joined training set of pop and classical scores, not two individual ones. The rules that govern the handling of voices may be fundamentally different in the two datasets, but we assign to the model the task of handling these differences. This approach ensures better future scalability on bigger and more diverse datasets. We compute the metrics separately on the test part of our two datasets.\nTable 1 reports results for three versions of our graph-based model: the complete model from Section 3, a variant without postprocessing, and a variant without chord prediction and postprocessing (our postprocessing technique cannot be run without the chord prediction task, since it pools nodes that belong to the same predicted chord). The method of Shibata requires the specification of the number of voices per staff. For compactness, we report only the results with one voice per staff (2 voices total); the results degrade by increasing the number of voices.\nOur results show that even our system without pooling and without postprocessing obtains consistently better results than both Shibata et al. [5] and our baseline. Interestingly, the chord prediction task improves the Voice F1 results even when the post-processing is not used; this confirms the benefits of multi-task training, and of enforcing notes of the same chord to have similar representations in the hidden space, with cosine similarity, to predict coherent voice edges. However, we observe a reduction in staff accuracy, probably for the same reason, since the same hidden representation is also used to predict chords, making it harder (though not impossible) to split notes of the same chord in different staves. When the full system is used, there are further improvements in Voice F1."}, {"title": "4.3 Qualitative Analysis", "content": "Let us take a closer look into the predictions of our deep-learning approach (GNN) on the excerpt of Figure 4 produced by our visualization tool. Our approach captures correctly the cross-staff voice in the first two bars, while such a situation causes performance degradation for all other voice separation approaches that don't support it. We observe some disagreements with the original score in Measure 3: our model predicts a single chord (instead of splitting across the staff) containing all the synchronous syncopated quarter notes, and also mispredicts the staff for the first D#4 note. A more in-depth study of why this happens is not trivial, as neural networks are not interpretable. This is a drawback compared to heuristic-based separation techniques.\nSynchronous notes with the same pitch are problematic."}, {"title": "5. CONCLUSION AND FUTURE WORK", "content": "This paper presented a novel graph-based method for homophonic voice separation and staff prediction in symbolic piano music. Our experiments highlight our system's effectiveness compared to previous approaches. Notably, we obtained consistent improvements over two datasets of different styles with a single model.\nFuture work will focus on integrating grace notes and the possibility of multiple voices converging on a single note. We aim to create a framework that produces complete engravings from quantized MIDI, including the prediction of clef changes, beams, pitch spelling, and key signatures."}]}