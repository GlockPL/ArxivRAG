{"title": "CLUSTER AND SEPARATE: A GNN APPROACH T\u03a4\u039f\nVOICE AND STAFF PREDICTION FOR SCORE ENGRAVING", "authors": ["Francesco Foscarin", "Emmanouil Karystinaios", "Eita Nakamura", "Gerhard Widmer"], "abstract": "This paper approaches the problem of separating the notes\nfrom a quantised symbolic music piece (e.g., a MIDI file)\ninto multiple voices and staves. This is a fundamental part\nof the larger task of music score engraving (or score type-\nsetting), which aims to produce readable musical scores for\nhuman performers. We focus on piano music and support\nhomophonic voices, i.e., voices that can contain chords,\nand cross-staff voices, which are notably difficult tasks\nthat have often been overlooked in previous research. We\npropose an end-to-end system based on graph neural net-\nworks that clusters notes that belong to the same chord\nand connects them with edges if they are part of a voice.\nOur results show clear and consistent improvements over\na previous approach on two datasets of different styles.\nTo aid the qualitative analysis of our results, we support\nthe export in symbolic music formats and provide a di-\nrect visualisation of our outputs graph over the musical\nscore. All code and pre-trained models are available at\nhttps://github.com/CPJKU/piano_svsep.", "sections": [{"title": "1. INTRODUCTION", "content": "The musical score is an important tool for musicians due\nto its ability to convey musical information in a compact\ngraphical form. Compared to other music representations\nthat may be easier to define and process for machines, for\nexample, MIDI files, the musical score is characterized by\nhow efficiently trained musicians can read it.\nAn important factor that affects the readability of a mu-\nsical score for instruments that can produce more than one\nnote simultaneously, is the separation of notes into different\nvoices (see Figure 1). This division may follow what a lis-\ntener perceives as independent auditory streams [1], which\ncan also be reflected in a clearer visual rendition of a musi-\ncal score [2]. A similar point can be made for the division\ninto multiple staves (generally 2) for instruments with a\nlarge pitch range, such as piano, organ, harp, or marimba.\nWe will consider in this paper piano music.\nThe term voice is frequently used to describe a sequence\nof musical notes that do not overlap, which we call a mono-\nphonic voice. However, this definition may be insufficient\nwhen considering polyphonic instruments. Voices could\ncontain chords, which are groups of synchronous notes (i.e.,\nnotes with the same onset and offset) and are perceived as a\nsingle entity [3]. We name a voice that can contain chords\na homophonic voice. Note that partially overlapping notes\ncannot be part of a homophonic voice.\nMusic encoded in MIDI (or similar) formats, even when\ncontaining quantized notes, time signature, or bar informa-\ntion, often does not contain voice and staff information.\nThe same can be said for the output of music generation [4],\ntranscription [5], or arranging [6] systems. Therefore, such\nmusic cannot be effectively converted into a musical score,\nto be efficiently read and played by human musicians. 1\nThe tasks of producing voice and staff information from\nunstructured symbolic music input are called voice sepa-\nration (or voice segregation in some papers [3]) and staff\nseparation, respectively.\nMost of the existing approaches to voice separation have\nfocused only on music with monophonic voices [7-12],\nwhich is not sufficient for our goal of engraving2 piano\nmusic. The task of homophonic voice separation is much\nharder to solve: the presence of chords within voices makes\nthe space of solutions grow much bigger; and the choice of\nthe \"true voice separation\" can be ambiguous, with multiple\nvalid alternatives among which experts may disagree.\nThe existing approaches to homophonic voice separa-\ntion can be divided into two groups: the first [1, 3, 5, 13]\nuse dynamic programming algorithms based on a set of\nheuristics, which makes for systems that are controllable\nand interpretable, but also hard to develop and tune. Such\nsystems are often prone to fail on exceptions and corner\ncases that are present in musical pieces. The second group\nof approaches [14-17] applies deep learning models to pre-\ndict a voice label for each note. Such an approach creates"}, {"title": "2. RELATED WORK", "content": "The most influential work for this paper is the monophonic\nvoice separation system by Karystinaios et al. [12]. Simi-\nlarly, we consider voice separation a edge prediction task\nand use a similar score-to-graph routine and the same GNN\nencoder. Differently from that work, we consider homo-\nphonic voices and staves and, therefore, we extend the\nmodel formulation, the deep learning architecture, and the\npostprocessing routine to deal with this information.\nShibata et al. [5] developed a voice and staff separation\ntechnique applied after music transcription to quantized\nMIDI files. It works in two stages: first, an HMM separates\nthe notes of the two hands (which will then be used as staff),\nand then a dynamic programming algorithm that maximizes\nthe adherence to a set of heuristics is applied to separate\nvoices in the two hands independently. We compare against\nthis method since it is the most recent approach focusing\nspecifically on homophonic voice separation.\nThere are some approaches based on neural net-\nworks [14-17], but they never perform this task in isolation,\nbut rather in combination with other tasks such as sym-\nbolic music transcription, full scorification, and automatic\narrangement. This means that they can only train on a much\nsmaller dataset and a comparison would not be fair.\nAll the approaches mentioned before, except [12], per-\nform voice separation as a label prediction task, which is\nproblematic, as discussed in the introduction, due to the\nlabel imbalance and choice of the maximum number of\nvoices. The former is particularly problematic for the neu-\nral network approaches."}, {"title": "3. METHODOLOGY", "content": "Our system inputs data in the form of a set of quantized\nnotes (e.g., coming from a quantized MIDI or a digitized\nmusical score), each characterized by pitch, onset, and\noffset. This information is modeled as a graph, which we\ncall input graph, and then passed through a GNN model\nto predict an output graph containing information about\nvoices, staves, and chord groupings. We remind the reader\nthat in our 'homophonic voice' scenario, chords are groups\nof synchronous notes that belong to the same voice.\n3.1 Input Graph\nFrom the set of quantized notes representing a musical\npiece we create a directed heterogeneous graph [18] $G_{in}$ =\n$(V, E_{in}, R_{in})$ where each node $v \u2208 V$ corresponds to one\nand only one note, and the edges $e \u2208 E_{in}$ of type $r \u2208 R_{in}$\nmodel temporal relations between notes [12]. $R_{in}$ includes\n4 types of relations: onset, during, follow, and silence,\ncorresponding, respectively, to two notes starting at the\nsame time, a note starting while the other is sounding, a\nnote starting when the other ends, and a note starting after a\ntime when no note is sounding. We also create the inverse\nedges for during, follows, and silence relations. Each node\ncorresponds to a vector of features: one of the 12 note pitch\nclasses 3 (C, C#, D, etc.), the octave in [1, . . ., 7], the note\nduration, encoded as a float value $d \u2208 [0, 1]$ computed as\nthe ratio of the note and bar durations, passed through a tanh\nfunction to limit its value and boost resolution for shorter"}, {"title": "3.2 Output Graph", "content": "The output graph $G_{out}$ = $(V, E_{out}, R_{out})$ has the same set\n$V$ of nodes as the input graph, but a staff number in {0, 1}\nis assigned to every node. There are two edge types in $E_{out}$:\nchord and voice, i.e. $R_{out}$ = {\u201cchord\u201d, \u201cvoice\u201d}.\nVoice edges [8, 12] are an alternative in the literature\nto the more straightforward approach of predicting a voice\nnumber for every note; the usage of voice edges has the\nadvantage of enabling a system to work with a non-specified\nnumber of voices, and avoiding the label imbalance problem\nfor high voice numbers. Voice edges are directed edges that\nconnect consecutive notes (without considering rests) in\nthe same voice. Formally, let $u_1, u_2 \u2208 V$ be two notes in\nthe same voice then $(u_1, \\text{\"voice\"}, u_2) \u2208 E_{out}$ if and only\nif $offset(u_1) < onset(u_2)$ and $\u2204 u_3 \u2208 V$ within the same\nvoice such that $offset(u_1) < onset(u_3) < onset(u_2)$.\nThe previous definition also holds in our setting with\nhomophonic voices. Let us extend the definition of chord\n(a set of synchronous notes) to include the limit case of a\nsingle note. Two chords are consecutive if any two notes,\nrespectively, from the first and second chords are consecu-\ntive. In the case of two consecutive chords with $m$ and $n$\nnotes in the same voice, there will be $m * n$ voice edges.\nChord edges are undirected and connect all notes that\nbelong to the same chord without self-loops, so for a $n$-note\nchord, there are $\\frac{n(n - 1)}{2}$ edges. They serve to unambigu-\nously identify which notes together form a single chord.\nThe same output graph can be created from an already\nproperly engraved score. To obtain the graph we only need\nto draw the true voice edges between consecutive notes\nin the same voice within a bar and for chord edges we\ndraw the chord ground truth between synchronous notes\nwith the same voice number assignment. This graph can\nsubsequently serve as the ground truth during training."}, {"title": "3.3 Problem Simplification", "content": "In this section, we apply some obvious musical constraints\nto reduce computation and memory usage in our pipeline,\nwithout impacting the results. Let us first focus on chord\nedge prediction. Given the simple constraint that all notes\nof a chord must start and end simultaneously, we can restrict\nthe chord edge prediction process to only consider pairs of\nsychronous notes (same onset and offset values) as candi-\ndates. We do this by creating a set of chord edge candidates\n$\\\\text{A}$ which are calculated automatically and associated with\nour input graph. Only notes connected by such candidate\nedges will be considered in the chord prediction part of the\nmodel (see next section).\nThe same reasoning can be applied to the voice edges,\nby creating a set of voice edge candidates $\\\\text{A}$ such\nthat $\u2200u_1, u_2 \u2208 V, (u_1, \\text{\"voice\"}, u_2) \u2208 A$ only when\n$offset(u_1) > onset(u_2)$. Another step can be taken to-\nwards reducing the number of candidates in the set $\\\\text{A}$ by\nincorporating some musical engraving considerations.\nThe separation of notes in multiple voices does not have\nto be consistent in the whole score, but only within each\nbar, to produce the intended visual representation. There\nare no graphical elements that show if two notes in different\nbars are or are not in the same voice4. Music engraving\nsoftware does not force users to use consistent voices across\nbars. This can be often observed in digitized musical scores\nwhere music motives that belong to the same voice, are as-\nsigned different voices in different bars. Such observations\nhave motivated projects such as the Symbolic Multitrack\nContrapuntal Music Archive [20] that explicitly encode a\n\"global\" voice number.\nSince cross-bar consistency is not necessary for our goal\nof engraving (and is often wrongly annotated in our data)\nwe limit the voice edge candidates $\\\\text{A}$ to contain only pairs\nof notes that occur within the same bar. This design choice\nis also reflected in our evaluation, i.e. we do not evaluate\nhow the voices propagate across bars, but only within each\nbar. Note that this process is different from processing each\nbar independently since our network (detailed in the next\nsection) considers music content across bars."}, {"title": "3.4 Model", "content": "We design an end-to-end model (see Figure 2) that receives\nan input graph as described in Section 3.1 and produces an\noutput graph as in Section 3.2. The model is organized as\nan encoder-decoder architecture.\nThe encoder receives an input graph created from a quan-\ntized MIDI score and passes it through three stacked Graph\nConvolutional Network (GCN) blocks to produce a node\nembedding for each note. We use the heterogeneous version\nof the Sage convolutional block [18] with a hidden size of\n256; the update function for each node $u$ is described by:\n$h_u^{(l+1)} = \\sum_{v \u2208 N(u)} (\\{h_v^{(l)}\\})$\n$h_u^{(l+1)} = \u03c3 (W \\cdot concat(h_u, h_u^{(l)}))$ (1)\nwhere $N(u)$ are the neighbors of node $u$, $\u03c3$ is a non-linear\nactivation function, $W$ is a learnable weight matrix.\nThe decoder consists of three parts that all use the same\nnode embedding as input: i) a staff predictor; ii) a voice\nedge predictor; and iii) a chord clustering (i.e., a chord\nedge predictor). The staff predictor is a 2-layer Multi-Layer\nPerceptron (MLP) classifier that produces probabilities for\neach graph node (i.e., each note) to belong to the first or\nsecond staff. The voice edge predictor receives the embed-\ndings of pairs of notes connected by edge candidates and\nproduces a probability for each pair to be in the same voice.\nIt works by concatenating the pairs of note embeddings and\napplying a 2-layer MLP. The final decoder part, chord clus-\ntering, receives the embeddings of pairs of notes connected\nby chord edge candidates (i.e., pairs of synchronous notes)\nand produces the probability for a pair to be merged into a\nchord. This is achieved by computing the cosine similarity\nbetween the elements of the pair. This process forces the"}, {"title": "3.5 Postprocessing", "content": "A straightforward approach to deciding whether to connect\ntwo notes with a voice edge would be to threshold the\npredicted voice edge probabilities. However, even when\nusing edge and chord candidates, we could still produce\nthree kinds of invalid output: (1) multiple voices merging\ninto one voice, (2) one voice splitting into multiple voices,\nand (3) notes in the same chord that are not in the same\nvoice. To eliminate these issues, we add a postprocessing\nphase that accompanies our model and guarantees a valid\noutput according to music engraving rules.\nThe first step, which we call chord pooling, merges\nall nodes that belong to the same chord to a single new\n\"virtual node\". This is done by looking for the connected\ncomponents considering only chord edges in the output\ngraph, then pooling in a single node all original nodes in\neach connected component, creating a new node which has\nas incoming and outgoing voice edges all edges entering and\nexiting the original nodes, respectively. If multiple edges\ncollapse in one edge (e.g. in the case of two consecutive\nchords in the same voice), the new edge has a probability\nthat is the average of the corresponding edge probabilities.\nAfter the first step, we are left with monophonic streams,\nwhich could still exhibit problems (1) and (2). We can solve\nthis with the technique proposed in [12] for monophonic\nvoices, i.e. by framing the voice assignment problem as a\nlinear assignment problem [21] over the adjacency matrix\nobtained by the updated edge candidates A. We follow\nthe linear assignment step by unpooling or unmerging the\nnodes that were previously pooled, in this way, obtaining the\noriginal nodes again. During unpooling, the incoming edges\nand outgoing edges of the \"virtual nodes\" are reassigned to\neach original node, thus resolving problem (3)."}, {"title": "3.6 Evaluation", "content": "We evaluate the predicted voice assignments with the metric\nproposed by Hiramatsu et al. [15], which formalizes the\nmetric of McLeod and Steedman [22]. This is a version of\nthe F1-score for voice separation [8] which is adapted to\nwork on homophonic voices, by reducing the importance\nof notes if they are part of a chord. This is important since\nchords create many voice edges (e.g., two 4-note chords in\nthe same voice are connected by 16 edges), which could\npotentially overshadow the importance of edges in mono-\nphonic voices (or voices with fewer/smaller chords).\nFormally, the homophonic voice F1-score F1 is calcu-\nlated as:\n$P = \\frac{\\sum_{i<j} a_{ij} \\hat{a_{ij}} / \\hat{w_i}}{\\sum_{i<j} \\hat{a_{ij}} / \\hat{w_i}}$, $R = \\frac{\\sum_{i<j} a_{ij} \\hat{a_{ij}} / w_i}{\\sum_{i<j} a_{ij} / w_i}$\n$F1 = \\frac{2PR}{P+R}$ (2)\nwhere $i < j$, in the sum, considers all pair of notes\n$i, j$ such that $offset(i) < onset(j)$; $a_{ij}, \\hat{a_{ij}}$ are equal to 1\nor 0 if a voice edge exists or not in the ground truth and\npredictions, respectively; and $w_i$ and $\\hat{w_i}$ are the number of\nnotes that are chorded together with the note $i$ in the ground\ntruth and predictions, respectively. Unlike [15], we consider"}, {"title": "3.7 From Network Prediction to Readable Output", "content": "The computation of voice and staff numbers is sufficient\nfor the system evaluation, but not for producing a usable\ntool, which we are interested in in this paper. The missing\nstep, to be described in this section, is the integration of\nthe network predictions into a readable musical score. To\nachieve this integration we need to undertake two essential\nsteps: beam together notes within the same voice, and infill\nrests to \"fill holes\" within each voice.\nFor the first step, we proceed according to the rules of\nengraving [2]. We beam two consecutive notes (or chords)\nin the same voices if their duration is less than a quarter note\n(excluding ties) unless they belong to different beats. Fol-\nlowing the music notation convention we consider the com-\npound time signatures,, i.e., $ \\frac{3}{8}, \\frac{6}{8}, \\frac{9}{8}, \\frac{12}{8}$ to have, respectively,\n2, 3, and 4 beats. When confronted with tied notes, the\nalgorithm prioritizes producing notations with the fewest\nnumber of notes, an heuristic with promotes easier-to-read\nnotation [23].\nThe second step consists of introducing rests so that\neach voice fills the entire bar and can be correctly displayed.\nSome rests could be set as invisible to improve the graphical\noutput when their presence and duration are easy to assume\nfrom other score elements, but we display all of them for\nsimplicity. As for the notes, we choose the rest types (with\neventual dots) to minimize the number of rests in the score.\nThe two steps described above cover common cases and\nproduce a complete score in MEI format [24]. However, the\nscore export is still a prototype, since developing one that\nis robust against all corner cases is an extremely complex\ntask, and is outside the scope of this paper. Since score\noutput problems may obscure the output of our system,\nwe also develop a graph visualization tool. Both the input\nand output graphs (including the candidate edges) can be\ndisplayed on top of the musical score in an interactive web-\nbased interface based on Verovio [25]. Some examples of\nthe output graph visualization are in Figure 4."}, {"title": "4. EXPERIMENTS", "content": "We train our model with the ADAM optimizer with a learn-\ning rate of 0.001 and a weight decay of 5 * 10-4 for 100\nepochs. For a quantitative evaluation, we compare our re-\nsults with those of a baseline algorithm and the method pro-\nposed by Shibata et al. [5], on two rather diverse datasets.\nOur baseline algorithm assigns all notes under C4 (mid-\ndle C) to the second staff and the rest to the first. Then it\ngroups all synchronous notes (per staff) as chords. Finally,\nit uses the time and pitch distances between the candidate\npairs of notes as weights to be minimized during the linear\nassignment process (the same as we use in our postprocess-\ning) which creates the voice edges."}, {"title": "4.1 Datasets", "content": "We use two piano datasets of different styles and difficul-\nties to evaluate our system under diverse conditions. The\nability to handle complex corner cases should not reduce\nthe performance on easier (and more common) pieces.\nThe J-Pop dataset contains pop piano scores introduced\nby [5]. Most of the scores consist of accompaniment chords\non the lower staff and some simple melodic lines on the\nupper staff. The dataset contains 811 scores; we randomly\nsampled 159 (roughly 20%) of these for testing and used\nthe rest for training and validation.\nThe DCML Romantic Corpus is more challenging. It\nwas created by [26] and contains piano pieces from the\n17th to 20th centuries with some virtuosic quality. It in-\ncludes characteristics such as cross-staff beaming, a high\nnumber of voices, challenging voicing, etc. Similarly to the\npop dataset we randomly sample 77 out of the 393 scores\n(approx. 20%) and use the rest for training and validation.\nThe J-Pop dataset is available in MusicXML format,\nwhile the DCML Romantic Corpus is in Musescore file\nformat. We use Musescore to convert DCML files to\nMusicXML and load them with the Python library Par-\ntitura [27] to extract the note list."}, {"title": "4.2 Results", "content": "Our model aims to be generic across a variety of music,\ntherefore we train a single model on the joined training\nset of pop and classical scores, not two individual ones.\nThe rules that govern the handling of voices may be funda-\nmentally different in the two datasets, but we assign to the\nmodel the task of handling these differences. This approach\nensures better future scalability on bigger and more diverse\ndatasets. We compute the metrics separately on the test part\nof our two datasets.\nOur results show that even our system without pool-\ning and without postprocessing obtains consistently better\nresults than both Shibata et al. [5] and our baseline. In-\nterestingly, the chord prediction task improves the Voice\nF1 results even when the post-processing is not used; this\nconfirms the benefits of multi-task training, and of enforc-\ning notes of the same chord to have similar representations\nin the hidden space, with cosine similarity, to predict co-\nherent voice edges. However, we observe a reduction in\nstaff accuracy, probably for the same reason, since the same\nhidden representation is also used to predict chords, making\nit harder (though not impossible) to split notes of the same\nchord in different staves. When the full system is used,\nthere are further improvements in Voice F1."}, {"title": "4.3 Qualitative Analysis", "content": "Let us take a closer look into the predictions of our deep-\nlearning approach (GNN) on the excerpt of Figure 4 pro-\nduced by our visualization tool. Our approach captures cor-\nrectly the cross-staff voice in the first two bars, while such a\nsituation causes performance degradation for all other voice\nseparation approaches that don't support it. We observe\nsome disagreements with the original score in Measure 3:\nour model predicts a single chord (instead of splitting across\nthe staff) containing all the synchronous syncopated quarter\nnotes, and also mispredicts the staff for the first D#4 note.\nA more in-depth study of why this happens is not trivial, as\nneural networks are not interpretable. This is a drawback\ncompared to heuristic-based separation techniques.\nSynchronous notes with the same pitch are problematic."}, {"title": "5. CONCLUSION AND FUTURE WORK", "content": "This paper presented a novel graph-based method for ho-\nmophonic voice separation and staff prediction in symbolic\npiano music. Our experiments highlight our system's ef-\nfectiveness compared to previous approaches. Notably, we\nobtained consistent improvements over two datasets of dif-\nferent styles with a single model.\nFuture work will focus on integrating grace notes and\nthe possibility of multiple voices converging on a single\nnote. We aim to create a framework that produces complete\nengravings from quantized MIDI, including the prediction\nof clef changes, beams, pitch spelling, and key signatures."}]}