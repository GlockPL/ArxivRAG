{"title": "LIFT: IMPROVING LONG CONTEXT UNDERSTANDING\nTHROUGH LONG INPUT FINE-TUNING", "authors": ["Yansheng Mao", "Jiaqi Li", "Fanxu Meng", "Jing Xiong", "Zilong Zheng", "Muhan Zhang"], "abstract": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper introduces Long Input Fine-Tuning\n(LIFT) for long context modeling, a novel framework that enhances LLM perfor-\nmance on long-context tasks by adapting model parameters to the context at test\ntime. LIFT enables efficient processing of lengthy inputs without the computa-\ntional burden of offline long-context adaptation, and can improve the long-context\ncapabilities of arbitrary short-context models. The framework is further enhanced\nby integrating in-context learning and pre-LIFT supervised fine-tuning. The com-\nbination of in-context learning and LIFT enables short-context models like Llama\n3 to handle arbitrarily long contexts and consistently improves their performance\non popular long-context benchmarks like LooGLE and LongBench. We also pro-\nvide a comprehensive analysis of the strengths and limitations of LIFT on long\ncontext understanding, offering valuable directions for future research.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs), such as GPT-4 (Achiam et al., 2023), have revolutionized the field\nof natural language processing, driving breakthroughs in text generation and significant advance-\nments in tasks like translation, summarization, and conversation. Lengthy sequences, which can\nspan up to millions of tokens, are common in real-world applications including long books (Ko\u010disk\u1ef3\net al., 2018), high-resolution videos (Wu et al., 2024; Tapaswi et al., 2016), and audio signals (Yang\net al., 2024). Extending the context window allows models to capture dependencies across larger\ntext spans and improve coherence, understanding, and accuracy in tasks that require reasoning over\nextended inputs.\nHowever, as the context length increases, the computational complexity of the self-attention mech-\nanism (Vaswani, 2017) grows quadratically, which limits the model's ability to process long inputs.\nAdditionally, storing a large number of attention weights and intermediate states places a heavy bur-\nden on hardware resources. Moreover, it's challenging to capture long dependencies among pieces\nof information scattered throughout raw texts and perform further comprehension and reasoning.\nDue to the limitation of context window, LLMs can hardly capture the overall information about a\nuser's query history or task input, resulting in suboptimal performance.\nTo address these challenges, researchers have developed various techniques to improve the\nlong-context abilities of LLMs. A line of research, including Retrieval-Augmented Generation\n(RAG) (Lewis et al., 2020; Xu et al., 2023) and prompt compression (Jiang et al., 2023), prepro-\ncesses inputs and provide a short sequence to LLMs (El-Kassas et al., 2021). However, the ef-\nfectiveness of these methods depends on the precision and relevance of the contextual information\nprovided within the context window. When limited, ambiguous, or conflicting information is pro-\nvided in the context window, it can lead to hallucination. Another line of research, long-context\nadaptation, focuses on fine-tuning pretrained LLMs on corpora of long texts to extend their context\nwindows (Chen et al., 2023b; Peng et al., 2023). However, it comes with significant costs in terms\nof training data and computational resources. Additionally, with the extended context window, the"}, {"title": "2 RELATED WORKS", "content": "Long context adaptation and efficient architectures. One conventional approach for handling\nlong contexts is to place all inputs into the context and leverage in-context learning (ICL). However,\nshort-context models fail to generalize to long contexts due to unseen positional encodings, resulting\nin poor performance on long-context tasks. To address this, many studies fine-tune LLMs on corpora\nof long texts to extend their context window. While this approach effectively enhances long-context\nunderstanding, it comes at the expense of efficiency, as both long-context adaptation and long-\ncontext ICL are computationally expensive.\nTo speed up long-context processing, one popular approach is developing efficient Transformers.\nSparse attention (Kitaev et al., 2020; Wang et al., 2020; Beltagy et al., 2020) reduces memory and\ncomputation costs by using techniques like local windows or strided attention, allowing models to\nfocus on relevant parts of the input. Linear attention (Shen et al., 2021) reduces complexity from\nquadratic to linear by approximating self-attention with kernel functions or low-rank representations.\nAnother direction involves Transformer alternatives, such as state-space models (SSMs), which are\nefficient in both training and inference due to their dual representations. However, these methods\noften reduce expressiveness and are less adopted by modern LLMs. In this work, we focus on the\nconventional self-attention mechanism (Vaswani, 2017).\nRetrieval-Augmented Generation (RAG). RAG (Lewis et al., 2020) improves the efficiency\nand effectiveness of LLMs in long-context understanding by integrating external memory compo-\nnents (Xu et al., 2023; Jiang et al., 2024; Wang et al., 2024a; Jin et al., 2024). It stores information\nover time, allowing the model to recall past information without requiring the entire context to fit\nwithin its context window. However, the retrieval quality is a bottleneck of RAG. Inaccurate or noisy\nretrieval leads to performance degradation and hallucination."}, {"title": "3 METHOD", "content": "As discussed earlier, our method features adaptation and inference with only a short-context model,\nensuring high efficiency. The comparison of our method, LIFT, with other long-context processing\nmethods, ICL and RAG, is illustrated in Table 1. In this section, we present how we implement\nLIFT and address the associated challenges.\n\u2022 In Section 3.1, we introduce our basic setup, which involves training on segments of long\ninput.\n\u2022 In Section 3.2, we compensate for potential capability loss and enable the model to perform\nreasoning over long input by incorporating auxiliary tasks (AT) during fine-tuning.\n\u2022 In Section 3.3, we further refine the model by supervised fine-tuning it on a diverse set of\nlong documents and synthetic tasks, making it familiar with our LIFT paradigm and adapts\nto new long texts better."}, {"title": "3.1 TRAINING WITH INPUT SEGMENTS", "content": "LLMs access knowledge either from contexts or their parameters. Unlike ICL, we propose storing\ntest-time knowledge in the parameters by adapting the model to the given long input.\nWe formalize memorizing the input as a language modeling task. Let the input be $x=\n(x_1, x_2, ..., x_L)$, where $L$ is a very large number. The objective function for the language mod-\neling task is defined as\n$L_{LLM}(x;\\theta) = \\sum_{i=1}^{L} log P(x_i | x_{1:i-1}; \\theta)$,\nwhere $\\theta$ is the parameters.\nHowever, directly adapting the model to a long text of length $L$ incurs a computational complex-\nity of $O(L^2)$ and becomes infeasible when the base model has a context window shorter than\n$L$. A straightforward approach is to truncate $x$ into non-overlapping short segments, denoted as"}, {"title": "3.2 TRAINING WITH AUXILIARY TASKS", "content": "Adapting a pretrained LLM to a specific task risks damaging its other capabilities. Similarly, while\nadapting to the input helps the model memorize the input, it probably degrades other abilities, such\nas instruction-following. Moreover, effectively memorizing the long input doesn't mean the model\ncan reason based on it.\nTo mitigate potential capability loss and enable the model to reason based on the long context, we\npropose synthesizing auxiliary question-answering (QA) tasks, denoted as $(q_i, a_i)_{i=1}^{m}$, based on the\nlong context. The objective function of the auxiliary tasks is defined as\n$L_{AT}((q_i, a_i)_{i=1}^{m}; \\theta) = -\\sum_{i=1}^{m} log P[a_i | q_i; \\theta]$.\nFollowing the mechanism of mix training (Allen-Zhu & Li, 2023), which asserts that LLMs can\nonly learn to perform inference based on $x$ when trained simultaneously on both $x$ and $(q_i, a_i)_{i=1}^{m}$,\nwe propose jointly optimizing the two objective functions, i.e.,\n$L(x, (q_i, a_i)_{i=1}^{m}; \\theta) = L_{input}(x; \\theta) + \\gamma \\cdot L_{AT}((q_i, a_i)_{i=1}^{m}; \\theta)$.\nThere are no strict constraints on the method used to synthesize $(q_i, a_i)_{i=1}^{m}$ based on $x$, except that\nis should avoid computationally expensive operations on $x$, such as inference over the entire $x$. In\nour experiments, we extract several short segments from $x$ and use a pretrained LLM to generate\nQA pairs based on the segments."}, {"title": "3.3 FURTHER IMPROVEMENT WITH PRE-LIFT SUPERVISED FINE-TUNING", "content": "While our framework LIFT is applicable to any model capable of fine-tuning, we suggest that pre-\ntrained LLMs may be unfamiliar with our training method, which leads to suboptimal results. We\nhypothesize that performance on downstream tasks can be enhanced by learning a new set of parame-\nters through multiple rounds of LIFT with auxiliary tasks, a process commonly known as Supervised\nFine-Tuning (SFT), which has been shown to be effective for long-context downstream tasks (Belt-\nagy et al., 2020; Zaheer et al., 2021). Based on this SFT model, we will then apply the normal LIFT\nprocess to further adapt the model to the given test input.\nThe SFT process involves training the model on a large corpus of long texts, combined with QA tasks\nsynthesized based on the corpus. To ensure the model becomes familiar with our LIFT method,\nthe supervised fine-tuning (SFT) tasks are designed to closely resemble those used in our LIFT\nframework. Unlike our main approach, where the model adapts to a single piece of long text, the\nSFT phase involves adapting the model to multiple pieces of long text simultaneously, preventing it\nfrom overfitting.\nFormally, we select the corpus $(x^{(i)})_{i=1}^{N}$ independent of the test datasets. For each $x^{(i)}$, we synthe-\nsize a set of QA tasks $(q_j^{(i)}, a_j^{(i)})_{j=1}^{K}$. The objective function for SFT is defined as\n$L_{SFT}((x^{(i)}, (q_j^{(i)}, a_j^{(i)})_{j=1}^{K})_{i=1}^{N}; \\theta) = \\frac{1}{N} \\sum_{i=1}^{N} (L_{input}(x^{(i)}; \\theta) + \\gamma \\cdot L_{AT}((q_j^{(i)}, a_j^{(i)})_{j=1}^{K}; \\theta))$."}, {"title": "4 EXPERIMENTS", "content": "Dataset and metrics To evaluate our method, we choose three popular long-context benchmarks,\nincluding LooGLE (Li et al., 2023), LongBench (Bai et al., 2023), BAMBOO (Dong et al., 2023)\nand Quality. They provide a relatively comprehensive evaluation, covering a wide variety of appli-\ncation scenarios. The evaluation metrics are task-specific and consistent with the respective original\nbenchmarks (Banerjee & Lavie, 2005; Zhang et al., 2020). Among these, the GPT-4 score evalu-\nates the correctness of responses of LLMs given corresponding questions and answers with GPT-4,\nwhich is proven to be highly aligned with human evaluations.\nModels For open-source LLMs, we select LLaMA3-8B-Instruct (Dubey et al., 2024) with 8k con-\ntext window. For closed-source commercial LLMs, we choose GPT3.5-turbo-16k (Chen et al.,\n2023a) with 16k context window. It has shown competitive performance on popular long context"}, {"title": "4.2 MAIN RESULTS", "content": "As shown in Table 2, LIFT+ICL consistently achieves the highest scores\nacross both LongQA and ShortQA tasks for both models, and is particularly effective in the ShortQA\ntask, which doesn't rely on long dependencies. Interestingly, LIFT_only performs the worst\namong all the settings.\nCompared to GPT-3.5, Llama 3 benefits more from LIFT+ICL, showing notable improvement in\nGPT4_score: from 30.88 (ICL) to 33.42 in LongQA, and from 44.23 to 50.44 in ShortQA. These\nresults highlight that LIFT significantly improves the performance of ICL, particularly for models\nwith short context windows. Notably, GPT-3.5 generally outperforms Llama 3 across the tasks,\nespecially in ShortQA, where it achieves a GPT4_score of 69.66 compared to 50.44 of Llama 3. No-\ntably, all models perform particularly poorly on LongQA, with GPT4_score falling below 50. This\nunderscores that modeling long dependencies in extended contexts remains a significant challenge\nfor existing models."}, {"title": "4.2.3 EFFICIENCY", "content": "Benefiting from our truncation strategy (Section 3.1), the computational complexity of our method\nscales linearly with the input context length. To further evaluate the efficiency of our approach\ncompared to ICL, we measure the time cost of a single Needle-In-A-Haystack (NIAH) task under\nboth methods. In this experiment, the input lengths are controllable and the primary computational\ncost stems from processing the input context rather than iterative generation.\nWe plot the GPU time against the input length along with the fitted curves in Figure 3.\nFirst, we observe that LIFT is significantly more memory-efficient than ICL. Notably, ICL runs\nout of memory when the input length exceeds 90k tokens on our A100 (80G) system. Upon closer\ninspection, we find that the cache of hidden states for previous tokens consumes most of the memory\nin ICL. In contrast, LIFT is capable of handling arbitrarily long inputs. Our truncation strategy\nensures that LIFT only involves adaptation and inference with short text segments, eliminating the\nneed for extensive caching.\nEmpirically, we find that the time cost of ICL grows quadratically with input length, while our\nmethod scales linearly. However, we also observe that the constant factor introduced by adaptation\nin the computational complexity of LIFT is non-negligible. As a result, our method only surpasses\nICL in time efficiency when the input length exceeds a certain threshold above 200k tokens. The\nprimary cost of our method arises from the multi-epoch fine-tuning. We hypothesize that by using\nbetter parallel fine-tuning techniques and designing tasks that are more aligned with the strengths of\nLIFT, the efficiency of the LIFT framework can be significantly improved."}, {"title": "4.3 FURTHER STUDIES ON ENHANCING LIFT CAPABILITY", "content": "Encouraged by the significant improvement observed in the timeline-reorder task from LooGLE,\nwe aim to further enhance the performance of LIFT on similar tasks like sorting and reordering, by\nincorporating auxiliary tasks (AT, Section 3.2) and pre-LIFT SFT (Section 3.3). For AT, we generate\nsynthetic QAs according to the input text simliar to the target task and fine-tunes the model on both\nthe input text and the QAs. For SFT, we generate synthetic QAs on independent corpus and fine-tune\nthe model on the corpus and QAs before applying LIFT on specific inputs.\nThe results are illustrated in Table 5. There are six models compared:"}, {"title": "5 CONCLUSION", "content": "In this paper, we proposed a novel framework, Long-Input Fine-Tuning (LIFT), to enhance LLMs'\nlong-context understanding. Our approach dynamically adapts to long inputs by efficiently fine-\ntuning the model parameters and utilizing the in-parameter knowledge to improve long-context un-\nderstanding. Experimental results across popular benchmarks like LooGLE and LongBench demon-\nstrate that the combination of ICL and LIFT enables short-context models to solve long-context tasks\nwith great improvement on some long-context tasks. In particular, LIFT is significantly more mem-\nory efficient than conventional ICL."}, {"title": "6 LIMITATIONS AND FUTURE WORKS", "content": "Limitations of LIFT without ICL. While we often employ truncated contexts to simplify infer-\nence on lengthy texts, this approach is proven insufficient for tasks that demand precise information\nextraction from extended contexts, such as the Needle in a Haystack (NIAH) task. Despite the prac-\ntical value of NIAH is arguable, we still perform the experiments and show the results in Appendix\nB. For NIAH tasks, LIFT_only is insufficient and ICL using a long context seems indispensable.\nMore advanced LIFT methods. We introduce an intuitive strategy, LIFT, for handling long con-\ntexts, showcasing its potential to address challenges associated with lengthy inputs. However, pre-\ntrained LLMs may not be naturally familiar with the LIFT framework. To bridge this gap, we\nintroduce pre-LIFT SFT, but our vision is to generalize the LIFT framework to any pretrained LLM,\nenhancing its flexibility and adaptability without requiring extensive retraining. This still needs\nextensive future study.\nStrategy to extract parametric knowledge after LIFT Through LIFT, embedding the inputs into\nthe model's internal parameters enhances its familiarity with the inputs. However, the effectiveness\nof downstream tasks still depends on the model's ability to autonomously extract and utilize the\nparametric knowledge gained during LIFT. Our experiments (Appendix B) reveal that explicitly\nproviding task-relevant knowledge outperforms using LIFT alone. Furthermore, supplying task-\nrelevant knowledge to the model after applying LIFT still significantly improves the performance.\nThis underscores the potential of developing strategies to effectively trigger and leverage LIFT-"}]}