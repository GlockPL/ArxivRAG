{"title": "Data Wrangling Task Automation Using Code-Generating Language Models", "authors": ["Ashlesha Akella", "Krishnasuri Narayanam"], "abstract": "Ensuring data quality in large tabular datasets is a critical challenge, typically addressed through data wrangling tasks. Traditional statistical methods, though efficient, cannot often understand the semantic context and deep learning approaches are resource-intensive, requiring task and dataset-specific training. To overcome these shortcomings, we present an automated system that utilizes large language models to generate executable code for tasks like missing value imputation, error detection, and error correction. Our system aims to identify inherent patterns in the data while leveraging external knowledge, effectively addressing both memory-dependent and memory-independent tasks.", "sections": [{"title": "Introduction", "content": "Tabular datasets in industrial use cases represent structured data with numerous rows and columns. Since data is pivotal in business decision-making, preserving data quality has become paramount. Data wrangling tasks like missing value imputation, error identification, and error correction are crucial to enhancing the overall data quality of tabular datasets by resolving data inconsistencies and errors.\nState-of-the-art data quality enrichment approaches found in the literature face distinct challenges. Traditional statistical methods applied to large tabular datasets efficiently leverage statistical properties of structured data with minimal time and computational requirements, however, they usually fail to account for the semantic context of the data (e.g., impute the column state given city). While deep learning approaches show promise, they require large training datasets and models tailored to specific tasks and datasets, making them time-consuming and resource-intensive to develop and deploy.\nLarge Language Models (LLMs) offer a novel approach to address these challenges in data quality enrichment due to their extensive knowledge gained from training on massive datasets. However, data wrangling with LLMs applied on row-level tabular data can become computationally expensive for large datasets, making it difficult to balance accuracy and efficiency.\nTo address this challenge, code-generating LLMs can be leveraged to automatically translate data patterns into concise executable code, enabling efficient tasks like imputation, error detection, and correction. While work has proposed code generation workflows, they often face limitations that require external knowledge or iterative refinement, highlighting the need for more adaptable systems that incorporate both inherent data patterns and external information.\nOur system uses LLMs to automatically generate code for data-wrangling tasks, incorporating relevant external knowledge as needed. As accommodating numerous columns in a dataset is resource-intensive, the system selects only the columns semantically relevant to the task. This targeted approach enables the LLM to efficiently generate code, allowing the system to execute data-wrangling tasks without additional LLM calls per row, thereby enhancing scalability for large datasets. Moreover, our system employs an iterative refinement technique to optimize the generated code snippets."}, {"title": "Leveraging Code-Generating LLMs", "content": "Tabular datasets often contain inherent patterns with dependencies between specific columns. E.g., the '24-Hour Ser-"}, {"title": "Code Generation", "content": "For data imputation and error correction, the system uses non-null rows in the target column as ground truth $G$. For error detection, users provide a small annotated dataset with 'Yes'/'No' labels as ground truth. Our system employs k-fold cross-validation on $G$, generating code with k-1 folds and validating with the kth fold. It generates multiple code snippets through k-fold cross-validation, each capturing different patterns. Data-wrangling task is executed with each code snippet, and final output is determined through a majority consensus among the outputs of all code snippets.\nIterative Prompt Generation: The prompts for the system are constructed through an iterative process. The initial prompt contains instructions and a small set of randomly selected rows from the ground truth data $G$. In subsequent iterations, the prompt is enhanced by incorporating the most effective code generated in previous iterations.\nIn each iteration, for Memory-Dependent tasks, code is generated using external data $KB\u014d$ and validated on ground truth $G$. For Memory-Independent tasks, system tries the Few-shot method only after exhausting Row-alone method.\nIn Row-alone method, a few diverse samples are selected from $G$ using K-means clustering algorithm (with samples representing centroids of the clusters) to generate a code snippet, which is applied on $G$. If accuracy of this operation is $\\geq 0.9$, then the same code is applied on $D$. In Few-shot method, one randomly selected sample $r$ from $G$ is provided along with few-shot examples from $G$ that are semantically similar to $r$ to generate a code snippet. We used the LLM model granite-34b-code-instruct-8k.\nThe prompt is structured with instructions: Task Description: Directing the LLM to \"Write a Python code..\". Function Behavior: Instructing to detect patterns, and return \"Unknown\" when no pattern is found. Example Data: A small randomly selected sample of n rows from the input table, with each row separated by newlines and column values by semicolons. Example Code: The most effective code is selected from the previous iteration of k-fold cross-validation. Reference Table: For Memory-Dependent tasks, a sample from the reference data $KB\u014d$ is added to the prompt."}, {"title": "Results", "content": "The results (using Airline and BigBasket datasets) show that our system achieves comparable performance to a system that calls the LLM for each row, while significantly reducing the number of LLM calls."}]}