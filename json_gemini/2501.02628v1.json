{"title": "Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets", "authors": ["Mahmoud Jahanshahi", "Audris Mockus"], "abstract": "A critical part of creating code suggestion systems is the pre-training of Large Language Models (LLMs) on vast amounts of source code and natural language text, often of questionable origin, quality, or compliance. This may contribute to the presence of bugs and vulnerabilities in code generated by LLMs. While efforts to identify bugs at or after code generation exist, it is preferable to pre-train or fine-tune LLMs on curated, high-quality, and compliant datasets. The need for vast amounts of training data necessitates that such curation be automated, minimizing human intervention.\nWe propose an automated source code autocuration technique that leverages the complete version history of open-source software (OSS) projects to improve the quality of training data. The proposed approach leverages the version history of all OSS projects to: (1) identify training data samples that have ever been modified, (2) detect samples that have undergone changes in at least one OSS project, and (3) pinpoint a subset of samples that include fixes for bugs or vulnerabilities. We evaluate this method using \"the Stack\" v2 dataset, comprising almost 600M code samples, and find that 17% of the code versions in the dataset have newer versions, with 17% of those representing bug fixes, including 2.36% addressing known CVEs. The clean, deduplicated version of Stack v2 still includes blobs vulnerable to 6,947 known CVEs. Furthermore, 58% of the blobs in the dataset were never modified after creation, suggesting they likely represent software with minimal or no use. Misidentified blob origins present an additional challenge, as they lead to the inclusion of non-permissively licensed code, raising serious compliance concerns.\nBy deploying these fixes and addressing compliance issues, the training of new models can avoid perpetuating buggy code patterns or license violations. We expect our results to inspire process improvements for automated data curation, a critical component of AI engineering, with the potential to significantly enhance the quality and reliability of outputs generated by AI tools.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) are already employed by popular tools such as GitHub Copilot and have a significant impact on how people interact with computing resources. LLM code-generation tools appear to increase productivity [1], are easy to access with little or no cost on popular coding plat-forms, and generated code is rapidly spreading (\"GitHub Copi-lot is behind an average of 46% of a developers' code\" [2]).\nQuality control of this code, however, is severely lacking in the LLM-based Software Supply Chain (SSC). LLMs are trained on vast amounts of source code and natural language text that are of questionable origin and quality. The output generated by LLMs, therefore, often contains bugs, vulnerabilities, or license violations that are copied or reused to train other LLM models, thus propagating the problem. Hubinger et al. [3] showed that LLMs can introduce vulnerabilities and this behavior is extremely difficult to change via fine-tuning. It is reasonable to assume that at least part of that buggy output may be attributed to the buggy files used to train LLMs. While existing approaches use AI to detect the most common insecure coding patterns [2], but many vulnerabilities do not fit such simple patterns. It is widely accepted that the size and quality of training corpus are essential for good performance of the models, yet common curation techniques, such as number of stars or forks, appear ineffective [4]. Independent of the intended coding tasks, a large body of training data is necessary for LLMs to be effective. As poor quality training data can reduce the quality of LLM-based tools, improving the state of art in source code training data curation is an important task that would impact all downstream efforts. It is worth noting that source code is often included in training data for natural language models as well. For example, the natural language collection in [5] has hundreds of gigabytes of source code and collection described in [6] nearly 100GB.\nPrevious work found instances of vulnerable or license-violating code in open source training datasets. This shows that by taking information from version control systems, it is feasible to identify vulnerable, buggy, or license-violating code and replace it with fixed versions [7, 8].\nIn summary, it is essential to exclude problematic code from LLM training datasets, or, at least, to flag it as high risk. The goals of this work is to investigate the quality of the source codes that are used to train LLMs and to develop automated approaches to improve it. Specifically, we propose a simple and effective way to identify (and fix) several types of problematic source code that is used to train LLMs.\nIn a nutshell, we leverage the fact that a file's content may undergo numerous changes over its lifetime, with some of these changes being bug fixes. By identifying cases where a file in the training data has been modified and updated, we can recommend these newer versions as replacements for older versions in the training dataset. In order for this approach"}, {"title": "II. BACKGROUND", "content": "A. Types of Software Source Code Supply Chains\nSoftware supply chain concept is helpful for assessing risks, as in traditional supply chains. However, software supply chains have substantially different nature from traditional supply chains. In particular, three types of software source code\u00b9 supply chains have been previously identified [12]. The most common, or Type I SSC is represented by code (runtime) dependencies. For example, an import statement in Java or include statement in C programming languages. The two primary risks for downstream projects in this scenario are: insufficient upstream maintenance, where bugs and vulnerabil-ities remain unresolved, and overly aggressive maintenance, where upstream changes disrupt downstream code [13].\nType II SSC involves copied code, a common practice in open-source software where code is shared publicly [14], allowing anyone to copy or fork it (within licensing re-quirements). While breaking changes are no longer a risk in Type II SSC, the absence of upstream maintenance becomes inevitable, as the code is now maintained within the destination project.\nType III SSC involves knowledge transfer where developers learn procedures techniques and tools by working in one project and then apply some of what they learned elsewhere. While learning, in general, is a good thing, some quality practices or API usage may introduce bugs or vulnerabilities that, if adopted by developers, are then spread by these developers to other projects.\nThe current state of the industry in source code SSCs is to capture dependencies based on package managers (Type I SSCs) and to rely on the \"official\" directories such as NVD\nLLMs introduce a novel type (Type IV) of Software Supply Chains that manifest by relationships between the LLM-generated code and the code used to train the LLM models. LLMSSCs, similar to Type II SSCs, are conceptually copying the code (including its bugs) in the training data but in a way that obfuscates the origin. The full scope of risks posed by Type II copy-based SSCs has yet to be studied in depth.\nC. The Stack v2 Dataset\nTo evaluate our approach we use a large open source dataset intentionally curated for training code LLMs: the Stack v2 [11]. \"The Stack v2 contains over 3B files in 600+ program-ming and markup languages. The dataset was created as part of the BigCode Project, an open scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs). The Stack serves as a pre-training dataset for Code LLMs, i.e., code-generating AI systems which enable the synthesis of programs from natural language descriptions as well as other from code snippets.\"\nThis dataset is widely adopted in AI and software de-velopment due to its extensive multi-language coverage and permissive licensing, enabling use in both academic and com-mercial contexts. The Stack (v2) fosters open collaboration, supporting model training across diverse coding ecosystems and advancing tools for software automation and analysis [16]."}, {"title": "D. Motivation for This Study", "content": "Evaluating large code datasets is essential to address the intricacies of version security and licensing, which collectively impact the reliability and ethical compliance of large language models (LLMs) for code.\n1) Security Vulnerabilities and Bugs: The security implica-tions of large datasets are significant, especially in the context of outdated or vulnerable code. If models are trained on datasets containing undetected security flaws, these vulnera-bilities may persist in model outputs, increasing the risk of insecure code suggestions. This issue is particularly concern-ing for code used in sensitive applications, where even minor security oversights can lead to substantial risks and exploita-tion potential. Security-focused dataset evaluation is therefore vital to prevent models from inadvertently embedding insecure practices into their code outputs [17].\nGiven the large-scale, open-source nature of The Stack v2 dataset, it is likely to contain instances of vulnerable and buggy code. This hypothesis (H1) is based on the prevalence of \u201corphan vulnerabilities\u201d in open-source projects, as described by Reid et al. [7], where vulnerabilities in copied code persist even after they are patched in the original source. In large datasets aggregated from numerous repositories, code reuse without consistent patching introduces security risks, as outdated or unpatched code versions may proliferate across projects, spreading known vulnerabilities further.\nMaintaining licensing integrity is fundamental for the lawful and ethical deployment of code-based AI. The provenance and licensing of code samples in these datasets must be meticulously tracked to prevent legal risks associated with licensing misrepresentation or inaccurate attributions. Open-source projects often involve significant code reuse, which can lead to fragmented metadata or altered licensing information as code is copied across projects. Proper licensing ensures that the models' outputs respect open-source constraints, which is crucial for both research and commercial applications. Without rigorous checks, models might generate code based on improperly licensed data, exposing end-users to compliance issues and potential litigation. Ensuring that datasets uphold licensing integrity not only fosters ethical"}, {"title": "III. METHODOLOGY", "content": "To address big data-related aspects of the proposed work, we leverage WoC research infrastructure [9, 10] for open source version control data. This data includes a vast majority of public open source projects and provides access to petabytes of data that includes versions of source code, information on time, authorship, and exact changes made to the source code over the entire activity history of most participants in OSS.\nA. Key Concepts\nThe proposed method for identifying issues in training data leverages unique capabilities of WoC. In particular, WoC's ability to cross-reference and track the history of code versions across nearly all public repositories, along with its curated data that addresses complex challenges like repository defork-ing [18] and author ID aliasing [19], makes this approach feasible.\nWe use a simple example to demonstrate the tracing and cross-referencing capabilities of WoC. Suppose we take a single sample b (version or, in git terms, blob) of source code from any training (or test) data. We can calculate git SHA-1 \u00b3 for this sample. All further calculations use git SHA-1 and do not require the content.\nFor a blob b to materialize in a version control repository, it has to be created by a commit c. Git commits include the time of the commit, commit message, SHA-1 of the parent commit(s) and SHA-1 of the tree (folder). WoC, by comparing the trees of the commit and its parent(s) determines all the modifications to the project done by the commit. Specifically, in case any of the project's files are modified, it extracts the tuple (bo, bm) representing the old and the new version of the file. These pairs are associated with the commit and its other attributes, like time, author and commit message.\nSuppose there is a commit, $c_t(b_0, b_m)$, which addresses a vulnerability v in project P. This commit, c, modifies a file f at time t, where the original version of the file is represented by the blob $b_0$ and the modified version by $b_m$. WoC's cross-referencing allows us to identify all repositories containing $b_0$ or $b_m$, all relevant commits, their parent and child commits, and the authors and projects associated with these commits.\nTypically, we need a repository and a commit to identify what files were changed, their content before and after the change, as well as the parent commit. By collecting and cross-referencing nearly all open source data, WoC allows us not only to go forward in version history (see child commits), but also to identify all commits that either created or modified a particular version of the file. To identify problems with the LLM training data, we will first match it to blobs or commits in WoC. Both the Stack and the Stack v2 contain versions of the files (blobs) and their git SHA-1 digests. We, therefore, just need the list of SHA-1 digests to match them to blobs in WoC. We further assume that if there exists at least one commit that modifies $b_0$, and its commit log message contains keywords (described below) indicating that it is a fix, then that blob is buggy. Similarly, if the commit indicates that it fixes a vulnerability, we assume that modified blob contains vulnerability.\nB. Identifying Potential Noncompliance\nThe Stack dataset provides information on repositories and their identified licenses for all blobs. Since code reuse through copying is common among developers [14], accurately tracing the originating projects for each blob can be challenging. WoC addresses this by offering a map [20] that, for blobs found in multiple projects, sorts them by the commit time of each blob's creation, allowing us to identify its first occurrence and the repository where it was initially committed. By comparing this origin information from WoC with the data in the Stack, we can verify whether the originating repository of each blob has been accurately identified.\nIf the origin identified by WoC does not match the origin listed in the Stack data, we then analyze the licenses associated with both the WoC-identified originating repository and those detected by the Stack. Using WoC's license map [21], we compare this information with the Stack's license data to identify potential instances of license noncompliance.\nC. Sampling\nWe used a $\\frac{1}{128}$th sample for certain quantitative analyses to balance computational feasibility with representativeness. The sampling was based on SHA-1 hashes of the blobs and commits, which ensures that the selection process is effectively random. This approach maintains statistical robustness while significantly reducing the computational overhead of process-ing the entire dataset."}, {"title": "IV. RESULTS AND DISCUSSIONS", "content": "A. Hidden Vulnerabilities\nAs described in Section III, we first extract git SHA-1 for all blobs in the Stack v2 (full) and the-stack-v2-train-smol-ids (smol) datasets. The former has 582,933,549 and the latter has 87,175,702 unique blobs. The total number of blobs in WoC version V3 (extracted at about the same time as the Stack v2) has over 26B blobs, or almost 45 times more blobs than the full version and 300 times more than the small deduplicated version.\nIn summary, despite careful curation and employment of sophisticated heuristics, even the clean version of the Stack v2 dataset contains millions of unfixed versions of the code, including thousands of unfixed vulnerabilities that supports our first hypothesis (H1).\nB. Potential Noncompliance\nThe Stack v2 dataset consists of code that is either licensed under permissive terms or lacks a specified license. To address potential licensing concerns, the Stack v2 allows authors to opt out of inclusion in the dataset. It is important to note that code without a license is distinct from unlicensed code. From a copyright perspective, code without a license defaults to \u201call rights reserved\u201d [23], which raises significant concerns about the inclusion of such code in this dataset.\nAs detailed in Section III-B, we analyzed blobs within the dataset that were reused across multiple OSS projects, as identified through WoC [20]. For each blob, we determined its originating project\u2014the project with the earliest commit timestamp containing that blob\u2014and cross-referenced it with the corresponding project in the Stack dataset.\nThe results reveal that 36.90% and 27.38% of the blobs with misidentified origins have licenses that differ from those identified in the Stack dataset. These discrepancies fall into four distinct categories. In the first case, the Stack identifies the license as permissive, while WoC identifies no license. In the second, the Stack identifies no license, but WoC identifies a permissive license. The third case involves the Stack iden-tifying no license, while WoC identifies a restrictive license. Finally, in the fourth case, the Stack identifies a permissive license, but WoC identifies a restrictive license. Among these, the second scenario does not pose a compliance risk and may even be advantageous, given the problematic nature of reusing code without a license, as previously discussed. However, the first scenario still raises some concerns. The third and fourth scenarios are particularly concerning as they indicate a high risk of license noncompliance due to the blobs originating from projects with restrictive licenses.\nIn summary, our analysis reveals that even the smaller version of the Stack dataset contains hundreds of thousands of blobs originating from projects with restrictive licenses, raising significant legal compliance concerns for any LLM trained on this dataset. These findings provide strong support for our second hypothesis (H2)."}, {"title": "V. LIMITATIONS", "content": "A. Internal Validity\n1) Impact of Buggy Code Removal on Model Outputs:\nEliminating all buggy code from pre-training or fine-tuning datasets does not guarantee that the resulting LLM will gen-erate bug-free code. However, it is reasonable to assume that some generated code may replicate buggy patterns observed in the training data. Therefore, removing bugs from the training data, especially through a low-cost approach like ours, is a sensible step toward improving the model's output quality.\n2) WoC Dataset Coverage: Some code may originate out-side public version control systems or may simply not be included in WoC's collection. However, as demonstrated with the Stack v2 dataset, only 2.5% of blobs could not be linked to commits already present in WoC, indicating that this is a relatively minor issue.\n3) Blob Updates and Quality: While updating blobs to newer versions eliminates known bugs, it can occasionally introduce new and unknown bugs. However, in most projects, only a small proportion of bug fixes result in new issues or fail to address the intended bugs. Consequently, applying fixes generally enhances the overall quality of the training data.\n4) Rebasing and Metadata Loss: Our approach relies on git SHA-1 hashes to track blobs, which ensures that content-based identification is robust to rebasing. However, rebasing may obscure certain metadata, such as precise commit lineage, which could limit the ability to fully trace the historical context of some blobs.\n5) Commit Keyword Usage for Fix Identification: Not all commits containing the keywords we used represent bug fixes, nor do all bug fixes include these keywords in their commit messages. Despite this, applying all changes, not just those identified as fixes, is likely necessary. These keywords and similar ones have been widely used in prior research to identify changes related to bug fixes. In our validation of 20 randomly selected commits, only three (15%) were found not to be clearly bug fixes.\n6) Reliability of CVE Detection: Our method successfully identified thousands of CVEs in the Stack v2 dataset, lever-aging commit messages as a primary indicator. However, this approach relies on the presence of explicit references to CVES in commit messages, which may not comprehensively capture all vulnerabilities. For instance, CVEs that were not docu-mented in commit messages or introduced through transitive dependencies might be missed. Future work could address this limitation by conducting a manual review of a representative sample or validating the method against additional datasets to evaluate recall more comprehensively.\nB. Construct Validity\n1) Impact of Dataset Vulnerabilities on Model Outputs: This study assumes that vulnerabilities and flaws in training datasets may influence the quality and security of model out-puts. While this assumption aligns with logical inference and prior research on LLM behavior, direct empirical validation of this relationship is currently lacking and represents an important avenue for future research.\n2) Never-Modified Code Assumption: While we suggest that never-modified code may indicate low use or untested quality, this is based on logical inference rather than direct empirical evidence. Future studies are needed to validate whether unmodified code consistently correlates with lower reliability or usability in practice.\n3) Blob Origin Identification: Identifying the origin of a blob is not always possible, particularly for blobs that did not originate in open-source projects. Accurate identification requires comprehensive access to all project data. However, the extensive coverage provided by WoC significantly reduces this risk.\n4) License Applicability Assumption: The licensing as-sumption for a blob is based on the identified license of the project from which it originated. However, not all files within a project necessarily fall under the project's overarching license, as some files may have distinct individual licenses.\nC. External Validity\n1) New Bugs and Iterative Updates: Even if all known bugs are addressed at time t, new bugs will inevitably be discovered at time t + 1. Therefore, regular updates are necessary. Fortu-nately, the approach outlined here can be automated, allowing it to be efficiently applied to each new version of the WoC dataset.\n2) Updating to Latest Versions: The updated version of a blob may not always represent the latest available version. As a result, the process may need to be repeated iteratively until the most recent fix is applied. The median timestamp of the commits updating blobs was June 2020, indicating that these updates were available well before the creation of the Stack v2 dataset in 2024."}, {"title": "VI. FUTURE WORK", "content": "A promising direction for future work is the development of automated curation tools specifically designed to enhance the quality of datasets used for pre-training large language models (LLMs) for code, such as Stack v2. Building on the cost-efficient approach introduced in this paper, these tools could automatically identify and apply patches for known fixes or vulnerabilities, ensuring that the datasets include secure and reliable code. They could also locate and update blobs to their latest versions, minimizing the inclusion of outdated or buggy code. Furthermore, the tools could enhance license compliance by automatically detecting and removing code with non-permissive licenses, ensuring that only code with appropriate licensing is included in the dataset. The feasibility of such automation is demonstrated by the scalability and efficiency of our approach in handling large-scale datasets. By automating these tasks, the proposed tools would streamline the iterative updates required for maintaining high-quality training data, ensuring practicality and cost-effectiveness in preparing datasets for LLM pre-training."}, {"title": "VII. CONCLUSIONS", "content": "Processes to ensure provenance, security, and compliance in SSCs are essential. This project sets the stage for future work on the curating LLM training data and provide several insights and interventions that can improve on the current state of the art.\nSeveral notable observations emerge from our analysis. First, the largest open-source training dataset, Stack v2, con-tains only a small fraction of all publicly available source code versions. These datasets could be significantly enhanced by incorporating intelligently selected data from comprehensive sources like WoC. Second, between 10% and 20% of the versions have updates, even though the WoC dataset version V3 is contemporaneous with Stack v2. Third, a substantial portion of the training data includes files with known bug fixes. While newer versions may incorporate updated APIs or additional features, applying these bug fixes is crucial to prevent LLMs from being trained on buggy code. Fourth, such fixes can be leveraged to train or align LLMs that specialize in generating changes or fixes. Fifth, training datasets should prioritize heavily or moderately modified code, which often has fewer bugs, rather than relying heavily on pristine, first-version code that dominates many existing datasets. Finally, misidentified code origins have resulted in non-permissive code being included in these datasets, raising compliance concerns.\nBeyond improving the curation practices for LLM training data, this work also introduces the concept of the LLM supply chain, highlighting its similarities to and differences from traditional software supply chains.\nWhile our primary focus has been on data curation for code LLMs, the insights generalize to any scenario involving version-controlled data."}]}