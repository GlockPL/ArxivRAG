{"title": "BSG4Bot:Efficient Bot Detection based on Biased\nHeterogeneous Subgraphs", "authors": ["Hao Miao", "Zida Liu", "Jun Gao"], "abstract": "Abstract\u2014The detection of malicious social bots has become a\ncrucial task, as bots can be easily deployed and manipulated\nto spread disinformation, promote conspiracy messages, and\nmore. Most existing approaches utilize graph neural networks\n(GNNs) to capture both user profile and structural features,\nachieving promising progress. However, they still face limitations\nincluding the expensive training on large underlying graph, the\nperformance degradation when \"similar neighborhood patterns\"\nassumption preferred by GNNs is not satisfied, and the dynamic\nfeatures of bots in a highly adversarial context.\nMotivated by these limitations, this paper proposes a method\nnamed BSG4Bot with an intuition that GNNs training on\nBiased SubGraphs can improve both performance and time/space\nefficiency in bot detection. Specifically, BSG4Bot first pre-\ntrains a classifier on node features efficiently to define the\nnode similarities, and constructs biased subgraphs by combining\nthe similarities computed by the pre-trained classifier and the\nnode importances computed by Personalized PageRank (PPR\nscores). BSG4Bot then introduces a heterogeneous GNN over the\nconstructed subgraphs to detect bots effectively and efficiently.\nThe relatively stable features, including the content category\nand temporal activity features, are explored and incorporated\ninto BSG4Bot after preliminary verification on sample data. The\nextensive experimental studies show that BSG4Bot outperforms\nthe state-of-the-art bot detection methods, while only needing\nnearly 1/5 training time.\nIndex Terms\u2014Graph Neural Networks, Social Bot Detection,\nBiased subgraphs", "sections": [{"title": "I. INTRODUCTION", "content": "Social bot detection, as a critical kind of outlier detection\nin social networks [1], has drawn increasing attention due to\nserious harm to user interests. The bots can be programmed\nand deployed in networks in a cost-efficient way to undertake\nspecific tasks with malicious purposes in general, like spread-\ning misinformation [2], manipulating public sentiment [3], and\neven interfering in political processes [4]. The proliferation of\nthese bots poses severe damage to the integrity of information\nflow on social media.\nThe existing methods can be categorized roughly according\nto different models used. The early studies mainly employ\ntraditional classifiers like Random Forests to distinguish bots\nfrom genuine users based on various features, including user\nmetadata [5], tweet content [6], and interaction patterns [7].\nBot manipulators began to craft these features meticulously\nin order to bypass these detection methods. Subsequently,"}, {"title": "II. PROBLEM FORMULATION AND DATA OBSERVATION", "content": "In this section, we first formulate the problem. We then\nattempt to uncover content and behavioral features that can\npotentially differentiate bots from genuine users. We also study\nthe graph homophily in the context of social bot detection,\naffirming the necessity of considering these factors in the\nfollowing model design."}, {"title": "A. Problem Formulation", "content": "The social network can be represented as a heterogeneous\ngraph with multi-relations G = {V,X,E,R}, where V =\n{V} 1 denotes the set of users, and X \u2208 Rn\u00d7s represents\nuser features, which have s-dimensional vector representations\nfor each node v. For any edge relation r\u2208 R, an edge\nej\u2208 Er indicates that there is an edge between nodes vi\nand vj under the relation r.\nWith the labeled dataset, the bot detection is to find a\nfunction f: (G) \u2192 Y to discriminate whether a node\nvi\u2208 V is a bot or not with the following objectives: f\nis expected to achieve high performance, in terms of the\ntraditional accuracy and F1 scores. Additionally, the learning\nof f can be computationally efficient in both time and space\ncost. Last, f should generalize well to the low training set and\nunseen node set well."}, {"title": "B. Observation for Distinguishable Features", "content": "The prior works [11], [12], [14] have investigated user fea-\ntures, such as metadata, user descriptions, and tweet contents\nin identifying bots. As these features are easily imitated or\nreplicated by bots [22], we attempt to find some relatively\nstable features accumulated over a long-range period.\nOur hypothesis posits that the bots tend to exhibit different\nbehaviors from genuine users as the bots are typically invoked\nto perform specific tasks. From this angle, we report two\npromising features including Tweet Content Categories and\nTweet Temporal Activities, which are verified in the prelimi-narily in sampled data.\nTweet Content Categories. We guess that the bots could tend\nto exhibit narrow focuses in their tweet content categories. In\ncontrast, genuine users, with a broader spectrum of interests\nand event followings, may display more varieties in their\ntweet categories. To test this hypothesis, we randomly select\n3 communities from the TwiBot-22 [17] benchmark. Each\ncommunity contains 5,000 bots and 5,000 genuine users. We\nanalyze the content of their last 200 tweets. Using a pre-\ntrained RoBERTa [23] model, we obtain the high-dimensional\nrepresentation of each tweet. These representations are then\nclustered into 20 categories using the K-Means algorithm, and"}, {"title": "Tweet Temporal Activities", "content": "We hypothesize that bots and\ngenuine users exhibit different temporal activities features,\nwhich is also observed by other works like Spotlight [24] over\ngraph stream. We conjecture that bots are often designed to\nperform tasks at regular intervals or in response to specific trig-\ngers, which may lead to more uniform or predictable patterns\nof tweet activities. To verify this hypothesis, we randomly\nselect 3 communities and record the number of tweets posted\nper month by each user over the past 18 months. We plot\ntime series curves of tweet postings for each community to\nanalyze the temporal patterns of tweet activity for both bots\nand genuine users.\nThe results, as shown in Figure 3, reveal noticeable dif-\nferences in tweet activity patterns between genuine users and\nbots. We can see that genuine users display high variability,\ndynamic activity spikes, and extremes in tweet counts that\nare not as prevalent in bots. Bots exhibit more consistent\nand stable tweeting patterns. These differences support our\nhypothesis and demonstrate that the temporal characteristics\nof tweet activities can be leveraged for bot detection."}, {"title": "Discussion of Features", "content": "We offer two features verified in\nthe sampled data above, provide possible explanation of the\nfeatures, and will further validate these two features across\nthree datasets in the experimental ablation studies. We stress\nthat the bot detection needs distinguishable features, but does\nnot focus on the detailed differences between two kinds of\nusers. In addition, the bot manipulators may adjust their policy\nto mimicking the corresponding features of genuine users,\neven though it takes a long time to change these two features.\nWe will investigate more useful features in the future."}, {"title": "C. Study on Homophily Ratio", "content": "Here, we perform study on the homophily ratio on the\ngiven dataset, as the following detection model fully considers\nthe relationship between GNN performance and the node\nhomophily ratio [21], [25]. The node homophily ratio is\nmeasured by the average fraction of neighbors with the same\nlabels in Equation 1, where N (vi) denotes the neighbor node\nset of vi and d\u2081 = |N (vi)| is the degree of vi. The center\nnode vi is considered to be homophilic when more neighbor\nnodes share the same label as vi with hi > 0.5.\n$h_i = \\frac{|\\{u \\in N(v_i): Y_u = Y_v\\}|}{d_i},$\nWe further define homophily ratio h for an entire graph\nin the following as the averaged node homophily ratios in\nEquation 2. A graph is considered homophilic if h > 0.5, and\nheterophilic otherwise. Research has shown that while GNNS\nperform well in classifying nodes in homophilic graphs where\nsimilar nodes are connected, they may underperform in het-\nerophilic settings where connections exist between dissimilar\nnodes, sometimes even performing worse than simpler models\nlike MLPs [21].\n$h = \\frac{\\sum_{v_i \\in V} h_i}{|V|}$"}, {"title": "III. METHODOLOGY", "content": "In this section, we first describe the architecture of\nBSG4Bot, and then present the major components in detail,\nincluding the feature initialization, pre-trained classifier, biased\nsubgraph construction, heterogeneous subgraph learning. We\nthen show the overall training and inference, and analyze the\ntime and space complexity finally."}, {"title": "A. Framework", "content": "We present the architecture of BSG4Bot in Figure 5, il-\nlustrated with a toy social network. The entire process can be\nroughly decomposed into data preparation, subgraph construc-\ntion, and subgraph learning. In the first phase, the method first\nextracts user features and user relationships from the social\nnetwork, and then enriches user features with tweet content\ncategories and temporal activity features discussed above. The\ncombined features are converted into vectors for each node\n(one vector in Figure 5 for simplicity). Then, we pre-train a\ncoarse classifier using efficient multilayer perception (MLP)\nmodel over all nodes in the graph. We can see that the nodes\nare roughly classified. For example, nodes such as 3, 9 with\nthe gray color have more chances to be bots, while other nodes\nare likely be genuine users.\nThe subgraph construction is a key step in BSG4Bot. For\neach node v in the graph, we construct the subgraph starting\nfrom v (red circle in Figure 5), in which the structural"}, {"title": "B. Node Feature Initialization", "content": "Node features for the following pre-trained classifier and\nGNN model are initialized into xi as follows. Here, Xd,i, Xt,i,\nnum cat cate\napi, real, reale, and time represent the user description, tweet\ncontent, numerical metadata features, categorical properties of\nmetadata, tweet categories, and tweet temporal activities, re-\nspectively. Among them, xd,i, Xt,i, xnum, and xcat x are extracted\nsimilarly to those in BotRGCN [11].\n$X_i = [X_{d,i}; X_{t,i}; X_{num}; X_{cat}; X_{cate}; X_{time}],$\nTwo features discussed in Section 3 are also extracted and\nencoded into the user features. For the content category feature\nxate, we select the most recent 200 tweets for each user. These\ntweets are encoded with ROBERTa and then clustered into\n20 categories using the K-means algorithm. The number of\ntweet categories for each user is normalized using z-score\nnormalization. Additionally, we calculate the percentage of\ntweets in each category for each user. The z-score normalized\nnumber of tweet categories is concatenated with the percentage\nof tweets in each category. For the user's tweet categories,\nthe concatenated result is processed through another fully\nconnected layer, yielding xate.\nFor the temporal activity feature time, we first extracted\nthe number of tweets posted by each user in the past 12"}, {"title": "C. Pre-trained Classifier on User Features", "content": "Initially, we introduce a pre-trained model to assist in the\nfollowing biased subgraph construction. We use the MLP on\nuser features in the pre-classification method, as the MLP\nmodel is a simple yet efficient method to achieve sufficient\nprecision, e.g. with F1-score 81% in Twitter-20, or with F1-\nscore 53% in a more complex benchmark Twitter-22. In this\nway, an MLP model can be used as an effective tool to enhance\nthe node homophily by selecting the neighbors with the same\nlabel.\nSpecifically, we train a two-layer MLP on both the training\nand validation sets to preliminarily obtain the probability\nthat a user is a human or a bot by leveraging cross-entropy\nloss in Equation 4, where Wo, W1, bo, and b\u2081 are learnable\nparameters. \u03c3 represents an activate function and we adopt\nleaky-relu as o for the rest of the paper.\n$Y = softmax(\\sigma(W_0. X + b_0)W_1 + b_1)$\nSubsequently, we obtain the hidden representations for users\nextracted by the pre-trained MLP model, and then calculate the\nsimilarity between the starting node vi and a neighbor node\nvj based on the output of the pre-trained MLP in Eq. 6, where\ncos(...) represents the cosine similarity. The cosine similarity\nscore si,j is normalized to the range [0,1]. This similarity\nreflects the proximity between user nodes in the feature space\nand influences the sampling probability of each neighbor node\nwhen constructing the biased subgraph.\n$h^{mlp} = W_0x_i + b_0$\n$S_{i,j} = \\frac{1+cos(h_i^{mlp}, h_i^{mlp})}{2}$"}, {"title": "D. Biased Subgraph Construction", "content": "The subgraph construction is the key component in the\nBSG4Bot, which aims to achieve two important goals in the\nbot detection. For each node v, the neighbor nodes have more\nchances to be selected into the v's subgraph if they are likely\nto share the same label as v, and thus homophilic ratios are\nexpected to increase, which further improve the performance\nof a GNN model. The model is trained using batches of\nsubgraphs rather than the entire graph, so as to explore the\navailable computational resources in a flexible way.\nWe consider three factors, including heterogeneous edge\nrelations, node importance in graphs, and node homophily in\nthe subgraph construction. We use the graph in Figure 5 to\nexplain the detailed subgraph construction steps in Figure 6.\nWe first extract homogeneous graphs for each edge relation-ship separately from a heterogeneous graph. For each starting\nnode v, we then select top-k neighbors into v's subgraph by\ncombining node importances using PPR and node homophily\nratio using the pretrained classifier. Note that the selected\nneighbors are not restricted by the direct neighbors, but the\nnodes which may play important roles in prediction on v,\nno matter hops to the v. The distributed version of PPR\nand similarity computation are used to reduce the cost in\nthe subgraphs. Finally, we combine these subgraphs to form\na heterogeneous subgraph with multiple edge relations, from\nwhich the bot detection patterns are learned. In the following,\nwe discuss these steps in detail.\nHeterogeneous Edge Relations. We consider the heteroge-\nneous edge relations in the social network. In social networks,\nusers interact through \"following\" and \"follower\" relation-\nships, and also communicate through tweets such as \"mention\"\nother users in a tweet, \"reply to\" someone's tweet, or \"like\"\na tweet. The \"following\", \"follower\", \"mention\" and other\nrelationships between users have different impacts on bot"}, {"title": "Node Importance in Graphs", "content": "We then take the node im-\nportance into the subgraph construction. Usually, the node\nimportance can be measured by the PPR score [28]. In social\nnetworks, while calculating the PPR score from a particular\nuser, nodes with higher scores may represent loyal followers\nor users who frequently interact with that user.\nThe PPR algorithm modifies traditional PageRank by adding\na restart option to a specific starting node during the random\nwalk. At each step, there's a chance that the walk will return\nto the starting node, thereby reflecting their importance to\nthe starting node. For any starting node vi, the calculation\nof its PPR score vector \u03c0\u2081 can be formally expressed as\nEquation 7, where a \u2208 (0, 1) is a predefined parameter called\nthe teleportation probability, and the indicator vector ei is\ncalled the preference vector for defining PPR.\n$\\pi_i = \\alpha (I \u2013 (1 \u2013 \\alpha)D^{-1}A)^{-1}e_i$\nWe utilize an approximate method [29] to efficiently com-\npute the PPR score. Roughly speaking, we initialize the resid-\nual score to be 1 for the starting node v and 0 for other nodes.\nAccording to the teleportation probability, part of residual\nscores are kept at the local nodes, and the remaining scores are\ndistributed to the neighbor nodes. As the current node may also\nreceive residual scores from its neighbors, the newly-added\nresidual scores are actually distributed recursively, until the\nnewly-added residuals are sufficiently small. Then, the residual\nscores on other nodes can serve as the importances to v.\nNode Homophily. The subgraph construction should also con-\nsider the node homophily, which plays a role complementary\nto the node importances computed by PPR. Then, we define\na combined score in Equation 8, where wij denotes the PPR\nscore of the neighbor node vj with respect to the starting node\nVi, and sij represents the similarity between the starting node\nvi and its neighbor vj calculated by Equation 6 using the pre-\ntrained classifier. BSG4Bot assumes that both PPR scores and\nnode homophily ratio are considered equally important, hence\nA is set to 0.5.\n$p_{ij} = \\lambda\\pi_{ij} + (1 \u2013 \\lambda)S_{ij}$"}, {"title": "E. Heterogeneous Subgraph Learning", "content": "Similar to the RGCN, we utilize GNN models on subgraphs\ngenerated for each relation type to obtain the hidden represen-\ntation of the starting node, and then apply semantic attention\nlayers to combine the representation from different graphs.\nWe further consider concatenating the hidden representation\nfrom different layers, as they carry different information to\novercome the possible mixed pattern in the subgraphs.\nGraph Encoder. We first transform the user features to obtain\nhidden vectors as Equation 9, where W2 and b2 are learnable\nparameters.\n$h = \\sigma (W_2.x_i + b_2)$\nThen, we utilize a GCN [30] for each subgraph with the\nsame edge relation to learn the node embeddings. At the l-\nth layer, the representation of a node under the relation ris\ndefined as Equation 10, where ci represents a normalization\nconstant, and Ni denotes the one-hop neighbors of node vi.\n$h_i^{(l)} = \\sigma (\\sum_{j\\in N_i} \\frac{1}{c_i}h_j^{(l-1)})$"}, {"title": "Intermediate Representation Concatenation", "content": "There may\nstill exist a mixture of homophily and heterophily in subgraphs"}, {"title": "Semantic Attention Layer", "content": "The representation of each node\nin heterogeneous graphs with multiple relations is obtained\nthrough multiple GCN-based embedding layers. Considering\nthe varying significance of relations [33], we employ a se-\nmantic attention layer to integrate the representations across\ndifferent relations. The importance of each relation, denoted\nas wr, is shown as Equation 12, where W is a weight matrix,\nb is a bias vector, and q is a semantic-level attention vector.\nWe have all the parameters mentioned above shared across all\nrelations and semantic-specific embeddings in BSG4Bot.\n$W_r = q^Ttanh (Wh_{final}^{(r)} + b),$\nThen, we normalize the importances of all relations using\na softmax function. The weight of relation r, denoted as \u1e9er,\ncan be obtained by normalizing the above importances of all\nrelations using the softmax function in Equation 13, which\ncan be interpreted as the contribution of the relation r to a\nspecific task. The higher Br, the more important the relation r\nis. Notably, for different heterogeneous graphs, relation r may\nhave different weights.\n$\\beta_r = \\frac{exp (w_r)}{\\sum_{r\\in \\mathcal{R}} exp (w_r)},$\nWith the learned weights as coefficients, we can fuse these\nsemantic-specific embeddings to obtain the final embedding\nhiinat as follows:\n$h_{final} = \\sum_{r=1}^{\\mathcal{R}}\\beta_r h_{final}^{(r)}$"}, {"title": "F. Model Training", "content": "We employ a softmax layer to make predictions on the\nfinal user representations from the graph neural network, using\nEquation 15, where \u0177r is the prediction of user vi.\n$Y_i = softmax (W_o.h_{final} + b_o)$\nThe loss function of BSG4Bot is constructed using Equa-\ntion 16, where yi represents the ground-truth label, and\nencompasses all learnable model parameters.\n$\\mathcal{L} = - \\sum_{i \\in \\mathcal{Y}} [y_i log (\\hat{y_i}) + (1 \u2013 y_i) log (1 \u2013 \\hat{y_i})] + \\lambda \\sum_{\\theta \\in \\Theta} \\omega^2$"}, {"title": "G. Complexity Analysis", "content": "We first introduce the symbols used in the following anal-ysis. Let n be the total number of nodes, f be dimension of\nthe features, d be the average degree of nodes, k be the total\nnumber of nodes in the subgraphs, h be the dimension of the\nhidden state, and l be the layers in the neural network. For\nsimplicity, we assume that both MLP and GNN networks share\nthe same h and l.\nWe analyze the time and space complexity of our proposed\nBSG4Bot. The following analysis is divided into three main\ncomponents: pre-trained MLP classification, biased subgraph\nconstruction, and heterogeneous subgraph learning, as we may\nneed different epochs for training in these components.\nIt involves forward and backward propagation in training an\nMLP classifier. For each training epoch, it costs time O(n. f.\nhl), as the MLP operates on features of all users separately.\nThe space cost is dominated by O(nf), and the space for the\nparameters is much less than that for the graph node features.\nIn the second phase of biased subgraph construction,\nBSG4Bot computes the PPR scores in time complexity O(n.\nd-log d), when the approximate method is used. The combined\nsimilarity scores need the node similarities using the pre-\ntrained MLP classifier, which takes O(nkh), when using\nthe approximate PPR scores to limit the candidate nodes to be\ncompared. The location of the nodes with the top-k combined\nsimilarities can be computed in O(nklogk) to sort the\ncandidate nodes. The total time complexity is dominated by\nthe PPR calculation, resulting in O(nd logd). The space\ncomplexity in this step is O(|G|), which is needed to store the\nnode features and relationships in the main memory.\nThe subgraph learning processing takes O(bkh.l.n/b)\nfor each epoch, as each node in a batch with b nodes along\nwith its k neighbors propagate their features into neighbors in\none layer, which are recursively processed in the following l\nlayers. By considering the total number nodes in one epoch,\nthe time cost becomes O(nkhl). The space complexity\ntakes O(b.kh.l), in which only the batched sample subgraphs\nare loaded into the memory for training.\nAlthough both full-graph GNN training and BSG4Bot need\nto load the entire graph, including features, into the memory,\nthe GNN model training needs GPU memory, while BSG4Bot\ncan compute the PPR scores in the main memory, and process\nmuch smaller subgraphs O(bk) than the entire graph O(n)\nduring the model training, which greatly improves the scala-\nbility. In addition, the enhanced homophily in the subgraphs"}, {"title": "IV. EXPERIMENT", "content": "In this paper, we first present the experimental settings\nand compare BSG4Bot to other methods. Then, we focus\non the roles of subgraphs, and study the effects of different\ncomponents in the BSG4Bot."}, {"title": "A. Experimental Setup", "content": "Datasets. We evaluate BSG4Bot on three widely-adopted\nTwitter bot detection benchmarks, including TwiBot-20 [34],\nTwiBot-22 [17] and MGTAB [26]. Table I summarizes the\nstatistics of each benchmark. We follow the original train,\nvalidation, and test splits of the benchmarks for a fair compar-\nison to previous works. We should note that TwiBot-22 dataset\nprovides additional 10 non-overlapped communities, each of\nwhich contains 10,000 nodes, with 5,000 labeled as bot and\n5,000 labeled as genuine users. These communities can be\nused to validate the generalization of the detection models.\nBaselines. We compare BSG4Bot to the following methods\nroughly in 5 categories, including basic methods (1-2), tradi-\ntional GNN models (3-4), GNN models with sampler (5-7), the\nexisting bot detection methods (8-10), and the GNN models\nthat consider homophily (11-12).\n1) ROBERTa [23] encodes user descriptions and tweets\nusing pre-trained RoBERTa, and feeds user features into\nan MLP for bot identification.\n2) MLP here is actually the pre-classifier in BSG4Bot.\n3) GCN [30] aggregates weighted features from neighbors,\nand passes user representations into an MLP for classi-\nfication.\n4) GAT [35] introduces the attention mechanism to distin-\nguish the importances of neighboring users in aggrega-\ntion, before feeding into an MLP for classification.\n5) SlimG [36] achieves efficient training on large-scale\ngraph data through a simplified model architecture,\nhyperparameter-free propagation functions, and effective\npreprocessing of features.\n6) GraphSAGE [37] performs uniform sampling in col-\nlecting neighbors in aggregation, which are then passed\ninto an MLP for bot detection.\n7) ClusterGCN [18] conducts GNN training over the sub-\ngraphs, each of which is a combined result of different\nclusters, thus enhancing the scalability of GNN training.\n8) BotRGCN [11] constructs a heterogeneous graph on\nTwitter and exploits relational graph convolution net-\nworks for user representation learning.\n9) RGT [12] employs relational graph transformers to\nleverage relation and influence heterogeneity of the\nTwitter networks.\n10) BotMoe [14] adopts a community-aware mixture-of-\nexperts architecture to learn various patterns in different\ncommunities.\n11) H2GCN [32] identifies a set of key designs that can\nboost learning from a heterophilic graph without trading\noff accuracy in homophilic structure.\n12) GPR-GNN [38] learns to jointly optimize node feature\nand topological information extraction adaptively, re-\ngardless of the extent to which the nodes are homophilic\nor heterophilic.\nImplementation. BSG4Bot is implemented in PyTorch, Py-\nTorch Geometric, Transformers, Scikit-learn, and NumPy. All\nour experiments are performed on a server with 256GB RAM,\ntwo Intel (R) Xeon (R) Silver 4210R CPUs @ 2.40GHz and a\n24GB GeForce RTX 3090 GPU. To avoid overfitting, dropout\nand early stopping techniques are used for training."}, {"title": "B. Performance and comparison", "content": "In this subsection, we compare BSG4Bot to various baseline\nmodels with the same training and test set in terms of the\naccuracy and F1-score metrics to study effectiveness. The\nperformance with varying training sets is also studied. We then\nexamine the training efficiency in terms of the training time\nand convergence rates. Finally, we study the generalization to\nunseen data of different methods.\nPerformance on different Baselines. We compare BSG4Bot\nwith 12 baseline methods. The performance is measured in\nterms of Accuracy (Acc) and F1-score. We run each experi-\nment for 5 times with random weight initializations and report\nthe average value as well as the standard deviation on the test\nset.\nThe experimental results are shown in Table II. We can\nsee that BSG4Bot outperforms all baseline methods across\nall three benchmarks. Specifically, BSG4Bot demonstrates\nenhancements over the state-of-the-art Twitter bot detection\nmethod BotMoe, achieving improvements of 1.0% in accuracy\nand a notable 4.5% in Fl-score on TwiBot-22. Similarly,\non the MGTAB dataset, BSG4Bot outperforms RGT with\ngains of 2.5% in accuracy and 2.3% in Fl-score. Furthermore,\non Twibot-20, BSG4Bot outperforms BotMoe by 1.5% in\naccuracy and 0.6% in F1-score.\nIn addition, the experiments demonstrate that the graph\nhomophily factor should be considered in the bot detection.\nAs shown in Table II, a simple MLP can outperform GCN\non all three benchmarks, outperform GAT on TwiBot-20 and\nMGTAB, and outperform GraphSAGE on TwiBot-22. At the\nsame time, H2GCN, GPR-GNN and BSG4Bot (ours), which is\ndesigned to consider heterophilic factors in graphs, outperform"}, {"title": "Runtime and Convergence", "content": "We record the average training\ntime per epoch, to investigate the running time of GNN based\ncompetitors. Table III reports the time per epoch and total\nnumber of epoches used, where \u201c# Epochs\" refers to the\nnumber of training epochs before early stopping is triggered\ndue to a lack of improvement on the validation set. We can see\nthat BSG4Bot has an overwhelming advantage in terms of total\ntraining time, requiring only 62 epochs to reach early stopping\nwhile time cost per epoch is similar. BSG4Bot requires only\n23.2% training time compared to RGT, and 21.9% training\ntime compared to BotMoe. Such a performance gains come\nfrom the relatively easy training process on biased subgraphs\nwith the enhanced homophily. SlimG is considerably faster\nthan BSG4Bot when it comes to runtime, completing the task\non the Twibot-22 dataset in a little over half the time. However,\nSlimG is not effective in the bot detection, as its accuracy is\nreduced by 6.47%, and its F1 score is reduced by 25.50%\ncompared to BSG4Bot, as illustrated in Table II."}, {"title": "Performance with Low Samples", "content": "The learned bot detection\nmethods rely on the labeled data, while labeling a bot requires"}, {"title": "Generalization Study", "content": "The generalization of a model is also\nrequired by the detection methods, as the bots constantly"}, {"title": "C. Biased Subgraph Study", "content": "Now, we go deeper into the biased subgraph to study\nwhether the homophily ratios really increase in the biased\nsubgraph constructed, the effects of the size of subgraph on\nthe performance, and whether the biased subgraph constructed\ncan be a valid plugin to other GNN models.\nStudy of Subgraph Homophily Ratio. We have claimed that\nthe increase of the homophily ratios can lead to the increase of\nthe GNN performances, and BSG4Bot has illustrated perfor-\nmance advantages above. Now, we are interested in whether\nthe homophily ratio really increases in the biased subgraph\ncompared to that in the original graph."}, {"title": "Study of Subgraph Size", "content": "We test the accuracy and F1-score\nof bot detection varying k to investigate the impact of the\nbiased subgraph size k on model performance. As shown\nin Figure 10, we observe that when the subgraph size is\nrelatively small, an enlarged subgraph leads to improvements\nin both accuracy and F1-score. This suggests that including\nmore neighbors, up to a point, is needed in BSG4Bot, as\nthese neighbors have high chances to share the same label\nas the starting node, and contribute positively as the high-order features. However, as k further increases (64 to 128 in\nTwibot-20 and Twibot-22, 16 to 128 in MGTAB), we observe\na slight decrease in performance. It is partially due to that\nbeyond a certain threshold, it is inevitable to select heterophilic\nnodes into the subgraph, and such a mixture heterophilic and"}, {"title": "Study of Biased Subgraph as a General Plugin Component", "content": "BSG4Bot trains a GNN model over the biased subgraph\nwith the enhanced homophily ratios. We are interested in\nwhether the subgraphs constructed can serve as a general\nplugin component used before other GNN models. In order\nto do so, we integrate the biased subgraphs into GCN, GAT,\nand BotRGCN models, and test their accuracy and F1 scores\nacross three datasets.\nTable IV shows that a significant improvement across all\nthe three models when our biased subgraph is incorporated,\nwhich further verifies the effectiveness of the biased subgraph.\nIn addition, it is reasonably expected that the biased subgraph\nplugin may be used in other GNN-based downstream tasks,\nwhich will be further studied in the future."}, {"title": "D. Ablation Study", "content": "We conduct ablation studies on three benchmarks to high-\nlight the contributions of different components to the over-\nall performance of our framework, including the subgraph\nconstruction rules, the two features introduced in Section II,\nand the concatenation of intermediate results and semantic\nattention in aggregation. The results are shown in Table V.\nWe can draw the following conclusions:\n\u2022 Overall results. These results demonstrate that each com-\nponent of the model contributes to the overall perfor-\nmance of BSG4Bot. We can observe that replacing or"}, {"title": "V. RELATED WORK", "content": "In this section, we review the advances of the social bot\ndetection and further discuss two extensions to the"}]}