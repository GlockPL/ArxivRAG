{"title": "Flexible game-playing AI with AlphaViT: adapting to multiple games and board sizes", "authors": ["Kazuhisa Fujita"], "abstract": "This paper presents novel game AI agents based on the AlphaZero framework, enhanced with Vision Transformers (ViT): AlphaViT, AlphaViD, and AlphaVDA. These agents are designed to play various board games of different sizes using a single model, overcoming AlphaZero's limitation of being restricted to a fixed board size. AlphaViT uses only a transformer encoder, while AlphaViD and AlphaVDA contain both an encoder and a decoder. AlphaViD's decoder receives input from the encoder output, while AlphaVDA uses a learnable matrix as decoder input. Using the AlphaZero framework, the three proposed methods demonstrate their versatility in different game environments, including Connect4, Gomoku, and Othello. Experimental results show that these agents, whether trained on a single game or on multiple games simultaneously, consistently outperform traditional algorithms such as Minimax and Monte Carlo tree search using a single DNN with shared weights, while approaching the performance of AlphaZero. In particular, AlphaViT and AlphaViD show strong performance across games, with AlphaViD benefiting from an additional decoder layer that enhances its ability to adapt to different action spaces and board sizes. These results may suggest the potential of transformer-based architectures to develop more flexible and robust game AI agents capable of excelling in multiple games and dynamic environments.", "sections": [{"title": "Introduction", "content": "In recent years, artificial intelligence (AI) has made remarkable progress, demonstrating its potential in a wide range of applications. One application area where AI has shown significant prowess is in mastering board games, surpassing the skills of top human players in many games. Historical achievements include AI outperforming humans in games such as checkers, chess (Campbell et al., 2002), and Othello (Buro, 1997). A significant milestone was reached in 2016 when AlphaGo (Silver et al., 2016), an AI specialized in the game of Go, defeated one of the world's top players. The subsequent introduction of AlphaZero (Silver et al., 2018), capable of mastering multiple board games such as Chess, Shogi, and Go, has further solidified the superhuman capabilities of AI in this domain.\nHowever, current game-playing AI agents have a significant drawback: they are designed to play only one specific game and cannot play other games. Even if the rules remain the same, they cannot handle variations in board size. In contrast, humans can easily switch between different board sizes. For instance, Go beginners often start by practicing on smaller boards (e.g., 9 \u00d7 9) before moving on to larger boards (e.g., 19 \u00d7 19). However, AI agents such as AlphaZero, which are designed for a specific game and fixed board size, cannot adapt to these changes without significant reprogramming.\nFor AlphaZero specifically, the core of this limitation lies in the architecture of AlphaZero's deep neural network (DNN), which requires a fixed input size. AlphaZero uses a DNN consisting of residual blocks and multilayer perceptrons (MLPs). These components are designed for a fixed input"}, {"title": "Related work", "content": "Game-playing AI agents have achieved superhuman-level performance in traditional board games such as Checkers (Schaeffer et al., 1993), Othello (Buro, 1997, 2003), and Chess (Campbell, 1999; Hsu, 1999; Campbell et al., 2002). In 2016, AlphaGo (Silver et al., 2016), a Go-playing AI, has defeated the world's top Go player, becoming the first superhuman-level Go-playing AI. AlphaGo relies on supervised learning from a large database of expert human moves and self-play data. Subsequently, AlphaGo Zero (Silver et al., 2017) has defeated AlphaGo without preparing a large training dataset. In 2018, Silver et al. (2018) have proposed AlphaZero, which has no restrictions on playable games. AlphaZero has outperformed other superhuman-level AIs at Go, Shogi, and Chess. Interestingly, AlphaZero's versatility extends beyond traditional two-player perfect information games. Research has explored its potential in more complex scenarios. For example, Hsueh et al. (2018) have shown AlphaZero's potential in nondeterministic games. Other extensions include handling continuous action spaces (Moerland et al., 2018) and support for multiplayer games (Petosa and Balch, 2019). However, AlphaZero cannot play various games simultaneously or handle games with the same rules but different board sizes using a single DNN with shared weights.\nThis limitation is due to the use of the DNN in AlphaZero for policy and value estimation. AlphaZero's DNN consists of residual blocks and MLPs. While the residual blocks excel at extracting features from input images, the MLPs are constrained by a fixed input size. As a result, the DNN is not sufficiently flexible to accommodate variations in board size. To address this limitation, the integration of Vision Transformer (ViT) (Dosovitskiy et al., 2021) into AlphaZero may provide a solution.\nThe transformer architecture, initially designed for natural language processing (Vaswani et al., 2017), has shown remarkable effectiveness in various domains. The transformer architecture has also been successfully applied to image-processing tasks. Transformer-based models have achieved exceptional performance in various image-related tasks, including image classification (Dosovitskiy et al., 2021), semantic segmentation (Xie et al., 2024), video classification (Li et al., 2022), and video captioning (Zhao et al., 2022). ViT, introduced by Dosovitskiy et al. (Dosovitskiy et al., 2021), is a remarkable example of a transformer-based model for image processing. ViT has achieved state-of-the-art performance in image classification at the time of its introduction.\nA key feature of ViT is its independence from the size of the input image (Dosovitskiy et al., 2021). Unlike convolutional neural networks (CNNs), which require fixed-size inputs, ViT can handle images of various sizes by dividing them into fixed-size patches and treating each patch as a token in the transformer architecture. This flexibility allows ViT to be highly adaptable and efficient in handling different image sizes.\nWhile AlphaZero can only play the game it was trained on, humans can play multiple games with a single brain, such as Chess, Othello, and Connect4. In addition, humans can adapt to different board sizes if they know the rules of a game. However, even if only the board size of a game changes, AI"}, {"title": "AlphaViT, AlphaViD, and AlphaVDA", "content": "AlphaZero's game-playing capability is constrained to games with identical board sizes and rules as those used during its training. This limitation is due to AlphaZero's ResNet-based deep neural network (DNN), which consists of residual blocks followed by multilayer perceptrons (MLPs) for value and policy computation. The MLPs receive input from the final residual block, assuming a fixed input size. Consequently, AlphaZero's DNN cannot adapt to variations in board size. Therefore, AlphaZero's performance is limited to games with fixed board size, restricting its performance to games with predetermined dimensions.\nTo overcome this limitation, AlphaViT, AlphaViD, and AlphaVDA have been developed as game-playing Als based on AlphaZero but using Vision Transformer (ViT) architecture. These game-playing AI agents use a combination of a DNN and MCTS (Fig. 1). The DNN receives board states and outputs value estimates and move probabilities (policies). The MCTS then searches the game tree using these outputs. The MCTS searches a game tree using the estimated value and move probability. By incorporating ViT instead of residual blocks, AlphaViT, AlphaViD, and AlphaVDA can overcome the limitation of AlphaZero and can play games with different board sizes and rules.\nAn overview of AlphaViT's DNN architecture is shown in Fig. 2. The DNN of AlphaViT is based on ViT, which has no image-size limitation and can classify an image even if the input image size differs from the training image size. This flexibility allows AlphaViT and AlphaViD to play games with different board sizes using the same network.\nIn AlphaViT, the boards are input to ViT. Initially, these inputs are transformed into patch embeddings using a convolutional layer. Using the convolution layer allows for easy adjustment of patch division parameters such as patch size, stride, and padding. The final outputs of the encoder layer are then used to compute value and policy estimates.\nAlphaViD and AlphaVDA incorporate both transformer encoder and decoder layers for value and move probability calculations. In AlphaViD, the decoder layer receives input derived from the encoder layer's output. Conversely, AlphaVDA utilizes learnable embeddings as input for its decoder layer.\nImportantly, AlphaViT, AlphaViD, and AlphaVDA can play any game that AlphaZero can, as they employ the same fundamental game-playing algorithm. Their enhanced flexibility in handling various board sizes and game rules represents a significant advancement in AI game-playing capabilities."}, {"title": "Architectures of DNNS", "content": "AlphaViT AlphaViT's deep neural network (DNN) predicts the value v(s) and the move probability distribution p with components p(a|s) for each action a, given a game state s. In a board game, s represents the state of a board, and a denotes a move.\nThe input to the DNN is an H \u00d7 W \u00d7 (2T+1) image stack $x \\in R^{H\\times W\\times(2T+1)}$, consisting consisting of 2T + 1 binary feature planes of size H \u00d7 W. Here, H and W are the dimensions of the board, and T is the number of history planes. The first T feature planes represent the occupancies of the first player's discs, where a feature value of 1 indicates that a disc occupies the corresponding cell and 0 otherwise. The following T feature planes represent the occupancies of the second player's discs. The final feature level represents the disc color of the current player, where 1 and -1 represent the first and second players, respectively.\nThe convolutional layer processes the image stack x to generate the patch embeddings xp. This layer divides the image stack into P \u00d7 P image patches with stride s and padding p and reshapes them into flattened 2D patches $x_p \\in R^{(W H)\\times N_e}$, where Ne is the embedding size. These flattened patches are treated as a sequence of token embeddings $z_0$:\n$z_0 = x = [x_p^1; ...; x_p^i; ...;x_p^{N}] ,$ \nwhere $x_p^i$ is the ith token embedding in the sequence."}, {"title": "AlphaViD and AlphaVDA", "content": "AlphaViT has a significant drawback: the size of the policy vector is fixed to the input size of the transformer encoder layer. To address this problem, we propose an improved AlphaViT called AlphaViD. Although similar to AlphaViT in many respects, AlphaViD has a transformer decoder, as shown on the left in Fig. 3. AlphaViD uses the transformer encoder and decoder to estimate the value and move probability, respectively.\nThe input board is linearly embedded using a convolutional layer and fed into a transformer encoder, similar to AlphaViT. The input embedding sequence is as follows\n$z_0 = [x_{value}; x_{game}; x_p^1; ...; x_p^{W H}] .$"}, {"title": "AlphaViT, AlphaViD, and AlphaVDA with ResNet", "content": "AlphaViT, AlphaViD, and AlphaVDA use linear transformation to make tokens for transformer encoder. The tokenizer is not limited to linear transformation. In this study, we evaluate ResNet instead of linear transformation when it is used for tokenizer."}, {"title": "Experimental settings", "content": "This study evaluates the performance of AlphaViT and AlphaViD in six games: Connect4, Connect4 5x4, Gomoku, Gomoku 6x6, Othello, and Othello 6x6. These games are two-player, deterministic, zero-sum games with perfect information. Connect4, published by Milton, is a connection game played on a 7 \u00d7 6 board. Players take turns dropping discs onto the board. A player wins by forming a straight line of four discs horizontally, vertically, or diagonally. 54Connect4 is Connect4 with a 5 \u00d7 4 board. Gomoku is a connection game in which players place stones on a board to form a line of five stones in a row, either horizontally, vertically, or diagonally. This study uses a 9 \u00d7 9 board for Gomoku and a 6 \u00d7 6 board for Gomoku 6x6. Othello (Reversi) is a two-player strategy game played on an 8 \u00d7 8 board. In Othello, the disc is white on one side and black on the other. Players take turns placing a disc with their assigned color facing up. During a game, discs of the opponent's color are flipped to the current player's color if they are in a straight line and bounded by the disc just placed and another disc of the current player's color. 66Othello is played on a 6 \u00d7 6 board in this study."}, {"title": "Opponents", "content": "This study evaluates the performance of AlphaViT and AlphaViD using five different AI methods: AlphaZero, two variants of Monte Carlo Tree Search (MCTS) labeled MCTS100 and MCTS400, Minimax, and Random. AlphaZero is trained using the method described in Appendix 1. The MCTS methods (MCTS100 and MCTS400) were implemented using different numbers of simulations (100 and 400, respectively). The details of MCTS can be found in Appendix ??. In these MCTS methods, the child nodes are expanded at the fifth visit to a node. Minimax selects a move by the minimax algorithm based on the evaluation table described in Appendix 4. Random selects moves uniformly at random from valid moves."}, {"title": "Results", "content": "In this results, we investigate the performance and characteristics of the proposed AI game-playing agents: AlphaViT, AlphaViD, and AlphaVDA. These agents have been trained on different games (Connect4, Gomoku, and Othello) with varying board sizes (small and large). These agents employ diverse architectures in terms of the number of encoder layers to examine how the numbers of encoder layers choices impact the learning outcomes and game skill of the AI agents.\nThe architectures of these agents vary primarily by the number of encoder layers, which influences their capacity to learn from the game environments. The table ?? shows the number of parameters for each agent configuration. AlphaViT, AlphaViD, and AlphaVDA are tested with different numbers of encoder layers, denoted by 'L' followed by a number (e.g., L1, L4, L5, L8). The number of parameters ranges from 11.2 million to 19.9 million, increasing with the number of encoder layers. For comparison, the AlphaZero agent, which serves as a baseline, has 7.1 million parameters.\nEach AI agent is trained on specific games with varying board sizes. The table 2 categorizes the agents based on the games which they are trained on and the board sizes used during training. The first group consists of agents trained on a single game with small board size, denoted as SB. The second group includes agents trained on a single game with large board size, denoted as LB. Finally, the third group comprises agents trained on multiple games, including Connect4, Gomoku, and Othello, with large board sizes, denoted as Multi. The agents in the third group are trained simultaneously the three games. They can play these three game using only one DNN. In other words, they are not specialized for one specific game. This diversity in training regimes allows us to evaluate the agents' adaptability and generalization capabilities across different game domains."}, {"title": "Elo rating of board game playing algorithms", "content": "Table 3 shows the Elo ratings of various AI agents for different games and board sizes. Elo ratings serve as a standard measure of relative skills in two-player games and allow a systematic comparison of the performance of each AI agent. The agents include variations of the proposed methods (AlphaViT, AlphaViD, and AlphaVDM) trained on large boards (LB), small boards (SB), and multiple board sizes (Multi), as well as other AI agents, including AlphaZero, Monte Carlo Tree Search (MCTS) with different numbers of simulations, Minimax, and a Random agent. The proposed methods and AlphaZero are trained through 1000 iterations. The Elo rating is calculated through 50 round-robin"}, {"title": "Conclusion and discussion", "content": "We propose AlphaViT, AlphaViD, and AlphaVDA, which are game-playing AI agents designed to address the limitations of AlphaZero using ViT. Unlike AlphaZero, which is limited to fixed board sizes, these proposed methods can effectively handle variations in board size, demonstrating flexibility and adaptability in their gameplay across different board sizes and game types. In addition, we showed that AlphaViT, AlphaViD, and AlphaVDA can simultaneously train and play multiple games, such as Connect4, Gomoku, and Othello, using a single model trained on all games. This ability to generalize across games with a single model represents a significant advance over traditional game-specific AI models.\nThe results of our experiments show that AlphaViT and AlphaViD outperform baseline methods such as Minimax and MCTS in most configurations. Although AlphaZero still achieves the highest Elo ratings in some cases, especially in games with larger boards, the proposed agents demonstrate competitive performance, especially in Othello, where AlphaViT approaches and even exceeds AlphaZero's Elo rating in specific configurations. Furthermore, the multi-game versions of AlphaViT and AlphaViD perform on par with their single-game counterparts, further highlighting the ability of these architectures to generalize and adapt across different board sizes.\nAlphaViT and AlphaViD showed strong adaptability across different games and board sizes. In particular, agents trained on single games often demonstrate performance comparable to AlphaZero, even when playing on board sizes for which they were not explicitly trained. This suggests effective knowledge transfer between different board sizes, mirroring human learning processes in which skills from simpler game variants (e.g., 9x9 Go) can be applied to more complex versions (19x19 Go). This similarity to human learning patterns suggests that such training paradigms may be beneficial for AI development, particularly in the context of multitask learning.\nA comparison of the three proposed architectures shows that AlphaViT performs slightly better than AlphaViD and AlphaVDA despite having the same number of parameters. This may be due to the fewer transformer encoder layers in AlphaViD and AlphaVDA, which rely on a more complex architecture with both encoder and decoder layers. AlphaViT's simpler architecture, consisting only of encoder layers, may benefit from having more layers dedicated to learning, resulting in more efficient performance in certain games. However, this simplicity limits AlphaViT's flexibility because its output size is fixed to the number of input tokens, reducing its applicability to games beyond the classic board games tested here. In contrast, AlphaViD's inclusion of a decoder layer allows it to adjust the size of its policy vector dynamically, providing greater adaptability to games with different action spaces. This architectural flexibility will make AlphaViD more versatile for handling complex games or environments with continuous action spaces.\nFuture work will explore the application of these architectures to a broader range of games, including those with more complex rules and non-deterministic elements. In addition, we will extend the flexibility of ViT to other deep reinforcement learning methods, such as deep Q-network, and develop a game AI agent that can play more flexibly, including computer games."}, {"title": "Appendix", "content": ""}, {"title": "AlphaZero", "content": "AlphaZero consists of a deep neural network (DNN) and Monte Carlo tree search (MCTS), as shown in Fig. 1. The DNN receives an input representing the current state of the board and the current player. It then outputs the estimated state value and the move probability. MCTS determines the best move based on the value and move probability. AlphaViT, AlphaViD, and AlphaVDA adopt this same fundamental structure, employing an identical decision-making process to select their next moves."}, {"title": "Deep neural network in AlphaZero", "content": "AlphaZero's deep neural network (DNN) predicts the value v(s) and the move probability p with components p(a|s) for each action a, given a state s. In a board game context, s and a represent the board state and the move, respectively. The DNN receives an input representing the current board state and the current player's disc color. Fig. ?? illustrates the DNN architecture, which consists of a"}, {"title": "Monte Carlo tree search in AlphaZero", "content": "This subsection provides an explanation of the Monte Carlo Tree Search (MCTS) algorithm used in AlphaZero. Each node in the game tree represents a game state, and each edge (s, a) represents a valid action from that state. The edges store a set of statistics: {N(s, a), W(s, a), Q(s, a), P(s,a)}, where N(s, a) is the visit count, W(s, a) is the cumulative value, Q(s, a) = W(s,a)/N(s, a) is the mean value, and P(s, a) is the move probability.\nThe MCTS for AlphaZero consists of four steps: Select, Expand and Evaluate, Backup, and Play. A simulation is defined as a sequence of Select, Expand and Evaluate, and Backup steps, repeated Nsim times. Play is executed after Nsim simulations.\nIn Select, the tree is searched from the root node $S_{root}$ to the leaf node $S_L$ at time step L using a variant of the PUCT algorithm. At each time step $t < L$, the selected action $a_t$ has a maximum score, as described by the following equation:\n$a_t = arg max_a(Q(s_t, a) + C_{puct}P(s_t, a) \\frac{\\sqrt{N(s_t)}}{1+ N(s_t, a)}),$ \nwhere $N (s_t)$ is the number of parent visits and $C_{puct}$ is the exploration rate. In this study, $C_{puct}$ is constant, whereas in the original AlphaZero, $C_{puct}$ increases slowly with search time.\nIn Expand and Evaluate, the DNN evaluates the leaf node and outputs $v(s_l)$ and $p_a(s_l)$. If the leaf node is a terminal node, $v(S_L)$ is the color of the winning player's disc. The leaf node is expanded and each edge $(S_L, a)$ is initialized to {N(SL, a) = 0, W (SL, a) = 0, Q(sL, a) = 0, P(sL,a) = Pa}.\nIn Backup, the visit counts and values are updated in a backward pass through each step, t \u2264 L. The visit count is incremented by 1, N(st, at) \u2190 N(st, at) + 1, and the cumulative and average values are updated, W(st, at) \u2190 W(st, at) + v, Q(st, a) \u2190 W(st, at)/N(st,at).\nFinally, in Play, AlphaZero selects the action corresponding to the most visited edge from the root node."}, {"title": "Training", "content": "AlphaViT, AlphaViD, AlphaVDA, and AlphaZero use a common training scheme. The training process consists of three main components: Self-play, Augmentation, and Learning, which are iterated $N_{iter}$ times. This training algorithm is a modified version of the original AlphaZero, adapted to allow training on a single computer.\nDuring the Self-play phase, the AI agent plays against itself $N_{self}$ times. For the first $T_{opening}$ turns, actions are stochastically selected from valid moves based on the softmax policy:\n$p(a | s) = \\frac{exp(N(s, a) / \\tau)}{\\Sigma_b exp(N(s, b) / \\tau)},$ \nwhere $\\tau$ is a temperature parameter that controls the exploration. This stochastic exploration enables the agent to explore new and potentially better actions. After $T_{opening}$ the most visited action is selected. Through Self-play, we collect board states, winners, and search probabilities. The search probabilities represent the probabilities of selecting valid moves at the root node in MCTS.\nIn the Augmentation phase, the dataset derived from Self-play is augmented by introducing symmetries specific to the game variant (e.g., two symmetries for Connect4 and eight for Othello and Gomoku)."}, {"title": "Parameters", "content": "The hyperparameters for AlphaViT, AlphaViD, AlphaVDA, and AlphaZero are detailed in Table ??. The weight decay and momentum values are consistent with those specified in the AlphaZero pseudocode (Silver et al., 2018). All other parameters for AlphaZero are consistent with the previous implementation Fujita (2022). The hyperparameters for the other models were carefully hand-tuned to optimize their performance.\nTable ?? lists the game-specific hyperparameters for AlphaViT, ALphaViD, AlphaVDA, and Alp- haZero. The number of MCTS simulations (Nsim) ranges from 200 to 400, depending on the game and board size. The number of self-play games per iteration is set to 30 for Connect4 variants and 10 for Gomoku and Othello variants. The opening phase (Topening) specifies the number of initial moves using softmax decision-making with a temperature parameter (7) that is adjusted based on the game and board size. These hyperparameters were also carefully hand-tuned."}, {"title": "Minimax algorithm", "content": "The minimax algorithm is a fundamental game-tree search technique that determines the optimal action by evaluating the best possible outcome for the current player. Each node in the tree contains a state, player, action, and value. The algorithm creates a game tree with a depth of $N_{depth}$ = 3. The root node corresponds to the current state and minimax player. Next, the states corresponding to leaf nodes are evaluated. Then, the algorithm propagates the values from the leaf nodes to the root node. If the player corresponding to the node is the opponent, the value of the node is the minimum value of its child nodes. Otherwise, the value of the node is the maximum value of its child nodes. Finally, the algorithm selects the action corresponding to the root's child node with the maximum value. The evaluation of leaf nodes is tailored to each game. For the Connect4 variants, the values of the connections of two and three same-colored discs are $R \\times C_{disc}C_{minimax}$ and $R^2 \\times C_{disc}C_{minimax}$, respectively, where R is the base reward, and $C_{disc}$ and $C_{minimax}$ are the colors of the connecting discs and the minimax player's disc, respectively. The value of a node is the sum of the values of all connections on the corresponding board. The terminal nodes have a value of $R^3C_{win}C_{minimax}$, where $C_{win}$ is the color of the winner, and R = 100.\nFor the Gomoku variants, the values of the connections of two, three, and four same-colored discs are $R \\times C_{disc}C_{minimax}$, $R^2 \\times C_{disc}C_{minimax}$, and $R^3 \\times C_{disc}C_{minimax}$, respectively. The value of a node is the sum of the values of all connections on the corresponding board. The terminal nodes have values of $R^a C_{win} C_{minimax}$ and R = 100."}, {"title": "Elo rating", "content": "Elo rating is a widely used metric for evaluating the relative skill levels of players in two-player games. It allows us to estimate the probability of one player defeating another based on their current ratings. Given two players A and B with Elo ratings e(A) and e(B), respectively, the probability that player A will defeat player B, denoted p(A defeats B), is calculated using the following formula:\n$p(A defeats B) = 1/(1 + 10^{(e(B)-e(A))/400}).$ \nAfter a series of NG games between players A and B, player A's Elo rating is updated to a new value e'(A) based on their performance:\n$e'(A) = e(A) + K(N_{win} - N_G \\times p(A defeats B)),$ \nwhere Nwin is the number of times player A has won, and K is a factor that determines the maximum rating adjustment after a single game. In this study, K = 8."}]}