{"title": "Flexible game-playing AI with AlphaViT: adapting to multiple games and board sizes", "authors": ["Kazuhisa Fujita"], "abstract": "This paper presents novel game AI agents based on the AlphaZero framework, enhanced with Vision Transformers (ViT): AlphaViT, AlphaViD, and AlphaVDA. These agents are designed to play various board games of different sizes using a single model, overcoming AlphaZero's limitation of being restricted to a fixed board size. AlphaViT uses only a transformer encoder, while AlphaViD and AlphaVDA contain both an encoder and a decoder. AlphaViD's decoder receives input from the encoder output, while AlphaVDA uses a learnable matrix as decoder input. Using the AlphaZero framework, the three proposed methods demonstrate their versatility in different game environments, including Connect4, Gomoku, and Othello. Experimental results show that these agents, whether trained on a single game or on multiple games simultaneously, consistently outperform traditional algorithms such as Minimax and Monte Carlo tree search using a single DNN with shared weights, while approaching the performance of AlphaZero. In particular, AlphaViT and AlphaViD show strong performance across games, with AlphaViD benefiting from an additional decoder layer that enhances its ability to adapt to different action spaces and board sizes. These results may suggest the potential of transformer-based architectures to develop more flexible and robust game AI agents capable of excelling in multiple games and dynamic environments.", "sections": [{"title": "1 Introduction", "content": "In recent years, artificial intelligence (AI) has made remarkable progress, demonstrating its potential in a wide range of applications. One application area where AI has shown significant prowess is in mastering board games, surpassing the skills of top human players in many games. Historical achievements include AI outperforming humans in games such as checkers, chess (Campbell et al., 2002), and Othello (Buro, 1997). A significant milestone was reached in 2016 when AlphaGo (Silver et al., 2016), an AI specialized in the game of Go, defeated one of the world's top players. The subsequent introduction of AlphaZero (Silver et al., 2018), capable of mastering multiple board games such as Chess, Shogi, and Go, has further solidified the superhuman capabilities of AI in this domain.\nHowever, current game-playing AI agents have a significant drawback: they are designed to play only one specific game and cannot play other games. Even if the rules remain the same, they cannot handle variations in board size. In contrast, humans can easily switch between different board sizes. For instance, Go beginners often start by practicing on smaller boards (e.g., 9 \u00d7 9) before moving on to larger boards (e.g., 19 \u00d7 19). However, AI agents such as AlphaZero, which are designed for a specific game and fixed board size, cannot adapt to these changes without significant reprogramming.\nFor AlphaZero specifically, the core of this limitation lies in the architecture of AlphaZero's deep neural network (DNN), which requires a fixed input size. AlphaZero uses a DNN consisting of residual blocks and multilayer perceptrons (MLPs). These components are designed for a fixed input"}, {"title": "2 Related work", "content": "Game-playing AI agents have achieved superhuman-level performance in traditional board games such as Checkers (Schaeffer et al., 1993), Othello (Buro, 1997, 2003), and Chess (Campbell, 1999; Hsu, 1999; Campbell et al., 2002). In 2016, AlphaGo (Silver et al., 2016), a Go-playing AI, has defeated the world's top Go player, becoming the first superhuman-level Go-playing AI. AlphaGo relies on supervised learning from a large database of expert human moves and self-play data. Subsequently, AlphaGo Zero (Silver et al., 2017) has defeated AlphaGo without preparing a large training dataset. In 2018, Silver et al. (2018) have proposed AlphaZero, which has no restrictions on playable games. AlphaZero has outperformed other superhuman-level AIs at Go, Shogi, and Chess. Interestingly, AlphaZero's versatility extends beyond traditional two-player perfect information games. Research has explored its potential in more complex scenarios. For example, Hsueh et al. (2018) have shown AlphaZero's potential in nondeterministic games. Other extensions include handling continuous action spaces (Moerland et al., 2018) and support for multiplayer games (Petosa and Balch, 2019). However, AlphaZero cannot play various games simultaneously or handle games with the same rules but different board sizes using a single DNN with shared weights.\nThis limitation is due to the use of the DNN in AlphaZero for policy and value estimation. AlphaZero's DNN consists of residual blocks and MLPs. While the residual blocks excel at extracting features from input images, the MLPs are constrained by a fixed input size. As a result, the DNN is not sufficiently flexible to accommodate variations in board size. To address this limitation, the integration of Vision Transformer (ViT) (Dosovitskiy et al., 2021) into AlphaZero may provide a solution.\nThe transformer architecture, initially designed for natural language processing (Vaswani et al., 2017), has shown remarkable effectiveness in various domains. The transformer architecture has also been successfully applied to image-processing tasks. Transformer-based models have achieved exceptional performance in various image-related tasks, including image classification (Dosovitskiy et al., 2021), semantic segmentation (Xie et al., 2024), video classification (Li et al., 2022), and video captioning (Zhao et al., 2022). ViT, introduced by Dosovitskiy et al. (Dosovitskiy et al., 2021), is a remarkable example of a transformer-based model for image processing. ViT has achieved state-of-the-art performance in image classification at the time of its introduction.\nA key feature of ViT is its independence from the size of the input image (Dosovitskiy et al., 2021). Unlike convolutional neural networks (CNNs), which require fixed-size inputs, ViT can handle images of various sizes by dividing them into fixed-size patches and treating each patch as a token in the transformer architecture. This flexibility allows ViT to be highly adaptable and efficient in handling different image sizes.\nWhile AlphaZero can only play the game it was trained on, humans can play multiple games with a single brain, such as Chess, Othello, and Connect4. In addition, humans can adapt to different board sizes if they know the rules of a game. However, even if only the board size of a game changes, AI"}, {"title": "3 AlphaViT, AlphaViD, and AlphaVDA", "content": "AlphaZero's game-playing capability is constrained to games with identical board sizes and rules as those used during its training. This limitation is due to AlphaZero's ResNet-based deep neural network (DNN), which consists of residual blocks followed by multilayer perceptrons (MLPs) for value and policy computation. The MLPs receive input from the final residual block, assuming a fixed input size. Consequently, AlphaZero's DNN cannot adapt to variations in board size. Therefore, AlphaZero's performance is limited to games with fixed board size, restricting its performance to games with predetermined dimensions.\nTo overcome this limitation, AlphaViT, AlphaViD, and AlphaVDA have been developed as game-playing Als based on AlphaZero but using Vision Transformer (ViT) architecture. These game-playing AI agents use a combination of a DNN and MCTS (Fig. 1). The DNN receives board states and outputs value estimates and move probabilities (policies). The MCTS then searches the game tree using these outputs. The MCTS searches a game tree using the estimated value and move probability. By incorporating ViT instead of residual blocks, AlphaViT, AlphaViD, and AlphaVDA can overcome the limitation of AlphaZero and can play games with different board sizes and rules.\nAn overview of AlphaViT's DNN architecture is shown in Fig. 2. The DNN of AlphaViT is based on ViT, which has no image-size limitation and can classify an image even if the input image size differs from the training image size. This flexibility allows AlphaViT and AlphaViD to play games with different board sizes using the same network.\nIn AlphaViT, the boards are input to ViT. Initially, these inputs are transformed into patch embeddings using a convolutional layer. Using the convolution layer allows for easy adjustment of patch division parameters such as patch size, stride, and padding. The final outputs of the encoder layer are then used to compute value and policy estimates.\nAlphaViD and AlphaVDA incorporate both transformer encoder and decoder layers for value and move probability calculations. In AlphaViD, the decoder layer receives input derived from the encoder layer's output. Conversely, AlphaVDA utilizes learnable embeddings as input for its decoder layer.\nImportantly, AlphaViT, AlphaViD, and AlphaVDA can play any game that AlphaZero can, as they employ the same fundamental game-playing algorithm. Their enhanced flexibility in handling various board sizes and game rules represents a significant advancement in AI game-playing capabilities."}, {"title": "3.1 Architectures of DNNS", "content": "AlphaViT AlphaViT's deep neural network (DNN) predicts the value $v(s)$ and the move probability distribution $p$ with components $p(a|s)$ for each action $a$, given a game state $s$. In a board game, $s$ represents the state of a board, and $a$ denotes a move.\nThe input to the DNN is an $H \\times W \\times (2T+1)$ image stack $x \\in \\mathbb{R}^{H\\times W\\times (2T+1)}$, consisting consisting of $2T + 1$ binary feature planes of size $H \\times W$. Here, $H$ and $W$ are the dimensions of the board, and $T$ is the number of history planes. The first $T$ feature planes represent the occupancies of the first player's discs, where a feature value of 1 indicates that a disc occupies the corresponding cell and 0 otherwise. The following $T$ feature planes represent the occupancies of the second player's discs. The final feature level represents the disc color of the current player, where 1 and -1 represent the first and second players, respectively.\nThe convolutional layer processes the image stack $x$ to generate the patch embeddings $x_p$. This layer divides the image stack into $P \\times P$ image patches with stride $s$ and padding $p$ and reshapes them into flattened 2D patches $x_p \\in \\mathbb{R}^{(WH)\\times N_e}$, where $N_e$ is the embedding size. These flattened patches are treated as a sequence of token embeddings $z_0$:\n$z_0 = x = [x_p^1; ...; x_p^i; ...;x_p^{N}],$\nwhere $x_p^i$ is the $i$th token embedding in the sequence."}, {"title": "5 Results", "content": "In this results, we investigate the performance and characteristics of the proposed AI game-playing agents: AlphaViT, AlphaViD, and AlphaVDA. These agents have been trained on different games (Connect4, Gomoku, and Othello) with varying board sizes (small and large). These agents employ diverse architectures in terms of the number of encoder layers to examine how the numbers of encoder layers choices impact the learning outcomes and game skill of the AI agents.\nThe architectures of these agents vary primarily by the number of encoder layers, which influences their capacity to learn from the game environments. The table ?? shows the number of parameters for each agent configuration. AlphaViT, AlphaViD, and AlphaVDA are tested with different numbers of encoder layers, denoted by 'L' followed by a number (e.g., L1, L4, L5, L8). The number of parameters ranges from 11.2 million to 19.9 million, increasing with the number of encoder layers. For comparison, the AlphaZero agent, which serves as a baseline, has 7.1 million parameters.\nEach AI agent is trained on specific games with varying board sizes. The table 2 categorizes the agents based on the games which they are trained on and the board sizes used during training. The first group consists of agents trained on a single game with small board size, denoted as SB. The second group includes agents trained on a single game with large board size, denoted as LB. Finally, the third group comprises agents trained on multiple games, including Connect4, Gomoku, and Othello, with large board sizes, denoted as Multi. The agents in the third group are trained simultaneously the three games. They can play these three game using only one DNN. In other words, they are not specialized for one specific game. This diversity in training regimes allows us to evaluate the agents' adaptability and generalization capabilities across different game domains."}, {"title": "5.1 Elo rating of board game playing algorithms", "content": "Table 3 shows the Elo ratings of various AI agents for different games and board sizes. Elo ratings serve as a standard measure of relative skills in two-player games and allow a systematic comparison of the performance of each AI agent. The agents include variations of the proposed methods (AlphaViT, AlphaViD, and AlphaVDM) trained on large boards (LB), small boards (SB), and multiple board sizes (Multi), as well as other AI agents, including AlphaZero, Monte Carlo Tree Search (MCTS) with different numbers of simulations, Minimax, and a Random agent. The proposed methods and AlphaZero are trained through 1000 iterations. The Elo rating is calculated through 50 round-robin\ntournaments between the agents. The Elo ratings of all agents are initialized to 1500. The details of the Elo rating calculation are given in Appendix 5.\nAlphaViT L4 LB shows strong performance, achieving Elo ratings of 1824 in Connect4, 1835 in Gomoku, and 2125 in Othello. However, for Connect4 and Gomoku AlphaViT is outperformed by AlphaZero. Interestingly, despite not being specifically trained on small boards, AlphaViT L4 LB performs as well or better than Minimax and MCTS in Connect4 5x4, Gomoku 6x6, and Othello 6x6. This suggests that AlphaViT L4 SB efficiently utilizes the knowledge obtained from large board training.\nAlphaViT L4 SB shows Elo ratings of 1732 in Connect4 5x4, 1764 in Gomoku 6x6 and 1969 in Othello 6x6. In these games AlphaViT L4 SB performed as well as or better than AlphaZero. For Gomoku and Othello, it outperformed Minimax and MCTS even though it is not trained on large board games. This result suggests that AlphaViT L4 SB effectively transfers knowledge from small board training to large boards.\nAlphaViT L4 Multi, trained simultaneously on Connect4, Gomoku and Othello, showed strong results in these games, achieving Elo ratings of 1700 in Connect4, 1969 in Gomoku and 1939 in Othello. Although it is weaker than AlphaZero in Connect4 and Gomoku, AlphaViT L4 Multi outperformed AlphaViT L4 LB (trained on Gomoku only) in Gomoku. In addition, AlphaViT L4 Multi shows comparable performance to both AlphaViT L4 SB and AlphaZero in Gomoku 6x6.\nAlphaViD L1 LB performs slightly worse than AlphaViT in the games with larger boards. In Gomoku 6x6 and Othello 6x6, AlphaViD L1 LB outperformed both Minimax and MCTS100, suggesting that it effectively transferred knowledge gained from training on larger boards. AlphaViD L1 SB achieved Elo ratings of 1742 in Connect4 5x4, 1785 in Gomoku 6x6 and 1852 in Othello 6x6, with performance comparable to AlphaZero and AlphaViT. AlphaViD L1 Multi showed comparable performance to AlphaViT in Connect4, but was weaker in Gomoku and Othello.\nSimilarly, AlphaVDA L1 LB is weaker than AlphaViT for the games with large boards. AlphaVDA L1 LB outperformed Minimax and MCTS100 for all games with small board sizes. AlphaVDA L1 SB shows Elo ratings of 1770 in Connect4 5x4, 1871 in Gomoku 6x6 and 1833 in Othello 6x6, with performance comparable to AlphaZero, AlphaViT and AlphaViD. AlphaVDA L1 Multi performed better than AlphaViT in Connect4, but was outperformed by AlphaViT in both Gomoku and Othello.\nThe Elo ratings presented in this table serve as a baseline for the subsequent experiments described in the following sections."}, {"title": "6 Conclusion and discussion", "content": "We propose AlphaViT, AlphaViD, and AlphaVDA, which are game-playing AI agents designed to address the limitations of AlphaZero using ViT. Unlike AlphaZero, which is limited to fixed board sizes, these proposed methods can effectively handle variations in board size, demonstrating flexibility and adaptability in their gameplay across different board sizes and game types. In addition, we showed that AlphaViT, AlphaViD, and AlphaVDA can simultaneously train and play multiple games, such as Connect4, Gomoku, and Othello, using a single model trained on all games. This ability to generalize across games with a single model represents a significant advance over traditional game-specific AI models.\nThe results of our experiments show that AlphaViT and AlphaViD outperform baseline methods such as Minimax and MCTS in most configurations. Although AlphaZero still achieves the highest Elo ratings in some cases, especially in games with larger boards, the proposed agents demonstrate competitive performance, especially in Othello, where AlphaViT approaches and even exceeds AlphaZero's Elo rating in specific configurations. Furthermore, the multi-game versions of AlphaViT and AlphaViD perform on par with their single-game counterparts, further highlighting the ability of these architectures to generalize and adapt across different board sizes.\nAlphaViT and AlphaViD showed strong adaptability across different games and board sizes. In particular, agents trained on single games often demonstrate performance comparable to AlphaZero, even when playing on board sizes for which they were not explicitly trained. This suggests effective knowledge transfer between different board sizes, mirroring human learning processes in which skills from simpler game variants (e.g., 9x9 Go) can be applied to more complex versions (19x19 Go). This similarity to human learning patterns suggests that such training paradigms may be beneficial for AI development, particularly in the context of multitask learning.\nA comparison of the three proposed architectures shows that AlphaViT performs slightly better than AlphaViD and AlphaVDA despite having the same number of parameters. This may be due to the fewer transformer encoder layers in AlphaViD and AlphaVDA, which rely on a more complex architecture with both encoder and decoder layers. AlphaViT's simpler architecture, consisting only of encoder layers, may benefit from having more layers dedicated to learning, resulting in more efficient performance in certain games. However, this simplicity limits AlphaViT's flexibility because its output size is fixed to the number of input tokens, reducing its applicability to games beyond the classic board games tested here. In contrast, AlphaViD's inclusion of a decoder layer allows it to adjust the size of its policy vector dynamically, providing greater adaptability to games with different action spaces. This architectural flexibility will make AlphaViD more versatile for handling complex games or environments with continuous action spaces.\nFuture work will explore the application of these architectures to a broader range of games, including those with more complex rules and non-deterministic elements. In addition, we will extend the flexibility of ViT to other deep reinforcement learning methods, such as deep Q-network, and develop a game AI agent that can play more flexibly, including computer games."}, {"title": "Appendix", "content": null}, {"title": "1 AlphaZero", "content": "AlphaZero consists of a deep neural network (DNN) and Monte Carlo tree search (MCTS), as shown in Fig. 1. The DNN receives an input representing the current state of the board and the current player. It then outputs the estimated state value and the move probability. MCTS determines the best move based on the value and move probability. AlphaViT, AlphaViD, and AlphaVDA adopt this same fundamental structure, employing an identical decision-making process to select their next moves."}, {"title": "1.1 Deep neural network in AlphaZero", "content": "AlphaZero's deep neural network (DNN) predicts the value $v(s)$ and the move probability $p$ with components $p(a|s)$ for each action $a$, given a state $s$. In a board game context, $s$ and $a$ represent the board state and the move, respectively. The DNN receives an input representing the current board state and the current player's disc color. Fig. ?? illustrates the DNN architecture, which consists of a"}, {"title": "1.2 Monte Carlo tree search in AlphaZero", "content": "This subsection provides an explanation of the Monte Carlo Tree Search (MCTS) algorithm used in AlphaZero. Each node in the game tree represents a game state, and each edge $(s, a)$ represents a valid action from that state. The edges store a set of statistics: ${N(s, a), W(s, a), Q(s, a), P(s,a)}$, where $N(s, a)$ is the visit count, $W(s, a)$ is the cumulative value, $Q(s, a) = W(s,a)/N(s, a)$ is the mean value, and $P(s, a)$ is the move probability.\nThe MCTS for AlphaZero consists of four steps: Select, Expand and Evaluate, Backup, and Play. A simulation is defined as a sequence of Select, Expand and Evaluate, and Backup steps, repeated $N_{sim}$ times. Play is executed after $N_{sim}$ simulations.\nIn Select, the tree is searched from the root node $s_{root}$ to the leaf node $s_L$ at time step $L$ using a variant of the PUCT algorithm. At each time step $t < L$, the selected action $a_t$ has a maximum score, as described by the following equation:\n$a_t = \\underset{a}{\\arg \\max}(Q(s_t, a) + C_{puct}P(s_t, a) \\frac{\\sqrt{N(s_t)}}{1+ N(s_t, a)}),$ \nwhere $N (s_t)$ is the number of parent visits and $C_{puct}$ is the exploration rate. In this study, $C_{puct}$ is constant, whereas in the original AlphaZero, $C_{puct}$ increases slowly with search time.\nIn Expand and Evaluate, the DNN evaluates the leaf node and outputs $v(s_L)$ and $p_a(s_L)$. If the leaf node is a terminal node, $v(s_L)$ is the color of the winning player's disc. The leaf node is expanded and each edge $(s_L, a)$ is initialized to ${N(s_L, a) = 0, W (s_L, a) = 0, Q(s_L, a) = 0, P(s_L,a) = P_a}$.\nIn Backup, the visit counts and values are updated in a backward pass through each step, $t \\leq L$. The visit count is incremented by 1, $N(s_t, a_t) \\leftarrow N(s_t, a_t) + 1$, and the cumulative and average values are updated, $W(s_t, a_t) \\leftarrow W(s_t, a_t) + v, Q(s_t, a) \\leftarrow W(s_t, a_t)/N(s_t,at)$.\nFinally, in Play, AlphaZero selects the action corresponding to the most visited edge from the root node."}, {"title": "2 Training", "content": "AlphaViT, AlphaViD, AlphaVDA, and AlphaZero use a common training scheme. The training process consists of three main components: Self-play, Augmentation, and Learning, which are iterated $N_{iter}$ times. This training algorithm is a modified version of the original AlphaZero, adapted to allow training on a single computer.\nDuring the Self-play phase, the AI agent plays against itself $N_{self}$ times. For the first $T_{opening}$ turns, actions are stochastically selected from valid moves based on the softmax policy:\n$p(a|s) = \\frac{\\exp(N(s,a)/\\tau)}{\\Sigma_{b} \\exp(N(s, b)/\\tau)},$ \nwhere $\\tau$ is a temperature parameter that controls the exploration. This stochastic exploration enables the agent to explore new and potentially better actions. After $T_{opening}$ the most visited action is selected. Through Self-play, we collect board states, winners, and search probabilities. The search probabilities represent the probabilities of selecting valid moves at the root node in MCTS.\nIn the Augmentation phase, the dataset derived from Self-play is augmented by introducing symmetries specific to the game variant (e.g., two symmetries for Connect4 and eight for Othello and Gomoku)."}, {"title": "3 Parameters", "content": "The hyperparameters for AlphaViT, AlphaViD, AlphaVDA, and AlphaZero are detailed in Table ??. The weight decay and momentum values are consistent with those specified in the AlphaZero pseudocode (Silver et al., 2018). All other parameters for AlphaZero are consistent with the previous implementation Fujita (2022). The hyperparameters for the other models were carefully hand-tuned to optimize their performance.\nTable ?? lists the game-specific hyperparameters for AlphaViT, ALphaViD, AlphaVDA, and Alp-haZero. The number of MCTS simulations $(N_{sim})$ ranges from 200 to 400, depending on the game and board size. The number of self-play games per iteration is set to 30 for Connect4 variants and 10 for Gomoku and Othello variants. The opening phase $(T_{opening})$ specifies the number of initial moves using softmax decision-making with a temperature parameter $(\\tau)$ that is adjusted based on the game and board size. These hyperparameters were also carefully hand-tuned."}, {"title": "4 Minimax algorithm", "content": "The minimax algorithm is a fundamental game-tree search technique that determines the optimal action by evaluating the best possible outcome for the current player. Each node in the tree contains a state, player, action, and value. The algorithm creates a game tree with a depth of $N_{depth} = 3$. The root node corresponds to the current state and minimax player. Next, the states corresponding to leaf nodes are evaluated. Then, the algorithm propagates the values from the leaf nodes to the root node. If the player corresponding to the node is the opponent, the value of the node is the minimum value of its child nodes. Otherwise, the value of the node is the maximum value of its child nodes. Finally, the algorithm selects the action corresponding to the root's child node with the maximum value. The evaluation of leaf nodes is tailored to each game. For the Connect4 variants, the values of the connections of two and three same-colored discs are $R \\times C_{disc}C_{minimax}$ and $R^2 \\times C_{disc}C_{minimax}$, respectively, where $R$ is the base reward, and $C_{disc}$ and $C_{minimax}$ are the colors of the connecting discs and the minimax player's disc, respectively. The value of a node is the sum of the values of all connections on the corresponding board. The terminal nodes have a value of $R^3C_{win}C_{minimax}$, where $C_{win}$ is the color of the winner, and $R = 100$.\nFor the Gomoku variants, the values of the connections of two, three, and four same-colored discs are $R \\times C_{disc}C_{minimax}$, $R^2 \\times C_{disc}C_{minimax}$, and $R^3 \\times C_{disc}C_{minimax}$, respectively. The value of a node is the sum of the values of all connections on the corresponding board. The terminal nodes have values of $R^{a}C_{win}C_{minimax}$ and $R = 100."}]}