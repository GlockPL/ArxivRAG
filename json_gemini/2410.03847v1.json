{"title": "Model-Based Reward Shaping for Adversarial Inverse Reinforcement Learning in Stochastic Environments", "authors": ["Simon Sinong Zhan", "Qingyuan Wu", "Philip Wang", "Yixuan Wang", "Ruochen Jiao", "Chao Huang", "Qi Zhu"], "abstract": "In this paper, we aim to tackle the limitation of the Adversarial Inverse Reinforce-\nment Learning (AIRL) method in stochastic environments where theoretical results\ncannot hold and performance is degraded. To address this issue, we propose a\nnovel method which infuses the dynamics information into the reward shaping\nwith the theoretical guarantee for the induced optimal policy in the stochastic\nenvironments. Incorporating our novel model-enhanced rewards, we present a\nnovel Model-Enhanced AIRL framework, which integrates transition model esti-\nmation directly into reward shaping. Furthermore, we provide a comprehensive\ntheoretical analysis of the reward error bound and performance difference bound\nfor our method. The experimental results in MuJoCo benchmarks show that our\nmethod can achieve superior performance in stochastic environments and competi-\ntive performance in deterministic environments, with significant improvement in\nsample efficiency, compared to existing baselines.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has achieved considerable success across various domains, including\nboard game (Schrittwieser et al., 2020), MOBA game (Berner et al., 2019), time-delayed system (Wu\net al., 2024), and cyber-physical systems (Wang et al., 2023a,b,c; Zhan et al., 2023). Despite these\nadvances, RL highly depends on the quality of reward function design which demands expertise,\nintensive labour, and a great amount of time (Russell, 1998). To address this, imitation learning (IL)\nmethods, such as Behavior Cloning (BC) (Torabi et al., 2018a) and Inverse Reinforcement Learning\n(IRL) (Arora & Doshi, 2021), leverage human or expert demonstrations to bypass the need for explicit\nreward functions. These methods aim to learn from the demonstrations to eventually match the\ndistribution of expert behavior, and have shown great promise in applications like autonomous driving\n(Codevilla et al., 2018; Sun et al., 2018), legged locomotion (Peng et al., 2020; Ratliff et al., 2009),\nand planning tasks (Choudhury et al., 2018; Yin et al., 2022).\nThe notable approaches within IRL are Adversarial Imitation Learning (AIL) methods that build\nupon maximum entropy framework (Ziebart et al., 2008). These adversarial methods frame imitation\nlearning as a maximum likelihood estimation problem on trajectory distributions, converting the\ndistribution into a Boltzmann distribution parameterized by rewards under deterministic environment\nsettings (Wu et al., 2024). This closely mirrors the distribution approximation found in generative"}, {"title": "2 Related Works", "content": "Adversarial Imitation Learning. Margin optimization based IRL methods (Ng et al., 2000; Abbeel\n& Ng, 2004; Ratliff et al., 2006) aim to learn reward functions that explain expert behavior better than\nother policies by a margin. Bayesian approaches were introduced with different prior assumptions\non reward distributions, such as Boltzmann distributions (Ramachandran & Amir, 2007; Choi &\nKim, 2011; Chan & van der Schaar, 2021) or Gaussian Processes (Levine et al., 2011). Other\nstatistical learning methods include multi-class classification (Klein et al., 2012; Brown et al., 2019)\nand regression trees (Levine et al., 2010). The entropy optimization approach has seen significant\ndevelopment. To avoid biases from maximum margin methods, the maximum entropy principle (Shore\n& Johnson, 1980) is used to infer distributions over trajectories parameterized by reward weights.\nZiebart et al. (2008, 2010) proposed a Lagrangian dual framework to cast the reward learning into a\nmaximum likelihood problem with linear-weighted feature-based reward representation. Wulfmeier\net al. (2015) extended the framework to nonlinear reward representations, and Finn et al. (2016b)\ncombines importance sampling techniques to enable model-free estimation. Inspired by GANS,\nadversarial methods were introduced for policy and reward learning in IRL (Ho & Ermon, 2016; Fu\net al., 2017; Torabi et al., 2018b). However, these methods typically work with Maximum Entropy\n(ME) formulation yet suffer from sample inefficiency. Although there have been efforts to combine\nadversarial methods with off-policy RL agents to improve sample efficiency (Kostrikov et al., 2018;\nBlond\u00e9 & Kalousis, 2019; Blond\u00e9 et al., 2022), few extend it to the model-based setting which might\nfurther the improvement, and none of these approaches addresses the rewards learning in stochastic\nMDP settings.\nRewards Shaping. Reward shaping (Dorigo & Colombetti, 1994; Randl\u00f8v & Alstr\u00f8m, 1998)\nis a technique that enhances the original reward signal by adding additional domain information,\nmaking it easier for the agent to learn optimal behavior. This can be defines as $R = R + F$, where\n$F$ is the shaping function and $R$ is the shaped reward function. Potential-based reward shaping\n(PBRS) (Ng et al., 2000) builds the potential function on states, $F(s, a, s') = \\Phi(s') \u2013 \\Phi(s)$, while\nensuring the policy invariance property, which refers to inducing the same optimal behavior under\ndifferent rewards $R$ and $\\bar{R}$. Nonetheless, there also exist other variants on the inputs of the potential\nfunctions such as state-action (Wiewiora et al., 2003), state-time (Devlin & Kudenko, 2012), and\nvalue function (Harutyunyan et al., 2015) as potential function input. There are also some latest\nattempts of reward shaping without utilization of domain knowledge potential function to solve\nexploration under sparse rewards (Hu et al., 2020; Devidze et al., 2022; Gupta et al., 2022; Skalse\net al., 2023).\nMBIRL. Model-Based RL (MBRL) has emerged as a promising direction for improving sample\nefficiency and generalization (Janner et al., 2019; Yu et al., 2020). MBRL combines various learned\ndynamics neural network structures with planning (Hansen et al., 2022; Sikchi et al., 2022).This\nframework has been successfully extended to vision-based control tasks (Hafner et al., 2019; Zhan\net al., 2023). Integrating IRL with MBRL has also shown success. For example, Das et al. (2021) and\nHerman et al. (2016) presented a gradient-based IRL approach using different policy optimization\nmethods with dynamic models for linear-weighted features reward learning. In Das et al. (2021),\nthe dynamic model is used to pass forward/backward the gradient in order to update the IRL and\npolicy optimization modules. Similarly, end-to-end differentiable adversarial IRL frameworks to\nvarious state spaces have also been explored (Baram et al., 2016, 2017; Sun et al., 2021; Rafailov\net al., 2021), where dynamic model serves a similar role. Despite these advancements, existing\nmethods rarely address the specific challenges posed by stochastic environments, which limit reward\nlearning performance. To our knowledge, this is the first study that provide a theoretical analysis\non the performance difference with learned dynamic model for the adversarial IRL problem under\nstochastic MDP."}, {"title": "3 Preliminaries", "content": "MDP. RL is usually formulated as a Markov Decision Process (MDP) $\\mathcal{M}$ (Puterman, 2014)\ndenoted as a tuple $(\\mathcal{S}, \\mathcal{A}, T, \\gamma, R, \\rho_0)$. $\\rho_0$ is the initial distribution of the state. $s \\in \\mathcal{S}, a \\in \\mathcal{A}$\nstands for the state and action space respectively. $T$ stands for the transition dynamic such that\n$T:\\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$. $\\gamma \\in (0, 1)$ is the discounted factor, $R$ stands for reward function such that"}, {"title": "4 Model Estimation in reward shaping", "content": "In this section, we illustrate the advantages of involving transition dynamics into the reward shaping,\nespecially in stochastic MDP settings. Most of literature work has various formulations and defini-\ntions (Table 1), but few considers transition dynamic information in the reward shaping. Defining\nrewards solely based on states, $R^s(s_t)$, offers limited utility in environments where actions are critical.\nEven though the state-action pair-based rewards $R^{sa}(s_t, a_t)$ can capture the missing information\nof the taken action, it fails to consider any future information, the successive state $s_{t+1}$. Transition\ntuple-based rewards $R^{tuple}(s_t, a_t, s_{t+1})$ incorporate the dynamics information in a sampling-based\nway, which requires abundant data to learn the underlying relationship of two consecutive states,\npotentially raising the sample efficiency issue in the stochastic environment with the huge state\nspace. To address this issue, we propose dynamics-based rewards shaping $R(s_t, a_t, T)$, which explic-\nitly infuse the dynamics information $T$ on the potential function, thus significantly improving the"}, {"title": "5 Model Enhanced Adversarial IRL", "content": "In this section, we first elaborate on the adversarial formulation of our reward shaping (Eq. (8)) and\npresent the theoretical insight (Proposition 5.1) of the equivalence between cross-entropy training loss\nof adversarial reward shaping formulation and maximum log-likelihood loss of original maximum\ncausal entropy IRL problem. Then, in the Sec. 5.2, we showcase our practical algorithm framework\nwith trajectory generation and transition model learning in the loop, as shown in Fig. 1. Furthermore,\nwe theoretically investigate the reward function bound (Thm. 5.3) and performance difference bound\n(Thm. 5.4) under the transition model learning error.\n5.1 Adversarial Formulation of Reward Shaping\nIn order to learn a robust reward function, we apply the AIRL framework (Fu et al., 2017). In-\nspired by GANS (Goodfellow et al., 2014), the idea behind AIRL is to train a binary discriminator\n$D(s_t, a_t, s_{t+1})$ to distinguish state-action-transition samples from an expert and those generated"}, {"title": "6 Experiments", "content": "In this section, we evaluate the performance and sample efficiency of our Model-Enhanced Adversar-\nial IRL framework. We aim to demonstrate the superiority of our method in stochastic environments,\nachieving better performance and sample efficiency compared to existing approaches. Addition-\nally, in deterministic settings, our method not only maintains competitive performance, but also\nachieves better sample efficiency with baselines. All experiments are conducted on the MuJoCo\nbenchmarks (Todorov et al., 2012). To simulate stochastic dynamics in MuJoCo, we introduce the\nagent-unknown Gaussian noise with a mean of 0 and a standard deviation of 0.5 to the environmental\ninteraction steps. All the expert data are generated by an expert trained with standard SAC under\ndeterministic or stochastic MuJoCo environments. Our experiments are designed to highlight the key\nadvantages of our framework:\n\u2022 Superiority in Stochastic Environments: In stochastic settings, our method significantly\noutperforms other approaches, consistently surpassing expert-level performance more\nrapidly. This enhanced ability to learn under uncertainty is attributed to our framework's\neffectiveness in leveraging model-based synthetic data.\n\u2022 Performance in Deterministic Environments: We demonstrate that our method is either\nbetter or competitive with existing AIL methods' performances in deterministic settings.\n\u2022 Sample Efficiency For both deterministic and stochastic settings, our method can reach\nexpert performance with fewer training steps than all the other baselines.\nWe primarily compare our approach with other Adversarial Imitation Learning (AIL) methods,\nincluding the on-policy algorithms GAIL (Ho & Ermon, 2016) and AIRL (Fu et al., 2017), and the\noff-policy method Discriminator Actor-Critic (DAC) (Kostrikov et al., 2018). For policy optimization,\nwe use Proximal Policy Optimization (PPO) (Schulman et al., 2017) for both GAIL and AIRL,\nand Soft Actor-Critic (SAC) (Haarnoja et al., 2018) for both DAC and our proposed method. All\nimplementations of PPO and SAC are referenced from the Clean RL library (Huang et al., 2022).\nEach algorithm is trained with 100k environmental steps and evaluated each 1k steps across 5\ndifferent seeds for tasks including InvertedPendulum-v4 and InvertedDoublePendulum-v4.\nFor Hopper-v3, AIRL and GAIL are trained with 10M steps and evaluated each 100k steps across 5\ndifferent seeds, but DAC and our algorithm are trained with 1M environmental steps and evaluated\neach 10k steps across 5 different seeds. We conduct the experiments under 3 different random seeds to\nseek more general trend and conclude systematic results. All the experiments are run on the Desktop\nequipped with RTX 4090 and Core-i9 13900K. \nPerformance in Stochastic MuJoCo. In Table 2, we present the performance under the stochastic\nMuJoCo environments. Our method can achieve the best performance compared to all of the\nbaselines in stochastic environments. Specifically, for InvertedDoublePendulum-v4, introduc-\ning stochasticity into the dynamics significantly degrades the performance. Our method still can still\nachieve performance better than all baselines. Furthermore, in the stochastic settings, the performance\nof DAC is decreased significantly, as DAC's ineffective reward formulation on state-action pairs under\nuncertain conditions, also resulting a trend of learning instability \nFor Hopper-v4, our\nmethod remarkably surpasses the all baselines with significantly large margins. These results indicate\nthat our approach can effectively learn the better reward function from demonstrations in stochastic\nenvironments, resulting in significant performance improvement.\nPerformance in Deterministic MuJoCo. The performance of deterministic MuJoCo environments\ncan be found in Table 3. For the tasks with deterministic dynamics, our method can achieve\nthe performance aligning with all of baselines and the expert in InvertedDoublePendulum-v4,\nInvertedDoublePendulum-v4 and Hopper-v3. Additionally, our method shows superiority in"}, {"title": "7 Conclusion", "content": "In this paper, we presented a novel model-enhanced adversarial inverse reinforcement learning frame-\nwork by incorporating model-based techniques with reward shaping, specifically designed to enhance\nperformance in stochastic environments and maintain competitive performance in deterministic\nsetting while significantly improve performance under the traditional AIL approaches. The theoretical\nanalysis provides guarantees on the optimal policy invariance under the transition model involved\nreward shaping and highlight the relationship between performance gap and transition model error,\nshowing that the gaps becomes negligible with a well-learned model. Empirical evaluations on\nMujoco benchmark environments validate the effectiveness of our method, showcasing its superior\nperformance and sample efficiency across different tasks. Future works will focus on further refining\nthe model estimation process to handle more complex and dynamic environments and exploring exten-\nsions of the framework to multi-agent and hierarchical reinforcement learning scenarios. Additionally,\nit would be valuable to investigate the generalization ability of our framework in a transfer learning\ntasks. Overall, our approach offers a promising direction for advancing model-based adversarial IRL,\nwith the potential to scale to a broader range of real-world applications."}, {"title": "8 Reproducible Statement", "content": "This work uses the open-source MuJoCo (Todorov et al., 2012) as the benchmark. The practical\nimplementation of our method is built on the CleanRL repository (Huang et al., 2022). All hyperpa-\nrameters to reproduce our experimental results, including learning rates and transition model settings,"}]}