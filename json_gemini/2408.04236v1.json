{"title": "Cluster-Wide Task Slowdown Detection in Cloud System", "authors": ["Feiyi Chen", "Yingying Zhang", "Lunting Fan", "Yuxuan Liang", "Guansong Pang", "Qingsong Wen", "Shuiguang Deng"], "abstract": "Slow task detection is a critical problem in cloud operation and maintenance since it is highly related to user experience and can bring substantial liquidated damages. Most anomaly detection methods detect it from a single-task aspect. However, considering millions of concurrent tasks in large-scale cloud computing clusters, it becomes impractical and inefficient. Moreover, single-task slowdowns are very common and do not necessarily indicate a malfunction of a cluster due to its violent fluctuation nature in a virtual environment. Thus, we shift our attention to cluster-wide task slowdowns by utilizing the duration time distribution of tasks across a cluster, so that the computation complexity is not relevant to the number of tasks. The task duration time distribution often exhibits compound periodicity and local exceptional fluctuations over time. Though transformer-based methods are one of the most powerful methods to capture these time series normal variation patterns, we empirically find and theoretically explain the flaw of the standard attention mechanism in reconstructing subperiods with low amplitude when dealing with compound periodicity. To tackle these challenges, we propose SORN (i.e., Skimming Off subperiods in descending amplitude order and Reconstructing Non-slowing fluctuation), which consists of a Skimming Attention mechanism to reconstruct the compound periodicity and a Neural Optimal Transport module to distinguish cluster-wide slowdowns from other exceptional fluctuations. Furthermore, since anomalies in the training set are inevitable in a practical scenario, we propose a picky loss function, which adaptively assigns higher weights to reliable time slots in the training set. Extensive experiments demonstrate that SORN outperforms state-of-the-art methods on multiple real-world industrial datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Slow task detection is a critical issue in cloud operations and maintenance, as it directly impacts user experience and can lead to significant penalties for service level agreement violations [39]. Most existing anomaly detection methods focus on detecting task slowdowns at the individual task level [23, 34, 44, 46]. However, with millions of tasks running concurrently [23, 49] in large-scale cloud computing clusters, these approaches become impractical and inefficient. Moreover, single-task slowdowns are common and may not indicate a cluster malfunction, given the random and dramatic fluctuations in task duration time within a virtual environment. To address these challenges, we pivot towards detecting slowdowns on a cluster-wide scale, which are more indicative of cluster malfunctions and can be identified without examining each individual task. Furthermore, unlike the random fluctuations observed in single-task duration time, the duration time of cluster-wide tasks exhibits more regular patterns, making slowdown detection more feasible.\nParticularly, we detect cluster-wide task slowdowns using the duration time distribution of a cluster, as illustrated in Fig. 1(a), in which for each time slot we partition the range of task duration time into intervals and calculate the proportion of tasks falling into each interval. This strategic shift not only significantly reduces the computational complexity of our algorithm, making it independent of the number of tasks, but also enhances the accuracy of cluster malfunction detection.\nNonetheless, the distribution of normal task duration time is not stable but varies over time. Hence, there arises a necessity to discern the patterns of distribution variation and differentiate routine slowdowns from anomalies. Among the various methods for extracting normal patterns, transformer-based methods stand out as one of the most powerful and effective unsupervised anomaly detection approaches, resulting in numerous distinguished methods [19, 37, 43, 44]. Despite the abundance of powerful neural networks available for normal pattern extraction, several challenges persist:\n\u2022 Compound periodicity: The distribution of cluster-wide task duration time often exhibits compound periodic variation patterns. Since different tasks exhibit different periodicity, the periodicity of cluster-wide task duration time distribution is compound and complicated. For example, in Fig. 1(b), it shows periodicity on both a weekly and daily basis. As depicted in Fig.1(c), when integrating two periodicities with different amplitudes and frequencies into a unified representation, the attention mechanism shows subpar performance in reconstructing the subperiodicity with relatively low amplitude in the presence of compound periodicity.\n\u2022 Non-slowing exceptional fluctuations: The temporal evolution of task duration time within the cluster manifests periodic characteristics on a global scale, interspersing with localized non-periodic exceptional fluctuations. Within these exceptional fluctuations, only a small fraction corresponds to cluster-wide slowdowns, while others are not the focus of our work (e.g., we are not concerned about exceptional task speedups). However, traditional anomaly detection methods can not reconstruct all of the exceptional fluctuations well and detect all of them as anomalies. To distinguish cluster-wide task slowdowns, it is imperative to accurately reconstruct other exceptional fluctuations while excluding the cluster-wide slowdowns.\n\u2022 Anomalies in the training set: In consideration of the substantial costs linked to manually labeling anomalies, our methodology has been intentionally crafted to function in an unsupervised manner. Nevertheless, it is noteworthy that several unsupervised methods operate on the assumption that anomalies are infrequent within the training set, a premise that tends to be overly optimistic in practical scenarios.\nAddressing these challenges is imperative for improving the detection accuracy of compound periodic time series and enhancing model robustness against anomaly contamination in the training set. Therefore, we propose SORN, which Skims Off the subperiodicity with different amplitudes layer by layer and selectively Reconstructs the Non-slowing fluctuations excluding the cluster-wide task slowdowns. It contains three innovative mechanisms to tackle the aforementioned three issues correspondingly: Skimming Attention, Neural Optimal Transport (OT), and Picky Loss.\nSpecifically, we first theoretically prove that the standard attention mechanism tends to allocate more attention to subperiods with higher amplitudes in compound periodic time series. This bias prevents it from effectively reconstructing subperiods with relatively low amplitudes. Building on this analysis, we introduce a skimming attention mechanism to capture the compound periodicity pattern, where we sequentially skim off subperiods from the original sequence in descending order of amplitudes and reconstruct iteratively from the remaining series. In this way, the subperiods with higher amplitudes are initially well reconstructed and skimmed off from the original time series. After that, the subperiods with low amplitudes in the original series become subperiods with relatively high amplitudes in the remaining series and can be better reconstructed.\nSubsequently, we use a Neural OT module to adjust the reconstructed series of skimming attention, where we innovatively transform the traditional optimal transport problem into a neural network, and by intricately designing a transportation cost matrix, we can selectively reconstruct the non-slowing fluctuations.\nFurthermore, to mitigate the negative effect of anomaly contamination in the training set, we design a novel picky loss function, which allocates different weights to time slots in the loss function according to their reliability.\nAccordingly, this work presents several novel and distinctive contributions to the field of cluster-wide slow task detection:\n\u2022 We present the first attempt to formalize the cluster-wide slowdown problem with the identification of the problem specifications and relevant challenges.\n\u2022 We provide a theoretical explanation for the limitations of the standard attention mechanism in effectively reconstructing subperiods with low amplitude in compound periodicity. Moreover, we introduce a novel skimming attention mechanism designed to extract subperiodic components with varying amplitudes and aggregate them to ensure accurate reconstruction of both high and low-amplitude subperiods.\n\u2022 We introduce a novel Neural OT module tailored to reconstruct the normal non-periodic fluctuations observed in the duration time distribution, while effectively filtering out the cluster-wide slow-down anomalies.\n\u2022 We propose a picky loss function that assigns higher weights to reliable time slots within the loss function.\nBesides, we conducted extensive experiments and demonstrated that our method outperforms the state-of-the-art (SOTA) methods in F1 score on real-world industrial datasets."}, {"title": "2 PRELIMINARY", "content": ""}, {"title": "2.1 Optimal Transport (OT)", "content": "It is given a set of value intervals $I = \\{(s_1, s_2], (s_2, s_3], ..., (s_{n-1}, s_n]\\}$ and two distributions $a \\in R^N$ and $b \\in R^N$, where $a_i = P(s_i < x < s_{i+1}), x \\sim a$. Similarly, $b_i = P(s_i < x \\leq s_{i+1}), x \\sim b$. The Optimal Transport problem aims at transforming distribution $a$ to $b$ by moving a fraction of the amount in each interval of $a$ to another interval. Moving a unit from $j$th interval to $i$th interval costs a price $C_{i,j}$. The Optimal Transport problem gropes for an optimal transport strategy $P$ costing the lowest price, where $P_{i,j}$ denotes the amount of unit moving from $j$th interval to the $i$th, as shown in Eq.1, where $< P, C >$ denotes the Frobenius dot-product.\n$\\min_P < P, C >$,\ns.t. $P. 1 = b, P^T. 1 = a.$\n(1)"}, {"title": "2.2 Problem Setup", "content": "Definition 1. $f_t$ and $f_t^*$ are used to denote the real-time distribution and expected distribution at time slot $t$. $f_t(a)$ and $f_t^*(a)$ are used to denote the $a$-quantile of distribution $f_t$ and $f_t^*$. $T$ is used to denote the threshold for tolerable fluctuation range of duration time distribution.\nDefinition 2. If there is a slowdown at time slot $t$, then $max_a f_t(a)-f_t^*(a) > \\Gamma, \\forall a$.\nDefinition 3. (Input data & output data) Given a set of intervals $I = \\{[s_1, s_2), [s_2, s_3), ..., [s_D, s_{D+1})\\}$, the input data is a $T$-length and $D$ dimensional multivariate time series $x \\in R^{T \\times D}$, where $x [t, d]$ is the number of tasks whose duration time falls into the $d$th interval $[s_d, s_{d+1})$. The reconstruction series of SORN is denoted by $\\hat{x}$.\nProblem Formalization. We use a SORN to obtain a reconstruction series $\\hat{x}$ from the original input data $x$. Subsequently, we use an anomaly score function $AnomalyScore(x, \\hat{x}, I)$. We aim to maximize the anomaly score gap between the slow-down time slots and the others."}, {"title": "3 METHODOLOGY", "content": "The overview of SORN is depicted in Fig. 2(a). We sequentially mask each time slot in $x$ and employ a multi-layer Skimming Attention mechanism to reconstruct the time slot by leveraging compound periodic information. Subsequently, we utilize Neural OT to finetune the reconstructed series obtained from Skimming Attention, capturing aperiodic but typical fluctuations in the time series. Finally, we apply the picky loss function to assign higher weights to normal time slots while assigning lower weights to occasional anomalous slots in the loss function."}, {"title": "3.1 Skimming Attention", "content": "The duration time distribution usually exhibits compound periodic fluctuations, as shown in Fig. 1(b). In a compound periodic series, different subperiods usually have different amplitudes (i.e., variation range) [40], as shown in Fig. 3(a). When dealing with this kind of compound periodicity, the standard attention mechanism falls short in reconstructing the subperiod with low amplitude, as shown in Fig.1(c), where we fuse two periodicities with different amplitudes and frequency, the standard attention only reconstructs the one with higher amplitude well. We theoretically explain this phenomenon in Theorem 1 and Theorem 2, where we prove that a self-attention mechanism pays more attention to the subperiod with relatively higher amplitude in compound periodic series, which degrades the performance of reconstructing the subperiods with lower amplitudes in compound periodic series. Thus, we propose a skimming attention that masks each time slot alternatively and aims at reconstructing it by compound periodic information. There are two challenges to achieving this. On the one hand, we need to prevent it from reconstructing time slots only by leveraging the similarity of adjacent time slots in each layer but neglecting the periodic information. On the other hand, we need to reconstruct every subperiod well rather than just those with high amplitudes.\nWe deduce Theorems 1-2 using the same setting as the self-attention mechanism in a patching transformer [25], where a time series is split into a set of p-length patches, which constitute the query, key, and value vectors of a self-attention mechanism. We start with a simple case and generalize it to a general situation. In the derivation, we omit the final step of applying softmax to the attention weights, as softmax does not alter the relative order of the attention weights assigned to different time slots in the sequence and will not affect the conclusion.\nDefinition 4. Given $a, b \\in Z, a \\neq b$, we set the patch length $p$ to $lcm(a, b)$, where $lcm(a, b)$ denotes the least common multiple of $a$ and $b$. It is given a series with compound periodicity $f(t) = c_1 cos(w_1t) + c_2 sin(w_2t)$, where $w_1 = \\frac{2a\\pi}{p}, w_2 = \\frac{2b}{p}$ and $C_1, C_2 \\in R, C_1 > C_2$. There are two subperiod component in $f(t)$: $f_1(t) = C_1 cos(w_1t)$ and $f_2(t) = c_2 sin(w_2t)$. We use $T_1$ and $T_2$ to denote the period length of $f_1$ and $f_2$ respectively.\nTheorem 1. In $f(t)$, when taking the patch starting from the time slot as the query, the attention weight of the patch starting from this is $[c_1^2 cos w_1\\Delta t + c_2^2 cos w_2\\Delta t]$, where $\\Delta t = (t_2 - t_1)$.\nProof. Please refer to Appendix A for more details.\nTaking a further look at the attention weight $[c_1^2 cos w_1 \\Delta t +c_2^2 cos w_2\\Delta t]$, it is a linear combination of $cos w_1\\Delta t$ and $cos w_2\\Delta t$. The first one distributes attention weight according to the periodicity of $f_1$: it assigns the highest attention weight to the time slot that is $nT_1$-slots apart from the query time slot, where $n\\in Z$ (i.e. when $\\Delta t = nT_1$, $cos w_1\\Delta t$ reaches its maximum value). Similarly, the second one distributes attention weight according to the periodicity of $f_2$ and assigns the highest attention weight to the time slot that is $nT_2$ apart from the query time slot. Moreover, their impact on the attention weight is decided by the amplitudes of their corresponding subperiod. Since $c_1 > C_2$, $cos w_1 \\Delta t$ contributes more to the attention weight. Thus, the periodic information of $f_1$ can obtain higher attention weight and $f_1$ will be reconstructed better. As shown in Fig. 3(b), the highest attention weights show up at the time slot that $nT_1$-slots apart from the query slot without concerning the subperiod with period length of $T_2$.\nTo generalize Theorem 1 to a general situation, given a time series $f(t)$ with compound periodicity, we use trigonometric series to decompose it as defined in Definition 5.\nDefinition 5. Given a compound periodic time series $f(t)$ with period length $p$, we set the patch length to $p$. We decompose $f(t)$ to a linear combination of trigonometric series as: $f(t) = \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} (a_n cos w_nt + b_n sin w_nt)$, where $w_n = \\frac{2n\\pi}{p}$ and $a_n$ and $b_n$ are coefficients for triangonometric series.\nTheorem 2. In $f(t)$, when taking the patch starting from $t$th time slot as the query, the attention weight of the patch starting from $t$th is $\\frac{a_0^2}{4} + \\sum_{n=1}^{\\infty} (a_n^2 + b_n^2) cos w_n\\Delta t$, where $\\Delta t = (t_2 - t_1)$.\nProof. Please refer to Appendix B for more details.\nSimilar to the analysis of $f(t)$, the attention weight of $f(t)$ is a linear combination of $cos w_n\\Delta t$. The subperiods with the higher amplitudes are more decisive to the attention weight distribution"}, {"title": "3.2 Neural OT", "content": "Besides the periodic patterns, there are still aperiodic but normal fluctuations in task duration time distribution. Since we only pay attention to slow-down anomalies but not others (e.g., the execution speed of homework has significantly increased), we target modeling these non-periodic fluctuations but only hinder the reconstruction of slow-down anomalies. Inspired by the Optimal Transport (OT) algorithm, we transform a standard OT problem into a neural network and embed it into our model so that our model becomes end-to-end.\nWe first establish an OT problem and then transform it into a neural network. For each time slot t, we take its reconstruction duration time distribution $\\hat{x}[t] \\in R^{1\\times d}$ as a source distribution and take its original duration time distribution $x[t] \\in R^{1\\times d}$ as a target distribution. The OT problem gropes for a transport strategy P to transform the source distribution to the target distribution with a minimum cost $< P * x[t], C >$, where $P[d, s]$ denotes the ratio of $\\hat{x} [t, s]$ transporting to $x[t, d]$, $C[d, s]$ denotes the cost of transporting a unit from $\\hat{x}[t, s]$ to $x[t, d]$ and $*$ denotes element-wise multiplication. According to the definition of P, $Px[t]$ denotes the distribution after applying the transport strategy P to the source distribution $\\hat{x}[t]$, which should approach the target distribution $x[t]$, and the sum of each column of P should be 1. Thus, we formulate $|Px[t] - x[t]|$ as an optimization goal and the $P^T1 = 1$ as a constraint in our OT problem. To reconstruct anomalies except the slow ones, we set C as follows:\n$C_{i,j} = \\begin{cases}\nM[i]-M[j], & i>j \\\\\n0, & else, \\\\\n\\end{cases}$\n(10)\nwhere M[i] is the midpoint of ith interval in I (I is defined in Definition 3). In this way, only the slow-down distribution shift is penalized by the transporting cost. Based on the setting above, we formulate an OT problem as:\n$\\min_P \\lambda. < P * x[t], C > + ||Px[t] - x[t]||_2$,\ns.t.$P^T1 = 1,$\n(11)\nwhere $\\lambda$ is a hyperparameter belonging to [0, 1].\nFurthermore, we transform it into a neural network. We take P as a trainable parameter. To meet its constraint in the OT problem, we manipulate P as $softmax(P^T)^T$, and the neural layer is specified as:\n$\\hat{x} = softmax(P^T)^Tx$\n(12)\nBesides, we also introduce the optimization objective of the OT problem to the loss function."}, {"title": "3.3 Picky Loss Function", "content": "The reconstruction-based methods assume that there are no anomalies in the training set. However, it is inevitable to have some anomalies in the training set in the scenario of unsupervised learning. Thus, we propose a picky loss function, which adaptively attributes a weight $W \\in R^T$ according to trustworthiness to the loss of each time slot. The more trustful a time slot is, the higher its weight is. Inspiring by AnomalyTransformer [43], which points out that the normal points can establish wide-broad informative association along the whole series in attention mechanism whereas the anomalies can only concentrate on adjacent time slots, we utilize"}, {"title": "4 EXPERIMENT", "content": "We have made extensive experiments on four datasets to verify the following conclusions:\n\u2022 SORN can achieve the best performances on the four datasets, compared with the SOTA methods.\n\u2022 SORN consumes tolerable time and memory overhead.\n\u2022 SORN is parameter insensitive.\n\u2022 SORN is resistant to noise and lax periodicity.\n\u2022 Each module in SORN has contributed to the performance."}, {"title": "4.1 Experiment Setup", "content": "Baseline Methods. We compare SORN with the SOTA anomaly detection methods: DCdetector [44], TranAD [37], AnomalyTransformer [43], VQRAE [17], OmniAnomaly [34], MSCRED [46]. Furthermore, we compare SORN with a method specifically designed for slow-down detection: IASO [26] and a method designed for distribution shift detection, feature-shift detection [18].\nDatasets. We perform our experiments on four datasets. Two of them (Ali1, Ali2) are monitoring data of industrial cloud clusters from Alibaba. One of them (Mustang) is disclosed by Carnegie Mellon Parallel Data Laboratory, and we label the slow-down anomalies"}, {"title": "3.4 Anomaly Score", "content": "Since the duration time distribution of different tasks does not distribute uniformly, we split the distribution intervals I according to the distribution density of task duration time. This leads to the heterogeneous importance of the reconstruction errors for different intervals. However, the trivial anomaly score, which adds the reconstruction errors for different intervals together, ignores this heterogeneity. Thus, we use the difference between the task duration time expectations of the original distribution and reconstruction one as the anomaly score:\n$AnomalyScore[t] = E(\\hat{T}(x[t])) - E(T(x[t]))$\n$= \\sum_{d=0}^D \\hat{x}[t, d] * M[d] - \\sum_{d=0}^D x[t, d] * M[d],$\n(15)\nwhere $AnomalyScore[t]$ denotes the anomaly score of tth time slot, and $T(x[t])$ and $\\hat{T}(x[t])$ denote two variables: the task duration time from distributions x[t] and $\\hat{x}[t]$ respectively."}, {"title": "4.2 Prediction Accuracy", "content": "We take 70% of each subset as the training set and take the remaining 30% as the testing set. For each subset, we train a unique model. This training strategy is also adopted by other marvelous works, such as [7, 34, 46]. We show the performance of SORN and baselines in Tab. 3, where we use \"Pre\" and \"Rec\" to stand for precision and recall respectively, and highlight the best performance as the boldfaced. When SORN achieves the best performance, we underline the best performance among baselines. Otherwise, we underline the second-best performance among all methods. SORN achieves the best F1 scores on all datasets compared with the state-of-the-art methods. Comparing the performance of our method on four datasets, we observe that it performs best on the Alil and Ali2 datasets, followed by Mustang and Sync. It can be seen that the effectiveness of our method is positively correlated to the strictness of periodicity in the datasets. It achieves impressive performance on datasets with strict periodicity, while also demonstrating competitive results on datasets with relaxed periodicity or non-periodic"}, {"title": "4.3 Time and Memory Overhead", "content": "We evaluated both time and memory overhead on a server equipped with a configuration comprising 32 Intel(R) Xeon(R) CPU E5-2620 @ 2.10GHz CPUs and 2 K80 GPUs. We use the checkpoint sizes to stand for the neural network memory overhead and use the time of training model for one epoch to stand for the time overhead. As for the non-neural network methods, IASO and feature-shift detection, we use the maximum memory consumption during its inferring process as its memory overhead. We show the time and memory overhead of different methods in Fig. 4(b), where SORN only introduces marginal time and memory overhead compared with some light methods, such as OmniAnomaly, but can achieve better performance on all the datasets. Compared with some transformer-based methods, such as AnomalyTransformer and DCdetector, we use less memory overhead yet achieve better accuracy. In this way, SORN can better meet the real-time requirements of the cloud center."}, {"title": "4.4 Hyperparameter Sensitivity", "content": "We test the performance of SORN when setting the number of skimming layers and patch size as the Cartesian product of {1,3,5,7,9} for skimming layers and {1,3,5,7,9,11,13} for patch size. We exhibit"}, {"title": "4.5 The Impact of Dataset Property", "content": "We investigate the impact of four factors on the performance of SORN on the Sync dataset: the noise, periodicity strictness, slow task ratio, and average slow-down time in slow-down anomalies. The noise introduced into the Sync data is a random variable with a mean of 0 and standard deviation of noise * A, where A is the amplitude of the original time series. To manipulate the periodicity strictness, we distort each period of the original series by using a scalar randomly sampled from a distribution (1, 1 + R] to extend it. When we test the impact of the noise, we make the time series strictly periodic before introducing noise and vice versa. The results are displayed in Fig. 5(a)-Fig. 5(d). Generally, when the time series is strictly periodic without any noise, SORN can achieve excellent performance on the Sync dataset. When the noise becomes more variable and the periodicity is more severely distorted, the performance degrades but SORN is still sensitive and accurate: SORN can achieve an F1 score over 0.9 as long as the slow task ratio overpasses 10% in all conditions of the noise and periodicity strictness explored in our experiment; SORN can achieve an F1 score over 0.9 as long as the average slow-down time overpasses 60 minutes in"}, {"title": "4.6 Ablation Study", "content": "To evaluate the contribution of each module in SORN, we alternatively remove each submodule and test the performance of the remaining model. Specifically, we denote SORN removing skimming attention as SORNT, denote SORN removing neural OT as SORNS and denote SORN replacing picky loss with MSE as SORN+. When removing the skimming attention mechanism, we replace it with a standard attention. When removing the picky loss, we substitute it with MSE. As shown in Table 3, the completed SORN achieves the best performance. Thus, each submodule of SORN does contribute to the performance."}, {"title": "5 RELATED WORK", "content": "To the best of our knowledge, we are the first to investigate the issue of cluster-wide task slowdowns. While numerous works delve into slow query detection [22, 51] and disk fail-slow detection [20, 21], they primarily focus on detecting slowdowns at the level of individual SQL queries or disks rather than considering the overall aspect. However, detecting slow tasks at the individual level can be unreliable in cloud virtual environments, where task duration time fluctuates randomly and significantly. Single-task slowdowns are common and do not necessarily indicate a cluster malfunction.\nMoreover, time series anomaly detection is another relevant area, as we need to capture the normal variation pattern and time dependencies of time series [16, 48]. Time series anomaly detection methods can be broadly categorized into three classes: classical methods [3, 11, 24, 27], signal-processing-based methods [1, 23, 50], and deep learning-based methods [6, 14, 30, 35, 41, 42, 47, 52]. Classical methods typically rely on statistical approaches and have relatively low computational overhead. However, they often make specific assumptions that limit their robustness in detecting anomalies in cloud environments [23]. Signal-processing-based methods leverage the sparsity inherent in the frequency domain to reduce computational overhead. However, they may overlook local subtle features [1] or struggle to handle heavy traffic loads in real-time scenarios [23]. Deep learning-based anomaly detection methods have reported promising performance and diversified into various approaches, including prediction-based [5, 14, 30, 52], reconstruction-based [6, 8, 10, 13, 15, 32, 36, 45], classification-based [12, 29, 31, 35, 42], and perturbation-based methods [4, 33]. Among them, reconstruction-based methods have shown strong advantages over others [17], in which the transformer-based methods have demonstrated good performance recently [28, 38, 43]. However, as we mentioned earlier, the standard attention mechanism may struggle to reconstruct compound periodic time series effectively."}, {"title": "6 CONCLUSION", "content": "In this study, we introduce SORN as a method for detecting cluster-wide task slowdowns in cloud clusters, offering three distinctive features: 1) Skimming Attention, where we provide a theoretical explanation for the limitations of standard attention mechanisms in reconstructing compound periodicity and propose a method to separately reconstruct subperiodic components to ensure accurate reconstruction of both high and low amplitude subperiods; 2) Neural OT, which selectively reconstructs non-slowing exceptional fluctuations; 3) Picky Loss, which assigns weights to time slots in the loss function based on their reliability. Additionally, extensive experiments demonstrate that SORN outperforms state-of-the-art methods in real-world datasets. In the future, we will use large language models for further analysis of the causes of slow-down tasks based on this foundation and employ multi-agent systems for automatic recovery."}, {"title": "A PROOF OF THEOREM 1", "content": "In the following, we use $AttentionWeight[t_1, t_2]$ to denote the attention weight of the patch starting from $t_2$th time slot, when using the patch starting from $t_1$th time slot as the query. We use the orthogonality of trigonometric functions when deriving Eq. 16 to Eq.17. Since $cos w_1t cos w_1 (t+\\Delta t) = \\frac{1}{2} cos(w_1t + w_1(t+\\Delta t))+cos(w_1t-w_1 (t+\\Delta t))$, $sin w_2t sin w_2(t+\\Delta t) = -(\\frac{1}{2} cos(w_2t + w_2(t+\\Delta t))-cos(w_2t-w_2(t+\\Delta t)))$, and $\\int_{t_1}^{t_1+p} cos(2w_1t + w_1\\Delta t) dt = 0$ (because p is integer multiple of the period length of $cos(2w_1t + w_1\\Delta t)$), we derive Eq. 17 to Eq. 18. Since $\\Delta t$ is a constant without relevance to $t$, we derive Eq. 18 to Eq. 19.\n$AttentionWeight[t_1, t_2] = \\int_{t_1}^{t_1+p} (c_1 cos w_1t + c_2 sin w_2t) [c_1 COS w_1 (t + \\Delta t) + c_2 sin w_2(t + \\Delta t)] dt$ (16)\n$= \\int_{t_1}^{t_1+p} c_1^2 cos(w_1t) cos w_1 (t + \\Delta t) + c_2^2 sin(w_2t) sin w_2(t + \\Delta t) dt$ (17)\n$= \\int_{t_1}^{t_1+p} \\frac{c_1^2}{2} cos w_1\\Delta t + \\frac{c_2^2}{2} cos w_2 \\Delta t dt$ (18)\n$= p(\\frac{c_1^2}{2} cos w_1\\Delta t + \\frac{c_2^2}{2} cos w_2 \\Delta t)$ (19)"}, {"title": "B PROOF OF THEOREM 2", "content": "We prove Theorem 2 in a similar way as in Theorem 1.\n$AttentionWeight[t_1, t_2] = \\int_{t_1}^{t_1+p} (\\frac{a_0}{2} + \\sum_{n=0}^{\\infty} a_n COS w_nt + b_n sin w_nt). [\\frac{a_0}{2} + \\sum_{n=0}^{\\infty} a_n cos w_n(t + \\Delta t) + b_n sin w_n(t + \\Delta t)] dt$ (20)\n$= \\int_{t_1}^{t_1+p} \\frac{a_0^2}{4} + \\sum_{n=0}^{\\infty} a_n cos w_nt cos w_n (t + \\Delta t) + b_n sin w_nt sin w_n(t + \\Delta t) dt$\n$= \\int_{t_1}^{t_1+p} \\frac{a_0^2}{4} + \\sum_{n=0}^{\\infty} (a_n^2 + b_n^2) cos w_n \\Delta t dt$\n$= p(\\frac{a_0^2}{4} + \\sum_{n=0}^{\\infty} \\frac{(a_n^2 + b_n^2)}{2} cos w_n \\Delta t)$"}, {"title": "C DATA PREPROCESSING", "content": "The code and some datasets are available at https://github.com/gyhswtxnc/SORN.\n\u2022 Ali1 & Ali2 (periodic): We collect these datasets by tracing 25 industrial cloud clusters from Alibaba for 15 days. Most of the labels in these two datasets are assigned manually according to the experience of our engineers. Some of the labels are assigned according to our customer's feedback. These two datasets were collected on server clusters in different regions, and there is a significant difference in the anomaly proportion between them. Each subset in Ali1 and Ali2 stands for a cluster.\n\u2022 Mustang (lax periodic) [2]: Mustang is a dataset that records task duration time for 5 years. We preprocess the original dataset as shown in Appendix. C and label the slow-down anomalies manually. Then, we equally divide the five years of tracing data into 35 intervals and constitute 35 subsets.\n\u2022 Sync (mixture of periodic and aperiodic): We synthesize this dataset by combining cosine waves with different frequencies and amplitudes. Then, we manually insert noise, distorted period and slow-down anomalies.\nFor every dataset, we count a task duration time distribution I at each time slot and divide the intervals in I according to the distribution density of the execution time. We show the interval division for every dataset in Tab. 4."}, {"title": "D HYPERPARAMETER SEARCHING SPACE", "content": "We use grid-search to figure out the optimal hyperparameter settings. We list the ranges for important hyperparameters in Tab.5."}, {"title": "E BASELINES INTRODUCTION"}]}