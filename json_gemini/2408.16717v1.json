{"title": "A GREAT ARCHITECTURE FOR EDGE-BASED GRAPH PROBLEMS LIKE TSP", "authors": ["Attila Lischka", "Jiaming Wu", "Morteza Haghir Chehreghani", "Bal\u00e1zs Kulcs\u00e1r"], "abstract": "In the last years, many neural network-based approaches have been proposed to tackle combinatorial optimization problems such as routing problems. Many of these approaches are based on graph neural networks (GNNs) or related trans- formers, operating on the Euclidean coordinates representing the routing prob- lems. However, GNNs are inherently not well suited to operate on dense graphs, such as in routing problems. Furthermore, models operating on Euclidean coor- dinates cannot be applied to non-Euclidean versions of routing problems that are often found in real-world settings. To overcome these limitations, we propose a novel GNN-related edge-based neural model called Graph Edge Attention Net- work (GREAT). We evaluate the performance of GREAT in the edge-classification task to predict optimal edges in the Traveling Salesman Problem (TSP). We can use such a trained GREAT model to produce sparse TSP graph instances, keep- ing only the edges GREAT finds promising. Compared to other, non-learning- based methods to sparsify TSP graphs, GREAT can produce very sparse graphs while keeping most of the optimal edges. Furthermore, we build a reinforce- ment learning-based GREAT framework which we apply to Euclidean and non-Euclidean asymmetric TSP. This framework achieves state-of-the-art results.", "sections": [{"title": "INTRODUCTION", "content": "Graph neural networks (GNNs) have emerged as a powerful tool for learning on graph-structured data such as molecules, social networks, or citation graphs Wu et al. (2020). In recent years, GNNs have also been applied in the setting of combinatorial optimization, especially routing problems Joshi et al. (2019); Hudson et al. (2021); Xin et al. (2021) since such problems can be interpreted as graph problems. However, the graph representations of routing problems, which are typically complete, dense graphs, are ill-suited for GNNs. This is because vanilla GNNs are not generally suitable for learning on complete graphs. GNNs are related to the Weisfeiler Leman algorithm which is known to exploit graph structure Morris et al. (2019). Complete graphs feature no such structure, resulting in poor GNN performance. Moreover, over-smoothing is a well-known problem happening in (deep) GNNs which means that feature vectors computed for different nodes become more and more similar with every layer Rusch et al. (2023). Naturally, in dense or even complete graphs this problem is even more present as all nodes share the same information leading to similar encodings. Consequently, Lischka et al. (2024) showed that the performance of GNNs operating on routing problems can be increased if graphs are made sparse in a preprocessing step. However, the proposed sparsification methods of Lischka et al. (2024) rely on hand-crafted heuristics which goes against the idea of data-driven, end-to-end machine learning frameworks.\nIn this paper, we overcome the limitations of regular GNNs by introducing the Graph Edge Attention Network (GREAT). This results in the following contributions:\n\u2022 Whereas traditional GNNs operate on a node-level by using node-based message pass- ing operations, GREAT is edge-based, meaning information is passed along edges shar- ing endpoints. This makes GREAT perfect for edge-level tasks such as routing problems where the edges to travel along are selected. We note, however, that the idea of GREAT is"}, {"title": "BASICS AND RELATED WORK", "content": "Graph neural networks are a class of neural architectures that operate on graph-structured data. In contrast to other neural architectures like MLPs where the connections of the neurons are fixed and grid-shaped, the connections in a GNN reflect the structure of the input data.\nIn essence, GNNs are a neural version of the well-known Weisfeiler-Leman (WL) graph isomorphism heuristic Morris et al. (2019); Xu et al. (2019). In this heuristic, graph nodes are assigned colors that are iteratively updated. Two nodes share the same color if they shared the same color in the previous iteration and they had the same amount of neighbors of each color in the last iteration. When the test is applied to two graphs and the graphs do not have the same amount of nodes of some color in some iteration, they are non-isomorphic. WL is only a heuristic, however, as there are certain non-isomorphic graphs it can not distinguish. Examples are regular graphs (graphs where all nodes have the same degree) Kiefer (2020). We note that complete graphs (that we encounter in routing problems) are regular graphs.\nGNNs follow a similar principle as the WL heuristic, but instead of colors, vector representations of the nodes are computed. GNNs iteratively compute these node feature vectors by aggregating over the node feature vectors of adjacent nodes and mapping the old feature vector together with the aggregation to a new node feature vector. Additionally, the feature vectors are multiplied with trainable weight matrices and non-linearities are applied to achieve actual learning. The node feature vectors of a neighborhood are typically scaled in some way (depending on the respective GNN architecture) and sometimes, edge feature vectors are also considered within the aggregations. An example can be found in fig. 1. Its mathematical formulation might look like this:\n$h^{l+1}_u = \\sigma\\Big(W_l h^l_u + \\sum_{v \\in N(u)} (W_l h^l_v + W_l^{edge} e_{uv})\\Big)$ (1)\nhere $W_l$, $W_l^{edge}$, $W'_l$ are trainable weight matrices of suitable sizes, $\\sigma$ is a non-linear activation function, $h^l_u$ denotes the feature vector of a node u in the lth update of the GNN and $e_{uv}$ denotes an edge feature of the edge (u, v) in the input graph. Sometimes, the edge features are also updated. The node feature vectors of the last layer of the GNN can be used for node-level classification or regression tasks. They can also be summarized (e.g. by aggregation) and used as a graph representation in graph-level tasks. Referring back to the WL algorithm, we note how the node colors there can also be considered as node classes. Furthermore, comparing the colors of different graphs to determine isomorphism can be considered a graph-level task. While GNNs are bounded in their expressiveness by the WL algorithm Morris et al. (2019); Xu et al. (2019) and can therefore not distinguish regular graphs (e.g., complete graphs), we acknowledge that these limitations of GNNs can be mitigated by assigning unique \u201cnode identifiers\u201d (like unique node coordinates) to graphs passed to GNNs (Abboud et al. (2021)). However, over-smoothing (Rusch et al. (2023)) is still a problem, especially in dense graphs. This results in a need for better neural encoder architectures in settings such as the routing problem."}, {"title": "GRAPH NEURAL NETWORKS", "content": "Graph Attention Networks (GATs Velickovic et al. (2017)) are a variety of GNNs. They leverage the attention mechanism Vaswani et al. (2017) to determine how to scale the messages sent between the network nodes. Overall, the node features are computed as follows:\n$x'_i = \\sum_{j \\in N(i) \\cup \\{i\\}} \\alpha_{i, j} \\Theta x_j$ (2)\nwhere $\\alpha_{i,j}$ is computed as\n$\\alpha_{i,j} = \\frac{\\exp(LeakyReLU(\\alpha^T \\Theta_s x_i + \\alpha^T \\Theta_t x_j + \\alpha^T \\Theta_e e_{i,j}))}{\\sum_{k \\in N(i) \\cup \\{i\\}} \\exp(LeakyReLU(\\alpha^T \\Theta_s x_i + \\alpha^T \\Theta_t x_k + \\alpha^T \\Theta_e e_{i,k}))}$ (3)\nand $\\Theta_\\epsilon, \\Theta_\\zeta, \\Theta_\\tau$, $\\alpha$, $\\alpha'$ are learnable parameters. We note that GAT uses the edge features only to compute the attention scores but does not update them nor uses them in the actual message passing.\nVariations of the original GAT also use edge features in the message-passing operations. For ex- ample, Chen & Chen (2021) propose Edge-Featured Graph Attention Networks (EGAT) which uses edge features by applying a GAT not only on the input graph itself but also its line graph represen- tation (compare Chen et al. (2017) as well) and then combining the computed features.\nAnother work incorporating edge features in an attention-based GNN is Shi et al. (2020) who use a \"Graph Transformer\u201d that incorporates edge and node features for a semi-supervised classification task.\nJin et al. (2023a) introduce \u201cEdgeFormers\u201d an architecture operating on Textual-Edge Networks where they combine the success of Transformers in LLM tasks and GNNs. Their architecture also augments GNNs to utilize edge (text) features."}, {"title": "LEARNING TO ROUTE", "content": "In recent years, many studies have tried to solve routing problems such as the Traveling Salesmen Problem or the Capacitated Vehicle Routing Problem (CVRP).\nPopular approaches for solving routing problems with the help of machine learning include rein- forcement learning (RL) frameworks, where encodings for the problem instances are computed. These encodings are then used to incrementally build solutions by selecting one node in the problem instance at a time. Successful works in this category include Deudon et al. (2018); Nazari et al. (2018); Kool et al. (2019); Kwon et al. (2020); Jin et al. (2023b).\nAnother possibility to use machine learning for solving routing problems is to predict edge probabil- ities or scores which are later used in search algorithms such as beam search or guided local search. Examples for such works are Joshi et al. (2019); Fu et al. (2021); Xin et al. (2021); Hudson et al. (2021); Kool et al. (2019); Min et al. (2024)."}, {"title": "NON-EUCLIDEAN ROUTING PROBLEMS", "content": "Many of the mentioned works, especially in the first two categories, use GNNs or transformer mod- els (which are related to GNNs via GATs Joshi (2020)) to capture the structure of the routing problem in their neural architecture. This is done by interpreting the coordinates of Euclidean routing prob- lem instances as node features. These node features are then processed in the GNN or transformer architectures to produce encodings of the overall problem. However, this limits the applicability of such works to Euclidean routing problems. This is unfortunate, as non-Euclidean routing problems are highly relevant in reality. Consider, e.g., one-way streets which result in unequal travel distances between two points depending on the direction one is going. Another example is variants of TSP that consider energy consumption as the objective to be minimized. If point A is located at a higher altitude than point B, traveling from A to B might require less energy than the other way around.\nSo far, only a few studies have also investigated non-Euclidean versions of routing problems, such as the asymmetric TSP (ATSP). One such study is Gaile et al. (2022) where they solve synthetic ATSP instances with unsupervised learning, reinforcement learning, and supervised learning approaches. Another study is Wang et al. (2023) that uses online reinforcement learning to solve ATSP instances of TSPLIB Reinelt (1991). Another successful work tackling ATSP is Kwon et al. (2021). There, the Matrix Encoding Network (MatNet) is proposed, a neural model suitable to operate on matrix en- codings representing combinatorial optimization problems such as the distance matrices of (A)TSP. Their model is trained using RL."}, {"title": "GRAPH EDGE ATTENTION NETWORK", "content": "Existing GNNs are based on node-level message-passing operations, making them perfect for node- level tasks as is also underlined by their connection to the WL heuristic. In contrast, we propose an edge-level-based GNN where information is passed along neighboring edges. This makes our model perfect for edge-level tasks such as edge classification (e.g., in the context of routing prob- lems, determining if edges are \u201cpromising\u201d to be part of the optimal solution or not). Our model is attention-based, meaning the \u201cfocus\" of an edge to another, adjacent edge in the update operation is determined using the attention mechanism. Consequently, similar to the Graph Attention Network (GAT) we call our architecture Graph Edge Attention Network (GREAT). A simple visualization of the idea of GREAT is shown in fig. 2. In this visualization, edge $e_{14}$ attends to all other edges it shares an endpoint with. While GREAT is a task-independent framework, it is suited perfectly for routing problems: Consider TSP as an example. There, we do not have any node features, only edge features given as distances between nodes. A normal GNN would not be suitable to process such information well. Existing papers use coordinates of the nodes in the Euclidean space as node features to overcome this limitation. However, this trick only works for Euclidean TSP and not other symmetric or asymmetric TSP adaptations. GREAT, however, can be applied to all these variants.\nInstead of purely focusing on edge features and ignoring node features, it would also be possible to transform the graph in its line graph and apply a GNN operating only on node features on this line graph. In the line graph, each edge $e_{i,j}$ of the original graph is a node $n_{i,j}$ and two nodes $n_{i,j}, n_{k,m}$ in the line graph are connected if the corresponding edges $e_{i,j}$ and $e_{k,m}$ in the original graph share an endpoint. However, in a TSP instance of n cities, there are $n^2$ many edges. Therefore, the line graph of this instance would have O($n^2$) many nodes. Furthermore, as the original TSP graph is complete, each of the endpoints {i, j} of an edge in the original graphs is part of n many edges. This means that in the line graph, each node has 2n many connections to other nodes. In other words, each of the O($n^2$) nodes has O(n) edges leading to O($n^3$) many edges in the line graph. This implies that the line graph has one order of magnitude more edges and nodes than the original graph.\nWe note that GREAT can be applied to extensions of the TSP such as the CVRP or TSP with time windows (TSPTW) as well: even though capacities and time windows are node-level features, we"}, {"title": "ARCHITECTURE", "content": "In the following, we provide the mathematical model defining the different layers of a GREAT model. In particular, we propose two versions of GREAT.\nThe first version is purely edge-focused and does not have any node features. Here, each edge exchanges information with all other edges it shares at least one endpoint with. The idea essentially corresponds to the visualization in fig. 2. In the following, we refer to this variant as \u201cnode-free\" GREAT.\nThe second version is also edge-focused but has intermediate, temporary node features. This essen- tially means that nodes are used to save all information on adjacent edges. Afterward, the features of an edge are computed by combining the temporary node features of their respective endpoints. These node features are then deleted and not passed on to the next layer, only the edge features are passed on. The idea of this GREAT variant is visualized in fig. 3 and fig. 4 In the remainder of this study, we refer to this GREAT version as \"node-based\"."}, {"title": "MATHEMATICAL FORMULATIONS", "content": "We now describe the mathematical formulas defining the internal operations of GREAT. We note that, inspired by the original transformer architecture of Vaswani et al. (2017), GREAT consists of two types of sublayers: attention sublayers and feedforward sublayers. We always alternate between attention and feedforward sublayers. The attention sublayers can be node-based (with temporary nodes features) or completely node-free. Using the respective sublayers leads to overall node-based or node-free GREAT. A visualization of the architecture can be found in fig. 5.\nNode-Based GREAT, Attention Sublayers: For each node in the graph, we compute a temporary node feature\n$x_i = \\sum_{j \\in N(i)} (\\alpha_{i,j}^\\text{in} W_1 e_{i,j} || \\alpha_{i,j}^\\text{out} W'_1 e_{j,i})$ (4)\nwith\n$\\alpha_{i,j}^\\text{out} = \\text{softmax}(\\frac{(W_2^q e_{i,j})^T W_2^k e_{i,j}}{\\sqrt{d}}), \\alpha_{i,j}^\\text{in} = \\text{softmax}(\\frac{(W'_2 e_{j,i})^T W'_2 e_{j,i}}{\\sqrt{d}})$ (5)\nNote that we compute two attention scores and concatenate the resulting values to form the temporary node feature. This allows GREAT to differentiate between incoming and outgoing edges which, e.g. in the case of asymmetric TSP, can have different values. If symmetric graphs are processed (where $e_{i,j} = e_{j,i}$ for all nodes i, j) we can simplify the expression to only one attention score."}, {"title": "EXPERIMENTS", "content": "We evaluate the performance of GREAT in two types of experiments. First, we train GREAT in a supervised fashion to predict optimal TSP edges. Secondly, we train GREAT in a reinforcement learning framework to construct TSP solutions incrementally directly. Our code was implemented in Python using PyTorch Paszke et al. (2019) and Pytorch Geometric Fey & Lenssen (2019). The code for our experiments, trained models, and test datasets will be publicly available after the paper is accepted."}, {"title": "LEARNING TO SPARSIFY", "content": "In this experiment, we demonstrate GREAT's capability in edge-classification tasks. In particular, we train the network to predict optimal TSP edges for Euclidean TSP of size 100. The predicted edges obtained from this network could later on be used in beam searches to create valid TSP solutions, or, alternately for TSP sparsification as done in Lischka et al. (2024). Therefore, we will evaluate the capability of the trained network for sparsifying TSP graphs and, while doing so, keeping optimal edges.\nThe hyperparameters in this setting are as follows. We train a node-based and a node-free version of GREAT. For both models, we choose 10 hidden layers. The hidden dimension is 64 and each attention layer has 4 attention heads. Training is performed for 200 epochs and there are 50,000 training instances in each epoch. Every 10 epochs, we change the dataset to a fresh set of 50,000 instances (meaning 200 \u00d7 50,000 = 10,000,000 instances in total). We used the Adam optimizer with a constant learning rate of 0.0001 and weighted cross-entropy as our loss function to account for the fact that there is an unequal number of optimal and non-optimal edges in a TSP graph. Targets for the optimal edges were generated using the LKH algorithm.\nThe evaluation of the trained networks is done on 100 instances. The performance of the network is benchmarked against the results of the \u201cclassical\" algorithms 1-Tree and k-nn used in the graph sparsification task of Lischka et al. (2024). The results are shown in table 1. For the \"classical\" sparsification methods, we can set a hyperparameter k specifying that the k most promising outgoing edges of each node in the graph are kept in the sparsified instance. This parameter k allows us to perfectly influence how many edges will be part of the sparse graph (k \u00d7 n, where n is the TSP size). We chose three different values of k, i.e., k = 3,5,10. For GREAT, there is no such a hyperparameter. We can, instead, set different thresholds for the probability p that the network predicts an edge to be part of the optimal solution. For this, we chose 0.00001, 0.5, and 0.999. We can see that choosing p = 0.999 results in a similar number of edges as k = 3. In this setting, the precision and recall of both GREAT versions are significantly better than the scores achieved by the \"classical\" algorithms. Here, by precision, we quantify the performance of only keeping edges that are indeed optimal. Recall refers to the ability of the approach to keep all optimal edges in the sparse graph. The result indicates that GREAT can produce very sparse graphs while missing relatively few optimal edges. Overall, however, we can see that for the classical algorithms, it is easier to just make the graphs less sparse and by this prevent deleting optimal edges. For GREAT, this is not possible, as lowering p further to increase recall leads to prohibitively low precision.\nOverall, we summarize that GREAT is a very powerful technique for creating extremely sparse TSP graphs while deleting only a small number of optimal edges. We hypothesize that creating such"}, {"title": "LEARNING TO SOLVE NON-EUCLIDEAN TSP", "content": "In this task, we train GREAT in a reinforcement learning framework to construct TSP solutions incrementally by adding one node at a time to a partial solution. Our framework follows the encoder- decoder approach (where GREAT serves as the encoder and a multi-pointer network as the decoder) and is trained using POMO Kwon et al. (2020). We focus on three different TSP variants, and by this aim to demonstrate GREAT's versatility to also apply to non-Euclidean TSP:\n1. Euclidean TSP where the coordinates are distributed uniformly at random in the unit square.\n2. Asymmetric TSP with triangle inequality (TMAT) as was used in Kwon et al. (2021). We use the code of Kwon et al. (2021) to generate instances. However, we normalize the distance matrices differently: Instead of a fixed scaling value, we normalize each instance individually such that the biggest distance is exactly 1. By this, we ensure that the distances use the full range of the interval (0,1) as well as possible.\n3. Extremely asymmetric TSP (XASY) where all pairwise distances are sampled uniformly at random from the interval (0,1). The same distribution was used in Gaile et al. (2022). Here, the triangle inequality does generally not hold.\nThe exact setting in this experiment is the following. For each distribution, we train three versions of GREAT. A node-based and a node-free network with hidden dimension 128 as well as a node-free network with hidden dimension 256. All networks have 5 hidden layers and 8 attention heads. Training is done for 400 epochs and there are 25,000 instances in each epoch. Again, every 10 epochs, we change the dataset to a fresh set of 25,000 instances (meaning 400 \u00d7 25,000 = 10, 000, 000 in- stances in total). We evaluate the model after each epoch and save the model with the best validation loss during these 400 epochs for testing. Furthermore, while training, the distances of all instances in the current data batch were multiplied by a factor in the range (0.5, 1.5) to ensure the models learn from a more robust data distribution. This allows us to augment the dataset at inference by a factor of \u00d78 like was done in Kwon et al. (2020). However, we want to note that while augment- ing the data by this factor at inference time improves performance, using even bigger augmentation factors like \u00d7128 in Kwon et al. (2021) does not lead to much better results (especially considering the enormous blowup in runtime). We suppose that this is due to our augmentation implementation having a disadvantage. While the augmentation method in, e.g., Kwon et al. (2020) which works by rotating coordinates, does not alter the underlying distribution much, our method by multiplying distances does change the distribution considerably. We note that instances multiplied with values close to 1 are favored in the end, compared to instances multiplied with values close to the borders 0.5 and 1.5.\nThe overall framework to construct solutions, as well as the decoder to decode the encodings pro- vided by GREAT and the loss formulation, are adapted from Jin et al. (2023b). We note that in this setting of incrementally building TSP solutions with an encoder-decoder approach, we would like to have node encodings as input for the decoder and not edge encodings like they are produced by GREAT. This is because we want to iteratively select the next node to visit, given a partial solution. As GREAT is generally used to compute edge encodings, all GREAT architectures in this experi- ment (node-free and node-based) have a final node-based layer where the results of the temporary node features (compare fig. 3) are returned instead of processing them further to obtain edge embed- dings again. By this, we can provide the decoder architecture with node encodings, despite having operated on edge-level during the internal operations of GREAT. A visualization of the framework can be found in fig. 6.\nIn the following, we provide an overview of the performance of our models in table 2 table 3 and table 4. Optimality gaps of our approaches are computed w.r.t. the optimal solver Gurobi Gurobi Optimization, LLC (2024). These (average) optimality gaps indicate how much worse the found solutions are in percent compared to the optimal solutions. When interpreting these results, we also point out the significant differences in the number of model parameters and the number of training instances."}, {"title": "CONCLUSION", "content": "In this work, we introduce GREAT, a novel GNN-related neural architecture for edge-based graph problems. While for previous GNN architectures it was necessary to transform graphs into their line graph representation to operate in purely edge-focused settings, GREAT can directly be ap- plied in such contexts. We evaluate GREAT in an edge-classification task to predict optimal TSP edges. In this task, GREAT is able to produce very sparse TSP graphs while deleting relatively few optimal edges compared to heuristic methods. Furthermore, we develop a GREAT-based RL framework to directly solve TSP. Compared to existing frameworks, GREAT offers the advantage of directly operating on the edge distances, overcoming the limitation of previous Transformer and GNN-based models that operate on node coordinates which essentially limits these architectures to Euclidean TSP. This limitation is rather disadvantageous in real-life settings, however, as distances (and especially other characteristics like time and energy consumption) are often asymmetric due to topography (e.g., elevation) or traffic congestion. GREAT achieves promising performance on several TSP variants (Euclidean, asymmetric with triangle inequality, and asymmetric without tri- angle inequality). We postpone it to future work to adapt GREAT to other routing problems such as CVRP (by translating node demands to edge demands). Furthermore, we aim to develop better data- augmentation methods for GREAT, allowing us to increase optimality at inference time by solving each instance multiple times. We further believe that GREAT could be useful in edge-regression tasks (e.g., in the setting of Hudson et al. (2021)) and, possibly, beyond routing problems."}]}