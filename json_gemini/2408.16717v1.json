{"title": "A GREAT ARCHITECTURE FOR EDGE-BASED GRAPH\nPROBLEMS LIKE TSP", "authors": ["Attila Lischka", "Jiaming Wu", "Morteza Haghir Chehreghani", "Bal\u00e1zs Kulcs\u00e1r"], "abstract": "In the last years, many neural network-based approaches have been proposed to\ntackle combinatorial optimization problems such as routing problems. Many of\nthese approaches are based on graph neural networks (GNNs) or related trans-\nformers, operating on the Euclidean coordinates representing the routing prob-\nlems. However, GNNs are inherently not well suited to operate on dense graphs,\nsuch as in routing problems. Furthermore, models operating on Euclidean coor-\ndinates cannot be applied to non-Euclidean versions of routing problems that are\noften found in real-world settings. To overcome these limitations, we propose\na novel GNN-related edge-based neural model called Graph Edge Attention Net-\nwork (GREAT). We evaluate the performance of GREAT in the edge classification\ntask to predict optimal edges in the Traveling Salesman Problem (TSP). We can\nuse such a trained GREAT model to produce sparse TSP graph instances, keep-\ning only the edges GREAT finds promising. Compared to other, non-learning-\nbased methods to sparsify TSP graphs, GREAT can produce very sparse graphs\nwhile keeping most of the optimal edges. Furthermore, we build a reinforce-\nment learning-based GREAT framework which we apply to Euclidean and non-\nEuclidean asymmetric TSP. This framework achieves state-of-the-art results.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph neural networks (GNNs) have emerged as a powerful tool for learning on graph-structured\ndata such as molecules, social networks, or citation graphs Wu et al. (2020). In recent years, GNNs\nhave also been applied in the setting of combinatorial optimization, especially routing problems\nJoshi et al. (2019); Hudson et al. (2021); Xin et al. (2021) since such problems can be interpreted\nas graph problems. However, the graph representations of routing problems, which are typically\ncomplete, dense graphs, are ill-suited for GNNs. This is because vanilla GNNs are not generally\nsuitable for learning on complete graphs. GNNs are related to the Weisfeiler Leman algorithm\nwhich is known to exploit graph structure Morris et al. (2019). Complete graphs feature no such\nstructure, resulting in poor GNN performance. Moreover, over-smoothing is a well-known problem\nhappening in (deep) GNNs which means that feature vectors computed for different nodes become\nmore and more similar with every layer Rusch et al. (2023). Naturally, in dense or even complete\ngraphs this problem is even more present as all nodes share the same information leading to similar\nencodings. Consequently, Lischka et al. (2024) showed that the performance of GNNs operating on\nrouting problems can be increased if graphs are made sparse in a preprocessing step. However, the\nproposed sparsification methods of Lischka et al. (2024) rely on hand-crafted heuristics which goes\nagainst the idea of data-driven, end-to-end machine learning frameworks.\nIn this paper, we overcome the limitations of regular GNNs by introducing the Graph Edge Attention\nNetwork (GREAT). This results in the following contributions:\n\u2022 Whereas traditional GNNs operate on a node-level by using node-based message pass-\ning operations, GREAT is edge-based, meaning information is passed along edges shar-\ning endpoints. This makes GREAT perfect for edge-level tasks such as routing problems\nwhere the edges to travel along are selected. We note, however, that the idea of GREAT is"}, {"title": null, "content": "task-independent and it can potentially also be applied in other suitable settings, possibly\nchemistry or road networks.\n\u2022 We evaluate GREAT in the task of edge classification, training the architecture to predict\noptimal edges in a Traveling Salesman Problem (TSP) tour in a supervised setting. By\nthis, GREAT can be used as a learning-based and data-driven sparsification method for\nrouting graphs. The produced sparse graphs are less likely to delete optimal edges than\nhand-crafted heuristics while being overall sparser.\n\u2022 We build a reinforcement learning framework that can be trained end-to-end to predict\noptimal TSP tours. As the inputs of GREAT are edge features (e.g., distances), GREAT\napplies to all variants of TSP, including non-Euclidean variants such as the asymmetric TSP.\nThe resulting trained framework achieves state-of-the-art performance for two asymmetric\nTSP distributions."}, {"title": "2 BASICS AND RELATED WORK", "content": null}, {"title": "2.1 GRAPH NEURAL NETWORKS", "content": "Graph neural networks are a class of neural architectures that operate on graph-structured data. In\ncontrast to other neural architectures like MLPs where the connections of the neurons are fixed and\ngrid-shaped, the connections in a GNN reflect the structure of the input data.\nIn essence, GNNs are a neural version of the well-known Weisfeiler-Leman (WL) graph isomor-\nphism heuristic Morris et al. (2019); Xu et al. (2019). In this heuristic, graph nodes are assigned\ncolors that are iteratively updated. Two nodes share the same color if they shared the same color in\nthe previous iteration and they had the same amount of neighbors of each color in the last iteration.\nWhen the test is applied to two graphs and the graphs do not have the same amount of nodes of\nsome color in some iteration, they are non-isomorphic. WL is only a heuristic, however, as there are\ncertain non-isomorphic graphs it can not distinguish. Examples are regular graphs (graphs where\nall nodes have the same degree) Kiefer (2020). We note that complete graphs (that we encounter in\nrouting problems) are regular graphs.\nGNNs follow a similar principle as the WL heuristic, but instead of colors, vector representations\nof the nodes are computed. GNNs iteratively compute these node feature vectors by aggregating\nover the node feature vectors of adjacent nodes and mapping the old feature vector together with\nthe aggregation to a new node feature vector. Additionally, the feature vectors are multiplied with\ntrainable weight matrices and non-linearities are applied to achieve actual learning. The node feature\nvectors of a neighborhood are typically scaled in some way (depending on the respective GNN\narchitecture) and sometimes, edge feature vectors are also considered within the aggregations. An\nexample can be found in fig. 1. Its mathematical formulation might look like this:\n$h_{u}^{l}=\\sigma\\left(W_{1} h_{u}^{l-1}+\\sum_{v \\in N(u)}\\left(W_{2} h_{v}^{l-1}+W_{3} e_{u v}\\right)\\right)$\nhere $W_1$, $W_2$, $W_3$ are trainable weight matrices of suitable sizes, $\\sigma$ is a non-linear activation func-\ntion, $h_{u}^{l}$ denotes the feature vector of a node $u$ in the $l$th update of the GNN and $e_{uv}$ denotes an edge\nfeature of the edge $(u, v)$ in the input graph. Sometimes, the edge features are also updated. The\nnode feature vectors of the last layer of the GNN can be used for node-level classification or regres-\nsion tasks. They can also be summarized (e.g. by aggregation) and used as a graph representation in\ngraph-level tasks. Referring back to the WL algorithm, we note how the node colors there can also\nbe considered as node classes. Furthermore, comparing the colors of different graphs to determine\nisomorphism can be considered a graph-level task. While GNNs are bounded in their expressiveness\nby the WL algorithm Morris et al. (2019); Xu et al. (2019) and can therefore not distinguish regular\ngraphs (e.g., complete graphs), we acknowledge that these limitations of GNNs can be mitigated\nby assigning unique \u201cnode identifiers\u201d (like unique node coordinates) to graphs passed to GNNs\n(Abboud et al. (2021)). However, over-smoothing (Rusch et al. (2023)) is still a problem, especially\nin dense graphs. This results in a need for better neural encoder architectures in settings such as the\nrouting problem."}, {"title": "2.2 ATTENTION-BASED GRAPH NEURAL NETWORKS", "content": "Graph Attention Networks (GATs Velickovic et al. (2017)) are a variety of GNNs. They leverage\nthe attention mechanism Vaswani et al. (2017) to determine how to scale the messages sent between\nthe network nodes. Overall, the node features are computed as follows:\n$x_{i}^{\\prime}=\\sum_{j \\in N(i) \\cup\\{i\\}} \\alpha_{i, j} \\Theta x_{j}$\nwhere $\\alpha_{i,j}$ is computed as\n$\\alpha_{i, j}=\\frac{\\exp \\left(\\operatorname{LeakyReLU}\\left(a^{\\top}\\Theta_{s} x_{i}+a^{\\top}\\Theta_{t} x_{j}+a^{\\top}\\Theta_{e} e_{i, j}\\right)\\right)}{\\sum_{k \\in N(i) \\cup\\{i\\}} \\exp \\left(\\operatorname{LeakyReLU}\\left(a^{\\top}\\Theta_{s} x_{i}+a^{\\top}\\Theta_{t} x_{k}+a^{\\top}\\Theta_{e} e_{i, k}\\right)\\right)}$\nand $\\Theta_{s}, \\Theta_{t}, \\Theta_{e}, a$ are learnable parameters. We note that GAT uses the edge features only\nto compute the attention scores but does not update them nor uses them in the actual message\npassing.\nVariations of the original GAT also use edge features in the message-passing operations. For ex-\nample, Chen & Chen (2021) propose Edge-Featured Graph Attention Networks (EGAT) which uses\nedge features by applying a GAT not only on the input graph itself but also its line graph represen-\ntation (compare Chen et al. (2017) as well) and then combining the computed features.\nAnother work incorporating edge features in an attention-based GNN is Shi et al. (2020) who use a\n\"Graph Transformer\u201d that incorporates edge and node features for a semi-supervised classification\ntask.\nJin et al. (2023a) introduce \u201cEdgeFormers\u201d an architecture operating on Textual-Edge Networks\nwhere they combine the success of Transformers in LLM tasks and GNNs. Their architecture also\naugments GNNs to utilize edge (text) features."}, {"title": "2.3 LEARNING TO ROUTE", "content": "In recent years, many studies have tried to solve routing problems such as the Traveling Salesmen\nProblem or the Capacitated Vehicle Routing Problem (CVRP).\nPopular approaches for solving routing problems with the help of machine learning include rein-\nforcement learning (RL) frameworks, where encodings for the problem instances are computed.\nThese encodings are then used to incrementally build solutions by selecting one node in the problem\ninstance at a time. Successful works in this category include Deudon et al. (2018); Nazari et al.\n(2018); Kool et al. (2019); Kwon et al. (2020); Jin et al. (2023b).\nAnother possibility to use machine learning for solving routing problems is to predict edge probabil-\nities or scores which are later used in search algorithms such as beam search or guided local search.\nExamples for such works are Joshi et al. (2019); Fu et al. (2021); Xin et al. (2021); Hudson et al.\n(2021); Kool et al. (2019); Min et al. (2024)."}, {"title": "2.3.1 \u039d\u039fN-EUCLIDEAN ROUTING PROBLEMS", "content": "Many of the mentioned works, especially in the first two categories, use GNNs or transformer mod-\nels (which are related to GNNs via GATs Joshi (2020)) to capture the structure of the routing problem\nin their neural architecture. This is done by interpreting the coordinates of Euclidean routing prob-\nlem instances as node features. These node features are then processed in the GNN or transformer\narchitectures to produce encodings of the overall problem. However, this limits the applicability of\nsuch works to Euclidean routing problems. This is unfortunate, as non-Euclidean routing problems\nare highly relevant in reality. Consider, e.g., one-way streets which result in unequal travel distances\nbetween two points depending on the direction one is going. Another example is variants of TSP\nthat consider energy consumption as the objective to be minimized. If point A is located at a higher\naltitude than point B, traveling from A to B might require less energy than the other way around.\nSo far, only a few studies have also investigated non-Euclidean versions of routing problems, such as\nthe asymmetric TSP (ATSP). One such study is Gaile et al. (2022) where they solve synthetic ATSP\ninstances with unsupervised learning, reinforcement learning, and supervised learning approaches.\nAnother study is Wang et al. (2023) that uses online reinforcement learning to solve ATSP instances\nof TSPLIB Reinelt (1991). Another successful work tackling ATSP is Kwon et al. (2021). There,\nthe Matrix Encoding Network (MatNet) is proposed, a neural model suitable to operate on matrix en-\ncodings representing combinatorial optimization problems such as the distance matrices of (A)TSP.\nTheir model is trained using RL."}, {"title": "3 GRAPH EDGE ATTENTION NETWORK", "content": "Existing GNNs are based on node-level message-passing operations, making them perfect for node-\nlevel tasks as is also underlined by their connection to the WL heuristic. In contrast, we propose\nan edge-level-based GNN where information is passed along neighboring edges. This makes our\nmodel perfect for edge-level tasks such as edge classification (e.g., in the context of routing prob-\nlems, determining if edges are \u201cpromising\u201d to be part of the optimal solution or not). Our model is\nattention-based, meaning the \u201cfocus\" of an edge to another, adjacent edge in the update operation is\ndetermined using the attention mechanism. Consequently, similar to the Graph Attention Network\n(GAT) we call our architecture Graph Edge Attention Network (GREAT). A simple visualization\nof the idea of GREAT is shown in fig. 2. In this visualization, edge e14 attends to all other edges\nit shares an endpoint with. While GREAT is a task-independent framework, it is suited perfectly\nfor routing problems: Consider TSP as an example. There, we do not have any node features, only\nedge features given as distances between nodes. A normal GNN would not be suitable to process\nsuch information well. Existing papers use coordinates of the nodes in the Euclidean space as node\nfeatures to overcome this limitation. However, this trick only works for Euclidean TSP and not other\nsymmetric or asymmetric TSP adaptations. GREAT, however, can be applied to all these variants.\nInstead of purely focusing on edge features and ignoring node features, it would also be possible to\ntransform the graph in its line graph and apply a GNN operating only on node features on this line\ngraph. In the line graph, each edge $e_{i,j}$ of the original graph is a node $n_{i,j}$ and two nodes $n_{i,j}, n_{k,m}$\nin the line graph are connected if the corresponding edges $e_{i,j}$ and $e_{k,m}$ in the original graph share\nan endpoint. However, in a TSP instance of n cities, there are $n^2$ many edges. Therefore, the line\ngraph of this instance would have $O(n^2)$ many nodes. Furthermore, as the original TSP graph is\ncomplete, each of the endpoints $\\{i, j\\}$ of an edge in the original graphs is part of n many edges.\nThis means that in the line graph, each node has 2n many connections to other nodes. In other\nwords, each of the $O(n^2)$ nodes has O(n) edges leading to $O(n^3)$ many edges in the line graph.\nThis implies that the line graph has one order of magnitude more edges and nodes than the original\ngraph.\nWe note that GREAT can be applied to extensions of the TSP such as the CVRP or TSP with time\nwindows (TSPTW) as well: even though capacities and time windows are node-level features, we"}, {"title": null, "content": "can easily transform them into edge features. Consider CVRP where a node $j$ has a demand $c_j$. We\ncan simply add demand $c_j$ to all edges $e_{i,j}$ in the graph. This is because we know that if we have an\nedge $e_{i,j}$ in the tour, we will visit node $j$ in the next step and therefore need a free capacity in our\nvehicle big enough to serve the demand of node $j$ which is $c_j$. An analogous extension works for\nTSPTW.\nWe further note that even though GREAT has been developed in the context of routing problems, it\ngenerally is a task-oblivious architecture and it might be useful in completely different domains as\nwell such as chemistry, road, or flow networks."}, {"title": "3.1 ARCHITECTURE", "content": "In the following, we provide the mathematical model defining the different layers of a GREAT\nmodel. In particular, we propose two versions of GREAT.\nThe first version is purely edge-focused and does not have any node features. Here, each edge\nexchanges information with all other edges it shares at least one endpoint with. The idea essentially\ncorresponds to the visualization in fig. 2. In the following, we refer to this variant as \u201cnode-free\"\nGREAT.\nThe second version is also edge-focused but has intermediate, temporary node features. This essen-\ntially means that nodes are used to save all information on adjacent edges. Afterward, the features\nof an edge are computed by combining the temporary node features of their respective endpoints.\nThese node features are then deleted and not passed on to the next layer, only the edge features are\npassed on. The idea of this GREAT variant is visualized in fig. 3 and fig. 4 In the remainder of this\nstudy, we refer to this GREAT version as \"node-based\"."}, {"title": "3.2 MATHEMATICAL FORMULATIONS", "content": "We now describe the mathematical formulas defining the internal operations of GREAT. We note\nthat, inspired by the original transformer architecture of Vaswani et al. (2017), GREAT consists of\ntwo types of sublayers: attention sublayers and feedforward sublayers. We always alternate between\nattention and feedforward sublayers. The attention sublayers can be node-based (with temporary\nnodes features) or completely node-free. Using the respective sublayers leads to overall node-based\nor node-free GREAT. A visualization of the architecture can be found in fig. 5.\nNode-Based GREAT, Attention Sublayers: For each node in the graph, we compute a temporary\nnode feature\n$x_{i}=\\sum_{j \\in N(i)} \\left(\\alpha_{i, j}^{\\prime} W_{1} e_{i, j} \\|\\alpha_{i, j}^{\\prime\\prime} W_{1}^{\\prime} e_{j, i}\\right)$\nwith\n$\\alpha_{i, j}^{\\prime}=\\operatorname{softmax}\\left(\\frac{\\left(W_{2} e_{i, j}\\right)^{\\top} W_{2} e_{i, j}}{\\sqrt{d}}\\right), \\quad \\alpha_{i, j}^{\\prime \\prime}=\\operatorname{softmax}\\left(\\frac{\\left(W_{2}^{\\prime} e_{j, i}\\right)^{\\top} W_{2}^{\\prime} e_{j, i}}{\\sqrt{d}}\\right)$\nNote that we compute two attention scores and concatenate the resulting values to form the tempo-\nrary node feature. This allows GREAT to differentiate between incoming and outgoing edges which,\ne.g. in the case of asymmetric TSP, can have different values. If symmetric graphs are processed\n(where $e_{i,j} = e_{j,i}$ for all nodes $i, j$) we can simplify the expression to only one attention score."}, {"title": null, "content": "The temporary node features are concatenated and mapped to the hidden dimension again to com-\npute the actual edge features of the layer.\n$e_{i, j}=W_{4}\\left(x_{i} \\|\\mid x_{j}\\right)$\nWe note that $W_1, W_1', W_2, W_2', W_1, W_1', W_1, W_1'$ are trainable weight matrices of suitable dimen-\nsion. $d$ is the hidden dimension and $\\|\\mid$ denotes concatenation. $W_1 e_{i,j}, W_1' e_{i,j}$ and $W_1 e_{i,j}$ correspond\nto the \"values\", \"keys\" and \u201cqueries\u201d of the original transformer architecture.\nNode-Free GREAT, Attention Sublayers: Here, edge features are computed directly as\n$e_{i, j}^{\\prime}=\\left(\\alpha_{i, j}^{\\prime} W_{1} e_{i, j} \\|\\alpha_{j, i}^{\\prime \\prime} W_{1}^{\\prime} e_{j, i} \\|\\alpha_{i, j} W_{1} e_{i, j} \\|\\alpha_{j, i}^{\\prime \\prime} W_{1}^{\\prime} e_{j, i}\\right)$\nNote that the edge feature consists of four individual terms that are concatenated. Due to the attention\nmechanism, these terms summarize information on all edges outgoing from node $i$, ingoing to node\n$i$, outgoing from node $j$, and ingoing to node $j$. The differentiation between in- and outgoing edges\nis again necessary due to asymmetric graphs. The $\\alpha'$ and $\\alpha''$ scores are computed as for the node-\nbased GREAT variant.\nFeedforward (FF) Subayer: Like in the original transformer architecture, the FF layer has the\nfollowing form.\n$e_{i, j}^{\\prime}=W_{2} \\operatorname{ReLU}\\left(W_{1} e_{i, j}+b_{1}\\right)+b_{2}$\nwhere $W_1, b_1, W_2, b_2$ are trainable weight matrices and biases of suitable sizes. Moreover, again like\nin Vaswani et al. (2017), the feedforward sublayers have internal up-projections, which temporarily\ndouble the hidden dimension before scaling it down to the original size.\nWe further note that we add residual layers and normalizations to each sublayer (Attention and FF).\nTherefore the output of each sublayer is (like in the original transformer architecture):\n$e_{i, j}^{\\prime}=\\operatorname{LayerNorm}\\left(e_{i, j}+\\operatorname{Sublayer}\\left(e_{i, j}\\right)\\right)$"}, {"title": "4 EXPERIMENTS", "content": "We evaluate the performance of GREAT in two types of experiments. First, we train GREAT in\na supervised fashion to predict optimal TSP edges. Secondly, we train GREAT in a reinforcement\nlearning framework to construct TSP solutions incrementally directly. Our code was implemented\nin Python using PyTorch Paszke et al. (2019) and Pytorch Geometric Fey & Lenssen (2019). The\ncode for our experiments, trained models, and test datasets will be publicly available after the paper\nis accepted."}, {"title": "4.1 LEARNING TO SPARSIFY", "content": "In this experiment, we demonstrate GREAT's capability in edge-classification tasks. In particular,\nwe train the network to predict optimal TSP edges for Euclidean TSP of size 100. The predicted\nedges obtained from this network could later on be used in beam searches to create valid TSP\nsolutions, or, alternately for TSP sparsification as done in Lischka et al. (2024). Therefore, we\nwill evaluate the capability of the trained network for sparsifying TSP graphs and, while doing so,\nkeeping optimal edges.\nThe hyperparameters in this setting are as follows. We train a node-based and a node-free version\nof GREAT. For both models, we choose 10 hidden layers. The hidden dimension is 64 and each\nattention layer has 4 attention heads. Training is performed for 200 epochs and there are 50,000\ntraining instances in each epoch. Every 10 epochs, we change the dataset to a fresh set of 50,000\ninstances (meaning $200 \\times 50,000 = 10,000,000$ instances in total). We used the Adam optimizer\nwith a constant learning rate of 0.0001 and weighted cross-entropy as our loss function to account\nfor the fact that there is an unequal number of optimal and non-optimal edges in a TSP graph. Targets\nfor the optimal edges were generated using the LKH algorithm.\nThe evaluation of the trained networks is done on 100 instances. The performance of the network\nis benchmarked against the results of the \u201cclassical\" algorithms 1-Tree and k-nn used in the graph\nsparsification task of Lischka et al. (2024). The results are shown in table 1. For the", "the": "lassical"}, {"title": "4.2 LEARNING TO SOLVE NON-EUCLIDEAN TSP", "content": "In this task, we train GREAT in a reinforcement learning framework to construct TSP solutions\nincrementally by adding one node at a time to a partial solution. Our framework follows the encoder-\ndecoder approach (where GREAT serves as the encoder and a multi-pointer network as the decoder)\nand is trained using POMO Kwon et al. (2020). We focus on three different TSP variants, and by\nthis aim to demonstrate GREAT's versatility to also apply to non-Euclidean TSP:\n1. Euclidean TSP where the coordinates are distributed uniformly at random in the unit square.\n2. Asymmetric TSP with triangle inequality (TMAT) as was used in Kwon et al. (2021). We\nuse the code of Kwon et al. (2021) to generate instances. However, we normalize the\ndistance matrices differently: Instead of a fixed scaling value, we normalize each instance\nindividually such that the biggest distance is exactly 1. By this, we ensure that the distances\nuse the full range of the interval (0,1) as well as possible.\n3. Extremely asymmetric TSP (XASY) where all pairwise distances are sampled uniformly\nat random from the interval (0,1). The same distribution was used in Gaile et al. (2022).\nHere, the triangle inequality does generally not hold.\nThe exact setting in this experiment is the following. For each distribution, we train three versions of\nGREAT. A node-based and a node-free network with hidden dimension 128 as well as a node-free\nnetwork with hidden dimension 256. All networks have 5 hidden layers and 8 attention heads. Train-\ning is done for 400 epochs and there are 25,000 instances in each epoch. Again, every 10 epochs,\nwe change the dataset to a fresh set of 25,000 instances (meaning $400 \\times 25,000 = 10,000,000$ in-\nstances in total). We evaluate the model after each epoch and save the model with the best validation\nloss during these 400 epochs for testing. Furthermore, while training, the distances of all instances\nin the current data batch were multiplied by a factor in the range (0.5, 1.5) to ensure the models\nlearn from a more robust data distribution. This allows us to augment the dataset at inference by a\nfactor of \u00d78 like was done in Kwon et al. (2020). However, we want to note that while augment-\ning the data by this factor at inference time improves performance, using even bigger augmentation\nfactors like \u00d7128 in Kwon et al. (2021) does not lead to much better results (especially considering\nthe enormous blowup in runtime). We suppose that this is due to our augmentation implementation\nhaving a disadvantage. While the augmentation method in, e.g., Kwon et al. (2020) which works\nby rotating coordinates, does not alter the underlying distribution much, our method by multiplying\ndistances does change the distribution considerably. We note that instances multiplied with values\nclose to 1 are favored in the end, compared to instances multiplied with values close to the borders\n0.5 and 1.5.\nThe overall framework to construct solutions, as well as the decoder to decode the encodings pro-\nvided by GREAT and the loss formulation, are adapted from Jin et al. (2023b). We note that in this\nsetting of incrementally building TSP solutions with an encoder-decoder approach, we would like\nto have node encodings as input for the decoder and not edge encodings like they are produced by\nGREAT. This is because we want to iteratively select the next node to visit, given a partial solution.\nAs GREAT is generally used to compute edge encodings, all GREAT architectures in this experi-\nment (node-free and node-based) have a final node-based layer where the results of the temporary\nnode features (compare fig. 3) are returned instead of processing them further to obtain edge embed-\ndings again. By this, we can provide the decoder architecture with node encodings, despite having\noperated on edge-level during the internal operations of GREAT. A visualization of the framework\ncan be found in fig. 6.\nIn the following, we provide an overview of the performance of our models in table 2 table 3 and\ntable 4. Optimality gaps of our approaches are computed w.r.t. the optimal solver Gurobi Gurobi\nOptimization, LLC (2024). These (average) optimality gaps indicate how much worse the found\nsolutions are in percent compared to the optimal solutions. When interpreting these results, we also\npoint out the significant differences in the number of model parameters and the number of training\ninstances."}, {"title": null, "content": "For Euclidean TSP, we observe that our model does not quite achieve the performance of existing\narchitectures. However, we note that MatNet, which operates on distance matrices, also seems\nto struggle, compared to the models operating on coordinates like the attention model (AM) with\nPOMO. MatNet still performs somewhat better than GREAT, however, we attribute this mainly to\nthe fact that MatNet has more parameters and, moreover, has been trained on a dataset more than 10\ntimes larger than ours. Within the different GREAT architectures, the node-free versions perform\nbetter than the node-based model. Furthermore, the GREAT with the most parameters, performs\nbest.\nThe TMAT distribution has several differences compared to the Euclidean distribution. Simple\nheuristics like nearest insertion (NI), farthest insertion (FI), and nearest neighbor (NN) perform\nconsiderably worse compared to the Euclidean case. Moreover, on TMAT, the only other avail-\nable neural solver is MatNet. We note that for MatNet different distances are reported because the\ndistances in the MatNet framework have been normalized differently (indicated with an asterisk).\nThe optimality gaps can still be compared, however, since both models (MatNet and GREAT) have\nbeen evaluated w.r.t. an optimal solver. We see that the node-free GREAT network with 1.26M\nparameters achieves better performance than MatNet (which has ~ 5 times more parameters and\nis trained on a dataset over 10 times larger) when no data augmentation is performed. MatNet has\nalso been evaluated with a \u00d7128 data augmentation which then leads to better results. However, the\nruntime of MatNet in this setting is considerably worse. Within the different GREAT versions, we\ncan see that the node-free versions perform considerably better than the node-based version. How-\never, the node-free model with only 1.26M parameters performs better than the model with 5.00M\nparameters.\nIn the extremely asymmetric distribution (XASY) case, we note that all simple heuristics (nearest\ninsertion, farthest insertion, and nearest neighbor) perform very poorly, achieving gaps of 185%\n310%. The node-based GREAT, however, achieves gaps of 21.51% (no augmentation) and 13.24%\n(\u00d78 augmentation). Node-free GREAT versions achieve gaps between 30% and 40% without aug-\nmentation, which is, contrary to the other distributions, worse than the node-based GREAT. No other\nneural solvers have been evaluated on this distribution with 100 nodes. However, Gaile et al. (2022)\ndeployed a neural model on the same distribution for instances of 50 cities. A small comparison\nbetween Gaile et al. (2022) and GREAT can be found in appendix A.\nOverall, we summarize that GREAT achieves state-of-the-art performance on the asymmetric TSP\ndistributions, despite often having fewer parameters than other architectures and being trained on\nsmaller datasets (we expect GREAT to have an even better performance when being trained on big"}]}