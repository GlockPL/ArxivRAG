{"title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision", "authors": ["Jay Shah", "Ganesh Bikshandi", "Ying Zhang", "Vijay Thakkar", "Pradeep Ramani", "Tri Dao"], "abstract": "Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FLASHATTENTION elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FLASHATTENTION-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FLASHATTENTION-3, achieves speedup on H100 GPUs by 1.5-2.0x with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FLASHATTENTION-3 achieves 2.6\u00d7 lower numerical error than a baseline FP8 attention.", "sections": [{"title": "Introduction", "content": "For the Transformer architecture [56], the attention mechanism constitutes the primary computational bottleneck, since computing the self-attention scores of queries and keys has quadratic scaling in the sequence length. Scaling attention to longer context will unlock new capabilities (modeling and reasoning over multiple long documents [24, 42, 49] and files in large codebases [30, 47]), new modalities (high-resolution images [11], audio [23], video [25]), and new applications (user interaction with long history [51], agent workflow with long horizon [59]). This has generated significant interest in making attention faster in the long-context regime, including by approximation [14, 27, 54] and software optimization ([17, 29, 44]), or even alternative architectures [22, 41, 53].\nIn this work, we build on the work of Dao et al. [17] on developing exact-attention algorithms that integrate knowledge of the GPU's execution model and hardware characteristics into their high-level design. In [17], Dao et al. introduced FLASHATTENTION, a novel tiling strategy for parallelizing attention that eliminates intermediate reads/writes to slow global memory through fusing all of the attention operations into a single GPU kernel. Dao [15] restructured the algorithm as FLASHATTENTION-2 to also parallelize over the sequence length dimension and perform the inner loop of the forward pass over blocks of the key and value matrices, thus improving the occupancy and distribution of work on the GPU. However, we observe that FLASHATTENTION-2 nonetheless achieves poor utilization on newer GPUs relative to optimized matrix-multiplication (GEMM) kernels, such as 35% vs. 80-90% on the Hopper H100 GPU. Partially, this may be attributed to implementation-level differences, such as not using Hopper-specific instructions in place of Ampere ones when targeting the Tensor Cores.\nMore fundamentally, FLASHATTENTION-2's algorithm adheres to a simplified synchronous model and makes no explicit use of asynchrony and low-precision in its design. Asynchrony is a result of hardware specialization to accelerate the most important operations in a ML workload: specific hardware units performing matrix multiplication"}, {"title": "Background: Multi-Head Attention and GPU Characteristics", "content": ""}, {"title": "Multi-Head Attention", "content": "Let $Q, K, V \\in \\mathbb{R}^{N \\times d}$ be the query, key and value input sequences associated to a single head, where $N$ is the sequence length and $d$ is the head dimension. Then the attention output $O$ is computed as:\n$S = aQK^T \\in \\mathbb{R}^{N \\times N}, \\quad P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}, \\quad O = PV \\in \\mathbb{R}^{N \\times d}$,\nwhere softmax is applied row-wise and one typically sets $a = 1/\\sqrt{d}$ as the scaling factor. In practice, we subtract $rowmax(S)$ from $S$ to prevent numerical instability with the exponential function. For multi-head attention (MHA),"}, {"title": "GPU hardware characteristics and execution model", "content": "We describe the aspects of the GPU's execution model relevant for FLASHATTENTION-3, with a focus on the NVIDIA Hopper architecture as a concrete instantiation of this model.\nMemory hierarchy: The GPU's memories are organized as a hierarchy of data locales, with capacity inversely related to bandwidth (Table 1). Global memory (GMEM), also known as HBM, is the off-chip DRAM accessible to all streaming multiprocessors (SMs). Data from GMEM gets transparently cached into an on-chip L2 cache. Next, each SM contains a small on-chip, programmer-managed highly banked cache called shared memory (SMEM). Lastly, there is the register file within each SM.\nThread hierarchy: The GPU's programming model is organized around logical groupings of execution units called threads. From the finest to coarsest level, the thread hierarchy is comprised of threads, warps (32 threads), warpgroups (4 contiguous warps), threadblocks (i.e., cooperative thread arrays or CTAs), threadblock clusters (in Hopper), and grids.\nThese two hierarchies are closely interlinked. Threads in the same CTA are co-scheduled on the same SM, and CTAs in the same cluster are co-scheduled on the same GPC. SMEM is directly addressable by all threads within a CTA, whereas each thread has at most 256 registers (RMEM) private to itself.\nAsynchrony and warp-specialization: GPUs are throughput processors that rely on concurrency and asynchrony to hide memory and execution latencies. For async memory copy between GMEM and SMEM, Hopper has the Tensor Memory Accelerator (TMA) as a dedicated hardware unit [38, \u00a77.29]. Furthermore, unlike prior architectures such as Ampere, the Tensor Core of Hopper, exposed via the warpgroup-wide WGMMA instruction [39, \u00a79.7.14], is also asynchronous and can source its inputs directly from shared memory.\nHardware support for asynchrony allows for warp-specialized kernels, where the warps of a CTA are divided into producer or consumer roles that only ever issue either data movement or computation. Generically, this improves the compiler's ability to generate optimal instruction schedules [4]. In addition, Hopper supports the dynamic"}, {"title": "FlashAttention-3: Algorithm", "content": "In this section, we describe the FLASHATTENTION-3 algorithm. For simplicity, we focus on the forward pass, with the backward pass algorithm described in Appendix B.1. We first indicate how to integrate warp-specialization with a circular SMEM buffer into the base algorithm of FLASHATTENTION-2. We then explain how to exploit asynchrony of WGMMA to define an overlapped GEMM-softmax 2-stage pipeline. Finally, we describe the modifications needed for FP8, both in terms of layout conformance and accuracy via block quantization and incoherent processing."}, {"title": "Producer-Consumer asynchrony through warp-specialization and pingpong scheduling", "content": "Warp-specialization As with FLASHATTENTION-2, the forward pass of FLASHATTENTION-3 is embarrassingly parallel in the batch size, number of heads, and query sequence length. Thus, it will suffice to give a CTA-level view of the algorithm, which operates on a tile $Q_i$ of the query matrix to compute the corresponding tile $O_i$ of the output. To simplify the description, we first give the warp-specialization scheme with a circular SMEM buffer that does not have in addition the GEMM-softmax overlapping. Let $d$ be the head dimension, $N$ the sequence length, and fix a query block size $B_r$, to divide $Q$ into $T_r = \\lceil \\frac{N}{B_r} \\rceil$ blocks $Q_1, \\dots, Q_{T_r}$."}, {"title": "Intra-warpgroup overlapping GEMMs and softmax", "content": "Even within one warpgroup, we can overlap some instructions in the softmax with some instructions in the GEMMs. We describe one technique to do so.\nIn the attention algorithm, operations within the inner loop (main loop) have sequential dependencies that impede parallelization within a single iteration. For example, (local) softmax (lines 18 to 19) relies on the output $S^{(j)}$ of the first GEMM, while the second GEMM takes its result $P^{(j)}$ as an operand. Indeed, the wait statements in lines 17 and 21 of Algorithm 1 serialize the execution of softmax and GEMMs. However, we can break these dependencies by pipelining across iterations through additional buffers in registers. Pursuing this idea, we propose the following two-stage6 GEMM-softmax pipelining algorithm:"}, {"title": "Low-precision with FP8", "content": "Efficiency: layout transformations. Computing the forward pass of FLASHATTENTION-3 in FP8 precision poses additional challenges not encountered for FP16 in terms of layout conformance."}, {"title": "Empirical Validation", "content": "We use the primitives from CUTLASS [55] such as WGMMA and TMA abstractions to implement FLASHATTENTION-3 and evaluate its efficiency and accuracy."}, {"title": "Benchmarking Attention", "content": "We measure the runtime of different attention methods on an H100 80GB SXM5 GPU for different settings (without / with causal mask, head dimension 64 or 128) for FP16 inputs. We report the results in Fig. 5 and Fig. 6, showing that FLASHATTENTION-3 is around 1.5-2.0\u00d7 faster than FLASHATTENTION-2 in the forward pass and 1.5-1.75\u00d7 faster in the backward pass. Compared to a standard attention implementation, FLASHATTENTION-3 can be up to 3-16\u00d7 faster. For medium and long sequences (1k and above), FLASHATTENTION-3 even surpasses the speed of a vendor's library (cuDNN \u2013 closed source) that has been optimized for H100 GPUs.\nBenchmark settings: We vary the sequence length as 512, 1k, ..., 16k, and set batch size so that the total number of tokens is 16k. We set the hidden dimension to 2048, and head dimension to be either 64, 128, or 256 (i.e., 32 heads, 16 heads, or 8 heads). To calculate the FLOPs of the forward pass, we use:\n$4 \\cdot \\text{seqlen}^2 \\cdot \\text{head dimension} \\cdot \\text{number of heads}$.\nWith causal masking, we divide this number by 2 to account for the fact that approximately only half of the entries are calculated. To get the FLOPs of the backward pass, we multiply the forward pass FLOPs by 2.5 (since there are 2 matmuls in the forward pass and 5 matmuls in the backward pass, due to recomputation).\nWe also measure the runtime for FP8 for the forward pass under similar settings. We report the results for headdim 256 in Fig. 7 and give the full results in Appendix C.2."}, {"title": "Ablation Study: 2-Stage Pipelining Experiments", "content": "We ablate both the 2-stage WGMMA-softmax pipelining and warp-specialization for non-causal FP16 FLASHATTENTION-3 with fixed parameters {batch, seqlen, nheads, hdim} = {4, 8448, 16, 128}. The result in Table 2 confirms that our algorithmic improvements (asynchrony with warp-specialization and overlapping between GEMM and softmax) lead to significant speedup, from 570 to 661 TFLOPS."}, {"title": "Numerical Error Validation", "content": "As there has been interest in the numerical error [21] of FLASHATTENTION, we compare FLASHATTENTION-2, FLASHATTENTION-3, and a standard implementation of attention against a reference implementation in FP64. To simulate outlier features and activations in LLMs [20, 52], we generate the entries of Q, K, V with the following"}, {"title": "Dicussion, Limitations, Conclusion", "content": "With FLASHATTENTION-3, we have demonstrated that new programming techniques and hardware features such as asynchrony and low-precision can have a dramatic impact on the efficiency and accuracy of attention. We are able to speed up attention by 1.5-2.0x times compared to FLASHATTENTION-2, and reduce FP8 numerical error by 2.6\u00d7 compared to standard per-tensor quantization. Some limitations of our work that we hope to address in the future include: optimizing for LLM inference, integrating a persistent kernel design into the FP8 kernel, 10 and understanding the effects of low-precision attention in large-scale training. Though we have focused on Hopper GPUs in this work, we expect that the techniques developed here will apply to other hardware accelerators. We hope that a faster and more accurate primitive such as attention will unlock new applications in long-context tasks."}, {"title": "Related Work", "content": "Attention variants and distributed attention Ever since attention became popular with the Transformer architecture [56], there has been a large body of work on approximating attention to scale it to longer sequences. These approximation methods can generally be categorized into two classes: sparse and low-rank. Sparse attention only computes some entries of the attention matrix (softmax(QK7)) and assumes that other entries are zero. Different methods have different ways of choosing which entries should be zero, either with a fixed pattern [12], with a sliding window [6], or with a dynamic pattern through hashing [28] or routing [46]. The low-rank approach instead assumes that the attention matrix has a low-rank structure, and apply a pointwise nonlinearity to the query and key [27] with random projection [13, 43, 58]. One can also combine the sparse and low-rank approximation for better quality [10, 60]. However, these approximation methods typically do not offer the same model quality as standard attention [54], and so most large-scale models do not employ these techniques.\nThere are other variants of attention aimed at reducing the size of the KV cache to improve inference efficiency. Multi-query attention [50] and grouped query attention [3] tie different heads of K and V, and multiple query heads interact with the same key and value head. Multi-head latent attention [19] parameterizes the K and V as low-rank projections of a shared matrix to further reduce the KV cache size. However, all of these approaches do not change the core computation softmax(QK7)V during training and simply change how Q, K, V are obtained. As a result, any efficiency or accuracy improvement to the standard attention computation benefits these methods.\nTo extend to even longer context, attention computation can be distributed across multiple GPUs. Methods such as Ring attention [31, 32] and variants [8] can reach a context length of up to 1 million. They use FLASHATTENTION (or FLASHATTENTION-2) as a primitive, and so the improvement from FLASHATTENTION-3 would benefit these distributed attention methods as well.\nAlternative architectures Motivated by the limitations of attention, a variety of alternative architectures have been proposed. They build on the connection between linear attention [27] and recurrent neural networks (RNNs). RWKV [41], H3 [18], MEGA [35], Retnet [53] enhance the expressivity of the simple cumulative sum in linear attention with more sophisticated recurrences. Mamba [22] and xLSTM [5] use learnable weighting for the recurrence and can match the quality of Transformers in language modeling at small or medium scale. These approaches can be connected to generalizations of linear attention through the lens of the structure of the token-mixing matrix [16]. These models have started to see some traction, seeing usage in some medium to large-scale models such as Jamba [2], Zamba [61], Megalodon [36], and Mamba2-hybrid [57]. For the highest quality, these SSM- and RNN-based models still employ many layers of attention. We expect that techniques to speed up attention presented in this work will be useful to speedup these alternative architectures.\nLow-precision attention Quantization is a promising approach to speed up attention, but they have mostly focused on reducing the space for KV cache for inference efficiency. QuIP [9] uses incoherent processing to reduce the quantization, and we adapted this technique for FP8 FLASHATTENTION-3. Recent work suggests that for inference the KV cache is highly compressible down to 4-, 3-, or even 2-bits [26, 33]. However, quantization during training is still challenging as higher precision is typically required for stable training.\nHardware-aware Algorithms Our work presented in this paper focuses on the micro-architecture specific tuning to leverage new instruction sets and adopt a natively asynchronous programming model. There are other orthogonal axes for hardware-aware algorithm co-design being explored. A recent example of this is LeanAttention [48], which recognizes the poor GPU occupancy and high memory bandwidth requirements of the sequential token generation phase as primary bottlenecks for inference and optimizes it via a smarter load balancing strategy similar to Stream-K load balancing [40] to achieve nearly peak occupancy. There is a large literature on optimizing GEMM for specific hardware that employs many of the same techniques. As an example, Abdelfattah et al. [1] presents a high performance batched GEMM kernel on K40c Graphics Processing Units (GPU) for both fixed and variable sizes, proposing specialized GEMM designs and a comprehensive autotuning process to deliver state-of-the-art performance."}, {"title": "Addition Details on Algorithms", "content": ""}, {"title": "Asynchrony Through Warp Specialization for the Backward Pass", "content": "Similar to the forward pass \u00a7 3.1, we use warp specialization to handle asynchrony. Instead of just a simple producer-consumer pattern in the forward pass, we add one extra role of a dQ writer, since we need to accumulate the value of dQ produced by each thread block to the global value of dQ. This dQ accumulation introduces memory contention (many thread blocks writing to the same location) so having a separate warp to handle this (along with asynchrony) will avoid blocking the rest of the warps in the thread block to perform the next computation (matmul). We include the backward pass with warp specialization in Algorithm 3."}, {"title": "2-Stage Pipelining SASS Analysis", "content": "We give simplified SASS code for the inside of the consumer warpgroup mainloop.\nWe make the following observations:\n1.  Softmax is reordered to the very beginning, even before the first WGMMA.\n2.  The first WGMMA is interleaved with softmax and FP32 \u2192 FP16 datatype conversion of S. This indicates that WGMMA and non-WGMMAs are executed in parallel.\n3.  exp2, row\\_sum, O rescaling and FP32 \u2192 FP16 conversions are interleaved together.\n4.  The second WGMMA is not overlapped with other instructions, as expected.\nOverall, SASS shows that the 2-stage pipelining idea works as expected."}, {"title": "3-Stage Pipelining Algorithm", "content": "We experiment with a 3-stage pipelining algorithm to parallelize the first WGMMA from iteration $j + 2$, softmax from iteration $j + 1$, and the second WGMMA from iteration $j$. We describe this algorithm in Algorithm 4. This algorithm behaves worse than the 2-stage pipelining algorithm due to the reasons below:\nOverlapping. We expected that softmax can be overlapped with (the first WGMMA + the second WGMMA). However, the compiler doesn't cooperate in this way. SASS code shows that only the first WGMMA is overlapped with softmax, while the second WGMMA is not. It's not clear why the compiler chooses to reorder instructions in this way."}, {"title": "Addition Details on Experiments and Benchmarking", "content": ""}, {"title": "System and libraries", "content": "We benchmark the speed on an H100 80GB SXM5 (700W). We generally use the latest versions of the libraries, at the time of writing (May 2024). Specifically, we use:\n\u2022 CUDA 12.3\n\u2022 cuDNN 9.1.1.17\n\u2022 CUTLASS 3.5\n\u2022 FLASHATTENTION 2.5.8\n\u2022 Triton nightly 3.0.0.post20240424212437\n\u2022 PyTorch 2.3.0\nTo reduce variability, we fix the GPU clock speed to 1830MHz (clock speed used to calculate the 989 TFLOPS FP16 theoretical max throughput). We repeat the benchmarks 100 times and take the average timing."}, {"title": "FP8 Attention Full Results", "content": "We use following sequence lengths: 512, 1024, 2048, 4224, 8448, 16896. When sequence length \u2265 4k, we make it also divisible by 132 (number of SMs in H100 SXM5) to avoid wave quantization."}]}