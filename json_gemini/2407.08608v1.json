{"title": "FlashAttention-3:\nFast and Accurate Attention with Asynchrony and Low-precision", "authors": ["Jay Shah", "Ganesh Bikshandi", "Ying Zhang", "Vijay Thakkar", "Pradeep Ramani", "Tri Dao"], "abstract": "Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language\nmodels and long-context applications. FLASHATTENTION elaborated an approach to speed up attention on GPUs\nthrough minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in\nrecent hardware, with FLASH ATTENTION-2 achieving only 35% utilization on the H100 GPU. We develop three\nmain techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA\nto (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise\nmatmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware\nsupport for FP8 low-precision. We demonstrate that our method, FLASHATTENTION-3, achieves speedup on\nH100 GPUs by 1.5-2.0x with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching\nclose to 1.2 PFLOPs/s. We validate that FP8 FLASHATTENTION-3 achieves 2.6\u00d7 lower numerical error than a\nbaseline FP8 attention.", "sections": [{"title": "Introduction", "content": "For the Transformer architecture [56], the attention mechanism constitutes the primary computational bottleneck,\nsince computing the self-attention scores of queries and keys has quadratic scaling in the sequence length. Scaling\nattention to longer context will unlock new capabilities (modeling and reasoning over multiple long documents [24,\n42, 49] and files in large codebases [30, 47]), new modalities (high-resolution images [11], audio [23], video [25]), and\nnew applications (user interaction with long history [51], agent workflow with long horizon [59]). This has generated\nsignificant interest in making attention faster in the long-context regime, including by approximation [14, 27, 54]\nand software optimization ([17, 29, 44]), or even alternative architectures [22, 41, 53].\nIn this work, we build on the work of Dao et al. [17] on developing exact-attention algorithms that integrate\nknowledge of the GPU's execution model and hardware characteristics into their high-level design. In [17], Dao et\nal. introduced FLASHATTENTION, a novel tiling strategy for parallelizing attention that eliminates intermediate\nreads/writes to slow global memory through fusing all of the attention operations into a single GPU kernel. Dao\n[15] restructured the algorithm as FLASHATTENTION-2 to also parallelize over the sequence length dimension and\nperform the inner loop of the forward pass over blocks of the key and value matrices, thus improving the occupancy\nand distribution of work on the GPU. However, we observe that FLASHATTENTION-2 nonetheless achieves poor\nutilization on newer GPUs relative to optimized matrix-multiplication (GEMM) kernels, such as 35% vs. 80-90% on\nthe Hopper H100 GPU. Partially, this may be attributed to implementation-level differences, such as not using\nHopper-specific instructions in place of Ampere ones when targeting the Tensor Cores.\nMore fundamentally, FLASHATTENTION-2's algorithm adheres to a simplified synchronous model and makes\nno explicit use of asynchrony and low-precision in its design. Asynchrony is a result of hardware specialization to\naccelerate the most important operations in a ML workload: specific hardware units performing matrix multiplication"}, {"title": "Background: Multi-Head Attention and GPU Characteristics", "content": "Let $Q, K, V \\in \\mathbb{R}^{N\\times d}$ be the query, key and value input sequences associated to a single head, where $N$ is the sequence\nlength and $d$ is the head dimension. Then the attention output $O$ is computed as:\n$S = \\alpha QK^T \\in \\mathbb{R}^{N \\times N}, \\quad P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}, \\quad O = PV \\in \\mathbb{R}^{N \\times d}$,\nwhere softmax is applied row-wise and one typically sets $\\alpha = 1/\\sqrt{d}$ as the scaling factor. In practice, we subtract\n$\\text{rowmax}(S)$ from $S$ to prevent numerical instability with the exponential function. For multi-head attention (MHA),"}, {"title": "GPU hardware characteristics and execution model", "content": "We describe the aspects of the GPU's execution model relevant for FLASHATTENTION-3, with a focus on the\nNVIDIA Hopper architecture as a concrete instantiation of this model.\nMemory hierarchy: The GPU's memories are organized as a hierarchy of data locales, with capacity inversely\nrelated to bandwidth. Global memory (GMEM), also known as HBM, is the off-chip DRAM accessible\nto all streaming multiprocessors (SMs). Data from GMEM gets transparently cached into an on-chip L2 cache.\nNext, each SM contains a small on-chip, programmer-managed highly banked cache called shared memory (SMEM).\nLastly, there is the register file within each SM.\nThread hierarchy: The GPU's programming model is organized around logical groupings of execution units\ncalled threads. From the finest to coarsest level, the thread hierarchy is comprised of threads, warps (32 threads),\nwarpgroups (4 contiguous warps), threadblocks (i.e., cooperative thread arrays or CTAs), threadblock clusters (in\nHopper), and grids.\nThese two hierarchies are closely interlinked. Threads in the same CTA are co-scheduled on the same SM, and\nCTAs in the same cluster are co-scheduled on the same GPC. SMEM is directly addressable by all threads within a\nCTA, whereas each thread has at most 256 registers (RMEM) private to itself.\nAsynchrony and warp-specialization: GPUs are throughput processors that rely on concurrency and asynchrony\nto hide memory and execution latencies. For async memory copy between GMEM and SMEM, Hopper has the\nTensor Memory Accelerator (TMA) as a dedicated hardware unit [38, \u00a77.29]. Furthermore, unlike prior architectures\nsuch as Ampere, the Tensor Core of Hopper, exposed via the warpgroup-wide WGMMA instruction [39, \u00a79.7.14], is\nalso asynchronous and can source its inputs directly from shared memory.\nHardware support for asynchrony allows for warp-specialized kernels, where the warps of a CTA are divided into\nproducer or consumer roles that only ever issue either data movement or computation. Generically, this improves\nthe compiler's ability to generate optimal instruction schedules [4]. In addition, Hopper supports the dynamic"}, {"title": "FlashAttention-3: Algorithm", "content": "In this section, we describe the FLASHATTENTION-3 algorithm. For simplicity, we focus on the forward pass, with\nthe backward pass algorithm described in Appendix B.1. We first indicate how to integrate warp-specialization with\na circular SMEM buffer into the base algorithm of FLASH ATTENTION-2. We then explain how to exploit asynchrony\nof WGMMA to define an overlapped GEMM-softmax 2-stage pipeline. Finally, we describe the modifications needed\nfor FP8, both in terms of layout conformance and accuracy via block quantization and incoherent processing."}, {"title": "Producer-Consumer asynchrony through warp-specialization and pingpong schedul-ing", "content": "Warp-specialization As with FLASHATTENTION-2, the forward pass of FLASHATTENTION-3 is embarrassingly\nparallel in the batch size, number of heads, and query sequence length. Thus, it will suffice to give a CTA-level\nview of the algorithm, which operates on a tile $Q_i$ of the query matrix to compute the corresponding tile $O_i$ of the\noutput. To simplify the description, we first give the warp-specialization scheme with a circular SMEM buffer that\ndoes not have in addition the GEMM-softmax overlapping. Let $d$ be the head dimension, $N$ the sequence length,\nand fix a query block size $B_r$, to divide $Q$ into $T_r = \\lceil\\frac{N}{B_r}\\rceil$ blocks $Q_1, \\dots, Q_{T_r}$."}, {"title": "Intra-warpgroup overlapping GEMMs and softmax", "content": "Even within one warpgroup, we can overlap some instructions in the softmax with some instructions in the GEMMs.\nWe describe one technique to do so.\nIn the attention algorithm, operations within the inner loop (main loop) have sequential dependencies that\nimped parallelization within a single iteration. For example, (local) softmax (lines 18 to 19) relies on the output\n$S^{(j)}$ of the first GEMM, while the second GEMM takes its result $P^{(j)}$ as an operand. Indeed, the wait statements\nin lines 17 and 21 of Algorithm 1 serialize the execution of softmax and GEMMs. However, we can break these\ndependencies by pipelining across iterations through additional buffers in registers. Pursuing this idea, we propose\nthe following two-stage GEMM-softmax pipelining algorithm:"}, {"title": "Low-precision with FP8", "content": "Efficiency: layout transformations. Computing the forward pass of FLASHATTENTION-3 in FP8 precision poses\nadditional challenges not encountered for FP16 in terms of layout conformance."}, {"title": "Empirical Validation", "content": "We use the primitives from CUTLASS [55] such as WGMMA and TMA abstractions to implement FLASHATTENTION-\n3 and evaluate its efficiency and accuracy."}, {"title": "Benchmarking Attention", "content": "We measure the runtime of different attention methods on an H100 80GB SXM5 GPU for different settings (without\n/ with causal mask, head dimension 64 or 128) for FP16 inputs. We report the results in Fig. 5 and Fig. 6, showing\nthat FLASHATTENTION-3 is around 1.5-2.0\u00d7 faster than FLASHATTENTION-2 in the forward pass and 1.5-1.75\u00d7\nfaster in the backward pass. Compared to a standard attention implementation, FLASHATTENTION-3 can be up to\n3-16\u00d7 faster. For medium and long sequences (1k and above), FLASHATTENTION-3 even surpasses the speed of a\nvendor's library (cuDNN \u2013 closed source) that has been optimized for H100 GPUs.\nBenchmark settings: We vary the sequence length as 512, ..., 16k, and set batch size so that the total\nnumber of tokens is 16k. We set the hidden dimension to 2048, and head dimension to be either 64, 128, or 256 (i.e.,\n32 heads, 16 heads, or 8 heads). To calculate the FLOPs of the forward pass, we use:\n$4 \\cdot \\text{seqlen}^2 \\cdot \\text{head dimension} \\cdot \\text{number of heads}$.\nWith causal masking, we divide this number by 2 to account for the fact that approximately only half of the entries\nare calculated. To get the FLOPs of the backward pass, we multiply the forward pass FLOPs by 2.5 (since there\nare 2 matmuls in the forward pass and 5 matmuls in the backward pass, due to recomputation).\nWe also measure the runtime for FP8 for the forward pass under similar settings. We report the results for\nheaddim 256 in Fig. 7 and give the full results in Appendix C.2."}, {"title": "Ablation Study: 2-Stage Pipelining Experiments", "content": "We ablate both the 2-stage WGMMA-softmax pipelining and warp-specialization for non-causal FP16 FLASHATTENTION-\n3 with fixed parameters {batch, seqlen, nheads, hdim} = {4, 8448, 16, 128}. The result in Table 2 confirms that our\nalgorithmic improvements (asynchrony with warp-specialization and overlapping between GEMM and softmax) lead\nto significant speedup, from 570 to 661 TFLOPS."}, {"title": "Numerical Error Validation", "content": "As there has been interest in the numerical error [21] of FLASHATTENTION, we compare FLASHATTENTION-2,\nFLASHATTENTION-3, and a standard implementation of attention against a reference implementation in FP64. To\nsimulate outlier features and activations in LLMs [20, 52], we generate the entries of $Q, K, V$ with the following"}, {"title": "Dicussion, Limitations, Conclusion", "content": "With FLASHATTENTION-3, we have demonstrated that new programming techniques and hardware features such\nas asynchrony and low-precision can have a dramatic impact on the efficiency and accuracy of attention. We are\nable to speed up attention by 1.5-2.0x times compared to FLASHATTENTION-2, and reduce FP8 numerical error\nby 2.6\u00d7 compared to standard per-tensor quantization. Some limitations of our work that we hope to address in\nthe future include: optimizing for LLM inference, integrating a persistent kernel design into the FP8 kernel, and\nunderstanding the effects of low-precision attention in large-scale training. Though we have focused on Hopper\nGPUs in this work, we expect that the techniques developed here will apply to other hardware accelerators. We\nhope that a faster and more accurate primitive such as attention will unlock new applications in long-context tasks."}, {"title": "Related Work", "content": "Attention variants and distributed attention Ever since attention became popular with the Transformer\narchitecture [56], there has been a large body of work on approximating attention to scale it to longer sequences.\nThese approximation methods can generally be categorized into two classes: sparse and low-rank. Sparse attention\nonly computes some entries of the attention matrix (softmax($QK^T$)) and assumes that other entries are zero.\nDifferent methods have different ways of choosing which entries should be zero, either with a fixed pattern [12],\nwith a sliding window [6], or with a dynamic pattern through hashing [28] or routing [46]. The low-rank approach\ninstead assumes that the attention matrix has a low-rank structure, and apply a pointwise nonlinearity to the query\nand key [27] with random projection [13, 43, 58]. One can also combine the sparse and low-rank approximation for\nbetter quality [10, 60]. However, these approximation methods typically do not offer the same model quality as\nstandard attention [54], and so most large-scale models do not employ these techniques.\nThere are other variants of attention aimed at reducing the size of the KV cache to improve inference efficiency.\nMulti-query attention [50] and grouped query attention [3] tie different heads of K and V, and multiple query heads\ninteract with the same key and value head. Multi-head latent attention [19] parameterizes the K and V as low-rank\nprojections of a shared matrix to further reduce the KV cache size. However, all of these approaches do not change\nthe core computation softmax($QK^T$)V during training and simply change how Q, K, V are obtained. As a result,\nany efficiency or accuracy improvement to the standard attention computation benefits these methods.\nTo extend to even longer context, attention computation can be distributed across multiple GPUs. Methods such\nas Ring attention [31, 32] and variants [8] can reach a context length of up to 1 million. They use FLASHATTENTION\n(or FLASHATTENTION-2) as a primitive, and so the improvement from FLASHATTENTION-3 would benefit these\ndistributed attention methods as well.\nAlternative architectures Motivated by the limitations of attention, a variety of alternative architectures have\nbeen proposed. They build on the connection between linear attention [27] and recurrent neural networks (RNNs).\nRWKV [41], H3 [18], MEGA [35], Retnet [53] enhance the expressivity of the simple cumulative sum in linear\nattention with more sophisticated recurrences. Mamba [22] and xLSTM [5] use learnable weighting for the recurrence\nand can match the quality of Transformers in language modeling at small or medium scale. These approaches can\nbe connected to generalizations of linear attention through the lens of the structure of the token-mixing matrix [16].\nThese models have started to see some traction, seeing usage in some medium to large-scale models such as Jamba [2],\nZamba [61], Megalodon [36], and Mamba2-hybrid [57]. For the highest quality, these SSM- and RNN-based models\nstill employ many layers of attention. We expect that techniques to speed up attention presented in this work will\nbe useful to speedup these alternative architectures.\nLow-precision attention Quantization is a promising approach to speed up attention, but they have mostly\nfocused on reducing the space for KV cache for inference efficiency. QuIP [9] uses incoherent processing to reduce the\nquantization, and we adapted this technique for FP8 FLASHATTENTION-3. Recent work suggests that for inference\nthe KV cache is highly compressible down to 4-, 3-, or even 2-bits [26, 33]. However, quantization during training is\nstill challenging as higher precision is typically required for stable training.\nHardware-aware Algorithms Our work presented in this paper focuses on the micro-architecture specific tuning\nto leverage new instruction sets and adopt a natively asynchronous programming model. There are other orthogonal\naxes for hardware-aware algorithm co-design being explored. A recent example of this is LeanAttention [48],\nwhich recognizes the poor GPU occupancy and high memory bandwidth requirements of the sequential token\ngeneration phase as primary bottlenecks for inference and optimizes it via a smarter load balancing strategy similar\nto Stream-K load balancing [40] to achieve nearly peak occupancy. There is a large literature on optimizing GEMM\nfor specific hardware that employs many of the same techniques. As an example, Abdelfattah et al. [1] presents a\nhigh performance batched GEMM kernel on K40c Graphics Processing Units (GPU) for both fixed and variable\nsizes, proposing specialized GEMM designs and a comprehensive autotuning process to deliver state-of-the-art\nperformance."}, {"title": "Addition Details on Algorithms", "content": null}, {"title": "Asynchrony Through Warp Specialization for the Backward Pass", "content": "Similar to the forward pass \u00a7 3.1, we use warp specialization to handle asynchrony. Instead of just a simple\nproducer-consumer pattern in the forward pass, we add one extra role of a dQ writer, since we need to accumulate\nthe value of dQ produced by each thread block to the global value of dQ. This dQ accumulation introduces memory\ncontention (many thread blocks writing to the same location) so having a separate warp to handle this (along with\nasynchrony) will avoid blocking the rest of the warps in the thread block to perform the next computation (matmul).\nWe include the backward pass with warp specialization in Algorithm 3."}, {"title": "2-Stage Pipelining SASS Analysis", "content": "We give simplified SASS code for the inside of the consumer warpgroup mainloop.\nWe make the following observations:\n1. Softmax is reordered to the very beginning, even before the first WGMMA.\n2. The first WGMMA is interleaved with softmax and FP32 \u2192 FP16 datatype conversion of S. This indicates\nthat WGMMA and non-WGMMAs are executed in parallel.\n3. exp2, row_sum, O rescaling and FP32 \u2192 FP16 conversions are interleaved together.\n4. The second WGMMA is not overlapped with other instructions, as expected.\nOverall, SASS shows that the 2-stage pipelining idea works as expected."}, {"title": "3-Stage Pipelining Algorithm", "content": "We experiment with a 3-stage pipelining algorithm to parallelize the first WGMMA from iteration j + 2, softmax\nfrom iteration j + 1, and the second WGMMA from iteration j. We describe this algorithm in Algorithm 4. This\nalgorithm behaves worse than the 2-stage pipelining algorithm due to the reasons below:\nOverlapping. We expected that softmax can be overlapped with (the first WGMMA + the second WGMMA).\nHowever, the compiler doesn't cooperate in this way. SASS code shows that only the first WGMMA is overlapped\nwith softmax, while the second WGMMA is not. It's not clear why the compiler chooses to reorder instructions in\nthis way."}, {"title": "Addition Details on Experiments and Benchmarking", "content": null}, {"title": "System and libraries", "content": "We benchmark the speed on an H100 80GB SXM5 (700W). We generally use the latest versions of the libraries, at\nthe time of writing (May 2024). Specifically, we use:\n\u2022 CUDA 12.3\n\u2022 cuDNN 9.1.1.17\n\u2022 CUTLASS 3.5\n\u2022 FLASHATTENTION 2.5.8\n\u2022 Triton nightly 3.0.0.post20240424212437\n\u2022 PyTorch 2.3.0\nTo reduce variability, we fix the GPU clock speed to 1830MHz (clock speed used to calculate the 989 TFLOPS\nFP16 theoretical max throughput). We repeat the benchmarks 100 times and take the average timing."}, {"title": "FP8 Attention Full Results", "content": "We use following sequence lengths: 512, 1024, 2048, 4224, 8448, 16896. When sequence length \u2265 4k, we make it also\ndivisible by 132 (number of SMs in H100 SXM5) to avoid wave quantization."}]}