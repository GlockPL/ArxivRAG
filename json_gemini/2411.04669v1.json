{"title": "EffiCANet: Efficient Time Series Forecasting with Convolutional Attention", "authors": ["Xinxing Zhou", "Jiaqi Ye", "Shubao Zhao", "Ming Jin", "Chengyi Yang", "Yanlong Wen", "Xiaojie Yuan"], "abstract": "The exponential growth of multivariate time series data from sensor networks in domains like industrial monitoring and smart cities requires efficient and accurate forecasting models. Current deep learning methods often fail to adequately capture long-range dependencies and complex inter-variable relationships, especially under real-time processing constraints. These limitations arise as many models are optimized for either short-term forecasting with limited receptive fields or long-term accuracy at the cost of efficiency. Additionally, dynamic and intricate interactions between variables in real-world data further complicate modeling efforts. To address these limitations, we propose EffiCANet, an Efficient Convolutional Attention Network designed to enhance forecasting accuracy while maintaining computational efficiency. EffiCANet integrates three key components: (1) a Temporal Large-kernel Decomposed Convolution (TLDC) module that captures long-term temporal dependencies while reducing computational overhead; (2) an Inter-Variable Group Convolution (IVGC) module that captures complex and evolving relationships among variables; and (3) a Global Temporal-Variable Attention (GTVA) mechanism that prioritizes critical temporal and inter-variable features. Extensive evaluations across nine benchmark datasets show that EffiCANet achieves the maximum reduction of 10.02% in MAE over state-of-the-art models, while cutting computational costs by 26.2% relative to conventional large-kernel convolution methods, thanks to its efficient decomposition strategy.", "sections": [{"title": "1 INTRODUCTION", "content": "As we advance into an era dominated by AI-driven systems and ubiquitous Internet of Things (IoT), time series data has become a cornerstone of modern industries [4, 35, 49]. From intelligent manufacturing and smart cities to precision medicine, vast sensor networks continuously generate complex multivariate data streams, driving real-time insights for proactive monitoring, resource scheduling, and predictive maintenance [10, 33, 44, 48]. This data surge underscores the need for advanced time series forecasting methods to enable data-driven decision-making and enhance operational intelligence across diverse emerging applications.\nTraditional time series forecasting techniques, including Transformer-based methods [31, 36, 52, 53], have demonstrated strong performance across various domains. However, while they excel at capturing long-term dependencies, they often struggle to balance computational efficiency with real-time processing in resource-constrained environments. In contrast, temporal convolutional networks (TCNs) [19] are more efficient but suffer from limited receptive fields, restricting their ability to capture long-term patterns. Linear models [21, 46], on the other hand, offer simplicity and computational efficiency but lack the capacity to capture complex, non-linear dependencies. These trade-offs reveal a crucial gap in current research: there is a critical need for models that can balance efficiency with accurate forecasting over short and long horizons, especially for real-time decision-making. Bridging this gap is crucial for transforming the potential of IoT and AI-driven systems into actionable, timely, and reliable insights that optimize operations, enhance resource allocation, and foster innovation in sectors such as smart cities, precision manufacturing, and health-care [5, 32, 37].\nIn real-world forecasting, time series data rarely exhibit independence. Interactions among variables are often complicated by asynchrony and lag effects. For example, Figure 1(a) shows three variables with asynchronous behavior due to measurement discrepancies, manifested as slight random shifts at specific timestamps. A common example can be found in climate monitoring, where temperature and humidity readings from different sensors may not align due to sensor calibration errors and environmental conditions, leading to misunderstandings regarding underlying environmental interactions. Besides, Figure 1(b) illustrates a lead-lag relationship, where one variable is influenced by the historical values of another. This scenario is often observed in supply chain management, where inventory levels may respond to changes in demand with a noticeable delay. Ignoring these relationships can cause models to miss important latent connections, reducing their ability to capture the dynamics of complex systems. Addressing these challenges is crucial for developing robust and reliable forecasting models that accurately reflect real-world behaviors.\nIn this paper, we study the problem of multivariate time series forecasting, focusing on efficiently capturing both short- and long-term dependencies, as well as the dynamic interactions between variables. However, existing approaches [31, 43, 50, 51] often struggle to balance accuracy, computational efficiency, and the complexity of inter-variable dynamics. Specifically, we identify two primary challenges in current methods that hinder their effectiveness in complex, real-time forecasting tasks.\nChallenge 1: Efficiently capturing long-term dependencies. While mainstream architectures like Transformers are highly effective at learning long-term dependencies, their high computational cost limits their feasibility in real-time or resource-constrained scenarios. In contrast, convolutional models, such as TCNs, offer computationally efficient local pattern recognition but suffer from limited receptive fields, making long-term dependency capture challenging. Expanding the convolutional receptive field through large kernels [27] partially alleviates this issue but increases computational complexity. The challenge lies in expanding the receptive field efficiently, enabling both short- and long-term forecasting without compromising speed or scalability.\nChallenge 2: Modeling dynamic inter-variable dependencies with asynchronous and lagged relationships. In multivariate time series forecasting, variable dependencies are often complex and evolve dynamically. These relationships are further complicated by issues like sensor noise, time lags, and the heterogeneous nature of the variables. Existing models frequently emphasize either temporal dependencies or inter-variable relationships in isolation, failing to account for asynchronous or lagged interactions. This limitation leads to an incomplete understanding of variable interdependencies, especially when variables influence one another with time offsets. Developing methods that dynamically model these evolving relationships is crucial for accurate and robust forecasting.\nTo address these challenges, we propose EffiCANet, an Efficient Convolutional Attention Network designed to capture both short- and long-term dependencies, while dynamically modeling complex inter-variable relationships.\nAddressing Challenge 1: To overcome the limitations of convolutional models in capturing long-term dependencies efficiently, we introduce the Temporal Large-kernel Decomposed Convolution (TLDC) module. This module innovatively decomposes large kernels into a series of smaller, computationally efficient convolutions, significantly expanding the receptive field without a proportional increase in computational cost. Combined with an efficiently designed global temporal attention mechanism in the Global Temporal-Variable Attention (GTVA) module that adaptively highlights relevant time steps, our model proficiently captures both short- and long-term dependencies. This hybrid approach of convolution and attention effectively mitigates the computational bottlenecks of traditional large-kernel methods, making the model highly suitable for scenarios requiring low-latency predictions.\nAddressing Challenge 2: To tackle the complexities of modeling dynamic inter-variable relationships, we present the Inter-Variable Group Convolution (IVGC) module. This module leverages group convolutions [28] to capture inter-variable interactions over adjacent time windows, organizing variable-time window pairs into shared kernel groups for efficient processing. By sharing kernels across these groups, the model adapts to evolving dependencies. Additionally, a global variable attention mechanism in GTVA module assigns adaptive weights to each variable, dynamically responding to shifts in variable importance. This combined approach not only enhances the model's ability to learn intricate relationships in multivariate time series data but also ensures efficient computation, making it well-suited for real-time applications.\nIn summary, the key contributions of this paper are as follows:\n\u2022 We identify the challenges of capturing long-term temporal dependencies and complex inter-variable relationships in resource-constrained environments and propose EffiCANet, a novel convolutional attention network designed to address these issues.\n\u2022 EffiCANet incorporates a large-kernel decomposed convolution module for efficient long-term temporal dependency modeling, a group convolution module for dynamic inter-variable dependency capture, and a global attention mechanism to enhance temporal and variable feature extraction."}, {"title": "2 RELATED WORK", "content": "2.1 Convolution-based time series forecasting\nConvolutional neural networks (CNNs) have long been utilized in time series forecasting due to their ability to capture local temporal patterns efficiently. Despite the rise of Transformer-based models, CNNs remain relevant for their computational efficiency and adaptability [13, 26, 30, 38, 40, 41].\nConvolutional techniques have evolved substantially over time. For instance, MLCNN [2] utilizes convolutional layers to generate multi-level representations for near and distant future predictions. SCINet [23] employs a recursive downsample-convolve-interact architecture, using multiple convolutional filters to extract temporal features from downsampled sequences. MICN [39] integrates multi-scale down-sampled and isometric convolutions to capture both local and global temporal patterns. TimesNet [42] transforms 1D time series into 2D tensors and applies inception blocks with 2D convolutional kernels to model period variations. Recently, the large-kernel convolution [6] and separable convolution [14], originating from computer vision, have proven effective for time series analysis. ModernTCN [27] extends the receptive field of traditional TCN architectures to enhance long-term dependency modeling, while ConvTimeNet [3] leverages depthwise and pointwise convolutions to model both global sequence and cross-variable dependencies.\nWhile large kernels or deep architectures improve long-term dependency capture, they often lead to excessive parameter growth, computational overhead, and increased memory consumption. EffiCANet addresses these issues by using large-kernel decomposed convolutions, efficiently modeling long-term dependencies without sacrificing computational efficiency.\n2.2 Attention-based time series forecasting\nAttention mechanisms, originally developed for tasks such as machine translation, have gained widespread adoption in time series forecasting. Their strength lies in dynamically focusing on relevant temporal patterns, enabling the modeling of both short-term and long-term dependencies.\n2.2.1 Transformer Models. Transformer-based architectures [36] have become foundational in attention-based time series forecasting due to their proficiency in capturing long-range dependencies. Early Transformer models struggled with scalability in time series applications, prompting the development of specialized variants [34]. LogTrans [20] introduces sparse attention to reduce computational overhead, while Informer [52] incorporates ProbSparse attention for enhanced scalability. Autoformer [43] and FEDformer [53] adopt decomposition-based methods to separate trend and seasonal components. Pyraformer [24] leverages hierarchical structures to balance efficiency and accuracy. Crossformer [47] and PatchTST [31] employs cross-dimensional attention and patch-based approaches to improve multivariate time series modeling. Recently, iTransformer [25] inverts the standard Transformer architecture to enhance multivariate correlation modeling by attending to variate tokens rather than temporal ones. Pathformer [1] adopts multi-scale modeling with adaptive pathways to capture temporal dynamics at various resolutions, while SAMformer [16] addresses overfitting issues through sharpness-aware optimization. TSLANet [7] further refines attention mechanisms by incorporating adaptive spectral and interactive convolution blocks to better manage noise and multi-scale dependencies.\n2.2.2 Other Attention-Based Methods. Beyond the Transformer paradigm, other attention-based models have also shown promise in time series forecasting. These models combine attention mechanisms with various techniques to enhance the extraction of temporal patterns. MH-TAL [8] integrates temporal attention within RNNs to discover hidden patterns. RGDAN [9] incorporates a graph diffusion attention module for learning spatial dependencies and a temporal attention module to capture time-related correlations. FMamba [29] merges the selective state space model of Mamba [11] with fast-attention techniques, optimizing temporal feature extraction while maintaining linear computational complexity.\nDespite their success, attention-based models face challenges such as high computational cost and reliance on positional encodings, which may not fully capture complex temporal dynamics. EffiCANet addresses these limitations by integrating a more efficient attention mechanism that dynamically captures both temporal and inter-variable dependencies.\n2.3 Linear models for time series forecasting\nRecently, linear models have re-emerged as powerful tools for time series forecasting due to their simplicity, interpretability, and effectiveness in capturing linear dependencies. Models such as DLinear and NLinear [46] first show that straightforward linear approaches can rival and occasionally surpass complex Transformer architectures in long-term forecasting. [21] explore the impact of reversible normalization (RevIN) [17] and channel independence (CI) on linear models, and propose RLinear, which consists of RevIN and a single linear layer that maps the input to the output time series. FITS [45] applies a real discrete Fourier transform (RFT) and optional high-frequency filtering in the frequency domain.\nHowever, linear models are inherently limited in their ability to capture the complex non-linear dependencies present in multivariate datasets. While they perform well on low-dimensional datasets, their performance deteriorates significantly on datasets with a larger number of variables, highlighting the need for more well-designed models like EffiCANet that can effectively capture both linear and non-linear dependencies."}, {"title": "3 METHODOLOGY", "content": "3.1 Problem Definition\nWe consider a multivariate time series $X \\in \\mathbb{R}^{M \\times T}$, where $M$ represents the number of variables and $T$ denotes the length of the time series. Each element $X_m \\in \\mathbb{R}$ corresponds to the value of the m-th variable at time step t. The sequence $X_m \\in \\mathbb{R}^T$ represents the full time series for the m-th variable. Given a time interval $\\Delta t$, the subsequence $X_{t_0:t_0+\\Delta t} \\in \\mathbb{R}^{M\\times \\Delta t}$ refers to the segment of all variables between time steps $t_0$ and $t_0 + \\Delta t$.\nOur goal is to define a predictive model $F$ that, given a historical window of the time series, can forecast the values for future time steps. Specifically, the model $F$ takes a historical time window $X_{t_0-H:t_0}$, where $H$ is the window length, and predicts the next $\\tau$ time steps, denoted as $\\hat{Y}_{t_0:t_0+\\tau}$. The relationship can be formalized as:\n$\\hat{Y}_{t_0:t_0+\\tau} = F(X_{t_0-H:t_0}; \\Theta) \\qquad(1)$\nwhere $\\Theta$ denotes the parameters of the model $F$.\n3.2 Model Overview\nEffiCANet is designed to efficiently capture both temporal dependencies and inter-variable dynamics in multivariate time series forecasting. As shown in Figure 2, the model processes input data through a sequence of stacked blocks, each integrating large-kernel convolutions, group convolutions, and global attention mechanisms to capture complex relationships across both temporal and variable dimensions.\nGiven the multivariate time series input $X_{in} = X_{t_0-H:t_0} \\in \\mathbb{R}^{M \\times H}$, we adopt a patching and embedding strategy inspired by [27]. Initially, the input is reshaped via an unsqueeze operation into $X_{in} \\in \\mathbb{R}^{M \\times 1 \\times H}$, introducing a channel dimension that enables independent processing of each variable. A convolutional stem layer then partitions the sequence into patches using a kernel size of $P$ and a stride of $S$, producing $N = \\lfloor \\frac{H-P}{S} \\rfloor + 2$ overlapping patches. This convolution maps the single input channel to $D$ output channels, resulting in an embedded tensor $X_{emb} \\in \\mathbb{R}^{M \\times D \\times N}$, where $N$ represents the number of patches.\nThe model extracts features through a series of $L$ stacked blocks, each designed to iteratively refine the feature representations by capturing both localized and global temporal-variable patterns. Formally, the output of the $l$-th block, denoted as $Z^{(l)}$, is computed as:\n$Z^{(l)} = f_{Block}(Z^{(l-1)}) \\qquad (2)$\nwhere $Z^{(0)} = X_{emb}$, and $f_{Block}()$ denotes the operations within the l-th block.\nAfter processing through all $L$ blocks, the refined feature representation is passed to generate the final forecast $\\hat{Y}_{t_0:t_0+\\tau} \\in \\mathbb{R}^{M \\times \\tau}$, predicting the values for the future time steps.\n3.3 Temporal Large-kernel Decomposed Convolution\nThe Temporal Large-kernel Decomposed Convolution (TLDC) module is designed to capture both short- and long-range temporal features in multivariate time series through an efficient decomposition of large-kernel convolutions. Instead of directly applying computationally expensive large kernels, TLDC approximates their impact using a sequence of smaller, more efficient convolutions. This approach significantly reduces the computational overhead while preserving the ability to model extensive temporal patterns.\nGiven the input tensor $X_{emb}$ in shape $(M, D, N)$, it is first reshaped to $(M\\times D, N)$ to facilitate group convolutions. This grouping structure divides the input into $M \\times D$ separate groups, each corresponding to a unique variable-channel pair, as illustrated in Figure 3. By processing temporal dependencies independently within each group, this configuration allows for parallel computation while preserving the distinct characteristics of each variable.\nTo approximate a large convolution kernel of size $K$, the module applies a two-stage hierarchical convolutional process. First, a Depth-Wise Convolution (DW Conv) [14] with a kernel size of $(2d - 1)$ is applied, capturing short-term dependencies within a local temporal window:\n$X_{local} = DW \\ Conv(X_{emb}) \\qquad (3)$\nThis operation focuses on adjacent temporal points, enabling the capture of localized patterns while maintaining computational efficiency. The depth-wise approach ensures that each variable-channel pair is processed individually, preserving feature-specific information.\nTo extend the receptive field, a Depth-Wise Dilated Convolution (DW-D Conv) [12] is subsequently applied to $X_{local}$. Using a kernel size of $\\lfloor \\frac{K}{d} \\rfloor$ and dilation rate $d$, this convolution broadens the temporal scope, enabling the model to capture long-range dependencies by sampling across wider intervals:\n$X_{dilated} = DW-D \\ Conv (X_{local}) \\qquad (4)$\nTo incorporate both local and distant temporal features, the outputs of the DW Conv and DW-D Conv are combined via element-wise addition:\n$X_{combined} = X_{dilated} + X_{local} \\qquad (5)$\nThis summation enables the module to capture information across varying temporal scales, effectively balancing the complexity of large-kernel operations with the ability to extract detailed and broader temporal patterns.\nThe complexity reduction achieved by the TLDC can be quantified by comparing its parameter count and computational cost in terms of floating-point operations per second (FLOPs), to that of a direct large-kernel convolution. Typically, a single convolution with kernel size $K$ requires:\n$Params_{standard} = (M \\times D) \\times (K + 1) \\qquad (6)$\n$FLOPs_{standard} = 2 \\times M \\times D \\times K \\times N \\qquad (7)$\nIn contrast, the two-stage TLDC requires:\n$Params_{TLDC} = M \\times D \\times (2d + 1 + \\lfloor \\frac{K}{d} \\rfloor + 1) \\qquad (8)$\n$FLOPs_{TLDC} = 2 \\times M \\times D \\times (2d-1+1) \\times N \\qquad (9)$\nThe reduction in complexity can be approximated by the ratios:\n$\\frac{Params_{TLDC}}{Params_{standard}} = \\frac{2d+1+\\lfloor\\frac{K}{d}\\rfloor}{K+1} \\qquad (10)$\n$\\frac{FLOPs_{TLDC}}{FLOPs_{standard}} = \\frac{2d - 1 + \\lfloor \\frac{K}{d} \\rfloor}{K} \\qquad (11)$\nWhen $d$ is moderately smaller than $K$, these ratios simplify to approximately:\n$\\frac{Params_{TLDC}}{Params_{standard}} \\sim \\frac{2d}{K+d}, \\frac{FLOPs_{TLDC}}{FLOPs_{standard}} \\sim \\frac{2d}{Kd} \\qquad (12)$\nWhen $\\frac{K}{d}$ is negligible for $d \\ll K$, the dominant term in both cases is $O(1/d)$. This indicates that the TLDC reduces parameter and computational complexity by a factor inversely proportional to the dilation rate $d$. For instance, with $K = 55$ and $d = 5$, the parameter reduction factor is approximately 0.39, and the FLOPs reduction factor is around 0.36, leading to a 61% decrease in parameters and a 64% decrease in FLOPs.\nThrough this decomposition of large kernels, the TLDC module effectively captures long-range temporal dependencies while significantly reducing computational cost, enabling efficient temporal feature extraction. This approach is particularly advantageous for data-intensive time series analysis tasks, as it maintains model complexity within practical limits while delivering strong predictive performance and adaptability.\n3.4 Inter-Variable Group Convolution\nThe Inter-Variable Group Convolution (IVGC) module is designed to efficiently model dependencies among variables in multivariate time series data. By convolving across structured groups of temporal patches, this module captures intricate inter-variable interactions, particularly in scenarios where variables exhibit asynchrony or subtle lags. The group convolution framework focus on local temporal patterns and enhances computational efficiency, making it suitable for data with complex temporal dynamics and variable dependencies.\nThe input tensor $X_{combined}$ in size $(M, D, N)$ is first reshaped into a matrix of size $(N \\times M, D)$. To enable group convolutions over multiple time windows, the temporal dimension $N$ is padded to be divisible by the predefined window size $W$. This padding ensures the group convolution to operate on non-overlapping time windows, with the number of groups defined as $\\frac{N_{padded}}{W}$, where $N_{padded}$ represents the padded temporal length. Each group spans $W$ consecutive time points for all variables, allowing a shared convolutional kernel across the entire window. This configuration captures dynamic inter-variable dependencies within localized windows while preserving computational efficiency.\nTo ensure consistent alignment of temporal windows across groups, we calculate the necessary padding length, denoted as $N_{pad1}$, as follows:\n$N_{pad1} =\\begin{cases}  0, & \\text{if } N = 0 \\pmod{W} \\\\   W - (N \\mod W), & \\text{otherwise} \\end{cases} \\qquad (13)$\nThis formula adjusts the temporal dimension to the nearest multiple of $W$, aligning the time windows for all groups.\nTo capture a diverse range of dynamic patterns, an additional head-tail padding strategy is applied. This involves shifting the window segmentation by adding padding of $\\lfloor \\frac{W}{2} \\rfloor$ to the start of the temporal dimension and $W - \\lfloor \\frac{W}{2} \\rfloor$ to the end. This configuration slightly offsets the time windows, expanding the convolutional receptive field and allowing the model to capture variable relationships across varying temporal shifts. The additional padding lengths are defined as:\n$N_{left\\_pad2} = \\lfloor \\frac{W}{2} \\rfloor \\newline N_{right\\_pad2} = W - \\lfloor \\frac{W}{2} \\rfloor + N_{pad1} \\qquad (14)$\nAfter applying both padding strategies, we obtain two versions of the input tensor: $X_{padded1}$ for standard padding and $X_{padded2}$ for head-tail padding. Each tensor undergoes a 1-dimensional group convolution along the channel dimension with a kernel size of 1, aggregating inter-variable information within each time window while preserving localized dependencies.\nThe group convolution for each padded tensor is formulated as follows:\n$Y_{padded1} = Conv(X_{padded1}) \\newline Y_{padded2} = Conv(X_{padded2}) \\qquad (15)$\nThese convolutions are performed separately to capture both standard and shifted temporal patterns. The outputs, $Y_{padded1}$ and $Y_{padded2}$, are then aligned by removing any extra channels resulting from padding.\nSubsequently, the outputs from both convolution paths are merged and processed by a final convolution to further refine the extracted temporal and inter-variable interactions:\n$Y = Conv(Y_{padded1} + Y_{padded2}) \\qquad (16)$\nFinally, the output tensor $Y$ is reshaped back to its original dimensions $(M, D, N)$, restoring its compatibility with subsequent model layers.\nThe IVGC module is based on the principle that variables are more likely to exhibit strong interactions within close temporal proximity, rather than over distant time points. By segmenting the data into localized time windows and applying group convolutions, the IVGC captures dynamic inter-variable dependencies that evolve over time, adapting efficiently to these temporal shifts. This design enhances the module's focus on relevant, time-varying relationships while minimizing computational costs by constraining the convolutional operations within compact groups, making it both scalable and effective for complex multivariate time series analysis.\n3.5 Global Temporal-Variable Attention\nThe Global Temporal-Variable Attention (GTVA) mechanism leverages the Squeeze-and-Excitation (SE) principle [15] to selectively enhance feature learning across temporal and variable dimensions in multivariate time series. By decoupling the SE operation into dual-path attention one focused on temporal dependencies and the other on inter - variable relationships - GTVA extends beyond standard SE applications. This design complements the convolutional layers' local feature extraction by integrating broader temporal and inter-variable contexts, which is critical for effectively modeling both long-range dependencies and complex variable interactions.\nGiven the input tensor $Y \\in \\mathbb{R}^{M \\times D \\times N}$, the GTVA module independently generates attention weights for the temporal and variable dimensions. This dual-path approach preserves the unique characteristics of each dimension, avoiding the conflation that may arise from a single-path attention structure. By emphasizing channel-sensitive attention, the mechanism refines feature learning across these axes, further augmenting the model's ability to capture intricate temporal and inter-variable relationships.\nTemporal Attention To capture global temporal dependencies, the input tensor is reshaped to $Y_{temp} \\in \\mathbb{R}^{(N \\times D) \\times M}$. Global average pooling is then applied along the variable dimension to distill a temporal-centric representation:\n$T_{pool} = AvgPool(Y_{temp}) \\in \\mathbb{R}^{(N \\times D)} \\qquad (17)$\nThis representation is then processed through a two-layer fully connected (FC) network with a reduction ratio r, where r is set to balance computational cost and representation capability:\n$T_{atten} = \\sigma (W_2 \\cdot ReLU(W_1 \\cdot T_{pool})) \\qquad (18)$\nHere, $W_1 \\in \\mathbb{R}^{(N \\times D) \\times \\frac{N \\times D}{r}}$ and $W_2 \\in \\mathbb{R}^{\\frac{N \\times D}{r} \\times (N \\times D)}$ are weight matrices, and $\\sigma$ denotes the sigmoid activation function. The resulting attention weights $T_{atten}$, reshaped back to $(1, D, N)$, provide a temporal attention mask that selectively enhances or suppresses features along the time axis, emphasizing relevant temporal patterns in the input tensor.\nVariable Attention In parallel, variable attention captures inter-variable dependencies. The tensor is reshaped to $Y_{var} \\in \\mathbb{R}^{(M \\times D) \\times N}$, and global average pooling over the temporal axis produces a variable-centric representation:\n$V_{pool} = AvgPool(Y_{var}) \\in \\mathbb{R}^{(M \\times D)} \\qquad (19)$\nThis representation is similarly processed through a FC network:\n$V_{atten} = \\sigma (W_4 \\cdot ReLU(W_3 \\cdot V_{pool})) \\qquad (20)$\nwhere $W_3 \\in \\mathbb{R}^{(M \\times D) \\times \\frac{M \\times D}{r}}$ and $W_4 \\in \\mathbb{R}^{\\frac{M \\times D}{r} \\times (M \\times D)}$. The attention weights $V_{atten}$, reshaped to $(M, D, 1)$, allow the model to adaptively emphasize or suppress features based on cross-variable dependencies.\nThe dual attention weights are then combined with the convolution output using the Hadamard product, applying both temporal and variable attention simultaneously:\n$Y_{out} = Y \\odot (T_{atten} \\cdot V_{atten}) \\qquad (21)$\nwhere $\\odot$ denotes the element-wise Hadamard product. This fusion allows the model to emphasize time-sensitive and variable-relevant features concurrently, effectively capturing long-term dependencies and variable-specific interactions.\nFeedback Integration To further refine feature representations, the GTVA output $Y_{out}$ is incorporated with the block's input via element-wise multiplication:\n$X_{emb} = Y_{out} \\otimes X_{emb} \\qquad (22)$\nHere, $X_{emb}$ is the input of the next Block. This design introduces a feedback mechanism that allows each block's refined output to dynamically interact with its input, fostering progressive feature enhancement across layers. Unlike conventional residual connections, this feedback strategy cumulatively recalibrates feature representations, continually amplifying relevant information throughout the network, which is particularly beneficial for long-horizon forecasting.\nThe GTVA module extends traditional SE mechanisms by explicitly considering channel-level interactions within the context of both temporal and variable dimensions. Building on prior convolutions, this design refines forecasting through global temporal and variable attention, enabling precise adjustments at a comprehensive level to capture fine-grained dependencies across the entire time series."}, {"title": "3.6 Summary", "content": "Algorithm algorithm 1 outlines the training process of EffiCANet, which incorporates three primary modules within each of the L blocks: TLDC, IVGC, and GTVA. First, the input data X undergoes patch embedding and is initialized as Z(0) (lines 4-5). In each block, the temporal features are captured using DW Conv and DW-D Conv in the TLDC module (lines 7-10), while inter-variable dependencies are extracted in the IVGC module through group convolutions with a specified time window size W (lines 11-13). The GTVA module then applies temporal-variable attention pooling, producing an adaptive output that adjusts to both temporal and variable dependencies (lines 14-18). This sequence concludes with an aggregation step, and the final representation Z(L) is passed to the prediction head for output generation (lines 20). The model parameters are iteratively optimized until convergence by minimizing the prediction loss L (lines 21-22)."}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Setup\n4.1.1 Datasets and Evaluation Metrics. We evaluate EffiCANet's performance across nine widely-adopted multivariate time series datasets.\n\u2022 ETT: The Electricity Transformer Temperature (ETT) dataset [52] contains transformer oil temperature data, including four subsets ETTm1, ETTm2, ETTh1, and ETTh2, each representing different temporal granularities and historical spans.\n\u2022 Electricity: The Electricity dataset [43] consists of hourly electricity consumption data from 321 households.\n\u2022 Weather: The Weather dataset [43] includes meteorological variables such as temperature, humidity, wind speed, and pressure, collected from multiple weather stations.\n\u2022 Traffic: The Traffic dataset [43] contains hourly occupancy rates of roadways in the San Francisco Bay Area.\n\u2022 Exchange: The Exchange dataset [18] records daily exchange rates of eight major world currencies.\n\u2022 ILI: The Influenza-like Illness (ILI) dataset [43] tracks the weekly percentage of doctor visits for influenza-like illnesses in the United States.\nTable 1 provides a detailed overview of each dataset, including the number of timesteps, features, and sampling frequency.\nFor evaluation, we use Mean Squared Error (MSE) and Mean Absolute Error (MAE) as performance metrics, where lower values reflect better forecasting accuracy.\n4.1.2 Baselines. We evaluate our model against a range of baselines across three categories: convolution-based, Transformer-based, and MLP-based models.\n\u2022 Convolution-based Baselines. ConvTimeNet [3] utilizes depthwise and pointwise convolutions to capture both global sequence and cross-variable dependence. ModernTCN [27] modernizes the traditional TCN by expanding the receptive field and offers a pure convolution structure for time series analysis. TimesNet [42] focuses on multi-scale temporal feature extraction through specialized convolutions, while MICN [39] integrates information across various time scales for improved prediction accuracy.\n\u2022 Transformer-based Baselines. PatchTST [31] introduces a patching mechanism with channel-independence, and Crossformer [47] applies cross-dimensional attention to capture both temporal and inter-variable dependencies.\n\u2022 MLP-based Baselines. MTS-mixer [22] replaces attention mechanisms with factorized MLP modules to separately model temporal and feature dependencies. DLinear [46], RLinear, RMLP [21] demonstrate the effectiveness of simple linear layers and MLP structures in long-term forecasting.\n4.1.3 Implementation Details. The experiments were implemented on an NVIDIA GeForce RTX 4090 GPU. We used the Adam optimizer with learning rates ranging from 1e-4 to 1e\u22122, adjusting for each dataset. The batch sizes were set to 32, 128, 256, or 512 depending on the dataset size. The model consisted of 1 to 3 blocks, with training running up to 100 epochs and early stopping triggered if validation loss stagnated for 20 epochs. A channel dimension size of 64 was employed consistently throughout all experiments. The simulated temporal large-kernel convolution size was set to 55 with a dilation rate of 5. Input sequence lengths were optimized per dataset, with values of 128, 336, 512, 720, and 960. Baselines followed the original configurations to ensure fair comparisons.\n4.2 Main Results\nIn this study, we evaluated EffiCANet against a comprehensive set of baseline models across nine datasets, using prediction lengths of 24, 36, 48, 60 for the ILI dataset, and 96, 192, 336, 720 for the others. Each model was evaluated under optimized conditions, ensuring fair and accurate comparisons across different methodologies in time series forecasting.\nEffiCANet consistently delivered strong results, as summarized in Table 2. Across 72 experimental evaluations, it ranked first in 51 cases and second in 13, demonstrating superior performance in most scenarios. For instance, on the ETTh2 dataset, EffiCANet achieved a 4.7% reduction in MSE and a 2.53% reduction in MAE compared to the second-best model. Similarly, on the ILI dataset, it achieved a 10.02% reduction in MSE and a 3.34% reduction in MAE. These consistent gains underscore the model's robustness and adaptability.\nAnalyzing the baseline models, convolution-based approaches like ModernTCN excel on datasets with shorter sequences or periodic patterns, such as Exchange and ILI, where local temporal dependencies dominate. However, they struggle with longer sequences due to limited capacity in capturing extended dependencies. Transformer-based models like PatchTST perform well on datasets with more variables or longer sequences, such as Traffic and Electricity, owing to their capability to model intricate interactions and long-term dependencies, but may underperform with irregular or shorter time series. MLP-based models, like RLinear, show efficiency on simpler datasets with fewer variables, such as the ETT series. However, they tend to fall short when dealing with complex temporal patterns and inter-variable relationships, particularly in datasets with a high number of variables, such as Traffic.\nEffiCANet outperforms these models by effectively capturing both long-range dependencies and short-term dynamics through large-kernel decomposed convolutions. The inter-variable group convolution further optimizes variable interactions over close time intervals, while global temporal-variable attention dynamically focuses on relevant features. This holistic design allows EffiCANet to excel in scenarios requiring nuanced analysis of both temporal and cross-variable relationships."}, {"title": "4.3 Ablation Study", "content": "To evaluate the contributions of key components in our model", "datasets": "ETTh1", "aspects": "the effect of removing temporal and variable-specific modules and a comparison between large-kernel decomposed convolution (TLDC) and standard large-kernel convolution (LKC). Results are averaged across four prediction horizons for a comprehensive assessment.\nIn Table 3, we analyze the impact of removing key modules. Replacing the temporal-specific components, including the TLDC and global temporal attention, with standard convolution (w/o TD) results in a noticeable performance drop across all datasets, underscoring the importance of capturing temporal dependencies. Similarly, when the variable-specific components, including the IVGC and global variable attention, are replaced by standard convolution (w/o VD), the model's performance degrades, particularly on"}]}