{"title": "SUPER MONOTONIC ALIGNMENT SEARCH", "authors": ["Junhyeok Lee", "Hyeongju Kim"], "abstract": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most popular algorithm in TTS to estimate unknown alignments between text and speech. Since this algorithm needs to search for the most probable alignment with dynamic programming by caching all paths, the time complexity of the algorithm is O(T \u00d7 S). The authors of Glow-TTS run this algorithm on CPU, and while they mentioned it is difficult to parallelize, we found that MAS can be parallelized in text-length dimension and CPU execution consumes an inordinate amount of time for inter-device copy. Therefore, we implemented a Triton kernel and PyTorch JIT script to accelerate MAS on GPU without inter-device copy. As a result, Super-MAS Triton kernel is up to 72 times faster in the extreme-length case. The code is available at https://github.com/supertone-inc/super-monotonic-align.", "sections": [{"title": "Introduction", "content": "Monotonic alignment search (MAS) is an algorithm introduced by Kim et al. [1], which is applied to estimate unknown alignments between text and speech in a self-supervised manner. Since this algorithm only needs text and speech pairs, many non-autoregressive TTS models are utilizing MAS during training [1, 2, 3, 4]. While original MAS utilized mel spectrogram, there are other studies utilizing Yingram [3, 5] or NANSY feature [4, 6] to MAS, we use the term speech representation instead of mel spectrogram. Since this algorithm needs to search for the most probable alignment with dynamic programming by caching all paths, the time complexity of the algorithm is O(T \u00d7 S), where T is the length of text and S is the length of speech representation. Official implementation of MAS is implemented with Cython [7] and calculating it on CPU with nested loops, while the authors mentioned it is difficult to parallelize. However, we found that MAS can be parallelized in text-length dimension and CPU execution is needed to copy large-size tensors between CPU and GPU, which consumes an inordinate amount of time. Therefore, we implemented MAS with Triton kernel [8] and PyTorch JIT script [9] to accelerate MAS on GPU without the nested loops and inter-device copy. Especially, the Super-MAS Triton kernel is at least 19 times faster and up to 72 times faster than the original Cython implementation."}, {"title": "Monotonic Alignment Search", "content": "Original MAS calculation contains nested loops like Algorithm 1 and the authors of Glow-TTS [1] noted:\n\"The time complexity of the algorithm is O(Ttext \u00d7 Tmel). Even though the algorithm is difficult to parallelize, it runs efficiently on CPU without the need for GPU executions. In our experiments, it spends less than 20 ms on each iteration, which amounts to less than 2% of the total training time. Furthermore, we do not need MAS during inference, as the duration predictor is used to estimate the alignment.\"\nHowever, we found issues while using MAS:\n1. MAS can be parallelized in the text-length dimension, while the original implementation uses nested loops.\n2. CPU execution consumes an inordinate amount of time for large inputs due to the need to copy large tensors between CPU and GPU.\n3. The hard-coded value of max_neg_val at -1e9 is insufficient to prevent alignment mismatches in the upper diagonal parts."}, {"title": "Triton Kernel", "content": "Figure 1 illustrates how the Triton kernel calculates MAS. While Cython parallelizes batch dimension by parallel for loop, Triton parallelizes it by batch dimension-wise grid. Since Triton requires the calculation block size to be a power of 2, we set it to the next power of 2 greater than T and mask the excluded region with max_neg_val. The described algorithm is accelerated through Triton's just-in-time (JIT) compilation. We named this kernel Super-MAS.\nWhile the Triton kernel loads the block from GPU DRAM (HBM/GDDR) to GPU SRAM (L1/L2 cache), it could mask a region with a specific value. Therefore, unlike the Cython implementation, which starts with complex initialization by computing the cumulative sum for the first row and column, the Triton kernel only requires initializing the first column without considering the first row.\nA forward loop iterates over each column j of the log-likelihood matrix, starting from the second column. For each element, the function loads log-likelihood from the current text position $q_{1:T, j-1}$ and previous text position $q_{0:T-1,j-1}$ blocks, comparing them to update the current elements $q_{1:T,j}$ with the maximum value plus the respective element in the log-likelihood matrix $q_{1:T,j}$ as a maximum likelihood to arrive that position. We also implemented to store the maximum likelihood to path matrix A*, while they share the same size, however, due to covering up all values to 0, it was much slower than the current implementation.\nA backward loop iterates over each column j to reconstruct the maximum path A*. To only update the alignment $A * (j) \\leftarrow argmax_{i\\in{A* (j+1)-1,A* (j+1)}}q_{i,j}$, we initialize the maximum path matrix A*(j) \u2208 $\\mathbb{R}^{T,S}$ to full zero matrix. The function updates the path matrix, marking the elements along the maximum path based on comparisons between left and left-down values in the value matrix. The algorithm efficiently computes the maximum path by using parallelization to improve computational speed, leveraging the GPU's capabilities. This is particularly beneficial for large tensors where traditional CPU-based methods would be significantly slower. The inplace computation further optimizes memory usage, preventing additional memory allocation and copying overhead."}, {"title": "PyTorch JIT Script", "content": "PyTorch also provides a JIT compiler called TorchScript, which returns the Script function [9]. With PyTorch JIT Script, we implement parallelized MAS in two versions, while writing to the path tensor is slower in small tensor size. Different from Triton and Cython implementation, they calculate batch dimensions together as a parallelization.\nUnlike the Triton kernel, during the forward loop, the JIT scripts slice the jth column from q as $q_{1:T,j}$, and use torch.roll to shift the index and masking first index to the maximum negative value. JIT_v1 is fully computed with pytorch tensor on the GPU, while JIT_v2 calculates the forward loop on the GPU, copy it to the CPU, backward loop on CPU numpy array, and sends the result to the GPU. Both versions share the same forward loop and differ only in the implementation of the backward loop."}, {"title": "Experiments", "content": "To check the performance of our kernel and JIT scripts, we run benchmarks in the identical system. The benchmark was run on a system with Intel Xeon Gold 6226R CPU and NVIDIA RTX 3090 GPU. The implementation requires PyTorch, tested with version torch==2.3.0+cu121, Triton-Lang, tested with version triton==2.3.0, and Cython, tested with version Cython==0.29.36.\nDuring the benchmark, we generate a random log-likelihood tensor of size [B, T, S], where B is the batch size, T is the text length, and S is the speech length. We fix B to 32, and S to four times T, with T being the only variable during the benchmark. The benchmark was conducted using Triton's triton.testing.perf_report."}, {"title": "Benchmark", "content": "Table 1 and Figure 2 show the results of the execution time. Super-MAS Triton implementation is at least 19 times faster and up to 72 times faster than the Cython implementation. PyTorch JIT implementations are faster than the Cython implementation for large-sized tensors, especially version v1, which does not involve any inter-device copying."}, {"title": "Conclusion", "content": "In this report, we implemented parallelized MAS in multiple frameworks and versions. Our Super-MAS Triton kernel shows the best time that it is at least 19 times faster and up to 72 times after than the original Cython implementation. Our kernel does not contain kernel fusing for calculating log-likelihood and other additional optimization, and it should be improved by additional efforts. We believe this work can be utilized in various applications, including future non-autoregressive TTS models, alignment estimation for automatic speech recognition models, and other scenarios requiring monotonic alignment."}]}