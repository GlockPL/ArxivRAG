{"title": "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation", "authors": ["Yoonjin Chung", "Pilsun Eu", "Junwon Lee", "Keunwoo Choi", "Juhan Nam", "Ben Sangbae Chon"], "abstract": "Although being widely adopted for evaluating generated audio signals, the Fr\u00e9chet Audio Distance (FAD) suffers from significant limitations, including reliance on Gaussian assumptions, sensitivity to sample size, and high computational complexity. As an alternative, we introduce the Kernel Audio Distance (KAD), a novel, distribution-free, unbiased, and computationally efficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and empirical validation, we demonstrate KAD's advantages: (1) faster convergence with smaller sample sizes, enabling reliable evaluation with limited data; (2) lower computational cost, with scalable GPU acceleration; and (3) stronger alignment with human perceptual judgments. By leveraging advanced embeddings and characteristic kernels, KAD provides an efficient, reliable, and perceptually aligned benchmark for evaluating generative audio models.", "sections": [{"title": "1 Introduction", "content": "As the demand for neural audio generation continues to grow across various domains such as content creation and virtual environments, innovative models are emerging to address a wide range of tasks. These include generating audio from textual descriptions, visual inputs, temporal data, or other audio signals, underscoring the importance of models that can process diverse types of inputs. Consequently, the need for robust and reliable methods to evaluate the quality of these models is becoming increasingly critical.\n\nThe Fr\u00e9chet Audio Distance (FAD) [1] is a widely used metric for evaluating the overall performance of audio generation models, measuring the dissimilarity between the statistical distributions of real and generated audio samples. FAD is considered a simple yet effective measure for objective evaluation, making it a popular choice for assessing generative audio models across various tasks."}, {"title": "2 Related Works and Preliminaries", "content": null}, {"title": "2.1 Fr\u00e9chet Audio Distance and its Limitations", "content": "The Fr\u00e9chet Audio Distance (FAD) [1] measures the difference between two sets of audio samples within their data embedding space. Specifically, it is an estimation of the Fr\u00e9chet distance between the underlying distributions of two given embedding sample sets. FAD is an adaptation of the Fr\u00e9chet Inception Distance (FID)[6] \u2013 originally proposed for evaluating image generation models \u2013 to the audio domain. The embeddings are typically extracted using an audio encoder model pretrained on real-world data such as VGGish[7], ensuring that the embeddings capture representative features of the audio samples for reliable evaluation.\n\nGiven the ground-truth reference set embeddings X = {x_i}_{i=1}^m and the target evaluation set Y = {y_j}_{j=1}^n, FAD is defined by:\n\nFAD^2(X,Y) = ||\u03bc_X \u2212 \u03bc_Y||^2 + tr (\u03a3_X + \u03a3_Y - 2\u221a\u03a3_X\u03a3_Y), (1)\n\nwhere X and Y are assumed to be sampled from multivariate Gaussian distributions, fully character-ized by their means \u03bc_X, \u03bc_Y and covariances \u03a3_X, \u03a3_Y.\n\nFAD is a conventional choice of metric for evaluating generative models in various domains, including text-to-audio [8-20] and vision-to-audio [21-36] tasks, and is considered one of the standards for"}, {"title": "2.2 Maximum Mean Discrepancy", "content": "To address the limitations of FAD, we adopt the Maximum Mean Discrepancy (MMD) [38]. Originally proposed for a statistical test to distinguish whether two samples come from the same distribution, MMD is capable of capturing differences not only in mean and variance but also in higher-order moments. It is also distribution-free, meaning that it does not assume that the samples belong to a specific family of distributions (e.g. Gaussian). This allows for a more comprehensive comparison of how two sets of audio samples differ in their embedding spaces.\n\nThe MMD between two distributions P and Q is defined as:\n\nMMD(F, P, Q) = sup_{f\u2208F} (E_{x~P}[f(x)] \u2013 E_{y~Q}[f(y)]), (2)\n\nwhere F is a class of functions chosen to detect differences between P and Q.\n\nWhen F is chosen to be the Reproducing Kernel Hilbert Space (RKHS) induced by a kernel function k(\u00b7,\u00b7), calculating the MMD corresponds to measuring the Euclidean distance between the mean em-bedding positions after mapping the data into a high-dimensional feature space. The high-dimensional mapping is necessary because it reveals the nonlinear differences between two distributions that may not be apparent in a lower-dimensional setting.\n\nRather than computing these high-dimensional representations explicitly, kernel operations can be used to calculate the distances in the RKHS directly in the original embedding space. This technique, often referred to as the \"kernel trick,\" allows the metric to leverage high-dimensional \u2013 or even infinite-dimensional \u2013 representations that would otherwise be infeasible to compute. With this setup, the MMD can be computed entirely through pairwise comparisons of the samples:\n\nMMD^2(P,Q) = E_{x,x'} [k(x, x')] + E_{y,y'} [k(y, y')] \u2013 2 E_{x,y}[k(x, y)], (3)\n\nwhere x, x' are drawn from P and y, y' are drawn from Q.\n\nFor the finite samples in the reference set X = {x_i}_{i=1}^n and the evaluation set Y = {y_j}_{j=1}^m, an unbiased estimator of 3 is:\n\nMMD^2_{unbiased}(X,Y) = \\frac{1}{n(n \u2212 1)} \\sum_{i\u2260j}^n k(x_i, x_j) + \\frac{1}{m(m - 1)} \\sum_{i\u2260j}^m k (y_i, y_j)\n\n\u2212 \\frac{2}{nm} \\sum_{i=1}^n \\sum_{j=1}^m k(x_i, y_j). (4)\n\nSeveral previous studies have explored MMD-based metrics for evaluating generative models such as the Kernel Inception Distance (KID) proposed by Binkowski et al. [39]. KID is computed as the MMD between samples embedded by the Inception model [40], yielding an unbiased measure with rapidly decaying deviation as sample size increases \u2013 addressing some of the well-known limitations of FID. However, Inception-based scores including FID have been shown to diverge considerably from human evaluations [41]. In response, Jayasumana et al. [2] introduced the CLIP Maximum Mean Discrepancy (CMMD), which leverages the CLIP [42] image encoder to generate embeddings that are more human-aligned when assessing generative models."}, {"title": "3 Kernel Audio Distance", "content": "In this section, we propose the Kernel Audio Distance (KAD), a reliable and computationally efficient metric for evaluating audio generation models.\n\nThe fundamental requirement for such a metric is its ability to capture perceptually meaningful differences between generated and reference audio. Provided that the embedding space sufficiently encodes these perceptually relevant features, a reliable metric must be capable of accurately com-paring embedding distributions without imposing restrictive assumptions. To this end, we adopt the MMD, whose distribution-free nature eliminates the need for parametric assumptions and enables a comprehensive comparison between two embedding distributions.\n\nWe define KAD as follows:\n\nKAD^2 = \u03b1 \u00b7 MMD^2_{unbiased}, (5)\n\nwhere \u03b1 is a resolution scaling factor introduced for convenient score comparison. We set \u03b1 = 1000 as the default."}, {"title": "3.1 The Strengths of KAD", "content": "Along with its robust theoretical foundation, KAD also provides key practical advantages over FAD:\n\nUnbiased Nature: The KAD score is independent of the sample size, making it robust to smaller samples without employing bias-correction procedures. By contrast, the bias-correction for FAD (e.g., FADis) [5] relies on linear fitting of results at multiple sample sizes. This independence makes KAD especially robust in data-scarce conditions, such as early-stage evaluations of generative models or when high-quality reference datasets are limited.\n\nOverall Computational Efficiency: KAD has a time complexity of O(dN^2). In practice, this can be significantly faster than O(dN^2 + d^3) for FAD, as the d^3 term can dominate with higher dimensionality. Therefore, KAD is more scalable for higher-dimensional embeddings typical of modern deep audio models.\n\nParallel Computation The pairwise operations in the computation of KAD (Eq. 4), \\sum_{i=1}^n \\sum_{j=1}^m k(x_i, y_j), can be performed in parallel, enabling substantial acceleration."}, {"title": "3.2 Kernel Function and Bandwidth Selection", "content": "KAD relies on evaluating pairwise relationships between embeddings through a kernel function. Many commonly used kernels such as Gaussian, Laplacian, and Mat\u00e9rn kernels are examples of a characteristic kernel, meaning that the MMD it induces i) fully distinguishes between two embedding distributions and ii) is zero if and only if the two distributions under test are identical [43]. This property is crucial for model evaluation, as it ensures that the KAD metric captures meaningful differences in the embedding distributions of real and generated audio samples. As an example, a second-order polynomial kernel (1 + x^T y)^2 cannot differentiate between distributions with the same mean and variance, but different kurtosis [44].\n\nFor the KAD, we choose the Gaussian radial basis function (RBF) kernel:\n\nk(x, y) = exp (\\frac{\u2212||x \u2212 y||^2}{2\u03c3^2}), (6)\n\nwhere \u03c3 is the bandwidth parameter. An implicit mapping \u03c6(x) to the RKHS of the Gaussian RBF kernel is infinite-dimensional and is defined by the property \u27e8\u03c6(x), \u03c6(y)\u27e9 = k(x, y). This kernel has been extensively analyzed and validated in MMD applications [2, 45] for its smoothness, balanced sensitivity to both local and global variations, and well-studied performance across diverse datasets and modalities.\n\nFor the value of the bandwidth parameter \u03c3, we follow the commonly adopted median distance heuristic [38], setting \u03c3 as the median pairwise distance between the embeddings within the reference set. This heuristic provides a stable baseline with minimal tuning, ensuring the kernel is neither too flat (insensitive to dissimilarities) nor too peaked (over-sensitive to noise). While exploring adaptive or data-driven kernel selection is beyond the scope of this work, our initial experiments indicate that the median heuristic is sufficiently effective. More details are discussed in Appendix A."}, {"title": "4 Experiments", "content": "In this section, we present our empirical findings on KAD and comparison with FAD across three key perspectives: (1) Alignment with Human Perception, (2) Convergence with Sample Size and (3) Computation Cost."}, {"title": "4.1 Experiment 1: Reliability of KAD in Perceptual Alignment", "content": "A reliable evaluation metric for generative audio should be closely aligned with the human perception of audio quality. While FAD relies on a normality assumption that may not accurately capture the multimodal nature of real-world audio embeddings, KAD takes a distribution-free approach. This flexibility allows it to handle complex acoustic feature representations and potentially align more closely with how humans perceive audio quality.\n\nTo validate the perceptual alignment of KAD in comparison with FAD, we use data from the DCASE 2023 Challenge Task 7 submissions [46\u201363] for Foley sound generation. This dataset provides human rating scores on audio quality for 9 different audio generation models, making it a reliable benchmark for correlating objective metrics with subjective judgments.\n\nWe compute both KAD and FAD using embedding from several well-known models, including VGGish [7], PANNs [64], CLAP [65], PaSST [66], and OpenL3 [67], all of which are trained on environmental sounds. These embedding models are widely used for the calculation of FAD scores for text-to-audio [8\u201320] and vision-to-audio generation[21\u201336]. Since music-focused models can differ substantially in their learned representations, we also include MERT [68] and CLAP-laion-music [69] for completeness. We then measure the Spearman rank correlation between each metric's scores and the average human evaluation scores, and also the p-value. Correlations with p > 0.05 are shaded in 3 to indicate a lack of statistical significance.\n\nAs shown in Figure 3, KAD exhibits a Spearman correlation of up to -0.93, notably outperforming FAD whose strongest correlation is -0.80. This suggests that KAD is more effective for differentiat-ing the perceptual nuances captured within the audio data embeddings from a wide range of common audio representations. In contrast, embeddings trained on music data (MERT and CLAP-laion-music) show weaker alignment, consistent with previous findings[4].\n\nAmong the tested embedding models, PANNs-WGLM(WaveGram-LogMel) achieves the strongest correlation with human judegments, aligning with prior research that highlighted its suitability for FAD-based evaluations [4]. Based on this observation, we select PANNS-WGLM as the primary embedding model in subsequent experiments to further investigate the performance of KAD."}, {"title": "4.2 Experiment 2: Convergence with Sample Size", "content": "To compare how KAD and FAD converge as the evaluation set size N increases, we use the eval split of the Clotho 2 dataset [37] with 1045 samples as the reference set, and samples generated using AudioLDM [9] as the evaluation set. The evaluation samples were generated by conditioning on text captions from the dev split of the Clotho 2 dataset. The number of generated samples starts at N = 100 and gradually increases up to N = 3839 (the total size of the Clotho 2 dev split). We compute both KAD and FAD under these varying N values to observe their biases and convergence rates.\n\nFigure 4 displays how KAD and FAD evolve as N increases, normalizing each metric by its extrap-olated value at N = \u221e. At small N, FAD shows a distinct positive bias, deviating substantially from its stable value. This deviation decreases roughly by half whenever the sample size doubles, indicating that a large N is needed for FAD to become reliable.\n\nBy contrast, KAD remains close to its asymptotic value even at relatively small N, reflecting its unbiased nature. While KAD does exhibit a relatively larger standard deviation (the shaded region) for smaller N, this uncertainty band narrows quickly. Notably, even when accounting for the standard deviation, the range of error for KAD is bounded by the magnitude of bias for FAD, up to the largest sample size tested (N = 3839). These results show that KAD can serve as a more stable evaluation metric, especially when the availability of generated audio samples is limited."}, {"title": "4.3 Experiment 3: Computation Cost Comparison", "content": "To assess the computational efficiency of KAD relative to FAD, we measure their wall-clock times on both CPU and GPU across varying embedding dimensions d and sample sizes N. We use PANNS-WGLM [64], VGGish [7], and CLAP [65] \u2013 encompassing dimension sizes from d = 128 (VGGish) to d = 2048 (PANNs-WGLM). The sample sizes range up to 10k to cover typical open-source audio-text datasets like Clotho [37] and AudioCaps [70].\n\nFor the measurements, AMD EPYC 7413 CPU (24 cores) and an Nvidia RTX 3090 GPU were used, and the code is implemented on PyTorch for both KAD and FAD calculations. For FAD, we refactored the Microsoft FAD toolkit [5] for consistency in CPU/GPU usage, thereby ensuring comparability of runtime measurements. All values were calculated in single-precision floating points.\n\nFigure 5 shows that FAD's computation time increases dramatically with dimension size d, whereas KAD remains relatively stable. This stark difference aligns with the theoretical d\u00b3 scaling of FAD, in contrast to KAD's weaker dependence on d. FAD exhibits significant computational overhead at high dimensions even for small sample sizes. Figure 5a highlights how FAD's wall-clock time (blue lines) escalates with d, while KAD (orange lines) remains nearly flat. At d = 2048, the runtime gap can reach three orders of magnitude. Figure 5b further confirms that the main bottleneck for FAD is dimension size, rather than the number of samples. This behavior indicates that FAD is less practical when evaluating embeddings with large d or on resource-limited systems.\n\nFurthermore, KAD benefits considerably from GPU acceleration (dotted vs. solid orange lines), achieving more than an order of magnitude of speedup."}, {"title": "5 Conclusion", "content": "In this paper, we addressed key limitations of the Fr\u00e9chet Audio Distance (FAD) for evaluating generative audio models and proposed the Kernel Audio Distance (KAD) as a more robust alternative. Built on the Maximum Mean Discrepancy (MMD), KAD avoids making statistical assumptions about the embedding distributions, provides unbiased results for all sample sizes, and offers a computational complexity that scale more efficiently, particularly at higher dimensionalities.\n\nWe define KAD as the MMD between reference and evaluation audio embedding sets using a Gaussian RBF kernel with the median-distance bandwidth heuristic. To validate its effectiveness, we compare both KAD and FAD against human evaluation data, observe their convergence behaviors with increasing sample sizes, and measure their CPU and GPU runtimes across a range of dimensionalities and sample sizes.\n\nOur findings show that KAD aligns more strongly with human judgments than FAD across various common audio embedding models, with especially high correlation with PANNs-WGLM. Moreover, its score remains consistent regardless of sample size, making it practical for resource-constrained or early-stage model evaluations, and its computational overhead is up to orders of magnitude lower for higher dimensional (~2024) embeddings compared to FAD due to the reduction of the complexity from O(dN^2 + d^3) to O(dN^2) and its amenability to parallel computation.\n\nThese advantages position KAD as an efficient, comprehensive, and scalable tool for benchmarking generative audio models. By more accurately capturing human-perceived audio quality, KAD can support the development of more reliable evaluation practices in the field. The accompanying open-source toolkit is provided to encourage widespread adoption, experimentation, and ongoing improvements to the development and assessment of generative audio models."}, {"title": "A Median Pair-wise Distance Heuristic for the Kernel Bandwidth", "content": "To assess the effectiveness of the median pairwise distance heuristic for setting the bandwidth parameter \u03c3, we compared the MMD values calculated at different bandwidth settings as the quality of the evaluation data is artificially degraded. If the heuristic to works as intended, the MMD at the median distance bandwidth should be well-correlated with the audio quality for the perceptually relevant ranges of degradation.\n\nWe compared the MMD scores between the clean audio files from the Clotho dataset's eval split, and the same files under various audio signal transformations, including Gaussian noise injection, bit depth reduction, low-pass filtering, dynamic range reduction (limiter), and pitch shifts. The degradations were applied using the audiomentations python library [71].\n\nFor each transformation, MMD scores were calculated using the median pairwise distance as the bandwidth, as well as bandwidths scaled by factors from 0.001\u00d7 up to 1000\u00d7 relative to the median. The scores were normalized such that the maximum MMD score for each bandwidth result is 1, allowing us to observe trends in the scores independently of their absolute values.\n\nThe results are shown in Figure 6. A reliable metric is expected to show a monotonic increase in score as the severity of the degradation increases, because higher KAD scores should correlate with worse audio quality. When the median pairwise distance is used as the bandwidth, the MMD scores consistently respect the expected monotonic trend across all degradations tested. This suggests that the median heuristic provides stable and meaningful results. Bandwidths scaled by 10\u00d7 and 100\u00d7 the median also produced reliable results, maintaining the monotonic relationship. However, smaller bandwidths (0.001\u00d7 to 0.1\u00d7 resulted in scores that drop too quickly, diminishing the discriminative power of the metric. Larger bandwidths (1000\u00d7 the median) tend to flatten the scores, reducing sensitivity to dissimilarities.\n\nThese initial experiments suggest that the median bandwidth heuristic is a robust and effective choice for calculating MMD scores. While certain scaled bandwidths may also be viable, the median heuristic avoids nonsensical results, ensuring stable and interpretable evaluations without additional tuning. Therefore, we recommend its use for kernel bandwidth selection in this context."}, {"title": "B Bias of FAD", "content": "In order to analyze the bias of FAD, we revisit Equation 1:\n\nFAD^2(X,Y) = ||\u03bc_X \u2212 \u03bc_Y||^2 + tr (\u03a3_X + \u03a3_Y - 2\u221a\u03a3_X\u03a3_Y).\n\nFor finite samples, \u03bc_X, \u03bc_Y and \u03a3_X, \u03a3_Y are replaced by their sample estimates \u03bc\u0302_X, \u03bc\u0302_Y and \u03a3\u0302_X, \u03a3\u0302_Y. This introduces bias due to finite sample effects.\n\nThe first term, ||\u03bc_X \u2013 \u03bc_Y ||^2, is unbiased since the sample means \u03bc\u0302_X and \u03bc\u0302_Y are unbiased estimators of the true means \u03bc_X and \u03bc_Y. Thus,\n\nE[||\u03bc\u0302_X \u2013 \u03bc\u0302_Y ||^2] = ||\u03bc_X \u2013 \u03bc_Y ||^2.\n\nThe second term, tr (\u03a3_X + \u03a3_Y \u2212 2\u221a\u03a3_X\u2211_Y), is affected by finite sample sizes. The sample covari-ance matrix is a biased estimator of the true covariance \u03a3:\n\nE[\u03a3] = \\frac{N-1}{N} \u00b7 \u03a3,\n\nwhere N is the sample size. Consequently, the expected value of the trace of the covariance matrices is:\n\nE[tr(\u03a3\u0302_X + \u03a3\u0302_Y)] = \\frac{N-1}{N} \u00b7 tr(\u03a3_X + \u03a3_Y).\n\nIn the second term, 2(\u03a3_X\u03a3_Y)^{1/2} is nonlinear in the covariances, and its exact bias is difficult to compute. However, to first-order approximation, the bias of FAD due to finite samples can be expressed as:\n\nBias_{FAD} ~ \\frac{1}{N} \u00b7 (tr(\u03a3_X) + tr(\u03a3_Y)) + O(\\frac{1}{N^2}).\n\nHere, the primary contribution to the bias arises from the underestimation of the covariance matrices, which scales inversely with the sample size N."}, {"title": "C kadtk: KAD Toolkit Release", "content": "We release a toolkit named kadtk that could calculate KAD scores from input audios. Given input audio directories for the ground-truth reference set and target evaluation set, the toolkit calculates the score and saves it to the given output filepath in csv format. The toolkit is written in Python and supports Pytorch and Tensorflow environments (refer to our Readme document for further details). It supports numerous models for embedding extraction: CLAP [65, 69], Encodec [72], MERT [68], VGGish [7], PANNs [64], OpenL3 [67], PaSST [66], DAC [73], CDPAM [74], Wav2vec2.0 [75], HuBERT [76], WavLM [77], and Whisper [78]. This covers a wide range of the audio domain including general environment sounds (sound effects, foley sounds, etc.), music, and speech. We also support FAD calculation for comparison.\n\nThe kadtk is released under the MIT License, allowing unrestricted use, modification, and distribution with proper attribution. The full license text is included in the repository. Some codes were brought from the FAD toolkit (fadtk) [5, 79, 80]. We sincerely thank the authors for sharing the code as open source. Note that fadtk was also licensed under the MIT License."}]}