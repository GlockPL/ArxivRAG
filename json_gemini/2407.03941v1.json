{"title": "NARROW TRANSFORMER: STARCODER-BASED JAVA-LM FOR DESKTOP", "authors": ["Kamalkumar Rathinasamy", "Balaji A J", "Ankush Kumar", "Gagan Gayari", "Harshini K", "Rajab Ali Mondal", "Sreenivasa Raghavan K S", "Swayam Singh"], "abstract": "This paper presents NT-Java-1.1B\u00b9, an open-source specialized code language model built on StarCoderBase-1.1B\u00b2, designed for coding tasks in Java programming. NT-Java-1.1B achieves state-of-the-art performance, surpassing its base model and majority of other models of similar size on MultiPL-E (Cassano et al., 2022 [1]) Java code benchmark. While there have been studies on extending large, generic pre-trained models to improve proficiency in specific programming languages like Python, similar investigations on small code models for other programming languages are lacking. Large code models require specialized hardware like GPUs for inference, highlighting the need for research into building small code models that can be deployed on developer desktops. This paper addresses this research gap by focusing on the development of a small Java code model, NT-Java-1.1B, and its quantized versions, which performs comparably to open models around 1.1B on MultiPL-E Java code benchmarks, making them ideal for desktop deployment. This paper establishes the foundation for specialized models across languages and sizes for a family of NT Models.", "sections": [{"title": "Introduction", "content": "The state-of-the-art code models, capable of understanding and generating code in numerous programming languages, are revolutionizing the way enterprises approach software development. With the ability to understand and generate code across a vast array of programming languages, these code models offer a significant boost in productivity. However, the one-size-fits-all approach of these generic multi-lingual code models often falls short in meeting the nuanced requirements of project-level coding tasks in an enterprise, which tend to be language-specific. This has led to the development of Narrow Transformers (NTs), specialized models further trained on a particular programming language, offering a more efficient solution for enterprises. These NTs are designed to optimize performance for a specific programming language, balancing the trade-offs between model size, inferencing cost, and operational throughput. As demand for tailored solutions grows, we can expect a surge in NT development, providing the precision and efficiency required by enterprise projects.\nHowever, in practice, the substantial economic cost associated with training and fine-tuning large code models renders language model experiments prohibitively expensive for most researchers and organizations. Additionally, deploying these massive models in everyday scenarios, such as on personal computers, proves either inefficient or unfeasible. These challenges emphasize the importance of shifting focus to explore Narrow Transformer approach on powerful yet smaller code language models (code SLMs). Consequently, we developed a Narrow Transformer for Java within a smaller parameter range (i.e., 1.1B), suitable for desktop deployment and democratizing code model experiments."}, {"title": "Related Work", "content": "Codex-12B (Chen et al., 2021 [2]) was built by extending pre-training of GPT (which contains strong natu-ral language representations), with 159 GB of unique Python files under 1MB, from public software repos-itories hosted on GitHub. Codex exhibits its highest proficiency in Python; however, it also demonstrates competence in over twelve additional programming languages. CodeGen-Mono-350M/2.7B/6.1B/16.1B (Nijkamp et al., 2023 [3]) were built by further pretraining CodeGen-Multi-350M/2.7B/6.1B/16.1B (which were trained with multi-lingual datasets comprising code from C, C++, Go, Java, JavaScript, and Python) with the mono-lingual dataset BIGPYTHON that contains public, non-personal, permissively licensed Python code from GitHub. CodeGen-Mono outperformed CodeGen-Multi on Python as per the HumanEval benchmark. In addition, the next generation model in CodeGen family, such as, CodeGen25-7B-mono (Nijkamp et al., 2023 [4]) outperformed CodeGen25-7B-multi only in python language but underperformed in rest of the programming languages in MultiPL-E benchmark. StarCoder-15.5B (Li et al., 2023 [5]) was built by extending pre-training of StarCoderBase-15.5B (which was trained with multi-lingual datasets comprising code from 80+ programming languages) with a Python subset of 35B tokens from the StarCoderBase training data. StarCoder outperformed StarCoderBase on Python as per the Human Eval benchmark. In the evaluation of StarCoder and StarCoderBase on 19 programming languages with MultiPL-E datasets, StarCoder outperformed StarCoderBase on Python, underperformed on 9 programming languages, and despite being further trained only on Python, it still outperformed StarCoderBase on 9 other programming languages. CodeLlama-PYTHON-7B/13B/34B/70B (Baptiste et al., 2023 [6]) were built by extending pre-training of CodeLlama-7B/13B/34B/70B (which were trained on 500B tokens of code data, except CodeLlama-70B, which was trained on 1T tokens) on 100B tokens of python heavy dataset with a composition of Python, multi-lingual code, natural language related to code and natural language at the proportions of 75%, 10%, 10%, 5% respectively. CodeLlama-PYTHON outclasses CodeLlama on Python on MultiPL-E benchmarks, but it is not consistent on rest of the languages. While there are speculations explaining this inconsistency, it is generally understood that although extending pretraining of multi-lingual code foundation models with dataset from a specific programming language does not guarantee performance improvement in other programming languages, it still guarantees performance improvement in that programming language. Hence, building a model like StarCoder using a specific programming language dataset can improve proficiency in that programming language. Enterprise projects are adopting either these pre-trained generic multi-lingual code models or python-trained multi-lingual code models to augment their project coding tasks. AI-mature enterprises are adopting these models as foundation models to further train with their project code base for better augmentation. However, if there is a pre-trained code model further trained on enterprise project's required programming language, then the enterprise project can use that language-specific model and can further train with their project code base for better augmentation. Due to the widespread adoption of Java in enterprise-level projects, this paper illustrates the development of such a pre-trained code model specialized on Java.\nSmall Language Models (SLMs) will pivot the focus of AI community in enterprise and consumer solutions. These models stand out for their ability to be deployed on end-user devices, such as personal computers and smartphones, even without a GPU. This enables large-scale deployment while ensuring data privacy and security. Significant examples in the present scenario of code SLMs include SantaCoder-1.1B (Ben Allal et al., 2023 [7]), Phi-1 (Gunasekar et al., 2023 [8]), DeciCoder-1B, StarCoderBase-1.1B, WizardCoder-1B-V1.0 (Luo et al., 2023 [9]), DeepSeek-Coder-1b-base (Guo et al., 2024 [10]) and Refact-1.6B. All these state-of-the-art models around 1B size are multi-lingual code models, indicating that no considerable work has been done towards extending training of multi-lingual code SLMs in building language-specific code SLMs."}, {"title": "Datasets", "content": "The foundation model identified for our experiment was StarCoderBase-1.1B. Enterprise projects shortlist the candidate code models for adoption of coding tasks based on their licenses, their training data, etc. Utilizing additional dataset, such as pretraining dataset from any model other than StarCoderBase, to extend the pretraining of StarCoderBase-1.1B would complicate the process of shortlisting the further trained StarCoderBase-1.1B model (NT-Java-1.1B) for any enterprise adoption, due to the concerns on licensing. Hence, a subset of StarCoderData5, which is a curated dataset from The Stack v16 used for StarCoderBase training, was considered for building NT-Java-1.1B.\nThe rationale behind building Python-trained models such as Codex, CodeGen-Mono, StarCoder, and CodeLlama-PYTHON might be the popularity of Python and the availability of the greater volume of Python code in the pretraining dataset compared to other programming languages. While the Python dataset in the StarCoderBase training dataset is 35B Python tokens, the Java dataset is around 22B tokens, which is still a considerable size. This Java dataset from StarCoderData was used for training NT-Java-1.1B."}, {"title": "Model Training", "content": ""}, {"title": "Data Preprocessing", "content": "For data preprocessing, we employed the Megatron-LM framework. The NT-Java-1.1B uses the Star-CoderBase tokenizer of type GPT2BPETokenizer (byte-level Byte-Pair-Encoding) and its vocabulary of 49,152 tokens. No additional tokens were added to this vocabulary. The Java dataset comprises 87 parquet files, which were converted into a single file and passed through the Megatron pre-processing module to get the corresponding .bin and .idx files. These files were used for model training. The pre-processing module also performs tokenization and adds an <EOD> token at the end of each Java sample."}, {"title": "Model Architecture", "content": "NT-Java-1.1B, similar to StarCoderBase-1.1B, is a decoder-only Transformer model with Multi-Query Attention (Shazeer, 2019 [11]), which uses FlashAttention. This speeds up the attention computation and reduces the training time of the model. The hyper-parameters for the architecture can be found in Table 1."}, {"title": "Training Details", "content": "NT-Java-1.1B was trained using the Megatron-LM Framework7. The training began with StarCoderBase-1.1B, serving as the initial checkpoint, to build its Java variant. In our experiments, we utilized a context length of 8192 tokens for tasks involving the Next token prediction and the Fill-in-the-Middle (FIM) (M"}, {"title": "Post Training", "content": "The NT-Java-1.1B model has bf16 precision and occupies a total size of 2.27 GB. After the development of the NT-Java-1.1B model, efforts were directed towards the development of quantized models that are tailored to operate on developer desktops. These models were designed to be more compact in size without substantially sacrificing their accuracy, and to be compatible with CPU-based inference frameworks. To achieve this, we built quantized variants of the NT-Java-1.1B model in GGUF format for frameworks like Ollama, GPT4ALL10 and LM Studio11. The quantized versions of the models (NT-Java-1.1B-GGUF12) are available in a range from 2-bit to 8-bit, with their overall sizes spanning from 511 MB to 1.32 GB correspondingly."}, {"title": "Compute", "content": "NT-Java-1.1B was trained with 6 A100 80 GB GPUs on a single-node GPU cluster. The training process remained stable overall, with only a few restarts."}, {"title": "Evaluation", "content": "This section presents evaluation of our proposed coding SLM to assess its capabilities in code generation and infilling tasks."}, {"title": "MultiPL-E", "content": "In our initial assessment, we evaluated the performance of the model from Experiment 2.1 on Java code generation tasks by utilizing the widely recognized benchmark, MultiPL-E. We calculated the pass@1 metric for this benchmark utilizing the BigCode Eval Harness13, ensuring the hyperparameter values were aligned with the established norms of the Big Code Models Leaderboard14. NT-Java-1.1B demonstrated a pass@1 score that surpassed its base model and its 3B variant, as detailed in Table 3. Furthermore, our model's performance surpassed majority of the base models within a similar parameter range, such as Phi-1, SantaCoder-1.1B, DeciCoder-1B, OctoGeeX-7B, StableCode-3B-alpha, WizardCoder-1B-V1.0 and CodeGen25-7B-mono, on the Big Code Models Leaderboard."}, {"title": "Fill-in-the-Middle Benchmark", "content": "Subsequently, we conducted an evaluation of the model's capabilities on the single-line code infilling task, utilizing the benchmark established in the SantaCoder. This benchmark gauges the model's proficiency in completing a single line of Java code within HumanEval solutions, using the 'line exact match' accuracy as the evaluation metric. Our analysis indicates that our model delivers results that are on par with the foundational model, StarCoderBase-1.1B, showcasing comparable performance, as outlined in Table 4."}, {"title": "Computational Capabilities", "content": "Furthermore, we evaluated the model's performance in terms of its efficiency and resource utilization. Our analysis (Table 5) indicates that our NT-Java quantized models achieve an optimal balance between accuracy and resource utilization, making them a suitable candidate for deployment in resource-constrained environments. For the computation of the MultiPL-E scores of the quantized variants, we employed the 'load in 4-bit' and 'load in 8-bit' parameters within the BigCode Eval Harness."}, {"title": "Conclusion", "content": "In this technical report, we outlined the rationale and training approach used to develop NT-Java-1.1B, a small language model trained specifically on Java code. We evaluated NT-Java-1.1B across various coding tasks and compared its performance against models with similar parameters. Our findings indicate that NT-Java-1.1B is competitive with or outperforms other Code SLMs in this parameter range in Java programming tasks.\nThis study demonstrates the successful achievement of its objective of enhancing the efficiency of a code SLM for a particular programming language by training it further with a subset of its dataset for that language. While the research employed the StarCoderBase-1.1B model and its Java language dataset, other SLMs and their associated programming language datasets can yield comparable experimental outcomes.\nThe release of NT-Java-1.1B and its variants aims to democratize code foundation models, making them accessible for deployment in memory-constrained environments such as developer desktops and laptops. By adhering to the principles of the OpenRAIL-M15 and by open-sourcing the corresponding scripts on GitHub, we hope to enable both the research and developer communities to experiment and adopt code SLMs."}]}