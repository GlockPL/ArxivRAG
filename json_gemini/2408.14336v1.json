{"title": "Equivariant Reinforcement Learning under Partial Observability", "authors": ["Hai Nguyen", "Andrea Baisero", "David Klee", "Dian Wang", "Robert Platt", "Christopher Amato"], "abstract": "Incorporating inductive biases is a promising approach for tackling challenging robot learning domains with sample-efficient solutions. This paper identifies partially observable domains where symmetries can be a useful inductive bias for efficient learning. Specifically, by encoding the equivariance regarding specific group symmetries into the neural networks, our actor-critic reinforcement learning agents can reuse solutions in the past for related scenarios. Consequently, our equivariant agents outperform non-equivariant approaches significantly in terms of sample efficiency and final performance, demonstrated through experiments on a range of robotic tasks in simulation and real hardware.", "sections": [{"title": "1 Introduction", "content": "A key challenge in robot learning is to improve sample efficiency, i.e., to reduce the number of experiences or demonstrations needed to learn a good policy. One way to do this is to identify domain symmetries that can structure the policy space. Recent works have demonstrated that symmetry-preserving (equivariant) neural network models are a particularly effective way of accomplishing this [1, 2, 3, 4, 5]. However, these works have focused primarily on fully observable Markov decision processes (MDPs) rather than partially observable systems encoded as partially observable MDPs (POMDPs) [6]. The question arises whether symmetric neural models can also be used to solve Partially Observable Reinforcement Learning (PORL) problems. This paper identifies the theoretical conditions under which this is indeed the case and describes an equivariant recurrent model that works well in practice.\nTo motivate, Fig. 1 illustrates the Drawer-Opening problem where a robot is presented with a chest containing two drawers, one locked and one unlocked. To solve this task, the robot must determine which drawer is unlocked and then open that drawer, relying only on top-down image observations. This task reflects a common POMDP when physical properties (whether a drawer is unlocked) are hidden from the visual input. The only way for the robot to distinguish between the two drawers is to attempt to open one of them. This is a classic feature of a POMDP \u2013 that the agent must perform information gathering actions to obtain information needed to solve the task. Notice that this problem is rotationally symmetric in the sense that its optimal solution (the blue end-effector"}, {"title": "2 Related Works", "content": "Learning under Partial Observability Unlike classical planning-based methods [9, 10, 11] that impractically require the complete dynamics of the environment, learning-based methods [12, 13, 14, 15, 16, 17, 18, 19] utilize recurrent versions of common reinforcement learning (RL) algorithms for policy learning by directly interacting with the environment. To speed up learning, some methods leverage privileged information assumed available during training, such as the states, the belief about the environment states, or the fully observable policy [20, 21, 22, 23], which are orthogonal to our approach. Only a few prior works exploited domain symmetries under partial observability. Kang and Kim [24], Doshi and Roy [25] leveraged the invariance of the value function of some POMDPS given a state permutation and experimented on a classical planning-based method [11] with the above limitations. Recently, Muglich et al. [26] used equivariant networks to enforce symmetry when multiple agents coordinate. In contrast, we use model-free RL agents in a single-agent setting.\nEquivariant Learning Equivariant networks have been successfully applied to a range of tasks such as point cloud analysis [27] and molecular dynamics [28, 29]. A common approach is to build networks with group equivariant convolutions [30] which are equivariant to arbitrary symmetry groups, such as 2D [31, 32] and 3D [27, 33, 34, 35] transformations. Recently, for MDPs, equivariant networks have been applied to robotics [2, 1, 5] and reinforcement learning [36, 37] to improve sample efficiency. Closest to our work is [1], which formalized group-invariant MDPs and used equivariant networks to perform robotic manipulation tasks. In contrast, this work extends equivariant reinforcement learning to partially observable environments, resulting in a new theory and method.\nEquivariance v.s. Data Augmentation Both methods leverage known domain symmetry to improve learning, but in different ways. On the one hand, data augmentation artificially expands the training data distribution with transformed versions of the data using the symmetry (e.g., rotating, cropping, or translating images [38, 39]); then training a non-equivariant model. On the other hand, an equivariant approach bakes the domain symmetry directly into the model's weights, so an equivariant model can automatically generalize across input transformations even before training. Compared to an equivariant approach, a model trained using data augmentation alone is often less sample efficient [32, 34], generalizes worse [40], and requires a bigger architecture and longer training time for the same performance due to the extra work of learning symmetry injected in the data."}, {"title": "3 Background", "content": "Here, we review some background about POMDPs, some specific group theories used in our work, and finally, the basis of our approach the framework of group-invariant MDPs [1]."}, {"title": "3.1 Partially Observable Markov Decision Processes", "content": "A POMDP is defined by a tuple (S, A, \u03a9, bo, T, R, O), where S, A, and \u03a9 are the state space, the action space, and the observation space, respectively. bo \u2208 AS is the starting state distribution (a.k.a."}, {"title": "3.2 Cn and SO(2) Symmetry Groups", "content": "In this work, we are mainly concerned with the symmetry group G = SO(2) of continuous planar rotation, defined as SO(2) = {Rote : 0 < \u03b8 < 2\u03c0}. For a reduced computation complexity, we use the cyclic subgroup Cn < SO(2) to approximate SO(2), which is defined as Cn = {Roto: \u03b8\u2208 {\\frac{2\u03c0i}{n} | 0 < i < n}}. In other words, Cn defines n rotations (i.e., group elements), which are multiples of \\frac{2\u03c0}{n}. For instance, C4 = {0, \u03c0/2, 2\u03c0/2, 3\u03c0/2} and C8 = {0,\u03c0/8, ..., 6\u03c0/8,7\u03c0/8}."}, {"title": "3.3 Group Representations", "content": "A group representation is a mapping from a group G to a d-dimensional general linear (GL) group, i.e., \u03c1 : G \u2192 GLd by assigning each group element g \u2208 G with an invertible matrix \u03c1(g) \u2208 Rd\u00d7d.\nWhen G = Cn, the effect of a rotation g \u2208 Cn on a signal x (i.e., gx) starts with a pixel-wise rotation \u03c1f(g)\u22121x (with a fixed group representation \u03c1f), followed by a channel-wise rotation, i.e., gx = \u03c1(g)(\u03c1f(g)\u22121x) (with the choice of group representation \u03c1). In this work, we consider three choices of the channel-wise representation \u03c1:\nTrivial Representation (\u03c1 = \u03c1t): For \u2200g \u2208 G, \u03c1t associates g with an identity matrix. For example, in Fig. 2a when g is a \u03c0/2 counter-clockwise (CCW) rotation, and x is a 1-channel feature map, \u03c1f rotates the pixels of x while \u03c1t does not change the pixel values (i.e., the colors are unchanged).\nStandard Representation (\u03c1 = \u03c1s): For \u2200g \u2208 G, \u03c1s associates g with a rotational matrix, i.e., \u03c1s(g) = g. As in Fig. 2b, when g is a \u03c0/2 CCW rotation and x is a vector field input, \u03c1f rotates the positions of vectors (denoted as colored arrows), and \u03c1s rotates their orientations.\nRegular Representation (\u03c1 = \u03c1r): For each g \u2208 G, when acting on an input x, \u03c1r will cyclically permute the coordinates of x. Fig. 2c illustrates when g is a \u03c0/2 CCW rotation and x is a 2-channel feature map, \u03c1f rotates each channel's pixels and \u03c1r permutes the orders of the channels.\nAn Illustrative Example Combining the group and the group representation fully characterizes how a signal will be transformed. For an illustrative example in a grid-world domain, see Appendix A."}, {"title": "3.4 Equivariance, Invariance, and Group-invariant MDPS", "content": "Given \u03c6 : X \u2192 Y and a symmetric group G that acts on X and Y, we say that \u03c6 is G-equivariant if \u03c6(gx) = g\u03c6(x), and G-invariant if \u03c6(gx) = \u03c6(x). For the remainder of this document, we drop the prefix G and simply refer to these properties as invariance and equivariance."}, {"title": "4 Group-Invariant POMDPS", "content": "In this section, we extend the ideas from [1] to POMDPs and identify the basic set of assumptions that a POMDP needs to satisfy to have analogous invariance properties. We also note that while other assumptions might also lead to an invariant POMDP, ours are probably the most natural.\nDefinition 1. We say a POMDP PG = (S,\u0391, \u03a9, bo, T, R, O) is group-invariant with respect to group G if it satisfies the following invariant properties for all g \u2208 G:\n$T(gs, ga, gs') = T(s,a,s')$ \n$R(gs, ga) = R(s, a)$\n$O(ga, gs', go) = O(a, s', o)$\n$bo(gs) = bo(s) $\n(1)\nThis extends the definition of the group invariant MDP from [1] by incorporating additional constraints on the observation function and the initial belief distribution. We also extend the group operations on histories.\nDefinition 2. Group operation g acts on history ht according to ght := (goo, gao,..., gat\u22121, got).\nFinally, we show that group-invariant POMDPs exhibit similar properties and benefits as group-invariant MDPs.\nTheorem 1. A group-invariant POMDP has an invariant optimal Q-function $Q^*(gh,ga) = Q^*(h, a)$, an invariant optimal value function $V^*(gh) = V^*(h)$, and at least one equivariant deterministic optimal policy \u03c0*(gh) = g\u03c0*(h).\nProof. See Appendix B.\nThe above analysis allows us to constrain the value function and policy for a G-invariant POMDP to be invariant and equivariant, respectively, without eliminating optimal solutions."}, {"title": "5 Equivariant Actor-Critic RL for POMDPS", "content": "In this section, we introduce an equivariant agent that directly exhibits the desired properties of the optimal value function and policy, backed by the analysis in Theorem 1. Fig. 3 shows the agent,"}, {"title": "5.1 Equivariant Modules", "content": "We describe the details of the equivariant modules within our agent below, with the core components being equivariant CNNs [45, 32]. For the implementation details, please see Appendix D."}, {"title": "Equivariant Feature Extractor", "content": "This module takes observations or actions and outputs intermediate features for further processing. It comprises multiple equivariant CNN components chained sequentially as shown in Fig. 4a. Its input representation is the observation representation po or the action representation pa. The intermediate and output representations are chosen to be the regular representation pr, which empirically outperforms other representations [32]. The input representation can be a single representation type or mixed, i.e., a sum of different representations. A mixed representation is necessary when the input has different components that transform differently under a group transformation, e.g., one component rotates with the transform, and one component is unchanged. Such a case can be seen in Appendix D.2 and is simplified in Fig. 4b, where $p_a = p_s + p_t$."}, {"title": "Equivariant Actor Outputter", "content": "Fig. 4c shows the input representation is regular because the signal coming in (the output of the RNN and the observation feature extractor modules) uses a regular representation. The output representation varies depending on the action type (discrete/continuous) and how a group transformation will affect an action. For discrete actions, the module produces a categorical distribution over the action space. In this case, the output representation is the regular one pr as we want to change the (discrete) action when the history is transformed (see Appendix A for an illustration). For continuous actions, this module outputs the means with some representation p\u00b5 and the standard deviations of actions with some representation p\u03c3 (as in A2C [7], PPO [46], or SAC [8]). The representations used for p\u00b5 and p\u03c3 are mixed, as each action component might change differently under a group transformation (see Appendix D.2)."}, {"title": "Equivariant Critic Outputter", "content": "Because the optimal critic is invariant, this module (Fig. 4d) uses the trivial representation pt at the output to keep the output the same under a group transformation. Its input representation is regular, enforced by the output of the RNN and the action feature extractor."}, {"title": "Equivariant Recurrent Neural Network", "content": "This is our contribution needed for constructing a POMDP equivariant agent (there is another similar component in [26], but only model weights are released). We utilize an LSTM [47] for this module, but the approach can also be modified for other types of RNNs. Specifically, given an input xt (e.g., the concatenated obs-action feature) and the previous hidden state ht\u22121, the input gate it, the forget gate ft, the memory cell candidate gt, and the output ot are computed as follows with Ws and bs being learnable weights and biases:\n$it = sigmoid(Wxixt + Whiht-1+bi)$\n$Ot = sigmoid(Wxoxt + Whoht\u22121 + bo)$\n$ft = sigmoid(Wxfxt + Whfht-1+bf)$\n$gt = tanh(Wxgxt + Whght\u22121+bg).$\n(2)\nThe above equations do not make an equivariant RNN module. To enforce the equivariance, we compute all equations at once using an equivariant CNN module (Fig. 5), similar to the ConvLSTM"}, {"title": "6 Experiments", "content": "We compare the performance of learning agents on two grid-world domains (discrete actions and feature-based observations) and four robot domains (continuous actions and pixel observations)."}, {"title": "6.1 Domains", "content": "We briefly describe our domains below. Please refer to Appendix C for more specific details."}, {"title": "6.1.1 Grid-World Domains", "content": "There are two versions of CarFlag [49] in Fig. 6, where an agent must reach a goal (green), whose position is visible only when the agent visits an unknown information region. For instance, in CarFlag-1D, the agent must visit the central blue flag to get the side (left/right) of the goal; or in CarFlag-2D, the agent must visit the central blue region to see the coordinate of the goal cell. We also illustrate the domain symmetry in the figure: in these domains, when the starting position and the goal location are transformed (flipped in CarFlag-1D or rotated by \u03c0/2 radians clockwise in CarFlag-2D, the optimal trajectories will be transformed similarly, i.e., black \u2192 blue trajectories."}, {"title": "6.1.2 Robot Manipulation Domains", "content": "Fig. 6 shows our robot manipulation domains (extended from the BulletArm suite [50]), where a robot arm must perform individual manipulation tasks (i.e., picking, pulling, pushing, and opening) using top-down depth images to win a sparse reward. In these domains, only one object is manipulable, but both objects are the same if only relying on the current image. Therefore, the agent must actively check the objects' mobility and remember past interactions with the objects to determine the next action. Specifically, in Block-Picking, the agent needs to pick the movable block up. In Block-Pulling, the agent needs to pull the movable block to be in contact with the other block. In Block-Pushing, the goal is to push the movable block to a goal pad. In Drawer-Opening, the agent is tasked to open an unlocked drawer between a locked and an unlocked one.\nIn these domains, the transition function is invariant because the Newtonian physics applied to the interaction is invariant to the location of the reference frame. The reward function is invariant by definition. Using top-down depth images makes the observation function invariant. If the initial belief is assumed invariant, then according to Definition 1, these domains are group-invariant POMDPs."}, {"title": "6.2 Agents", "content": "We compare our proposed agents (instances of the structure in Fig. 3 applied to A2C [7] and SAC [8]) against a diverse set of baselines, including on-policy/off-policy, model-based/model-free, and generic/specialized POMDP methods (see Appendix D and Appendix E for more details)."}, {"title": "6.2.1 Grid-world Domains", "content": "RA2C [51] is a recurrent version of A2C [7]. Equi-RA2C is our proposed architecture applied to A2C. DPFRL [14] is a state-of-the-art model-based POMDP baseline where an A2C agent is given features produced by a differentiable particle filter. DreamerV2 [52] and DreamerV3 [53] are strong model-based methods that learn a recurrent world model, thus, can work with POMDPs.\nNo Data Augmentation for All Since all methods are on-policy algorithms, augmented data using the domain symmetry only becomes on-policy only for Equi-RA2C dues to its unique symmetry-awareness. Therefore, for a fair comparison, we do not perform any data augmentation."}, {"title": "6.2.2 Robot Manipulation Domains", "content": "While on-policy RA2C or Dreamer-v2 can handle continuous action spaces in these domains, there is no clear way to leverage expert demonstrations necessary to efficiently solve the robot manipulation tasks with sparse rewards in Fig. 6. Thus, we switch to SAC [8] as the base RL algorithm, where we can pre-populate its replay buffer with demonstration episodes. In our experiments, RSAC [13] is a non-equivariant recurrent SAC agent. Equi-RSAC is our proposed method applied to SAC. We also compare with recurrent versions of two strong data augmentation baselines: RAD-Crop-RSAC [54] and DrQ-Shift-RSAC [55]. These specific data augmentation techniques, i.e., random cropping and shifting (see Appendix G for visualizations), are chosen among others because they were reported to perform best [55]. To train RAD-Crop-RSAC, for each training episode, an auxiliary episode is created by using the same random cropping for every depth image inside the original episode. DrQ-Shift-RSAC applies two random shifts to each depth image in a training episode to create two. The Q-target and the Q-values are then computed by averaging the values computed on the two episodes. Finally, SLAC [56] learns a latent model from pixels and then uses SAC on the latent space by using the observation-action history (instead of the latent state) for the actor and the latent state samples for the critic. This enables SLAC to scale to more difficult tasks.\nDemonstrations + Rotational Data Augmentation for All All replay buffers are pre-populated with 80 expert episodes to overcome the reward sparsity. Moreover, we augment the training data by applying the same random rotation for every action and observation inside a training episode (see Appendix G for visualizations). Note that these rotational data augmentations are applied in addition to the existing data augmentation techniques in RAD-Crop-RSAC and DrQ-Shift-RSAC."}, {"title": "6.3 Results", "content": "Grid-world Domains Fig. 7 shows that Equi-RA2C is significantly more sample efficient than the baselines. Moreover, the dominance of our method is also seen with variants of these domains with different sizes (see Appendix I). DPFRL did not perform well potentially because of the reward sparsity, which was also previously reported in [57]. DreamerV2 and DreamerV3 also perform poorly, even with many more learnable parameters of the models, potentially indicating that learning a good model under partial observability and sparse rewards might be more challenging than in the domains originally tested. For instance, most Atari games and locomotion tasks in the DeepMind Control suite [58] have low levels of partial observability and provide dense rewards.\nRobot Manipulation Domains Clearly from Fig. 7, Equi-RSAC strongly outperforms other baselines in all domains, with itself being the only agent that can reach a satisfactory performance. Across all domains, without the equivariant LSTM module (denoted as Non-Equi-RSAC (LSTM), which can be considered as a naive extension of [1]), the performance degrades significantly, even though it starts pretty well. SLAC, surprisingly, performs the worst. A possible reason is that SLAC"}, {"title": "6.4 Using Equivariant Models on Domains with Imperfect Symmetry", "content": "We investigate the performance when the perfect symmetry does not hold in asymmetric variants of CarFlag, created by offsetting the information region a distance d from the world center (see Appendix C). From the final success rates shown in Table 1 (see Appendix I.1 for learning curves), we can see that equivariant Equi-RA2C still outperforms non-equivariant RA2C when the domains are close to perfect symmetry, i.e., when d = d1D = 5 in CarFlag-1D or d = d2D = 1,2 in CarFlag-2D. However, a bigger symmetry gap might lead to the sub-optimality of equivariant agents. As evidence, Equi-RA2C performs worse than RA2C when d = 10, -10 in CarFlag-1D."}, {"title": "6.5 Zero-shot Transfers to Real Hardware", "content": "Because only our agents can perform well in simulation, we transfer their best policies in simulation to a UR5 robot (see Fig. 8). We combine the point clouds from two side-view cameras to create a top-down depth image using a projection at the gripper's position. We roll out 50 episodes, divided equally into test cases when the agents first manipulate the immovable or movable objects. Table 2 shows that the learned policies can be zero-shot transferred well in the real world regardless of small performance drops in all domains (see our supplementary video for policy visualizations). The biggest performance drop is in Draw-Opening, in which the transferred policies sometimes clumsily move one drawer far away from the other, creating a novel scene never seen in simulation."}, {"title": "7 Conclusion and Limitations", "content": "Conclusion In this work, we introduced group-invariant POMDPs and proposed equivariant actor-critic RL agents as an effective solution method. Through extensive experiments, our proposed equivariant agents can tackle realistic and challenging robotic manipulation domains much better than non-equivariant approaches with learned policies zero-shot transferable to a real robot.\nLimitations A limitation of most equivariant approaches, including ours, is the requirement of imperfect symmetry, which might be present when images are affected by non-symmetric factors, e.g., side view instead of top-down view or asymmetric noises. Fortunately, under full observability, recent empirical [60, 61] and theory work [62] show that an equivariant model can still outperform non-equivariant approaches in many such cases. Together with the results in Section 6.4, our approach might still perform better than unstructured agents even under imperfect symmetry."}, {"title": "A Illustration of Group and Group Representation in CarFlag-2D", "content": "Domain We consider a small version of CarFlag-2D (see Fig. 9) with a grid size of 3x3, where the agent (red) must navigate to an unknown target cell (green) in a grid world. The agent can always observe its current location but only observe the target cell when it visits the information cell (blue), which is also unknown to the agent.\nObservation The observation is a two-channel image size 2x3x3, where the first channel encodes the agent's location and the second encodes the target location. The values of the second channel are non-zero only when the agent is at the information cell (Fig. 9).\nActions Movements in four directions (the location does not change if going out of the world)."}, {"title": "B Proof of Theorem 1", "content": "In this section, we introduce the framework of history representation MDP [63] and prove a supporting lemma before arriving at the proof."}, {"title": "B.1 History Representation MDP", "content": "A POMDP can be converted into a history representation MDP (HR-MDP) [63] whose state is a sufficient statistic of the POMDP history for control, e.g., the well-known Belief-MDP [64] construct is a special case of an HR-MDP based on the belief representation. Useful representations such as the belief might require a known POMDP model; however, we adopt a model-free approach with no such knowledge and, therefore, use the trivial identity representation whereby the history is represented by itself. This effectively converts the POMDP into an equivalent History-MDP, which is defined by the tuple (H, A, T, R), where:\n$T(h, a, h') = Eo\\h,a [[{h' = hao}]$\n$ R(h, a) = Es\\h [R(s,a)], $\n(5)\nwhere I{\u00b7} is the indicator function, and\n$Pr(oh, a) = Esh \\sum\\limits_{s'} T(s, a, s')O(a, s', o),$\n(6)\n$Pr(s' | h') x Es\\h [T(s, a, s')] O(a, s', o) .$\n(7)"}, {"title": "B.2 Supporting Lemma", "content": "Lemma 1. The belief function of a group-invariant POMDP (as defined by Definition 1) is group-invariant,\n$Pr(gs | gh) = Pr(s | h) .$\n(8)\nProof By Induction.\nBase Case. We first prove that the belief after the first observation is invariant. We note here that the observation function for the first timestep takes the form O(s, 0), with no preceding action.\n$Pr(gso | goo) x bo(gso)O(gso, goo) = bo(so)O(50,00) x Pr(so | 00).$\n(9)\nSince Pr(gso | goo) and Pr(so | 00) are both proportional to the same quantity, and they are both normalized to be distributions over states, then they are themselves equal.\nInductive Step. We then prove that if Pr(st | ht) is invariant, then Pr(st+1 | ht+1) is also invariant. Per Eq. (7),\n$Pr(gst+1|ght+1) \u00d7 Pr(gst | ght)T(gst, gat, gst+1)O(gat, gst+1, got+1) = Pr(st | ht)T(st, at, St+1)O(at, St+1, Ot+1) \u00d7 Pr(st+1 | ht+1).$\n(10)\nSince Pr(gst+1 | ght+1) and Pr(st+1 | ht+1) are both proportional to the same quantity, and they are both normalized to be distributions over states, then they are themselves equal. By induction, given the base case and the inductive step, the belief function Pr(st | ht) is invariant for any t."}, {"title": "B.3 Proof", "content": "Proof. We begin by constructing the History-MDP associated with a group-invariant POMDP and showing that it is itself a group-invariant MDP. The transition and reward functions of the History-MDP are shown in Eq. (5) and satisfy the group invariance properties.\nFor this proof, it is simpler to express the history transition function as T(h, a, h') = Pr(o | h, a), where o is the observation (if any exists) s.t. h' = hao. If no such observation exists, then\n$T(h,a,h')$ = 0 is trivially invariant. If it does exist, then it is necessarily the last observation of h',\n$\\bar{T}(gh, ga, gh') = Pr(go | gh, ga) = \\sum\\limits_{s,s'} Pr(s | gh) Pr(s' | s, ga) Pr(go | ga, s')$\nsince g permutes the elements of S, we can re-index using s = gs and s' = gs',\n$\\qquad = \\sum\\limits_{s,s'} Pr(gs | gh)T(gs' | gs, ga)O(go | ga, gs') = \\sum\\limits_{s,s'} Pr(s | h)T(s' | s,a)O(o | a, s') = T(h,a,h').$\\n(11)\nBy using s = gs, we proceed similarly for history rewards,\n$\\bar{R}(gh, ga) = \\sum\\limits_{S} Pr(s | gh)R(s, ga) = \\sum\\limits_{S} Pr(gs | gh)R(gs, ga) = \\sum\\limits_{S} Pr(s | h)R(s, a) = R(h, a) .$\\n(12)\nTherefore, T(h, a, h') and R(h, a) are invariant, and History-MDPs are group-invariant MDPs. By the theory developed in [1], this implies that the optimal Q-value function Q* (h, a) is invariant and that there exists at least one equivariant deterministic optimal policy \u03c0*(h). Moreover,\n$V*(gh) = Q*(gh, \u03c0*(gh)) = Q*(gh, g\u03c0*(h)) = Q*(h, \u03c0*(h)) = V*(h),$\n(13)\nthis ends our proof by showing that V*(h) is invariant."}, {"title": "C Environment Details", "content": "C.1 Grid-world Domains\nC.1.1 CarFlag-1D\n\u2022 Action: Go-Left or Go-Right\n\u2022 Observation (Discrete): The position of the car, the side of the green flag (-1 or 1 if the car is at the blue flag, and 0 otherwise)\n\u2022 Reward: step reward: -0.01, reaching the green flag: 1.0, and reaching the red flag: -1.0\n\u2022 Episode Initialization: The car is randomized such that it is not at the information location (blue flag). The goal (green flag) is always either at the leftmost or rightmost end. The red flag is on the opposite end\n\u2022 Episode Termination: Reaching either flags or an episode lasts more than 50 timesteps\n\u2022 World size: The distance between the red and the green flag is 50"}, {"title": "C.1.2 CarFlag-2D", "content": "\u2022 Action: Right/Left/Up/Down\n\u2022 Observation: The observation is encoded as an N \u00d7 N \u00d7 2 image, where N is the grid size, the first channel encodes the car's position, and the second channel encodes the position of the green cell. The second channel is only informative when the agent is inside the information region (blue)\n\u2022 Reward: Reaching the green cell: 1.0, otherwise 0.0\n\u2022 Episode Initialization: The agent and the goal cell are randomized such that the minimum distance between them is at least two steps. Moreover, both the agent and the goal are not initialized inside the information region (blue)\n\u2022 Episode Termination: Reached the goal or an episode lasts more than 50 timesteps"}, {"title": "C.2 Robot Manipulation Domains", "content": "An episode is terminated for these domains when it lasts over 50 timesteps or the task is achieved. Because all robot domains share the same observation and action, we only describe them below.\nAction. An action a = (\u03b4\u03c9, \u03b4\u03b1, \u03b4\u03b7, \u03b4z, dr), where \u03b4\u03c9 \u2208 [0, 1] is the absolute openness of the gripper (0: fully open, 1: fully closed), \u03b4x,y,z \u2208 [-0.05, 0.05] are the displacements of the gripper in the X, Y, and Z axis, and \u03b4, \u2208 [-\u03c0/8, \u03c0/8] is the angular rotation around the Z axis (see Fig. 14a)\nObservation. An observation is a top-down depth image taken from a camera located at the end-effector. Specifically, an observation o = (I,k), where I \u2208 []R84\u00d784 is the depth image and k \u2208 {1,0} indicates the current holding status of the gripper. I and k are combined to create a unified"}, {"title": "C.2.1 Block-Picking", "content": "\u2022 Reward: A reward of 1.0 only when the movable block is picked and brought higher than 8cm\n\u2022 Episode Initialization: The poses of the two blocks are randomized. The arm is initialized at a fixed pose\n\u2022 Expert Generation: An expert (a planner with access to all object poses) randomly chooses one block to pick. If the expert picks the movable block, it will bring the block up to achieve the task. Otherwise, the expert keeps trying for several timesteps before switching to pick the movable block to achieve the task"}, {"title": "C.2.2 Block-Pulling", "content": "\u2022 Reward: A reward of 1.0 only when two blocks are in contact\n\u2022 Expert Generation: An expert randomly chooses one block to pull towards the other block. If the block is pullable, it will be pulled towards the other block to achieve the task. Otherwise, the expert keeps trying for a while before pulling the other block"}, {"title": "C.2.3 Block-Pushing", "content": "\u2022 Reward: A reward of 1.0 only when the pushable block is within 5cm from the center of the goal pad. The agent additionally receives a penalty of 0.1 per timestep if it changes the height of the movable block by 5mm to prevent picking the block instead of pushing it"}, {"title": "C.2.4 Drawer-Opening", "content": "\u2022 Reward: A reward of 1.0 only when the unlocked drawer is opened more than 5cm\n\u2022 Episode Initialization: Two drawers are randomly placed next to each other with the same heading angle\n\u2022 Expert Generation: An expert randomly chooses one drawer to open. If it chooses the unlocked drawer, it will then open the drawer to achieve the task. Otherwise, the expert keeps opening the unlocked drawer several timesteps before opening the other drawer"}, {"title": "D Implementation Details", "content": "D.1 Network Structure of Equivariant Recurrent A2C (Equi-RA2C)\nFig. 16 shows the specific architecture of Equi-RA2"}]}