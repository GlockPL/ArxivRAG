{"title": "Equivariant Reinforcement Learning under Partial Observability", "authors": ["Hai Nguyen", "Andrea Baisero", "David Klee", "Dian Wang", "Robert Platt", "Christopher Amato"], "abstract": "Incorporating inductive biases is a promising approach for tackling challenging robot learning domains with sample-efficient solutions. This paper identifies partially observable domains where symmetries can be a useful inductive bias for efficient learning. Specifically, by encoding the equivariance regarding specific group symmetries into the neural networks, our actor-critic reinforcement learning agents can reuse solutions in the past for related scenarios. Consequently, our equivariant agents outperform non-equivariant approaches significantly in terms of sample efficiency and final performance, demonstrated through experiments on a range of robotic tasks in simulation and real hardware.", "sections": [{"title": "1 Introduction", "content": "A key challenge in robot learning is to improve sample efficiency, i.e., to reduce the number of experiences or demonstrations needed to learn a good policy. One way to do this is to identify domain symmetries that can structure the policy space. Recent works have demonstrated that symmetry-preserving (equivariant) neural network models are a particularly effective way of accomplishing this [1, 2, 3, 4, 5]. However, these works have focused primarily on fully observable Markov decision processes (MDPs) rather than partially observable systems encoded as partially observable MDPs (POMDPs) [6]. The question arises whether symmetric neural models can also be used to solve Partially Observable Reinforcement Learning (PORL) problems. This paper identifies the theoretical conditions under which this is indeed the case and describes an equivariant recurrent model that works well in practice.\nTo motivate, Fig. 1 illustrates the Drawer-Opening problem where a robot is presented with a chest containing two drawers, one locked and one unlocked. To solve this task, the robot must determine which drawer is unlocked and then open that drawer, relying only on top-down image observations. This task reflects a common POMDP when physical properties (whether a drawer is unlocked) are hidden from the visual input. The only way for the robot to distinguish between the two drawers is to attempt to open one of them. This is a classic feature of a POMDP \u2013 that the agent must perform information gathering actions to obtain information needed to solve the task. Notice that this problem is rotationally symmetric in the sense that its optimal solution (the blue end-effector"}, {"title": "2 Related Works", "content": "Learning under Partial Observability Unlike classical planning-based methods [9, 10, 11] that impractically require the complete dynamics of the environment, learning-based methods [12, 13, 14, 15, 16, 17, 18, 19] utilize recurrent versions of common reinforcement learning (RL) algorithms for policy learning by directly interacting with the environment. To speed up learning, some methods leverage privileged information assumed available during training, such as the states, the belief about the environment states, or the fully observable policy [20, 21, 22, 23], which are orthogonal to our approach. Only a few prior works exploited domain symmetries under partial observability. Kang and Kim [24], Doshi and Roy [25] leveraged the invariance of the value function of some POMDPS given a state permutation and experimented on a classical planning-based method [11] with the above limitations. Recently, Muglich et al. [26] used equivariant networks to enforce symmetry when multiple agents coordinate. In contrast, we use model-free RL agents in a single-agent setting.\nEquivariant Learning Equivariant networks have been successfully applied to a range of tasks such as point cloud analysis [27] and molecular dynamics [28, 29]. A common approach is to build networks with group equivariant convolutions [30] which are equivariant to arbitrary symmetry groups, such as 2D [31, 32] and 3D [27, 33, 34, 35] transformations. Recently, for MDPs, equivariant networks have been applied to robotics [2, 1, 5] and reinforcement learning [36, 37] to improve sample efficiency. Closest to our work is [1], which formalized group-invariant MDPs and used equivariant networks to perform robotic manipulation tasks. In contrast, this work extends equivariant reinforcement learning to partially observable environments, resulting in a new theory and method.\nEquivariance v.s. Data Augmentation Both methods leverage known domain symmetry to improve learning, but in different ways. On the one hand, data augmentation artificially expands the training data distribution with transformed versions of the data using the symmetry (e.g., rotating, cropping, or translating images [38, 39]); then training a non-equivariant model. On the other hand, an equivariant approach bakes the domain symmetry directly into the model's weights, so an equivariant model can automatically generalize across input transformations even before training. Compared to an equivariant approach, a model trained using data augmentation alone is often less sample efficient [32, 34], generalizes worse [40], and requires a bigger architecture and longer training time for the same performance due to the extra work of learning symmetry injected in the data."}, {"title": "3 Background", "content": "Here, we review some background about POMDPs, some specific group theories used in our work, and finally, the basis of our approach the framework of group-invariant MDPs [1]."}, {"title": "3.1 Partially Observable Markov Decision Processes", "content": "A POMDP is defined by a tuple (S, A, \u03a9, b\u2080, T, R, O), where S, A, and \u03a9 are the state space, the action space, and the observation space, respectively. b\u2080 \u2208 \u2206^S is the starting state distribution (a.k.a."}, {"title": "3.2 C\u2099 and SO(2) Symmetry Groups", "content": "In this work, we are mainly concerned with the symmetry group G = SO(2) of continuous planar rotation, defined as SO(2) = {Rot_\u03b8 : 0 < \u03b8 < 2\u03c0}. For a reduced computation complexity, we use the cyclic subgroup C\u2099 < SO(2) to approximate SO(2), which is defined as C\u2099 = {Rot_\u03b8: \u03b8 \u2208 {\\frac{2\u03c0i}{n} | 0 < i < n}}. In other words, C\u2099 defines n rotations (i.e., group elements), which are multiples of \\frac{2\u03c0}{n}. For instance, C\u2084 = {0, \u03c0/2, 2\u03c0/2, 3\u03c0/2} and C\u2088 = {0, \u03c0/8, ..., 6\u03c0/8, 7\u03c0/8}."}, {"title": "3.3 Group Representations", "content": "A group representation is a mapping from a group G to a d-dimensional general linear (GL) group, i.e., \u03c1 : G \u2192 GL_d by assigning each group element g \u2208 G with an invertible matrix \u03c1(g) \u2208 \u211d^(d\u00d7d).\nWhen G = C\u2099, the effect of a rotation g \u2208 C\u2099 on a signal x (i.e., gx) starts with a pixel-wise rotation \u03c1_f(g)\u207b\u00b9x (with a fixed group representation \u03c1_f), followed by a channel-wise rotation, i.e., gx = \u03c1(g)(\u03c1_f(g)\u207b\u00b9x) (with the choice of group representation \u03c1). In this work, we consider three choices of the channel-wise representation \u03c1:\nTrivial Representation (\u03c1 = \u03c1_t): For \u2200g \u2208 G, \u03c1_t associates g with an identity matrix. For example, in Fig. 2a when g is a \u03c0/2 counter-clockwise (CCW) rotation, and x is a 1-channel feature map, \u03c1_f rotates the pixels of x while \u03c1_t does not change the pixel values (i.e., the colors are unchanged).\nStandard Representation (\u03c1 = \u03c1_s): For \u2200g \u2208 G, \u03c1_s associates g with a rotational matrix, i.e., \u03c1_s(g) = g. As in Fig. 2b, when g is a \u03c0/2 CCW rotation and x is a vector field input, \u03c1_f rotates the positions of vectors (denoted as colored arrows), and \u03c1_s rotates their orientations.\nRegular Representation (\u03c1 = \u03c1_r): For each g \u2208 G, when acting on an input x, \u03c1_r will cyclically permute the coordinates of x. Fig. 2c illustrates when g is a \u03c0/2 CCW rotation and x is a 2-channel feature map, \u03c1_f rotates each channel's pixels and \u03c1_r permutes the orders of the channels.\nAn Illustrative Example Combining the group and the group representation fully characterizes how a signal will be transformed. For an illustrative example in a grid-world domain, see Appendix A."}, {"title": "3.4 Equivariance, Invariance, and Group-invariant MDPS", "content": "Given \u03c6: X \u2192 Y and a symmetric group G that acts on X and Y, we say that \u03c6 is G-equivariant if \u03c6(gx) = g\u03c6(x), and G-invariant if \u03c6(gx) = \u03c6(x). For the remainder of this document, we drop the prefix G and simply refer to these properties as invariance and equivariance."}, {"title": "4 Group-Invariant POMDPS", "content": "In this section, we extend the ideas from [1] to POMDPs and identify the basic set of assumptions that a POMDP needs to satisfy to have analogous invariance properties. We also note that while other assumptions might also lead to an invariant POMDP, ours are probably the most natural.\nDefinition 1. We say a POMDP PG = (S,A, \u03a9, b\u2080, T, R, O) is group-invariant with respect to group G if it satisfies the following invariant properties for all g \u2208 G:\n$$\nT(gs, ga, gs') = T(s,a,s') \\\\\nR(gs, ga) = R(s, a) \\\\\nO(ga, gs', go) = O(a, s', o) \\\\\nb\u2080(gs) = b\u2080(s) .\n$$\nThis extends the definition of the group invariant MDP from [1] by incorporating additional constraints on the observation function and the initial belief distribution. We also extend the group operations on histories.\nDefinition 2. Group operation g acts on history h_t according to gh_t := (go\u2080, ga\u2080,..., ga_(t-1), go_t).\nFinally, we show that group-invariant POMDPs exhibit similar properties and benefits as group-invariant MDPs.\nTheorem 1. A group-invariant POMDP has an invariant optimal Q-function Q*(gh,ga) = Q*(h, a), an invariant optimal value function V*(gh) = V*(h), and at least one equivariant deterministic optimal policy \u03c0*(gh) = g\u03c0*(h).\nProof. See Appendix B.\nThe above analysis allows us to constrain the value function and policy for a G-invariant POMDP to be invariant and equivariant, respectively, without eliminating optimal solutions."}, {"title": "5 Equivariant Actor-Critic RL for POMDPS", "content": "In this section, we introduce an equivariant agent that directly exhibits the desired properties of the optimal value function and policy, backed by the analysis in Theorem 1. Fig. 3 shows the agent,"}, {"title": "5.1 Equivariant Modules", "content": "We describe the details of the equivariant modules within our agent below, with the core components being equivariant CNNs [45, 32]. For the implementation details, please see Appendix D."}, {"title": "Equivariant Feature Extractor", "content": "This module takes observations or actions and outputs intermediate features for further processing. It comprises multiple equivariant CNN components chained sequentially as shown in Fig. 4a. Its input representation is the observation representation po or the action representation pa. The intermediate and output representations are chosen to be the regular representation pr, which empirically outperforms other representations [32]. The input representation can be a single representation type or mixed, i.e., a sum of different representations. A mixed representation is necessary when the input has different components that transform differently under a group transformation, e.g., one component rotates with the transform, and one component is unchanged. Such a case can be seen in Appendix D.2 and is simplified in Fig. 4b, where p_a = p_s + p_t."}, {"title": "Equivariant Actor Outputter", "content": "Fig. 4c shows the input representation is regular because the signal coming in (the output of the RNN and the observation feature extractor modules) uses a regular representation. The output representation varies depending on the action type (discrete/continuous) and how a group transformation will affect an action. For discrete actions, the module produces a categorical distribution over the action space. In this case, the output representation is the regular one pr as we want to change the (discrete) action when the history is transformed (see Appendix A for an illustration). For continuous actions, this module outputs the means with some representation p\u00b5 and the standard deviations of actions with some representation po (as in A2C [7], PPO [46], or SAC [8]). The representations used for p\u00b5 and po are mixed, as each action component might change differently under a group transformation (see Appendix D.2)."}, {"title": "Equivariant Critic Outputter", "content": "Because the optimal critic is invariant, this module (Fig. 4d) uses the trivial representation pt at the output to keep the output the same under a group transformation. Its input representation is regular, enforced by the output of the RNN and the action feature extractor."}, {"title": "Equivariant Recurrent Neural Network", "content": "This is our contribution needed for constructing a POMDP equivariant agent (there is another similar component in [26], but only model weights are released). We utilize an LSTM [47] for this module, but the approach can also be modified for other types of RNNs. Specifically, given an input x_t (e.g., the concatenated obs-action feature) and the previous hidden state h_(t-1), the input gate i_t, the forget gate f_t, the memory cell candidate g_t, and the output o_t are computed as follows with Ws and bs being learnable weights and biases:\n$$\ni_t = sigmoid(W_{xi}x_t + W_{hi}h_{t-1} + b_i) \\\\\nf_t = sigmoid(W_{xf}x_t + W_{hf}h_{t-1} + b_f) \\\\\no_t = sigmoid(W_{xo}x_t + W_{ho}h_{t-1} + b_o) \\\\\ng_t = tanh(W_{xg}x_t + W_{hg}h_{t-1} + b_g).$$\nThe above equations do not make an equivariant RNN module. To enforce the equivariance, we compute all equations at once using an equivariant CNN module (Fig. 5), similar to the ConvLSTM"}, {"title": "6 Experiments", "content": "We compare the performance of learning agents on two grid-world domains (discrete actions and feature-based observations) and four robot domains (continuous actions and pixel observations)."}, {"title": "6.1 Domains", "content": "We briefly describe our domains below. Please refer to Appendix C for more specific details."}, {"title": "6.1.1 Grid-World Domains", "content": "There are two versions of CarFlag [49] in Fig. 6, where an agent must reach a goal (green), whose position is visible only when the agent visits an unknown information region. For instance, in CarFlag-1D, the agent must visit the central blue flag to get the side (left/right) of the goal; or in CarFlag-2D, the agent must visit the central blue region to see the coordinate of the goal cell. We also illustrate the domain symmetry in the figure: in these domains, when the starting position and the goal location are transformed (flipped in CarFlag-1D or rotated by \u03c0/2 radians clockwise in CarFlag-2D, the optimal trajectories will be transformed similarly, i.e., black \u2192 blue trajectories."}, {"title": "6.1.2 Robot Manipulation Domains", "content": "Fig. 6 shows our robot manipulation domains (extended from the BulletArm suite [50]), where a robot arm must perform individual manipulation tasks (i.e., picking, pulling, pushing, and opening) using top-down depth images to win a sparse reward. In these domains, only one object is manipulable, but both objects are the same if only relying on the current image. Therefore, the agent must actively check the objects' mobility and remember past interactions with the objects to determine the next action. Specifically, in Block-Picking, the agent needs to pick the movable block up. In Block-Pulling, the agent needs to pull the movable block to be in contact with the other block. In Block-Pushing, the goal is to push the movable block to a goal pad. In Drawer-Opening, the agent is tasked to open an unlocked drawer between a locked and unlocked one.\nIn these domains, the transition function is invariant because the Newtonian physics applied to the interaction is invariant to the location of the reference frame. The reward function is invariant by definition. Using top-down depth images makes the observation function invariant. If the initial belief is assumed invariant, then according to Definition 1, these domains are group-invariant POMDPs."}, {"title": "6.2 Agents", "content": "We compare our proposed agents (instances of the structure in Fig. 3 applied to A2C [7] and SAC [8]) against a diverse set of baselines, including on-policy/off-policy, model-based/model-free, and generic/specialized POMDP methods (see Appendix D and Appendix E for more details)."}, {"title": "6.2.1 Grid-world Domains", "content": "RA2C [51] is a recurrent version of A2C [7]. Equi-RA2C is our proposed architecture applied to A2C. DPFRL [14] is a state-of-the-art model-based POMDP baseline where an A2C agent is given features produced by a differentiable particle filter. DreamerV2 [52] and DreamerV3 [53] are strong model-based methods that learn a recurrent world model, thus, can work with POMDPs.\nNo Data Augmentation for All Since all methods are on-policy algorithms, augmented data using the domain symmetry only becomes on-policy only for Equi-RA2C dues to its unique symmetry-awareness. Therefore, for a fair comparison, we do not perform any data augmentation."}, {"title": "6.2.2 Robot Manipulation Domains", "content": "While on-policy RA2C or Dreamer-v2 can handle continuous action spaces in these domains, there is no clear way to leverage expert demonstrations necessary to efficiently solve the robot manipulation tasks with sparse rewards in Fig. 6. Thus, we switch to SAC [8] as the base RL algorithm, where we can pre-populate its replay buffer with demonstration episodes. In our experiments, RSAC [13] is a non-equivariant recurrent SAC agent. Equi-RSAC is our proposed method applied to SAC. We also compare with recurrent versions of two strong data augmentation baselines: RAD-Crop-RSAC [54] and DrQ-Shift-RSAC [55]. These specific data augmentation techniques, i.e., random cropping and shifting (see Appendix G for visualizations), are chosen among others because they were reported to perform best [55]. To train RAD-Crop-RSAC, for each training episode, an auxiliary episode is created by using the same random cropping for every depth image inside the original episode. DrQ-Shift-RSAC applies two random shifts to each depth image in a training episode to create two. The Q-target and the Q-values are then computed by averaging the values computed on the two episodes. Finally, SLAC [56] learns a latent model from pixels and then uses SAC on the latent space by using the observation-action history (instead of the latent state) for the actor and the latent state samples for the critic. This enables SLAC to scale to more difficult tasks.\nDemonstrations + Rotational Data Augmentation for All All replay buffers are pre-populated with 80 expert episodes to overcome the reward sparsity. Moreover, we augment the training data by applying the same random rotation for every action and observation inside a training episode (see Appendix G for visualizations). Note that these rotational data augmentations are applied in addition to the existing data augmentation techniques in RAD-Crop-RSAC and DrQ-Shift-RSAC."}, {"title": "6.3 Results", "content": "Grid-world Domains Fig. 7 shows that Equi-RA2C is significantly more sample efficient than the baselines. Moreover, the dominance of our method is also seen with variants of these domains with different sizes (see Appendix I). DPFRL did not perform well potentially because of the reward sparsity, which was also previously reported in [57]. DreamerV2 and DreamerV3 also perform poorly, even with many more learnable parameters of the models, potentially indicating that learning a good model under partial observability and sparse rewards might be more challenging than in the domains originally tested. For instance, most Atari games and locomotion tasks in the DeepMind Control suite [58] have low levels of partial observability and provide dense rewards.\nRobot Manipulation Domains Clearly from Fig. 7, Equi-RSAC strongly outperforms other baselines in all domains, with itself being the only agent that can reach a satisfactory performance. Across all domains, without the equivariant LSTM module (denoted as Non-Equi-RSAC (LSTM), which can be considered as a naive extension of [1]), the performance degrades significantly, even though it starts pretty well. SLAC, surprisingly, performs the worst. A possible reason is that SLAC"}, {"title": "6.4 Using Equivariant Models on Domains with Imperfect Symmetry", "content": "We investigate the performance when the perfect symmetry does not hold in asymmetric variants of CarFlag, created by offsetting the information region a distance d from the world center (see Appendix C). From the final success rates shown in Table 1 (see Appendix I.1 for learning curves), we can see that equivariant Equi-RA2C still outperforms non-equivariant RA2C when the domains are close to perfect symmetry, i.e., when d = d\u2081D = 5 in CarFlag-1D or d = d\u2082D = 1,2 in CarFlag-2D. However, a bigger symmetry gap might lead to the sub-optimality of equivariant agents. As evidence, Equi-RA2C performs worse than RA2C when d = 10, -10 in CarFlag-1D."}, {"title": "6.5 Zero-shot Transfers to Real Hardware", "content": "Because only our agents can perform well in simulation, we transfer their best policies in simulation to a UR5 robot (see Fig. 8). We combine the point clouds from two side-view cameras to create a top-down depth image using a projection at the gripper's position. We roll out 50 episodes, divided equally into test cases when the agents first manipulate the immovable or movable objects. Table 2 shows that the learned policies can be zero-shot transferred well in the real world regardless of small performance drops in all domains (see our supplementary video for policy visualizations). The biggest performance drop is in Draw-Opening, in which the transferred policies sometimes clumsily move one drawer far away from the other, creating a novel scene never seen in simulation."}, {"title": "7 Conclusion and Limitations", "content": "Conclusion In this work, we introduced group-invariant POMDPs and proposed equivariant actor-critic RL agents as an effective solution method. Through extensive experiments, our proposed equivariant agents can tackle realistic and challenging robotic manipulation domains much better than non-equivariant approaches with learned policies zero-shot transferable to a real robot.\nLimitations A limitation of most equivariant approaches, including ours, is the requirement of imperfect symmetry, which might be present when images are affected by non-symmetric factors, e.g., side view instead of top-down view or asymmetric noises. Fortunately, under full observability, recent empirical [60, 61] and theory work [62] show that an equivariant model can still outperform non-equivariant approaches in many such cases. Together with the results in Section 6.4, our approach might still perform better than unstructured agents even under imperfect symmetry."}, {"title": "A Illustration of Group and Group Representation in CarFlag-2D", "content": "Domain We consider a small version of CarFlag-2D (see Fig. 9) with a grid size of 3x3, where the agent (red) must navigate to an unknown target cell (green) in a grid world. The agent can always observe its current location but only observe the target cell when it visits the information cell (blue), which is also unknown to the agent.\nObservation The observation is a two-channel image size 2x3x3, where the first channel encodes the agent's location and the second encodes the target location. The values of the second channel are non-zero only when the agent is at the information cell (Fig. 9).\nActions Movements in four directions (the location does not change if going out of the world)."}, {"title": "B Proof of Theorem 1", "content": "In this section, we introduce the framework of history representation MDP [63] and prove a supporting lemma before arriving at the proof."}, {"title": "B.1 History Representation MDP", "content": "A POMDP can be converted into a history representation MDP (HR-MDP) [63] whose state is a sufficient statistic of the POMDP history for control, e.g., the well-known Belief-MDP [64] construct is a special case of an HR-MDP based on the belief representation. Useful representations such as the belief might require a known POMDP model; however, we adopt a model-free approach with no such knowledge and, therefore, use the trivial identity representation whereby the history is represented by itself. This effectively converts the POMDP into an equivalent History-MDP, which is defined by the tuple (H, A, T, R), where:\n$$\nT(h, a, h') = E_{o\\vert h,a} [I\\{{h' = hao}\\}] \\\\\nR(h, a) = E_{s\\vert h} [R(s,a)],\n$$\nwhere I{\\cdot} is the indicator function, and\n$$\nPr(o\\vert h, a) = E_{s\\vert h} \\sum_{s'}T(s, a, s')O(a, s', o)\n$$\n$$\nPr(s' \\vert h') \\propto E_{s\\vert h} [T(s, a, s')] O(a, s', o) .\n$$"}, {"title": "B.2 Supporting Lemma", "content": "Lemma 1. The belief function of a group-invariant POMDP (as defined by Definition 1) is group-invariant,\n$$\nPr(gs | gh) = Pr(s | h) .\n$$\nProof By Induction.\nBase Case. We first prove that the belief after the first observation is invariant. We note here that the observation function for the first timestep takes the form O(s, o), with no preceding action.\n$$\nPr(gs\u2080 | go\u2080) \\propto b\u2080(gs\u2080)O(gs\u2080, go\u2080) = b\u2080(s\u2080)O(s\u2080, o\u2080) \\propto Pr(s\u2080 | o\u2080).\n$$\nSince Pr(gs\u2080 | go\u2080) and Pr(s\u2080 | o\u2080) are both proportional to the same quantity, and they are both normalized to be distributions over states, then they are themselves equal.\nInductive Step. We then prove that if Pr(st | ht) is invariant, then Pr(st+1 | ht+1) is also invariant. Per Eq. (7),\n$$\nPr(gs_{t+1}|gh_{t+1}) \\propto Pr(gs_t | gh_t)T(gs_t, ga_t, gs_{t+1})O(ga_t, gs_{t+1}, go_{t+1})\n$$\n$$\n= Pr(s_t | h_t)T(s_t, a_t, s_{t+1})O(a_t, s_{t+1}, o_{t+1}) \\propto Pr(s_{t+1} | h_{t+1}).\n$$\nSince Pr(gs_{t+1} | gh_{t+1}) and Pr(s_{t+1} | h_{t+1}) are both proportional to the same quantity, and they are both normalized to be distributions over states, then they are themselves equal. By induction, given the base case and the inductive step, the belief function Pr(st | ht) is invariant for any t."}, {"title": "B.3 Proof", "content": "Proof. We begin by constructing the History-MDP associated with a group-invariant POMDP and showing that it is itself a group-invariant MDP. The transition and reward functions of the History-MDP are shown in Eq. (5) and satisfy the group invariance properties.\nFor this proof, it is simpler to express the history transition function as T(h, a, h') = Pr(o | h, a), where o is the observation (if any exists) s.t. h' = hao. If no such observation exists, then"}, {"title": "C Environment Details", "content": "Action. An action a = (\u03b4\u03c9, \u03b4x,y,z, \u03b4r), where \u03b4\u03c9 \u2208 [0, 1] is the absolute openness of the gripper (0: fully open, 1: fully closed), \u03b4x,y,z \u2208 [-0.05, 0.05] are the displacements of the gripper in the X, Y, and Z axis, and \u03b4r \u2208 [-\u03c0/8, \u03c0/8] is the angular rotation around the Z axis (see Fig. 14a)"}, {"title": "D Implementation Details", "content": "D.1 Network Structure of Equivariant Recurrent A2C (Equi-RA2C)\nFig. 16 shows the specific architecture of Equi-RA2C used in CarFlag domains. Because the actions can be inferred from the observations in these domains, we do not include the feature extractor for the previous actions. We also omit the skip-connections. The input representation is some representation of the observation po, depending on the domains (see below).\nD.2 Network Structure of Equivariant Recurrent SAC (Equi-RSAC) with C4 Group\nFig. 19 shows the details of Equi-RSAC used in the robot manipulation domains with the C4 group. The input representation is mixed for the action feature extractor because the action input has components that transform differently under a rotation. Specifically, given an action \u03b1 = (\u03b4\u03c9, \u03b4x,y, \u03b4z, \u03b4r), the trivial representation pt is chosen for the \u03b4\u03c9, \u03b4z, \u03b4r components (which should be unchanged under the rotation). In contrast, the standard representation ps is chosen for the lateral components (dx, dy), which should rotate. For the same reason, for the actor outputter, Pu is mixed, i.e., the trivial representations pt are used for the w, z, r components, and ps is used for the x, y components.\nD.3 Implementation Using The ESCNN Library\nGiven the definition of each equivariant component above, we can easily implement it with escnn. For instance, the following PyTorch [66] code defines the observation feature extractor in Fig. 18a with ReLU as a non-linearity component:\nD.4 Training Details\nWe implement using PyTorch. The batch size for all agents is 32 (episodes). The replay buffer has a capacity of 100,000 transitions. We use the Adam optimizer [67] with a learning rate of 3e-4 for actors and critics and 1e-3 for optimizing \u03b1 for SAC-based agents. The target entropy H for SAC-based agents is -dim(A) followed the common practice, and \u03b1 is initialized at 0.1. After prepopulating the replay buffer with 80 expert episodes, the buffer is filled with 20 episodes with random actions. We use the same 1:1 environment/gradient step ratio for all agents.\nD.5 Implementing Equivariant LSTM\nWe implement the equivariant LSTM [47] based on a public code of ConvLSTM [48] at https://github.com/Hzzone/Precipitation-Nowcasting as the authors did not release the official code."}, {"title": "E Baseline Details", "content": "RA2C [51] We modified the code at https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail. We used 16 environments in parallel and used recurrent policies. Other hyper-parameters are kept at default.\nDPFRL [14] We used the authors' code at https://github.com/Yusufma03/DPFRL. We used 30 particles, MGF particle aggregation type, and the hidden dimension is 128.\nRAD [54] We collected depth images of size 90x90 to perform random cropping to reduce the size to 84x84. We perform the same type of random cropping for every depth image within an episode.\nDrQ [55] We used random shift of \u00b14 pixels as suggested by the original work. The same type of shifting is used for every depth image within a sequence. We also followed the authors' suggestions when using the numbers of augmentations for calculating the Q-targets, and the Q-values are K = 2 and M = 2, respectively.\nSLAC [56] We used a Pytorch implementation at https://github.com/toshikwa/slac. pytorch, which has been benchmarked against the performance reported in the original paper. We pre-train the latent variable model for 2k steps before iterating between data collection, model up-date, and evaluation. We also pre-fill the replay buffer with the same number of expert and random episodes before training and use four extra augmented episodes for each episode during training to ensure a fair comparison. The sequence length is extended from 8 (originally) to 50 (maximum episode length). We varied the sequence length for better performance, but the performance did not improve much. For any episode shorter than 50 steps, we zero-pad dummy transitions in front.\nDreamerV2 [52] We used the official code at https://github.com/danijar/dreamerv2. For CarFlag domains, we mainly keep the default hyper-parameters (suggested by the authors). In CarFlag-2D, the observation image is extended to have the size of 64\u00d764\u00d73 by zero-padding around the original image and is added with a dummy channel (all zero).\nDreamerV3 [53] We used the official code at https://github.com/danijar/dreamerv3 and performed similar steps like in the case of DreamerV2. We used the small world models with about 18M trainable parameters (predefined in the repo's configuration file) for our CarFlag domains."}, {"title": "H Ablation Studies", "content": "H.1 Equivariant Actor or Critic Only\nIn Fig. 24, we additionally show the learning performance when only either actor or critic is equivariant in Block-Pushing and Drawer-Opening. From the figure, having an equivariant critic (purple) is more beneficial than having an equivariant actor (blue). However, having both being equivariant (green) yields"}]}