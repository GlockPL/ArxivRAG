{"title": "RRADistill: Distilling LLMs' Passage Ranking Ability for Document Re-Ranking of Long-Tail Queries in a Search Engine", "authors": ["Nayoung Choi", "Youngjune Lee", "Gyu-Hwung Cho", "Haeyu Jeong", "Jungmin Kong", "Saehun Kim", "Keunchan Park", "Jaeho Choi", "Sarah Cho", "Inchang Jeong", "Gyohee Nam", "Sunghoon Han", "Wonil Yang"], "abstract": "Large Language Models (LLMs) excel at understanding the semantic relationships between queries and documents, even with lengthy and complex long-tail queries. These queries are challenging for feedback-based rankings due to sparse user engagement and limited feedback, making LLMs' relevance ranking ability highly valuable. However, the large size and slow inference of LLMs necessitate the development of smaller, more efficient Small Language Models (SLMs). Recently, integrating ranking label generation into distillation techniques has become crucial, but existing methods underutilize LLMs' capabilities and are cumbersome. Our research, RRADistill (Re-Ranking Ability Distillation), propose an efficient label generation pipeline and novel SLM training methods for both encoder and decoder models. We introduce an encoder-based method using a Term Control Layer to capture term matching signals and a decoder-based model with a ranking layer for enhanced understanding. Experimental results including A/B testing on NAVER, South Korea's leading search platform, demonstrate effectiveness of our approach in re-ranking for long-tail queries.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as ChatGPT (OpenAI, 2022) and GPT-4 (Achiam et al., 2023), have shown remarkable potential across diverse search tasks, including query rewriting (Mao et al., 2023; Dhole and Agichtein, 2024), query and document expansions (Wang et al., 2023a; Mackie et al., 2023; Ma et al., 2023a). As LLMs advance in complex tasks, they also show potential for passage ranking, which involves understanding relationships between queries and multiple documents. Recent studies (Qin et al., 2024; Ma et al., 2023b; Zhuang et al., 2024; Pradeep et al., 2023a) show that instruction-tuned LLMs like GPT-4, Flan-T5 (Chung et al., 2024) and Vicuna (Chiang et al., 2023) effectively handle passage ranking in zero-shot settings. Motivated by these studies, we also conducted zero-shot re-ranking with our in-house LLMs, HyperCLOVA X (HCX) (Yoo et al., 2024) which comprises Large (HCX-L) and Small (HCX-S) variants. We put a query and a list of document snippet texts to HCX, using a Korean translation of list-wise prompt from RankGPT (Sun et al., 2023). We found that HCX-L effectively returns document identifiers in the desired order, excluding low-relevance ones, as depicted in Figure 1. Unlike short-head queries, which are popular and short like keywords, long-tail queries are complex and involve longer, specific phrases (A.1.1). These queries benefit more from relevance than feedback, and semantic than syntactic matching due to their rich semantic content but lack of user feedback. Thus, LLMs' ability to rank complex long-tail queries by relevance is highly valuable.\nHowever, the slow inference speed challenges the direct use of LLMs in search engine. To address this, we trained a much smaller Language Model (SLM) to retain HCX-L's ranking ability, following a trend known as LLM distillation, similar to RankGPT and TWOLAR (Baldelli et al., 2024)."}, {"title": "2 Methodology", "content": "This involves two stages: 1) Generating ranking label using LLMs, and 2) Training the SLM ranker. When generating ranking label with LLMs, previous studies utilize the list-wise permutation generation method, which inputs a query and a set of documents to LLM and receives an ordered list of document identifiers. However, these approach require sliding windows, which infer multiple times on partial lists due to the prompt length constraints of LLMs, causing a burden. Moreover, previous studies viewed the missing phenomenon as a problem, where LLMs fail to include all input documents in the output. However, we observed that in most cases, excluded documents due to missing are significantly irrelevant to the query, making it a valuable signal. Consequently, we reframed the missing and highlighted its impact. We developed our own label generation pipeline to address these issues, including two key techniques: 1) Pre-rank to filter documents, retaining only those effective to train SLM rankers and bypass the sliding window, 2) Consider missing as useful signals, and utilize excluded documents as hard negatives, to train SLM rankers. Our pipeline speeds up labeling and provides compact yet effective training data.\nIn training SLM rankers, we explored both BERT (Devlin et al., 2019) and GPT (Radford et al., 2019) styles, incorporating our training techniques. For BERT ranker, we integrate a term control layer into the training process to utilize specific term matching signals. For GPT ranker, we developed techniques to effectively utilize classification (whether relevant or irrelevant) and reasoning (rationale for relevant or irrelevant) during training, with a light-weight ranking layer. Both rankers incorporate additional training layers, but only specific parts of the model architecture (Encoder plus a classification head for BERT, Decoder plus a dense layer for GPT) are utilized during inference, reducing the burden for service applications.\nIn this paper, we provide various experiments on our BERT ranker (RRA-BERT) and GPT ranker (RRA-GPT), trained to mimic LLMs' relevance ranking, aiming to improve long-tail search quality. We tested the effectiveness of our methodology through rigorous online and offline evaluations."}, {"title": "2.1 Label Generation with LLMs", "content": "First, we sampled 7,000 long-tail queries from NAVER search logs based on length, complex phrasing and frequency criteria, as detailed in A.1.1. Then, we retrieved 50 documents per query with multiple retrievers of NAVER search engine. Given a query q and retrieved documents $D = [d_1, d_2, ..., d_n]$, we ranked D using our pre-ranker. From pre-ranker (Rankerpre), we obtained the top 10 (Dtop10) and bottom 10 (Dbottom10) documents, and labeled them with HCX-L (Yoo et al., 2024) in a list-wise manner. All document inputs are snippet texts. Figure 2 depicts the overview of our label generation pipeline with LLM. Further details, including the pre-ranker (A.1.2) are described in A.1.\n$D' = Rankerpre(D)$\n$D_{top10} = D'[: 10]; D_{bottom10} = D'[-10:]$\n$D_{pre} = D_{top10} \\cup D_{bottom10}$ (1)\n$D_{ranked} = HCXL(Prompt(q, D_{pre}))$\n$D_{excluded} = D_{pre} \\backslash D_{ranked}$\nPrevious study (Sun et al., 2023) has highlighted the missing phenomenon, where LLMs rank only part of the input list, suggesting its frequency varies depending on LLMs. HCX-L also exhibited frequent missing occurrences. However, as shown in Figure 1, we observed that in most cases only relevant documents were included in the output (Dranked), excluding documents with significantly low relevance to the query (Dexcluded). Hence, we reframed the missing as a valuable signal, leveraging excluded documents as hard negatives. Through comparison experiments in Section 3.1.1, training with and without excluded documents, we demonstrated the usefulness of the missing signal. For GPT ranker training, we generated reasoning, which is the rationale for why q and d is relevant or irrelevant, as described in A.1.3."}, {"title": "2.2 BERT-sytle Distillation: RRA-BERT", "content": "Lengthy and complex queries require attention not only to the overall semantic information but also to specific terms that are particularly noteworthy within the query. To address this problem, we propose a novel training approach designed to inject term matching signals between queries and documents as hints into dense representations to effectively enhance performance. BERT-style distillation consists of three components: (1) Token Selection (TS) method to select tokens from the document matched to the query. (2) Term Control Layer (TCL) that utilizes information of selected tokens as hints for training. (3) Optimization that"}, {"title": "2.2.1 Token Selection", "content": "We propose a Token Selection (TS) method that captures terms matching signals between the query and the documents as hints, allowing the model to consider both the overall and specific semantics. We select top-k document tokens that match each query token. The process involves identifying and selecting matched tokens $T_{q,d}$ within document d for a given query q using word embeddings $(Embedding_{word})$, as follows.\n$E_q = Embedding_{word}(Tokenizer(q))$\n$E_d = Embedding_{word}(Tokenizer(d))$\n$Sim_{q,d} = E_qE_d^T$\n$T_{q,d} = {Top_k(Sim_{q,d}[i, :])|i = 1, ..., n_q}$ (2)\nwhere $n_q$ is the number of tokens in q. In this process, we select $k \\times n_q$ tokens from the document, while excluding duplicate tokens."}, {"title": "2.2.2 Term Control Layer", "content": "We propose Term Control Layer (TCL) that effectively integrates the term matching signals into the overall training process. Unlike the overall semantic score (Score1 in Figure 3), TCL utilizes only selected document tokens $T_{q,d}$ to focus on specific tokens. We designed TCL with a multi-head self-attention (Vaswani et al., 2017) mechanism using the last hidden states of encoder as the input, enabling the aggregation of information from each token. The corresponding formula is as follows.\n$H_T = Concat(h_{[CLS]}, h_q, h_{[SEP]}, h_{T_{q,d}})$ (3)\n$TCL(H_T) = Attention_{multi-head}(H_T)$\nwhere $h_{[CLS]}, h_q, h_{[SEP]}$ and $h_{Tq,d}$ represent the last hidden states of [CLS], the query, [SEP] and $T_{q,d}$, respectively. The number of attention heads is 8."}, {"title": "2.2.3 Optimization", "content": "To compute the basic ranking score $S_{base}$ (Score1 in Figure 3), we input the representation $h_{[CLS]}$ to the classification head (CLFhead). Then, we calculate the score $S_{TCL}$ (Score2 in Figure 3) in the same manner, using the TCL-derived $h_{[CLS]}$ which is TCL(HT)[CLS] in equation 3. Both calculations share the same CLFhead. The final relevance score s is obtained as follows.\n$S_{base} = CLF_{head}(h_{[CLS]})$\n$S_{TCL} = CLF_{head}(TCL(H_T){[CLS]})$ (4)\n$S = S_{base} + \\alpha * S_{TCL}$\nwhere $\\alpha$ controls the effects of TCL. Finally, given S, a list of outputs s for n documents, we compute the training loss using RankNet (Burges et al., 2005), a pairwise loss function.\n$L = L_{RankNet}(S); where S = [s_1, s_2,..., s_n]$ (5)"}, {"title": "2.3 GPT-sytle Distillation: RRA-GPT", "content": "Existing GPT and T5 (Raffel et al., 2020) rankers primarily use decoder output as a relevance score, calculating from the decoder logits of either the first generated token or the generated target tokens (e.g., Yes or No). To optimize relevance scores, training tasks typically fall into two types: classification, as implemented in MonoT5 (Nogueira et al., 2020), and ranking, exemplified by RankT5 (Zhuang et al., 2023), TWOLAR, and RankGPT. Furthermore, ExaRanker (Ferraretto et al., 2023), a T5-based ranker, enhances ranking performance by training to generate explanations for relevant or irrelevant. In this paper, we explored which task combinations help GPT ranker training and whether reasoning enhances the ranking performance.\nIn previous studies (Sun et al., 2023; Zhang et al., 2023a; Pradeep et al., 2023b), it was surprising to see that despite GPT's much larger parameter size, its performance often matched or fell behind that of BERT and T5 rankers, which suggests GPT lacks a dedicated input encoding(=understanding) module. To address this, we enhanced GPT ranker with a dense layer, which we call a ranking layer. Selecting the appropriate input for this layer is critical, akin to [CLS] token in BERT, to effectively represent the (q, d) relationship. We tested token embeddings from both the input text and the generated texts, <|Response |> and <|Reason |> respectively, as input to the ranking layer. Our final GPT ranker is depicted in Figure 4."}, {"title": "2.3.1 Ranking layer", "content": "The relevance score s for a (q, d) pair is obtained through a dense layer as follows.\n$X = Tokenizer(Prompt(q, d))$\n$H = Decoder(X)$\n$h_{<|Resp. |>} = H[-1, i]; where i is the index of <|Response |> in X$\n$s = Dense(h_{<|Resp. |>}); where din = dhidden, dout = 1$ (6)\nwhere H refers to all layers of hidden states, and $h_{<|Resp. |>}$ is the last hidden state of a special token. The Dense layer has an input dimension equal to the model's hidden size and outputs a single relevance score. Given a list of outputs s for n documents, where $S = [s_1, s_2,...,s_n]$, the calculation of the loss $L_{RankNet}(S')$, where $S' = MinMaxScaling(S)$, follows the BERT ranker training formula as in Equation 5."}, {"title": "2.3.2 Ranking with classification & reasoning", "content": "We propose leveraging the decoder's generative abilities for ranking layer training. By simultaneously training the decoder to generate labels and reasoning, we enhance the ranking layer. Label"}, {"title": "3 Experiment", "content": "We tested the effectiveness of our label generation and training method, which leverages BERT and GPT structures, alongside following baselines, HCX-L (Yoo et al., 2024), BM25 (Robertson et al., 2009), MonoBERT (Nogueira and Cho, 2019), MonoT5 (Nogueira et al., 2020) and RankGPT (Sun et al., 2023). MonoBERT and MonoT5 utilize our labeled dataset for training. RankGPT's training labels are generated using sliding windows, without our pre-ranking process. While exact parameter sizes of backbones are undisclosed, they follow the order: T5 (small) < BERT << GPT < T5 (large), all of which are below 1 billion parameters. For evaluation, we used our custom NAVER testset along with Korean-translated public testsets for passage re-ranking: MS MARCO (Bajaj et al., 2018), MIRACL (Zhang et al., 2023b), DL19 (Craswell et al., 2020), and DL20 (Craswell et al., 2021). More detailed settings about dataset, metrics, baselines, and implementation are outlined in A.2."}, {"title": "3.1 Results", "content": "Table 1 presents the overall performance. RRA-BERT excels on our long-tail query testset"}, {"title": "3.1.1 RRA-BERT", "content": "We provide an ablation study and analysis of RRABERT addressing three questions: (1) The impact of our label generation method, (2) The effectiveness of Token Selection (TS) and Term Control Layer (TCL), and (3) Inference efficiency. The experimental results are presented in Table 2. First, despite having the same architecture, RankGPT (bert) significantly underperforms MonoBERT and RRA-BERT trained on our dataset, confirming the effectiveness of our label generation pipeline. In addition, an ablation study that excludes LLMs' missing documents from the training data (w/o missing) showed a significant drop in performance, demonstrating that the LLMs' missing phenomenon is a useful signal. Second, training with TS and TCL (w/ TS+TCL) enhances overall performance while reducing the standard deviation without DL20. TS and TCL contributes to both performance improvement and stable training. Third, training with TS and TCL but removing TCL during inference (infer w/o) did not cause performance degradation. At the same time, removing TCL reduced the inference time by 5.58%. This indicates that TCL enhances sbase in Equation 4 by naturally integrating the term signal into its computation. Therefore, we can remove the TCL during inference without degrading performance. As part of our qualitative evaluation, we provide real-world examples of long-tail query ranking results using RRA-BERT in A.4. These examples demonstrate"}, {"title": "3.1.2 RRA-GPT", "content": "Here we explore three questions: (1) The most effective training task combinations, (2) The impact of the ranking layer and whether input or generated tokens are preferable, and (3) The influence of reasoning on ranking training. The findings are summarized in Table 3. First, models trained with all three tasks: classification (clf), ranking (rank), and generation (gen), showed the best ranking performance. However, adding clf or rank task individually had no effect in our experiments. Second, training a ranking layer was effective, and using the input token embedding <|Response|> (abbr. <|Resp. |>) was better than using the generated one <|Reason|> (abbr. <|Rsn. |>), for the input. Third, adding the ranking layer greatly improves performance with reasoning, while without it, reasoning worsens ranking, a consistent trend across all our experimental units. We found that simply adding reasoning without extra training layer to GPT does not enhance performance. Undoubtedly, even if reasoning does not directly improve ranking performance, it remains valuable for explainable ranking. Given the challenges of interpreting results from neural models, obtaining explanations for the output of the ranker (i.e., why q and d are relevant or not) is crucial. Moreover, we observed that jointly training the ranking layer alongside label and reasoning generation yields substantial advantages. Our best model significantly outperformed the rank only model shown in Table 3, which was solely trained to optimize relevance scores (Score in Figure 4), without label and reasoning generation (Label, Reasoning in Figure 4). This improvement shows the efficacy of our training approach, maintaining simplicity in inference. Furthermore, the ranking layer accelerates learning. Our best model converges in half the steps compared to the second-best model, which does not use a ranking layer. Specifically, the average training convergence steps were 5666.67(\u00b12624.67) and 11333.33(\u00b11885.62), respectively."}, {"title": "3.1.3 Serving", "content": "We compared response latency and ranking performance according to model types and sizes in Figure 7. All results were obtained using a single A100 GPU and model sizes follow the order: T5(small) < BERT < T5(large). As RRA-BERT shows the best ranking performance with reasonable speed we chose it as our final model and successfully deployed using TensorRT-LLM* in real-world scenarios. More details are described in A.3."}, {"title": "3.2 A/B Testing", "content": "We conducted online and offline A/B tests comparing search results ranked by RRA-BERT with the current search results of NAVER search engine for long-tail queries. In the 7-day online A/B testing, RRA-BERT increased CTR by 5.63%, top-1 document clicks by 5.9%, and dwell time (the duration of time spending on a search result) by 7.97%."}, {"title": "4 Conclusion", "content": "In this paper, we present an efficient label generation pipeline using LLMs that utilizes a pre-ranker to select effective documents and leverages LLMs' missing phenomenon as a useful signal. We also present effective training methods to capture long and complex query-document relevance in both BERT and GPT. However, only key parts of the model structure are employed during inference, minimizing the service deployment burden. Through extensive experiments, including A/B testing on NAVER Search, we demonstrate the effectiveness of our approach for long-tail queries re-ranking."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Details of Label Generation with LLMs", "content": ""}, {"title": "A.1.1 Long-tail query sampling", "content": "The NAVER search engine has predominantly handled short keyword-based queries, which constituted the majority of incoming queries as shown in Figure 8. Due to the high volume of popular short queries (short-head), ranking based on query-specific feedback features was effective, with feedback features often proving more significant than relevance features. However, this approach does not apply well to long-tail queries which are long and complex with lower user engagement which is depicted in Figure 9. Individual long-tail queries lacks sufficient user feedback, necessitating a reliance on relevance-based ranking. Since most of the incoming queries were short-head queries that are brief and simple, lacking contextual information, the combination of syntactic matching (like BM25) and feedback features is effective at scale. However, for long-tail queries, relevance is more effective than feedback and semantic matching is more effective than syntactic matching. This is because they lack user feedback but contain rich semantic information."}, {"title": "A.1.2 Pre-ranking", "content": "Pre-ranking is a crucial step in our ranking label generation pipeline, where all retrieved documents are ranked before being inputted into the LLM. Our pre-ranker, which is BERT-based, was trained on a small dataset of 1,000 queries and corresponding document snippets crawled from NAVER search pages, employing our label generation approach. As several studies (Wang et al., 2023b; Qin et al., 2024; Sun et al., 2023) have shown that the order of inputs significantly influences the performance of LLMs, we initially rank the documents using pre-ranker before inputting them into LLMs. Any rankers can be served as a pre-ranker, and the performance of ours can be found in Table 4."}, {"title": "A.1.3 Reasoning generation", "content": "To train the GPT ranker, we utilized HCX-S to create reasoning, which explains the basis for considering q and d as either relevant or irrelevant.\n$label = irrelevant \\ if \\ d\\in Dexcluded \\ else \\ relevant$\n$reasoning(q,d,label) = HCXs(Prompt(q, d, label))$ (9)\nGenerating reasoning along with HCX-L list-wise labeling results in either grouping multiple documents together for explanation or omitting reasoning for Dexcluded. Therefore, we opted for pointwise reasoning generation using HCX-S, a small model of HCX that provides adequate reasoning. Here we utilized a simple prompt like \"Explain why the given q and d pair is classified as label (= relevant or irrelevant).\""}, {"title": "A.2 Experimental settings", "content": ""}, {"title": "A.2.1 Datasets & Metrics", "content": "The method of constructing training data is in Section 2.1. For evaluation, we set aside 10% of this data. To ensure the robust performance of our models, we also evaluated on public testsets for passage re-ranking: MS MARCO (Bajaj et al., 2018), MIRACL (Zhang et al., 2023b), DL19 (Craswell et al., 2020), and DL20 (Craswell et al., 2021). The statistics of testsets are in Table 5. Since MS MARCO, DL19, and DL20 are in English, we translated them into Korean using NAVER Papago, which is a Korean machine translation service provided by NAVER. MIRACL is a multilingual dataset, so we directly used its Korean version. We used nDCG (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) as the evaluation metric, widely used for measuring ranking performance."}, {"title": "A.2.2 Baselines", "content": "We include the following baselines. MonoBERT and MonoT5 are trained on the dataset created by our label generation pipeline. For RankGPT training labels, we employed sliding windows on D, following the paper (Sun et al., 2023), with a window size of 20 and a step size of 10, without our pre-ranking process.\n\u2022 HCX-L (Yoo et al., 2024): With our teacher model HCX-L which is an instruction-tuned LLM including diverse Korean data, we utilized the list-wise ranking prompt from RankGPT (Sun et al., 2023).\n\u2022 BERT (naive): Before tuning our backbone BERT, we rank based on the cosine similarity of embeddings for q and d.\n\u2022 GPT (vanilla): Before tuning our backbone GPT, we rank using the relevance score prel pirrel (see Equation 7, changing <|Relevant |> to Relevant and to <|Irrelevant|> to Irrelevant)."}, {"title": "A.2.3 Implementation details", "content": "We used internally pre-trained small Korean LMs, specifically RoBERTa (Liu et al., 2019) for RRABERT and GPT (Radford et al., 2019) for RRAGPT. The same backbones were also used for MonoBERT and RankGPT in baseline experiments. Additionally, we used an in-house T5 backbone for MonoT5. For hyperparameters, the optimizer used for all models is AdamW (Loshchilov and Hutter, 2017), with a learning rate of 1e-5. We trained each model three times and reported the averaged performance. Training data was randomly split into a 9:1 ratio for training and validation. BERT was validated every 300 steps, GPT every 1,000 steps. Early stopping occurred if performance didn't improve for five consecutive validations. The generated ranking labels were used in training as follows:\n$2-i * 0.1 \\ for \\ d_i \\in Dranked$\n$0.2 \u2013 (j + 1) * 0.01 \\ for \\ d_j \\in Dexcluded$\n$0 \\ for \\ dneg; where \\ dneg \\ are negatives with a size of 3$ (10)\nTo clarify, j is randomly assigned, since there is no order in Dexcluded. For MonoBERT and MonoT5 training, which utilize a classification task framework, we label Dranked as relevant documents and Dexcluded as irrelevant ones. In addition, for TCL training of RRA-BERT, we set k, the number of selected document tokens per query token, to 3 for token selection (Section 2.2.1), and \u03b1 to 0.3 as the hyperparameter for combining TCL loss (Section 2.2.3)."}, {"title": "A.3 Serving details", "content": "First of all, despite using a small-sized GPT, its speed and throughput were not as good as BERT and T5, making it difficult to use it for search"}, {"title": "A.4 Real-world qualitative examples", "content": "We provide a few examples of long-tail queries ranked by our final model, RRA-BERT. The original query and document were in Korean and have been translated into English; the document refers to the [title] and snippet. Table 6 illustrates two queries where a single character change completely alters the meaning: \"8\uac1c\uc6d4 \uac15\uc544\uc9c0\uac00 \uc7a0\ub9cc\uc790\uc694\" (8-month-old dog only sleeps)\" vs \"8\uac1c\uc6d4 \uac15\uc544\uc9c0\uac00 \uc7a0\uc548\uc790\uc694\" (8-month-old dog doesn't sleep). This example shows how RRA-BERT successfully distinguishes between the two semantically different queries despite their minimal character variation. Additionally, Table 7, 8 and 9 present ranking results for single long-tail queries, demonstrating how our model ranks relevant documents at the top by capturing important term signals while still considering the overall semantic context."}]}