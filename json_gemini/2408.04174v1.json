{"title": "wav2graph: A Framework for Supervised Learning Knowledge Graph from Speech", "authors": ["Khai Le-Duc", "Quy-Anh Dang", "Tan-Hanh Pham", "Truong-Son Hy"], "abstract": "Knowledge graphs (KGs) enhance the performance of large language models (LLMs) and search engines by providing structured, interconnected data that improves reasoning and context-awareness. However, KGs only focus on text data, thereby neglecting other modalities such as speech. In this work, we introduce wav2graph, the first framework for supervised learning knowledge graph from speech data. Our pipeline are straightforward: (1) constructing a KG based on transcribed spoken utterances and a named entity database, (2) converting KG into embedding vectors, and (3) training graph neural networks (GNNs) for node classification and link prediction tasks. Through extensive experiments conducted in inductive and transductive learning contexts using state-of-the-art GNN models, we provide baseline results and error analysis for node classification and link prediction tasks on human transcripts and automatic speech recognition (ASR) transcripts, including evaluations using both encoder-based and decoder-based node embeddings, as well as monolingual and multilingual acoustic pre-trained models. All related code, data, and models are published online.", "sections": [{"title": "Introduction", "content": "In the field of Artificial Intelligence (AI), KGs have emerged as a powerful approach to knowledge representation and reasoning. KGs leverage graph-structured models to encode entities (objects, events, concepts) and the relationships linking them (Fensel et al., 2020; Ji et al., 2021). This structured representation enables efficient storage, retrieval, and reasoning over vast amounts of interconnected information (Hogan et al., 2021; Chen et al., 2020).\nThe utility of KGs spans a variety of high-impact applications. For example, prominent search engines such as Bing, Google, and Yahoo employ KGs to enhance search relevance and personalization (Steiner et al., 2012; Uyar and Aliyu, 2015; Juel Vang, 2013). Knowledge engines and question-answering systems, such as Wolfram Alpha, Siri, and Alexa, leverage KGs to deliver precise and contextually appropriate responses (He et al., 2020; Fei et al., 2021). Social networks, including LinkedIn and Facebook, also utilize KGs to enrich user profiles and enable sophisticated social recommendations (Pellissier Tanon et al., 2016; Lehmann et al., 2015). Furthermore, over the past three years, KGs have become pivotal in enhancing the reasoning capabilities of LLMs by providing structured, interconnected data that enhances the model's ability to understand and generate contextually relevant and accurate information (Pan et al., 2024; Yasunaga et al., 2021; Ji et al., 2020).\nDespite their significant advantages, the construction and training of voice-based KGs remains a complex and largely unexplored process. Among limited number of relevant works we found, to the best of our knowledge, Fu et al. (2021) claimed to present the first automatic KG construction system from speech. Wu et al. (2022) proposed a new information extraction task, speech relation extraction, that used extracted relations between synthetic ASR transcripts to build a KG. However, there has been no training conducted on such KGs to the best of our knowledge. Training KGs is necessary because GNN models can learn to extract and generalize complex patterns and relationships"}, {"title": "Data", "content": "We selected the VietMed-NER dataset (Le-Duc et al., 2024) as an initialization for our KG construction due to its status as the largest spoken named entity recognition (NER) dataset in the world in terms of the number of entity types, characterized by 18 distinct entity types. The dataset focused on real-world medical conversations.\nAs shown in Figure 2, to construct the knowledge graph, we employ an entity-utterance-entity methodology (Al-Moslmi et al., 2020). NER is used to extract named entities (NEs) from text and categorize them into types such as person, location, and organization. Instead of utilizing an automatic NER system based on machine learning like most previous works (Thukral et al., 2023; Jia et al., 2018; Nie et al., 2021), we opted for gold-standard labels provided by human annotators to extract NEs. These NEs are then linked to the corresponding utterances that mention them, forming relational edges. Consequently, our knowledge graph comprises two types of nodes (entity_type attribute): utterance and named_entity. Named entities have an additional attribute, as they are classified into 18 distinct types, such as PERSON and LOCATION, etc,."}, {"title": "wav2graph", "content": "Let $x := x_1,x_2,...,x_T$ be an audio signal of length T and let $k \\in K$ be an NE in the set of all NEs in database, the aim is to build a learning model f that conduct 2 single tasks:\n\u2022 Node classification: Estimating the node attribute probability $p(c|x \\lor k)$ for each $c \\in C$, where C is the number of attribute classes\n\u2022 Link prediction: Estimating the edge probability $p(e|x_i, k_j)$, where $e \\in \\{0,1\\}$ is the edge presence between 2 nodes, $k \\in K$ is an NE in the set of all NES\nTherefore, the decision rule to predict a single class for node classification task is:\n$x \\lor k \\rightarrow \\hat{e} = arg \\max_{c \\in C} f(c|x \\lor k)$              (1)\nAnd the decision rule to predict a single class for link prediction task is:\n$x_i, k_j \\rightarrow \\hat{e} = arg \\max_{e \\in \\{0,1\\}} f(e|x_i,k_j)$     (2)"}, {"title": "ASR Model", "content": "An ASR model is used to transcribe audio signal into text by mapping an audio signal $x_1^T$ of length T to the most likely word sequence $w_1^N$ of length N. The relation $w^*$ between the acoustic and word sequence is:\n$w^* = \\underset{w}{\\mathrm{arg\\ max}} p(w|x_1^N)$          (3)\nAccording to Bayes' Theorem, the probability $p(x_1^N)$ can be ignored during maximization because it only serves as a normalization factor and does not affect the outcome:\n$p(w|x_1^N) = \\frac{P(x|w)p(w)}{p(x)}$\n$\\propto p(x|w)p(w)$         (4)\nTherefore:\n$w^* = \\underset{w}{\\mathrm{arg\\ max}} \\underbrace{p(x|w)}_{acoustic model} \\underbrace{p(w)}_{language model}$      (5)"}, {"title": "Node Embeddings", "content": "Feature vectors for text in each node will be generated using selected pre-trained embedding models. Given a word sequence $w_1^N$ and a NE k, node features generated by embedding functions are represented as an embedding vector:\n$z_q = Embedding(w_1^N)$         (6)"}, {"title": "Node Classification and Link Prediction", "content": "\u2022 Node classification is the task of predicting the labels of nodes in a graph G = (V, E) where V is the set of nodes and E is the set of edges. In a KG built from speech, V is the set of ASR transcript $w_1^N$ and NE k. Given node features $Z \\in R^{|V|\\times d}$ and (one-hot) node labels $Y \\in R^{|V|\\times C}$ where d is the number of input features associating with each node, we aim to learn a function $g : V \\rightarrow \\{1, . . .,C\\}$ that maps each node v with its corresponding label g(v).\n\u2022 Link prediction is the task of predicting the existence of edges between node pairs in the graph. The goal is to predict a function $h : V \\times V \\rightarrow \\{0,1\\}$ where h(u, v) = 1 indicates the presence of an edge between two nodes u and v."}, {"title": "GNN Models", "content": "We explore the performance of various GNN models for node classification and link prediction. We use SAGE, GCN, GAT, and SuperGAT because they are well-suited for non-heterogeneous graphs, efficiently capturing local and global graph structure, and offering strong performance with scalable, interpretable architectures."}, {"title": "SAGE (Sample and Aggregate)", "content": "Efficient GNN model that generates embeddings by sampling and aggregating features from a node's local neighborhood.\n$h_v^l = \\sigma(W_l \\cdot AGG(\\{ h_u^{l-1} : u \\in N(v)\\} \\cup \\{ h_v^{l-1} \\})),(7)$"}, {"title": "GCN (Graph Convolutional Network)", "content": "Spectral-based GNN learning node representations based on local graph structure.\n$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$         (8)"}, {"title": "GAT (Graph Attention Network)", "content": "Assigns attention weights to neighboring nodes, focusing on the most informative ones.\n$h'_u = \\sigma(\\sum_{v\\in N(u)\\cup \\{u\\}} \\alpha_{uv} Wh_v),  (9)$"}, {"title": "SuperGAT", "content": "Extends GAT to incorporate node and edge features in the attention mechanism.\n$h'_u = \\sigma(\\sum_{v\\in N(u)\\cup \\{u\\}} \\alpha_{uv} W[h_v ||e_{uv}]), (12)$\nSuperGAT uses an auxiliary loss:\n$L_{att} = \\sum_{k,v} || \\alpha_{vu} - w^*_{vu} ||^2,         (13)$"}, {"title": "Experimental Setups", "content": "We employed hybrid ASR approach to transcribe audio to text. First, we generated Gaussian Mixture Model (GMM) labels as input for Deep Neural Network / Hidden Markov Model (DNN/HMM) training. We employed wav2vec 2.0 encoder (Baevski et al., 2020b) for the DNN, which was unsupervised pre-trained on either monolingual data or multilingual data. Their WERs on the test set were 29.0% and 28.8%, respectively. More details are shown in Section B.1 in the Appendix."}, {"title": "KG and GNN Models", "content": "GNN model training: Our research includes three training settings. Initially, the data is divided into training (60%), validation (20%), and testing (20%) sets. Each GNN model is trained on the training set, with hyperparameter tuning on the validation set using metrics such as AP and AUC, then validate on test sets (known as inductive graph learning). In the second setting, models are trained on one dataset and inferenced on an unseen dataset, meaning the two KGs are independent in this scenario (known as transductive graph learning). Lastly, models are trained on the complete dataset from the first setting and directly inferenced on two KGs extracted from ASR (transductive learning) with WER 28.8% - monolingual acoustic pre-training, and 29.0% - multilingual acoustic pre-training. More details are shown in Section B.2 in the Appendix.\nEmbeddings: Our study investigates the influence of pre-trained embeddings on node classification and link prediction tasks within a KG. We used both encoder-based embeddings and decoder-based embedding , as well as random embeddings . Feature vectors for text in each node will be generated"}, {"title": "Evaluation Metrics", "content": "In our study, we employ two evaluation metrics: Average Precision Score (AP) and Area Under the Receiver Operating Characteristic Curve (ROC AUC or AUC) to assess the performance of our GNN models on both node classification and link prediction tasks. Details are shown in Section B.4 in the Appendix."}, {"title": "Experimental Results", "content": ""}, {"title": "Node Classification on Human Transcript", "content": ""}, {"title": "Inductive Learning", "content": ""}, {"title": "Transductive Learning", "content": ""}, {"title": "Link Prediction on Human Transcript", "content": ""}, {"title": "Inductive Learning", "content": ""}, {"title": "Transductive Learning", "content": ""}, {"title": "Node Classification on ASR Transcript", "content": ""}, {"title": "Monolingual Acoustic Pre-training (WER=29.0%)", "content": "ASR-extracted utterances often lead to reduced accuracy due to noisy transcripts compared to human transcripts. The evaluation results in Table 6 for the node classification task exhibit a consistent trend with the previous findings, but with some differences. SAGE consistently outperforms others across all embedding types, achieving the highest AP score of 0.8851 and AUC score of 0.9204 with random embeddings, surpassing even the more sophisticated embedding methods, while GCN, GAT, and SuperGAT models lag behind significantly."}, {"title": "Multilingual Acoustic Pre-training (WER=28.8%)", "content": "Results in Table 7 show patterns in model performance for the node classification task on ASR transcript using multilingual acoustic pre-training. SAGE outperforms GCN, GAT, and SuperGAT across all embedding types. Notably, SAGE combined with the vinai/phobert-base-v2 embedding achieves peak performance, with AUC score of 0.9266. SAGE is further well-performed even with random embeddings. Comparatively, multilingual acoustic pre-training shows slight improvements across most models compared to monolingual pre-training, although WERs are relatively comparable. However, the Alibaba-NLP/gte-Qwen2-7B-instruct embedding significantly outperforms other embeddings and GNN models using multilingual acoustic pre-training. Therefore, multilingual LLM text embeddings achieve optimal performance on node classification task when applied on multilingual acoustic pre-training ASR transcript."}, {"title": "Link Prediction on ASR Transcript", "content": ""}, {"title": "Monolingual Acoustic Pre-training (WER=29.0%)", "content": ""}, {"title": "Multilingual Acoustic Pre-training (WER=28.8%)", "content": "Our empirical analysis of the link prediction task on ASR transcript using multilingual acoustic pre-training in Table 9 also reveals a distinct performance hierarchy among GNN architectures. The GCN emerges as the preeminent model, consistently outperforming its counterparts across all embedding configurations. Notably, the GCN variant utilizing the Alibaba-NLP/gte-large-en-v1.5 embedding achieves state-of-the-art performance, with an AP score of 0.9184 and AUC score of 0.9413. This stands in stark contrast to the SAGE architecture, which, despite its prowess in node classification, exhibits suboptimal performance in this link prediction task. The GAT and SuperGAT models demonstrate intermediate efficacy, marginally surpassing SAGE but falling significantly short of GCN's benchmark. The intfloat/multilingual-e5-large-instruct embedding consistently augments model performance, but with varying magnitudes of impact across architectures."}, {"title": "Error Analysis", "content": "In inductive learning, BERT-based embeddings are essential for achieving optimal performance, whereas in transductive learning, random text embeddings demonstrate competitiveness with pre-trained embeddings.\nNode classification on ASR transcript: Firstly, in the context of noisy ASR transcripts, both monolingual and multilingual acoustic pre-training settings demonstrate that random text embeddings perform competitively with BERT-based text embeddings. For comparison, this transductive learning approach for ASR transcripts is similar to the transductive learning used for node classification on human transcripts. Secondly, multilingual LLM text embeddings notably outperform others in node classification tasks when applied to multilingual acoustic pre-training ASR transcripts. As our study is the first evaluation of training KGs from speech, there is no existing literature for direct comparison. However, combination of multilingual LLM text embeddings and multilingual acoustic pre-training generally yield higher accuracy across various downstream tasks, e.g. ASR , speech translation , text-to-speech . Thirdly, within the same transductive learning setting, node classification on ASR transcripts generally achieved competitive results compared to human transcripts, despite relatively high WERs of 28.8% and 29%.\nLink prediction on ASR transcript: Firstly, in the context of noisy ASR transcripts, both in monolingual and multilingual acoustic pre-training ASR settings, random text embeddings demonstrate performance comparable to BERT-based or LLM text embeddings. This transductive learning performance is also observed in the transductive learning of link prediction on human transcripts. Secondly, in the same transductive learning setting, link prediction on ASR transcripts generally outperformed that on human transcripts, despite relatively high WERs of 28.8% and 29%. This result is noteworthy, as high WERs in ASR transcripts typically degrade accuracy in various downstream NLP tasks, as widely demonstrated by the AI community . We hypothesized that the influence of text embedding, which primarily focuses on the generalized context of text segments (semantics), reduces the impact of ASR errors on prediction performance . Thirdly, our transductive learning on ASR transcripts for both node classification and link prediction tasks was conducted in a zero-shot setting. We hypothesize that the adaptation of trained GNN models to the ASR transcripts of the training set could further enhance performance"}, {"title": "Conclusion", "content": "In this study, we introduce wav2graph, the first framework for supervised learning of KG from speech data. Additionally, we present the first real-world KG derived from speech, along with its baseline results.\nOur study demonstrates that, first of all, for node classification and link prediction tasks on ASR transcripts, both monolingual and multilingual acoustic pre-training with random text embeddings perform competitively with encoder-based and decoder-based embeddings. This trend is also observed in transductive learning on human transcripts. Secondly, multilingual LLM text embeddings significantly outperform other embeddings in node classification tasks when applied to multilingual acoustic pre-trained ASR transcripts. Thirdly, node classification on ASR transcripts generally achieves competitive results compared to human transcripts, while link prediction on ASR transcripts generally outperforms that on human transcripts, despite relatively high WERS of 28.8% and 29%. This unexpected behavior is likely due to the influence of text embeddings, which primarily focus on the generalized semantic context of text segments and therefore reduce the impact of ASR errors on prediction performance. This contrasts with most previous works on other downstream tasks, where high WERs in ASR transcripts typically degrade accuracy."}, {"title": "Related Works", "content": "This section describes works relevant to our wav2graph framework.\nKG from speech: In the domain of knowledge graph construction, traditional methodologies have predominantly focused on extracting information from textual sources (Zhong et al., 2023; Wang et al., 2021, 2014; Chen et al., 2021). Although there has been progress incorporating multimodal inputs, such as images and text (Zhu et al., 2022; Li et al., 2024; Alberts et al., 2021; Yu et al., 2021), the challenge of directly constructing knowledge graphs from speech data remains largely unexplored. Among the limited number of relevant works we could find to our best knowledge, Fu et al. (2021) introduced what they claimed to be the first automatic KG construction system from speech. Also, Wu et al. (2022) proposed a novel information extraction task, termed speech relation extraction, which utilized extracted relations from synthetic ASR transcripts to construct a KG. However, to the best of our knowledge, no training has been conducted on such KGs. Training KGs is crucial as GNN models can learn to extract and generalize complex patterns and relationships from the data within a KG, thereby enabling predictions on unseen data in the KG-capabilities that rule-based methods alone cannot achieve.\nInformation extraction from speech: The challenge of accurately identifying entities and their relationships within vast and unstructured data sources persists as a major barrier to broader text-based KG adoption (Peng et al., 2023). Such relationships are typically extracted through NER systems (Li et al., 2022, 2020). However, performing NER on speech, which is necessary for constructing voice-based KGs, remains a significant challenge (Chen et al., 2022; Sui et al., 2021; Szyma\u0144ski et al., 2023; Yadav et al., 2020; Caubri\u00e8re et al., 2020).\nGNNs for speech applications: GNNs have shown promise in various speech-related applications. For instance, Wang et al. (2020); Singh et al. (2023) applied GNNs to improve speaker diarization, while Pentari et al. (2024); Joshi et al. (2022); Li et al. (2023a) employed them for speech emotion recognition. Also, GNNs could be used in speaker verification task and ASR hypothesis decoding . Despite these advancements, existing GNN-based approaches do not address the direct construction and training of KGs from speech data, which enables the prediction of attributes and relations between spoken utterances.\nGNNs: Graph representation learning has evolved significantly in recent years, with various approaches designed to incorporate graph structure into meaningful node features. For example, Graph Convolutional Networks (GCN)  and Message Passing Neural Networks (MPNN)  pioneered this field by proposing the message passing scheme, in which each node aggregates features from the adjacent nodes. Subsequent works like Graph Attention Networks (GAT) introduced mechanisms to prioritize important nodes. However, these classical approaches often struggle with capturing long-range relationships (Dwivedi et al., 2022). To address this limitation, researchers have explored virtual nodes and k-hop neighborhoods within the message passing framework. More recently, graph transformers have gained prominence, with models such as TokenGT  and Graphormer incorporating sophisticated encodings such as centrality and spatial information. GraphGPS further advanced this approach by combining various positional and structural encodings with multiple graph block types . Additionally, Ngo et al. (2023) proposed a multiscale graph transformer that learns hierarchical graph coarsening and utilizes graph wavelet transforms for positional encoding. These advancements in graph representation learning have significantly improved the ability to capture both local and global graph structures, paving the way for more effective graph-based machine learning models."}, {"title": "Details about Experimental Setups", "content": "This section describes details about experimental setups for experimental reproducibility, which extends the Section 4 in the main paper."}, {"title": "ASR Models", "content": "This section extends details of Section 4.1 in the main paper.\nGaussian Mixture Model / Hidden Markov Model (GMM/HMM): For language modelling and initial GMM/HMM, we followed the same setups and hyperparameters as in (L\u00fcscher et al., 2023). First, we used the toolkit Sequitur Grapheme-To-Phoneme (g2p) (Bisani and Ney, 2008) to convert pronunciation lexica found in human transcript, so that the seed lexicon was extended, creating the lexica for training. Secondly, we created an n-gram language model using previously extended lexica and human transcript based on Kneser-Ney Smoothing (Ney et al., 1994). Thirdly, we created alignments obtained by using GMM/HMM as labels for wav2vec 2.0 (Baevski et al., 2020b) neural network training, which later resulted to Deep Neural Network / Hidden Markov Model (DNN/HMM) training. The acoustic modeling employed context-dependent phonemic labels, triphones to be specific. In GMM/HMM process, we used a CART (Classification And Regression Tree) (Breiman, 2017) to tie the states s, resulting 4501 CART labels. During GMM/HMM process, we stopped at Speaker Adaptive Training stage (SAT) (Miao et al., 2015) instead of going beyond Speaker Adaptive Training + Vocal Tract Length Normalization (Eide and Gish, 1996) (SAT+VTLN) for the sake of good WER labels:\n$p(x_1^N) = \\sum_{[s^T]} \\prod_{t=1}^{T}p(x_t, s_t|s_{t-1}, w_1^N)$        (14)\nUnsupervised DNN pre-training: For unsupervised DNN pre-training, we used wav2vec 2.0 (Baevski et al., 2020a) as DNN encoder with the help of Fairseq (Ott et al., 2019) toolkit. We employed similar vanilla configurations and hyperparameters as in (Le-Duc, 2023). All models had 118M parameters including 7 Convolutional Neural Network (CNN)  layers and 8 Transformer (Vaswani et al., 2017) layers. The last CNN layer had a stride halved for the 8kHz data (Vieting et al., 2023).\nDNN/HMM training: We loaded unsupervised pre-trained wav2vec 2.0 models for fine-tuning in a supervised DNN/HMM approach. We optimized the model with Framewise Cross-Entropy (fCE) loss. The SpecAugment  data augmentation was applied for the entire 33 fine-tuning epochs. We used RETURNN toolkit  for supervised training. ASR hypothesis decoding: The entire ASR hypothesis decoding was done using RASR toolkit . In this stage, we integrated the acoustic model with the n-gram language model using Bayes' decision rule and the Viterbi algorithm (Forney, 1973). The Viterbi algorithm recursively computes the optimal path through the alignment graph of all potential word sequences, thereby identifying the best alignment with the acoustic observations. Then, pruning of the acoustic model and the n-gram language model through beam search was employed to focus exclusively on the most likely predicted words at each time step t (Ortmanns et al., 1997). Viterbi algorithm is described as:\n$\\hat{w_1^N} = \\underset{w_1^N}{arg \\ max} p (\\prod_{n=1}^{N}p(W_n|w_{n-m}^N)$ (15)\nMonolingual and multilingual pre-training: We utilized two best baseline models for ASR task on the VietMed dataset, as described by Le-Duc (2024). The models employed were a monolingual acoustic pre-trained w2v2-Viet and a multilingual acoustic pre-trained XLSR-53-Viet."}, {"title": "KG and GNN Models", "content": "This section extends details of Section 4.2 in the main paper.\nKG preprocessing: The KG is preprocessed, involving the identification of entity and relation types, attribute normalization, and potential feature engineering for edge features. The KG consists of two types of nodes: utterance (e.g., \"Doctors go to hospital\") and named_entity (e.g., \"doctors\"(PERSON) and \"hospital\" (LOCATION)). NEs are extracted from the utterances.\nHyperparameter Tuning: Hyperparameter tuning will focus on optimizing the combination of hidden layers and message passing aggregation functions for each GNN model. In the first setting, models will be trained with fixed hyperparameters: 250 epochs (10 epochs for SAGE), a learning rate of 0.005, weight decay of 0.05, Adam optimizer , and dropout rates  of 0.2 for node classification tasks and 0.5 for link prediction tasks. The same hyperparameters will be applied for both the second and third settings."}, {"title": "Node Embeddings", "content": "This section extends details of Section 4.2 in the main paper.\nWe utilized both encoder-based embeddings, including the English Alibaba-NLP/gte-large-en-v1.5 , the multilingual intfloat/multilingual-e5-large-instruct , and the Vietnamese vinai/phobert-base-v2, as well as decoder-based embeddings, such as the multilingual LLM Alibaba-NLP/gte-Qwen2-7B-instruct, in addition to random embeddings . We used the following models to get embeddings:\n\u2022 Random embedding: Features are initialized randomly\u00b2 , a tensor filled with random numbers from a normal distribution with mean 0 and variance 1, serving as a baseline for embedding comparison.\n\u2022 Alibaba-NLP/gte-large-en-v1.5 : General-purpose general text embeddings with multi-stage contrastive learning that built upon the encoder backbone + ROPE + GLU .\n\u2022 intfloat/multilingual-e5-large-instruct : The opensource multilingual E5 text embedding models, released in mid-2023. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets.\n\u2022 vinai/phobert-base-v2 : A pre-trained RoBERTa language models for Vietnamese.\n\u2022 Alibaba-NLP/gte-Qwen2-7B-instruct (qwe, 2024): It is the latest model in the gte (General Text Embedding ) model family with 7 billion parameters."}, {"title": "Evaluation Metrics", "content": "AP metric: The AP summarizes the precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight. It provides a single number to summarize the classifier's performance, which is especially useful in the context of imbalanced datasets where one class may be underrepresented. For the node classification task, let \u0109 be the true label of node v and cu be the predicted probability of node v belonging to a specific class. For the link prediction task, let \u00eau,v be the true label indicating the presence of an edge between nodes u and v, and eu,v be the predicted probability of an edge existing between nodes u and v. We then sort the nodes in descending order of their predicted probabilities ca and eu,v respectively. Finally, the AP score is calculated as:\n$AP = \\sum_{n}(R_n - R_{n-1})P_n$,        (16)\nAUC metric: AUC represents the degree or measure of separability, indicating how much the model is capable of distinguishing between classes. The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. TPR(t) and FPR(t) be the true positive rate and false positive rate at threshold t, respectively. The AUC is computed as:\n$AUC = \\int_{0}^{1} TPR(FPR^{-1}(x)) dx$       (17)"}]}