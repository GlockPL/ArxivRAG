{"title": "wav2graph: A Framework for Supervised Learning Knowledge Graph from Speech", "authors": ["Khai Le-Duc", "Quy-Anh Dang", "Tan-Hanh Pham", "Truong-Son Hy"], "abstract": "Knowledge graphs (KGs) enhance the performance of large language models (LLMs) and search engines by providing structured, interconnected data that improves reasoning and context-awareness. However, KGS only focus on text data, thereby neglecting other modalities such as speech. In this work, we introduce wav2graph, the first framework for supervised learning knowledge graph from speech data. Our pipeline are straightforward: (1) constructing a KG based on transcribed spoken utterances and a named entity database, (2) converting KG into embedding vectors, and (3) training graph neural networks (GNNs) for node classification and link prediction tasks. Through extensive experiments conducted in inductive and transductive learning contexts using state-of-the-art GNN models, we provide baseline results and error analysis for node classification and link prediction tasks on human transcripts and automatic speech recognition (ASR) transcripts, including evaluations using both encoder-based and decoder-based node embeddings, as well as monolingual and multilingual acoustic pre-trained models. All related code, data, and models are published online.", "sections": [{"title": "Introduction", "content": "In the field of Artificial Intelligence (AI), KGS have emerged as a powerful approach to knowledge representation and reasoning. KGs leverage graph-structured models to encode entities (objects, events, concepts) and the relationships linking them (Fensel et al., 2020; Ji et al., 2021). This structured representation enables efficient storage, retrieval, and reasoning over vast amounts of interconnected information (Hogan et al., 2021; Chen et al., 2020).\nThe utility of KGs spans a variety of high-impact applications. For example, prominent search engines such as Bing, Google, and Yahoo employ KGs to enhance search relevance and personalization (Steiner et al., 2012; Uyar and Aliyu, 2015; Juel Vang, 2013). Knowledge engines and question-answering systems, such as Wolfram Alpha, Siri, and Alexa, leverage KGs to deliver precise and contextually appropriate responses (He et al., 2020; Fei et al., 2021). Social networks, including LinkedIn and Facebook, also utilize KGs to enrich user profiles and enable sophisticated social recommendations (Pellissier Tanon et al., 2016; Lehmann et al., 2015). Furthermore, over the past three years, KGs have become pivotal in enhancing the reasoning capabilities of LLMs by providing structured, interconnected data that enhances the model's ability to understand and generate contextually relevant and accurate information (Pan et al., 2024; Yasunaga et al., 2021; Ji et al., 2020).\nDespite their significant advantages, the construction and training of voice-based KGs remains a complex and largely unexplored process. Among limited number of relevant works we found, to the best of our knowledge, Fu et al. (2021) claimed to present the first automatic KG construction system from speech. Wu et al. (2022) proposed a new information extraction task, speech relation extraction, that used extracted relations between synthetic ASR transcripts to build a KG. However, there has been no training conducted on such KGs to the best of our knowledge. Training KGs is necessary because GNN models can learn to extract and generalize complex patterns and relationships"}, {"title": "Data", "content": "We selected the VietMed-NER dataset (Le-Duc et al., 2024) as an initialization for our KG construction due to its status as the largest spoken named entity recognition (NER) dataset in the world in terms of the number of entity types, characterized by 18 distinct entity types. The dataset focused on real-world medical conversations.\nAs shown in Figure 2, to construct the knowledge graph, we employ an entity-utterance-entity methodology (Al-Moslmi et al., 2020). NER is used to extract named entities (NES) from text and categorize them into types such as person, location, and organization. Instead of utilizing an automatic NER system based on machine learning like most previous works (Thukral et al., 2023; Jia et al., 2018; Nie et al., 2021), we opted for gold-standard labels provided by human annotators to extract NEs. These NEs are then linked to the corresponding utterances that mention them, forming relational edges. Consequently, our knowledge graph comprises two types of nodes (entity_type attribute): utterance and named_entity. Named entities have an additional attribute, as they are classified into 18 distinct types, such as PERSON and LOCATION, etc,."}, {"title": "wav2graph", "content": "Let $x := x_1,x_2,...,x_T$ be an audio signal of length $T$ and let $k \\in K$ be an NE in the set of all NEs in database, the aim is to build a learning model $f$ that conduct 2 single tasks:\n*   Node classification: Estimating the node attribute probability $p(c|x \\lor k)$ for each $c \\in C$, where $C$ is the number of attribute classes\n*   Link prediction: Estimating the edge probability $p(e|x, k)$, where $e \\in \\{0,1\\}$ is the edge presence between 2 nodes, $k \\in K$ is an NE in the set of all NEs\nTherefore, the decision rule to predict a single class for node classification task is:\n$\\hat{c} = \\underset{c \\in C}{\\arg \\max } f(c | x \\lor k)\\tag{1}$\nAnd the decision rule to predict a single class for link prediction task is:\n$\\hat{e} = \\underset{e \\in \\{0,1\\}}{\\arg \\max } f(e | x_i, k)\\tag{2}$"}, {"title": "ASR Model", "content": "An ASR model is used to transcribe audio signal into text by mapping an audio signal $x_1^T$ of length $T$ to the most likely word sequence $w_1^N$ of length $N$. The relation $w^*$ between the acoustic and word sequence is:\n$w^* = \\underset{w}{\\arg \\max } p(w|x^N)\\tag{3}$\nAccording to Bayes' Theorem, the probability $p(x^N)$ can be ignored during maximization because it only serves as a normalization factor and does not affect the outcome:\n$p(w|x^N) = \\frac{P(x^N|w)p(w)}{p(x)}\\tag{4}$\n$\\propto p(x^N|w)p(w)$\nTherefore:\n$w^* = \\underset{w}{\\arg \\max } \\underset{\\text{acoustic model}}{\\underbrace{p(x^N|w)}} \\underset{\\text{language model}}{\\underbrace{p(w)}}\\tag{5}$"}, {"title": "Node Embeddings", "content": "Feature vectors for text in each node will be generated using selected pre-trained embedding models. Given a word sequence $w_1^N$ and a NE $k$, node features generated by embedding functions are represented as an embedding vector:\n$z_q = Embedding(w_1^N)\\tag{6}$"}, {"title": "Node Classification and Link Prediction", "content": "*   Node classification is the task of predicting the labels of nodes in a graph $G = (V, E)$ where $V$ is the set of nodes and $E$ is the set of edges. In a KG built from speech, $V$ is the set of ASR transcript $w_1^N$ and NE $k$. Given node features $Z \\in R^{|V| \\times d}$ and (one-hot) node labels $Y \\in R^{|V| \\times C}$ where $d$ is the number of input features associating with each node, we aim to learn a function $g : V \\rightarrow \\{1, ..., C\\}$ that maps each node $v$ with its corresponding label $g(v)$.\n*   Link prediction is the task of predicting the existence of edges between node pairs in the graph. The goal is to predict a function $h : V \\times V \\rightarrow \\{0,1\\}$ where $h(u, v) = 1$ indicates the presence of an edge between two nodes $u$ and $v$."}, {"title": "GNN Models", "content": "We explore the performance of various GNN models for node classification and link prediction. We use SAGE, GCN, GAT, and SuperGAT because they are well-suited for non-heterogeneous graphs, efficiently capturing local and global graph structure, and offering strong performance with scalable, interpretable architectures.\n*   SAGE (Sample and Aggregate) (Hamilton et al., 2017): Efficient GNN model that generates embeddings by sampling and aggregating features from a node's local neighborhood.\n$h_v^l = \\sigma(W_l AGG(\\{h_u^{l-1} : u \\in N(v)\\} \\cup \\{h_v^{l-1}\\})),\\tag{7}$\nwhere $h_v^l$ is node v's hidden state at layer l,$\\sigma$ is a non-linear activation, $W_l$ is a learnable weight matrix, AGG is an aggregation function, and $N(v)$ is v's neighborhood.\n*   GCN (Graph Convolutional Network) (Kipf and Welling, 2017): Spectral-based GNN learning node representations based on local graph structure.\n$H^{(l+1)} = \\sigma(\\~{D}^{-\\frac{1}{2}}\\~{A}\\~{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})\\tag{8}$\nwhere $H^{(l)}$ is the l-th layer activation matrix, $\\~{A} = A + I_N$, $\\~{D}$ is $\\~{A}$'s degree matrix, and $W^{(k)}$ is a trainable weight matrix, $I_N$ is is the identity matrix.\n*   GAT (Graph Attention Network) (Velickovic et al., 2018): Assigns attention weights to neighboring nodes, focusing on the most informative ones.\n$h'_i = \\sigma(\\sum_{v\\in N(u)\\cup\\{u\\}} \\alpha_{uv} Wh_v),\\tag{9}$\nwith attention coefficient defined as:\n$\\alpha_{uv} = \\frac{exp(e_{uv})}{\\sum_{t\\in N(u)\\cup\\{u\\}} exp(e_{ut})},\\tag{10}$\n$e_{uv} = LeakyReLU(a^T [Wh_u||Wh_v]),\\tag{11}$\nwhere a is a learnable attention vector.\n*   SuperGAT (Kim and Oh, 2021): Extends GAT to incorporate node and edge features in the attention mechanism.\n$h'_i = \\sigma(\\sum_{v\\in N(u)\\cup\\{u\\}} \\alpha_{uv} W[h_v ||e_{uv}]),\\tag{12}$\nwhere $e_{uv}$ is the edge feature vector. SuperGAT uses an auxiliary loss:\n$L_{att} = \\sum_{k,v} ||\\alpha_v - w_v^*||^2,\\tag{13}$\nwith $w_v^*$ denotes the ground truth attention weight."}, {"title": "Experimental Setups", "content": "We employed hybrid ASR approach to transcribe audio to text. First, we generated Gaussian Mixture Model (GMM) labels as input for Deep Neural Network / Hidden Markov Model (DNN/HMM) training. We employed wav2vec 2.0 encoder (Baevski et al., 2020b) for the DNN, which was unsupervised pre-trained on either monolingual data or multilingual data. Their WERs on the test set were 29.0% and 28.8%, respectively. More details are shown in Section B.1 in the Appendix."}, {"title": "KG and GNN Models", "content": "GNN model training: Our research includes three training settings. Initially, the data is divided into training (60%), validation (20%), and testing (20%) sets. Each GNN model is trained on the training set, with hyperparameter tuning on the validation set using metrics such as AP and AUC, then validate on test sets (known as inductive graph learning). In the second setting, models are trained on one dataset and inferenced on an unseen dataset, meaning the two KGs are independent in this scenario (known as transductive graph learning). Lastly, models are trained on the complete dataset from the first setting and directly inferenced on two KGs extracted from ASR (transductive learning) with WER 28.8% - monolingual acoustic pre-training, and 29.0% - multilingual acoustic pre-training. More details are shown in Section B.2 in the Appendix.\nEmbeddings: Our study investigates the influence of pre-trained embeddings on node classification and link prediction tasks within a KG. We used both encoder-based embeddings (such as the English Alibaba-NLP/gte-large-en-v1.5 (Li et al., 2023b), the multilingual intfloat/multilingual-e5-large-instruct (Wang et al., 2024), and Vietnamese vinai/phobert-base-v2 (Nguyen and Nguyen, 2020)) and decoder-based embedding (like the multilingual LLM Alibaba-NLP/gte-Qwen2-7B-instruct (qwe, 2024)), as well as random embeddings (Paszke et al., 2019). Feature vectors for text in each node will be generated"}, {"title": "Evaluation Metrics", "content": "In our study, we employ two evaluation metrics: Average Precision Score (AP) and Area Under the Receiver Operating Characteristic Curve (ROC AUC or AUC) to assess the performance of our GNN models on both node classification and link prediction tasks. Details are shown in Section B.4 in the Appendix."}, {"title": "Experimental Results", "content": "The experimental results in Table 2 show that using pre-trained embeddings significantly enhances node classification performance across all models, with perfect AP and AUC scores of 1, compared to lower scores (AP: 0.9116, AUC: 0.8373 for SAGE with random embeddings and AP: 0.8017, AUC: 0.714 for Alibaba-NLP/gte-Qwen2-7B-instruct).\nUtterance nodes generally have a degree greater than 1, while named_entity nodes have a degree of 1; the SAGE architecture uses node degrees and local neighborhood features, leading to perfect model accuracy.\nThe significant contribution of pre-trained embeddings and architecture to model performance is evident, with GAT and SuperGAT models achieving high AP and AUC scores using Alibaba-NLP/gte-large-en-v1.5 embeddings, while the GCN model shows limited variation in performance with different embeddings, indicating its potential limitations of leveraging these embeddings.\nThe Alibaba-NLP/gte-Qwen2-7B-instruct embedding exhibits poor performance across most models, indicating that high-dimensional LLM embeddings can suffer from the curse of dimensionality, and the learned information may be insufficient when there are not enough data points to establish a general pattern as recently argued in some other natural language processing (NLP) tasks (Petukhova et al., 2024; Wang et al., 2023)."}, {"title": "Transductive Learning", "content": "The evaluation results for the node classification task in Table 3 demonstrate varied performance across different models and embeddings. Among the models, SAGE combined with random embedding shows relatively high AP and AUC scores (AP: 0.792, AUC: 0.8564), indicating robust performance."}, {"title": "Link Prediction on Human Transcript", "content": "Table 4 shows that pre-trained embeddings greatly improve model performance in link prediction, with SAGE's AP score rising from 0.4649 to 0.7613 and its AUC score from 0.4912 to 0.7869 using vinai/phobert-base-v2 embeddings. Similarly, the GAT model's performance improves significantly with these embeddings, achieving an AP score of 0.7801 and an AUC score of 0.8144, compared to 0.47 and 0.4617 with random embeddings; however, the GCN model shows less consistent improvements. These results underscore the critical importance of embedding quality in link prediction tasks."}, {"title": "Transductive Learning", "content": "The link prediction task results in Table 5 reveal differences in model effectiveness. The GCN model demonstrates superior performance, particularly when paired with random embeddings (AP: 0.9132, AUC: 0.9243) and the intfloat/multilingual-e5-large-instruct embedding (AP: 0.9015, AUC: 0.9183). In contrast, SAGE and GAT models generally exhibit lower performance, with AP and AUC scores hovering around 0.54 and 0.56, respectively. Notably, SuperGAT with intfloat/multilingual-e5-large-instruct achieves higher scores (AP: 0.6323, AUC: 0.6673), indicating a moderate level of effectiveness."}, {"title": "Node Classification on ASR Transcript", "content": "ASR-extracted utterances often lead to reduced accuracy due to noisy transcripts compared to human transcripts. The evaluation results in Table 6 for the node classification task exhibit a consistent trend with the previous findings, but with some differences. SAGE consistently outperforms others across all embedding types, achieving the highest AP score of 0.8851 and AUC score of 0.9204 with random embeddings, surpassing even the more sophisticated embedding methods, while GCN, GAT, and Super-GAT models lag behind significantly."}, {"title": "Multilingual Acoustic Pre-training (WER=28.8%)", "content": "Results in Table 7 show patterns in model performance for the node classification task on ASR transcript using multilingual acoustic pre-training. SAGE outperforms GCN, GAT, and SuperGAT across all embedding types. Notably, SAGE combined with the vinai/phobert-base-v2 embedding achieves peak performance, with AUC score of 0.9266. SAGE is further well-performed even with random embeddings. Comparatively, multilingual acoustic pre-training shows slight improvements across most models compared to monolingual pre-training, although WERs are relatively comparable. However, the Alibaba-NLP/gte-Qwen2-7B-instruct embedding significantly outperforms other embeddings and GNN models using multilingual acoustic pre-training. Therefore, multilingual LLM text embeddings achieve optimal performance on node classification task when applied on multilingual acoustic pre-training ASR transcript."}, {"title": "Link Prediction on ASR Transcript", "content": "The evaluation results for the link prediction task on ASR transcript using monolingual pre-training, as presented in Table 8, indicate the performance of different models and embeddings. Among the models, GCN consistently outperforms SAGE, GAT, and SuperGAT in both AP and AUC scores, achieving the highest scores with the intfloat/multilingual-e5-large-instruct embedding (AP: 0.9159, AUC: 0.9324). This suggests that GCN is more effective for link prediction tasks in this context, particularly when combined with the multilingual-e5-large-instruct embedding."}, {"title": "Multilingual Acoustic Pre-training (WER=28.8%)", "content": "Our empirical analysis of the link prediction task on ASR transcript using multilingual acoustic pre-training in Table 9 also reveals a distinct performance hierarchy among GNN architectures. The GCN emerges as the preeminent model, consistently outperforming its counterparts across all embedding configurations. Notably, the GCN variant utilizing the Alibaba-NLP/gte-large-en-v1.5 embedding achieves state-of-the-art performance, with an AP score of 0.9184 and AUC score of 0.9413. This stands in stark contrast to the SAGE architecture, which, despite its prowess in node classification, exhibits suboptimal performance in this link prediction task. The GAT and SuperGAT models demonstrate intermediate efficacy, marginally surpassing SAGE but falling significantly short of GCN's benchmark. The intfloat/multilingual-e5-large-instruct embedding consistently augments model performance, but with varying magnitudes of impact across architectures."}, {"title": "Error Analysis", "content": "Node classification and link prediction on human transcript: In inductive learning, BERT-based embeddings are essential for achieving optimal performance, whereas in transductive learning, random text embeddings demonstrate competitiveness with pre-trained embeddings.\nNode classification on ASR transcript: Firstly, in the context of noisy ASR transcripts, both monolingual and multilingual acoustic pre-training settings demonstrate that random text embeddings perform competitively with BERT-based text embeddings. For comparison, this transductive learning approach for ASR transcripts is similar to the transductive learning used for node classification on human transcripts. Secondly, multilingual LLM text embeddings notably outperform others in node classification tasks when applied to multilingual acoustic pre-training ASR transcripts. As our study is the first evaluation of training KGs from speech, there is no existing literature for direct comparison. However, combination of multilingual LLM text embeddings and multilingual acoustic pre-training generally yield higher accuracy across various downstream tasks, e.g. ASR (Lam-Yee-Mui et al., 2023; Radford et al., 2023), speech translation (Bapna et al., 2022; Babu et al., 2022; Zhang et al., 2023), text-to-speech (Saeki et al., 2023; Zhang et al., 2019). Thirdly, within the same transductive learning setting, node classification on ASR transcripts generally achieved competitive results compared to human transcripts, despite relatively high WERs of 28.8% and 29%.\nLink prediction on ASR transcript: Firstly, in the context of noisy ASR transcripts, both in monolingual and multilingual acoustic pre-training ASR settings, random text embeddings demonstrate performance comparable to BERT-based or LLM text embeddings. This transductive learning performance is also observed in the transductive learning of link prediction on human transcripts. Secondly, in the same transductive learning setting, link prediction on ASR transcripts generally outperformed that on human transcripts, despite relatively high WERs of 28.8% and 29%. This result is noteworthy, as high WERs in ASR transcripts typically degrade accuracy in various downstream NLP tasks, as widely demonstrated by the AI community (Desot et al., 2019; Sundararaman et al., 2021; Omachi et al., 2021). We hypothesized that the influence of text embedding, which primarily focuses on the generalized context of text segments (semantics), reduces the impact of ASR errors on prediction performance (Voleti et al., 2019). Thirdly, our transductive learning on ASR transcripts for both node classification and link prediction tasks was conducted in a zero-shot setting. We hypothesize that the adaptation of trained GNN models to the ASR transcripts of the training set could further enhance performance (Dinh, 2021; Ma et al., 2023)."}, {"title": "Conclusion", "content": "In this study, we introduce wav2graph, the first framework for supervised learning of KG from speech data. Additionally, we present the first real-world KG derived from speech, along with its baseline results.\nOur study demonstrates that, first of all, for node classification and link prediction tasks on ASR transcripts, both monolingual and multilingual acoustic pre-training with random text embeddings perform competitively with encoder-based and decoder-based embeddings. This trend is also observed in transductive learning on human transcripts. Secondly, multilingual LLM text embeddings significantly outperform other embeddings in node classification tasks when applied to multilingual acoustic pre-trained ASR transcripts. Thirdly, node classification on ASR transcripts generally achieves competitive results compared to human transcripts, while link prediction on ASR transcripts generally outperforms that on human transcripts, despite relatively high WERs of 28.8% and 29%. This unexpected behavior is likely due to the influence of text embeddings, which primarily focus on the generalized semantic context of text segments and therefore reduce the impact of ASR errors on prediction performance. This contrasts with most previous works on other downstream tasks, where high WERs in ASR transcripts typically degrade accuracy."}, {"title": "Related Works", "content": "This section describes works relevant to our wav2graph framework.\nKG from speech: In the domain of knowledge graph construction, traditional methodologies have predominantly focused on extracting information from textual sources (Zhong et al., 2023; Wang et al., 2021, 2014; Chen et al., 2021). Although there has been progress incorporating multimodal inputs, such as images and text (Zhu et al., 2022; Li et al., 2024; Alberts et al., 2021; Yu et al., 2021), the challenge of directly constructing knowledge graphs from speech data remains largely unexplored. Among the limited number of relevant works we could find to our best knowledge, Fu et al. (2021) introduced what they claimed to be the first automatic KG construction system from speech. Also, Wu et al. (2022) proposed a novel information extraction task, termed speech relation extraction, which utilized extracted relations from synthetic ASR transcripts to construct a KG. However, to the best of our knowledge, no training has been conducted on such KGs. Training KGs is crucial as GNN models can learn to extract and generalize complex patterns and relationships from the data within a KG, thereby enabling predictions on unseen data in the KG-capabilities that rule-based methods alone cannot achieve.\nInformation extraction from speech: The challenge of accurately identifying entities and their relationships within vast and unstructured data sources persists as a major barrier to broader text-based KG adoption (Peng et al., 2023). Such relationships are typically extracted through NER systems (Li et al., 2022, 2020). However, performing NER on speech, which is necessary for constructing voice-based KGs, remains a significant challenge (Chen et al., 2022; Sui et al., 2021; Szyma\u0144ski et al., 2023; Yadav et al., 2020; Caubri\u00e8re et al., 2020).\nGNNs for speech applications: GNNs have shown promise in various speech-related applications. For instance, Wang et al. (2020); Singh et al. (2023) applied GNNs to improve speaker diarization, while Pentari et al. (2024); Joshi et al. (2022); Li et al. (2023a) employed them for speech emotion recognition. Also, GNNs could be used in speaker verification task (Jung et al., 2021; He et al., 2024; Jung et al., 2022) and ASR hypothesis decoding (Dighe et al., 2020; Sun et al., 2022). Despite these advancements, existing GNN-based approaches do not address the direct construction and training of KGs from speech data, which enables the prediction of attributes and relations between spoken utterances.\nGNNs: Graph representation learning has evolved significantly in recent years, with various approaches designed to incorporate graph structure into meaningful node features. For example, Graph Convolutional Networks (GCN) (Kipf and Welling, 2016) and Message Passing Neural Networks (MPNN) (Gilmer et al., 2017) pioneered this field by proposing the message passing scheme, in which each node aggregates features from the adjacent nodes. Subsequent works like Graph Attention Networks (GAT) (Velickovic et al., 2017) introduced mechanisms to prioritize important nodes. However, these classical approaches often struggle with capturing long-range relationships (Dwivedi et al., 2022). To address this limitation, researchers have explored virtual nodes (Pham et al., 2017; Cai et al., 2023) and k-hop neighborhoods (Nikolentzos et al., 2020) within the message passing framework. More recently, graph transformers have gained prominence, with models such as TokenGT (Kim et al., 2024) and Graphormer (Ying et al., 2021) incorporating sophisticated encodings such as centrality and spatial information. GraphGPS further advanced this approach by combining various positional and structural encodings with multiple graph block types (Ramp\u00e1\u0161ek et al., 2022). Additionally, Ngo et al. (2023) proposed a multiscale graph transformer that learns hierarchical graph coarsening and utilizes graph wavelet transforms for positional encoding. These advancements in graph representation learning have significantly improved the ability to capture both local and global graph structures, paving the way for more effective graph-based machine learning models."}, {"title": "Details about Experimental Setups", "content": "This section describes details about experimental setups for experimental reproducibility, which extends the Section 4 in the main paper."}, {"title": "ASR Models", "content": "This section extends details of Section 4.1 in the main paper.\nGaussian Mixture Model / Hidden Markov Model (GMM/HMM): For language modelling and initial GMM/HMM, we followed the same setups and hyperparameters as in (L\u00fcscher et al., 2023). First, we used the toolkit Sequitur Grapheme-To-Phoneme (g2p) (Bisani and Ney, 2008) to convert pronunciation lexica found in human transcript, so that the seed lexicon was extended, creating the lexica for training. Secondly, we created an n-gram language model using previously extended lexica and human transcript based on Kneser-Ney Smoothing (Ney et al., 1994). Thirdly, we created alignments obtained by using GMM/HMM as labels for wav2vec 2.0 (Baevski et al., 2020b) neural network training, which later resulted to Deep Neural Network / Hidden Markov Model (DNN/HMM) training. The acoustic modeling employed context-dependent phonemic labels, triphones to be specific. In GMM/HMM process, we used a CART (Classification And Regression Tree) (Breiman, 2017) to tie the states s, resulting 4501 CART labels. During GMM/HMM process, we stopped at Speaker Adaptive Training stage (SAT) (Miao et al., 2015) instead of going beyond Speaker Adaptive Training + Vocal Tract Length Normalization (Eide and Gish, 1996) (SAT+VTLN) for the sake of good WER labels:\n$p(x_1^N) = \\sum_{[s_1^T]} \\prod_{t=1}^T p(x_t, s_t|s_{t-1}, w_1^N)$\n$= \\sum_{[s_1^T]} \\prod_{t=1}^T p(s_t|s_{t-1}, w_1^N) \\cdot p(x_t|s_t, s_{t-1}, w_1^N)\\tag{14}$\nUnsupervised DNN pre-training: For unsupervised DNN pre-training, we used wav2vec 2.0 (Baevski et al., 2020a) as DNN encoder with the help of Fairseq (Ott et al., 2019) toolkit. We employed similar vanilla configurations and hyperparameters as in (Le-Duc, 2023). All models had 118M parameters including 7 Convolutional Neural Network (CNN) (Fukushima, 1979, 1980) layers and 8 Transformer (Vaswani et al., 2017) layers. The last CNN layer had a stride halved for the 8kHz data (Vieting et al., 2023).\nDNN/HMM training: We loaded unsupervised pre-trained wav2vec 2.0 models for fine-tuning in a supervised DNN/HMM approach. We optimized the model with Framewise Cross-Entropy (fCE) loss. The SpecAugment (Park et al., 2019) data augmentation was applied for the entire 33 fine-tuning epochs. We used RETURNN toolkit (Zeyer et al., 2018) for supervised training.\nASR hypothesis decoding: The entire ASR hypothesis decoding was done using RASR toolkit (Rybach et al., 2011). In this stage, we integrated the acoustic model with the n-gram language model using Bayes' decision rule and the Viterbi algorithm (Forney, 1973). The Viterbi algorithm recursively computes the optimal path through the alignment graph of all potential word sequences, thereby identifying the best alignment with the acoustic observations. Then, pruning of the acoustic model and the n-gram language model through beam search was employed to focus exclusively on the most likely predicted words at each time step t (Ortmanns et al., 1997). Viterbi algorithm is described as:\n$\\hat{w}_1^N = \\underset{\\hat{N}_w}{\\arg \\max}  p (\\prod_{n=1}^N p(W_n|W_{n-m}) \\\\ \\max_{[s_1^T]} (\\prod_{t=1}^T P(x_t, S_t|S_{t-1}, w_1^N)))\tag{15}$\nMonolingual and multilingual pre-training: We utilized two best baseline models for ASR task on the VietMed dataset, as described by Le-Duc (2024). The models employed were a monolingual acoustic pre-trained w2v2-Viet and a multilingual acoustic pre-trained XLSR-53-Viet."}, {"title": "KG and GNN Models", "content": "This section extends details of Section 4.2 in the main paper.\nKG preprocessing: The KG is preprocessed, involving the identification of entity and relation types, attribute normalization, and potential feature engineering for edge features. The KG consists of two types of nodes: utterance (e.g., \"Doctors go to hospital\") and named_entity (e.g., \"doctors\"(PERSON) and \"hospital\" (LOCATION)). NEs are extracted from the utterances.\nHyperparameter Tuning: Hyperparameter tuning will focus on optimizing the combination of hidden layers and message passing aggregation functions for each GNN model. In the first setting, models will be trained with fixed hyperparameters: 250 epochs (10 epochs for SAGE), a learning rate of 0.005, weight decay of 0.05, Adam optimizer (Kingma and Ba, 2014), and dropout rates (Srivastava et al., 2014) of 0.2 for node classification tasks and 0.5 for link prediction tasks. The same hyperparameters will be applied for both the second and third settings."}, {"title": "Node Embeddings", "content": "This section extends details of Section 4.2 in the main paper.\nWe utilized both encoder-based embeddings, including the English Alibaba-NLP/gte-large-en-v1.5 (Li et al., 2023b), the multilingual"}]}