{"title": "COOD: Concept-based Zero-shot OOD Detection", "authors": ["Zhendong Liu", "Yi Nian", "Henry Peng Zou", "Li Li", "Xiyang Hu", "Yue Zhao"], "abstract": "How can models effectively detect out-of-distribution (OOD) samples in complex, multi-label settings without extensive retraining? Existing OOD detection methods struggle to capture the intricate semantic relationships and label co-occurrences inherent in multi-label settings, often requiring large amounts of training data and failing to generalize to unseen label combinations. While large language models have revolutionized zero-shot OOD detection, they primarily focus on single-label scenarios, leaving a critical gap in handling real-world tasks where samples can be associated with multiple interdependent labels. To address these challenges, we introduce COOD, a novel zero-shot multi-label OOD detection framework. COOD leverages pre-trained vision-language models, enhancing them with a concept-based label expansion strategy and a new scoring function. By enriching the semantic space with both positive and negative concepts for each label, our approach models complex label dependencies, precisely differentiating OOD samples without the need for additional training. Extensive experiments demonstrate that our method significantly outperforms existing approaches, achieving approximately 95% average AUROC on both VOC and COCO datasets, while maintaining robust performance across varying numbers of labels and different types of OOD samples.", "sections": [{"title": "1. Introduction", "content": "As machine learning (ML) models become essential in a range of real-world applications, out-of-distribution (OOD) detection has gained increasing importance [36]. OOD detection is particularly critical in fields such as autonomous driving [3], medical diagnostics [10], and surveillance [29], where managing data that deviates from the training distribution\u2014known as in-distribution (ID)\u2014is necessary to avoid safety risks or incorrect decisions [9]. Thus, developing robust OOD detection techniques is key to ensuring model reliability in unpredictable environments.\nLarge Models for OOD Detection (LM-OOD). The rise of large models, particularly Vision-Language Models (VLMs) and MultiModal Large Language Models (MLLMs), has redefined the landscape of OOD detection [34]. Traditional OOD methods, such as Maximum Softmax Probability (MSP) and Mahalanobis distance-based techniques [15], typically require extensive task-specific training on in-distribution (ID) data. In contrast, LLM-based methods leverage pre-trained knowledge, allowing for zero-shot and few-shot OOD detection capabilities. Models like CLIP [27] exemplify this shift by achieving effective OOD detection with minimal task-specific training, relying on robust, pre-trained representations from multimodal datasets. Recent advancements, such as NegLabel [11], refine zero-shot OOD detection by introducing negative mining strategies to identify semantically meaningful OOD categories. This approach highlights that carefully selected negative labels can significantly improve detection performance without retraining. LM-OOD methods offer distinct advantages over traditional approaches, including strong performance in limited data, high adaptability across various tasks, and improved computational efficiency through reduced dependence on task-specific data preparation and retraining [22, 24, 34].\nLimitations in Current LM-OOD. Large models for OOD detection are primarily designed for single-label tasks, where each input corresponds to a single, definitive label [6, 22]. However, many real-world applications are inherently multi-label in nature: medical imaging often requires identifying multiple coexisting conditions [10, 12]. Current state-of-the-art methods like NegLabel [11] encounter limitations in these multi-label contexts, as seen in Fig. 1: First, the complexity of semantic similarity computations increases significantly with multiple concurrent labels compared to single labels. Second, these methods assume OOD samples are semantically distant from ID classes, which may not hold in multi-label settings where novel combinations of known concepts could form valid OOD cases [25, 33]. Additionally, these approaches struggle to effectively model complex label"}, {"title": "2. Proposed COOD Method", "content": "2.1. Preliminaries on OOD Detection\nIn multi-label OOD detection, we aim to determine whether a given input image $I \\in I$ belongs to the ID or represents an unknown OOD sample. We denote the set of base labels as $B = \\{1,2,..., l\\vert_{18}\\vert\\}$, representing known classes within the ID. Each image $I$ can be associated with multiple labels from B, capturing the multi-label nature of this task.\nTo formalize this, we define a scoring function, $S_{ID} : I \\rightarrow R$, which assigns an ID score to each image. For simplicity, we frame our approach around this ID score rather than the OOD score, with the following criteria:\n$S_{ID}(I) = \\begin{cases}\n> \\gamma, & \\text{if } I \\sim D_{ID}, \\\\\n\\leq 1, & \\text{if } I \\sim D_{OOD},\n\\end{cases}$ (1)\nwhere $\\gamma$ is a threshold determined through validation, and $D_{ID}$ and $D_{OOD}$ are ID and OOD distributions, respectively. In this setup, $S_{ID}$ provides a measure of similarity to the ID distribution, aiding in distinguishing between known and unknown samples in a multi-label context.\n2.1.1. Revisit VLM-based OOD Detection\nVision-Language Models (VLMs) bring powerful multi-modal capabilities for OOD detection, allowing more flexible, context-aware identification of OOD samples [24, 34]. Large-scale pre-trained VLMs, such as CLIP [27] and GPT-4V [38], enable models to process images and text prompts together, enhancing adaptability and precision across diverse visual and textual domains [6]."}, {"title": "2.2. Overview of COOD Method", "content": "The COOD framework improves OOD detection by refining the decision boundary between ID and OOD samples using fine-grained concepts. By introducing positive and negative concept sets, P and N, COOD leverages LLMs to add contextual information around ID samples, thereby enhancing sensitivity to OOD cases.\nCOOD is built on two core components (see Fig. 2):\n1. Concept Generation and Similarity Measure (\u00a72.3):\nTo distinguish ID from OOD samples, we generate positive concepts (P) and negative concepts (N) that are closely and distantly related to ID classes, respectively. Positive concepts, mined using LLMs, capture domain-relevant features of ID samples at a fine-grained level, while negative concepts are chosen based on their semantic distance from ID classes to enhance contrast.\n2. Similarity and ID Score Computation (\u00a72.4): For each input image I, we compute similarity scores with these concept sets and the base label set B, forming a robust semantic space for evaluating ID-OOD relationships. After this, we define an ID score $S_{ID}(I)$ that aggregates the image's alignment with ID concepts (B and P) and contrasts it with the negative concepts (N). When this score falls below a predefined threshold $\\gamma$, the image is classified as OOD. This scoring method sharpens the decision boundary by leveraging contrasts between positive and negative concepts, giving more accurate OOD detection.\nThe pseudocode in Algo. 1 outlines the full procedure for COOD. To formalize the OOD detection decision, we compute an ID score and classify an image X as OOD if this score falls below a threshold $\\gamma$. Specifically, the decision function $\\Upsilon$ is defined as follows:\n$\\Upsilon = I(S_{ID}(h, P, N) <\\gamma)$, where $h = f_{img}(X)$,\n$P = f_{text}(prompt(P))$, $N = f_{text}(prompt(N))$. (3)\nHere, h is the image embedding for X, obtained using an image encoder $f_{img}$, while P and N are the positive and negative concept embeddings generated via text prompts for concept sets P and N, respectively. The indicator function $I$ produces the final OOD classification.\nTheoretical Justification. we derive theoretical bounds (\u00a72.5) that validate the scoring function's effectiveness in assigning higher scores to ID samples than OOD samples, underscoring the robustness of our approach.\nAdvantages. The proposed COOD approach is computationally efficient, leveraging pre-trained embeddings without requiring additional model training. By incorporating positive and negative concepts, the model gains enhanced semantic understanding, allowing for clearer differentiation between ID and OOD samples. The focus on top-k similarity values makes the method robust to noise, ensuring stable performance across varied datasets and label sets."}, {"title": "2.3. Concept Generation", "content": "Motivation. The concept generation process in COOD creates a rich semantic space to strengthen ID-OOD distinctions. By constructing a set of positive concepts, P, closely aligned with ID samples, and a set of negative concepts, N, that enhances the contrast with OOD samples, we can refine the decision boundary between ID and OOD.\nPositive Concept Mining P. To build a comprehensive set of positive concepts that accurately represents ID characteristics, we use a two-stage approach inspired by LF-CBM [26]. This process enriches each base label by capturing specific attributes that reinforce its identity within the ID class.\n1. Concept Querying: We prompt GPT-4 to generate concepts in three distinct contexts for each target object, using structured prompts tailored to elicit three categories: features ($prompt_{F}$), superclasses ($prompt_{S}$), and commonly associated items ($prompt_{C}$). This contextual querying ensures that each concept captures detailed, complex information, improving the clarity and consistency of the concept pool. Each prompt is crafted to encourage concise, relevant responses without qualifiers, which enhances precision, detailed in the Appendix ??.\n2. Concept Filtering: After generating these candidate concepts, we apply filtering criteria to retain only the most distinctive and relevant features. This step is essential to ensure that the concept set effectively captures the ID characteristics required to differentiate ID from OOD. Our filtering process, detailed in the Appendix ??, refines the generated concepts into a cohesive and meaningful set of positive labels.\nThe final positive concept set, P, is constructed as:\n$P = \\bigcup_{C_i \\in B} (prompt_F(c_i) \\cup prompt_S(c_i) \\cup prompt_C(c_i))$, (4)\nwhere B denotes the set of ID labels, and each $c_i \\in B$ is queried to generate its positive concept labels."}, {"title": "2.4. Similarity and ID Score Computation", "content": "After generating our positive and negative concepts, COOD computes an ID score for each input image I, based on its similarity with the positive concept set P, negative concept set N, and base labels B. This score helps determine if the image aligns more closely with ID or OOD characteristics.\nID Score Definition. The ID score $S_{ID}(I)$ for an image I is:\n$S_{ID}(I) = w_B\\cdot\\mu_k(B, I) - w_P\\cdot\\mu_k(P, I) - w_N\\cdot\\mu_k(N, I)$ (7)\nwhere $w_B$, $w_P$, and $w_N$ are weighting coefficients that determine the relative contribution of each set in computing the ID score, reflecting the importance of the base labels B, positive concepts P, and negative concepts N.\nTop-k Mean Similarity Calculation. The term $\\mu_k(S, I)$ in Eq. (7) represents the top-k mean similarity score between"}, {"title": "2.5. Theoretical Justification", "content": "Lemma 1 (Separability of ID-OOD). The scoring function can distinguish between in-distribution (ID) and out-of-distribution (OOD) samples based on their semantic relationships by appropriately selecting weights $w_P$ and $w_N$.\nProof. We assume the following bounds on the similarity scores $\\mu_k(\\cdot,\\cdot)$ between an image I and class representations in different contexts when I is either ID or OOD:\n$0 \\leq \\mu_k (B, I_{OOD}) < a < \\mu_k(B, I_{ID}) < A \\leq 1$,\n$0 \\leq \\mu_k (P, I_{OOD}) \\leq b < \\mu_k(P, I_{ID}) \\leq B \\leq 1$,\n$0 \\leq \\mu_k (N, I_{ID}) \\leq c < \\mu_k(N, I_{OOD}) < C < 1$.\nTo establish separability between ID and OOD samples, we examine bounds on the scoring function for in-distribution and out-of-distribution images.\n1. Lower Bound for $S_{ID}$: Based on the assumed bounds, the lower bound of the scoring function $S_{ID}$ for ID samples is: $S_{lower} = w_B A - w_P B - w_N c$.\n2. Upper Bound for $S_{ID}$: The upper bound of the scoring function $S_{ID}$ for OOD samples is: $S_{upper} = w_B A - w_N C$.\n3. Separability Condition: For the scoring function to effectively distinguish between ID and OOD samples, we"}, {"title": "3. Experimental Results", "content": "3.1. Experimental Setup\nDatasets and Settings. We evaluate our proposed multi-label OOD detection method on widely used datasets. For ID datasets, we use Pascal VOC [5], MS-COCO [19], and Objects365 [28]. For OOD datasets, we use Textures [2] and Filtered ImageNet22K [8]. These datasets provide diverse categories and rich multi-label annotations, making them ideal for assessing OOD detection in complex real-world scenarios. To quantify the OOD detection performance, we report two standard metrics, FPR@95 and AUROC. See Appx. ?? for more datasets and metrics details.\nImplementation Details. Our method is implemented using PyTorch and evaluated on NVIDIA 3090 GPU. As a zero-shot approach, our method leverages pre-computed concept embeddings and does not require any additional training. This results in a computationally efficient solution, with an average inference time of approximately 800 images per second. This efficient design allows for parallelized OOD detection alongside zero-shot classification tasks, making it suitable for deployment in real-time applications. For reproducibility, all hyperparameters and settings follow standard configurations, and further implementation details are provided in Appx. ??.\n3.2. Experimental Results and Analysis\nOOD Detection Performance Comparison. Table 1 presents a comparison of the OOD detection performance of COOD against several baseline approaches. Where * represents our reproduction of the missing results from the original paper, and \u2020 means that we used the values from the original paper, and \u2020* represents the best value we take from the replicated results and the reported results in the original paper. The gray boxes represent our approach. In addition, (Joint)-Energy denotes the better of the JointEnergy [32] method and the Energy [20] method. Bold values indicate the best results. Our method demonstrates strong OOD detection performance across both ResNet-based and ViT-based architectures. Specifically, our ResNet-based CLIP achieves superior results, particularly on the Pascal VOC ID dataset and Textures OOD dataset, with FPR95 of 8.76% and AUROC of 97.79%, outperforming all baseline methods."}, {"title": "3.3. Visual Interpretability of COOD Components", "content": "Figure 3 presents a detailed interpretability analysis of COOD through a t-SNE visualization [31] of text embeddings and several qualitative examples. The interpretability of COOD is a key feature, as it allows us to visualize and understand how specific concepts contribute to OOD detection.\nt-SNE Visualization. The t-SNE plot on the left side of Fig. 3 visualizes the embeddings of concepts from our model, showing a clear separation between ID labels (Pascal VOC, shown in red) and OOD concepts. Notably, the embeddings for positive concepts (shown in red) form distinct clusters separate from negative concepts (shown in blue). This clustering reflects the model's ability to encode meaningful semantic information that distinguishes ID from OOD samples. The clustering of positive concepts around ID points and the scattering of negative concepts in more distant regions confirm that the model effectively captures relationships that contribute to its decision-making process.\nQualitative Examples. The right side of Fig. 4 provides several example images with corresponding bar plots, illustrating the concept weights associated with each image. In each bar plot, positive concepts are marked in red, while negative concepts are marked in blue. This visual representation offers insight into which semantic features are influencing the OOD score, making it easier to interpret why certain samples are classified as ID or OOD.\n3.4. Ablation Studies and Additional Analysis\nTo assess the contributions of different components in our approach, we conduct an ablation study with variations in the parameters top-k, $S_A$, and $S_B$ on the Pascal VOC and COCO datasets, as shown in Table 2. For more experiments"}, {"title": "4. Related Work", "content": "4.1. Zero-Shot and Multi-Label OOD Detection\nWith the rise of contrastive learning-based cross-modal models like CLIP, traditional OOD detection methods face new challenges, particularly in zero-shot settings. In addition, the increasing complexity of image understanding tasks has introduced challenges in multi-label OOD detection.\nFor OOD detection in CLIP-based settings, several methods have been proposed, including both zero-shot and fine-tuning approaches. For example, ZOC [4] introduces a trainable generator and additional data to create extra OOD labels. Maximum Concept Matching (MCM) [22] uses textual embeddings of ID classes as concept prototypes, calculating an MCM score based on the cosine similarity between image and textual features to identify OOD instances. NegLabel [11] enhances OOD detection by incorporating a large set of negative labels, which are semantically distant from in-distribution (ID) labels, thereby improving the model's ability to distinguish OOD samples. GL-MCM [23] builds upon CLIP by considering both global and local visual-text alignments in the detection process. SeTAR [17] applies selective low-rank approximation to weight matrices with a greedy search algorithm for optimized OOD detection. NegPrompt [25] introduces transferable negative prompts for OOD detection, where each ID class is paired with learnable negative prompts, improving OOD detection by associating OOD samples more closely with negative prompts. However, these approaches lack a discussion of multi-label scenarios.\nIn multi-label tasks, some researchers have explored OOD detection as well. The JointEnergy-based method [32] is one of the first to address OOD detection in multi-label scenarios. A later improvement [21] further refines this approach. Additionally, YolOOD [40] explores OOD detection in multi-label tasks, using an object detection model as the backbone. However, these methods do not explore CLIP-based architectures. Methods utilizing multi-label data with CLIP, such as GL-MCM [23] and SeTAR [17], typically focus on curated datasets and fail to address the challenge of detecting OOD samples when multiple classes co-exist in the same image. In contrast, our work is the first to comprehensively explore how to effectively perform zero-shot OOD detection in multi-label tasks using CLIP models.\n4.2. Concept Bottlenecks\nConcept bottlenecks have emerged as a promising approach to improve model interpretability and robustness by explicitly modeling the intermediate concepts that influence predictions. Koh et al. [14] proposed using a feature extractor and a concept predictor to generate these \"bottleneck\" concepts, which are then used in a final predictor to determine class labels. Oikarinen et al. [26] introduced the Label-Free Concept Bottleneck Model (LF-CBM), which generates concepts without requiring manually labeled concept data. Instead, it employs unsupervised methods to automatically identify and extract meaningful concepts from the data. Xu et al. [35] introduced Energy-Based Concept Bottleneck Models (ECBM), which learn positive and negative concept embeddings to capture high-order nonlinear interactions between concepts, enabling richer concept explanations. Probabilistic Concept Bottleneck Models (ProbCBMs) [13] integrate uncertainty estimation with concept predictions. Recently, various other CBM variants have emerged, expanding the versatility of this approach. While these methods have been effective in extracting concepts and describing multiple labels within images, to the best of our knowledge, none have directly applied these concept-based techniques to multi-label and complex OOD detection tasks. Our approach focuses on the novel application of concept bottlenecks specifically for improving OOD detection in such settings."}, {"title": "5. Conclusion, Limitations, and Future Work", "content": "This work presents an in-depth exploration of using the CLIP model for efficient zero-shot OOD detection in complex multi-label scenarios. Our concept-based approach demonstrates superior OOD detection performance, achieving over 95% average AUROC on VOC and COCO (as ID datasets) with ImageNet and Texture as OOD datasets. Additionally, we conducted extensive experiments to evaluate the explainability and parameter sensitivity of the method."}]}