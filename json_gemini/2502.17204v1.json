{"title": "Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following", "authors": ["Jie Zeng", "Qianyu He", "Qingyu Ren", "Jiaqing Liang", "Yanghua Xiao", "Weikang Zhou", "Zeye Sun", "Fei Yu"], "abstract": "Real-world instructions with multiple constraints pose a significant challenge to existing large language models (LLMs). An observation is that the LLMs exhibit dramatic performance fluctuation when disturbing the order of the incorporated constraints. Yet, none of the existing works has systematically investigated this position bias problem in the field of multi-constraint instruction following. To bridge this gap, we design a probing task where we quantitatively measure the difficulty distribution of the constraints by a novel Difficulty Distribution Index (CDDI). Through the experimental results, we find that LLMs are more performant when presented with the constraints in a \"hard-to-easy\" order. This preference can be generalized to LLMs with different architecture or different sizes of parameters. Additionally, we conduct an explanation study, providing an intuitive insight into the correlation between the LLM's attention and constraint orders. Our code and dataset are publicly available at https://github.com/meowpass/PBIF.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have made impressive progress in massive natural language tasks (Wan et al., 2024; Zhang et al., 2024b) and have been applied to various real-world scenarios (Bai et al., 2023; Bi et al., 2024). To achieve satisfactory performance, it is crucial for LLMs to understand the user's instructions and convey desired outputs, which is known as the Instruction Following capacity of LLM (Yin et al., 2023; Xu et al., 2024).\nIn practice, instructions are usually incorporated with multiple constraints of different types, e.g., format constraint which limits the model's output to a specific format. Nevertheless, existing LLMs often struggle to follow multi-constraint instructions, making multi-constraint instruction follow-ing an obstacle to hinder LLMs' real-world application (Wen et al., 2024; Yin et al., 2023).\nRecently, a lot of works have demonstrated that LLMs are sensitive to the position of the referred context in many tasks, such as multi-document question answering, text evaluation, and list-wise ranking (Liu et al., 2024; Zheng et al., 2023; Tang et al., 2024). Since there are usually multiple constraints coexisting in the complex instruction, the position bias problem is also significant in multi-constraint instructions. As shown in Fig. 1, in the single-round scenario, the LLM's performance varies significantly when presented with instructions that have different constraint orders, even though the two instructions are semantically identical. When it comes to the multi-round scenario, different constraint orders impose different impacts on the intermediate responses, thus inevitably leading to a discrepancy in the quality of the final responses.\nNevertheless, the position bias of constraint orders in the multi-constraint instruction following remains an under-explored problem. Existing work manually assigns difficulty to different constraints based on a predefined rule and orders the constraints according to their difficulty. They empirically demonstrate the existence of LLMs' performance fluctuation brought by different constraint order (Chen et al., 2024). However, on the one hand, handcraft difficulty categorization fails to reflect the real difficulty disparity of different constraints (Dentella et al., 2024; Srivastava et al., 2023). On the other hand, they merely analyze the constraint order in a qualitative way, lacking a quantitative metric to measure the disparity of constraint order. Additionally, none of the existing works has provided an intuitive explanation for the position bias in multi-constraint instructions. It remains unclear how the LLMs handle instructions with different constraint orders.\nTo address all the problems above, we systematically investigate the position bias problem in the multi-constraint instructions. First, we propose a novel metric called the Constraint Difficulty Distribution Index (CDDI) to quantitatively describe the disparity of constraint order from the perspective of constraint difficulty. We leverage the accuracy of the LLM to quantify the difficulty of different constraints, thus precisely reflecting their disparity. Then, for a thorough study of the position bias problem, we design a probing task. As shown in Fig. 2, we construct a large number of multi-constraint instances with different constraint orders and explore two practical scenarios: single-round inference and multi-round inference. Our experiments find existing LLMs commonly perform better with the \u201chard-to-easy\u201d constraint orders, i.e., possibly placing harder constraints in former positions. Finally, to make an intuitive explanation of our findings, we resort to a gradient-based method (Wu et al., 2023). We visualize the importance of different constraints located in different positions. We observe that the constraint order will affect how the LLM handle the constraints and is highly correlated to the LLM's performance on a specific constraint.\nIn summary, our main contributions are as follows: (1) We are the first to systematically investigate"}, {"title": "2 Related Work", "content": "2.1 Complex Instruction Following\nRiding on the wave of the large language model, the instruction following has attracted increasing attention for it is easy to be perceived by the users (Zhou et al., 2023a; Lou et al., 2024). Practical instructions are complex, usually incorporated with multiple constraints of different types (Zhou et al., 2023b; He et al., 2024). A lot of evaluation benchmarks have found that multi-constraint instruction following is nontrivial for the LLMs (Jiang et al., 2023b; Wen et al., 2024; Qin et al., 2024). Consequently, several works propose to improve the LLM's complex instruction following capacity by introducing additional instruction fine-tuning (Sun et al., 2024; Cheng et al., 2024; Zhang et al., 2024a).\nDifferent from these works, we focus on the inference stage of the LLMs instead of model training. Especially, we aim to investigate the position bias problem brought by the constraint order, which poses an essential impact on the model performance.\n2.2 Position Bias in the LLM\nThe position bias problem is common in the various LLM tasks (Liu et al., 2024; Zheng et al., 2023; Zeng et al., 2023). Researchers fisrt find that the LLM's performance degrades dramatically by merely changing the order of relevant information in the long-context question answering. A lot of works have studied the position bias problem in the field of logical reasoning (Chen et al.; Liu et al., 2023; Berglund et al., 2023). They find the LLM is sensitive to the order of premises, although such ordering actually does not alter the reasoning task (Chen et al.; Liu et al., 2023).\nDespite so, none of these works has studied the position bias problem in the field of instruction following, especially multi-constraint instruction following. SIFo (Chen et al., 2024) is the most related work to ours. They manually differentiate the constraints based on the context length they will influence and conduct an empirical study to verify whether the model performance will be affected by the constraint order. However, Their investigation of position bias is fairly qualitative. Different from them, we are the first to make a systematical and thorough investigation on the position bias of constraints in multi-constraint instruction following."}, {"title": "3 Method", "content": "3.1 Background\nIn this paper, we mainly focus on the multi-constraint instruction $I_c$. It can be formulated as a seed instruction incorporated with n constraints:\n$I_c \\triangleq I_s \\oplus C_1 ... C_n$,\n(1)\nwhere the seed instructions $I_s$ describe a task, e.g., write a story, while these constraints $\\sum_{i=1}^{n} C_i$ limit the output from different aspects, e.g., format, length, content, etc. $\\oplus$ stands for the concatenation operation.\n3.2 Probing Task\n3.2.1 Task Formulation\nTo investigate the impact of constraint order, we introduce a probing task. In this task, the LLM is given multi-constraint instructions with constraints arranged in various orders. The LLM's task is to generate a response that follows all constraints. We evaluate the LLM in two practical scenarios: single-round and multi-round inference. The LLM's responses are then evaluated to determine its performance across various constraints. The overall procedure is illustrated in Fig. 2. In the following sections, we will provide a detailed explanation.\n3.2.2 Multi-constraint Instruction Synthesis\nTo ensure the generalizability of probing data, we construct the initial multi-constraint instructions which include a variety of tasks and diverse constraint combinations. The multi-constraint instruction synthesis can be further divided into two parts: seed sampling and constraint sampling."}, {"title": "3.2.3 Constraint Reordering", "content": "To quantitatively construct instructions with different constraint orders, here are two questions that need to be answered: (1) How do we distinguish the disparity of different constraints? (2) After we order the constraints based on their disparity, how do we quantitatively describe the disparity of constraint orders?\nAn appropriate solution for the first question is to categorize the constraints based on their difficulty (Chen et al., 2024). In this paper, we also sort the constraints based on their difficulty. However, different from existing works which designate the difficulty of the constraints based on handcraft rules, we measure the difficulty of a constraint via the overall accuracy of following it in our probing datasets. The formulation is as follows:\n$Dff_c = Softmax(1 - Acc_c)$,\n(2)\n$Acc_c = \\frac{1}{N_x}\\sum_{i=1}^{N_x}c_i$,\n(3)\nThe $C_r$ refers to a specific type of constraint, the $N_x$ stands for the total number of instructions corresponding to the constraint $C_r$, and the $c_i$ is a binary value to reflect whether the constraint $C_x$ is followed in the $i^{th}$ instruction.\nTo quantitatively describe the disparity of constraint order, we propose a novel metric called the Constraint Difficulty Distribution Index (CDDI) which quantifies a specific constraint order based on its difficulty distribution. Given the difficulty of different types of constraints, we can readily attain the difficulty distribution of the constraints incorporated in the multi-constraint instructions. Specifically, for a multi-constraint instruction, we rank the incorporated constraints based on their difficulty, from the hardest to the easiest. We set this \"hard-to-easy\u201d constraint order as an anchor since it depicts an extreme situation, i.e., we designate the CDDI = 1 when the constraints fall in this order. Consequently, akin to the Kendall tau distance (Cicirello, 2020), we measure the difficulty distribution of a specific constraint order o by comparing it with the \u201chard-to-easy\u201d constraint order $O_{max}$. The formula is shown as:\n$CDDI = \\frac{N_{con}-N_{dis}}{N_{pair}} = \\frac{2(N_{con}-N_{dis})}{n(n-1)}$,\n(4)\nwhere $N_{con}$ and $N_{dis}$ represent the number of concordant and discordant distribution pairs of constraints between o and $O_{max}$, respectively. The $N_{pair}$ is the total number of compared constraint pairs. Overall, we select $n_{dd}$ different difficulty distributions, finally comprising $N_{seed} \\times N_{cc} \\times N_{dd}$ instances."}, {"title": "3.2.4 Sequential-Sensitive Inference", "content": "Given the multi-constraint instructions with different constraint orders, we evaluate the model's performance in two common scenarios: single-round inference and multi-round inference. In single-round inference, the LLM is directly given the multi-constraint instructions with different constraint distributions. We argue that different constraint distributions could impose different levels of difficulty on the LLM to handle. The multi-round inference introduces a more typical setting: the user will first provide the LLM with the core intention (i.e., the seed instruction in this work), and then iteratively put forward the constraints in order to obtain a final response.\nTo evaluate the model performance, apart from the constraint following accuracy mentioned in"}, {"title": "4 Empirical Study", "content": "4.1 Experiment Setup\nModels For our probing task, to ensure the generalizability of our study, we conduct experiments on both closed and open-source LLMs with varying architectures and parameter sizes. Specifically, we introduce the following models: (1) LLaMA3-8B-Instruct and LLaMA3-70B-Instruct (Dubey et al., 2024). (2) LLaMA2-13B-Chat (Touvron et al., 2023). (3) Mistral-7B-Instruct (Jiang et al., 2023a).2 (4) Qwen2.5-7B-Instruct (Yang et al., 2024). (5) GPT40-mini (Achiam et al., 2023).\nDatasets We construct various multi-constraint instructions with different constraint orders (Sec.3.2). We empirically set the number of constraints n to 7. To ensure the diversity and complexity, we set the number of constraint combinations $n_{cc}$ to 10 and the number of difficulty distributions $n_{dd}$ to 12, finally obtaining 200 \u00d7 10 \u00d7 12 = 24K samples. To verify the influence of constraint number, we also conduct experiments on the setting"}, {"title": "4.2 Results", "content": "LLMs prefer to \u201chard-to-easy\u201d constraint distribution. As shown in Fig. 4, most of the LLMs exhibit a dramatic performance fluctuation on instructions with varying constraint distributions. When the constraint number is set to 7, the LLaMA3-8B-Instruct and Qwen2.5-7B-Instruct show approximately 7% and 5% performance disparity in extreme situations. This indicates the vulnerability of existing LLMs to the position bias brought by the constraint order. Also, the LLMs tend to be more performant to instructions with higher CDDI values. Even the LLaMA3-70B-Instruct exposes a clear preference for higher CDDI value as the number of constraints increases to 9, demonstrating that \"hard-to-easy\" is a superior constraint distribution for existing LLMs.\nMulti-round inference exhibits more severe position bias compared with the single-round inference. The LLMs' performance in multi-round inference is presented in the Fig. 5. Compared with the results in the single-round inference, the performance gap becomes more prominent. All the LLMs gain approximately 10% improvement on C_level accuracy. Surprisingly, the LLaMA3-8B-Instruct and LLaMA3-70B-Instruct achieve approximately 25% performance improvement by changing the constraint distribution from \u201ceasy-to-hard\" (CDDI=-1) to \u201chard-to-easy\u201d (CDDI=1). This indicates that the LLMs are more sensitive to the position bias problem in a multi-round scenario.\nLLMs perform better in multi-round inference when provided with the instructions in appropriate constraint order Comparing the results in single-round (Fig. 4) and multi-round inference (Fig. 5), we observe that the LLMs reach better performance if the incorporated constraints are arranged in an appropriate order. Specifically, when the CDDI value is negative, the performance of LLMs in multi-round inference lags behind that in single-round inference. Nevertheless, with the increase of the CDDI value, the LLMs can achieve superior performance in multi-round inference and reach their best performance in CDDI=1. An exception is the Mistral-7B-Instruct-v0.3. We attribute this to its inferiority in processing multi-round information (Chen et al., 2024)."}, {"title": "4.3 Robustness of CDDI", "content": "Since the CDDI is calculated by comparing the concordant and discordant pairs of two different constraint orders, there are usually multiple constraint orders sharing the same CDDI value. Therefore, we conduct a testing experiment to assess whether the LLM exhibits significant fluctuations across different constraint orders with the same CDDI value. Specifically, we set the CDDI to -0.05, a value that includes the most constraint orders in our setting, and conduct single-round inference for 3 times. The experiment results are shown in Tab 2. We calculate the P-value of the data, finding that the P-value is much larger than 0.05. This indicates that the fluctuation of LLM's performance is negligible among different constraint orders in the same CDDI value."}, {"title": "5 Explanation Study", "content": "5.1 Explanation Metric\nTo make an explanation for the influence brought by the constraints of different orders, we make an explanation study on where the LLMs mainly focus when handling multi-constraint instructions via a feature attribution-based explanation method (Li"}, {"title": "5.3 Results", "content": "Hard-to-easy constraint order induces the LLM to pay more attention to the constraint part in the multi-constraint instructions. We visualize the importance weights of the model on the constraints in different positions. As shown in Fig. 6 (a), in the multi-constraint instruction following, the model's attention on different positions varies with changes in the constraint orders. Specifically, when the constraints are randomly distributed across different positions (represented by CDDI=-0.05), the model assigns similar attention to all positions. As the constraint order becomes more structured (represented by CDDI=-1 and CDDI=1), the model's attention neither exhibits the \"lost in the middle\" phenomenon observed in long-context processing (Liu et al., 2024), nor a simply sequential distribution, but follows an iterative, laddered order. Then, in Fig. 6 (b), we present the total importance weight the model assigns to the constraint part. We observe that the \"hard-to-easy\" constraint order attracts the most attention from the model towards the constraint part, which provides an explanation for the superiority of this constraint order.\nThe LLM's performance on various constraints is strongly correlated with its attention patterns. The importance weights of the model on different types of constraints are presented in Fig. 7. Among the three distinct difficulty distributions, the \"hard-to-easy\" (represented by CDDI = 1) assigns the highest importance weights to various types of constraints except for the Content and Startend. It is worth noting that this is exactly in accord with quantitative results in Tab. 1, i.e., as the CDDI value increases, the model's performance on the Content and Startend constraints shows a decreasing trend instead. Overall, the results show that the model's accuracy in following a specific type of constraint is strongly correlated with the attention assigned to it by the model."}, {"title": "6 Conclusion", "content": "In this paper, we systematically investigate the position bias problem in the multi-constraint instruction following. To quantitatively measure the disparity of constraint order, we propose a novel Difficulty Distribution Index (CDDI). Based on the CDDI, we design a probing task. First, we construct a large number of instructions consisting of different constraint orders. Then, we conduct experiments in two distinct scenarios. Extensive results reveal a clear preference of LLMs for \u201chard-to-easy\u201d constraint orders. To further explore this, we conduct an explanation study. We visualize the importance of different constraints located in different positions and demonstrate the strong correlation between the model's attention distribution and its performance."}, {"title": "7 Limitations", "content": "Our work mainly focuses on the position bias problem in the multi-constraint instruction following. We make a quantitative analysis of the influence brought by different constraint orders in the instructions. However, there are still some limitations. The constraints in our work are usually parallel to each other, which means the order change will not affect the semantic meaning of the instructions. The position bias problem for for those sequential constraints need to be further explored. Moreover, we only investigate the phenomenon of position bias in existing LLM without offering a solution. In further work, we will conduct a further probing task in sequential constraints to improve the generalization of our findings."}, {"title": "A Appendix", "content": "A.1 Implementation Details\nWe utilize 8 NVIDIA A800 80GB GPUs to conduct all the experiments. We employ the vLLM framework (Kwon et al., 2023) to accelerate the model inference. For reproducibility, we employ the greed search in the whole inference (i.e., setting the \"do_sample\" to false.).\nA.2 More details for Comstraint Sampling\nIn this work, We categorize the constraints into 8 different groups. The categorization is shown in the Tab. 3. For each group, there are multiple types of constraints. Specifically, the constraints are designated to: (1) Keyword constraints. These constraints focus on controlling the inclusion or exclusion of specific words or phrases within the response. (2) Language constraints. Language constraints govern the linguistic properties of the response, including the language in which the response is written (e.g., English). (3) Length constraints. These constraints focus on controlling the overall length of the response, including the number of paragraphs, words, and sentences. (4) Content Constraints. Content-related constraints define additional rules to ensure the response contains specific elements. (5) Format constraints. Formatting"}]}