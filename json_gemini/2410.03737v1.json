{"title": "Meta Reinforcement Learning Approach for Adaptive Resource Optimization in O-RAN", "authors": ["Fatemeh Lotfi", "Fatemeh Afghah"], "abstract": "As wireless networks grow to support more complex applications, the Open Radio Access Network (O-RAN) architecture, with its smart RAN Intelligent Controller (RIC) modules, becomes a crucial solution for real-time network data collection, analysis, and dynamic management of network resources including radio resource blocks and downlink power allocation. Utilizing artificial intelligence (AI) and machine learning (ML), O-RAN addresses the variable demands of modern networks with unprecedented efficiency and adaptability. Despite progress in using ML-based strategies for network optimization, challenges remain, particularly in the dynamic allocation of resources in unpredictable environments. This paper proposes a novel Meta Deep Reinforcement Learning (Meta-DRL) strategy, inspired by Model-Agnostic Meta-Learning (MAML), to advance resource block and downlink power allocation in O-RAN. Our approach leverages O-RAN's disaggregated architecture with virtual distributed units (DUs) and meta-DRL strategies, enabling adaptive and localized decision-making that significantly enhances network efficiency. By integrating meta-learning, our system quickly adapts to new network conditions, optimizing resource allocation in real-time. This results in a 19.8% improvement in network management performance over traditional methods, advancing the capabilities of next-generation wireless networks.", "sections": [{"title": "I. INTRODUCTION", "content": "The advancement of wireless networks to support diverse and demanding applications is greatly enhanced by the Open Radio Access Network (O-RAN) architecture, particularly its RAN Intelligent Controller (RIC) modules [1], [2]. These modules boost network functionality through intelligent resource management and sophisticated control techniques, essential for delivering advanced services by enabling real-time data collection and analysis [1]. Moreover, integrating artificial intelligence (AI) and machine learning (ML) within these modules facilitates dynamic resource allocation, enhancing operational efficiency and adaptability to rapidly changing conditions. Central to this innovation, RIC modules employ open and standardized interfaces for both real-time and non-real-time control, making the network more intelligent, fully virtualized, and interoperable [3].\nML-based strategies, particularly for adaptive network configurations, are crucial in the dynamic realm of wireless networks. The RIC's ability to utilize key performance indicators (KPIs) and perform real-time service analysis enables the network to adjust to fluctuating demands dynamically. Despite extensive research into ML-based power and resource allocation, significant challenges remain in managing the complexities of real-time resource management under unpredictable conditions [4]\u2013[8]. These challenges are exacerbated in the dynamic and complex landscape of O-RAN, where the network must rapidly adapt by integrating virtualized distributed units (DUs) and deploying xApps based on real-time analysis. Scheduling presents a particularly acute challenge in this context, especially in densely populated or hotspot areas where demand can surge unexpectedly and where corner cases, scenarios beyond standard operating parameters, commonly occur. This unpredictability necessitates highly adaptive scheduling systems capable of responding swiftly to changing conditions. To address these critical needs, we have chosen the enhanced mobile broadband (eMBB) service slice, known for its high data rate requirements, as a representative use case in densely populated areas. This selection highlights the imperative for advanced scheduling solutions that can adeptly handle and respond to users' unpredictable and varied demands.\nAdvanced scheduling strategies are essential to address these intricate challenges, including managing corner cases and achieving rapid convergence. We utilize meta-learning in a scheme that significantly enhances the efficiency and adaptability of Deep Reinforcement Learning (DRL) approaches, proving particularly advantageous in few-shot learning scenarios where deriving meaningful insights from small data samples is crucial [4], [9]\u2013[11]. Unlike federated learning (FL), which focuses on decentralized model training without exchanging local data, meta-learning optimizes the learning algorithm to enhance its ability to generalize from past experiences to new situations. Meta-learning stands out by leveraging this prior knowledge to accelerate the learning process, distilling reusable skills to facilitate rapid adaptation to new tasks. This is especially beneficial in the dynamic landscapes of wireless networks, where conditions and service demands frequently shift and are often application-dependent. Building on this foundation, we develop a novel meta-learning-based joint resource block and downlink power allocation solution inspired by Model-Agnostic Meta-Learning (MAML) in advanced wireless O-RAN network architectures [12]. Implemented within advanced wireless O-RAN architectures, our strategy mitigates system performance disruptions. It ensures optimal service delivery to user equipment (UEs), establishing a new network optimization standard in highly dynamic environments.\nOur architecture capitalizes on the disaggregated structure of O-RAN by strategically placing distributed DRL agents at DU locations for targeted learning tasks. This configuration exploits the distributed nature of DU modules, each augmented with individual xApps, to achieve remarkable scalability, adaptability, and localized decision-making-crucial for navigating"}, {"title": "II. RELATED WORKS", "content": "The evolution of wireless networks necessitates advancements in O-RAN architecture, particularly by adopting intelligent frameworks such as meta-learning and DRL, to enhance network efficiency and adaptability. The study by Erdol et al. [11] demonstrates a meta-federated RL-based traffic steering algorithm within the non-real-time RIC (non-RT RIC) for Radio Access Technology (RAT) allocation aimed at fulfilling users' Quality of Service (QoS) requirements. This algorithm employs Deep Q-Networks (DQN) to dynamically steer traffic between different RATs, thus optimizing network performance and user experience. While federated RL offers potential for distributed learning, meta-RL stands out in O-RAN scenarios for its quick adaptability and seamless architectural fit, promising more effective scalability and implementation. Furthermore, Ji et al. [4] discuss distributed resource allocation strategies to maximize energy efficiency and maintain QoS for UEs through a meta-federated RL approach. This method optimizes both channel assignment and transmission power concurrently. In parallel, Yuan et al. [10] explore the application of meta-federated RL to solve resource and power allocation challenges within vehicle-to-everything (V2X) communication scenarios. However, the approaches by Ji et al. and Yuan et al., which utilize power quantization for allocation, are noted to introduce a certain error level.\nAdditionally, research presented in [6] outlines a sequential method for resource optimization, which, despite its potential, may yield suboptimal solutions due to a failure to account for interdependencies between parameters. This method's iterative nature, focusing on fixing and optimizing parameters in isolation, could lead to slower convergence and inferior outcomes. Kalntis et al. [13] introduce an adaptive meta-learning-based online learning framework for resource allocation within O-RAN's virtualized Base Stations (vBS), showcasing the framework's adaptability to network changes. Nevertheless, the simplicity of their policy selection and the static nature of their power allocation suggest that further enhancements are necessary to fully exploit O-RAN's dynamic capabilities. Lastly, Raftopoulos et al. [5] delve into managing fluctuating Service Level Agreements (SLAs), highlighting the challenge of balancing latency requirements against other network demands as an area ripe for further exploration. Together, these studies point towards developing more autonomous, efficient, and intelligent O-RAN architectures, underscoring the importance of comprehensive strategies that integrate diverse technologies to optimize a wide range of network performance metrics."}, {"title": "III. SYSTEM MODEL", "content": "In our envisioned O-RAN architecture for a wireless communication network, we integrate multiple Central Units (CUs), including the Control Plane (CU-CP) for network orchestration and the User Plane (CU-UP) for data processing, along with DUs and Radio Units (RUs) covering macro and small cells. All components are coordinated by the RIC to dynamically allocate resource blocks and downlink power, as illustrated in Fig. 1.\nWe consider a downlink orthogonal frequency-division multiple access (OFDMA) system where a single CU module oversees multiple DU/RU paired units. Within this framework, each DU/RU unit is pre-configured and equipped with its own $K_p$, dedicated resource blocks (RBs) to serve $N_o$ assigned UEs. These units are integrated into the network infrastructure and connect dynamically as needed based on real-time demand or operational requirements. To address the challenge of fair service delivery, especially in densely populated areas with high demand for eMBB services, we frame our system goal as a max-min problem. This approach aims to maximize the minimum data rate among all UEs, ensuring that no single user suffers from extremely poor service due to the skewed allocation of resources. Such an approach is crucial in maintaining"}, {"title": "A. Achievable Data Rate", "content": "service fairness, as it guarantees that the network's resources are distributed in a manner that lifts the lowest-performing connections to an acceptable standard. Each UE's data rate, significantly influenced by the allocated RBs and transmission power, becomes a focal point of our optimization efforts. Given $K_p$ RBs, and the maximum ($P^{max}$) and minimum ($P^{min}$) transmission powers, we navigate through the constraints of available RBs and downlink transmission power to ensure efficient and equitable resource distribution.\nThe achievable data rate of each UE $u$ from the assigned RU and DU $p$, by considering RU-UE channels as Rayleigh fading and assuming the proposed system model works in a discrete time frame as $t\\in [1, 2, .., T]$ can be expressed as:\n$C_{u,p}(t) = \\sum_{k=1}^{K_p} B e_{u,k} \\log_2 (1 + \\frac{P_{k,p}(t)e_{u,k}|h_{u,k}(t)|^2}{I_{u,k}(t) + \\sigma^2}),$ (1)\nwhere $e_{u,k} \\in {0, 1}$ is a binary variable indicating RB allocation for user $u$ in RB $k$, and $B$ represents RB bandwidth. $P_{k,p}$ denotes transmission power allocated to the RB $k$ of RU $p$, and $d_{u,p}(t)$ represents the distance between RU $p$ and UE $u$ in each time frame which is time-varying variable because of the UEs mobility. Moreover, $\\eta$ shows the path loss exponent, and $|h_{u,k}(t)|^2$ indicates the time-varying Rayleigh fading channel gain each time. In equation (1), the term $I_{u,k}(t) = \\sum_{\\rho'\\neq \\rho} \\sum_{u' \\neq u} C_{u',k}P_{k,p'}(t)d_{u',p'}(t)^{-\\eta}|h_{u,k}(t)|^2,$ represents downlink interference from neighboring RUs on RB $k$, while $\\sigma^2$ is indicative of the variance associated with the additive white Gaussian noise (AWGN)."}, {"title": "B. Problem Formulation", "content": "Our study aims to maximize the minimum data rate among all UEs in the network, ensuring fair access to resources. To achieve this, we define $C_P(p, e) = \\min(c_u(t))$, $u\\in N_p$, where $c_u(t)$ is a vector of $c_{u,p}(t)$, and $p$, $e$ indicates the vector of downlink transmission powers $p_{k,p}(t)$ and the matrix of RB allocation indicator $e_{u,k}$, respectively. We have thus formulated an optimization problem aiming for the best policy to jointly allocate resource blocks and transmission powers to UEs, constrained by the availability of network RBs and transmission powers to control interference. This methodology seeks to balance the diverse demands of UEs under variable channel conditions, optimizing the network's overall performance while adhering to operational constraints.\n$\\arg \\max_{p,e} C_P(p, e),$ (2a)\ns.t.,$\\sum_{u=1}^{N_o} \\sum_{k=1}^{K_p} e_{u,k} \\leq K_p,$ (2b)\n$P_{min} \\leq P_{k,p} \\leq P_{max},$ (2c)\n$e_{u,k} \\in {0, 1}, \\forall u \\in N_p,$ (2d)\nwhere constraints (2b) and (2c) define the limits on RB availability and transmission power, ensuring that interference management and allocations stay within the network's capacity. This underscores the challenge of allocating resources and power intelligently to ensure fair data rate distribution among UEs. Given the NP-hard nature and mixed-integer stochastic elements of this problem, direct solutions are challenging. Thus, applying the Markov decision process (MDP) framework to make informed decisions in uncertain conditions becomes vital. Translating the problem into an MDP and using dynamic resolution methods like DRL provides a strategic solution to this complex optimization issue."}, {"title": "IV. PROPOSED MAML-INSPIRED META-RL FRAMEWORK", "content": "To make a structured method for making informed decisions under uncertainty and using dynamic resolution techniques, we model the defined optimization problem (2a) as an MDP."}, {"title": "A. MDP model", "content": "The optimization problem (2a) can be represented as an MDP with tuples $(S, A, T, \\gamma, r)$, where $S$, $A$, and $T$ represent the state space, action space, and transition probability from the current state to the next state as $P(s_{t+1} \\vert s_t)$, respectively, and $\\gamma$ represents the discount factor and $r$ is reward function. The MDP tuples are described as follows:\n1) State: At each timestep, $t$, the state $s_t \\in S$ represents the current O-RAN status, encompassing UEs' QoS metrics-average $Q_a$, minimum $Q_m$, and maximum $Q_x$ values, alongside the most recent resource $a_{r,t-1}$ and power allocation actions $a_{p,t-1}$. In our scenario, the UEs' QoS is defined as UEs' throughput. Hence, the agent's observation at time $t$ is denoted as $s_t = \\{Q_a, Q_m, Q_x, a_{r,t-1}, a_{p,t-1} | \\forall u \\in N,\\}.\n2) Action: At each timestep, $t$, the action vector $a_t \\in A$ specifies the required number of resource blocks and the necessary transmission power for each UE. Consequently, the agent employs its policy at time $t$ to select an optimal action, represented as $a_t = \\{e,p\\}$, where $e$ denotes the indicator vector of RB's assigned to the UE and $p$ represents the vector of transmission powers assigned to the UEs.\n3) Reward: We incorporate reward and penalty components to design an efficient resource utilization function. This function assesses RB usage and transmission power to UEs, rewarding strategies that optimize resource and power efficiency while penalizing excess use to mitigate interference. Specifically, penalties are applied based on excess transmission power $p_c = \\sum_{u=1}^{N_K} \\sum_{k=1}^{N_K} C_{u,k}P_{k,p}$ to prevent interference. To enforce resource conservation, a penalty is also imposed for exceeding the available RBs, calculated as $K = max(0, \\sum_{k=1}^{N_K} \\sum_{u=1}^{N_K} C_{u,k}- K_p)$. This method ensures the reward function promotes efficient allocation, enhances UE performance, and prevents excessive consumption of RBs. The reward $r_t$ combines sigmoid functions of UEs' normalized minimum QoS, consumed power, and remaining RBs, is represented as $r_t = sigmoid(Q_m) - sigmoid(p_c) - sigmoid(K)$, with $Q_m = \\frac{Q_m-C_m}{C_x-C_m}$, where $C_m$ and $C_x$ show the minimum and maximum values of service demands, representing normalized minimum QoS. This approach optimizes bandwidth and transmission power while meeting UEs' minimum QoS requirements. It guides the exploration of an MDP model using a DRL approach to discover the optimal policy $\\pi^*(a_t|s_t; \\theta_p)$ for RB allocation and power assignment.\nEmploying a DRL strategy, we address adaptive resource and power allocation in dynamic networks, optimizing performance within constraints and ensuring training stability."}, {"title": "B. Proposed MAML-DRL approach", "content": "We develop a novel framework that deploys distributed RL agents across the network to enhance stability and convergence, innovatively optimizing physical resource blocks and power allocation based on MAML-RL principles. Furthermore, to address the inaccuracies in power allocation identified in prior research [4], [10], which arise from power quantization, we propose a continuous power allocation method using the Deep Deterministic Policy Gradient (DDPG) technique, aiming to enhance the accuracy and efficiency of system performance significantly.\nIn O-RAN wireless networks, which are equipped with RIC modules that provide sufficient computational resources, MAML outshines other meta-learning approaches like Reptile. It rapidly adapts to changes in the dynamic wireless environment through a few gradient steps, offering quicker and more efficient adjustments in complex RL scenarios [15]. This adaptability makes MAML particularly suitable for the fast-evolving conditions of wireless environments. In the context of meta-RL, a 'shot' refers to each distinct interaction or episode an agent uses to adapt to new tasks quickly. Few-shot learning involves the agent effectively handling new challenges with minimal prior exposure, typically from just a few interactions. This capability is critical for rapid adaptation in our network's continuously evolving demand scenarios. Further implementing our meta-learning strategy through a meta-controller within the near-real-time RIC module, functioning as an xApp, allows for continuous training of meta-learner RL agents across the network. When there is a need to integrate a new RL agent (i.e., a virtualized DU) due to increased UE density or critical conditions, the meta-controller facilitates the agent's adaptation, enabling more effective decision-making."}, {"title": "C. Meta-learning tasks", "content": "Our goal in meta-learning is to train $N_g$ distinct meta-learner agents located in distributed DUs, each capable of quickly adapting to new tasks/environments with minimal training. These agents operate in unique environments, each with its own state spaces S, action spaces A, and customized reward functions $R_g$, preparing them for various environments and varying demands across network locations. By creating tasks that reflect the diversity of potential operational scenarios, we aim to improve our model's robustness and adaptability, allowing for swift adjustment to new challenges with minimal adaptation steps, as detailed in [15]. $R_g = \\{r_t | Q_m = \\frac{Q_m-C_{m,g}}{C_{x,g}-C_{m,g}}, \\forall g \\in [0, N_g], \\}$ shows each task specific reward function where $C_{m,g}$ are considered distinct for each individual environment. Our goal in meta-learning is to train $N_g$ Here, the meta-learner agent g has a support set $B_{tor}$ to train the model within each task and a query set $B_{ual}$ for weight updating of meta-model training. It helps to calculate the meta-objective, which is the loss computed on the query set predictions, and this meta-objective guides the update of the meta-parameters according to the following equations.\n$\\theta_{t,g} = \\theta_{t-1,g} - \\alpha \\nabla_{\\theta_g} L(\\theta_{t-1,g}, \\beta_{tr}), \\forall g \\in [0, N_g],$ (3)\n$\\theta_{\\tau,M} = \\theta_{\\tau-1,M} - \\alpha \\nabla_{\\theta_M} \\sum_{g=1}^{N_g} L(\\theta_{\\tau-1,g}, B_{ual}),$ (4)\nwhere $L(\\theta_{t,g})$ indicates the loss function of DDPG at the agent g in time t. Thus, (3) updates the individual RL agent policy parameters based on the gradient descent method, and (4) updates the meta-model policy parameters by aggregating the adaptation ability of the trained agents for each task."}, {"title": "D. Meta training", "content": "Here, we organize the process into four main stages, focusing on the interactions and contributions of $N_g$ meta-learner agents, each employing a DDPG model within its environment.\n1) Initialization: Each meta-learner initializes its own DDPG model. Simultaneously, the meta-model is initialized separately to facilitate collective learning without direct interference.\n2) Training and Interaction: Meta-learners engage with their environments, gathering experiences (state, action, reward, next state) for learning. The learning process involves iterative updates to their actor and critic networks via DDPG, enhancing their decision-making capabilities based on observed outcomes.\n3) Contribution and Meta-update: Upon completing a set number of episodes ($T_e$), meta-learners evaluate their advancements and relay their insights and effective gradients to the meta-model. This process ensures a collaborative update mechanism, enhancing the system's collective intelligence.\n4) Meta-model Optimization and Feedback: Integrating these contributions, the meta-model optimizes its learning strategy, enhancing adaptability and task performance. Enhanced strategies or parameters are then distributed to the meta-learners, supporting their continuous improvement and ability to tackle new challenges efficiently.\nAlgorithm 1 outlines a MAML-inspired RL approach for addressing the joint RB and power allocation optimization problem, as defined in (2a)-(2d). It describes a collaborative learning mechanism where individual learners (meta-learners) enrich a shared knowledge base (meta-model), improving each learner's efficiency and adaptability."}, {"title": "V. EVALUATION RESULTS", "content": "In our O-RAN framework simulation for eMBB services, we model $N_g = 6$ distributed DUs covering different network areas, tasked with meta-learning varied tasks. The effectiveness of these tasks within the MAML-RL framework depends on their complexity and diversity. The framework caters to 30 users per DU, uniformly and randomly spread across the network, serviced by dynamically allocated bandwidths of 12, 16, 20 MHz or 60, 80, 100 RBs. Users, exhibiting mobility with speeds between $10m/s$ and $20m/s$ and moving in one of seven possible directions $\\pm\\pi/3$, $\\pm\\pi/6$, $\\pm\\pi/12$, 0, traverse DU-assigned areas. Traffic profiles for these UEs shift among four levels\u2014idle, low, mid, high\u2014with a 0.01 transition probability each step, impacting RB allocation and bandwidth consumption based on real-time demands as outlined in [7], [16]. To deploy the DDPG strategy, we use a PyTorch-based actor-critic setup with three fully-connected layers of 300, 400, and 400 neurons, utilizing tanh functions and an Adam optimizer at a $10^{-4}$ learning rate. Distributed as distinct task agents across the network, with a meta-model in the RIC module for information aggregation, our setup acknowledges non-uniform service demands, making DUs face varying traffic. We evaluate our MAML-RL method using a comprehensive set of benchmarks, starting DRL from scratch, transfer learning (TL) from a selected RL agent, and multi-task learning (MTL), which concurrently incorporates both a random task and a new task. This selection enables us to assess the flexibility and efficiency of our approach thoroughly. Spanning traditional to state-of-the-art techniques, TL and MTL are particularly noted for their advanced capabilities in RL. Detailed alongside other parameters in Table I, these benchmarks facilitate a robust comparison of learning strategies within our framework.\nFig. 2 compares the cumulative return achieved by various RL approaches across several episodes. The results were measured with $\\gamma = 0.99$ and averaged over a sufficient number of runs. As the graph shows, the proposed MAML-RL approach can provide up to 19.8% gain over other baseline approaches that show the efficiency of the proposed approach."}, {"title": "VI. CONCLUSION", "content": "In the emerging O-RAN technology, optimizing resource and power allocation presents significant challenges due to the dynamic nature of wireless networks. Addressing this, we introduce a novel Meta-DRL approach inspired by MAML, which primarily enhances the adaptivity to dynamic network conditions. Our approach achieves a substantial 19.8% improvement over conventional methods, affirming the capability of the proposed architecture to increase the efficiency of wireless networks and facilitate quick adaptation to dynamic conditions. This research enhances network optimization by improving generalization and rapid adaptivity for new decision-making agents within the O-RAN ecosystem, which is crucial for advanced network management. It also opens avenues for applications such as autonomous network repairs and dynamic spectrum management, illustrating its potential impact on adaptive technologies in complex environments."}]}