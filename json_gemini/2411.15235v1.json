{"title": "CODE-CL: COnceptor-Based Gradient Projection for DEep Continual Learning", "authors": ["Marco P. E. Apolinario", "Kaushik Roy"], "abstract": "Continual learning, or the ability to progressively integrate new concepts, is fundamental to intelligent beings, enabling adaptability in dynamic environments. In contrast, artificial deep neural networks face the challenge of catastrophic forgetting when learning new tasks sequentially. To alleviate the problem of forgetting, recent approaches aim to preserve essential weight subspaces for previous tasks by limiting updates to orthogonal subspaces via gradient projection. While effective, this approach can lead to suboptimal performance, particularly when tasks are highly correlated. In this work, we introduce COnceptor-based gradient projection for DEep Continual Learning (CODE-CL), a novel method that leverages conceptor matrix representations, a computational model inspired by neuroscience, to more flexibly handle highly correlated tasks. CODE-CL encodes directional importance within the input space of past tasks, allowing new knowledge integration in directions modulated by $1-S$, where S represents the direction's relevance for prior tasks. Additionally, we analyze task overlap using conceptor-based representations to identify highly correlated tasks, facilitating efficient forward knowledge transfer through scaled projection within their intersecting subspace. This strategy enhances flexibility, allowing learning in correlated tasks without significantly disrupting previous knowledge. Extensive experiments on continual learning image classification benchmarks validate CODE-CL's efficacy, showcasing superior performance with minimal forgetting, outperforming most state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "The ability to continually acquire, retain, and adapt knowledge over time is a defining characteristic of intelligent behavior in humans and animals. This capacity to learn incrementally from new information, while retaining past knowledge, enables adaptation to dynamic and unpredictable environments [11, 14, 19]. In contrast, conventional deep learning models typically excel when trained in a static, batch-based manner on fixed datasets, but they face significant difficulties when adapting to new information sequentially without revisiting previous data, a limitation known as catastrophic forgetting [8, 14, 30]. This phenomenon, where knowledge of previous tasks degrades as new tasks are learned, poses a fundamental obstacle to achieving continual learning (CL) in artificial neural networks.\nRecent research in continual learning has led to the development of strategies aimed at mitigating catastrophic forgetting while enabling the integration of new information. These strategies broadly fall into three categories: regularization-based, expansion-based, and memory-based methods. Regularization-based methods seek to constrain updates to important model parameters for previous tasks, preserving essential features while allowing flexibility in unimportant regions of the parameter space [12, 18, 27, 28, 36]. Expansion-based methods dynamically allocate new network resources to accommodate the growing complexity of sequential tasks [20, 23, 32-34]. Memory-based approaches, by contrast, store representative samples or features from previous tasks to maintain performance on earlier data distributions [2, 3, 6, 17, 21, 25, 31, 35]. While each of these approaches provides valuable contributions, they often require trade-offs between flexibility and retention, and many still rely on extensive memory storage or the allocation of dedicated resources for each new task.\nA promising direction in the pursuit of efficient and scalable continual learning involves leveraging gradient projection methods, a group of techniques within the memory-based approaches. Recent work in this area [6, 7, 9, 16, 24, 25, 31, 35] aims to preserve essential subspaces in the weight vector space of neural networks that correspond to previously learned tasks. By restricting model updates to orthogonal subspaces, through projecting gradients into those subspaces, these methods effectively limit interference with prior knowledge, thereby reducing catastrophic forgetting. However, subspace-restricting gradients face inherent limitations, as they often exclude potentially beneficial regions of the weight space, thereby limiting the model's capacity to fully utilize shared information across tasks. This restriction is particularly problematic in scenarios where tasks share common characteristics, as such overlap could be leveraged for positive forward transfer of knowledge [4, 16, 24].\nIn this work, we propose Conceptor-based gradient projection for DEep Continual Learning (CODE-CL), a novel method that extends gradient projection approaches through leveraging conceptor matrix representation [10], a computational model inspired by neuroscience, to more flexibly manage overlapping tasks' vector subspaces in continual learning. Conceptor matrices model the task subspaces based on input feature vectors, encoding the importance of directions in the input space of previous tasks and enabling selective updating for new tasks. By encoding past knowledge in conceptor matrices, CODE-CL enables a more comprehensive yet controlled exploration of the weight space, allowing learning to occur in previously restricted areas that do not interfere with critical directions from past tasks.\nA core contribution of CODE-CL lies in its adaptive mechanism for task overlap analysis. We introduce a method to analyze overlap by computing intersection subspaces among tasks based on their conceptor representations, identifying highly correlated tasks with substantial shared feature space. For these correlated tasks, CODE-CL employs a gradient projection strategy that allows the model to learn a scaled projection within the intersection subspace. This approach promotes efficient forward knowledge transfer, leveraging shared information between tasks to enhance performance on new tasks without significant interference with previously learned knowledge. For less correlated tasks, CODE-CL constrains updates to avoid disrupting prior knowledge, preserving task-specific information effectively. Furthermore, we conduct extensive evaluations on widely used continual learning benchmarks in image classification, such as Permuted MNIST [15], Split CIFAR100 [13], Split miniImageNet [29], and 5-Datasets [5], to demonstrate the efficacy of CODE-CL. The results show that CODE-CL consistently achieves superior performance with minimal forgetting across tasks, outperforming several comparable state-of-the-art continual learning methods. In summary, CODE-CL provides a scalable, memory-efficient approach that enables both retention of past knowledge and the flexible acquisition of new information. The main contributions of this work are summarized as:\n\u2022 Introduce a novel continual learning method, CODE-CL, that leverage the conceptors' pseudo-Boolean algebra for promoting retention of past knowledge and flexible acquisition of new information.\n\u2022 Validate effectiveness of the proposed method extensively on multiple continual image classification benchmarks achieving performance superior to most of the state-of-the-art gradient projection methods."}, {"title": "2. Background", "content": "In this section, we outline essential properties of conceptor matrices that are central to our approach and provide an overview of related work in continual learning."}, {"title": "2.1. Conceptor matrices", "content": "Conceptor matrices are a computational model inspired by neuroscience [10]. This mathematical framework was introduced to encode and control the dynamics of recurrent neural networks [10]. Given a batch of feature vectors $X \\in \\mathbb{R}^{b \\times n}$, where b is the batch size and n is the dimension of the feature vector space, a conceptor matrix $C(X, a)$ is defined as the solution to the following minimization problem:\n$C(X, a) = \\arg \\min_C \\frac{1}{2} ||X - XC||_F^2 + \\frac{a}{2} ||C||_*^2$ (1)\nHere, $a \\in (0,\\infty)$ is called the aperture and serves as a regularization factor. Also, note that this optimization problem has a closed-form solution:\n$C(X, \\alpha) = \\frac{XX^T}{b} \\left( \\frac{XX^T}{b} + a^{-2}I \\right)^{-1}$ (2)\nTherefore, given the singular value decomposition (SVD) of the matrix $X = USV^T$, the conceptor matrix can be expressed as $C = USU^T = U\\Sigma^2(\\Sigma^2 + ba^{-2}I)^{-1}U^T$. Thus, the singular values of C lie between 0 and 1 (0 < $S_{i,i}$ < 1, $\\forall i \\in {0,1, . . ., n}$), representing the importance of a particular direction $U_{:,i}$ in the feature vector space X. In this way, C acts as a soft projection matrix onto the linear subspace of the feature vectors of X.\nOne important feature of the conceptor matrices is that those satisfy most laws of Boolean logic, as described by Jaeger [10], allowing to define a pseudo-Boolean algebra that provide a simple and intuitive framework to handle the linear subspaces defined within a conceptor matrix. Some of the boolean operations supported by conceptor matrices are NOT (\u00ac), OR (V), and AND (\u0245). Specifically, for two conceptor matrices (C and B), the operations are defined as follows:\n$\\neg C = I - C$ (3)\n$C \\land B = (C^{-1} + B^{-1} - I)^{-1}$ (4)\n$C \\lor B = \\neg (\\neg C \\land \\neg B)$ (5)\nHere $\\neg C$ can be interpreted as a soft projection onto a linear subspace that is the pseudo-orthogonal complement of the subspace characterized by C. Moreover, $C \\land B$ compute the conceptor matrix that describes approximately a space that lies in the intersection between the subspaces characterized by C and B. Similarly, $C \\lor B$ describes approximately the union between the linear subspaces represented by C and B."}, {"title": "3. CODE-CL: Conceptor-based Deep Contin-ual Learning", "content": "In this section, we describe the steps of our Conceptor-based gradient projection for DEep Continual Learning (CODE-CL) method. We consider a supervised continual learning setting where T tasks are learned sequentially, with each task having sufficient labeled samples. We explore both domain-incremental and task-incremental learning scenarios in this supervised setting [30].\nEach task is identified by $t\\in\\mathcal{T}$ = {1,2,..., T}, and its associated dataset is represented as $\\mathcal{D}^{t}$ = {(x,y)${i}^{nt}}_{i=1}$, where nt is the number of samples, \u00e6 is the input sample, and y is the corresponding label. Using these datasets, we train a neural network with parameters $W^{t}$ = {(W(1),t){L}1}, where L is the model's number of layers.\nFor all tasks beyond the first, learning follows a three-step process: (1) Analyzing the overlap of the input space between the current task and previous tasks, using their respective conceptor matrices, (2) training the model on the current task with gradients constrained to subspaces defined by previous tasks' conceptor matrices, and (3) merging the information of the current and previous tasks into an updated conceptor matrix. As each layer follows the same procedure, the following discussion focus on a single layer."}, {"title": "3.1. Learning on Task t = 1", "content": "For the initial task, learning proceeds with random weight initialization, $W = W^{0}$, and the model is trained on dataset $D^{1}$ by minimizing a loss function:\n$W^{1} := \\arg \\min_W \\mathcal{L}(W; D^{1})$ (8)"}, {"title": "3.2. Learning on Task t \u2208 {2,3,...,T}", "content": "For each new task, we follow a three-step process, as illustrated in Fig. 1."}, {"title": "3.2.1. Task overlap analysis: Tasks 1 : t \u2212 1 and Task t", "content": "Before training on task t, we analyze the overlap between its input space and that of previous tasks, represented by $C^{t-1}$. In order to do this, we compute a conceptor matrix for task t. First, we sample a batch of samples from $D^{t}$ to construct $X^{(0),t,pre}$, where the superscript \u2018pre' indicates pre-training status. Then, for hidden layers, we propagate inputs forward through the model with weights from the prior task, $W^{t-1}$, to obtain $X^{(l),t,pre} = f(x^{(l-1),t,pre}; W^{(l),t-1})$, where f() denotes the model's non-linear function (e.g ReLU). We then compute the pre-training conceptor for task t as $C^{t,pre} = C(x^{t,pre}, a)$. The input space overlap is represented by the intersection $C^{t,and} = C^{t,pre} \\land C^{t-1}$, depicted geometrically in Fig. 1. The similarity between $C^{t,and}$ and $C^{t-1}$ indicates task correlation. So, if many directions for the current task are encoded in $C^{t-1}$, tasks are highly correlated. Task correlation is measured by the capacity ratio between conceptor matrices (7), defining high (low) correlation when the ratio surpasses (falls below) a threshold \u0454:\n$\\text{Case 1 } (\\frac{\\Theta(C^{t,and})}{\\Theta(C^{t-1})} > \\epsilon)$: In this high-correlation scenario (illustrated in Fig. 1), directions encoded in $C^{t-1}$ are important for task t. The model is permitted to learn in the top K directions of $C^{t,and}$ without disturbing prior tasks. To achieve this, weights are projected onto the subspace defined by the top K directions and optimized using a matrix $M \\in \\mathbb{R}^{K\\times K}$:\n$W^{t,eff} = W + WU_{:,1:K}^{t,and} M U_{:,1:K}^{t,and T}$ (9)\nHere, $U_{:,1:K}^{t,JK}$ and are the top-K singular vectors of $C^{t,and}$. This is explicitly allowing to effectively forward transfer knowledge form old tasks to the current task."}, {"title": "Case 2", "content": "($\\frac{\\Theta(C^{t,and})}{\\Theta(C^{t-1})} < \\epsilon$): In this low-correlation case, task overlap is minimal, so no previous directions need adjusting. Thus, the effective weights remain as $W^{t,eff} = W$."}, {"title": "3.2.2. Optimization for task t", "content": "With $W^{t,eff}$ defined, weights are initialized as $W = W^{t-1}$, and the model is trained on $D^{t}$ to minimize a loss function:\n$W^{t}, M^{t} := \\arg \\min_{W,M} \\mathcal{C}(W^{t,eff}; D^{t})$\n$\\text{s.t.} \\nabla_w \\mathcal{L} = \\nabla_w \\mathcal{L}(I - C^{t-1})$ (10)\nMinibatch SGD optimizes the weights with gradients constrained to lie in the pseudo-orthogonal subspace of the conceptor matrix defined by $\\neg C^{t-1}$. This process continues until certain stopping criteria are met."}, {"title": "3.2.3. Conceptor update after training on task t", "content": "After training, we merge current and past task knowledge into a new conceptor matrix. Firsr, a new post-training conceptor matrix $C^{t,post}$ is computed by sampling a batch from $D^{t}$, constructing $X^{(0),t,post}$, and forward propagating through the model using the updated weights $W^{t}$, similarly to Section 3.2.1. The layer-wise conceptor matrix for task t post-training is $C^{t,post} = C(X^{t,post}, a)$. We then merge $C^{t,post}$ and $C^{t-1}$ into a new conceptor matrix consolidating all learned tasks: $C^{t} = C^{t,post} \\lor C^{t-1}$, illustrated in Fig. 1."}, {"title": "4. Experimental evaluation", "content": "In this section, we present an empirical assessment of CODE-CL, demonstrating its effectiveness across multiple continual learning image classification benchmarks. Our evaluation focuses on measuring the method's ability to retain prior knowledge while adapting to new tasks, comparing its performance against other CL approaches."}, {"title": "4.1. Experimental Setup", "content": "This subsection describes the general experimental setup applied across all evaluations of CODE-CL. We outline the benchmarks and models, training details, and metrics, to ensure consistent and fair evaluation."}, {"title": "4.1.1. Benchmarks and models:", "content": "We evaluate our method on widely used continual learning (CL) benchmarks, including Permuted MNIST [15], Split CIFAR100 [13], Split miniImageNet [29], and 5-Datasets [5]. The Permuted MNIST benchmark is a variation of the MNIST dataset where the pixel positions of each image are randomly permuted, creating distinct versions of the original images for each class. In our experiments, we use ten sequential tasks, each with a different permutation, and train a three-layer fully connected network with 100 neurons in each hidden layer, similar to the setup in [16, 17, 25], in a domain-incremental setting. For Split CIFAR100, the original CIFAR100 dataset is divided into T groups, each containing an equal number of classes (100/T). In our experiments, we split the dataset into 10 groups, with each group representing a separate task, and train a 5-layer AlexNet model in a multi-head setting, where each task has its own output head, following the methodology of [16, 24, 25]. Similarly, the Split miniImageNet benchmark consists of a subset of 100 classes from the ImageNet dataset, divided into T groups. For our experiments, we split the dataset into 20 groups, each containing 5 classes, and trained a reduced ResNet18 architecture in a multi-head setting, as done in prior works [16, 24, 25]. The 5-Datasets benchmark involves training a model sequentially on five different datasets, CIFAR10, MNIST, SVHN, notMNIST, and Fashion MNIST, with each dataset treated as a single task. For this benchmark, we again use a reduced ResNet18 model in a multi-head setting. In all experiments, we refrained from using data augmentation to align with prior works. The dataloaders for Split CIFAR100 and 5-Datasets were sourced from [25], while those for Permuted MNIST and Split mini-ImageNet were provided by the Avalanche library [1]."}, {"title": "4.1.2. Training details:", "content": "For all our experiments we use vanilla stochastic gradient descent (SGD), with dynamic learning rate reduction based on a validation metric and using early stopping criteria. Each task in Split CIFAR100 is trained for 200 epochs with a batch size of 64 with an aperture a = 6, similarly, each task on Split miniImageNet and 5-Datasets is trained for a maximum of 100 epochs with a batch size of 64, with \u03b1 = 8 and a = 4 respectivetly. Additionally, each task on Permuted MNIST is trained for 5 epochs with a batch size of 100 with a = 3. All the experiments reported on Table 1 and Table 2 used K = 80. More details of training setup, hyperparameters and implementation are given in the Supplementary Material."}, {"title": "4.1.3. Metrics:", "content": "Similar to previous works [16, 17, 25], we use two metrics to evaluate the performance of our method, such as: the average final accuracy over all tasks, Accuracy (ACC), and Backward Transfer (BWT), which measures the forgetting of old tasks when learning new tasks. ACC and BWT are defined as:\n$ACC = \\frac{\\sum_{i=1}^{T} A_{T,i}}{T}; BWT = \\frac{\\sum_{i=1}^{T-1} A_{T,i} - A_{i,i}}{T-1}$ (11)\nHere, T is the number of tasks, Aj,i is the accuracy of the model on i-th task after learning the j-th task sequentially (i < j)."}, {"title": "4.2. Results", "content": "In this subsection, we present the performance of CODE-CL in comparison with prior approaches, along with a detailed analysis of its memory complexity. Additionally, we conduct ablation studies to assess the impact of varying the number of free dimensions, K, on the method's performance."}, {"title": "4.2.1. Performance Comparison:", "content": "As shown in Table 1 and Table 2, our method demonstrates high accuracy with minimal forgetting across all benchmarks. Specifically, CODE-CL consistently delivers competitive results, outperforming previous methods on most datasets.\nOn Permuted MNIST, Table 2, CODE-CL surpasses existing methods such as GPM and TRGP in terms of accuracy. Moreover, on Split CIFAR100, CODE-CL achieves an impressive accuracy of 77.21%, coming close to the upper bound set by Multitask learning (79.58%), which serves as an ideal, but unrealistic, comparison point. Notably, CODE-CL outperforms other state-of-the-art continual learning methods, registering 4.73%, 2.75%, and 1.16% higher accuracy than GPM, TRGP, and SGP, respectively. Similarly, on Split MiniImageNet, CODE-CL once again performs exceptionally well, achieving an accuracy second only to Multitask learning, and outperforming GPM by 8.42%, TRGP by 7.05%, and SGP by 6%. This further underscores CODE-CL's robustness, particularly when handling more challenging datasets, where competing methods tend to experience significant performance drops. On the 5-Datasets benchmark, CODE-CL reaches an accuracy of 93.32%, outperforming GPM by 2.1% and Multitask learning by 1.78%. Although it falls just 0.24% short of TRGP on this benchmark, the difference is marginal, highlighting CODE-CL's strong overall performance across diverse tasks.\nIn terms of backward transfer (BWT), our results further illustrate the effectiveness of CODE-CL in mitigating catastrophic forgetting. On Permuted MNIST, CODE-CL achieves a BWT of -0.24%, which is lower than previous methods like GPM and TRGP. Furthermore, on Split CIFAR100, CODE-CL records a BWT of -1.1%, indicating minimal performance loss on previously learned tasks, and this result is comparable to those obtained by GPM, TRGP, and SGP. Likewise, on Split MiniImageNet, CODE-CL reports a BWT of -1.1%, which is consistent with state-of-the-art methods, demonstrating its capacity to retain learned knowledge with minimal degradation. Finally, in the 5-Datasets benchmark, CODE-CL achieves a BWT of -0.25%, outperforming GPM and performing slightly worse than TRGP. In summary, the high accuracy and low forgetting of CODE-CL highlight its ability to effectively balance the trade-off between plasticity and stability, maintaining strong performance across a range of continual learning tasks while minimizing forgetting."}, {"title": "4.2.2. Memory Complexity:", "content": "Efficient memory usage is crucial, especially when scaling to larger networks and a greater number of tasks. In this section, we compare the memory complexity of our proposed method against other established methods, including GPM, TRGP, and SGP.\nFor simplicity, the following analysis focuses on a single fully-connected layer with N inputs and M outputs after training sequentially on T tasks. For CODE-CL, the memory complexity is influenced by the conceptor matrices that has a dimension N2 that encode the information of the input vector space, more over as discussed in Section 3, CODE-CL uses a discrete fixed number of free dimensions per task (K) to learn the an optimal linear combination of the K most important direction of the subspace represented by the intersection of the conceptor of old tasks with the new task. This term incurs into additional memory proportional to TKN +TK2, where TKN represent the K most important direction of dimension N for T tasks. Therefore, reaching a memory complexity of O(N2 + TNK +TK2)\nIn the case of GPM and SGP, the additional memory usage is determine by the dimension of the inputs N, the number of important directions per task B and the number of tasks T. So the maximum dimension in those cases is in the order of O(N2). Finally, TRGP requires the same memory than GPM to store the important direction of all learned tasks, but in addition to this, TRGP requires to store the important directions per tasks as well as the important direction in the trusted region projection subspaces, which incur in additional memory TNB + TB2. summarizes the memory complexity for each method.\nNote that, as the methods scale for deeper models, the number of free dimension used in our method is fixed and significantly less than the dimension of the input space, $K \\ll N$, which make the ratio between memories requirements of GPM and CODE-CL close to ~ 1. However, for deeper models B scales with the size of the model, therefore the memory of TRGP becomes dominated by the term TNB, which make the method essentially require ~ B/K times more memory than our method."}, {"title": "4.2.3. Effect of K in performance:", "content": "To investigate the influence of the number of free dimensions (K) on model performance, we conduct ablation studies using different values of K. The results for all four benchmarks are presented in Fig. 2. In general, we observe that increasing K enhances model accuracy while maintaining a low Backward Transfer (BWT). However, this trend varies across the benchmarks, revealing two distinct patterns.\nFor the 5-Datasets and Permuted MNIST benchmarks, increasing K consistently improves accuracy and substantially reduces BWT. In contrast, for Split CIFAR100 and Split miniImageNet, the model achieves higher accuracy when K \u2265 40, while setting K = 20 leads to a small accuracy decline. Moreover, in these latter benchmarks, higher values of K slightly increase BWT, though the effect remains relatively minor. These contrasting results can be attributed to the nature of the benchmarks. In the 5-Datasets and Permuted MNIST benchmarks, the tasks are inherently diverse, as the images originate from distinct distributions with minimal overlap. Conversely, the tasks in Split CIFAR100 and miniImageNet are derived from a shared distribution, leading to greater overlap among tasks. Consequently, while increasing K generally enhances accuracy by facilitating knowledge transfer from previous tasks to new tasks, the reduction in BWT is less pronounced for tasks with significant data distribution overlap. The similarity in data distributions across tasks in Split CIFAR100 and miniImageNet likely limits BWT reduction, even as accuracy benefits from a larger K."}, {"title": "5. Conclusions", "content": "The ability to learn incrementally, retaining past knowledge while adapting to new information, is essential for continual learning in artificial neural networks. This work proposed CODE-CL, a novel continual learning method that leverages conceptor-based gradient projection. CODE-CL addresses the limitations of traditional gradient projection by employing conceptor matrices, a model inspired by neuroscience, allowing the model to flexibly manage overlapping task's subspaces for continual learning. By encoding past tasks into conceptor matrices, CODE-CL enables a nuanced approach to managing shared and unique information across tasks, facilitating both knowledge retention and efficient forward transfer. The method's adaptive mechanism analyzes task overlap, enhancing the model's ability to leverage shared knowledge between tasks without compromising performance on previously learned tasks. Extensive evaluations on diverse continual learning benchmarks, including Split CIFAR100, Split miniImageNet, 5-Datasets, and Permuted MNIST, demonstrate CODE-CL's superior performance and reduced forgetting compared to other comparable state-of-the-art methods. CODE-CL's ability to dynamically allocate memory for correlated and independent tasks makes it scalable and efficient for real-world applications where adaptability to evolving environments is essential."}, {"title": "A. Conceptor Implementation Details", "content": "We implement the conceptor operations following the equations presented in Section 2, with one exception: the AND operation (4).\nAs noted in [10], the operation defined in (4) is only valid when the conceptor matrices are invertible. However, in practice, since we use a limited number of samples to compute the conceptors, the resulting matrices are often not full rank. To address this, we adopt a more general version of the AND operation, as proposed in [10]:\n$C \\land B = D(D^T (C^{\\dagger} + B^{\\dagger} - I)D)^{-1}D^T$, (12)\nHere, $C^{\\dagger}$ and $B^{\\dagger}$ denote the pseudo-inverses of C and B, respectively. The matrix D consists of columns that form an arbitrary orthonormal basis for the intersection of the column spaces of C and B.\nThe procedure for computing D is outlined in Algorithm 1."}, {"title": "B. Experimental Setup", "content": "This section provides details on the architecture of all models used in this work, the dataset statistics, the hyperparameters for each experiment, and the compute resources employed."}, {"title": "B.1. Model Architecture", "content": "In this work, we utilize two models: an AlexNet-like architecture, as described in [27], and a Reduced ResNet18 [17].\nThe AlexNet-like model incorporates batch normalization (BN) in every layer except the classifier layer. The BN layers are trained during the first task and remain frozen for subsequent tasks. The model consists of three convolutional layers with 64, 128, and 256 filters, using kernel sizes of 4 \u00d7 4, 3 x 3, and 2 \u00d7 2, respectively. These are followed by two fully connected layers, each containing 2048 neurons. ReLU activation functions are used throughout, along with 2\u00d72 max-pooling layers after each convolutional layer. Dropout is applied with rates of 0.2 for the first two layers and 0.5 for the remaining layers.\nThe Reduced ResNet18 follows the architecture detailed in [25]. For the Split miniImageNet experiments, the first layer uses a stride of 2, while for the 5-Datasets benchmark, it uses a stride of 1.\nFor all models and experiments, cross-entropy loss is employed as the loss function."}, {"title": "B.2. Dataset Statistics", "content": "The statistics for the four benchmarks used in this work for continual image classification are summarized in Table 4 and Table 5. For all benchmarks, we follow the same data partitions as those used in [16, 24, 25].\nFor the 5-Datasets benchmark, grayscale images are replicated across all RGB channels to ensure compatibility with the architecture. Additionally, all images are resized to 32 \u00d7 32 pixels, resulting in an input size of 3 \u00d7 32 \u00d7 32 for this benchmark."}, {"title": "B.3. Hyperparameters", "content": "The hyperparameters used in our experiments are detailed in Table 6."}, {"title": "B.4. Compute resources", "content": "All experiments were conducted on a shared internal Linux server equipped with an AMD EPYC 7502 32-Core Processor, 504 GB of RAM, and four NVIDIA A40 GPUs, each with 48 GB of GDDR6 memory. Additionally, code was implemented using Python 3.9 and PyTorch 2.2.1 with CUDA 11.8."}, {"title": "C. CODE-CL algorithm", "content": "The pseudo-code for CODE-CL is shown in Algorithm 2."}]}