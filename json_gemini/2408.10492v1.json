{"title": "IS THE LECTURE ENGAGING FOR LEARNING? LECTURE VOICE\nSENTIMENT ANALYSIS FOR KNOWLEDGE GRAPH-SUPPORTED\nINTELLIGENT LECTURING ASSISTANT (ILA) SYSTEM", "authors": ["Yuan An", "Samarth Kolanupaka", "Jacob An", "Matthew Ma", "Unnat Chhatwal", "Alex Kalinowski", "Michelle Rogers", "Brian Smith"], "abstract": "This paper introduces an intelligent lecturing assistant (ILA) system that utilizes a knowledge graph\nto represent course content and optimal pedagogical strategies. The system is designed to support\ninstructors in enhancing student learning through real-time analysis of voice, content, and teaching\nmethods. As an initial investigation, we present a case study on lecture voice sentiment analysis,\nin which we developed a training set comprising over 3,000 one-minute lecture voice clips. Each\nclip was manually labeled as either engaging or non-engaging. Utilizing this dataset, we constructed\nand evaluated several classification models based on a variety of features extracted from the voice\nclips. The results demonstrate promising performance, achieving an F1-score of 90% for boring\nlectures on an independent set of over 800 test voice clips. This case study lays the groundwork for\nthe development of a more sophisticated model that will integrate content analysis and pedagogical\npractices. Our ultimate goal is to aid instructors in teaching more engagingly and effectively by\nleveraging modern artificial intelligence techniques.", "sections": [{"title": "1 Introduction", "content": "Engaging students and facilitating the retention of knowledge in long-term memory are fundamental aspects of effective\nteaching, yet many lectures often fail to achieve this goal. Although extensive research in cognitive science [1] and\nneuroscience [2] has suggested a multitude of scientifically-based strategies for effective teaching, the application of\nthese findings in real classrooms remains limited. Advancements in artificial intelligence offer a promising avenue for\ntransforming the science of learning to real-world experience. This paper introduces a novel knowledge graph-supported\nintelligent lecturing assistant (ILA) system designed to help teachers enhance student learning during lectures by\nintegrating insights from cognitive science, neuroscience, and established pedagogical best practices. By leveraging the\npower of AI, this system aims to empower instructors and create more interactive and engaging learning environments,\nultimately contributing to improved student learning outcomes."}, {"title": "2 Related Work", "content": "Knowledge graph (KG) represents the knowledge in a domain using a graphical structure consisting of (typed-) vertices\nand (typed-) links [18, 19, 20]. They have gained significant attention for their ability to represent complex information\nand enhance various AI applications. Knowledge graphs are pivotal in organizing and conveying vast amounts of data,\nmaking them invaluable in fields such as education, scientific research, social networks, and more [21].\nKnowledge graphs have been increasingly applied in educational contexts to improve both face-to-face and online\nlearning experiences [22, 23, 24]. They facilitate the organization and retrieval of educational content, enhance\npersonalized learning, and support intelligent tutoring systems. By structuring educational data, knowledge graphs\nenable the development of adaptive learning systems that can tailor content to individual learners' needs. The review in\n[22] highlights the role of KGs in personalized learning, curriculum design, concept mapping, and educational content\nrecommendation systems. The research in [23] discusses the fundamental research methods in knowledge maps and\nanalyzes classic cases in the education field. The study in [24] compares different KG models utilized in the education\ndomain."}, {"title": "3 Constructing Knowledge Graph for Intelligent Lecturing System", "content": "The knowledge graph, which represents the course content, schedule, objectives, and assessment is the foundation of\nthe proposed intelligent lecturing assistant (Figure 1). The key requirements for the construction and representation of\nthis knowledge graph (KG) include:\n\u2022 Accurate Representation: The KG must accurately capture the detailed content, schedule, objectives, and\nassessment materials of the course.\n\u2022 Automatic Construction: The KG should be automatically constructed for a wide range of topics across various\neducational contexts.\nSupport for Effective Teaching: The KG should support to the generation of relevant quizzes and questions to\nengage students at optimal time.\n\u2022 Support for Improving Learning: The KG should enable reasoning with pedagogical strategies to enhance\nteaching effectiveness and support student learning.\nPrior to constructing the knowledge graph, we need to develop an ontology to provide a structured framework and\ncommon understanding of the domain. An ontology defines the key concepts and their relationships that guide the\ncreation of the knowledge graph. The development of ontology is essential for the intelligent lecturing assistant, as it\nenables the efficient management, organization, and utilization of educational content. Existing ontology [49] models\nuniversity curricula and students' profiles. It focuses on curriculum recommendation for college students. For lecturing\nassistant, we propose a lecture ontology presented in Figure 2."}, {"title": "4 The Problem of Lecture Voice Sentiment Analysis", "content": "The problem of lecture voice sentiment analysis is to classify whether the lecture speech in a certain period of time\nas engaging or not engaging. Formally, Let S represent a lecture speech with a duration of T minutes. Let y denote"}, {"title": "5 Building Data Sets for Lecture Voice Sentiment Analysis", "content": "Given the absence of an existing dataset specifically tailored for training a model for lecture voice sentiment analysis,\nwe decided to create such a data set from scratch. Our objective was to compile a comprehensive set of 1-minute lecture\nvoice clips, which would serve as the foundation for training and evaluating our model.\nTo compile the dataset, each researcher carefully watched publicly available lecture videos from several video sharing\nor massive open online course sites. We use the following criteria to manually label a lecture video as 'engaging'\nor 'boring' (i.e., 'not engaging'). A lecture is labeled as 'engaging' if it posses the following characteristics: clear\nenunciation, appropriate loudness, appropriate pacing, varying pitch, enthusiasm, pauses, stressing keywords, varying\nvoice, appropriate rhythm, asking questions, etc.. A lecture is labeled as 'boring' if it posses the following characteristics:\nMumbling, monotonous, low volume, slow pacing, too fast, no breaks, lack of emphasis, dull voice, unenthusiastic tone,\netc..\nWe extracted the voice from a labeled lecture and write a Python program to segment it into 1-minute clips. To ensure\nconsistency and reliability in the labeling process, each researcher independently reviewed and labeled the 1-minute\nclips according to above criteria. Additionally, we also conducted cross validation whereby a subset of the labeled clips\nwas reviewed by cross researchers to verify the accuracy and reliability of the labels.\nIn order to enhance the dataset and introduce variations, we also recorded our own voice clips. These clips were\ndeliberately recorded to produce examples of both engaging and boring lectures. This supplemental data enriched the\ndataset by providing a broader range of speaking styles and engagement levels, thereby enhancing the model's ability to\ngeneralize across contexts.\nFinally, we collected total 3,025 lecture voice clips. Figure 3 illustrates the distribution of the clips labeled as 'engaging' and \u2018boring' by the researchers. It should be\nnoted that the target labels are evenly distributed in the collected data set for the binary classification problem."}, {"title": "5.2 Independent Validation Data", "content": "The set of 1-minute clips used for training may contain multiple clips from the same speaker. This can potentially lead\nthe model to learn and memorize specific voice characteristics instead of generalizable sentiment features. Consequently,\nthe evaluation results may not accurate reflect the performance of the model. To mitigate this issue, we collected a\nseparate set of validation data that is independent of the training set, ensuring that the validation data does not include\nany speakers present in the training set. We will use the independent validation set to evaluate model performance for\nfeature selection and hyperparameter tuning. The independent validation set contains total 804 voice clips. Figure 4\nillustrates the binary label distribution in the independent validation set."}, {"title": "6 Extracting Features from Raw Voice Signals", "content": "Raw audio signals represent sound waves captured over time, typically in the form of a one-dimensional array of\namplitude values. Digitized audio signals are characterized by amplitude, duration, and sample rate that is the number\nof samples taken per second.\nRaw audio signals can be visualized using waveforms, which are graphical representations of the amplitude values over\ntime. Figure 5 shows the waveform of an example lecture voice clip. While waveforms are useful for visual inspection,\nthey do not directly reveal the frequency content or other more complex features such as pitch, loudness, or timbre,\nwhich are crucial for tasks like sentiment analysis."}, {"title": "7 Building Models for Lecture Voice Sentiment Analysis", "content": "Given the 13 extracted features, we focus on building and evaluating various classification models to identify the most\neffective approach for lecture voice sentiment analysis. Our strategy involves experimenting with traditional machine\nlearning models as well as deep neural networks. For traditional model, we select three widely-recognized models,\nnamely, logistic regression, random forest, and XGBoost. For deep neural networks, we explore a fully connected\nnetwork and a convolutional neural network (CNN).\nFeature Combinations: To identify the best feature set for classification, we evaluate all possible combinations of the\n13 features. This exhaustive approach ensures that we explore the potential synergies and interactions between different\nfeatures, allowing us to select the most informative subset. Given that there are 13 features, the total number of possible\nsubsets is 2^{13} = 8192 including the empty set. For each subset (excluding the empty set), we evaluate a model on the\nindependent validation set to find the best hyper-parameters. This approach also provides valuable insights into which"}, {"title": "8 Experimental Results", "content": "We conduct a set of comprehensive experiments. For each model, we evaluate it on a combination of features. If it\nis a type of traditional model, we use the independent validation set to select the best parameters. We use Accuracy,\nPrecision, Recall, and F1-Measure for evaluation metrics. For the lecture voice sentiment analysis problem, we are\nmore interested in correctly identifying a boring lecture for intervention. We set the label boring=1 as the positive\ncase. The metrics with respect to boring=1 are defined as follows:\nAccuracy = \\frac{TP + TN}{TP + FP + TN + FN}\nPrecision = \\frac{TP}{TP + FP}\nRecall = \\frac{TP}{TP + FN}"}, {"title": "9 Discussion", "content": "The analysis of lecture voice sentiment analysis presents promising outcomes; however, the current study is limited\nby its focus on a binary classification of engagement (engaging vs. non-engaging), which may not fully capture all\naspects of the lecturer delivery. Additionally, the model development and evaluation are constrained by the use of a"}, {"title": "10 Conclusion", "content": "The study presented in this paper explores the development of an intelligent lecturing assistant (ILA) system that\nutilizes a knowledge graph to represent course content and pedagogical strategies. The system is designed to support\ninstructors in enhancing student engagement in learning by analyzing lecture voice sentiment. The paper focuses on the\ndevelopment of a model that can classify lecture voice as either engaging or non-engaging, and the results demonstrate\npromising performance with an F1-score of 90% for boring lectures on an independent set of test voice clips.\nThis research on lecture voice sentiment analysis lays the groundwork for developing additional components of the\nintelligent lecturing assistant (ILA) system. An ILA system is a multifaceted platform that integrates knowledge\nrepresentation, reasoning, speech recognition, machine learning, and intelligent intervention. The next phase of\ndevelopment involves incorporating content analysis and pedagogical principles into the model, which would enable the\nsystem to deliver relevant interventions for instructors. Moreover, the system could be further developed to analyze\nstudent responses and provide real-time feedback.\nThe overall AI-powered system has the potential to enhance student engagement and learning outcomes. Future research\nwill also address ethical considerations related to the use of AI in education. The research team aims to deploy the\nsystem in both experimental environments and real classrooms to evaluate the effectiveness of the system on both\ninstructors and students."}, {"title": "11 Sources and Data Sets", "content": "For the sake of reproducibility, the source notebooks utilized for data analysis and model evaluation are accessible in\nthe public GitHub repository: https://github.com/anyuanay/KG_ILA .\nThe training data and the independent validation sets can be made available for download upon request."}]}