{"title": "IS THE LECTURE ENGAGING FOR LEARNING? LECTURE VOICE\nSENTIMENT ANALYSIS FOR KNOWLEDGE GRAPH-SUPPORTED\nINTELLIGENT LECTURING ASSISTANT (ILA) SYSTEM", "authors": ["Yuan An", "Samarth Kolanupaka", "Jacob An", "Matthew Ma", "Unnat Chhatwal", "Alex Kalinowski", "Michelle Rogers", "Brian Smith"], "abstract": "This paper introduces an intelligent lecturing assistant (ILA) system that utilizes a knowledge graph\nto represent course content and optimal pedagogical strategies. The system is designed to support\ninstructors in enhancing student learning through real-time analysis of voice, content, and teaching\nmethods. As an initial investigation, we present a case study on lecture voice sentiment analysis,\nin which we developed a training set comprising over 3,000 one-minute lecture voice clips. Each\nclip was manually labeled as either engaging or non-engaging. Utilizing this dataset, we constructed\nand evaluated several classification models based on a variety of features extracted from the voice\nclips. The results demonstrate promising performance, achieving an F1-score of 90% for boring\nlectures on an independent set of over 800 test voice clips. This case study lays the groundwork for\nthe development of a more sophisticated model that will integrate content analysis and pedagogical\npractices. Our ultimate goal is to aid instructors in teaching more engagingly and effectively by\nleveraging modern artificial intelligence techniques.", "sections": [{"title": "1 Introduction", "content": "Engaging students and facilitating the retention of knowledge in long-term memory are fundamental aspects of effective\nteaching, yet many lectures often fail to achieve this goal. Although extensive research in cognitive science [1] and\nneuroscience [2] has suggested a multitude of scientifically-based strategies for effective teaching, the application of\nthese findings in real classrooms remains limited. Advancements in artificial intelligence offer a promising avenue for\ntransforming the science of learning to real-world experience. This paper introduces a novel knowledge graph-supported\nintelligent lecturing assistant (ILA) system designed to help teachers enhance student learning during lectures by\nintegrating insights from cognitive science, neuroscience, and established pedagogical best practices. By leveraging the\npower of AI, this system aims to empower instructors and create more interactive and engaging learning environments,\nultimately contributing to improved student learning outcomes."}, {"title": "2 Related Work", "content": "Knowledge graph (KG) represents the knowledge in a domain using a graphical structure consisting of (typed-) vertices\nand (typed-) links [18, 19, 20]. They have gained significant attention for their ability to represent complex information\nand enhance various AI applications. Knowledge graphs are pivotal in organizing and conveying vast amounts of data,\nmaking them invaluable in fields such as education, scientific research, social networks, and more [21].\nKnowledge graphs have been increasingly applied in educational contexts to improve both face-to-face and online\nlearning experiences [22, 23, 24]. They facilitate the organization and retrieval of educational content, enhance\npersonalized learning, and support intelligent tutoring systems. By structuring educational data, knowledge graphs\nenable the development of adaptive learning systems that can tailor content to individual learners\u2019 needs. The review in\n[22] highlights the role of KGs in personalized learning, curriculum design, concept mapping, and educational content\nrecommendation systems. The research in [23] discusses the fundamental research methods in knowledge maps and\nanalyzes classic cases in the education field. The study in [24] compares different KG models utilized in the education\ndomain.\nKGs can be constructed manually or automatically. The studies in [25, 26] discuss the automatic construction of\neducational KGs. Recently, a great deal of effort has been put into applying pre-trained large language models (LLMs)\nfor constructing knowledge graphs (KGs) [27, 28]. The research in [29] reviews methods for generative KG construction.\nThe studies in [30, 31] explore the (semi-)automatic construction of KGs facilitated by open-source LLMs. Applying\nLLMs to automatically construct KGs for course content and teaching delivery remains under-explored.\nKGs are widely used for information retrieval [32] and question answering [33, 34]. An intelligent lecturing assistant\nsystem involves multimodal information including KGs, voice, text, and images. KGs play an important role in\nretrieving and reasoning relevant information for lecture assistant. A latest study has revealed that generative AI/LLM-\ndriven context personalization positively affected learning motivation [35]. A critical aspect is to combine the strengths\nof KGs and LLMs for accurate intervention [36].\nIntelligent educational systems (IES) and intelligent tutoring systems (ITS) have leveraged KGs to address several\nkey aspects of learning, including exercise recommendation and selection [37], student interactions diagnosis [38],\npersonalized learning guidance [39], multi-dimensional knowledge representation [37], integration with other AI\ntechniques [37, 38], and automatic grading [39]. By leveraging the structured representation of knowledge and the\nrelationships between concepts, these systems can better assess student knowledge, recommend appropriate learning\nmaterials, and guide the learning process.\nTo assist teachers in real-time lecturing, an intelligent lecturing assistant system must integrate and reason about\nthe multimodal information available in the classroom. One crucial aspect of this multimodal data is the sentiment\nconveyed through the lecturer's voice. The integration of voice sentiment analysis in educational technology builds upon\nfoundational research in affective computing and speech processing [40, 41, 42]. Previous studies have demonstrated\nthat emotional cues in speech, such as tone, pitch, and rhythm, can significantly impact communication efficacy and\naudience engagement [43]. Speech emotion recognition (SER) is a task to determine the speaker's emotional state,\nsuch as happiness, anger, sadness, or frustration, from speech patterns like tone, pitch, and rhythm [44]. Deep learning\napproaches have become prominent solutions for SER in recent years. Various neural network architectures have\nbeen applied [45]. Feature extraction is a crucial step in SER. Common features used include Mel-frequency Cepstral\nCoefficients (MFCCs) and spectral features [46, 47]\nIn educational settings, a teacher's emotional expressiveness has been linked to student motivation, comprehension, and\nretention of information [48]. Hence, accurately detecting and responding to these vocal sentiments can be pivotal in\ncreating a more engaging lecturing environment. In this study, we focus on a binary lecture voice sentiment analysis\nas engaging or not engaging. To build an effective machine learning model, we need a large set of training and test\ndata for benchmarking. We will build such a dataset as a benchmark for future researchers to investigate and improve\nupon. This dataset is publicly available upon request\u00b2 to facilitate further advancements in the field of lecture sentiment\nanalysis and intelligent lecturing assistant systems."}, {"title": "3 Constructing Knowledge Graph for Intelligent Lecturing System", "content": "The knowledge graph, which represents the course content, schedule, objectives, and assessment is the foundation of\nthe proposed intelligent lecturing assistant (Figure 1). The key requirements for the construction and representation of\nthis knowledge graph (KG) include:\n\u2022 Accurate Representation: The KG must accurately capture the detailed content, schedule, objectives, and\nassessment materials of the course.\n\u2022 Automatic Construction: The KG should be automatically constructed for a wide range of topics across various\neducational contexts.\n\u2022 Support for Effective Teaching: The KG should support to the generation of relevant quizzes and questions to\nengage students at optimal time.\n\u2022 Support for Improving Learning: The KG should enable reasoning with pedagogical strategies to enhance\nteaching effectiveness and support student learning.\nPrior to constructing the knowledge graph, we need to develop an ontology to provide a structured framework and\ncommon understanding of the domain. An ontology defines the key concepts and their relationships that guide the\ncreation of the knowledge graph. The development of ontology is essential for the intelligent lecturing assistant, as it\nenables the efficient management, organization, and utilization of educational content. Existing ontology [49] models\nuniversity curricula and students' profiles. It focuses on curriculum recommendation for college students. For lecturing\nassistant, we propose a lecture ontology presented in Figure 2."}, {"title": "4 The Problem of Lecture Voice Sentiment Analysis", "content": "The problem of lecture voice sentiment analysis is to classify whether the lecture speech in a certain period of time\nas engaging or not engaging. Formally, Let S represent a lecture speech with a duration of T minutes. Let y denote\nthe label associated with speech S, where y \u2208 {0,1}. Here, y = 0 indicates that the lecture is \"engaging,\" and y = 1\nindicates that the lecture is \"not engaging.\" We want to train a model M such as M(S) = y \u2208 {0,1}.\nWe train the model, M, using a set of 1-minute voice clips, considering shorter clips allowing for more focused\nand relevant acoustic feature extraction. Also, a data set consisting of all short 1-minute clips makes the data\nconsistent and easier to apply data augmentation techniques (e.g., adding noise, varying pitch) to reduce overfitting. In\nparticular, we divide S into T segments, each of which has 1-minute duration. Let S = {81, 82, ..., ST} denote these\nT segments, where si is the ith 1-minute segment of S. For each si, the model produces a binary classification as\n\u0177i = M(si) \u2208 {0, 1}. We compute the average classification score across all T segments as:\n$\\displaystyle Score(S) = \\frac{1}{T} \\sum_{i=1}^{T} Y_i$\nThe final classification of the entire lecture speech S is determined based on whether the average score exceeds a\nthreshold of 0.5:\n$\\displaystyle Classify(S) = \\begin{cases} \\text{engaging} & \\text{if Score(S) > 0.5} \\\\ \\text{not engaging} & \\text{otherwise} \\end{cases}$\nIn next section, we describe the process and results of collecting and labeling 1-minute lecture voice clips used for\ntraining the model."}, {"title": "5 Building Data Sets for Lecture Voice Sentiment Analysis", "content": "Given the absence of an existing dataset specifically tailored for training a model for lecture voice sentiment analysis,\nwe decided to create such a data set from scratch. Our objective was to compile a comprehensive set of 1-minute lecture\nvoice clips, which would serve as the foundation for training and evaluating our model.\nTo compile the dataset, each researcher carefully watched publicly available lecture videos from several video sharing\nor massive open online course sites. We use the following criteria to manually label a lecture video as 'engaging'\nor 'boring' (i.e., 'not engaging'). A lecture is labeled as 'engaging' if it posses the following characteristics: clear\nenunciation, appropriate loudness, appropriate pacing, varying pitch, enthusiasm, pauses, stressing keywords, varying\nvoice, appropriate rhythm, asking questions, etc.. A lecture is labeled as 'boring' if it posses the following characteristics:\nMumbling, monotonous, low volume, slow pacing, too fast, no breaks, lack of emphasis, dull voice, unenthusiastic tone,\netc..\nWe extracted the voice from a labeled lecture and write a Python program to segment it into 1-minute clips. To ensure\nconsistency and reliability in the labeling process, each researcher independently reviewed and labeled the 1-minute\nclips according to above criteria. Additionally, we also conducted cross validation whereby a subset of the labeled clips\nwas reviewed by cross researchers to verify the accuracy and reliability of the labels.\nIn order to enhance the dataset and introduce variations, we also recorded our own voice clips. These clips were\ndeliberately recorded to produce examples of both engaging and boring lectures. This supplemental data enriched the\ndataset by providing a broader range of speaking styles and engagement levels, thereby enhancing the model's ability to\ngeneralize across contexts.\nFinally, we collected total 3,025 lecture voice clips. Table 1 shows the numbers of clips collected by each researcher.\nFigure 3 illustrates the distribution of the clips labeled as 'engaging' and \u2018boring' by the researchers. It should be\nnoted that the target labels are evenly distributed in the collected data set for the binary classification problem."}, {"title": "5.2 Independent Validation Data", "content": "The set of 1-minute clips used for training may contain multiple clips from the same speaker. This can potentially lead\nthe model to learn and memorize specific voice characteristics instead of generalizable sentiment features. Consequently,\nthe evaluation results may not accurate reflect the performance of the model. To mitigate this issue, we collected a\nseparate set of validation data that is independent of the training set, ensuring that the validation data does not include\nany speakers present in the training set. We will use the independent validation set to evaluate model performance for\nfeature selection and hyperparameter tuning. The independent validation set contains total 804 voice clips. Figure 4\nillustrates the binary label distribution in the independent validation set."}, {"title": "6 Extracting Features from Raw Voice Signals", "content": "Raw audio signals represent sound waves captured over time, typically in the form of a one-dimensional array of\namplitude values. Digitized audio signals are characterized by amplitude, duration, and sample rate that is the number\nof samples taken per second.\nRaw audio signals can be visualized using waveforms, which are graphical representations of the amplitude values over\ntime. Figure 5 shows the waveform of an example lecture voice clip. While waveforms are useful for visual inspection,\nthey do not directly reveal the frequency content or other more complex features such as pitch, loudness, or timbre,\nwhich are crucial for tasks like sentiment analysis.\nTo address these limitations, it is essential to extract meaningful features from the raw audio signals. For this purpose,\nwe applied the librosa [50] Python package to extract a comprehensive set of features from the lecture voice clips,\ncapturing various aspects of the audio signal that are relevant to sentiment analysis:\n\u2022 Zero Crossing Rate (ZCR) [50]: A measure of the rate at which the signal changes sign during the duration of\na particular frame. High ZCR values typically indicate more noise or rapid changes in the audio signal, which\ncan be correlated with certain speech characteristics.\n\u2022 Chroma STFT (Short-Time Fourier Transform) [51]: refers to the chroma feature representation derived from\nthe short-time Fourier transform of an audio signal. Chroma features, or chromagrams, represent the energy\ndistribution among the twelve different pitch classes (C, C#, D, ..., B) of the musical octave. This feature\ncaptures harmonic content and can be useful in distinguishing different tones and pitches used by the speaker,\nwhich are often indicative of engagement levels.\n\u2022 Mel Spectrogram [52]: A representation of the power spectrum of a sound signal, where the frequencies\nare converted to the Mel scale. The Mel scale is designed to mimic the human ear\u2019s perception of sound,\nwhere each Mel unit corresponds to a perceived equal step in pitch. This feature provides a time-frequency\nrepresentation that emphasizes perceptually relevant aspects of the speech signal, aiding in the analysis of its\nemotional and engaging content.\n\u2022 Mel Frequency Cepstral Coefficients (MFCC) [53]: A cepstral representation where the frequency bands are\ndistributed according to the Mel scale. They are particularly effective in capturing the timbral characteristics\nof speech, which can be crucial for sentiment analysis.\n\u2022 Root-Mean-Square (RMS) Value [50]: Computed either from the audio samples or from a spectrogram,\nprovides a measure of the signal\u2019s power. RMS is indicative of the loudness of the speech, which can be a\nsignificant factor in determining the engagement level of the lecture.\n\u2022 Chroma CQT (Constant-Q Transform) [54]: Captures the pitch class energy distribution using a logarithmically\nspaced frequency axis, providing robustness to variations in the speech signal.\n\u2022 Chroma CENS (Chroma Energy Normalized Statistics) [54]: A variant that emphasizes long-term tonal content\nand is robust to variations in dynamics and articulation.\n\u2022 Chroma VQT (Variable-Q Transform) [54]: Similar to CQT but with variable-Q factor, providing flexibility in\nanalyzing different frequency ranges with varying resolutions.\n\u2022 Spectral Centroid [55]: Represents the center of gravity of the spectrum, indicating where the majority of the\nsignal\u2019s energy is located.\n\u2022 Spectral Bandwidth [55]: Measures the width of the spectrum, providing information about the range of\nfrequencies present in the speech signal.\n\u2022 Spectral Contrast [56]: Captures the difference in amplitude between peaks and valleys in the spectrum,\nreflecting the harmonic structure of the speech.\n\u2022 Spectral Flatness [57]: Measures the flatness of the spectrum, distinguishing between tonal and noisy signals.\n\u2022 Spectral Rolloff [50]: The frequency below which a specified percentage of the total spectral energy lies. It is\nused to approximate the maximum or minimum frequency content of the signal.\nThe raw audio signal is typically divided into small, overlapping frames to analyze the time-varying spectral proper ties. For a feature that is computed across frames, we compute the mean across all frames to obtain a more stable\nrepresentation for the entire audio clip."}, {"title": "7 Building Models for Lecture Voice Sentiment Analysis", "content": "Given the 13 extracted features, we focus on building and evaluating various classification models to identify the most\neffective approach for lecture voice sentiment analysis. Our strategy involves experimenting with traditional machine\nlearning models as well as deep neural networks. For traditional model, we select three widely-recognized models,\nnamely, logistic regression, random forest, and XGBoost. For deep neural networks, we explore a fully connected\nnetwork and a convolutional neural network (CNN).\nFeature Combinations: To identify the best feature set for classification, we evaluate all possible combinations of the\n13 features. This exhaustive approach ensures that we explore the potential synergies and interactions between different\nfeatures, allowing us to select the most informative subset. Given that there are 13 features, the total number of possible\nsubsets is 213 = 8192 including the empty set. For each subset (excluding the empty set), we evaluate a model on the\nindependent validation set to find the best hyper-parameters. This approach also provides valuable insights into which"}, {"title": "8 Experimental Results", "content": "We conduct a set of comprehensive experiments. For each model, we evaluate it on a combination of features. If it\nis a type of traditional model, we use the independent validation set to select the best parameters. We use Accuracy,\nPrecision, Recall, and F1-Measure for evaluation metrics. For the lecture voice sentiment analysis problem, we are\nmore interested in correctly identifying a boring lecture for intervention. We set the label boring=1 as the positive\ncase. The metrics with respect to boring=1 are defined as follows:\n$\\displaystyle Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$\n$\\displaystyle Precision = \\frac{TP}{TP+FP}$\n$\\displaystyle Recall = \\frac{TP}{TP+FN}$\n$\\displaystyle F1-Measure = \\frac{1}{\\frac{1}{Precision} + \\frac{1}{Recall}}$\nIn these formulas:\n\u2022 TP stands for true positives (correctly classified as boring),\n\u2022 TN stands for true negatives (correctly classified as engaging),\n\u2022 FP stands for false positives (incorrectly classified as boring),\n\u2022 FN stands for false negatives (incorrectly classified as engaging).\nThe metrics with respect to engaging=0 can be defined in the same way.\nTable 4 presents the best evaluation results for each model type specified in Table 3. Specifically, Table 4 lists the\nparameters of the optimal model and the feature combinations that yielded the highest F1-measure score for the boring\nlabel. Additionally, the table includes accuracy and other metrics for the engaging label for reference. In Table 4, the\nmetric Precision_1 refers to the metric calculated for label 1 (boring). This interpretation applies similarly to other\nmetrics with suffixes.\nThe results indicate that the logistic regression model with a regularization parameter of C = 10 outperformed\nothers across all evaluation metrics. The optimal feature set for this model includes the following four features:\nzcrate_mean, chroma_vqt_mean, spcent_mean, and spband_mean. Among these, zcrate_mean, spcent_mean,\nand spband_mean are single-value features, whereas chroma_vqt_mean is a vector with 12 dimensions. Analyzing the\nbest-performing models across different metrics reveals that spband_mean consistently appears in all top feature sets.\nOther features frequently found in high-performing models include spcent_mean, chroma_vqt_mean, zcrate_mean,\nand rms_mean. These results suggest that spectral features are particularly effective for distinguishing between engaging\nand non-engaging lecture voices.\nTime Complexity: On an Apple M2 MacBook Pro with 16 GB of memory, it took over 40 hours to select and evaluate\nthe traditional models and over 30 hours to evaluate the neural networks. The model selection and evaluation processes\nwere conducted across all 8,191 feature combinations."}, {"title": "9 Discussion", "content": "The analysis of lecture voice sentiment analysis presents promising outcomes; however, the current study is limited\nby its focus on a binary classification of engagement (engaging vs. non-engaging), which may not fully capture all\naspects of the lecturer delivery. Additionally, the model development and evaluation are constrained by the use of a"}, {"title": "10 Conclusion", "content": "The study presented in this paper explores the development of an intelligent lecturing assistant (ILA) system that\nutilizes a knowledge graph to represent course content and pedagogical strategies. The system is designed to support\ninstructors in enhancing student engagement in learning by analyzing lecture voice sentiment. The paper focuses on the\ndevelopment of a model that can classify lecture voice as either engaging or non-engaging, and the results demonstrate\npromising performance with an F1-score of 90% for boring lectures on an independent set of test voice clips.\nThis research on lecture voice sentiment analysis lays the groundwork for developing additional components of the\nintelligent lecturing assistant (ILA) system. An ILA system is a multifaceted platform that integrates knowledge\nrepresentation, reasoning, speech recognition, machine learning, and intelligent intervention. The next phase of\ndevelopment involves incorporating content analysis and pedagogical principles into the model, which would enable the\nsystem to deliver relevant interventions for instructors. Moreover, the system could be further developed to analyze\nstudent responses and provide real-time feedback.\nThe overall AI-powered system has the potential to enhance student engagement and learning outcomes. Future research\nwill also address ethical considerations related to the use of AI in education. The research team aims to deploy the\nsystem in both experimental environments and real classrooms to evaluate the effectiveness of the system on both\ninstructors and students."}, {"title": "11 Sources and Data Sets", "content": "For the sake of reproducibility, the source notebooks utilized for data analysis and model evaluation are accessible in\nthe public GitHub repository: https://github.com/anyuanay/KG_ILA .\nThe training data and the independent validation sets can be made available for download upon request."}]}