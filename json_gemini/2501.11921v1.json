{"title": "Goal-oriented Transmission Scheduling: Structure-guided DRL with a Unified Dual On-policy and Off-policy Approach", "authors": ["Jiazheng Chen", "Wanchun Liu"], "abstract": "Abstract-Goal-oriented communications prioritize application-driven objectives over data accuracy, enabling intelligent next-generation wireless systems. Efficient scheduling in multi-device, multi-channel systems poses significant challenges due to high-dimensional state and action spaces. We address these challenges by deriving key structural properties of the optimal solution to the goal-oriented scheduling problem, incorporating Age of Information (Aol) and channel states. Specifically, we establish the monotonicity of the optimal state value function-a measure of long-term system performance-w.r.t. channel states and prove its asymptotic convexity w.r.t. Aol states. Additionally, we derive the monotonicity of the optimal policy w.r.t. channel states, advancing the theoretical framework for optimal scheduling. Leveraging these insights, we propose the structure-guided unified dual on-off policy DRL (SUDO-DRL), a hybrid algorithm that combines the stability of on-policy training with the sample efficiency of off-policy methods. Through a novel structural property evaluation framework, SUDO-DRL enables effective and scalable training, addressing the complexities of large-scale systems. Numerical results show SUDO-DRL improves system performance by up to 45% and reduces convergence time by 40% compared to state-of-the-art methods. It also effectively handles scheduling in much larger systems, where off-policy DRL fails and on-policy benchmarks exhibit significant performance loss, demonstrating its scalability and efficacy in goal-oriented communications.", "sections": [{"title": "I. INTRODUCTION", "content": "Conventional communications focus on accurate bit-by-bit data transmission, achieving near-Shannon-capacity efficiency in 5G networks. However, as we transition to 6G, goal-oriented communications emerge as a transformative paradigm, prioritizing application-driven objectives over raw data accuracy [1]. This shift is critical for enabling intelligent and efficient next-generation networks. Goal-oriented communications encompass two main categories: human-centric and machine-centric. Human-centric applications, such as extended reality (XR) [2] and augmented reality (AR) [3], focus on preserving semantic meaning for accurate human comprehension. Machine-centric applications, including industrial Internet of Things (IIoT) [4] and autonomous driving [5], prioritize transmitting information that directly optimizes system performance. This paradigm shift transcends traditional communication approaches, enabling smarter, more efficient interactions between humans, machines, and their environments [6]."}, {"title": "A. Scheduling in Goal-oriented Communications", "content": "Efficient scheduling is crucial in goal-oriented communications to maximize the performance of systems with limited communication resources. Scheduling determines how devices share communication channels, directly impacting the timeliness and relevance of transmitted information-key factors for achieving system goals. Unlike conventional communication systems that typically use throughput, latency, or reliability as performance metrics, goal-oriented communications adopt application-specific metrics that evaluate the importance of the transmitted information in achieving the desired objective. Among these, the age of information (AoI) [7], which measures the freshness of data, is particularly important for machine-type applications where outdated messages can be-come irrelevant or even harmful to system performance.\nTo address scheduling challenges, optimizing scheduling policies has garnered significant attention in the communications community [1]. Existing works often use Aol and related metrics to guide scheduling decisions. For example, in [8], a control cost minimization problem in a single-loop network is reformulated as an AoI-based optimization problem and solved using Markov decision processes (MDPs) with value iteration. Beyond Aol, other metrics such as the value of information (VoI) [9] and mean square error (MSE) [10] have been introduced to assess information importance in various contexts. In [9], an optimal scheduling and power allocation problem is proposed for a single-sensor-single-controller system, maximizing VoI through an event-triggered policy. Similarly, [10] addresses remote estimation in a multi-sensor system by formulating an MSE minimization problem, solved using Whittle's index heuristic to derive a suboptimal policy. However, these approaches have notable limitations. Heuristic methods, while computationally efficient, cannot guarantee optimality. On the other hand, conventional dynamic programming methods, such as value and policy iteration, are computationally infeasible for large-scale systems with high-dimensional state and action spaces. These challenges underscore the need for scalable and optimal solutions tailored to the complexities of modern goal-oriented communication systems."}, {"title": "B. Off-policy and On-policy DRL Solutions", "content": "In recent years, deep reinforcement learning (DRL) has emerged as a powerful tool for solving large-scale MDPS"}, {"title": "C. Initial Studies on Structure-Enhanced DRL Algorithms", "content": "Most existing works apply general DRL algorithms to solve specific scheduling problems without incorporating domain-specific insights, focusing instead on brute-force optimization techniques. As a result, these algorithms are prone to getting stuck in local minima, leading to performance losses compared to the theoretical optimal policy. A major limitation of these approaches is the lack of investigation into the structural prop-erties of optimal policies, which, if utilized, could significantly enhance the efficiency and effectiveness of DRL algorithms.\nRecently, there has been growing interest in leveraging the structural properties of optimal policies to improve DRL-based"}, {"title": "II. SYSTEM MODEL", "content": "We consider a wireless goal-oriented communication sys-tem with N edge devices (e.g. cameras or sensors) and a remote destination (e.g. a base station or remote estimator) as illustrated in Fig. 1. The devices transmit local data to the remote destination through M channels (e.g. subcarriers) where M < N."}, {"title": "A. Communication Model", "content": "In this paper, wireless channels are modeled as indepen-dent and identically distributed (i.i.d.) block fading channels, where the channel state remains constant during each packet transmission, but changes independently between each trans-mission. At time step t, we denote the system channel state between device n and the remote destination at channel m as $g_{n,m,t} \\in G \\triangleq \\{1,...,\\bar{g}\\}$ with $\\bar{g}$ quantization levels. The overall system channel state is represented by an N \u00d7 M matrix $G_t$ with $g_{n,m,t}$ being the element in the mth column and nth row. The channel state $g_{n,m,t}$ follows the probability distribution:\n$P(g_{n,m,t} = i) = \\Gamma_{n,m}^{i}$,                                                                                                                     (1)\nwhere $\\sum_{i=1}^{\\bar{g}} \\Gamma_{n,m}^{i} = 1,\\forall n,m$. The packet drop rate for state $g_{n,m,t}$ is denoted as $\\psi_{n,m,t}$, with higher channel states corresponding to higher packet drop rates. The remote destination acquires the instantaneous channel state $G_t$ using standard channel estimation methods [23].\nThe channel assignment for device n at time t is denoted as\n$a_{n,t} = \\begin{cases}\nm, & \\text{ if channel m is allocated to device n}, \\\\\n0, & \\text{ if no channel is allocated to device n},\n\\end{cases}$                                      (2)\nThis assignment satisfies the constraint:\n$\\sum_{m=1}^{M} \\mathbb{1} (a_{n,t} = m) = 1, \\sum_{n=1}^{N} \\mathbb{1} (a_{n,t} = m) \\leq 1$,                                                                                           (3)\nwhere $\\mathbb{1}(.)$ is the indicator function. The constraint ensures that each channel is assigned to only one device, and each device is allocated at most one channel."}, {"title": "B. Goal-oriented Communication Performance Metric", "content": "We define $d_{n,t} \\in \\{1,2,...\\}$ as the Aol of the device n at time t, which refers to the time elapsed since the last successful receive of device packet at the destination [24], [25]:\n$\\delta_{n,t+1} = \\begin{cases}\n1, & \\text{ if remote destination receive}\\\\\n& \\text{ device n's packet at time t} \\\\\n\\delta_{n,t}+1, & \\text{ otherwise}.\n\\end{cases}$                                                                                                                                                                                                                                                                                                                                                                                                                      (4)\nTo characterize the importance of information in a goal-oriented communication system, we define a positive cost function $C_n (d_{n,t}),\\forall n \\in \\{1,2,..., N\\}$. This cost function, a critical performance metric for device n, is non-decreasing with respect to Aol and varies based on the system's goals, with lower values indicating better performance.\nNext, we provide an example of a goal-oriented communi-cation system [22]."}, {"title": "III. GOAL-ORIENTED TRANSMISSION SCHEDULING", "content": "Our goal is to determine a dynamic scheduling policy \u03c0(\u00b7) that, based on the AoI state $d_t \\triangleq \\{\\delta_{1,t},...,\\delta_{n,t}\\}$ and the channel state $G_t$, minimizes the infinity-horizon expected sum of cost functions across all N devices, with a discount factor \u03b3\u2208 (0,1). The problem is formulated as follows:\nProblem 1.\n$\\min_\\pi \\lim_{T \\to \\infty} E  \\sum_{t=1}^{T} \\gamma^{t}  \\sum_{n=1}^{N} C_n(\\delta_{n,t}) $."}, {"title": "A. MDP Formulartion", "content": "In Problem 1, the channel states are assumed to be i.i.d. over time, and the cost $C_n (d_{n,t})$ is defined as a function solely dependent on the Markovian Aol state $d_{n,t}$, as described in (4). Consequently, Problem 1 is a sequential decision-making problem that satisfies the Markov property, allowing it to be formulated as an MDP as below.\n\u2022 States: At time t, given the instantaneous Aol state vector $\\delta_t \\in \\mathbb{N}^{N}$ and the real-time full channel state matrix $G_t \\in \\mathbb{G}^{N \\times M}$, the MDP state is defined as $s_t \\equiv (\\delta_t, G_t)$ and the state space is $S \\triangleq \\mathbb{N}^{N} \\times \\mathbb{G}^{N \\times M}$.\n\u2022 Actions: Given a policy function \u03c0(\u00b7), which maps a state to an action, the action at time t is defined as $a_t = \\pi(S_t)=(a_{1,t},...,a_{n,t}) \\in \\{0,1,2,...,M\\}$, subject to the constraint (3). Under this constraint, the action space is $A \\subset \\{0, 1, 2, . . ., M\\}^{N}$ with the size of N!/(N \u2212 M)!.\n\u2022 Transitions: In the MDP, the probability of transitioning to the next state $s_{t+1}$ from the current state $s_t$ after executing the action $a_t$ is denoted as the state transition probability $P(s_{t+1} | s_t, a_t)$. Since the optimal policy of an infinite-horizon"}, {"title": "B. Value Functions for the Optimal Policy", "content": "For the optimal scheduling policy of the MDP, i.e., \u03c0*(\u00b7), we define the optimal action-value function $Q(s_t, a_t) : S \\times A \\rightarrow \\mathbb{R}$ and the optimal state-value function $v^*(s_t) : S \\rightarrow \\mathbb{R}$ as below.\nGiven the current state $s_t$ and action $a_t$, the action-value function, also known as Q function, represents the expected cumulative discounted cost of executing action $a_t$ and follow-ing the optimal policy \u03c0*(\u00b7), i.e.,\n$Q(s_t, a_t)=E_{\\tau = t}^{\\infty}   \\gamma^{\\tau-t} c(s_{\\tau})| s_t, a_t, a_{\\tau} = \\pi^*(s_{\\tau}), \\forall \\tau > t$,                                                                                                                                                                                                                                                                                                                                                                                                                          (7)\nwhich satisfies the Bellman optimality equation:\n$Q(S_t,a_t) = c(S_t)+ \\gamma \\sum_{S_{t+1}} P_r(S_{t+1} | S_t,a_t) min_{a_{t+1}\\in A}Q(S_{t+1},a_{t+1})$.                                                                                                                                (8)\nThe optimal action given the optimal policy \u03c0*(\u00b7) is\n$\\pi^*(s_t) = arg \\min_{a_t \\in A} Q(s_t, a_t)$.                                                                                                                               (9)\nThen, the optimal state-value function, also called the optimal V function, is defined as\n$v^*(s_t) = \\min_{a_t \\in A} Q(s_t, a_t)$,                                                                                                                                 (10)\nwhich is the minimum expected discounted sum of the future cost starting in state $s_t$ under the optimal policy \u03c0*(\u00b7). Based on (7), (8) and (9), the following inequality holds:\n$Q(s_t, a_t) \\geq v^* (S_t)$.                                                                                                                                         (11)\nConventional MDP algorithms, such as value iteration and policy iteration, solve MDP problems by computing the opti-mal V function $v^*(\\cdot)$ in (9) or the optimal policy \u03c0*(\u00b7) in (8)."}, {"title": "IV. STRUCTURAL PROPERTIES OF OPTIMAL POLICY", "content": "In this section, we derive structural properties of the optimal V function and the optimal scheduling policy, which will be leveraged in the design of advanced DRL algorithms in Section V. Similar to the MDP formulation in Section III-A, we use a, s, and s\u207a to denote at, st, and st+1, respectively, for simplicity in notation."}, {"title": "A. Monotonicity of Optimal V function", "content": "In our earlier work, we established the following result about the monotonicity of the optimal V function w.r.t. the Aol state:\nLemma 1 (Monotonicity of optimal V function w.r.t. Aol states [21]). Consider states $s = (\\delta, G)$ and $s'^{AoI} = (\\delta'^{(n)}, G)$, where $\\delta'^{(n)}$ is identical to \u03b4 except for the nth Aol state, which is $\\delta'_n$, and $\\delta'_n \\geq \\delta_n$, then, the optimal V function holds the inequality:\n$v^* (s'^{AoI}) \\geq v^* (s)$.\nWe now prove that the optimal V function is also monoton-ically decreasing in terms of the channel states as below.\nTheorem 1 (Monotonicity of the optimal V function w.r.t. channel states). Consider states $s = (\\delta,G)$ and $s^{ch} = (\\delta, G'^{(n,m)})$, where G and $G'^{(n,m)}$ are identical except for the element in the nth row and mth column $g_{n,m} < g'_{n,m}$. The optimal V function holds the inequality:\n$v^*(s^{ch}) \\geq v^* (s)$.\nProof. To prove Theorem 1 based on (10), it is sufficient to prove\n$Q(s^{ch}, a^*) \\geq Q(s, a^*)$,                                                                                                                                                                                                (11)\nwhere $a^*$ is the optimal action given the state s, i.e., $a^* = \\pi^*(s)$. From (6), we have the transition probability of the Aol state\n$P(\\delta^+|\\delta, G, a) = P(\\delta|\\delta_{n}, g_{n}, a_{n})$\n$\\times P (\\delta_{\\{n\\}}|\\delta_{\\{n\\}}, G_{\\{n\\}},a_{\\{n\\}})$,                                                                    (12)\nwhere $a_{\\{n\\}} \\triangleq (A_1,...,A_{n-1},A_{n+1},...,A_v)$ and $\\delta_{\\{n\\}} \\triangleq (\\delta_1,..., \\delta_{n-1}, \\delta_{n+1},...,\\delta_v)$ denote all actions and Aol states without the device n, respectively, and $G_{\\{n\\}} \\triangleq (g_1,...,g_{n-1},g_{n+1},...,g_n)$. By substituting (9) and (12) in the right-hand side of (7), we derive that\n$Q(s, a) = c(s)+\\gamma \\sum_{G^+} \\sum_{\\delta^+} P(G^+) P(\\delta^+|\\delta,G, a)v^*(s^+)$\n$= c(s) + \\gamma \\sum_{G^+} \\sum_{\\delta_{\\{n\\}}} \\sum_{\\delta_{n}} P(G^+) P(\\delta|\\delta_{n}, g_{n}, a_{n})$\n$\\times P(\\delta_{\\{n\\}}|\\delta_{\\{n\\}}, G_{\\{n\\}},a_{\\{n\\}})v^*(s^+)$.                                (13)"}, {"title": "B. Convexity of Cost function and Optimal V function", "content": "Since the input state of the optimal V function takes only discrete values, we define its convexity as below.\nDefinition 1 (Discrete convexity of optimal V function and cost function w.r.t. Aol). Consider states $s = (\\delta, G)$, $s'^{AOL} = (\\delta'^{(n)}, G)$, and $s''^{AoI} = (\\delta''^{(n)}, G)$, where $\\delta = (\\delta_1,..., \\delta_n, ..., \\delta_N)$, $\\delta'^{(n)} = (\\delta_1,..., \\delta'_n, ..., \\delta_N)$, $\\delta''^{(n)} = (\\delta_1,..., \\delta''_n, ..., \\delta_N)$, and $\\delta'_n \\geq \\delta_n \\geq \\delta''_n$. The cost function"}, {"title": "C. Monotonicity of Optimal Policy", "content": "In addition to the properties of the optimal V function, our earlier work establishes the following monotonicity of the optimal policy w.r.t. the channel state:\nTheorem 4 (Monotonicity of optimal policy w.r.t. chan-nel states [21]). Consider states $s = (\\delta,G)$ and $s^{ch} = (\\delta, G'^{(n,m)})$, where G and $G'^{(n,m)}$ are identical except for the element in the nth row and mth column $g_{n,m} \\geq g'_{n,m}$, and the corresponding optimal actions are $a^*$ and $a^{ch}$, respectively. If $a^*_n = m \\neq 0$, then the optimal action $a^{ch,*}_{n}$ satisfies the following equality:\n$a^{ch,*}_n = m$.\nThis monotonicity demonstrates that if the optimal action for device n is to schedule it to channel m for state s, then for another states\", where channel m of device n has better quality while all other state components remain identical to s, device n should still be scheduled to channel m.\nPlease note that we have also developed optimal policy monotonicity in terms of AoI in [21] but only for some special cases, e.g., a two-device-single-channel scenario. Since no general results have been derived, we will not present them here or consider them in the design of our DRL algorithm in the subsequent section."}, {"title": "D. Greedy Structure of Optimal Policy", "content": "In this section, beyond analyzing the properties of the optimal V function, we aim to establish the structure of the optimal scheduling policy. Deriving the structure of the optimal scheduling policy for a general multi-sensor, multichannel system is not feasible due to the complexity of the problem. Instead, we focus on a special case involving co-located devices with identical channel states. This simplification, which focuses solely on the Aol states of different devices while disregarding variations in their channel states, allows us to address the problem more tractably and extract meaningful insights.\nTo achieve this, we first define the mandatory scheduling set as follows:\nDefinition 2 (Mandatory scheduling set). Consider an N-device-M-channel system. If there exists a threshold 8 such that the asymptotic cost function satisfies the following order-ing inequality:\n$C_{i_1} (\\delta) \\geq C_{i_2} (\\delta) . . . . . . \\geq C_{i_N} (\\delta), \\forall \\delta \\geq \\delta$,\nwhere $i_n \\in \\{1,..., N\\}$ represents an device index, then the following holds:\nGiven the Aol state $({\\delta_1},..., {\\delta_N})$, if there exists a largest number \u00d1 < M such that the set $I \\triangleq \\{i_1,...,i_{\\tilde{N}}\\}$ includes the \u00d1 devices with the largest Aol states, each greater than 8, then I is defined as the mandatory scheduling set.\nThe mandatory scheduling set is time-varying due to the dynamics of the AoI states. If the set exists, it is intuitive that all devices within it should be scheduled. This is because the instantaneous cost of scheduling any device in the set exceeds that of any device outside the set. Moreover, leaving a device in the set unscheduled keeps resulting in a higher instantaneous cost than scheduling a device not belonging to the set, thereby increasing the future long-term cost. Consequently, the optimal scheduling action aligns with a greedy action, which seeks to minimize the immediate cost at each time step. This alignment with a greedy action justifies referring to this structure as the greedy structure of the optimal policy. The result and its detailed proof are provided below.\nTheorem 5 (Asymptotic greedy structure of the optimal scheduling policy for co-located devices). Consider a multi-device-multi-channel system with co-located devices. If the mandatory scheduling set in Definition 2 exists, the optimal policy schedules all devices within the set, i.e., $a^*_n \\neq 0, \\forall n \\in I$."}, {"title": "V. STRUCTURE-GUIDED UNIFIED ON-OFF POLICY DRL", "content": "In this section, we leverage the theoretical results ob-tained to develop a structure-guided unified dual on-off policy (SUDO) DRL method. This approach combines the strengths of both off-policy and on-policy DRL, utilizing the state-of-the-art on-policy PPO algorithm, widely regarded as one of the most advanced DRL methods available. First, we briefly"}, {"title": "A. Overview of PPO Algorithm", "content": "A PPO agent has two neural networks (NNs): an actor NN and a critic NN. The actor NN, with the parameter set $\\varphi$, approximates the original deterministic policy \u03c0*(s) by a stochastic policy \u03c0(\u00e3|s; \u03c6), which outputs a probability distribution over actions \u00e3 given the state s. Note that in our original scheduling problem, the action \u00e3 is selected from the discrete action space of size N!/(N \u2013 M)!. Here to apply the PPO algorithm, which operates in a continuous action space, we implement an action mapping method [17]. This approach maps the N-dimensional continuous action \u00e3 generated by the actor NN into a corresponding discrete action a. For simplicity in notation, the process of obtaining a from the actor NN \u03c6 is represented as a stochastic function:\na = f(s; \u03c6).\nThe critic NN, with the parameter set $v$, approximates the optimal V function v* (s) as v(s; v), outputting the estimated value of the optimal V function for a given state s. Training a PPO agent involves two iterative steps: generating an experience trajectory and updating both NNs.\nStep 1: Experience generation. The PPO agent generates a trajectory of length T, resulting in the trajectory:\n$\\mathcal{T}_{on} = \\{ (s_t, \\bar{a}_t, C_t) \\}_{t=0}^{T-1}$.\nAt each time step t, the actor NN uses the current stochastic policy \u03c0(\u00e3t| St; \u03c6old) to sample a continuous action \u00e3t, which is then mapped to the discrete (real) scheduling action at. The next state $s_{t+1}$ and the cost $c_t$ are obtained by executing the real action at. The critic NN computes the estimated optimal V function of the state v($s_t$; v). Using the trajectory Ton, the advantage function $A_t$ and the cost-to-go function $C_t$ are calculated as\n$A_t = \\sum_{t'=0}^{T-t-1} (\\gamma\\lambda)^{t'}\\delta_{t+t'}$,                                                                                                                                                                                                                           (17)\n$C_t = c_t + \\gamma v(s_{t+1}; v)$,                                                                                                                     (18)\nwhere X is the generalized advantage estimation (GAE) pa-rameter, and $\\delta_t = c_t + \\gamma v(s_{t+1}; v) - v(s_t; v)$. The trajectory is then updated as\n$\\mathcal{T}_{on} = \\{ (s_t, \\bar{a}_t, A_t, C_t) \\}_{t=0}^{T-1}$.\n(19)\nStep 2: NN update. To update the actor and critic NNs, the PPO agent randomly samples $B_1$ data points from Ton to create a mini-batch dataset:\n$\\mathcal{B}_1 = \\{ (s_l, \\bar{a}_l, A_l, C_l) \\}_{l=1}^{B_1}$.\nFor the critic NN, the temporal difference (TD) error is defined as:\n$TD_l \\equiv C_l \u2212 v(s_l; v)$,                                                                                       (20)"}, {"title": "B. Proposed SUDO-DRL", "content": "To leverage the advantages of on-policy DRL, known for its stable training performance, and off-policy DRL, which offers higher sampling efficiency by reusing past data and facilitates better exploration without getting trapped in local minima, the proposed SUDO-DRL algorithm innovatively integrates concepts from both on-policy and off-policy approaches. Fundamentally, the effectiveness of SUDO-DRL lies in its carefully designed loss functions for the actor and critic NNs. These loss functions combine both on-policy and off-policy components as follows:\n$L_{SUDO}(v) = L_{on}(v) + \\beta_1 L_{off}(v)$                                                                                                                                                                                                                  (22)\n$L_{SUDO}(\\varphi) = L_{on}(\\varphi) + \\beta_2 L_{off} (\\varphi)$,                                                                                                                                                                                                      (23)\nwhere $B_1$ and $B_2$ are the hyperparameters to balance the contributions of the on-policy and off-policy loss functions.\nIn the following, we first present a holistic structural prop-erty evaluation framework based on the theoretical results discussed in the previous section.\u00b9 Building on this foundation, we then propose methods for constructing the on-policy and off-policy loss functions, respectively."}, {"title": "VI. NUMERICAL EXPERIMENTS", "content": "In this section, we evaluate and compare the performance of the proposed SUDO-DRL with PPO, the benchmark on-policy DRL, and three off-policy DRL algorithms: DDPG [28], structure-enhanced DDPG (SE-DDPG) [21], and type II monotonicity-regularized DDPG (MRII-DDPG) [22]. Notably, the latter two algorithms represent state-of-the-art structure-guided DRL approaches for addressing goal-oriented commu-nication scheduling problems."}, {"title": "A. Experiment Setups", "content": "Our numerical experiments were conducted on a computa-tional platform equipped with an Intel Core i7 9700 CPU @ 3.0 GHz, 32GB RAM, and an NVIDIA RTX 3060Ti GPU. The experimental environment is based on a remote state esti-mation system as described in Example 1, with the estimation MSE considered as the performance metric. For this system, the dimensions of the process state and measurement are set to rn = 2 and Cn = 1, respectively. The system matrices An are randomly generated with spectral radii uniformly drawn from the range (1,1.3).\nThe discrete fading channel state is quantized into \u011f = 5 levels, with corresponding packet drop rates set to 0.2, 0.15, 0.1, 0.05, and 0.01. These values are derived from the Rayleigh distribution with a scale parameter randomly generated within the range (0.5, 2) [29].\nFor a fair comparison, the actor and critic NNs of the SUDO-DRL and benchmark agents are implemented as fully connected NNs, each with three hidden layers, as described in [17]. The input dimension of the actor NN matches the state dimension, i.e., N + N \u00d7 M, for all algorithms. The"}, {"title": "VII. CONCLUSION", "content": "We have derived key structural properties of the optimal solution to the goal-oriented scheduling problem, establish-ing monotonicity and asymptotic convexity for the optimal"}]}