{"title": "Adversarial Watermarking for Face Recognition", "authors": ["Yuguang Yao", "Anil Jain", "Sijia Liu"], "abstract": "Watermarking is an essential technique for embedding an identifier (i.e., watermark message) within digital images to assert ownership and monitor unauthorized alterations. In face recognition systems, watermarking plays a pivotal role in ensuring data integrity and security. However, an adversary could potentially interfere with the watermarking process, significantly impairing recognition performance. We explore the interaction between watermarking and adversarial attacks on face recognition models. Our findings reveal that while watermarking or input-level perturbation alone may have a negligible effect on recognition accuracy, the combined effect of watermarking and perturbation can result in an adversarial watermarking attack, significantly degrading recognition performance. Specifically, we introduce a novel threat model, the adversarial watermarking attack, which remains stealthy in the absence of watermarking, allowing images to be correctly recognized initially. However, once watermarking is applied, the attack is activated, causing recognition failures. Our study reveals a previously unrecognized vulnerability: adversarial perturbations can exploit the watermark message to evade face recognition systems. Evaluated on the CASIA-WebFace dataset, our proposed adversarial watermarking attack reduces face matching accuracy by 67.2% with an $l_\\infty$ norm-measured perturbation strength of 2/255 and by 95.9% with a strength of 4/255.", "sections": [{"title": "Introduction", "content": "Face recognition systems have become increasingly prevalent in various domains, such as access control and surveillance [1\u20133]. Ensuring the integrity and ownership of facial images used for training and evaluation in such systems is crucial. Image watermarking has offered a viable solution for proprietary face image protection [4-6]. Watermarking can embed hidden information (also called 'watermark message') in digital faces to assert ownership, authenticate content, and verify data integrity [7-9].\nHowever, as machine learning (ML) models become more sophisticated, they also become susceptible to adversarial attacks. Adversarial perturbations (also known as evasion attacks) are carefully crafted modifications to input data that deceive ML models without noticeable changes in the image to human observers [10-12]. In the context of face recognition, such perturbations can cause recognition errors, leading to security breaches; See the literature review in Section 2.\nAlthough watermarking aims to protect and authenticate images, the interaction between watermarking processes and adversarial attacks remains underexplored. The presence of watermarking and adversarial attacks, along with their interaction, has added substantial complexity to evaluation of face recognition systems. Inspired by the above, we address the following question:\n(Q) How does watermarking affect the adversarial robustness of face recognition systems, and can adversarial attacks exploit watermarking to even degrade face matching performance?\nTo the best of our knowledge, our work unveils the joint effects of watermarking and adversarial attacks on face recognition models for the first time. We summarize our contributions below."}, {"title": "Related Work", "content": "Watermarking in Face Recognition. Watermarking techniques have long been used to embed imperceptible information into digital images for purposes such as copyright protection, authentication, and integrity verification [6, 15, 16]. In the realm of face recognition, watermarking serves as a tool to protect personally identifiable images from unauthorized use and tampering [4, 17-21]. Various methods have been proposed to integrate watermarking into facial images without significantly affecting recognition performance. Traditional watermarking approaches use frequency domain transformations such as Discrete Cosine Transform (DCT) and Discrete Wavelet Transform (DWT) to embed watermarks in images, with the aim of robustness against common image processing attacks [22, 23]. In contrast, recent methods leverage deep neural networks (DNNs) for watermarking, such as the HiDDeN framework, which employs end-to-end trainable networks to embed and extract watermarks, enhancing resilience against various attacks [5]. Other recent studies have focused on ensuring that the watermarking process preserves critical facial features essential for accurate recognition [7, 8, 17]. However, these methods mainly focus on robustness against non-adversarial distortions and fail to account for the impact of adversarial perturbations specifically designed to deceive ML models, particularly when watermarking is applied.\nAdversarial Attacks in Face Recognition. Adversarial attacks involve introducing subtle, often imperceptible perturbations to input data with the intent of deceiving ML models [10, 13, 24]. In face recognition systems, adversarial examples can lead to recognition errors, impersonation, or evasion,"}, {"title": "Methods", "content": "Watermarking System. We start by introducing the technique used for generating watermarked face images and its application in the subsequent face recognition task, as shown in Figure 1-(\u0391). \u03a4\u03bf formalize the watermarking problem, let the input image be denoted as $I \\in R^{H\\times W\\times C}$, and a binary watermark message as $m \\in \\{0, 1\\}^L$ (an L-bit digital signature) embedded into the facial images [5, 7, 29]. Our goal is to produce a watermarked image $I_w$ that maintains visual similarity to the original image $I$ containing the watermark message $m$. Furthermore, the watermarked image should allow extraction of $m$, allowing provenance of the image.\nWe implement the watermarking system using the open source neural network-based HiDDeN framework [5]. This system consists of an encoder network $f_e$ and a decoder network $g_d$. The encoder takes the input image $I$ and the watermark message $m$ as inputs and generates the watermarked image $I_w = f_e(I,m)$. The decoder takes the watermarked image $I_w$ as input and reconstructs the embedded watermark message $m = g_d(I_w)$. The encoder and decoder networks are jointly trained using a combination of image reconstruction loss and message decoding loss. The loss of image reconstruction $l_{recons}$ (e.g., mean squared error) ensures that the watermarked image is visually similar to the original, while the loss of message decoding $l_{decode}$ (e.g., bitwise binary cross-entropy loss) minimizes the difference between embedded and extracted watermark messages. The overall training objective for watermarking encoder and decoder is:\n$\\min_{\\theta,\\varphi} E_{I,m} [l_{recons} (I_w, I) + \\lambda l_{decode} (m, m)]$ (1)\nwhere $\u03bb$ is a regularization parameter balancing the two losses. During training, a random message generator produces random bits for $m$. This randomness allows the network to generalize to any watermark message, enabling us to embed user-defined messages in face images later on. Table 1 shows that our watermarking system is fairly robust against different data transformations. However, as demonstrated later, this does not guarantee adversarial robustness for the downstream task when using watermarked data.\nFace Recognition on Watermarked Images. With watermarked face images acquired above, we proceed to face recognition to assess the impact of the watermarking. In what follows, we provide a brief background on face recognition. Given an input face image $I$, the face recognition model $h_\\psi$ maps the image to a feature representation $z$: $z = h_\\psi(I)$, where $\u03c8$ represents the learnable parameters of the model. The feature $z$ is typically extracted from the penultimate layer of a convolutional neural network (CNN), such as ResNet [30]. The model is trained to minimize a classification loss, such as the softmax loss [31] or margin-based losses [31\u201333], which encourage facial features from the same identity to be close in the embedding space while pushing apart facial features from different"}, {"title": "Adversarial Watermarking Attack for Face Recognition", "content": "We introduce an adversarial water-marking attack that exploits the interaction between adversarial perturbations and the watermarking process to degrade face recognition performance. The adversary aims to craft a minimal perturbation \u03b4 added to a probe face image $I_p$ and find a specific watermark message $m \\in \\{0,1\\}^L$ such that:\n1. Pre-watermark recognition success: The perturbed image $I = I_p + \u03b4$ is correctly matched with the reference image $I_r$ by the face recognition model $h_\u03c8$, i.e., the similarity between their feature representations remains high. Here $\u03b4 \\in R^{H\u00d7W\u00d7C}$ denotes adver-sarial perturbations bounded by $||\u03b4||_\u221e < \u03f5$, where $\u03f5$ is the perturbation strength ensuring imperceptibility.\n2. Post-watermark recognition failure: After applying the watermarking encoder $f_e$ with the adversary-learned watermark message $m$, the perturbed input image $I$, and its watermarked counterpart $I_w = f_e(I, m)$ lead to a low similarity with the reference image $I_r$, causing the face recognition model $h_\u03c8$ to fail.\nOur rationale has two key aspects. First, satisfying both conditions 1 and 2 ensures that the adversarial attack (\u03b4) stays stealthy when watermarking is absent, but is triggered upon watermark application, leading to recognition failures. Second, this design reveals a unique adversarial challenge in face recognition with watermarking, where the optimization of the watermark message in condition 2 interacts synergistically with the input perturbations \u03b4 to amplify the adversarial effect.\nWe propose the following joint optimization problem to find the adversarial perturbation \u03b4 and the watermark message m:\n$\\min_{m \\in \\{0,1\\}^L} \\min_{||\u03b4||_\u221e < \u03f5} -s(z_p, z_r) + s(z_w, z_r)$ (4)\nwhere the optimization variables are the binary watermark message m and the input perturbations \u03b4, and $s(\u00b7,\u00b7)$ and $z_i$ are defined in (2). Recall that $z_p = h_\u03c8(I_p)$ and $z_w = h_\u03c8(I_w)$ are the feature representations given the probe image $I_i = I_p + \u03b4$ and $I_w = f_e(I_p, m)$, respectively. In (4), the original similarity term $s(z_p, z_r)$ ensures that the perturbed face is still recognized as the same identity in the absence of watermarking. And the watermarked similarity term $s(z_w, z_r)$ minimizes the similarity between the watermarked, perturbed image and the reference image, causing face recognition failure post-watermarking.\nTo solve the optimization in (4), we then adopt an alternative optimization procedure to jointly optimize 8 and m. Specifically, we use the PGD (projected gradient descent) method [14] to iteratively minimize one variable while keeping the other fixed. In the optimization process, we face the challenge of the discrete nature of the watermark message m. Direct optimization over binary variables is computationally intractable for large dimensionality L. To address this, we relax m to be continuous in the range [0, 1] during the optimization. This relaxation allows us to employ PGD in an efficient way. That is, after performing gradient descent on the relaxed m, we project back onto the binary set {0, 1} by rounding each element to 0 or 1. This ensures the watermark message remains valid for the encoder. By alternately optimizing over 8 and m, we minimize the joint objective. This approach finds a combination of adversarial perturbation and a watermark message that maintains high genuine similarity before watermarking and cause misrecognition afterward."}, {"title": "Experiments", "content": "Experimental Setup. We use the CASIA-WebFace dataset [34], containing face images of 10,575 individuals, for evaluating face recognition models. We extract 1,000 individuals with two matching face images for each identity ($I_p$ and $I_r$), and pre-processed them by aligning and resizing the images to 112 \u00d7 112 pixels. We adopt our face recognition model from the AdaFace framework [35]. AdaFace is known for its adaptive margin loss that accounts for the quality of the face images, improving recognition performance. The model is trained on MS-Celeb-1M dataset [31] using standard training protocols with a ResNet-50 backbone [30]. For watermarking, we follow the HiDDeN framework [5] to solve the problem (1). The encoder and decoder networks are trained on the MS-COCO dataset [36] with random 48-bit watermark messages. The trained encoder is then used to embed watermarks in the CASIA-WebFace face images. In generating the adversarial watermarking attack (4), the step sizes for optimizing 8 and m are set to $\u03b1 = 0.1/255$ and $\u03b2 = 1/T$, respectively, where $T = 10$ represents the number of iterations for the PGD-10 attack.\nEvaluation. We assess the effectiveness of the adversarial watermarking attack by analyzing face recognition performance under two key conditions. First, in the case of recognition with adversarial perturbations, adversarial perturbations are applied to the probe images without watermarking. Next, in the case of recognition with the adversarial watermarking attack (with watermarking), both adversarial perturbations and an optimized watermark message are applied, following the joint optimization in (4).\nAdversarial Watermarking: Joint Effects of Watermarking and Adversarial Perturbations.\nTo analyze the effect of the adversarial watermarking attack on face recognition performance, we first examine the similarity scores between probe and reference images across different perturbation strengths \u03f5. Figure 2 shows violin plots of the similarity distributions for face recognition, both with and without watermarking, when evaluated using input perturbations 8 from the proposed adversarial watermarking attack. As the perturbation strength \u03f5 increases, the similarity between probe and reference images decreases significantly in the presence of watermarking, while it remains largely unaffected without watermarking. This is because in the absence of watermarking, the first loss term in (4) aims to maximize the similarity between the probe image and the reference image for the applied perturbations \u03b4. With watermarking in the face recognition process, the similarity score quickly drops with increased perturbation strength. In fact, when $\u03f5 = 0.5/255$, the similarity has tended to be smaller than the matching threshold \u03c4 (commonly set at \u03c4 = 0.3). This shows that even a small adversarial perturbation can disrupt face recognition after watermarking, although performance remains stable without watermarking."}, {"title": "Conclusion", "content": "Our study investigated the vulnerabilities of face recognition systems when adversarial perturbations are combined with watermarking. While watermarking alone had a minimal effect on recogni-tion accuracy, the introduction of adversarial perturbations before watermarking caused significant performance degradation. Our findings show that adversarial watermarking attacks could severely undermine recognition systems even if they remain stealthy when watermarking is absent, highlighting the need for improved defenses in both watermarking and face recognition models."}]}