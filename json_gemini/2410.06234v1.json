{"title": "TEOCHAT: A LARGE VISION-LANGUAGE ASSISTANT\nFOR TEMPORAL EARTH OBSERVATION DATA", "authors": ["Jeremy Andrew Irvin", "Emily Ruoyu Liu", "Joyce Chuyi Chen", "Ines Dormoy", "Jinyoung Kim", "Samar Khanna", "Zhuo Zheng", "Stefano Ermon"], "abstract": "Large vision and language assistants have enabled new capabilities for interpret-\ning natural images. These approaches have recently been adapted to earth obser-\nvation data, but they are only able to handle single image inputs, limiting their\nuse for many real-world tasks. In this work, we develop a new vision and lan-\nguage assistant called TEOChat that can engage in conversations about temporal\nsequences of earth observation data. To train TEOChat, we curate an instruction-\nfollowing dataset composed of many single image and temporal tasks including\nbuilding change and damage assessment, semantic change detection, and temporal\nscene classification. We show that TEOChat can perform a wide variety of spa-\ntial and temporal reasoning tasks, substantially outperforming previous vision and\nlanguage assistants, and even achieving comparable or better performance than\nspecialist models trained to perform these specific tasks. Furthermore, TEOChat\nachieves impressive zero-shot performance on a change detection and change\nquestion answering dataset, outperforms GPT-40 and Gemini 1.5 Pro on multiple\ntemporal tasks, and exhibits stronger single image capabilities than a compara-\nble single EO image instruction-following model. We publicly release our data,\nmodels, and code: https://github.com/ermongroup/TEOChat.", "sections": [{"title": "1 INTRODUCTION", "content": "Many earth observation (EO) tasks require the ability to reason over time. For example, change\ndetection is a widely studied task where the goal is to identify salient changes in a region using\nmultiple EO images capturing the region at different times (Chughtai et al., 2021; Bai et al., 2023;\nCheng et al., 2023). Automated methods to effectively perform change detection can significantly\naid humanitarian and sustainability efforts, including planning disaster relief (Rahnemoonfar et al.,\n2021; Gupta et al., 2019; Ban et al., 2020), tracking urban changes (Tamilenthi & Baskaran, 2013;\nFyleris et al., 2022; Verma et al., 2021), and monitoring deforestation (Shermeyer & Haack, 2015;\nDe Bem et al., 2020; Decuyper et al., 2022), crops, (Gim et al., 2020; Kaur et al., 2023; Wang et al.,\n2024a), and ecological conditions (Willis, 2015; Xu et al., 2019). Previous methods to automatically\ndetect change in EO imagery have been specialist models, constraining their use to a single task or\nsmall set of tasks that they were explicitly trained to perform (Bai et al., 2023; Cheng et al., 2023).\nAdvancements in the modeling of multimodal data have enabled generalist vision-language models\n(VLMs) that can perform a variety of natural image interpretation tasks specified flexibly through\nnatural language (Achiam et al., 2023; Team et al., 2023; Liu et al., 2023). These models have\nrecently gained capabilities to reason over temporal sequences of natural images (video) (Maaz\net al., 2023; Lin et al., 2023; Jin et al., 2023; Li et al., 2023; Zhang et al., 2023b; Liu et al., 2024b)\nand single EO images (Hu et al., 2023; Kuckreja et al., 2023; Zhan et al., 2024; Muhtar et al., 2024;\nPang et al., 2024). However, no prior VLMs can model temporal EO data (left of Figure 1), notably\nincluding change detection tasks. We investigate the performance of Video-LLaVA (Lin et al., 2023),\na strong natural image pre-trained VLM that can receive images and videos as input, and GeoChat\n(Kuckreja et al., 2023), a strong VLM fine-tuned on single EO image tasks (right of Figure 1). We\nfind that Video-LLaVA generates inaccurate information, likely because it has primarily been trained\non natural images and videos, whereas GeoChat can only input single images and cannot process\ninformation across time."}, {"title": "2 DEVELOPING A VLM FOR TEMPORAL EO DATA", "content": "We aim to develop a VLM that can input a natural language instruction (prompt) and a temporal\nsequence of EO images, and output a natural language response. For example, given a sequence of\nsatellite images capturing an urban area before and after a hurricane, the model should be able to\nrespond to questions about damaged buildings or flooded regions. The vast majority of VLMs that\ncan perform this task on natural images follow a LLaVA-like architecture (Liu et al., 2024a) consist-\ning of (i) a large language model (LLM; commonly LLaMA 2 (Touvron et al., 2023) initialized with\nVicuna weights (Chiang et al., 2023)) (ii) a vision encoder (commonly a ViT (Dosovitskiy et al.,\n2020) pre-trained with CLIP (Radford et al., 2021)) to generate the visual representations, and (iii) a\nvision-language connector (commonly an MLP (Liu et al., 2023)) to align the visual representations\nwith the natural language token embeddings input to the LLM.\nTraining the VLM requires aligning the language representations with the visual concepts in EO\nimages, but these concepts are not well-represented in natural image data. EO imagery is taken from\na birds-eye view and captures land use and land cover changes like flooding or road construction.\nTo align the the EO imagery with natural language and allow the model to provide responses to\ninstructions about the EO data, we require a dataset of triplets consisting of sample instructions,\nEO images, and ground truth responses, commonly referred to as an instruction-following dataset."}, {"title": "3 TEOCHATLAS: A TEMPORAL EO INSTRUCTION-FOLLOWING DATASET", "content": "The main obstacle to developing a temporal EO VLM is the absence of a suitable instruction-\nfollowing dataset. To overcome this, we curate an instruction-following dataset of temporal EO\nimagery called TEOChatlas. A few key desired capabilities of the VLM induce important dataset\ndesign decisions. First, the model should be able to follow instructions about a wide variety of\ntasks requiring spatial and temporal reasoning capabilities, so we construct a diverse composition of\ninstruction-following tasks (Section 3.1). Second, the model should be able to reason over variable-\nlength temporal sequences of EO data from different sensors, so we ensure that many sensors and\nsequence lengths are represented (Section 3.2). Third, the model should be able to reference spe-\ncific images in its input and output and support user-friendly conversation, so we design the dataset\nprompts to support these features (Section 3.3)."}, {"title": "3.1 TEOCHATLAS: TASK COMPOSITION", "content": "We curate various instruction-following tasks in TEOChatlas in order to develop a range of capa-\nbilities in the model. Importantly, because our goal is to add new temporal reasoning capabili-\nties without compromising single image reasoning abilities, we include both single and temporal\ninstruction-following tasks in the dataset, described in detail below."}, {"title": "3.1.1 SINGLE IMAGE TASKS", "content": "To support the development of single image capabilities like object recognition and spatial reason-\ning, we include many single image instruction-following tasks in TEOChatlas. Specifically, we use\nthe GeoChat dataset (Kuckreja et al., 2023), a composition of single image instruction-following\ntasks including scene classification, visual question answering, referring expression, region caption-\ning, detailed description, grounded description, and multi-turn conversation. The GeoChat dataset\nwas used to train the GeoChat model, which exhibits strong performance on these single image\ntasks. Appendix A has more detail about the GeoChat dataset and instruction-following tasks."}, {"title": "3.1.2 TEMPORAL TASKS", "content": "We construct several temporal instruction-following tasks using four common temporal EO datasets.\nWe group the tasks into seven categories spanning two common real-world applications, namely\ndisaster response and urban development monitoring. Figure 2 shows examples of the temporal\ntasks and Table 8 in the Appendix provides the full set of tasks and associated prompts. TEOChatlas\nis the first instruction-following EO dataset to have temporal tasks (Table 6).\nWe construct an instruction-following task from a standard\nEO task called temporal scene classification, where the goal is to classify a sequence of satellite\nimages into a set of predefined categories. For this task, we use the Functional Map of the World\n(fMoW) dataset (Christie et al., 2018), consisting of satellite image time series classified as one of\n62 categories. To convert fMoW to an instruction-following task, we use a standard prompt which\ninstructs the model to classify the image by selecting one of the 62 classes provided in the prompt.\nWe curate multiple instruction-following tasks for change detection, a\nwidely studied earth vision task where the goal is to identify changes between images of the same\nregion over time. We include two canonical change detection tasks in the dataset, namely building\ndamage assessment and building change detection.\nFor building damage assessment, we use xBD (Gupta et al., 2019), a building damage detection\ndataset consisting of bitemporal pre- and post-disaster images, where every building is localized\nand classified into four damage severity categories. Following standard practice on this dataset,\nwe divide this task into two subtasks, namely building localization (Loc.) and building damage"}, {"title": "4 TEOCHAT", "content": "4.1 TEOCHAT ARCHITECTURE\nWe adopt a LLaVA-1.5-like architecture (Liu et al., 2023) consisting of (i) a temporally-shared im-\nage encoder (CLIP ViT-L/14 (Radford et al., 2021)) to obtain representations of each image in the\nsequence, (ii) a 2-layer MLP to project the visual representations to the input of the LLM, and (iii)\nan LLM decoder (Llama 2 (Touvron et al., 2023)) which inputs the instruction and temporal se-\nquence of projected visual representations to generate the response (Figure 3). A recent extension of\nLLaVA-1.5 to handle videos called Video-LLaVA (Lin et al., 2023) uses joint natural image-video\ninstruction-tuning by using the frozen CLIP ViT image encoder for images and a frozen Language-\nBind (CLIP-aligned) video encoder for videos (Zhu et al., 2023), while keeping the rest of the\narchitecture the same as LLaVA-1.5. We test whether these projector and LLM weights learned\non video data are better for temporal EO data by comparing Video-LLaVA initialization to LLaVA\ninitialization, as well as freezing vs. fine-tuning the projector, in our experimental ablations.\nAnother key architectural decision is whether to use the image encoder or video encoder from Video-\nLLaVA (Lin et al., 2023) to generate visual representations for the sequence. We choose to use\nthe image encoder for two reasons: (1) prior change detection approaches have demonstrated that\nSiamese encoders with weights shared across time (temporal-agnostic) are highly effective for iden-\ntifying changes in sequences of EO data (Zheng et al., 2021a;b; 2022), and (2) the video encoder was\ndesigned to input fixed sequences of 8 images. While it is possible to remove this second constraint,\nwe observed performance degradation when using shorter sequences in preliminary experiments,\nlikely because the video encoder was trained strictly on 8 image sequences (Zhu et al., 2023). Repli-\ncating images in the sequence may address this but makes training and inference much slower.\nAltogether, our vision encoder design is fast, memory efficient, and achieves strong performance."}, {"title": "4.2 TEOCHAT TRAINING DETAILS", "content": "Following prior work (Kuckreja et al., 2023), to retain the strong capabilities of the pre-trained image\nencoder and LLM, and to minimize memory usage during training, we freeze the vision encoder and\nprojector weights then fine-tune the LLM using Low-Rank Adaptation (LoRA) (Hu et al., 2021).\nTo further reduce memory footprint, we use 8-bit quantization of the base LLM weights. These\nstrategies allow us to train the large multimodal model on temporal sequences of up to 8 images using\nan NVIDIA A4000 GPU (16GB VRAM). We provide additional training details in the Appendix."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "We evaluate TEOChat in a variety of settings. First, we find that TEOChat demonstrates impressive\ntemporal reasoning capabilities, outperforming two strong VLMs, one for videos (Video-LLaVA)\nand one for single EO images (GeoChat), and even rivals or outperforms specialist approaches\n(Section 5.1). Second, we test model design decisions in experimental ablations, and find our de-\nsign of TEOChat is superior to many others (Section 5.2). Third, we demonstrate that TEOChat\nachieves impressive zero-shot generalization to new temporal datasets not in TEOChatlas (Sec-\ntion 5.3). Fourth, we find that TEOChat outperforms two strong proprietary foundation models\ntrained to model sequences of images (Section 5.4). Fifth, we observe that TEOChat possesses\nstrong single image capabilities, outperforming GeoChat on average across zero-shot scene classifi-\ncation and visual question answering (Section 5.5)."}, {"title": "5.1 COMPARISON TO OTHER VLMS AND SPECIALIST APPROACHES", "content": "Baselines and Evaluation Metrics We compare TEOChat to Video-LLaVA (Lin et al., 2023),\nGeoChat (Kuckreja et al., 2023), and specialist models (Gupta et al., 2019; Shen et al., 2021; Verma\net al., 2021). To obtain GeoChat's predictions on the temporal tasks, we perform temporal post-\nprocessing of its single image predictions. For example, for building change detection, we obtain\nbuilding location predictions in each image separately, then compute a per-pixel difference to iden-\ntify the changed buildings over time. We describe all temporal post-processing strategies in the\nAppendix. For the specialist models, we use a strong self-supervised EO model adapted to fMoW\n(SatMAE (Cong et al., 2022)) and strong models trained on each individual dataset for the change\ndetection tasks (a modified UNet (Ronneberger et al., 2015) from Gupta et al. (2019), FC-Siam-Diff\n(Daudt et al., 2018) from Shen et al. (2021), and a modified UNet from Verma et al. (2021)).\nWe use task-specific evaluation metrics. For previously existing tasks, we follow the metrics re-\nported in the original works. For new tasks, we use accuracy. See Appendix for a full description of\nmetrics, and see Figure 4 for qualitative examples of TEOChat responses on the temporal tasks."}, {"title": "5.2 MODEL ABLATIONS", "content": "Temporal task vs. joint single-temporal training. We investigate the effect of joint single image\nand temporal task training by training a model on only the temporal tasks which we call TEOChat-T.\nTEOChat-T underperforms TEOChat on all but two tasks, matching TEOChat on fMoW Sentinel\nand slightly outperforming it on S2Looking QA (Table 1). This suggests that joint training on single\nimage tasks also largely benefits temporal reasoning capabilities.\nWe test various design decisions of TEOChat by comparing their performance on\nthe canonical temporal tasks in TEOChatlas, namely fMoW RGB, xBD damage classification,\nS2Looking change detection, and QFabric RQA with 2 and 5 images. Specifically, we test the\nimpact of using a LLaVA initialization instead of Video-LLaVA, using an image encoder initial-\nization from strong remote sensing pre-trained weights (SkyScript (Wang et al., 2024b)) instead of\nCLIP, fine-tuning versus freezing the projector layer, and including image references in the prompt.\nWe note that we couple the LLM and projector initialization as they were pretrained together. Fur-\nthermore, to fairly compare to a new image encoder initialization, we also test the impact of using\na fine-tuned projector with the alternate initialization to allow it to adapt to the new image encoder\noutputs. Finally, we measure the performance improvement from training on TEOChatlas (without\nusing image references) and the effect of training for longer (7k iters [1 epoch] vs 14k [2 epochs])."}, {"title": "5.3 ZERO-SHOT PERFORMANCE ON CHANGE DETECTION TASKS", "content": "We assess TEOChat's performance on two new temporal remote sensing datasets, namely change de-\ntection using ABCD (Fujita et al., 2017) and change visual question answering using CDVQA (Yuan\net al., 2022). TEOChat demonstrates impressive generalization to thse new datasets, outperforming\nVideo-LLaVA (+35.6 on ABCD, +17.4 on CDVQA) and approaching specialist performance."}, {"title": "5.4 COMPARISON TO PROPRIETARY FOUNDATION MODELS", "content": "We compare TEOChat to GPT-40 and Gemini 1.5 Pro with three demonstrating examples on xBD\ndamage classification and S2Looking change detection (Table 4). TEOChat performs better than\nboth proprietary foundation models with in-context learning on xBD damage classification (+11.7\nF1 compared to GPT-40, +12.8 F1 compared to Gemini 1.5 Pro) and S2Looking change detection\n(+14.2 F1 compared to GPT-40, +17.1 F1 compared to Gemini 1.5 Pro). We could not evaluate the\nmodels on fMoW or QFabric (which are larger both in size and sequence length) due to cost."}, {"title": "5.5 SINGLE IMAGE PERFORMANCE", "content": "We measure TEOChat's single EO image capabilities by evaluating it on (1) scene classification\ntasks (AID (Xia et al., 2017) and UCMerced (Yang & Newsam, 2010)) and (2) visual question\nanswering tasks (LRBEN and HRBEN (Lobry et al., 2020)). We note that evaluation on AID,\nUCMerced, and HRBEN is zero-shot, as they are not in TEOChat's training set.\nTEOChat achieves better performance than both models on all but one single image task (Table 5). It\noutperforms GeoChat on both LULC classification tasks, notably by a large margin on AID (+8.9),"}, {"title": "6 CONCLUSION", "content": "In this work, we propose TEOChat, the first vision and language assistant that can engage in con-\nversation about temporal EO imagery. We introduce a novel dataset called TEOChatlas to train\nTEOChat on a diverse set of spatial and temporal instruction-following tasks. Our experiments sug-\ngest that TEOChat exhibits strong performance, outperforming previous VLMs substantially, and\neven competing with or outperforming specialist models. TEOChat also demonstrates impressive\nzero-shot generalization to two change detection datasets and outperforms strong proprietary models\nwhich can handle sequences of images.\nThere are several interesting directions for future work. First, it would be interesting to explore\nother architectures to process the visual data across time. The LLM is responsible for performing\nthe temporal information aggregation in TEOChat, but it may be helpful to use a dedicated part of\nthe architecture for this. Second, it would be useful to enable the model to process multi-spectral\nbands which is common in EO data. Third, exploring methods to improve object localization is\nworthwhile, including regressing the coordinates instead of representing them with natural language\ntokens (Zhang et al., 2023a), using oriented bounding boxes to allow for tighter localization, or\nincreasing memory efficiency of the architecture to enable inputting higher resolution images."}, {"title": "J BROADER IMPACTS", "content": "Automated methods for processing temporal EO data have the potential to lead to significant, pos-\nitive societal impacts, from supporting disaster relief to monitoring deforestation. We show that\nTEOChat is capable of temporal reasoning, achieving strong performance on several real world\ntasks like building damage detection. We hope our work can inspire more work on developing vi-\nsion and language assistants for important temporal EO tasks and lead to helpful new technologies\nthat can benefit society.\nWe highlight a few potential negative societal impacts of TEOChat. First, TEOChat can automat-\nically extract information from Earth observation imagery, which may lead to privacy concerns,\nespecially as the imagery achieves higher and higher spatial resolution. Second, TEOChat, like\nother large multimodal models, does not always produce accurate outputs, sometimes hallucinat-\ning information or objects in the images, which has the potential to mislead users if misused. It is\nalso possible but unlikely that fine-tuning on TEOChatlas may affect safeguards employed in Llama\n2. Ongoing work on mitigating hallucinations in LLMs (Tonmoy et al., 2024) and LMMs (Zhong\net al., 2024), as well as work addressing safety of LLMs (Huang et al., 2024), both have the potential\nto translate to LMMs for Earth observation, but while these approaches continue to mature, users\nshould check outputs for important applications."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "1. Jeremy Andrew Irvin: Developed the initial research idea, formulated instruction-\nfollowing tasks, prepared datasets, implemented training experiments and evaluation code,\ndesigned and conducted training experiments, drafted the paper.\n2. Emily Ruoyu Liu, Joyce Chuyi Chen, Ines Dormoy: Prepared datasets, helped design ar-\nchitecture and formulate instruction-following tasks, implemented evaluation code, drafted\nand edited the paper.\n3. Jinyoung Kim: Prepared data, edited the paper.\n4. Samar Khanna: Helped design architecture, designed and conducted training experi-\nments, edited the paper.\n5. Zhuo Zheng: Supervised the project, helped design architecture and formulate instruction-\nfollowing tasks, edited the paper.\n6. Stefano Ermon: Co-developed the research idea, supervised the project, edited the paper."}]}