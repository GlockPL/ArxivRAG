{"title": "MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation", "authors": ["Jialin Luo", "Yuanzhi Wang", "Ziqi Gu", "Yide Qiu", "Shuaizhen Yao", "Fuyun Wang", "Chunyan Xu", "Wenhua Zhang", "Dan Wang", "Zhen Cui"], "abstract": "Recently, the diffusion-based generative paradigm has achieved impressive general image generation capabilities with text prompts due to its accurate distribution modeling and stable training process. However, generating diverse remote sensing (RS) images that are tremendously different from general images in terms of scale and perspective remains a formidable challenge due to the lack of a comprehensive remote sensing image generation dataset with various modalities, ground sample distances (GSD), and scenes. In this paper, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) dataset and benchmark for text-to-image generation in diverse remote sensing scenarios. Specifically, we first collect nine publicly available RS datasets and conduct standardization for all samples. To bridge RS images to textual semantic information, we utilize a large-scale pretrained vision-language model to automatically output text prompts and perform hand-crafted rectification, resulting in information-rich text-image pairs (including multi-modal images). In particular, we design some methods to obtain the images with different GSD and various environments (e.g., low-light, foggy) in a single sample. With extensive manual screening and refining annotations, we ultimately obtain a MMM-RS dataset that comprises approximately 2.1 million text-image pairs. Extensive experimental results verify that our proposed MMM-RS dataset allows off-the-shelf diffusion models to generate diverse RS images across various modalities, scenes, weather conditions, and GSD. The dataset is available at https://github.com/ljl5261/MMM-RS.", "sections": [{"title": "1 Introduction", "content": "Remote sensing (RS) image, as a domain-specific image, plays an important role in the applications of sustainable development for human society, such as disaster response, environmental monitoring, crop yield estimation, and urban planning [29] [1] [38] [36]. Over the last decade, RS-based deep learning models have demonstrated substantial success in various computer vision tasks such as scene classification [8], object detection [41], semantic segmentation [18], and change detection [15], which can facilitate the above applications of sustainable development. Nevertheless, these models may suffer from limited performance due to the lack of large-scale high-quality dataset, and obtaining RS images is often not easy and expensive (i.e., need to launch a RS satellite)."}, {"title": "2 Background", "content": "Remote sensing (RS) imaging data are widely used in various computer vision tasks such as scene classification [37, 3, 45, 16, 30], object detection [40, 35, 12], segmentation [28, 14, 26, 30], change detection [2, 27, 7], and RS image caption [22, 17, 4, 10, 43]. For scene classification, the classic UC Merced Land Use Dataset [37] contains 21 scene classes, and each class has 100 images. MRSSC2.0 [16] is a multi-modal remote sensing scene classification dataset that contains 26,710 images of 7 typical scenes such as city, farmland, mountain, etc. In object detection field, the classic dataset HRSC2016 [40] consists of 1,070 images with 2,976 ship bounding boxes for ship detection in RS scenarios. For segmentation task, GID [28] contains 150 large-size (7200 \u00d7 6800) RS image with fine-grained pixel-level annotations. WHU-OPT-SAR [14] is a multi-modal segmentation dataset containing three diverse modalities, i.e., RGB, SAR, and NIR. For change detection task, the LEVIR-CD [2], Hi-UCD [27], and CDD [7] are used to train a model predicting the changes in the same region. In the RS image caption domain, the classic datasets, such as UCM-Captions [22], RSICD [17], NWPU-Captions [4], contain images with simple text descriptions to conduct image-to-text transferring. Despite the great success, there is no publicly available dataset for RS text-to-image generation task. In this work, we aim to propose a Multi-modal, Multi-GSD, Multi-scene RS dataset and benchmark for text-to-image generation in diverse RS scenarios."}, {"title": "3 MMM-RS Dataset", "content": "This section provides basic statistics of the MMM-RS dataset. The MMM-RS dataset is derived from 9 publicly available RS datasets: MRSSC2.0 [16], Inria [19], NaSC-TG2 [45], GID [28], WHU-OPT-SAR [14], HRSC2016 [40], TGRS-HRRSD [42], fMoW [5], and SEN1-2 [25]. With standardized processing, MMM-RS finally contains 2,103,273 text-image pairs, and the percentage of different datasets is presented in Fig. 2 (a)."}, {"title": "3.1 Dataset Statistics", "content": "Statistics for Different Modalities. The MMM-RS dataset contains three modalities: RGB image, Synthetic Aperture Radar (SAR) image, and Near Infrared (NIR) image. Note that the three modalities are aligned. Fig. 2 (b) shows the number of different modalities, we can observe that the number of RGB modality is 1,806,889 which dominates the entire dataset. In contrast, the number of SAR modality and NIR modality are much smaller, with the number of 289,384 and 7000, respectively. This is because the three modal-aligned samples are very difficult to obtain requiring multiple simultaneous satellites to perform computational imaging of the same region [14].\nStatistics for Different Ground Sample Distance (GSD) Levels. Fig. 2 (c) shows the number of different GSD levels, in which the all samples are standardized five category: Ultra-high Precision Resolution (GSD < 0.5 m/pixel), High Precision Resolution (0.5 m/pixel < GSD < 1 m/pixel), Ordinary Precision Resolution (1 m/pixel < GSD < 5 m/pixel), Low Precision Resolution (5 m/pixel < GSD < 10 m/pixel), and Ultra-low Precision Resolution (GSD \u2265 10 m/pixel).\nVisualization of Category Distribution. Fig. 2 (d) visualizes the category distribution of MMM-RS, we can observe that the categories are mainly focused on \"Recreational Facility\" and \"Crop Field\". The reason behind this phenomenon is that more than half of the samples in the MMM-RS are from fMoW [5] that is dominated by two categories \u201cRecreational Facility\u201d and \u201cCrop Field\"."}, {"title": "3.2 Dataset Preprocessing and Standardization", "content": "The sizes of the samples in different datasets are often inconsistent, thus the first step in the dataset construction is to standardize all samples. Referring to the most popular open source text-to-image diffusion model, i.e., Stable Diffusion [23], all samples are standardized to a uniform size of 512\u00d7512. The algorithm for dataset standardization is illustrated in Algorithm 1. Concretely, for samples with the size higher than 512 \u00d7 512, we crop all non-overlapping images with size of 512 \u00d7 512 as new samples. For samples with the size lower than 512 \u00d7 512, we first calculate the minimum $L$ of the height and the width, and then crop a image with the size of $L \\times L$. Note that the sizes of the samples in the original dataset are greater than or equal to 256 \u00d7 256. To preserve more high-frequency detail while upsampling the images, an ESRGAN [31] super-resolution model with the scale factor \u00d72 is introduced (denoted as ESRGAN(\u00b7)) to super-resolve the cropped image. Finally, we use Bicubic interpolation to resize the super-resolved image and output the standardized image with the size of 512 x 512. In addition, we need to update the GSD of sample according to the change in image size."}, {"title": "3.3 Information-rich Text Prompt Generation", "content": "We now elaborate on how to generate an information-rich text prompt for each sample. The framework is shown in Fig. 3, which consists of three components: GSD-related prompts, annotation prompts, and vision-language model prompts. For the GSD-related prompts, we output the GSD prompt of input image according to the predefined GSD level in Sec. 3.1. For the annotation prompts, we first extract the annotation contents such as satellite type, weather type, category, etc. that may (or may not, e.g., SEN1-2 [25] does not include annotations) exist in the original datasets, and then the satellite type and weather type are extracted as output. The category information is used for final manual text prompt proofreading. For the vision-language model prompts, we aim to utilize the pretrained large-scale vision-language model BLIP-2 [13] to output a simple text prompt describing input image content. Finally, we combine the outputs of the above three components and obtain"}, {"title": "3.4 Multi-scene Remote Sensing Image Synthesis", "content": "To address the problem of multi-scenes RS data scarcity, we aim to synthesize some common scene data by leveraging existing techniques. Specially, we select 10,000 samples from standardized dataset to be used for synthesizing images with three common scenes: fog scene, snow scene, and low-light scene. The overview framework of multi-scene RS image synthesis is shown in Fig. 4.\nFog scene synthesis. To synthesize fog image, we use the classic atmospheric scattering-based degradation model [6] to generate photorealistic fog images.\nLow-light scene synthesis. For synthesizing RS images in the low-light scene, we leverage the latest low-light image generation model TPSENCE [44] to synthesize realistic low-light RS images. In practice, we directly use the pretrained model provided by the authors to generate low-light RS images because it is sufficient to fit the RS scenarios.\nSnow scene synthesis. For synthesizing snow RS images, a straightforward way is to use TPSENCE model to generate target images, as TPSENCE also provides the pretrained snow image generation model. However, in practice, we observe that the pretrained model is difficult to synthesize snow RS images. To address the above issue, we first screen all RS images containing snow scene in the dataset (460 images in total) and select another 460 RS images that do not contain snow scene. Then, we utilize the CycleGAN [46] that is an unpaired image-to-image translation model and use the above selected unpaired data to train a clear-to-snow generation model based on CycleGAN. Finally, we use the well-trained model to synthesize the snow RS images."}, {"title": "3.5 Generating Different GSD Images for the Same Sample.", "content": "Existing RS datasets often contain various GSD image, e.g., the fMoW [5] contains images with GSD ranging from 0.5 m/pixel to 2 m/pixel, the GSD of the SEN1-2 [25] is 10 m/pixel, and the GSD of the MRSSC2.0 [16] is 100 m/pixel. However, there is no dataset that contains different GSD images for a single sample. In other words, these datasets cannot allow the model to generate images with different GSDs for the same scene. To address the above issues, we design a GSD sample extraction strategy to extract different GSD images for each sample. The main idea of this strategy is to crop images with different sizes (same height and width) from a large-size RS image and ensure that the cropped images of different sizes have obvious GSD changes. Then, all cropped images are standardized to the size of 512 \u00d7 512, so that the GSD of standardized images can be computed as $G_{std} = (L/512) \\times G_{ori}$, where $G_{std}$ and $G_{ori}$ denote the GSD of the standardized image and original image, respectively. $L$ denotes the height and width of the cropped image.\nIn practice, we perform the above strategy on the Inria dataset [19] to generate different GSD images because its image size is 5000 \u00d7 5000 that is enough to crop images with different sizes. As shown in Fig. 5, we show an example of generating different GSD images for the same sample. Specially, the size and GSD of the original image are 5000 \u00d7 5000 and 0.3 m/pixel, respectively. We then crop four images with four different sizes (i.e., 4096 \u00d7 4096, 2048 \u00d7 2048, 1024 \u00d7 1024, and 512 \u00d7 512) from the original image. Note that the higher resolution images completely cover the lower resolution images, which ensures that all cropped images maintain consistent scene content. Finally, with the four cropped images, we standardize them to a uniform size of 512 \u00d7 512, and the GSD is updated to 2.4 m/pixel, 1.2 m/pixel, 0.6 m/pixel, and 0.3 m/pixel, respectively. The above process could facilitate the generation of various GSD images, ensuring that the model can perceive variations between different GSDs while maintaining scene consistency."}, {"title": "4 Experiments", "content": "To validate the effectiveness of the MMM-RS dataset in the RS text-to-image generation task, we use this dataset along with the currently prominent Stable Diffusion [23] to achieve RS Text-to-Image Generation. Those experiments are a crucial part of our work."}, {"title": "4.1 Fine-tuning Stable Diffusion for RS Text-to-Image Generation", "content": "Experiment settings. Our experiments utilize the Stable Diffusion-V1.5 model [23] (called by SD1.5) as the foundational pre-trained model. To optimize its performance for our specific requirements in RS text-to-image generation, the LoRA [11] technique is adopted to update the stable diffusion model. In the generative phase, our generative model undergoes a training regimen of 200,000 iterations on our MMM-RS datasets. We use a learning rate of 0.0001 and employ the Adam optimizer to ensure effective training. Our text prompt, which is crucial for directing the image generation process, is meticulously constructed using a combination of four components: {Ground Sample Distance level}, {Type of weather}, {Simple text prompt describing image content}, and {Type of satellite} (such as: High precision resolution, snow, a satellite image shows a park in the city, Google Earth). This method allows us to explore various textual inputs and their impact on the generated images. To evaluate our generative model's performance, we utilize two widely recognized metrics: the Frechet Inception Distance [9] (FID) and the Inception Score [24] (IS). These metrics are crucial for assessing the quality and diversity of the images generated by our model, allowing us to compare them against real images in terms of their distribution and visual clarity. We conduct all experiments using the PyTorch framework on 8 NVIDIA RTX 4090 GPUs."}, {"title": "4.2 Cross-modal Generation based on ControlNet", "content": "In the above part, we conduct experiments to prove the validity of MMM-RS dataset in generating diverse RGB RS images. However, the multi-modal part remains to be further investigated. In this part, we aim to perform more interesting cross-modal generation experiments to verify the plausibility and validity of multi-modal data rather than simply fine-tuning the Stable Diffusion.\nExperiment settings. We select the ControlNet [39] as the base model of cross-modal generation, which is a neural network architecture that can improve large-scale pretrained text-to-image diffusion models with input task-specific prior conditions. Concretely, we also use the pretrained Stable Diffusion-V1.5 model [23] as the backbone of the ControlNet, and the batch size is set to 4. We use a learning rate of 0.00005 and employ the Adam optimizer to train the models. For training the cross-modal generative models between RGB modality and SAR modality, we use approximately 290,000 RGB-SAR pairs in our MMM-RS dataset to train this model with 80,000 iterations. For training the cross-modal generative models between RGB modality and NIR modality, we use only 7,000 RGB-NIR pairs to train this model with 20,000 iterations."}, {"title": "5 Conclusion", "content": "In this paper, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) dataset and benchmark for text-to-image generation in diverse RS scenarios. MMM-RS is inspired by the investigation that there is no publicly available RS dataset that contains both multi-modal RS images and information-rich text descriptions for diverse and comprehensive RS image generation. Through the collection and standardization of nine publicly available RS datasets, we created a unified dataset comprising approximately 2.1 million well-crafted text-image pairs. With extensive experiments, we demonstrated the effectiveness of our dataset in generating multi-modal, multi-GSD, and multi-scene RS images."}]}