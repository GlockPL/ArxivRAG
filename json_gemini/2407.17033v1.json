{"title": "Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference", "authors": ["Jian Xu", "Delu Zeng", "John Paisley"], "abstract": "Deep Gaussian processes (DGPs) provide a robust paradigm for Bayesian deep learning. In DGPs, a set of sparse integration locations called inducing points are selected to approximate the posterior distribution of the model. This is done to reduce computational complexity and improve model efficiency. However, inferring the posterior distribution of inducing points is not straightforward. Traditional variational inference approaches to posterior approximation often lead to significant bias. To address this issue, we propose an alternative method called Denoising Diffusion Variational Inference (DDVI) that uses a denoising diffusion stochastic differential equation (SDE) to generate posterior samples of inducing variables. We rely on score matching methods for denoising diffusion model to approximate score functions with a neural network. Furthermore, by combining classical mathematical theory of SDES with the minimization of KL divergence between the approximate and true processes, we propose a novel explicit variational lower bound for the marginal likelihood function of DGP. Through experiments on various datasets and comparisons with baseline methods, we empirically demonstrate the effectiveness of DDVI for posterior inference of inducing points for DGP models.", "sections": [{"title": "1. Introduction", "content": "Deep Gaussian Processes (DGPs) (Damianou & Lawrence, 2013) have emerged as a robust framework for Bayesian deep learning (Fortuin, 2022) that allows for flexible modeling of complex functions. DGPs extend the idea of Gaussian Processes (GPs) (Rasmussen, 2003) to multiple layers, enabling the modeling of hierarchical structures and capturing intricate dependencies within the data. A crucial aspect of DGPs is the selection of inducing variables (Titsias, 2009; Snelson & Gharahmani, 2005; Qui\u00f1onero-Candela & Rasmussen, 2005), which are sparse integration locations used to approximate the posterior distribution of the model. By leveraging these inducing points, DGPs can efficiently handle large datasets and reduce the computational burden.\nVariational inference methods (Blei et al., 2017; Zhang et al., 2018) aim to approximate the true posterior distribution with a parameterized variational distribution by minimizing their KL divergence. In the context of DGPs, traditional variational methods include mean-field Gaussian variational inference (DSVI) (Salimbeni & Deisenroth, 2017) and Implicit Posterior Variational Inference (IPVI) (Yu et al., 2019). However, both of these methods have their limitations and can introduce significant bias when learning the posterior distribution of inducing points.\nDSVI approximates the posterior distribution of inducing points with a simple Gaussian distribution. Although this approximation is analytically tractable, it often leads to substantial bias when dealing with nonlinear likelihood functions. The simplifying assumptions made in the mean-field approximation can fail to capture the complex dependencies and correlations between the inducing points, resulting in suboptimal results. On the other hand, IPVI uses a neural network to parameterize the posterior distribution of inducing points. Posterior inference is formulated as a Nash equilibrium (Awerbuch et al., 2008) similar to that of generative adversarial networks (GANs) (Goodfellow et al., 2014), requiring adversarial learning for the max-max problem. However, optimizing this objective function can be challenging, especially when dealing with non-convex neural networks, and lead to instability during training and contribute to significant bias in the posterior inference of inducing points (Jenni & Favaro, 2019).\nThese limitations of traditional variational methods for inference of DGPs inspires the exploration of alternative approaches. Motivated by the success of denoising diffusion models (Rombach et al., 2022) in deep learning, we propose a Denoising Diffusion Variational Inference (DDVI) method that utilizes the denoising diffusion SDE and incorporates principles similar to the score matching method (Song et al., 2020) in order to construct the objective function.\nBy employing the denoising diffusion SDE, we can accurately capture the complex dependencies and correlations among the inducing points. Additionally, similar to the score matching method, we can approximate the intricate score functions required for accurate posterior inference using a neural network. This combination finally allows us to explicitly derive a variational lower bound for the marginal likelihood function by KL divergence minimization, thereby addressing the bias introduced by traditional variational methods. Furthermore, DDVI incorporates numerous unique insights, including the well-developed mathematical theory of SDEs (Anderson, 1982; Haussmann & Pardoux, 1986), the bridge process trick, stochastic optimization techniques, reparameterization techniques, and gradient backpropagation. These collectively enable us to efficiently obtain posterior samples from the denoising diffusion network. As a result, our approach improves not only the computational efficiency but also ensures stable and reliable training in DGPs.\nIn summary, our contributions can be outlined as follows:\n\u2022 We propose a novel parameterization approach for the posterior distribution of inducing points in DGPs, utilizing a denoising diffusion process. This method not only guarantees model efficiency by accurately capturing the complex dependencies and correlations among the inducing points, but also facilitates optimization and training.\n\u2022 We exploit the minimization of KL divergence between the approximate and true processes to derive an explicit variational lower bound. To efficiently obtain posterior samples, we employ stochastic optimization and reparameterization techniques for gradient backpropagation within the denoising diffusion network.\n\u2022 Through extensive experiments on various datasets and comparisons with baseline methods, we empirically demonstrate the effectiveness of the DDVI method in posterior inference of DGP models."}, {"title": "2. Method", "content": ""}, {"title": "2.1. Model Review", "content": ""}, {"title": "2.1.1. GAUSSIAN PROCESS", "content": "Consider a random function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$ that maps $N$ training inputs $X = \\{X_n\\}_{n=1}^N$ to a set of noisy observed outputs $y = \\{y_n\\}_{n=1}^N$. Often, a zero mean Gaussian Process (GP) prior is assumed for the function, $f \\sim GP(0, k)$, where $k$ denotes the covariance kernel function $k: \\mathbb{R}^D \\times \\mathbb{R}^D \\rightarrow \\mathbb{R}$. Let $f \\triangleq (f(x_1),..., f(x_N))^T$ represent the latent function values at the inputs $X$. The GP prior assumption then induces a multivariate Gaussian prior over the function values, expressed as $p(f) = \\mathcal{N}(f|0, K_{XX})$, where the covariance matrix $K_{XX}$ is defined by $[K_{XX}]_{ij} = k(x_i, x_j)$. The observed outputs $y$ are then assumed to be contaminated by i.i.d. noise, modeled as $p(y|f) = \\mathcal{N}(y|f, \\sigma^2I)$, where $\\sigma^2$ is the noise variance. The GP posterior distribution of the latent output $p(f_y)$ has a closed-form solution. However, the computational cost is $O(N^3)$ and the storage requirement is $O(N^2)$, making it challenging to scale to large datasets without introducing additional techniques.\nSparse methods have been developed that introduce inducing points $Z = \\{Z_m\\}_{m=1}^M$ from the input space, along with corresponding inducing variables: $u = \\{f(z_m)\\}_{m=1}^M$. These methods reduce the computational complexity to $O(NM^2)$. In the Sparse Gaussian Processes (SGPs) framework, the inducing variables $u$ and the function values $f$ share a joint multivariate Gaussian distribution, expressed as $p(f, u) = p(f|u)p(u)$, with the conditional distribution given by\n$p(f|u) = \\mathcal{N}(f|K_{XZ}K_{ZZ}^{-1}u, K_{XX} - K_{XZ}K_{ZZ}^{-1}K_{ZX})$ (1)\nand $p(u) = \\mathcal{N}(u|0, K_{ZZ})$ is the prior over the outputs of the inducing points."}, {"title": "2.1.2. DEEP GAUSSIAN PROCESSES", "content": "A multi-layer Deep Gaussian Process (DGP) model is a hierarchical composition of Gaussian Process (GP) models constructed by stacking multiple-output Sparse GPs (SGPs) together, as described in (Damianou & Lawrence, 2013). Consider a DGP model with $L$ layers, where each layer $l = 1,..., L$ consists of $D_l$ independent random functions. The output of the $(l - 1)$th layer, denoted as $F_{l-1}$, serves as the input to the $l$th layer. Formally, the outputs of the $l$th layer are defined as $F_l = \\{f_{l,1}(F_{l-1}),..., f_{l,D_l}(F_{l-1})\\}$, where $f_{l,d} \\sim GP(0, k_l)$ for $d = 1, ..., D_l$, and $F_0 \\equiv X$. The inducing points and their corresponding inducing variables for each layer are denoted by $Z \\triangleq \\{Z_l\\}_{l=1}^L$ and $U \\triangleq \\{U_l\\}_{l=1}^L$, respectively. Here, $U_l = \\{f_{l,1}(Z_l), \\dots, f_{l, D_l}(Z_l)\\}$. Let $F \\equiv \\{F_l\\}_{l=1}^L$. The design of the DGP model leads to the following joint model density,\n$p(y, F, U) = p(y|F_L) \\prod_{l=1}^L p(F_l|F_{l-1}, U_l)p(U).$ (2)\nHere we place independent GP priors within and across layers on $U$,\n$p(U) = \\prod_{l=1}^L p(U_l) = \\prod_{l=1}^L \\prod_{d=1}^{D_l} \\mathcal{N}(U_{l,d}|0, K_{Z_lZ_l}).$ (3)\nand the condition similar to Eq. (1) is defined as follows,\n$p(F_l | F_{l-1}, U_l) = \\prod_{d=1}^{D_l} \\mathcal{N}(F_{l,d} | \\mu_{l,d}, \\Sigma_{l,d}),$ (4)\nwhere we define\n$\\mu_{l,d} = K_{F_{l-1}Z_l}K_{Z_lZ_l}^{-1}U_{l,d}$\n$\\Sigma_d = K_{F_{l-1}F_{l-1}} - K_{F_{l-1}Z_l} K_{Z_lZ_l}^{-1} K_{Z_lF_{l-1}}$\nIn the context of DGPs, traditional variational methods primarily include mean-field Gaussian variational inference (DSVI) (Salimbeni & Deisenroth, 2017) and Implicit Posterior Variational Inference (IPVI) (Yu et al., 2019). However, both of these methods have their limitations and can introduce significant bias in inferring the posterior distribution of inducing points.\nDSVI approximates the posterior by a mean-field Gaussian, $q(U_{l,1:D_l}) = \\mathcal{N}(U_{l,1:D_l} | m_{l,1:D_l}, S_{l,1:D_l})$, where $m_{l,1:D_l}$ and $S_{l,1:D_l}$, are variational parameters. However, this assumption is overly strict and may limit the effectiveness and expressiveness of the model. The likelihood $p(y|U)$ is difficult to compute because the latent functions $F_1, ..., F_{L-1}$ are all non-linear kernel functions.\nOn the other hand, IPVI utilizes a neural network $\\mathcal{G}_\\xi$ to parameterize the posterior distribution of inducing points. Posterior inference is formulated as a Nash equilibrium (Awerbuch et al., 2008) similar to that of generative adversarial networks (GANs) (Goodfellow et al., 2014), requiring adversarial learning for the max-max problem,\n$\\ell_{IPVI}(\\xi) = E_{q_{\\mathcal{G}_\\xi}(U)}[\\log p(y|U) - D_\\psi^*(U)]$\ns.t. $\\psi^* = \\arg \\max_\\psi E_{p(U)} [\\log (1 - \\sigma(D_\\psi(U)))]$\n$+ E_{q_{\\mathcal{G}_\\xi}(U)} [\\log \\sigma(D_\\psi(U))],$ (5)\nwhere $D_\\psi$ is another discriminator network. However, optimizing such an implict objective $\\ell(\\mathcal{G}_\\xi)$ can be challenging, especially when dealing with the non-convex neural network $D_\\psi$. This can lead to instability during training and contribute to significant bias in the posterior inference of inducing points (Jenni & Favaro, 2019).\nTo address this issue, we propose a novel parameterization approach for the posterior distribution of inducing variables $U$ that uses a denoising diffusion process. This method not only ensures model efficiency by accurately capturing the complex dependencies and correlations among the inducing points, but also facilitates optimization and training."}, {"title": "2.2. Denoising Diffusion Variational Inference", "content": ""}, {"title": "2.2.1. PARAMETERIZING INDUCING POINT POSTERIORS", "content": "Let $H = D \\times M \\times L$ denote the dimension of the inducing points. We aim to sample from the true posterior distribution $q(U)$ in $\\mathbb{R}^H$, $q(U) = p(U|y)$. Following a similar setup to prior works (Tzen & Raginsky, 2019; Zhang & Chen, 2021; Vargas et al., 2023), we start by sampling from a fixed distribution $\\mathcal{P}_{fix}$ and then follow a Markov process in which we consider a sequential latent variable model with a joint distribution denoted as $Q(U_0, ..., U_T)$, for step $t_s \\in \\{0, .., T - 1\\}$,\n$U_{t_s+1} \\sim T(U_{t_s+1} | U_{t_s}), \\qquad U_0 \\sim \\mathcal{P}_{fix}$ (6)\nHere $T(U_{t_s+1} | U_{t_s})$ denotes a transition probability distribution. Through this sequence model, we use the marginal distribution $Q(U_T)$ at the terminal step $T$ to approximate the true posterior distribution $q(U_T)$."}, {"title": "2.2.2. TIME-REVERSAL REPRESENTATION OF DIFFUSION SDE", "content": "In this paper, we constrain the Markov process $Q(U_0,..., U_T)$ to be a time-reversal process of the following forward noising diffusion stochastic differential equation (SDE),\n$d\\overline{U_t} = h(t, \\overline{U_t})dt + g(t)dB_t, \\qquad \\overline{U_0} \\sim q,$ (7)\nwhere $h(t, \\cdot): \\mathbb{R}^H \\rightarrow \\mathbb{R}^H$ is the drift coefficient, $g(t) \\in \\mathbb{R}$ is the diffusion coefficient, and $(B_t)_{t \\in [0,T]}$ is an $H$-dimensional Brownian motion. This diffusion induces the path measure $P$ on the time interval $[0, T]$ and the marginal density of $\\overline{U_t}$ is denoted $p_t$. Note that by definition we always have $p_0 = q$ when using an SDE to perturb this distribution. In DDPM (Ho et al., 2020; Song et al., 2020), $p_T$ is an unstructured prior distribution that contains no information of $p_0$, such as a Gaussian distribution with fixed mean and variance.\nFrom (Anderson, 1982; Haussmann & Pardoux, 1986), the time-reversal representation of Eq. (7), $\\overline{U_t} = U_{T-t}$, where equality is here in distribution, satisfies\n$dU_t = g(T-t)^2 \\nabla \\log (p_{T-t}(U_t)) dt - h(T-t, U_t) dt\n+g(T - t) dW_t, \\qquad U_0 \\sim p_T,$ (8)\nwhere $(W_t)_{t \\in [0,1]}$ is another $H$-dimensional Brownian motion. By definition, this time-reversal starts from\n$U_0 \\sim p_T \\approx \\mathcal{P}_{fix}$\nand is such that $U_T \\sim q$. Since the distribution of $U_T$ is consistent with the true posterior $q$, we can parameterize the transition probability $T(U_{t_s+1} | U_{t_s})$ in the Euler discretized form of Eq. (8)."}, {"title": "2.2.3. SCORE MATCHING TECHNIQUE", "content": "This suggests that if we could approximately simulate the diffusion of Eq. (8), then we could obtain approximate samples from the target $q$. However, putting this idea in practice requires being able to approximate the intractable scores $\\nabla \\log (p_t(\\cdot))$ for $t \\in [0,T]$. To achieve this, DDPM (Ho et al., 2020; Song et al., 2020) rely on score matching techniques. Specially, to approximate $P$ consider a path measure $\\mathbb{P}^\\sharp$ whose time-reversal is induced by\n$d\\widehat{U_t} = g(T-t)^2s_\\phi(T - t, U)dt - h(T - t, U)dt\n+ g(T - t) dW_t, \\qquad \\widehat{U_0} \\sim \\mathcal{P}_{fix},$ (9)\nso that the backward process $\\widehat{U_t} \\sim \\mathbb{Q}^\\sharp$, where $\\mathcal{P}_{fix}$ represents a fixed distribution. To obtain $s_\\phi(t, \\cdot) \\approx \\nabla \\log (p_t(\\cdot))$, we parameterize $s_\\phi(t, \\cdot)$ by a neural network whose parameters are obtained by minimizing $KL(\\mathbb{P}||\\mathbb{P}^\\sharp)$. From the chain rule for the KL divergence (L\u00e9onard, 2013) we have,\n$KL(\\mathbb{P}||\\mathbb{P}^\\sharp) = KL(\\mathbb{P}_T||\\mathcal{P}_{fix}) + KL(\\mathbb{P}(\\cdot|U_T)||\\mathbb{Q}^\\sharp(\\cdot|\\widehat{U_T}))$ (10)\nwhere by the well-known Girsanov Theorem (Oksendal, 2013) and the martingale property of It\u00f4 integrals the second term on the RHS is\n$KL(\\mathbb{P}(\\cdot|U_T)||\\mathbb{Q}^\\sharp(\\cdot|\\widehat{U_T})) = \\\n\\frac{1}{2} \\int_0^T E_{\\mathbb{P}} [g(t)^2||\\nabla \\log (p_t(\\overline{U_t})) - s_\\phi(t, \\overline{U_t})||^2] dt$ (11)\nFrom the denoising score matching derivation (Vincent, 2011), this can also be written as\n$\\frac{1}{2} \\int_0^T E [g(t)^2||\\nabla \\log (p_t(o)) - s_\\phi(t, o)||^2] dt$\nplus a constant term, where the expectation is over the joint distribution $p_0(o)p_t(\\cdot|o)$.\nAs the main loss function in DDPM, diffusion-based generative modeling approaches typically rely on Eq. (10), which involves sampling from $p_0$, the original data such as images, and then backpropagating to estimate the parameters of the neural network $s_\\phi$. However, unlike traditional score matching techniques, this loss function is not applicable to our model since our $p_0$ is the posterior probability $q$ and we only have access to the joint likelihood, and cannot sample from it. To address this issue, we propose an alternative approach by minimizing $KL(\\mathbb{Q}^\\sharp||\\mathbb{P})$. Analogous to Eq. (10), considering that we can only obtain samples from $\\mathbb{Q}$, we have,\n$KL(\\mathbb{Q}^\\sharp||\\mathbb{P}) = KL(\\mathbb{Q}^\\sharp||\\mathbb{Q})$ (12)\n$= KL(\\mathcal{P}_{fix}||p_T) + KL(\\mathbb{Q}^\\sharp(\\cdot|U)||\\mathbb{Q}(\\cdot|\\overline{U_0}))$\nwhere\n$KL(\\mathbb{Q}^\\sharp(\\cdot|U)||\\mathbb{Q}(\\cdot|\\overline{U_0})) = \\frac{1}{2} \\int_0^T E_{\\mathbb{Q}} [g(T - t)^2||\\gamma(t)||^2] dt$\nwhere the expectation is over $\\mathbb{Q}^\\sharp$ and we have defined\n$\\gamma(t) \\triangleq \\nabla \\log (p_{T-t}(\\overline{U})) - s_\\phi(T - t, \\overline{U})$ (13)\nHowever, the current challenge we face is that, although we can obtain samples from $\\mathbb{Q}^\\sharp$ by simulating the SDE (9), dealing with the nonlinear drift function of SDE (9) makes it difficult to obtain $\\nabla \\log (p_{T-t}(\\overline{U}))$ in Eq. (13)."}, {"title": "2.2.4. BRIDGE PROCESS TRICK", "content": "Therefore, we propose an alternative approach by constructing a bridge process $\\mathbb{P}^{Bri}$ to assist in measuring $KL(\\mathbb{Q}^\\sharp||\\mathbb{P})$. First, we observe that\n$KL(\\mathbb{P}||\\mathbb{P}^\\sharp) = E_{\\mathbb{P}} \\log \\frac{d\\mathbb{P}}{d\\mathbb{P}^\\sharp}$ (14)\n$= E_{\\mathbb{P}} \\log \\frac{d\\mathbb{P}}{d\\mathbb{P}^{Bri}} + E_{\\mathbb{P}} \\log \\frac{d\\mathbb{P}^{Bri}}{d\\mathbb{P}^\\sharp}$\nwhere we represent the stochastic process KL with the Radon-Nikodym derivative. Given the specific form in Eq. (14), we define the bridge process $\\mathbb{P}^{Bri}$ to follow the diffusion formula as in Eq. (7), but initialized at $p^{Bri}(\\widehat{U_0}^{Bri}) = \\mathcal{P}_{fix}$ instead of $q$, which aligns with the distribution of $U_0$ in Eq. (9),\n$d\\widehat{U_t}^{Bri} = h(t, \\widehat{U_t}^{Bri})dt + g(t)dB_t, \\qquad \\widehat{U_0}^{Bri} \\sim \\mathcal{P}_{fix}.$ (15)\nWe typically assume $h(\\cdot, t)$ is affine, $h(x,t) = -\\lambda(t) x$ and $\\mathcal{P}_{fix} = \\mathcal{N}(0, \\sigma^2I)$. Then the transition kernel $p_t(\\widehat{U_t}^{Bri}|\\widehat{U_0}^{Bri})$ is always a Gaussian distribution $\\mathcal{N}(\\ell_t, \\Sigma_t)$, where the mean $\\ell_t$ and variance $\\Sigma_t$ are often known in closed-forms (S\u00e4rkk\u00e4 & Solin, 2019) by\n$d\\ell_t = -\\lambda(t)\\ell_t, \\qquad \\ell_0 = 0$\n$\\frac{d\\Sigma_t}{dt} = -2\\lambda(t)\\Sigma_t + g(t)^2I, \\qquad \\Sigma_0 = \\sigma^2I$ (16)\nBy the calculations of ordinary differential equations (Hale & Lunel, 2013), we obtain the following general solution to Eq. (16),\n$\\ell_t = \\ell_0 e^{-\\int_0^t \\lambda(s)ds},$\n$\\Sigma_t = (\\int_0^t g(r)^2 e^{\\int_0^r \\lambda(s)ds}dr I + \\Sigma_0) e^{-\\int_0^t \\lambda(s)ds}$ (17)\nAccording to Eq. (17) we can derive from the Gaussian linear transformation principle that for any $t$, the distribution $p^{Bri}$ of $\\widehat{U_t}^{Bri}$ is a zero-mean Gaussian distribution,\n$p^{Bri}(\\widehat{U_t}^{Bri}) = \\int p_t(\\widehat{U_t}^{Bri}| \\widehat{U_0}^{Bri})p^{Bri}(\\widehat{U_0}^{Bri})d\\widehat{U_0}^{Bri}$ (18)\n$= \\mathcal{N}(0, \\kappa_tI)$\nwhere we have defined the variance\n$\\kappa_t = (\\int_0^t g(r)^2 e^{\\int_0^r \\lambda(s)ds} dr + \\sigma^2) e^{-\\int_0^t \\lambda(s)ds}.$\nWe can write the SDE equation for the reverse process $\\mathbb{Q}^{Bri}$ of $\\mathbb{P}^{Bri}$ as\n$d\\widehat{U_t}^{Bri} = g(T-t)^2\\nabla \\log (p_t^{Bri}(\\widehat{U_t}^{Bri})) dt$\n$- h(T - t, \\widehat{U_t}^{Bri})dt\n+ g(T-t) dW_t, \\qquad \\widehat{U_0}^{Bri} \\sim p^{Bri}$ (19)\nAccording to Eq. (18), we can obtain an analytical expression for the derivative of the log-likelihood function with respect to $\\widehat{U_t}^{Bri}$,\n$\\nabla \\log (p_t^{Bri}(\\widehat{U_t}^{Bri})) = - \\frac{\\widehat{U_t}^{Bri}}{\\kappa_{T-t}}$ (20)\nNext we calculate the value of Eq. (14). For the first term, according to the chain rule for KL and Girsanov Theorem (Oksendal, 2013), incorporating Eqs. (9, 19, 20), we have\n$E_{\\mathbb{P}} \\log \\frac{d\\mathbb{P}}{d\\mathbb{P}^{Bri}} = KL(\\mathbb{Q}^\\sharp||\\mathbb{P}^{Bri}) = KL(\\mathbb{Q}^\\sharp||\\mathbb{Q}^{Bri})$\nwhich can be broken down into the sum of two terms,\n$KL(\\mathcal{P}_{fix}||p_T) + KL(\\mathbb{Q}^\\sharp(\\cdot|U)||\\mathbb{Q}(\\cdot|\\widehat{U}^{Bri}))$ (21)\nwhere\n$KL(\\mathbb{Q}^\\sharp(\\cdot|U)||\\mathbb{Q}(\\cdot|\\widehat{U}^{Bri})) = \\\n\\frac{1}{2} \\int_0^T E_{\\mathbb{Q}} g(T - t)^2 || \\frac{\\widehat{U}^{Bri}}{\\kappa_{T-t}} + s_\\phi(T-t, \\widehat{U}^{Bri}) ||^2 dt$\nAt this point, we can simulate the SDE (9) to compute the first term in Eq. (14). The integral term can be computed using either ODE solvers (Chen et al., 2018) or by employing Riemann summation methods. For the second term, $E_{\\mathbb{P}} \\log \\frac{d\\mathbb{P}^{Bri}}{d\\mathbb{P}^\\sharp}$, we can see from Eq. (7) and Eq. (15) that $\\mathbb{P}$ and $\\mathbb{P}^{Bri}$ have the same dynamic system $\\tau$, except for different initial values. Therefore, we have\n$E_{\\mathbb{P}} \\log \\frac{d\\mathbb{P}^{Bri}}{d\\mathbb{P}^\\sharp} = E_{\\mathbb{P}} \\log \\frac{p^{Bri}(\\tau|\\cdot) p^{Bri}(\\cdot)}{\\mathbb{P}^\\sharp(\\tau) p(\\cdot)}$\n$= E \\log \\frac{p^{Bri}(\\cdot)}{p(\\cdot)}$\n$= E \\log \\frac{\\mathcal{P}_{fix}}{q}$\n$= E \\log \\frac{\\mathcal{P}_{fix}}{p(y)p(U)}$ (22)\n$= E \\log \\frac{\\mathcal{P}_{fix}}{p(y)} + \\log p(y)$"}, {"title": "2.2.5. A NEW EVIDENCE LOWER BOUND", "content": "Let $\\ell_1(\\phi) = E_{\\mathbb{P}} \\log \\frac{d\\mathbb{P}}{d\\mathbb{P}^{Bri}}$. Combining Eqs. (2, 14, 21, 22), we obtain a new variational lower bound $\\ell(\\phi)$ for the marginal likelihood $\\log p(y)$ of our method,\n$\\log p(y) = KL(\\mathbb{P}||\\mathbb{P}^\\sharp) - \\ell_1(\\phi) - E_{\\mathbb{Q}} \\log \\frac{\\mathcal{P}_{fix}}{p(y)p(U)}$\n$= KL(\\mathbb{P}||\\mathbb{P}^\\sharp) - \\ell_1(\\phi) - E_{\\mathbb{Q}} \\log \\frac{\\mathcal{P}_{fix}}{p(y)}$\n$+ E_{\\mathbb{Q}} \\log p(U) + E_{\\mathbb{Q}, F_1,...,F_L} \\log p(y|F_L)$\n$\\geq E_{\\mathbb{Q}} \\log p(U) + E_{\\mathbb{Q}, F_1,...,F_L} \\log p(y|F_L)$\n$- \\ell_1(\\phi) - E_{\\mathbb{Q}} \\log \\mathcal{P}_{fix}$\n$= \\ell(\\phi)$ (23)\nIn our derivation, $p(\\cdot)$ represents the prior function of $U$. By introducing a new variational lower bound for $\\log p(y)$, our proposed model, compared to the initial mean-field variational inference model (DSVI) where $q$ is approximated by a Gaussian distribution, approximates the posterior distribution through a denoising diffusion process. The flexibility of the denoising neural network $s_\\phi$ intuitively suggests that our model has an advantage in approximating the posterior distribution. On the other hand, compared to IPVI, DDVI provides an explicit evidence lower bound (ELBO), which means it is easier to train and allows for efficient backpropagation."}, {"title": "2.3. Reparameterization Trick and SGD", "content": "For ease of sampling, we consider a reparameterization version of Eq. (23) based on the approaximate transition probability $T_\\phi(U_{t_s+1} | U_{t_s})$ given by\n$T_\\phi(U_{t_s+1}) = U_{t_s} - h(U_{t_s},T - t_s) +\n+ g(T - t_s)^2s_\\phi(T - t_s, U_{t_s}) + g(T - t_s)\\epsilon_{t_s}$ (24)\nwhere $\\epsilon_{t_s} \\sim \\mathcal{N}(0, I)$. Given that $U_{t_s+1} = T_\\phi(U_{t_s})$, we have a representation of $U_{t_s}$ by a stochastic flow,\n$U_{t_s+1} = T_\\phi(U_{t_s}) = T_\\phi \\circ T_\\phi \\circ \\cdots T_\\phi(U_0).$ (25)\nMoreover, for DGP models, we also have a reparameterization version (Salimbeni & Deisenroth, 2017) of the conditional distribution in Eq. (4) of the form\n$F_{l,d} = K_{F_{l-1}Z_l}K_{Z_lZ_l}^{-1}U_{l,d}$\n$+ \\sqrt{K_{F_{l-1}F_{l-1}} - K_{F_{l-1}Z_l}K_{Z_lZ_l}^{-1}K_{Z_lF_{l-1}}} \\epsilon_{l,d}$ (26)\nwhere $\\epsilon_{l,d} \\in \\mathbb{R}^N$ are standard Gaussian random variables. In order to accelerate training and sampling in our inference scheme, we propose a scalable variational bound that is tractable in the large data regime based on stochastic variational inference (Kingma & Welling, 2013; Hoffman & Blei, 2015; Salimbeni & Deisenroth, 2017; Naesseth et al., 2020) and stochastic gradient descent (Welling & Teh, 2011; Chen et al., 2014; Zou et al., 2019; Alexos et al., 2022). Specifically, instead of computing the full log likelihood, we use a stochastic variant to subsample datasets into a mini-batches $\\mathcal{D}_I$ with $|X_I| = B$, where $I \\subset \\{1, 2, .., N\\}$ is the index of a mini-batch. We present the resulting stochastic inference for our Denoising Diffusion Variational Inference algorithm for DGP models in Algorithm 1."}, {"title": "2.4. Predictive Distribution", "content": "To obtain the final layer density for making predictions, we first sample from the optimized generator and transform the input locations x to the test locations $x^\\ast$ using Eq. (2). We subsequently compute the function values at the test locations, which are represented as $F_l$. Finally, we use the equation below to estimate the density of the final layer, which enables us to make predictions for the test data\n$q(F_l)=\\int \\prod_{d,l} p(F_{l,d}|F_{l-1}, U_{l,d}) \\mathbb{Q}_q(U_{l,d}) dF_{l-1}dU_{l,d}$\nwhere $\\mathbb{Q}_q$ represents the output of the denoising diffusion process at time $T$ and the first term of the integral $p(F_{l,d}|F_{l-1}, U_{l,d})$ is conditional Gaussian. We leverage this to draw samples from $q(F_l)$ and further perform the sampling according to the problem considered."}, {"title": "3."}]}