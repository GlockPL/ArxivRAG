[{"title": "The Pitfalls of Memorization: When Memorization Hurts Generalization", "authors": ["Reza Bayat", "Mohammad Pezeshki", "Elvis Dohmatob", "David Lopez-Paz", "Pascal Vincent"], "abstract": "Neural networks often learn simple explanations that fit the majority of the data while memorizing exceptions that deviate from these explanations. This behavior leads to poor generalization when the learned explanations rely on spurious correlations. In this work, we formalize the interplay between memorization and generalization, showing that spurious correlations would particularly lead to poor generalization when are combined with memorization. Memorization can reduce training loss to zero, leaving no incentive to learn robust, generalizable patterns. To address this, we propose memorization-aware training (MAT), which uses held-out predictions as a signal of memorization to shift a model's logits. MAT encourages learning robust patterns invariant across distributions, improving generalization under distribution shifts.", "sections": [{"title": "1 Introduction", "content": "Watching the stars at night gives the illusion that they orbit around the Earth, leading to the longstanding belief that our planet is the center of the universe a view supported by great philosophers such as Aristotle and Plato. This Earth-centric model could explain the trajectories of nearly all celestial bodies. However, there were exceptions; notably, five celestial bodies, including Mars and Venus, occasionally appeared to move backward in their trajectories (Kuhn, 1992). To account for these anomalies, Egyptian astronomer Claudius Ptolemy introduced \u201cepicycles\u201d\u2014orbits within orbits\u2014a complex yet effective system for predicting these movements. Over 1400 years later, Nicolaus Copernicus proposed an alternative model that placed the Sun at the center. This Sun-centric view not only simplified the model but also naturally explained the previously perplexing backward motions.\nDrawing a parallel to modern machine learning, we observe a similar phenomenon in neural networks. Just as the Earth-centric model provided a incomplete explanation that required epicycles to account for exceptions, neural networks can learn simple explanations that work for the majority of their training data (Geirhos et al., 2020; Shah et al., 2020; Dherin et al., 2022). These models might then treat minority examples-those that do not conform to the learned explanation\u2014as exceptions (Zhang et al., 2021). This becomes particularly problematic if the learned explanation is spurious, meaning it does not hold in general or is not representative of the true data distribution (Idrissi et al., 2022; Sagawa et al., 2020; Pezeshki et al., 2021; Puli et al., 2023).\nEmpirical Risk Minimization (ERM), the standard learning algorithm for neural networks, can exacerbate this issue. ERM enables neural networks to quickly capture spurious correlations and, with sufficient capacity, memorize the remaining examples rather than learning the true patterns that explain the entire dataset. This has real-world implications; for example, neural networks designed to detect COVID-19 from x-ray images have been found to rely on spurious correlations, such as whether a patient is standing or lying down (Roberts et al., 2021). This could be dangerously misleading, as a model that appears to excel in most cases may have actually captured a spurious correlation. Memorization of the remaining minority examples can fully mask a neural network's failure to grasp the true patterns in the data, giving a false sense of reliability. Again, the Earth-centric model of the universe was able to explain all celestial trajectories with complex epicycles but ultimately failed to reveal the true nature of our solar system."}, {"title": "2 The Interplay between Memorization and Spurious Correlations in ERM", "content": "Problem Setup and Preliminaries. We consider a standard supervised learning setup for a K-class classification problem. The data consists of input-label pairs {(xi, Yi)}_1, where x is the input vector and yi\u017c \u2208 {1, ..., K} is the class label. Let a\u017c denote any attribute or combination of attributes within x\u2081 that may or may not be relevant for predicting the target yi. The objective is to train a model \u00ee\u00ee(y | x; w) parameterized by w. Given an input xi, let f(xi; w) \u2208RK represent the output logits of the model, then:\n$p(y | x_i; w) = softmax(f(x_i; w)).$\nUnder the i.i.d. assumption that p(y, x) is invariant between training and test sets, empirical risk minimization (ERM) seeks to minimize the following loss over the training dataset:\n$L^{ERM} = \\frac{1}{n} \\sum_{i=1}^{n} l(p(y | x_i; w),y_i) + \\frac{\\lambda}{2} ||w||^2,$\nwhere l(., .) is the cross-entropy loss, and \uc2be||w||\u00b2 is the weight-decay regularization."}, {"title": "2.1 Memorization Can Exacerbate Spurious Correlations", "content": "Spurious correlations violate the i.i.d. assumption, and when combined with memorization, can hurt general-ization. We now study such scenario. Adapting the frameworks introduced in Sagawa et al. (2020) and Puli et al. (2023), we look into the interplay between memorization and spurious correlations in an interpretable setup.\nSetup 2.1 (Spurious correlations and memorization). Consider a binary classification problem with labels y \u2208 {\u22121,+1} and an unknown spurious attribute a \u2208 {\u22121,+1}. Each input x \u2208 Rd+2 is given by x = (xy, xa, \u20ac), where xy \u2208 R is a core feature dependent only on y. xa \u2208 R is a spurious feature dependent only on a, and \u2208 \u2208 Rd are example-specific features Gaussian noise, uncorrelated with both y and a. The scalar y \u2208 R modulates the rate at which the model learns to rely on the spurious feature xa, effectively acting as a scaling factor that increases the feature's learning rate relative to the core feature xy. The attribute a is considered spurious; it is assumed to be correlated with the labels y at training but has no correlation with y at test time, potentially leading to poor generalization if the model relies on xa. Specifically, the data generation process is defined as:\n$x=\\begin{cases} X_y \\sim \\mathcal{N}(y, \\sigma_y^2) \\\\ X_a \\sim \\mathcal{N}(a, \\sigma_a^2) \\in \\mathbb{R}^{d+2} \\text{ where, } a = \\begin{cases} y & w.p. \\rho \\\\ -y & w.p. 1 - \\rho \\end{cases} \\text{ and } \\rho = \\begin{cases} \\rho^{tr} \\text{ (train)} \\\\ 0.5 \\text{ (test)} \\end{cases} \\\\ \\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2 I) \\end{cases}$\nTo better understand this setup, one can think of a classification task between cows and camels. In this example, \u00e6 represents the pixel data, y \u2208 {cow, camel} are the class labels, and a \u2208 {grass, sand} are the background labels. Here, xy represents the pixels associated with the animal itself (either cow or camel), xa represents the pixels associated with the background (grass or sand), and \u0454 represents irrelevant pixels that varies from one example to another. The key assumption is that the joint distribution of class labels and attribute labels differs between training and test datasets, i.e., ptr (a, y) \u2260 pte (a, y). For example, in the training set, most cows (camels) might appear on grass (sand), while in the test set, cows (camels) appear equally on each background.\nIllustrative Scenarios. We first empirically study a configuration of the above setup where ptr = 0.9 makes the a spuriously correlated with y. We set y = 5 making the spurious feature easier for the model to learn. In contrast, the core feature xy is fully correlated with y, but due to a smaller norm, it is learned more slowly. Here we consider two cases:"}, {"title": "3 Memorization-Aware Training (MAT)", "content": "As exemplified in the previous section, spurious correlations between labels y and certain attributes a can lead to poor generalization, particularly when models memorize specific training examples rather than learning robust patterns. To mitigate this issue, we propose memorization-aware training (MAT), which leverages calibrated held-out probabilities to shift the logits, prioritizing learning of examples with worse generalization performance."}, {"title": "3.1 Shifting the Logits with Calibrated Held-Out Probabilities", "content": "MAT modifies the empirical risk minimization (ERM) objective by introducing a per-example logit shift based on calibrated probabilities. The training loss is redefined as:\n$L^{MAT} = \\frac{1}{n} \\sum_{i=1}^{n} l(softmax(f(x_i; w) + log p^{ho}( \\cdot | x_i)), y_i),$\nwhere f(xi; w) are the logits for input xi, and pho(. | x\u2081) are the calibrated held-out probabilities, defined as:\n$p^{ho}(y | x) := \\sum_{y^{ho}} p(y | y^{ho}) p(y^{ho} | x),$\nwhere p(yho | x) is derived from Eq. (4), and p(y | yho) is the calibration matrix, given by:\n$p(y | y^{ho}) = \\frac{p(y, y^{ho})}{\\sum_{y'} p(y', y^{ho})},$ where $p(y, y^{ho}) = \\frac{1}{n} \\sum_{{i:y_i=y}} p(y^{ho} | x_i).$\nInterpretation. The calibration matrix p(y | yho) represents the empirical confusion matrix, capturing the observed relationship between true labels y and held-out probabilities. Combining this with the auxiliary model's held-out probabilities p(yho | x), the calibrated probabilities pho(y | x) adjust the training focus, such that correct held-out predictions lower the loss for an example, while incorrect predictions increase it. This ensures MAT prioritizes minority or hard-to-classify examples with poor generalization. See Appendix B for a discussion on the effect of loss reweighting and logit shifting.\nEstimating p(yho | xi). The final challenge in implementing MAT is that we require an auxiliary model to provide held-out probabilities for every single example. This auxiliary model can be derived using Cross-Risk Minimization (XRM) (Pezeshki et al., 2023).\nXRM, originally proposed as an environment discovery method, trains two networks on random halves of the training data, encouraging each to learn a biased classifier. It uses cross-mistakes (errors made by one model on the other's data) to annotate training and validation examples. Here, MAT does not require environment annotations but uses held-out logits, fho (x), from a pretrained XRM model. For model selection using the validation set, either ground-truth or inferred environment annotations from XRM can be used.\nThe pseudo-code in Algorithm 1 summarizes the MAT algorithm, detailing the steps to compute p(yho | x) and incorporate calibrated probabilities into training. Note that MAT introduces a single hyper-parameter T, the softmax temperature in Eq. (4)."}, {"title": "4 Experiments", "content": "We first validate the effectiveness of MAT in improving generalization under subpopulation shift. We then provide a detailed analysis of the memorization behaviors of models trained with ERM and MAT."}, {"title": "4.1 Experiments on Subpopulation Shift", "content": "We evaluate our approach on four datasets under subpopulation shift, as detailed in Appendix A. In all experiments, we assume that training environment annotations are not available. For the validation set and for the purpose of model selection, we consider two settings: (1) group annotations are available in the validation set for model selection, and (2) no annotations are available even in the validation set.\nFor evaluation, we report two key metrics on the test set: (1) average test accuracy and (2) worst-group test accuracy, the latter being computed using ground-truth annotations."}, {"title": "4.2 Analysis of Memorization Scores", "content": "To understand the extent of memorization in models trained with ERM, we analyze the distribution of memorization scores across subpopulations. We focus on the Waterbirds dataset, which includes two main classes-Waterbird and Landbird -each divided into majority and minority subpopulations based on their background (e.g., Waterbird on water vs. Waterbird on land).\nThe memorization score is derived from the influence function, which measures the effect of each training sample on a model's prediction. Formally, the influence of a training sample i on a target sample j under a training algorithm A is defined as:\n$infl(A, D, i, j) := p^{(A)}_{D}(y_j|x_j) \u2013 p^{(A)}_{D \\setminus (x_i,y_i)}(y_j | x_j)$,\nwhere D is the training dataset, D\u00ac(x,y) denotes the dataset with the sample (xi, yi) removed. The memorization score is a specific case of this function where the target sample (xj, yj) is the same as the training sample. It measures the difference between a model's performance on a training sample when that sample is included in the training set (held-in) versus when it is excluded (held-out).\nCalculating self-influence scores using a naive leave-one-out approach is computationally expensive. However, recent methods, such as TRAK (Park et al., 2023), provide an efficient alternative. TRAK approximates the data attribution matrix, and the diagonal of this matrix directly gives the self-influence scores (see Appendix A.2 for more details)."}, {"title": "5 Memorization: The Good, the Bad, and the Ugly", "content": "In this work, we showed that the combination of memorization and spurious correlations, could be key reason for poor generalization. Neural networks can exploit spurious features and memorize exceptions to achieve zero training loss, thereby avoiding learning more generalizable patterns. However, an interesting and somewhat controversial question arises: Is memorization always bad?\nTo explore this, we look into a simple regression task to understand different types of memorization and their effects on generalization. We argue that the impact of memorization on generalization can vary depending on the nature of the data and the model's learning dynamics, and we categorize these types of memorization into three distinct forms. The task is defined as follows,\nSetup 5.1. Let xy \u2208 R be a scalar feature that determines the true target, y* = f(xy). Let D = {(xi, Yi)}=1 be a dataset consisting of input-target pairs (x,y). Define the input vector as x = concat(xy, \u2208) \u2208 Rd+1, where \u03b5 ~ \u039d(0, \u03c32\u0399) \u2208 Rd represents input example-specific features concatenated with the true feature xy. The target is defined as y = y* + \u03be, where \u03be ~ N(0,\u03c3\u03b5) represents additive target noise.\nIn this context, xy can be interpreted as the core feature (e.g., the object in an object classification task), e as irrelevant random example-specific features, and \u0121 as labeling noise or error. Now, consider training linear regression models \u0177 = g(x) on this dataset. Fixing \u03c3\u03b5, we train three models under three different input example-specific features levels: \u03c3\u03b5 \u2208 {0,10\u22124,10-3}. The results, summarized in Figure 3, showcases three types of memorization:\nThe Good: when memorization benefits generalization. At an intermediate level of example-specific features, \u03c3\u03b5 = 10-4, the model effectively captures the true underlying function, f(xy). However, due to the label noise, the model cannot achieve a zero training loss solely by learning f(xy). As a result, it begins to memorize the residual noise in the training data by using the example-specific features \u20ac. This is evidenced by sharp spikes at each training point, where the model, g(x), precisely predicts the noisy label if given the exact same input as during training. Nevertheless, for a neighboring test example with no example-specific features, the model's predictions align well with f(xy), demonstrating good generalization.\nThis phenomenon is often referred to as \u201cbenign overfitting", "benign": "ecause the model's memorization by relying on example-specific features does not compromise the underlying structure of the true signal. Instead, the model retains a close approximation to the true function on test data, even though it memorizes specific noise in the training data. This has been shown to occur particularly in over-parameterized neural networks (Belkin et al., 2019b; Nakkiran et al., 2021).\nThe Bad: when memorization prevents generalization. At a higher level of example-specific features, \u03c3\u03b5 = 10-3, the model increasingly rely on thesefeatures e rather than fully learning the true underlying function f(xy). In this case, memorization is more tempting for the model because the example-specific features dominates the input, making it difficult to recover the true signal. As a result, the model g(x) might achieve zero training loss by only partially learning f(xy) and instead relying heavily on the example-specific features in the inputs to fit the remaining variance in the training data.\nThis is an instance of bad memorization as it hinders the learning of generalizable patterns, the case we studied in this work. This phenomenon is referred to as \"malign overfitting\" in Wald et al. (2022), where a model fits the training data but in a way that compromises its ability to generalize, especially in situations where robustness, fairness, or invariance are critical.\nIt is important to note that both good and bad memorization stem from the same learning dynamics. ERM, and the SGD that drives it, do not differentiate between the types of correlations or features they are learning. Whether a features contributes to generalization or memorization is only revealed when the model is evaluated on held-out data. If the features learned are generalizable, the model will perform well on new data; if they are not, the model will struggle, showing its reliance on memorized, non-generalizable patterns.\nThe Ugly: Catastrophic overfitting Finally, consider the case where there is no example-specific features, \u03c3\u03b5 = 0.0. In this case, the model may initially capture the true function f(xy), but due to the presence of label noise, it cannot achieve zero training loss by learning only f(xy). Unlike the previous cases, the absence of example-specific features means the model has no additional features to leverage in explaining the residual error. As a result, the model is forced to learn a highly non-linear and complex function of the input x = Xy to fit the noisy labels.\nIn this situation, memorization is ugly: The model may achieve perfect predictions on the training data, but this comes at the cost of catastrophic overfitting where the model overfits so severely that it not only memorizes every detail of the training data, including noise, but also loses its ability to generalize to new data (Mallinar et al., 2022). Early stopping generally can prevent this type of severe overfitting."}, {"title": "6 Related Work", "content": "Detecting Spurious Correlations. Early methods for detecting spurious correlations rely on human annota-tions (Kim et al., 2019; Sagawa et al., 2019; Li and Vasconcelos, 2019), which are costly and susceptible to bias. Without explicit annotations, detecting spurious correlations requires assumptions. A common assumption is that spurious correlations are learned more quickly or are simpler to learn than core features (Geirhos et al., 2020; Arjovsky et al., 2019; Sagawa et al., 2020). Based on this, methods like Just Train Twice (JTT) (Liu et al., 2021), Environment Inference for Invariant Learning (EIIL) (Creager et al., 2021), Too-Good-To-Be-True Prior (Dagaev et al., 2023), and Correct-n-Contrast (CnC) (Zhang et al., 2022) train models with limited capacity to identify \"hard\" (minority) examples. Other methods such as Learning from Failure (LfF) (Nam et al., 2020) and Logit Correction (LC) (Liu et al., 2022) use generalized cross-entropy to bias classifiers toward spurious features. Closely related to this work is Cross-Risk Minimization (XRM) Pezeshki et al. (2023), where uses the held-out mistakes as a signal for the spurious correlations.\nMitigating Spurious Correlations. Reweighting, resampling, and retraining techniques are widely used to enhance minority group performance by adjusting weights or sampling rates (Idrissi et al., 2022; Nagarajan et al., 2020; Ren et al., 2018). Methods like Deep Feature Reweighting (DFR) (Kirichenko et al., 2022) and Selective Last-Layer Finetuning (SELF) (LaBonte et al., 2024) retrain the last layer on balanced or selectively sampled data. Automatic Feature Reweighting (AFR) (Qiu et al., 2023) extends these methods by automatically upweighting poorly predicted examples without needing explicit group labels. GroupDRO (Sagawa et al., 2019) minimizes worst-case group loss, while approaches like LfF and JTT increase loss weights for likely minority examples."}, {"title": "7 Conclusion", "content": "In this work, we show that while spurious correlations are inherently problematic, they become particularly harmful when paired with memorization. This combination drives the training loss to zero too early, stopping learning before the model can capture more meaningful patterns. To address this, we propose Memorization-Aware Training (MAT), which leverages the negative effects of memorization to mitigate the influence of spurious correlations. Notably, we highlight that memorization is not always detrimental; its impact varies with the nature of the data. While MAT mitigates the negative effects of memorization in the presence of spurious correlations, there are cases where memorization can benefit generalization or even be essential (Feldman and Zhang, 2020). Future work could focus on distinguishing these scenarios and exploring the nuanced role of memorization in large language models (LLMs). Recent work (Carlini et al., 2022; Schwarzschild et al., 2024; Antoniades et al., 2024) have highlighted the importance of defining and understanding memorization in LLMs, as it can inform how these models balance between storing training data and learning generalizable patterns."}, {"title": "A Experimental Details", "content": "A.1 Experiments on Subpopulation Shift\nFor the results presented in table 1, we follow the experimental settings in Pezeshki et al. (2023). The hyperparameter search involves testing 16 random hyperparameter combinations sampled from the search space described in table 3, using a single random seed. We select the hyperparameter combination and the early-stopping iteration that achieve the highest validation worst-group accuracy, either with ground truth group annotations or pseudo annotations, depending on the method, or the worst-class accuracy if groups are not available. Then, to calculate the results with mean and standard deviation, we repeat the best-chosen hyperparameter experiment 10 times with different random seeds. Finally, to ensure a fair comparison between different methods, we always report the test worst-group accuracy based on the ground-truth (human annotated) group annotations provided by each dataset.\nWe tested our method on 4 standard datasets: two image datasets, Waterbirds (Sagawa et al., 2019) and CelebA (Liu et al., 2015), and two natural language datasets, MultiNLI (Williams et al., 2017) and CivilComments (Borkan et al., 2019). The configuration of each dataset is provided below. For CelebA, predictors map pixel intensities into a binary \"blonde/not-blonde\" label. No individual face characteristics, landmarks, keypoints, facial mapping, metadata, or any other information was used to train our CelebA predictors. We use a pre-trained ResNet-50 (He et al., 2016) for image datasets. For text datasets, we use a pre-trained BERT (Devlin et al., 2018). We initialized the weights of the linear layer added on top of the pre-trained model with zero. All image datasets have the same pre-processing scheme, which involves resizing and center-cropping to 224 \u00d7 224 pixels without any data augmentation. We use SGD with momentum of 0.9 for the Waterbirds dataset, and we employ AdamW (Loshchilov and Hutter, 2017) with default values of \u03b2\u2081 = 0.9 and B2 = 0.999 for the other datasets.\nDataset. The detailed statistics for all datasets are provided in Table 2, together with the descriptions of each task below.\n\u2022 Waterbirds (Sagawa et al., 2019): The Waterbirds dataset is a combination of the Caltech-UCSD Birds 200 dataset (Wah et al., 2011) and the Places dataset (Zhou et al., 2017). It consists of images where two types of birds (Waterbirds and Landbirds) are placed on either water or land backgrounds. The objective is to classify the type of bird as either \"Waterbird\" or \"Landbird\", with the background (water or land) introducing a spurious correlation.\n\u2022 CelebA (Liu et al., 2015): The CelebA dataset is a binary classification task where the objective is to classify hair as either \u201cBlond\u201d or \u201cNot-Blond\u201d, with gender considered a spurious correlation.\n\u2022 MultiNLI (Williams et al., 2017): The MultiNLI dataset is a natural language inference task in which the objective is to determine whether the second sentence in a given pair is \"entailed by\", \"neutral with\", or \"contradicts\" the first sentence. The spurious correlation is the presence of negation words.\n\u2022 CivilComments (Borkan et al., 2019): The CivilComments dataset is a natural language inference task in which the objective is to classify whether a sentence is \"Toxic\" or \"Non-Toxic\"."}, {"title": "B Empirical Insights into the Effects of Reweighting and Logit-Shifting", "content": "In this section, we empirically illustrate how gradient descent dynamics in logistic regression are influenced by different weighting and shifting schemes. ERM with uniform weights converges to the max-margin solution, aligning with the theoretical expectation (Soudry et al., 2018). Reweighting examples changes the optimization path but ultimately converges to the max-margin fixed point. In contrast, introducing a shift that scales with the logits changes the optimization landscape, leading to a distinct fixed point and a different classifier. These results highlight how both reweighting and logit-scaling shifts impact the intermediate learning dynamics, but only shifts fundamentally alter the convergence behavior. Below we provide details on the dataset, model, and experimental configurations, with results shown in Figure 5.\nDataset. The dataset consists of two-class synthetic data generated using Gaussian mixtures with overlapping clusters. We generate n = 100 samples in R2 with each class drawn from distinct Gaussian distributions. The features are standardized to zero mean and unit variance using a standard scaler, and the labels are assigned as y \u2208 {\u22121,1}.\nModel and Optimization. The logistic regression model is parameterized as:\n$f(x; w,b) = Xw + b,$\nwhere X \u2208 Rnx2 is the input data, w \u2208 R\u00b2 is the weight vector, and b \u2208 R is the bias term. The gradient descent update is computed for the cross-entropy loss:\n$L = \\frac{1}{n} \\sum_{i=1}^{n} w_i log(\\frac{1}{1 + exp(-y_i f (x_i; w, b))}),$\nwhere wi are per-example weights. We compare three configurations:\n\u2022 ERM: Uniform weights are applied to all examples, i.e., w\u2081 = 1 for all i.\n\u2022 Reweight: We reduce the weights of examples for half of the examples of one class (e.g., y = 1) by setting w\u2081 = 0.1 for these examples and w\u2081 = 1 for the rest. This simulates scenarios where certain groups are more important during training, e.g. are minority.\n\u2022 Shift: A margin-dependent shift is introduced by modifying the logits as:\n$f(x; w,b) = Xw + b + \\delta_i \\cdot ||w||,$\nwhere \u03b4\u2081 = 2 for the same half of class y = 1 and d\u2081 = 0 for the other. The scaling with the norm of the weight vector effectively adjusts the margin on each example."}, {"title": "C Further Discussion on Experimental Results", "content": "Subpopulation shift methods operate under three setups regarding access to group annotations, each progressively increasing the task's difficulty.\nThe least restrictive scenario assumes full access to group annotations for both training and validation sets. This allows methods such as GroupDRO to leverage loss re-weighting with group-aware model selection and early stopping. While effective, this setup requires strong assumptions about the availability of group annotations. As noted in the original GroupDRO paper (Sagawa et al., 2020), achieving consistent performance in this setup often necessitates \"stronger-than-typical L2 regularization or early stopping\". Without such measures, as shown in Sagawa et al. (2020) (Figure 2), GroupDRO's behavior can revert to ERM-like performance due to the implicit bias of gradient descent toward the max-margin solution.\nA slightly more restrictive scenario assumes access to group annotations only for the validation set, which are used for model selection and early stopping. However, most algorithms, including GroupDRO, still rely on explicit group annotations during training, which must often be inferred using methods such as XRM. In contrast, MAT does not depend on training group annotations, making it more robust in cases where such information is unavailable.\nThe most restrictive and realistic scenario assumes no access to group annotations at any stage, either for training or validation. In this context, methods like MAT, which use per-example logits from held-out predictions, become particularly interesting. MAT operates without explicit group-specific information,"}, {"title": "D Additional Influence-Score Experiments", "content": "In this section, we extend our experiments on influence-score analysis (refer to 4.2) across three methods: ERM, GroupDRO, and MAT. Figure 6 demonstrates that MAT effectively reduces self-influence scores relative to the other methods. Additionally, GroupDRO also shows some level reduction in the self-influence score of groups, specifically for the minority groups. This further indicates that memorization is limiting generalization, and mitigating it contributes to improved generalization in subpopulation shift settings."}, {"title": "E Multiple spurious features setup", "content": "To test the capability of MAT in handling multiple spurious features, we have adapted our original setup in Section 2.1 and added a second spurious feature. Specifically, the data generation process is defined as:\n$x=\\begin{cases} X_y \\sim \\mathcal{N}(y, \\sigma_y^2) \\\\ X_{a_1} \\sim \\mathcal{N}(a, \\sigma_a^2) \\\\ X_{a_2} \\sim \\mathcal{N}(a, \\sigma_a^2) \\\\ \\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2 I) \\end{cases} \\in \\mathbb{R}^{d+2} \\text{ where, } a = \\begin{cases} y & w.p. \\rho \\\\ -y & w.p. 1 - \\rho \\end{cases} \\text{ and } \\rho = \\begin{cases} \\rho^{tr} \\text{ (train)} \\\\ 0.5 \\text{ (test)} \\end{cases}$\nIllustrative Scenarios. Similarly, we chose ptr = 0.9 (i.e., the correlation of spurious features with labels) and y = 5 (i.e., the strength of the spurious features) for both spurious features. In this new setup, with one main feature and two spurious features, there are four groups within each class, resulting in a total of eight groups. For simplicity in our visualization, we identified the group with the highest number of samples as the majority group and the group with the fewest samples as the minority group (i.e., the worst-performing group). Note that the number of samples in this minority group is even smaller than before, as the addition of the second spurious feature splits the previous minority group, introduced by the first spurious feature, into two even smaller groups.\nConsistent with the single spurious feature setup (Section 2.1), Figure 7 shows that ERM, with memorization capacity, struggles to learn generalizable features and achieves only random-guess performance for the minority samples. In contrast, MAT effectively handles this scenario and nearly achieves perfect test accuracy for all groups."}, {"title": "F Proofs", "content": "Lemma F.1. Let p1(y = j | x) =\n$\\frac{e^{\\phi_j(x)}}{\\sum e^{\\phi_i(x)}}$ be a softmax over the logits $j(x), and define p2(y = j | x) such that p2(y = j | x) x w(j, x) \u00b7 P\u2081(y = j | x) for some weighting function w(j, x). Then:\n$p_2(y = j | x) = \\frac{e^{\\phi_j (x)+log w (j,x)}}{\\sum_{k=1} e^{\\phi_i (x)+log w(i,x)}}$.\nProof. Starting with the definition:\n$p_2(y = j | x) \\propto w(j, x) \\cdot p_1(y = j | x)$.\nSubstituting the expression for p\u2081 (y = j | x):\n$p_2(y = j | x) \\propto w(j, x) \\frac{e^{\\phi_j(x)}}{\\sum_{i=1} e^{\\phi_i(x)}}$"}, {"title": "G Proof of Theorem 2.2", "content": "Setting the derivative of the objective function LERM (Equation (2)) with respect to w to zero gives the normal equation according to appendix G.4\n$\\frac{\\partial L^{ERM"}]}, {"\u03b1": "with \u03c0\u2081 := 1{y\u2081>0"}, {}]