{"title": "The Pitfalls of Memorization: When Memorization Hurts Generalization", "authors": ["Reza Bayat", "Mohammad Pezeshki", "Elvis Dohmatob", "David Lopez-Paz", "Pascal Vincent"], "abstract": "Neural networks often learn simple explanations that fit the majority of the data while memorizing exceptions that deviate from these explanations. This behavior leads to poor generalization when the learned explanations rely on spurious correlations. In this work, we formalize the interplay between memorization and generalization, showing that spurious correlations would particularly lead to poor generalization when are combined with memorization. Memorization can reduce training loss to zero, leaving no incentive to learn robust, generalizable patterns. To address this, we propose memorization-aware training (MAT), which uses held-out predictions as a signal of memorization to shift a model's logits. MAT encourages learning robust patterns invariant across distributions, improving generalization under distribution shifts.", "sections": [{"title": "1 Introduction", "content": "Watching the stars at night gives the illusion that they orbit around the Earth, leading to the longstanding belief that our planet is the center of the universe a view supported by great philosophers such as Aristotle and Plato. This Earth-centric model could explain the trajectories of nearly all celestial bodies. However, there were exceptions; notably, five celestial bodies, including Mars and Venus, occasionally appeared to move backward in their trajectories (Kuhn, 1992). To account for these anomalies, Egyptian astronomer Claudius Ptolemy introduced \u201cepicycles\u201d\u2014orbits within orbits\u2014a complex yet effective system for predicting these movements. Over 1400 years later, Nicolaus Copernicus proposed an alternative model that placed the Sun at the center. This Sun-centric view not only simplified the model but also naturally explained the previously perplexing backward motions.\nDrawing a parallel to modern machine learning, we observe a similar phenomenon in neural networks. Just as the Earth-centric model provided a incomplete explanation that required epicycles to account for exceptions, neural networks can learn simple explanations that work for the majority of their training data (Geirhos et al., 2020; Shah et al., 2020; Dherin et al., 2022). These models might then treat minority examples-those that do not conform to the learned explanation\u2014as exceptions (Zhang et al., 2021). This becomes particularly problematic if the learned explanation is spurious, meaning it does not hold in general or is not representative of the true data distribution (Idrissi et al., 2022; Sagawa et al., 2020; Pezeshki et al., 2021; Puli et al., 2023).\nEmpirical Risk Minimization (ERM), the standard learning algorithm for neural networks, can exacerbate this issue. ERM enables neural networks to quickly capture spurious correlations and, with sufficient capacity, memorize the remaining examples rather than learning the true patterns that explain the entire dataset. This has real-world implications; for example, neural networks designed to detect COVID-19 from x-ray images have been found to rely on spurious correlations, such as whether a patient is standing or lying down (Roberts et al., 2021). This could be dangerously misleading, as a model that appears to excel in most cases may have actually captured a spurious correlation. Memorization of the remaining minority examples can fully mask a neural network's failure to grasp the true patterns in the data, giving a false sense of reliability. Again, the Earth-centric model of the universe was able to explain all celestial trajectories with complex epicycles but ultimately failed to reveal the true nature of our solar system."}, {"title": "2 The Interplay between Memorization and Spurious Correlations in ERM", "content": "Identifying whether a model with nearly perfect accuracy on the training data has learned generalizable patterns or merely relies on a mix of spurious correlations and memorization is critical. The answer lies in the model's performance on held-out data, particularly on minority examples. Metrics such as held-out average accuracy or more fine-grained group accuracies can help us identify a better model. A question that arises is: How can one use held-out performance signals to proactively guide a model toward learning generalizable patterns?\nTraditionally, held-out performance signals are mainly used for hyperparameter tuning and model selection. However, in this work, we propose a novel approach that leverages these signals strategically to guide the learning process. Towards this goal, our paper makes the following contributions:\n\u2022 Formalizing the interplay between memorization and generalization: We study how memorization affects generalization in an interpretable setup. We show that while spurious correlations are inherently problematic, they by themselves do not always lead to poor generalization in neural networks. Instead, it is the combination of spurious correlations with memorization that leads to this problem. Our analysis shows that models trained with empirical risk minimization (ERM) tend to rely on spurious features for the majority of the data while memorizing exceptions, achieving zero training loss but failing to generalize on minority examples.\n\u2022 Introducing memorization-aware training (MAT): MAT is a novel learning algorithm that leverages the flip side of memorization by using held-out predictions to shift a model's logits during training. This shift guides the model toward learning invariant features that generalize better under distribution shifts. Unlike ERM, which relies on the i.i.d. assumption, MAT is built upon an alternative assumption that takes into account the instability of spurious correlations across different data distributions."}, {"title": "Problem Setup and Preliminaries", "content": "We consider a standard supervised learning setup for a K-class classification problem. The data consists of input-label pairs {(xi, Yi)}_1, where x is the input vector and yi\u017c \u2208 {1, ..., K} is the class label. Let a\u017c denote any attribute or combination of attributes within x\u2081 that may or may not be relevant for predicting the target yi. The objective is to train a model \u00ee\u00ee(y | x; w) parameterized by w. Given an input xi, let f(xi; w) \u2208RK represent the output logits of the model, then:\n$p(y | x_i; w) = \\text{softmax}(f(x_i; w)).$                                                                                       (1)"}, {"title": null, "content": "Under the i.i.d. assumption that p(y, x) is invariant between training and test sets, empirical risk minimization (ERM) seeks to minimize the following loss over the training dataset:\n$C_{ERM} \\frac{1}{n}\\sum_{i=1}^{n}l(p(y | x_i; W),y_i) + \\frac{\\lambda}{2}||w||^2,$\nwhere l(., .) is the cross-entropy loss, and \uc2be||w||\u00b2 is the weight-decay regularization."}, {"title": "2.1 Memorization Can Exacerbate Spurious Correlations", "content": "Spurious correlations violate the i.i.d. assumption, and when combined with memorization, can hurt generalization. We now study such scenario. Adapting the frameworks introduced in Sagawa et al. (2020) and Puli et al. (2023), we look into the interplay between memorization and spurious correlations in an interpretable setup."}, {"title": "Setup 2.1 (Spurious correlations and memorization)", "content": "Consider a binary classification problem with labels y \u2208 {\u22121,+1} and an unknown spurious attribute a \u2208 {\u22121,+1}. Each input x \u2208 Rd+2 is given by x = (xy, xa, \u20ac), where xy \u2208 R is a core feature dependent only on y. xa \u2208 R is a spurious feature dependent only on a, and \u2208 \u2208 Rd are example-specific features Gaussian noise, uncorrelated with both y and a. The scalar y \u2208 R modulates the rate at which the model learns to rely on the spurious feature xa, effectively acting as a scaling factor that increases the feature's learning rate relative to the core feature xy. The attribute a is considered spurious; it is assumed to be correlated with the labels y at training but has no correlation with y at test time, potentially leading to poor generalization if the model relies on xa. Specifically, the data generation process is defined as:\nx=\n$x=\n\\begin{cases}\nX_y \\sim N(y, \\sigma_y) \\\\\nX_{\\gamma a} \\sim N(a, \\sigma_a) \\in R^{d+2} \\text{ where, } a = \\begin{cases}\ny \\text{ w.p. } \\rho \\\\\n-y \\text{ w.p. } 1 - \\rho\\end{cases} \\text{ and } \\rho = \\begin{cases}\n\\rho_{tr} \\text{ (train)} \\\\\n0.5 \\text{ (test)}\\end{cases} \\\\\n\\epsilon \\sim N(0, \\sigma_\\epsilon I)\n\\end{cases}$"}, {"title": "Illustrative Scenarios", "content": "We first empirically study a configuration of the above setup where ptr = 0.9 makes the a spuriously correlated with y. We set y = 5 making the spurious feature easier for the model to learn. In contrast, the core feature xy is fully correlated with y, but due to a smaller norm, it is learned more slowly. Here we consider two cases:"}, {"title": "1. No example-specific features \u2192 No Memorization \u21d2 ERM generalizes well.", "content": "Figure 1-(left) presents a case where there are no input example-specific features (\u03c3\u03b5 \u2192 0). As training progresses, the model first learns xa due to its larger norm, resulting in perfect accuracy on the majority examples. Once the model achieve nearly perfect accuracy on the majority examples, it starts to learn the minority examples. At this point, the model must adjust its decision boundary to place more emphasis on the core feature Xy, ultimately achieving perfect generalization on both majority and minority examples."}, {"title": "2. With example-specific features \u2192 Spurious Features + Memorization \u21d2 ERM fails to generalize.", "content": "Figure 1-(middle) presents a similar setup to the former, but this time with input example-specific features (\u03c3\u03b5 > 0). Again, initially, the model learns to rely on the spurious feature xa. However, unlike Case 1, the example-specific features e provides the model an opportunity to memorize minority examples directly. As a result, the model achieves zero training loss by memorizing minority examples using the example-specific dimensions instead of learning to rely on the core feature xy. Consequently, the model fails to adjust its decision boundary to align with xy, and does not generalize on held-out minority examples.\nThese results illustrate that the combination of spurious correlations and memorization creates a 'loophole' for the model. When memorization happens, it leaves no more incentives for the model to learn the true, underlying patterns necessary for robust generalization."}, {"title": "Theoretical Analysis", "content": "We now provide a formal analysis to formalize our empirical observations. Complete proofs are provided in Appendix F.\nTheorem 2.2. Consider a binary classification problem under the setup described in Setup 2.1, where a linear model f(x; w) = x\u00afw is trained using ERM on a dataset Dtr. Let \u0175ERM = (\u0175y, Wa, \u0175e) \u2208 Rd+2 be the learned parameters, and \u0177(x) = argmaxy fy(x; \u0175ERM) the learned classifier.\nUnder the conditions where, \u5165 \u2192 0+, n \u2192 \u221e, d\u221an \u2192 \u221e, ptr > 0.501, we have,\n0. Perfect training accuracy: The classifier achieves perfect accuracy on all training examples:\n$p (\\hat{y}(x) = y) \\rightarrow 1, \\forall x \\in D^{tr}.$\n1. No example-specific features: If \u03c3\u03b5 \u2192 0+, the classifier converges to one that relies solely on the core feature xy. For a test point x:\n$p (\\hat{y}(x) = y) \\rightarrow 1, \\forall x \\notin D^{tr}.$\n2. With example-specific features: If \u03c3\u03b5 > 0 is bounded away from zero and d \u226b logn and \u03b3 \u00bb \u03c3\u03b5\u221ad/m, where m := ptrn is the number of majority samples in the training set. Then, for a test point x, the classifier relies pathologically on the spurious feature xa, i.e.,\n$p (\\hat{y}(x) = a) \\rightarrow 1, \\forall x \\notin D^{tr}.$"}, {"title": null, "content": "The condition d \u226b logn ensures that example-specific features from different samples are approximately orthogonal, and y\u226b \u03c3\u03b5\u221ad/m guarantees that the spurious feature xa is learned faster by gradient descent than other features.\nTheorem 2.2 suggests that in the presence of spurious correlations and example-specific features, p(y(x) = a) \u2192 1 for held-out examples. This indicates that the model confidently predicts a as the label for any held-out input x \u00a2 Dtr, leading to poor generalization. The probability that a model assigns to the held-out examples is then defined as:\n$p(y^{ho} | x) \\text{softmax} = \\frac{\\exp(f(x; W_{ERM})))}{\\tau} \\forall x \\notin D^{tr},$\nwhere the superscript 'ho' stands for held-out and \u315c > 0 is a temperature parameter that controls the sharpness of the distribution. This probability serves as the basis for the shifts introduced in the next section."}, {"title": "3 Memorization-Aware Training (MAT)", "content": "As exemplified in the previous section, spurious correlations between labels y and certain attributes a can lead to poor generalization, particularly when models memorize specific training examples rather than learning robust patterns. To mitigate this issue, we propose memorization-aware training (MAT), which leverages calibrated held-out probabilities to shift the logits, prioritizing learning of examples with worse generalization performance."}, {"title": "3.1 Shifting the Logits with Calibrated Held-Out Probabilities", "content": "MAT modifies the empirical risk minimization (ERM) objective by introducing a per-example logit shift based on calibrated probabilities. The training loss is redefined as:\n$C_{MAT}=\\frac{1}{n}\\sum_{i=1}^{n}l(\\text{softmax}(f(x_i; w) + \\text{log} p^{ho}(\\cdot | x_i)), Y_i),$\nwhere f(xi; w) are the logits for input xi, and pho(. | x\u2081) are the calibrated held-out probabilities, defined as:\n$p^{ho} (y | x) := \\sum_{y^{ho}}p(y | y^{ho})p(y^{ho} | x),$\nwhere p(yho | x) is derived from Eq. (4), and p(y | yho) is the calibration matrix, given by:\n$p(y | y^{ho}) = \\frac{p(y, y^{ho})}{\\sum_{y'} p(y', y^{ho})}, \\text{ where } p(y, y^{ho}) = \\frac{1}{n} \\sum_{i:{y=y}}p(y^{ho} | x_i).$"}, {"title": "Interpretation", "content": "The calibration matrix p(y | yho) represents the empirical confusion matrix, capturing the observed relationship between true labels y and held-out probabilities. Combining this with the auxiliary model's held-out probabilities p(yho | x), the calibrated probabilities pho(y | x) adjust the training focus, such that correct held-out predictions lower the loss for an example, while incorrect predictions increase it. This ensures MAT prioritizes minority or hard-to-classify examples with poor generalization. See Appendix B for a discussion on the effect of loss reweighting and logit shifting."}, {"title": "Estimating p(yho | xi)", "content": "The final challenge in implementing MAT is that we require an auxiliary model to provide held-out probabilities for every single example. This auxiliary model can be derived using Cross-Risk Minimization (XRM) (Pezeshki et al., 2023).\nXRM, originally proposed as an environment discovery method, trains two networks on random halves of the training data, encouraging each to learn a biased classifier. It uses cross-mistakes (errors made by one model on the other's data) to annotate training and validation examples. Here, MAT does not require environment annotations but uses held-out logits, fho (x), from a pretrained XRM model. For model selection using the validation set, either ground-truth or inferred environment annotations from XRM can be used.\nThe pseudo-code in Algorithm 1 summarizes the MAT algorithm, detailing the steps to compute p(yho | x) and incorporate calibrated probabilities into training. Note that MAT introduces a single hyper-parameter T, the softmax temperature in Eq. (4)."}, {"title": "4 Experiments", "content": "We first validate the effectiveness of MAT in improving generalization under subpopulation shift. We then provide a detailed analysis of the memorization behaviors of models trained with ERM and MAT."}, {"title": "4.1 Experiments on Subpopulation Shift", "content": "We evaluate our approach on four datasets under subpopulation shift, as detailed in Appendix A. In all experiments, we assume that training environment annotations are not available. For the validation set and for the purpose of model selection, we consider two settings: (1) group annotations are available in the validation set for model selection, and (2) no annotations are available even in the validation set.\nFor evaluation, we report two key metrics on the test set: (1) average test accuracy and (2) worst-group test accuracy, the latter being computed using ground-truth annotations.\nTable 1 compares the performance of MAT with several baseline methods, including ERM, GroupDRO (Sagawa et al., 2019), and other invariant methods like LfF (Nam et al., 2020), JTT (Liu et al., 2021), LC (Liu et al., 2022), uLA (Tsirigotis et al., 2024), AFR (Qiu et al., 2023), XRM+GroupDRO (Pezeshki et al., 2023). These methods vary in their assumptions about access to annotations, both in training and validation for model selection. For instance, ERM does not assume any training group annotations, while GroupDRO has full access to group annotations for training and validation data. Further details on the experimental setup and methods are in Appendix A."}, {"title": "4.2 Analysis of Memorization Scores", "content": "To understand the extent of memorization in models trained with ERM, we analyze the distribution of memorization scores across subpopulations. We focus on the Waterbirds dataset, which includes two main"}, {"title": null, "content": "classes - Waterbird and Landbird -each divided into majority and minority subpopulations based on their background (e.g., Waterbird on water vs. Waterbird on land).\nThe memorization score is derived from the influence function, which measures the effect of each training sample on a model's prediction. Formally, the influence of a training sample i on a target sample j under a training algorithm A is defined as:\n$infl(A, D, i, j) := P_D^{(A)}(y_j | x_j) - P_{D\\neg (x_i, y_i)}^{(A)}(y_j | x_j)$"}, {"title": null, "content": "where D is the training dataset, D\u00ac(x,y) denotes the dataset with the sample (xi, yi) removed. The memorization score is a specific case of this function where the target sample (xj, yj) is the same as the training sample. It measures the difference between a model's performance on a training sample when that sample is included in the training set (held-in) versus when it is excluded (held-out).\nCalculating self-influence scores using a naive leave-one-out approach is computationally expensive. However, recent methods, such as TRAK (Park et al., 2023), provide an efficient alternative. TRAK approximates the data attribution matrix, and the diagonal of this matrix directly gives the self-influence scores (see Appendix A.2 for more details).\nFigure 2 depicts the distribution of self-influence scores across subpopulations in the Waterbird dataset. We note that minority subpopulations (e.g., Waterbirds on land) show higher self-influence scores compared to their majority counterparts (e.g., Waterbirds on water) in a model trained with ERM. However, a model trained with MAT shows a similar distribution of self-influence scores for both the majority and minority examples, with overall lower scores compared to ERM. These results show that MAT effectively reduced memorization, while leading to improved generalization."}, {"title": "5 Memorization: The Good, the Bad, and the Ugly", "content": "In this work, we showed that the combination of memorization and spurious correlations, could be key reason for poor generalization. Neural networks can exploit spurious features and memorize exceptions to achieve zero"}, {"title": null, "content": "training loss, thereby avoiding learning more generalizable patterns. However, an interesting and somewhat controversial question arises: Is memorization always bad?\nTo explore this, we look into a simple regression task to understand different types of memorization and their effects on generalization. We argue that the impact of memorization on generalization can vary depending on the nature of the data and the model's learning dynamics, and we categorize these types of memorization into three distinct forms. The task is defined as follows,"}, {"title": "The Good: when memorization benefits generalization", "content": "At an intermediate level of example-specific features, \u03c3\u03b5 = 10-4, the model effectively captures the true underlying function, f(xy). However, due to the label noise, the model cannot achieve a zero training loss solely by learning f(xy). As a result, it begins to memorize the residual noise in the training data by using the example-specific features \u20ac. This is evidenced by sharp spikes at each training point, where the model, g(x), precisely predicts the noisy label if given the exact same input as during training. Nevertheless, for a neighboring test example with no example-specific features, the model's predictions align well with f(xy), demonstrating good generalization.\nThis phenomenon is often referred to as \u201cbenign overfitting\" where a model can perfectly fit (overfit in fact) the training data while relying on example-specific features, yet still generalize well to unseen data (Belkin et al., 2019a; Muthukumar et al., 2020; Bartlett et al., 2020). The key insight is that the overfitting in this case is \"benign\" because the model's memorization by relying on example-specific features does not compromise the"}, {"title": "The Bad: when memorization prevents generalization", "content": "At a higher level of example-specific features, \u03c3\u03b5 = 10-3, the model increasingly rely on thesefeatures e rather than fully learning the true underlying function f(xy). In this case, memorization is more tempting for the model because the example-specific features dominates the input, making it difficult to recover the true signal. As a result, the model g(x) might achieve zero training loss by only partially learning f(xy) and instead relying heavily on the example-specific features in the inputs to fit the remaining variance in the training data.\nThis is an instance of bad memorization as it hinders the learning of generalizable patterns, the case we studied in this work. This phenomenon is referred to as \"malign overfitting\" in Wald et al. (2022), where a model fits the training data but in a way that compromises its ability to generalize, especially in situations where robustness, fairness, or invariance are critical.\nIt is important to note that both good and bad memorization stem from the same learning dynamics. ERM, and the SGD that drives it, do not differentiate between the types of correlations or features they are learning. Whether a features contributes to generalization or memorization is only revealed when the model is evaluated on held-out data. If the features learned are generalizable, the model will perform well on new data; if they are not, the model will struggle, showing its reliance on memorized, non-generalizable patterns."}, {"title": "The Ugly: Catastrophic overfitting", "content": "Finally, consider the case where there is no example-specific features, \u03c3\u03b5 = 0.0. In this case, the model may initially capture the true function f(xy), but due to the presence of label noise, it cannot achieve zero training loss by learning only f(xy). Unlike the previous cases, the absence of example-specific features means the model has no additional features to leverage in explaining the residual error. As a result, the model is forced to learn a highly non-linear and complex function of the input x = Xy to fit the noisy labels.\nIn this situation, memorization is ugly: The model may achieve perfect predictions on the training data, but this comes at the cost of catastrophic overfitting where the model overfits so severely that it not only memorizes every detail of the training data, including noise, but also loses its ability to generalize to new data (Mallinar et al., 2022). Early stopping generally can prevent this type of severe overfitting."}, {"title": "6 Related Work", "content": "Detecting Spurious Correlations. Early methods for detecting spurious correlations rely on human annota-tions (Kim et al., 2019; Sagawa et al., 2019; Li and Vasconcelos, 2019), which are costly and susceptible to bias. Without explicit annotations, detecting spurious correlations requires assumptions. A common assumption is that spurious correlations are learned more quickly or are simpler to learn than core features (Geirhos et al., 2020; Arjovsky et al., 2019; Sagawa et al., 2020). Based on this, methods like Just Train Twice (JTT) (Liu et al., 2021), Environment Inference for Invariant Learning (EIIL) (Creager et al., 2021), Too-Good-To-Be-True Prior (Dagaev et al., 2023), and Correct-n-Contrast (CnC) (Zhang et al., 2022) train models with limited capacity to identify \"hard\" (minority) examples. Other methods such as Learning from Failure (LfF) (Nam et al., 2020) and Logit Correction (LC) (Liu et al., 2022) use generalized cross-entropy to bias classifiers toward spurious features. Closely related to this work is Cross-Risk Minimization (XRM) Pezeshki et al. (2023), where uses the held-out mistakes as a signal for the spurious correlations.\nMitigating Spurious Correlations. Reweighting, resampling, and retraining techniques are widely used to enhance minority group performance by adjusting weights or sampling rates (Idrissi et al., 2022; Nagarajan et al., 2020; Ren et al., 2018). Methods like Deep Feature Reweighting (DFR) (Kirichenko et al., 2022) and Selective Last-Layer Finetuning (SELF) (LaBonte et al., 2024) retrain the last layer on balanced or selectively sampled data. Automatic Feature Reweighting (AFR) (Qiu et al., 2023) extends these methods by automatically upweighting poorly predicted examples without needing explicit group labels. GroupDRO (Sagawa et al., 2019) minimizes worst-case group loss, while approaches like LfF and JTT increase loss"}, {"title": null, "content": "weights for likely minority examples. Data balancing can also be achieved through data synthesis, feature augmentation, or domain mixing (Hemmat et al., 2023; Yao et al., 2022; Han et al., 2022).\nLogit adjustment methods are another line of work for robust classification under imbalanced data. Menon et al. (2020) propose a method that corrects model predictions based on class frequencies, building on prior work in post-hoc adjustments (Collell et al., 2016; Kim and Kim, 2020; Kang et al., 2019). Other methods, such as Label-Distribution-Aware Margin (LDAM) loss (Cao et al., 2019), Balanced Softmax (Ren et al., 2020), Logit Correction (LC) (Liu et al., 2022), and Unsupervised Logit Adjustment (uLA) (Tsirigotis et al., 2024), adjust classifier margins to handle class or group imbalance effectively.\nMemorization Memorization in neural networks has received significant attention since the work of Zhang et al. (2021), which showed that these models can perfectly fit the training data, even with completely random labels. Several studies have studied the nuances of memorization across various scenarios (Zhang et al., 2019; Feldman, 2020; Feldman and Zhang, 2020; Brown et al., 2021, 2022; Anagnostidis et al., 2022; Garg et al., 2023; Attias et al., 2024). Feldman (2020); Feldman and Zhang (2020) examined how memorization contributes to improved performance on the tail of the distribution, especially on visually similar training and test examples, and highlighted the role of memorizing outliers and mislabeled examples in preserving generalization, akin to our concept of \"good memorization\" terminology (section 5).\nMemorization and Spurious Correlations. Research has shown that memorization in neural networks can significantly affect model robustness and generalization. Arpit et al. (2017); Maini et al. (2022); Stephenson et al. (2021); Maini et al. (2023); Krueger et al. (2017) explore memorization's impact on neural networks, examining aspects like loss sensitivity, curvature, and the layer where memorization occurs. Yang et al. (2022) investigate \"rare spurious correlations,\" which are akin to example-specific features that models memorize. Bombari and Mondelli (2024) provide a theoretical framework quantifying the memorization of spurious features, differentiating between model stability with respect to individual samples and alignment with spurious patterns. Finally, Yang et al. (2024) propose Residual-Memorization (ResMem), which combines neural networks with k-nearest neighbor-based regression to fit residuals, enhancing test performance across benchmarks."}, {"title": "7 Conclusion", "content": "In this work, we show that while spurious correlations are inherently problematic, they become particularly harmful when paired with memorization. This combination drives the training loss to zero too early, stopping learning before the model can capture more meaningful patterns. To address this, we propose Memorization-Aware Training (MAT), which leverages the negative effects of memorization to mitigate the influence of spurious correlations. Notably, we highlight that memorization is not always detrimental; its impact varies with the nature of the data. While MAT mitigates the negative effects of memorization in the presence of spurious correlations, there are cases where memorization can benefit generalization or even be essential (Feldman and Zhang, 2020). Future work could focus on distinguishing these scenarios and exploring the nuanced role of memorization in large language models (LLMs). Recent work (Carlini et al., 2022; Schwarzschild et al., 2024; Antoniades et al., 2024) have highlighted the importance of defining and understanding memorization in LLMs, as it can inform how these models balance between storing training data and learning generalizable patterns."}, {"title": "A Experimental Details", "content": "A.1 Experiments on Subpopulation Shift\nFor the results presented in table 1, we follow the experimental settings in Pezeshki et al. (2023). The hyperparameter search involves testing 16 random hyperparameter combinations sampled from the search space described in table 3, using a single random seed. We select the hyperparameter combination and the early-stopping iteration that achieve the highest validation worst-group accuracy, either with ground truth group annotations or pseudo annotations, depending on the method, or the worst-class accuracy if groups are not available. Then, to calculate the results with mean and standard deviation, we repeat the best-chosen hyperparameter experiment 10 times with different random seeds. Finally, to ensure a fair comparison between different methods, we always report the test worst-group accuracy based on the ground-truth (human annotated) group annotations provided by each dataset.\nWe tested our method on 4 standard datasets: two image datasets, Waterbirds (Sagawa et al., 2019) and CelebA (Liu et al., 2015), and two natural language datasets, MultiNLI (Williams et al., 2017) and CivilComments (Borkan et al., 2019). The configuration of each dataset is provided below. For CelebA, predictors map pixel intensities into a binary \"blonde/not-blonde\" label. No individual face characteristics, landmarks, keypoints, facial mapping, metadata, or any other information was used to train our CelebA predictors. We use a pre-trained ResNet-50 (He et al., 2016) for image datasets. For text datasets, we use a pre-trained BERT (Devlin et al., 2018). We initialized the weights of the linear layer added on top of the pre-trained model with zero. All image datasets have the same pre-processing scheme, which involves resizing and center-cropping to 224 \u00d7 224 pixels without any data augmentation. We use SGD with momentum of 0.9 for the Waterbirds dataset, and we employ AdamW (Loshchilov and Hutter, 2017) with default values of \u03b2\u2081 = 0.9 and B2 = 0.999 for the other datasets."}, {"title": "Dataset", "content": "The detailed statistics for all datasets are provided in Table 2, together with the descriptions of each task below.\n\u2022 Waterbirds (Sagawa et al., 2019): The Waterbirds dataset is a combination of the Caltech-UCSD Birds 200 dataset (Wah et al., 2011) and the Places dataset (Zhou et al., 2017). It consists of images where two types of birds (Waterbirds and Landbirds) are placed on either water or land backgrounds. The objective is to classify the type of bird as either \"Waterbird\" or \"Landbird\", with the background (water or land) introducing a spurious correlation.\n\u2022 CelebA (Liu et al., 2015): The CelebA dataset is a binary classification task where the objective is to classify hair as either \u201cBlond\u201d or \u201cNot-Blond\u201d, with gender considered a spurious correlation.\n\u2022 MultiNLI (Williams et al., 2017): The MultiNLI dataset is a natural language inference task in which the objective is to determine whether the second sentence in a given pair is \"entailed by\", \"neutral with\", or \"contradicts\" the first sentence. The spurious correlation is the presence of negation words.\n\u2022 CivilComments (Borkan et al., 2019): The CivilComments dataset is a natural language inference task in which the objective is to classify whether a sentence is \"Toxic\" or \"Non-Toxic\"."}, {"title": "A.2 Analysis of Memorization Scores", "content": "For the results presented in figure 2, we used the TRAK framework (Park et al., 2023) to calculate self-influence scores for individual training data points. The TRAK framework computes the influence of each training point on a target dataset, generating an influence matrix, as illustrated in figure 4. By setting the target dataset as the training"}]}