{"title": "Patherea: Cell Detection and Classification for the 2020s", "authors": ["Dejan \u0160tepec", "Maja Jer\u0161e", "Sne\u017eana \u0110oki\u0107", "Jera Jeruc", "Nina Zidar", "Danijel Sko\u010daja"], "abstract": "This paper presents a Patherea, a framework for point-based cell detection and classification that provides a complete solution for developing and evaluating state-of-the-art approaches. We introduce a large-scale dataset collected to directly replicate a clinical workflow for Ki-67 proliferation index estimation and use it to develop an efficient point-based approach that directly predicts point-based predictions, without the need for intermediate representations. The proposed approach effectively utilizes point proposal candidates with the hybrid Hungarian matching strategy and a flexible architecture that enables the usage of various backbones and (pre)training strategies. We report state-of-the-art results on existing public datasets - Lizard, BRCA-M2C, BCData, and the newly proposed Patherea dataset. We show that the performance on existing public datasets is saturated and that the newly proposed Patherea dataset represents a significantly harder challenge for the recently proposed approaches. We also demonstrate the effectiveness of recently proposed pathology foundational models that our proposed approach can natively utilize and benefit from. We also revisit the evaluation protocol that is used in the broader field of cell detection and classification and identify the erroneous calculation of performance metrics. Patherea provides a benchmarking utility that addresses the identified issues and enables a fair comparison of different approaches. The dataset and the code will be publicly released upon acceptance.", "sections": [{"title": "Introduction", "content": "The emergence of whole-slide scanners has enabled the scanning of a complete microscopic tissue slide and the creation of a single high-resolution digital file a gigapixel whole slide image (WSI). The so-called digital pathology represents recent digitalization efforts in the medical field of pathology to omit the need for traditional diagnostics to be performed under the microscope (Mukhopadhyay et al., 2018). However, with the emergence of whole slide scanners and progress in AI and computer vision, it is simultaneously undergoing another big transformation, shifting from digital to computational pathology \u2013 the analysis of digitized data using AI (Bera et al., 2019; Cui and Zhang, 2021). Various computer vision approaches have been applied for histopathology analysis and diagnostics with proven comparative performance with the pathologists (Bejnordi et al., 2017; Bulten et al., 2022).\nCancer diagnostics is traditionally performed by pathologists under the microscope, which includes quantifying the immunohistochemical expression of various proteins on specially stained tissue samples. In practice, this means counting hundreds or thousands of cells of a particular class and interest (e.g., positively, and negatively stained tumor cells) which forms a basis to derive prognostic scoring markers (Dowsett et al., 2011). Ki-67 proliferation index represents one of the most widely used prognostic markers in different cancer types (e.g., breast cancer, neuroendocrine tumors, lymphomas, sarcomas) and identifies the proportion of tumor cells in the proliferation phase (i.e., tumor growth rate). Computing such a scoring is a highly time-consuming manual work for a pathologist, however it enables classification and grading of tumors, evaluation of their malignant potential, and is the basis for determining an effective treatment. In practice, less accurate approaches are used Mikami et al. (2013); Polley et al. (2013), which leads to lower interobserver concordance and reproducibility.\nAutomating cell detection and classification represents a key step for performing diagnostics in a more efficient and reproducible manner and has been one of the first topics to be addressed in histopathology image analysis Xing and Yang (2016). Supervised-learning-based approaches require expertly labeled datasets, which are costly to obtain in sufficient quantities in comparison with natural images in the broader computer vision domain. Most approaches utilize point-based annotations, which are easier to obtain in comparison with bounding boxes or segmentation masks and are most often not needed to support the end application in terms of diagnostics. Various point-based deep-learning-based approaches have been proposed in recent years (Xie et al., 2015, 2018; Lee et al., 2021; Abousamra"}, {"title": "Related Work", "content": "Cell detection and classification in histopathology is a challenging problem due to variance in cell shape and appearance. This is further exacerbated by overlapping cells, staining artifacts, scanning artifacts, out-of-focus regions, and in general, the sheer amount of cells present in a selected field-of-view. This represents a significant challenge even for trained pathologists when performing diagnostics under the miscroscope (Mikami et al., 2013; Polley et al., 2013). While limited success can be achieved with conventional image processing approaches, that are included in some of the open-source pathology software's Bankhead et al. (2017),\nwe limit ourselves to learning-based approaches.\nThe early approaches were based on classifying small patches centered on the ground-truth cell-center locations of whether the center of the patch belongs to the foreground or background, or by learning a distance function for each pixel in a centered patch (Kainz et al., 2015; Sirinukunwattana et al., 2016). The intermediate representation in the form of different distance-based functions has later evolved by taking into account the neighboring context and a direct regression on full-sized patches (Xie et al., 2018; Huang et al., 2020). MCSpatNet (Abousamra et al., 2021) has recently introduced Ripley's K-function that represents an expected number of neighbors of a specific type, around the limited vicinity of the"}, {"title": "Methods", "content": "The proposed Patherea-P2P architecture is presented in Figure 1. The overall architecture was inspired by the crowd counting and localization Point-to-Point Network (P2PNet) (Song et al., 2021), which was further extended with a novel Hybrid Hungarian-based regression loss, classification capability and modern building blocks that enable the use of self-supervised approaches to train or use existing foundational models tailored for digital pathology. The used notation mostly follows (Song et al., 2021)."}, {"title": "P2P Network", "content": "Patherea-P2P method training input represents a (histology tissue) patch with a set of P = {pi} point-based annotations pi = (Xi, Yi, ti), i \u2208 {1,...,N},t \u2208 {1, ..., T}, which represents a cell center location (xi, Yi) and a cell type (ti) for a particular cell i - among N labeled cells and T cell types in a given patch (Figure 1, left, green dots). The trained model similarly predicts a set of points P = {p;} in a given patch pj = (xj, \u0177j, c\u2208T), j \u2208 {1, ..., M}, based on the M proposal candidates (Figure 1, top-right, red dots), with an additional confidence score ceT for each of the T possible cell types, which we threshold to produce a final set of predictions (Figure 1, bottom-right, red dots). Predicted points p; should be as close as possible to ground-truth cell locations and of the same type ti, while maximizing a confidence score cj, for the task of cell localization. Similarly, the overall number of predicted number of cells (N = |cheT > Cthresh) should be as close as possible to the actual number of cells N in a given patch, for the task of cell counting. Patherea-P2P combines and optimizes both tasks at the same time, while taking into account individual cell types.\nThe overall architecture consists of a backbone, which encodes an image patch to the embedding Fs, where s represents the downsampling stride (each positional embedding in F, represents a s \u00d7 s receptive field in the input patch). The lightweight regression head Hreg and classification head Has are attached to the backbone, which are executed in parallel. We start by a set of proposal candidate points P (Figure 1, top, red dots), which are initialized in a regular grid along the input patch. This is achieved by initializing K proposal candidates for each receptive field of size s \u00d7 s in a Hf \u00d7 Wf sized embedding F\u2084 (M = Hf * Wf * K). Regression head Hreg then predicts the offsets Akerk for each proposal k \u2208 K. Similarly, classification head Has predicts cell class for each of the K proposal candidates.\nx,y\nFor every ground truth target from P, we need to assign a proposal from a set of predicted points P, using a one-to-one matching strategy. Point proposals"}, {"title": "Hybrid Hungarian Matching", "content": "With the P2P framework presented in Section 3.1, only one proposal candidate is selected per ground truth target (one-to-one matching). Selected targets are supervised both, with the classification loss Lels (2) and regression loss Lreg (3). Unmatched proposal candidates are only supervised with the classification loss. Given that M > N, often M >> N, there is a lack of positive supervision, especially for the regression loss. Additionally, in the earlier phases of the training, different proposal candidates are often selected and partially optimized, resulting in many candidates being localized and classified \"correctly\", however, in the later stages of training, these candidates are treated as negative examples. This is depicted in Figure 1 (top-right), where multiple proposal candidates are positioned on the cell (or even within the more restrictive ground-truth radius). The one-to-one matching described in 3.1 selects only the closest one by distance and highest confidence when in practice, multiple candidates can be equally good. This mixed supervision - \"good\" candidates are being treated as background - results in suboptimal optimization and reduced performance.\nWe propose a hybrid matching scheme, that first performs one-to-many matching, followed by the standard one-to-one matching, as depicted in Figure 1. The prediction head outputs are used to perform the one-to-many matching, while being further refined in the last layer with the standard one-to-one matching. This ensures more supervision signal from one-to-many optimization, while still enabling end-to-end point-based object detection, without the need for post-processing (e.g., non-maxima suppression).\nWe implement this by relaxing the Hungarian algorithm to enable multiple candidates to match the ground truth targets. This is achieved by replicating ground truth targets in a cost matrix D by a factor of \u03b2, where \u03b2 represents the number of candidate proposals per ground truth target. This allows the Hungarian algorithm to match multiple proposal candidates per ground truth target, with \u03b2 = 1 representing the standard P2P framework presented in 3.1. This results in a new cost matrix D with dimensions (N \u00d7 \u03b2) \u00d7 M. The losses (2) and (3) are computed in the same manner, the only difference being more matched positive samples (N = \u03b2 \u00d7 N), resulting in L1vN. The combined loss is then:\nL = L1v1 + done2manyL1vN, \nwhere done2many denotes the balancing weight between the one-to-one and one-to-many matchings."}, {"title": "Foundational Backbone", "content": "The proposed Patherea-P2P method removes the need for any intermediate representations on the input side, as well as any post-processing, thus making it fully end-to-end. Additionally, we design the architecture in a simplified manner, reducing the need for complex task-specific building blocks, beyond the lightweight heads. Most of the existing work (Huang et al., 2023b,a; Pina et al., 2024) base their architecture on DETR-based frameworks. Vision, or even tasks-specific (e.g., object detection) approaches are usually more complex, but have usually achieved better performance due to the inherent inductive bias present in the architecture. We design the architecture around general-purpose architectures like ConvNext (Liu et al., 2022) and ViT (Dosovitskiy et al., 2021), which directly support (multi-modal) pre-training in a self-supervised manner. We utilized ConvNext and ViT backbones, together with Feature Pyramid Networks (FPN) (Lin et al., 2017), which enables to use of higher-resolution features, beneficial for dense prediction tasks. For ViT, we additionally utilized ViT-Adapter (Chen et al., 2023), which allows plain ViT to achieve comparable performance to vision-specific transformers.\nMasked image modelling can be used to train foundational models from scratch using MAE (He et al., 2022) for ViT or SparK (Tian et al., 2023) for ConvNext. Alternatively, existing, open-source pathology foundational models (Nechaev et al., 2024) can be used, which are most often only available for ViTs."}, {"title": "Patherea Dataset", "content": "We present, to the best of our knowledge, the largest public dataset for cell detection and classification on immunohistochemistry samples for the clinical problem of Ki-67 proliferation index estimation in neuroendocrine tumors (NETs) at different locations (small and large intestine, appendix, larynx, pharynx, lungs) and breast cancer. Both cancer types were selected due to well-standardized usage of Ki-67 as a classification/grading parameter by World Health Organization and other relevant standardization bodies and comparative studies (Polley et al., 2013;\nReid et al., 2015; Dowsett et al., 2011; Nielsen et al., 2021). There is also a significant lack of publicly available datasets with immunohistochemistry stainings, with vast majority of the public pathology datasets being the standard H&E staining. Ki-67 proliferation index is calculated as a proportion of positive tumor cells against the negative tumor cells. For example, International Ki-67 in Breast Cancer Working Group (Dowsett et al., 2011) recommends counting at least 1000 tumor"}, {"title": "Background", "content": "cells in at least three high-power (x40 objective) fields. In practice, \"eye-balling\" is often used (Reid et al., 2015), which basically means estimating the percentage of Ki-67 positive tumor cells by \"scanning\" the entire slide at intermediate power (x10 objective), without actually counting the individual cells. This results in a low inter-observer agreement (Reid et al., 2015) in comparison with manual counting. The development of AI-based approaches holds the potential to enable scoring Ki-67 at a fraction of the time needed, even in comparison with eye-balling, while reaching the inter-observer agreement of the manual counting. The focus of this work is to develop an efficient approach for cell detection and classification and compare its performance against the ground truth data from pathologists. Clinical relevance of an AI-assisted Ki-67 proliferation scoring and comparison against the pathologists will be the scope of the subsequent publications."}, {"title": "Dataset", "content": "We collected 42 samples of NETs at the Institute of Pathology, Faculty of Medicine, University of Ljubljana and 29 samples of breast cancer at the Institute of Oncology Ljubljana. All the samples were scanned at x40 objective using Hamamatsu NanoZoomer S360. Ethical approvals were obtained from the National Medical Ethics Committee of the Republic of Slovenia for NETs (No. 0120-357/2023/23), as well as breast cancer (No. 0120-147/2024-2711-3).\nIn comparison with the related work (Sirinukunwattana et al., 2016; Huang et al., 2020; Graham et al., 2021; Abousamra et al., 2021; Ryu et al., 2023), we directly used WSIs for labeling and not the selected extracted patches, such that we directly mimicked the actual clinical workflow, if it were to be digitized. We deployed the Digital Slide Archive platform (Gutman et al., 2017) in a cloud environment for the management of WSIs, as well as for the labeling. The selected slides were labeled by 4 pathologists and each of the pathologists had access to the platform and labeled the slides through a web browser. The pathologists were free to select the regions to be labeled, which were marked with a polygon, with all of the cells and their types labeled as depicted in Figure 2. The pathologists had access to the WSI and were able to freely navigate across the slide and use different magnifications to label the cells. Five different classes of cells were labeled: positive and negative tumor cells, cells that show proliferation, but are not tumor cells - others positive, others negative, as well as normal healthy cells. The selected samples represent biopsies of the cancer tissue, resulting in a low occurrence of healthy cells. As described in Section 4.1, the ratio between positive and negative tumor cells is important to derive the Ki-67 proliferation index."}, {"title": "Evaluation Protocol", "content": "As described in Section 4, we only obtain point-based annotations and subsequently define a ground truth region as a circular region with radius r centered at a cell center labeled by a pathologist. We can then match all the detected cell centroids with the corresponding pathologist annotations. We can then compute a per-class F1 score based on true-positives (TP), false-positives (FP) and false-negatives (FN), as depicted in Figure 3 - left and equation (6).\nF1cls = TP / (TP + (FP + FN))\nThe Hungarian algorithm is used by most of the established public datasets and benchmarks (Xie et al., 2018; Graham et al., 2019; Huang et al., 2023b,a) to perform the one-to-one matching between the detections and labeled ground"}, {"title": "Experiments", "content": "In this section we report results on two established public datasets for point-based cell detection and classification - Lizard (Graham et al., 2021) and BRCA-M2C (Abousamra et al., 2021) in Section 6.2. Additionally, we also report results on BCData (Huang et al., 2020) in Appendix A.3. We report results on our newly introduced Patherea dataset in Section 6.3. We compare the proposed Patherea-P2P method introduced in Section 3 against the recent DETR-based approaches ACFormer (Huang et al., 2023b) and PGT (Huang et al., 2023a), as well as against more traditional approaches with an intermediate representation(s). For the traditional approaches, we compared against MCSpatNet (Abousamra et al., 2021), which additionally utilizes spatial context information and against our implementation of the approach from (Xie et al., 2018) - named Patherea-FCRN, which was modernized with ResNet-34 backbone and added support for cell classification, mostly following (Lee et al., 2021). We used the official code for ACFormer,"}, {"title": "Implementation Details", "content": "Patherea-P2P and FCRN (Xie et al., 2018) methods were implemented in PyTorch, while the official code and parameters were used to reproduce MCSpatNet (Abousamra et al., 2021), ACFormer (Huang et al., 2023b) and PGT methods (Huang et al., 2023a) on selected datasets. Patherea-P2P proposal candidates were initialized with K = [2, 2] and a Feature Pyramid Network (FPN) (Lin et al., 2017) was used for ConvNext (Liu et al., 2022) and ViT (Dosovitskiy et al., 2021) backbones with number of features set to 256 for ConvNext-B and 768 for ViT-B. Only higher-resolution features at level 2 of the FPN were used as an input to Hreg and Hcls. Lightweight regression and classification heads were attached with two ResNet blocks (He et al., 2016) and two 3 \u00d7 3 convolutions for one-to-one and one-to-many hybrid Hungarian matching.\nWe use \u0442 = 0.05 as a weight term for the pixel distance in the Hungarian matching. In a hybrid Hungarian matching setup, \u03b2 = 2 was used for public datasets reported in Section 6.2, while \u03b2 = 6 was used for the Patherea dataset, due to the increased resolution of the dataset. The influence of parameter B is investigated in Section 6.5. A class weighting term At in Las was set to 0.5 for the background class and 10 for all the foreground classes t \u2208 T. Regression loss weight in L1v1 and L1vN was set to 2e \u2013 3, while Xone2many in a combined loss L was set to 0.5. Threshold on confidence Cthresh CET. in inference was set to 0.9 for all classes t\u2208T for Lizard(Graham et al., 2021) and BRCA-M2C (Abousamra et al.,\n2021) datasets, while 0.5 was used for Patherea dataset.\nImageNet pre-trained weights were used to initialize the backbone, except when pathology foundational model (Nechaev et al., 2024) was specifically mentioned. Both, Patherea-P2P and FCRN methods were trained for 1000 epochs on public datasets (Section 6.2), while 100 epochs were used for the Patherea dataset in Section 6.3. This is due to the much larger Patherea dataset, which makes training competing methods inefficient for larger training cycles. The batch size was set to 16 and AdamW (Loshchilov and Hutter, 2019) was used as an optimizer with a learning rate set to 1e \u2013 4, weight decay to 2e \u2013 3 and cosine annealing scheduler, with a linear warm-up in all experiments."}, {"title": "Public Datasets", "content": "Lizard: Lizard dataset was introduced in (Graham et al., 2021) and consists out of 6 different public datasets of colon cancer, where 291 images were extracted at x20 objective magnification. The Lizard dataset was primarily developed for nuclear instance segmentation and classification, but also provided cell center locations which can be utilized to develop and evaluate point-based approaches. The dataset was mostly automatically labeled, with a HoVer-Net (Graham et al., 2019) trained on existing public data and used as an initial segmentation result. The segmentation results were later refined by a pathologists and a model was re-trained. Similarly, cell class refinement was augmented with pathologists-in-the-loop. This semi-automatic approach enabled the annotation of 495,179 cells of 6 different classes (epithelial, lymphocyte, plasma, neutrophil, eosinophil, connective).\nThe dataset was split into 3 folds and we followed the evaluation protocol used in ACFormer and PGT and used fold 3 for training, fold 2 for validation and fold 1 for testing. The ground truth radius r was set to 6 pixels. We report the results in Table 3."}, {"title": "Patherea Dataset", "content": "We evaluate the proposed Patherea-P2P method on our newly proposed Patherea dataset introduced in Section 4. We split the Patherea dataset into 3 folds across different patients in a random manner and perform 3-fold cross-validation for all the approaches. The splits across the folds are released alongside the Patherea dataset. The normal cells were excluded from the analysis, as only 80 cells were labeled across all parts of the Patherea dataset. We used a fixed training schedule of 100 epochs for all the methods and report the average across the folds at 100 epochs. We report the results across different parts of the Patherea dataset - LNET in Table 5, GNET in Table 6 and Breast in Table 7.\nOverall, Patherea-P2P approach outperforms all of the competing approaches by a significant margin (e.g., 4-13%) in comparison with the second best approach across different Pathera datasets. The improvement is more significant with less abundant classes of others positive and others negative classes. Interestingly, density-estimation-based approaches MCSpatNet and FCRN outperformed recently proposed DETR-based approaches."}, {"title": "Qualitative Results", "content": "Qualitative results for the Patherea-P2P model are presented in Figure 4. First, we notice that proposal candidates are successfully filtered down with the Hungarian one-to-one matching and confidence thresholding (0.5), such that multiple detection occur rarely. This also holds true in most of the extreme cases, where cell size is significantly larger (e.g., LNET-4, GNET-3). Cells are successfully detected in dense regions (e.g., LNET-2, GNET-2), as well as when cell morphology is less apparent (e.g., GNET-5, Breast-4). There are some multiple detections present when cells are significantly overlapped and clustered (e.g., Breast-5)."}, {"title": "Ablation Studies", "content": "Hybrid Hungarian Matching: In Table 8 we evaluate the influence of the parameter 3 which is used to select the number of candidates each ground truth target is matched against for an additional one-to-many loss in Patherea-P2P, as presented in Section 3.2. We demonstrate the effectiveness of the proposed hybrid matching, with relative performance gains of up to 7% for sufficiently represented classes and up to almost 25% for less abundant classes."}, {"title": "Foundational Model", "content": "In Table 9 we report the average F1 performance on Lizard (Graham et al., 2021), BRCA-M2C (Abousamra et al., 2021) and Patherea-LNET datasets when using different model training strategies. For Lizard and BRCA-M2C we report the Flavg across different classes with a 5x re-training strategy, directly comparable with results reported in Tables 3 and 4. Similarly to Table 5, we report Flavg across the 3 folds for Patherea-LNET dataset.\nWe evaluated three different training strategies for training Patherea-P2P using the ViT backbone. i) With the Scratch strategy, we trained the Patherea-P2P model from scratch (backbone, ViT-Adapter, heads). ii) The 2nd approach, fine-tuning from ImageNet initialized ViT backbone represents the default approach used to report results in Sections 6.2 and 6.3. iii) Lastly, we utilized the recently released Hibou family of open-source foundational models for pathology (Nechaev et al., 2024). For ImageNet, we fine-tuned the whole Patherea-P2P architecture, while for Hibou, we only fine-tined ViT-Adapter (Chen et al., 2023) and heads and kept the ViT backbone frozen.\nWe notice that training from scratch significantly reduces the performance on Lizard and BRCA-M2C datasets, while the drop on the Patherea-LNET dataset is smaller. We hypothesize that this is due to significantly smaller size of Lizard and BRCA-M2C datasets in terms of the actual number of training samples. We also notice a 3-5% improvement over ImageNet when using a foundational model on Lizard and BRCA-M2C datasets. We were able to improve upon ConvNext-based Patherea-P2P results on BRCA-M2C, reported in Table 4, when using a foundational"}, {"title": "Conclusion", "content": "Cell detection and classification represents an essential tool to perform diagnostics in various pathology workflows. Such diagnostics workflows are extremely time-consuming for pathologists, which in practice results in performing them less accurately, leading to lower interobserver concordance and reproducibility. Automatisation of such workflows requires a significant amount of labeled data, which for the task of cell detection and classification is most efficiently obtained as point-based annotations. There is also a lack of approaches that can effectively utilize such point-based annotations and can be easily deployed in clinical practice. Ideally, the models can effectively utilize pre-trained foundational models to generalize across different cell detection and classification tasks with limited training data and/or enable efficient re-training or fine-tuning on newly collected data.\nIn this work, we introduced a Patherea framework that directly addresses the development of AI models in a clinical setting. First, we collected the largest fully manually labeled point-based cell detection and classification dataset for a particular clinical task of Ki-67 proliferation index estimation in Neuroendocrine tumors (NETs) and breast cancer. The annotation process was designed to mimic the ideal clinical setting, where pathologists would be manually counting the cells for the estimation of the Ki-67 index, instead of the less accurate \"eye-balling\" technique. Secondly, we introduced a new approach for an automated cell detection and classification that can directly utilize point-based annotations, without the need for intermediate representations. We proposed a more efficient approach for the target-candidate association that can effectively utilize multiple (good) proposal candidates for a particular cell. We reported state-of-the-art results on existing large-scale public datasets and demonstrated that the newly proposed Patherea dataset presents a significant challenge for existing approaches, while the proposed Patherea-P2P achieving significantly better results. We also demonstrated that the proposed architecture can be effectively used to utilize existing large-scale unlabeled data to pre-train large-scale foundational models and apply them to specific tasks with a limited amount of labeled data."}]}