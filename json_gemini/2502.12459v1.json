{"title": "Stress Testing Generalization: How Minor Modifications Undermine Large Language Model Performance", "authors": ["Guangxiang Zhao", "Saier Hu", "Xiaoqi Jian", "Jinzhu Wu", "Yuhan Wu", "Change Jia", "Lin Sun", "Xiangzheng Zhang"], "abstract": "This paper investigates the fragility of Large Language Models (LLMs) in generalizing to novel inputs, specifically focusing on minor perturbations in well-established benchmarks (e.g., slight changes in question format or distractor length). Despite high benchmark scores, LLMs exhibit significant accuracy drops and unexpected biases (e.g., preference for longer distractors) when faced with these minor but content-preserving modifications. For example, Qwen 2.5 1.5B's MMLU score rises from 60 to 89 and drops from 89 to 36 when option lengths are changed without altering the question. Even GPT40 experiences a 25-point accuracy loss when question types are changed, with a 6-point drop across all three modification categories. These analyses suggest that LLMs rely heavily on superficial cues rather than forming robust, abstract representations that generalize across formats, lexical variations, and irrelevant content shifts. This work aligns with the ACL 2025 theme track on the Generalization of NLP models, proposing a \"Generalization Stress Test\" to assess performance shifts under controlled perturbations. The study calls for reevaluating benchmarks and developing more reliable evaluation methodologies to capture LLM generalization abilities better.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved near-human performance across a variety of natural language processing (NLP) benchmarks, from elementary tests (Cobbe et al., 2021) to university-level challenges (Hendrycks et al., 2021). This success has spurred claims that LLMs are approaching human-like generalization capabilities (OpenAI, 2024; Bubeck et al., 2023; Jones and Bergen, 2024). However, it remains unclear whether their high benchmark scores reflect genuine generalization or if LLMs are simply exploiting superficial cues that fail under slight perturbations.\nWhile LLMs perform well in established benchmarks, concerns have been raised about the validity of these evaluations. Data contamination, where models unintentionally learn from benchmark data included in their training, can inflate performance estimates (Brown et al., 2020; Xu et al., 2024; Ravaut et al., 2024; Zhou et al., 2023). These issues suggest that patterns of existing benchmarks have been exposed. The existing benchmarks may not truly assess generalization.\nRecent work has focused on uncovering the actual limits of LLM generalization. One direction involves the development of dynamic evaluation methods that modify the evaluation process on the fly (Zhu et al., 2024; Yu et al., 2024). Another approach emphasizes creating more challenging or adversarial test sets that push models beyond their current capabilities, such as MMLU-Pro (Wang et al., 2024) and GSM-Plus (Li et al., 2024a). A third line of inquiry involves introducing subtle modifications to benchmark datasets to test LLM robustness, such as altering the order of multiple-choice options or changing the format of questions (Zheng et al., 2024; Li et al., 2024b; Gupta et al., 2024; Alzahrani et al., 2024). While these approaches have contributed to a better understanding of LLM performance, they either increase the complexity of the evaluation or focus on relatively limited formatting changes like option ID adjustments.\nIn this paper, we introduce a novel evaluation framework, Generalization Stress Tests, which examines LLM performance under three types of minor, content-preserving perturbations:\n\u2022 Changing question types (e.g., converting multiple-choice questions to boolean judgments).\n\u2022 Altering option length (e.g., increasing the"}, {"title": "2 Methods: Generalization Stress Tests", "content": "We conduct generalization stress tests by applying minor modifications to the original benchmark, focusing on variations in option length, scoring type, and the replacement of irrelevant nouns.\nWe investigate typical tasks for LLMs that include multiple-choice questions (MCQ) and open-ended question answering (Open-ended QA)."}, {"title": "2.1 Alter Option Length to Analyze LLMs' Length Bias", "content": "To analyze whether LLMs are generalized across option length or whether LLMs are biased toward long options in MCQ. We first make all options"}, {"title": "2.2 Change Problem Type to Fairly Analyze LLMs' Scoring Bias", "content": "Previous work found LLMs do not generalize to different option IDs in MCQ Zheng et al. (2024) and tried to solve this by changing the task to clozeAlzahrani et al. (2024). However, the cloze task reduces the expected value of selecting the correct answer. Therefore, we propose changing the multiple-choice questions to bool questions, requiring two judgments to be accurate, so that the difficulty of the questions is as similar as possible to that of multiple-choice questions.\nAs illustrated in Figure 3, We derive one true"}, {"title": "2.3 Replace Irrelevant Nouns to Analyze Bias towards Irrelevant Content", "content": "In open-ended QA like those in GSM8K(Cobbe et al., 2021), the questions may contain nouns that are unrelated to the answers. In this subsection, we explore the impact of changes to these unrelated nouns on the decision-making of large models. As shown in Figure 4, we replaced nouns in the questions, such as names of people and animals, ensuring that these replacements do not alter human decision-making.\nSemantic relevance control Additionally, regarding noun replacements, we also examined the impact of the semantic proximity of the replacements. We conducted experiments in this area by instructing GPT-40 mini to perform replacements with varying degrees of semantic similarity."}, {"title": "3 Experiments", "content": "We perform evaluations on harness framework (Gao et al., 2024) and adopt its default setting. We evaluate models of Llama3.1 series (Dubey et al., 2024), Qwen2.5 series (Yang et al., 2024b), and GPT40. Llama3.1, and Qwen2.5 are the most powerful small models, while GPT4o is"}, {"title": "3.1 Results of Altering Option Length", "content": "LLMs struggle to generalize across option length: From Table 1, it is evident that across all LLMs, from 1.5B to GPT40, scores increase significantly when the length of the correct option is extended and decrease significantly when we make an incorrect option longer. Smaller models generalize worse.\nLength matters, especially when we lengthen the right option. As shown in Table 2, changing the length can result in a difference of more than ten points in the RL setting."}, {"title": "3.2 Results of Altering Scoring Type", "content": "LLMs do not have invariant knowledge that can generalize across scoring types. As in Table 3, all models tend to score lower when the benchmarks are changed from the original format to boolean questions. Qwen2.5 1.5B and Llama3.1 8B score"}, {"title": "3.3 Results of Replacing Irrelevant Nouns", "content": "Replacing irrelevant nouns degrades performance consistently across various models. As seen in Table 5, the scores of all models drop when the terms are renamed, with the magnitude of the decrease being similar across models. GPT40 models still show a decline.\nReplacing irrelevant nouns with semantically distant words further reduces the effectiveness."}, {"title": "4 Conclusion", "content": "This paper highlights the fragility of Large Language Models (LLMs) in generalizing to minor perturbations in benchmark tasks. Our experiments reveal that LLMs exhibit significant performance"}, {"title": "Limitations", "content": "This work focuses solely on non-chain-of-thought LLMs, such as GPT-40, and does not consider emerging 01."}, {"title": "Ethnic Statement", "content": "This work adheres to ACL's ethical guidelines, we state that there are no ethical concerns to our knowledge."}, {"title": "A.1 Prompts", "content": "We chose the GPT-40 to lengthen options.\nThe default prompt to lengthen options is:\nThe user will give you a question, the choices, and the answer from a dataset. Rewrite the four choices into longer ones. Make sure not to change the question willingly. Make sure that the rewritten options do not contain a hint of the correct answer.\nThe prompt to control option length is: We concat the default prompt to one of the below prompts.\n\u2022 Make sure that each rewritten option contains no more than 10 words.\n\u2022 Make sure that each rewritten option at least 10 words and no more than 20 words.\n\u2022 Make sure that each rewritten option contains at least 20 words.\nWe set the temperature to 0."}, {"title": "A.2 Verification Process", "content": "We manually verified the rewritten sentences to check whether lengthening the sentence introduced factors related to the answer or changed the question's meaning. We manually checked 100 examples from MMLU and found that 99 had no issues, while 1 changed the original meaning of the question. The rewriting accuracy was 99%."}, {"title": "B Prompts in Replacing Irrelevant Nouns", "content": "We found that GPT-40 and GPT-40 mini perform similarly on this task. To reduce carbon emissions, we chose the GPT-40 mini.\nThe prompt to simply replace irrelevant nouns is: Assist in creatively substituting nouns in mathematical problems to prevent students from memorizing solutions. The replacements should be imaginative, ensuring the mathematical relationships and the accuracy of the solutions are preserved. \"input_text\" Other than replacing nouns, do not alter the original word order, sentence structure, or add or remove any sentences. Give the modified question directly."}, {"title": "C Experiment Setup Details", "content": "This section describes the foundational setup of our experiments and analyses, including the evaluation framework and methods we used and the benchmarks and models we evaluated."}, {"title": "C.1 Evaluation Protocol", "content": "We perform evaluations on harness framework (Gao et al., 2024). We chose harness because it is a flexible, configurable, reproducible framework. Unless otherwise specified, our evaluations are conducted in a 5-shot manner, with few-shot examples drawn from the benchmarks' corresponding training sets."}, {"title": "C.2 Models", "content": "We evaluate models of Llama3.1 series (Dubey et al., 2024), Qwen2 series (Yang et al., 2024a), and GPT40. Llama3.1 and Qwen2.5 are the most powerful small models, while GPT4o is the most powerful LLM. We list all models below."}, {"title": "C.3 Benchmarks", "content": "We evaluate LLMs on MMLU, ARC, Helaswag, GSM-MCQ, and GSM8k. The first four are MCQ benchmarks, and the last consists of open-ended questions.\n\u2022 MMLU (Hendrycks et al., 2021) is a multitask benchmark that covers 57 tasks ranging from elementary to college level. These tasks cover multiple disciplines, e.g., math, physics, law, history, etc. The whole test set consists of 14,042 examples. Following common practice, we calculate the accuracy of each task and report the average score across all tasks.\n\u2022 ARC (Clark et al., 2018) is also a multitask dataset that includes data from eight types of tasks, testing aspects such as common sense, multi-hop reasoning, and algebraic operations, with 3,548 samples. ARC has two subsets: one is ARC-Challenge (abbreviated as ARC-C), and the other is ARC-Easy (abbreviated as ARC-E). The challenge set includes only those data that cannot be answered through retrieval and word co-occurrence methods, making it more difficult.\n\u2022 GSM-8K (Cobbe et al., 2021) examines multi-step math word problems, which are relatively easy and designed to be solvable by middle school students. GSM8K is presented in an open-ended question format, unlike multiple-choice questions. It consists of 1,319 test questions."}, {"title": "C.4 Budget", "content": "We performed experiments with an H800 GPU; the total experiments cost about 1000 GPU hours."}, {"title": "D Additional Results", "content": ""}, {"title": "D.1 Make ALL Wrong Options Longer.", "content": "Making all wrong options could expose the right answer. From Table 6, we can see that if all the incorrect options are lengthened, the model will choose the only correct option that hasn't been lengthened."}]}