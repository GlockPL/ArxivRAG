{"title": "Cross-modal Information Flow in Multimodal Large Language Models", "authors": ["Zhi Zhang", "Srishti Yadav", "Fengze Han", "Ekaterina Shutova"], "abstract": "The recent advancements in auto-regressive multimodal large language models (MLLMs) have demonstrated promising progress for vision-language tasks. While there exists a variety of studies investigating the processing of linguistic information within large language models, little is currently known about the inner working mechanism of MLLMs and how linguistic and visual information interact within these models. In this study, we aim to fill this gap by examining the information flow between different modalities-language and vision\u2014in MLLMs, focusing on visual question answering. Specifically, given an image-question pair as input, we investigate where in the model and how the visual and linguistic information are combined to generate the final prediction. Conducting experiments with a series of models from the LLaVA series, we find that there are two distinct stages in the process of integration of the two modalities. In the lower layers, the model first transfers the more general visual features of the whole image into the representations of (linguistic) question tokens. In the middle layers, it once again transfers visual information about specific objects relevant to the question to the respective token positions of the question. Finally, in the higher layers, the resulting multimodal representation is propagated to the last position of the input sequence for the final prediction. Overall, our findings provide a new and comprehensive perspective on the spatial and functional aspects of image and language processing in the MLLMs, thereby facilitating future research into multimodal information localization and editing.", "sections": [{"title": "1. Introduction", "content": "Multimodal large language models (MLLMs) [5, 11, 24, 27, 28] have demonstrated notable performance across a wide range of vision-language tasks, which is largely attributed to the combination of powerful auto-regressive large language models [39, 40, 44, 47] and visual encoders [13, 16, 35]. Specifically, LLMs generate responses based on both visual and linguistic inputs where visual representations extracted from an image encoder precede the word embeddings in the input sequence. Despite the successful performance and wide applicability of MLLMs, there is still a lack of understanding of their internal working mechanisms at play when solving multimodal tasks. Acquiring deeper insights into these mechanisms could not only enhance the interpretability and transparency [31, 33] of these models but also pave the way for developing more efficient and robust models for multimodal interactions.\nSome initial studies have begun to explore the internal states corresponding to external behaviors of MLLMs, focusing on specific aspects such as information storage in the model's parameters [6], reflecting undesirable content generation through logit distributions of the generated tokens [46], the localization and evolution of object-related visual information [32, 34, 37], the localization of safety mechanism [43] and the reduction of redundant visual tokens [45]. However, the information flow between the two modalities within MLLMs remains poorly-understood, thus prompting our main question: Where in the model and how is visual and linguistic information integrated within the auto-regressive MLLMs to generate the final prediction in vision-language tasks?\nTo address this question, we investigate the interaction of different modalities by locating and analyzing the information flow [15] between them, across different layers. Our focus is on the task of visual question answering (VQA), a popular multimodal task, where the answer is generated by MLLMs based on the input image and the corresponding question. Specifically, we aim to reverse engineer the information flow between the two modalities at inference time, by selectively inhibiting specific attention patterns between tokens corresponding to visual and linguistic inputs and by observing the resulting changes in the performance of the answer prediction.\nIn modern auto-regressive MLLMs, which employ Transformer decoder-only architecture [41], the attention layer is the sole module enabling communication between hidden representations corresponding to different positions of the input. To inhibit cross-modal information flow, we therefore adopt an attention knockout approach, proposed by Geva et al. [19]. We use it to block attention edges connecting different types of hidden representations (e.g. image and question) at specific transformer layers.\nWe apply this method to a range of MLLMs from the LLaVA series, including LLaVA-1.5-7b, LLaVA-1.5-13b [27], LLaVA-v1.6-Vicuna-7b [28] and Llama3-LLaVA-NEXT-8b [2] and a number of diverse question types in VQA, as shown in Table 1. Our experiments focus on the following research questions: (1) How is the (more general) visual information from the whole image fused with the linguistic information in the question? (2) How is the more targeted visual information (i.e. specific image regions directly relevant to answering the question) integrated with linguistic information from the question? and (3) In what ways do the linguistic and visual components of the input contribute to the final answer prediction? To answer these questions we conduct a series of experiments, blocking information flow between (1) the input positions corresponding to the whole image to the different parts of the question; (2) the input positions corresponding to image regions containing objects relevant to answering the question, to the question; (3) the input positions corresponding to the image and the question to the final prediction, across different layers of the MLLM.\nOur results reveal that in MLLMs, visual information undergoes a two-stage integration into the language representation within the lower-to-middle layers: first in a comprehensive manner, and subsequently in a more targeted fashion. This integrated multimodal representation is then propagated to the hidden representations in the subsequent layers, ultimately reaching the last position for generating an accurate response. The visualization of this mechanism is shown in Figure 1. To the best of our knowledge, ours is the first paper to elucidate the information flow between the two modalities in auto-regressive MLLMs. It thus contributes to enhancing the transparency of these models and provides novel and valuable insights for their development."}, {"title": "2. Related work", "content": "MLLMS multimodal large language models have demonstrated remarkable performance across a wide range of vision-language tasks, which is largely attributed to the development of the auto-regressive large language models. The representative MLLMs [5, 11, 24\u201328] consist of an image encoder [13, 16, 35] and a powerful decoder-only large language model [39, 40, 44, 47]. The visual and linguistic information are integrated in original LLM. In this paper, we will investigate this inner working mechanism of multimodal information processing into these models.\nInterpretability of multimodal models The interpretability of multimodal models has attracted a great deal of attention in the research community. Works in [7, 17] treat the model as a black box, analyzing input-output relationships to interpret the behavior of models, such as comparing the importance of different modalities [7] and the different modalities' contribution to visual or textual tasks [17]. The works from [3, 8, 29, 38] aim to explain predictions by tracing outputs to specific input contributions for a single sample, including through merging the attention scores [3, 38], using gradient-based methods [8] or model disentanglement [29]. Additionally, some works [9, 20, 36] adopt a top-down approach, probing learned representations to uncover high-level concepts, such as visual-semantics [9], verb understanding [20], shape and size [36]. In contrast, our work focuses on the model's internal processing mechanisms when solving multimodal tasks.\nMechanistic interpretability of MLLMs Mechanistic interpretability [31, 33] is an emerging research area in NLP, aiming to reverse-engineer detailed computations within neural networks. While it has gained attraction in NLP, research in the multimodal domain remains limited. Palit et al. [34] introduced a causal tracing tool for image-conditioned text generation on BLIP [23], marking one of the few early efforts in this area. Several initial studies have started to explore the internal states of MLLMs by linking external behaviours to specific mechanisms, such as information storage in model parameters [6], undesirable content generation reflected in the logit distributions of the first generated token [46], localization and evolution of object-related visual information [32, 34, 37], safety mechanism localization [43], and reducing redundant visual tokens [45]. However, research offering a comprehensive understanding of the internal mechanisms behind multimodal information integration in MLLMs is still lacking. This paper makes an important first step towards filling this gap."}, {"title": "3. Tracing information flow in MLLMs", "content": "The focus of this paper is on auto-regressive multimodal large language models, which consist of an image encoder and a decoder-only language model, as shown in Figure 2. The image encoder transforms images into representations that the language model can take as input, while the language model integrates these visual cues with any provided text, generating responses one word at a time. Often, these components are initialized from a pre-trained image encoder (e.g. CLIP-ViT-L-336px [35]) and a large language model (e.g. Llama 2 [40]) respectively. Since the interaction between modalities only occurs in the decoder-only transformer, our analysis centers around it and we refer to it as MLLM for brevity unless otherwise specified.\nInput The input to an MLLM typically comprises image and text features, with the image features being initially extracted from an image encoder and the text being encoded through word embeddings. Formally, an image x is evenly split into fixed-size patches and encoded by an image encoder to obtain $N_v$ visual patch features $V = [v_i]_{i=1}^{N_v}$, $v_i \\in \\mathbb{R}^d$. Similarly, the text t, consisting of $N_T$ tokens, is embedded into representations through a lookup table of word embeddings, resulting in the text input $T = [t_i]_{i=1}^{N_T}$, $t_i \\in \\mathbb{R}^d$. By concatenation of V and T, the multimodal input sequence $I = [v_1 ... v_{N_v}, t_1 ...t_{N_T}] \\in \\mathbb{R}^{N \\times d}$, where $N = N_v + N_T$, is fed into MLLM.\nHidden representation The input sequence is fed into the MLLM, where the hidden representation at each token position is encoded across L transformer layers. Each layer primarily consists of two modules: a masked multi-head attention (MHAT) followed by a fully connected feed-forward network (FFN) [41]. For conciseness, we have excluded the bias terms and layer normalization, as they are not crucial for our analysis. Formally, the hidden representation $h_i^l \\in \\mathbb{R}^d$ in the position i of the input sequence at layer l can be expressed as\n$h_i^l = h_i^{l-1} + a_i^l + f_i^l$, (1)\nwhere $a_i^l \\in \\mathbb{R}^d$ and $f_i^l \\in \\mathbb{R}^d$ are the outputs of MHAT and FFN modules at layer l, respectively. $h_i^{l-1}$ represents a vector in the input I with position of i. All hidden representations at layer l corresponding to the whole input I can be denoted by $H^l = [h_i^l]_{i=1}^N \\in \\mathbb{R}^{N \\times d}$.\nMHAT The masked multi-head attention (MHAT) module in each transformer layer l contains four projection matrixes: $W_q^l, W_k^l, W_v^l, W_o^l \\in \\mathbb{R}^{d \\times d}$. For the multi-head attention, the input $H^{l-1}$ is first projected to query,key and value: $Q^l = H^{l-1}W_q^l$, $K^l = H^{l-1}W_k^l$, $V^l = H^{l-1}W_v^l$. Then the projected query, key and value matrices are evenly split along the columns to H different heads: {$Q^{l,j}$}$_{j=1}^H$, {$K^{l,j}$}$_{j=1}^H$, {$V^{l,j}$}$_{j=1}^H \\in \\mathbb{R}^{N \\times \\frac{d}{H}}$, respectively.\nAfter splitting $W_o^l$ into {$W_o^{l,j}$}$_{j=1}^H \\in \\mathbb{R}^{d \\times \\frac{d}{H}}$, we follow works in [12, 15, 19] to represent the output of MHAT $A^l = [a_i^l]_{i=1}^N \\in \\mathbb{R}^{N \\times d}$ at layer l as the sum of the output from different heads\n$A^l = \\sum_{j=1}^H A^{l,j}W_o^{l,j}$ (2)\n$A^{l,j} = softmax(\\frac{Q^{l,j}(K^{l,j})^T}{\\sqrt{d/H}} + M^{l,j})$ (3)\nwhere $M^{l,j}$ is a strictly upper triangular mask for $A^{l,j}$ for j-th head at layer l. For an auto-regressive transformer model, $M^{l,j}$ is used to guarantee that every position of the input sequence cannot attend to succeeding positions and attends to all preceding positions. Therefore, for the element $M_{s,t}^{l,j}$ with the coordinate (s, t) in $M^{l,j}$,\n$M_{s,t}^{l,j} = \\begin{cases}\n-\\infty & \\text{if } t > s, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$ (4)\nFFN FFN computes the output representation through\n$f^l = W_2^l \\sigma(W_1^l(a^l + h^{l-1}))$ (5)\nwhere $W_1^l \\in \\mathbb{R}^{d \\times d_{ff}}$ and $W_2^l \\in \\mathbb{R}^{d_{ff} \\times d}$ are projection matrices with inner-dimensionality $d_{ff}$, and $\u03c3$ is a nonlinear activation function.\nOutput The hidden representation $h_N^L$ corresponding to the last position N of the input sequence at final layer L is projected by an unembedding matrix $E \\in \\mathbb{R}^{\\vert V \\vert \\times d}$ and finally the probability distribution over all words in the vocabulary V is computed by\n$P_N = softmax(Eh_N^L)$, (6)\nwhere the word with the highest probability in $P_N$ is the final prediction."}, {"title": "3.2. Attention knockout", "content": "In this paper, we mainly investigate the interaction between different modalities by locating and analyzing the information flow between them. We adopt a reverse-engineering approach to trace the information flow. Specifically, by intentionally blocking specific connections between different components in the computation process, we trace the information flow within them by observing changes in the probability of final prediction.\nIn MLLMs, the attention module (MHAT) is the only module, which has the function of communication between different types of hidden representation corresponding to different positions in the input sequence. Therefore, we intentionally block the attention edges between hidden representations at different token positions (termed as attention knockout) to trace the information flow between them. We take inspiration from the work of [19], where the authors use attention knockout to assess how the factual information is extracted from a single-modality LLM by evaluating the contribution of certain words in a sentence to last-position prediction. We extend this method to multimodal research by not only examining the contribution of each modality to the last-position prediction but also the transfer of information between different modalities.\nIntuitively, when blocking the attention edge connecting two hidden representations corresponding to different positions of the input sequence leads to a significant deterioration in model performance, it suggests that there exists functionally important information transfer between these two representations. Therefore, we locate the information flow between different hidden representations corresponding to different positions of the input sequence, such as visual inputs, linguistic inputs, and the last position in the input sequence (the position of answer prediction), by blocking the attention edge between them in the MHAT module and observing the resulting decline in performance as compared to the original model with an intact attention pattern.\nFormally, in order to prevent information flow from the hidden representations $h_s^l$ with position s in the source set S (e.g. all positions of visual tokens in the input sequence) to the hidden representations $h_t^l$ with position t in the target set T (e.g. all positions of linguistic tokens in the input sequence) at a specific layer l < L, we set the corresponding element $M_{s,t}^{l,j}$ in $M^{l,j}$ to \u2212\u221e and the updated Eq. (4) is\n$M_{s,t}^{l,j} = \\begin{cases}\n-\\infty & \\text{if } (t > s) \\text{ or } (s \\text{ in S and t in T}), \\\\\n0 & \\text{otherwise}.\n\\end{cases}$ (7)\nThis prevents the token position in the target set from attending to that in the source set when MLLM generates the predicted answer."}, {"title": "4. Experimental setting", "content": "Setup Our paper investigates the inner working mechanism of MLLMs, focusing on visual question answering (VQA). Typically, the VQA setup involves an image and a corresponding question about this image, which the model needs to answer. We first investigate where the information from different modalities (image and textual question) is processed in MLLMs, and then how it is integrated within the model. Finally, we explore how the MLLM makes the final decision using this multimodal information.\nTasks and data We collect our data from the validation set of GQA dataset [21]. GQA is a dataset designed to support visual reasoning and compositional question-answering, offering the semantic and visual richness of real-world images. It is derived from the Visual Genome dataset, which includes detailed scene graph structures [22]. In GQA, the questions are categorized through two dimensions: structure and semantics. The former defines the question format (5 classes) and the latter refers to the semantic information for the main subject of the question (5 classes). The answers to these questions consist of only one word or phrase, which is easy to evaluate. Based on the two dimensions, the questions in GQA are categorized into 15 groups. We exclude most groups that consist of simple binary questions (yes/no) and demonstrate poor performance on the model investigated in this paper. Finally, we select 6 out of 15 groups (4 structural and 4 semantic classes) in which their performance is higher than 80% in average performance, as shown in Table 1. The difficulty of our selected groups ranges from simple multimodal perception tasks to more complex multimodal reasoning. For example, ChooseAttr and ChooseCat ask about basic object attributes and categories for one object in the image, ChooseRel and QueryAttr involve spatial reasoning, and CompareAttr and LogicalObj require more challenging comparisons and logical reasoning between two objects in the image. For each selected group, we sample an average of 920 image-question pairs that are correctly predicted by most models used in this paper. For each model, we only use correctly predicted samples for analysis (Each model achieves an accuracy greater than 95% on the dataset we collected). More details about the dataset and the process of collection can be found in Appendix A.\nFormat Formally, given an image i and a question q (the question may contain answer options $o_s = [o_1, o_2]$), the model is expected to generate the answer a in the last position of the input sequence. In addition, the correct one in the options is referred to as the true option ($o_t$) while the other ones are denoted as the false option ($o_f$). Since the image, question and options might contain multiple input tokens, we use $I, Q, O_t, O_f$ to represent the set of input positions corresponding to image, question, true option and false option, respectively.\nEvaluation We quantify the information flow between different input parts by evaluating the relative change in the probability of the answer word which is caused by blocking connections between different input parts (attention knockout). Formally, given an image-question pair, the MLLM generates the answer a with the highest probability $p_1$ from the output distribution $P_N$ defined in Equation (6). After applying attention knockout at specific layers, we record the updated probability $p_2$ for the same answer a as in $p_1$. The relative change in probability, $p_c\\%$, is calculated as $p_c\\% = ((p_2 - p_1)/p_1) \\times 100$. In this paper, attention knockout is applied to each transformer layer (within a defined window) individually and evaluate their respective $p_c$ values."}, {"title": "5. Contribution of different modalities to the final prediction", "content": "For a successful answer prediction for the task of VQA, the MLLM will process the input image-question pair [i, q] and generate the final answer from the output layer of the model corresponding to the last position. We first investigate whether the different modalities directly contribute to the final prediction.\nExperiment 1 For each layer l in the MLLM, we block the target set (the last position) from attending to each source set (I or Q) respectively at the layers within a window of k = 9 layers around the l-th layer\u00b9, and measure the change in the probability of the correct answer word. The last position means N-th position in the input sequence and it is also the first generated sub-word for the predicted answer. Typically, the answers contain a single word or phrase, which might sometimes be tokenized into several sub-word tokens. Therefore, we also conduct the same experiment and observe the probability change at the final generated sub-word of the predicted answer. Both the first and final generated sub-words yield similar results. Thus, we present all the results of the first generated sub-words in the main body of the paper, with details on the final sub-words provided in Appendix C.\nObservation 1: the contribution to the prediction at the last position is derived from other input components, rather than the input itself at this position. First of all, as an auto-regressive model, it is assumed that the input generated from preceding steps at the final position already encompasses the crucial information required for predicting the correct answer. However, as shown in Figure 3, when we block the attention edge from the last position to itself (Last \u2192 Last), there is negligible change observed in the probability of final prediction. This implies that the input at the last position does not encompass crucial information for the final prediction of the model. The prediction is, therefore, mainly influenced by other parts of the input sequence.\nObservation 2: The information from the question positions plays a direct and predominant role in influencing the final prediction. As shown in Figure 3, blocking attention from the last position to the hidden representations in Q (Question \u2192 Last) results in significant reduction in prediction probabilities across all six tasks. For example, in the ChooseAttr task, this decreases the prediction probability by up to ~ 30%. This highlights the critical flow of information from Q to the last position, directly affecting the final prediction. It is worth noting that this information flow pattern is observed primarily in the middle layers, where performance reductions consistently occur across all six tasks. In contrast, information from the image positions (I) does not directly and significantly impact the final prediction in most tasks, except for QueryAttr, where a slight information flow from I to the last position is observed. However, this direct influence is negligible compared to its indirect effects, discussed below. The additional experiment about the information flow between different parts of question, such as options os and object words, and last position can be found in Appendix F.\nExperiment 2 As the MLLM is auto-regressive and the input format is image followed by the question in our setting, the information from the image (I) can propagate to the positions of the question (Q), but not the other way around. To establish whether this indeed occurs, for each layer l, we block Q from attending to I with the same window size (k = 9) around the l-th layer and observe the change in the probability of the answer word at the last position as above.\nObservation: Information flow from the image positions to question positions occurs twice As shown in Figure 4, blocking the question positions from attending to the image positions leads to a reduction in prediction probability. This is visible in lower layers, in two different parts of the model. We first observe a sharp drop in layers ~ 0 - 4 and then a second smaller drop around 10th layer. This indicates a two-stage integration process of visual information into the representations of the question. In the first drop, attention knockout reduces the prediction probability by an average of ~ 60% across all six tasks. In the second drop, tasks such as ChooseAttr, ChooseCat, ChooseRel, and QueryAttr show another average reduction of ~ 21% while CompareAttr and LogicalObj exhibit smaller decreases. Despite the variability in the magnitude of the reduction, the layers responsible for information flow remain consistent across all tasks, which is also observed during the first drop. The additional experiment about the information flow between image and different parts of question, such as option os and object words, can be found in Appendix F."}, {"title": "6. How is the linguistic and visual information integrated?", "content": "The results of the above analysis suggest a two-stage integration process of the two modalities within an MLLM. In this section, we further investigate how the information about specific visual and linguistic concepts is integrated across these two stages.\nExperiment To investigate how the model uses the image to answer the question, we conducted attention knockout experiments at the level of individual objects and individual words. The dataset used in the paper consists of questions targeting specific objects and each object is annotated with the bounding box for a certain image region. Based on whether an image patch includes the corresponding bounding boxes (objects), we divide the input image patch features V into two groups: $V_{obj}$ corresponding to the patches containing the objects mentioned in the question, and $V_{oth}$ containing the remaining patches. Then, for each layer l, we use the same attention knockout method to block Q from attending each source set, $I_{obj}$ and $I_{oth}$, corresponding to the position of $V_{obj}$ and $V_{oth}$ in the input sequence respectively, at the layers with a window of k = 9 layers around the l-th layer, and observe the change in the probability of the correct answer word.\nObservation: Shifting focus from comprehensive representation to specific regions of interest As illustrated in Figure 5, blocking the attention edges between the position of $V_{obj}$ and Q (related image patches\u2192question) and between the position of $V_{oth}$ and Q (other image patches\u2192 question) appear to account for the two performance drops observed in Figure 4, individually. Specifically, other image patches \u2192 question clearly results in a significant and predominant reduction in prediction probability during the first stage of cross-modal integration, while related image patches \u2192 question plays a dominant role at the second stage. It is noteworthy that both types of cross-modal information transfer occur in similar layers within the MLLM across all six tasks. Even for the CompareAttr and LogicalObj tasks, although slight changes in probability are observed during the second stage, the layers in which this happens remain consistent with those of the other tasks. This suggests in the lower layers, the model integrates the information from the whole image into the question positions building a more generic representation. And it is only in the later layers, that the model starts to pay attention to the specific regions in the image relevant to the question, fusing the more fine-grained linguistic and visual representations. The other MLLMs also present similar results as shown in Appendix E. The additional more fine-grained analysis on intervention of the attention edge between object words in question and image region can be found in Appendix F. Moreover, we find compared with LLaVA-1.5-13b, the model LLaVA-1.5-7b with smaller size has less information flow from the position of $V_{oth}$ to that of question in the first stage, as shown in Appendix E."}, {"title": "7. How is the final answer generated?", "content": "Experiment To track the process of answer generation in the MLLM, motivated by the approach of logit lens [1], we monitor the probability of the correct answer from the hidden representations at the last position of the input sequence across all layers. Formally, for each layer l at the last position N, we use the unembedding matrix E (as defined in Equation (6)) to compute the probability distribution over the entire vocabulary V:\n$P^l = softmax(Eh_N^l)$, (8)\nwhere the probability of the target answer word $w_a$ is given by the corresponding entry in $P^l$, denoted as $P^l(w_a)$. As the tokenizer in most MLLMs distinguishes the case of the word, especially the initial letter of the word, we monitor the probability of the answer word with both those starting with uppercase (Capitalized Answer) and lowercase letters (Noncapitalized Answer).\nObservation 1: The model is able to predict the correct answer starting at the layer immediately following multimodal integration As illustrated in Figure 6, the probability of the answer word with a lowercase initial letter (Noncapitalized Answer) rises sharply from near 0 to a range of ~20% to ~70% across the six VQA tasks, around the model's middle layers. This implies that the model rapidly acquires the capability to predict correct answers in these middle layers, where the phase of multimodal information integration has just fully completed (see Figure 5) and the multimodal information is still transforming from question position to last position (see Figure 3).\nObservation 2: Semantic generation is followed by syntactic refinement As shown in Figure 6, across all VQA tasks, the probability of Noncapitalized Answer starts to gradually decrease to nearly zero after an increase in middle layers. In contrast, the probability of Capitalized Answer remains low in the initial layers following 20th but starts to increase in subsequent layers. This indicates the model has already semantically inferred the answer by about halfway through layers and in the higher layers, the model starts to refine the syntactic correctness of the answer. Similar findings on other models are shown in Appendix E."}, {"title": "8. Conclusion", "content": "In this paper, we unveil the inner working mechanisms of auto-regressive multimodal large language models in handling multimodal tasks. Our experiments reveal that different multimodal tasks exhibit similar processing patterns within the model. Specifically, when provided with an input consisting of an image and a question, within the lower-to-middle layers, the model initially propagates the overall image information into the hidden representations of the question in the lower layers and then the model selectively transfers only the question-relevant image information into the hidden representations of the question, facilitating multimodal information integration. In the middle layers, this integrated multimodal information is propagated to the hidden representation of the last position for the final prediction. In addition, we find that the answers are initially generated in lowercase form in middle layers and then converted to uppercase for the first letter in higher layers. These findings enhance the transparency of such models, offering new research directions for better understanding the interaction of the two modalities in MLLMs and ultimately leading to improved model designs."}]}