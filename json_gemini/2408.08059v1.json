{"title": "Maximally Permissive Reward Machines", "authors": ["Giovanni Varricchione", "Natasha Alechina", "Mehdi Dastani", "Brian Logan"], "abstract": "Reward machines allow the definition of rewards for temporally extended tasks and behaviors. Specifying \"informative\" reward machines can be challenging. One way to address this is to generate reward machines from a high-level abstract description of the learning environment, using techniques such as AI planning. However, previous planning-based approaches generate a reward machine based on a single (sequential or partial-order) plan, and do not allow maximum flexibility to the learning agent. In this paper we propose a new approach to synthesising reward machines which is based on the set of partial order plans for a goal. We prove that learning using such \"maximally permissive\" reward machines results in higher rewards than learning using RMs based on a single plan. We present experimental results which support our theoretical claims by showing that our approach obtains higher rewards than the single-plan approach in practice.", "sections": [{"title": "1 Introduction", "content": "Reward machines were introduced in [21] as a way of defining temporally extended (i.e., non-Markovian relative to the environment) tasks and behaviors. A reward machine (RM) is a Mealy machine where states represent abstract 'steps' or 'phases' in a task, and transitions correspond to observations of high-level events in the environment indicating that an abstract step/phase in the task has (or has not) been completed [2, 23]. The RM-based algorithm proposed in [2] has been shown to out-perform state-of-the-art RL algorithms, especially in tasks involving temporally extended behaviours. However, while learning with a reward machine is guaranteed to converge to an optimal policy with respect to the reward machine, in general RMs provide no guarantees that the resulting policy is optimal with respect to the task encoded by the reward machine. For example, a reward machine may specify that event a should be observed before event b, while in some environment states, it may be more efficient to achieve b before a. In general, for an RM-based policy to be optimal with respect to a task, the reward machine for the task must encode all possible ways the task can be achieved.\nAnother problem with reward machines is how to to generate them. While a declarative specification in terms of abstract steps or phases in a task is often easier to write than a conventional reward function, specifying a reward machine for a non-trivial task is challenging and prone to errors. Reward machines can be computed from task specifications expressed in a range of goal and property specification languages, including LTL and LTLf, in a straightforward way [2]. However, reward machines generated from an abstract temporal formula may not expose significant task structure. Writing more \"informative\" specifications can be challenging, and, moreover, may inadvertently over-prescribe the order in which the steps are performed. One way to address this problem, is to generate a reward machine from a high-level abstract description of the learning environment, using techniques such as AI planning [11, 12], or (in a multi-agent setting) ATL model checking [24]. For example, Illanes et al. [11] consider a high-level model, in the form of a planning domain, of the environment in which the agent acts. They show how planning techniques can be used to synthesise a plan for a task, which is then used to generate a reward machine for that task. The reward machine is used to train a meta-controller for a hierarchical RL agent. The controller chooses which option (corresponding to an abstract action in the planning domain) to execute next. Their results indicate that an agent trained using a plan-based reward machine outperforms (is more sample efficient than) a standard HRL agent. They also show that reward machines based on partial-order plans outperform reward machines generated from sequential plans, arguing that this is because partial-order plans allow more ways of completing a task.\nWhile the results presented by Illanes et al. are encouraging, their approach does not allow maximum flexibility to the agent, and thus cannot ensure learning an optimal policy for the task. The reward machine they generate is based on a single partial-order plan. In many cases, a goal may be achieved by different plans and each plan might be more appropriate in different circumstances, e.g., depending on the agent's location, or the resources available.\nIn this paper we propose a new approach to synthesising reward machines which is based on the set of partial-order plans for a goal. We present an algorithm which computes the set of partial-order plans for a planning task, and give a construction for synthesising a maximally permissive reward machine (MPRM) from a set of partial-order plans. We prove that the expected discounted future reward of optimal policies learned using an MPRM is greater than or equal to that obtained from optimal policies learned using a RM synthesised from any single partial-order plan. We introduce a notion of the adequacy of planning domain abstractions, which intuitively characterises when a planning domain captures all the relevant features of an MDP, and prove that the expected reward of an optimal policy learned using an MPRM synthesised from a goal-adequate planning domain is the same as that of an optimal policy for the underlying MDP. Finally, we evaluate MPRMs using three tasks in the CRAFTWORLD environment [1] used in [11, 12], and show that the agent obtains higher reward than with RMs based either on a single partial-order plan or on a single sequential plan."}, {"title": "2 Preliminaries", "content": "In this section, we provide formal preliminaries for both reinforcement learning and planning.\n2.1 Reinforcement Learning\nIn RL the model in which agents act and learn is generally assumed to be a Markov Decision Process (MDP) $M = (S, A, r, p, \\gamma)$, where S is the set of states, A is the set of actions, $r: S\\times A\\times S \\rightarrow R$ is the reward function, $p : S \\times A \\rightarrow \\Delta(S)$ is the transition function, and $\\gamma \\in [0, 1]$ is the discount factor. It is assumed that the agent does not have access to the model in which it acts, i.e., r and p are hidden to it. The agent's goal in RL is to learn a policy $\\rho: S \\rightarrow \\Delta(A)$, i.e., a map from each state of the MDP to a probability distribution over the set of actions. In particular, we are mostly interested in so-called \"optimal policies\", i.e., policies that maximise the expected discounted future reward from any state $s \\in S$:\n$\\rho^* = \\arg \\max_{\\rho} \\sum_{s \\in S} v_{\\rho}(s)$\nwhere $v_{\\rho}(s)$ is the \u201cvalue function\", i.e., the expected discounted future reward obtained from state s by following policy $\\rho$:\n$v_{\\rho}(s) = E_{\\rho} [\\sum_{t=0}^{\\infty} \\gamma^t r_t | S_0 = s]$\nwhere rt is the reward obtained at timestep t.\nAs the MDP's dynamics and reward are hidden, the agent is sup- posed to learn a policy by trial and error. This is achieved by the agent taking an \"exploratory\" action a in a state s, and observing which state s' (sampled from p(s, a)) is reached and the reward r' = r(s,a,s') that is obtained. By collecting these experiences (s, a, s', r') $\\in S\\times A\\times S \\times R$, or \"samples\", the agent can learn a policy $\\rho$ via RL algorithms, such as Q-learning [25].\n2.2 Labelled MDPS\nAs in this work we assume the presence of planning domains and reward machines, we also assume that we are given a so-called \"la- belling function\" L. This function will be the link between the low- level MDP, in which agents learn how to act, and the high-level plan- ning domain and reward machine, which describe how agents can achieve a task using high-level symbols and actions.\nDefinition 2.1 (Labelled MDP). Let P be a set of propositional sym- bols. Then, a labelled MDP is a tuple $M = (S, A, r, p, \\gamma, L)$, where S, a, r, p and $\\gamma$ are as in an MDP, and $L : S\\rightarrow 2^P$ is the labelling function, mapping each state of the MDP to a set of propositional symbols.\n2.3 Reward Machines\nReward machines [23] are a tool recently introduced in the RL lit- erature to define non-Markovian reward functions via finite state au- tomata. Let $M = (S, A, r, p, \\gamma, L)$ be a labelled MDP for some set of propositional symbols P.\nDefinition 2.2 (Reward Machine). A reward machine (RM) is a tuple $R = (U, u_0, \\Sigma, \\delta_u, \\delta_r)$, where U is the set of states of the RM, $u_0$ is the initial state, $\\Sigma \\subseteq 2^P$ is the input alphabet, $\\delta_u : U \\times \\Sigma \\rightarrow U$ is the state transition function, and $\\delta_r : U \\times U \\rightarrow R$ is the reward transition function."}, {"title": null, "content": "When using RMs, training is usually done over the so-called \"product\" between the labelled MDP and the RM, also known as a \"Markov Decision Process with a Reward Machine\" (MDPRM) [23].\nDefinition 2.3 (MDPRM). A Markov Decision Process with a Reward Machine (MDPRM) is a tuple $M_a = (S, A, p, \\gamma, L, U, u_0, \\delta_u, \\delta_r)$, where S, A, p, $\\gamma$, L are as in the definition of a labelled MDP, and U, $u_0$, $\\Sigma$, $\\delta_u$ and $\\delta_r$ are as in the definition of a reward machine.\nAt each timestep, the RM is in some state u. As the agent moves the MDP into state s', the RM updates its internal state via the obser- vation L(s'), i.e., the new RM state is u' = $\\delta_u(u, L(s'))$. Accord- ingly, the RM also outputs the reward $\\delta_r(u, u')$, which is the reward the agent obtains. As in \"vanilla\" MDPs, the agent learns a policy by taking exploratory actions and collecting rewards from the RM's reward function $\\delta_r$. Thus, samples include also the states of the RM, i.e., each sample is a tuple (s, u, a, s', u', r') $\\in S \\times U \\times R \\times S \\times U. For this reason, any RL algorithm that works with standard MDPS can also be used in MDPRMs. Moreover, algorithms exploiting ac- cess to the RM have also been proposed, e.g., CRM [23].\n2.4 Symbolic Planning\nA planning domain D = (F, A), is a pair where F $\\subseteq$ P is a set of fluents (propositions), and A is a set of planning actions. Planning states are subsets S $\\subseteq$ F, where a proposition is in S if and only if it is true in S. Actions a $\\in$ A are tuples a = (pre+, pre\u00af, eff+, eff\u00af) such that each element of a is a subset of F, pre+ \u2229 pre\u00af = \u00d8 and eff+ \u2229 eff = \u00d8. The \u201cpre\u201d sets are the sets of \u201cpreconditions\u201d, whereas the \"eff\u201d are the sets of \u201ceffects\u201d, or \u201cpostconditions\u201d. pre+ are the propositions that must be true to perform the action, whereas pre those that must be false. Analogously, eff are the proposi- tions that are made true by the action, whereas eff those that are made false. Thus, an action a can be executed from a planning state S if and only if pre+ $\\subseteq$ S and pre\u00af \u2229 S = \u00d8. Executing action a in state S results in the new state S' = (S \\ eff\u00af) \u222a eff+. Given an MDP M and a planning domain D, we assume that A \u2229 A = 0, i.e., the planning actions are not the same as the actions the agent can perform in the MDP. Intuitively, the planning actions can be seen as high-level or abstract actions which correspond to sequences of ac- tions in the MDP. For example, in a Minecraft-like scenario where the agent can move up, down, left, and right on a grid, a planning ac- tion might be \"get wood\" corresponding to a sequence of movement actions ending in a cell containing wood.\nAs an example of a planning domain, consider the CRAFTWORLD environment [1] in which an agent moves in a grid and has to gather resources which can then be used to produce items. For example, the agent can build a bridge in order to reach the gold ore. We assume the agent can build two different types of bridge: an iron bridge or a rope bridge. The iron bridge requires gathering wood and iron and then processing them in a factory. The rope bridge requires gathering grass and wood and processing them in a toolshed. The corresponding planning domain D can be formalised as:\n$(F = {has\\text{-}wood, has\\text{-}grass, \\\\ has\\text{-}iron, has\\text{-}bridge}, \\\\ A = {get\\text{-}wood, get\\text{-}grass, get\\text{-}iron, \\\\ use\\text{-}factory, use\\text{-}toolshed})$"}, {"title": null, "content": "The get-x actions have no preconditions, and only one positive postcondition, i.e., that has-x is true. The use-factory action has the preconditions has-wood and has-iron, the pos- itive postcondition has-bridge, and the negative postcondi- tions, has-wood and has-iron, i.e., the use-factory action makes a bridge, \"consuming\" the resources collected by the agent in the process. The use-toolshed action has has-wood and has-grass as preconditions, the positive postcondition has-bridge, and the negative postconditions, has-wood and has-grass.\nA planning task is a triple T = (D, S\u2081, G), where D is a planning domain, S\u2081 is the initial planning state, and G = (G+, G\u00af) is a pair containing two subsets of F which are disjoint. Any planning state S such that G+ $\\subseteq$ S and S \u2229 G\u00af = \u00d8 is a goal state. For example, the planning task to build a bridge is given by the domain D we have previously defined, the initial state S\u2081 = \u00d8, and the goal G = ({has-bridge}, \u00d8).\nA sequential plan $\\pi$ = [$a_0$,..., $a_n$] for a planning task T is a sequence of planning actions $a_i \\in A$ such that: (i) it is possible to execute them sequentially starting from S\u2081, and (ii) by doing so, the planning domain reaches a goal state. For example, the following sequential plan allows the agent to produce a rope bridge:\n[get-wood, get-grass, use-toolshed]\nA partial-order plan (POP) $\\varpi$ = (A', <) is a pair where A' is a multiset of actions from A and < is a partial order over A' [3, 16]. We write a < a' to denote (a, a') $\\in \\prec$, meaning that action a must be performed before action a'. For example, the following partial-order plan allows the agent to produce an iron bridge:\n$\\varpi_{iron-bridge} = (\\\\ (A=get\\text{-}wood, get\\text{-}iron, use\\text{-}factory},\\\\ \\prec=get\\text{-}wood \\prec use\\text{-}factory, \\\\ get\\text{-}iron \\prec use\\text{-}factory\\}))$.\nSequential plans are a special case of partial-order plans where < is a total order. In general, a partial-order plan corresponds to a set of sequential plans, i.e., the set of all sequential plans that can be ob- tained by extending the partial order < to a total order (referred to as a \u201clinearisation\u201d of the partial-order plan). Compared to sequen- tial plans, partial-order plans allow the agent greater flexibility in choosing the order in which actions are executed. While a sequential plan constrains the agent to follow the total order of the plan, with a partial-order plan the agent can perform any action a, so long as all actions a' such that a' < a have already been executed.\nTypically, given a planning task, a partial-order planner, e.g., [18], returns a single partial-order plan $\\varpi$ = (A', <). However, in general, a planning task can be achieved using multiple partial-order plans, i.e., plans $\\varpi'$ where the actions in A' are ordered differently, or which use different multisets of actions.\nDefinition 2.4 (Set of all partial-order plans). The set of all partial- order plans for a planning task (D, S\u2081, G), \u03a0, is the set of plans (A', <) where A' $\\subseteq$ A and any linearisation [$a_0$,..., $a_n$] of A' consistent with < results in a goal state S, i.e., G+ $\\subseteq$ S and G\u00af \u2229 S = \u00d8.\nIt is straightforward to give an algorithm that returns the set of all partial-order plans \u03a0 for a planning task, see Algorithm 1. We as- sume the following definitions. steps($\\pi$) is the multiset of actions in the plan and ord($\\pi$) is the set of ordering constraints. In addition,"}, {"title": null, "content": "the algorithm maintains a set links($\\pi$) of causal links of the form (a', p, a) where a' and a are steps and p is a literal in the postcondi- tion of a' and in the precondition of a. Causal links record the reason for adding step a' to the plan (in order to establish precondition of a), and are used to generate ordering constraints. A step a\" threatens a causal link (a', p, a) if a\" makes p false. To resolve the threat a\" should be placed either before a' in the order, or after a. An ordering is consistent if it is transitive and does not contain cycles, i.e., there is no $a_i$, $a_j$ such that ($a_ia_j$), ($a_ja_i$) $\\in$ ord. Given a planning action a, pre(a) is the set containing the positive and negative literals of the propositional symbols appearing in the sets pre+ and pre\u00af of a, and eff (a) is the set of positive and negative literals in eff+ and eff\u00af. A precondition p of a step a is termed open if there is no causal link (a', p, a) $\\in$ links($\\pi$) establishing p. A plan is complete if it has no open preconditions. Initially, the set of plans is empty, and is initialised to a plan consisting of two steps: start and finish: start has no preconditions and the initial state S\u2081 as a postcondition; finish has no postconditions and the goal G as a precondition. ord contains the single ordering constraint {start < finish}, and links is empty.\nThe procedure POP-PLAN takes a partial-order plan as input. If $\\pi$ has no open preconditions, i.e., the plan is complete, then we remove the steps start and finish, add it to the set of plans, and POP-PLAN re- turns (lines 5-6). Otherwise, we iterate over each open precondition in the set of open preconditions, open (lines 8-14). For each open precondition p, an action a' from the set of actions A of the planning domain is chosen which establishes p (line 9; if there are no actions which establish p, i.e., the plan cannot be extended to a complete plan, this branch of the computation terminates and is discarded). The procedure ORDER is then called (line 14) to resolve any threats introduced by the addition of a'. If there are no threats, then POP-"}, {"title": "3 Maximally Permissive Reward Machines", "content": "In this section, we show how the set of all partial-order plans, \u03a0, for a planning task T = (D,S\u2081, G), can be used to synthesise a reward machine R that is maximally permissive, i.e., which allows the agent maximum flexibility in learning a policy.\nLet $\\Pi$ be the set of all linearisations of all the partial-order plans in \u03a0. We denote by pref($\\pi$) the set of all proper prefixes (of arbitrary length) of $\\pi \\in \\Pi$. Note that the prefixes are finite, as the set of ac- tions in $\\Pi$ is finite. Then, let states(pref($\\pi$)) be the set of sequences of planning states that is induced by the prefixes in pref($\\pi$), assum- ing that the initial planning state is S\u2081. We denote with steps($\\pi$) the set of actions in a sequential plan, and with post(A') the set con- taining post(a), as defined in Section 2.4, for each planning action a $\\in$ A'. For an arbitrary sequence of planning states u, we denote with last(u) the last element of the sequence. For sets of literals P, we, respectively, denote with P+ and P\u00af the sets of propositional symbols with positive and negative literals in P.\nConstruction 1 (Maximally Permissive Reward Machine (MPRM)). Fix the set of all partial-order plans $\\Pi = {\\pi_1,..., \\pi_n}$ for some planning task T = (D,S\u2081, G). Then, $R_{\\Pi}$, the maximally permissive RM corresponding to $\\Pi$, is defined as follows:\n$\\bullet$ U = $[\\cup_{i=1}^n states(pref (\\pi_i))] \\cup {u_g};$\n$\\bullet$ $u_0 = [S_1];$\n$\\bullet$ $\\Sigma = \\cup_{i=1}^n post(steps(\\pi_i));$\n$\\bullet$ $\\delta_u(u, P) = uS$, where S = (last(u) \\ P\u00af) \u222a P+ and uS $\\in$ states(pref($\\pi_i$)) for some linearisation $\\pi_i \\in \\Pi$, or = ug if G+ $\\subseteq$ S and G\u00af \u2229 S = \u00d8;\n$\\bullet$ $\\delta_r(u, u') = \\{\\begin{array}{ll} 0 & \\text{if $u' = u_g$}\\\\ -1 & \\text{otherwise.} \\end{array}$"}, {"title": null, "content": "In the construction of the MPRM, the set of states correspond to the set of all possible prefixes of planning states across all POPs in the set used to build the reward machine. Then, the RM transi- tions from a state u to a state u' = uS when it observes the set of propositional symbols P which are exactly the conditions such that S = (last(u) \\ P\u00af) \u222a P+ and, most importantly, uS is a prefix of some linearisation of a POP in \u03a0. As soon as the RM \"reaches\" a sequence of states such that the last state is a goal state for the task (meaning also that a linearisation has been \"completed\"), it gives a reward of 0 to the agent and terminates in state ug, while for all other transitions the agent gets a reward of -1. Note that if uS is not the prefix of any linearisation of a POP in \u03a0, or S is not a goal state, then $\\delta_u(u, P) = u$. Figure 1 shows the MPRM synthesised from the set $\\Pi_{bridge}$ of all partial-order plans for the bridge example we gave in Section 2.4.\nIn the remainder of this section, we provide a theoretical analy- sis linking the optimal policies that can be learned by an agent de- pending on the kind of reward machine it is equipped with. We con- sider RMs that can be built from the set of all partial-order plans (RM-\u03a0), a single partial-order plan (RM-$\\pi$), and a single sequential plan (RM-${\\pi}^{\\'}$), over the same planning domain D. All RMs issue a non-negative reward only in the final state. We denote the optimal policy learnt using the set of all partial-order plans by ${\\rho}_{RM-\\Pi}$, using a single partial-order plan by ${\\rho}_{RM-\\pi}$, and using a single sequential plan by ${\\rho}_{RM-{\\pi}^{\\'}}$.\nTheorem 3.1. Let M be a labelled MDP, D a planning domain over M, and RM-\u03a0, RM-$\\pi$ and RM-${\\pi}^{\\'}$ final state reward machines generated from D for the same task. Then,\n${\\rho}_{RM-\\Pi} \\geq {\\rho}_{RM-\\pi} \\geq {\\rho}_{RM-{\\pi}^{\\'}}$\nwhere $\\rho_1 \\geq \\rho_2$ if and only if $v(\\rho_1(s)) \\geq v(\\rho_2(s))$ for all states $s \\in S$ of M.\nProof. The proof follows from the fact that any policy that can be learned using an RM synthesised from a single sequential plan can also be learned using an RM synthesised from a partial-order plan which has the sequential plan as its linearisation (if it remains an optimal policy). Similarly, a policy learned using an RM synthesised from a single partial-order plan can also be learned using an RM that is synthesised from the set of all partial-order plans.\nTheorem 3.1 shows that MPRMs allow an agent to learn an op- timal policy with respect to the planning domain and the planning task. A natural question to ask is whether it learns a goal-optimal policy $\\rho^*$, i.e., a policy that achieves the goal using the smallest num- ber of actions in the underlying MDP. For example, an agent using Q-learning is guaranteed to learn a goal-optimal policy on an MDP where the agent is always given the same negative reward and the discount factor $\\gamma$ is exactly 1."}, {"title": null, "content": "An agent using an MPRM will learn a goal-optimal policy if the planning domain and labelling are \"adequate\" for the goal.\nDefinition 3.2. Given a labelled MDP, planning domain D, and goal G, we say that D is adequate for G if, and only if:\n$\\bullet$ G corresponds to a set of planning domain fluents, i.e., G $\\subseteq$ F;\n$\\bullet$ a goal-optimal policy encounters all the state labels in some plan $\\pi \\in \\Pi$ for G, in the order consistent with the order in $\\pi$.\nFor example, if any policy to build a bridge has to encounter la- bels corresponding to getting wood, getting iron and using a factory, a planning domain and labelling containing only these fluents is ad- equate for the goal of having a bridge. However, if there is an al- ternative way of building a bridge that involves getting grass, and this label is missing in the planning domain, then the domain is not adequate for the goal of building a bridge.\nTheorem 3.3. $\\rho^* = {\\rho}_{RM-\\Pi}$ if RM-\u03a0 is synthesized from a goal- adequate planning domain.\nProof. From the definition of a planning domain adequate for the goal, any goal-optimal policy has to go through the way-points en- coded in the reward machine.\n4 Empirical Evaluation\nIn this section, we evaluate maximally permissive reward machines in three tasks in the CRAFTWORLD environment, and show that the agent obtains higher reward with an MPRM than with RMs based on a single partial-order plan or a single sequential plan. In the first task, the agent has to build a bridge, as in the example in Section 2.4 For the second task, the agent has to collect gold. In the third task, the agent has to collect gold or a gem, and the task is considered achieved when the agent collects at least one of the two items. For the gold-or-gem task we have to slightly modify the definition of goal states in planning tasks: the goal is the pair G = (G+ = {has-gold, has-gem},G\u00af = \u00d8), and a planning state S is a goal state if and only if G+ \u2229 S \u2260 0 and G\u00af \u2229 S = \u00d8. The gold and the gem are collected as described in [1]: gold is col- lected by using a(ny) bridge, whereas the gem is collected using an axe. To produce an axe, the agent must combine a stick, which can be obtained by processing wood at the workbench, with iron at the toolshed. We refer to these, respectively, as the \"bridge task\u201d, \u201cgold task\", and \"gold-or-gem task\u201d. In the planning domain, we add the following fluents:\n$\\bullet$ For the gold task: has-gold;\n$\\bullet$ For the gold-or-gem task: has-gold, has-stick, has-axe, has-gem;\nand the following planning actions:\n$\\bullet$ For the gold task:\n$\\qquad$ get-gold, with one positive precondition, has-bridge, and one positive postcondition, has-gold;\n$\\bullet$ For the gold-or-gem task:\n$\\qquad$ use-workbench, with one positive precondition, has-wood, one positive postcondition, has-stick, and one negative postcondition, has-wood;\n$\\qquad$ use-toolshed-for-axe, with positive preconditions, has-stick and has-iron, one positive postcondition, has-axe, and two negative postcondition, has-stick and has-iron;"}, {"title": null, "content": "$\\qquad$ get-gem, with one positive precondition, has-axe, and one positive postcondition, has-gem;\nThe set of partial-order plans for the bridge task $\\Pi_{bridge} = {\\varpi_{iron-bridge}, \\varpi_{rope-bridge}}$ is given in Section 2.4. For the gold task, we extend $\\varpi_{iron-bridge}$ and $\\varpi_{rope-bridge}$ by adding the get-gold action, and by having use-factory < get-gold and use-toolshed < get-gold. For the gold-or-gem task, the set of partial-order plans consists of $\\varpi_{iron-bridge}$, $\\varpi_{rope-bridge}$ and $\\varpi_{gem}$ in which the agent makes an axe and uses it to mine the gem. $\\varpi_{gem}$ is defined as follows:\n$\\varpi_{gem} = (\\\\ (A = {get\\text{-}wood, get\\text{-}iron, use\\text{-}workbench, \\\\ use\\text{-}toolshed\\text{-}for\\text{-}axe, get\\text{-}gem},\\\\ \\prec = {get\\text{-}wood \\prec use\\text{-}workbench, \\\\ get\\text{-}iron \\prec use\\text{-}toolshed\\text{-}for\\text{-}axe, \\\\ use\\text{-}workbench \\prec \\\\ use\\text{-}toolshed\\text{-}for\\text{-}axe, \\\\ use\\text{-}toolshed\\text{-}for\\text{-}axe \\prec get\\text{-}gem}))$.\nFor each task, we also generate all sequential plans that can be ob- tained by linearising the POPs that can be used to achieve the task. Thus, for both the bridge and gold tasks, there are a total of 4 sequen- tial plans and 2 partial-order plans. For the gold-or-gem task, there are a total of 7 sequential plans and 3 partial-order plans. In the Ap- pendix\u00b2, we provide formal definitions of the planning domains, and give also the plans for the gold and gold-or-gem tasks.\n4.1 Experimental Setup\nThe maximally permissive RMs for each task were synthesised using the construction given in Section 3. The RMs for each partial-order and sequential plan were generated using the approach presented in [11]. Training is carried out by using Q-learning over the resulting MDPRMS [23].\nFor each task we generated 10 different maps of size 41 by 41 cells. The maps and initial locations were chosen so that from some locations a task can be completed more quickly by following a par- ticular sequential plan. For example, in the first map for the bridge task, if the agent starts from a location in the upper half of the map (i.e., in the first 20 rows) it is more convenient to build an iron bridge, while in the lower half of the map it is more convenient to build a rope bridge. The MDP reward function r returns -1 for each step taken by the agent, until it achieves the task or the episode terminates. When the task is completed, the map and agent are \"re-initialised\": the agent is placed on a random starting cell and its \"inventory\" is emp- tied, i.e., it contains no items. For each set of plans and single partial- order/sequential plan for a task, and for each of the 10 maps for the task, an agent was trained with the corresponding RM for 10,000,000 training steps. Training was carried out in episodes, lasting at most 1,000 steps, after which the environment was re-initialised regardless of whether the agent has achieved the task or not. Every 10,000 train- ing steps the agent was evaluated on the same map used for training from 5 (predetermined) starting positions. We set the learning rate $\\alpha$ = 0.95, the discount rate $\\gamma$ = 1, and the exploration rate $\\varepsilon$ = 0.1."}, {"title": null, "content": "Our implementation", "11": "is available in the following GitHub repository.\n4.2 Results\nFor each approach", "aggregated\" results for the se- quential and partial-order plan-based RMs": "for each kind of plan we plot the median and the 25th and the 75th percentiles of all agents trained with an RM generated using that type of plan. In the plots", "QRM-MPRM\" denotes the performance of the agent trained with a maximally permissive RM, while \"Aggregated-QRM-Seq\" and \"Aggregated-QRM-POP\" are, respectively, the aggregated per- formance of agents trained with sequential plan RMs and partial- order plan RMs. On the x-axis we plot the number of steps (in mil- lions), while on the y-axis we plot the performance obtained during the evaluations run at the corresponding timestep.\nis consistent with Theorem 3.1 and the results in [11": ".", "RM": "note that none of them achieved the task in fewer steps than the agents shown in the figure. In the supplementary ma- terial we provide, for each agent, a file containing its trajectory (i."}]}