{"title": "Maximally Permissive Reward Machines", "authors": ["Giovanni Varricchione", "Natasha Alechina", "Mehdi Dastani", "Brian Logan"], "abstract": "Reward machines allow the definition of rewards for temporally extended tasks and behaviors. Specifying \"informative\" reward machines can be challenging. One way to address this is to generate reward machines from a high-level abstract description of the learning environment, using techniques such as AI planning. However, previous planning-based approaches generate a reward machine based on a single (sequential or partial-order) plan, and do not allow maximum flexibility to the learning agent. In this paper we propose a new approach to synthesising reward machines which is based on the set of partial order plans for a goal. We prove that learning using such \"maximally permissive\" reward machines results in higher rewards than learning using RMs based on a single plan. We present experimental results which support our theoretical claims by showing that our approach obtains higher rewards than the single-plan approach in practice.", "sections": [{"title": "1 Introduction", "content": "Reward machines were introduced in [21] as a way of defining temporally extended (i.e., non-Markovian relative to the environment) tasks and behaviors. A reward machine (RM) is a Mealy machine where states represent abstract 'steps' or 'phases' in a task, and transitions correspond to observations of high-level events in the environment indicating that an abstract step/phase in the task has (or has not) been completed [2, 23]. The RM-based algorithm proposed in [2] has been shown to out-perform state-of-the-art RL algorithms, especially in tasks involving temporally extended behaviours. However, while learning with a reward machine is guaranteed to converge to an optimal policy with respect to the reward machine, in general RMs provide no guarantees that the resulting policy is optimal with respect to the task encoded by the reward machine. For example, a reward machine may specify that event a should be observed before event b, while in some environment states, it may be more efficient to achieve b before a. In general, for an RM-based policy to be optimal with respect to a task, the reward machine for the task must encode all possible ways the task can be achieved.\nAnother problem with reward machines is how to to generate them. While a declarative specification in terms of abstract steps or phases in a task is often easier to write than a conventional reward function, specifying a reward machine for a non-trivial task is challenging and prone to errors. Reward machines can be computed from task specifications expressed in a range of goal and property specification languages, including LTL and LTLf, in a straightforward way [2]. However, reward machines generated from an abstract temporal formula may not expose significant task structure. Writing more \"informative\" specifications can be challenging, and, moreover, may inadvertently over-prescribe the order in which the steps are performed. One way to address this problem, is to generate a reward machine from a high-level abstract description of the learning environment, using techniques such as AI planning [11, 12], or (in a multi-agent setting) ATL model checking [24]. For example, Illanes et al. [11] consider a high-level model, in the form of a planning domain, of the environment in which the agent acts. They show how planning techniques can be used to synthesise a plan for a task, which is then used to generate a reward machine for that task. The reward machine is used to train a meta-controller for a hierarchical RL agent. The controller chooses which option (corresponding to an abstract action in the planning domain) to execute next. Their results indicate that an agent trained using a plan-based reward machine outperforms (is more sample efficient than) a standard HRL agent. They also show that reward machines based on partial-order plans outperform reward machines generated from sequential plans, arguing that this is because partial-order plans allow more ways of completing a task.\nWhile the results presented by Illanes et al. are encouraging, their approach does not allow maximum flexibility to the agent, and thus cannot ensure learning an optimal policy for the task. The reward machine they generate is based on a single partial-order plan. In many cases, a goal may be achieved by different plans and each plan might be more appropriate in different circumstances, e.g., depending on the agent's location, or the resources available.\nIn this paper we propose a new approach to synthesising reward machines which is based on the set of partial-order plans for a goal. We present an algorithm which computes the set of partial-order plans for a planning task, and give a construction for synthesising a maximally permissive reward machine (MPRM) from a set of partial-order plans. We prove that the expected discounted future reward of optimal policies learned using an MPRM is greater than or equal to that obtained from optimal policies learned using a RM synthesised from any single partial-order plan. We introduce a notion of the adequacy of planning domain abstractions, which intuitively characterises when a planning domain captures all the relevant features of an MDP, and prove that the expected reward of an optimal policy learned using an MPRM synthesised from a goal-adequate planning domain is the same as that of an optimal policy for the underlying MDP. Finally, we evaluate MPRMs using three tasks in the CRAFTWORLD environment [1] used in [11, 12], and show that the agent obtains higher reward than with RMs based either on a single partial-order plan or on a single sequential plan."}, {"title": "2 Preliminaries", "content": "In this section, we provide formal preliminaries for both reinforcement learning and planning.\n2.1 Reinforcement Learning\nIn RL the model in which agents act and learn is generally assumed to be a Markov Decision Process (MDP) $\\mathcal{M} = (S, A, r, p, \\gamma)$, where $S$ is the set of states, $A$ is the set of actions, $r: S\\times A\\times S \\rightarrow \\mathbb{R}$ is the reward function, $p : S \\times A \\rightarrow \\Delta(S)$ is the transition function, and $\\gamma \\in [0, 1]$ is the discount factor. It is assumed that the agent does not have access to the model in which it acts, i.e., $r$ and $p$ are hidden to it. The agent's goal in RL is to learn a policy $\\rho: S \\rightarrow \\Delta(A)$, i.e., a map from each state of the MDP to a probability distribution over the set of actions. In particular, we are mostly interested in so-called \"optimal policies\", i.e., policies that maximise the expected discounted future reward from any state $s \\in S$:\n$\\rho^* = \\arg \\max_{\\rho} \\sum_{s \\in S} v_{\\rho}(s)$\nwhere $v_{\\rho}(s)$ is the \u201cvalue function\", i.e., the expected discounted future reward obtained from state $s$ by following policy $\\rho$:\n$v_{\\rho}(s) = \\mathbb{E}_{\\rho}[\\sum_{t=0}^{\\infty} \\gamma^t r_t | S_0 = s]$\nwhere $r_t$ is the reward obtained at timestep $t$.\nAs the MDP's dynamics and reward are hidden, the agent is supposed to learn a policy by trial and error. This is achieved by the agent taking an \"exploratory\" action $a$ in a state $s$, and observing which state $s'$ (sampled from $p(s, a)$) is reached and the reward $r' = r(s,a,s')$ that is obtained. By collecting these experiences $(s, a, s', r') \\in S\\times A\\times S \\times \\mathbb{R}$, or \"samples\", the agent can learn a policy $\\rho$ via RL algorithms, such as Q-learning [25].\n2.2 Labelled MDPS\nAs in this work we assume the presence of planning domains and reward machines, we also assume that we are given a so-called \"labelling function\" $L$. This function will be the link between the low-level MDP, in which agents learn how to act, and the high-level planning domain and reward machine, which describe how agents can achieve a task using high-level symbols and actions.\nDefinition 2.1 (Labelled MDP). Let $P$ be a set of propositional symbols. Then, a labelled MDP is a tuple $\\mathcal{M} = (S, A, r, p, \\gamma, L)$, where $S, A, r, p$ and $\\gamma$ are as in an MDP, and $L : S\\rightarrow 2^P$ is the labelling function, mapping each state of the MDP to a set of propositional symbols.\n2.3 Reward Machines\nReward machines [23] are a tool recently introduced in the RL literature to define non-Markovian reward functions via finite state automata. Let $\\mathcal{M} = (S, A, r, p, \\gamma, L)$ be a labelled MDP for some set of propositional symbols $P$.\nDefinition 2.2 (Reward Machine). A reward machine (RM) is a tuple $R = (U, u_0, \\Sigma, \\delta_u, \\delta_r)$, where $U$ is the set of states of the RM, $u_0$ is the initial state, $\\Sigma \\subseteq 2^P$ is the input alphabet, $\\delta_u : U \\times \\Sigma \\rightarrow U$ is the state transition function, and $\\delta_r : U \\times U \\rightarrow \\mathbb{R}$ is the reward transition function.\nWhen using RMs, training is usually done over the so-called \"product\" between the labelled MDP and the RM, also known as a \"Markov Decision Process with a Reward Machine\" (MDPRM) [23].\nDefinition 2.3 (MDPRM). A Markov Decision Process with a Reward Machine (MDPRM) is a tuple $\\mathcal{M}^R = (S, A, p, \\gamma, L, U, u_0, \\delta_u, \\delta_r)$, where $S, A, p, \\gamma, L$ are as in the definition of a labelled MDP, and $U, u_0, \\Sigma, \\delta_u$ and $\\delta_r$ are as in the definition of a reward machine.\nAt each timestep, the RM is in some state $u$. As the agent moves the MDP into state $s'$, the RM updates its internal state via the observation $L(s')$, i.e., the new RM state is $u' = \\delta_u(u, L(s'))$. Accordingly, the RM also outputs the reward $\\delta_r(u, u')$, which is the reward the agent obtains. As in \"vanilla\" MDPs, the agent learns a policy by taking exploratory actions and collecting rewards from the RM's reward function $\\delta_r$. Thus, samples include also the states of the RM, i.e., each sample is a tuple $(s, u, a, s', u', r') \\in S \\times U \\times \\mathbb{R} \\times S \\times U$. For this reason, any RL algorithm that works with standard MDPS can also be used in MDPRMs. Moreover, algorithms exploiting access to the RM have also been proposed, e.g., CRM [23].\n2.4 Symbolic Planning\nA planning domain $D = (F, A)$, is a pair where $F \\subseteq P$ is a set of fluents (propositions), and $A$ is a set of planning actions. Planning states are subsets $S \\subseteq F$, where a proposition is in $S$ if and only if it is true in $S$. Actions $a \\in A$ are tuples $a = (pre^+, pre^-, eff^+, eff^-)$ such that each element of $a$ is a subset of $F$, $pre^+ \\cap pre^- = \\emptyset$ and $eff^+ \\cap eff^- = \\emptyset$. The \u201cpre\u201d sets are the sets of \u201cpreconditions\u201d, whereas the \"eff\u201d are the sets of \u201ceffects\u201d, or \u201cpostconditions\". $pre^+$ are the propositions that must be true to perform the action, whereas $pre^-$ those that must be false. Analogously, $eff^+$ are the propositions that are made true by the action, whereas $eff^-$ those that are made false. Thus, an action $a$ can be executed from a planning state $S$ if and only if $pre^+ \\subseteq S$ and $pre^- \\cap S = \\emptyset$. Executing action $a$ in state $S$ results in the new state $S' = (S \\setminus eff^-) \\cup eff^+$. Given an MDP $\\mathcal{M}$ and a planning domain $D$, we assume that $A \\cap A = \\emptyset$, i.e., the planning actions are not the same as the actions the agent can perform in the MDP. Intuitively, the planning actions can be seen as high-level or abstract actions which correspond to sequences of actions in the MDP. For example, in a Minecraft-like scenario where the agent can move up, down, left, and right on a grid, a planning action might be \"get wood\" corresponding to a sequence of movement actions ending in a cell containing wood.\nAs an example of a planning domain, consider the CRAFTWORLD environment [1] in which an agent moves in a grid and has to gather resources which can then be used to produce items. For example, the agent can build a bridge in order to reach the gold ore. We assume the agent can build two different types of bridge: an iron bridge or a rope bridge. The iron bridge requires gathering wood and iron and then processing them in a factory. The rope bridge requires gathering grass and wood and processing them in a toolshed. The corresponding planning domain $D$ can be formalised as:\n$(F = {has-wood, has-grass, has-iron, has-bridge},\\ A = {get-wood, get-grass, get-iron, use-factory, use-toolshed})$"}, {"title": "3 Maximally Permissive Reward Machines", "content": "In this section, we show how the set of all partial-order plans, $\\Pi$, for a planning task $T = (D,S_I, G)$, can be used to synthesise a reward machine $R$ that is maximally permissive, i.e., which allows the agent maximum flexibility in learning a policy.\nLet $\\mathbb{I}$ be the set of all linearisations of all the partial-order plans in $\\Pi$. We denote by $pref(\\pi)$ the set of all proper prefixes (of arbitrary length) of $\\pi \\in \\mathbb{I}$. Note that the prefixes are finite, as the set of actions in $\\mathbb{I}$ is finite. Then, let $states(pref(\\pi))$ be the set of sequences of planning states that is induced by the prefixes in $pref(\\pi)$, assuming that the initial planning state is $S_I$. We denote with $steps(\\pi)$ the set of actions in a sequential plan, and with $post(A')$ the set containing $post(a)$, as defined in Section 2.4, for each planning action $a \\in A'$. For an arbitrary sequence of planning states $u$, we denote with $last(u)$ the last element of the sequence. For sets of literals $P$, we, respectively, denote with $P^+$ and $P^-$ the sets of propositional symbols with positive and negative literals in $P$.\nConstruction 1 (Maximally Permissive Reward Machine (MPRM)). Fix the set of all partial-order plans $\\Pi = {\\pi_1,..., \\pi_n}$ for some planning task $T = (D,S_I, G)$. Then, $R_{\\Pi}$, the maximally permissive RM corresponding to $\\Pi$, is defined as follows:\n$\\bullet U = [\\cup_{\\pi \\in \\Pi} states(pref (\\pi))] \\cup {u_g};$\n$\\bullet u_0 = [S_I];$\n$\\bullet \\Sigma = \\cup_{\\pi \\in \\Pi} post(steps(\\pi));$\n$\\bullet \\delta_u(u, P) = uS$, where $S = (last(u) \\setminus P^-) \\cup P^+$ and $uS \\in states(pref (\\pi))$ for some linearisation $\\pi \\in \\Pi$, or $= u_g$ if $G^+ \\subseteq S$ and $G^- \\cap S = \\emptyset$;\n$\\bullet \\delta_r(u, u') = \\begin{cases} 0 & \\text{if } u' = u_g\\\\ -1 & \\text{otherwise}. \\end{cases}$\nIn the construction of the MPRM, the set of states correspond to the set of all possible prefixes of planning states across all POPs in the set used to build the reward machine. Then, the RM transitions from a state $u$ to a state $u' = uS$ when it observes the set of propositional symbols $P$ which are exactly the conditions such that $S = (last(u) \\setminus P^-) \\cup P^+$ and, most importantly, $uS$ is a prefix of some linearisation of a POP in $\\Pi$. As soon as the RM \"reaches\" a sequence of states such that the last state is a goal state for the task (meaning also that a linearisation has been \"completed\"), it gives a reward of 0 to the agent and terminates in state $u_g$, while for all other transitions the agent gets a reward of -1. Note that if $uS$ is not the prefix of any linearisation of a POP in $\\Pi$, or $S$ is not a goal state, then $\\delta_u(u, P) = u$.\nIn the remainder of this section, we provide a theoretical analysis linking the optimal policies that can be learned by an agent depending on the kind of reward machine it is equipped with. We consider RMs that can be built from the set of all partial-order plans (RM-$\\Pi$), a single partial-order plan (RM-$\\pi$), and a single sequential plan (RM-$\\\\\\pi'$) over the same planning domain $D$. All RMs issue a non-negative reward only in the final state. We denote the optimal policy learnt using the set of all partial-order plans by $\\rho_{RM-\\Pi}$, using a single partial-order plan by $\\rho_{RM-\\pi}$, and using a single sequential plan by $\\rho_{RM-\\\\\\\\\\pi'}$.\nTheorem 3.1. Let $\\mathcal{M}$ be a labelled MDP, $D$ a planning domain over $\\mathcal{M}$, and $RM-\\Pi$, $RM-\\pi$ and $RM-\\\\\\pi'$ final state reward machines generated from $D$ for the same task. Then,\n$\\rho_{RM-\\Pi} \\geq \\rho_{RM-\\pi} \\geq \\rho_{RM-\\\\\\\\\\pi'}$\nwhere $p_1 \\geq p_2$ if and only if $v(\\rho_1(s)) \\geq v(\\rho_2(s))$ for all states $s \\in S$ of $\\mathcal{M}$.\nProof. The proof follows from the fact that any policy that can be learned using an RM synthesised from a single sequential plan can also be learned using an RM synthesised from a partial-order plan which has the sequential plan as its linearisation (if it remains an optimal policy). Similarly, a policy learned using an RM synthesised from a single partial-order plan can also be learned using an RM that is synthesised from the set of all partial-order plans.\nTheorem 3.1 shows that MPRMs allow an agent to learn an optimal policy with respect to the planning domain and the planning task. A natural question to ask is whether it learns a goal-optimal policy $\\rho^*$, i.e., a policy that achieves the goal using the smallest number of actions in the underlying MDP. For example, an agent using Q-learning is guaranteed to learn a goal-optimal policy on an MDP where the agent is always given the same negative reward and the discount factor $\\gamma$ is exactly 1."}, {"title": "4 Empirical Evaluation", "content": "In this section, we evaluate maximally permissive reward machines in three tasks in the CRAFTWORLD environment, and show that the agent obtains higher reward with an MPRM than with RMs based on a single partial-order plan or a single sequential plan. In the first task, the agent has to build a bridge, as in the example in Section 2.4 For the second task, the agent has to collect gold. In the third task, the agent has to collect gold or a gem, and the task is considered achieved when the agent collects at least one of the two items. For the gold-or-gem task we have to slightly modify the definition of goal states in planning tasks: the goal is the pair $G = (G^+ = {has-gold, has-gem},G^- = \\emptyset)$, and a planning state $S$ is a goal state if and only if $G^+ \\cap S \\neq \\emptyset$ and $G^- \\cap S = \\emptyset$. The gold and the gem are collected as described in [1]: gold is collected by using a(ny) bridge, whereas the gem is collected using an axe. To produce an axe, the agent must combine a stick, which can be obtained by processing wood at the workbench, with iron at the toolshed. We refer to these, respectively, as the \"bridge task\u201d, \u201cgold task\", and \"gold-or-gem task\". In the planning domain, we add the following fluents:\n$\\bullet$ For the gold task: has-gold;\n$\\bullet$ For the gold-or-gem task: has-gold, has-stick, has-axe, has-gem;\nand the following planning actions:\n$\\bullet$ For the gold task:\n$\\quad$ get-gold, with one positive precondition, has-bridge, and one positive postcondition, has-gold;\n$\\bullet$ For the gold-or-gem task:\n$\\quad$ use-workbench, with one positive precondition, has-wood, one positive postcondition, has-stick, and one negative postcondition, has-wood;\n$\\quad$ use-toolshed-for-axe, with positive preconditions, has-stick and has-iron, one positive postcondition, has-axe, and two negative postcondition, has-stick and has-iron;\nThe maximally permissive RMs for each task were synthesised using the construction given in Section 3. The RMs for each partial-order and sequential plan were generated using the approach presented in [11]. Training is carried out by using Q-learning over the resulting MDPRMS [23].\nFor each task we generated 10 different maps of size 41 by 41 cells. The maps and initial locations were chosen so that from some locations a task can be completed more quickly by following a particular sequential plan. For example, in the first map for the bridge task, if the agent starts from a location in the upper half of the map (i.e., in the first 20 rows) it is more convenient to build an iron bridge, while in the lower half of the map it is more convenient to build a rope bridge. The MDP reward function r returns - 1 for each step taken by the agent, until it achieves the task or the episode terminates. When the task is completed, the map and agent are \"re-initialised\": the agent is placed on a random starting cell and its \"inventory\" is emptied, i.e., it contains no items. For each set of plans and single partial-order/sequential plan for a task, and for each of the 10 maps for the task, an agent was trained with the corresponding RM for 10,000,000 training steps. Training was carried out in episodes, lasting at most 1,000 steps, after which the environment was re-initialised regardless of whether the agent has achieved the task or not. Every 10,000 training steps the agent was evaluated on the same map used for training from 5 (predetermined) starting positions. We set the learning rate $\\alpha = 0.95$, the discount rate $\\gamma = 1$, and the exploration rate $\\varepsilon = 0.1$."}, {"title": "5 Related Work", "content": "Reward machines have been used both in single-agent RL [21, 6, 4, 7] and multi-agent RL [17, 10]. As mentioned in the Introduction, approaches to synthesise reward machines from high-level specifications have also been proposed; however, to the best of our knowledge, only [11, 12] and our work generate reward machines from plans.\nAnother line of research focuses on learning reward machines from experience. In [22] an approach is proposed that uses Tabu search to update the RM hypothesis, by searching through the trace data generated by the agent exploring the environment. [27] presented an approach where the RM hypothesis is updated whenever traces that are inconsistent with the current one are detected. In [9] RM-learning is reduced to SAT solving, and solved using DPLL. While these approaches do not require an abstract model of the environment in the form of a planning domain, they focus on learning a reward machine for a single task. In [22] and [27] the agent learns a policy for each state of the RM hypothesis. However, when the latter is updated, it has to re-learn such policies from scratch ([27] tries to mitigate this issue by transferring a subset of the policies, but this is not always possible). Moreover, all these approaches assume that the agent is able to generate \"positive\" traces, i.e., traces in which the task is achieved. While in simple environments this is a reasonable assumption, for more complex environments with sparse rewards it may be difficult to generate positive traces.\nPlanning has been applied to reinforcement learning since at least [8], which combined Q-learning with STRIPS planning. More recently, [28] proposed an approach integrating planning with options [20, 5]. In [15] a framework is introduced that exploits planning to improve sample efficiency in deep RL. In both of these approaches the RL experience is then used to improve the planning domain, similarly to what happens in model-based RL. Then, the new plan obtained using the updated domain is used to train again the RL agent. In [19, 13, 26] abstract models for the low-level MDP and/or its actions are learned so that planning can be leveraged to improve learning. However, all of these approaches assume that learning is guided by a single (sequential) plan."}, {"title": "6 Conclusions", "content": "We have proposed a new planning-based approach to synthesising maximally permissive reward machines which uses the set of partial-order plans for a goal rather than a single sequential or partial-order plan as in previous work. Planning-based approaches have the advantage that it is straightforward to train agents to achieve new tasks \u2014 given a planning domain, we can automatically generate a reward machine for a new task. We have provided theoretical results showing how agents trained using maximally permissive reward machines learn policies that are at least as good as those learned by agents trained with a reward machine built from an individual sequential or partial-order plan, and the expected reward of an optimal policy learned using an MPRM synthesised from a goal-adequate planning domain is the same as that of an optimal policy for the underlying MDP. Experimental results from three different tasks in the CRAFTWORLD environment suggest that these theoretical results apply in practice. However, our results also show that agents trained with maximally permissive RMS converge more slowly than agents trained using RMs based on a single plan. We believe this is because the increased flexibility of maximally permissive RMs trades solution quality for sample complexity. Our approach is therefore most useful when the quality of the resulting policy is paramount.\nA limitation of our approach is that, in the worst case, the set of all partial order plans for a task may be exponential in the number of actions in the planning domain. In future work we would like to investigate the use of top-k planning techniques, e.g., [14], to sample a diverse subset of the set of all plans. Intuitively, such an approach could allow the quality of the resulting policy to be traded off against the number of plans in the sample.\nAnother line of future work is to investigate option-based approaches to learning [20, 5] as in [12], where each abstract action in a plan is \"implemented\" as an option. We expect results similar to the ones in this paper, where the agent trained with all partial-order plans is able to achieve a better policy but converging slower.\nFinally, the experiments in Section 4 are limited to discrete environments. However, our approach is applicable to environments with continuous action and state spaces. Reward machines have previously been successfully applied in such environments [23, 7], and planning domains, which form the basis our approach, are agnostic about the underlying environment, as they are defined in terms of states resulting from (sequences of) MDP actions rather than the actions themselves. Nevertheless, learning in continuous environments is more challenging than learning in discrete ones, and evaluating the benefits of our approach in such environments is future work."}]}