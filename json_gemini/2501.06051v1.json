{"title": "Benchmarking Rotary Position Embeddings for Automatic Speech Recognition", "authors": ["Shucong Zhang", "Titouan Parcollet", "Rogier van Dalen", "Sourav Bhattacharya"], "abstract": "Rotary Position Embedding (RoPE) encodes relative and absolute positional information in Transformer-based models through rotation matrices applied to input vectors within sequences. While RoPE has demonstrated superior performance compared to other positional embedding technologies in natural language processing tasks, its effectiveness in speech processing applications remains understudied. In this work, we conduct a comprehensive evaluation of RoPE across diverse automatic speech recognition (ASR) tasks. Our experimental results demonstrate that for ASR tasks, RoPE consistently achieves lower error rates compared to the currently widely used relative positional embedding. To facilitate further research, we release the implementation and all experimental recipes through the SpeechBrain toolkit.\nIndex Terms: speech recognition, rotary position embedding", "sections": [{"title": "1. Introduction", "content": "Transformer [1] models have demonstrated impressive results for natural language processing (NLP) and speech processing tasks. These models rely on positional embedding technologies to learn the positional information of input vectors within the input sequence, and these methods plays a crucial role in Transformer models' performance. In particular, Relative Positional Embedding (RelPOS) [2] has emerged as one of the most widely used methods. RelPOS enables Transformer models to learn the relative positional relationships between input vectors in the sequence. Widely adopted open-source toolkits from the speech processing community including SpeechBrain [3, 4], ESPnet [5], and NeMo [6], rely on RelPOS as their default positional embedding method for Transformer models.\nIn the NLP community, Rotary Position Embedding (ROPE) [7] has been proposed as an alternative positional embedding technology to RelPOS. ROPE rotates each vector composing the input sequence according to its position, thereby encoding absolute positional information through this rotation process. Through the dot product in the self-attention operation, Transformer models can then derive relative positional information between these rotated input vectors. As a result, ROPE has demonstrated superior performance compared to RelPOS in NLP tasks. It also has been used for state-of-the-art NLP models, such as the Llama family [8].\nHowever, ROPE has not been thoroughly studied for speech processing tasks yet. While [9, 10] have compared ROPE with RelPOS, these studies were limited to only one or two automatic speech recognition (ASR) tasks. In this short study, we conduct a comprehensive benchmark comparing RoPE against RelPOS across diverse ASR tasks. Our experiments cover four different languages, including both reading and spontaneous speech,"}, {"title": "2. Rotary positional embedding", "content": "Rotary positional embedding (RoPE) [7] rotates each input vector according to its position in the input sequence. This section will briefly describe how ROPE works for Transformer-based models. Suppose $x_{1:T}$ is an input sequence of a Transformer-based model, and $T$ is the sequence length of the input sequence. Then, the Transformer-based model will transformer the input sequence $x_{1:T}$ to a value sequence $v_{1:T}$ and a key sequence $k_{1:T}$. Suppose each vector in the key and value sequence has a dimension $d$, where $d$ is an even number. For a key vector $k_t$ or value vector $v_t$ at position $t$ of the sequence, ROPE will rotate the vector with a rotation matrix $R_t \\in \\mathbb{R}^{d \\times d}$. The rotation matrix $R_t$ can be described as:\n$R = \\begin{bmatrix}\nR_{t1} & 0 & 0 & ... & 0 \\\\\n0 & R_{t2} & 0 & ... & 0 \\\\\n0 & 0 & ... & ... & ... \\\\\n: & : & ... & ... & : \\\\\n0 & 0 & ... & ... & R_{t\\frac{d}{2}}\n\\end{bmatrix}$\n$R_{ti} = \\begin{bmatrix}\ncos\\;t\\theta_i & -sin\\;t\\theta_i \\\\\nsin\\;t\\theta_i & cos\\;t\\theta_i\n\\end{bmatrix}$\n(1)\nwhere $\\theta_i = 10000^{-\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]$.\nDue to the sparsity of the rotation matrix, rotating any vector $a \\in \\mathbb{R}^{d}$ with the rotation matrix $R_t$ can be reformulated for computational efficiency as:\n$R_t a = \\begin{bmatrix}\na_1 \\\\\na_2 \\\\\na_3 \\\\\na_4 \\\\\n:\\\\\na_{d-1} \\\\\na_d\n\\end{bmatrix} \\bigodot \\begin{bmatrix}\ncos\\;t\\theta_1 \\\\\ncos\\;t\\theta_1 \\\\\ncos\\;t\\theta_2 \\\\\ncos\\;t\\theta_2 \\\\\n:\\\\\ncos\\;t\\theta_{\\frac{d}{2}} \\\\\ncos\\;t\\theta_{\\frac{d}{2}}\n\\end{bmatrix} + \\begin{bmatrix}\n-a_2 \\\\\na_1 \\\\\n-a_4 \\\\\na_3 \\\\\n:\\\\\n-a_d \\\\\na_{d-1}\n\\end{bmatrix} \\bigodot \\begin{bmatrix}\nsin\\;t\\theta_1 \\\\\nsin\\;t\\theta_1 \\\\\nsin\\;t\\theta_2 \\\\\nsin\\;t\\theta_2 \\\\\n:\\\\\nsin\\;t\\theta_{\\frac{d}{2}} \\\\\nsin\\;t\\theta_{\\frac{d}{2}}\n\\end{bmatrix}$\n(2)"}, {"title": "3. Experimental Results", "content": "In this section, we compare ROPE and RelPOS for speech recognition across multiple datasets and model architectures. For English ASR, we use the LibriSpeech [11] and Libriheavy [12] datasets, while for non-English ASR, we employ Common Voice 18.0 [13] in Italian, French, and Dutch. Additionally, we include Voxpopuli [14] to evaluate ASR performance on more spontaneous speech.\nExperimental details. For offline ASR tasks, we implement Conformer [15] encoder-decoder models. These models contain approximately 110M parameters, except for those trained on LibriHeavy, which are scaled up to about 160M parameters to accommodate the larger dataset. For unified offline and online streaming ASR, we employ Conformer Transducer models with approximately 80M parameters. The Conformer Transducer models are trained with dynamic chunk training (DCT) [16], a technique that segments the input sequence into random-length independent chunks during training. The chunk size dynamically varies from a single frame to the full sequence length, enabling the model to adapt to both offline decoding and online streaming scenarios with flexible chunk sizes during inference. We use NVIDIA Tesla A100 GPUs for the Libriheavy experiments, while NVIDIA Tesla A40 GPUs are employed for all other experiments. We maintain identical experimental configurations between all experiments with RoPE and all experiments with RelPOS, with one exception: the Conformer model with ROPE for Libriheavy requires an extended warmup period. We found that using the same warmup steps as the RelPOS model led to training divergence on Libriheavy, while extending the warmup period for RelPOS did not yield improved performance. All experiments are conducted using the SpeechBrain toolkit. The exhaustive list of hyperparameters can be found on the released recipes 1."}, {"title": "4. Conclusion", "content": "In this short study, we benchmark Rotary Position Embedding (RoPE) for automatic speech recognition using Conformer encoder-decoder and Conformer Transducer models. Experiments encompass multiple languages (English, French, Italian, and Dutch), diverse training data volumes (ranging from 100 to 50,000 hours), and various speech types. The results demonstrate that Conformer models equipped with RoPE consistently match or outperform those using RelPOS across both streaming and non-streaming scenarios, indicating RoPE's broad effectiveness for ASR tasks. To facilitate further research in this area, we release our implementation and training recipes with SpeechBrain."}]}