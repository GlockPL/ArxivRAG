{"title": "CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs", "authors": ["Jingzhe Shi", "Jialuo Li", "Qinwei Ma", "Zaiwen Yang", "Huan Ma", "Lei Li"], "abstract": "Businesses and software platforms are increasingly utilizing Large Language Models (LLMs) like GPT-3.5, GPT-4, GLM-3, and LLaMa-2 as chat assistants with file access or as reasoning agents for custom service. Current LLM-based customer service models exhibit limited integration with customer profiles and lack operational capabilities, while existing API integrations prioritize diversity over precision and error avoidance which are crucial in real-world scenarios for Customer Service. We propose an LLMs agent called CHOPS (CHat with customer Profile in existing System) that: (1) efficiently utilizes existing databases or systems to access user information or interact with these systems based on existing guidance; (2) provides accurate and reasonable responses or executing required operations in the system while avoiding harmful operations; and (3) leverages the combination of small and large LLMs together to provide satisfying performance while having decent inference cost. We introduce a practical dataset, CPHOS-dataset, including a database, some guiding files, and QA pairs collected from CPHOS, which employs an online platform to facilitate the organization of simulated Physics Olympiads for high school teachers and students. We conduct extensive experiments to validate the performance of our proposed CHOPS architecture using the CPHOS-dataset, aiming to demonstrate how LLMs can enhance or serve as alternatives to human customer service. Our code and dataset will be open sourced soon.", "sections": [{"title": "1 Introduction", "content": "In most organizations with human customer service, a system usually stores customer information. Responses are based on the customer's profile, like user type and purchase history, following set guidelines. Customer service can also update the customer's status upon request. Large language models (LLMs), such as GPT-3.5 (OpenAI, 2022),GPT-4.0 (OpenAI, 2023), GLM (Zeng et al., 2022; Du et al., 2022)) and LLaMa(Touvron et al., 2023), have emerged as a representative achievement of AI development in the past decade. With their mastering of common knowledge and their ability to understand prompts and generate contextually relevant answers, LLMs have been used as assistants across a wide range of application scenarios, including chatting assistants, coding assistants, automatic assistant agents, etc(Kalla & Smith, 2023).\nA common paradigm for equipping LMs with external knowledge while avoiding long context length that are either costly or technically hard is RAG(Lewis et al., 2021), represented by the widely used and efficient Vector Database using a sentence embedding model such as the Universal Sentence Encoder(Cer et al., 2018). Most publicly available architectures for utilizing LLMs as customer service mainly follow this paradigm, e.g. Databricks(Databricks) enables users to upload guiding files thus building a Vector Database-based customer service agent to augment human customer service. However, for most software platforms or businesses, to answer based on a series of guides is not enough: to query information or to manipulate the system using a set of APIs is necessary in some scenarios."}, {"title": "2 Related Works", "content": "Retrieval-Augmented Generation with LLMs. Incorporating external knowledge sources into LLMs for enhanced performance on knowledge-intensive tasks has seen advancements through Retrieval-Augmented Generation (RAG), with the use of vector-based databases for PDF files being a notable example Tripathi (2023). This approach encodes user queries into vectors, using k-nearest neighbors (KNN) to retrieve relevant information. In customer service, LLMs augmented with large databases have aimed to provide encyclopedic support for user inquiries dat (2022); Wulf (2022); cms (2022). However, such methods sometimes struggle in scenarios requiring modifications to a user's profile within existing systems, a crucial aspect of customer service. This highlights the importance of integrating LLMs with software systems for direct interaction tasks, essential for operational efficiency in businesses.\nLLM Agents. The research on LLMs as specialized agents is an evolving field in artificial intelligence and natural language processing. Initial research focused on using predefined prompts or fine-tuning to enhance LLMs for specific tasks, establishing their potential in specialized applications like natural language understanding. Recent studies (Hu et al., 2023a; Doe & Smith, 2023) explore LLMs in agent-based architectures for complex problem solving, such as mathematical puzzles, highlighting the importance of architectural design"}, {"title": "3 CPHOS-dataset:A real-scene dataset for customer service", "content": "The CPHOS-dataset is collected from an online platform of CPHOS, a non-profit organization dedicated to holding Simulated Physics Olympiads online through the online platform as Figure 2 shown."}, {"title": "3.1 Database", "content": "The online system of the Simulated Olympiad utilizes a MySQL database. We provided 9 data desensitized table. The detailed description can be found in the appendix A.1.\nIn short, given a user's nickname, one can do a series of queries on tables in the database to obtain or modify partially the profile of the user. The most important field of a user profile includes: (0) approved_to_use_online_platform; (1) user_name; (2) school_id; (3) user type: team leader, vise team leader, arbiter; (4) marking_question_id, etc.\nUnlike previous works (Hu et al., 2023a) directly using LLMs to generate SQL commands for query or modification to the database, we wrapped the query and modification into a series of python APIs, following the idea of Repository Pattern in software design. We provide 9 Data Managing APIs and 18 Data Query APIs, 10 of which are available to LLMs. We collect instructions and queries to the system and augment them with GPT-4 into 104 System-related queries and instructions. There are several advantages of wrapping SQL commands and LLMs manipulate database through these APIs, in that:\n\u2022 Properly named APIs are much easier for LLMs to understand and to generate compared to complicated table structure and SQL commands for the database.\n\u2022\nBy limiting APIs LLMs can use, or by checking status inside the APIs, one can prevent unwanted or harmful operations that might be carried out by LLMs in extreme conditions.\n\u2022 In codes of software or websites that are written following the Repository Pattern (or more broadly, the principle of 'encapsulation' in software architecture), a series of pre-defined APIs for database is likely to exist. Much less effort is needed to modify these APIs into APIs suitable for LLMs than to check for correct SQL commands generated by LLMs.\nThe diversity of the apis not only comes from the number of apis. Calling them from a different user and with different arguments would give different results as shown in the middle and right part of Figure 2."}, {"title": "3.2 PDF-based guides", "content": "There are two main guiding files provided by CPHOS: the mini-program guiding file and questions that are commonly asked by users, together with their answers. These QA files, together with the mini-program guiding file, is what we refer to as PDF-based guides. All files are translated by us into English. In practice we merge them into one file for RAG. Full pdf-based guides are appended in the supplementary materials. Collected QA pairs from real scene, we augment them through GPT-4 and repitition into 102 QA pairs on Guide Files, example of which can be shown in the left part of Figure 2."}, {"title": "4 Methods", "content": "Given the previous setup, in Figure 3, we define our task as follows. Given a user's nickname and its question, the task is to give a proper answer or an appropriate execution command to the system, based on the status of the user in the existing system and the guiding files."}, {"title": "4.1 Framework Overview", "content": "We propose a three-agent architecture for the task: the classifier-executor-verifier architecture. For this three-stage architecture:\nC: The Classifier is given the UserTexts, the System API descriptions and several relevant (and short) chunks from the guiding files. The Classifier classifies the UserTexts based on information needed for the following pipeline. The classifier"}, {"title": "4.1.1 Input classifier", "content": "Previous work (Chen et al., 2023) has shown that the longer and more complicated the retrieved content is, the more difficult for the Executor LLM to find the exact piece of information and return it. Also, feeding the Executor everytime with retrieved chunks from guide file and API infos is token-consuming. We utilize a Classifier to classify the information domain that need to be given to the Executor in advance in order to solve this two issues.\nA simple and experimentally-proved effective and efficient design for the Classifier is a binary classifier, or a 1-level Classifier as shown in the left part of Figure 4. This classifier only choose between two categories: (1) that the User Texts are about a query to the guiding file, and (2) that the User Texts are related to a query to the system or an instruction to the system. Non-classifiable cases are dealt with in the same way as (1) in the following pipeline. Note that although we use the same RAG method to retrieve file chunks for the Classifier and the Executor, the chunk length for Classifier is set to be lower so as to save inference cost.\nMoreover, we observe that many queries and questions are about basic information of the user in the system that is much shorter and less token-consuming compared to the retrieved chunks. Inspired by the idea of Cache, we further add one categorized: (0) the 'Basic Info' apart from (1)\u2018Guide File', (2)\u02bbSystem API'. We design a 2-level Classifier Architecture shown in the right part of Figure 4. One classifier will first decide whether the User Texts are solvable only given the Basic Information (without the need of 'Guide Files' and 'System APIs'). If it asserts yes, then the pipeline will go on and the Executor will only see the Basic Information. If not, then it goes to the second level classifier to categorize user texts into class (1) or class (2), and further provide the corresponding information to the Executor. This design also improves accuracy while saving token consumption."}, {"title": "4.1.2 Executor", "content": "The executor needs to return an answer or give an appropriate execution command given information provided. In practice, we do prompt-tuning separately for different cases given by the Classifier."}, {"title": "4.1.3 Verifier", "content": "Previous works (Weng et al. (2023),Zhang et al. (2023)) have proved the effectiveness of a Verifier for re-checking correctness.\nIn our work, the Verifier verifies the result and, if valid, summarize the answer or generate a response to the user based on the executed operation.\nLike the Executor, we do prompt tuning separately for different cases given by the Classifier. For the Guide File cases, we feed the retrieved results to the Verifier as well.\nThe Verifier is required to output valid score (1-10) and reasons at the same time. If the verification result is invalid, then we redo the whole C-E-V process while giving the Classifier and Executor with the invalid reason.\nIn practice to ensure fast response, we restrict the loop iterations into 5. For latter iterations, a lower score would be seen as valid: we use a simple linear scheduling of the passing score with the iteration index. If all iterations fail, we ask the LLM to choose between one answers provided before."}, {"title": "4.2 Tools used", "content": "For PDF Retrieval, we follow the well-practised method of using a sentence encoder to encode chunks from guide files into a vector database and retrieve top-K closest chunks in latent space as related information given by the files. We follow pdfgpt(Tripathi, 2023) and utilize Universal Sentence Encoder (Cer et al., 2018) as the embedding model. For Database manipulation, we use the wrapped Apis as available tools."}, {"title": "5 Experiments", "content": "Our study evaluates the model's performance through several metrics: Instruction Set Accuracy, Guiding File Question Accuracy, and Input/Output character consumed per Question. More details ref to Appendix A.3"}, {"title": "5.1 Metrics", "content": "Our study evaluates the model's performance through several metrics: Instruction Set Accuracy, Guiding File Question Accuracy, and Input/Output character consumed per Question. More details ref to Appendix A.3"}, {"title": "5.2 Main Experiment Results", "content": "We evaluate the proposed CHOPS agent architecture on the CPHOS-dataset. We conduct prompt-tuning on gpt-4 (gpt-4-0125-preview) as our baseline, which is labeled as Executor Only in our result tables. We show our main experiment results in Table 1."}, {"title": "5.3 Ablation Studies", "content": "Starting from the baseline using gpt-3.5-turbo as plain Executor, we add all designed block and ablate the effectiveness of them. Finally, we substitute the Executor backbone with gpt-4 and make a comparison to plain Executor using gpt-4 as another baseline. Detailed experiment results and figure can be seen in Table 2 and Figure 5."}, {"title": "5.3.1 Effectiveness and Efficiency of our proposed classifier-executor-verifier architecture", "content": "Starting from the baseline using gpt-3.5-turbo as plain Executor, we add all designed block and ablate the effectiveness of them. Finally, we substitute the Executor backbone with gpt-4 and make a comparison to plain Executor using gpt-4 as another baseline. Detailed experiment results and figure can be seen in Table 2 and Figure 5."}, {"title": "6 Conclusion", "content": "Targeting the important scenario of Customer Service, we have collected, processed related data and proposed our CPHOS-dataset. Furthermore, we proposed CHOPS-architecture, a Classifier-Executor-Verifier agent architecture that Chat with custOmer Profile in existing Systems, offering a flexible architecture for Customer Service scenarios. Our experiments have shown that this architecture (1) improves accuracy while controlling token consumption, achieving better accuracy compared to naively using state-of-the-art LLMs, (2) provides a flexible architecture to utilize different LLMs for agent tasks with different level of requirements, thus achieving satisfying accuracy with decent cost. However, though this architecture is flexible and is not domain-specific to Customer Service in Olympiad domain and we expect it not hard to apply it to other Customer Service data, more datasets with QA pairs and Database for Customer Service is needed to further evaluate the effectiveness of our CHOPS-architecture. We hope future works may further augment our CPHOS-dataset based on the guide files, database and APIs we provide, or propose larger real-world datasets targeting the scenario of Customer Service."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 A Real-Scene dataset: CPHOS:Cyber Physics Olympiad Simulations", "content": "In the realm of customer service, datasets such as the Customer Support on Twitter dataset Vector (2018), which gathers over 3 million tweets and replies from prominent brands on Twitter, and the Recommender Systems and Personalization Datasets McAuley, which compile a variety of user/item interactions, ratings, and timestamps, have been instrumental. The core task within customer service can be articulated as providing accurate responses or executing specific commands in response to a user's queries or directives. This involves leveraging guiding documents or appropriately utilizing APIs. Prior datasets in file-based question answering have concentrated on reading comprehension tasks, as seen in Rajpurkar et al. (2016) and Joshi et al. (2017). Meanwhile, datasets focusing on API calling, such as Patil et al. (2023), Qin et al. (2023), and Tang et al. (2023), have primarily emphasized the use of extensive API collections for task completion, intricate reasoning, and solving mathematical problems.\nHowever, existing datasets in customer service predominantly lack detailed information about internal guiding documents or systems that can be interacted with. Similarly, datasets dedicated to file QA or API calling seldom address the customer service domain specifically."}, {"title": "A.2 Pricing Estimation", "content": "Price information is obtained from OpenAI in March 2024. The prices are listed in the following Table 5."}, {"title": "A.3 Metrics", "content": "We make a comparison at the character level since different LLMs utilize different tokenization methods. Input character and output characters are separated since generating output tokens is more resource consuming than reading input tokens for LLMs (and are more expensive in terms of API price). For clarity, we define the first two metrics mathematically.\nBoth Instruction Set Accuracy and Guiding File Question Accuracy are measured as:\nAccuracy = \\frac{N_{correct}}{N_{total}} (1)\nWhere:\n\u2022 $N_{correct}$ is the number of questions correctly answered by the model.\n\u2022 $N_{total}$ is the total number of questions and instructions.\nWe validate whether the answer is correct by a combination of GPT4-based evaluation and human verification afterward.\nThis formula encapsulates the model's efficiency in accurately processing and responding to queries based on the instructions provided or the information contained within the guiding"}, {"title": "A.4 More LLM Backbones", "content": "We further do experiments on the robustness of our proposed architecture one different LLMs (GLM-3, llama-2-70b). We find it hard to restrict GLM or LLaMa based verifier to output results in the regulated format, hence we set the Verifier and Classifier into gpt-3.5-turbo and mainly focuses on experimenting the effectiveness of substituting the LLM backbone for the Executor. Please refer to Table 6 for the results.\nAs shown by the experiment, GLM-3 can provide rather decent performance in this case while llama-2-70b-chat shows some difficulty generating answers in a wanted format."}]}