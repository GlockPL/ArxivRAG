{"title": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method", "authors": ["Alexander Kozachinskiy", "Felipe Urrutia", "Hector Jimenez", "Tomasz Steifer", "Germ\u00e1n Pizarro", "Mat\u00edas Fuentes", "Francisco Meza", "Cristian B. Calderon", "Crist\u00f3bal Rojas"], "abstract": "We propose a novel method to evaluate the theoretical limits of Transformers, allowing us to prove the first lower bounds against one-layer softmax Transformers with infinite precision. We establish those bounds for three tasks that require advanced reasoning. The first task, Match3 (Sanford et al., 2023), requires looking at all triples of positions. The second and third tasks address compositionality-based reasoning: one is composition of functions (Peng et al., 2024) and the other is composition of binary relations. We formally prove the inability of one-layer softmax Transformers to solve any of these tasks. In an attempt to overcome these limitations, we introduce Strassen attention and prove that with this mechanism a one-layer Transformer can in principle solve all these tasks. We also show that it enjoys sub-cubic running-time complexity, making it more scalable than similar previously proposed mechanisms, such as higher-order attention (Sanford et al., 2023). To complement our theoretical findings, we experimentally studied Strassen attention and compared it against standard (Vaswani et al, 2017), higher-order attention (Sanford et al., 2023) and triangular attention (Bergen et al. 2021). Our results help to disentangle all these attention mechanisms, highlighting their strengths and limitations. In particular, Strassen attention outperforms standard attention significantly on all the tasks. Altogether, understanding the theoretical limitations can guide research towards scalable attention mechanisms that improve the reasoning abilities of Transformers.", "sections": [{"title": "1 Introduction", "content": "Limitations of Transformers A fundamental question in modern AI is understanding why and under which conditions a given deep network architecture succeeds or fails on a given task. Numerous recent works have shown that tasks requiring compositional reasoning stand out as particularly challenging [6, 26, 17, 16]. Compositionality refers to the ability of composing blocks of knowledge to generate new content or solve new tasks, and is believed to play a major role in the emergence of systematic generalization in language [18, 4]; making the question for these kind of tasks even more relevant.\nTo address this problem, benchmark datasets such as SCAN [15], PCFG [11], CLUTRR [23] or CoGS [13] have been introduced. Moreover, different empirical methods have been developed to test, quantify and compare compositional capabilities [12], as well as to assess the impact of design choices on the performance of Transformers on compositional tasks [19]. While many of these works provide strong empirical evidence"}, {"title": "Lower bounds", "content": "A natural first step to explain why models struggle with some tasks is to study their expressivity whether a given architecture is in-principle capable of solving the task in question. That is, whether there exists a choice of parameters that results in a model computing the function underlying the task solutions. If the answer is positive, then the chances are that the difficulty lies in efficiently learning the appropriate parameters. If the answer is negative, then a formal proof of this fact can be directly related to the poor performance observed in practice. Mathematical results of the latter type are referred to as lower bounds.\nThe first lower bounds were obtained for Transformers using hardmax attention [10]. Instead of computing attention as a convex combination of all input tokens using softmax, one takes a single token where the attention is maximal. Using this simplification, Hahn showed that hardmax Transformers with O(1) layers cannot compute formal languages such as PARITY, MAJORITY, or Dyck-1.\nLower bounds against softmax Transformers have recently been obtained by employing a different proof technique, one based on communication complexity [20]. The idea is to show that a Transformer solving a given task can be used to construct a corresponding communication protocol for a problem whose communication complexity is known. From this, one obtains lower bounds on the size of Transformers capable of solving the task for inputs of a given length. In Peng et al. [20] for example, the authors apply this technique to show that any one-layer softmax Transformer that can compute the composition of two functions must have $\\Omega(n)$ embedding dimension, where n is the size of the input. Crucially, for this conclusion to hold, one must assume that the Transformer works with a relatively low number of precision bits, namely sub-linear in n. The technique has subsequently been applied to show lower bounds for other tasks such as string equality [3] and Match3 [21] (see Section 3.2.2 for a definition)."}, {"title": "Overcoming Transformers limitations", "content": "Equipped with a better understanding of these limitations, the next question is how they can guide research towards the construction of more expressive machines. A key observation is that the standard attention mechanism can only see interactions between pairs of tokens, whereas compositional tasks require to reason about the interaction of three or more tokens simultaneously [2, 21]. Consequently, suitably modified attention mechanisms have been proposed, which would track interactions between more than two tokens. For instance, triangular attention [2] (see Section 2 for a definition) outperforms standard attention in compositional tasks such as CLUTRR [23] or CoGS [13]. Similarly, it has been shown that higher-order tensor attention embedded in a constant size one-layer Transformer can theoretically solve Match3, a task that cannot be solved by the standard attention [21]. However, both these mechanisms suffer from a significant increase in running-time complexity, e.g., order 3 requires cubic-time, limiting its scalability and potentially affecting its practical relevance. In the case of triangular attention, the mechanism works with an adjacency matrix of a graph (in which case it is square-time in the size of the graph). It is possible to produce an adjacency matrix from the sequence of tokens, as already shown in Bergen et al. [2], but this increases the complexity to cubic. The higher-order tensor attention, on top of being cubic-time as well, has been only considered in theoretical work and has not been evaluated empirically yet."}, {"title": "Our contributions", "content": "In this work we aim at addressing these fundamental questions from complementary angles. Our main results can be summarized as follows:\n\u2022 We present a new technique for proving lower bounds\u00b9 We introduce the notion of Splitting VC"}, {"title": "2 Preliminaries", "content": "Throughout the paper, we denote [n] = {1, ..., n} for n \u2208 N. For a set \u2211, we will denote by En the collection of sequences of elements of \u2211 of length n, and by 2* the collection of all finite sequences. We start by briefly recalling the basics of the Transformer architecture and formally defining the attention mechanisms studied in this paper.\nThe main block of the Transformer layer is the attention function, formally defined as a length-preserving function a: (Rd)* \u2192 (Rd)*, where d is the embedding dimension. In this paper, we consider 4 types of attention functions (Figure 1).\nStandard attention [25], receives as input a sequence xi \u2208 Rd,i = 1,...,n and outputs a sequence ai \u2208 Rd, i = 1, ..., n, computed as follows:\n$\\begin{aligned} \\alpha_{i} &= \\sum_{j=1}^{n} a_{i j} v_{j} \\\\ a_{i j} &= \\operatorname{Softmax}_{j}\\left(q_{i} k_{j} / \\sqrt{d}\\right) \\\\ q_{i} &=W^{q} x_{i} \\\\ k_{j} &=W^{k} x_{j} \\\\ v_{j} &=W^{v} x_{j}, \\end{aligned}$ where Wq, Wk, Wv \u2208 Rd\u00d7d\nTriangular attention [2] is defined for n = m\u00b2, with input tokens indexed by pairs (i, j), i, j = 1, ..., m."}, {"title": "3 Lower bounds via Split-VC dimension", "content": "We now introduce the notion of splitting dimension for a boolean function f. Let X be a set and H C {0,1}* be a collection of functions h : X \u2192 {0, 1} which we will refer to as hypothesis class. We say that an hypothesis class H shatters a subset X = {x1,...,xm } \u2286 X if for any Boolean vector C\u2081 . . . Cm \u2208 {0,1}m there exists h\u2208 \u0397 with h(x1) = c1,..., h(xm) = cm. The maximal m for which H shatters some X \u2264 X of cardinality m is called the VC dimension of H [22].\nConsider a function f : \u2211n \u2192 {0,1}, where \u2211 is a finite set of symbols. For a set of positions AC {1, ..., n}, we define a Boolean matrix MA as follows. Its rows will be indexed by all the words w\u00b9 \u2208 \u03a3\u0391 and its columns by the words w\u00b2 \u2208 \u03a3\u0392, where B = {1, ..., n} \\ A. Thus, M\u2081 will be a |\u03a3||4| x |\u03a3|n-|A| Boolean matrix. The value of M at (w\u00b9, w\u00b2) is then defined as\n$M\\left(w^{1}, w^{2}\\right)=f\\left(w^{1} \\oplus w^{2}\\right)$\nwhere w\u00b9 \u2295 w\u00b2 \u2208 \u03a3\" is obtained by merging w\u00b9 and w\u00b2 according to the positions indicated by A and B.\nDefinition 3.1. We define the splitting VC dimension of f: \u2211n \u2192 {0,1}, denoted by split-VC(f), as the maximum over A \u2286 {1, ..., n} of the VC dimension of the set of columns of MA, understood as Boolean functions on the set of rows.\""}, {"title": "3.1 Main Theorem", "content": "In order to state our main Theorem, we first need to specify a way to see a Transformer as computing a given boolean function f: \u2211n \u2192 {0,1}. We assume that an input word w = 01 ... \u03c3\u03b7 is given to the Transformer using n + 1 tokens. The first n tokens are used to encode the n symbols of w, while the (n + 1)-st auxiliary token (initially encoding the empty symbol) is used to encode the output f(w) of the function being computed. More specifically, the output of the Transformer in the auxiliary token has to be a real number yn+1, satisfying f(w) = sign(yn+1). When a Transformer T fulfills this requirement for a given function f, we will say that the Transformer T computes f in an auxiliary token. We can now state our main result.\nTheorem 3.2. Let T be a one-layer standard-attention Transformer and let f : \u2211\" \u2192 {0,1} be a Boolean function. If T computes f in an auxiliary token, then size(T) = split-VC(f)~(1).\nProof. Denote m = split-VC(f). Let A \u2286 {1, ..., n} be such that the VC dimension of the set of columns of MA is m. Denote B = [n] \\ A. Assume for contradiction that there exists a one-layer standard-attention Transformer T that computes f and whose size (that is, embedding dimension, number of attention heads, and the size of the output MLP) is m\u00ba(1).\nConsider any w\u00b9 \u2208 \u03a3\u0391, \u03c9\u00b2 \u2208 \u03a3\u0392 and define w = w\u00b9 \u2295 w\u00b2 = \u03c3\u2081 ...\u03c3\u03b7 \u2208 \u03a3\u03b7. For h = 1, ..., H, observe that the output of the h-th attention head in the (n + 1)-st token (the auxiliary one, where the output of the function is computed), can be written as:\n$a_{n+1}^{(h)}=\\frac{\\alpha^{(h)}\\left(w^{1}\\right)+\\beta^{(h)}\\left(w^{2}\\right)+\\gamma^{(h)}}{\\lambda^{(h)}\\left(w^{1}\\right)+\\mu^{(h)}\\left(w^{2}\\right)+\\nu^{(h)}}$\""}, {"title": "3.2 Applications to three concrete tasks", "content": "We now apply Theorem 3.2 to three different tasks: function composition, Match3 and binary relation composition."}, {"title": "3.2.1 Function Composition", "content": "Introduced in Peng et al. [20], in this task we receive a description of two functions g: [n] \u2192 [n] and h: [n] \u2192 [n], and we are asked the value of h(g(x)) for a given x \u2208 [n]. The task is presented to a Transformer in the form of a sequence of 2n + 1 tokens, divided in three parts. The first two parts encode the values of g and h, and the third part encodes x in a single token, where the output has to be computed. More specifically, the output has to be a real number y2n+1 satisfying |y2n+1 - h(g(x))| < 0.5. That is, with h(g(x)) being the closest integer to Y2n+1.\nTheorem 3.4. Let T be any one-layer standard-attention Transformer with size(T) = n\u00ba(1). Then T cannot solve the function composition task.\nProof. We show that any Transformer that solves this task can be converted into a Transformer computing in the auxiliary token the following Boolean function:\n$\\operatorname{Ind}_{n}:[n]^{n+1} \\rightarrow\\{0,1\\},\\\\ \\operatorname{Ind}_{n}\\left(p_{1}, q_{1}, \\ldots, q_{n}\\right)=\\left\\{\\begin{array}{ll} 1 & \\text { if } q_{p_{1}}=1, \\\\ 0 & \\text { otherwise }, \\end{array}\\right.$\nand that this transformation requires adding just O(1) neurons to the output MLP. Indeed, by fixing x = 1 and setting g(1) = p1, h(1) = q1, ..., h(n) = qn, we obtain that the token with x outputs a real number y with |y - h(g(1))| = |y - qp1| < 0.5. It now remains to change the output to ReLU(1.5 \u2013 y) which will be positive exactly when qp\u2081 = 1.\nThe result now follows from Theorem 3.2 and the following:\nLemma 3.5. split-VC(Indn) \u2265 n."}, {"title": "3.2.2 The Match3 task", "content": "Next, we define the Match3 [n, m] task [21]. It is a sequence-to-sequence task. The input is presented to the Transformer as a sequence of n tokens, encoding a vector of integers (p1,..., pn) \u2208 [m \u2212 1]n. The output is a vector (y1,..., Yn) \u2208 Rn, required to satisfy:\n$\\operatorname{sign}\\left(y_{i}\\right)=\\left\\{\\begin{array}{l} 1 \\text { if } \\exists j, k \\in[n] \\text { s.t. } p_{i}+p_{j}+p_{k} \\equiv 0 \\quad(\\bmod m) \\\\ 0 \\text { otherwise } \\end{array}\\right.$\nNote that we deliberately exclude the value pi = 0 for the input positions. This is to avoid inputs that make the task trivial. Indeed, if pi = 0 for some i, then the output is trivially 1 since we always have pi + pi + Pi = 0.\nTheorem 3.6. Let T be any one-layer standard-attention Transformer with size(T) = n\u00ba(1). Then T cannot solve the Match3[n, m] task."}, {"title": "3.2.3 Binary Relation Composition", "content": "Finally, we define the binary relation composition task. This is a sequence-to-sequence task, where on input we get two Boolean matrices A, B \u2208 {0,1}\u221a\u00f1\u00d7\u221an. The input is presented to a Transformer using n tokens,"}, {"title": "4 Strassen attention An efficient mechanism to solve complex tasks", "content": "Both the third-order mechanism of Sanford et al. [21] and the Strassen mechanism define attention as the interaction between three tokens (i.e., a triplet). The crucial difference is that Strassen attention is computed using pairwise dot-products of vectors in the triplet, while the third-order involves coordinates products of all 3 vectors. This allows us to decompose Strassen attention scores in a way that reduces the whole layer to the product of a constant number of n \u00d7 n matrices. Famously, the n\u00d7n matrix product admits an O(n)-time algorithm for w < 3, with currently best known upper bound on w being 2.372 [7]."}, {"title": "5 Disentangling Strassen from Standard and Triangular attentions", "content": "So far, we have evaluated tasks that are challenging for one-layer standard attention Transformers but (in principle) easy for one-layer Strassen attention Transformers. In this section, we extend our analysis to triangular attention. As a reminder, running the triangular attention mechanism on a general sequence of length n, requires the creation of n\u00b2 tokens. In this regime, the triangular attention running time becomes n\u00b3 However, when the input is already structured as a \u221an \u00d7 \u221an matrix, one can run the triangular attention on it directly, making the running time O(n\u00b3/2). One such task example using structured input is the binary relation composition task. In this case, a one-layer triangular attention can perform this task with one attention head, constant embedding dimension and constant-size output MLP.\nWe devise a variant of the binary relation task that allows us to disentangle the performance of Strassen attention with that of the triangular and standard attentions, namely the quotient binary relation compo-sition task. The latter takes as inputs two Boolean matrices A, B \u2208 {0,1}m\u00d7m and a \u201ccoloring\u201d function col: [m] \u2192 [m], where m = \u221an. There are n = m\u00b2 input tokens, indexed by pairs from [m]\u00b2, with the (i, j)-th token receiving Ai,j, Bi,j and (col(i), col(j)) as inputs. The quotient of the composition B. A by col is a Boolean matrix Bo A/col \u2208 {0,1}m\u00d7m, defined by:\n$\\left(B \\circ A / \\text { col }\\right)_{i j}=\\left\\{\\begin{array}{l} 1 \\exists k_{1}, k_{2} \\in[m] \\text { s.t. } A_{i k_{1}}=B_{k_{2} j}=1, \\\\ \\operatorname{col}\\left(k_{1}\\right)=\\operatorname{col}\\left(k_{2}\\right), \\text { and } k_{1} \\neq k_{2}, \\\\ 0 \\text { otherwise. } \\end{array}\\right.$\nThe task is to output, for all (i, j) \u2208 [m]\u00b2, a real number yij such that (Bo A/col)ij = sign(Yij).\nTo illustrate an instance of this task, imagine that A = B encodes the graph of co-authorship between a set of researchers, and c assigns each researcher its university. Then we have (Bo A/col)ij = 1 if and only if researchers i and j have co-authors from the same university, with the condition that these co-authors must be different people.\nTheorem 5.1. The quotient binary relation composition task is solvable by a one-layer Transformer with 1 Strassen-attention head, constant embedding dimension and constant-size output MLP. At the same time, this task cannot be solved by any one-layer Transformer T with n\u00ba(1) standard-attention heads, n\u00ba(1) triangular-attention heads, n\u00ba(1) embedding dimension, and n\u00ba(1)-size output MLP."}, {"title": "6 Experiments and Results", "content": "In this section, we systematically compare the performance of standard, triangular, third-order and Strassen at-tention in four tasks: (i) Indicator of the 1st coordinate in the Function Composition (with f = g ), (ii) Binary Relation Composition (with A = B), (iii) Match3 (over position-aware permutations) and (iv) Quotient Binary Relation Composition (with B = AT). To obtain a tighter upper bound on the capability of the standard attention, we chose to implement simple special cases of the tasks (see Appendix A for all the details on data generation). To compare these attention mechanisms, we evaluate their performance (accuracy per epoch) and training time (accuracy per minute) on the test sets of these tasks. Furthermore, Appendix B provides a thor-ough analysis in terms of computational performance, evaluating forward pass times, GPU and memory usage. Code for our experiments can be found at https://anonymous.4open.science/r/StrassenAttention-70D8/.\nFigure 3 displays our main experimental results. First, both the Strassen and third-order attentions are the only two mechanisms that present high accuracy levels across all tasks. Note that Strassen attention displays slight advantages on the Function Composition and Match3 tasks (Figure 3a). Second, Strassen attention consistently outperforms third-order attention in training time (to peak performance), across all tasks (Figure"}, {"title": "7 Future directions and limitations", "content": "Our results pave the way for several directions of future research. First, we have introduced a new method to obtain lower bounds for one-layer Transformer, based on splitting the VC dimension. We hope that the method could be applied to other architectures and other tasks involving complex reasoning in more naturalistic settings (e.g., semantic parsing). Second, given the differences observed in learning stability between the third-order and Strassen attentions, the latter seems to be associated with a smoother loss landscape, an hypothesis that needs to be confirmed and studied. Third, Strassen attention can be adapted to incorporate interactions that involve more than three tokens, possibly capturing more complex patterns in data. Yet, practical learnability of such higher-order interactions needs to be assessed. Finally, and related to the previous point, although the main goal of our work was to gain a deeper theoretical understanding of the abilities of the Transformer, our conclusions are limited by using toy tasks. Our next step is to test the"}, {"title": "B Further Experiments", "content": "We evaluate the computational performance of attention mechanisms exclusively on an NVIDIA RTX A6000 GPU. This analysis focuses on three metrics: (a) forward pass time, (b) GPU utilization, and (c) memory utilization. These metrics provide insights into the computational efficiency and hardware constraints associated with different configurations of hidden dimension and input length. Below, we detail the evaluation methodology and the observed limitations in the Figure 4.\nTime We measure the forward pass time as the delta time before and after passing a 1\u00d7 n \u00d7 d random tensor through the attention mechanism, where n is input length and d hidden dimension. We implement this using time.time() function from Python. As we expect, the results indicate that forward pass time increases significantly for higher hidden dimensions and input lengths.\nGPU Usage We monitor GPU utilization using the pynvml library to query the current CUDA device. Specific configurations of attention mechanisms, such as Strassen attention with large hidden dimensions (e.g., 512 or higher) and long input lengths (e.g., 1600 or higher), result in 100% GPU usage.\nMemory Reserved We record the memory reserved during tensor processing using CUDA memory management functions from PyTorch: torch.cuda.memory_reserved.\nOur computational performance results show that (Figure 4):\n\u2022 The computational constraints of Strassen attention become evident for hidden dimensions of 512 or higher and input lengths exceeding 1600.\n\u2022 Those of Third-Order attention appear with even lower hidden dimensions (64 or 128) and input lengths exceeding 1600.\n\u2022 Standard and Triangular (n =Nodes\u00d7Nodes) GPU memory usage does not shows a bottleneck even for configurations with hidden dimensions of 2048 and input lengths of 16,384."}]}