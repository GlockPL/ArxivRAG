{"title": "Grammar Induction from Visual, Speech and Text", "authors": ["Yu Zhao", "Hao Fei", "Shengqiong Wu", "Meishan Zhang", "Min Zhang", "Tat-seng Chua"], "abstract": "Grammar Induction could benefit from rich heterogeneous signals, such as text, vision, and acoustics. In the process, features from distinct modalities essentially serve complementary roles to each other. With such intuition, this work introduces a novel unsupervised visual-audio-text grammar induction task (named VAT-GI), to induce the constituent grammar trees from parallel images, text, and speech inputs. Inspired by the fact that language grammar natively exists beyond the texts, we argue that the text has not to be the predominant modality in grammar induction. Thus we further introduce a textless setting of VAT-GI, wherein the task solely relies on visual and auditory inputs. To approach the task, we propose a visual-audio-text inside-outside recursive autoencoder (VaTiora) framework, which leverages rich modal-specific and complementary features for effective grammar parsing. Besides, a more challenging benchmark data is constructed to assess the generalization ability of VAT-GI system. Experiments on two benchmark datasets demonstrate that our proposed VaTiora system is more effective in incorporating the various multimodal signals, and also presents new state-of-the-art performance of VAT-GI.", "sections": [{"title": "1. Introduction", "content": "Human language knowledge involves various aspects, such as the vocabulary, phonetics, morphology, syntax, semantics and pragmatics of languages [4, 36, 1, 8].\nAmong these, inferring the underlying grammatical rules and linguistic patterns of a language plays a significant role in language learning. The process is also tasked as grammar induction (GI) [24, 55, 44, 12], aiming to uncover the latent structure of language constituents from natural inputs in an unsupervised manner.\nIn the community, the mainstream explorations on GI pay an extensive focus on the textual signals [7, 23, 48, 3, 17, 22, 58, 45], as texts are the major medium of language. However, it is important to note that human always acquire language knowledge with multimodal signals, i.e., text, vision and acoustic. Knowing this fact, researchers carry out GI using different modalities of information, such as vision-language grammar learning [58, 14, 53, 46, 47], speech-based textless GI [31] and video-aided GI [57, 56, 38, 41].\nUnfortunately, most existing researches on GI tend to overlook the mutually complementary contributions of the information in broad-covering modalities, which together can play crucial roles in the acquisition of language throughout phylogenetic development. This thus motivates us to introduce a novel task of unsupervised visual-audio-text grammar induction (VAT-GI), with the aim of exploring the language learning of NLP related AI systems in realistic scenario. Particularly, VAT-GI operates over parallel images, audios and texts, and extracts the shared latent constituency structures that encompass all these modalities (see \u00a72 for task definition). As depicted in Fig. 1, the terminal nodes of the tree are the individual words, while non-terminal nodes are notional constituent spans, where nodes also associate with visual regions and speech clips. Different sources of modalities together form an integral constituency grammar structure.\nTo effectively facilitate the multimodal grammar induction, a VAT-GI system should take into full consideration the distinct yet complementary characteristics of each modality:\n\u2022 Text: Language intrinsically encompasses structural information by virtue of its inherent compositionality [4, 34, 43], thereby providing a straightforward basis for grammar induction.\n\u2022 Vision: The smaller spatial regions of pixels combine to form larger regions with higher-level visual semantics [14, 58, 53], which correspond to the hierarchical structure observed among textual constituent spans.\n\u2022 Audio: Speech conveys structural information through various intonation and rhythm patterns, naturally depicting the discontinuity and continuity of phonemes [40, 49, 2, 35, 13].\nWith the aforementioned observations, in this work we present a nichetargeting framework to approach VAT-GI. Built upon the deep inside-outside recursive autoencoder (Diora) model [7] for effective text-based GI, we devise a novel visual- audio-text Diora (namely VaTiora), which extends the Diora with the further capability of capturing specific structural characteristics of three modalities of sources, such as the compositionality of texts, the hierarchical spatial regions of images and the rhythm patterns of speech, and integrating them properly. As shown in Fig. 3, first, rich modal-specific and complementary features, such as text embedding, visual regions and voice pitch, etc., are constructed from the input text, image and audio sources, respectively, all of which are effectively integrated into the inside-outside recursive autoencoding. Then, in the feature interaction phase, text embeddings are first fused with audio features via cross-attention to obtain the token-wise text-audio representations for constructing the span vectors; these span vectors are further mapped with fine-grained visual region features by an attention-aware fusion, thereby enhancing the span representations. Moreover, the composition probabilities of visual regions are considered when calculating the span scores, during which the intonation pitch frequency features and voice activity features (representing the rhythm pattern) are also incorporated. Through such cohesive integration, the cross-modal signals mutually reinforce each other for more accurate constituency parsing in the inside-outside pass of VaTiora. We follow [7] to train the VaTiora with structure reconstruction and contrastive learning. In addition, we further introduce a cosine similarity learning objective across three types of features for representation alignment, bridging the heterogeneity among modalities. Additionally, we contend that text is not a fundamental modality for VAT-GI, as supported by the physical fact that language grammar natively exists beyond the texts. Hence, we introduce a textless setting for VAT-GI, wherein the task solely relies on visual and auditory inputs, and the constituent tree is no longer structured around individual words but instead the segmented speech clips. Accordingly, we specially propose the aligned span-clip F1 metric (SCF1) to measure the textless GI, due to the inaccessibility of golden word segments of speeches in this setting.\nVAT-GI can be evaluated on the public SpokenCOCO dataset [16] with well- aligned images, texts and audio speeches. However, SpokenCOCO has inherent limitations in terms of monotonous styles of visual scenes and speech tones, as well as short captions, which largely weakens the model generalization ability to complex signal inputs. To combat that, we present a more challenging test set for VAT-GI, namely SpokenStory, which advances in richer visual scenes, timbre-rich audio and deeper constituent structures. We collect 1,000 aligned image-text from Localized Narratives [42], and further record speech for each sentence by human speakers. The experimental results on the datasets suggest that leveraging multiple modality features helps better grammar induction. Further analyses demonstrate that VaTiora is effective in incorporating the various multimodal signals, and yielding new state-of-the-art induction performance of VAT-GI task.\nIn summary, this paper contributes in five key aspects.\n\u2022 We for the first time introduce an important yet challenging task, unsupervised visual-audio-text grammar induction, to emulate the phylogenetic language acquisition;\n\u2022 We present a novel VAT-GI framework, VaTiora, to properly navigate multi- modal input sources;\n\u2022 We also propose a new metric for the textless VAT-GI evaluation;\n\u2022 We newly contribute a challenging benchmark data, SpokenStory, for VAT-GI;\n\u2022 New state-of-the-art performance on the benchmarks are shown. Our re- sources will be open to facilitate the community."}, {"title": "2. Task Definition", "content": "Formulation. Given a sentence X = {x\u2081, x\u2082, ..., x\u2099} with n words, an associated image I and a piece of speech S for X, the goal of unsupervised VAT-GI is to induce a constituency tree structure T from X, I and S without any supervision of constituent structure annotations for training. As shown in Fig. 1, the output tree structure T is formed in a phrase constituent tree by Chomsky Normal Form (CNF) [5], where each non-terminal node in the tree has exactly two children. Each node in T contains a text span (i, j), 1 \u2264 i \u2264 j \u2264 n, a region box b\u1d62,\u2c7c \u2208 \u211d\u2074 in I and a speech clip c\u1d62,\u2c7c \u2208 \u211d\u00b2, where the text span (i, j) is grounded to the region box b\u1d62,\u2c7c and aligned to the speech clip c\u1d62,\u2c7c. For the textless setting, the inputs of VAT-GI are only I and S. Thus the node set of output T contains only c\u1d62,\u2c7c and b\u1d62,\u2c7c.\nEvaluation. We adopt two widely-used metrics, averaged corpus-level F1 (Corpus- F1) and averaged sentence-level F1 (Sent-F1), following [23]. Corpus-level F1 calculates precision/recall at the corpus level to obtain F1, while sentence-level F1 calculates F1 for each sentence and averages across the corpus. In this paper, we follow [23] discard trivial spans (length < 2) and evaluate on sentence-level F1 per recent work. In textless settings, where there is no text script for speech-to-text alignment, we introduce a new metric, aligned span-clip F1 (SCF1), to measure the quality of the textless constituency tree. SCF1 first aligns predicted speech clips to ground-truth speech clips via temporal IoU [37]. We use a greedy mapping from start to end, aligning predicted clip and ground-truth segmented clip if their tIoU is over a predefined threshold p. The tIoU [37] is a metric to measure the overlap ratio on the temporal dimension. If a ground-truth clip has multiple mapped predictions,"}, {"title": "3. Methodology", "content": "We propose a novel visual-audio-text inside-outside recursive auto- encoder (dubbed VaTiora) framework based on the Diora model [7]. As shown in Fig. 3, VaTiora performs grammar induction with two key modules: the multimodal feature extraction and the inside-outside recursive autoencoding.\n3.1. Feature Extraction\nIn the first step, we extract the modal-preserving features of each input modal from various perspectives, which are summarized in Table 1.\nTextual Features. For text inputs, we follow the conventional GI methods, taking pre-trained word embeddings ELMo [39] to obtain the textual representations W = {W\u2081, ..., W\u2099} from the input X."}, {"title": "3.2. Inside-Outside Recursive Autoencoder", "content": "Similar to Diora, VaTiora operates as an autoencoder, encoding the sentence into span representations and then reconstructing the bottom words of the con- stituent tree, i.e., the terminal nodes. The encoding process involves mapping the input words, denoted as X, to a latent constituent tree. To efficiently explore all valid trees, a dynamic programming-based inside-outside algorithm [32] is employed. Notably, VaTiora incorporates external multi-modal cues during the inside-outside pass, thereby extending the functionality of the Diora framework. Fig. 3 provides an overview of the interaction of features and the inside-outside pass.\nFeature Interaction. We first initialize the bottom-most terminal nodes h\u1d62,\u1d62^{in} with the aligned word embedding W = {w\u1d62} and speech clip representation R = {r\u1d62} via cross-attention:\n{h\u1d62,\u1d62^{in}} = CrossAttn(W, R).\nSimilar to Diora, the VaTiora maintains a N \u00d7 N chart T to store intermediate span vectors and scores, i.e., h\u1d62,\u2c7c^{in}, h\u1d62,\u2c7c^{out} and s\u1d62,\u2c7c^{in}, s\u1d62,\u2c7c^{out} for inside and outside representation respectively. We further fuse the visual and speech features into span vector h\u1d62,\u2c7c^{in} and score s\u1d62,\u2c7c^{in}. First, for each span (i, j) and its decomposition (i, k, j), we enhance h\u1d62,\u2c7c^{in} by:\nh\u1d62,\u2c7c^{in} = h\u1d62,\u2c7c^{in} + \u03b3u\u1d62,\u2c7c + \u03bbp\u1d62,\u2c7c,\nh\u1d62,\u2c7c,\u2096^{in} = MLP(h\u1d62,\u2096^{in}; h\u2096\u208a\u2081,\u2c7c^{in}),\nwhere 1 < i < k < j \u2264 N, and (; ) denotes the concatenation. \u03b3 and \u03bb are the fusion weights. MLP is the composition function to merge two spans (see Diora [7]). Then h\u1d62,\u2c7c^{in} could be weighted sum of h\u1d62,\u2c7c,\u2096^{in} u\u1d62,\u2c7c and p\u1d62,\u2c7c are the visual and\nIn the textless settings, we initialize h\u1d62,\u2c7c^{in} with only segmented speech clips r\u1d62, i.e., h\u1d62,\u1d62^{in} = Norm(r\u1d62)."}, {"title": "3.3. Overall Training", "content": "Structure Reconstruction and Contrastive Learning. Following [53, 7], we adopt structure reconstruction and contrastive learning For reconstruction, a self- supervised blank-filling objective is defined as:\nL_{rec} = - \\frac{1}{n} \\sum_{i} log P(x_{i}|h_{i,i}^{out}).\nFor contrastive learning, we randomly select unpaired image I' and span (i, j)' as negative samples within a training batch, and calculate the contrastive objective:\nl_{span}(I, i, j) = max{0, d(I, (i, j)') \u2013 d(I, (i, j)) + \u03f5}\n + max{0, d(I', (i, j)) \u2013 d(I, (i, j)) + \u03f5},\nd(I, (i, j)) =sim((i, j), I) \u00d7 q(i, j), i \u2260 j,\nsim(i, j, I) = \\underset{m \u2208 [0,M]}{max} {v_{m}(h_{i,j}^{in} + h_{i,j}^{out})},\nwhere \u03f5 is the positive margin. For bottom-most (i, i), e.g., the word w\u1d62 (or the speech clip c\u1d62 in textless setting), we compute:\nl_{word}(I, i) = -log \\frac{exp(sim(i, I))}{\\sum_{\\hat{I} \\in batch} exp(sim(i, \\hat{I}))},\nwhere sim(i, I) = \\underset{\\hat{I} \\in batch}{max} \\underset{m \u2208 [0,M]}{max} {v_{m}h_{i,i}}\nThe final contrastive loss will be:\nL_{el} = \\sum_{i,j,i \\ne j} l_{span}(I, i, j) + \\sum_{i} l_{word}(I, i).\nRepresentation Learning. Furthermore, we propose to use a representation learning objective to align the vectors of multi-modal inputs in the feature space:\nL_{rep} = Cos(r_{i}, w_{i}) + Cos(Avg({r_{i}}), Avg({v_{m}})),"}, {"title": "4. Novel Dataset", "content": "VAT-GI can be evaluated on the SpokenCOCO dataset [16], which contains approximately 600,000 speech recordings of MSCOCO image captions\u00b2. Neverthe- less, SpokenCOCO can fall prey to limitations in terms of its visual scenario styles, short captions, and monotonous speech tone, resulting in inferior generalization performance of VAT-GI parser across diverse language environments. Thus, we introduce a more challenging test set, namely SpokenStory. The dataset is built by extending the image caption data with speech records. We comprises 1,000 images extracted from OpenImages [30] across various scenarios, and accompanied by corresponding caption annotations from [42]. OpenImages is widely used for computer vision research and applications, which contains over 9 million labeled images sourced from the web and covers more than 6,000 topics and categories. Each image is annotated with labels that describe the objects or concepts present in the image. We adopt the annotations of Localized Narratives [42] for aligned"}, {"title": "5. Experiments", "content": "5.1. Settings\nFollowing [59], we use the Faster-RCNN model to detect object regions and extract visual features of the image. We follow [53] that use ELMo [9] for text embedding. The ground-truth constituent structures are parsed with the Benepar [25]. We use VG-HuBert [37] model for the speech encoding and word segmenta- tion, which is pre-trained on unlabeled speech-image pairs. For the inside-outside recursive autoencoder, we follow [7] to use an MLP as the composition function for both inside and outside passes. We compare our VaToria with current state- of-the-art methods on two kinds of settings: 1) text-only grammar induction. 2) Visual-text grammar induction on the same dataset, so that we could explicitly explore the influence of different modalities. More implementation details and hyper-parameters are shown in Appendix \u00a7Appendix D.\n5.2. Main Results\nAs shown in Table 3, we compare our VAT-GI with two settings, i.e., text-only and visual-text. When comparing the PCFG-based methods (i.e., C-PCFG and 5.3. Module Ablation\nWe display the quantified contributions of each component of VaTiora in Table 5. The results reveal that speech features and visual features contribute 2.63 and 1.98 corpus-F1, respectively, with speech exhibiting a more significant impact. More concretely, we conduct ablations on fine-grained speech and vision features 5.4. Analysis and Discussions\nWe now take one step further, exploring the task and the proposed method from various angles, so as to gain a deeper understanding."}, {"title": "6. Related Work", "content": "Grammar induction [32] has long been a fundamental topic in natural language processing (NLP), especially the text-based GI [6, 48, 27, 17, 22, 58, 14, 53, 46]. GI aims to unsupervisedly determine the constituent syntax of a text, in the format of a phrase-structure tree, which intuitively depicts the language compositionality, i.e., how the small components (e.g., words or phrases) combine to larger constituents (e.g., clauses or sentences). Such a process essentially emulates the human language acquisition.\nText Grammar Induction. The majority of previous GI studies pay the focus on the language domain with textual corpora [6, 32, 48, 27, 17, 22]. Generally, there are two types of frameworks for text-only grammar induction. The first is PCFG-based methods, which assume the context-free grammar rules exist with a probability and estimates the likelihood of each latent rule [23, 29, 54]. One way of inducing this probabilistic grammar is to fix the structure of a grammar and then find the set of optimal parameters such that the resulting grammar best explains the language, which is usually approximated by a training corpus. Early works Visual-text Grammar Induction. The visually-grounded GI has received in- creasing research attention [58, 14, 53, 46]. Since visual regions (e.g., objects of interest) intuitively encompass the correspondence of the textual spans (e.g., noun phrases), the visions are imported as a type of enhanced signal for better constituency tree parsing. Shi et al. [46] first propose to import visual features into grammar induction, mapping visual and text via visual-semantic embedding. The following works extend text-only GI models with extra visual information, such as VC-PCFG [58] and Cliora [53]. There are also video-aided GI, where, compared with a single image, video frames can further facilitate the understanding of motion dynamics for the correspondence of textual predicates [57, 56, 38, 41].\nSpeech Based Grammar Induction. On the other hand, acquiring languages from speech has also gained consistent research interests [26, 18, 19, 11, 12], where the acoustic-prosodic features (e.g., phonetics, phonology) can offer important clues for the syntax induction from different perspectives than the visions and texts [2, 35, 13]. In this work, we take a combined holistic viewpoint, and investigate the GI under multimodal information sources, i.e., introducing a novel visual-audio- text grammar induction task. This can be quite intuitive, as we humans always perceive the world with varied sensory inputs that can partially share the same common structures and meanwhile preserve distinctly complementary features, which together help achieve more effective GI."}, {"title": "7. Conclusion", "content": "In this paper, we introduce a novel task, visual-audio-text grammar induction (VAT-GI), unsupervisedly inducing the constituent trees from aligned images, text, and speech inputs. First, we propose a visual-audio-text inside-outside recursive autoencoder (VaTiora) to approach the VAT-GI task, in which the rich modal- specific and complementary features are fully leveraged and effectively integrated. To support the textless setting for VAT-GI, we newly devise an aligned span-clip F1 metric (SCF1), which helps conveniently measure the textless constituent structures of the aligned sequences. Further, we construct a new test set SpokenStory, where the data characterizes richer visual scenes, timbre-rich audio and deeper constituent structures, posing new challenges for VAT-GI. Experiments on two benchmark datasets indicate that leveraging multiple modality features helps better grammar induction, and also our proposed VaTiora system is more effective in incorporating the various multimodal signals for stronger performance."}, {"title": "Appendix A. Broad Impact", "content": "Appendix A.1. Limitation\nIn this work, we propose the VAT-GI task that leverages the multi-modal information from image, speech and text to enhance grammar induction. VAT-GI explores the phylogenetic language acquiring process from the machine learning perspective. Our proposed VaTiora can properly integrate multi-modal structure features into the grammar parsing, providing a beginning attempt for VAT-GI. We further discuss the limitations. First, in VaTiora, the multi-modal feature extraction and grammar parsing is conducted in a pipeline way, which may import noise information due to the limitation of upstream algorithms. Second, the textless setting of VAT-GI is not fully explored, which may reveal the core mechanism of language acquisition and help enhance the current language models.\nAppendix A.2. Ethical Statement\nWe construct a small test datasets SpokenStory for VAT-GI. All the images are chosen from the publicly available OpenImage and do not contain sensitive and harmful information. All the annotators are from our research team and have the freedom to choose whether to partake in the study. Moreover, the entire annotation task is conducted anonymously, with no connection to any private information of the annotators. We have not made any changes to the annotation results. Overall, the establishment of our datasets is compliant with ethics.\nAppendix B. Future Work\nAs future work of VAT-GI, the following aspects are worth exploring:\n1) End-to-end multi-modal feature integration. Studying the method to extract multi-modal features from inputs in an end-to-end manner. Currently, the feature extraction in VaTiora tackles three types of inputs, i.e., text, image and speech, respectively and fuse them through a rough vector combination. In this way, the feature quality depends on the design of upstream algorithms, which have inherent limitations. A better way to take an end-to-end architecture is to extract the final feature from multiple inputs, where the fusion is processed in an internally implicit way, such that the noise can be avoided.\n2) Embedding the GI into LLMs for enhanced compositionality. Exploring enhancing the large language models (LLMs), and multi-modal LLMs, with the strategy of grammar induction, to help them obtain better performance in multi- modal alignment and strengthen the compositionality ability of language and other modalities."}, {"title": "Appendix C. Extended Technical Details", "content": "Appendix C.1. Feature Extraction Details\nIn Table 1 we introduced the various features used in our framework for VAT-GI. Here we extend the details on feature crafting.\nText Feature.. For text inputs, we use ELMo to encode a sentence to the word-level embeddings. In practice, we can also adopt other pre-trained word embeddings, such as Glove, or use randomly initialized embedding. We report the comparison of the text embedding options in Appendix \u00a7Appendix E.\nObject Feature.. For visual inputs, we adopt the Faster-RCNN to extract M object proposals {o\u2081, ..., o\u2098 } for each image, along with their labels and bounding boxes. The object feature v\u2098 is the flattened RoI representation of object o\u2098.\nLocation Feature.. We further embed the object bounding box b\u2098 to the high dimension representation l\u2098, suppose:\nb\u2098 = (x_{min}, y_{min}, x_{max}, y_{max})\nwith the top-left as the origin point, and we normalize it to:\nb'_{m} = (x_{min}/W, y_{min}/h, x_{max}/W, y_{max}/h),\nwhere w, h is the width and length of the image. The we embed b'_{m} by:\nl_{m} = FC(b_{m}).\nWe consider the pair features of object regions to determine the likelihood of object pairs forming a larger region. We use a pair relevance score to represent"}, {"title": "Appendix C.2. Inside-Outside Recursive Autoencoding", "content": "We introduce more about the details of the inside-outside recursive autoencod- ing process here. The model should fill the N \u00d7 N chart T with span vectors and span scores in each item. We denote the h_{i", "cross-attention": "nCrossAttn(W", "as": "h_{i", "j)": "nh_{i", "h_{i,j}^{in}": "nh_{i"}, {}, {"title": "Appendix C.3. Training Objectives", "content": "Structure Reconstruction.. With the bottom-most span vector h_{i,j}^{out}, we take the self-supervised blank-filling objective as:\nL_{rec}= - \\frac{1}{n} \\sum_{i} log P(x_{i}|h_{i,i}^{out}).\nContrastive Learning.. Contrastive learning aims to maximize the matching score between paired RoI-span elements. For a span (i, j) in a caption sentence of the image I, all the other images except I in the current batch are the negative samples. We use the similarity score to represent the semantic similarity between the span (i, j) and the image I:\nsim(i, j, I) = \\underset{m \u2208 [0,M]}{max} {v_{m}(h_{i,j}^{in}+ h_{i,j}^{out})},\nsim(i,i, I) = \\underset{m \u2208 [0,M]}{max} {v_{m}h_{i,i}},\nThen the contrastive objective is:\nl_{span}(I, i, j) = max{0, d(I, (i, j)') \u2013 d(I, (i, j)) + \u03f5}\n+ max{0, d(I', (i, j)) \u2013 d(I, (i, j)) + \u03f5},\nwhere d(I, (i, j)) = sim((i, j), I) \u00d7 q(i, j), i \u2260 j.\nRepresentation Learning.. We propose to carry out representation learning to align the vectors of multi-modal inputs, pulling the representations between clip features and word embeddings, clip features and object features, in the feature space:\n//Full setting\nL_{rep} = Cos(r_{i}, w_{i}) + Cos(Avg({r_{i}}), Avg({v_{m}})),\n||Textless setting\nL_{rep} = Cos(Avg({r_{i}}), Avg({v_{m}})),"}]}