{"title": "Anytime Incremental pPOMDP Planning in Continuous Spaces", "authors": ["Ron Benchetrit", "Idan Lev-Yehudi", "Andrey Zhitnikov", "Vadim Indelman"], "abstract": "Partially Observable Markov Decision Processes (POMDPS) provide a robust framework for decision-making under uncertainty in applications such as autonomous driving and robotic exploration. Their extension, pPOMDPs, introduces belief-dependent rewards, enabling explicit reasoning about uncertainty. Existing online pPOMDP solvers for continuous spaces rely on fixed belief representations, limiting adaptability and refinement critical for tasks such as information-gathering. We present pPOMCPOW, an anytime solver that dynamically refines belief representations, with formal guarantees of improvement over time. To mitigate the high computational cost of updating belief-dependent rewards, we propose a novel incremental computation approach. We demonstrate its effectiveness for common entropy estimators, reducing computational cost by orders of magnitude. Experimental results show that PPOMCPOW outperforms state-of-the-art solvers in both efficiency and solution quality.", "sections": [{"title": "1 Introduction", "content": "Autonomous agents must make informed decisions in the face of uncertainty, stemming from both the environment's dynamics and the agent's perception. Sources of uncertainty include sensor noise, modeling approximations, and stochastic changes in the environment over time. A common framework for addressing these challenges is the Partially Observable Markov Decision Process (POMDP).\nIn POMDPs, decision-making relies on the history of past actions and observations, but storing this history over long trajectories is impractical. Instead, beliefs\u2014probability distributions over unobserved states- serve as sufficient statistics, encoding all necessary information for optimal decision-making [Thrun et al., 2005]. A solution to a POMDP is a policy that maps each belief to an action that maximizes the expected sum of future rewards. However, finding exact solutions is computationally infeasible except for trivial cases [Papadimitriou and Tsitsiklis, 1987], prompting the development of approximate algorithms.\nTree search algorithms are a prominent method for approximating solutions to POMDPs and are the focus of this work. Instead of evaluating all possible belief states an infeasible task due to the immense size of the belief space-these algorithms concentrate on the subset of beliefs that can be reached from the initial belief through a sequence of actions and observations. In the online paradigm, a planner builds a search tree at each time-step to determine an approximately optimal action. Anytime algorithms are particularly valuable in this setting, as they can provide progressively better solutions as computation time permits.\nA POMDP with state-dependent rewards addresses uncertainty only indirectly, limiting its suitability for tasks like uncertainty reduction and information gathering. In contrast, belief-dependent rewards-defined over the state distribution-offer a more intuitive and effective framework for these tasks. For example, in active localization [Burgard et al., 1997], the goal is to minimize uncertainty about the agent's state rather than reaching a specific destination. Belief-dependent rewards have also been applied to problems with sparse rewards, aiding decision-making through reward-shaping techniques [Flaspohler et al., 2019; Fischer and Tas, 2020] or as heuristics for guiding tree search [do Carmo Alves et al., 2023].\nWhen the reward depends on the belief, the problem extends to a pPOMDP [Araya et al., 2010], also known as Belief Space Planning (BSP) [Platt et al., 2010; Van Den Berg et al., 2012; Indelman et al., 2015]. Belief-dependent rewards are often derived from information-theoretic measures such as entropy or information gain. However, calculating these rewards for general continuous distributions is infeasible and relies on costly sample-based methods, including kernel density estimation and particle filtering [Boers et al., 2010]. These methods are computationally expensive, with costs scaling quadratically with the number of samples, and they provide only asymptotic guarantees-requiring a large number of samples to achieve sufficient accuracy.\nIn this paper, we address the challenge of planning in continuous-spaces pPOMDPs. We introduce pPOMCPOW, an anytime online solver for pPOMDPs that incrementally refines belief representations with formal guarantees of improvement over time and efficiently performs incremental computation of belief-dependent rewards. Before detailing our contributions, we review the most relevant prior work."}, {"title": "2 Related Work", "content": "We will first review the most relevant online POMDP solvers, followed by those specifically designed for pPOMDPS."}, {"title": "2.1 State of the art online POMDP solvers", "content": "Online POMDP solvers for large or infinite state spaces typically approximate the belief using a set of state samples, commonly referred to as particles. This particle-based representation is flexible and well-suited for capturing complex, multimodal beliefs [Thrun et al., 2005]. For discussion purposes, these solvers can be broadly categorized into two groups: state simulators and belief simulators. See Figure 1 for a simple illustration.\nState Simulators: State simulators focus on simulating state trajectories directly, incrementally updating visited beliefs with new particles at each visitation. Examples of state simulators include:\nPOMCP [Silver and Veness, 2010], which extends the UCT algorithm [Kocsis and Szepesv\u00e1ri, 2006] to the POMDP framework. DESPOT [Somani et al., 2013] and its successors [Ye et al., 2017; Garg et al., 2019], which use heuristics to guide the search process. POMCPOW [Sunberg and Kochenderfer, 2018], which extends POMCP to continuous action and observation spaces by incorporating progressive widening. LABECOP [Hoerger and Kurniawati, 2021], an algorithm for continuous observation spaces, extracts the belief from scratch for each sampled observation sequence.\nA common trait of these algorithms is that each time a belief node is visited, the belief is updated with additional particles. Intuitively, this approach improves the belief representation in frequently visited nodes, aligning with the exploration-exploitation trade-off.\nBelief Simulators: Belief simulators, on the other hand, treat POMDP belief states as nodes in an equivalent Belief-MDP. Examples include:\nPFT-DPW [Sunberg and Kochenderfer, 2018], which represents each belief node with a fixed number of particles. This makes the approach simple to implement and particularly effective for belief-dependent rewards, as rewards are computed once upon node creation. AdaOPS [Wu et al., 2021], which dynamically adapts the number of particles per belief node and aggregates similar beliefs, achieving competitive results compared to other state-of-the-art solvers.\nA key limitation of belief simulators is their fixed belief representation, which does not improve over time. This inefficiency leads to unpromising regions of the search space receiving the same computational effort as promising ones. Moreover, these algorithms are less flexible when planning times vary. Given ample time, belief simulators can construct dense trees, but belief representations at individual nodes may remain suboptimal. Under time constraints, however, they often produce shallow, sparse trees, as significant computational effort is spent maintaining fixed belief representations rather than effectively exploring the search space."}, {"title": "2.2 PPOMDP Solvers", "content": "Several algorithms have been proposed to address the challenges of online planning in pPOMDPS.\nPFT-DPW [Sunberg and Kochenderfer, 2018] was introduced to accommodate belief-dependent rewards in POMDPs, though it was not demonstrated for this application. Building on PFT-DPW, IPFT [Fischer and Tas, 2020] introduced the concept of reward shaping using information-theoretic rewards. It reinvigorates particles at each traversal of posterior nodes and estimates information-theoretic rewards by a kernel density estimator (KDE).\nAI-FSSS [Barenboim and Indelman, 2022] reduces the computational cost of information-theoretic rewards by aggregating observations, providing bounds on the expected reward and value function to guide the search. Despite this improvement, its approach remains constrained by a fixed observation branching factor and a fixed number of particles per belief node. [Zhitnikov et al., 2024] introduce an adaptive multilevel simplification paradigm for pPOMDPs, which accelerates planning by computing rewards from a smaller subset of particles while bounding the introduced error. While their current implementation builds upon PFT-DPW, future extensions could complement our approach.\nAll the above algorithms belong to the belief-simulators family and share the limitation of fixed belief representations. An exception, closely related to our work, is pPOMCP [Thomas et al., 2021], which extends POMCP to handle belief-dependent rewards by propagating a fixed set of particles from the root instead of simulating a single particle per iteration. Their approach includes variants such as Last-Value-Update (LVU), which use the most recent reward estimates to reduce bias, unlike POMCP's running average.\nHowever, pPOMCP is limited to discrete spaces and re-computes belief-dependent rewards from scratch whenever a belief node is updated. This is costly in general and especially in continuous spaces, where the number of particles in the belief can grow indefinitely. These limitations highlight the need for efficient incremental updates to avoid full recomputation-an issue directly addressed by our approach."}, {"title": "3 Contributions", "content": "To address the limitations of current state-of-the-art PPOMDP solvers, we introduce PPOMCPOW, an anytime solver for PPOMDPs in continuous spaces. Our contributions are as follows:\n\u2022 Algorithm Design: We introduce PPOMCPOW, a novel anytime solver for pPOMDPs in continuous spaces. PPOMCPOW uses state simulations to explore the belief space, focusing computational resources on promising regions. Its design improves belief representations over time, enabling more accurate and efficient computation of belief-dependent rewards.\n\u2022 Theoretical Foundations: We provide a general theoretical result establishing deterministic lower bounds on node visitation counts in online tree search algorithms. This result is applied in PPOMCPOW to guarantee that belief representations improve as a function of time, ensuring progressively refined accuracy with continued planning.\n\u2022 Algorithmic Innovations: We introduce a novel incremental computation framework for belief-dependent rewards, integrated directly into pPOMCPOW. Specifically, we demonstrate how to incrementally compute Shannon entropy and an entropy estimator proposed by [Boers et al., 2010], significantly reducing computational overhead compared to traditional full recomputation approaches.\nAdditionally, we present an incremental update mechanism for the Last-Value-Update (LVU) framework, further reducing computational costs by updating value estimates without full recomputation.\n\u2022 Empirical Validation: We conduct extensive experiments to demonstrate the advantages of pPOMCPOW. The results show superior performance compared to state-of-the-art solvers in terms of computational efficiency, solution quality, and scalability."}, {"title": "4 Background", "content": "This section reviews mathematical formulations and notations used in this work."}, {"title": "4.1 POMDPS", "content": "A partially observable Markov decision process (POMDP) generalizes Markov decision processes (MDPs) to environments where the agent lacks full observability. Formally, a POMDP is defined by the tuple (S, A, O,T, Z, R, \u03b3), where: S, A, and O are the state, action, and observation spaces; T(s'|s, a) is the state transition model; Z(o|a, s') is the observation model; R(s,a,s') is the state-dependent reward function; and \u03b3\u2208 (0, 1) is the discount factor.\nSince the agent cannot directly observe the state, it maintains a belief bt, a probability distribution over states, updated using the history of past actions and observations. We denote the history at time t as ht = (bo, ao, 01, ..., at-1, Ot), and the belief at time t as bt(s) = P(s|ht). The belief is a sufficient statistic for optimal decision-making in POMDPs [Thrun et al., 2005].\nThe agent's objective is to find a policy \u3160 that maps beliefs to actions and maximizes the expected sum of discounted rewards: \u03c0* = arg max \u0395 [\u03a3\u03c4o \u03b3\u207aR(st, \u03c0(bt), St+1)|bo]."}, {"title": "4.2 PPOMDPS", "content": "In this work, we focus on an extension of POMDPs, often referred to as pPOMDPs, where the reward function depends on the belief state. We replace the state-dependent reward R with the belief-dependent reward p, structured as p(b, a, b')\nThe value function under policy \u03c0 is given by:\n$V^{\\pi} (b_0) = \\sum_{t=0}^{\\infty} \\gamma^t \\rho(b_t, \\pi(b_t), b_{t+1})$\nwhere the expectation is over future beliefs. The corresponding action-value function satisfies:\n$Q^{\\pi}(b, a) = E_{b'}[\\rho(b, a, b') + \\gamma V^{\\pi}(b')]$\nFor notational convenience, we may use hao to implicitly encode the relevant data of the belief b' resulting from history h, action a, and observation o, enabling shorthand expressions such as p(hao) and V\" (hao)."}, {"title": "4.3 Online Tree Search", "content": "Solving infinite-horizon POMDPs is computationally intractable. Online tree search approximates the optimal policy by constructing a search tree in real-time, starting from the current belief and exploring future actions and observations up to a predefined depth. However, constructing a full tree is infeasible due to the exponential growth of the search space-commonly known as the curse of history.\nMonte Carlo Tree Search (MCTS) addresses this by iteratively performing four steps: selection, where the tree is traversed using a strategy like UCB [Kocsis and Szepesv\u00e1ri, 2006] to balance exploration and exploitation; expansion, which adds new child nodes to the tree; simulation, where trajectories are simulated from the expanded nodes to estimate values; and backpropagation, which updates statistics along the path from the leaf to the root. This approach builds an asymmetric tree focused on promising regions of the search space and provides anytime solutions.\nPOMCP [Silver and Veness, 2010] extends UCT [Kocsis and Szepesv\u00e1ri, 2006] to POMDPs by representing beliefs as particle sets and propagating state particles through the tree. This enables scalable planning for large discrete POMDPs but struggles in continuous spaces, where each simulation tends to create a new branch, resulting in sparse and shallow trees. POMCPOW [Sunberg and Kochenderfer, 2018] extends POMCP to continuous action and observation spaces using progressive widening, which limits the creation of new branches and allows existing branches to accumulate particles. It also employs a weighted particle filter to mitigate particle degeneracy, ensuring a robust belief representation."}, {"title": "5 PPOMCPOW", "content": "We introduce pPOMCPOW, an online tree search algorithm for solving pPOMDPs with continuous state, action, and observation spaces. PPOMCPOW extends POMCPOW by incorporating belief-dependent rewards and modifying the backpropagation step to adopt the Last-Value-Update (LVU) framework [Thomas et al., 2021], ensuring that only the latest reward estimates are used in value updates."}, {"title": "5.1 Algorithm Overview", "content": "Similar to POMCPOW, PPOMCPOW iteratively constructs a search tree by simulating state trajectories through alternating layers of action and observation nodes. Each iteration begins by sampling a state from the initial belief and traversing the tree using predefined selection strategies, such as progressive widening for continuous spaces. State particles are updated along the trajectory by simulating actions, weighting by the observation model, and resampling to maintain a representative particle distribution. The process continues until either a depth limit is reached or a leaf node is encountered, at which point a rollout is performed to estimate the node's value.\nUnlike POMCPOW, PPOMCPOW supports belief-dependent rewards, requiring modifications to the backpropagation step. Instead of using the classical Monte Carlo running average, which aggregates cumulative state-dependent rewards, pPOMCPOW updates node values based on the most recent estimates from child nodes. While the LVU framework was introduced in [Thomas et al., 2021], PPOMCPOW further differs by implementing an incremental update mechanism for both value and action-value estimators. This mechanism efficiently adjusts estimates without recalculating them from scratch, significantly reducing computational overhead, particularly in continuous spaces. For full derivations of this novel incremental update mechanism, see Appendix A.\nA key conceptual difference is that pPOMCPOW simulation returns explicit value and action-value estimates rather than cumulative rewards along a trajectory. This adjustment is necessary for handling belief-dependent rewards and aligns with the LVU framework's emphasis on using the most recent reward estimates. To reflect these changes, PPOMCPOW restructures the simulation process into two procedures: SimulateV for propagating value estimates and SimulateQ for updating action-value estimates. Algorithm 1 details the framework.\nThe\nActionSelection\nand\nObservationSelection functions are abstracted for flexibility and can incorporate strategies such as the ones presented in [Sunberg and Kochenderfer, 2018].\nTo enhance the accuracy of initial belief-dependent rewards and enable rollouts with belief-dependent rewards, a potential extension involves propagating a \"bag of particles\" a fixed set of particles initialized at the root node and carried through each tree traversal, as suggested in [Thomas et al., 2021]. While promising, this extension lies beyond the scope of this work and is left for future exploration."}, {"title": "5.2 Challenges and Discussion", "content": "The PPOMCPOW algorithm introduces a novel approach for solving pPOMDPs, but two critical aspects require deeper exploration to fully harness its capabilities.\nFirst, belief representation within the search tree is vital for tasks such as information gathering, where accurately modeling uncertainty is essential. While the algorithm accumulates particles in belief nodes based on visitation counts, this does not guarantee adequate representation across the belief tree. Some belief nodes may remain underrepresented, limiting the algorithm's effectiveness. In the next section, we formally analyze this behavior and prove that, under action and observation selection strategies satisfying a specific property, the belief representation of each node improves over time, leading to a more accurate depiction of the belief space.\nSecond, since pPOMCPOW updates each visited belief along the simulated trajectory, belief-dependent rewards must also be updated, posing a significant computational challenge. These rewards are typically non-linear functions of the belief, making efficient updates non-trivial. Recomputing rewards from scratch for every new particle is expensive, particularly in continuous state spaces, where the number of particles grows unbounded. For instance, both KDE and [Boers et al., 2010] entropy estimators scale quadratically with the number of particles. In a subsequent section, we introduce an incremental belief-dependent reward computation, demonstrating how it enables efficient updates for various reward functions."}, {"title": "6 Visitation Count and Belief Refinement", "content": "Online tree search algorithms must balance broad exploration of the search space with refining estimates of existing nodes, a challenge known as the exploration-exploitation trade-off.\nAlgorithms such as UCT and POMCP address this trade-off by adopting the principle of optimism in the face of uncertainty, where actions are assumed to be promising until sufficient evidence suggests otherwise.\nThis challenge is amplified in continuous spaces, where the search tree can grow indefinitely, making it difficult to ensure adequate visitation across all nodes. In fact, it has been shown that in POMCP-DPW [Sunberg and Kochenderfer, 2018], when operating in continuous spaces, posterior nodes in the belief tree are visited only once, severely limiting their representation and hindering uncertainty estimation.\nPOMCPOW mitigates this issue through Progressive Widening, which controls the expansion rate of new child nodes. However, its effect on node visitation has not been formally analyzed, leaving open questions about how well belief representations improve over time.\nTo address this, we introduce the concept of a consistent selection strategy to ensure sufficient node visitations for each node and derive a deterministic lower bound on visitation counts. While broadly applicable to tree search, we demonstrate its use in pPOMCPOW, guaranteeing improved belief representations over time."}, {"title": "6.1 Consistent Selection Strategies", "content": "Let N(v; t) denote the visitation count of node v at the tth iteration of the algorithm. We define a consistent selection strategy as follows:\nDefinition 1 (Consistent Selection Strategy). A selection strategy is consistent if there exist non-decreasing functions f and F, where $\\lim_{n\\to\\infty} F(n) = \\infty$, such that for any node v with N(v;t) \u2265 f(i), the visitation count of its ith child vi satisfies:\n$N(v_i; t) > F(N(v;t))$\nThis definition ensures that once a parent node has been visited sufficiently often, its child nodes are also visited proportionally. The function F guarantees that visitation counts grow over time, enabling all parts of the tree to be explored adequately. In Example 1, we provide specific instances of consistent selection strategies, derived from the work of [Auger et al., 2013]."}, {"title": "6.2 Node Visitation Lower Bound", "content": "Using consistent selection strategies, we establish a deterministic lower bound on the visitation count of each node in the belief tree.\nTheorem 1 (Node Visitation Lower Bound). Assume the action and observation selection strategies are consistent with functions f, F and g, G, respectively. For a belief tree path h, the visitation counts satisfy:\n\u2022 For $h = a_{i_0} o_{j_1} a_{i_1} o_{j_2} ... a_{i_{\\tau-1}} o_{j_{\\tau}}$, with t \u2265 k($i_0$, $i_1$,..., $i_{\\tau-1}$, $j_{\\tau}$):\n$N(h;t) \\geq K_{\\tau}(t) = G \\circ F \\circ... \\circ G(t)$,\n$\\tau$ times\nHere, k(io,..., j+) ensures sufficient initial visitation counts. A more detailed version of this theorem, including the explicit closed-form expression for k and its complete proof, is provided in Appendix B."}, {"title": "6.3 Anytime Belief Refinement in pPOMCPOW", "content": "We now demonstrate how Theorem 1 ensures that belief representations in pPOMCPOW improve over time.\nCorollary 1 (Anytime Belief Refinement). In pPOMCPOW, under consistent action and observation selection strategies, Theorem 1 guarantees that the visitation count of each node increases over time. Consequently, belief representations improve as planning progresses.\nThus, using selection strategies like those in Example 1, PPOMCPOW overcomes the limitations of fixed-particles approaches by guaranteeing progressively refined belief representations, leading to more accurate modeling of the belief space. While this property may or may not hold for other online POMDP solvers in continuous spaces, to the best of our knowledge, this is the first formal proof of such a property."}, {"title": "7 Incremental Reward Computation", "content": "As discussed in previous sections, updating belief-dependent rewards from scratch each time a belief is updated is prohibitively inefficient, particularly in continuous spaces where the number of particles grows indefinitely.\nMost common belief-dependent rewards are information-theoretic and rely on entropy. We demonstrate incremental computation of Shannon entropy and the entropy estimator proposed by [Boers et al., 2010], referred to as the Boers entropy estimator, significantly reducing computational overhead compared to full recomputation.\nThese principles are general and extend to other belief-dependent rewards. For example, [He et al., 2020] proposes an incremental KDE update, which could be applied to KDE-based entropy estimators used in [Fischer and Tas, 2020]. Additionally, they may benefit other algorithms, such as PPOMCP.\nWe represent the belief as a set of particles {si, wi}i=1, where si is a state sample and wi is the normalized weight of the particle. The normalized weight is computed as $w_i = \\frac{\\tilde{w}_i}{\\sum_i \\tilde{w}_i}$, where wi is the unnormalized weight of particle si."}, {"title": "7.1 Incremental Computation of Shannon Entropy", "content": "While Shannon entropy is traditionally defined for discrete distributions, which are not the focus of this work, we present an incremental computation method tailored for particle-based belief representations to illustrate the broader feasibility of incremental computation for belief-dependent rewards.\nThe Shannon entropy of the particle belief is defined as:\n$\\hat{H}(b) = - \\sum_{s_i \\in S} \\hat{w}_i \\log \\hat{w}_i$\nRecomputing the entropy from scratch after introducing new particles incurs a computational cost proportional to the total number of particles, O(N). As the number of particles grows, this cost can become prohibitive. To address this, we use the following factorization:\n$\\hat{H}(b) = - \\sum_{s_j \\in S} \\hat{w}_j \\sum_{s_i \\in S_j} w_i \\log w_i + \\log(\\sum w_i)$,\nWhen a new particle sk is introduced, only $w_{s_k}$ changes, while other weights remain unchanged.\u00b9 By caching the previous entropy H(b) and the sum of weights $\\sum_{s_j \\in S_j} w_j$, the entropy can be incrementally updated in O(1) time, avoiding the need of recalculating from scratch. For detailed derivations of the incremental update formula, see Appendix D."}, {"title": "7.2 Incremental Computation of Boers Entropy Estimator", "content": "While Shannon entropy provides a simple way to capture uncertainty, it is not suitable for continuous state spaces, which are the focus of this work. For a detailed discussion on its limitations in this context, see [Boers et al., 2010]. The Boers entropy estimator is more suitable for this setting and is known to converge to the true entropy under mild assumptions.\nHowever, the computational cost of the Boers entropy estimator scales quadratically with the number of particles, making it impractical to compute from scratch each time the belief is updated. To address this, we present a method to incrementally update the Boers entropy estimator, significantly reducing computational overhead.\nThe Boers entropy estimator is defined as:\n$\\hat{H}(b') = \\log \\Bigg[ \\frac{1}{N} \\sum_{i=1}^N \\sigma(s_i, a, s_i) \\Bigg] - \\sum_{i=1}^N \\log \\Bigg[\\frac{1}{N} \\sum_{j=1}^N T(s_i | s_j, a) \\hat{w}_j \\Bigg] \\Bigg]$\nwhere (') denotes quantities associated with the posterior belief, e.g., s and w represent the state and normalized weight of the ith particle in the posterior belief, respectively. When the beliefs b and b' are updated with a new particle, the affected terms are denoted with a tilde, e.g., $w_i$, $w$, and $c_i$. While all terms in Equation 8 can be updated incrementally, we focus on the incremental update of the last, as it is the computational bottleneck of the Boers entropy estimator.\nThe updated $c_i$ for i = 1,..., N is:\n$\\tilde{c}_i = \\sum_{j=1}^N T(s_i | s_j, a) \\tilde{w}_j = \\sum_{j=1}^N T(s_i | s_j, a) w_j + T(s_i | s_{N+1}, a) w_{N+1} = c_i + T(s_i | s_{N+1}, a) w_{N+1}$.\nWe assume identical state particles are merged and their weights added.\nThus, by caching $c_i$ and the sum of weights and reusing previously computed terms, Equation 9 can be updated in O(1) time for i = 1,..., N. For i = N + 1, the term $c_{N+1}$ must be computed from scratch, incurring O(N) cost. However, this computation is only required once for each new particle. Overall, the Boers entropy estimator can be updated incrementally in O(N) significantly reducing computational cost."}, {"title": "8 Experiments", "content": "We implemented pPOMCPOW in Julia via the POMDPs.jl framework [Egorov et al., 2017], with code on GitHub2.\nWe evaluate it on two benchmark problems: the Continuous 2D Light-Dark problem and the Active Localization problem. We compare its performance against IPFT and PFT-DPW\u00b3, two state-of-the-art pPOMDP solvers, under varying planning time budgets. Performance is measured as the mean return with standard error over 1000 trials.\nNext, we assess the impact of incremental reward computation by comparing planning time across iterations for pPOMCPOW with and without incremental updates.\nDetailed hardware specifications and solver hyperparameters used in the experiments are provided in Appendix E."}, {"title": "8.1 Benchmark Problems", "content": "Both problems share a common structure. The agent operates in a continuous 2D environment with an uncertain initial belief. Several beacons are scattered throughout the environment, and the agent receives noisy relative pose observations from the nearest beacon, where accuracy improves with proximity. The action space consists of movement in eight directions on the unit circle, along with a \"stay\" action that terminates the episode. State transitions are stochastic, and each step incurs a movement cost.\nThe key difference between the two problems lies in their objectives. In the Light-Dark problem, the agent aims to reach a goal region, while in Active Localization, the objective is to minimize uncertainty about its position. For detailed parameters for both problems, see Appendix F.\nContinuous 2D Light-Dark Problem\nIn the 2D Light-Dark problem, the agent's task is to navigate toward a goal, starting with a highly uncertain initial belief. The reward function is sparse, granting a large positive reward upon termination if the agent reaches the goal region and a large penalty otherwise. To reach the goal successfully, the agent must rely on beacons to improve localization. To encourage this behavior, information gain, defined as IG(b, b') = H(b) \u2013 H(b'), is used as reward shaping, benefiting solvers that explicitly handle belief-dependent rewards. We also evaluate POMCPOW without information gain to highlight pPOMCPOW's ability to incorporate belief-dependent rewards. Results are presented in Table 1.\nResults show that pPOMCPOW finds better solutions significantly faster. The comparison with POMCPOW highlights PPOMCPOW's ability to utilize belief-dependent rewards and shows that while these rewards introduce expensive computations, potentially slowing down simulations, they can significantly aid in finding better policies.\nActive Localization Problem\nThe Active Localization problem tasks the agent with minimizing uncertainty about its position. As in the Light-Dark problem, the agent starts with a highly uncertain belief and must use beacon observations for localization. However, unlike Light-Dark, obstacles are scattered throughout the environment, incurring a penalty upon collision, and distant beacons provide more informative observations, making decision-making more challenging. Figure 2 shows simulated trajectories resulted from each solver in this problem. Unlike Light-Dark, the reward is pure information gain, directly driving uncertainty reduction. Results are presented in Table 4.\nInitially, PPOMCPOW is outperformed by PFT-DPW but surpasses it as planning progresses. We attribute this to poor initial belief nodes that refine over time. IPFT lags behind; running the same problem without obstacles suggests they may be the cause (see Appendix G)."}, {"title": "8.2 Effect of Incremental Reward Computation", "content": "To evaluate the advantage of incremental reward computation, we compare the planning time of pPOMCPOW with and without incremental updates in the Continuous 2D Light-Dark problem. Using the same random seed and parameters for both variants ensures identical search tree expansion, isolating the impact of incremental updates on efficiency. Figure 4 presents planning time as a function of iterations. The results reveal a significant performance gap, with full recomputation scaling at a higher order, demonstrating that incremental updates are crucial for pPOMCPOW to be scalable.\nA complexity analysis and a similar comparison with POMCPOW, assessing the cost of belief-dependent rewards, are presented in Appendix H."}, {"title": "9 Conclusions", "content": "In this work, we introduced PPOMCPOW, an online tree search algorithm for solving pPOMDPs in continuous spaces. Its belief representations are dynamic and guaranteed to improve over time, adressing a key limitation of state-of-the-art pPOMDP solvers. Additionally, we introduced incremental reward computation and demonstrated its effectiveness on common entropy estimators, significantly reducing the computational overhead of belief-dependent rewards and enhancing scalability to continuous spaces. Experimental results highlight the necessity of incremental reward computation and validate PPOMCPOW's effectiveness in both solution quality and computational efficiency.\nWhile the convergence of the algorithm remains an open question, [Lim et al., 2023] establish a link between particle count and performance guarantees, emphasizing the importance of improving belief representations over time. This is particularly crucial for belief-dependent rewards, where convergence guarantees are typically provided only in the limit. Theorem 1 takes a key step in this direction by ensuring that beliefs refine over time, but a full convergence analysis is left for future work.\nDespite efficient incremental updates, belief-dependent rewards remain the main computational bottleneck in pPOMDP solvers (Appendix H). Addressing this challenge through approximation, parallelization, or reward function simplification is a key direction for future research."}, {"title": "A Incremental Last Value Update", "content": "[Thomas et al., 2021] introduced the concept of Last-Value Update (LVU) for pPOMDPs, which focuses on updating the value function of belief nodes based on the most recent reward estimates. To the best of our knowledge, [Thomas et al., 2021] does not provide an incremental update mechanism for the value and action-value functions within the LVU framework.\nIn this section, we present an incremental update mechanism for these functions, which is crucial for efficiently handling belief-dependent rewards in pPOMCPOW."}, {"title": "A.1 Incremental Update of the Value Function", "content": "[Thomas et al.", "2021": "introduced value estimates for belief nodes", "h": "n$V(h) = \\frac{1"}, {"Q(ha)": 10, "by": "n$V(h; t) = \\frac{1}{N(h; t)} [Rollout(h) + \\sum N(ha; t) Q(ha; t)", "Q(ha';t)": 12}, {"Q(ha';t)": 13, "N(ha';t)Q(ha';t)": 14}]}