{"title": "Exploring the Alignment Landscape: LLMs and Geometric Deep Models in Protein Representation", "authors": ["Dong Shu", "Bingbing Duan", "Kai Guo", "Kaixiong Zhou", "Jiliang Tang", "Mengnan Du"], "abstract": "Latent representation alignment has become a foundational technique for constructing multimodal large language models (MLLM) by mapping embeddings from different modalities into a shared space, often aligned with the embedding space of large language models (LLMs) to enable effective cross-modal understanding. While preliminary protein-focused MLLMs have emerged, they have predominantly relied on heuristic approaches, lacking a fundamental understanding of optimal alignment practices across representations. In this study, we explore the alignment of multimodal representations between LLMs and Geometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate three state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with four protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines alignment factors from both model and protein perspectives, identifying challenges in current alignment methodologies and proposing strategies to improve the alignment process. Our key findings reveal that GDMs incorporating both graph and 3D structural information align better with LLMs, larger LLMs demonstrate improved alignment capabilities, and protein rarity significantly impacts alignment performance. We also find that increasing GDM embedding dimensions, using two-layer projection heads, and fine-tuning LLMs on protein-specific data substantially enhance alignment quality. These strategies offer potential enhancements to the performance of protein-related multimodal models. Our code and data are available at https://github.com/Tizzzzy/LLM-GDM-alignment.", "sections": [{"title": "1 Introduction", "content": "Recently, multimodal large language models (MLLM) such as GPT-4V [21], Gemini [23], Llama-3.2 [17], LLaVA-1.5 [15], etc., have received increasing attention from the community. A MLLM is an LLM-based model with the ability to receive, reason, and output with multimodal information. This extends beyond just text to potentially include various forms of input and output such as image, audio, video, and other sensory data, allowing the model to process and generate various types of information in an integrated manner. In the realm of protein science, specialized MLLMs such as ProtChatGPT [27], ProteinChat [6] and ProteinGPT [29] have emerged as powerful tools for comprehensive protein analysis. These models can process both protein sequences and structures, integrating this information with vast language understanding capabilities. By combining protein sequence and structure encoders with large language models, these systems can perform complex tasks such as property prediction, structure understanding, and even engage in interactive conversations about protein characteristics. This fusion of modalities allows for a more comprehensive and efficient approach to protein analysis, potentially revolutionizing fields such as drug development and biotechnological advancements."}, {"title": "2 Methods", "content": "The development of protein-focused MLLMs has emerged as a promising direction for comprehensive protein analysis. These MLLMs aim to jointly process and understand both the structural characteristics and textual descriptions of proteins to enable more sophisticated analysis and prediction tasks. Two critical components in building effective protein MLLMs are GDMs, which capture the complex three-dimensional structural information of proteins, and LLMs, which process textual descriptions of protein properties and functions. The performance of these MLLMs heavily depends on how well the representations from GDMs and LLMs are aligned in a shared semantic space. However, current approaches to this alignment often rely on heuristic methods without a thorough understanding of the underlying factors that influence alignment quality.\nOur methodology aims to systematically investigate this alignment challenge through three interconnected components: (1) data preprocessing to standardize different protein representations, (2) representation extraction that preserves both structural and semantic information, and (3) alignment techniques that bridge these different modalities while maintaining biological relevance. The overview of our methodology is shown in Figure 1. Through this approach, we seek to understand what factors contribute to successful alignment between GDM and LLM representations, how different protein properties affect alignment quality, and what strategies can improve alignment performance. This understanding is essential for developing more effective protein-focused MLLMs that can better integrate structural and textual information for advanced protein analysis tasks.\nModels: In this paper, we selected four state-of-the-art GDMs specialized in the protein domain, as well as three pre-trained Large Language Models (LLMs). Each model has a distinct embedding dimensionality, adding diversity to our experiments. For GDMs, we used the following models: GAT with an embedding size of 64 [26], ScanNet with an embedding size of 128 [25], GVP with"}, {"title": "2.1 Step I: Data Preprocessing", "content": "Our approach begins with the collection of protein data, denoted as $\\mathcal{D}$. We ensure that each protein sample, $d_i \\in \\mathcal{D}$, contains both a FASTA file and a PDB file."}, {"title": "2.1.1 FASTA Preprocessing", "content": "Since the selected LLMs are pre-trained on general text data without specific knowledge of proteins, we convert the FASTA file into a clear and detailed text description that the LLMs can understand. This is shown in Figure 1 Data Preprocessing upper part. The process requires stripping unnecessary information and retaining only the essential content. A typical FASTA file contains several key details: protein ID, chains, molecule name, organism, and the amino acid sequence. While we consider most of this information important, the amino acid sequence itself is often lengthy and may introduce noise, so we exclude it from the text description and instead include the sequence length. Additionally, the FASTA file allows us to determine whether a protein consists of a single chain or multiple chains. For single-chain proteins, we use regular expressions to extract relevant details. However, for multi-chain proteins, the complexity increases, and regular expressions alone often fail to capture all information. In such cases, we use GPT-40 to generate accurate descriptions, extracting the chains and corresponding organisms. The GPT-40 prompt is provided in Appendix A.2.\nAfter processing, each protein is described with text similar to: \u201cThe protein structure [protein_id] has a sequence length of [number] amino acids. Here is more information: The protein structure [protein_id] involves the following chains: [chains]. The protein is named [protein_name] and is derived from the organism [organism].\" We manually validated the GPT-generated descriptions for all multi-chain proteins and found them to be accurate compared to the original FASTA files. Examples of protein descriptions are shown in Appendix A.3."}, {"title": "2.1.2 PDB Preprocessing", "content": "PDB file preprocessing is more complex as it involves constructing appropriate graph structures for different GDM models. This is shown in Figure 1 Data Preprocessing lower part. For GearNet and ScanNet, both models accept PDB files directly and handle preprocessing internally, so no additional preprocessing is required on our end. For the GAT model, the node feature and graph structure was parsed directly from the PDB file, which together form a graph $G = (V, E)$. Each node $i \\in V$ represents an atom. The graph structure is defined by the edges $(i, j) \\in E$, which indicate the connections between the nodes. For the GVP model, the protein PDB data is converted into a JSON format as shown below:\n[{\n'ID': 'ID',\n\u2018seq': \u2018TQDCSFQHSP...\u2019,\n'coords': [[[74.46, 58.25, -21.65],..]]}.\nEach protein includes three key elements: \u2018ID' refers to the protein's id, 'seq' represents the amino acid sequence, and 'coords' stores the 3D coordinates of each residue in the protein's structure. Each coordinate is represented as a list of three floating-point values corresponding to the x, y, and z positions of the residue."}, {"title": "2.2 Step II: Latent Representation Extraction", "content": "Once the data has been processed, the next step is to feed it into the respective models for latent representation extraction. This process is shown in Figure 1 \u201cRepresentation Extraction\" part."}, {"title": "2.2.1 LLMs Representation", "content": "For all three LLMs, we follow a consistent representation extraction procedure. First, we load the pre-trained models directly from Huggingface. The protein's text description is tokenized using the model's associated tokenizer, converting it into input IDs that can be processed by the LLM. We then pass these tokenized inputs through the model and extract the hidden states from the final layer. Specifically, we select the representation of the last token, as it typically holds the most information from the protein's description, given that it is derived from the final layer, which holds the richest contextual information. It is worth noting that for Gemma2-2B and LLaMa3.1-8B, we utilized the full-weight models. However, due to hardware constraints, LLaMa3.1-70B was loaded with float16 precision to fit within the available GPU memory."}, {"title": "2.2.2 GDMs Representation", "content": "Different from LLMs, each GDM has its own way to extract representation.\n\u2022 GearNet: GearNet encodes protein structure information through a relational graph neural network (GNN) and an edge message-passing mechanism. The input to the model consists of a residue-level relational graph constructed from the 3D structure of the protein, which Gearnet's code will derived from the input PDB file automatically. The detail is shown in Appendix A.4.1. In the end, to obtain the final protein representation, the features from all hidden layers are concatenated, which results in the final protein representation has the shape [1, 3072].\n\u2022 GVP: Previously, we have processed the PDB file into the appropriate format supported by the GVP model, which includes a list of residues and their associated 3D coordinates. The detail of how we feed the data into GVP is shown in Appendix A.4.2. In the end, the representation for each protein has the shape [1, node_size, 148], where node_size corresponds to the number of residues in the protein. Finally, to obtain a fixed-size representation for the entire protein, we apply average pooling across the node dimension. This results in a protein representation of shape [1, 148], which will be stored and used later.\n\u2022 ScanNet: To obtain the protein representation using ScanNet, we start by feeding the raw PDB file, which contains detailed information about the atomic coordinates of the protein. ScanNet will automatically preprocess and operates directly on the structure of the protein as described in the PDB file. The process can be divided into several key steps: parsing the PDB file, constructing atomic and amino acid representations, and passing these through ScanNet's geometric deep learning architecture. The detail is shown in Appendix A.4.3. In the end, the protein representation has the shape of [1, node_size, 128], where node_size corresponds to the number of amino acids in the protein, and each amino acid has a 128-dimensional latent feature vector. Then, we use average pooling to reduce the node_size dimension, resulting with [1, 128] for each protein.\n\u2022 GAT: Previously, we already processed the protein PDB file into the format that GAT supports. Therefore, graph G is fed into a GAT model to compute latent representations. The detail is shown in Appendix A.4.4. In GAT architecture, they use 8 attention heads, with each head computing 8 features, leading to a concatenated output of size 64 for each node, which results in protein's final representation size of [1, node_size, 64]. Then we simply perform average pooling to remove the 'node_size' dimension. This process yields the latent representation of the protein with size of [1, 64], which will be stored and used later."}, {"title": "2.3 Step III: Representation Alignment", "content": "After we extracted protein representation from all models, we can combine these models into 12 model pairs (i.e. one LLM and one GDM). For each model pairs, we train two projection heads, one for each model. All the projection head has the same structure, with one simple linear layer that map the model's embedding dimension (input dimension) to the LLM embedding dimension (output dimension). Then we normalize the output embedding. Therefore, after projection, the projected graph representation will have the same dimension as the projected text representation. This process is shown in Figure 1 Representation Alignment."}, {"title": "2.3.1 Contrastive Loss Function", "content": "To maximize the cosine similarity of positive pairs to 1 and minimize that of negative pairs to 0, we apply a contrastive loss function during training. Specifically, we use a modified version of the InfoNCE loss [20]. Let $g_i$ be the projected graph representation for protein $i$, $t_j$ be the projected text representation for protein $j$, and $sim(g_i, t_j)$ represent the cosine similarity between $g_i$ and $t_j$, adjusted to the range [0, 1] by using $\\frac{sim(g_i,t_j)+1}{2}$. The temperature parameter $\\tau$ is set to 0.2 [19], and the batch size $B$ is 32. For each protein $i$, the positive pair similarity is given by $sim_+ = \\frac{sim(g_i,t_i)+1}{2}$, and the negative pair similarities are $sim_i = \\frac{sim(g_i,t_j)+1}{2}$ for all $j \\neq i$ within the batch. After calculating these similarities, we apply exponentiation and scale them by the temperature $\\tau$:\n$L_{total} = \\frac{1}{B}\\sum_{i=1}^{B} -log(\\frac{exp(\\frac{sim(g_i,t_i)+1}{2\\tau})}{\\sum_{j=1}^{B}exp(\\frac{sim(g_i,t_j)+1}{2\\tau}) + \\sum_{j \\neq i}^{M}\\sum_{l=1}^{N} exp(\\frac{sim(g_i,t_j)+1}{2\\tau})})$\nThis overall loss function averages the loss over all proteins in the batch, effectively encouraging high similarity for positive pairs and low similarity for negative pairs."}, {"title": "2.3.2 Implementation Details", "content": "All projection heads were trained for 40 epochs. To ensure reproducibility, we set the random seed to 42. The learning rate was set to $1 \\times 10^{-3}$, and the batch size was 32. We used Adam as the optimizer and applied model checkpointing to retaining the best projection heads (i.e. only store the model weight when the loss is decreased in validation set). We used four A100 GPUs and one A6000 GPU for hardware support. The A100 GPUs were primarily used for experiments involving the LLaMa3.1-70B model, while the A6000 GPU was sufficient for the remaining experiments."}, {"title": "3 Results", "content": "Our results are organized around six key research questions, which can be organized into three essential perspectives (i.e., Model Perspective, Protein Perspective, and Strategies for Improvement) that comprehensively address the challenges of multimodal alignment in the protein domain. First, the Model Perspective Analysis examines which combinations of LLMs and GDMs achieve optimal alignment and how different model pairs correlate, providing fundamental insights into model selection and architecture design. Second, the Protein Perspective Analysis investigates the characteristics of proteins that influence alignment quality, crucial for understanding the biological factors that affect model performance and identifying potential limitations in current approaches. Finally, the Strategies for Improvement section explores practical methods to enhance alignment performance, including architectural modifications and training techniques, offering concrete solutions to the challenges identified in the previous analyses. Through these three complementary perspectives, we provide a thorough understanding of both the theoretical foundations and practical considerations in protein-focused multimodal alignment."}, {"title": "3.1 Evaluation Metric", "content": "In this section, we define key terms for our evaluation. The projected representation refers to the representation after passing through the projection head, while a positive pair represents two modality representations originating from the same protein, and a negative pair represents representations belonging to different proteins. We also define two types of alignment scores: (1) Alignment score between a protein pair, which is the cosine similarity between a projected graph representation and"}, {"title": "3.2 Model Perspective Analysis", "content": "We report the results in Table 1. The results indicate that, when no training is applied to the model pairs' projection heads shown on the right of the table, directly mapping the projected graph and text representations results in alignment scores close to zero. This suggests that without any learned mapping between the modalities, there is no meaningful alignment between the models. However, after training the projection heads, shown on the left of the table, the alignment performance improves across almost all model pairs, except for those involving GAT, where the alignment scores remain near zero. Notably, model pairs involving ScanNet or GearNet exhibit significantly higher alignment scores compared to other model pairs. This is because, when processing a protein's 3D structure, both ScanNet and GearNet account not only for the graph structure but also for geometric features, such as the angles between 3D edges, atomic-level features, etc. Another important observation is the positive correlation between the size of the LLM and the alignment performance: as the parameter and embedding dimensionality of the LLMs increase (from Gemma2-2B to LLaMa3.1-70B), the alignment scores also improve. This suggests that larger LLMs, with their higher-capacity embedding spaces, capture richer information about the protein, which in turn enhances alignment performance. Thus, controlling the GDM side of the model pair and increasing the dimension of the LLM leads to better alignment, highlighting the impact of embedding dimensionality on representation quality."}, {"title": "3.2.2 Pearson Correlation (RQ2): Is there a correlation between different model pairs?", "content": "We computed the Pearson Correlation Coefficient for all trained model pairs, plus the untrained model pairs that have GearNet on the GDM side. For each protein in the testing set, we first obtain its corresponding GDM and LLM representations. We then calculate the alignment score between these two projected representations for each model pair. This process is repeated across all model pairs, resulting in a list of proteins where each protein is associated with alignment scores from different model pairs. Finally, we analyze these alignment scores to compute the correlation between different model pairs. The results, presented in Figure 3a, reveal several interesting patterns."}, {"title": "3.3 Protein Perspective Analysis", "content": "To analyze the alignment of different proteins, we first define three key properties of proteins: (1) amino acid sequence length, (2) rarity, and (3) single/multiple chains. Within each model pair, we calculate the alignment score between the projected graph and text representations for each protein."}, {"title": "3.4 Strategies for Improvement", "content": ""}, {"title": "3.4.1 GDM Dimension Analysis (RQ4): Does increasing the GDM dimension improve alignment performance?", "content": "From Research Question 1, we reveal that if we control the GDM side of the model pair, and increase the parameter and dimension of the LLM, the model pair will show better alignment. In this section, we control the GDM side to GearNet model. Then, we retrained the GearNet with different size of hidden layer dimension. We aim to investigate how the dimensional size of GearNet influences the alignment between the model pairs. In our paper, we retrain the GearNet so that its output dimension will be {64, 128, 256, 512, 1024, 3072}. Different from the previous experiment that directly use the pre-trained GearNet, our retrained task for the GearNet is Multiple Binary Classification.\nMultiple Binary Classification is a task in which several binary classification problems are solved simultaneously. A protein may possess several distinct functional properties, each of which can be treated as a separate binary classification problem. Formally, given a dataset of n proteins $\\{(x_i, Y_i)\\}_{i=1}^{n}$, where $x_i \\in X$ represents the input features (e.g., protein graph representations) and $Y_i \\in \\{0,1\\}^k$ denotes the binary labels for k different tasks (functions), the objective is to learn a function $f: X \\rightarrow \\{0,1\\}^k$ that predicts a binary label for each task. Each task $j \\in \\{1, 2, ..., k\\}$ is a binary classification problem, where the predicted probability $\\hat{y}_{i,j} \\in [0, 1]$ represents the likelihood that the i-th protein possesses the j-th function. The learning objective is defined by minimizing a binary cross-entropy loss across all tasks:\n$L = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} [Y_{i,j} log(\\hat{y}_{i,j}) + (1 - Y_{i,j}) log(1 - \\hat{y}_{i,j})]$,"}, {"title": "3.4.2 Projection Head Analysis (RQ5): Does adding layers to the GDM's projection head enhance alignment performance?", "content": "In previous experiments, all projection heads consisted of a single linear layer. In this section, we explore whether increasing the complexity of the GDM's projection head by adding more layers improve alignment performance. To investigate this, we added one and two additional linear layers to the GDM's projection head while keeping the LLM's projection head structure unchanged. This results in GDM projection heads with 2 or 3 linear layers, with ReLU activation functions between layers. We then re-trained the projection heads and evaluated alignment performance as in previous experiments. Further details of the implementation are provided in Appendix A.7."}, {"title": "3.4.3 LLM Fine-Tuning Analysis (RQ6): Does fine-tuning an LLM on protein data enhance alignment performance?", "content": "In previous experiments, all model pairs used pre-trained LLMs directly downloaded from Huggingface. In this section, we fine-tuned all LLMs on the dataset constructed during the FASTA preprocessing stage, where we generated a detailed text description containing all relevant information for each protein. The fine-tuning process involved framing the task as a question-answering problem, where the input was a prompt such as, \"What is the protein: XXX?\" and the expected output was the corresponding text description. Further implementation details are provided in Appendix A.8. After fine-tuning the LLMs, we trained the 1-layer projection heads for the each model pairs and evaluated the alignment performance as before. As shown in Figure 6, the results clearly show that fine-tuning improves alignment scores across all model pairs. This indicates that providing the LLM with domain-specific knowledge enhances its ability to align with GDM representations. These findings suggest that LLMs fine-tuned on domain-specific data are more capable of generating representations that align effectively with other modalities. This underscores the importance of domain-specific LLMs when developing MMLMs, as they facilitate better alignment with non-text modalities such as graphs."}, {"title": "4 Discussions", "content": "In this paper, we investigated the alignment between LLMs and GDMs in the protein domain through six research questions, focusing on mapping their respective representations into a common space defined by LLM embedding dimensions. Our comprehensive analysis revealed key insights into effective alignment strategies and produced concrete recommendations for designing future protein-based multimodal LLMs."}, {"title": "4.1 Alignment Observations and Takeaways", "content": "Our research questions can be broadly divided into three categories. Research Questions 1-2 focused on model perspective analysis. We explored which pairs of LLMs and GDMs achieve better alignment. We found that GDMs capable of learning not only from the graph structure of proteins but also from their 3D geometric information tend to align more effectively with LLMs. Additionally, LLMs with higher embedding dimensions consistently demonstrated better alignment with GDMs. We also observed that model pairs with strong alignment tend to have higher correlation with other high-performing model pairs. Our analysis also uncovered significant challenges when aligning multimodal models in the protein domain. One major issue is that different protein IDs may represent different perspectives or angles of the same underlying protein, complicating alignment efforts. Additionally, proteins are often homologous, meaning that even distinct proteins can share structural and functional similarities. On a more granular level, Research Question 3 focused on protein perspective analysis, we discovered that popular proteins are generally easier to align across model pairs, whereas rare proteins present greater challenges in achieving alignment. Research Questions 4-6 examined methods for improving alignment between model pairs. Our findings indicate that increasing the embedding dimension size of a GDM enhances its alignment with LLMs. Moreover, adding an extra linear layer to the GDM's projection head can significantly improve alignment performance, though this improvement stopped after two layers, suggesting diminishing returns with further complexity. Another key insight is that fine-tuning LLMs with protein-specific knowledge can lead to better alignment with GDMs, highlighting the value of domain-specific adaptation.\nOur analysis also revealed several fundamental limitations in current protein databases that pose significant challenges for multimodal alignment. First, the imbalance in protein documentation presents a major obstacle. Protein databases often exhibit biases, with certain proteins being studied and documented multiple times under different IDs, while others are rarely studied or only appear once. This over-representation of well-studied proteins leads to models that align more effectively with these familiar proteins but struggle with rare or underrepresented ones. Therefore, biologists should also focus more on studying rare proteins to ensure that all proteins have a similar level of documentation and representation quality. Second, the biological reality of protein homology creates inherent complexity, where distinct proteins share structural and functional similarities due to common evolutionary origins. This makes it difficult to determine appropriate similarity thresholds, as even different proteins may share similar features in sequence and structure. Third, we discovered a notable dataset imbalance where popular, well-studied proteins generally achieve better alignment scores across model pairs compared to rare proteins. This points to a fundamental sampling bias in protein databases, where frequently studied proteins are overrepresented while rare proteins lack sufficient documentation for effective alignment. These database limitations highlight the need for more sophisticated alignment approaches that can account for protein relationships beyond simple ID matching, as well as methods to address the inherent imbalances in protein documentation and representation."}, {"title": "4.2 Suggestions for Developing Multimodal LLMs", "content": "Our findings from Research Questions 4-6 provide important insights for the design of future multimodal LLMs in the protein domain. We discovered that GDMs which incorporate both graph and 3D structural information of proteins demonstrate superior alignment with LLMs, suggesting future designs should prioritize models capable of capturing these multidimensional protein representations. The complex relationships between different proteins must be carefully considered during the alignment process to avoid oversimplified mappings that fail to capture subtle protein interactions. Our research also revealed that higher-dimensional embeddings in LLMs contribute to better alignment by capturing richer semantic information, indicating that increasing the capacity of the LLM's embedding space can enhance overall performance. When designing projection heads, we found that a two-layer architecture offers the optimal balance between simplicity and performance, as additional layers provide diminishing returns. Furthermore, fine-tuning LLMs with domain-specific data, such as protein descriptions, significantly improves alignment with GDMs, highlighting the importance of customizing LLMs to better understand the intricacies of the protein domain for more effective cross-modal integration."}, {"title": "5 Conclusions", "content": "In this paper, we conducted a comprehensive analysis of multimodal alignment between LLMs and GDMs in the protein domain, addressing six key research questions. Our study analyzed the models alignment and proteins alignment, such as the types of models that align well together, and specific factors that influence alignment performance, such as protein rarity and model complexity. Through these experiments, we uncovered significant challenges in multimodal alignment and provided valuable insights into its underlying mechanics. Our findings offer practical guidelines for designing more effective multimodal models. We highlighted the importance of accounting for nuanced and homologous relationships between proteins, emphasizing that simple mappings may overlook critical similarities. We demonstrated the importance of leveraging 3D geometric information in GDMs, maximizing embedding dimensions in LLMs, and fine-tuning LLMs with domain-specific knowledge. Additionally, we identified optimal layers for projection heads to enhance alignment without unnecessary complexity. These insights pave the way for future research and development, helping to create more robust and accurate multimodal systems that can better understand and represent protein data."}, {"title": "A.1 Dataset Statistic", "content": "Our dataset consists of 20,000 proteins, each with an associated FASTA file and PDB file. Detailed statistics for the dataset are presented in Figure 7, where each pie chart represents a different categorical breakdown of the same dataset. For the \"Number of Chains\" statistic, the chart shows the count of single-chain proteins versus multiple-chain proteins, with further subdivisions indicating the exact number of chains for multi-chain proteins. For the \"Sequence Length\" statistic, proteins are categorized by sequence length range. Each section of the pie chart is labeled with the relevant number of chains or sequence length range at the top, with the exact count and corresponding percentage displayed below each label."}, {"title": "A.2 Prompt Used in FASTA Preprocess", "content": "During the FASTA file preprocessing stage, we used GPT-40 to handle proteins with multiple chains. Below is the GPT prompt we employed for this task:\n==== System Prompt ====\nYou are a biologist with expertise in protein sequence analysis.\nYour task is to summarize complex protein sequence data into two or three sentences that highlight key features such as molecute type, chains, structural motifs, organism, etc.\n==== User Query ====\nSummarize the following protein knowledge, start with the sentence:\n'The protein structure {protein_id} has a sequence length of: {sequence_length} amino acids.'\nHere is more information about {protein_id}:\n{fasta_text}"}, {"title": "A.3 Protein Examples", "content": "As shown in Figure 9, we listed several proteins and their corresponding description. The meaning of 'popular' and 'rare' protein is discussed in Research Question 3: Protein Perspective Analysis."}, {"title": "A.4 GDM Representation Details", "content": ""}, {"title": "A.4.1 GearNet", "content": "Protein Graph Construction: The structure of a protein is represented as a residue-level relational graph $G = (V, E, R)$, where V represents the set of nodes, E the set of edges, and R the set of edge types. Each node $v_i \\in V$ corresponds to a residue in the protein, with its 3D coordinates $x_i \\in \\mathbb{R}^3$, and each edge $e_{ij} \\in E$ represents a relationship between residues based on either sequential proximity or spatial proximity. In particular, GearNet incorporates three types of edges:\n\u2022 Sequential edges: connect residues within a distance of 2 in the sequence.\n\u2022 Radius edges: connect residues whose Ca atoms are within a given radius in 3D space.\n\u2022 K-nearest neighbor edges: connect each residue to its k-nearest neighbors in 3D space.\nEach node $v_i$ is initially represented by its residue type and spatial coordinates, while each edge $e_{ij}$ is represented by its edge type (sequential or spatial) and spatial distance.\nRelational Graph Convolution: GearNet applies relational message passing on the constructed protein graph, leveraging both node and edge features. The relational graph convolutional layer is defined as:\n$h_i^{(0)} = f_i, u_i^{(l)} = \\sigma (BN(\\sum_{r \\in R} W_r h_{j}^{(l-1)})), h_i^{(l)} = h_i^{(l-1)} + u_i^{(l)}$"}, {"title": "Edge Message Passing", "content": "In addition to node message passing, GearNet incorporates an edge message-passing mechanism to explicitly model interactions between edges. The model constructs an edge-level graph $G' = (V', E', R')$, where each node in G' corresponds to an edge in the original protein graph G. The edge message passing is defined as:\n$m_{(i,j,r)}^{(0)} = f_{(i,j,r)}, m_{(i,j,r)}^{(l)} = \\sigma (BN(\\sum_{r' \\in R'} \\sum_{(w,k) \\in N_{r'} ((i,j,r))} m_{(w,k,r')}^{(l-1)}))$,\nwhere $m_{(i,j,r)}^{(l)}$ represents the message for edge $(i, j,r)$ at layer $l$, $N_{r'} ((i, j, r))$ is the set of neighboring edges in G', and W, is a learnable weight matrix for edge type r'. The angular information between edges is used to determine edge types, with edges having smaller angles expected to interact more strongly.\nThe output from the edge message passing is integrated into the node update step by modifying the aggregation function as follows:\n$u_i^{(l)} = \\sigma (BN(\\sum_{r \\in R} \\sum_{j \\in N(i)} \\sigma(W_r h_{j}^{(l)} + FC(m_{ij}^{(l)}))))$"}, {"title": "Protein Representation", "content": "After multiple rounds of relational graph convolution and edge message passing, each residue $v_i$ in the protein is represented by a feature vector $h_i^{(L)}$, where L is the number of layers in the model. In the original GearNet model, the hidden dimensions are set to hidden_dims = [512, 512, 512, 512, 512, 512], meaning that each residue is represented by a feature vector of size 512 after each layer.\nTo obtain the final protein representation, the features from all layers are concatenated:\n$h_{protein} = [h_1^{(1)}, h_2^{(1)},..., h_i^{(6)}] \\in \\mathbb{R}^{3072}$"}, {"title": "A.4.2 GVP", "content": "Graph Construction: Using the JSON input format", "3": "nested list provides precise spatial positioning essential for structural insights. For each residue", "Perceptrons": "The core of the GVP model is its message-passing architecture", "edge": "n$h_m^{(j \\rightarrow i)} = GVP([h_s^{(j)}, h_v^{(j)}, b_{(j \\rightarrow i)}"}]}