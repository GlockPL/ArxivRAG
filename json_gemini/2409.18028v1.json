{"title": "COMPOSITIONAL HARDNESS OF CODE IN LARGE LANGUAGE MODELS - A PROBABILISTIC PERSPECTIVE", "authors": ["Yotam Wolf", "Binyamin Rothberg", "Dorin Shteyman", "Amnon Shashua"], "abstract": "A common practice in large language model (LLM) usage for complex analytical tasks such as code generation, is to sample a solution for the entire task within the model's context window. Previous works have shown that subtask decomposition within the model's context (chain of thought), is beneficial for solving such tasks. In this work, we point a limitation of LLMs' ability to perform several sub-tasks within the same context window an in-context hardness of composition, pointing to an advantage for distributing a decomposed problem in a multi-agent system of LLMs. The hardness of composition is quantified by a generation complexity metric, i.e., the number of LLM generations required to sample at least one correct solution. We find a gap between the generation complexity of solving a compositional problem within the same context relative to distributing it among multiple agents, that increases exponentially with the solution's length. We prove our results theoretically and demonstrate them empirically.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), based on the transformer archietecture, Vaswani et al. (2017), have become very efficient problem solvers in many domains, such as broad-scoped question answering, writing assistance, teaching, and more Brown (2020); Radford et al. (2019); OpenAI (2023); Bubeck et al. (2023); Nori et al. (2023); West (2023). Yet their analytical skills, such as coding capabilities, are slow to develop - Chen et al. (2021b); Li et al. (2022a); Alp (2023); Ridnik et al. (2024) show that even with millions of generations, LLMs may not produce a single correct solution to competitive coding problems. Dziri et al. (2024) show that LLMs solve compositional tasks such as long multiplication and dynamic programming without developing systematic problem-solving skills.\nOne way to empower LLMs in analytical tasks, is to use subtask decomposition, otherwise known as chain of thought (COT) - a method in which an LLM breaks down a problem to smaller, more manageable tasks, solves them, and integrates it into a solution. The method has been empirically demonstrated by Wei et al. (2022), that show reasoning capabilities of language models improve when they are prompted to break down a task. Its efficiency has also been studied theoretically - Wies et al. (2022); Malach (2023) prove that through the autoregressive nature of language models, problems that cannot be solved directly, can be solved by subtask decomposition, Merrill & Sabharwal (2023) prove that while transformers are limited in the computational problems they can solve directly, using a polynomial number of intermediate steps, they can represent any polynomial time Turing Machine, and Sanford et al. (2024) provide a similar result on expressing more complex arithmetic circuits using more steps of COT.\nYet even with task decomposition, there is a limitation to transformer based models on analytical tasks with COT, due to their limited ability to compose functions - Sanford et al. (2024) show single layer attention can only learn pairwise relations between tokens, limiting the ability to integrate intermediate steps for a fixed model size, and the work of Peng et al. (2024) shows that iterative composition over a domain is limited by the size of the model, even when COT is used. Thus while theoretically possible, some tasks require an arbitrarily long COT for an LLM to solve. However, in"}, {"title": "2 FRAMEWORK", "content": "In practice, LLMs are limited in their context length - beyond the constraint of context length during training, Hsieh et al. (2024) introduce the RULER benchmark, for measuring the utility of LLMs on long context tasks. They show that in practice many models can perform tasks only on a much shorter context length than they were trained on. Similarly, Ebrahimi et al. (2024) show empirically that LLMs are limited in random access to tokens within the context, in the bit parity problem. Consequently, even though COT can in theory allow an LLM to solve arbitrarily complex analytical problems, in practice, they will be limited by the effective context length.\nA rising approach to remedy this limitation is to solve problems through the use of multi-agent systems, that tackle complex problems through the use of agents, where each agent is an LLM instance that solves a different aspect of the problem. While it has been used for simulating social interactions, Park et al. (2023); Li et al. (2023); Pang et al. (2024), it has also been shown as an effective tool for analytical problem solving. This can be done by decomposing a large task and distributing the sub-tasks between agents. Ishibashi & Nishimura (2024) use this method for building large code bases and Liu et al. (2023), use a dynamic LLM-agent network for solving code problems and analytical tasks.\nIn this work, we theoretically study a compositional hardness of coding problems originating from context processing limitations of LLMs, and the resultant effectiveness of a multi-agent system over a single model instance in composite coding problems. We model a composite coding problem using a pair of simpler coding problems, where the output to the composite problem can be a basic manipulation on the outputs to the pair of problems (e.g. composition, product of outputs, concatenation, logical function of the outputs, etc.), such that the solution to the problem can be obtained from concatenating the solutions to the problem pair. The model's usefulness on a coding task, is quantified by a generation complexity metric (definition 2) - the number of LLM generations required to sample at least one correct solution. The appeal of this metric, is that due to the existence of code testing units, it suffices to turn an LLM into a good program candidate generator and simply output a candidate that is correct Kambhampati et al.; Thakur et al. (2023); Luo et al. (2024). The single LLM instance solves the entire problem, while the multi-agent system is comprised of two agents, each is an LLM instance tasked with solving one of the problems in the pair. Thus the LLM's helpfulness in the single instance case is the generation complexity for the composite problem, while in the multi-agent case, it is the product of generation complexities of the pairs of problems, as a correct solution is attained when independently sampling a correct solution to each problem.\nWe theoretically model an LLM as an autoregressive model, where a solution is sampled token by token, based on the hidden representations. When combining two problems whose solutions are grammatically similar but semantically different (such as different coding problems), we assume this combination injects noise into the model's representations during solution generation for each sub-problem (assumption 1), which we denote as screening. We show that a compositional problem may have an exponential gap in generation complexity relative to the product of sub problems' generation complexities (theorem 1), meaning an exponential hardness of composition in-context. Essentially, the model is less capable of solving two problems if they are presented within the same context, than if presented in separate contexts. This points to an advantage of decomposing a problem not only within the same LLM context, but to distribute the problems among multiple agents (i.e. solve each sub-task within a different context). Additionally, this result provides a view of the model's effective context length through the lens of screening, which is the model's ability to isolate the relevant context at each decoding step \u2013 additional irrelevant context may reduce model's performance on other tasks within the context window exponentially with length."}, {"title": "2.1 GENERATION COMPLEXITY", "content": "We focus on coding problems, meaning each problem is written in natural language and is solved by a function, and the goal is for the model to generate a code that implements it.\nDefinition 1. Let L be a programming language. Let x be a natural language description of a problem, that is solved by function f, then a computer program y is a correct solution to x, if it implements f."}, {"title": "2.2 SCREENING IN AUTOREGRESSIVE MODELS", "content": "Here we introduce a source of hardness in code composition based on the autoregressive nature of LLMs. Typically, latent representations of the model contain information about the context beyond the next token prediction, e.g. the structure of the solution to problems Ye et al. (2024). Thus when composing two code problems, we expect the representations during the generation of the second program to contain information about the first program and vice versa, which is grammatically similar (same programming language) but semantically very different. As a result, this additional information creates noise that can harm the generation process.\nFormally, we denote by $r^{(L)}(x)$ the model's last hidden layer representation of the prompt x, by U, the model's unembedding matrix (hidden dimension to vocabulary). The logit of the i'th token is thus defined as the token's score before the softmax operation: $(r^{(L)}(x), U^Te_i)$, where $e_i$ is the one-hot vector of the token. The probability distribution at each decoding step is the softmax applied to the logits, $P_{LLM}(i|x) = softmax((r^{(L)}(x), U^Te_i))$.\nIn the process of generating a solution to a compositional code problem, x, that is implicitly or explicitly decomposable to $x_1$ and $x_2$, the model will implement a solution to the first part $y_1$ and then to the second part $y_2$. The sequence is generated based on the hidden representations. Informally, we expect the representation of the solution to the first problem, $y_1$, within the compositional problem, x, to be a noisy version of the solution's representation in the non-compositional problem, $x_1$:\n$r^{(L)} (x \\oplus y_1) = r^{(L)}(x_1 \\oplus y_1) + noise$                                                                                                          (2)\nSimilarly, the representation of the second problem's solution, $y_2$, in the compositional problem, x, is expected to be a noisy version of the solution's representation in the non-compositional problem, $x_2$:\n$r^{(L)} (x \\oplus y_1 \\oplus y_2) = r^{(L)} (x_2 \\oplus y_2) + noise$                                                                                                       (3)\nEssentially, this means the model attempts to generate the same solutions as in the non-compositional case, but noise may interfere in the process. The projection of this noise onto the dictionary creates noise in the logits during decoding, which can lead the model to make mistakes.\nAs the two problems $x_1$, $x_2$ and their solutions $y_1$, $y_2$ may be different semantically, we do not expect the noise to \"push\" the model towards the correct solutions more than to incorrect solutions, thus when projected onto the vocabulary, V, the noise on the logit of the correct token minus the noise on an incorrect token, $(noise, U^Te_{i_{correct \\ next \\ token}}) - (noise, U^Te_{i_{an \\ incorrect \\ next \\ token}})$, should be symmetric on average. Additionally it should be bounded within some range [-M,+M], as it changes the hidden"}, {"title": "2.3 EFFECT OF NOISE ON DECODING", "content": "representation to a finite extent. In practice, we only expect this to be true for the high probability tokens, as the vocabulary V is very large, and some low probability tokens may be systematically enhanced. To avoid this issue, we make our assumptions only on the weighted average of the noise, where the weights are given by the probability mass that the model assigns them. This way, low probability tokens with asymmetric noise or large norms receive low weight and are averaged with the noise of other tokens. Denote by $P(i|context)$ the probability assigned to the i'th token given the context. We will make our assumptions on the weighted average of the noise on the incorrect token logits minus correct token logits:\n$\\bar{X} = \\frac{\\sum_{i\\in V/\\{correct \\ next \\ token\\}} P(i|context)(noise, U^Te_i - U^Te_{i_{correct \\ next \\ token}})}{\\sum_{i\\in V/\\{correct \\ next \\ token\\}} P(i|context)}$                                                                                                       (4)\nAssumption 1. Denote by $\\bar{X}$ the weighted noise on the logits as defined in equation 4. We assume that at every given decoding step it is a continuous, symmetric random variable and bounded within [-M,+M] for some M > 0.\nIn experiment subsection 4.3 we show the noise satisfies these assumptions, with M \u2248 3 - 4.\nWhile the noise onto the logits, $\\bar{X}$, averages to zero, its effect on the decoding process does not. The probability of each token in a decoding step is changed to:\n$P(i|context) \\rightarrow P'(i|context) = \\frac{P(i|context)}{P(i|context) + (1 - P(i|context))e^{\\bar{X}}}$                                                                                                           (5)\nThe denominator, $P(i|context) + (1 - P(i|context))e^{\\bar{X}}$ can be thought of as a renormalizing term, which redistributes the probability of the tokens. For $\\bar{X}$ = 0, the token's probability does not change, for $\\bar{X}$ < 0 it increases and for $\\bar{X}$ > 0 it decreases. Note that if $P(i|context) \\in (\\epsilon, 1 - \\epsilon)$ for $\\epsilon > 0$ (the model has finite confidence), then on average, the noise decreases the probability $P(i|context)$, by a factor of $exp(-\\Delta(\\epsilon, \\bar{X}))$, where $\\Delta$ is the renormalizing term's mean:\n$\\Delta(\\epsilon, X) := E_X [log(\\epsilon + (1 - \\epsilon)e^X)]$                                                                                                      (6)\nIntuitively $\\Delta$ is the average renormalization of the correct token's probability. In experiment subsection 4.3, we calculate $\\Delta(\\epsilon, X)$ empirically as a function of $\\epsilon$, and find that for $\\epsilon$ = 0.1 for example, $\\Delta$ \u2248 0.2. The consequence of this, is that on average, most long sequences have their probability reduced by the noise, while few random long sequences have their probability greatly enlarged. Since for long coding problems most sequences are incorrect, the probability of a correct solution getting enlarged is small. We formally show this in the next section. To do so, we will use concentration inequalities, for which we note that:\n$|log(\\epsilon + (1 - \\epsilon)e^X)| < M$                                                                                                                             (7)\nThus the renormalizing term's variance is also bounded:\n$\\sigma^2(\\epsilon, X) := Var_X [log(\\epsilon + (1 - \\epsilon)e^X)] < M^2$                                                                                                               (8)"}, {"title": "3 RESULTS", "content": "Here we show that composing two coding problems can be significantly harder than solving each on its own. A natural quantification of compositional hardness is the gap between generation complexity of the problem's components and the complete problem. For example, we say that composition is hard if:\n$N(P,x) \\gg N(P, x_1) \\cdot N(P, x_2)$                                                                                                                      (9)\nWhile it is easy if:\n$N(P,x) \\approx N(P, x_1) \\cdot N(P, x_2)$                                                                                                                        (10)\nThe rational behind this, is that $N(P, x_1) \\cdot N(P, x_2)$ is the number of attempts required to independently sample a correct solution to $x_1$ and to $x_2$, while $N(P, x)$ is the number of attempts required to sample a solution to problem that integrates both problems. In an easy composition scenario, the"}, {"title": "4 EXPERIMENTS", "content": "model solves the sub-problems to the best of its abilities as it would if each was solved independently. In a hard composition scenario, seeing both problems combined reduces its performance on each sub-problem, and it is better to use a multi-agent system, by feeding the model the subproblems in different contexts and sample solutions independently.\nAs there are typically more incorrect solutions to coding problems than correct ones, the random noise inserted into the logits generally harms the model's performance. The following lemma quantifies an exponential decrease in the model's probability of a correct solution to a compositional problem, $P(y_1 \\oplus y_2|x)$, relative to the probabilties of the sub-problem solutions $P(y_1|x_1) \\cdot P(y_2|x_2)$:\nLemma 1. Let $\\epsilon, \\delta \\in (0,1)$, and $M > 0$. Let x be a compositional problem and $y_1 \\oplus y_2$ a solution, with $x_1$, $x_2$ being the corresponding sub-problems. Suppose that the noise injected to the logits as defined in equation 4, satisfies assumption 1 - for all decoding steps, it is continuous, symmetric and bounded within [-M,+M]. Suppose further that the probability assigned to the correct token at each decoding step is bounded within $[\\epsilon, 1 \u2013 \\epsilon]$. Denote by $\\Delta := E_X [log(\\epsilon + (1 \u2013 \\epsilon)e^X)]$ and $\\sigma^2 := Var_X [log(\\epsilon+(1- \\epsilon)e^X)]$ the renormalizing term's mean and variance (as defined in equations 6 and 8 respectively). Under the assumption, $\\Delta$, $\\sigma$ are strictly positive, and if $|y_1|+|y_2| > \\frac{M^2}{\\sigma^2} \\cdot \\big(3\\frac{\\Delta \\cdot M}{4\\sigma^2}\\big)$, where $h(x) = (x + 1) ln(1 + x) \u2212 x > 0, we have with probability of at least 1- \u03b4 that:\n$\\frac{P(y_1 \\oplus y_2|x)}{P(y_1|x_1) \\cdot P(y_2|x_2)} < e^{-\\Delta(|y_1|+|y_2|)}$                                                                                                                                  (11)\nWhere P(y|x) is the probability assigned to y by the model, given context x.\nThe proof is presented in appendix A. The intuition behind this result is that as there are typically more incorrect choices to make when generating code, random noise usually reduces the probability for sequences with finite confidence. Thus most sequences get their probability reduced, while very few random sequences get a large increase, and these are usually not correct solutions.\nThe assumption on bounded probability for the correct token $[\\epsilon, 1 \u2013 \\epsilon]$ implies we are considering solutions where the model has high but limited confidence in each decoding step. While LLMs are often very confident, in practice, when sampling, nucleus sampling is commonly used, where sampling only occurs if the model is not overly confident. e.g. with p = 0.95, if a token's probability P, is larger than 0.95 or much smaller than 0.05, then the probability is rounded to 0 or 1. Thus when considering probabilities of sequences, it suffices to look only at decoding steps where the model is not too confident, hence we can consider $\\epsilon \u2248 0.05$.\nIn subsection 4.3, we see empirically that for $\\epsilon = 0.1$, $\\Delta \u2248 0.2$, $\\frac{\\sigma^2}{\\Delta} \u2248 1.5$, $M \u2248 4$, making $\\frac{\\sigma^2}{\\Delta h(\\frac{3M}{\\sigma})} \u2248 200$, thus for solutions with length $> 200$ tokens, the results apply.\nIn practice, there may be multiple solutions to the same problem, e.g. multiple implementations of the same function. So it is necessary to take all of them into account when considering the generation complexity. With this taken into account using a union bound, we obtain the following result of an exponential gap in generation complexity between a composition of problems and the sub-problems, indicating a compositional hardness that is exponential in the solution's length:\nTheorem 1. Let $\\epsilon, \\delta \\in (0, 1)$, and N, M > 0. Let x be a compositional problem, with $x_1$, $x_2$ being the corresponding sub-problems. Denote by $L_1$, $L_2$ the minimal solution length to $x_1$, $x_2$ respectively, and the total number of solutions to x by N. Define $\\Delta$, $\\sigma$, the renormalizing term's mean and variance (as defined in equations 6 and 8 respectively) and by M the bound on the logit noise (assumption 1). Under the assumptions of lemma 1, they are strictly positive, $\\Delta$, $\\sigma$, M > 0, and if the minimal solution length $L_1 + L_2$, satisfies $L_1+L_2> \\frac{M^2}{2\\cdot \\sigma^2} \\cdot h(\\frac{3\\Delta \\cdot M}{4\\sigma^2})$, where h(x) = (x + 1) ln(1 + x) \u2212 x > 0, we have with probability of at least 1 8 that the generation complexity (definition 2) satisfies:\n$N(P,x) \\geq N(P, x_1)N(P, x_2)\\cdote^{\\frac{\\Delta}{4}(L_1+L_2)}$                                                                                                       (12)\nThe proof is presented in appendix B. We see that longer problems become harder to solve due to the noise injected into the decoding steps by previously generated tokens. This shows that a model that fully utilizes its context in decoding (i.e. the next token probability distribution is explicitly a function of all the previous tokens), can have a hard time mixing different concepts due to the screening effect. This result implies that for long coding problems, it is more beneficial to distribute sub-tasks between different instances of the LLM, and not expose it to the full context. Additionally, this result provides"}]}