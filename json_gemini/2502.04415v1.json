{"title": "TERRAQ: SPATIOTEMPORAL QUESTION-ANSWERING ON SATELLITE IMAGE ARCHIVES", "authors": ["Sergios-Anestis Kefalidis", "Konstantinos Plas", "Manolis Koubarakis"], "abstract": "TerraQ is a spatiotemporal question-answering engine for satellite image archives. It is a natural language processing system that is built to process requests for satellite images satisfying certain criteria. The requests can refer to image metadata and entities from a specialized knowledge base (e.g., the Emilia-Romagna region). With it, users can make requests like \"Give me a hundred images of rivers near ports in France, with less than 20% snow coverage and more than 10% cloud coverage\", thus making Earth Observation data more easily accessible, in-line with the current landscape of digital assistants.", "sections": [{"title": "I. INTRODUCTION", "content": "The field of Natural Language Processing is undergoing major advancements caused by the rapid development of Language Models [1-3]. An outcome of this development is the proliferation of digital assistants and natural language interfaces for all manners of systems and knowledge repositories (e.g., Alexa from Amazon, Siri from Apple, ChatGPT from OpenAI, and Claude from Anthropic). The resulting increase in accessibility enables non-technical users to intuitively interact with computer systems and access high-quality information, while also improving efficiency for expert users. In this climate, the task of Knowledge-Graph Question-Answering (QA), also known as Text-to-SPARQL, is as relevant as ever [4, 5].\nQA systems take as input queries in natural language and generate semantically equivalent SPARQL\u00b9 queries over a specific knowledge graph (KG). These SPARQL queries are subsequently executed on an RDF store, which in turn returns the answer. The answer can either be directly presented to the user, or integrated into a larger system and used as part of a Retrieval-Augmented Generation [6] pipeline, as is the case with digital assistants that rely on knowledge grounding.\nIn our work, we are developing TerraQ a spatiotemporal QA engine for satellite image archives. User requests can refer to image metadata and geographic entities (e.g., the Loch Ness Lake or the city of Munich) both of which are included in the target knowledge graph. For example, \"Show me images of Athens with VV polarization.%\". The goal of our research is to make Earth Observation data archives accessible via natural language, to the benefit of both novice and expert users.\nTerraQ belongs in the same family of engines as GeoQA2 [7] and EarthQA [8], two template-based question-answering engines previously developed by our group. GeoQA2 is a geospatial QA engine, and EarthQA is a satellite-image archive QA engine that reuses some components of GeoQA2 while also adding some additional specialized components. In comparison to these two systems, TerraQ has a number of advantages. First, it does away with template-based query generation. As a result, it is able to answer a wider array of questions and has improved accuracy. Second, TerraQ targets a purpose-built KG with high-quality geospatial information, allowing for more fine-grained results, which was one of the limitations of the original EarthQA paper. Third, unlike EarthQA, the engine does not use specialized components. All thematic information can be integrated into the core engine architecture, making the engine easier to adapt to different domains.\nIn this paper, we make the following original contributions:\n1) We present a specialized knowledge graph that interlinks geospatial information about natural features and administrative divisions with satellite image metadata. Knowledge graph resources are integrated into a hierarchical structure, making it easier to expand with additional geospatial information or thematic knowledge.\n2) We develop TerraQ, a spatiotemporal QA engine for image archives. The engine is able to answer simple and complex queries both reliably and quickly in dynamic fashion, while also avoiding the use of query-templates or computationally demanding neural models.\nThe version of TerraQ presented in this paper has been developed in the context of the European Space Agency project DA4DTE: Demonstrator Precursor Digital Assistant Interface for Digital Twin Earth\u00b2 and a demo is available publicly at http://terraq.di.uoa.gr/."}, {"title": "II. KNOWLEDGE GRAPH", "content": "To provide TerraQ with a powerful geospatial knowledge base, we compiled information from various sources and combined them under a common KG. Our aim was to create a polymorphic database of spatial resources, by integrating natural features and administrative geo-entities under a compact, non-complex ontology, that better facilitates the task of geospatial question answering. The aforementioned facts were collected from the following sources:\n\u2022 GADM: We collected geospatial features from the Global Administrative Areas (https://gadm.org/) dataset, focusing on features located in Europe. Utilized features from this dataset include countries, cities, regional units, and national administrative divisions.\n\u2022 Rivers, Points of Interest, and Ports: This dataset is provided by our partners in the DA4DTE project e-GEOS (https://www.e-geos.it/) and includes spatial characteristics for various features within these categories. In addition to spatial data, the dataset also contains comprehensive metadata for each feature.\n\u2022 Sentinel-1 Images: We incorporated Sentinel-1 satellite image data, including metadata about the images and the satellite's location at the time each image was captured. Links to the images are stored in the knowledge graph, ensuring that they are easily accessible by the Question-Answering engine and external sources. Sentinel-1 images were collected for the years 2020 and 2021.\n\u2022 Sentinel-2 Images: Similarly to the Sentinel-1 satellite images, we included image links and metadata from Sentinel-2 image collections to enhance the information available in our data model. Sentinel-2 images were collected for the years 2020, 2021 and 2022.\n\u2022 Sea sectors: A collection of sea sectors covering global water spaces and oceans A total of 101 sea sectors along with their polygons were integrated into the knowledge graph of the Marine Regions [9] data source.\nOntology. We built our ontology on top of well-known and standardized ontologies, namely the YAGO2geo [10] ontology and the GeoSPARQL [11] ontology. The main class of the knowledge graph is named Feature. The Feature class is extended by various subclasses that represent the knowledge provided by the various datasets that we examined in the previously, namely, rivers, ports, pois (points of interest), Sentinel-1 and Sentinel-2 images and GADM geoentities.\nTranslation of named location labels to English. While integrating our data, we noticed that many geospatial features were named only in their original languages, with no English equivalents provided. For instance, the city of Rome was listed only as \"Roma\" in Italian within the metadata. To provide a consistent framework and to simplify the task of recognizing labels for our Question-Answering engine, we implemented a comprehensive translation pipeline. Using the Mistral-7b LLM [3], we were able to correctly identify and translate a total of 56657 labels from various European languages."}, {"title": "III. THE TERRAQ ENGINE", "content": "TerraQ consists of a number of components, each of which performs a specific task. Information is propagated from one component to the next. This pipeline is split in four distinct conceptual steps.\nFirst, the WHERE clause is generated by combining basic SPARQL/GeoSPARQL building blocks. This is subsequently passed as additional input to the components responsible for the generation of the SELECT/ASK clause. When both clauses have been constructed, the query generator merges them and makes any necessary additions to construct a complete SPARQL query. In the last step, the query is rewritten to make use of materialized geospatial relations.\nThe complete architecture of TerraQ is shown in Figure 1. Below, we present the functionality of the system in detail. As our running example we use the request \"Show me all images taken in January 2021 with rivers less than 2km away from towns and forests in the Emilia Romagna region, having cloud coverage less than 10%\u201d.\nDependency Parse Tree Generator. This module generates a dependency parse tree of the input question using Stanford-CoreNLP [12]. The dependency parse tree is used to identify and store information.\nInstance Identifier. This module does named-entity recognition and disambiguation. In the example question, it identifies the entity \"Emilia Romagna\" and maps it to the resource yago:Emilia_(region_of_Italy) in the KG. The mapping to the KG resource happens in two steps. First, WAT [13] links the named entity to a Wikipedia page. Subsequently, the component searches the Knowledge Graph for the resource that best matches the entity returned by WAT. In addition to identifying the instance, this component is responsible for creating the block that will be used in the WHERE clause for the identified instance. The generated block is the following:\n<URI> geo:hasGeometry/geo:asWKT ?iWKTID.\nConcept Identifier. This module identifies and maps concepts present in the input question to the appropriate resource of the KG ontology. For instance, from the example question, it will identify and map the concepts River, Town, Forest. The mapping is done using a class label dictionary and string similarity based on n-grams. Additionally, this component is responsible for creating the block that will be used in the WHERE clause for the identified concepts:\n?cID a <URI> ;\ngeo: hasGeometry/geo:asWKT ?cWKTID.\nAt the end of the concept identification stage, and after all Instances have been identified, we employ a heuristic of consolidation between concepts and instances. Concepts and Instances that are not separated by any token are consolidated to reduce the complexity of the generated WHERE-clause and help the query generator produce a correct query. For example, in the question \"Where is the Tagus river located?\" only the Instance of Tagus is kept and the river concept is consolidated into it."}, {"title": "Property Identifier.", "content": "The property identifier identifies attributes of features or types of features specified by the user in input questions and maps them to the corresponding properties in the knowledge graph. In the example question, the property \"cloud coverage\" of the type of feature image will be identified and mapped to the corresponding property in the KG. For each identified concept, we try to match, using string similarity on n-grams, its properties to the words in the sentence. Matched properties are identified as candidate properties for this concept. Multiple concepts might have the same candidate property. To resolve this conflict, we introduced a heuristic that selects the closest concepts as the targets to the properties. This process is similar for instances inside the question.\nAgain, this component is also responsible for generating the block that will be used in the WHERE clause for the identified properties:\nINSTANCE/CONCEPT_VARIABLE <URI> ?pID.\nIn addition, this component uses the dependency parse tree and Part-of-Speech tags to identify words that denote the use of comparatives and superlatives. These are subsequently matched to the appropriate Concept or Property, using a node-distance heuristic on the dependency parse tree."}, {"title": "Spatial relation Identifier.", "content": "This module identifies spatial relations present in the input question and maps them to appropriate stSPARQL/GeoSPARQL functions. For instance, in the example question, it will identify the spatial relations \"in\" and \"away from\" and map them to geof:sfWithin and geof:distance respectively. Then these relations are mapped to the appropriate previously identified Instances and Concepts by using the following heuristic:\ndistance dependency_parse_tree_distance+ (\nword_distance\n100\nAgain, this component is also responsible for generating the block that will be used in the WHERE clause:\nFILTER (<URI> (FIRST_FEATURE,\nSECOND_FEATURE))\n)\n)"}, {"title": "and", "content": "FILTER (geof: distance (FIRST_FEATURE,\nSECOND_FEATURE, uom: metre)\n{<, >, <=, >=, =, ~} DISTANCE)"}, {"title": "Numeric Solver.", "content": "This module is responsible for identifying numbers, understanding their use in the input question and enhancing the previously identified elements with additional information. For this purpose we utilize Part-of-Speech tags and the previously described distance heuristic.\nIn our working example, \u201cless than 2km\u201d is matched to the spatial function of distance and \"less than 10%\" is matched to the cloud coverage property."}, {"title": "Conjunction Solver.", "content": "The Conjunction Solver is responsible for handling conjunctions, as those are identified by the dependency parse tree. To that end, it selects all edges of the parse tree tagged as \"conj:and\". The vertices connected by each of those edges are checked for meaningful conjunctions. Number-to-Property, Number-to-Number and Geospatial-to-Geospatial conjunctions are supported. For Geospatial-to-Geospatial conjunctions additional spatial relations are generated and stored, as if they were created by the Geospatial Relation Identifier, according to the information provided by the vertices. In our example, this is the case with \"towns and forests\". Number and Property conjunctions faction similarly."}, {"title": "Temporal Identifier.", "content": "This module uses HeidelTime [14] to identify temporal keywords in the input question and annotates them with the appropriate date and/or duration. For instance, in the example input question it will identify \"January 2021\" and map it to 2021-01."}, {"title": "Return Type Identifier.", "content": "This module is responsible for identifying the expected form/type of the answer to the question. The supported types are Name, Coordinates, Number-Property, Number-Count, Image . For our example, Image is the most appropriate return type. For identifying the expected return types, this component leverages the sophisticated language understanding of Llama 2 [2]. We fine-tune our model to output correctly formatted answers. A fallback mechanism that uses heuristics is provided to enable using TerraQ without hardware acceleration (GPU)."}, {"title": "Query Form Identifier.", "content": "This component is responsible for generating the final ASK/SELECT clause, which will be used by the query generator. It takes as input the return types generated by the return type identifier. For each expected return type, we follow an iterative approach as follows: If the type is Name, we search for the next concept. If the type is Coordinates, we seek the next concept or instance. When the return type is Number-Property, we look for the next property, and if it's Number-Count, we search for the next concept.\nAdditionally, we enhance the query by introducing a COUNT aggregation and the necessary GROUP BY clauses. In the case of Image, we insert the appropriate code in the query. To determine the 'next' object, we traverse the dependency parse tree."}, {"title": "Query Generator.", "content": "The query generator is responsible for generating the final query. Within this stage of the pipeline, it assimilates all the information provided by the preceding components and combines them into a suitable, executable SPARQL or GeoSPARQL query. Information about superlatives, limits and other structures is taken into account in the generation process."}, {"title": "Query Enhancer.", "content": "The query enhancer is an optional component responsible for modifying the query produced by the Query Generator to fix any mistakes and/or oversights. It is implemented using the Mistral-7b LLM fine-tuned on the dataset GeoQuestions1089 [15]. It serves as a performance-enhancement module that increases the capacity of TerraQ to answer complex questions following the Execution Refinement paradigm [16]."}, {"title": "GoST.", "content": "The GoST transpiler [15] takes the query generated by the Query Generator and rewrites it to use materialized geospatial relations if that is possible. Because geospatial relations like geof:sfWithin are computationally expensive we do offline materialization using the tool JedAI-Spatial [17]."}, {"title": "IV. EVALUATION", "content": "To the best of our knowledge, there is no publically available dataset that is suitable for evaluating systems on the task of Text-to-SPARQL for Earth Observation archives. For this reason, we decided to deploy our engine as a pure geospatial QA engine and run an evaluation on the geospatial QA dataset GeoQuestions1089 [15]. Although this did require some tinkering, since GeoQuestions1089 targets the YAGO2geo ontology, the process was straightforward and painless, and we believe that the resulting evaluation is useful for measuring the performance of most dimensions of our engine. Unfortunately, the questions in GeoQuestions1089 do not include temporal information.\nTo accept an answer as correct, it must match the gold result (included in GeoQuestions1089) exactly. We do not consider partially correct answers as correct. Likewise, for supersets of the answers in the gold set."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "The success of modern digital assistants has clearly shown that natural language interfaces for computer systems and knowledge repositories can be a great boon for productivity and accessibility. Users nowadays expect to be able to interact with their computers through natural language, reducing the barrier of technical knowledge required for utilizing modern computing capabilities.\nThis paper presents TerraQ, a spatiotemporal QA system for satellite image archives. Our engine targets a high-quality, purpose-built knowledge graph that contains Sentinel-1 and Sentinel-2 image metadata, as well as geospatial information for administrative divisions and natural features. Requests made in natural language are translated to SPARQL queries, which are subsequently executed by an RDF store. This enables users to request, in natural language, satellite images satisfying a number of complex spatial, temporal and thematic criteria.\nOur engine is easily deployable and responsive on commodity hardware, since it does not rely on exceedingly large LLMs. Instead, it utilizes a combination of small-scale LLMs, heuristics and expert knowledge. This development is another step towards our vision of making Earth Observation archives more accessible by both novice and expert users, no matter the available computing capacity.\nIn the future, we are planning on expanding the capabilities of our system by integrating Visual Question-Answering systems into our Knowledge Graph creation pipeline. This will enable users to express even more specific criteria for image selection, while also maintaining performance."}]}