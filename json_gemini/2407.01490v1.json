{"title": "LLM See, LLM Do: Guiding Data Generation\nto Target Non-Differentiable Objectives", "authors": ["Lu\u00edsa Shimabucoro", "Sebastian Ruder", "Julia Kreutzer", "Marzieh Fadaee", "Sara Hooker"], "abstract": "The widespread adoption of synthetic data raises new questions about how models generating\nthe data can influence other large language models (LLMs) via distilled data. To start, our work\nexhaustively characterizes the impact of passive inheritance of model properties by systematically\nstudying the consequences of synthetic data integration. We provide one of the most comprehensive\nstudies to-date of how the source of synthetic data shapes models' internal biases, calibration and\ngenerations' textual attributes and preferences. We find that models are surprisingly sensitive\ntowards certain attributes even when the synthetic data prompts appear \"neutral.\u201d which invites the\nquestion whether this sensitivity can be exploited for good.\nOur findings invite the question can we explicitly steer the models towards the properties we want at\ntest time by exploiting the data generation process? This would have historically been considered\ninfeasible due to the cost of collecting data with a specific characteristic or objective in mind.\nHowever, improvement in the quality of synthetic data, as well as a shift towards general-purpose\nmodels designed to follow a diverse way of instructions, means this question is timely. We propose\nactive inheritance as a term to describe intentionally constraining synthetic data according to a\nnon-differentiable objective. We demonstrate how active inheritance can steer the generation profiles\nof models towards desirable non-differentiable attributes, e.g. high lexical diversity or low toxicity.", "sections": [{"title": "1 Introduction", "content": "Historically, high-quality labeled data has been costly to curate due to, amongst other factors,\nscarcity of available data (Bansal et al., 2022; Singh et al., 2024a) and financial cost (Gilardi et al.,\n2023; Boubdir et al., 2023). This high cost has precluded adapting training sets \"on-the-fly\" to\nincrease coverage or task diversity. As a result, researchers often treated datasets as static instead of\nmalleable. Rather than incurring the cost of collecting new data, recent work has focused on making\nbetter use of the existing data by optimizing in the data space. This includes efforts around data\naugmentation (Mumuni & Mumuni, 2022; Feng et al., 2021), creating auxiliary data fields through\npseudo-labeling (Ratner et al., 2017), data weighting (Thakkar et al., 2023; Dou et al., 2020), data\npruning to identify a high-quality subset (Marion et al., 2023; Attendu & Corbeil, 2023; Abbas et al.,\n2024; Groeneveld et al., 2024; Allal et al., 2023; Li et al., 2023) or curriculum learning (Soviany\net al., 2022; Xu et al., 2020).\nHowever, all these methods still adhere to the convention that the goal is to enhance an existing\n\"fixed\" dataset by re-formatting, transforming, or pruning existing data. As a result, their success\ndepends on the desired properties being present in the dataset to begin with. This limits the\nfeasibility of introducing new properties, or explicitly optimizing for task-specific metrics. What if\ninstead, we exploit the dataset generation process to steer towards the characteristics we want at test\ntime?\nWe turn to synthetic data generation (Wang et al., 2023b; Mitra et al., 2023; \u00dcst\u00fcn et al., 2024) as\na way to rapidly shape the data space with latent, desirable characteristics. In this process, we hope\nto capture more fine-grained and often non-differentiable characteristics such as increased length\nand lexical diversity as well as low toxicity that are known to be correlated with human preferences\n(Bai et al., 2022; Singhal et al., 2023; Singh et al., 2024b). While desirable, these attributes are\nnot explicitly optimized when training or aligning LLMs. We aim to leverage the phenomenon of\ninheritance to steer model behaviour to accentuate desirable attributes and attenuate negative ones.\nWe first exhaustively benchmark what we term passive inheritance\u2014profiling what changes happen\nwhen a student model is trained on synthetic data from a teacher model using a variety of social\nbias, textual characteristics, and calibration metrics. Furthermore, we study the effects of this\ndistillation on LLMs as evaluators, expanding upon prior work on self-preference (Singhal et al.,\n2023). We take a wider view and perform a systematic investigation into how different attributes are\ntransferred across models via synthetic data usage and how these changes are manifested both in\nLLMs' generations and their evaluator preferences.\nOverall, our profiling highlights what properties are most sensitive to passive inheritance when\ncomparing different student and teacher models. Next, we use this systematic view to inform the\nselection of properties to explicitly optimize for. We introduce the term active inheritance where we\nsteer iterative synthetic data distillation and targeted sampling towards specific characteristics.\nThis enables us to steer model behavior towards non-differentiable objectives. Most other approaches\nfor non-differentiable optimization rely on reinforcement learning (Roit et al., 2023), Bayesian\noptimization (Gopakumar et al., 2018), and evolutionary algorithms (Lange et al., 2023), which\nrequire complex methods that are difficult to scale and can be unstable with large models (Powell,\n2019; Daulton et al., 2022; Ouyang et al., 2022; Liu et al., 2023a). Our approach instead relies on\nthe simplicity of guiding generations in the synthetic data space and is interpretable because it is\nanchored to observable data characteristics.\nWe study a diverse set of models including LLaMa2-7B, LLaMa2-13B (Touvron et al., 2023), Mixtral-\n8x7B (Jiang et al., 2024), Gemma-7B (Team et al., 2024), Aya-8B (Aryabumi et al., 2024) and\nCommand-R+ (103B parameters)\u00b9, and trace the impact of an exhaustive set of over 26 metrics\nacross 4 categories (i.e. textual characteristics, social bias, toxicity and calibration) which we release\nas part of an open-source toolkit.\u00b2 Our main contributions are:\n1.  We establish that models trained on synthetic data are sensitive to passive property\ninheritance. We systematically study the consequences of synthetic data integration-a funda-\nmental step towards understanding how to leverage synthetic data responsibly. We introduce\na comprehensive toolkit enabling easy and automatic monitoring of LLMs' latent characteristics\nduring training.\n2.  Passive property inheritance from synthetic data impacts model behavior preferences\nwhen used as evaluators. Due to the prevalence of LLM judges in current evaluation pipelines\n(Zheng et al., 2023; Dubois et al., 2024b; Chiang & Lee, 2023), we also examine how synthetic\ndatasets alter the students' behaviors and preferences when they are used as evaluators (e.g.,\nbiasing the student towards the teacher model).\n3.  We propose active inheritance as a mechanism for steering synthetic data curation\ntowards desirable properties. We show that strategic gathering and curation of synthetic\ndata can significantly amplify desired characteristics and reduce undesired ones. In particular,\nwe show that by targeted sampling of generations from a single or multiple LLMs, we can steer\nmodel behavior with gains of up to 116% and 43% in length and lexical diversity respectively and\ndecrease toxicity up to 40%."}, {"title": "2 Methods", "content": "2.1 Learning from Synthetic Data\nIn the simplest form of knowledge distillation (Liu et al., 2019; Gou et al., 2021) and LLM-as-a-teacher\nsetups (Feng et al., 2023; Tian et al., 2024), the parameters \u03b8 of a student LLM are finetuned to\nmaximize the log-likelihood of a teacher's (another LLM with parameters (1) generation \u0177 ~ P\u00f4(\u00b7 | x)\nfor a given prompt x:\narg max E(x,y)~ [log po(\u0177 | x)]\n\u03b8\n                                                                                                           (1)\nThe teacher's generations serve as a proxy to a gold sequence, that is unattainable or non-existent.\nPairs of prompts and proxy labels form the synthetic dataset D that is the basis for the optimization\nprocess. In imitation learning, this strategy is known as behavioral cloning (Pomerleau, 1988), as\nthe goal is for the student to mimic the teacher's behavior as closely as possible.\n2.2 Measuring Data Characteristics\nThe proxy labels are expected to be generally superior to the initial student's generations, as they are\nsourced from a stronger model (larger, more specialized or more recent). However, the optimization\nobjective is agnostic to how this is manifested in the data. Our work focuses on characterizing the\ngenerations with a set of profiling functions f : VN \u00d7 VM \u2192 R, that return scalar values for a given\npair of prompt and generation sequences (i.e., token sequences over a vocabulary V). These functions\nallow us to track the passive inheritance of characteristics from teacher to student. Examples for\nsuch functions are detailed in Section 2.5.\n2.3 Active Inheritance\nHow can we directly guide the amplification of desired properties when learning from teachers? Our\nkey idea is to select proxy labels based on their presence of desired characteristics. We generate\nmultiple samples for each prompt (either from repeatedly sampling from a single model or sampling\nfrom multiple models), and then select the sample for finetuning that maximizes the presence of the\ncharacteristic. We now sample from the following distribution during student finetuning (Eq. 1):\np( x) = { 1 if f(x,.) = max f(x,y')\n0\nY'ED\notherwise\n(2)\nwhere the set of k candidate generations y' \u2208 Y can contain generations from various sources, such\nas the student itself or multiple teachers (discussed below). The resulting synthetic dataset is steered\ntowards favoring this particular attribute, and the student model is thus directly optimized towards\nit.\nThis best-of-k or rejection sampling strategy has been used as one component of the optimization\nin previous works to align models to human preferences (Dong et al., 2023; Gulcehre et al., 2023;\nTouvron et al., 2023), but these need large-scale reward models to compute f and are restricted to\nsingle teachers that remain close to the student model. Working with explicit metrics of desired\ndata characteristics is attractive, as it can work with any non-differentiable function f and black-box\nteachers (e.g., closed-source LLMs). Section 4 will present practical instances of successful steering\nof synthetic data.\n2.4 Learning from Multiple Teachers\nNaturally, the success of the active steering of inheritance is limited by the quality of the pool of\nsamples. We maximize the chance of obtaining samples with high values for f by employing a set of\ndiverse teacher models (01, 02,..., \u03b8k) rather than a single teacher (above). Thereby, we benefit\nfrom an ensembling effect and make use of the wisdom of the crowd (Zaras et al., 2021; Wu et al.,\n2021; 2022; Zuchniak, 2023; Ko et al., 2023). In Section 4.1.3 we will show the empirical benefits of\nlearning from multiple teachers."}, {"title": "2.5 Experimental Setup", "content": "2.5.1 Profiling Metrics\nWe profile models and their generations through a set of non-differentiable metrics along multiple\naxes of interest: Textual characteristics, social bias, toxicity, and calibration. We analyze passive\ninheritance of these properties through finetuning on synthetic data (Section 3), and examine active\ninheritance by leveraging generated synthetic data to target potential points of improvement based\nupon these metrics (Section 4). Table 1 provides an overview of the metrics that we gather for\nour toolbox. Each of them comes with their own evaluation metric, implementation, and for the\nmajority-custom set of prompts (see Appendix B for details). We chose these metrics as they offer\ninsight into the LLM's inherited characteristics, which are often overlooked in general benchmarks.\nDetails about the models used, training, data distillation and evaluation benchmarks can be found\nin Appendix A.\n2.5.2 Passive Inheritance Experiments\nFor the first set of experiments, we study LLaMa2-7B and LLaMa2-13B Touvron et al. (2023) and\nMixtral-8x7B Jiang et al. (2024). All 3 LLMs take the role of the student model (i.e., model which is\ntrained on the synthetic dataset) and LLaMa2-7B and Mixtral-8x7B also take the role of the teacher\n(i.e., model used to generate synthetic data), resulting in a total of 6 student-teacher combinations.\nWe start by distilling data using the Alpaca prompts Taori et al. (2023) (52k instances) from each\nLLM and then use the created datasets to finetune each LLM as a student. By considering these\ncombinations we are able to examine two distinct scenarios: self-distillation where LLMs are\ntrained on data generated by themselves (LLaMa2-LLaMa2, Mixtral-Mixtral), and the standard\ndistillation scenario, where LLMs are trained on data generated by other models (LLaMa2-Mixtral,\nMixtral-LLaMa2) (see Section A for further details)."}, {"title": "3 Results: Passive Inheritance of Teacher Properties", "content": "3.1 Impact on Model Generation Properties\nIn this section we ask: how does passive inheritance impact model generation properties? We find\nthat while synthetic data might not impact general performance significantly (Table 5), it can cause\nremarkable changes in the scores across the profiling benchmarks (Figure 2).\n3.1.1 Overall changes\nWe consistently observe changes across various experiments involving different student and teacher\nmodels. Even though the Alpaca prompts used for data generation are neutral and not deliberately\nfocused on eliciting specific attributes, models are influenced in unforeseen ways (e.g. the student\nmodel does not strictly move towards the teacher's profile and other non-trivial directions of change).\n3.1.2 Social Bias\nIn Figure 2, we plot some of the changes due to passive inheritance. Firstly, looking at the social\nbias metrics, we see that, despite the domain of the prompts being neutral, there are noticeable\nchanges to the Stereotype Scores across all domains (e.g. race, gender, religion etc) considered\nin our chosen benchmarks. We observe relative changes of the overall social bias profile of some\nLLMs of up to 36% (i.e. Mixtral-LLaMa2-7B in Table 12). We also observe that some relative\nindividual changes are surprisingly large, with the disability bias score increasing by 80% (i.e., the\nLLaMa2-13B-LLaMa2-7B bias score increases from 7.71% to 13.88%). Interestingly, training on\ndata distilled from a model does not necessarily lead to replicating the model's profile. In fact, our\nresults show the opposite effect: the social bias metrics of a student model can decrease even when\nthe teacher model has higher social bias metrics (see Table 11).\n3.1.3 Textual characteristics\nSecondly, for textual characteristics, as seen in Figure 2, we observe varying behaviours depending\non the metrics analysed. We see smaller relative changes of around 8% for our chosen readability\nmetrics Gunning-Fog and Rix, which are proxies to measuring complexity in text. When it comes to\nlexical diversity, we are able to see changes of up to 16%, which are considered significant (Treffers-\nDaller et al., 2016). Finally, the metric where we see the biggest change by a large margin, is the\nmean number of tokens per generation, with over 100% increase in some instances (LLaMa2-7B on\nMixtral and Mixtral on LLaMa2-7B). On a related note, we observe that models that are trained on\nself-distilled data (LLaMa2-LLaMa2 and Mixtral-Mixtral) are less sensitive to changes than models\nthat were not self-distilled and trained on data distilled from another model (LLaMa2-Mixtral and\nMixtral-LLaMa2). Self-distilled models displayed not only smaller changes but also a slight decrease\nin mean number of tokens (see Table 16).\n3.1.4 Toxicity\nIn the case of toxicity, we observe noticeable changes across all models for both \"Expected Maximum\nToxicity\" and \"Toxicity Probability\" metrics, with an increase of up to 40% in the worst case observed\n(Mixtral finetuned on LLaMa2-7B distilled data). Interestingly, the toxicity scores followed the\nopposite trend of the social bias metrics, with the scores of 5 out of 6 models analysed increasing\nby at least 8% (see Table 14). This is consistent with previous works which observed increases in\nharmfulness after models were finetuned on utility-oriented datasets such as Alpaca (Qi et al., 2023).\nThey hypothesize that models might forget their initial safety alignment, which could explain the\nchanges with regards to toxicity.\nIn the Appendix Section E, we include a complete set of numbers for each finetuned model and\nabsolute changes between models.\n3.2 Impact on Model Preferences\nMotivated by the increasing use of LLMs as evaluators we examine how passive inheritance impacts\nmodel preferences when used in an LLM-as-a-judge scenario (Zheng et al., 2023; Dubois et al., 2024b).\nWe find that the origin of the synthetic data specifically, the LLM used to distill the data-directly\ninfluences the preferences of the models trained on this data. Details of our full experiment setup are\ngiven in Appendix C.\n3.2.1 Influence on Inter-Model Preference Agreements\nIn Figure 3 we illustrate the agreement rate, i.e., the percentage of times two models agree on the\nbest answer when shown the same pair of candidate generations, between all models before and\nafter data distillation. We observe that when models are trained on synthetic datasets generated\nby other models they inherit similar preferences from those models. At a maximum, we observe\nthat inter-model agreement increases by 13.20% after passive inheritance (for LLaMa2-Mixtral and\nMixtral). Additionally, we see that while self-distilled models start diverging slightly in terms of\nagreement after finetuning, their preferences mostly retain similarity to the teacher model, always\nstaying above 80%.\nFurthermore, we observe opposing behaviours when it comes to human agreement, namely that\nmodels finetuned on Mixtral's data increased their human agreement rate while the opposite happened\nwhen using LLaMa2's data. Mixtral, as a Mixture-of-Expert model, has a significantly larger effective\nsize of 35B and delivers higher-quality generations compared to its smaller LLaMa2 counterpart\nwith 7B parameters. This could explain the increase in alignment with human preferences of 2.7%\non average when Mixtral generations are used during finetuning versus the decrease of 5.67% when\nLlaMa2-7B-distilled data is used."}, {"title": "4 Active Inheritance of Desirable Non-Differentiable Properties", "content": "Our results in Section 3 confirm that even without constraining synthetic data generation, distillation\nresults in passive inheritance of teacher model properties and preferences. This motivates our next\nresearch question: \u2018Can we intentionally guide a model's discrete behavior and tendencies through\ndeliberate shaping of the data space?'. We explicitly constrain synthetic data to target specific\nattributes, thereby mitigating or enhancing desired characteristics.\n4.1 Enhancing Desired Attributes\nWe use prompts from the Alpaca dataset to generate responses from 5 distinct models: LLaMa2-7B,\nMixtral-8x7B, Gemma-7B, Aya-8B and Command-R+. This approach results in a high variety of\ngenerations per prompt in terms of textual characteristics.\n4.1.1 Comparison with random baseline\nAs described in Section 2, active inheritance involves choosing the sample for a given prompt with\nthe maximum for the desired property (or minimum if it is a lower-if-better metric). As a baseline,\nwe compare to a random selection from the available sample pool, sampling generations uniformly\nwith p(\u00b7 | x) = 1/k rather than the choosing the generation maximizing the targeted attribute\n(Eq. 2). We term this our \"random\" variant in plots.\n4.1.2 Sample Pool\nWe compare results given two different sample pools, either involving multiple samples of the same\nmodel (i.e., single-source strategy) or samples from multiple models (i.e., multi-source strategy). Note\nthat the prompts remain the same across all experiments and only the generations differ based on\nthe source they were sampled from.\n4.1.3 Multi-Source Generated Data\nTable 3 (Multi-source) shows the results. We observe that active inheritance effectively instills our\ndesired characteristics into the models while maintaining the overall performance. This pattern is\nconsistent across both LLaMa2-7B and Mixtral-8x7B models with the latter demonstrating more\nsignificant improvements. Finetuning these models with the filtered version of these datasets leads\nto an increase of the mean number of tokens per generation by at least 66% when compared to the\nbase model. However, while Mixtral shows improvements over the baseline, the LLaMa2 targeted\nmodel falls a bit short despite still increasing the mean length of generations if compared to the\nbase model prior to finetuning. As for lexical diversity, the mean MTLD score increases by 8% and\n15% points for LLaMa2-7B and Mixtral-8x7B, respectively. In both cases we observe substantial\nincreases over the baseline.\n4.1.4 Single-Source Generated Data\nCan the variability of generations of one model offer a similar range of diversity as using multiple\nmodels? This would allow us to streamline the process and reduce the overhead of having to sample\nfrom multiple models. In the case of this single-source strategy we sample from k = 10 candidate\nanswers generated by LLaMa2-7B. The results in Table 3 (Single-source) confirm that we successfully\nincrease both targeted metrics (length and lexical diversity) even when leveraging responses coming\nfrom a single model. While the increase in the mean number of tokens per generation is not as\nlarge as in the multi-source experiment, it is still considerable, especially for Mixtral-8x7B, which\nundergoes an increment of 111%, with both models surpassing the baseline by at least 8% tokens.\nOn the other hand, the increase in the MTLD score is greater for both models in this scenario, with\nimprovements of up to 40%, being at least 15% better than baseline.\n4.2 Mitigating Negative Attributes\nAfter successfully amplifying desired attributes using synthetic data, we investigate whether the\nsame strategies could be used to instead mitigate undesirable characteristics, such as toxicity. To"}, {"title": "5 Related Work", "content": "5.1 LLM circularity\nLLMs' quick quality improvement and widespread use in recent years have allowed for its use in\nmany research areas and also made it prevalent on the Internet (Shumailov et al., 2023), increasingly\ncontributing to the text found online. As a consequence of this recent phenomenon the issue of LLM\ncircularity (i.e., models influencing other LLMs via distilled data) has gained focus. Research to-date\nhas focused on two main areas: model degradation via recursive training (Dohmatob et al., 2024;\nBriesch et al., 2023; Shumailov et al., 2023) and self-preference in a LLM-as-a-Judge setting. On the\nside of model degradation, works have shown that training LLMs with data iteratively generated by\nother LLMs impairs performance as the tails of the original distribution start to disappear. Including\nwork on focusing solely on high frequency-contexts and therefore neglecting long-tail knowledge\n(Briesch et al., 2023; Bertrand et al., 2024; Shumailov et al., 2024) and loss of diversity (Guo et al.,\n2024). In contrast, our work explores how the transfer of characteristics via passive inheritance\noccurs when synthetic data generated by different LLMs is involved. We also conduct a far more\nextensive evaluation of traits such as social bias, toxicity and textual characteristics might be altered\nand/or amplified with the introduction of synthetic data.\nAs for self-preference, it has been shown that models tend to prefer their own generations when used\nas evaluators (Panickssery et al., 2024) aside from also displaying other cognitive biases (Zheng et al.,\n2023; Koo et al., 2023; Chen et al., 2024) which also affect their behavior and stray their preferences\naway from gold-standards. Nonetheless, previous studies have not investigated the potential influence\nof synthetic data on preference dynamics within this circular setting. Our research addresses this\ngap by examining the extent to which preferences can be influenced and/or altered through the\nincorporation of this type of data.\n5.2 Profiling LLMs\nAs LLMs become more prevalent in real world applications establishing benchmark and metrics to\nevaluate these models abilities in a diverse range of tasks becomes a crucial step to better understand\ntheir strengths and identify potential areas of improvement. LLMs are often evaluated across a\ndiverse set of tasks, such as reasoning (Zellers et al., 2019; Srivastava et al., 2023; Chollet, 2019)\nand QA abilities (Hendrycks et al., 2021; Lin et al., 2022), multilingual performance (\u00dcst\u00fcn et al.,\n2024; Aryabumi et al., 2024). Aside from these general performance benchmarks, many works have\nalso explored ways in which to quantify biases and other inherent characteristics related to these\nmodels, including but not limited to social biases and stereotypes (Nadeem et al., 2020; Nangia\net al., 2020; Parrish et al., 2022), toxicity (Gehman et al., 2020), preference biases (Koo et al., 2023),\nuncertainty (Liang et al., 2023) and lexical and stylistics characteristics pertaining to the generations\nof LLMs Hansen et al. (2023). By benchmarking these models in a wide range of categories we are\nnot only able to create a comprehensive profile of surface-level characteristics and tendencies of\nLarge Language Models but also explore how to make use of these metrics to improve our models\n(Meade et al., 2022; Schick et al., 2021).\n5.3 Optimizing for non-differentiable attributes\nThere is a rich history of optimizing for non-differentiable attributes within NLP research. Policy-\ngradient based reinforcement learning (RL) algorithms have been a popular choice to this aim,\ne.g., for maximizing various non-differentiable evaluation metrics like BLEU(RT) (Shen et al., 2016;\nRanzato et al., 2016; Sokolov et al., 2016; Kreutzer et al., 2017; Nguyen et al., 2017; Shu et al., 2021)\nor ROUGE (Ranzato et al., 2016). However, most of these methods focus on an online learning\nscenario, and some require additional estimators (Williams, 1992; Sutton et al., 1999). Thus, they are\ngenerally more unstable and computationally expensive than simple cross-entropy updates as in our\ncase (Bahdanau et al., 2017; Ding & Soricut, 2017; Ammanabrolu & Hausknecht, 2020; Ammanabrolu\net al., 2022; Martin et al., 2022), requiring multiple samples (Shen et al., 2016), or regularization (Ding\n& Soricut, 2017; Ranzato et al., 2016) to stabilize the optimization process. However, in the case\nof the recently popularized paradigm of RL from human feedback (RLHF) (Ziegler et al., 2019;\nStiennon et al., 2020), recent work show that the same instabilities are much less pronounced\n(Ahmadian et al., 2024). However, RLHF typically has the overhead of maintaining at least a reward\nmodel representing human preferences, where the scalar is directly used in online RL optimization\nthrough algorithms such as PPO (Schulman et al., 2017) or REINFORCE (Williams, 1992). Offline\nRLHF methods require access to the log-probabilities of the teacher policy (Ammanabrolu et al.,\n2022; Shu et al., 2021), or require filtering multiple generations in an iterative fashion (Dong et al.,\n2023). RLHF also typically requires maintaining a reference model in memory to prevent \"reward\nhacking\" (Hendrycks et al., 2022). In contrast, our work is not based upon an RL framework. Active\ninheritance does not require a reward model, nor does it need to maintain a reference model in\nmemory, but instead uses explicit scores with a non-differentiable metric of choice. Furthermore, our\nmethod does not require access to log probabilities of the model that generated the samples. This is\nparticularly useful given that often closed models do not provide log-probabilities."}, {"title": "6 Conclusion", "content": "This work explores the implications of integrating synthetic data into LLMs, specifically examining its\ninfluence on the models' characteristics and preferences. Through our analysis, we show how synthetic\ndata originating from different sources can shape and impact model attributes. Finally, we introduce\nactive inheritance as a strategy to steer generations towards desirable discrete non-differentiable\nattributes. Overall, our findings contribute to a deeper understanding of the unintended consequences\nof synthetic data usage and provide insights into how to tailor models towards desirable generation\nprofiles."}, {"title": "Limitations", "content": "This study provides preliminary insights into the viability of targeted data distillation as an\nenhancement technique for machine learning models. It is important to acknowledge several\nlimitations that may impact the generalizability of our findings, we leave them for future work:\nThere are various potential modifications (teacher and student choices, sampling hyperparameters,\nfinetuning iterations, etc.) that could be explored for studying the guided distillation framework even\nmore comprehensively. Additionally, the metrics we employ in guided distillation are not entirely\nindependent of other latent variables. While we aim to isolate the impact of individual metrics,\nchanges in one metric could inadvertently cause variations in others, which were not monitored or\naccounted for. Lastly, the metrics within our profiling toolbox vary in nature. Some metrics depend\non leveraging custom data sets (i.e., social bias and calibration), while others are more flexible\nand can be computed on any generated sequence, and therefore be optimized directly. The ease of\napplying active inheritance varies across these metric types, offering varying levels of flexibility and\ncomplexity in our ability to actively steer models."}]}