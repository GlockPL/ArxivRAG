{"title": "Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks", "authors": ["Lukas Netz", "Jan Reimar", "Bernhard Rumpe"], "abstract": "We present and evaluate a method called grammar masking, which is used to guide large language models (LLMs) toward producing syntactically correct models for a given context-free grammar. Prompt engineering methods such as few-shot learning or priming can be used to improve the chances of an LLM producing correct syntax, but the more complex the grammar, the more time-consuming and less promising these methods become. Previous work is focused primarily on the usage of either language model training or prompt engineering. In this work, a method is presented that restricts the output to a given grammar using constrained decoding to ensure the output adheres to a valid syntax. We use several DSLs built with MontiCore and task multiple LLMs to produce models with and without constrained decoding. A corresponding parser is used to confirm the syntactic correctness of each model. We show that grammar masking can dramatically improve the modeling capabilities of several LLMs, reducing the need for well-refined prompting while increasing the chance of producing correct models.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLM) [21, 37] are highly sophisticated tools that, among other things, are capable of generating code artifacts based on a natural language input [7, 14, 28]. As we operate in the context of model-driven software engineering, we focus on the synthesis of textual models using the predefined syntax of a given domain-specific language (DSL). Given that the syntax definition of the targeted DSL might not be included in the corpus of training data used for the language model, it is necessary to rely on post-training optimization techniques such as few-shot learning [9, 16], fine-tuning [27], or prompt engineering [13, 23]. However, as these methods rely on prompt engineering, they have one common element: they only improve the likelihood that the LLM produces syntactically correct models but cannot guarantee it. In this work, we introduce an approach that uses the context-free grammar (CFG) of the targeted DSL to filter out any syntactically invalid output during the generation process of the LLM. We will evaluate results by comparing this approach to previous successful modeling tasks for LLMs using only few-shot learning."}, {"title": "2 Foundations", "content": "We introduce several foundations, such as the used framework Guidance, and the DSLs for which we will generate models."}, {"title": "2.1 Large Language Model", "content": "A LLM is a language model that is trained on a vast amount of text data. It is distinguished by its capability for general-purpose language understanding and generation. These models acquire their abilities by learning statistical relationships from text documents through a computationally intensive self-supervised and semi-supervised training process [32]. LLMs can perform text generation, a type of generative AI [4], by taking an input text and iteratively predicting the next token or word. In the context of software engineering, LLMs present significant potential to enhance and automate various tasks [22], particularly those related to Model-Driven Software Engineering (MDSE) and modeling languages [8, 24]."}, {"title": "2.2 Few-Shot learning", "content": "Few-shot learning (FSL) is a well-established in-context learning approach for large language models [9, 16, 27]. A pre-trained LLM can be prompted with a set of N exemplary question-answer pairs $(q^i, a^i)_{i=1}^N$ before being provided with the actual question q. The FSL output a for the question q is defined as $P_{LLM}(a | q, (q^i, a^i)_{i=1}^N)$. In addition, further instructions can be added to improve the results [35]. Further work is published on the FSL improvement introducing intermediate reasoning steps (e.g. Chain of thought) [15, 34, 36]. The success of few-shot in-context learning depends on the utility of the implicit knowledge within the provided examples and the clarity with which the task specifications are communicated through the provided examples. In the case of Domain-Specific Languages, the structured nature of the combinatorial output space, represented by the CFG of the DSL, is not easily covered by the limited number of demonstrations. Thus, generating models for a DSL with an FSL-based approach remains a significant challenge for LLMs."}, {"title": "2.3 Guidance-AI", "content": "Guidance\u00b9 is a tool designed to optimize and enhance the process of generating text output with Large Language Models. It provides a flexible and efficient way to control and 'guide' the output of these models to achieve specific goals or adhere to desired formats. In our case, we use the formatting capabilities of guidance to produce output that adheres to the formatting rules of a given DSL. Guidance internally defines a DSL implemented by developers from Microsoft for structured prompting of LLMs. Any prompting template consisting of a mix of unconstrained generation, function calls, constant strings, or grammar-constrained generation is transformed into a tree-like data structure, where each node represents different parts of the grammar. The core classes are Function and its subclasses GrammarFunction, which represents grammar rules, and RawFunction, which is used to interleave native Python functions within a grammar. Grammar functions are either a join or a select, and regex expressions in the grammar are reduced to these two operations. Terminals are either individual bytes or byte ranges. Figure 1 illustrates a simple grammar tree with the possible productions \"a\" or \"ab\".\nThe main idea for achieving a structured output is online parser-guided generation synchronizing a parser and scanner with an LLM to determine valid tokens at each step dynamically. In the main Loop, as seen in figure Figure 3. When LLM generates text, it predicts the next Token in a sequence. For each possible token, a logit is generated, representing the confidence that this token is correct based on the training of the LLM. To those confidence values, a softmax function is applied to the confidence scores so they all add up to 1 and can be used as probabilities. Based on these probabilities, a multinomial distribution is calculated. Now, for each step, the tokens are tried one by one and checked by the guidance parser to see if they result in a valid partial parse.\nThe Parser is an Earley Parser constructed by Guidance based on the supplied grammar. An Earley parser efficiently processes context-free grammars in three phases. During prediction, it generates new states based on grammar rules for non-terminals. In the scanning phase, it matches and consumes terminal symbols in the input. Completion advances states when a rule ends, preparing for the next parsing steps. This method handles all possible paths, accommodating ambiguous and complex grammars effectively. Many other Frameworks only support subsets of CFGs since they are based on LR(1) or LALR(1) parsers. The Guidance parser enhances the standard Earley parser by introducing commit points, which force the parser to commit to specific parse paths, effectively pruning the search space and avoiding backtracking. No other alternatives are considered once a commit point is reached, ensuring that the parser adheres strictly to chosen paths.\nThe parser in the guidance framework employs several optimization strategies to enhance performance and efficiency by minimizing the frequency of calls to the LLM. In figure Figure 3, we can also see that tries are used in two instances. A trie, or prefix tree, is a data structure that efficiently stores and retrieves keys, typically strings. Each node represents a common prefix shared by some keys, allowing fast lookups by a prefix. The tokens of the LLM are converted to a trie, making it possible to identify possible"}, {"title": "2.4 MontiCore based Modeling Languages", "content": "As the chair for Software Engineering, we develop and maintain the language workbench MontiCore2 [18]. The language workbench is employed to generate a number of DSLs. For brevity, we will focus on two languages: one used to define requirements and specifications in simplified structured English (SEN) and another used to define UML class diagrams: CD4A."}, {"title": "2.4.1 Structured English \u2013 A Controlled Natural English (DSL) for Regulatory Comliance", "content": "This DSL was primarily developed to standardize the definition of requirements and is based on the work of Konrad and Cheng [19]. The DSL can be used to write requirements and expressions in controlled simplified English [17], while still being able to be parsed and processed by tooling."}, {"title": "3 Related Work", "content": "Although open-source frameworks such as Guidance have been published in recent years, little research has been done on using LLMs with constrained decoding as a modeling tool.\nInitial work was published by Wang et al. in [33]. The presented approach uses grammar prompting to guide an LLM towards a constrained output. They demonstrate the viability of their approach with a selection of DSLs and LLMs."}, {"title": "3.1 MBSE with generative AI", "content": "Bader et al. use an FSL-based approach on the GPT-3.5-turbo-1106 LLM to produce textual UML Models in XML notation based on natural-language input. In this work, Bader shows that valid models can be created; however also points out some challenges of the LLM-based approach, such as limited context length and hallucination-related problems with the generated models [6].\nTimperley et al. assess the usage of LLMs to generate model-based spacecraft system architectures [30]. The approach relies on generating textual models for system architectures, requirements, and ontologies. The analysis concludes that LLMs can provide a high degree of assistance in modeling tasks in the early stages of"}, {"title": "3.2 SynCode", "content": "SynCode [1, 31] is a framework for grammar-guided generation with large language models. SynCode tries to address these limitations by using an offline-constructed lookup table called the DFA mask store. This table is based on the DFA of the language grammar terminals and is designed to retain only syntactically valid tokens during the generation process.\nThe core of SynCode's approach involves a two-step process during the LLM decoding stage. First, the partial output generated by the LLM is parsed to produce accept sequences and a remainder. Accept sequences represent valid terminal sequences that can follow the current partial output, while the remainder accounts for any unparsed or partially parsed tokens. Next, using the accept sequences and remainder, SynCode traverses the DFA states and retrieves masks from the DFA mask store. These masks filter out syntactically invalid tokens from the LLM's vocabulary, ensuring that only valid tokens are considered during each step of the generation process.\nOne of the greatest advantages of this approach is that the entire constraint infrastructure can be pre-computed. By accepting a longer initial setup time to generate the mask stores, the inference process becomes significantly faster, with only a minimal overhead of about 10%, even for complex grammars. This makes it particularly well-suited for handling complex grammars, such as those found in MontiCore."}, {"title": "4 Approach", "content": "Within this work, we focus on the syntactic correctness of the models produced by the LLM. We evaluate by comparing two approaches: one that only uses FSL and one that combines FSL with grammar masking. Even if a model is syntactically correct, there can still be semantic errors, e.g. the model could be an empty model, which might be syntactically valid, but does not satisfy the given"}, {"title": "4.1 Using Few-Shot learning-based Modeling Method", "content": "So far, FSL is one of the best prompting approaches to get an LLM to produce syntax in a predefined grammar. One drawback is that its performance heavily relies on the complexity of the grammar, the dependency on the LLM's familiarity with the concepts underlying the modeling task, and a good selection of examples to represent the rules of the grammar of the targeted DSL. FSL is limited to a set of examples to convey all syntactic relevant elements of a grammar [33], while also passing on 'best practices' for a modeling task in this language. As LLMs have a tendency to lose accuracy on increasingly larger prompts [20], we have to choose the examples for each grammar, or even for each use case carefully. In our FSL approach, generic domain-independent examples for each grammar were selected, as our overreaching goal is the development of a domain-independent DSL-specific modeling approach, that is not optimized for a specific use case or domain. A higher accuracy is very likely, by narrowing down the approach to specific target domains and thus choosing corresponding examples from corresponding use cases.\nWithin this work, we compare our approach with the performance of the FSL approach developed in [24]. The approach is depicted in Figure 2. A user informally defines a modeling task, that is extended with sample models of the target DSL. The extended prompt is provided to a LLM and the resulting model is checked by a parser. All models are provided with the same prompt-engineering and with the same set of molding tasks. The computation, with the exception of the OpenAI model, was run on the same hardware."}, {"title": "4.2 Using a Grammar Masking-based Modeling Method", "content": "In our constrained decoding approach we use Guidance as discussed in Section 2.3. The constrained decoding approach is evaluated similarly as the FSL approach (cf. Figure 4. The prompt containing the modeling task is supplemented with the same additional models, as in the previous approach. The grammar of the target DSL is transformed and provided through Guidance to the LLM. All generated models are parsed with a parser that is based on the same grammar. The pipeline involves transforming the MontiCore Grammar using a Visitor Pattern into a Lark Grammar [2] (cf. Listing 2), which is then integrated with Guidance. A detailed setup is shown in Figure 3. The framework starts with an initial prompt, that at the beginning is also the current prompt. The framework is limited to a fixed set of tokens; if there are still tokens left, the system checks with the help of a grammar trie if there is an unambiguous continuation for the current prompt (e.g., 'bool' has to be completed to 'boolean'). This is used as a shortcut to circumvent LLM usage. If this is not the case, the LLM is used to recommend tokens (transformer), which are passed to an earley parser, which can identify"}, {"title": "5 Results", "content": "To evaluate the presented approach several class diagrams and structured English models were generated, the parsing subset (4.225 CD4A models and 359 SEN models) can be found here: [25]. The results (cf. Table 1) indicate that the constrained generation method significantly increases the percentage of syntactically correct outputs from 46.52% to 92.63% (Llama 3). However, this improvement comes at the cost of increased generation time, with constrained generation taking an average of 74.09 seconds compared to 5.71 seconds for unconstrained generation. Similar results are observed for other LLMs. 36.57% of the models produced by Phi3 Mini in 4 Bit Quantization in an unconstrained mode are parsable, compared to 86.98 % in the constrained mode. Gemma 7B in 4-Bit Quantization produces only 0.003% in an unconstrained mode, compared to 93.00% in the constrained mode. Mistral 7B in 3-Bit Quantization produces 20.99 % parsable models compared to 92.37% parsable models in a constrained configuration. Quantization was chosen to accommodate the hardware constraints of the experimental setup. Models were limited to 8GB. All models took significantly more computation time when using constrained decoding. Using the Phi3 model in a constrained configuration took 34 times longer than using the Model in an unconstrained configuration. These increases could be combated by pre/computing the constraints, e.g. by using the Syncode approach (cf. subsection 3.2) which is connected to further challenges.\nUsing the same first 100 prompts, GPT-40 [26] was also tested. We were unable to apply constrained decoding as it is a closed-source model. At the time of writing, GPT-40 is one of the most capable LLMs in several benchmarks [3]. Thus, it is expected that the model performs better (76% parsable models) in an unconstrained mode. However, most likely due to the communication overhead, the time needed to create a model is, on average, doubled in comparison to the locally running open-source models.\nWe also tested the Structured English DSL SEN in addition to the Class Diagram DSL CD4A. We observe the same patterns as in CD4A: In an unconstrained setting using Llama 3, 26.54% of the produced models are parsable, whereas in a constrained configuration, 90.26% are parsable. The same can be shown for the LLMs Phi3 and Mistral: significantly more produced models are parsable using constrained decoding, than using unconstrained generation. In comparison to the CD4A modeling task, GPT-40 does not perform significantly better than the locally running LLMs: with 24.63% parsable models in an unconstrained configuration.\nNot all runs could be completed in both cases (CD4A and SEN). Although 1000 prompts for CD4A and 123 promts for SEN were provided to the LLMs, a run was aborted in case a token limit was reached. Thus for example Llama3 8B 4-Bit only produced 991 models instead of 1000.\nConstrained decoding does not currently achieve complete correctness because Monticore's grammar includes keywords not yet supported by our approach. For example, enum (on, off, finished) is interpreted as a function. In contrast, Monticore's implementation would not allow enum to be read as a function name, thereby guiding the generation incorrectly. Most of these differences were adapted by hand.\nIn Table 2, we can see a peculiarity of constrained generation. While the overall number of language constructs used is similar, the number of compositions and associations is the same. This occurs because the model tries to extend tokens maximally. Due to the prompting and grammar constraints, both constructs, which have exactly 11 characters, are equally likely to be generated."}, {"title": "6 Discussion", "content": "The results shown in Table 1 and Table 2 show very promising results, in the following we discuss aspects such as limitations and generalizability of the approach."}, {"title": "6.1 Applicability to other grammars", "content": "The approach presented in this work is based on MontiCore Grammars, which are transformed into LARK grammars. Hence this approach can be applied to any MontiCore Grammar. MontiCore provides infrastructure that permits the developers to define context conditions (CoCos) [11]. These CoCos are rules that check the well-formedness of models. These context conditions are crucial"}, {"title": "6.2 Impact Grammar Masking on Model Semantics", "content": "The approach presented in this paper relies primarily on filtering out syntactically invalid tokens. Although we did not notice any changes in the semantics of the resulting models, we cannot rule out the possibility that relevant content is excluded from the model or modified so that the semantics of the model are changed. A closer look at random samples, in which both approaches resulted in syntactically correct models, revealed that models generated with the constrained approach do not systematically contain fewer elements than those generated with the unconstrained approach. Table 2 implies the opposite. The differences observed were mainly in the formatting and naming of elements.\nModels generated with contained decoding might be less detailed, as this does not necessarily appear in quantitative analysis (e.g. counting classes and attributes). A further in-depth analysis of the content would be necessary."}, {"title": "6.3 Choosing Between Constrained and Unconstrained Generation", "content": "As we could show in the case of the DSL CD4A, FSL alone can suffice to create an approach that is very likely to yield a syntactically correct model (e.g. by using a sufisticated LLM such as GPT-40 cf. Table 1). This approach is not limited to the DSLs presented in this paper and can be applied to further modeling languages with sufficient prompt engineering. Nevertheless, this approach requires experts in the field of generative AI (e.g. a Prompt Engineer) and experts in the specific modeling language to provide specific and ideal representative examples of the targeted language. Hence, an FSL-only-based approach would be unsuitable for developers unfamiliar with the targeted DSL. In contrast, the grammar-masking-based approach is less reliant on good prompting and can be derived from a given MontiCore grammar, thus only needing the grammar file and a few samples of the targeted modeling language to operate. In addition, the grammar masking approach presented in this work enables smaller less performant models to serve as modeling tools. Smaller models such as the Llama 3 8B in a 4-bit quantization can be executed on hardware that is available for the end user (e.g. NVIDIA GeForce RTX 3070), making this approach independent from external server clusters such as the ones needed for an OpenAI based approach."}, {"title": "6.4 Limitations", "content": "One of the key limitations of this approach is its missing support for context conditions. As context conditions often need the entire model to be applied, they can not be used in a filtering capacity during model creation. Thus, they have to be applied in a succeeding step after a model for a specific DSL has been created. Grammar masking reduces the number of models that do not adhere to the provided grammar, thus also increasing the overall number of correct models that potentially comply with the context conditions, as any syntactically invalid models are filtered out at a very early stage. Another limitation is the high degree of specialization in one grammar. This approach limits the LLM to only producing models for one specific grammar. A second setup and a delegator are needed if the framework is meant to switch between DSLs. Switching out the prompting alone will not suffice.\nAs mentioned above, grammars might need to be adapted and refined to operate with this approach, as LLMs tend to find loopholes in 'incompletely' defined grammars. This requires the developer who sets up the framework to have some experience in language design."}, {"title": "7 Conclusion", "content": "We were able to show that frameworks that enable constrained decoding enable smaller, less performant LLMs to produce syntactically correct models at a reasonable rate. Our experiments show, that grammar masking can significantly increase the chance of an LLM-based approach to produce valid models for a given DSL. We could demonstrate this improvement for two DSLs. Within this paper, we only address syntactic validation of the produced models. Further analysis has to be performed to systematically evaluate if there are systematic differences in the semantics of unconstrained and constrained models. In addition, an extensive difference in computation time between unconstrained and constrained generation was measured. As a result, we recommend that the method described in this work should only be used if a satisfactory outcome cannot be achieved using conventional prompt engineering methods. Improvements in frameworks, such as precomputations (Syncode) and runtime optimations, could soon reduce the gap in computation time."}]}