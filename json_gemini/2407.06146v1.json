{"title": "Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks", "authors": ["Lukas Netz", "Jan Reimar", "Bernhard Rumpe"], "abstract": "We present and evaluate a method called grammar masking, which\nis used to guide large language models (LLMs) toward produc-\ning syntactically correct models for a given context-free gram-\nmar. Prompt engineering methods such as few-shot learning or\npriming can be used to improve the chances of an LLM produc-\ning correct syntax, but the more complex the grammar, the more\ntime-consuming and less promising these methods become. Pre-\nvious work is focused primarily on the usage of either language\nmodel training or prompt engineering. In this work, a method is\npresented that restricts the output to a given grammar using con-\nstrained decoding to ensure the output adheres to a valid syntax.\nWe use several DSLs built with MontiCore and task multiple LLMs\nto produce models with and without constrained decoding. A corre-\nsponding parser is used to confirm the syntactic correctness of each\nmodel. We show that grammar masking can dramatically improve\nthe modeling capabilities of several LLMs, reducing the need for\nwell-refined prompting while increasing the chance of producing\ncorrect models.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLM) [21, 37] are highly sophisticated tools\nthat, among other things, are capable of generating code artifacts\nbased on a natural language input [7, 14, 28]. As we operate in\nthe context of model-driven software engineering, we focus on\nthe synthesis of textual models using the predefined syntax of\na given domain-specific language (DSL). Given that the syntax\ndefinition of the targeted DSL might not be included in the corpus\nof training data used for the language model, it is necessary to\nrely on post-training optimization techniques such as few-shot\nlearning [9, 16], fine-tuning [27], or prompt engineering [13, 23]."}, {"title": "2 Foundations", "content": "We introduce several foundations, such as the used framework\nGuidance, and the DSLs for which we will generate models."}, {"title": "2.1 Large Language Model", "content": "A LLM is a language model that is trained on a vast amount of\ntext data. It is distinguished by its capability for general-purpose\nlanguage understanding and generation. These models acquire\ntheir abilities by learning statistical relationships from text doc-\numents through a computationally intensive self-supervised and\nsemi-supervised training process [32]. LLMs can perform text gen-\neration, a type of generative AI [4], by taking an input text and\niteratively predicting the next token or word. In the context of\nsoftware engineering, LLMs present significant potential to en-\nhance and automate various tasks [22], particularly those related\nto Model-Driven Software Engineering (MDSE) and modeling lan-\nguages [8, 24]."}, {"title": "2.2 Few-Shot learning", "content": "Few-shot learning (FSL) is a well-established in-context learning\napproach for large language models [9, 16, 27]. A pre-trained LLM\ncan be prompted with a set of N exemplary question-answer pairs\n(q\u00b9, a\u00b9)\u2081 before being provided with the actual question q. The\nFSL output a for the question q is defined as $P_{LLM}(a|q, (q^i, a^i)_{i=1}^{N})$.\nIn addition, further instructions can be added to improve the results\n[35]. Further work is published on the FSL improvement introducing\nintermediate reasoning steps (e.g. Chain of thought) [15, 34, 36].\nThe success of few-shot in-context learning depends on the\nutility of the implicit knowledge within the provided examples and\nthe clarity with which the task specifications are communicated\nthrough the provided examples. In the case of Domain-Specific\nLanguages, the structured nature of the combinatorial output space,\nrepresented by the CFG of the DSL, is not easily covered by the\nlimited number of demonstrations. Thus, generating models for a\nDSL with an FSL-based approach remains a significant challenge\nfor LLMs."}, {"title": "2.3 Guidance-AI", "content": "Guidance\u00b9 is a tool designed to optimize and enhance the process\nof generating text output with Large Language Models. It provides\na flexible and efficient way to control and 'guide' the output of\nthese models to achieve specific goals or adhere to desired for-\nmats. In our case, we use the formatting capabilities of guidance\nto produce output that adheres to the formatting rules of a given\nDSL. Guidance internally defines a DSL implemented by developers\nfrom Microsoft for structured prompting of LLMs. Any prompting\ntemplate consisting of a mix of unconstrained generation, func-\ntion calls, constant strings, or grammar-constrained generation is\ntransformed into a tree-like data structure, where each node repre-\nsents different parts of the grammar. The core classes are Function\nand its subclasses GrammarFunction, which represents grammar\nrules, and RawFunction, which is used to interleave native Python\nfunctions within a grammar. Grammar functions are either a join\nor a select, and regex expressions in the grammar are reduced to\nthese two operations. Terminals are either individual bytes or byte\nranges. Figure 1 illustrates a simple grammar tree with the possible\nproductions \"a\" or \"ab\".\nThe main idea for achieving a structured output is online parser-\nguided generation synchronizing a parser and scanner with an\nLLM to determine valid tokens at each step dynamically. In the\nmain Loop, as seen in figure Figure 3. When LLM generates text,\nit predicts the next Token in a sequence. For each possible token,\na logit is generated, representing the confidence that this token\nis correct based on the training of the LLM. To those confidence\nvalues, a softmax function is applied to the confidence scores so\nthey all add up to 1 and can be used as probabilities. Based on these\nprobabilities, a multinomial distribution is calculated. Now, for each\nstep, the tokens are tried one by one and checked by the guidance\nparser to see if they result in a valid partial parse.\nThe Parser is an Earley Parser constructed by Guidance based\non the supplied grammar. An Earley parser efficiently processes\ncontext-free grammars in three phases. During prediction, it gen-\nerates new states based on grammar rules for non-terminals. In\nthe scanning phase, it matches and consumes terminal symbols in\nthe input. Completion advances states when a rule ends, prepar-\ning for the next parsing steps. This method handles all possible\npaths, accommodating ambiguous and complex grammars effec-\ntively. Many other Frameworks only support subsets of CFGs since\nthey are based on LR(1) or LALR(1) parsers. The Guidance parser\nenhances the standard Earley parser by introducing commit points,\nwhich force the parser to commit to specific parse paths, effectively\npruning the search space and avoiding backtracking. No other al-\nternatives are considered once a commit point is reached, ensuring\nthat the parser adheres strictly to chosen paths.\nThe parser in the guidance framework employs several opti-\nmization strategies to enhance performance and efficiency by min-\nimizing the frequency of calls to the LLM. In figure Figure 3, we\ncan also see that tries are used in two instances. A trie, or prefix\ntree, is a data structure that efficiently stores and retrieves keys,\ntypically strings. Each node represents a common prefix shared\nby some keys, allowing fast lookups by a prefix. The tokens of the\nLLM are converted to a trie, making it possible to identify possible"}, {"title": "2.4 MontiCore based Modeling Languages", "content": "As the chair for Software Engineering, we develop and maintain the\nlanguage workbench MontiCore2 [18]. The language workbench is\nemployed to generate a number of DSLs. For brevity, we will focus\non two languages: one used to define requirements and specifica-\ntions in simplified structured English (SEN) and another used to\ndefine UML class diagrams: CD4A."}, {"title": "2.4.1 Structured English \u2013 A Controlled Natural English (DSL) for Regulatory Comliance", "content": "This DSL was primarily developed to stan-\ndardize the definition of requirements and is based on the work of\nKonrad and Cheng [19]. The DSL can be used to write requirements\nand expressions in controlled simplified English [17], while still\nbeing able to be parsed and processed by tooling."}, {"title": "3 Related Work", "content": "Although open-source frameworks such as Guidance have been\npublished in recent years, little research has been done on using\nLLMs with constrained decoding as a modeling tool.\nInitial work was published by Wang et al. in [33]. The presented\napproach uses grammar prompting to guide an LLM towards a con-\nstrained output. They demonstrate the viability of their approach\nwith a selection of DSLs and LLMs."}, {"title": "3.1 MBSE with generative AI", "content": "Bader et al. use an FSL-based approach on the GPT-3.5-turbo-1106\nLLM to produce textual UML Models in XML notation based on\nnatural-language input. In this work, Bader shows that valid models\ncan be created; however also points out some challenges of the LLM-\nbased approach, such as limited context length and hallucination-\nrelated problems with the generated models [6].\nTimperley et al. assess the usage of LLMs to generate model-\nbased spacecraft system architectures [30]. The approach relies on\ngenerating textual models for system architectures, requirements,\nand ontologies. The analysis concludes that LLMs can provide a\nhigh degree of assistance in modeling tasks in the early stages of\nspacecraft design. However, the modeling process still requires\nhuman supervision and can not be yet fully automated. A simi-\nlar conclusion is drawn by Busch et al. [10]. In their approach, a\nLow-Code is developed using a visual modeling language. Similar\nto Timberly et al. full automation is not yet possible due to the\nuncertainty introduced by relying on an LLM to generate code.\n[5] explores the capabilities of current LLMs to create general-\npurpose code."}, {"title": "3.2 SynCode", "content": "SynCode [1, 31] is a framework for grammar-guided generation\nwith large language models. SynCode tries to address these limita-\ntions by using an offline-constructed lookup table called the DFA\nmask store. This table is based on the DFA of the language grammar\nterminals and is designed to retain only syntactically valid tokens\nduring the generation process.\nThe core of SynCode's approach involves a two-step process\nduring the LLM decoding stage. First, the partial output generated\nby the LLM is parsed to produce accept sequences and a remain-\nder. Accept sequences represent valid terminal sequences that can\nfollow the current partial output, while the remainder accounts\nfor any unparsed or partially parsed tokens. Next, using the ac-\ncept sequences and remainder, SynCode traverses the DFA states\nand retrieves masks from the DFA mask store. These masks filter\nout syntactically invalid tokens from the LLM's vocabulary, ensur-\ning that only valid tokens are considered during each step of the\ngeneration process.\nOne of the greatest advantages of this approach is that the en-\ntire constraint infrastructure can be pre-computed. By accepting a\nlonger initial setup time to generate the mask stores, the inference\nprocess becomes significantly faster, with only a minimal overhead\nof about 10%, even for complex grammars. This makes it particu-\nlarly well-suited for handling complex grammars, such as those\nfound in MontiCore."}, {"title": "4 Approach", "content": "Within this work, we focus on the syntactic correctness of the\nmodels produced by the LLM. We evaluate by comparing two ap-\nproaches: one that only uses FSL and one that combines FSL with\ngrammar masking. Even if a model is syntactically correct, there\ncan still be semantic errors, e.g. the model could be an empty model,\nwhich might be syntactically valid, but does not satisfy the given"}, {"title": "4.1 Using Few-Shot learning-based Modeling Method", "content": "So far, FSL is one of the best prompting approaches to get an LLM to\nproduce syntax in a predefined grammar. One drawback is that its\nperformance heavily relies on the complexity of the grammar, the\ndependency on the LLM's familiarity with the concepts underlying\nthe modeling task, and a good selection of examples to represent\nthe rules of the grammar of the targeted DSL. FSL is limited to\na set of examples to convey all syntactic relevant elements of a\ngrammar [33], while also passing on 'best practices' for a modeling\ntask in this language. As LLMs have a tendency to lose accuracy on\nincreasingly larger prompts [20], we have to choose the examples\nfor each grammar, or even for each use case carefully. In our FSL\napproach, generic domain-independent examples for each gram-\nmar were selected, as our overreaching goal is the development\nof a domain-independent DSL-specific modeling approach, that\nis not optimized for a specific use case or domain. A higher accu-\nracy is very likely, by narrowing down the approach to specific\ntarget domains and thus choosing corresponding examples from\ncorresponding use cases.\nWithin this work, we compare our approach with the perfor-\nmance of the FSL approach developed in [24]. The approach is\ndepicted in Figure 2. A user informally defines a modeling task, that\nis extended with sample models of the target DSL. The extended\nprompt is provided to a LLM and the resulting model is checked by a\nparser. All models are provided with the same prompt-engineering\nand with the same set of molding tasks. The computation, with the\nexception of the OpenAI model, was run on the same hardware."}, {"title": "4.2 Using a Grammar Masking-based Modeling Method", "content": "In our constrained decoding approach we use Guidance as discussed\nin Section 2.3. The constrained decoding approach is evaluated sim-\nilarly as the FSL approach (cf. Figure 4. The prompt containing the\nmodeling task is supplemented with the same additional models, as\nin the previous approach. The grammar of the target DSL is trans-\nformed and provided through Guidance to the LLM. All generated\nmodels are parsed with a parser that is based on the same grammar.\nThe pipeline involves transforming the MontiCore Grammar using\na Visitor Pattern into a Lark Grammar [2] (cf. Listing 2), which\nis then integrated with Guidance. A detailed setup is shown in\nFigure 3. The framework starts with an initial prompt, that at the\nbeginning is also the current prompt. The framework is limited to a\nfixed set of tokens; if there are still tokens left, the system checks\nwith the help of a grammar trie if there is an unambiguous contin-\nuation for the current prompt (e.g., 'bool' has to be completed to\n'boolean'). This is used as a shortcut to circumvent LLM usage. If\nthis is not the case, the LLM is used to recommend tokens (trans-\nformer), which are passed to an earley parser, which can identify\ninvalid token suggestions. Valid tokens are passed on and added to\nthe current output. The cycle starts again at the current output."}, {"title": "5 Results", "content": "To evaluate the presented approach several class diagrams and\nstructured English models were generated, the parsing subset (4.225\nCD4A models and 359 SEN models) can be found here: [25]. The\nresults (cf. Table 1) indicate that the constrained generation method\nsignificantly increases the percentage of syntactically correct out-\nputs from 46.52% to 92.63% (Llama 3). However, this improvement\ncomes at the cost of increased generation time, with constrained\ngeneration taking an average of 74.09 seconds compared to 5.71\nseconds for unconstrained generation. Similar results are observed\nfor other LLMs. 36.57% of the models produced by Phi3 Mini in 4\nBit Quantization in an unconstrained mode are parsable, compared\nto 86.98 % in the constrained mode. Gemma 7B in 4-Bit Quantiza-\ntion produces only 0.003% in an unconstrained mode, compared to\n93.00% in the constrained mode. Mistral 7B in 3-Bit Quantization\nproduces 20.99 % parsable models compared to 92.37% parsable\nmodels in a constrained configuration. Quantization was chosen to\naccommodate the hardware constraints of the experimental setup.\nModels were limited to 8GB. All models took significantly more\ncomputation time when using constrained decoding. Using the Phi3\nmodel in a constrained configuration took 34 times longer than\nusing the Model in an unconstrained configuration. These increases\ncould be combated by pre/computing the constraints, e.g. by using\nthe Syncode approach (cf. subsection 3.2) which is connected to\nfurther challenges.\nUsing the same first 100 prompts, GPT-40 [26] was also tested. We\nwere unable to apply constrained decoding as it is a closed-source\nmodel. At the time of writing, GPT-40 is one of the most capable\nLLMs in several benchmarks [3]. Thus, it is expected that the model\nperforms better (76% parsable models) in an unconstrained mode.\nHowever, most likely due to the communication overhead, the time\nneeded to create a model is, on average, doubled in comparison to\nthe locally running open-source models.\nWe also tested the Structured English DSL SEN in addition to\nthe Class Diagram DSL CD4A. We observe the same patterns as in\nCD4A: In an unconstrained setting using Llama 3, 26.54% of the\nproduced models are parsable, whereas in a constrained configura-\ntion, 90.26% are parsable. The same can be shown for the LLMs Phi3\nand Mistral: significantly more produced models are parsable us-\ning constrained decoding, than using unconstrained generation. In\ncomparison to the CD4A modeling task, GPT-40 does not perform\nsignificantly better than the locally running LLMs: with 24.63%\nparsable models in an unconstrained configuration.\nNot all runs could be completed in both cases (CD4A and SEN).\nAlthough 1000 prompts for CD4A and 123 promts for SEN were\nprovided to the LLMs, a run was aborted in case a token limit\nwas reached. Thus for example Llama3 8B 4-Bit only produced 991\nmodels instead of 1000.\nConstrained decoding does not currently achieve complete cor-\nrectness because Monticore's grammar includes keywords not yet\nsupported by our approach. For example, enum (on, off, finished)\nis interpreted as a function. In contrast, Monticore's implementa-\ntion would not allow enum to be read as a function name, thereby\nguiding the generation incorrectly. Most of these differences were\nadapted by hand."}, {"title": "6 Discussion", "content": "The results shown in Table 1 and Table 2 show very promising\nresults, in the following we discuss aspects such as limitations and\ngeneralizability of the approach."}, {"title": "6.1 Applicability to other grammars", "content": "The approach presented in this work is based on MontiCore Gram-\nmars, which are transformed into LARK grammars. Hence this\napproach can be applied to any MontiCore Grammar. MontiCore\nprovides infrastructure that permits the developers to define con-\ntext conditions (CoCos) [11]. These CoCos are rules that check the\nwell-formedness of models. These context conditions are crucial\nfor ensuring that models adhere to the specified rules and con-\nstraints of the language. DSLs with fewer CoCos are transferable\nto this method with less effort, as this approach only impacts the\nadherence to the grammar and not the CoCos. In addition, gram-\nmar masking is expected to yield fewer improvements on DSLs\nalready encountered in pretraining, such as PlantUML [29], or SQL,\nas these languages should already perform well. LLMs that are\nalready trained on a specific DSL will be more likely to produce\nsyntactically correct models.\nThis approach was developed with MontiCore grammars in mind.\nHowever, the approach can be applied to any grammar that is\ntransformable into a LARK grammar (cf. Figure 3, Figure 4)."}, {"title": "6.2 Impact Grammar Masking on Model Semantics", "content": "The approach presented in this paper relies primarily on filtering\nout syntactically invalid tokens. Although we did not notice any\nchanges in the semantics of the resulting models, we cannot rule\nout the possibility that relevant content is excluded from the model\nor modified so that the semantics of the model are changed. A\ncloser look at random samples, in which both approaches resulted\nin syntactically correct models, revealed that models generated\nwith the constrained approach do not systematically contain fewer\nelements than those generated with the unconstrained approach.\nTable 2 implies the opposite. The differences observed were mainly\nin the formatting and naming of elements.\nModels generated with contained decoding might be less detailed,\nas this does not necessarily appear in quantitative analysis (e.g.\ncounting classes and attributes). A further in-depth analysis of the\ncontent would be necessary."}, {"title": "6.3 Choosing Between Constrained and Unconstrained Generation", "content": "As we could show in the case of the DSL CD4A, FSL alone can suf-\nfice to create an approach that is very likely to yield a syntactically\ncorrect model (e.g. by using a sufisticated LLM such as GPT-40\ncf. Table 1). This approach is not limited to the DSLs presented in\nthis paper and can be applied to further modeling languages with\nsufficient prompt engineering. Nevertheless, this approach requires\nexperts in the field of generative AI (e.g. a Prompt Engineer) and ex-\nperts in the specific modeling language to provide specific and ideal\nrepresentative examples of the targeted language. Hence, an FSL-\nonly-based approach would be unsuitable for developers unfamiliar\nwith the targeted DSL. In contrast, the grammar-masking-based\napproach is less reliant on good prompting and can be derived from\na given MontiCore grammar, thus only needing the grammar file\nand a few samples of the targeted modeling language to operate. In\naddition, the grammar masking approach presented in this work\nenables smaller less performant models to serve as modeling tools.\nSmaller models such as the Llama 3 8B in a 4-bit quantization can\nbe executed on hardware that is available for the end user (e.g.\nNVIDIA GeForce RTX 3070), making this approach independent\nfrom external server clusters such as the ones needed for an OpenAI\nbased approach."}, {"title": "6.4 Limitations", "content": "One of the key limitations of this approach is its missing support\nfor context conditions. As context conditions often need the entire\nmodel to be applied, they can not be used in a filtering capacity dur-\ning model creation. Thus, they have to be applied in a succeeding\nstep after a model for a specific DSL has been created. Grammar\nmasking reduces the number of models that do not adhere to the\nprovided grammar, thus also increasing the overall number of cor-\nrect models that potentially comply with the context conditions,\nas any syntactically invalid models are filtered out at a very early\nstage. Another limitation is the high degree of specialization in one\ngrammar. This approach limits the LLM to only producing models\nfor one specific grammar. A second setup and a delegator are needed\nif the framework is meant to switch between DSLs. Switching out\nthe prompting alone will not suffice.\nAs mentioned above, grammars might need to be adapted and\nrefined to operate with this approach, as LLMs tend to find loopholes\nin 'incompletely' defined grammars. This requires the developer\nwho sets up the framework to have some experience in language\ndesign."}, {"title": "7 Conclusion", "content": "We were able to show that frameworks that enable constrained\ndecoding enable smaller, less performant LLMs to produce syntac-\ntically correct models at a reasonable rate. Our experiments show,\nthat grammar masking can significantly increase the chance of an\nLLM-based approach to produce valid models for a given DSL. We\ncould demonstrate this improvement for two DSLs. Within this\npaper, we only address syntactic validation of the produced models.\nFurther analysis has to be performed to systematically evaluate if\nthere are systematic differences in the semantics of unconstrained\nand constrained models. In addition, an extensive difference in\ncomputation time between unconstrained and constrained gener-\nation was measured. As a result, we recommend that the method\ndescribed in this work should only be used if a satisfactory out-\ncome cannot be achieved using conventional prompt engineering\nmethods. Improvements in frameworks, such as precomputations\n(Syncode) and runtime optimations, could soon reduce the gap in\ncomputation time."}]}