{"title": "PREDICTING EMOTION INTENSITY IN POLISH POLITICAL TEXTS: COMPARING SUPERVISED MODELS AND LARGE LANGUAGE MODELS IN A RESOURCE-POOR LANGUAGE", "authors": ["Hubert Plisiecki", "Piotr Koc", "Maria Flakus", "Artur Pokropek"], "abstract": "This study explores the use of large language models (LLMs) to predict emotion intensity in Polish political texts, a resource-poor language context. The research compares the performance of several LLMs against a supervised model trained on an annotated corpus of 10,000 social media texts, evaluated for the intensity of emotions by expert judges. The findings indicate that while the supervised model generally outperforms LLMs, offering higher accuracy and lower variance, LLMs present a viable alternative, especially given the high costs associated with data annotation. The study highlights the potential of LLMs in low-resource language settings and underscores the need for further research on emotion intensity prediction and its application across different languages and continuous features. The implications suggest a nuanced decision-making process to choose the right approach to emotion prediction for researchers and practitioners based on resource availability and the specific requirements of their tasks.", "sections": [{"title": "1 Introduction", "content": null}, {"title": "1.1 Significance of emotions in psychological science and present advancements in research on emotions", "content": "Over the past few decades, social scientists have broadened their research to understand the significant role that emotions play in human behavior and societal dynamics. This exploration has yielded important findings in political sciences (Mintz et al., 2022), sociology (Bericat, 2016; Turner & Stets, 2006), economics (Loewenstein, 2000), anthropology (Lutz & White, 1986), organizational research (Diener et al., 2020) as well other fields of social (Kleef, 2018) and psychological sciences (Derks et al., 2008).\nWhile investigating this role of emotions, many researchers concentrate on the question of whether an emotion is present, focusing on the categorical aspects of emotions (Fritz et al., 2009; Saarim\u00e4ki et al., 2016; Siedlecka & Denson, 2019; Tanaka-Matsumi et al., 1995). However, beyond the sole presence or absence of emotions, there is also their intensity, which was early recognized as necessary to understand human behaviors (Brehm, 1999; Plutchik, 1965). People often describe emotions like anger, sadness, or happiness in varying degrees, from none at all to very intense, and research indicates that emotion intensity is crucial in cognitive processing, social behavior, and communication within groups (Frijda et al., 1992; Niedenthal & Brauer, 2012; Reisenzein, 1994).\nThere is also the third general approach to studying emotions, i.e., dimensional models. In contrast to categorical and intensity approaches, dimensional models offer a different perspective by suggesting that emotions can be placed within a continuous space defined by specific dimensions representing fundamental properties of emotional states (Gendron & Feldman Barrett, 2009). The most recognized model from this branch is Russell's (1980) circumplex model of affect, which posits that all emotions can be characterized by two fundamental dimensions: valence, which is the degree of pleasure or displeasure, and arousal, the level of activation or deactivation associated with an emotion.\nAccess to numerous text data sources, including social media content, responses to open-ended questions in computer-based assessments, political declarations, newspapers, and online forums, offers an unprecedented opportunity to study emotions beyond the traditional settings of psychological laboratories. To capture emotions in text, scholars initially focused on valence, i.e., differentiating between positive and negative sentiment. However, an increasing body of research has demonstrated that emotions of the same valence can affect social processes in different ways (Druckman & McDermott, 2008; Nabi, 2003; Valentino et al., 2011; Vasilopoulos et al., 2019) and that those distinct (discrete) emotions, like anger and happiness, can be identified in text (e.g., Pennebaker & Francis, 1996). As a result, various tools for discrete emotion detection were created, mainly for the English language context, with far fewer tools available for other languages (Mohammad, 2016; \u00dcveges & Ring, 2023). The emotion intensity approach has been largely overlooked in natural language processing (NLP) applications. While some attempts at predicting intensity exist, they are very rare (e.g. Akhtar et al., 2020). We can attribute it to the straightforwardness of the discrete approach. For instance, annotating items regarding the binary occurrence of emotions is way easier than their intensity.\nRecently, large language models (LLMs) have contributed to advancements in NLP, including emotion classification. These models have demonstrated their effectiveness in accurately identifying discrete emotions in the text by leveraging vast data and complex pattern recognition capabilities (Koco\u0144 et al., 2023). Their success in this domain suggests the potential of LLMs to also aid in predicting emotion intensity, which has not yet been thoroughly investigated. The approach using LLMs is an auspicious direction for \u201cresource-poor languages\" (Mohammad 2016, 203), where researchers often encounter the problem of lacking adequate tools for analyzing emotions. A problem, which is difficult to solve as the expression of emotions is language (Bazzanella, 2004) and domain (Haselmayer & Jenny, 2017; Rauh, 2018) specific, which requires researchers to use or create linguistically adapted tools for a particular kind of corpora (\u00dcveges and Ring 2023).\nIn this work, we explore the potential of LLMs to replace human annotators and traditional predictive models in the task of predicting emotion intensity in one of the resource-poor languages, Polish, focusing on political texts. To do so, we build a corpus of political texts using different social media sources and have 10, 000 texts annotated by expert judges, whose reliability in assessing the intensity of emotions is evaluated. Then, we compare the performance of several LLMs to a supervised model that was trained on annotated datasets in predicting the intensity of emotions.\nThe results show that the supervised model trained on the annotated data generally outperforms the LLMs, offering marginally higher accuracy and lower variance. However, this comes at the cost of the resources needed for the data annotation. Overall, the findings hold promise for using LLMs to assess other continuous features in Polish and potentially extend to other resource-poor languages.\""}, {"title": "1.2 Emotions and social media - previous research", "content": "Researching emotions in social media is of high importance as these social platforms have evolved into significant channels for spreading opinions and emotions (Beskow & Carley, 2019) - primarily due to their striking popularity, with 77.8% of people over the age of 18 using them globally (DataReportal, 2023). Research indicates that social media emotions may motivate people to share certain content (Brady et al., 2017), buy commercial products (Lu et al., 2021), change their behaviors (McBride & Ball, 2022), or even divide and disrupt populations (Whitehead, 2016).\nIt also has been demonstrated that a reaction to social media posts may depend on the specific emotions enhanced by this content and the context of its origin. As for the former, for instance, emotions rich in arousal (e.g., anger and anxiety) may increase information sharing, regardless of emotional valence (Berger & Milkman, 2012; Stieglitz & Dang-Xuan, 2013). As for the latter, for example, in political discourse, content sharing is more probable if it includes fear bait or support communication (Walker et al., 2017).\nHowever, although there is general scientific consensus regarding the great utility of emotion evaluation in social media texts, the studies that have done so to date have been largely limited. They have either (1) measured emotion to a limited extent (e.g., limiting the number of estimated emotions or keywords to refer to them) or (2) failed to capture the nuances of emotional responses (e.g., their dimensional nature and intensity), ignoring emotions' complexity (Elfenbein & Ambady, 2002)."}, {"title": "1.3 Emotions intensity - promising research gap or scientific dead end?", "content": "The intensity of emotion was recognized early as an important and natural extension of the basic classification scheme in both theoretical and practical contexts (Ferrara & Yang, 2015; Qiu et al., 2020). One of the earlier significant attempts to use continuous emotion metrics was made by Strapparava and Mihalcea (2007). However, their results were not entirely valid. The dataset they used consisted of news headlines from major outlets like the New York Times, CNN, and BBC News, as well as from the Google News search engine. They prepared two datasets: a development dataset with 250 annotated headlines and a test dataset with 1,000 annotated headlines. Annotators were provided with six predefined emotion labels (i.e., anger, disgust, fear, joy, sadness, surprise) and asked to classify the headlines with the appropriate emotion label and/or with a valence indication (positive/negative). Additionally, an intensity scale ranging from 0 to 100 was added. The agreement between the six annotators on the emotions (calculated as the Pearson's correlation of their scores to the averaged scores of the other annotators) was as follows: 0.50 for anger, 0.44 for disgust, 0.64 for fear, 0.60 for joy, 0.68 for sadness, 0.36 for surprise, and 0.78 for valence.\nUsing NLP and a lexicon-based method, they were able to detect emotions with high accuracy (binary classification): 93.6% for anger, 97.3% for disgust, 87.9% for fear, 82.2% for joy, 89.0% for sadness, and 89.1% for surprise. However, the performance of automated systems for emotion intensity prediction, calculated as the correlation between the original scores and the system predictions, was low: 0.32 for anger, 0.19 for disgust, 0.45 for fear, 0.26 for joy, 0.41 for sadness, and 0.17 for surprise.\nThe next significant study on emotion intensity was conducted by Mohammad and Bravo-Marquez in (2017b). They created the first dataset called the \"Tweet Emotion Intensity Dataset\", which consisted of 7,097 tweets annotated regarding anger, fear, joy, and sadness intensities. The reliability of annotation for intensity, supported by best-worst scaling (BWS) technique, showed high Pearson correlation coefficients: 0.80 for anger, 0.85 for fear, 0.88 for joy, and 0.85 for sadness.\nIn the shared task using this dataset, 22 teams participated (Mohammad & Bravo-Marquez, 2017a), with the best-performing system achieving a Pearson correlation of 0.747 with the gold intensity scores, indicating that predicting emotion intensity is possible but challenging. Akhtar et al. (2020b) achieved the following correlations for predicted emotions with annotated emotions: 0.75 for anger, 0.71 for joy, 0.76 for sadness, and 0.78 for fear, with an average correlation of 0.75. This demonstrates that predicting the intensity of emotions, although difficult, is feasible with a reasonable level of reliability. However, this task is significantly more challenging than simple classification.\nDespite the apparent success in predicting emotion intensity, further research in this area has been limited, with few exceptions where emotion intensity has been applied to the study of empirical problems (Sharifirad et al., 2019). This leaves the intensity of emotions as a theoretically valid yet rarely explored area in emotion sentiment analysis."}, {"title": "1.4 LLMs as a method of classifying emotions - previous research", "content": "LLM's have been successfully used to predict some dimensions of emotions in text snippets. One of the experiments with Open Al models, both GPT3.5 and GPT4, have tested a variety of different annotation tasks, sentiment analysis and emotion recognition included and showed promising results, however its performance did not match that of the available State of the Art (SOTA) models at the time (Koco\u0144 et al., 2023), and compared to other annotation tasks fared poorly especially on those tasks which were related to emotion annotation where the difference between its results and those of the SOTA models ranged from 71.3% to 21.8%. This result has been confirmed by other research projects, where the models developed by Open AI have also fallen short of the SOTA (Amin et al., 2023; Krugmann & Hartmann, 2024). This however should not be interpreted as a rule for all LLMs as Amin and his team (2023) showed that the Llama model developed by Meta can match the SOTA performance on some benchmarks.\nWhile the models developed by Open AI might not provide the best results with regards to English benchmarks, they have been shown to be superior for the task of cross-lingual sentiment analysis (P\u0159ib\u00e1\u0148 et al., 2024) owing largely to their vast multilingual training data. For example, while the Llama model has been shown to be superior for some English emotion related tasks, due to its limited training set compared to the OpenAI models it fared worse on multilingual tasks. For that reason in the current study, we choose to focus on the performance of GPT3.5 and GPT4 models. Furthermore, the bar set by the SOTA models for the utilization of LLMs in resource-poor languages is considerably lower as the lack of the resources also leads to lower SOTA performance.\nAs LLMs can accept context alongside the task that they are supposed to complete, the In-Context Learning (ICL) technique has been used repeatedly to enhance their performance (Chochlakis et al., 2024; Koco\u0144 et al., 2023). This method relies on providing examples of the items the LLM is supposed to annotate, alongside their ground truth values in order to guide the model towards better solutions. It is also often referred to as multi-shot prediction. While this technique indeed has elevated the accuracy of LLM predictions for the most part, deeper analysis has shown that the model in many cases does not learn from the provided ground truth, but rather pays attention to the examples alone, which in turn prime the model towards similar examples that it has learned from its training set, resulting in better performance (Chochlakis et al., 2024). This could mean that multi-shot prompting should be less performant for low-resource languages as the model has been trained on comparatively less texts associated with them. While testing this hypothesis directly is beyond the scope of this research as we lack reliable control groups, we do employ multi-shot prompting in order to push the LLM to the edge of its performance, whether it works."}, {"title": "2 Materials and Methods", "content": null}, {"title": "2.1 Database preparation", "content": "Our research utilizes a comprehensive database of Polish political texts from social media profiles (i.e., YouTube, Twitter, Facebook) of 25 journalists, 25 politicians, and 19 non-governmental organizations (NGOs). The complete list of the profiles is available in the Appendix. For each profile, all available posts from each platform were scraped (going back to the beginning of 2019). In addition, we also used corpora, which consists of texts written by \u201ctypical\" social media users, i.e., non-professional commentators of social affairs. Our data consists of 1,246,337 text snippets (Twitter: 789490 tweets; Youtube: 42252 comments; Facebook: 414,595 posts).\nAs transformer models have certain limits, i.e., their use imposes limits on length, we implemented two types of modification within the initial dataset. First, since texts retrieved from Facebook were longer than the others, we have split them into sentences. Second, we deleted all texts that were longer than 280 characters.\nThe texts were further cleaned from social media artifacts, such as dates scrapped alongside the texts. Next, the langdetect (Danilak, 2021) software was used to filter out text snippets that were not written in Polish. Also, all online links and user names in the texts were replaced with \"_link_\u201d and \u201c_user_\", respectively, so that the model does not overfit the sources of information nor specific social media users.\nBecause most texts in the initial dataset were emotionally neutral, we filtered out the neutral texts and included only these snippets which had higher emotional content in the final dataset. Accordingly, the texts were stemmed and subjected to a lexicon analysis (Imbir, 2016) using lexical norms for valence, arousal, and dominance - the three basic components of emotions. The words in each text were summed up in terms of their emotional content extracted from the lexical database and averaged to create separate metrics for the three emotional dimensions. These metrics were then summed up and used as weights to choose 8,000 texts for the final training dataset. Additionally, 2,000 texts were selected without weights to ensure the resulting model could process both neutral and emotional texts. The proportions of the texts coming from different social media platforms reflected the initial proportions of these texts, resulting in 496 YouTube texts, 6,105 Twitter texts, and 3,399 Facebook texts.\""}, {"title": "2.2 Annotation Process", "content": "The final dataset consisting of 10,000 texts was annotated by 20 expert annotators (age: M = 23.89, SD = 4.10; gender: 80% female). All annotators were well-versed in Polish political discourse and were students of Psychology (70% of them were graduate students, which in the case of Polish academic education denotes people studying 4th and 5th year). Thus, they underwent at least elementary training in psychology.\nThe entire annotation process lasted five weeks. Each week, every annotator was given five sets of texts (out of 100 sets with 100 randomly assigned sentences each) that should be annotated in the given week. The sets were randomly assigned to annotators, considering the general assumption that five different annotators should annotate each set. Generally, annotators simultaneously annotated no more than 500 texts each week, preventing them from cognitive depletion's negative effects.\nAnnotators labeled each text based on the five basic emotions: happiness, sadness, anger, disgust, and fear. In addition, annotators were asked to label the texts with regard to an additional emotion, namely pride, and two general dimensions of emotions: valence and arousal. In all cases, annotators used a 5-point scale (in the case of emotions: 0 = emotion is absent, 4 = very high level of emotion; in the case of valence and arousal, we used a pictographic 5-point scale provided in the Appendix).\nSince two additional emotional dimensions might not have been familiar to annotators, before the formal annotation process began, all annotators were informed about the characteristics of valence and arousal (note that we did not provide formal definitions of basic emotions). General annotation guidelines were provided to ensure consistency and minimize subjectivity (all instructions used within the training process are available in the Appendix)."}, {"title": "2.3 Statistical analyses", "content": null}, {"title": "2.3.1 Annotation Agreement", "content": "We assessed the agreement between raters using the intraclass correlation coefficient (ICC). The ICC coefficients are based on the random model ANOVA for independent groups (McGraw & Wong, 1996; Shrout & Fleiss, 1979). ICC(1) measures the reliability of single-ratings. ICC(1) compares the variability between raters to the total variability across all ratings. It assesses how much of the total variance in the scores is due to the variance between the rated texts. It assumes that a different rater rates each text, and the raters are randomly selected. It determines the consistency of raters' evaluations across texts when a randomly selected rater assesses each text. The ICC(1,k) coefficient extends the concept of single-rating reliability, as measured by ICC(1), to scenarios where the average ratings from a set of k raters evaluate each subject. Specifically, it assesses the absolute agreement among these raters, considering the mean of their ratings for each text. This approach acknowledges the increased reliability expected when aggregating evaluations from multiple raters. The ICC values range from 0 to 1, with 0 indicating no agreement among raters and 1 indicating perfect reliability. Koo and Li (2016) provide a guideline for interpreting ICC values, categorizing them as follows: values below 0.50 are considered poor; values ranging from 0.50 to 0.75 indicate moderate reliability; values between 0.75 and 0.90 suggest good reliability; and values above 0.90 are deemed excellent. To estimate the ICC, we used the pingouin Python package (Vallat, 2018)."}, {"title": "2.3.2 Data for training, validation and testing", "content": "After the annotation steps, we averaged the annotations corresponding to specific emotional metrics for each text. As the emotional load of the texts was still highly skewed towards lack of emotions, z scores for all of the emotions were computed, summed up, and used as weights to sample the test set, which constituted 10% of the total dataset. We did this to prevent the model from overfitting the lack of emotions by assigning low emotions to every predicted text. The remaining data was split into a training set and validation set, rearing a split of (8:1:1)."}, {"title": "2.3.3 Model Architecture", "content": "We considered two alternative base models: the Trelbert transformer model developed by a team at DeepSense (Szmyd et al., 2023), and the Polish Roberta model (Dadas, 2020). The encoders of both models were each equipped with an additional regression layer with a sigmoid activation function. The maximum number of epochs in each training run was set to 100. At each step, we computed the mean correlation of the predicted metrics with their actual values on the evaluation batch, and the models with the highest correlations on the evaluation batch were saved to avoid overfitting. We used the MSE criterion to compute the loss alongside the AdamW optimizer with default hyperparameter values. Both of the base models were then subjected to a Bayesian grid search using the WandB platform (Wandb/Wandb, 2017/2024) with the following values: dropout - 0; 0.20, 0.40, 0.60; learning rate - 5e-3, 5e-4, 5e-5; weight decay -"}, {"title": "2.3.4 Robustness Analysis", "content": "To assess the robustness of the model when trained on different subsets of the data, we performed a k-fold analysis with the same parameters as those chosen through the Bayesian grid search. We split the dataset into ten folds. On each iteration, one partition was held out, and the rest were split into the training and validation set (889 to 111 ratio to ensure approximately the exact size of the validation and test set). Then, we trained the model using the exact same method as described in the Model Architecture section."}, {"title": "2.3.5 LLM Testing", "content": "To assess the ability of LLMs to annotate the dataset properly, we have queried both gpt3_5_turbo_0125 (GPT3.5) and gpt-4-0613 (GPT4) with the multiple shot technique. Also, we have tested the GPT3.5 on the zero, one, and up to five-shot setup to estimate the best-performing multiple-shot setup. The tests have been completed on the validation set in order not to overfit the test set. The discrete emotions were tested with the following query (The prompts have been translated for the purpose of presentation):\nTranslation:\n\"To what extent does the text below manifest the emotion 'emotion'? Respond using a 5-point scale, where 1 means the emotion is not present at all and 5 means the emotion is very distinctly present. Please respond with a single number. Text: 'text' Your response:\"\nWhile the dimensions of valence and arousal had these prompts:\nValence:\n\"What emotional valence do you read in the following text? Respond using a 5-point scale, where 1 indicates a negative emotion is present and 5 indicates a positive emotion is present. Please respond with a single number.\"\nArousal:\n\"What level of arousal do you read in the following text? Respond using a 5-point scale, where 1 means no arousal and 5 means extreme arousal. Please respond with a single number.\"\nDue to the difference in the prompts as well as the qualitative difference between the dimensions and basic emotion, we have conducted two separate tests for each type of emotion taxonomy (basic vs dimensional affective metrics). The prompts were created based on the questions that annotators provided during the annotation process. They were structured in accordance with the official OPENAI prompt engineering guidelines (OpenAI Platform, n.d.). For an in-depth description of how the prompts were structured see Appendix.\nThe examples for the multiple-shot scenarios were picked in the following manner. First, we have vectorized the training set using the text-embedding-3-small model from the OPENAI API. Based on the resulting vectors, we calculated the centroid of the embeddings to represent the central point of our dataset. We then determined each text's distance from this centroid to assess its representativeness or deviation from the rest of the texts in the dataset. We wanted the example texts to be as representative of the whole dataset as possible. Then, for the one-shot scenario, we calculated the distance of each text from the midpoint on their corresponding emotional scales for each emotion separately. By combining these two types of metrics, we have then picked the texts that are both the most representative in terms of vector similarity and were rated to express their corresponding emotional constructs in neither a high nor low manner. We repeated the same operation for the two-shot scenario. However, the texts were picked based on the distance from the lowest point (first text) and the highest point (second text) on the emotional scale. The three-shot scenario combined the examples from one-shot and two-shot. The four-shot scenario picked texts were picked based on distance from the 0.20, 0.40, 0.60, and 0.80 points of the emotional scale. Finally, the five-shot scenario texts were picked based on the distance from the distance from the points of the emotional scale represented as the following fraction points: 1/6, 2/6, 3/6, 4/6, 5/6. The logic was to gradually present the LLM with a more fine-grained representation of the emotional spectrum.\nThere were multiple cases where the LLM did not respond to the request with an intelligible number, either refusing to honor the request based on the query not complying with OPENAI regulations or simply saying that it cannot assess the emotionality of the specific snippet. We considered this when picking the best multiple-shot scenario for each emotion taxonomy. The test results in the basic emotions condition showed that the three-shot method reared the best results for this task (see Table 1). The averaged correlation between the actual data and the scores provided by the LLM for all basic emotions achieved the highest level for this setting (r = 0.72). The averaged standard deviation of the scores for"}, {"title": "2.3.6 Costs", "content": "The participants in the annotation process were paid around $2,400 in total, split equally between them. At the same time, the calls to the API that were required to perform the multiple shot search totaled $8.38. The test set annotations, on the other hand, cost us $65.6, which was driven mostly by the GPT4 API calls."}, {"title": "3 Results", "content": "The ICC results were presented in Table 3. The reliability of individual rater's assessments ranged from poor to moderate across the tested emotions, with ICC (1) values extending from 0.29 for arousal to 0.60 for valence. In contrast, the reliability of average ratings from multiple raters indicated moderate to good consistency, with ICC (1, k) values ranging from 0.63 for fear to 0.88 for valence."}, {"title": "3.1 Supervised model results", "content": "The results of the main model, as summarized in Table 4, demonstrated the model's performance across different emotion categories and two general affect dimensions: valence and arousal. The table presents correlation coefficients and standard deviations (SDs) for the model predictions compared to human annotations, along with the original standard deviations observed in the human annotations.\nThe model exhibited strong correlations with human ratings, particularly in predicting happiness and valence, both achieving the highest correlation of 0.87. It indicated a high level of agreement between the model's predictions and human judgments for these emotional dimensions, suggesting that the model was particularly effective at identifying positive emotional content and overall emotional valence.\nCorrelations for other emotions, such as sadness (r = 0.75), anger (r = 0.85), disgust (r = 0.81), fear (r = 0.73), and pride (r = 0.80), also indicated a substantial agreement with human annotations, although to a slightly lesser extent than happiness and valence. These results suggested that the model can generally capture a wide range of emotional states, with varying degrees of effectiveness across different emotions.\nThe SDs of the predictions generated by the model (Model's SD) were consistently lower than those observed in averaged human annotations (\u201cAnnotator's SD\u201d) for all emotions and affect dimensions. This difference in variability indicated that the model's predictions tend to be more consistent than human ratings. For example, the model's predictions for happiness had an SD of 0.22, compared to the original human annotation SD of 0.26. Similar patterns were observed across all emotions and affect dimensions, with the model's predictions showing less variability than averaged human annotations.\nIn conclusion, the model demonstrated a strong ability to predict human emotional annotations across diverse emotions and affect dimensions. The high correlation values indicated a significant agreement between the model's predictions and human judgments. In contrast, the lower SDs in the model's predictions suggested a higher consistency in the model's performance compared to the variability inherent in human annotations. These results underscored the model's potential in effectively capturing and predicting human emotional responses to textual content."}, {"title": "3.2 K-fold validation", "content": "The results of the k-fold validation are presented in Table 5.\nIn assessing the robustness of the supervised model, we conducted a 10-fold cross-validation, focusing on a spectrum of emotional dimensions and affective states, including happiness, sadness, anger, disgust, fear, pride, valence, and arousal. The results revealed a generally high level of reliability across these dimensions. Specifically, the model exhibited strong performance in identifying happiness (mean correlation of 0.83, for 95% CI see Table 5), anger (r = 0.81), and valence (r = 0.84), indicating a consistent ability to assess these emotional states across different data subsets.\nModerate to strong correlations were observed for sadness (r = 0.68), disgust (r = 0.75), fear (r = 0.67), pride (r = 0.76), and arousal (r = 0.71), with the confidence intervals suggesting a stable performance across the folds, albeit with some variability, particularly in detecting fear. These outcomes did not highlight the supervised model's overall reliability and generalizability."}, {"title": "3.3 LLM Annotation Results", "content": "The LLM annotation attempts explored two distinct scenarios: a two-shot setup for assessing valence and arousal, and a three-shot setup for discrete emotions including happiness, sadness, anger, fear, disgust, and pride. The two-shot approach involved GPT3.5 two-shot and a variant leveraging GPT4, while the three-shot scenario explored a GPT3.5 three-shot alongside a three-shot GPT4 variant. These setups were selected based on prior tests to optimize the LLM's performance in emotion annotation.\nIn the two-shot setup for valence and arousal (Table 6), the GPT3.5 two-shot approach yielded correlations of 0.79 for valence and 0.53 for arousal, with standard deviations (SD) of 1.30 and 1.02, respectively. The two-shot GPT4 variant showed slightly improved performance with correlations of 0.79 for valence and 0.55 for arousal, and reduced SDs of 1.23 and 0.93, respectively. Notably, the GPT4 variant exhibited no rejected texts, suggesting enhanced reliability or acceptance criteria compared to the GPT3.5 two-shot setup, which had 36 rejected texts for valence and 2 for arousal.\nThe three-shot scenario focused on discrete emotions (Table 7) demonstrated varied performance across different emotions. The GPT3.5 three-shot approach showed correlations ranging from 0.46 for fear (the lowest) to 0.78 for anger (the highest), with corresponding SDs spanning from 1.22 for pride to 1.73 for fear. The number of rejected texts varied significantly across emotions, with fear seeing the highest rejection at 12 texts, indicating potential challenges in consistently annotating this emotion.\nThe three-shot GPT4 variant, however, marked a noticeable improvement in both correlation and SD across all emotions, with correlations improving to 0.88 for happiness and 0.83 for anger, among others. SDs were generally lower,"}, {"title": "3.4 Direct comparison", "content": "For the direct comparison we took the best performing LLM model results for both emotion categories, which was the GPT4. As can be seen in Table 8, for happiness, the GPT4 variant slightly outperformed the supervised model with a correlation of 0.88 (SD = 1.12) compared to the supervised model's 0.87 (SD = 0.22). This indicated a marginally higher accuracy in the GPT4 model, albeit with increased variability.\nIn the case of sadness, the supervised model exhibited a higher correlation of 0.75 (SD = 0.15) relative to the GPT4 variant's 0.66 (SD = 1.00), suggesting the supervised model's superior ability to accurately annotate sadness with less variability. For anger, the supervised model also showed a higher correlation of 0.85 (SD = 0.24) against the GPT4 variant's 0.83 (SD = 1.21), indicating a slight edge in accurately capturing expressions of anger, despite the GPT4 variant's broader range of responses. When assessing fear, the supervised model demonstrated a significantly higher correlation of 0.81 (SD = 0.19) compared to the GPT4 variant's 0.65 (SD = 1.09), underscoring the supervised model's enhanced capability in identifying fear-related expressions with greater consistency. For disgust, the correlation values were more similar, with the supervised model at 0.73 (SD = 0.11) and the GPT4 variant at 0.72 (SD = 1.00), suggesting comparable performance levels, though the GPT4 model exhibits greater variability. In evaluating pride, the supervised model's correlation of 0.80 (SD = 0.20) surpassed the GPT4 variant's 0.67 (SD = 0.85), indicating the supervised model's better performance in consistently capturing expressions of pride.\nRegarding valence, both models showed equivalent top performance with a correlation of 0.87 for the supervised model (SD = 0.22) and 0.88 for the GPT4 variant (SD = 1.12), albeit with the"}]}