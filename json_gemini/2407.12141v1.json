{"title": "PREDICTING EMOTION INTENSITY IN POLISH POLITICAL TEXTS: COMPARING SUPERVISED MODELS AND LARGE LANGUAGE MODELS IN A RESOURCE-POOR LANGUAGE", "authors": ["Hubert Plisiecki", "Piotr Koc", "Maria Flakus", "Artur Pokropek"], "abstract": "This study explores the use of large language models (LLMs) to predict emotion intensity in Polish political texts, a resource-poor language context. The research compares the performance of several LLMs against a supervised model trained on an annotated corpus of 10,000 social media texts, evaluated for the intensity of emotions by expert judges. The findings indicate that while the supervised model generally outperforms LLMs, offering higher accuracy and lower variance, LLMs present a viable alternative, especially given the high costs associated with data annotation. The study highlights the potential of LLMs in low-resource language settings and underscores the need for further research on emotion intensity prediction and its application across different languages and continuous features. The implications suggest a nuanced decision-making process to choose the right approach to emotion prediction for researchers and practitioners based on resource availability and the specific requirements of their tasks.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Significance of emotions in psychological science and present advancements in research on emotions", "content": "Over the past few decades, social scientists have broadened their research to understand the significant role that emotions play in human behavior and societal dynamics. This exploration has yielded important findings in political sciences (Mintz et al., 2022), sociology (Bericat, 2016; Turner & Stets, 2006), economics (Loewenstein, 2000), anthropology (Lutz & White, 1986), organizational research (Diener et al., 2020) as well other fields of social (Kleef, 2018) and psychological sciences (Derks et al., 2008).\nWhile investigating this role of emotions, many researchers concentrate on the question of whether an emotion is present, focusing on the categorical aspects of emotions (Fritz et al., 2009; Saarim\u00e4ki et al., 2016; Siedlecka & Denson, 2019; Tanaka-Matsumi et al., 1995). However, beyond the sole presence or absence of emotions, there is also their intensity, which was early recognized as necessary to understand human behaviors (Brehm, 1999; Plutchik, 1965). People often describe emotions like anger, sadness, or happiness in varying degrees, from none at all to very intense, and research indicates that emotion intensity is crucial in cognitive processing, social behavior, and communication within groups (Frijda et al., 1992; Niedenthal & Brauer, 2012; Reisenzein, 1994).\nThere is also the third general approach to studying emotions, i.e., dimensional models. In contrast to categorical and intensity approaches, dimensional models offer a different perspective by suggesting that emotions can be placed within a continuous space defined by specific dimensions representing fundamental properties of emotional states (Gendron &\nFeldman Barrett, 2009). The most recognized model from this branch is Russell's (1980) circumplex model of affect, which posits that all emotions can be characterized by two fundamental dimensions: valence, which is the degree of pleasure or displeasure, and arousal, the level of activation or deactivation associated with an emotion.\nAccess to numerous text data sources, including social media content, responses to open-ended questions in computer-based assessments, political declarations, newspapers, and online forums, offers an unprecedented opportunity to study emotions beyond the traditional settings of psychological laboratories. To capture emotions in text, scholars initially focused on valence, i.e., differentiating between positive and negative sentiment. However, an increasing body of research has demonstrated that emotions of the same valence can affect social processes in different ways (Druckman\n& McDermott, 2008; Nabi, 2003; Valentino et al., 2011; Vasilopoulos et al., 2019) and that those distinct (discrete)\nemotions, like anger and happiness, can be identified in text (e.g., Pennebaker & Francis, 1996). As a result, various tools for discrete emotion detection were created, mainly for the English language context, with far fewer tools available for other languages (Mohammad, 2016; \u00dcveges & Ring, 2023). The emotion intensity approach has been largely overlooked in natural language processing (NLP) applications. While some attempts at predicting intensity exist, they are very rare (e.g. Akhtar et al., 2020). We can attribute it to the straightforwardness of the discrete approach. For instance, annotating items regarding the binary occurrence of emotions is way easier than their intensity.\nRecently, large language models (LLMs) have contributed to advancements in NLP, including emotion classification. These models have demonstrated their effectiveness in accurately identifying discrete emotions in the text by leveraging vast data and complex pattern recognition capabilities (Koco\u0144 et al., 2023). Their success in this domain suggests the potential of LLMs to also aid in predicting emotion intensity, which has not yet been thoroughly investigated. The approach using LLMs is an auspicious direction for \u201cresource-poor languages\" (Mohammad 2016, 203), where researchers often encounter the problem of lacking adequate tools for analyzing emotions. A problem, which is difficult to solve as the expression of emotions is language (Bazzanella, 2004) and domain (Haselmayer & Jenny, 2017; Rauh, 2018) specific, which requires researchers to use or create linguistically adapted tools for a particular kind of corpora (\u00dcveges and Ring 2023).\nIn this work, we explore the potential of LLMs to replace human annotators and traditional predictive models in the task of predicting emotion intensity in one of the resource-poor languages, Polish, focusing on political texts. To do so, we build a corpus of political texts using different social media sources and have 10, 000 texts annotated by expert judges, whose reliability in assessing the intensity of emotions is evaluated. Then, we compare the performance of several LLMs to a supervised model that was trained on annotated datasets in predicting the intensity of emotions.\nThe results show that the supervised model trained on the annotated data generally outperforms the LLMs, offering marginally higher accuracy and lower variance. However, this comes at the cost of the resources needed for the data annotation. Overall, the findings hold promise for using LLMs to assess other continuous features in Polish and potentially extend to other resource-poor languages.\""}, {"title": "1.2 Emotions and social media - previous research", "content": "Researching emotions in social media is of high importance as these social platforms have evolved into significant channels for spreading opinions and emotions (Beskow & Carley, 2019) - primarily due to their striking popularity, with 77.8% of people over the age of 18 using them globally (DataReportal, 2023). Research indicates that social media emotions may motivate people to share certain content (Brady et al., 2017), buy commercial products (Lu et al., 2021), change their behaviors (McBride & Ball, 2022), or even divide and disrupt populations (Whitehead, 2016).\nIt also has been demonstrated that a reaction to social media posts may depend on the specific emotions enhanced by this content and the context of its origin. As for the former, for instance, emotions rich in arousal (e.g., anger and anxiety) may increase information sharing, regardless of emotional valence (Berger & Milkman, 2012; Stieglitz &\nDang-Xuan, 2013). As for the latter, for example, in political discourse, content sharing is more probable if it includes fear bait or support communication (Walker et al., 2017).\nHowever, although there is general scientific consensus regarding the great utility of emotion evaluation in social media texts, the studies that have done so to date have been largely limited. They have either (1) measured emotion to a limited extent (e.g., limiting the number of estimated emotions or keywords to refer to them) or (2) failed to capture the nuances of emotional responses (e.g., their dimensional nature and intensity), ignoring emotions' complexity (Elfenbein\n& Ambady, 2002)."}, {"title": "1.3 Emotions intensity - promising research gap or scientific dead end?", "content": "The intensity of emotion was recognized early as an important and natural extension of the basic classification scheme in both theoretical and practical contexts (Ferrara & Yang, 2015; Qiu et al., 2020). One of the earlier significant attempts to use continuous emotion metrics was made by Strapparava and Mihalcea (2007). However, their results were not entirely valid. The dataset they used consisted of news headlines from major outlets like the New York Times, CNN, and BBC News, as well as from the Google News search engine. They prepared two datasets: a development dataset with 250 annotated headlines and a test dataset with 1,000 annotated headlines. Annotators were provided with six predefined emotion labels (i.e., anger, disgust, fear, joy, sadness, surprise) and asked to classify the headlines with the appropriate emotion label and/or with a valence indication (positive/negative). Additionally, an intensity scale ranging from 0 to 100 was added. The agreement between the six annotators on the emotions (calculated as the Pearson's correlation of their scores to the averaged scores of the other annotators) was as follows: 0.50 for anger, 0.44 for disgust, 0.64 for fear, 0.60 for joy, 0.68 for sadness, 0.36 for surprise, and 0.78 for valence.\nUsing NLP and a lexicon-based method, they were able to detect emotions with high accuracy (binary classification): 93.6% for anger, 97.3% for disgust, 87.9% for fear, 82.2% for joy, 89.0% for sadness, and 89.1% for surprise. However, the performance of automated systems for emotion intensity prediction, calculated as the correlation between the original scores and the system predictions, was low: 0.32 for anger, 0.19 for disgust, 0.45 for fear, 0.26 for joy, 0.41 for sadness, and 0.17 for surprise.\nThe next significant study on emotion intensity was conducted by Mohammad and Bravo-Marquez in (2017b). They created the first dataset called the \"Tweet Emotion Intensity Dataset\", which consisted of 7,097 tweets annotated regarding anger, fear, joy, and sadness intensities. The reliability of annotation for intensity, supported by best-worst scaling (BWS) technique, showed high Pearson correlation coefficients: 0.80 for anger, 0.85 for fear, 0.88 for joy, and 0.85 for sadness.\nIn the shared task using this dataset, 22 teams participated (Mohammad & Bravo-Marquez, 2017a), with the best-performing system achieving a Pearson correlation of 0.747 with the gold intensity scores, indicating that predicting emotion intensity is possible but challenging. Akhtar et al. (2020b) achieved the following correlations for predicted emotions with annotated emotions: 0.75 for anger, 0.71 for joy, 0.76 for sadness, and 0.78 for fear, with an average correlation of 0.75. This demonstrates that predicting the intensity of emotions, although difficult, is feasible with a reasonable level of reliability. However, this task is significantly more challenging than simple classification.\nDespite the apparent success in predicting emotion intensity, further research in this area has been limited, with few exceptions where emotion intensity has been applied to the study of empirical problems (Sharifirad et al., 2019). This leaves the intensity of emotions as a theoretically valid yet rarely explored area in emotion sentiment analysis."}, {"title": "1.4 LLMs as a method of classifying emotions - previous research", "content": "LLM's have been successfully used to predict some dimensions of emotions in text snippets. One of the experiments with Open Al models, both GPT3.5 and GPT4, have tested a variety of different annotation tasks, sentiment analysis and emotion recognition included and showed promising results, however its performance did not match that of the available State of the Art (SOTA) models at the time (Koco\u0144 et al., 2023), and compared to other annotation tasks fared poorly especially on those tasks which were related to emotion annotation where the difference between its results and those of the SOTA models ranged from 71.3% to 21.8%. This result has been confirmed by other research projects, where the models developed by Open AI have also fallen short of the SOTA (Amin et al., 2023; Krugmann & Hartmann, 2024). This however should not be interpreted as a rule for all LLMs as Amin and his team (2023) showed that the Llama model developed by Meta can match the SOTA performance on some benchmarks.\nWhile the models developed by Open AI might not provide the best results with regards to English benchmarks, they have been shown to be superior for the task of cross-lingual sentiment analysis (P\u0159ib\u00e1\u0148 et al., 2024) owing largely to their vast multilingual training data. For example, while the Llama model has been shown to be superior for some English emotion related tasks, due to its limited training set compared to the OpenAI models it fared worse on multilingual tasks. For that reason in the current study, we choose to focus on the performance of GPT3.5 and GPT4 models. Furthermore, the bar set by the SOTA models for the utilization of LLMs in resource-poor languages is considerably lower as the lack of the resources also leads to lower SOTA performance.\nAs LLMs can accept context alongside the task that they are supposed to complete, the In-Context Learning (ICL) technique has been used repeatedly to enhance their performance (Chochlakis et al., 2024; Koco\u0144 et al., 2023). This method relies on providing examples of the items the LLM is supposed to annotate, alongside their ground truth values in order to guide the model towards better solutions. It is also often referred to as multi-shot prediction. While this technique indeed has elevated the accuracy of LLM predictions for the most part, deeper analysis has shown that the model in many cases does not learn from the provided ground truth, but rather pays attention to the examples alone, which in turn prime the model towards similar examples that it has learned from its training set, resulting in better performance (Chochlakis et al., 2024). This could mean that multi-shot prompting should be less performant for low-resource languages as the model has been trained on comparatively less texts associated with them. While testing this hypothesis directly is beyond the scope of this research as we lack reliable control groups, we do employ multi-shot prompting in order to push the LLM to the edge of its performance, whether it works."}, {"title": "2 Materials and Methods", "content": ""}, {"title": "2.1 Database preparation", "content": "Our research utilizes a comprehensive database of Polish political texts from social media profiles (i.e., YouTube,\nTwitter, Facebook) of 25 journalists, 25 politicians, and 19 non-governmental organizations (NGOs). The complete list\nof the profiles is available in the Appendix. For each profile, all available posts from each platform were scraped (going\nback to the beginning of 2019). In addition, we also used corpora, which consists of texts written by \u201ctypical\" social\nmedia users, i.e., non-professional commentators of social affairs. Our data consists of 1,246,337 text snippets (Twitter:\n789490 tweets; Youtube: 42252 comments; Facebook: 414,595 posts).\nAs transformer models have certain limits, i.e., their use imposes limits on length, we implemented two types of\nmodification within the initial dataset. First, since texts retrieved from Facebook were longer than the others, we have\nsplit them into sentences. Second, we deleted all texts that were longer than 280 characters.\nThe texts were further cleaned from social media artifacts, such as dates scrapped alongside the texts. Next, the\nlangdetect (Danilak, 2021) software was used to filter out text snippets that were not written in Polish. Also, all online\nlinks and user names in the texts were replaced with \"_link_\u201d and \u201c_user_\", respectively, so that the model does not\noverfit the sources of information nor specific social media users.\nBecause most texts in the initial dataset were emotionally neutral, we filtered out the neutral texts and included only\nthese snippets which had higher emotional content in the final dataset. Accordingly, the texts were stemmed and\nsubjected to a lexicon analysis (Imbir, 2016) using lexical norms for valence, arousal, and dominance - the three basic\ncomponents of emotions. The words in each text were summed up in terms of their emotional content extracted from\nthe lexical database and averaged to create separate metrics for the three emotional dimensions. These metrics were\nthen summed up and used as weights to choose 8,000 texts for the final training dataset. Additionally, 2,000 texts were\nselected without weights to ensure the resulting model could process both neutral and emotional texts. The proportions\nof the texts coming from different social media platforms reflected the initial proportions of these texts, resulting in 496\nYouTube texts, 6,105 Twitter texts, and 3,399 Facebook texts.\""}, {"title": "2.2 Annotation Process", "content": "The final dataset consisting of 10,000 texts was annotated by 20 expert annotators (age: M = 23.89, SD = 4.10; gender:\n80% female). All annotators were well-versed in Polish political discourse and were students of Psychology (70% of\nthem were graduate students, which in the case of Polish academic education denotes people studying 4th and 5th year).\nThus, they underwent at least elementary training in psychology.\nThe entire annotation process lasted five weeks. Each week, every annotator was given five sets of texts (out of 100\nsets with 100 randomly assigned sentences each) that should be annotated in the given week. The sets were randomly\nassigned to annotators, considering the general assumption that five different annotators should annotate each set.\nGenerally, annotators simultaneously annotated no more than 500 texts each week, preventing them from cognitive\ndepletion's negative effects.\nAnnotators labeled each text based on the five basic emotions: happiness, sadness, anger, disgust, and fear. In addition,\nannotators were asked to label the texts with regard to an additional emotion, namely pride, and two general dimensions\nof emotions: valence and arousal. In all cases, annotators used a 5-point scale (in the case of emotions: 0 = emotion is\nabsent, 4 = very high level of emotion; in the case of valence and arousal, we used a pictographic 5-point scale provided\nin the Appendix).\nSince two additional emotional dimensions might not have been familiar to annotators, before the formal annotation\nprocess began, all annotators were informed about the characteristics of valence and arousal (note that we did not\nprovide formal definitions of basic emotions). General annotation guidelines were provided to ensure consistency and\nminimize subjectivity (all instructions used within the training process are available in the Appendix)."}, {"title": "2.3 Statistical analyses", "content": ""}, {"title": "2.3.1 Annotation Agreement", "content": "We assessed the agreement between raters using the intraclass correlation coefficient (ICC). The ICC coefficients are\nbased on the random model ANOVA for independent groups (McGraw & Wong, 1996; Shrout & Fleiss, 1979). ICC(1)\nmeasures the reliability of single-ratings. ICC(1) compares the variability between raters to the total variability across\nall ratings. It assesses how much of the total variance in the scores is due to the variance between the rated texts. It\nassumes that a different rater rates each text, and the raters are randomly selected. It determines the consistency of\nraters' evaluations across texts when a randomly selected rater assesses each text. The ICC(1,k) coefficient extends the\nconcept of single-rating reliability, as measured by ICC(1), to scenarios where the average ratings from a set of k raters\nevaluate each subject. Specifically, it assesses the absolute agreement among these raters, considering the mean of their\nratings for each text. This approach acknowledges the increased reliability expected when aggregating evaluations from\nmultiple raters. The ICC values range from 0 to 1, with 0 indicating no agreement among raters and 1 indicating perfect\nreliability. Koo and Li (2016) provide a guideline for interpreting ICC values, categorizing them as follows: values\nbelow 0.50 are considered poor; values ranging from 0.50 to 0.75 indicate moderate reliability; values between 0.75 and\n0.90 suggest good reliability; and values above 0.90 are deemed excellent. To estimate the ICC, we used the pingouin\nPython package (Vallat, 2018)."}, {"title": "2.3.2 Data for training, validation and testing", "content": "After the annotation steps, we averaged the annotations corresponding to specific emotional metrics for each text. As\nthe emotional load of the texts was still highly skewed towards lack of emotions, z scores for all of the emotions were\ncomputed, summed up, and used as weights to sample the test set, which constituted 10% of the total dataset. We did\nthis to prevent the model from overfitting the lack of emotions by assigning low emotions to every predicted text. The\nremaining data was split into a training set and validation set, rearing a split of (8:1:1)."}, {"title": "2.3.3 Model Architecture", "content": "We considered two alternative base models: the Trelbert transformer model developed by a team at DeepSense (Szmyd\net al., 2023), and the Polish Roberta model (Dadas, 2020). The encoders of both models were each equipped with an\nadditional regression layer with a sigmoid activation function. The maximum number of epochs in each training run\nwas set to 100. At each step, we computed the mean correlation of the predicted metrics with their actual values on the\nevaluation batch, and the models with the highest correlations on the evaluation batch were saved to avoid overfitting.\nWe used the MSE criterion to compute the loss alongside the AdamW optimizer with default hyperparameter values.\nBoth of the base models were then subjected to a Bayesian grid search using the WandB platform (Wandb/Wandb,\n2017/2024) with the following values: dropout - 0; 0.20, 0.40, 0.60; learning rate - 5e-3, 5e-4, 5e-5; weight decay -"}, {"title": "2.3.4 Robustness Analysis", "content": "To assess the robustness of the model when trained on different subsets of the data, we performed a k-fold analysis\nwith the same parameters as those chosen through the Bayesian grid search. We split the dataset into ten folds. On\neach iteration, one partition was held out, and the rest were split into the training and validation set (889 to 111 ratio to\nensure approximately the exact size of the validation and test set). Then, we trained the model using the exact same\nmethod as described in the Model Architecture section."}, {"title": "2.3.5 LLM Testing", "content": "To assess the ability of LLMs to annotate the dataset properly, we have queried both gpt3_5_turbo_0125 (GPT3.5) and\ngpt-4-0613 (GPT4) with the multiple shot technique. Also, we have tested the GPT3.5 on the zero, one, and up to\nfive-shot setup to estimate the best-performing multiple-shot setup. The tests have been completed on the validation set\nin order not to overfit the test set. The discrete emotions were tested with the following query (The prompts have been\ntranslated for the purpose of presentation):\nTranslation:\n\"To what extent does the text below manifest the emotion 'emotion'? Respond using a 5-point scale, where 1 means the\nemotion is not present at all and 5 means the emotion is very distinctly present. Please respond with a single number.\nText: 'text' Your response:\"\nWhile the dimensions of valence and arousal had these prompts:\nValence:\n\"What emotional valence do you read in the following text? Respond using a 5-point scale, where 1 indicates a negative\nemotion is present and 5 indicates a positive emotion is present. Please respond with a single number.\"\nArousal:\n\"What level of arousal do you read in the following text? Respond using a 5-point scale, where 1 means no arousal and\n5 means extreme arousal. Please respond with a single number.\"\nDue to the difference in the prompts as well as the qualitative difference between the dimensions and basic emotion,\nwe have conducted two separate tests for each type of emotion taxonomy (basic vs dimensional affective metrics).\nThe prompts were created based on the questions that annotators provided during the annotation process. They were\nstructured in accordance with the official OPENAI prompt engineering guidelines (OpenAI Platform, n.d.). For an\nin-depth description of how the prompts were structured see Appendix.\nThe examples for the multiple-shot scenarios were picked in the following manner. First, we have vectorized the training\nset using the text-embedding-3-small model from the OPENAI API. Based on the resulting vectors, we calculated the\ncentroid of the embeddings to represent the central point of our dataset. We then determined each text's distance from\nthis centroid to assess its representativeness or deviation from the rest of the texts in the dataset. We wanted the example\ntexts to be as representative of the whole dataset as possible. Then, for the one-shot scenario, we calculated the distance\nof each text from the midpoint on their corresponding emotional scales for each emotion separately. By combining these\ntwo types of metrics, we have then picked the texts that are both the most representative in terms of vector similarity\nand were rated to express their corresponding emotional constructs in neither a high nor low manner. We repeated the\nsame operation for the two-shot scenario. However, the texts were picked based on the distance from the lowest point\n(first text) and the highest point (second text) on the emotional scale. The three-shot scenario combined the examples\nfrom one-shot and two-shot. The four-shot scenario picked texts were picked based on distance from the 0.20, 0.40,\n0.60, and 0.80 points of the emotional scale. Finally, the five-shot scenario texts were picked based on the distance from\nthe distance from the points of the emotional scale represented as the following fraction points: 1/6, 2/6, 3/6, 4/6, 5/6.\nThe logic was to gradually present the LLM with a more fine-grained representation of the emotional spectrum.\nThere were multiple cases where the LLM did not respond to the request with an intelligible number, either refusing to\nhonor the request based on the query not complying with OPENAI regulations or simply saying that it cannot assess the\nemotionality of the specific snippet. We considered this when picking the best multiple-shot scenario for each emotion\ntaxonomy. The test results in the basic emotions condition showed that the three-shot method reared the best results for\nthis task (see Table 1). The averaged correlation between the actual data and the scores provided by the LLM for all\nbasic emotions achieved the highest level for this setting (r = 0.72). The averaged standard deviation of the scores for"}, {"title": "2.3.6 Costs", "content": "The participants in the annotation process were paid around $2,400 in total, split equally between them. At the same\ntime, the calls to the API that were required to perform the multiple shot search totaled $8.38. The test set annotations,\non the other hand, cost us $65.6, which was driven mostly by the GPT4 API calls."}, {"title": "3 Results", "content": "The ICC results were presented in Table 3. The reliability of individual rater's assessments ranged from poor to\nmoderate across the tested emotions, with ICC (1) values extending from 0.29 for arousal to 0.60 for valence. In\ncontrast, the reliability of average ratings from multiple raters indicated moderate to good consistency, with ICC (1, k)\nvalues ranging from 0.63 for fear to 0.88 for valence."}, {"title": "3.1 Supervised model results", "content": "The results of the main model, as summarized in Table 4, demonstrated the model's performance across different\nemotion categories and two general affect dimensions: valence and arousal. The table presents correlation coefficients\nand standard deviations (SDs) for the model predictions compared to human annotations, along with the original\nstandard deviations observed in the human annotations."}, {"title": "3.4 Direct comparison", "content": "For the direct comparison we took the best performing LLM model results for both emotion categories, which was the\nGPT4. As can be seen in Table 8, for happiness, the GPT4 variant slightly outperformed the supervised model with\na correlation of 0.88 (SD = 1.12) compared to the supervised model's 0.87 (SD = 0.22). This indicated a marginally\nhigher accuracy in the GPT4 model, albeit with increased variability.\nIn the case of sadness, the supervised model exhibited a higher correlation of 0.75 (SD = 0.15) relative to the GPT4\nvariant's 0.66 (SD = 1.00), suggesting the supervised model's superior ability to accurately annotate sadness with less\nvariability. For anger, the supervised model also showed a higher correlation of 0.85 (SD = 0.24) against the GPT4\nvariant's 0.83 (SD = 1.21), indicating a slight edge in accurately capturing expressions of anger, despite the GPT4\nvariant's broader range of responses. When assessing fear, the supervised model demonstrated a significantly higher\ncorrelation of 0.81 (SD = 0.19) compared to the GPT4 variant's 0.65 (SD = 1.09), underscoring the supervised model's\nenhanced capability in identifying fear-related expressions with greater consistency. For disgust, the correlation values\nwere more similar, with the supervised model at 0.73 (SD = 0.11) and the GPT4 variant at 0.72 (SD = 1.00), suggesting\ncomparable performance levels, though the GPT4 model exhibits greater variability. In evaluating pride, the supervised\nmodel's correlation of 0.80 (SD = 0.20) surpassed the GPT4 variant's 0.67 (SD = 0.85), indicating the supervised\nmodel's better performance in consistently capturing expressions of pride.\nRegarding valence, both models showed equivalent top performance with a correlation of 0.87 for the supervised model\n(SD = 0.22) and 0.88 for the GPT4 variant (SD = 1.12), albeit with the GPT4 variant displaying higher variability. For\narousal, the supervised model's correlation of 0.75 (SD = 0.15) was notably higher than the GPT4 variant's 0.66 (SD =\n1.00), indicating the supervised model's superior accuracy and consistency in annotating arousal.\nIn summary, while the GPT4 variant demonstrated competitive or slightly superior performance in some respects\n(particularly for happiness and valence), the supervised model generally exhibited higher accuracy and significantly\nlower variability across most emotions and affective states, highlighting its robustness and reliability in emotion\nannotation tasks. The standard deviations of GPT4's predictions, on the other hand, were more similar to the standard\ndeviations of original annotations, before they were averaged to produce training data, while the standard deviations of\nthe supervised model, mirrored those of the averaged labels on which it was trained."}, {"title": "4 Discussion", "content": "As the results indicated, the question of whether researchers should use existing LLM models when annotating political\ntexts in low-resource languages such as Polish is nuanced. On the one hand, the supervised models provided marginally,\nyet visibly, more accurate results. They were either just as good as GPT-4 (in cases of happiness, disgust, and arousal)\nor better (for all other emotions). While the standard deviation of the LLMs' predictions was more similar to individual,\nnon-aggregated labels, it is not clear whether this should be considered an asset. This is because the standard deviations\nof arguably more representative, aggregated human emotionality labels were far smaller. These smaller values were\nmirrored in the distribution of the predictive model's labels. One significant advantage of the supervised model is its\nresilience to external circumstances, such as API availability. Once trained, the model can be stored on the researchers'\nmachines and reused at any time for practically free (excluding computing costs). The availability of the API, while not\ncompletely uncertain, is less reliable.\nOn the other hand, the supervised models require a costly annotation process that is orders of magnitude more resource\nintensive. One significant upside of this annotation process is that the data gathered can be opened to the large public as\nwe do so for this paper, and thus reused for different projects. The annotation issue is further complicated by the fact\nthat without gathering at least some annotations it is hard to estimate the reliability of LLMs for the specific task that it\nis supposed to be used for. Therefore, it is hard to avoid this laborious process. However, for evaluating the performance\nof the LLM without previous parameter searches with regards to a specific multiple shot setup the size of the annotated\ndataset can be significantly smaller than that required for supervised learning and can perhaps be carried out by the\nresearchers themselves. Of course, a smaller dataset also makes it difficult to choose examples for the multiple shot\nsetup. It also limits the possibility of the parameter search for the prompting technique which, when not carried out on a\nseparate validation set, can result in overfitting.\nThese considerations imply that the preference for use of either approach largely depends on the availability of resources,\nboth of financial and substantive nature. Research programs as well as commercial projects that have the option to\nengage in large scale annotation projects and train their own models will be rewarded for doing so by higher accuracy\nof their predictions, as well as more confidence in the long-term utility and reliability of their predictive solutions. On\nthe other hand, those teams which either do not have the resources necessary or do not want to spend them can opt for\nthe LLM-based approach, which will be marginally worse in performance but at the same time offers a fairly easier and\nfaster-to-implement solution.\nThe nature of the task such research teams strive to accomplish can thus be considered as another guide to choosing\nwhich approach works best for the team. Tasks that permit forgoing some accuracy for the sake of fast resolution are\ntherefore best suited for the LLM approach, while those in which small accuracy errors can propagate and multiply\nshould be tackled with the supervised method. Another important issue is the amount of data that has to be assessed.\nLLM approaches, while simpler and faster to implement, can run into scaling issues. This has to be considered before\nchoosing the approach by estimating the number of predictions that need to be made for the project and checking\nthe current prices of OPENAI calls. While for research purposes the cost of API calls will rarely be higher than\nthe cost of the annotation process, this might be of greater import to commercial projects. The study's findings\nneed to be viewed in light of the fast pace at which Large Language Models (LLMs) are evolving, which could\naffect the relevance of our results over time. As new models are developed, the performance and capabilities of\nLLMs might change, potentially limiting the applicability of our current conclusions. However, by making our code\npublicly available, we allow for the replication and updating of this study by others, which helps in maintaining the\nrelevance of the findings despite the rapid advancements in the field. The supervised model training code is avail-\nable at https://colab.research.google.com/drive/1ZIMIicDyEUVA-kHNXfHOoiUAPIVXCZyh?usp=drive_\nlink while the rest of the code, including LLM querying can be found at https://github.com/hplisiecki/\nPredicting-Emotion-Intensity-in-Polish-Political-Texts. We also welcome other researchers to use\nthe pretrained model introduced in this paper. To let them do that we have published it under the following url\nhttps://huggingface.co/hplisiecki/polemo_intensity. Since the data used to train and validate the model\ncome from social media profiles we choose to not publish it at this stage due to legal concerns, although we are working\non making it available in the future.\nFuture research could explore whether the findings of this study also hold for other resource-poor languages and,\npotentially, other continuous features. Also, one could add another approach to the comparison, involving machine\ntranslation into a language with existing labeled data, like English, to see if that is a viable option at least for some\nproblems (Licht et al., 2024)."}, {"title": "5 Funding", "content": "This research is funded by a grant from the National Science Centre (NCN) 'Research Laboratory for Digital Social\nSciences' (SONATA BIS-10, No. UMO-020/38/E/HS6/00302)."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 LLM Prompts", "content": "The prompts used for the LLM annotation process were structured as follows:\nBasic emotions (Happiness, Sadness, Anger, Disgust, Fear, Pride):\nTranslation:\n\"To what extent does the text below manifest the emotion 'emotion'? Respond using a 5-point scale, where 1 means the\nemotion is not present at all and 5 means the emotion is very distinctly present. Please respond with a single number.\nText: 'text' Your response:\"\nOriginal:\n\"Na ile przedstawiony poni\u017cej tekst manifestuje emocje \"emotion\". Odpowiedz u\u017cywaj\u0105c 5 stopniowej skali, gdzie 1 -\nemocja wog\u00f3le nie wyst\u0119puje a 5 - emocja jest bardzo wyra\u017anie obecna. Odpowiadaj za pomoc\u0105 pojedynczego numeru.\nTekst: \"text\" Twoja odpowied\u017a:\"\nValence\nTranslation:\n\"What emotional valence do you read in the following text? Respond using a 5-point scale, where 1 indicates a negative\nemotion is present and 5 indicates a positive emotion is present. Please respond with a single number.\"\nOriginal\n\u201cJaki znak emocji wyczytujesz w poni\u017cszym tek\u015bcie? Odpowiedz u\u017cywaj\u0105c 5 stopniowej skali, gdzie 1 - obecna jest\nnegatywna emocja a 5 - obecna jest pozytywna emocja. Odpowiadaj za pomoc\u0105 pojedynczego numeru.\u201d\nArousal\nTranslation:\n\"What level of arousal do you read in the following text? Respond using a 5-point scale, where 1 means no arousal and\n5 means extreme arousal. Please respond with a single number.\"\nOriginal:\n\u201cJaki poziom pobudzenia wyczytujesz w poni\u017cszym tek\u015bcie? Odpowiedz u\u017cywaj\u0105c 5 stopniowej skali, gdzie 1 - brak\npobudzenia a 5 - ekstremalne pobudzenie. Odpowiadaj za pomoc\u0105 pojedynczego numeru.\u201c\nFor the multiple-shot scenarios, the exemplars were added by appending them to the end of the queries above. They had\nthe following structure:\nTranslation:\n\"Text {text number}: \"\"\"{text}\"\"\" Your response: \"\"\"{score}\"\"\" ###\"\nOriginal:\n\"Tekst {text number}: \"\"\"{text}\"\"\" Twoja odpowied\u017a: \"\"\"{score}\"\"\" ###\"\nThe target text was finally appended in the same manner as the exemplars:\nTranslation:\n\"Text {text number}: \"\"\"{text}\"\"\" Your response:\nOriginal\n\"\n\"Tekst {text number}: \"\"\"{text}\"\"\" Twoja odpowied\u017a: \""}, {"title": "A.2 Annotation Process - instruction for annotators", "content": "Translation:\nYou will evaluate the emotional content displayed in some short texts.\nYour task will be to mark on a five-point scale the degree to which you think that a given sentence is characterized by\neach of the following emotions: joy, sadness, anger, disgust, fear, and pride. Use a five-point scale as described below:\n0 - the emotion does not occur at all\n1 - low level of emotion\n2 - moderate level of emotion\n3 - high level of emotion\n4 - very high level of emotion.\nThen, we will ask you to estimate the intensity of two additional emotion parameters: the direction of sensations\n(negative versus positive) and emotional arousal (no arousal versus extreme arousal). On the next screen you will learn\nthe definitions of both parameters and how you will evaluate them.\nRead the descriptions of two emotion parameters: the sign of sensations and emotional arousal. You can do this several\ntimes to make sure you understand them - it will make it easier for you to complete the task ahead of you.\nYou will rate each of the emotion dimensions described above on a five-point scale. To make it easier to imagine the\nstates we have in mind, you can use pictograms symbolizing different directions of experiences and the intensity of the\nemotional states.\nFor the direction of sensations, use the following scale: The first pictogram shows a person who is visibly depressed -\nspecific experiences may include: panic, irritation, disgust, despair, failure, or crisis. The last image shows a person\nwho is visibly excited - specific experiences may include: fun, delight, happiness, relaxation, satisfaction, or rest. The\nremaining pictograms represent intermediate states.\nFor emotional arousal, use the following scale: The first pictogram shows a person who is very calm, almost sleepy -\nspecific experiences may include: relaxation, calm, inactivity, meditation, boredom, or laziness. The last image shows\na person who is intensely aroused - appropriate emotional states may include: excitement, euphoria, arousal, rage,\nagitation, or anger.\nSave the link to this manual for later - you can return to it at any time during the examination.\nVery important: you can take a break while assessing your statements and return to them at any time - your current\nwork will be saved and you will be able to resume it after the break. If you want to do this, in the upper right corner\nof the screen you will find the option: \"Postpone for later\" - click on it, enter the data necessary to save, and confirm\nthe operation. In case you are ready to get back to work: when you enter the study page, an option \"Load unfinished\nsurvey\" will appear in the upper right corner of the screen - select it to load your work."}, {"title": "A.3 Social Media profiles", "content": "In this research we have scraped the posts of following:\nA) Journalists:\nAdrian Klarenbach, Agnieszka Gozdyra, Bartosz T. Wieli\u0144ski, Bartosz W\u0119glarczyk, Bianka Miko\u0142ajewska, Cezary\nKrysztopa, Daniel Liszkiewicz, Dawid Wildstein, Dominika D\u0142ugosz, Dominika Wielowieyska, Ewa Siedlecka, Jacek\nKarnowski, Jacek Kurski, Jacek Nizinkiewicz, Janusz Schwertner, Jaros\u0142aw Olechowski, Konrad Piasecki, Krzysztof\nZiemiec, \u0141ukasz Bok, \u0141ukasz Warzecha, Magdalena Og\u00f3rek, Magdalena Rigamonti, Marcin Gutowski, Marcin Wolski,\nMicha\u0142 Karnowski, Micha\u0142 Kolanko, Micha\u0142 Racho\u0144, Mi\u0142osz K\u0142eczek, Pawe\u0142 \u017buchowski, Piotr Kra\u015bko, Piotr Semka,\nRadomir Wit, Rafa\u0142 Ziemkiewicz, Renata Grochal, Robert Mazurek, Samuel Pereira, Szymon Jadczak, Tomasz Lis,\nTomasz Sakiewicz, Tomasz Sekielski, Tomasz Sommer, Tomasz Terlikowski, Wojciech Bojanowski, Agaton Kozi\u0144ski,\nPiotr Witwicki, Jacek Tacik, Magdalena Lucyan, Agata Adamek, Kamil Dziubka, Jaros\u0142aw Kurski, Dorota Kania, Ewa\nBugala, Zuzanna D\u0105browska, Karol Gac, Marcin Tulicki, Marzena Nykiel, Jacek Prusinowski, Pawe\u0142 Wro\u0144ski\nB) Politicians:\nDonald Tusk, Andrzej Duda, Rafa\u0142 Trzaskowski, Mateusz Morawiecki, S\u0142awomir Mentzen, Janusz Korwin-Mikke,\nGrzegorz Braun, Szymon Ho\u0142ownia, Rados\u0142aw Sikorski, Krzysztof Bosak, W\u0142adys\u0142aw Kosiniak-Kamysz, Borys\nBudka, Artur E. Dziambor, Marek Belka, Leszek Miller, Mariusz B\u0142aszczak, Roman Giertych, Franek Sterczewski,\nKonrad Berkowicz, Marek Jakubiak, Micha\u0142 Szczerba, Przemys\u0142aw Czarnek, Zbigniew Ziobro, Krzysztof Brejza,\nLeszek Balcerowicz, Izabela Leszczyna, Klaudia Jachira, Janusz Piechoci\u0144ski, Patryk Jaki, Robert Biedro\u0144, Krystyna\nPaw\u0142owicz, Katarzyna Lubnauer, Anna Maria Sierakowska, \u0141ukasz Kohut, Marcin Kierwi\u0144ski, Anna Maria \u017bukowska,\nMarian Bana\u015b, Dariusz Jo\u0144ski, Kamila Gasiuk-Pihowicz, Barbara Nowacka, Adrian Zandberg, Krzysztof \u015amieszek,\nPaulina Matysiak, Pawe\u0142 Kukiz, Micha\u0142 W\u00f3jcik, Sebastian Kaleta, Ma\u0142gorzata Wassermann, Joachim Brudzi\u0144ski,\nMaciej Konieczny, Marcelina Zawisza\nC) NGOS:\nPolska Akcja Humanitarna, Helsi\u0144ska Fundacja Praw Cz\u0142owieka, Polski Czerwony Krzy\u017c, Fundacja Dialog, Fundacja\nOcalenie, Fundacja Og\u00f3lnopolski Strajk Kobiet, Stowarzyszenie Amnesty International, Fundacja Centrum Praw Kobiet,\nStowarzyszenie S\u0119dzi\u00f3w Polskich IUSTITIA, Stowarzyszenie Marsz Niepodleg\u0142o\u015bci, Lekarze bez Granic, Fundacja\nTVN, Fundacja Dzieciom \"Zd\u0105\u017cy\u0107 z Pomoc\u0105\", Wielka Orkiestra \u015awi\u0105tecznej Pomocy, Szlachetna Paczka, Fundacja\nWWF Polska, Fundacja Greenpeace Polska, Liga Ochrony Przyrody, Zwi\u0105zek Stowarzysze\u0144 Polska Zielona Sie\u0107,\nM\u0142odzie\u017cowy Strajk Klimatyczny, Stowarzyszenie Mi\u0142o\u015b\u0107 Nie Wyklucza, Kampania Przeciw Homofobii, Stowarzysze-\nnie Lambda - Warszawa, Fundacja Trans-Fuzja, Stowarzyszenie Grupa Stonewall.\"\n    }"}]}