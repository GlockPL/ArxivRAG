{"title": "SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning", "authors": ["Heyi Zhang", "Yule Liu", "Xinlei He", "Jun Wu", "Tianshuo Cong", "Xinyi Huang"], "abstract": "Federated learning (FL) enables collaborative model training while preserving data privacy, but its decentralized nature exposes it to client-side data poisoning attacks (DPAs) and model poisoning attacks (MPAs) that degrade global model performance. While numerous proposed defenses claim substantial effectiveness, their evaluation is typically done in isolation with limited attack strategies, raising concerns about their validity. Additionally, existing studies overlook the mutual effectiveness of defenses against both DPAs and MPAS, causing fragmentation in this field. This paper aims to provide a unified benchmark and analysis of defenses against DPAs and MPAs, clarifying the distinction between these two similar but slightly distinct domains. We present a systematic taxonomy of poisoning attacks and defense strategies, outlining their design, strengths, and limitations. Then, a unified comparative evaluation across FL algorithms and data heterogeneity is conducted to validate their individual and mutual effectiveness and derive key insights for design principles and future research. Along with the analysis, we frame our work to a unified benchmark, FLPoison, with high modularity and scalability to evaluate 15 representative poisoning attacks and 17 defense strategies, facilitating future research in this domain. Code is available at https://github.com/violetus/FLPoison.", "sections": [{"title": "Introduction", "content": "Federated learning (FL) is a distributed learning paradigm that enables multiple devices or organizations to collaboratively train a machine learning model without sharing their private data [31]. It effectively leverages distributed data across different users without compromising their privacy, making it especially valuable for protecting individuals' sensitive information and complying with strict data privacy regulations in industries such as healthcare, mobile devices, Internet of Things, and finance [29, 32, 33]. FL has been applied in real-world privacy-preserving distributed scenarios, such as next-word and emoji prediction in Google's Gboard [16, 37], personalization of Apple's Siri [35], and credit rating for small enterprises at WeBank [10]. The growth of data protection laws like GDPR [2] and CCPA [1] has further driven the adoption of FL, as it aligns with privacy-by-design principles, enabling companies to gain data insights while meeting regulatory requirements.\nNevertheless, the decentralized nature of federated learning introduces new attack surfaces due to its limited visibility of local data and model training. One of the most critical security concerns is poisoning attacks, in which adversaries deliberately inject malicious data or manipulate local model training to degrade the global model's performance or bias its predictions [4, 52]. Based on the attack vector, either data manipulation or model training manipulation, poisoning attacks are typically categorized as MPAs and DPAS. According to the attack objective, they can further be classified as untargeted or targeted attacks. These attacks can lead to severe consequences. For instance, in a targeted attack, a self-driving car might misidentify stop signs with specific trigger stickers as speed limit signs, posing significant safety risks [14].\nNumerous defense strategies have been proposed to mitigate poisoning attacks. Based on the attack vector they defend against, they are categorized into model poisoning and data poisoning defenses. Model poisoning defenses [7, 21, 36, 56] aim to protect the model's performance from degradation caused by MPAs, while data poisoning defenses [8, 12, 39] focus on removing embedded targets or backdoors from the model while maintaining good performance. These two types of poisoning defenses are often evaluated separately for their respective attacks. Besides, several studies [13, 40, 47] have theoretically analyzed poisoning attacks and defenses separately based on attack vectors and attack objectives, yielding a taxonomy that offers a degree of comprehensiveness.\nHowever, most existing studies evaluate these attacks and defenses in isolation, often with inconsistent evaluation configurations and limited comparison strategies, raising concerns about validity. Furthermore, most literature tends to focus separately on either model poisoning defenses or data poisoning defenses, without a unified consideration and evaluation perspective, leading to a fragmented understanding and situation in this field."}, {"title": "Literature Review and Gap Analysis", "content": ""}, {"title": "Preliminaries and Problem Formulation", "content": "In this section, we first introduce federated learning and three commonly used FL algorithms in the field of poisoning attacks to establish a system model in Appendix B.1 We then provide the principles of poisoning attacks in Appendix B.2 and develop a threat model in Appendix B.3."}, {"title": "A Taxonomy of Poisoning Attacks", "content": "We categorize poisoning attacks into MPAs, DPAs, and hybrid poisoning attacks based on their attack point, following existing research [30, 46]. While some surveys distinguish between untargeted and targeted attacks, we focus on attack vectors to highlight their attack points, similarities, and differences. Model poisoning attacks are mainly untargeted attacks, while data and hybrid poisoning attacks are typically targeted attacks. Following Figure 1, we provide a systematic taxonomy analysis below, incorporating hybrid poisoning attacks into the data poisoning analysis due to their similar attack objective. Table 2 provides a summary of poisoning attacks."}, {"title": "Model Poisoning Attacks", "content": "MPAs manipulate local training processes and are classified into three types based on the training stage: training-time, post-training, and update-time poisoning. Most MPAs are untargeted, aiming to disrupt convergence (Byzantine attacks). Below, we describe each category and summarize 8 representative attacks, including their designs, assumptions, limitations, behaviors, and key parameters."}, {"title": "Training-time Poisoning Attacks", "content": "In these attacks, the adversary can alter training configurations such as the number of epochs, learning rate, loss function, and other parameters. These attacks often combine with data poisoning attacks, forming hybrid poisoning attacks. We detail these attacks on hybrid poisoning attacks (see Section 4.3)."}, {"title": "Post-training Poisoning Attacks", "content": "These attacks allow the adversary to manipulate the training results. Since these attacks require only knowledge of the"}, {"title": "Update-time Poisoning Attacks", "content": "These attacks occur when clients submit updates, enabling malicious clients to eavesdrop on benign ones or collude with others to launch attacks. Known as omniscient attacks, they rely on information from others. We first outline their main categories, followed by detailed explanations. These attacks can be classified into two types based on how malicious updates are constructed: Statistic-based evasion [4, 22, 52] and Optimization-based evasion [11, 41]. The former primarily generates malicious updates resembling benign ones by eavesdropping on benign updates using statistical measures like mean and standard deviation. The latter optimizes updates to bypass server defenses, such as L2 distance or Krum, making them appear benign. Some attacks require knowledge of the aggregation rule or defense method (aggregator-specific). In the absence of this knowledge, a random aggregation rule may be chosen instead [11, 41].\nA Little is Enough (ALIE) Attack [4] identifies that instead of relying on large changes to parameters, carefully crafted small perturbations of a few clients are sufficient to undermine convergence performance and bypass defenses"}, {"title": "Data Poisoning Attacks", "content": "Data poisoning attacks primarily manipulate training data to achieve their objectives. Some data poisoning attacks are often combined with training-time poisoning attacks, classifying them as hybrid poisoning attacks. A specific subtype of these attacks, known as backdoor attacks, aims to embed backdoors into the model so that when samples with the corresponding triggers are fed into the model, attack-specific predictions are produced. Below, we present two representative data poisoning attacks.\nLabel Flipping Attacks [6, 11, 12] alter training labels to attacker-specific labels with a label substitution model, which can produce random, inverse, or target labels. In the random strategy, the source label is replaced with a random one; in the inverse strategy, label l is flipped to L \u2212 l \u2212 1, where L is the number of classes; and in the target strategy, a specific label is assigned to label l. It supports both targeted and untargeted poisoning, and the targeted version is used in our paper.\nBadNets Attacks [3, 14, 48, 50] first introduces backdoor attacks, where the goal is to train a model that performs normally on the user's training and test data but misclassifies inputs containing a backdoor trigger to an attacker-specified target label. It is achieved by injecting a certain ratio of malicious samples into the training set, created by adding triggers to images and altering their corresponding labels to the target label."}, {"title": "Hybrid Poisoning Attacks", "content": "Hybrid poisoning attacks are conducted at both data poisoning and model poisoning attack points to enhance attack effectiveness and evade defenses."}, {"title": "Distributed Backdoor Attack (DBA)", "content": "Distributed Backdoor Attack (DBA) [51], unlike centralized backdoor attacks, BadNets where each adversary embeds the same global trigger, leverages the distributed nature of federated learning by breaking a global trigger pattern into distinct local patterns. These local patterns are embedded into the training data of different adversarial participants. Moreover, scale techniques [3] are used to amplify malicious updates during training, while samples with a global trigger are used during inference to activate backdoor attacks.\nAlternating Minimization Attack (AlterMin) [5] assumes IID training and specific validation data samples, where the adversary controls a small number of non-colluding clients (typically one) without access to other clients' updates. It jointly optimizes the model with the adversarial objective and the stealth objective (in an alternative manner). The adversarial objective is to let the global model misclassify the crafted trigger-embedded input to an attacker-specified target label. Meanwhile, the stealth objective is to make the malicious local updates deviate subtly from the benign updates, avoiding detection by distance-based defenses.\nModel Replacement Attack (ModelRep) [3], also known as the Constrain-and-Scale Attack, demonstrates that the adversary can replace the global model with a malicious backdoored version by employing constrain-and-scale techniques. It first constrains the training process by incorporating an anomaly detection term $L_{ano}$ to loss function $L_{model}$, based on various metrics, such as the p-norm distance between weight matrices or cosine distance (as used in the paper):\n$L_{model} = \\alpha L_{class} + (1-\\alpha) L_{ano}$,\nwhere \u03b1 controls the trade-off between evading anomaly detection and achieving the attack objectives. Then, it scales the (pseudo) gradient update before submission by a factor of $\\gamma = \\frac{n}{\\eta}$, where n is the number of participants and \u03b7 is the global learning rate at the server. Since clients typically lack knowledge of n and \u0ed7, the authors suggest that the attacker can empirically estimate \u03b3.\nEdge-case Backdoor [48] introduces a new type of backdoor attack where adversaries exploit edge-case samples for targeted attack, which are unlikely to appear in normal training or test data and reside in the tail of the input distribution. The adversary can create edge-case datasets by mixing normal datasets with rare but similar ones, such as using the ARDIS dataset [24] for the MNIST dataset [25]. While both datasets involve handwritten digits, ARDIS exhibits greater variability in handwriting styles and more noise from historical sources. Additionally, they proposed two attack strategies based on the edge-case backdoor. The first involves adversaries using projected gradient descent (PGD) at each FL round to ensure the malicious model evades norm clipping defenses. The second strategy combines PGD with a Model Replacement Attack [3], applying scaling before model submission.\nNeurotoxin [59] introduces a durable backdoor attack strategy that involves updating coordinates infrequently used by benign clients to counter backdoor detection. This approach is based on the observation that the sparsity of gradients in stochastic gradient descent (SGD) causes the majority of the"}, {"title": "Summary of Attacks", "content": "Poisoning attacks in FL progress from treating model updates as black boxes to understanding the importance of weight parameters. Early MPAs, like Gaussian and Sign Flipping, are simple and low-cost, requiring minimal attacker knowledge while still causing significant disruption. However, they can be effectively mitigated by defenses based on some robust statistical metrics, like Krum [7], Multi-Krum [7], Coordinate-wise Median [56], RFA [36]. Their impact remains notable against basic outlier detection defenses, such as Auror [43]. Then, it advances to statistical-based evasion like IPM and ALIE, which exploit the statistical properties of benign client updates for stealth, making them hard to detect but less damaging. Advanced attacks such as FangAttack, MinSum, and MinMax are designed to evade common defense metrics. FangAttack stands out by integrating metric evasion with statistical deviation techniques. Besides, data heterogeneity also begins to be considered for deviation, such as Mimic attack. Data poisoning attacks start with basic methods like Label Flipping and BadNets and advance to techniques such as ModelRep and AlterMin, which enhance attack performance but suffer from poor generalization due to many hyper-parameters. More advanced strategies involve backdoor implantation, like DBA, which distributes backdoors across clients, and Edge-case backdoor, poisoning datasets with tail-distribution samples. Fundamental attacks like BadNets, DBA, and Edge-case require minimal tuning to ensure stability. As attacks evolve, model parameters are no longer being treated as black boxes, with approaches like Neurotoxin using key parameters for backdoor durability. This broadens the attack surface but increases complexity, raising generalization concerns.\nTakeaways. Our analysis categorizes key poisoning attack methodologies to guide comprehensive defense development. These include: (1) Random Noise, adding noise to disrupt gradients (e.g., Gaussian Random Attack); (2) Sign Manipulation, altering gradient signs (e.g., Sign Flipping, IPM); (3) Statistic-based Evasion, using update statistics deviations (e.g., ALIE, FangAttack); (4) Optimization-Based Evasion, optimizing updates to avoid detection (e.g., FangAttack, MinMax); (5) Scaling-based Amplification, as in Model Replacement, DBA, AlterMin; (6) Distance-based Constraints, used in MinSum and MinMax; (7) Loss-based constraints, seen in AlterMin; (8) Alternating Optimization, as in AlterMin; (9) Adaptive Attacks, with stronger robustness; (10) Feedback-based Optimization, in emerging methods like 3DFed [27]."}, {"title": "A Taxonomy of Poisoning Defenses", "content": "Poisoning Defenses, also known as aggregation strategies, are often considered separately for model and data poisoning attacks. We discuss both perspectives, highlighting that these defenses can be categorized into robust aggregation and outlier detection classes. Robust aggregation aims to generate reliable outputs based on robust statistics [19] even with malicious updates, while outlier detection identifies and removes abnormal updates that may harm performance. Below, we outline the defense strategies for model and data poisoning attacks. Table 3 provides a summary of defense strategies with their knowledge, assumptions, complexity, and categories."}, {"title": "Model Poisoning Defenses", "content": "Model poisoning defenses, also referred to as Byzantine defenses, are designed to safeguard FL models against model poisoning attacks or Byzantine attacks. Below, we discuss 11 representative defense strategies.\nKrum [7] is the first algorithm proven to be Byzantine-resilient for distributed SGD. It utilizes the majority and squared-distance principles to select one client's gradient update (vector) that is closest to its n - f neighbors, thereby excluding the vectors that are too far away. Specifically, following the majority assumption, $f < \\frac{n}{2} - 1$, the selected vector, as the aggregation result, is:\n$Krum(g_1,...,g_n) = g_k,  where k = \\underset{i\\in[n]}{argmin} s(i)$,\n$s(i) = \\sum_{j\\in[n],j\\neq i} ||g_i - g_j||^2$,\nwhere s(i) is the score for gi. i \u2192 j indicates that gj belongs to the n-f- 2 closest vectors to gi measured by squared Euclidean distance, which can be obtained by iterating over all j\u2208 [n], then sorting to get the n \u2013 f \u2013 2 smallest ones. The time complexity of Krum is O(n\u00b2 \u00b7d) for the d-dimensional vector and distances calculation.\nMulti-Krum (M.K.) [7], a variant of Krum, different from Krum selecting only one closest vector among the gradient update vectors, selects the m closest ones, and computes their average. Specifically, the aggregation results are:\n$Multi-Krum(g_1,\\dots,g_n) = f_{avg}(g_k)$,   $k\\in \\{1,2,\\dots, i_m\\} = \\underset{i\\in[n]}{argmin^m} s(i)$,\nwhere s(i) is the same as in Krum.\nCoordinate-wise Median (Median) [56] computes the aggregation result as CM(g1,\u00b7,gn), where the j-th coordinate is:\n$CM(g_1,\\dots,g_n) [j] = median(g_1 [j],\\dots, g_n[j])$,\nwith j\u2208 [d] and the median being the one-dimensional median. Accordingly, the time complexity is O(dn) to get the median of the d-dimensional vector.\nTrimmedMean (T.M.) [56] assumes the majority of clients are benign. It first sorts each coordinate j to get the"}, {"title": "Data Poisoning Defenses", "content": "Data poisoning defenses aim to protect FL models from data poisoning attacks, with most of them focusing on backdoor defenses that aim to eliminate potential backdoors in the models. We consider 6 representative defense algorithms.\nAuror [43] identifies that benign clients' indicative features follow a consistent distribution, while adversaries' features exhibit anomalies. It selects indicative features by clustering each update coordinate into two clusters and filtering based on the distance between their cluster centers and a threshold. Indicative features are then clustered, with the majority determining benign clients for aggregation. Designed for IID data, its time complexity is O(dn) for initial coordinate clustering and O(sn) for clustering s selected features."}, {"title": "Summary of Defenses", "content": "For defenses against model poisoning attacks, early methods employ robust statistical metrics, such as Krum, M.K., T.M., and Median, for robust aggregation. They benefit from the independence from specific pattern analysis or setting constraints, thus improving the robustness and generalization ability against noise and outliers. However, they fail to capture anomalies in update parameters when confronted with complex attacks. Another line of defense is outlier detection, which relies on certain patterns in parameter updating. For instance, sign statistics (SignGuard), cosine similarity (FLTrust), and cross-round historical momentum information (CenteredClipping) are all efficient features in defense. However, outlier detection may weaken under minor perturbations or unsupported assumptions, limiting generalization. Early data poisoning defenses, such as Auror and F.G., rely on pattern-based clustering with indicative update parameters. However, these approaches often struggle with poor pattern generalization and are limited to either IID or non-IID scenarios. Recent methods improve performance by integrating robust statistical metrics with pattern-based clustering. For example, DeepSight employs median-based thresholding and pattern clustering, while FLAME combines cosine similarity clustering with median-of-norm clipping. Nonetheless, excessive complexity in analysis and parameter configuration can hinder generalization and robustness.\nTakeaways. We identify key defense methods against poisoning attacks, including L2 Distance-Based (e.g., Krum, SignGuard), Median-Based (e.g., Median, FLAME), Coordinate-Wise (e.g., RFA, Neurotoxin), Sign Distribution Analysis (e.g., SignGuard), Cosine Similarity (e.g., FLTrust, FLAME), and PCA (e.g., DnC). These methods vary in robustness. Besides, current defenses primarily address label and quantity skew, with feature and quality skew underexplored [54]. Further research is encouraged to close these gaps and improve real-world protection."}, {"title": "Evaluation", "content": "In this section, we first outline the experimental setup. Next, we systematically evaluate the performance of various attacks and defenses across 2,040 unique settings (15 attacks \u00d7 17 defenses \u00d7 2 datasets \u00d7 2 algorithms \u00d7 2 levels of data heterogeneity). Specifically, we first analyze their overall effectiveness, identify the most effective attack and defense strategies, and the factors contributing to their success or limitations, and highlight the distinctions between defenses against MPAs and DPAs. We then compare and analyze these strategies in terms of FL algorithms and data heterogeneity"}, {"title": "Experimental Setup", "content": "Federated Learning System. We simulate a FL system with a central server and fully participating clients. For FL algorithms, we observe that FedSGD and FedOpt are widely utilized in poisoning attacks, as highlighted in prior studies [3, 5, 34, 39, 39, 45, 50, 51, 59]. As shown in Table 2 and Table 3, most model poisoning attacks and defenses are developed based on the FedSGD algorithm [31], while the majority of data poisoning attacks and defenses are built upon the FedOpt algorithm [38]. Notably, while some data poisoning studies claim to use FedAvg, a closer analysis of their algorithm descriptions or code implementations reveals they actually employ FedOpt [38] with its server learning rate set to 1. Therefore, both of them are evaluated for a comprehensive and unified evaluation. The general settings, including the number of epochs, adversary ratio, batch size, optimizer, learning rate, configurations of attacks and defenses, etc., are detailed in Table 9 and Appendix B.4.\nModels and Datasets. This paper trains LeNet-5 [26] on MNIST [26] and ResNet-18 [17] on CIFAR-10 [23] dataset. MNIST includes 60,000 training and 10,000 testing grayscale images (28x28 pixels) of handwritten digits across 10 classes. CIFAR-10 consists of 50,000 training and 10,000 testing color images (32x32 pixels) across 10 classes. These models and datasets are widely used in FL evaluations [4, 8, 21, 52, 53].\nData Heterogeneity. For data heterogeneity, we consider both IID and non-IID scenarios for evaluation. Specifically, IID data is quantity- and class-balanced. For non-IID, we observe two main partition strategies: FangAttack's partition [11], followed by [8, 41, 58], and the Dirichlet distribution-based partition, pioneered by [18, 57] and followed by [3, 27, 34, 51]. We choose the Dirichlet distribution-based strategy for its widespread use in both poisoning attacks and federated learning field [3, 18, 27, 28, 34, 42, 49, 51, 57]. Specifically, the local data is sampled using label ratios from a Dirichlet distribution Dir(\u03b1). \u03b1 (0.5 in our evaluation) controls data heterogeneity, with smaller \u03b1 indicating greater class imbalance. Figure 4 visualizes our balanced IID and non-IID partition results on 20 clients for labels on the CIFAR-10 dataset.\nEvaluation Metrics. Two commonly used metrics are used to assess the effectiveness of attacks and defenses, (1) Accuracy (ACC) indicates the ratio of the number of correctly predicted labels to the total number of test samples (2) Attack Success Rate (ASR) indicates the ratio of correctly misclassified targets to the total number of poisoned test samples."}, {"title": "Evaluation of Attacks", "content": "Table 4 presents the performance of model poisoning attacks over different defenses (smaller values indicate better performance). We observe that under the MNIST dataset, ALIE, IPM, and MinSum achieve better performance than others, and under CIFAR-10 dataset, Gaussian, SignFlipping, and FangAttack achieve better performance than others, and in general, Gaussian, SignFlipping, and FangAttack achieve the best attack effectiveness than others.\nFor simple attacks, our analysis reveals that although attacks Gaussian and SignFlipping are relatively straightforward, they exhibit the highest effectiveness across various settings, especially under more heterogeneous datasets. We believe the effectiveness of Gaussian attacks benefits from the fact that as model training converges, the distribution of model updates in machine learning approaches a Gaussian distribution, making Gaussian attacks increasingly challenging to identify at later stages. Besides, both Gaussian attack and Sign Flipping attack can introduce large perturbations to defense algorithms that rely solely on outlier detection without robust statistical measures or other reliable references, such as [12, 22, 43, 50, 56]. For advanced attacks, ALIE performs well on MNIST but poorly on CIFAR-10 due to the small attack hyper-parameter values from the $z_{max}$ estimation in [4]. This suggests that zmax requires manual fine-tuning for stronger attack impact, highlighting the non-adaptiveness of ALIE across datasets. As noted in the original paper, IPM indeed exhibits stronger attack effectiveness against some specific defenses such as Krum and coordinate-wise methods. However, its overall impact on system compromise remains limited. Furthermore, FangAttack demonstrates superior performance compared to attacks that rely solely on statistical deviation on benign updates, such as ALIE and IPM, or solely on distance-evasion methods like MinMax and MinSum, especially with a more heterogeneous CIFAR dataset. FangAttack's effectiveness stems from (1) its targeted evasion of distance-based Krum defenses, which enhances its attack capability, and (2) its balance between attack impact and defense evasion through statistical deviation on benign updates. Mimic shows improved performance on CIFAR-10, attributed to the advantages provided by its heterogeneity.\nTable 5 shows the performance of data poisoning attacks over different defenses. We find that BadNets (centralized backdoor), DBA (distributed backdoor), and Edge-case attacks (Edge-case backdoor), represent the most powerful DPAs, in general. We believe that their simplicity and high adaptivity are the key reasons for their effectiveness. These attacks do not rely on complicated fine-tuning, which can be applied to various datasets and federated learning algorithms. Conversely, other backdoor attacks that require adjusting multiple hyper-parameters based on different datasets and algorithms [3, 5, 59] and may degrade the backdoor performance as the model converges or result in training failure.\nData poisoning attacks, especially backdoor attacks, are influenced by various parameters such as data poisoning ratio, the number of malicious nodes, trigger size, and data distribution. Methods like scaling and alternating optimization are increasingly used to enhance poisoning effects, with"}, {"title": "Evaluation of Defenses", "content": "Table 6 demonstrates the defense performance against model poisoning attacks. We find that Median, FLTrust, DnC, and FLAME are the best defenses and perform well across various experiment settings, in general. For model poisoning defenses (Median, FLTrust, and DnC), Median demonstrates its effectiveness as a good robust statistic. FLTrust benefits from a server-maintained model trained on the same data distribution as the client, guiding correct gradient updates and adapting to different datasets and algorithms. DnC uses singular value decomposition and majority-based clustering, without relying on dataset or algorithm-specific hyper-parameters. We believe that these two methods benefit from their strong robustness and adaptability, making them stable across variations in datasets, algorithms, and data heterogeneity. As a backdoor defense, FLAME derives advantages from (1) dynamic capture differences with HDBSCAN, (2) robust statistics like the median of norms for updates clipping. Some defenses like Krum [7] and S.G. [53] fail for relying solely on outlier detection or robust statistical metrics, while others, like F.G. [12] and D.S. [39], introducing data-dependent hyper-parameters degrade their performance.\nTable 7 and Table 10 show the defense performance against data poisoning attacks. First, we find that Krum, M.K., Bulyan, FLTrust, DnC, and FLAME outperform other methods generally. As model poisoning defenses, Krum, M.K., and Bulyan demonstrate that robust statistics-based methods can be effective for defending against data poisoning attacks. As a combination of trimmed mean and coordinate-wise methodology, Bulyan captures update anomalies more effectively in backdoor attack scenarios,"}, {"title": "Comparison of Defenses in Terms of FL Algorithms", "content": "Defense Against MPAs. As shown in Figure 7a and Table 6, most defenses achieve better performance against MPAs under FedOpt compared to FedSGD. This improvement can be attributed to the multiple local rounds per aggregation in FedOpt, which help mitigate small deviations and amplify significant ones, enhancing detection effectiveness. Under FedSGD, FLTrust, F.G., DnC, Median, and FLAME demonstrate strong defense capabilities, while under FedOpt, FLTrust, DnC, S.G., and FLAME perform effectively. This indicates that FLTrust, DnC, and FLAME maintain consistent effectiveness across both algorithms, highlighting their adaptability, thus underscoring the effectiveness of leveraging cosine similarity, the SVD method, and update magnitude control in enhancing defense performance.\nDefense Against DPAs. Considering the defense goal against DPAs, we formulate a metric, named Targeted Defense Robustness (TDR), for a defense strategy,\nTDR = $\\frac{Accuracy under DPA}{Accuracy without Attack}+1-ASR$.\nA higher TDR indicates greater defense robustness. As illustrated in Figure 7b, Table 7, and Table 10, there is no clear evidence that defenses against DPAs perform consistently better under one algorithm compared to another. Specifically, under FedSGD, the top-performing defenses are FLAME, Bulyan, M.K., Krum, and S.G. Meanwhile, under FedOpt, the best-performing defenses are FLAME, M.K., Krum, Bulyan, and S.G. Notably, except for FLAME, the other four defenses were originally proposed for MPA, suggesting that defenses designed for MPA can also be quite effective against DPA. Furthermore, it is worth mentioning that only FLAME and M.K. achieve a performance of about 1.75, which is still far below the optimal metric of 2. It indicates that there is significant room for improving defense robustness against DPAS."}, {"title": "Comparison of Defenses in terms of Data Heterogeneity", "content": "Defense Against MPAs. As illustrated in Figure 8a and Table 6, it is evident that all defense mechanisms exhibit better performance under IID conditions compared to non-IID conditions. In the IID setting, methods such as DnC, FLTrust, FLAME, Median, and F.G. demonstrate superior performance, maintaining an average accuracy of approximately 80% or higher. However, the scenario in the non-IID setting is less favorable. Among the defenses, only FLTrust achieves an average accuracy of approximately 80%, while the second-tier defenses, such as DnC and NormClipping, manage to sustain an average accuracy of around 66%. This highlights a significant gap in the availability of effective defenses for non-IID settings.\nDefense Against DPAs. As presented in Figure 8b, Table 7, and Table 10, under IID setting, FLAME, M.K., Krum, and Bulyan demonstrate the best performance, achieving targeted defense robustness of approximately 1.8. However, under non-IID setting, the top-performing defenses, FLAME, FLTrust, M.K., and Bulyan only achieve a robustness of around 1.6. This significant drop in performance raises concerns about the lack of effective defenses against DPAs in non-IID settings."}, {"title": "Conclusion", "content": "This paper offers a Systematization of Knowledge on benchmarking poisoning attacks and defenses in federated learning. A comprehensive taxonomy and a large-scale, unified evaluation of 17 defense strategies against 15 representative poisoning attacks are provided. Our extensive analysis highlights the strengths and limitations of these strategies, identifies the most advanced methods, and empirically reveals the key connections and distinctions between model poisoning and data poisoning. Moreover, our discussion illuminates the current state of research both theoretically and empirically, providing design principles and highlighting future research directions from a unified perspective."}, {"title": "Ethics Considerations", "content": "Federated learning has been widely applied in security-sensitive scenarios. However, its distributed training nature introduces vulnerabilities to poisoning attacks. Our SoK paper aims to comprehensively analyze the performance of both attacks and defenses against poisoning attacks in federated learning scenarios through a unified evaluation framework. We affirm the ethical compliance of this research and strive to advance the secure deployment of federated learning systems in the future."}, {"title": "Open Science", "content": "To ensure compliance with availability, functionality, and reproducibility standards, we will release our source code and data to the public."}, {"title": "Overview of Our FLPoison", "content": "In this section, we provide a concise overview of our FLPoison framework. As illustrated in Figure 2, our FLPoison is highly decoupled and modular, consisting of three layers: the federated learning layer, the attack layer, and the aggregation layer. Each layer is configurable through a global argument.\nIn the federated learning layer, a Worker class is implemented with foundational training and inference functions, serving as a base for the Client and Server classes. A Coordinator facilitates the federated learning workflow, including client initialization and federated learning algorithm configuration. Supported algorithms, such as FedSGD, FedOpt, and FedAvg, are injected into clients as hooks to facilitate easy customization. The attack layer comprises three main components. The model and data poisoning modules enable various attack types by coupling with the client class. An attack is defined by some of these four attributes: training-time poisoning, post-training poisoning, update-time poisoning, or"}, {"title": "Detailed Literature Review and Gap Analysis", "content": "With increasing security concerns in FL and the emergence of new poisoning attacks, significant attention has been focused on this field. Table 1 presents a comparative summary of our SoK and the current literature on poisoning attacks and defenses in FL from 2022 to 2024. A comprehensive evaluation and comparative analysis of these strategies from a unified perspective, as well as open-source benchmarks for consistent performance assessment, are lacking.\nMost existing studies focus on surveys and analyses of either model or data poisoning attacks separately, often with limited scope. Gong et al. [13] evaluate backdoor attacks as part of data poisoning attacks and their effectiveness against a few defenses. Sharma et al. [40] and Wang et al. [47] consider only partial data poisoning attacks and defense strategies without comprehensive analysis and evaluation. Although a few studies [30, 46] summarize and theoretically analyze both types of attacks, they lack experimental evaluations and fine-grained categories. In summary, none provide"}, {"title": "Configurations of Attacks and Defenses.", "content": "Since both data poisoning and hybrid poisoning attacks in our paper are targeted attacks, we classify them collectively as data poisoning for targeted evaluation purposes. For data poisoning attacks, we set the poisoning ratio to 0.32 (20 images are poisoned per batch of size 64) following [3, 14, 51], unless otherwise stated. For the specific configurations of attacks and defenses, we strive to follow the settings of the original work as closely as possible. However, due to variations in algorithms, datasets, models, and space limitations, we have included these details we used in the \"configs\" folder of our codebase. It is worth noting that some defenses converge slowly due to strict outlier filtering under non-IID settings, requiring a lower learning rate, as specified in \"batchrun.py\" of our codebase."}, {"title": "Comparison of Time Overhead", "content": "In this section, we evaluate the empirical time overhead of poisoning attacks and defenses. Specifically, we train a LeNet-5 model on the MNIST dataset in the IID setting under the FedSGD algorithm, as mentioned previously, for 300 rounds, reporting the average time overhead cost by each attack and defense strategy per epoch.\nIn Figure 9a, advanced optimization-based evasion strategies, such as FangAttack, AlterMin, MinSum, and MinMax, incur the most significant time overhead. Among these, FangAttack incurs the highest cost, taking 97\u00d7 times longer than NoAttack's normal"}]}