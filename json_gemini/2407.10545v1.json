{"title": "Efficient Continual Learning with Low Memory Footprint For Edge Device", "authors": ["Zeqing Wang", "Fei Cheng", "Kangye Ji", "Bohu Huang"], "abstract": "Continual learning(CL) is a useful technique to acquire dynamic knowledge continually. Although powerful cloud platforms can fully exert the ability of CL,e.g., customized recommendation systems, similar personalized requirements for edge devices are almost disregarded. This phenomenon stems from the huge resource overhead involved in training neural networks and overcoming the forgetting problem of CL. This paper focuses on these scenarios and proposes a compact algorithm called LightCL. Different from other CL methods bringing huge resource consumption to acquire generalizability among all tasks for delaying forgetting, LightCL compress the resource consumption of already generalized components in neural networks and uses a few extra resources to improve memory in other parts. We first propose two new metrics of learning plasticity and memory stability to seek generalizability during CL. Based on the discovery that lower and middle layers have more generalizability and deeper layers are opposite, we Main-tain Generalizability by freezing the lower and middle layers. Then, we Memorize Feature Patterns to stabilize the feature extracting patterns of previous tasks to improve generalizability in deeper layers. In the experimental comparison, LightCL outperforms other SOTA methods in delaying forgetting and reduces at most 6.16\u00d7 memory footprint, proving the excellent performance of LightCL in efficiency. We also evaluate the efficiency of our method on an edge device, the Jetson Nano, which further proves our method's practical effectiveness.", "sections": [{"title": "1 INTRODUCTION", "content": "The wave of deep learning has significantly boosted the deployment of the neural network from cloud platforms to edge devices [19, 20]. As the dynamic realistic environment requires the adaptive ability of models, a potential technique known as continual learning (CL) empowers these models to continually acquire, update, and accumulate knowledge on the device from the data by the training behavior. Although powerful cloud platforms can leverage CL to adapt to service expansion, e.g., personalized recommendation systems[16], the similar and intense personalized requirements for edge devices are almost disregarded by academia and industry. This overlook stems from scarce resources in edge devices, such as limited computation, constrained storage, and insufficient memory, which significantly hinders the further application of CL. Hence, efficient CL on edge devices becomes a considerable challenge.\nMost existing CL methods ignore edge scenarios and only concentrate on addressing the catastrophic forgetting problem [14, 22] model training on a new task tends to \"forget\" the knowledge learned from previous tasks. Specifically, these methods explore two factors of generalizability[29], learning plasticity and memory stability, and make a trade-off between them to delay forgetting. Note that learning plasticity denotes the adaptation to the new task, while memory stability denotes the memory capacity of previous tasks. Hence, most of CL methods [2, 3, 5, 14, 25] simultaneously train on the trace of historical information and new data for generalizability. It is well known that the training overhead of the neural network is already significant. Meanwhile, learning from history in CL requires additional resources for the training process. Thus, the casual CL methods are unsuitable for resource-intensive scenarios.\nTo mitigate this problem, compression techniques are inevitably introduced to the on-device CL. Especially, sparse learning [11] has become a popular paradigm of the efficient CL[24, 30]. Although they [24, 30] greatly reduce computation by the sparsity, intermediate feature maps cannot be effectively reduced, leading to an enormous memory footprint during training.\nLet's dive into the challenges of the on-device CL. Firstly, the forgetting problem is the first and foremost issue in CL, whether on edge devices or cloud platforms. Most CL methods train on historical information and new data together for generalizability to delay forgetting. This training behavior requires more significant resources than usual, including intermediate feature maps and gradients in memory, as well as additional computations for gradient calculations. This greatly hinders the efficiency. Secondly, the efficiency of CL heavily influences its prevalence in edge devices. The computation and memory footprint are two main factors that hinder the deployment. Meanwhile, memory rather than computation has become the primary bottleneck in AI applications, and the time taken for memory transfers greatly limits computational capacity[8], which impedes the process of overcoming the forgetting during CL. Actually, the above two challenges are not orthogonal. The former inevitably interferes with the latter, and vice versa. This tangle prompts us to solve two challenges together. Considering the neural network has the potential to train in new tasks with the help of previous knowledge during CL, it absolutely has redundancy of generalizability. In other words, if we could evaluate the distribution of generalizability in the neural network during CL, we can compress the training redundancy with the over-strong generalizability and use just a few extra resources to maintain memory in less-generalized parts, which hopefully gives us a more favorable and efficient CL on edge devices.\nTherefore, we start with the two factors of generalizability, learning plasticity and memory stability, and design their metrics to quantitatively assess the generalizability of neural networks during CL. This evaluation demonstrates that the generalizability of"}, {"title": "2 RELATED WORK", "content": "Continual Learning Methods: CL methods can be categorized into replay methods [2-5, 12], regularization-based methods [1, 14, 18, 31], and architecture-based methods [21, 25, 28]. Replay methods address catastrophic forgetting by replaying a subset of old data stored in storage. They must continually store samples when they meet new tasks, and bring extra computation and memory footprint for training on old data. Regularization-based methods are designed to restrict the updates of neural networks through regularization, preserving valuable information about previous tasks. Most of them [1, 14, 31] limit parameter changes necessary for old tasks, leading to extra consumption for recording the importance of every parameter. Architecture-based methods can prevent forgetting by distributing different model structures to each task. Similar work[25] instantiates a new network for each new task, while others[28] divides a network into task-specific parts through pruning. Hence, most of them grow the model size continually or train on a whole dense model to learn new tasks, which causes significant resource overhead during the CL process. Meanwhile, they must know the identity of the task before the model's evaluation, which is unsuitable for edge scenarios.\nEfficient Learning: Many efficient algorithms aim at the deployment of neural networks on edge devices, such as pruning [10, 17], quantization[9] and etc. Pruning compresses networks by discarding weight redundancies, while quantization compresses the bit-width of weights. These methods mainly accelerate the inference procedure on edge [26], and degrade the ability of compressed models to accommodate dynamic environments. That's because large training consumption hinders deployment development, such as massive usage of intermediate feature maps and gradients to memory footprint and more training FLOPs on computation. Although [19] can train in resource-limited settings, they focus more on training a single task and don't consider catastrophic forgetting between multiple tasks. To cope with catastrophic forgetting of CL in the edge device scenario, SparCL[30], combined with the dynamic pruning method, greatly reduces training FLOPs and delays the forgetting problem. However, SparCL still occupies massive memory footprints for intermediate feature maps."}, {"title": "3 CL SETTING", "content": "In the CL setting, tasks {1, 2, ..., T} are executed sequentially during training, and data from previous tasks are inaccessible when training on the new task. Specifically, the t-th task represents \\(D_t = \\{(x,y)\\}_{i=1}^{n_t}\\), where nt is the sample number of t-th task and (x, y) is input-label pair. \\(\\theta_t^i\\) represents the i-th layer of the neural network after training (or during) t-th task. The goal of CL is to let the AI model perform well on all tasks when learning a new task. Overall performance is typically evaluated by average accuracy (AA) [29, 30] on all tests after training. AA defined as \\(AA = \\frac{1}{T}\\sum_{i=1}^T a_i(\\theta_T)\\), where \\(a_i(\\theta_T)\\) represents the testing accuracy of model \\(\\theta_T\\) on i-th task. There are two main CL scenarios[30]: 1) Task incremental learning (TIL), the identity t of the task Dt are available during training and testing; 2) Class incremental learning (CIL), the task identity t are only available during training. This paper evaluates CL methods under TIL and CIL scenarios."}, {"title": "4 ANALYSIS OF GENERALIZABILITY", "content": "In this section, we analyze the distribution of generalizability in the neural network during CL. Training consumption on generalized components in the neural network can be a kind of redundancy since they already have the ability to fit new tasks. That's the motivation for evaluating generalizability. Considering generalizability means can be utilized for both previous and new tasks, we utilize memory stability (MS) and learning plasticity (LP) to represent these two different characters respectively. Similar work[23] analyzes the forgetting parts in the neural network during CL. In fact, forgetting means strong learning plasticity and less memory stability, so generalizability and forgetting are not orthogonal and we cannot infer generalizability from it[23]. Thus, we need to design two metrics of LP and MS to quantitatively evaluate generalizability.\nMS measures the loss of previous knowledge and LP measures adaptation to new knowledge. We evaluate them from the perspective of layers in the neural network. Specifically, we replace the parameters of i-th layer in the model \\(\\theta_T\\) with the parameters from the previous model \\(\\theta_T-1\\). The new model \\(\\theta^{new}\\) can be described as\n\\[\n \\theta_j^{new} = \\begin{cases}\n    \\theta_j^{T-1} & \\text{if } j = i \\\\\n    \\theta_j^{T}   & \\text{otherwise}\n  \\end{cases}\n  \\tag{1}\n\\]\nThen, we separately test the accuracy of \\(\\theta^{new}\\) on (T-1)-th task and T-th task. Two metrics of LP and MS can be described as\n\\[\n\\begin{cases}\nMS = a_{T-1}(\\theta^{new}) - a_{T-1}(\\theta^{T}) \\\\\nLP = a_{T}(\\theta^{new}) - a_{T}(\\theta^{T})\n\\end{cases} \\tag{2}\n\\]\nIf the value of MS is high, it indicates that the previous knowledge in i-th layer has been lost to a large extent, suggesting that the memory stability of i-th layer is weak. The low value of LP indicates that the previous i-th layer doesn't have a good adaptation for the new tasks, meaning that the previous i-th layer has weak plasticity.\nFor more reasonable, we evaluate the above metrics using different class numbers for multiple tasks across 5 runs on two typical backbones, ResNet-18[13] and VGG16[27]. As shown in Figure 2, different numbers of classes exhibit a similar phenomenon during CL learning. Lower layers have small values of MS and high values of LP. In other words, the lower layers have high values of memory stability and learning plasticity, which exhibit good generalizability. The middle layers in ResNet-18 display good generalization ability, while the counterpart of VGG16 exhibits poor memory stability (second row in Figure 2(a)). It should be noted that LP suggests that middle layers already exhibit learning plasticity before being trained on the new task in Figure 2(b), indicating that learning the new task will impair the stability of the middle layers and degrade the existing generalizability. This phenomenon proves the opinion that forgetting parts is not orthogonal to generalized parts. For the deeper layers, the values of MS are higher and the values of LP are lower. This suggests that the deeper layers are less-generalizability and forgetting parts. From the above analysis, we conclude that during CL, lower and middle layers have strong generalizability, and deeper layers have less generalizability. This conclusion guides the design of LightCL in Section 5."}, {"title": "5 METHODOLOGY", "content": "LightCL, as shown in Figure 3, efficiently delays forgetting through Maintaining Generalizability and Memorize Feature Patterns. The whole pipeline of them is shown in Figure ??. All these methods are based on the discovery of generalizability in Section 4. We first introduce Maintaining Generalizability in Section 5.1 and then illustrate Memorize Feature Patterns in Section 5.2."}, {"title": "5.1 Maintaining Generalizability", "content": "According to the conclusion (Section 4) that lower and middle layers have strong generalizability, the training consumption on them can be a kind of redundancy. Hence, we easily freeze them to maintain generalizability without the training process, illustrated in the gray background section at the top of the Figure 3. Specifically, we must pretrain a network, freeze its lower and middle layers, and finally deploy it on the edge device for CL. Training the t-th task can be formulated as\n\\[\n\\theta \\leftarrow \\theta + \\alpha\\nabla_{\\theta} L_c (f(\\theta^t, x_t), y_t) \\text{ s.t. } i > FZ, \\tag{3}\n\\]\nwhere FZ is the last layer index of frozen layers, Lc is the cross-entropy loss function, and function f represents forward pass."}, {"title": "5.2 Memorize Feature Patterns", "content": "Based on the conclusion (Section 4) that deeper layers are less generalized and forgetting parts, we propose an efficient regulation method to memorize previous knowledge in deeper layers, shown in the pink background in the middle of the Figure 3. We go through the following three subsections to illustrate it.\n5.2.1 How to Memorize Efficiently. Forgetting occurs because the gradient update of the current task does not consider generalizability among all tasks. Restricting the updating of vital weights of previous tasks, as the regularization-based method [14] does, is the direct way to our goal. [19] points out that the deeper the layers, the more the memory footprint occupied by weights increases, while the size of feature maps becomes smaller and smaller. Our idea is that since both can reflect output patterns, we can memorize previous feature-extracting patterns by regulating important feature maps to overcome forgetting with a little extra consumption.\n5.2.2 How to Select Reference. The intuitive idea is to regulate changes of vital feature maps produced by each sample of previous tasks during training for a new task. However, it is impossible to store samples continuously in resource-limited scenarios. As the number of tasks increases, so does the number of samples required. Alternatively, we regulate feature maps from the current sample instead of previous samples. That's because regulating output patterns of feature maps is unrelated to the information of samples. The only effort is to recognize feature maps that require regulation. To realize this, we identify locations of feature maps vital to previous tasks by \\(l_1\\)-norm [17] with ratio R.\n5.2.3 Detailed Process. After training the (t-1)-th task, neural network \\(\\theta^{t-1}\\) is generated. We get important positions of feature maps from the first batch of (t-1)-th task. Assume (i, j) denotes j-th feature map in i-th layer to be position, all important positions are recorded in a set I. Before training new t-th task, we first select a few samples, defined as M, from the new task to run it on the neural network \\(\\theta^{t-1}\\) and get feature map standard, called FS, to be referenced. The process is defined as \\(FS = F(\\theta^{t-1}, M, I)\\), where F is the function to get feature maps of important position I. Note that we only get FS before training the new task without keeping acquiring during training, shown in the gray background part at the bottom of the Figure 3.\nDuring training the t-th task, we integrate M in each batch and get the corresponding feature maps. FM is calculating from \\(FM = F(\\theta, M, I)\\). The limiting changes of vital feature maps between FS and FM can be described as the regulation loss function\n\\[\nL_f(FM, FS) = \\beta \\sum_{(i, j) \\in I} (FM_{i, j} - FS_{i, j})^2. \\tag{4}\n\\]\nThe entire loss function L is the combination of cross-entropy loss function Lc and the regulation loss Lf, described as follow\n\\[\nL(\\theta^*) = L_c(f(\\theta^*, x_t), y_t) + L_f(FM, FS). \\tag{5}\n\\]\nThe backward process of L is described as\n\\[\n\\theta_j \\leftarrow \\theta_j + \\alpha\\nabla_{\\theta}L(f(\\theta)) \\text{ s.t. } i > FZ. \\tag{6}\n\\]\nAfter the t-th task, we still use the first batch to get the important position I' of the current task with ratio R and combine it with the last version, \\(I = I \\lor I'\\). The whole process of the algorithm is shown in Algorithm 1. As shown in Section 6.3, Memorize Feature Patterns successfully delays forgetting with a few extra resources. The combination of Maintaining Generalizability and Memorize Feature Patterns outperforms other state-of-the-art CL methods(see in Table 1). Different from other replay methods, we are not required to store samples in storage, as they are only utilized during the current training. This also enhances privacy security."}, {"title": "6 EXPERIMENT", "content": "6.1 Experiment Setting\nDatasets: We evaluate LightCL on Split CIFAR-10[15] and Split Tiny-ImageNet[7]. To compare with other CL methods, we strictly follow [3, 30] by splitting CIFAR-10 and Tiny-ImageNet into 5 and 10 tasks, and each task including 2 and 20 classes, respectively. For Split CIFAR-10 and Tiny-ImageNet, we train each task for 50 and 100 epochs with a batch size of 32.\nBaseline Mehtods: We select typical CL methods from three categories, mentioned in section 2, including regularization-based (EWC [14]), replay (DER[3], ER[5], FDR[2]), and architecture-based (PNN[25]) methods. Since these above methods do not consider the edge scenario, we also compare our method against the state-of-the-art efficient CL method SparCL [30] combined with the replay method DER++ and ER. We use SGD without any CL method as a lower bound and train all tasks jointly (JOINT) as an upper bound."}, {"title": "6.2 Main Result", "content": "6.2.1 Comparison with CL methods on the pre-trained model. We evaluate results on Split CIFAR-10 and Tiny-ImageNet with a pre-trained ResNet-18 model under both CIL and TIL settings. The compared methods are divided into normal CL methods (EWC, PNN, FDR, ER, DER++) and an efficient CL method (SparCL). As shown in Table 1, the accuracy of LightCL without sparsity increases compared with ER, which performs best in normal CL methods except PNN. On Split Tiny-ImageNet, FLOPs of LightCL greatly reduce at most 3.07\u00d7 and Memory footprint at most 4.79x, compared with normal CL methods. That's because LightCL reduces the computation of backward process, and memory footprint of gradient and intermediate feature maps in frozen layers. Although the architecture-based method PNN has excellent accuracy under the TIL setting, it has tremendous FLOPs and memory footprint usage and also ignores the real application always under the CIL setting where the task's identity is not known during testing. On the other hand, compared with the efficient method SparCL, the delaying performance of LightCL with the same sparsity ratio is still competitive. Note that we employ a simple sparsity strategy without any optimization, such as dynamic sparsity [30]. Surprisingly, the memory footprint is reduced at most 6.19\u00d7 compared with SparCLDER++. Although the FLOPs of LightCL is higher than SparCL, LightCL is faster than SparCL in the real application since SparCL still costs abundant time on memory transfer. This will be discussed in Section 6.4.\n6.2.2 Comparison from scratch. Although deploying a pre-trained model on edge devices is more practical, most CL methods ignore it and focus more on CL from scratch. To better illustrate the effectiveness of LightCL, we also compare LightCL with other CL methods in the setting of CL from scratch. Since LightCL needs to freeze the lower and middle layers, we take the first task in the dataset as a pre-trained process, and we freeze the lower and middle layers in the following tasks. From Table 2, LightCL is still outstanding compared with other methods. LightCL0.9 with sparsity ratio 90% also shows better performance compared with the SparCL. Although replay methods (FDR, ER, DER++) are ineffective with a limited buffer size of 15, LightCL is favorable in a limited buffer size."}, {"title": "6.3 Ablation", "content": "In this section, we illustrate the effectiveness of Maintaining Generalizability and Memorize Feature Patterns of LightCL in Figure 4.\nDiscussion of Maintaining Generalizability: Maintaining Generalizability is an easy but effective method to delay forgetting and reduce resource usage. From Figure 4, compared with red and blue bars, all CL methods can enhance performance with the pre-trained model. Most of them (SGD, EWC, FDR, ER, LightCL) can even further delay the forgetting problem by Maintaining Generalizability. Note that the accuracy of SparCL decreases when using Maintaining Generalizability. This phenomenon is because frozen layers weaken the dynamic pruning process, leading to the accuracy drop. Although most of CL methods can benefit from Maintaining Generalizability, LightCL still performs better than other methods.\nDiscussion of Memorize Feature Patterns: We compare the performance of LightCL with and without Memorize Feature Patterns. As shown in Figure 4, the green bars of SGD and LightCL are the LightCL with and without Memorize Feature Patterns, respectively. We can see that LightCL with Memorize Feature Patterns can further increase the accuracy, proving its effectiveness. Let's consider the resource consumption of this method. We need to store the Feature Standard FS, mentioned in Section 5.2, as a reference to regulate. Since we only need to store important feature maps after Layer4.0.conv1 (included), the overall consumption of memory footprint, suppose all feature maps need to be stored, occupies about 1.9MB (on Split CIFAR-10), which is pretty small compared to what we reduce by Maintaining Generalizability."}, {"title": "6.4 Verification on Edge Device", "content": "Although we theoretically calculate the peak memory footprint for model parameters, gradients, and intermediate feature maps, some real-applied consumption, such as extra consumption in loading datasets, memory fragmentation, and some additional intermediate variables, also need to be considered. Hence, we deploy LightCL on an edge device, Jetson Nano, to verify the efficiency in the real environment. We compare LightCL with SparCLDER++. As shown in Figure 5, although the peak memory of them is all greater than the theoretical value shown in Table 1, the peak memory of LightCL is still greatly smaller compared to SparCL. Note that the consumption of the memory footprint is not always equal to the peak memory footprint. We also need to observe the reserved memory, red line in Figure 5. Since the peak memory of LightCL is smaller than SparCL, the reserved memory is also smaller than SparCL. From the x-axis that represents the change in time, the training process of LightCL is faster than SparCL since the memory transfer consumption has greatly hidden the training speed in SparCL. All of these findings suggest the effectiveness of LightCL in edge devices."}, {"title": "7 CONCLUSION", "content": "This paper proposes a compact CL algorithm LightCL to overcome the catastrophic forgetting problem in edge scenarios. It is the first study to explore the distribution of generalizability in neural networks by evaluating two designed metrics of learning plasticity and memory stability. This evaluation demonstrates that the generalizability of different layers in a neural network exhibits significant variation. Thus, we proposed LightCL with Maintaining Generalizability and Memorize Feature Patterns. Experiments demonstrate that LightCL is outstanding beyond other state-of-the-art CL methods with fewer resources, especially a lower memory footprint. Also, our method performs more favorably on edge devices and shows significant potential for efficient CL."}]}