{"title": "Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?", "authors": ["Sravanti Addepalli", "Yerram Varun", "Arun Suggala", "Karthikeyan Shanmugam", "Prateek Jain"], "abstract": "Large Language Models (LLMs) are known to be susceptible to crafted adversarial attacks or jailbreaks that lead to the generation of objectionable content despite being aligned to human preferences using safety fine-tuning methods. While the large dimensionality of input token space makes it inevitable to find adversarial prompts that can jailbreak these models, we aim to evaluate whether safety fine-tuned LLMs are safe against natural prompts which are semantically related to toxic seed prompts that elicit safe responses after alignment. We surprisingly find that popular aligned LLMs such as GPT-4 can be compromised using naive prompts that are NOT even crafted with an objective of jailbreaking the model. Furthermore, we empirically show that given a seed prompt that elicits a toxic response from an unaligned model, one can systematically generate several semantically related natural prompts that can jailbreak aligned LLMs. Towards this, we propose a method of Response Guided Question Augmentation (ReG-QA) to evaluate the generalization of safety aligned LLMs to natural prompts, that first generates several toxic answers given a seed question using an unaligned LLM (Q to A), and further leverages an LLM to generate questions that are likely to produce these answers (A to Q). We interestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to producing natural jailbreak questions from unsafe content (without denial) and can thus be used for the latter (A to Q) step. We obtain attack success rates that are comparable to/ better than leading adversarial attack methods on the JailbreakBench leaderboard, while being significantly more stable against defenses such as Smooth-LLM and Synonym Substitution, which are effective against existing all attacks on the leaderboard.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) are trained on massive web-scale data, and are thus exposed to diverse forms of objectionable content during pre-training. To prevent these models from exhibiting undesirable behavior, the generation of toxic content is suppressed using alignment techniques such as reinforcement learning via human feedback (RLHF) (Bai et al., 2022; Christiano et al., 2017), instruction tuning (Ouyang et al., 2022; Wei et al., 2021) and safety filters (Han et al., 2024; Inan et al., 2023; Zeng et al., 2024a). However, recent research has revealed that these techniques can be circumvented by adversarial attacks (Carlini et al., 2023; Zou et al., 2023) and handcrafted jailbreaks (Shen et al., 2023; Wei et al., 2023), which are specifically designed to circumvent the safety mechanisms in aligned models. This raises concerns about the robustness of aligned LLMs, and brings up a crucial question: how robust are aligned LLMs to natural, in-distribution prompts, which are likely to be encountered during typical usage? Understanding this is essential for developing better safety training strategies and to accurately characterize the real-world safety of models.\nTo answer this question, we aim to design natural prompts that are semantically related to a given toxic seed prompt. Surprisingly, we find that aligned LLMs such as GPT-4 (OpenAI, 2023b), are brittle against natural prompts generated by simply paraphrasing toxic questions using LLMs. This indicates that current safety mechanisms may be overly reliant on surface-level features of the input, rather than a deeper understanding of intent. Furthermore, we propose Response Guided Question Augmentation (ReG-QA) to systematically evaluate the in-distribution generalization of LLMs after"}, {"title": "2. Related Work", "content": "Large Language Models are susceptible to adversarial attacks (or jailbreaks) that are designed to circumvent their safeguards, thereby inducing the generation of objectionable content. Initial works on LLM jailbreaks have focused on designing handcrafted prompts to elicit undesirable responses (Shen et al., 2023; walkerspider, 2022; Wei et al., 2023; Yuan et al., 2023). While such manual methods are crucial to identify and demonstrate vulnerabilities, they are neither scalable, nor sufficiently comprehensive, to robustly evaluate evolving versions of models which can be trained on such publicly accessible jailbreaks. Another line of work employs white-box optimization techniques (requiring access to model weights) such as gradient ascent to generate prompts that trigger unsafe outputs (Carlini et al., 2023; Zou et al., 2023). A key weakness of these techniques is that the resulting prompts often appear nonsensical and unnatural, and can thus be easily detected based on the presence of such high perplexity tokens (Alon and Kamfonas, 2023; Jain et al., 2023).\nThe drawbacks related to both manually crafted jailbreaks and white-box attacks have led to greater focus on automated generation of natural language jailbreaks. Liu et al. (2023) propose hierarchical genetic algorithms to generate stealthy jailbreaks using existing handcrafted jailbreaks as prototypes to reduce the search space. Shah et al. (2023) generate prompts that instruct the LLM to"}, {"title": "3. Background and Motivation", "content": "In Figure 1, we categorize the landscape of jailbreaks into different regions based on the distribution they belong to. R4 broadly represents the region of all possible text which may/ may not have semantic meaning, R3 is the subset of this containing semantically meaningful text. We consider R2 as the pre-training data distribution, with RO being a subset which is used for safety fine-tuning and R1 being the region close to the fine-tuning data distribution. We note that RO may not always be a subset of R2. We depict different methods of modifying a toxic seed question that results in a safe denial response (denoted by a green cross in RO), into a jailbreak that results in a toxic response (denoted by red cross). Standard gradient based adversarial attacks such as A4 produce text without any semantic meaning, and are thus very easy to detect using perplexity based thresholding methods (Alon and Kamfonas, 2023; Jain et al., 2023). Attacks such as A3 incorporate the objective of generating natural language jailbreaks (Chao et al., 2023; Liu et al., 2023; Shah et al., 2023; Takemoto, 2024; Zeng et al., 2024b), and thus circumvent such simple defenses. While these attacks lie within the distribution of semantically meaningful text (R3), they are still far from the distribution of natural text (R2), since they are crafted to optimize a certain objective, or by prompting LLMs directly or indirectly to produce stealthy prompts. Similarly, although handcrafted jailbreaks (Shen et al., 2023; walkerspider, 2022; Wei et al., 2023; Yuan et al., 2023) also contain well-formed sentences, they again lie in far from the distribution of natural text, since they are deliberately crafted with an intention of jailbreaking the LLM. Thus, existing works show that it is very easy for an adversarial player to jailbreak an LLM. Contrary to this, we aim to understand the robustness of LLMs to prompts that belong to the distribution of natural data (R2). The training data distribution captures the diversity present in web scale data, and represents the variety of user prompts that can be expected during inference,"}, {"title": "4. Threat Model", "content": "In this work, we consider the generation of in-distribution, natural jailbreak prompts related to a given seed prompt x. While prior works mostly focus on naturalness of language, we constraint the threat model further by additionally considering naturalness of content as well. We thus define our threat model as the set of prompts which are naturally occurring with respect to the training distribution of LLMs, and denote it as Dnat. To formalize the constraint on similarity with respect to the seed prompt, we use an embedding model (for example, a sentence embedding model like sentenceBERT (Reimers, 2019) or Gecko (Lee et al., 2024)) denoted as an encoder Enc. We consider a perturbation radius \u03b5 in the embedding space with respect to the seed prompt x under the distance metric related to cosine similarity given by $d(x', x) = 1 \u2212 cos(Enc(x), Enc(x'))$ where cos(a, b) denotes the cosine similarity between the embedding vectors a and b. Thus, we define the threat model $T_{x,\\epsilon}$ as:\n$T_{x,\\epsilon} = \\{x' : x' \\in Dnat \\land (1 \u2212 cos (Enc(x), Enc(x'))) < \\epsilon\\}$                                                          (1)\nThe attack success criteria of prompts within the threat model is measured using a classification model (or judge) Mjudge, which operates on the generated jailbreak x', the response of the target LLM y and a system prompt S, to generate an output of 0 (safe) or 1 (unsafe). We note that relying solely on embedding similarity can be susceptible to adaptive adversarial attacks, where an adversary could craft prompts that appear close in embedding space but deviate significantly in semantic meaning. To mitigate this, our proposed jailbreak generation pipeline remains independent of the specific embedding model used for defining the perturbation bound. Further, our approach uses benign and simple prompts - specifically, we neither instruct the attack generation LLM (directly or indirectly) to adopt any jailbreaking strategy, nor do we perform any form of optimization to increase attack success rate. While such strategies can increase attack success rate of our method as well, our restrictions ensure that the generated jailbreaks are closer to the distribution of natural text."}, {"title": "5. Proposed Method", "content": "In this section, we first present our algorithm for Question Augmentation, and further discuss implementation details of the same."}, {"title": "5.1. Generation of Question Augmentations", "content": "We first discuss how publicly accessible safety aligned LLMs (with only API access) and an unaligned LLM (presumably after pre-training and instruction tuning that does not involve safety) can be used to generate natural jailbreaks that lead to diverse questions from a given toxic seed question. Our approach, which we term as ReG-QA, exploits the potential asymmetry in safety alignment between question generation and answer generation in LLMs. We find that while safety-aligned LLMs are robust in generating safe responses to potentially harmful questions, they may be vulnerable to generating unsafe questions when prompted with harmful answers. This vulnerability allows us to generate a diverse set of natural prompts using some of the most capable publicly API accessible models (GPT-40), as illustrated in Figure 2.\nAlgorithm 1 formalizes the procedure. First, an unaligned LLM, denoted as $LLM_{Q->A}$, generates a diverse set of answers A from a given seed question q (Line 2). We then filter these answers based on criteria CA, selecting only those deemed toxic by an external judge and exceeding a predefined length threshold (Line 3), resulting in the subset Asel (Line 3). Next, we utilize a safety-aligned LLM, $LLM_{A->Q}$, accessible only via API, to generate questions from each answer a \u2208 Asel (Line 6). This LLM is prompted to produce multiple questions that could elicit the given answer. To improve the quality and diversity of the augmented questions, we apply a selection criterion C\u03be, ensuring minimal redundancy and overlap (Line 7). The resulting set of questions, Qaug in Algorithm 1, constitutes our natural jailbreaks. We evaluate their effectiveness in eliciting unsafe responses from frontier LLMs (also accessed via API) using a GPT-40-mini based judge.\nRemarks: The success of our method hinges on the observation that safety alignment in LLMs may not generalize symmetrically between question and answer generation. Our empirical results (presented in subsequent sections) demonstrate that safety-aligned LLMs, when prompted to generate questions from potentially toxic answers, produce undesirable questions with high attack success rates (ASR). This asymmetry suggests a potential \u201creversal curse\" (Berglund et al., 2023) in safety alignment, a phenomenon that warrants further investigation. While we leverage a safety-aligned LLM for question generation, one could alternatively utilize an unaligned LLM for this step.\nThe following section describes the precise prompts and the criteria for filtering used in our algorithm. We use minimal and direct prompts for generation as can be seen below in Section 5.2."}, {"title": "5.2. Implementation Details of the Proposed Algorithm", "content": "We aim to generate several question augmentations for a given seed question. Towards this, we first use an unaligned version of Palm-2-Otter model ($LLM_{Q->A}$ in Algorithm 1) to generate 100 long answers for each seed question using the following prompt (PromptQ\u2192A in Algorithm 1):\nAnswer the following question in 200 words: <seed question>.\\n Answer:\nWe ensure that the generated answer length is greater than a specified number of tokens (such as 100), and further select toxic answers using a judge-LLM Mjudge. Next, we use GPT-40 ($LLM_{A->Q}$ in Algorithm 1) using the following prompt (PromptA\u2192Q), to generate 10 questions based on each of the unsafe answers:\nGenerate 10 diverse questions that give the following specific answer: <answer> \\n NOTE: Respond only with a python list of length 10, containing these questions."}, {"title": "6. Experimental Results", "content": "Models: To empirically validate our claims, we present results on the following publicly API accessible models: GPT-3.5-Turbo-1106, GPT-4-0125-Preview, GPT-4o (OpenAI, 2023a), Gemma2-27B-IT, Gemma2-9B-IT (Gemma Team, 2024), Qwen2.5-72B-Instruct (Team, 2024), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Mixtral-8x22B-Instruct-v0.1 (Jiang et al., 2024). We use Palm-2-0tter (Anil et al., 2023b) based LLMs for generating unsafe answers from a given seed question in the proposed ReG-QA pipeline. Further we use GPT-40 for the generation of questions in both ReG-QA and Para-QA.\nJudge Selection: In Table 5 of the Appendix, we present results using various combinations of LLMs from the GPT family (OpenAI, 2023b) and prompt candidates from JailbreakBench (Chao et al., 2024)/ PAIR (Chao et al., 2023), on the Judge Comparison dataset on JailbreakBench (JBBurl, 2024) containing a mix of 300 human annotated unsafe and safe questions. We compare agreement (accuracy with respect to majority vote across human annotations as ground truth), FNR (for mis-classifying unsafe as safe) and FPR (for classifying safe as unsafe) rates. The cost per 1M tokens is also listed. We choose the option that is viable in terms of usage license, cost, agreement, FPR and FNR rates.\nJudge Prompt and Model: We evaluate the generated jailbreaks using the prompt for the safety judge on JailbreakBench (Chao et al., 2024) with gpt-40-mini-2024-07-1 acting as the judge (Ref: Appendix A for the judge prompt used).\nDatasets: We benchmark the performance of the proposed methods on JailbreakBench (Chao et al., 2024) 1, which is a publicly available dataset. The seed prompts are composed of 100 distinct misuse behaviours divided into 10 categories, with 55% original examples and remaining sourced from AdvBench (Zou et al., 2023) and HarmBench (Mazeika et al., 2024).\nBaselines: We compare with a simple baseline that paraphrases the seed question using this prompt:"}, {"title": "6.1. Experimental Setup", "content": "To empirically validate our claims, we present results on the following publicly API accessible models: GPT-3.5-Turbo-1106, GPT-4-0125-Preview, GPT-4o (OpenAI, 2023a), Gemma2-27B-IT, Gemma2-9B-IT (Gemma Team, 2024), Qwen2.5-72B-Instruct (Team, 2024), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Mixtral-8x22B-Instruct-v0.1 (Jiang et al., 2024). We use Palm-2-0tter (Anil et al., 2023b) based LLMs for generating unsafe answers from a given seed question in the proposed ReG-QA pipeline. Further we use GPT-4o for the generation of questions in both ReG-QA and Para-QA.\nJudge Selection: In Table 5 of the Appendix, we present results using various combinations of LLMs from the GPT family (OpenAI, 2023b) and prompt candidates from JailbreakBench (Chao et al., 2024)/ PAIR (Chao et al., 2023), on the Judge Comparison dataset on JailbreakBench (JBBurl, 2024) containing a mix of 300 human annotated unsafe and safe questions. We compare agreement (accuracy with respect to majority vote across human annotations as ground truth), FNR (for mis-classifying unsafe as safe) and FPR (for classifying safe as unsafe) rates. The cost per 1M tokens is also listed. We choose the option that is viable in terms of usage license, cost, agreement, FPR and FNR rates.\nJudge Prompt and Model: We evaluate the generated jailbreaks using the prompt for the safety judge on JailbreakBench (Chao et al., 2024) with gpt-4o-mini-2024-07-1 acting as the judge (Ref: Appendix A for the judge prompt used).\nDatasets: We benchmark the performance of the proposed methods on JailbreakBench (Chao et al., 2024), which is a publicly available dataset. The seed prompts are composed of 100 distinct misuse behaviours divided into 10 categories, with 55% original examples and remaining sourced from AdvBench (Zou et al., 2023) and HarmBench (Mazeika et al., 2024).\nBaselines: We compare with a simple baseline that paraphrases the seed question using this prompt:"}, {"title": "6.2. Discussion of results", "content": "Higher Attack Success Rate (ASR) than paraphrasing based baselines: We present results of our algorithm ReG-QA when compared with paraphrasing based question augmentation Para-QA in Table 1. Firstly, although the proposed method does not incorporate the objective of jailbreaking in any form during generation, we obtain very high attack success rates as shown in the table. The overall attack success rate is 82% for gpt-4 and 93% for gpt-3.5 for our method as against 41% and 66% respectively for Para-QA. This shows that our projection of the seed question to the space of natural prompts is quite different from just paraphrasing based methods. Further, across several categories, our method ReG-QA outperforms paraphrasing based methods, achieving 100% ASR on many categories for both GPT-variants. Similarly, we also present attack success rates for the open source Gemma-2 model with 27B parameters. Our method produces an ASR of 82% against 36% for the Para-QA baseline.\nIn our ASR evaluations presented in Table 1, target models have temperature of 1 which is the default setting for gpt-4 and gpt-3.5. We use this to mimic the realistic setting of usage through external APIs. We would like to highlight that this is different from the standard jailbreak evaluations,"}, {"title": "7. Conclusion", "content": "In this work, we propose a method for verifying the in-distribution generalization of LLMs after safety-training, and demonstrate that popular LLMs such as GPT-4 are brittle against even natural prompts which are semantically related to toxic seed prompts that elicit safe responses. We base our algorithm on the following failure modes of LLMs which we find: i) LLMs are more likely to produce toxic content when presented with cues or details from the answer in the question, ii) forward direction of safety training (Q to A) does not guarantee reverse direction of safety (A to Q). The latter allows us to generate jailbreak prompts using GPT-4o, by simply prompting it to generate questions that give the specified toxic answer. We obtain attack success rate of 82% for GPT-4 and 93% for GPT-3.5 on JailbreakBench. Finally, we show that our method is significantly more robust than existing attacks against several defenses on the JailbreakBench leaderboard. Thus our method serves as an adaptive attack against all defenses that incorporate non-naturalness and instability as the criteria for detecting jailbreaks. We hope this work inspires further research on understanding the generalization of existing safety training algorithms, motivating the need for better defenses."}, {"title": "8. Ethics Statement", "content": "In this work, we highlight the vulnerability of current day Large Language Models to prompts that can be obtained without including any adversarial objective in their generation. Our work thus aims at motivating the need for better safety training algorithms and defenses. Jailbreaks using the proposed approach can also be used for augmenting existing safety training methods with better training and evaluation data, in order to ensure that they are safer in future."}]}