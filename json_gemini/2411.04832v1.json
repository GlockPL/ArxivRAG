{"title": "Plasticity Loss in Deep Reinforcement Learning: A Survey", "authors": ["Timo Klein", "Lukas Miklautz", "Kevin Sidak", "Claudia Plant", "Sebastian Tschiatschek"], "abstract": "Akin to neuroplasticity in human brains, the plasticity of deep neural networks enables their quick adaption to new data. This makes plasticity particularly crucial for deep Reinforcement Learning (RL) agents: Once plasticity is lost, an agent's performance will inevitably plateau because it cannot improve its policy to account for changes in the data distribution, which are a necessary consequence of its learning process. Thus, developing well-performing and sample-efficient agents hinges on their ability to remain plastic during training. Furthermore, the loss of plasticity can be connected to many other issues plaguing deep RL, such as training instabilities, scaling failures, overestimation bias, and insufficient exploration. With this survey, we aim to provide an overview of the emerging research on plasticity loss for academics and practitioners of deep reinforcement learning. First, we propose a unified definition of plasticity loss based on recent works, relate it to definitions from the literature, and discuss metrics for measuring plasticity loss. Then, we categorize and discuss numerous possible causes of plasticity loss before reviewing currently employed mitigation strategies. Our taxonomy is the first systematic overview of the current state of the field. Lastly, we discuss prevalent issues within the literature, such as a necessity for broader evaluation, and provide recommendations for future research like gaining a better understanding between an agent's neural activity and its behavior.", "sections": [{"title": "1. Introduction", "content": "Deep Reinforcement Learning (RL) has recently seen many successes and breakthroughs: It has beaten the best human players in Go [96] and Dota [11], discovered new matrix multiplication algorithms [30], endows language models with the ability to generate human-like replies for breaking the Turing test [13], and has allowed for substantial progress in robotic control [86]. Its capabilities to react to environmental changes and make near-optimal decisions in challenging sequential decision-making problems are likely crucial for any generally capable agent. Also, RL's mode of learning through interaction purely from trial-and-error mimics human learning, making it a natural paradigm for modeling learning in artificial agents [98].\nDespite all the aforementioned successes, deep RL is still in its infancy, and in many ways- deep RL approaches are not yet reliable and mature. For example, most RL algorithms still use the comparatively small network from the seminal DQN paper [75]. Furthermore, to reach high levels of performance, deep RL typically needs substantial tweaking and elaborate stabilization techniques that are notoriously difficult to get right: From replay buffers and target networks [75] to noise de-correlation [102] and pessimistic value functions [33], and finally to idiosyncratic optimizer settings [4, 66] and bespoke hyperparameter schedules [94].\nThere are many reasons why this is the case: First and foremost, deep RL is inherently non-stationary, making it a substantially harder learning problem than supervised learning. Additionally, it suffers from its own optimization issues, such as under-exploration, sample correlation, and overestimation bias. Much recent work has been devoted to tackling these problems with ever-more elaborate algorithms, many of them aiming to transfer insights from tabular RL to the deep RL setting [18, 88].\nBut what if the problems in current deep RL can be attributed to a significant extent to optimization pathologies arising from applying deep neural networks to non-stationary tasks [12, 34, 80]? Recently, this view has gained traction under the umbrella term plasticity loss. In the context of deep learning, plasticity refers to a network's ability to quickly adapt to new targets. Consequently, plasticity loss characterizes a network state associated with a lost ability to learn. There is hope and evidence that if the problem of plasticity loss is resolved, this will also alleviate many of the aforementioned RL-specific challenges. The line of work on plasticity loss can be broadly described as trying to find answers to the following two main research questions:\n\u2022 Why do the neural networks of deep RL agents lose their learning ability [26, 66, 68, 80, 82, 97]?\n\u2022 How can the ability to learn be maintained [24, 61, 62]?\nThese questions are not only relevant for RL but also cover issues relevant to most modern machine learning: They address the fundamental problem of applying machine learning techniques in settings requiring adaptation to changing circumstances. This makes plasticity loss not just relevant for deep RL, but also for other areas applying deep learning, e.g., continual learning [26] or the ubiquitous pre-train/fine-tune setup in supervised learning [10, 62].\nScope The focus of this survey is on the phenomenon of plasticity loss in deep RL. As mentioned above, plasticity loss also occurs in continual learning or supervised learning, and while our survey touches on these settings, they are not our focus. Some surveys on continual learning also cover plasticity loss and catastrophic forgetting [104] but do not exclusively focus on plasticity loss\u2014as we do\u2014, thereby naturally limiting the depths of the exposition. Our in-depth focus on plasticity loss also distinguishes our work from Khetarpal et al. [53]'s survey, which discusses several relevant RL-specific sub-areas such as credit assignment or skill learning. In our survey, we emphasize connections between plasticity loss and other issues afflicting deep RL, such as overestimation bias [80] and its inability to scale [29]. Within deep RL, we concentrate on the single-agent setting, as the understanding of plasticity loss is most advanced there.\nStructure Our survey starts with an overview of the RL formalism and definitions relevant to plasticity loss in Section 4. As we will see, plasticity loss is intuitively easy to define as networks losing their ability to learn, but there is no single accepted definition in the literature yet. We also use this section to review different experimental setups to test for plasticity loss, including synthetic benchmarks and RL environments. Next, we categorize and present possible hypothesized causes for plasticity loss from the literature in Section 5, followed by a taxonomy of currently deployed remedies (Section 6). Section 7 then discusses some of the factors that deep RL researchers and practitioners should consider when using deep RL algorithms from the perspective of plasticity loss. Finally, our survey concludes with a discussion of the current state of the field and an outlook on future directions in Section 8."}, {"title": "2. Notation and Preliminaries", "content": "This section introduces our notation and presents some relevant quantities subsequently used to describe causes and mitigation strategies of plasticity loss. In particular, Section 2.3 presents multiple definitions of a network's feature rank used to measure a representation's quality. These form the basis of multiple regularizes in Section 6.4. In Section 2.4, we also introduce the gradient covariance matrix, which can be used to analyze a network's optimization landscape. Lastly, Section 2.5 reviews synthetic benchmarks and RL environments used to study plasticity loss."}, {"title": "2.1 General Notation", "content": "We adopt the following notation: We use lower-case bold symbols for vectors, e.g., $\\mathbf{x} \\in \\mathcal{X} \\subseteq \\mathbb{R}^{d'}$ to denote an input sample from the data space $\\mathcal{X}$ of dimension $d'$. Upper-case bold symbols denote matrices, e.g., $\\mathbf{X} \\in \\mathbb{R}^{n \\times d'}$ denotes the design matrix whose rows contain samples from $\\mathcal{X}$. Expectations with respect to a distribution $\\mathbb{P}$ are denoted as $\\mathbb{E}_{a \\sim \\mathbb{P}}[\\cdot]$. If it is clear from the context, we skip the subscript for brevity. We use $\\text{SVD}(\\mathbf{A})$ to denote the multiset of all singular values of $\\mathbf{A}$, $\\sigma$ to denote a single singular value, $\\sigma_i(\\mathbf{A})$ to denote the $i$th largest singular value of matrix $\\mathbf{A}$ and $\\sigma_{\\text{min}}$ and $\\sigma_{\\text{max}}$ to denote the smallest and largest singular value, respectively. For $\\phi: \\mathbb{R}^{d'} \\rightarrow \\mathbb{R}^d$ being a function mapping samples to features, we denote the feature matrix as $\\phi(\\mathbf{X}) \\in \\mathbb{R}^{n \\times d}$, where $d$ is the dimension of the representation."}, {"title": "2.2 Reinforcement Learning", "content": "Reinforcement learning is concerned with optimizing intelligent agents' actions via trial-and-error to maximize the so-called cumulative reward (defined below). The agents' interactions with an environment are formalized via Markov Decision Processes (MDPs) that can be described as tuples $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, r, \\rho_0, \\gamma)$, where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, i.e., set of possible actions, $\\mathcal{P}: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ is the transition kernel specifying the probability of transitioning from one state to another state upon taking a specific action, $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function specifying the reward the agent obtains for taking an action in a state, $\\rho_0$ is the initial state distribution, and $\\gamma$ is the so-called discount factor. The behavior of an agent is defined by its possibly stochastic policy $\\pi: \\mathcal{S} \\rightarrow [0, 1]^{|\\mathcal{A}|}$ specifying for each state a distribution over the actions the agent can take. We often write $\\pi(a|s)$ to denote the probability of action $a$ in state $s$ according to policy $\\pi$. The agent aims to maximize the (discounted) cumulative reward\n$\\displaystyle J(\\pi) = \\mathbb{E} \\bigg[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\bigg| \\pi \\bigg], \\qquad (1)$\nwhere the expectation is over the randomness of the transitions, the agent's policy, and the initial state. An optimal policy $\\pi^*$ maximizes $J(\\pi)$.\nKey quantities for RL algorithms are the state-value,\n$\\displaystyle V^{\\pi}(s) = \\mathbb{E} \\bigg[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\bigg| s_0 = s \\bigg], \\qquad (2)$\ni.e., the expected cumulative reward when starting from state $s$ and following policy $\\pi$ from there, and the action-value,\n$\\displaystyle Q^{\\pi}(s, a) = \\mathbb{E} \\bigg[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\bigg| s_0 = s, a_0 = a \\bigg], \\qquad (3)$\ni.e., the expected return starting from state $s$, taking action $a$, and following policy $\\pi$ afterwards. An optimal policy can be found by maximizing the expected value of the initial state, i.e.,\n$\\pi^* \\in \\arg \\max_{\\pi} \\mathbb{E}_{s \\sim \\rho_0} [V^{\\pi}(s)] \\qquad (4)$\nNote that state-values (and, similarly, action-values) can also be defined recursively:\n$\\displaystyle V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi(s)} \\bigg[r(s, a) + \\gamma \\sum_{s'} \\mathcal{P}(s, s', a) V^{\\pi} (s')\\bigg], \\qquad (5)$\nInspired by these recursive definitions are so-called temporal-difference learning approaches, e.g., approaches based on iteratively updating state-value estimates as\n$\\displaystyle V^{\\pi}(s_t) \\leftarrow V^{\\pi}(s_t) + \\alpha\\bigg[r_{t+1} + \\gamma V^{\\pi} (s_{t+1}) - V^{\\pi} (s_t)\\bigg]. \\qquad (6)$"}, {"title": "2.3 Definitions of Effective Rank", "content": "The effective rank is a measure commonly used to assess the quality of the representation learned by a neural network [46, 56, 67]. \u03a4o build an intuition of why the effective rank matters, recall the computation of of an RL agent's value function as $V(s) = (\\phi(s), W)$, i.e., as the inner product of non-linear features of the state and weights $W$. Now suppose that $\\phi(s) \\in \\mathbb{R}^d$ is low rank: This implies that the network's features lie in a lower dimensional subspace of $\\mathbb{R}^d$, potentially mapping dissimilar states to similar feature vectors, which in turn makes it harder to learn distinct values for these dissimilar states [71]. For a full-rank $\\phi(s)$, the network maps different states to dissimilar feature vectors by distributing them along all directions of $\\mathbb{R}^d$, facilitating easier learning of distinct values for different states.\nMultiple possible definitions of the effective rank have been considered in the literature, some of which we review here. The effective rank definition below is based on the energy ratio (also called cumulative explained variance) commonly used in principal component analysis and defined as the minimum $k$ such that a rank-$k$ approximation of the feature matrix explains a $(1 - \\delta)$ fraction of its total variance."}, {"title": "2.4 Gradient Covariance Matrix and Empirical Neural Tangent Kernel", "content": "The gradient covariance matrix and empirical neural tangent kernel (eNTK) are closely related quantities, giving insights into the optimization behavior and the generalization abilities of a neural network [66] by analyzing the dot product of the gradients of different samples $x_i$ and $x_j$, i.e., $(\\nabla_{\\theta}L(\\theta, x_i), \\nabla_{\\theta}L(\\theta, x_j))$. Here, $L$ denotes the used loss function and $\\nabla_{\\theta}$ the derivative of a neural network w.r.t. to its parameters. In the following, we will use the gradient covariance matrix $C_k$, noting that the eNTK is just the unnormalized gradient covariance matrix [68]. Given sample indices $i$ and $j$, the gradient covariance matrix is defined as\n$\\displaystyle C_k[i, j] = \\frac{(\\nabla_{\\theta}L(\\theta, x_i), \\nabla_{\\theta}L(\\theta, x_j))}{\\|\\nabla_{\\theta}L(\\theta, x_i)\\|\\|\\nabla_{\\theta}L(\\theta, x_j)\\|} \\qquad (9)$\nIn practice, it is impossible to compute this matrix over the complete state space $\\mathcal{X}$ of a deep RL environment. Thus, it is commonly approximated with $k$ samples. From a generalization perspective, $C_k$ gives insights into whether updates between a pair of inputs $x_i$ and $x_j$ generalize. This is the case when the dot product of their gradients is positive. When it is negative, it means that updates between samples do not generalize, but instead interfere [66]. From an optimization perspective, a pronounced block structure of the gradient covariance matrix indicates a sharp and unstable loss landscape. Additionally, when $C_k$ is low-rank, i.e., when most values are positive or negative values of similar magnitude, then the network has learned a degenerate function. This means that updates on single samples generalize to the whole input space [68], making it difficult to distinguish between individual samples."}, {"title": "2.5 Common Benchmarks in Deep Reinforcement Learning and Plasticity Loss", "content": "For this section, we abstractly use the term plasticity loss for a loss in a network's ability to learn and refer the reader to Section 4.1 for a formal definition. Factors commonly leading to a loss of learning ability can either occur naturally as part of a learning task or be induced artificially, enabling us to establish two categories of benchmarks for plasticity loss: The first category consists of RL environments, whereas the second consists of stationary datasets from supervised learning with (artificially) added non-stationarity. We start by reviewing the commonly used RL environments before discussing the artificially generated non-stationary datasets.\nThe most widely-used RL benchmarks to evaluate plasticity loss are the Atari suite [7] and the DeepMind Control Suite [99]. Both have in common that they are very well-established and provide a wide range of environments with different characteristics and difficulties. Atari differs from DeepMind Control in that its action space is discrete, whereas DeepMind Control uses a continuous action space. Additionally, DeepMind Control allows for training from image observations and vector states, whereas Atari only entails training from images. Due to the diversity of both benchmarks, they include environments where either plasticity loss or the associated pathologies occur strongly. For Atari, plasticity is lost for instance during training on Phoenix [83] and Space invaders, whereas Demon Attack suffers from inactive neurons [97]. For DeepMind Control, all tasks associated with the DOG environment pose a significant challenge to current algorithms due to quickly exploding gradient norms [80].\nRegarding other benchmarks, Atari-100k [52] is a subset of the full Atari suite consisting of 26 games, where the goal is to achieve the maximum possible reward within 100,000 steps. For model-free algorithms, this entails training with many updates per environment step, often exacerbating plasticity loss [25, 82]. For continuous control, the MuJoCo environments [100] are still used in some papers but have mostly been superseded by DeepMind Control, as evidenced by the number of papers using either of them recently. MetaWorld [111] is a benchmark for continuous control in robotics, containing 50 distinct tasks. From the perspective of plasticity loss, Nauman et al. [80] show that MetaWorld exhibits different pathologies than DeepMind Control, e.g., the parameter norms and gradient norms of the agent play more of a role in determining agent performance. This makes it a useful benchmark to use alongside DeepMind Control, enabling verification of algorithms on environments with substantially different characteristics.\nUtilizing the widely-used MNIST dataset as a foundation, Permuted MNIST is a continual learning benchmark where the pixels of the individual MNIST images are shuffled for each new task [23]. An alternative to pixel shuffling is label permutation [66], where the class labels are changed. For example, this could assign all nines the label \"one\" in MNIST. Label permutation differs from label randomization in that labels are changed consistently across classes, whereas for label randomization, the changes must not be consistent for a class. In contrast to assigning all nines the label \"one\" for permutation, randomization would amount to drawing a new, random label for each individual sample. Label randomization can also be applied to only a fraction of a dataset, termed label noise in the literature [47, 62]. These modifications can also be applied to other image datasets, such as CIFAR-10 [47] or ImageNet [23]. A specific benchmark building on ImageNet is Continual ImageNet [23], where the network has to solve a sequence of binary classification tasks. For each task, two labels with the corresponding samples are chosen from the 1000 labels of ImageNet. This results in the network solving a new task for each sequence step, as neither the input images nor the labels have been seen previously [23].\nAll benchmarks from above building on supervised learning datasets use classification tasks. In contrast, value-based deep RL uses regression with mean-squared error as loss [39, 75]. Utilizing this insight, Lyle et al. [68] define a regression benchmark using oscillating targets based on CIFAR-10 inputs. Here, each sample gets assigned a random target $y_i = \\sin (10^5 \\cdot f_\\theta(x_i)) + b$, where $b$ is an \"offset\" or bias parameter and $f_\\theta$ the parameter of a randomly initialized network. The bias parameter $b$ allows controlling the magnitude of the target, which is potentially associated with plasticity loss as discussed in Section 5.6.\nLastly, a particularly relevant training scenario is warm-starting [5, 10, 62], which refers to pre-training a network on one task before fine-tuning it on another task. This type of two-stage training is, for example, ubiquitous when using large foundation models. We can evaluate loss of plasticity when warm-starting by pre-training a model with one of the aforementioned non-stationarities, i.e., label noise, label shuffling, or a dataset with reduced size, followed by a second training stage, where the full and correctly labeled dataset is available. If the warm-started model shows subpar performance compared to a model trained only on all samples with correct labels, it is a strong indicator that it has lost plasticity [10, 27, 62, 68]."}, {"title": "3. Related Work", "content": "In the context of deep RL, the study of plasticity loss has recently (re-)gained traction [47, 82]. Consequently, general surveys on deep RL often do not yet cover plasticity loss [3, 105], whereas newer surveys mainly cover unrelated sub-areas of deep RL, e.g., structure in MDPs [76] or credit assignment [91]. To the best of our knowledge, our survey is the first to exclusively review plasticity loss for deep RL.\nThe two areas most closely connected to plasticity loss are (a) continual learning, and (b) continual RL. Continual learning is concerned with learning a sequence of tasks characterized by a dynamic data distribution [104]. It occurs in many contexts, such as when fine-tuning a pre-trained model [112] or when training LLMs [106]. To enable efficient learning under distribution changes, continual learning algorithms must balance a trade-off between \u201clearning plasticity and memory stability\" [104]. In other words, continual learning deals with both plasticity loss and catastrophic forgetting, putting an emphasis on the latter [23]. In contrast, our survey focuses solely on plasticity loss but in a scenario where data distribution shifts occur naturally\u2014 even without changing tasks\u2014, namely deep RL. Examples of such distribution shifts are changes in the state visitation distribution due to an updated policy or changes in an agent's target distribution due to an improved value function in TD learning (cf. Section 2.2).\nIn continual RL, the agent is faced not only with non-stationarity arising from changes in its own policy but also time-dependent changes to the MDP's transition and reward functions [2, 53]. From a conceptual perspective, this results in the agent facing the same stability-plasticity dilemma as a network does in supervised continual learning [104]. Our survey exclusively reviews the plasticity part dilemma. On the other hand, continual RL also comprises many RL-specific sub-areas, such as credit assignment, exploration under non-stationarity, and goal-conditioned RL [53]."}, {"title": "4. Definitions of Plasticity Loss and Related Phenomena", "content": "In this section, we first provide and generalize definitions of plasticity loss from recent literature and then relate them to aspects of related phenomena in continual learning."}, {"title": "4.1 Definitions of Plasticity Loss", "content": "In the literature, there is no single accepted definition of plasticity loss. Here, we attempt to unify some existing definitions and illustrate how many definitions from the literature are special cases of our result. Intuitively, all existing definitions try to capture a model's loss of ability to fit new targets but formalize this and the underlying training and evaluation setup differently.\nOur unified definition is as follows:"}, {"title": "4.2 Plasticity Loss in Other Fields and Related Phenomena", "content": "This section briefly discusses the relevance of plasticity loss in fields other than RL and its relation to other phenomena in deep learning.\nNetwork plasticity is also a crucial property for facilitating continual learning, where a system learns from a sequence of data distributions, each defining a new task [78]. \u03a4\u03bf succeed in continual learning, the learner must not only retain plasticity but also mitigate another well-known pathology of deep learning, namely catastrophic forgetting [72]. It refers to a network suddenly unable to solve a previously learned task after learning a new one. In continual learning, the system must find a balance between tackling plasticity loss and catastrophic forgetting: If the network is very plastic and immediately adapts to data from a different distribution, it is also more prone to quickly forgetting previously learned knowledge, resulting in catastrophic forgetting. The other extreme is a network with no plasticity, e.g., a network with frozen weights: It certainly cannot forget previous tasks but also exhibits no plasticity, i.e., freezing the weights corresponds to a complete loss of plasticity.\nWhen warm-starting training by fine-tuning a pre-trained network, task shifts are arguably one reason for exacerbated plasticity loss [5, 10]. However, plasticity can also be lost quickly in RL environments [83] without an obvious task shift through a changed objective. Whether an agent in an RL environment faces a single task is a peculiar question: In value-based RL with target networks, it can be argued that with each new target network update, the agent needs to solve a new task [4]. However, completely stationary learning problems may also result in plasticity loss. For example, Lyle et al. [68] show that using large-mean targets in stationary regression is enough for networks to lose their plasticity.\nLastly, plasticity loss is a broad categorization, likely subsuming several individual phenomena and causes. As we will discuss in Section 5.5, plasticity loss entails different sub-categories, such as adaptability to input distribution shifts and to target shifts [61]. Similarly, there is a generalizability component to plasticity loss. It occurs, for example, when a network pre-trained under non-stationarity has sub-par test performance when fine-tuned, despite comparable train performance to a network not pre-trained under non-stationarity. What is commonly referred to as plasticity loss and used as the basis for the definitions in Section 4.1, however, is trainability: It means that a network is unable to update its parameters, even given a high loss [1, 62]."}, {"title": "5. Causes of Plasticity Loss", "content": "This section presents and categorizes possible causes of plasticity loss from the literature. Note that these causes reside on different conceptual levels: For instance, non-stationarity (Section 5.5) is a common high-level property of a learning problem that seems to occur in almost all settings where plasticity loss occurs. In contrast, the curvature of a neural network's optimization landscape discussed in Section 5.4 is a very specific network property. Furthermore, we review saturated neurons in Section 5.1, collapse of a network's effective rank in Section 5.2, gradient issues of networks that have lost plasticity in Section 5.3, the regression loss in Section 5.6, and parameter norms in Section 5.7. Training with high replay ratios, which refers to doing many gradient updates per environment step, is discussed in Section 5.8, followed by early overfitting (Section 5.9). Having finished the exposition of potential causes of plasticity loss, we examine relationships between them in Section 5.10."}, {"title": "5.1 Reduced Capacity due to Saturated Units", "content": "Saturated [15, 68] or dormant [97] units are one of the most prominent pathologies associated with reduced agent performance and plasticity loss. They are an obvious and objectively measurable sign indicating that the network cannot utilize its full capacity. Therefore, it is easy to relate them to reduced network expressivity and slow learning [97]. However, it is unclear whether dormant neurons are the main cause of plasticity loss or are just a pathology associated with networks that have lost their plasticity. For example, Sokar et al. [97] discuss target non-stationarity as contributing to an increase in dormant neurons throughout training. Other aspects of modern deep RL algorithms might also exacerbate the phenomenon, such as training with high replay ratios [25, 94].\nHow can units with reduced capacity be formally defined? The literature provides a plethora of options to do this, which we group into two categories: Saturated units for which a shift in the pre-activation distribution reduces the capacity of the neuron to produce meaningfully different outputs given inputs from its input distribution. For example, this could be a ReLU neuron where all inputs are positive or negative, rendering it inactive (\"dead\") [70] or linear [68], respectively. Dormant units [97] are instead characterized by low post-nonlinearity activations. When considering a tanh unit, one can easily see how these categories differ: For large pre-activations, the unit's output will be close to one for all inputs. Importantly, the output will be close to one (i.e., show small numerical differences), even considering significant differences in the pre-activation. On the other hand, such a saturated tanh unit is clearly not dormant because its post-nonlinearity activation is high.\nSaturated Neurons have first been discussed in Bjorck et al. [15] in the context of pixel-based continuous control. The authors took a closer look at runs of the then-state-of-the-art agent DrQ-v2 [110] and observed that most of them exhibited saturated tanh policies, with runs failing to learn and not being able to move out of this regime. This results in actions at the boundaries of the [-1,1] interval from which actions are usually selected in continuous control combined with vanishing gradients, as $\\forall x: |\\text{tanh}(g(x))| \\approx 1 \\Rightarrow \\text{tanh}(g(x)) = [1 - \\text{tanh}^2(g(x))]g'(x) \\approx 0$. Interestingly, their proposed solution to normalize the features of the penultimate layer increases performance when applied not only to the actor but also to the critic [15], highlighting the fact that bounded activations may be just as important to prevent critic divergence [12].\nMore recently, Lyle et al. [68] partition non-stationary learning problems into an erasing or unlearning phase and a disentanglement phase, finding that the first causes a form of saturation in ReLU units. These linearized units are characterized by their pre-activation distribution having only positive support. To be more precise, the unlearning phase after a task shift causes a distribution shift in the neuron's pre-activation distribution through gradients that either increase or decrease the preactivation values for all training samples. If all pre-activations for a neuron are now positive, the unit becomes linearized, removing its nonlinear component. If a neuron's pre-activations are negative for the training dataset, it moves into the dead neuron regime. Both of the aforementioned pathologies effectively reduce the expressive capacity of the network. As a remedy, the authors propose to apply LayerNorm [6] before a unit's nonlinearity [68].\nDormant Neurons have been introduced by Sokar et al. [97] as a measure for the reduced expressivity of a neural network. In experiments, they have shown that dormant neurons are associated with performance plateaus and reduced performance of DQN [75] agents trained on different Atari games. To determine the level of dormancy, one has to first calculate normalized activation scores $s_i^l$ for each neuron $i$ in all non-final layers $l$ [97]:\n$\\displaystyle s_i^l = \\frac{\\mathbb{E}_{x \\in \\mathcal{D}} [|h_i^l(x)|]}{\\frac{1}{H^l} \\sum_{k \\in [H^l]} \\mathbb{E}_{x \\in \\mathcal{D}} [|h_k^l(x)|]} \\qquad (13)$\nHere $x \\in \\mathcal{D}$ are samples drawn from an input distribution, e.g., the replay buffer in off-policy deep RL, $h_i^l(x)$ denotes the post-nonlinearity activation of a neuron in layer $l$, and $H^l$ the number of neurons in layer $l$."}, {"title": "Neuron Dormancy", "content": "If the score in Equation (13) is below a threshold $\\tau$, i.e., $s_i^l < \\tau$, then neuron $i$ in layer $l$ is $\\tau$-dormant. Denoting $H_{\\text{D}}^l$ as the number of dormant neurons per layer, and $N^l$ as the total number of neurons per layer, the dormancy ratio is the fraction $H_{\\text{D}}^l/N^l$ of all layers except the final layer [97, 107]:\n$\\displaystyle \\beta = \\sum_{l \\in \\Theta} H_{\\text{D}}^l / \\sum_{l \\in \\Theta} N^l \\qquad (14)$\nAn agent exhibits the dormant neuron phenomenon if its fraction of dormant neurons increases over the course of the training.\nThe above definition is certainly not the only sensible way of defining a measure for neurons with reduced activity, but it is currently widely accepted within the literature [69, 84, 85, 97, 107] due to its simplicity and intuitive understanding. Dohare et al. [23] define a more complex measure of neuron utility based on a product between the magnitudes of a unit's summed weights and its activations. A more straightforward option for ReLU networks is to simply count the number of zero activations [1, 26].\nTo summarize: While there are different options for defining inactive neurons [23, 26, 97], all of the works cited above agree that they are a symptom of reduced expressivity caused by some form of non-stationarity [1, 69, 97]. Where they differ is in the proposed solution to address inactive units: Sections 6.1 and 6.2 discuss a plethora of reset strategies as possible remedies [23, 82, 97, 107], Section 6.5 considers activation functions for non-stationary problems such as CReLU [1], and Section 6.3 examines parameter regularization [63]."}, {"title": "5.2 Effective Rank Collapse", "content": "Multiple works observe a correlation between an agent's performance degradation and its representation becoming low-rank. This phenomenon is dubbed \"rank collapse\" and has been observed both in online [42, 56, 65] and offline [38, 56] RL. Kumar et al. [56] establish a connection between the effective rank of an agent's representation and its ability to learn: A decrease in the representation's rank leads to increased TD error. In theoretical and empirical analysis, Kumar et al. [56] show that a drop in rank comes by the largest singular values of the representation outgrowing the smaller ones, most likely caused by bootstrapping in value-based deep RL. This effect may be exacerbated in sparse-reward environments such as Montezuma's revenge on Atari [66]. G\u00fcl\u00e7ehre et al. [38] presents a thorough empirical study and find that the association between effective rank and agent performance is not as straightforward as previously assumed in offline RL. In particular, it depends on potentially confounding hyperparameter and architecture choices, such as the activation function and the learning rate. However, they show that the collapse of the effective rank to a very small value is reliable and allows for identifying underfitting agents with suboptimal performance at the end of training.\nThe phenomenon of rank collapse is tightly intertwined with other phenomena pertaining to loss of plasticity. In offline RL, there is a strong correlation between the effective rank and the number of dead units in the agent's network [38]. Similarly, resetting dead or dormant neurons increases feature rank at the end of online RL training [97]. As of now, the exact causal relationship between dead neurons and rank collapse is unclear."}, {"title": "5.3 First-order Optimization Effects", "content": "Deep neural networks are most commonly trained using stochastic gradient descent (SGD); looking at a neural network's gradients is, therefore, a plausible direction to look for causes of plasticity loss. Vanishing gradients [44", "1": "."}]}