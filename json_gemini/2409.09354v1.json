{"title": "PeriGuru: A Peripheral Robotic Mobile App Operation Assistant based on GUI Image Understanding and Prompting with LLM", "authors": ["Kelin Fu", "Yang Tian", "Kaigui Bian"], "abstract": "Smartphones have significantly enhanced our daily learning, communication, and entertainment, becoming an essential component of modern life. However, certain populations, including the elderly and individuals with disabilities, encounter challenges in utilizing smartphones, thus necessitating mobile app operation assistants, a.k.a. mobile app agent. With considerations for privacy, permissions, and cross-platform compatibility issues, we endeavor to devise and develop PeriGuru in this work, a peripheral robotic mobile app operation assistant based on GUI image understanding and prompting with Large Language Model (LLM). PeriGuru leverages a suite of computer vision techniques to analyze GUI screenshot images and employs LLM to inform action decisions, which are then executed by robotic arms. PeriGuru achieves a success rate of 81.94% on the test task set, which surpasses by more than double the method without PeriGuru's GUI image interpreting and prompting design. Our code is available on https://github.com/Z2sJ4t/PeriGuru.", "sections": [{"title": "I. INTRODUCTION", "content": "Smartphones have become an integral part of daily life, serving as tools for reading, learning, socializing, and shopping. As anticipated in 2024, an estimated 4.88 billion individuals possessing smartphones, dedicating an average of 4.37 hours daily to their usage [1]. Moreover, the dynamic market of mobile apps illustrates its vitality as close to 1000 novel apps are introduced daily on Google Play [2].\nHowever, not all users find mobile apps easy to navigate. Seniors, for instance, often encounter barriers, including difficulty in locating small text and/or icons, confusion with user interfaces and functionalities, and compounded by a lack of familiarity with app operations [3]. Furthermore, global statistics from the World Health Organization (WHO) exhibit that approaching 2.2 billion people worldwide experience near or distant vision impairment [4]. Previous studies have revealed that the measures for visually impaired individuals in mobile apps are insufficient, with 70% of apps lacking accessibility tags [5]. Thus, an intelligent app operation agent has the potential to play an assistant role and facilitate the lives of this demographic. Simultaneously, even adults without perceptual or cognitive impairments can immensely benefit from these assistants, whether to bypass learning new app operations or expedite repetitious processes.\nWhile numerous studies have explored automated app operations within the domain of software testing, the prevailing methodologies often rely on tools that capture screen position information at the software level, such as UIAutomator [6]. Yet, in the realm of user assistants, we advocate for a purely peripheral approach. Our investigation has identified several key reasons for this preference:\n\u2022 Privacy concerns of built-in software on phones: Mobile phones frequently contain substantial personal data. Generally, users would be resistant to granting software assistants the rights to oversee screens, manipulate files, and control software on their devices. A purely peripheral solution emerges as a safer alternative.\n\u2022 Permission constraints of voice control in mobile apps: Dependence on software for screen information and mobile phone manipulation demands elevated permissions. Many voice-control assistants are restricted from operating within third-party apps, due to these permission constraints. This is particularly challenging for user groups such as the elderly, as they often struggle with high-permission operating systems, fearing the potential for irreversible misoperations.\n\u2022 Compatibility/applicability across platforms: Although Android and iOS dominate the global mobile operating system landscape, emerging platforms such as Surface Duo [7] and HarmonyOS [8] are gaining traction. Additionally, some manufacturers have crafted simplified operating systems targeted at seniors. Software-based assistants struggle to maintain compatibility across this diverse array of systems.\nHence, we seek to develop an embodied agent as a purely- peripheral mobile app assistant. It does not require any data access, voice-control, or any other permission from the mobile OS/apps, being naturally cross-platform compatible.\nIn this paper, we present our embodied agent, PeriGuru. Leveraging Computer Vision (CV) methodologies, PeriGuru interprets screenshot images to formulate prompts and further exploits large language models (LLMs) for effective decision-making. This approach enables it to autonomously execute a range of app operations on behalf of users, with no need to invoke any software APIs of the smartphone. PeriGuru represents a significant advancement in the development of mobile app agent based on image understanding. Upon evaluation, PeriGuru demonstrates an impressive performance, achieving a total plan success rate of 89.71% and an execution success rate of 81.94%. These results underscore PeriGuru's effectiveness in facilitating app operations for users, thereby enhancing accessibility and usability."}, {"title": "II. LIMITATIONS OF GENERALIZED LARGE LANGUAGE MODELS", "content": "The evolution of Large Language Models (LLMs) has been remarkable, with models like GPT-4V [9] setting new benchmarks by demonstrating the capacity of LLMs to process and interpret visual data. This capability significantly extends the reasoning and decision-making potential of LLMs, positioning them as increasingly viable solutions for embodied intelligent agents.\nHowever, without precise text prompting, generalized LLMs display several shortcomings. For a comprehensive analysis, we conducted an experiment comprising 72 task sets using the grid mode of AppAgent [10], a state-of- the-art LLM-based app operation agent. In this mode, the identification of GUI elements is only achieved through the coordinate representation of the gridded image, without utilizing the software GUI metadata typically required for AppAgent.  From the 30 tasks that ended in failure, we derived several primary sources of errors\u00b9, as documented in Table I. The results highlight the key challenges in app operation tasks for agents:\n\u2022 Icon recognition failure: Icons pose a significant challenge in app operation tasks. Enhancing the agent's capability to correctly identify and interpret these visual elements is instrumental in the successful execution of app operation tasks.\n\u2022 Coordinate and ordinal misunderstanding: LLMs exhibit difficulties in comprehending coordinates and"}, {"title": "III. METHOD", "content": "Fig. 2 showcases the overall architecture of PeriGuru. The process comprises three distinct stages as follows:\nPerception stage (Sec.III-A). Initially, a high-speed camera captures a raw image, which undergoes fundamental CV processing to extract a screenshot. PeriGuru then performs element and layout recognition to formulate a detailed screenshot description. This description, along with the labeled screenshot, is forwarded as screen observation data to the subsequent stage.\nDecision-making stage (Sec.III-B). Leveraging the screen observation information, along with a record of historical actions and analogous task instances, PeriGuru crafts prompts to interface with a LLM to obtain the subsequent action decision in the form of a function call.\nAction stage (Sec.III-C). PeriGuru then translates the decision formulated in the preceding stage into a series of robot actions. Additionally, identifiable errors are integrated back into observation information, enhancing the framework's capacity for error-aware decision-making."}, {"title": "A. Perception: GUI Layout Understanding", "content": "As shown in Fig. 3, PeriGuru adopts a purely image-based methodology for identifying the components, layout, and interaction logic of mobile GUIs. This approach bypasses the need for access to any underlying code or GUI metadata of the apps. The recognition process encompasses several key components.\nGUI widgets detection. Compared to images captured by cameras, software screenshots are much easier to collect and thus are more conducive to the assembly of large- scale training datasets. However, in practical application environments, images obtained through cameras are frequently subject to variations in ambient lighting and camera angles, which can compromise the image quality when compared to those derived from software screenshots. To address the discrepancy between the quality of training data from software screenshots and the validation data from camera captures, we implemented a data augmentation strategy aimed at bolstering the robustness of our detection model. We augmented data in terms of both color and shape by adding uneven light masks and noise, and applying rotations and perspective deformations. This can make the distribution of the training set more similar to the validation set. In Sec. IV-B, we verified the performance improvement of this enhancement for GUI widgets detection.\nThe dataset we leveraged for the detection of non-textual GUI components is VINS [11]. It provides a more accurate re-annotation of the well-established GUI image dataset RICO [12], enriched with nearly 2000 additional sets of screenshot data from Android and Apple apps. This dataset enables the precise recognition of significant GUI components, including icons, images, checkboxes, text input fields, pop-ups, buttons, and sidebar menus. As for the setup of object detection algorithm, We opted for the contemporary YOLOv5 framework [13] which showed impressive results in terms of both detection accuracy and response speed.\nComplementarily, to identify and process texts a key GUI feature signalling function\u2014we resort to mature optical character recognition (OCR) services, such as Google OCR [14] or Baidu OCR [15].\nWe have also ensured a modular design for our code, granting seamless replacements of GUI widget detection methods and other GUI perception pipeline components with alternative algorithms. Consequently, this section can deploy varying object detection algorithms, regardless of their foundations in traditional CV schemes or learning-based approaches, as well as different OCR interfaces.\nList recognition through clustering. GUI interfaces are not merely random amalgamations of independent widgets. Rather, they are thoughtfully organized with widgets of similar semantics grouped together to aid human comprehension of interface functionality. This approach is supported by research in psychology and biological vision [16], and it continues to be widely implemented in the field of GUI design. Consequently, the ability to correctly discern this list-like structure is crucial for our agent to understand the functionality of GUIs.\nBuilding upon previous research [17], we adopt a similar methodology to identify list structures. We utilize the DBSCAN clustering algorithm [18]-implementing it on the spatial distributions, such as widget's area, coordinates, and inter-widget gaps. Significantly, this clustering process also allows us to rectify any missed or incorrectly-detected widgets resulting from the previous stage.\nGenerate textual descriptions for icons. Beyond text, icons also play an important role in conveying functionality information within a GUI interface. Icons are frequently crafted following a consistent design language, enabling experienced users to discern their meanings without relying on accompanying text. For instance, a magnifying glass typically symbolizes \u201csearch\u201d, while a left-facing arrow indicates \"back\". As a result, assuming a level of user familiarity, not all icons are complemented with textual annotations in certain GUI designs.\nGiven the significance of interpreting icons for fully understanding GUI functionality, we employ the state-of- the-art icon label generation framework, LabelDroid [5] to generate textual descriptions for all common icons within the interface. These annotations are appended to the final GUI summary, enhancing PeriGuru's comprehension of GUI functionality.\nHierachical structure establishment and component sorting. In this phase, we establish the parent-child relationship between screen components based on intersection over union (IoU) calculations, which facilitates the construction of a tree-like hierarchical structure. Following this, we organize all components based on the natural human eye reading pattern using the widely accepted recursive XY-cut page segmentation algorithm [19].\nOutput formal GUI summary. Given that the majority of LLMs' training data originates from raw web pages, an HTML-like format is highly conducive to conveying screen observation information to LLMs [20]. With this format, each screen element includes these essential attributes:\n\u2022 id: The component's unique resource id.\n\u2022 class: The category of the component, which typically encompasses types such as Text, Image, Icon, CheckedTextView, etc.\n\u2022 content: Additional descriptions of components, such as textual content and icon explanations."}, {"title": "B. Decision-Making: LLM Agent Establishment", "content": "The domain of utilizing LLMs for decision-making encompasses two principal methods: one is to generate long- term action plans as seen in works such as ProgPrompt [21] and LLM-Planner [22]; the other involves selecting the immediate, most relevant action to the task, epitomized by the work on discussing robotic affordances, SayCan [23].\nIn our scenario, the limited observational scope of operating app tasks makes it challenging to directly create long- term action plans. As a result, we are obliged to design prompts for the LLM to choose a single action step. The design of GURU's LLM prompt is designed as follows.\n\u2022 Task: The description of mobile app operation task that should be executed.\n\u2022 Screen description: This is an HTML-like representation generated during the perception stage, encapsulating the visual elements of the GUI interface. For LLMs that accommodate multimodal inputs, a screenshot image is selectively integrated alongside the description.\n\u2022 Historical actions: These are the actions previously undertaken by the agent, complemented by feedback on their actual execution. For instance, if a keyboard is absent when required for a Text() action, the corresponding error information will be reported in this section.\n\u2022 Prompt examples (optional): By using a K-Nearest Neighbors (KNN) search to find the most analogous task in the case database and adding the execution steps of it into the prompts, the LLM agent can draw guided reference from case studies and make better decisions.\nThe output from the LLM is required through prompts to provide a summary of the current state, reflections on the forthcoming decision, a natural language explanation of the subsequent action, and ultimately the requisite function call, which is exhibited in detail in Fig. 2. Aligning with previous study [24], this approach facilitates the generation of a \"Chain of Thought (CoT)\" by the LLM, thereby enhancing its decision-making capabilities."}, {"title": "C. Action: Action Space Design", "content": "Taking into account the capabilities of the robotic arm and the common actions ordinarily undertaken by human users, we have delineated the following primitive actions, encapsulated as a functional construct:\n\u2022 Tap(id: int): This function simulates a tap on a UI element identified by its unique ID.\n\u2022 Long_press(id: int): This function emulates a long press on a UI element identified by its unique ID.\n\u2022 Text(text: str): To expedite text input, this function process, rather than emulating a series of Tap () operations for letter keys, we leverage this function to perform a sequence of typing actions when the keyboard is invoked.\n\u2022 Scroll (direction: str): This function executes a scrolling gesture on the screen, with direction specifying the desired scrolling direction.\n\u2022 Back (): To facilitate the agent's navigation away from irrelevant interfaces, this function is established as a distinct primitive action, rather than utilizing the Tap () function on a back button.\n\u2022 Finish(): This function signifies the completion of the given task, requiring no further action from the robotic arm."}, {"title": "IV. EXPERIMENTS", "content": "A. Methodology\nTask dataset. To construct a task dataset, we firstly selected 10 apps spanning four prevalent domains: news, social media, shopping and learning, and create a series of tasks manually. For each app, we manually designed a series of tasks. These tasks encompass the utilization of core app functionalities, such as searching for news articles in news apps and selecting products in shopping apps, as well as common user interactions, including adjusting settings and navigating to a certain interface. This initial task set comprises 36 single-step tasks and an equal number of multi-step tasks. For the multi-step tasks, the average optimal step count for multi-step tasks stands at 3.528.\nThen, to expand our task dataset, we extracted a collection of screenshots from the RICO [12] dataset. The RICO dataset offers valuable user interaction traces and UI hierarchy files. By utilizing these resources, we leveraged LLM to generate an additional 64 task descriptions. These generated tasks will are henceforth referred to as RICO tasks.\nMetrics. For performance comparison, we employed three distinct metrics:\n\u2022 Plan Successful Rate (Plan SR): This metric is derived from simulation operations performed through software interfaces. After the mobile app agent provides the type and coordinates of the next operation, we perform it on the proximate interactive element found by GUI metadata acquired through UIAutomator [6]. The success rate in this context is referred to as the plan SR.\n\u2022 Execution Successful Rate (Execution SR): To measure the success rate in practical operation, we established a testbed with Yahobom DOFBOT SE [25] robotic arm, which is shown in Fig.5. Within this setup, we assessed each method's execution SR.\n\u2022 Average Steps: This metric represents the average step count for all successfully executed tasks, serving as a reference for efficiency.\nBaseline. We take the grid version of AppAgent as the baseline model. A detailed explanation of it can refer to Sec.II. Furthermore, we conducted an investigation into the differences in performance when relying solely on GUI element detection and icon meaning recognition, as opposed to developing a hierarchical screen description, denoted as element labels in Table.II or label in Fig.6, or utilizing a comprehensive PeriGuru prompt, referred to as the HTML-like document in Table.II or document in Fig.6."}, {"title": "B. Results", "content": "Performance enhancement by PeriGuru. As demonstrated in Table II and Fig. 6, PeriGuru has achieved a 24.89% improvement in plan success rate over AppAgent (grid). This increase underscores the framework's effectiveness in enhancing mobile app agents' comprehension of GUI elements and hierarchical structures. Additionally, the execution success rate has seen a marked increase, rising to 2.3 times the baseline rate, which attests to the significant role of PeriGuru's object detection capabilities in improving the precision of coordinate determination.\nComparison of the multimodal and pure text LLM backbones. When employing multimodal models capable that support images, as evidenced in Table II and Fig. 6, the performance of PeriGuru with and without a GUI hierarchical document\u2014presented in the second and fourth rows, respectively\u2014does not exhibit significant variation. However, upon comparing the third and fifth rows, it is observed that the incorporation of documents enhances the plan SR by 13.21% and the execution SR by 16.33% when using a pure text LLM backbone. Additionally, a comparison between the fourth and fifth rows indicates that similar task success rates can be achieved without the reliance on original screenshot images. The generation of documents renders that original images and multimodal models are no longer absolutely necessary, thereby broadening the range of LLM backbone choices, reducing token consumption, and accelerating response speed.\nHowever, solutions that forgo the use of images often necessitate additional iterations of trial and error and encounter challenges in ascertaining task completion. As demonstrated in Table II, the average number of steps required to complete the task for image-independent solutions\u2014represented in the third and fifth rows\u2014is 26.91% higher than that of solutions which leverage images\u2014depicted in the second and fourth rows. This observation presents the next potential optimization direction for PeriGuru.\nThe effect of data augmentation on enhancing object detection models. We further conducted a comparative analysis of the precision, recall, and F1-score for PeriGuru's YOLOv5 object detection models, both with and without the implementation of the data augmentation strategies outlined in Section III-A. The results are presented in Table III. The findings indicate that data augmentation focused on shape and color, when applied independently, effectively enhances model performance, increasing the average F1 score by 4.78% and 1.73%, respectively. Moreover, the model that incorporates both augmentation techniques achieves the most substantial improvement, with an average F1 score increase of 11.80%, which proves the role of data augmentation strategies in improving model robustness."}, {"title": "V. RELATED WORK", "content": "LLM for Robotics. Large Language Models (LLMs) are advanced neural networks trained via unsupervised learning with vast parameter sets. The evolution of LLMs, exemplified by GPT-4V [9] demonstrates their ability, to process visual data, broadening their role in cognitive tasks and decision-making for embodied agents [27]. Noteworthy contributions in this domain include SayCan [23], who have combined task relevance with robotic affordances to inform action selection, deeply impacting LLM applications for embodied agents. ProgPrompt [21] embodies the benefits of code-based prompts, while Text2Motion [28] has adeptly generated actionable plans for complex tasks by leveraging varying levels of prompt granularity.\nAutomated mobile APP manipulation. The growing reliance on mobile applications in daily life boosts the demand for their automated operation. VTest [29] has introduced a robotic testing framework for mobile apps, and another research [20] proposed a LLM-based automated testing system. Additionally, there are LLM agents, such as [30], [10], that serve in the user assistance domain. However, they differ from PeriGuru by requiring software GUI metadata.\nImage-based GUI Understanding. In GUI design, software testing, and data monitoring, a lack of GUI metadata often hampers accessibility and label clarity. Studies like ReDraw [31], UIED [32], Xianyu [17], and screen recognition [33] have tackled this with image-based GUI understanding methods.\nMeanwhile, icon interpretation is crucial for understanding GUI functionality. In response, researchers have explored various strategies: IconIntent [34] used key-location-based CV methods to detect sensitive UI elements, while another study [35] pursued a deep learning-based method for this purpose. Tools like LabelDroid [5] and COALA [36] focus on generating concise icon descriptions. All these contributions have significantly informed PeriGuru's development."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In summary, PeriGuru advances assistive robotics by addressing accessibility issues in smartphone use for challenged populations, combining sophisticated computer vision algorithms with the decision-making prowess of LLMs to efficiently operate mobile apps.\nHowever, its focus is mainly on GUI image understanding, with less emphasis on enhancing LLM decision-making, leading to challenges like inaccurate task completion assessment. The untapped potential of LLMs in generating complex robotic actions is a recognized area for PeriGuru's future optimizations."}]}