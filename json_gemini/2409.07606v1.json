{"title": "The Role of Deep Learning Regularizations on Actors in Offline RL", "authors": ["Denis Tarasov", "Anja Surina", "Caglar Gulcehre"], "abstract": "Deep learning regularization techniques, such as dropout, layer normalization, or weight decay, are widely adopted in the construction of modern artificial neural networks, often resulting in more robust training processes and improved generalization capabilities. However, in the domain of Reinforcement Learning (RL), the application of these techniques has been limited, usually applied to value function estimators [Hiraoka et al., 2021, Smith et al., 2022], and may result in detrimental effects. This issue is even more pronounced in offline RL settings, which bear greater similarity to supervised learning but have received less attention. Recent work in continuous offline RL has demonstrated that while we can build sufficiently powerful critic networks, the generalization of actor networks remains a bottleneck. In this study, we empirically show that applying standard regularization techniques to actor networks in offline RL actor-critic algorithms yields improvements of 6% on average across two algorithms and three different continuous D4RL domains.", "sections": [{"title": "1 Introduction", "content": "Building modern Deep Learning (DL) systems often involves the application of various regularization techniques, such as dropout, weight decay, layer normalization, etc. These techniques enhance the performance, generalization, and robustness of neural networks, making them essential components in the success of models across diverse domains, including computer vision and natural language processing.\n\nThe primary motivation behind regularization is to prevent model overfitting given a finite amount of available data. In online RL, the issue of overfitting does not occur explicitly, resulting in a limited body of work focused on this area, with most investigations centering on value function estimators [Kumar et al., 2020b, Kostrikov et al., 2021, An et al., 2021]. In practice, applying regularization techniques to online RL often does not yield significant benefits. However, in the offline RL setting, the goal is to learn an optimal policy using a fixed dataset collected by other agents. This distinction makes offline RL conceptually more aligned with the use of regularization techniques. Furthermore, a recent study by Park et al. [2024] highlights that the actor networks are a critical bottleneck for the performance of offline RL algorithms. We hypothesize that common DL regularization techniques may alleviate this issue."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Offline Reinforcement Learning", "content": "We follow the conventional definition of RL problems as Markov Decision Processes, represented by the tuple (S, A, P, R, \u03b3), where: SC Rn denotes the state space, A C Rm denotes the action space, P:S\u00d7A\u2192S is the transition function, R : S \u00d7 A \u2192 R is the reward function, and \u03b3\u2208 (0, 1) is the discount factor. The objective of the RL agent is to maximize the expected sum of discounted rewards: \u03a3\u03c4\u03bf +R(st, at).\n\nIn online RL, the agent learns through trial and error by interacting with the environment. In contrast, the offline RL setup differs in that the agent does not interact with the environment in any way and must instead learn from a static dataset of transitions D collected by other agents. This setup introduces additional challenges, such as the need to handle out-of-distribution states and actions."}, {"title": "2.2 Deep Learning regularization techniques", "content": "Regularization techniques are essential in deep learning to prevent overfitting and improve model generalization. In this section, we provide an overview of several regularization methods that are used in our work.\n\nDropout (DO) [Hinton et al., 2012]. Dropout is a widely used regularization technique that mitigates overfitting by randomly setting a fraction of units in a neural network layer to zero during training. This prevents the network from becoming too reliant on specific neurons and forces it to learn more robust features. During inference, all neurons are used, but their output is scaled by the dropout rate to maintain consistency with the training phase.\n\nWeight Decay [Goodfellow, 2016]. Weight decay is a regularization technique that penalizes large weights in a neural network by adding a regularization term to the loss function. This term is often a combination of L\u2081 and L2 norms, which is also known as the Elastic Net regularization. The loss function with Elastic Net regularization is given by:\n\nL(0) = Loriginal(0) + \u03c9 (\u03b1||0||1 + (1 \u2212 a)||0||2)\n\nwhere Loriginal(0) is the original loss function, |0|1 is the L\u2081 norm, and |0|2 is the L2 norm, and w controls the weight of the penalty. The coefficient a \u2208 (0, 1) controls the dominance between the L1 and L2 penalties. In our experiments, we call regularization L1 when a = 1, L2 when a = 0, and Elastic Net (EN) when a = 0.5."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 General experimental design", "content": "Benchmark. For our experiments, we utilize the popular D4RL [Fu et al., 2020] benchmark as our testbed. Following the selection of tasks in Park et al. [2024], we focus on the medium datasets for Gym-MuJoCo tasks, medium- and large-diverse datasets for AntMaze tasks, and cloned datasets for Adroit. In sections subsection 4.1, subsection 4.2, and subsection 4.5, we reserve 5% of the data as a validation set to observe the differences in tracked metrics, aiming to determine whether regularizations offer benefits in offline RL and to understand their impact on the actor network's internal structure. For the results presented in subsection 4.3 and subsection 4.4, we use complete data sets for training, evaluating the entire set of Gym-MuJoCo, AntMaze, and Adroit tasks. These sections focus on assessing the extent to which regularizations can enhance performance and improve the generalization of actor networks.\n\nAlgorithms. To enable a study with sufficient evidence, we select two competitive, ensemble-free algorithms: ReBRAC [Tarasov et al., 2023], a minimalist policy-regularized approach, and IQL [Kostrikov et al., 2021], a powerful algorithm from the implicit regularization family. We use the ReBRAC + CE + MT modification of Tarasov et al. [2024a], where ReBRAC is augmented with a cross-entropy objective [Farebrother et al., 2024], leading to significant performance improvements and improved training stability on various tasks. For brevity, this modification is simply referred to as ReBRAC until specified. The choice of these two algorithms is motivated by the findings in Park et al. [2024] and Tarasov et al. [2024b], where the regular policy methods, such as ReBRAC, were identified as the most promising for offline RL, and where IQL was recognized as a strong ensemble-free alternative. When modifying ReBRAC, we implement large batch optimization [Nikulin et al., 2022] for Adroit tasks, ensuring that this aspect of ReBRAC is consistent in all tasks. It is important"}, {"title": "3.2 Regularizations leveraging", "content": "Dropout and Normalizations. In our experiments we place dropout and normalization layers after the activation function of each hidden layer within the actor network. When both regularizations are applied simultaneously, dropout is applied after the normalization, which is a conventional approach.\n\nNoise regularization For each noise-based regularization, we modify the relevant values \u2013 such as inputs, objective targets, or gradients using the following approach:\n\n\u1ef9 = y + ve\n\nwhere y represents the original value, \u0454 ~ N(0, 1) is drawn from a standard normal distribution, and v is a hyperparameter controlling the noise magnitude.\n\nFollowing Neelakantan et al. [2015], we apply a decayed version of the gradient noise. Gradient noise decays over time according to $\\frac{V_{gr}}{\\sqrt{1+t}}$, where t is the number of training iterations, and y is set to the recommended value of 0.55. We found this decayed approach to be more effective than using a constant noise level throughout training.\n\nFor objective noise, the modifications differ based on the algorithm:\n\n\u2022 ReBRAC: The objective noise modifies the actor's behavior cloning (BC) term as follows:\n\nLBC(0) = (\u03b1\u03b8 \u03b1 + \u03bd\u03bfb\u20ac) 2\n\nwhere as is the action predicted by the actor, a is the action from the dataset, and vob controls the noise level.\n\n\u2022 IQL: For IQL, objective noise is introduced into the policy extraction process:\n\nLAWR(0) = exp[\u03b2(Q(s, a) \u2013 V(s)) + \u03bd\u03bfbe] log \u03c0\u03c1(\u03b1|s)\n\nwhere (s, a) is a state-action pair from the dataset, Q(s, a) is the action-value function, V(s) is the value function, and \u1e9e is a temperature parameter. The noise term vobe is added to the advantage function."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Which regularizations independently help actors?", "content": "In our initial experiments, we investigate the impact of individually applying each regularization technique on the performance of the algorithms. Using the D4RL subset described in section 3, we report the rliable RAR metrics in Figure 1."}, {"title": "4.2 Can we combine regularizations?", "content": "In this set of experiments, we explore whether combining multiple regularization techniques can lead to better performance compared to using them individually. We present the RAR rliable metrics, including the median, interquartile mean (IQM), mean, and optimality gap scores in Figure 2, with probabilities of improvement detailed in Appendix D. We use the following notation for the modifications: Algorithm+MODs+TUNED, where MODs are modifications with already tuned hyperparameters when they were applied without TUNED and TUNED is the addition to the modifications prefix who's parameters are tuned. This experimental setup allows us to identify whether regularizations run into conflicts with each other and require additional hyperparameters tuning when combined.\n\nThe results for ReBRAC show that combining regularizations generally improves performance in most cases. Notably, the combination of dropout (with a rate of 0.1) and LayerNorm appears to be a strong default choice, as it integrates well with other techniques. However, weight decays are less compatible with other regularizations. The best combination we identified for ReBRAC was dropout + LayerNorm + gradient noise (DO+LN+GrN). Unfortunately, the results did not reveal a consistent pattern across different combinations, making it difficult to predict the outcome based solely on the individual performance of each regularization.\n\nOne clear takeaway is that hyperparameters that work well individually may not perform optimally when combined. For example, the combination of elastic net, dropout, and LayerNorm (EN+DO+LN)"}, {"title": "4.3 Per-dataset tuning", "content": "In the previous subsections, we established that incorporating deep learning regularization techniques into the actor network generally enhances algorithm performance. However, the results reported were based on the best hyperparameter settings across various tasks, which may not be optimal"}, {"title": "4.4 Actor generalization", "content": "Next, we aimed to determine whether regularizations enhance the generalization abilities of the algorithms. To assess this, we introduced Gaussian noise during inference to either the actions produced by the actor (\u03c3 = 0.2) or the observations (\u03c3 = 0.05). Adding noise to the actions simulates the impact of deploying the policy in a more stochastic environment, while noise in the states evaluates the actor's robustness to input perturbations. The results of these experiments are presented in Figure 4, with per-domain results available in Appendix F."}, {"title": "4.5 What is happening inside actor?", "content": "In this section, we explore the internal changes within the actor network when various regularization techniques are applied. We monitor several metrics by comparing them between training and validation sets, focusing on the output of the penultimate linear activation layer. These differences are of particular interest as they reveal how the actor perceives potential out-of-distribution data. The results are illustrated in Figure 5. We also track the network plasticity by optimizing Behaviour Cloning term using the validation set. Per-domain results are shown in Appendix G.\n\nFor both ReBRAC and IQL, applying normalization results in a complete elimination of dead neurons, which is generally considered a desirable property. This adjustment makes the validation data nearly indistinguishable from the training data in terms of feature norms for ReBRAC, but not for IQL. Weight decay also reduces the number of dead neurons in the validation set for both algorithms and has varied effects on other metrics altering the internal representations. Dropout, on the other hand, primarily reduces the number of dead neurons without significantly affecting other metrics."}, {"title": "5 Related Work", "content": "Offline Reinforcement Learning (RL) [Levine et al., 2020] is a rapidly advancing field with numerous approaches being introduced to address the challenges of out-of-distribution state-action pairs and policy learning from static datasets. Prominent methods such as CQL [Kumar et al., 2020b], IQL [Kostrikov et al., 2021], TD3+BC [Fujimoto and Gu, 2021], and others [An et al., 2021, Chen et al., 2021, Akimov et al., 2022, Yang et al., 2022, Ghasemipour et al., 2022, Nikulin et al., 2023] have progressively tackled these challenges with increasingly sophisticated solutions. However, as these methods have evolved, they have also become more nuanced and complex.\n\nReBRAC [Tarasov et al., 2023] demonstrated that augmenting a minimalist offline RL approach TD3+BC [Fujimoto and Gu, 2021] with straightforward design choices can yield state-of-the-art performance across a variety of tasks. Notably, most of ReBRAC's enhancements focused on the critic component of the actor-critic architecture. Inspired by this work and the findings of Park et al. [2024], which highlighted the actor network as a potential bottleneck in offline RL, our research explores the impact of applying popular DL regularization techniques specifically to actor networks, aiming to unlock their potential benefits.\n\nDL regularization techniques have been extensively studied in the context of supervised learning [Kuka\u010dka et al., 2017, Smith, 2018, Tian and Zhang, 2022], where their use has become standard practice in the development of robust models. However, their application to RL has been limited. For instance, Hiraoka et al. [2021] explored the application of dropout and LayerNorm in RL, showing performance improvements, and Smith et al. [2022] found that such modifications were critical in real-world tasks. Additionally, Bhatt et al. [2019] revealed that the application of BatchNorm in RL is nuanced, with performance gains not easily achievable through straightforward implementation.\n\nIn the context of offline RL, a few studies have shown that applying layer normalizations to the critic network can be beneficial [Kumar et al., 2022, Nikulin et al., 2022, Ball et al., 2023, Tarasov et al., 2023]. However, all mentioned works focus on critic modifications and largely restrict their exploration to dropout and LayerNorm. To our knowledge, IQL [Kostrikov et al., 2021] is the only notable method where dropout is applied to the actor network in some tasks, yet no analysis of this application has been provided.\n\nOur work builds on these insights by systematically evaluating the impact of various DL regularizations on actor networks within offline RL, filling a gap in the existing literature and contributing to the broader understanding of how these techniques can enhance RL performance."}, {"title": "6 Conclusion", "content": "This work demonstrates the clear benefits of applying DL regularization techniques to actor networks within the offline RL setup when evaluation budget allows some hyperparameters search. By systematically evaluating the impact of various regularizations, we have not only shown performance improvements across multiple tasks but also provided insights into the internal mechanisms driving these gains. Our findings help to bridge the gap between the extensive use of DL regularizations in supervised learning and their limited application in RL, particularly in the context of actor networks.\n\nWhile our study focuses on two algorithm ReBRAC and IQL \u2013 the techniques we explored are broadly applicable and not inherently tied to any specific algorithm. This suggests that the benefits observed here could extend to other RL frameworks and architectures, opening avenues for future research."}]}