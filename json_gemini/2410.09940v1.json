{"title": "GENERALIZED GROUP DATA ATTRIBUTION", "authors": ["Dan Ley", "Shichang Zhang", "Suraj Srinivas", "Gili Rusak", "Himabindu Lakkaraju"], "abstract": "Data Attribution (DA) methods quantify the influence of individual training data points on model\noutputs and have broad applications such as explainability, data selection, and noisy label identifica-\ntion. However, existing DA methods are often computationally intensive, limiting their applicability\nto large-scale machine learning models. To address this challenge, we introduce the Generalized\nGroup Data Attribution (GGDA) framework, which computationally simplifies DA by attributing to\ngroups of training points instead of individual ones. GGDA is a general framework that subsumes\nexisting attribution methods and can be applied to new DA techniques as they emerge. It allows\nusers to optimize the trade-off between efficiency and fidelity based on their needs. Our empirical\nresults demonstrate that GGDA applied to popular DA methods such as Influence Functions, TracIn,\nand TRAK results in upto 10x-50x speedups over standard DA methods while gracefully trading\noff attribution fidelity. For downstream applications such as dataset pruning and noisy label identi-\nfication, we demonstrate that GGDA significantly improves computational efficiency and maintains\neffectiveness, enabling practical applications in large-scale machine learning scenarios that were\npreviously infeasible.", "sections": [{"title": "1 Introduction", "content": "In the era of data-driven machine learning, the impact of training data on model performance has become increasingly\nevident. As such, the ability to discern and quantify the influence of individual training examples on model behavior\nhas emerged as a critical area of research. Data Attribution (DA) methods have risen to powerful tools designed to\naddress this need, which identify the influence of each training data point for specific test predictions, offering insights\ninto the complex relationships between training data and model behavior [Koh and Liang, 2017, Ghorbani and Zou,\n2019, Pruthi et al., 2020, Schioppa et al., 2022, Jia et al., 2021, Kwon and Zou, 2022, Ilyas et al., 2022, Park et al.,\n2023, Wang and Jia, 2023]. The applications of DA are far-reaching and diverse, encompassing crucial tasks such\nas model debugging, strategic data selection, and the identification of mislabeled instances. These capabilities hold\nimmense potential for enhancing model reliability.\nHowever, existing DA techniques face significant challenges that impede their practical application. Foremost among\nthese is the issue of computational efficiency. These methods incur prohibitive computational costs, for example, by\nrequiring training a large number of models in the order of the training data size, which is computationally intractable\nfor large-scale machine learning models [Ghorbani and Zou, 2019, Ilyas et al., 2022]. On the other hand, many existing\nmethods struggle with attribution fidelity, often failing to faithfully represent the true impact of training data on model\noutcomes [Basu et al., 2020a]. This lack of fidelity can lead to misinterpretations and potentially misguided model\ndevelopment and deployment decisions. These efficiency and fidelity limitations combined have severely restricted\nthe widespread adoption and utility of DA techniques in large-scale applications.\nTo address these critical issues, we propose a novel approach called Generalized Group Data Attribution (GGDA).\nThis method shifts the paradigm from attributing influence to individual data points to considering groups of data\npoints collectively. By aggregating data into meaningful groups, GGDA substantially reduces the computational\noverhead of attribution, making it particularly well-suited for large-scale applications where traditional DA methods"}, {"title": "2 Related Work", "content": "Data Attribution quantify the influence of training samples on model predictions. One type of DA is retraining-based,\nwhich systematically retrain models with varying subsets of training data to measure sample influence. Leave-One-\nOut (LOO) influence Tukey [1958] measures the prediction difference between models trained on the full dataset\nand those trained with one sample removed. Data Shapley [Ghorbani and Zou, 2019, Jia et al., 2019] extends this\nconcept by considering all possible subsets of training data, providing a more comprehensive measure of sample\nimportance. DataModels [Ilyas et al., 2022] attempts to learn model predictions for each subset of the training dataset,\nrequiring significant computational resources. While these methods can capture complex data interactions, they are\ncomputationally expensive due to the need for multiple model retrainings. Another type of gradient-based method\noffers more scalable solutions by providing closed-form attribution scores using gradients. Influence functions [Koh\nand Liang, 2017] approximate the effect of upweighting a training sample on the loss function, providing a foundation\nfor many subsequent studies. TracIn [Pruthi et al., 2020] traces loss changes on test points during the training process,\noffering insights into sample influence throughout model training. TRAK [Park et al., 2023] employs the neural\ntangent kernel with random projection to assess influence, presenting a novel approach to attribution. These gradient-\nbased methods significantly reduce computational costs compared to retraining-based approaches but may suffer from\nperformance degradation on non-convex neural networks due to their reliance on convexity assumptions and Taylor\napproximations.\nGrouping in Data Attribution. Several prior works considered attributing to groups, emphasizing different aspects\nof the problem than ours. For example, Koh et al. [2019] examines influence functions when applied to large groups\nof training points, and their empirical analysis revealed a surprisingly strong correlation between the predicted and\nactual effects of groups across various datasets. While it focuses primarily on linear classification models, we partic-\nularly focus on large-scale non-linear models and study the fidelity and computational efficiency of group attribution.\nBasu et al. [2020b] proposed a second-order influence function to identify influential groups of training samples bet-\nter. While their estimator results in improved fidelity estimates, their runtime is also strictly worse than first-order\ninfluence methods. On the other hand, our focus is on improving computational efficiency, even at slight costs to\nfidelity. To overcome the computational cost of Data Shapley, Ghorbani and Zou [2019] also briefly mentioned the\nidea of grouping data points to manage large datasets more effectively as an experimental design choice. On the other\nhand, our work studies group attribution more systematically and shows that (1) it is possible to generalize both re-\ntraining-based and gradient-based attribution methods to the group setting and (2) it demonstrates experimentally the\nsuperiority of group approaches.\nData Attribution Acceleration. Various approaches have been proposed to accelerate data attribution methods, in-\ncluding subsampling [Guo et al., 2021] and better gradient/Hessian approximations [Schioppa et al., 2022]. These\ntechniques focus on optimizing existing attribution algorithms to improve computational efficiency. While valuable,\nthese methods are orthogonal to our GGDA framework, which fundamentally alters the attribution paradigm by con-\nsidering groups of data points. As such, GGDA can potentially be combined with these acceleration techniques to\nachieve even greater efficiency gains, highlighting the complementary nature of our approach."}, {"title": "3 Generalized Group Data Attribution Framework", "content": "This section introduces the formal definitions of Data Attribution (DA) and our proposed Generalized Group Data\nAttribution (GGDA) frameworks. We examine the limitations of the conventional DA methods, which attributes\nimportance to individual training points, and show that they often result in prohibitive computational costs. We then\npresent the rationale behind the development of GGDA as a solution to these challenges. We begin by establishing the\nnotations and definitions.\nNotation. Given a dataset of n training data points $D = \\{(x_0, y_0), (x_1, y_1), \\ldots(x_n, y_n)\\}$ in the supervised learning\nsetting2, and a learning algorithm $A(D) : 2D \\rightarrow \\mathbb{R}^P$ that outputs weights $\\theta \\in \\mathbb{R}^P$, where $f(x_{\\text{test}}; \\theta) \\in \\mathbb{R}$ is the model\nprediction function for a given test input $x_{\\text{test}}$, we define a counterfactual dataset $D_S = D \\setminus \\{(x_i, y_i; i \\in S)\\}$ as the\ntraining dataset with points in S removed. We use $\\theta_S \\sim A(D_S)$ to denote the model weights trained on $D_S$.\nDefinition 1 (Data Attribution) Given a learning algorithm A, dataset D and test point $x_{\\text{test}}$, data attribution for the\nith training point $(x_i, y_i)$ is a scalar estimate $T_i(A, D, x_{\\text{test}}) \\in \\mathbb{R}$ representing the influence of $(x_i, y_i)$ on $x_{\\text{test}}$.\nTo ground our discussion with a concrete example, we first introduce the conceptually most straightforward DA\nmethod, namely the Leave-One-Out (LOO) estimator. This computes the influence of individual training points, by\nthe average prediction difference between the model trained on the original dataset, and the one trained on the dataset\nwith a single training point excluded.\nDefinition 2 (Leave-One-Out estimator) The LOO estimator of a learning algorithm A, dataset D and test point $x_{\\text{test}}$,\nw.r.t. to training point $(x_i, y_i)$ is given by $T_i(A, D, x_{\\text{test}}) = \\mathbb{E}_{\\theta \\sim A(D)} f(x_{\\text{test}}; \\theta) - \\mathbb{E}_{\\theta_i \\sim A(D \\setminus \\{(x_i, y_i)\\})} f(x_{\\text{test}}; \\theta_i)$\nThe LOO estimator forms the conceptual basis for methods in the DA literature. For example, the influence function\n[Koh and Liang, 2017] based methods are gradient-based approximations of the LOO estimator. On the other hand,\nmethods like datamodels [Ilyas et al., 2022] and TRAK [Park et al., 2023] aim to generalize beyond LOO estimators\nby estimating importance of training points by capturing the average effect of excluding larger subsets that include that\ntraining point. However, the underlying paradigm of measuring the importance of individual training points $(x_i, y_i)$,\nfor a given test point $x_{\\text{test}}$, remains common among DA methods. However, as we shall now discuss, this paradigm of\ndata attribution has several critical drawbacks.\nDrawback: Attribution Computation Scales with Data Size. A critical challenge in data attribution is its substantial\ncomputational cost. From an abstract perspective, the definition of DA necessitates computing attributions for each\ntraining point individually, resulting in computational requirements that scale with the size of the training dataset n.\nThis scaling becomes prohibitive in modern learning settings with large n. The computational burden can be even\nmore severe in practice. For instance, the LOO estimator demands training at least n + 1 models, with this number\nincreasing significantly in non-convex settings due to Monte Carlo estimation for the expectation in Definition 2 [Jia\net al., 2019]. Gradient-based methods, such as influence functions, aim to reduce attribution computation by avoiding\nretraining. However, these methods still require (1) computation of per-training-sample gradients w.r.t. parameters and\n(2) computation of the Hessian inverse of models parameters. Both these steps are computationally intensive, as (1)\nrequires n forward and backward passes, which is prohibitive when n is in the billions, and (2) requires manipulating\nthe Hessian matrix whose dimensions are p \u00d7 p for a model $\\theta \\in \\mathbb{R}^P$, which is again intractable when p is in the billions.\nDrawback: Pointwise Attribution may be Ill-Defined. Another issue with DA is that estimates of individual training\npoint importance may not always be meaningful. For example, recall that the LOO estimator involves repeatedly train-\ning models on nearly identical training sets, with merely a single point missing. However, for training data involving\nbillions of data points, a single missing example out of billions is unlikely to lead to any statistically significant change\nin model parameters or outputs. This intuition is captured in the machine learning literature via properties such as (1)\nstability of learning algorithms [Elisseeff et al., 2005], or (2) differential privacy [Dwork, 2008]. These capture the\nintuition that the models output by learning algorithms must not change when individual points are removed, and these\nhave been linked to (1) model generalization for the case of stability and (2) privacy guarantees on the resulting model.\nThus, when we employ learning algorithms that satisfy these conditions, the LOO estimator, which is the conceptual\nbasis for many DA methods, may be ill-defined.\nDrawback: Focus on Individual Predictions Limits Scope. A primary motivation for DA is to explain individual\nmodel predictions via their training data. However, there exist many applications beyond explainability of individual\npredictions that require reasoning about the relationship between model behaviour and training data. For instance, we\nmay wish to identify training points that cause models to be inaccurate, non-robust, or unfair, all of which are \"bulk\"\nmodel properties as opposed to individual predictions. One way to adapt DA to analyze such bulk model properties"}, {"title": "4 Generalizing Existing Data Attribution Methods to Groups", "content": "In this section, we show how to generalize existing DA algorithms to the GGDA setting, retaining the computational\nbenefits of attribution in groups. In particular, we focus on deriving GGDA variants of gradient-based influence\nmethods, given their inherent computational superiority over re-training-based methods. To facilitate the derivation of\nthe group setting, we unify two gradient-based DA methods - TRAK and TracIn as instances of influence function\nmethods, even as this differs with the original motivation presented in these papers. To derive these group variants,\nthere are two principles we follow:\n\u2022 the GGDA variants must scale with k, the number of groups, and not n, the number of datapoints;\n\u2022 the GGDA variants must subsume the ordinary DA variants when groups are defined as singleton datapoints, and\nthe property function as the loss on a single test point.\nWith these principles in mind, we start our analysis with influence functions."}, {"title": "Generalized Group Influence Functions.", "content": "Given a loss function $l \\in \\mathbb{R}^+$, the Influence Function data attribution\nmethod [Koh and Liang, 2017] is given by:\n$\\tau_{\\text{inf}}(A, D, x_{\\text{test}}) = \\nabla_{\\theta}l(x_{\\text{test}}; \\theta)^T H_{\\theta}^{-1} [\\nabla_{\\theta}l(x_0; \\theta); \\nabla_{\\theta}l(x_1; \\theta); ... \\nabla_{\\theta}l(x_n; \\theta)] \\qquad \\theta \\sim A(D)$\nThe primary computational bottlenecks for computing influence functions involve (1) computing the Hessian inverse,\nand (2) computing the n gradients terms, which involve independent n forward and backward passes. While several\ninfluence approximations [Guo et al., 2021, Schioppa et al., 2022] focus on alleviating the bottleneck due to computing\nthe Hessian inverse, here we focus on the latter bottleneck of requiring order n compute. While efficient computation\nof per-sample gradients has received attention from the machine learning community Bradbury et al. [2018] via tools\nsuch as vmap, the fundamental dependence on n remains, leading to large runtimes. In the Appendix A.1, we show that\ndue to the linearity of influence functions, the influence of groups is given by the sum of influences of individual points\nwithin the group, which is consistent with the analysis in Koh et al. [2019]. Overall, we show that the corresponding\nGGDA generalization is:\n$\\tau_{\\text{inf}}(A, D, Z, g) = \\nabla_{\\theta}g(\\theta)^T H_{\\theta}^{-1} [\\nabla_{\\theta}l(z_0; \\theta); \\nabla_{\\theta}l(z_1; \\theta); ...\\nabla_{\\theta}l(z_k; \\theta)] \\qquad \\theta \\sim A(D) \\qquad \\qquad (1)$\nwhere $l(z_i) := \\sum_{x \\in z_i} l(x)$\nFrom equation 1, we observe that the corresponding group influence terms involve k batched gradient computations\ninstead of n per-sample gradient computations, where all \u223c n/k points belonging to each group are part of the\nsame batch. Experimentally, we observe that computing a single batched gradient is roughly equivalent in runtime to\ncomputing individual per-sample gradients. This combined with the fact that typically k << n, leads to considerable\noverall runtime benefits for GGDA influence functions.\nHowever, we also note that there exists a lower limit on the number of groups k where this analysis holds. Practically,\nhardware constraints result in maximum allowable value for batch size, which we denote by b. If b > n/k, then points\nin each group can be fit into a single batch, leading to the computational advantage mentioned above. Thus a practical\nguide to set the number of groups is using $k \\geq n/b$ to obtain the maximum computational benefit. Beyond this value,\nusing GGDA influence provides a constant factor speedup of b, that still nevertheless results in significant real-world\nspeedups. We use this key insight regarding speeding up group influence functions for two more methods: TracIn and\nTRAK, by viewing them as special cases of influence methods."}, {"title": "TracIn as a Simplified Influence Approximation.", "content": "Pruthi et al. [2020] propose an influence definition based on\naggregating the influence of model checkpoints in gradient descent. Formally, tracein can be described as follows:\n$\\tau_{\\text{identity-inf}}(\\theta, D, x_{\\text{test}}) = \\nabla_{\\theta}l(x_{\\text{test}}; \\theta)^T [\\nabla_{\\theta}l(x_0; \\theta); \\nabla_{\\theta}l(x_1; \\theta); ...\\nabla_{\\theta}l(x_n; \\theta)] \\qquad \\qquad (2)$\n$\\tau_{\\text{tracein}}(A, D, x_{\\text{test}}) = \\sum_{i=1}^T \\tau_{\\text{identity-inf}}(\\theta_z, D, x_{\\text{test}}) \\text{ where } {\\theta_z | i \\in [1, T]} \\text{ are checkpoints }  \\qquad (3)$\nThe core DA method used by tracein is the standard influence functions, with the Hessian being set to identity, which\ndrastically reduces computational cost. The GGDA extension of tracein thus amounts to replacing the identity-inf\ncomponent (equation 4) with equation 1, with the Hessian set to identity."}, {"title": "TRAK as Ensembled Fisher Influence Functions", "content": "One of the popular techniques for Hessian approximation in\nmachine learning is via the Fisher Information Matrix, and the empirical Fisher matrix. Some background material on\nthese is presented in the Appendix A.3. The DA literature has also adopted these approximations, with Barshan et al.\n[2020] using influence functions with the Fisher information matrix, TRAK [Park et al., 2023] using the empirical\nFisher estimate. While the TRAK paper itself views their estimate as a (related) Gauss-Newton approximation of the\nHessian, we show in the Appendix A.4 that under the setting of the paper, this is also equivalent to using an empirical\nFisher approximation. Atop the empirical Fisher approximation, TRAK also further reduces the computational cost\nby using random projections of gradients to compute the empirical Fisher. Given this, we denote TRAK as follows:"}, {"title": "5 Experiments", "content": "This section presents highlights from a comprehensive evaluation of our proposed GGDA framework. We conduct\nexperiments on multiple datasets using various models to demonstrate the effectiveness and efficiency of our approach\ncompared to existing data attribution methods. Full configuration details for datasets, models, attribution methods,\ngrouping methods, and evaluations are listed in Appendix B, while the remainder of results across our grid of experi-\nments is in Appendix C."}, {"title": "5.1 Datasets and Models", "content": "We evaluate across four diverse datasets encompassing tabular (HELOC), image (MNIST and CIFAR-10), and text\ndata (QNLI). These datasets represent a range of task complexities, data types, and domains, allowing us to assess\nthe generalizability of our approach across different scenarios. The models we fit on these datasets range from simple\nlogistic regression (LR) to medium-sized artificial neural networks (ANNs, e.g., MLPs and ResNet) and pre-trained\nlanguage models (BERT)."}, {"title": "5.2 Grouping Methods", "content": "We employ several grouping methods to evaluate the effectiveness of our GGDA framework. For each grouping\nmethod, we also experiment with various group sizes to analyze the trade-off between computational efficiency and\nattribution effectiveness. The choice of grouping method and group size can significantly impact the performance of\nGGDA, as we subsequently demonstrate.\nRandom We uniformly randomly assign data points to groups of a specified size. This serves as our baseline method\nfrom which we may assess the impact of more sophisticated methods.\nK-Means We use the standard K-means algorithm to cluster data points based on their raw features.\nRepresentation K-Means (Repr-K-Means) We first obtain hidden representations of the data points from a model\ntrained on the full training set. We then apply K-means clustering on these representations. This approach groups data\npoints that are similar in the model's learned feature space, potentially capturing higher-level semantic similarities.\nGradient K-Means (Grad-K-Means) We compute the gradient of the loss with respect to the activations (NOT\nparameters) of the pre-classification layer of the model for each data point, and then apply K-means on the gradients.\nThis approach groups instances with similar effects on the model's learning process."}, {"title": "5.3 Evaluation Metrics", "content": "To illustrate the effectiveness of GGDA at identifying important datapoints, especially in comparison to DA, it is\ncritical to have metrics that allow us to compare both on equal terms. To this end, we utilize the following metrics:\nRetraining Score (RS) This metric quantifies the impact of removing a proportion of training data points on model\nperformance. The process involves first identifying the most influential data points or groups using GGDA. A given\npercentile of the high-importance data points are then removed from the training set, and the model is retrained on\nthe remaining data. We then compare the performance of this retrained model to the original model's performance on\nthe test set. A significant decrease in performance (i.e., an increase in loss or decrease in accuracy) after removing\ninfluential points indicates that they were indeed important for the model's learning process. This metric provides a\ntangible measure of the effectiveness of GGDA in identifying highly useful training instances.\nNoisy Label Detection (AUC) This metric quantifies the effectiveness with which GGDA methods can identify\nmislabelled training data points. This involves randomly flipping a proportion of the training set labels, and training a\nmodel on the corrupted dataset. We then examine the labels of the training data points that belong to groups with the\nlowest scores, and plot the change of the fraction of detected mislabeled data with the fraction of the checked training\ndata, following Ghorbani and Zou [2019], Jia et al. [2021]. The final metric is computed as the area under the curve\n(AUC)."}, {"title": "5.4 Results", "content": "In the following subsections, we present detailed evaluation results from across our range of datasets and models,\ndiscussing the performance of our GGDA framework in terms of retraining score, computational efficiency, and ef-\nfectiveness in downstream tasks such as dataset pruning and noisy label detection. Our pipeline first trains a model\non the full training set, before determining groups as described in Section 5.2. For each grouping method, we ablate\nextensively over a range of group sizes, beginning at 1 (standard DA) and increasing to 1024, in ascending powers of\n2."}, {"title": "5.4.1 Retraining Score", "content": "To evaluate the retraining score metric, we compute Test Accuracy upon removal of 1%, 5%, 10%, and 20% of\nthe most important points in the train set. For groups, this translates to sequentially removing the most important groups\nup until the desired number of data points (points are randomly selected from the final group in the case of overlap).\nLower test accuracies are thus more favorable, indicating that the removed points were indeed important.\nWhich Grouping Method to Use? Before presenting our results, we first investigate the effect that the particular\ngroup arrangement has on GGDA methods, to select an appropriate grouping method. We plot a subset of group sizes\nthat decreases from left to right as runtimes simultaneously increase, up unto the point where our GGDA framework\nsubsumes traditional individual DA at group size 1. Among the grouping methods, we find that across the board, Grad-"}, {"title": "5.4.2 Dataset Pruning", "content": "To evaluate the effectiveness of GGDA in dataset pruning, we compute test accuracy after removing varying per-\ncentages (25%, 50%, 75%) of the least important points from the training set. We follow a similar sequential data\nremoval as the retraining score evaluation. Higher test accuracies after pruning are more favorable, indicating that the\nremoved points were indeed not influential for model performance. Our results demonstrate that GGDA significantly\noutperforms individual DA methods in this task, achieving higher test accuracies while requiring substantially less\ncompute. Tables 1 and 2 present example results for a range of attribution methods compared between on the ANN\ndataset, illustrating the superior performance and efficiency of GGDA compared to individual DA methods in dataset\npruning."}, {"title": "5.4.3 Noisy Label Identification", "content": "Noisy label detection is a critical task in machine learning, addressing the common issue of inaccurate labels in real-\nworld datasets. These inaccuracies can arise from various sources, including automated labeling processes, non-expert\nannotations, or intentional corruption by adversarial actors. Identifying and mitigating noisy labels is essential for\nmaintaining model performance and reliability. In this context, we evaluate the effectiveness of GGDA in detecting\nnoisy labels across different datasets and model architectures. Our analysis focuses on the trade-off between detection\naccuracy, measured by Area Under the Curve (AUC), and computational efficiency. We present our findings in Table 3,\ndemonstrating superior performance for GGDA, particularly on image datasets.\nOur analysis reveals several key findings regarding the performance of different DA methods in noisy label detection.\nFor text classification using BERT models, the Fisher approximation variant of influence functions shows improved\nperformance when combined with grouping, while also reducing runtime by an order of magnitude. TracIn demon-\nstrates superior performance on the CIFAR-10 dataset, particularly when grouping datapoints with similar penultimate-\nlayer gradients. However, we observe that TRAK performs poorly within the given computational budget, which can\npotentially be due to the number of ensembles being small (10 ensembles in our case). We choose not to further in-\ncrease the ensemble as retraining make TRAK much slower compared to ours and all the baselines. Increasing it makes\nthese DA methods not really comparable in terms of run time. Also, methods like Inf LiSSA show less pronounced\ngroup-wise speedups due to other computational bottlenecks. Nevertheless, applying GGDA to all of these DA meth-\nods improves their efficiency. These results highlight the effectiveness of GGDA in enhancing both performance and\nefficiency across various datasets and model architectures."}, {"title": "6 Conclusion", "content": "In this work, we have proposed a new framework, generalized group data attribution, that offers computational advan-\ntages over data attribution while maintaining similar fidelity. While our work has focussed more on influence-based\napproaches that are more non-trivial to extend to the group setting, any data attribution approach can, in principle,\nbe extended to the group setting to obtain these computational benefits. The key insight here is that many down-\nstream applications of attribution involve manipulating training datasets at scale, such as dataset pruning, or noisy\nlabel identification, and a fine-grained approach of individual point influence may be unnecessary in such cases.\nThere are several exciting avenues for future work. First, it is desirable to conduct a formal analysis of the fidelity\ntradeoffs in the group setting for various methods of interest, analytically quantifying the fidelity loss due to grouping.\nSecond, while our experiments have focused on small-to-medium-scale settings to systematically demonstrate the\nadvantages of group attribution, evaluation on a genuinely large-scale, billion-data-point learning setting can help\nuncover any remaining computational bottlenecks limiting the practical adoption of data attribution."}, {"title": "A Derivations", "content": ""}, {"title": "A.1 Generalized Group Influence functions", "content": "In this section, we present a derivation for group influence functions, mirroring the influence function derivation\nadopted by Koh and Liang [2017]. The argument involves two steps. First, we analyse the change in model parameters\nupon upweighting the loss of a group of training points. Second, we compute how the change in model parameters\naffects the downstream property function. To begin, we consider what happens if we weight a group of training\nsamples, Z. The modified loss and minimizer are:"}, {"title": "A.2 TracIn as a Simplified Influence Approximation", "content": "Pruthi et al. [2020] propose an influence definition based on aggregating the influence of model checkpoints in gradient\ndescent. Formally, tracein can be described as follows:"}, {"title": "A.3 Background on Fisher information matrix", "content": "One of the popular techniques for Hessian approximation in machine learning is via the Fisher Information Matrix.\nWhen the quantity whose Hessian is considered is a log-likelihood term, it turns out that the Hessian has a simple\nform:"}, {"title": "A.4 TRAK as Ensembled Fisher Influence", "content": "TRAK Park et al. [2023] is an influence method derived for binary classification setting. Specifically, given a logistic\nclassifier l(0) = log(1 + exp(-ygt f (x))), TRAK without projection + ensembling (Equation 13 in the TRAK paper)\nis equivalent to the following (note distinction between l vs f):"}, {"title": "B Experimental Setup", "content": ""}, {"title": "B.1 Datasets and Models", "content": "Here we provide further details regarding the specifics of the datasets and models used."}, {"title": "B.1.1 Dataset Details", "content": "HELOC (Home Equity Line of Credit) A financial dataset used for credit risk assessment. It contains 9,871 instances\nwith 23 features, representing real-world credit applications. We use 80% (7,896 instances) for training and 20%\n(1,975 instances) for testing. The task is a binary classification problem to predict whether an applicant will default\non their credit line. For this dataset, we employ three models: LR, and two ANNs with ReLU activation functions.\nANN-S has two hidden layers both with size 50, while ANN-M has two hidden layers with sizes 200 and 50.\nMNIST A widely-used image dataset of handwritten digits for image classification tasks [LeCun et al., 1998]. We\nuse the standard train/test split of 60,000/10,000 images. For MNIST, we utilize an ANN with ReLU activation and\nthree hidden layers, each with 150 units.\nCIFAR-10 A more complex image classification dataset consisting of 32x32 color images in 10 classes [Krizhevsky\nand Hinton, 2009]. We use the standard train/test split of 50,000/10,000 images. For CIFAR-10, we employ ResNet-18\n[He et al., 2016].\nQNLI (Question-answering Natural Language Inference) A text classification dataset derived from the Stanford\nQuestion Answering Dataset [Wang and Manning, 2018]. It contains 108,436 question-sentence pairs, with the task\nof determining whether the sentence contains the question's answer. Following TRAK [Park et al., 2023], we sample\n50,000 instances for train and test on the standard validation set of 5,463 instances. For QNLI, we utilize a pre-trained\nBERT [Devlin, 2018] model with an added classification head, obtained from Hugging Face's Transformers library\n[Wolf, 2019]."}, {"title": "B.1.2 Model Hyperparameters", "content": "Across all datasets and models, we use a (mean-reduction) cross entropy loss function with Adam optimizer on Py-\nTorch's default OneCycleLR scheduler (besides for QNLI/BERT, where a linear scheduler is used). Weight decay is\nutilized in the case of training on tabular data to prevent overfitting."}, {"title": "B.2 Grouping Hyperparameters", "content": "We whiten all inputs to KMeans clustering (raw inputs, intermediate representations, and penultimate-layer gradients).\nBatch sizes for distance computations are computed programmatically based on available GPU memory. We use a\nconvergence tolerance of 0.001 on the center shift of the KMeans algorithm, with the maximum number of iterations\nset to 60."}, {"title": "B.3 Baseline Attributors", "content": "We implement TracIn using the final model checkpoint. Random projection dimensions used in Inf. Fisher and TRAK\nare 16, 32, 64, and 32 for HELOC, MNIST, CIFAR-10 and QNLI respectively, owing to GPU constraints in the case\nof QNLI. TRAK follows Park et al. [2023] with a subsampling fraction of 0.5. Inf. LiSSA contains the largest number\nof parameters to choose from, and we use recommended values of damp = 0.001, repeat = 20, depth = 200, scale = 50."}, {"title": "C Additional Results", "content": "This appendix details remaining results from our extensive experiments across all datasets, models, attribution meth-\nods, grouping methods, and group sizes."}, {"title": "C.1 Retraining Scores", "content": "Ablations over grouping methods are displayed for all HELOC models in Figure 3, CIFAR-10 / ResNet1"}]}