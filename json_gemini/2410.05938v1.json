{"title": "EMMA: EMPOWERING MULTI-MODAL MAMBA WITH STRUCTURAL AND HIERARCHICAL ALIGNMENT", "authors": ["Yifei Xing", "Xiangyuan Lan", "Ruiping Wang", "Dongmei Jiang", "Wenjun Huang", "Qingfang Zheng", "Yaowei Wang"], "abstract": "Mamba-based architectures have shown to be a promising new direction for deep learning models owing to their competitive performance and sub-quadratic deployment speed. However, current Mamba multi-modal large language models (MLLM) are insufficient in extracting visual features, leading to imbalanced cross-modal alignment between visual and textural latents, negatively impacting performance on multi-modal tasks. In this work, we propose Empowering Multi-modal Mamba with Structural and Hierarchical Alignment (EMMA), which enables the MLLM to extract fine-grained visual information. Specifically, we propose a pixel-wise alignment module to autoregressively optimize the learning and processing of spatial image-level features along with textual tokens, enabling structural alignment at the image level. In addition, to prevent the degradation of visual information during the cross-model alignment process, we propose a multi-scale feature fusion (MFF) module to combine multi-scale visual features from intermediate layers, enabling hierarchical alignment at the feature level. Extensive experiments are conducted across a variety of multi-modal benchmarks. Our model shows lower latency than other Mamba-based MLLMs and is nearly four times faster than transformer-based MLLMs of similar scale during inference. Due to better cross-modal alignment, our model exhibits lower degrees of hallucination and enhanced sensitivity to visual details, which manifests in superior performance across diverse multi-modal benchmarks. Code will be provided.", "sections": [{"title": "INTRODUCTION", "content": "Recently there has been a notable increase in the development of domain-general AI agents Kalla et al. (2023); Zhao et al. (2023a) which can simultaneously solve a diverse range of tasks and exhibit superior performance. Among them, multi-modal large language models (MLLMs) Achiam et al. (2023); Team et al. (2023); Liu et al. (2024a) have emerged as a promising direction due to their effectiveness in visual perception and logical reasoning. MLLMs usually consist of an image encoder that converts images to visual tokens and a strong large language model (LLM) backbone to process the visual and textual tokens concurrently. This integration of visual and textual information not only enhances the understanding of visual content but also provides a more comprehensive context for language understanding and generation. As a result, these cross-modal models have consistently achieved state-of-the-art performances in tasks such as image captioning Hossain et al. (2019), visual reasoning Johnson et al. (2017), and visual question answering Antol et al. (2015).\nHowever, a gravid challenge for current MLLMs is the substantial computational cost associated with training and deployment Achiam et al. (2023); Han et al. (2022). In fact, current MLLMs are predominately transformer-based, and consequently suffer from an input-dependent attention mechanism that is quadratic in complexity Katharopoulos et al. (2020). Transformers also struggle with"}, {"title": "RELATED WORKS", "content": ""}, {"title": "MAMBA-BASED MODELS", "content": "Mamba Dao & Gu (2024); Gu & Dao (2023) is a form of state-space model that originally stemmed from classical signal processing theory Kalman (1960). These sequential models inherently excel at capturing long-range dependencies and are designed to be computationally efficient Gu et al. (2020; 2021b;a); Gupta et al. (2022). Expanding upon these works, Mamba Gu & Dao (2023) introduces a more effective selection mechanism that is input-dependent and a hardware-aware framework that is computationally efficient. Since then, the Mamba architecture has been successfully applied towards a variety of sequential tasks including NLP Yuan et al. (2024); Behrouz et al. (2024), speech Zhang et al. (2024a); Chen et al. (2024b), and motion Wang et al. (2024); Zhang et al. (2024b), rivaling the performance of transformer-based models while being more computationally efficient Qu et al. (2024). Mamba has also been adapted to non-sequential data such as vision and point clouds Xu et al. (2024); Yang et al. (2024); Xing et al. (2024); Han et al. (2024), which differs by not adhering to any particular ordering Huang & Schneider (2011). These tasks are more challenging due to the lack of suitable and appropriate structural constraints in Mamba layers to preserve order-invariant information, such as position encoding in their transformer counterparts Vaswani (2017). Visual State Space model Liu et al. (2024b) introduces a novel 2D Selective Scan (SS2D) module that gathers contextual visual information from various perspectives. Vision Mamba Zhu et al. (2024a) applies Mamba to the Vision Transformer architecture and proposes a bi-directional SSM to better process visual features. However, despite gaining success in small-scale scenarios with less than 100M parameters, these vision Mamba models often falter when scaled-up in more complex scenarios. Ren et al. (2024) enhances Mamba's visual capability through autoregressive pretraining, which results in higher classification accuracy over its supervised-trained counterparts and successfully scales vi-"}, {"title": "MULTI-MODAL LARGE LANGUAGE MODELS (MLLMS)", "content": "MLLMs take advantage of a powerful large language model (LLM) Chang et al. (2024); Achiam et al. (2023) to simultaneously process data from both visual and textual modalities. Cho et al. (2021); Wang et al. (2022) initially proposed a unified transformer architecture for joint visual and language reasoning tasks that demonstrated better generalizability than conventional vision models. Since then, research on MLLMs have established strong baselines for general-AI assistants Liu et al. (2024a); Achiam et al. (2023); Alayrac et al. (2022), which typically consists of a vision encoder, a cross-modal projector, and a LLM backbone. While most current MLLMs utilize a text-only loss function for optimization Li et al. (2024b); Zhu et al. (2024b); Lin et al. (2024), EMU Sun et al. (2023; 2024) takes interleaved visual and textual inputs in a unified training object of predicting next text or image token in an autoregressive way. However, they require multiple training stages and additional computational cost of training a Stable Diffusion Rombach et al. (2022) visual decoder.\nA major drawback of current MLLMs resides in the inherent computational overhead Katharopoulos et al. (2020), predominantly attributed to their foundation in transformer architectures. Ma et al. (2024a); Ning et al. (2024) proposes architectural changes to enable more efficient processing of data in MLLMs. Transformer-based MLLMs also struggle with learning long-range dependencies Zhu et al. (2021); Gu & Dao (2023), which is crucial for vision-language tasks. Chen et al. (2024a) repurposes the image embedding projector to encode long textual context, improving the MLLM's ability to handle longer sequences. Nevertheless, current progress in improving the computational complexity and learning long-range dependencies mostly depends on architectural changes in the transformer backbone. In this work, we explore how to effectively integrate the Mamba model into the transformer-dominated MLLM domain. Mamba offers a three to five times higher throughput than its transformer counterparts, while capable of processing million-length sequential data."}, {"title": "MAMBA-BASED MLLMS", "content": "Mamba has been extended to MLLMs due to their capabilities in both vision and NLP. Current multi-modal Mamba models Qiao et al. (2024); Zhao et al. (2024) follow a simple LLaVA-based regimen Liu et al. (2024a) through a pretrained image encoder, Mamba LLM backbone, and a llama-like textual loss. These models utilize efficient down-sampling techniques Chu et al. (2024); Zhao et al. (2024) and multi-direction scanning mechanisms Liu et al. (2024b); Qiao et al. (2024) as cross-modal projection modules to align visual and textual information. However, they lack explicit visual supervision during training. Given that Mamba models are inherently less effective in processing visual information in large-scale settings Ren et al. (2024), this results in poor-quality visual feature that weakens cross-modal alignment in Mamba MLLMs that easily surpass billions of parameters. Our work seeks ways to address the current bottleneck in mamba-based MLLMs where the mamba LLM extracts insufficient visual feature details that results in disparity in the alignment between images and text features. To this end, we impose additional structural constraints on visual features generated by the Mamba LLM, effectively enhancing the overall efficacy of multi-modal representations and cross-modal alignment."}, {"title": "METHOD", "content": "In this section, we present the overall framework for EMMA. EMMA aims to empower the insufficient extraction of visual information in Mamba MLLMs through structural and hierarchical alignment. We first provide the model architecture in Sec. 3.1. Then, we describe our pixel-level visual alignment for preserving structural visual features in Sec. 3.2, and multi-scale fusion module that hierarchically constrains intermediate features for retaining fine-grained visual cues in Sec. 3.3."}, {"title": "ARCHITECTURE", "content": "Inspired by Zhao et al. (2024); Qiao et al. (2024); Liu et al. (2024a), the architecture of EMMA consists of a pre-trained vision encoder $f_v$, a projection MLP $M_{proj}$, a pre-trained Mamba LLM $f_{\\phi}$ parameterized by $\\phi$, and a multi-scale fusion module $\\psi$, as shown in Fig. 2. Taking an image $X_v$ and its corresponding tokenized caption $X_t$ as input, we first obtain visual features through the visual encoder:\n$X_v = f_v(X_v)$ (1)\nThe MLP then maps visual tokens into the word embedding space, which is concatenated with the tokenized captions to form a multi-modal input. The combined tokens are fed into the Mamba LLM:\n$X_{LLM} = concat(X_v, X_t), X = f_{\\phi}(X_{LLM}, \\psi)$ (2)\nWhere the multimodal response $X$ can be separated into a textual response $X_t$ and a visual feature $X_v$. Note that the visual feature is generated through the multi-scale fusion module which combines intermediate LLM features to alleviate the gradual loss of fine-grained visual features. Lastly, a pixel-wise alignment loss is applied to the visual feature $X_v$ for preserving structural visual cues, and an autoregressive NLP loss is applied to the textural feature $X_t$. More implementation details can be found in Appendix."}, {"title": "STRUCTURAL ALIGNMENT VIA PIXEL-WISE ALIGNMENT LOSS", "content": "We first consider the standard Mamba MLLM training procedure, given multimodal input $X_{LLM}$ and Mamba LLM $f_{\\phi}$:\n$f_{\\phi}(X_{LLM}) = [X, X_t] = X$ (3)\nThe target text token sequence $X_t = \\{x_{t,i}\\}$ of length $L$ is generated by computing the following probability and optimized by minimizing the corresponding negative log-likelihood function $L_{text}$:\n$p(X_t | X_v, X_t) = \\prod_{i=1}^{L} P_{\\phi} (x_{t,i} | X_v, \\{x_{t,j} | j < i\\}), L_{text} = -logp (X_t | X_v, X_t)$ (4)\nWhere $x_{t,i}$ depends on the visual features $X_v$ and the set of previous text tokens $\\{x_{t,j} | j < i\\}$. We observe the lack of supervision on image-level features, as they serve solely as conditional prior for predicting text tokens $X_t$. As Mamba models inherently struggle with visual tasks in larger models Ren et al. (2024), this may result in poor quality image features during cross-modal alignment, negatively impacting model performance. It becomes crucial to construct additional constraints on the image-level features for better cross-modal alignment. Our insight is to condition the generation"}, {"title": "HIERARCHICAL ALIGNMENT VIA MULTI-SCALE FUSION MODULE", "content": "While the pixel-wise alignment loss forces the Mamba LLM to retain structural characteristics of the visual input, we find that Mamba LLMs have a tendency to gradually lose fine-grained visual details through intermediate layers, as shown in Fig. 1. This could be due to the inherent lack of 'positional embedding'-analogues within the Mamba LLM backbone, such that spatial and fine-grained information is more easily distorted inside the LLM. Consequently, we contend that the visual feature derived from the final layer of Mamba LLM alone proves inadequate in preserving fine visual intricacies. To enable the Mamba LLM to extract more sufficient visual details and prevent gradual feature loss from the image modality, we devise a Multi-scale Feature Fusion (MFF) module that hierarchically integrates multiple intermediate features of the pretrained visual encoder for the visual alignment. Combining multi-level features that concern different levels of granularity enables the model to effectively capture intricate details at various scales, enhancing its ability to comprehend and interpret complex visual information with greater precision. Specifically, our multi-scale feature fusion module consists of $I$ fusion blocks $\\psi = \\{B\\}_{i=1}^{I}$, where each $B$ can be regarded as a tiny Mamba network consisting of a cross-attention module and a Mamba layer, which combines intermediate features in a pairwise fashion. For instance, given hidden visual features $\\{X_i, X_j, X_k\\}$ from layers $i, j, k$ and the corresponding two-layer MFF $\\psi = \\{B_\\{1,2\\}\\}$, the final aggregated output of the features $X_v$ is generated as follows:\n$X_v = \\psi(X_i, X_j, X_k) = B_2(B_1(X_i, X_j), X_k)$ (7)\nWhere each block B is given by residually-connected Mamba and cross attention layers:\n$B(X,Y) = \\tilde{B}(X,Y) + Mamba(\\tilde{B}(X,Y)); \\tilde{B}(X,Y) = X + cross\\_attn(X,Y)$ (8)\nIn practice, we utilize three intermediate layers along with the final output layer for producing the aggregated visual feature for pixel-wise alignment. Thus, the final pixel-wise alignment becomes:\n$L_{pixel} = ||f_{dec}(X) - X_v ||_2$ (9)\nBy introducing additional contribution of intermediate features in the pixel-wise alignment loss, we force the model to retain structural and fine-grained information in these layers and alleviate the gradual loss of visual features. This results in better-quality visual features, as evident in Fig. 1. Additionally, we note that the feature fusion and visual decoding stage only occurs during training where loss calculations are needed, and poses no additional computational overhead in inference."}, {"title": "EXPERIMENTS", "content": "In this section, we conduct extensive experiments from multiple perspectives to demonstrate the effectiveness of our method. First, we provide experiment settings including training data, training recipes, and evaluation benchmarks in Sec. 4.1. Next, we provide experimental results from the aforementioned benchmarks and compare with contemporary MLLMs in Sec. 4.2. EMMA surpasses other Mamba-based models on a majority of tasks and is highly competitive with transformer models of similar size. Then, we conduct a detailed inference speed comparison between EMMA and other Mamba and transformer-based models of similar size in Sec. 4.3. At last, we present ablation studies to investigate the effectiveness of our model design choices in Sec. 4.4."}, {"title": "EXPERIMENT SETTINGS", "content": "Training Data. Following Zhao et al. (2024), we train EMMA on a combination of datasets consisting of LLaVA-v1.5-mixed-665k Liu et al. (2024a), LVIS-Instruct-4V Wang et al. (2023), and LRV-Instruct Liu et al. (2023b). These are conversational multi-modal and pure textual data that contains roughly 1.2 million images and dialogues.\nBackbone Models. We select cobra as our baseline model. Following Zhao et al. (2024), we utilize SigLIP and DINOv2 as the visual encoder, where the final output is the concatenated feature of both models. The image decoder consists of a combination of 4 Mamba and linear layers. The MLP projector consists of stacked linear layers. We provide two versions of our model with MambaV1-2.8b Gu & Dao (2023) and MambaV2-2.7b Dao & Gu (2024) backbones.\nTraining Recipes. We directly finetune the Mamba LLM backbone, the multi-scale fusion module, the image decoder, and the MLP projector on the training data for two epochs, discarding the pretrain phase. The visual encoder is frozen at all times. We select a global batch size of 128 and a starting learning rate of 2e-5 with AdamW optimization. Our models are trained on eight 40G A100 GPUs with fully sharded data parallelism Zhao et al. (2023b).\nEvaluation Benchmarks. We evaluate our model variants on four open-ended visual question-answer benchmarks: VQAv2 Goyal et al. (2017) and VizWiz Gurari et al. (2018) test general visual reasoning, GQA Hudson & Manning (2019) validates spatial reasoning, and TextVQA Singh et al. (2019) assesses reasoning around text. We also evaluate our models on nine comprehensive closed-set benchmarks: VSR Liu et al. (2023a) tests coarse object spatial relationships. POPE Li et al. (2023b) tests object hallucinations while HallusionBench Guan et al. (2024) assesses object illusion and visual hallucinations. MMB Liu et al. (2023c) and MME Fu et al. (2024) are both robust and holistic evaluations of MLLMs, SEED-IMG Li et al. (2024a) evaluates generative comprehension. AI2d Hiippala et al. (2021) and ScienceQA Saikh et al. (2022) both evaluate science-related topics. CCBench is a benchmark on Chinese culture Liu et al. (2023c). We use reproduced results from the Cobra codebase Zhao et al. (2024) and obtain the performance of other models in their respective papers. More details for experimental settings can be found in the Appendix."}, {"title": "EVALUATION RESULTS", "content": "We compare our model with a variety of Mamba-based and transformer-based MLLMs. These include large-scaled MLLMs: OpenFlamingo Awadalla et al. (2023), BLIP-2 Li et al. (2023a), MiniGPT-4 Zhu et al. (2023), EMU Sun et al. (2023), EMU 2 Sun et al. (2024), InstructBLIP Wenliang et al. (2023), Shikra Chen et al. (2023a), IDEFICS Hua & Artzi (2024), Qwen-VL Bai et al. (2023), LLaVA Liu et al. (2024a), Prism Karamcheti et al. (2024), ShareGPT4V Chen et al. (2023b), MoE-LLaVA Lin et al. (2024). We also include transformers with similar scales, which encompass LLaVA-Phi Zhu et al. (2024b), MobileVLM Chu et al. (2023), MobileVLM V2 Chu et al. (2024), and TinyLLaVA Zhou et al. (2024). Lastly, we also compare with current Mamba-based MLLMs, consisting of VL Mamba Qiao et al. (2024) and Cobra Zhao et al. (2024). We also provide the backbone LLM of each model, as well as the training data size for more fair comparison.\nComparison with similar-scaled Mamba MLLMs. Performance comparisons between EMMA and other Mamba-based MLLMs can be found in the bottom group of Tab. 1 as well as 2. We underline the best-performing models in this category. All models utilize a similar amount of data and Mamba backbone. We surpass the performance of cobra, our baseline model, on every evaluated"}, {"title": "Analysis for Hallucination", "content": "We analyze the degree of hallucination in our models and report corresponding performances on the POPE and HallusionBench benchmark in Tab. 3. POPE concerns object hallucination which refers to generated contents inconsistent with ground-truth objects in the input, which reflect hallucination leaning towards the language modality. HallusionBench focuses on diagnosing both the visual illusion and knowledge hallucination of MLLMs. Visual illusion refers to the misinterpretation of accurate visual information, and knowledge hallucination denotes perceptions formed without relevant visual input. Thus, HallusionBench reveals hallucinations more on the effective extraction of information in the visual modality. Our model achieves the highest score on POPE, demonstrating the superior NLP capabilities of the Mamba LLM backbone. We also achieve significant gain (51.0 vs 41.4) in HallusionBench from our baseline, demonstrating the effectiveness of higher-quality visual features in reducing visual hallucinations. Our model is also competitive in HallusionBench with LLaVA-1.5 and BLIP2-T5, a 7B and 12.4B model respectively."}, {"title": "MODEL LATENCY", "content": "We evaluate model latency between EMMA and other similar-sized MLLMs, which include cobra, TinyLLaVA, and MobileVLM V2 in Tab. 4. While VL Mamba is not open-sourced yet, we assume the latency of VL Mamba to mirror that of Cobra, as both models leverage the MambaV1-2.8b backbone architecture. All models are given the same sample image and the exact textural prompt of \"Describe this image in detail\". Each model is then forced to generate 256 tokens in response to this prompt for a total of 200 times, and the overall time taken is recorded as $T_{overall}$. Finally, we calculate the average time taken for each model to generate 256 tokens ($T_{avg}$) as well as the number of tokens generated per second ($N_{avg}$) as follows:\n$T_{avg} = T_{overall}/200; N_{avg} = 256/T_{avg}$ (10)\nAll evaluations are conducted on a single 40G A100 GPU. EMMA has a significant time advantage over transformer-based MLLMs and a non-trivial edge over other Mamba-based MLLMs. Mamba models, given their better theoretical guarantees, are three to five times faster than current state-of-the-art transformer-based models of similar sizes. Our model achieves even better runtime than cobra due to more efficient processing in the MambaV2 LLM backbone."}, {"title": "ABLATION STUDIES", "content": "In this section, we present ablation studies for the design choices of our model in Tab. 5."}, {"title": "Component-wise Ablation", "content": "We first explore the impact of the two proposed components on the overall performance of our model. By removing the multi-scale feature fusion module, we observe a noticeable reduction in MME, as well as slight degradation in VQAv2, GQA, POPE, and HallusionBench. This shows that the multi-scale feature fusion module plays a crucial role in enhancing performance across multiple tasks, by preventing visual information loss in the LLM, thereby enhancing the overall quality of visual features. We then remove the pixel-wise alignment loss, which is equivalent to training the plain cobra model. We observe a substantial decline in TextVQA and HallusionBench, along with marginal decreases in GQA, VQA-V2, and MMB. Notably, TextVQA involves open-set VQA, relying heavily on intricate visual details, whereas HallusionBench addresses visual hallucinations. This outcome underscores the impact of our pixel-wise alignment loss in improving the quality of visual representations, leading to enhanced processing of fine visual details and a reduction in visual hallucinations."}, {"title": "Preserving Structural Information in MambaLLM", "content": "We note that many works Qiao et al. (2024); Liu et al. (2024b) utilize a visual selective scan mechanism to incorporate structural information and enhance the quality of visual features. Consequently, we opt to integrate this mechanism on top of our method (before the projection MLP) to assess its impact on the quality of visual representations and model performance (+CSM). The results demonstrate a marginal change in performance, suggesting that our structural constraint framework might already suffice in generating high-quality structural visual features. The introduction of the cross-scan module may appear to be unnecessary."}, {"title": "Pixel vs. Feature Alignment", "content": "Given that the Mamba LLM receives visual features from the vision encoder, we also explore directly aligning these processed features instead of the raw image pixels (+AVF). However, the resulting performance experiences a notable decline across various metrics. We attribute this degradation to the robust structural information inherent in pixel-level images, contrasting with the extracted features from the vision encoder that may lack such details. However, despite the severe performance degradation, the model achieves a high hallusionBench score which implies the efficacy of supervising visual features in mitigating visual hallucinations."}, {"title": "CONCLUSION", "content": "In this study, we introduce EMMA, a novel approach designed to rectify the imbalanced quality between visual and textual latents within the MLLM framework, which adversely impacts cross-modal alignment and leads to suboptimal performance. By expanding the autoregressive generation of text tokens to operate on image-wise patches, we leverage this process as visual supervision for the visual features within the Mamba LLM. Subsequently, we introduce a pixel-wise alignment loss to align the generated visual features with the original image, thereby preserving crucial structural information. On the other hand, the absence of structural constraints in the Mamba LLM leads to a gradual"}]}