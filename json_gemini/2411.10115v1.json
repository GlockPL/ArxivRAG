{"title": "Memorization in Attention-only Transformers", "authors": ["L\u00e9o Dana", "Muni Sreenivas Pydi", "Yann Chevaleyre"], "abstract": "Recent research has explored the memorization capacity of multi-head attention, but these findings are constrained by unrealistic limitations on the context size. We present a novel proof for language-based Transformers that extends the current hypothesis to any context size. Our approach improves upon the state-of-the-art by achieving more effective exact memorization with an attention layer, while also introducing the concept of approximate memorization of distributions. Through experimental validation, we demonstrate that our proposed bounds more accurately reflect the true memorization capacity of language models, and provide a precise comparison with prior work.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern large language models, especially Transformers, showcase great memorization capacity (Karpukhin et al. 2020; Roberts et al. 2020). Among recent works, researchers have shown that facts are memorized in the MLPs of a Transformer, and have even identified which MLPs (Meng, Bau, et al. 2022; Meng, Sharma, et al. 2023; Nanda et al. 2023). However, they were not able to understand how these MLPs store information. Both exact and approximate theoretical memorization in an MLP are well documented in the literature: a ReLU MLP can memorize exactly as many real-valued label as it has neurons n (Bubeck et al. 2020), and can memorize exactly n discrete labels with only \\(\\tilde{O}(\\sqrt{n})\\) neurons (Vardi et al. 2022).\nContrary to the MLPs, the memorization power of multi-head attention layers has not been empirically studied. The main role of the attention layer is not viewed as remembering information but rather as moving between residual streams the information retrieved by the MLPs (Nanda et al. 2023; Wang et al. 2023; Variengien et al. 2024). For the theoretical aspect of the memorization in attention layers, there exists results on the expressivity of the attention patterns (Bhojanapalli et al. 2020), the memorization capacity of attention layers (Mahdavi et al. 2024), and the memorization capacity of Transformers (Kim et al. 2023). We will discuss related works in depth in section 6.\nIn this article, we are interested in moving the state-of-the-art in terms of memorization capacity for the attention layer. We will thus consider the memorization capacity of an Attention-only Transformer (AoT). We need to specify what memorization capacity means. We will distinguish two types of memorization tasks, namely the association task and the distribution task. The association task, already studied in Bietti et al. 2023; Cabannes et al. 2024; Kim et al. 2023; Mahdavi et al. 2024, consists of predicting a token given a sequence of tokens as input. We only require the AoT to predict the right next-token at the last position. This memorization is exact and hence, we want to know the maximal set of sequence-token associations \\((t_{in}, t_{out})\\) that can be exactly memorized by an AoT.\nThe distribution task consists of predicting the correct distribution, measured using the KL-divergence, for an input sequence of tokens. We use the KL-divergence since it is the default loss function used to train most Transformers. To our knowledge, we are the first to introduce and study this task. Memorizing distribution happens in natural language modeling: take the sentence \"Arnold Schwarzenegger was a\", it can be completed with \"actor\", \"writer\" or \"bodybuilder\". Thus, language models need to memorize not one but several correct next-tokens, each with a possibly different probability depending on the importance of the answer.\nOur contributions are:\n1.  We improve the state-of-the-art on the association task by proving that a one layer AoT with H heads each of dimension \\(d_h\\), and an embedding dimension d can memorize \\(Hd_h + d\\) associations. In the context of language model, this improves on the previous result on the attention layer expressivity by Mahdavi et al. 2024 which requires a limited context-windows and has memorization capacity of \\(H(d_h - 1) + 1\\). We compare our result"}, {"title": "2 FORMALISM", "content": "We study an Attention-only Transformer (AoT) that has only one layer of multi-head attention (MHA) mechanism denoted by A. Let \\([N] = \\{1, ..., N\\}\\) be the token dictionary. Each token is embedded in dimension d by the embedding \\(e: [N] \\rightarrow \\mathbb{R}^d\\), and a positional embedding is added based on its position s. Thus, we denote a sequence of S tokens by \\(t_{1:s}\\). We also denote \\(t_{s+1}\\) for the output token. The MHA contains H heads, each of inner dimension \\(d_h\\), with \\(d_h \\leq d\\), meaning that \\(W_h^Q, W_h^K, W_h^V \\in \\mathbb{R}^{d,d_h}\\) and \\(W_h^O \\in \\mathbb{R}^{d_h,d}\\). Following the intuition from Elhage et al. 2021, we choose to separate output matrices in each head, as well as combine \\(W_h^K = (W_h^Q)^T W_h^V\\). Each attention head can be written as follows.\n\\begin{equation}\nA^h(t_{1:s}) = W_h^O W_h^V a^h(t_{1:s})\n= W_h^O W_h^V \\sum_{s=1}^S a_s^h(t_{1:s}) (e(t_s) + \\text{pos}_s)\na_s^h(t_{1:s}) = \\text{Softmax}(\\text{raw}_s(t_{1:s}), s = 1 : S)\n\\text{raw}_s(t_{1:s}) = (e(t_s) + \\text{pos}_s)^T W_h^Q W_h^K (e(t_s) + \\text{pos}_s)\n\\end{equation}\nWe concatenate the output matrices into \\(W_O \\in \\mathbb{R}^{d,Hd_h}\\) and the attention before output \\(A(t_{1:s}) \\in \\mathbb{R}^{Hd_h}\\). We also construct the matrix \\(W_V \\in \\mathbb{R}^{Hd_h,Hd}\\) as block diagonal with block \\(W_V^h\\), which has full rank when each \\(W_V^h\\) has full rank. Then, the output of the attention layer is added to the residual stream and goes through an unembedding matrix \\(W_U \\in \\mathbb{R}^{N,d}\\) to obtain logits\nfor the next token. Since we are only interested in the next-token prediction at the last position, we denote the AoT's computation by\n\\begin{equation}\nT(t_{1:s}) = W_U (e(t_s) + \\text{pos}_s + W_O W_V A(t_{1:s}))\n\\end{equation}\nFor both the association and distribution tasks, we define the conditional distribution \\(\\pi_{t_{1:s}}\\) over next token \\(t_{s+1}\\) and a prior distribution \\(\\pi\\) over token sequences \\(t_{1:s}\\). The task of the Transformer is to minimize the KL-divergence with the conditional distribution for each input sequence, averaged over the prior distribution.\n\\begin{equation}\nd_{KL}(\\pi, T) := E_{t_{1:s} \\sim \\pi}[d_{KL}(\\pi_{t_{1:s}} || \\text{Softmax} \\circ T(t_{1:s}))]\n\\end{equation}\nThe association case is a restriction of the distribution case to conditional distributions with 0 entropy, which is equivalent to having one next-token of probability 1. We denote this setting as assumption 1 below, and we will use it when referring to the association task.\nAssumption 1. For all sequence token \\(t_{1:s}\\), there exists \\(t_{s+1}\\) such that \\(\\pi(t_{s+1}|t_{1:s}) = 1\\). This is equivalent to \\(\\pi\\) having conditional distributions with O entropy.\nIn the association case, we say that the Transformer memorizes an example \\((t_{1:s}, t_{s+1})\\) if \\(T(t_{1:s})_{t_{s+1}}\\) is the maximum logit, and we let \\(T_O\\) the number of sequence-token association to memorize, which is at most \\(N^S\\). In the distribution case, the Transformer memorizes example \\(t_{1:s}\\) if \\(T(t_{1:s}) = \\log(\\pi_{t_{1:s}})\\). We introduce another assumption that arises in the distribution case.\nAssumption 2. For all \\(t_{1:s},t_{s+1}\\), \\(\\pi(t_{s+1}|t_{1:s}) \\neq 0\\). This is equivalent to \\(\\pi\\) having conditional distributions with full support."}, {"title": "3 THE MEMORIZATION LIMIT OF TRANSFORMERS", "content": "Looking at equation (2), we see that the AoT can be written as \\(T(t_{1:s}) = W_U E(t_{1:s})\\) where \\(E(t_{1:s}) = e(t_s) + \\text{pos}_s + W_O W_V A(t_{1:s})\\) is a sequence embedding. Our AoT belongs to the set of sequence encoders defined below.\nDefinition 1. The set of maps that embed token sequence and unembed them into logits is denoted\n\\[\\mathcal{L}(N, S, d) := \\{f_{W,E} | W \\in \\mathbb{R}^{d,N}, E : [N]^S \\rightarrow \\mathbb{R}^d \\}\\]\nwith \\(f_{W,E}(t_{1:s}) = WE(t_{1:s})\\). We call them sequence encoders and we define\n\\[d_{KL}(\\pi, \\mathcal{L}(N, S, d)) := \\inf_{f \\in \\mathcal{L}(N,S,d)} d_{KL}(\\pi, f).\\]"}, {"title": "4 \u039c\u0395\u039cORIZATION CAPACITY OF AOT", "content": "In this section, we present our main results that respectively give upper bounds on memorization in the distribution and association settings. We will start with the result on remembering distributions, making the association task a corollary.\nDefine \\(T_{\\epsilon}\\) as the smallest number of token sequences whose cumulative probability is greater than \\(1 - \\epsilon\\). We have \\(T_{\\epsilon} \\leq \\lceil(1 - \\epsilon)N^S\\rceil\\), the upper bound being attained when the probability distribution over token sequences is uniform. This notation is consistent with \\(T_O\\) defined earlier, the number of non-zero probability sentences. The theorem below states that we can construct a Transformer which approximates the lower bound set in Proposition 1 arbitrarily.\nTheorem 1. Let \\(\\epsilon > 0\\) and \\(\\gamma > 0\\). Under Assumption 2 there exists \\(f_{W,E}\\) and an AoT T with embedding dimension d, head dimension \\(d_h\\), and H attention heads, satisfying \\(d_h H + d > T_{\\epsilon}\\), such that\n\\begin{equation}\nd_{KL}(\\pi, T) \\leq d_{KL}(\\pi, \\mathcal{L}(N, S, d)) + C_{\\epsilon}||W_E||_2^{2+\\gamma}\n\\end{equation}\nT has \\(d(S + 2N + 4d_h H)\\) parameters.\nRemarks 1.\n*   In the parameter count, dS and dN are necessary for the word embedding, positional embedding and unembedding. The \\(dd_hpH > dT - d^2\\) scaling comes from the attention heads and is the comparison point with previous work as we will detail in section 6.\n*   In practice, sequence-token pairs don't have the same probability. Thus, our AoT remembers all most likely sequences up to \\(\\epsilon\\), which explains that we are close to the lower bound for small epsilon. The term \\(||W_E||_2\\) corresponds to the worst possible prediction, and is standard in the literature on MLP expressivity2. The term in \\(\\gamma\\) is simply a term that can be taken infinitely close to 0, and accounts for approximating the skip connection with a special attention head.\n*   The constant \\(C \\geq 1\\) depends on the matrix \\(W_V A\\) and is finite thanks to its high rank. In order to bound C effectively, one has to understand more precisely the singular value decomposition of \\(W_V A\\). See equation (14) of the appendix.\n*   One can take \\(\\epsilon = 0\\) to achieve the lower bound set in Proposition 1 with \\(Hd_h + d = T_O\\) attention heads, and uses \\(d(S + 2N + 4(T_O - d))\\) parameters. In that case, one could say that the Transformer remembers exactly the distributions, even if the divergence is not 0, since it is the smallest loss attainable by the Transformer architecture.\nWe can also use our Theorem 1 under Assumption 1: we use it on the probability \\(\\pi_{\\delta} = \\frac{\\pi + N \\delta}{1 + N \\delta}\\) with \\(\\delta > 0\\) small enough to satisfy Assumption 2, such that the Transformer has perfect accuracy for \\(\\pi\\). In this association task, the AoT thus remembers \\(T_O\\) associations.\nCorollary 1. Under Assumption 1, there exist an AoT T with embedding dimension 2, H attention heads"}, {"title": "4.1 Sketch of the Proof", "content": "We present below the proof of our main result. First, we prove the case \\(\\epsilon = 0\\) as it is useful for the \\(\\epsilon > 0\\) case. Assumption 2 is used in both cases for Lemma 1 below.\nLemma 1. Under Assumption 2, there exists \\(f \\in \\mathcal{L}(N, S, d)\\) such that \\(d_{KL}(\\pi, f) = d_{KL}(\\pi, \\mathcal{L}(N, S, d))\\)\n1.  Case \\(\\epsilon = 0\\): we have our AoT as in equation (2), and thanks to Lemma 1 we take \\(f_{W,E}\\) an optimal sequence encoder that we approximate. By choosing \\(W_U = W\\), we want to solve the system \\(E(t_{1:s}) = e(t_s) + \\text{pos}_s + W_O W_V A(t_{1:s})\\) for all token sequence \\(t_{1:s}\\).\nWe start by showing that the skip connection \\(e(t_s) + \\text{pos}_s\\) can almost be written as an attention head, by a change of basis\n\\[e(t_s) + \\text{pos}_s = W_O W_O^V (e'(t_s) + \\text{pos}'_s)\\]\nand using the attention pattern \\(W_O^Q W_O^K = Id\\). When A is large enough, the attention head with matrices \\(W_O^Q, W_h^K, W_h^O\\) becomes arbitrarily close to \\(e(t_s) + \\text{pos}_s\\). Thus, we can augment A', the attention before output, by adding head 0, which has an inner dimension of d. Since the result is only valid in the limit of \\(A \\rightarrow +\\infty\\), this explains the constant \\(\\gamma > 0\\) in the theorem.\nNow, \\(W_V A' \\in \\mathbb{R}^{Hd_h+d,T_O}\\), and since \\(T_O \\leq Hd_h + d\\), the linear system \\(E(t_{1:s}) = W_O W_V A' (t_{1:s})\\) is solvable, where the variable is \\(W_O\\), if the family \\(\\{W_V A'(t_{1:s})\\}_t\\) has rank \\(T_O\\), which is equivalent to having \\(\\{A'(t_{1:s})\\}_t\\) with rank \\(T_O\\) since \\(W_V\\) reduces the rank. We are left with proving Lemma 2.\nLemma 2. Let A the attention before output of a model with H heads. For \\(T_O\\) token sequences, there exists matrices \\(W_O^Q\\) and embeddings e and pos such that the family \\(\\{A(t_{1:s})\\}_t\\) has rank greater than \\(\\min(T_O, Hd)\\).\nFirst, one can see that it is sufficient to prove the Lemma when \\(T_O > Hd\\), since otherwise one can add dummy examples to have \\(T_O = Hd\\). We compute the rank in terms of the rows, head by head, meaning that Lemma 2 reformulates as proving that the function \\(W_O^Q \\rightarrow A^h\\) has its image not contained in any hyperplane of \\(\\mathbb{R}^{d,T_O}\\).\nThis is now a technical result: recall that an attention head output is a weighted average of the tokens by the"}, {"title": "5 UPPER BOUND FOR SEQUENCE ENCODERS", "content": "In Theorem 1, we are able to upper bound the divergence of the best AoT by the divergence of the best sequence encoder. In this section, we want to upper bound the sequence encoder divergence to have better control on the performance of the best AoT.\nThis task is hard in general and is highly dependent on the fact that we are optimizing the KL divergence. For example, when a distribution is close to uniform, it will be easily approximable by a sequence encoder f. Especially, the centered logit difference\n\\begin{align*}\nZ_{t_{s+1}} := f(t_{1:s})_{t_{s+1}} - \\log(\\pi(t_{s+1}|t_{1:s}))\n\\mathbb{E}_{t_{s+1}}[f(t_{1:s})_{t_{s+1}} - \\log(\\pi(t_{s+1}|t_{1:s}))]\n\\end{align*}\nwill be close to 0. Doing a Taylor on the KL divergence reveals that the first order term will be the L2 norm of \\(Z_{t+1}\\). In comparison, if we optimize for the total variation loss, one would find a different first order approximation.\nIn a different setting than the previous remark, we are able to control the divergence when the target probability satisfies Assumption 1. We can think of \\(\\pi\\) as implementing a look-up table with some noise, and we define \\(g: [N]^S \\rightarrow [N]\\) the true look-up table meaning that \\(\\pi(g(t_{1:s})|t_{1:s}) \\sim 1\\). Although Theorem 2 holds for any distribution and function g, the upper bound is to be read with the above relation between \\(\\pi\\) and g in mind."}, {"title": "B Experiments", "content": "We give here details on the experiments in section 4.2. The training procedure for every model was the following: generate a distribution over 3 tokens, uniform over the first 2, and then one next-token was randomly chosen. This way, \\(\\pi\\) satisfies Assumption 1. From \\(\\pi\\), we generated 1000 batches of \\(2^{10}\\) elements each, and trained the model for 10 epochs. We used Adam with default parameters and a learning rate of \\(10^{-3}\\). Each model was trained twice on different seeds, and the accuracy was averaged. This repo contains the code to reproduce the experiments using the notebook experiments.ipynb. All experiments were done on a single MacBook Air with an M2 chip and 16 Go of memory.\nAs explained earlier, we present here another scaling laws whose intend is to be a fair comparison with our Corollary 2. Exceptionally, we used N = 10 to avoid training issues. We train an AoT with d = 2, \\(d_h\\) = 5 and H from 1 to 20. Corollary 2 states that with H = 20, the model should be able to obtain exactly 1 accuracy.\nNow, how far is Corollary 2 from the empirical scaling? Figure 4 shows that the true scaling at d = 2 seems to be around \\(1.7Hd_h\\)."}]}