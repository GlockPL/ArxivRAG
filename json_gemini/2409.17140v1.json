{"title": "Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents", "authors": ["JUNTING LU", "ZHIYANG ZHANG", "FANGKAI YANG", "JUE ZHANG", "LU WANG", "CHAO DU", "QINGWEI LIN", "SARAVAN RAJMOHAN", "DONGMEI ZHANG", "QI ZHANG"], "abstract": "Multimodal large language models (MLLMs) have enabled LLM-based agents to directly interact with application user interfaces (UIs), enhancing agents' performance in complex tasks. However, these agents often suffer from high latency and low reliability due to the extensive sequential UI interactions. To address this issue, we propose AXIS, a novel LLM-based agents framework prioritize actions through application programming interfaces (APIs) over UI actions. This framework also facilitates the creation and expansion of APIs through automated exploration of applications. Our experiments on Office Word demonstrate that AXIS reduces task completion time by 65%-70% and cognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans. Our work contributes to a new human-agent-computer interaction (HACI) framework and a fresh UI design principle for application providers in the era of LLMs. It also explores the possibility of turning every applications into agents, paving the way towards an agent-centric operating system (Agent OS).", "sections": [{"title": "1 Introduction", "content": "As personal computer, mobile devices, and internet become an indispensable part of every day's work and life, application industries are under great pressure to rapidly evolve their software applications with more features and functionalities to meet peoples' growing demand [1, 38]. Nonetheless, those new applications also demand a much higher investment in both time and cognitive effort from regular users. To learn how to use a new application effectively, Users generally have to firstly spend significant time just to get familiar with the user interface (UI) and corresponding functionalities. And to complete various tasks with the new application efficiently, users need to further invest time and effort to learn how to break complex tasks into steps and use the right UIs and commands to complete each step. While both the application providers and research community are fully aware of this pain point, existing efforts have been focusing on providing detailed tutorials and establishing engaging learning platforms, which could only provide limited support in alleviating users' cognitive burden [6, 10, 35, 42].\nLarge language models (LLMs) [2, 13, 34] has demonstrated near-human capabilities in reasoning, planning, and collaboration and are highly promising in completing complex tasks [22, 27, 46]. Since then, researchers has been exploring how LLMs can be utilized to reduce users' cognitive burden in learning and operating software applications. In particular, multimodal large language models (MLLMs) [14, 53, 56] expand the usage scenarios of LLMs to various tasks that require vision capability [47, 60]. Recent works [43, 54, 55, 60] utilizes MLLMs to design LLM-based UI agents capable of serving as users' delegates, translating user requests expressed in natural language, and directly interacting with the UI of software applications to fulfill users' needs. With the help of LLM-based UI agents, users could simply ask the application to complete tasks without a deep understanding of application's UIs and functionalities, which significantly reduces users' cognitive load of learning new applications.\nHowever, just like the transition from steam-powered to electric-powered industry took much more than replacing central steam engines with electric motors in the factories, simply building LLM-based agent upon the UIs of applications cannot magically deliver a satisfied and worry-free user experience. In particular, today's application UIs are designed for human-computer interaction (HCI) [7, 25], which often involves multiple UI interactions for completing a single task. For instance, inserting a 2\u00d72 table in an Office Word document requires a sequence of UI interactions: \"Insert \u2192 Table \u2192 2\u00d72 Table\". Although the HCI-based design suits the habits of humans, training LLM-based UI agents to emulate such interactions would generate quite a few challenges that are difficult to overcome."}, {"title": "2 Related Work", "content": "2.1 LLM-based UI Agent\nLLM-based agents are designed to utilize the advanced context understanding and reasoning skills of LLMs to interact with and manipulate environments with human-like cognitive abilities [16, 26, 44, 49]. The advent of MLLM [14, 53, 56], including GPT-40 [32] and Gemini [41], further broadens the scope of LLMs in practical applications with the capability of processing multi-modal inputs, including text and images. Supported by the vision understanding capability of MLLM, new LLM-based UI agents can acquire crucial abilities of navigating and controlling UIs in software applications for completing complex tasks. As such, study on LLM-based UI agents has emerged as a hot area for developing intelligent assistants that can automatically interact with applications following users' commands. In the mobile platform, methods such as MM-Navigator [52], AppAgent [55], and MobileAgent [43] leverage GPT-4V [33] to operate smartphone applications through human-like interactions (tapping and swiping) without requiring back-end access. More broadly, UFO [54], SeeAct [60] and Cradle [40] support the navigation and operation of UIs in Windows OS applications, websites and games respectively following commands in natural language. Other notable examples include CogAgent [20] and SeeClick [9], which focus on the training and finetuning of visual language models for UI understanding and navigation in downstream mobile and desktop tasks. While these LLM-based UI agents are trained to complete tasks in a human-like manner, UIs in existing applications were originally designed for HCIs rather than for agent-computer interactions. Consequently, emulating UI-based interactions directly can result in unnecessary time costs, especially in complex tasks that require numerous or repeated UI interaction steps, such as changing multiple titles to the same format. In contrast, application providers usually offer APIs that can accomplish such tasks with a single API call that eliminate the need of multi-step UI interactions. To overcome the limitation of existing UI agents, reduce unnecessary UI interactions, and lower humans' learning curve for applications, we will study how the application APIs can be leveraged for building LLM-based agents and explore the new design principles of UIs in the era of LLMs.\n2.2 Agent Operating System\nThe LLM-based agents discussed in Section 2.1 are usually designed to work within a narrow environment such as a specific application or web page which limits their applicability in general computer tasks that often involves cross-application collaboration. For example, a simple task \"Create a report with docs under the Presentation folder and send it to Jack.\" requires multiple steps with multiple applications to complete: read all docs, summarize key contexts, compose a report with a word processing application like Microsoft Word, and draft an email with attached report using an email client like Outlook, and sent it to the recipient Jack. To support the completion of complex tasks with minimal human interventions, emerging works have explored the possibility of developing an operating system (OS) fully supported by LLMs. Works like AIOS [28] and OS-Copilot [48] propose an OS-level agent that can effectively interact with the OS and a vast number of third-party applications in completing complex tasks. OSWorld [51] and AndroidWorld [36] also provide benchmarks for evaluating the performance of multimodal agents with diverse tasks and cross-application workflows across various OS. In the industry, commercial Agent OS such as Apple Intelligence [3], Copilot+PC [29], HarmonyOS [23], and MagicOS [21], are evolving to be more accessible and productive for customers with the potential of leading a new era of HCI. A common approach adopted by existing Agent OS is to divide complex into sub-tasks and assign them to individual applications. However, for each sub-task, the LLM-based agents still rely on human-like interactions such as UI clicking and swiping for completion, which can be inefficient compared to API calls."}, {"title": "3 Design of AXIS", "content": "We develope AXIS as a framework that can automatically explore within existing applications, learn insights from exploration trajectories, and consolidate available insights and learned knowledge into actionable \u201cskills\u201d. Skill is a high-level representation of UI- and API-based actions with a priority in API actions\u00b2, which is generated with AXIS exploration within the application. Illustrated by Figure 2, AXIS system consists of three major modules: the app environment, the skills, and the workflows. AXIS employs numerous LLM-based agents to explore the app environment, usually a set of applications running on the OS, through a unified interface to obtain the state of the environment and to interact with it. The knowledge learnt during this process will be consolidated into skills containing structured code segments capable of accomplishing various tasks within the environment. Specific execution and validation methods will also be designed to improve the performance of those skills. Finally, we establish two workflows: the explorer workflow and the follower workflow to facilitate the learning of skills from the environment.\nIn the subsequent sections, we will layout the details of all the 3 modules and discuss how they work together to explore and discover valuable skills from the applications.\n3.1 Environment of Application\nIn the context of AXIS, the environment of applications refers to the collection of interactive entities within the exploration scope of the agents. In this paper, those entities mainly consist of a set of applications running on the Windows operating system. Applications in the environment often share common elements, such as controls [54] and XML elements obtained after unpacking.\nAgents in AXIS not only observe the state of the environment, but also actively interact with the environment. To facilitate the observation and interaction between agents and the environment, we have designed two general interfaces: state() and step(). state() interface returns the state of the environment, which includes detailed information on the current elements of the entities within the environment. The environment state encompasses key UI information including the position of controls, the type of controls, and whether a control is selected. For applications that can\n\u00b2If the skill can be represented with UI or API actions, the skill is represented in API-only actions."}, {"title": "3.2 Skills in Application", "content": "A skill contains skill code, description, and usage example, and is designed to accomplish one specified task within the environment.\n3.2.1 skill structure.\n\u2022 Skill Code: a piece of code structured to be compatible with the executor described in the following section. Skill code includes a uniform set of parameters and adheres to the standard PEP 257 documentation. The initial set of skills is generated by the restructure of the fundamental APIs from the application provider. Based on these initial skills, AXIS can explore and develop additional new skills. It is worth noting that while AXIS prioritizes API-based skills, it is still designed for general purpose and can incoporate both UI-based and API-based skill code.\n\u2022 Description: A description of the functionality of a skill for assisting the LLM in selecting and invoking the appropriate skil in the process of task execution."}, {"title": "3.2.2 skill executor", "content": "As discussed in Section 3.1, our application environment incorporates a step() interface to facilitate the interaction between agents and the environment. This interface also hosts the skill executor responsible for executing the skill generated or selected by the agents. The skill executor keeps caching of application documents and simultaneously supports multiple functionalities including locating application controls, invoking methods on those controls, and calling application APIs (independent of controls), to enable the UI actions and API actions in the same time and serve as an efficient foundation for skill-driven operations."}, {"title": "3.2.3 skill types", "content": "Following a versatile design principle, the skills in AXIS can be categorized into four types based on the composition of their code fragments: Atomic UI Skill, Atomic API Skill, Composite UI Skill, Composite API Skill, and API-UI Hybrid Skill."}, {"title": "3.2.4 Skill Hierarchy", "content": "We define \"skill hierarchy\" as the number of skill components that make up a skill. A single basic skill thus has a skill hierarchy of 1. The skill hierarchy of insert_header_footer skill mentioned in table 1 is 2."}, {"title": "3.3 Workflows of Skill Exploration", "content": "As shown in Figure 2, the skill exploration in AXIS is guided by two driving mechanisms: follower-driven skill exploration and explorer-driven skill exploration.\n3.3.1 Follower-driven skill exploration. Follower-driven skill exploration refers to the process of exploring skills from the applications' help documents, which is primarily accomplished through the collaboration of the following agents:\n\u2022 FollowerAgent. FollowerAgent utilizes a skill library composed of a set of primitive actions based on the step-to-step instructions provided by the help document. At each step, it selects the most appropriate action according to the current state of the environment.\n\u2022 MonitorAgent. MonitorAgent monitors every action taken by the FollowerAgent along with the impacts on the environment. It also tracks the entire trajectory of the FollowerAgent. When deemed appropriate, it places a breakpoint and summarizes the observed trajectory into a complete skill including the functional summary of the skill and its logic. For initial skills composed of basic UI actions, the summarized logic is also often closely tied to the UI.\n\u2022 SkillGeneratorAgent. Based on the summaries and logical descriptions provided by the MonitorAgent, Skill-GeneratorAgent generates the code, functional descriptions, and use cases for the skill in accordance with the specifications outlined in Section 3.2. The generated skill code is highly correlated with the logical descriptions generated by the MonitorAgent, and therefore often involves the stacking of basic UI actions.\n\u2022 APITranslatorAgent. Based on the skill code generated by the SkillGeneratorAgent, APITranslatorAgent queries the relevant API documentation to translate the UI-based actions in the skill code into API calls, thus completing the API-ification of the code. Finally, the generated skills are further validated through a skill validation process. It is worth noting that both the SkillGeneratorAgent and the APITranslatorAgent search for reusable skills from the original skill set during the skill code generation, thereby obtaining skills at different hierarchies. This approach efficiently facilitates the construction of skills.\n3.3.2 Explorer-driven skill exploration. Unlike Follower-driven skill exploration, the explorer-driven skill exploration kicks off the exploration process with a different initialization method: the step-to-step instructions are automatically generated rather than extracting from the help document. During each step, explorer proposes the next action based on the current environment information and the history of previously explored steps. The subsequent steps utilize the same agents in the follower-driven mode to generate skills with explored trajectories. In this process, application seed files with varying pre-filled content are often required to obtain different initial environments for discovering a more diverse range of skills.\n3.3.3 Skill Validation. To validate the new skills generated from exploration, we have implemented two verification methods: static validation and dynamic validation.\n\u2022 Static Validation: This validation method utilizes structural method to verify the skill code, including checking whether the skill's parameters contain the mandatory parameters (such as the executor instance and args list), whether the methods and properties of the executor are correctly invoked in the code, and whether any non-existent skills are imported when reusing the skill.\n\u2022 Dynamic Validation: This validation method evaluates a skill's performance in the app environment with the help of two agents: ValidatorAgent and EvaluateAgent. When a skill is submitted for validation, ValidatorAgent"}, {"title": "4 Feasibility Study", "content": "To validate the usability and effectiveness of AXIS framework, we conducted a feasibility study. We first use AXIS to explore Microsoft Office Word and discover 73 skills. Then we extracted 50 tasks from the wikihow page \"Use Microsoft Word\" and the official Microsoft Word website . These tasks were executed using both AXIS and UI Agent, and the results were analyzed and compared.\n4.1 SKill Exploration\nBefore the exploration, AXIS was provided with 6 basic actions, as shown in Table 2. Then, 347 seed files were used for the AXIS to explore. After the exploration, AXIS discovered 73 skills with different hierarchies. Majority of the skills (44) discovered have a skill hierarchy 1. The rest is composed of 24 skills with hierarchy 2, 3 skills with hierarchy 3, and 2 skills with hierarchy 4. Table 3 displays several successfully validated skills discovered during the exploration process.\n4.2 Task Completion\nThe 50 Word-related tasks collected above were used to test and compare the performance of UI Agent and AXIS with the explored skills. In our experiment, we choose UFO [54] as the representative of UI Agent considering its good performance on word tasks. The results are presented in Table 4, which includes the average time taken to complete different tasks, the success rates, the average number of steps per task, and the corresponding costs of LLM backend (GPT-40, version 20240513) for both agents.\nIn terms of execution time, AXIS significantly outperforms the UI Agent, with an average task completion time of 29.9 seconds compared to 59.5 seconds for the UI Agent. This result shows that AXIS is nearly twice as fast as the UI Agent. AXIS also achieves a higher success rate in completing the tasks. Moreover, thanks to the abstraction and integration of basic actions into higher-level skills, AXIS could complete tasks with fewer steps on average, which also incurs lower costs compared to the UI Agent UFO.\nTo better understand the reasons behind AXIS's higher efficiency, we analyzed the number of UI and API-type actions invoked by AXIS and the UI Agent during task execution and recorded the proportion of API and Advanced API (defined as skills with a hierarchy level of 2 or higher) calls made by both methods. As shown in Table 5, AXIS invoked significantly fewer UI actions compared to the UI Agent during task execution. Notably, the total number of UI actions performed by AXIS across all tasks was greater than the number of invoked API actions. Upon inspection, this was found that AXIS tends to use a single and integrated API skill to complete a whole task, resulting a low overall API actions count. We further calculated the API usage rate and the proportion of Advanced API usage among the API actions. The data shows that AXIS's proportion of API actions reached 55.7% with a 23.1% usage rate of advanced API. In contrast, the API usage rate of UI Agent is only 8.1%. Based on the above results, we conclude that AXIS indeed adopts an API-first approach and tends to use skills to complete tasks when the matching skills are available. And the integration of skills into actions also significantly contributes to the increased efficiency of AXIS."}, {"title": "5 User Study", "content": "We carry out extensive user experiment to evaluate the performance of AXIS. The experiment and the associated evaluation metrics were designed to explore the following research questions (RQs) regarding the roles of LLM-based agents in work and life scenarios:\n\u2022 RQ1: Does LLM-based agent lower the cognitive load of the users and make them have less effort to learn?\n\u2022 RQ2: Does LLM-based agent enhance the efficiency of users?\n\u2022 RQ3: What are the differences between a UI Agent and an API-based Agent in user experience?\nIn our user experiment, participants were asked to complete specified tasks within an application through three methods: manually, with the assistance of a UI Agent, and with the assistance of AXIS and recorded the entire process. Microsoft Word is chosen as the experimental application considering its popularity in our daily work and life as well as the rich API documentations [31]). Motivated by the RQs, we set three objectives for the user experiment:\n(1) To evaluate the cognitive load on participants when completing tasks using different methods. (2) To compare the efficiency and reliability of task completion across the three methods. (3) To assess user preferences regarding the use of different Agents. This study is approved by the Institutional Review Board (IRB) of Peking University."}, {"title": "5.1 Experiment Procedure", "content": "The entire user experiment lasted for 30 minutes. During the preparation phase, we sampled five different tasks in Microsoft Word from both official Word documentation and GPT-generated results. Those tasks were categorized into two levels of difficulty: low difficulty (L1) and high difficulty (L2), based on factors such as the number of UI interactions required, the depth of the UI functions, and the number of ribbon switches. Our experimental results also confirmed that tasks in L2 are indeed more difficult than tasks in L1. In the subsequent discussion, we will simply refers tasks in different categories as L1 tasks and L2 tasks. Additionally, we designed a user information form to collect participants' background information, including their familiarity with Microsoft Word."}, {"title": "5.2 Participants", "content": "We recruited candidates by posting on social media. 20 individuals were randomly selected as participants for the experiment from the list of candidates who confirmed their willingness to participate. Our participants ranged in age from 18 to 40 years with educational backgrounds spanning from undergraduate to postgraduate levels. Their occupations included engineers, students, researchers, and full-time homemakers, among others. 100% of the participants had some experience with Microsoft Word with varying levels of proficiency and different usage frequency ranging from daily to monthly. The user experiment lasted 30 minutes on average per participant and each participant received 50 CNY as compensation."}, {"title": "5.3 Subjective Metric Collection", "content": "As mentioned in 5.1, we used four questionnaires to collect users' subjective experence for completing the tasks using different methods. Questionnaires 1 to 4 were administered separately after completing L1 tasks manually, completing L2 tasks manually, completing L1 tasks with the assistance of an Agent, and completing L2 tasks with the assistance of an Agent, respectively.\nTo determine whether LLM-based agents can indeed reduce users' cognitive load for completing tasks compared to manual work, in all the four questionnaires, we include questions based on the NASA Task Load Index (NASA-TLX) [19] and an additional question on the required learning efforts. For NASA-TLX, our questions cover all the six metrics, including Mental Demand, Physical Demand, Temporal Demand (how hurried or rushed of the tasks), performance (feeling of success in completing the task), frustration level, and completion effort (how hard the users need to work on the tasks). Lower values in those six metrics indicate higher cognitive loads, better feeling of success, lower frustration level, and less efforts during task completion. For the learning efforts, lower score also indicates less learning efforts.\nTo further compare users' perceptions on the UI Agent and AXIS, Questionnaires 3 and 4 contained questions on the ratings on fluency, reliability, UI dependency, decision consistency, and perceived speed for both Agents. Specifically, UI dependency measures the degree of users' reliance on the UI while observing the Agent complete tasks. Decision consistency assesses how closely the decisions made by the Agent align with the decisions users might make to complete the same tasks manually (the experimental web page displayed all the decisions made by the Agents in a step-by-step manner). Perceived speed refers to users' subjective perception on how fast the Agents completed the tasks."}, {"title": "5.4 Objective Metric Collection", "content": "We recorded experimental logs throughout the experiment, including screen recordings of the manual completion of tasks by the users, screen recordings of the two Agents (UI agent and AIXS) performing tasks, decision-making"}, {"title": "6 Results", "content": "In line with our research questions, we divided the experimental results into three parts for analysis. Firstly, We investigate how the adoption of agent reduces cognitive load for users. Secondly, We analyze and compare different agents' abilities of enhancing task efficiency. Finally, we discuss users' preferences between the traditional UI agent and our AXIS agent.\n6.1 Cognitive load\nTo investigate the reduction of cognitive load by LLM-based Agents, we analyzed the NASA-TLX and learning effort scoring collected from users during the task execution process and summarized results in Table 6 and Figure 3. It is worthy noting that, in our experiment, L2 tasks generally scores higher than L1 tasks across multiple dimensions of the NASA-TLX scale and learning efforts, which indicates that our task difficulty classification is consistent with the users' experience.\nAs shown in Table 6, at both L1 and L2 difficulty levels, the agent based method shows significant improvements over the manual based method in most of the NASA-TLX scales. In particular, the agent based method is much less mentally and physically demanding, generate less frustration for users, and requires less effort in completing tasks than the manual based method. The reduction in the cognitive effort by the agent based method is also generally more pronounced for the more difficulty L2 tasks. For the performance metric, while the difference between agent based method and manual method is insignficant for the easy L1 tasks, the agent based method does boost the users' feeling of success significantly (p<0.05) for the difficult L2 tasks. In Figure 3 (b), we also summarized the average scores across all the six NASA-TLX scales. This figure shows that, when using the Agents, the users' experiences in completing L1 and L2 tasks are similar, which demonstrates the stability of LLM-based agents in addressing tasks with different complexities. Finally, Figure 3 (c) shows that the LLM-based agents can also significantly reduced users' learning efforts in completing the task compared to the manual approach. Similar to the NASA-TLX scales, the reduction is also bigger at the higher task difficulty level.\nAll those results clearly demonstrate the value of agent based method in helping users in completing various tasks and answer the first research question (RQ1): the LLM-based agent does indeed lower the cognitive load of users and reduces their effort to learn, especially for more difficult tasks.\n6.2 Efficiency and reliability\nTo compare the efficiency and reliability between the manual method, UI agents, and AXIS, we also collected metrics on the completion time, success rate, as well as the number of steps and costs for completing tasks in our experiment. Those information are summarized in Table 7 and Table 8.\nIn terms of time efficiency, AXIS consistently took significantly less time than both the manual and UI Agent methods for both L1 and L2 tasks (p < 0.001), with a larger advantage for the more difficult L2 tasks. For L1 tasks, the manual method was actually faster than the UI Agent as the UI Agent generally took many steps to complete a task.\nFor the accuracy, unsurprisingly the manual method is the best among all the three methods. Still, AXIS can achieve a high level accuracy that is only slightly worse than human performance. In contrast, the accuracy of the UI agent is"}, {"title": "6.3 Affinity preference", "content": "To explore the differences in user experience when executing tasks with UI agents versus AXIS, We also conducted a subjective preference evaluation on five aspects for both L1 and L2 tasks and summarized the results in Figure 4. At"}, {"title": "7 Discussion", "content": "7.1 AXIS help to digest unnecessary Application Uls\nTo build new APIs on top of existing API and UI functions, AXIS leverages a LLM-powered self-exploration framework to identify all control elements within an application that can be converted into APIs. This exploration procedure helps uncover potentially unnecessary UI elements or redundant UI designs for improvement under the HACI paradigm. To illustrate this process, in Figure 5, the UI hierarchical relationships between UIs are represented as a tree, in which each node represents a UI element with higher-level UI elements as parent nodes and lower-level ones as child nodes. We further use red nodes to represent UI locations that can be API-ified after explored by AXIS, and use blue nodes represent general UI elements. In this example, the root node that represents the \"Home\" tab is a blue node as not all its sub-UI nodes are red (API-ified). However, the second-level node \"Highlight Color\" (node 2-2) and all its third-level child nodes can all be API-ified and are colored in red. Generally, we define a node N as non-essential if this node along with all their child nodes can all be API-ified:\nNonEssential (N) = \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue,if N and all its child nodes are red nodes (API-enabled)\nFalse, otherwise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnlike the HCI paradigm that emphasizes the interactions between human and interfaces, in the future Agent OS powered by LLM-based agents, non-essential UI elements can be simplified or even eliminated from the application interface, with their original functions replaced by the API calls. By categorizing UI elements as essential or non-essential, AXIS can provide valuable insights on how the UI might be improved and re-designed in an agent-based system for the application providers."}, {"title": "7.2 Turn An Application into an Agent", "content": "In the experiment section, we use Microsoft Word to illustrate how to explore and construct new API agents using the AXIS framework. It is worthy noting that the AXIS framework is highly adaptable and scalable, and can be extended to any new application with a basic API and documentation support. Specifically, to adapt AXIS, the application provider needs to supplement operational manuals on the applications as well as the following interfaces:\n\u2022 Environment State Interface for obtaining information about the state of the environment.\n\u2022 Basic Action Interface for supporting basic interactions with the environment.\nStarting from those basic resources, AXIS can automatically and continuously explore the applications, discover new skills, and extend its functionalities. This adaptability also means that AXIS can be integrated into various software environments to enhance functionality and user experience with API-driven interactions."}, {"title": "8 Conclusion", "content": "In this paper, we introduce AXIS, a pioneering framework designed to enhance human-agent-computer interaction (HACI) and address the inefficiencies and cognitive burdens associated with multimodal large language models (MLLMs) in complex task execution by prioritizing API calls over traditional UI interactions. Through user experiments with tasks from Office Word, AXIS has proven to be highly effective, reducing task completion time by 65%-70% and cognitive workload by 38%-53%, while maintaining a high level of accuracy comparable to human performance. These results underscore the potential of API-first LLM-based agents to streamline interactions, minimize latency, and enhance reliability in task execution.\nOur research contributes to the broader field of human-agent interaction by highlighting the limitations of traditional UI-based approaches and proposing a novel solution that leverages API calls to simplify and accelerate task completion. By enabling applications to act as agents through a reduced set of essential UIs and enhanced API sets, AXIS not only improves efficiency but also paves the way towards the development of a comprehensive Agent OS. This paradigm shift suggests that every application has the potential to transform into an intelligent agent capable of executing tasks with minimal user intervention.\nIn conclusion, AXIS represents a significant step forward in reducing cognitive load and enhancing the efficiency of task completion with LLM-based agents. Our findings provide valuable insights for application developers and researchers, encouraging them to rethink UI designs and explore new ways to integrate API-driven interactions. In our future work, we will focus on extending this framework to a broader range of applications, exploring its impact on various user groups, and advancing the potential of LLMs in creating a more intuitive and efficient human-computer interface."}]}