{"title": "Case Study: Leveraging GenAI to Build AI-based Surrogates and Regressors for Modeling Radio Frequency Heating in Fusion Energy Science", "authors": ["E. Wes Bethel", "Vianna Cramer", "Alexander del Rio", "Lothar Narins", "Chris Pestano", "Satvik Verma", "Erick Arias", "Nicola Bertelli", "Talita Perciano", "Syun'ichi Shiraiwa", "\u00c1lvaro S\u00e1nchez Villar", "Greg Wallace", "John C. Wright"], "abstract": "This work presents a detailed case study on using Generative AI (GenAI) to develop AI surrogates for simulation models in fusion energy research. The scope includes the method-ology, implementation, and results of using GenAI to assist in model development and optimization, comparing these results with previous manually developed models.", "sections": [{"title": "I. INTRODUCTION", "content": "The need for real-time experiment predictions and control in fusion research has highlighted the limitations of traditional simulation codes like GENRAY, which do not run fast enough for real-time applications. There is significant interest in using AI as surrogates to approximate the results of full numerical physics computations within known or predictable error bounds, as demonstrated in our previous work [1]. Currently, Generative AI (GenAI) has gained substantial attention for its capabilities in creating images, videos, text, code, and even new Al models. Our focus in this work is to leverage GenAI for creating numerical surrogates that approximate high-fidelity physics simulations. This study explores using GenAI as a well informed assistant to aid in all steps of the model building, optimization, and evaluation process. In this context, we engage in a conversation with the GenAI about each of these stages, and it provides suggestions about approach as well as initial code templates that we then adapt for use in our particular problem. This type of approach has proven effective for using AI in other contexts, such as education [2]. We leverage GenAI to suggest approaches and provide code templates for the various stages of the model development pipeline, including exploratory data analysis; initial model development and evaluations; more extensive model optimization through k-fold cross-validation; final model training and evaluation. Being able to leverage GenAI as part of this process has enabled us to reproduce most of our previous work in a shorter amount of development time and with marginally better model performance as well as perform a deeper exploration of model optimization strategies. Those include finding ways to reduce the number of input model features through different approaches like principal component analysis and by studying correlations between input features and output targets. The organization of this paper reflects the conversational nature of our interactions with GenAI. We first reflect on background and previous work related to GenAI and its role in developing software tools (\u00a7II). Next, the narrative about approach and implementation begins with an overview then delves into each of the stages of model development, which is aimed at discovering the best parameters for each of the different types of models (\u00a7III). Because preliminary results at earlier stages in the model development pipeline inform and impact decisions and action in later stages, these preliminary results are included as part of the narrative about imple-mentation. Once the best model parameters are identified, the final models undergo evaluation in terms of accuracy and computational speed (\u00a7IV). \u00a7V provides observations and reflections about the results of the process of using GenAI and about the nature of the quantitative and qualitative results of the study.\nThe primary contributions of this paper are:\n\u2022 A case study on using GenAI to produce code templates for building AI surrogates for fusion physics simulations and for identifying and implementing strategies for model optimization.\n\u2022 A comparison of AI-assisted codes in this work with human-generated codes in previous work in terms of the accuracy of the resulting models and computational requirements for model training and inference.\n\u2022 Use of GenAI to identify potential pathways for improv-ing model performance and code templates for imple-menting them along with some analysis results to support their use, or not, as the case may be."}, {"title": "II. BACKGROUND AND PREVIOUS WORK", "content": "In fusion energy science, a tokamak is a machine that confines a burning plasma using a strong magnetic field [3]. They are widely believed to be one of the most promising designs for a practical fusion reactors. There is a robust international community of fusion energy scientists who study tokamak designs with an eye towards creating viable reactors for sustainably producing energy.\nAmong the challenges in the design and operation of a tokamak is the need to understand what is happening inside the burning plasma along as well as the ability to manipulate it in various ways, such as reducing instabilities. Radio Frequency (RF) systems such as lower hybrid current drive (LHCD) and high harmonic fast wave (HHFW) are well suited for use in tokamaks as they do not require line-of-sight access through radiation shields and they have a high degree of technology readiness. [1]. As such, being able to predict RF wave heating and current drive is essential for present-day real-time observation and control in fusion experiments and for modeling the designs of future reactors.\nAs a practical matter, the computational methods used to predict RF wave heating and current drive have significant costs. A single GENRAY/CQL3D simulation without radial diffusion of fast electrons requires 10s of minutes of wall-clock time to complete. This runtime may be acceptable for some purposes, like offline modeling, but is much too slow for use in integrated modeling and real-time experimental control applications.\nIn previous work, Wallace et al., 2022 [1] describe develop-ment of AI-based surrogate models to perform predictions of RF power absorption and current density profiles. The results show a dramatic reduction in computational time, going from 10s of minutes to a few milliseconds once models are trained. While the surrogate-based computations are not identical to those of the GENRAY/CQL3D code, that study quantifies differences using mean squared error.\nA significant amount of human effort was required by that time to generate a database of runs, followed by manual code generation to produce three different AI-based surrogates, and to measure their runtime and numerical accuracy performance. In this case, significant effort is somewhat difficult to quantify, but can be characterized as the involvement of a team of four fusion physicists and four computer scientists working part-time over the period of about two years. In an effort to reduce time-to-solution for such an endeavor, the main objective for the work in this study is to explore the use of GenAI for the purpose of producing similar AI surrogates, as well as to explore ways to improve model optimization and reduce computational cost of model training and inference."}, {"title": "III. APPROACH, IMPLEMENTATION, AND INITIAL RESULTS", "content": "We begin with a brief overview of the process of creating AI models to provide a starting point for our implementation (\u00a7III-A). Even though we are focusing on regression models, many of these concepts are also applicable to classification models. After a discussion of the source data, transformations, key assumptions and that influence aspects of our design and implementation (\u00a7III-B), we dedicate a separate subsection to each of the three different AI-based regression methods; Random Forest Regression (\u00a7III-C), Multilayer Perceptron Re-gression (\u00a7III-D), and Gaussian Process Regression (\u00a7III-E). Included in the discussion of these methods is information about optimizing models through k-fold cross validation. We then examine approaches for eliminating input features in an effort to reduce the computational cost for model training and inference (\u00a7III-F). We also provide a brief comparison of preliminary model results from these steps before moving on to final model training and evaluation in the next section."}, {"title": "A. Overview of Model Development", "content": "The process of developing an AI-based regressor involves several steps, including method selection, hyperparameter op-timization, model training, and accuracy evaluation. While the details of each step will vary depending upon the specific method being employed, the same overall set of themes is present for all methods. Fig. 1 shows the sequence of processing steps we use in this study, and this sequence form the organization of material in the subsections that follow.\nA first step likely includes exploratory data analysis for the purpose of deepening understanding of both the problem domain and characteristics of the data at hand [9]. For our specific problem, an initial exploration of the source data revealed that certain combinations of inputs resulted in code outputs that were not physically realistic. This can happen when the physical models do not perform well in certain ranges of input parameters. Ultimately, those data were filtered out so as to provide only physically realistic and meaningful inputs to model training [1].\nA next step is to choose one or more AI models for the task at hand where consideration is given to strengths, weaknesses, and applicability to the problem at hand [10]. For example, decision trees and random forests may be better suited for problems with non-linear relationships and interactions. Neural networks may perform well at capturing highly non-linear relationships between input features and output targets. Kernel-based methods might provide a better fit between a particular kernel combination choice and the underlying data. Typically, one might begin with initial model testing using default parameters to help identify promising methods. In our case, we are focusing on use of three different methods that span three fundamentally different approaches: random forest, Gaussian process regression, and multilayer perceptron. These three models were the focus of previous work where they were developed by hand prior to the prevalance of GenAI tools [1].\nAn integral part of AI model development and testing is to evaluate its performance: how well do its predictions match the ground truth? There are several different common metrics such as mean-squared error (MSE), R2, mean absolute error (MAE) and others, each of which has particular strengths and weaknesses [11]. For our purposes, we are using MSE as a measure of model accuracy.\nGiven a particular model, a next step is adjusting model parameters to improve model predictive performance. Each different model has its own unique set of parameters, and this process of parameter tuning is referred to as hyperparameter optimization. Because many models' parameters cannot be directly estimated from the data [12], the process of finding the optimal setting for model parameters may entail an iterative approach of model adjustment, evaluation, and testing. Regular and systematic evaluation of the range of parameter values is known as a grid search while a random search uses a random values for parameters from a range and then makes adaptive search decisions based on model performance [13].\nA well established methodology for hyperparameter opti-mization is known is k-fold cross validation (CV) [11]. The basic idea is to split a dataset into K equally sized subsets, or folds. Then the model is trained K times using K \u2212 1 folds for training and the remaining fold for validation. The process ensures that each fold is used for validation only once. The average model performance, e.g., using MSE, is computed as the average of all individual model MSES.\nComplicating matters further is that each different model has a different set of parameters. Random forest hyperparameters include the size of the forest, the maximum depth of the trees, and others [14]. Multilayer perceptron model hyperparameters include the number of network layers and connectivity, acti-vation functions, learning rate, and regularization [15]. GPR hyperparameters the choice of one or more kernel functions,"}, {"title": "B. Source Data Analysis and Preparation", "content": "For this study, we are using the same dataset reported in Wallace et al., 2022 [1]. It consists of 13,347 records consisting of 9 simulation input variables and 2 output fields (current deposition and wave power profile) consisting of 23 variables each. The 9 input simulation variables, names, units, and numeric ranges are shown in Fig. 2. During original data preparation in 2022, the original 16K GENRAY outputs were filtered to eliminate data records from certain input parameter ranges known to correspond to output values that were physically not meaningful.\nA common practice during supervised model training is to randomly divide the dataset into portions used for model training and model testing. Because the 2022 study involved comparing 3 different models, to achieve consistency in which the same partitions of data are used for training and testing across the 3 different models, the study methodology included steps to perform data partitioning once then use those labeled partitions for all subsequent stages of the study.\nIn the 2022 study, a new column was added to the simulation data that identifies its \"fold\". The source data was first partitioned into an 80%/20% train/test split. The test split is held out from model training and optimization and used only in final model evaluation. The purpose of this \"hold-out\" fold of data is to validate the generalization ability of an AI model and ensuring that it will perform well on new, unseen data. For the remaining 80% of the simulation data, it was partitioned into 5 equal-size folds. We adopt the same convention in our work here so as to achieve consistency in the data subsets used for k-fold CV as well as final model training and evaluation.\nWe use a common preprocessing step that normalizes the data values associated with the input features; their features and their ranges appear in Fig. 2. We made use of the sklearn.StandardScaler method, which transforms data from its native range to a new range with a zero mean and unit variance. The reason for such resampling is two-fold. First, some methods like GPR kernels assume that features are centered around a mean of zero. Another reason is the desire for the variances of all features to have the same magnitude so that features of larger numerical ranges do not dominate those with lower ranges during evaluation of objective functions [17]."}, {"title": "C. Random Forest Regression", "content": "The Random Forest model for classification or regression is an ensemble method that uses the averaging of predictions from many decision trees applied to random subsets of the training data [18]. Its advantages include robustness to over-fitting through the averaging of predictions from multiple trees, as well as being effective with a larger number (dimension) of features.\nWorking with ChatGPT-40, we issued a prompt asking for a description of a random forest regression along with key references. In addition to some useful text-based information, it also provided the source code template shown in Fig. 3. That source code includes some key processing steps, including:\n\u2022 Dividing data into partitions to be used for model training and testing;\n\u2022 Setting up a set of parameters and ranges to be used in evaluating different model configurations;\n\u2022 Code to perform a systematic (grid) search of the param-eter space;\n\u2022 Printing out the parameters of the best model\nStarting with this code, we added our code to load the data file and separate features and targets, perform normalization as discussed in \u00a7III-B, and extract one fold from the dataset for use in exploratory analysis. The initial results were promising: the MSE was a bit higher than the mean MSE from 5-fold testing from 2022 as shown in Tab. II but still promising.\nIt is important to note that in the code shown in Fig. 3 that there is a grid of model parameters with initial values, and what follows is a \"mini\" k-fold CV combined with a random grid search courtesy of the GridSearchCV call. Again, ChatGPT-40 produced this code template in response to a query about RFR and their models. While the grid search approach is common practice, changed from a grid search to a randomized search (RandomizedSearchCV), as it is known to run more quickly and to produce better results [13].\nNext, we pursued further exploratory model evaluation aimed at better understanding the RFR hyperparameters and their impacts. This was an iterative process of asking questions of the GenAI, tinkering with the code template by expanding the parameter grid variables and ranges, rerunning the code, and observing the changes in model MSE. We used the best model parameters from this portion of the study for subsequent hyperparameter optimization. The results of this early optimization step for all models is shown in Tab. I.\nNext, we engaged in a discussion with the GenAI asking it to describe hyperparameter optimization in general terms as well as k-fold CV in particular. It shared the concept of nested cross-validation that consists of an \"inner loop\", where a given test/train split is subject to the randomized parameter search using a CV strategy, and an \"outer loop\" where the different K folds of data are rotated so that only one fold is used for validation. This nested CV is an addition to the methodology from the 2022 study. The resulting code has a structure shown in the Listing 1 is an abbreviated version of the actual code but that contains all key ideas."}, {"title": "D. Multilayer Perceptron Regression", "content": "A Multilayer Perceptron (MLP) is a feed-forward artificial neural network composed of multiple layers of neurons (per-ceptrons) with activation functions. It typically consists of three types of layers of nodes, an input layer, one or more hidden layers and an output layer. The MLP is a supervised learning algorithm that \"trains\u201d a neural network to map from inputs to outputs through a process of iteratively adjusting the layer weights and parameters backwards from the outputs to the inputs [19].\nWorking with ChatGPT-40, we issued a prompt asking for information about MLPs along with key references, and key steps in hyperparameter optimisation. In addition to useful text output, ChatGPT-40 generated an initial code template shown in Fig. 4.\nAs in the case of the RFR code template, the MLP code template consists of the steps needed to build a basic MLP regressor starting with a simple set of model parameters and parameter ranges. This starter code also performs a call to GridSearchCV to perform both a systematic search through the model parameter space that includes a 5-fold CV as part of the search. The return value from the grid search is information about the model parameters that produced the best MSE results.\nLike the RFR template example, we added our own code to load in one fold from the original dataset for the purpose of exploratory model analysis. We also changed from a grid search to a randomized search, as it is known to run more quickly and to produce better results [13]."}, {"title": "E. Gaussian Processes Regression", "content": "In this study, we began to explore the same process for building GPR models as with the RFR and MLP models. These codes have significant computational cost, as evidence by results from 2022 [1] showing that the GPR methods have 10x the computational cost of RFR and MLP, but the resulting MSE is somewhere inbetween the two. For the purpose of this study, we abandoned this particular line of investigation on the basis of computational costs. Future work may examine finding ways to significantly reduce the computational costs. GPR involves the inversion of an N \u00d7 N covariance matrix (also known as the kernel matrix), where N is the number of training samples [20]. The time complexity of inverting this matrix is O(N\u00b3) and the memory complexity is O(N\u00b2). As the number of data points increases, the computational cost grows cubically and the memory cost grows quadratically, making GPR impractical for large datasets."}, {"title": "F. Reducing the Number of Features", "content": "In an effort to reduce computational cost, we again con-sulted with ChatGPT-40. Among its suggestions was one about reducing the number of input features, which in turn could reduce the computational cost of model training an inference.\nTwo different strategies came up in the conversation. One is to study the correlations between input features and output targets and perhaps exclude those features that do not have any significant impact on output features. Another strategy is to perform Principal Component Analysis (PCA) [21] on the input features, and then perform model fitting on the resulting PCA components rather than the original input features.\nSince the key idea is the correlation between inputs and outputs, we iterated with ChatGPT-40 to generate an initial code template for creating correlation maps. We used that code first to create maps showing the correlation strength between input simulation variables (features) and outputs. We iterated with ChatGPT-40 to generate a code template that computes PCA components, and also created maps showing the correlation strength between 5 PCA components and the output variable. All correlation maps are shown in Fig. 5.\nBased upon the results of this study, the suggested approach was to eliminate those features where no strong positive or negative correlation exists between the input feature and any output variable, where strongly correlated would be defined as |t| >= 0.1. From Fig. 5, for the variable Current, we can eliminate one input feature R0_eqdsk, and for the variable Powers, we can eliminate three input features: R0_eqdsk, elecfld, and zeff. We modified versions of our RFR and MLP codes to use these reduced features. There was some minor impact on the model in terms of computational rate and MSE both at the exploratory analysis phase (see Table I and during final model construction and analysis (\u00a7IV-C.\nLooking at the correlation maps for the 5 PCA components in Fig. 5, none of the PCA components would appear to qualify for exclusion using the thresholding criteria of the absolute threshold value being smaller than some minimum, e.g., t < 0.1. Nonetheless PCA-1 and PCA-5 appear to have strong positive correlations for the variables Current and Pow-ers, respectively. Therefore, we created derivative codes for RFR and MLP that would perform PCA using 5 components, then used them as input features for model training. The results shown in Table I reveal the model accuracy in terms of MSE is far inferior to that of the original simulation variables. The fact that PCA did not produce advantageous results here is not a surprise: the original simulation features are more or less evenly distributed through their own parametric ranges owing to a Latin Hypercube sampling used for setting up the simulation runs [1]. We did not further pursue the idea of using"}, {"title": "IV. FINAL MODEL RESULTS", "content": "This section begins with a description of the hardware and software environment used for the model development and testing. Next, we present quantitative findings from final model creation and evaluation, along with a comparison with earlier work. We also examine the qualitative issue of whether or not development time was reduced compared to earlier efforts."}, {"title": "A. Computational Environment - Software", "content": "In this study we leveraged two different GenAI systems, Github's Copilot [22] and OpenAI's ChatGPT-40 [23]. When attempting to ascertain a version number for Copilot, we asked it directly and it said \"As an AI developed by OpenAI and GitHub, I don't have a specific version number like a traditional software application might. However, I'm based on the GPT-4 version of OpenAI's GPT models.\"\nSimilarly, with ChatGPT-4o, it responded \"I am an AI language model developed by OpenAI, based on the GPT-4 architecture. I do not have a specific version number like traditional software, but I am part of the GPT-4 series, which was released by OpenAI in March 2023. You can refer to me as \"GPT-4\".\"\nThere are some troubling implications with respect to re-producibility given that there is no specific software version number to associate with a particular body of work, especially when that software is central as an assistant for concepts and code templates.\nOther software that was leveraged as part of this work on both laptop/desktops and large-scale computational facilities includes:\n\u2022 sklearn [17], v1.5.0 for x86 Linux and MacOS\n\u2022 GPflow [24], v2.9.1 for x86 Linux (Perlmutter@NERSC)\n\u2022 Visual Studio Code (code) [25], v1.89.1 on MacOS\n\u2022 Python 3.10.0"}, {"title": "B. Computational Environment \u2013 Hardware", "content": "The team's general workflow is to use personal computers like laptops for interactions with Copilot through VScode. There was a diversity of platforms including x86 and Apple M1/M2 chipsets. Interactions with the GenAI systems, initial code development and testing occured on these personal platforms.\nFor longer computational runs, the team made use of the Perlmutter system at the National Energy Research Scientific Computing Center (NERSC). Perlmutter is a HPE (Hewlett Packard Enterprise) Cray EX supercomputer. It is a heteroge-neous system comprised of 3,072 CPU-only and 1,792 GPU-accelerated nodes."}, {"title": "C. Model Results and Comparison to Previous Implementation", "content": "The model accuracy results in terms of MSE for all models are shown in Table II. These results include MSE from the four new models from this study: MLP versions for all features and reduced features, and RFR versions all features and reduced features applied to both Current and Powers variables. For the sake of comparison, we are including MSE data for the two original models (MLP TensorFlow 2022, RFR sklearn 2022) applied to both Current and Powers variables.\nIn terms of overall model accuracy, both the MLP and RFR full-feature versions show minor accuracy improvement compared to their counterparts from 2022 for the Current variable and identical MSE for the Powers variable. While both 2022 and present RFR codes are implemented using sklearn, the 2022 MLP is implemented using TensorFlow and the present MLP is implemented using sklearn. The fact both RFR implementations use sklearn, use the same data, and virtually identical methodology leads us to expect the MSE will be nearly identical. For MLP, the implementation differences along with differences in hyperparameter settings for the final model, due to differences in 5-fold CV outcomes, gives rise to a somewhat larger MSE difference for the Current variable.\nOf the models that make use of reduced features, the value of the MLP metric MSE outcome is worse than (larger than) than the MSE metric of both 2022 and present MLP full-feature models. On the other hand, the RFR reduced feature version show some MSE improvement compared to both 2022 and present versions for the Current variable. Results are identical for the powers variable for all RFR versions.\nFor each of the different models, we accumulated the accuracy using MSE during both 5-fold training and final model construction. The results for all models in both stages of training appear in Table II, where they are juxtaposed with the corresponding model results from Wallace et al., 2022 [1].\nThe runtime costs for building models and performing inference workloads appear in Table III. The present day models required significantly more runtime for 5-fold CV than those from 2022, most likely due to a greater number of different parameter configurations, a broader numeric range of parameter values, and the use of nested cross-validation. This extra effort appears to have paid off in terms of the present day models requiring substantially less final model training"}, {"title": "D. Qualitative Results: Was Development Time Reduced?", "content": "While some previous works have painstakingly measured developer performance to get a better feel for how GenAI improves productivity (c.f. \u00a7II, in this case we can offer some anecdotal observations.\nFor the 2022 study [1], the original team worked part-time over the course of two years on producing the first results. A significant amount of time was generating and validating the simulation database that is the input for model training. There were separate persons working on each of the three models, though each was concurrently working on multiple additional projects. The 2022 study software also includes the creation of digital data artifacts that include the simulation database along with file-based versions of the trained models and Jupyter notebooks to load the train models and study data.\nThe present effort began in February 2024 and these results represent about 4 months effort for a faculty and 3 students working part-time on the project. All on the team are in agreement that we all feel more productive in terms of being able to interact with a capable GenAI assistant who can present and summarize in-depth technical material along with code templates to get started. Our own experience echoes that of other studies that conclude that GenAI helps to improve coding and learning productivity."}, {"title": "V. FINDINGS AND DISCUSSION", "content": "GenAI may not do so well with data dependent operations. While Copilot and ChatGPT-40 did a reasonably good job of producing general purpose code, it was not well suited for situations that require knowledge about the characteristics of a particular dataset or specific methods for working with those specific characteristics. For example, when prompting Copilot to perform hyper-parameter optimization for GPR, the resulting code exhibited lengthy runtimes and would not finish within the 6-hour limit for Juypter jobs. In response to a prompt about improving GPR runtime, ChatGPT-40 suggested \"using a more efficient optimizer: The Scipy optimizer used in the code is a general-purpose optimizer. If you have some knowledge about the structure of the problem, you might be able to use a more efficient optimizer. However, this requires a deep understand-ing of the problem and the optimizers.\"\nResults from GenAI can be buggy. ChatGPT-40 sometimes generates code with bugs, which we would repair using Copilot from inside VScode. In addition, ChatGPT-40 would sometimes provide citations to works that simply do not exist. All results from GenAI should be subject to careful scrutiny and validation.\nIt is well understood that there is bias in the output produced by GenAI tools. This bias originates from several key factors related to the data used for training, the design of the models, and the algorithms that guide their learning. For example, GenAI models are trained on vast datasets that are often scraped from the internet. These datasets can reflect the biases present in the real world, including cultural, social, and linguistic biases. In our studies, some of the initial code suggestions might not be the best for one reason or another. For example, both ChatGPT-40 and Copilot always turn to a grid search for doing hyperparameter optimization. It is well known that a randomized search can often produce superior results in reduced runtime [13]. We would often try out the initial grid search suggestion but then switch to a randomized search instead.\nThe GPR model was not included in this study because it was too computationally expensive. From the 2022 study, GPR was 10x more computationally expensive to train models compared to RFR and MLP, and its model accuracy was somewhere between RFR and MLP. While we did endeavor to find ways to reduce the computational runtime through a conversation with ChatGPT-40, none of the approaches provided satisfactory results. An open question is whether or not there are other approaches we have not found yet that would result in GPR being more competitive in terms of cost vs. accuracy for this particular problem."}, {"title": "VI. CONCLUSION", "content": "The overall objective for this work is leverage GenAI for all stages of AI-based model conception, development, opti-mization, and evaluation. The results of this effort is compared both quantitatively and qualitatively with results from a 2022 study [1]. The primary findings are that the current generation models have comparable performance in terms of accuracy, and there are some tradeoffs in terms of computational cost where more time spent optimizing current models also results in faster full model training and inference time.\nA key finding of this study is that use of GenAI is far from being \"turnkey\". The GenAI is best viewed as a capable assistant, and the information and code products it produces require careful scrutiny. In addition, the deeper the domain knowledge of the human conversing with the GenAI, the better the quality of results."}]}