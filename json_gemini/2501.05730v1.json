{"title": "Element-wise Attention Is All You Need", "authors": ["Guoxin Feng"], "abstract": "The self-attention (SA) mechanism has demonstrated superior performance across various domains, yet it suffers from substantial complexity during both training and inference. The next-generation architecture, aiming at retaining the competitive performance of SA while achieving low-cost inference and efficient long-sequence training, primarily focuses on three approaches: linear attention, linear RNNs, and state space models. Although these approaches achieve reduced complexity than SA, they all have built-in performance degradation factors, such as diminished \"spikiness\u201d and compression of historical information. In contrast to these approaches, we propose a novel element-wise attention mechanism, which uses the element-wise squared Euclidean distance, instead of the dot product operation, to compute similarity and approximates the quadratic complexity term $exp(q_{ic}k_{jc})$ with a Taylor polynomial. This design achieves remarkable efficiency: during training, the element-wise attention has a complexity of O(tLD), making long-sequence training both computationally and memory efficient, where L is the sequence length, D is the feature dimension, and t is the highest order of the polynomial; during inference, it can be reformulated as recurrent neural networks, achieving a inference complexity of O(tD). Furthermore, the element-wise attention circumvents the performance degradation factors present in these approaches and achieves performance comparable to SA in both causal and non-causal forms.", "sections": [{"title": "1. Introduction", "content": "Traditional deep learning methods have significant limitations: Convolutional Neural Networks (CNNs) struggle to capture long-range dependencies, while traditional Recurrent Neural Networks (RNNs) (Hochreiter, 1997; Chung et al., 2014) do not support parallel training and are prone to exploding or vanishing gradients. In contrast, the self-attention (SA) (Vaswani, 2017) mechanism overcomes these limitations and has demonstrated superior performance across various domains (Dai et al., 2019; Dosovitskiy, 2020; Brown et al., 2020), including natural language processing (NLP), computer vision (CV), and time-series analysis.\nDespite its superior performance, SA incurs significant complexity in both training and inference. During inference, the KV-caching (Pope et al., 2023) technique stores historical key and value matrices, leading to an inference complexity of O(LD), where L is the sequence length and D is the feature dimension. Consequently, the inference cost grows with the sequence length. During training, SA has computational and memory complexities of O(L2), which makes training on long sequences both time-consuming and memory-intensive.\nSignificant efforts have been devoted to developing the next-generation architecture, aiming at simultaneously achieving low-cost inference, efficient long-sequence training, and SA-comparable performance, i.e., breaking the \u201cimpossible triangle\". Current research can be classified into three main categories. First, linear attention (LA) (Katharopoulos et al., 2020; Sun et al., 2023; Yang et al., 2024; 2025) employs kernels $\\phi(q_i) \\cdot \\phi(k_j)$ to approximate the $exp(q_ik_j)$ in SA, achieving linear training complexity and constant inference complexity with respect to the sequence length. However, LA's performance often lags behind SA, primarily due to its lack of \u201cspikiness\u201d (Zhang et al., 2024). Spikiness refers to the attention mechanisms assigning major weights to a few critical tokens while assigning little weights to others, leading to sharp attention weights distributions, which greatly contributes to the effectiveness of attention mechanisms. SA attains spikiness by using the exponential function to amplify larger dot products and suppress smaller ones. In contrast, LA loses this spikiness by eliminating the exponential function. Secondly, linear RNNs (Bradbury et al., 2017; Orvieto et al., 2023; Peng et al., 2023) eliminate the non-linear functions in traditional RNNs, enabling efficient parallel training while retaining the low-cost inference characteristic in traditional RNNs. However, linear RNNs generally exhibit inferior performance compared to"}, {"title": "2. Notations", "content": "Let $X \\in \\mathbb{R}^{L \\times D}$ denote a sequence of L token vectors, each comprising D channels. We introduce the following notation: $X_{i:} \\in \\mathbb{R}^D$ or $X_{:c} \\in \\mathbb{R}^L$ denotes the i-th token vector; $X_{:c} \\in \\mathbb{R}^L$ denotes the elements in the c-th channel across all tokens; $X_{ic} \\in \\mathbb{R}$ denotes the element at the i-th token vector and c-th channel.\nIn Self-Attention (SA), dot-product operations are frequently used. Specifically, for $q_i = (q_{i1},...,q_{iD}) \\in \\mathbb{R}^D$ and $k_j = (k_{j1},...,k_{jD}) \\in \\mathbb{R}^D$, their dot product is calculated as $q_ik_j = q_{i1}k_{j1} + ... + q_{iD}k_{jD} \\in \\mathbb{R}$. In Element-wise Attention (EA), element-wise operations are used. Specifically, the element-wise squaring of $q_i$ is $q_i^2 = (q_{i1}^2,...,q_{iD}^2) \\in \\mathbb{R}^D$; the element-wise multiplication between $q_i$ and $k_j$ is $q_ik_j = (q_{i1}k_{j1},...,q_{iD}k_{jD}) \\in \\mathbb{R}^D$; and the element-wise division is given by $\\frac{q_i}{k_j} = (\\frac{q_{i1}}{k_{j1}},...,\\frac{q_{iD}}{k_{jD}}) \\in \\mathbb{R}^D$."}, {"title": "3. Element-wise Attention", "content": "In section 3.1, we formalize the full version of Element-wise Attention (EA), which can serve as a replacement of SA in Transformer architectures. Subsequently, in section 3.2, the EA-series, with only linear training complexity, is derived by approximating the quadratic complexity term in EA using a Taylor polynomial. In section 3.3, we formalize the causal EA-series, which can be reformulated as recurrent neural networks (RNNs) to enable efficient inference. Finally, in section 3.4, EA and EA-series are compared with various methods."}, {"title": "3.1. EA", "content": "In this section, we introduce the full version of EA. Figure 1 illustrates the computation process of EA.\nSpecifically, the input sequence $X \\in \\mathbb{R}^{L \\times D}$ is first projected onto the corresponding representations: queries (q), keys (k), values (v) $\\in \\mathbb{R}^{L \\times D}$. Then, a feature tensor is constructed using q and k. The element $o_{ijc} \\in \\mathbb{R}$ in the feature tensor, which directly quantifies the similarity between $q_{ic} \\in \\mathbb{R}$ and $k_{jc} \\in \\mathbb{R}$, is calculated as follows:\n$o_{ijc} = -(q_{ic} - k_{jc})^2$ (1)\nSubsequently, a Softmax function normalizes $o_{i:c} \\in \\mathbb{R}^L$, transforming them into weights that are then assigned to $v_{:c} \\in \\mathbb{R}^L$ to produce $y_{ic} \\in \\mathbb{R}$.\nThe complete computational formula for EA is presented below, where squaring, multiplication and division operations"}, {"title": "3.2. EA-series", "content": "EA construct a feature tensor to meticulously mobilizes the value elements, but it introduces significant complexity. To reduce the complexity and facilitate efficient training and inference, we approximate the quadratic complexity term in EA using a Taylor polynomial, deriving the EA-series. The derivation process is outlined below.\nGiven that the squaring, multiplication and division operations are performed element-wise, equation 2 can be reformulated as follows:\n$y_{ic} = \\frac{\\sum_{j=1}^L e^{-(q_{ic}-k_{jc})^2}v_{jc}}{\\sum_{j=1}^L e^{-(q_{ic}-k_{jc})^2}}$ (2)\n$y_{ic} = \\frac{\\sum_{j=1}^L e^{-k_{jc}^2}e^{-q_{ic}^2}e^{2q_{ic}k_{jc}}v_{jc}}{\\sum_{j=1}^L e^{-k_{jc}^2}e^{-q_{ic}^2}e^{2q_{ic}k_{jc}}} = \\frac{\\sum_{j=1}^L e^{-k_{jc}^2}e^{2q_{ic}k_{jc}}v_{jc}}{\\sum_{j=1}^L e^{-k_{jc}^2}e^{2q_{ic}k_{jc}}}$ (3)\nWe can find that the quadratic complexity arises from computing $e^{2q_{ic}k_{jc}}$. To reduce the complexity, we approximate $e^{2q_{ic}k_{jc}}$ using a Taylor polynomial:\n$e^{2q_{ic}k_{jc}} = \\sum_{n=0}^{+ \\infty} \\frac{(2q_{ic}k_{jc})^n}{n!} = 1 + 2q_{ic}k_{jc} + \\frac{2^2}{2!}q_{ic}^2k_{jc}^2 + ...$ (4)\nSubstituting this polynomial into equation 3, we can obtain the EA-series with only linear complexity:\n$y_{ic} = \\frac{\\sum_{j=1}^L e^{-k_{jc}^2} \\sum_{n=1}^t \\frac{(2q_{ic}k_{jc})^n}{n!} v_{jc}}{\\sum_{j=1}^L e^{-k_{jc}^2} \\sum_{n=1}^t \\frac{(2q_{ic}k_{jc})^n}{n!}}$"}, {"title": "3.3. Causal EA-series", "content": "Attention mechanisms have two forms: causal and non-causal. In non-causal attention, each token can attend to all tokens within the sequence. Equation 5 is the non-causal EA-series. Conversely, causal attention allows each token to attend only to preceding and current tokens, excluding subsequent tokens. Large language models (LLMs) typically use causal attention. The causal EA-series is formally defined in equation 6 and can be implemented by replacing the sum() in figure 2 with cumsum().\n$y_{ic} = \\frac{\\sum_{j=1}^i e^{-k_{jc}^2}v_{jc} + 2q_{ic} \\sum_{j=1}^i k_{jc}e^{-k_{jc}^2}v_{jc} + ...}{\\sum_{j=1}^i e^{-k_{jc}^2} + 2q_{ic} \\sum_{j=1}^i k_{jc}e^{-k_{jc}^2} + ...}$ (6)\nThe causal EA-series can be trained in parallel with linear complexity. When it comes to inference, the causal EA-series can be rewritten as recurrent neural networks:\n$constant = (1, 2, \\frac{2^2}{2!},...,\\frac{2^{t-1}}{(t-1)!}) \\in \\mathbb{R}^{D \\times t}$ (7)\n$S_0 = 0 \\in \\mathbb{R}^{D \\times t}$ (8)\n$Z_0 = 0 \\in \\mathbb{R}^{D \\times t}$ (9)\n$K_i = (1, k_i, k_i^2,...,k_i^{t-1}) \\in \\mathbb{R}^{D \\times t}$ (10)\n$Q_i = (1, q_i, q_i^2,...,q_i^{t-1}) \\in \\mathbb{R}^{D \\times t}$ (11)\n$S_i = S_{i-1} + K_ie^{-k_i^2}v_i$ (12)\n$Z_i = Z_{i-1} + K_ie^{-k_i^2}$ (13)"}, {"title": "3.4. Relation to and Differences from Previous Methods", "content": "Table 1 compares the EA-series with typical attention mechanisms from various perspectives, including computational and memory complexity during training, and inference complexity. Generally, the sequence length L is much greater than the feature dimension D, and D is significantly larger than the highest order t in the Taylor polynomial. The EA-series demonstrates reduced memory complexity and the lowest computational and inference complexity. Detailed comparisons with specific methods are provided below:\nSA In SA, each token vector of dimension D is divided into H heads, where each head is a sub-vector containing D/H channels. The output at each head is computed as follows, with the scaling factor omitted for simplicity:\n$y_i = \\frac{\\sum_{j=1}^L e^{q_jk_j}v_j}{\\sum_{j=1}^L e^{q_jk_j}}$ (17)\nFour key comparisons can be made. (1) The SA uses dot-product operations to compute the similarity, while the EA uses element-wise squared Euclidean distance. (2) The SA generates a total of H head-level feature maps, while the EA generates D channel-level feature maps, allowing for more comprehensive interactions between tokens. (3) The exponential function introduces \u201cspikiness\u201d which contributes much to attention's effectiveness, while the exponential function incurs significant complexity. We use a Taylor polynomial to approximate the exponential function, preserving its effectiveness while reducing complexity. (4) During inference, SA caches all previous keys and values, causing its inference cost to scale with sequence length. Conversely, the EA-series achieves constant inference complexity with respect to the sequence length, enabling efficient long-sequence inference.\nLA As shown in equation 18, Linear Attention (LA) approximates the $exp(q_ik_j)$ in SA with kernels $\\phi(q_i) \\cdot \\phi(k_j)$, thereby achieving linear training complexity and constant"}, {"title": "5. Conclusion", "content": "In this work, we propose an element-wise attention mechanism as a replacement for SA. The mechanism substitutes the dot-product operation with element-wise operation for similarity computation and approximates the quadratic complexity term using a Taylor polynomial. By doing so, it simultaneously achieves SA-comparable performance and incredible efficiency (a training complexity of O(tLD) and an inference complexity of O(tD). The proposed model offers a foundation for the next-generation architecture and may have significant impact across various domains."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}