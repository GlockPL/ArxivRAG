{"title": "Extraction de relations multi-\u00e9tiquettes en utilisant des mod\u00e8les pr\u00e9-entra\u00een\u00e9s et des couches de Transformer", "authors": ["Ngoc Luyen Le", "Gildas Tagny Ngomp\u00e9"], "abstract": "Nous pr\u00e9sentons dans cet article le mod\u00e8le BTransformer18, une architecture d'apprentissage profond con\u00e7ue pour l'extraction de relations multi-\u00e9tiquettes dans des textes en fran\u00e7ais. Notre approche combine les capacit\u00e9s de repr\u00e9sentation contextuelle de mod\u00e8les de langage pr\u00e9-entra\u00een\u00e9s de la famille BERT, tels que BERT, ROBERTa, ainsi que leurs versions francophones CamemBERT et FlauBERT, avec la puissance des encodeurs Transformer pour capturer les d\u00e9pendances \u00e0 long terme entre les tokens. Les exp\u00e9rimentations men\u00e9es sur le jeu de donn\u00e9es du d\u00e9fi TextMine'25 montrent que notre mod\u00e8le atteint des performances sup\u00e9rieures, en particulier en utilisant CamemBERT-Large, avec un score F1-macro de 0,654, surpassant les r\u00e9sultats obtenus avec FlauBERT-Large. Ces r\u00e9sultats d\u00e9montrent l'efficacit\u00e9 de notre approche pour l'extraction automatique de relations complexes dans des rapports de renseignement.", "sections": [{"title": "1 Introduction", "content": "L'extraction de relations est une t\u00e2che fondamentale en traitement automatique du langage naturel (TALN), visant \u00e0 identifier et classifier les relations s\u00e9mantiques entre des entit\u00e9s nomm\u00e9es au sein d'un texte (Nasar et al., 2021). Dans le cas des rapports de renseignement, cette t\u00e2che rev\u00eat une importance particuli\u00e8re pour la structuration de l'information et la facilitation de l'analyse. Les approches traditionnelles reposent souvent sur des m\u00e9thodes d'apprentissage supervis\u00e9 n\u00e9cessitant des ressources annot\u00e9es consid\u00e9rables et peinent \u00e0 capturer les relations complexes pr\u00e9sentes dans les textes non structur\u00e9s.\nLe d\u00e9fi TextMine'25 (Prieur et al., 2024) fournit un jeu de donn\u00e9es pr\u00e9cieux pour faire progresser la recherche dans ce domaine. Ce jeu de donn\u00e9es est compos\u00e9 de 800 rapports de renseignement factices en fran\u00e7ais, annot\u00e9s avec des mentions d'entit\u00e9s, leurs types, attributs et relations. Il pose un d\u00e9fi significatif en raison de la complexit\u00e9 des relations et de la n\u00e9cessit\u00e9 de mod\u00e8les capables de g\u00e9rer la classification multi-\u00e9tiquettes.\nAvec l'\u00e9mergence des mod\u00e8les de langage pr\u00e9-entra\u00een\u00e9s, tels que ceux de la famille BERT, des am\u00e9liorations significatives ont \u00e9t\u00e9 r\u00e9alis\u00e9es dans diverses t\u00e2ches de TALN. BERT (Devlin, 2018) et ROBERTa (Liu, 2019) ont \u00e9tabli de nouveaux standards pour la langue anglaise, tandis que leurs \u00e9quivalents francophones, CamemBERT (Martin et al., 2019) et FlauBERT (Le et al., 2019), ont \u00e9t\u00e9 d\u00e9velopp\u00e9s pour traiter les sp\u00e9cificit\u00e9s de la langue fran\u00e7aise. Ces mod\u00e8les, membres de la famille BERT, exploitent un pr\u00e9-entra\u00eenement \u00e0 grande \u00e9chelle sur des corpus massifs pour produire des embeddings contextualis\u00e9s riches.\nParall\u00e8lement, les encodeurs Transformer (Vaswani, 2017; Wolf et al., 2020) ont d\u00e9montr\u00e9 une capacit\u00e9 exceptionnelle \u00e0 mod\u00e9liser les d\u00e9pendances \u00e0 long terme dans les s\u00e9quences gr\u00e2ce aux m\u00e9canismes d'attention. Combiner les forces des mod\u00e8les de langage pr\u00e9-entra\u00een\u00e9s et des encodeurs Transformer offre une direction prometteuse pour am\u00e9liorer les capacit\u00e9s d'extraction de relations.\nDans cet article, nous introduisons BTransformer18, un mod\u00e8le qui int\u00e8gre les mod\u00e8les de langage pr\u00e9-entra\u00een\u00e9s de la famille BERT avec des encodeurs Transformer pour l'extraction de relations multi-\u00e9tiquettes dans des textes en fran\u00e7ais. Notre architecture comprend trois couches principales : Embeddings Contextuels, o\u00f9 nous obtenons les embeddings des tokens en utilisant des mod\u00e8les comme CamemBERT et FlauBERT; Encodeurs Transformer, qui capturent les d\u00e9pendances complexes entre les tokens; et Agr\u00e9gation et Classification, o\u00f9 nous agr\u00e9geons les repr\u00e9sentations et pr\u00e9disons les relations.\nNous \u00e9valuons notre mod\u00e8le sur le jeu de donn\u00e9es du d\u00e9fi TextMine'25, en r\u00e9alisant des exp\u00e9rimentations approfondies pour \u00e9valuer l'impact des diff\u00e9rents mod\u00e8les pr\u00e9-entra\u00een\u00e9s sur la t\u00e2che. Nos r\u00e9sultats montrent que l'utilisation de CamemBERT-Large conduit \u00e0 des gains de performance significatifs, atteignant un score F1-macro de 0,654, surpassant les mod\u00e8les bas\u00e9s sur FlauBERT-Large.\nDans la section 2, nous d\u00e9taillons notre approche propos\u00e9e en d\u00e9crivant l'architecture de BTransformer18. La section 3 pr\u00e9sente le cadre exp\u00e9rimental, y compris le jeu de donn\u00e9es, les \u00e9tapes de pr\u00e9traitement, les proc\u00e9dures d'entra\u00eenement, et des r\u00e9sultats de notre mod\u00e8le. Enfin, dans la section 4, nous concluons et proposons des perspectives pour de futures recherches."}, {"title": "2 Approche propos\u00e9e", "content": "Notre approche, illustr\u00e9e dans la figure 1, s'appuie sur la logique de fine-tuning d'un mod\u00e8le de langage pr\u00e9-entra\u00een\u00e9, compos\u00e9e d'un body (g\u00e9n\u00e9raliste pour la langue ou le domaine) et d'une head (sp\u00e9cifique \u00e0 la t\u00e2che d'extraction de relations). Dans notre mod\u00e8le, le body est assur\u00e9 par CamemBERT-Large, choisi pour sa sp\u00e9cialisation en fran\u00e7ais. La head de classification inclut une couche cach\u00e9e bas\u00e9e sur l'architecture Transformer (Vaswani, 2017), permettant d'exploiter le m\u00e9canisme d'attention pour l'extraction de relations. La sortie est ensuite produite par une simple couche dense, qui assure la classification multi-label."}, {"title": "2.1 Embeddings contextuels", "content": "Un des mod\u00e8les de langage pr\u00e9-entra\u00een\u00e9s de la famille BERT (par exemple, BERT, CamemBERT, FlauBERT, etc.) est employ\u00e9 afin d'obtenir les embeddings contextuels des tokens du texte d'entr\u00e9e. Soit $X = [x_1, x_2,...,x_T]$ la s\u00e9quence de tokens, o\u00f9 T est la longueur de la s\u00e9quence. Ces mod\u00e8les de langage pr\u00e9-entra\u00een\u00e9s g\u00e9n\u00e8re pour chaque token $x_t$ un embedding contextuel $h_t \\in R^d$, o\u00f9 d est la dimension cach\u00e9e du mod\u00e8le, c'est-\u00e0-dire la taille du vecteur latent (par exemple, 768 pour la version BERT-base et 1024 pour la version BERT-Large)."}, {"title": "2.2 Encodeurs transformer", "content": "Les embeddings contextuels H sont ensuite transmis \u00e0 travers L couches d'encodeurs Transformer (Vaswani, 2017), afin de capturer les d\u00e9pendances \u00e0 long terme entre les tokens. Cette architecture permet de mod\u00e9liser efficacement la structure du texte et de faire \u00e9merger des repr\u00e9sentations de plus en plus riches \u00e0 mesure que les couches s'empilent. Chaque couche l transforme la repr\u00e9sentation $Z^{(l-1)}$ en $Z^{(l)}$ comme suit :\n$Z^{(l)} = TransformerEncoder^{(l)}(Z^{(l-1)}), \\qquad l = 1, ..., L,$\noalign{\\bigskip}\noalign{\\bigskip}\noalign{\\bigskip}\noalign{\\bigskip}o\u00f9 $Z^{(0)} = H$. Le fonctionnement de chaque couche peut \u00eatre d\u00e9compos\u00e9 en quatre \u00e9tapes principales: Attention multi-t\u00eates, 1er Add & Norm, Feed-forward positionnel et 2\u00e8me Add & Norm.\nAttention multi-t\u00eates : La premi\u00e8re \u00e9tape consiste \u00e0 appliquer un m\u00e9canisme d'attention multi-t\u00eates, not\u00e9 MultiHeadAttention, sur $Z^{(l-1)}$ :\n$A^{(l)} = MultiHeadAttention(Z^{(l-1)})$"}, {"title": "1er Add & Norm", "content": "Une fois la matrice d'attention $A^{(l)}$ calcul\u00e9e, on l'additionne \u00e0 l'entr\u00e9e de la couche $Z^{(l-1)}$. Cette somme est ensuite normalis\u00e9e :\n$U^{(l)} = LayerNorm(Z^{(l-1)} + A^{(l)})$\nL'addition permet de conserver les informations initiales (via le skip connection), tandis que la normalisation de couche (LayerNorm) (Ba, 2016) stabilise l'apprentissage et facilite la convergence."}, {"title": "Feed-forward positionnel", "content": "Chaque position dans $U^{(l)}$ est ensuite trait\u00e9e de mani\u00e8re ind\u00e9pendante par un r\u00e9seau pleinement connect\u00e9, souvent appel\u00e9 position-wise feed-forward network (FFN). On calcule :\n$F^{(l)} = FFN(U^{(l)})$\nCe r\u00e9seau permet d'enrichir localement la repr\u00e9sentation de chaque token, en transformant lin\u00e9airement puis en appliquant une fonction d'activation non lin\u00e9aire (par exemple, ReLU)."}, {"title": "2\u00e8me Add & Norm", "content": "Enfin, on ajoute la sortie du feed-forward $F^{(l)}$ \u00e0 $U^{(l)}$, puis on normalise \u00e0 nouveau :\n$Z^{(l)} = LayerNorm(U^{(l)} + F^{(l)})$\n\u00c0 l'issue de cette \u00e9tape, on obtient la sortie finale de la l-i\u00e8me couche, qui sert d'entr\u00e9e \u00e0 la couche suivante. En r\u00e9p\u00e9tant ces op\u00e9rations L fois, l'encodeur Transformer parvient \u00e0 agr\u00e9ger des informations \u00e0 diff\u00e9rentes \u00e9chelles, am\u00e9liorant ainsi sa capacit\u00e9 \u00e0 mod\u00e9liser les relations s\u00e9mantiques et structurelles pr\u00e9sentes dans la s\u00e9quence d'entr\u00e9e."}, {"title": "2.3 Agr\u00e9gation et classification", "content": "La sortie du dernier encodeur Transformer est une s\u00e9quence de repr\u00e9sentations $Z^{(L)} = [z_1, z_2, \u2026\u2026\u2026, z_T]$, o\u00f9 $z_t \\in R^d$ repr\u00e9sente la repr\u00e9sentation contextuelle du t-i\u00e8me token. Pour r\u00e9duire cette s\u00e9quence \u00e0 un unique vecteur refl\u00e9tant l'information globale, nous effectuons une agr\u00e9gation par moyenne :\n$z_{pool} = \\frac{1}{T} \\sum_{t=1}^T z_t$"}, {"title": "3 Exp\u00e9rimentations et r\u00e9sultats", "content": "Dans cette section, nous d\u00e9crivons les exp\u00e9rimentations men\u00e9es pour \u00e9valuer les performances de notre mod\u00e8le BTransformer18 sur la t\u00e2che d'extraction de relations multi-\u00e9tiquettes. Nous pr\u00e9sentons d'abord le jeu de donn\u00e9es utilis\u00e9, puis les d\u00e9tails de l'impl\u00e9mentation et des param\u00e8tres d'entra\u00eenement. Enfin, nous discutons des r\u00e9sultats obtenus."}, {"title": "3.1 Jeu de donn\u00e9es et pr\u00e9traitement des donn\u00e9es", "content": "Le jeu de donn\u00e9es utilis\u00e9 pour les exp\u00e9rimentations est constitu\u00e9 de 800 rapports de renseignement factices fournis dans le cadre du d\u00e9fi TextMine'25 (Prieur et al., 2024). En g\u00e9n\u00e9ral, les donn\u00e9es d'entr\u00e9e se composent d'un texte, des mentions et des types des entit\u00e9s ainsi que des attributs associ\u00e9s.\nPar rapport \u00e0 la tokenisation : les textes sont tokenis\u00e9s en utilisant le tokenizer associ\u00e9 au mod\u00e8le de language pr\u00e9-entra\u00een\u00e9, garantissant une correspondance optimale avec les embeddings. Les entit\u00e9s annot\u00e9es sont align\u00e9es avec les tokens pour former des paires d'entit\u00e9s potentielles $(e_i, e_j)$.\nSur la construction des Paires d'Entit\u00e9s : pour chaque document, nous g\u00e9n\u00e9rons toutes les paires possibles d'entit\u00e9s annot\u00e9es, o\u00f9 $e_i$ et $e_j$ sont des entit\u00e9s du texte. Chaque paire est associ\u00e9e \u00e0 un vecteur de labels multi-\u00e9tiquettes $y_{ij} \\in \\{0,1\\}^C$, indiquant les relations existantes entre $e_i$ et $e_j$."}, {"title": "3.2 Entra\u00eenement du mod\u00e8le", "content": "Fonction de perte: La Binary Cross-Entropy (BCE) est utilis\u00e9e comme fonction de perte pour la classification multi-\u00e9tiquettes. Cette fonction est d\u00e9finie par l'\u00e9quation suivante :\n$L = - \\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^C [y_{nc} log(\\hat{y}_{nc}) + (1 - y_{nc}) log(1 - \\hat{y}_{nc})],$"}, {"title": "3.3 R\u00e9sultats", "content": "Les r\u00e9sultats obtenus par notre mod\u00e8le sont pr\u00e9sent\u00e9s dans le tableau 2, avec une comparaison entre deux mod\u00e8les de langage pr\u00e9-entra\u00een\u00e9s pour le fran\u00e7ais. Les exp\u00e9rimentations ont \u00e9t\u00e9 r\u00e9alis\u00e9es en utilisant le mod\u00e8le BTransformer18, en suivant les configurations et les hyperparam\u00e8tres d\u00e9crits dans la table 3.2.\nLes r\u00e9sultats montrent que le mod\u00e8le BTransformer18 utilisant CamemBERT-Large obtient un score de 0,654, tandis que celui utilisant FlauBERT-Large atteint un score de 0,620, indiquant une am\u00e9lioration de 3,4 points de pourcentage gr\u00e2ce \u00e0 l'int\u00e9gration de CamemBERT-Large. Comme illustr\u00e9 dans la figure 2, les courbes d'entra\u00eenement de BTransformer18 avec CamemBERT montrent une convergence rapide des pertes d'entra\u00eenement et de validation au cours des premi\u00e8res \u00e9poques, suivie d'une stabilisation. Simultan\u00e9ment, la F1-mesure macro de validation progresse rapidement avant d'atteindre un plateau, indiquant une am\u00e9lioration significative des performances de classification multi-\u00e9tiquettes. L'\u00e9cart mod\u00e9r\u00e9 entre les pertes d'entra\u00eenement et de validation met en \u00e9vidence une bonne g\u00e9n\u00e9ralisation du mod\u00e8le, sans signe de surapprentissage, m\u00eame apr\u00e8s plusieurs \u00e9poques. Ces r\u00e9sultats soulignent l'efficacit\u00e9 de BTransformer18 \u00e0 exploiter les repr\u00e9sentations contextuelles riches de CamemBERT-Large, permettant d'apprendre des relations complexes tout en maintenant des performances stables sur les donn\u00e9es de validation."}, {"title": "4 Conclusion et perspectives", "content": "Dans cet article, nous avons pr\u00e9sent\u00e9 le mod\u00e8le BTransformer18, une architecture combinant des mod\u00e8les de langage pr\u00e9-entra\u00een\u00e9s francophones, tels que CamemBERT-Large et FlauBERT-Large, avec des couches Transformer pour l'extraction de relations multi-\u00e9tiquettes dans des rapports de renseignement. Les r\u00e9sultats exp\u00e9rimentaux ont d\u00e9montr\u00e9 la sup\u00e9riorit\u00e9 de CamemBERT-Large, qui a obtenu un score F1 macro sup\u00e9rieur \u00e0 celui de FlauBERT-Large. L'analyse des courbes d'entra\u00eenement a mis en \u00e9vidence une convergence rapide et une bonne g\u00e9n\u00e9ralisation, montrant l'efficacit\u00e9 du mod\u00e8le pour capturer des relations complexes tout en \u00e9vitant le surapprentissage. En exploitant les avanc\u00e9es r\u00e9centes en traitement du langage naturel, notre mod\u00e8le d\u00e9montre sa capacit\u00e9 \u00e0 relever le d\u00e9fi de l'extraction automatique de relations complexes, tout en maintenant une classification pr\u00e9cise gr\u00e2ce aux couches Transformer.\nBien que les r\u00e9sultats obtenus soient prometteurs, plusieurs axes d'am\u00e9lioration peuvent \u00eatre explor\u00e9s dans de futurs travaux. Tout d'abord, l'enrichissement des donn\u00e9es avec des corpus suppl\u00e9mentaires ou annot\u00e9s dans d'autres domaines pourrait renforcer la robustesse du mod\u00e8le et am\u00e9liorer sa g\u00e9n\u00e9ralisation. Par ailleurs, l'incorporation de graphes de connaissances ou l'utilisation de mod\u00e8les d'apprentissage par graphes pourrait am\u00e9liorer la mod\u00e9lisation des relations complexes entre entit\u00e9s. De plus, l'int\u00e9gration de grands mod\u00e8les de langage (Large Language Models, LLMs), comme GPT (Achiam et al., 2023), Mistral (Jiang et al., 2023), LLama (Touvron et al., 2023), et des autres, pourrait offrir des repr\u00e9sentations contextuelles encore plus riches et dynamiques, notamment pour des relations subtiles ou rares. Enfin, des techniques d'augmentation des donn\u00e9es, combin\u00e9es \u00e0 des m\u00e9thodes de r\u00e9gularisation avanc\u00e9es, ainsi qu'une optimisation des ressources computationnelles, permettraient de d\u00e9velopper des mod\u00e8les plus robustes et efficaces, adapt\u00e9s \u00e0 des applications en temps r\u00e9el ou dans des environnements \u00e0 ressources limit\u00e9es. Ces pistes visent \u00e0 \u00e9largir l'applicabilit\u00e9 du mod\u00e8le tout en am\u00e9liorant ses performances dans des t\u00e2ches d'extraction de relations complexes."}]}