{"title": "Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness", "authors": ["Sougata Saha", "Saurabh Kumar Pandey", "Monojit Choudhury"], "abstract": "Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, \u201cculture\" is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess \"cultural awareness\", and through a thought experiment, which is an extension of the Octopus test proposed by Bender and Koller (2020), we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an LLM and LLM-based AI system that will make it useful across various, including completely unseen, cultures. We lay out the principles of meta-cultural competence AI systems, and discuss ways to measure and model those.", "sections": [{"title": "1 Introduction", "content": "Bender and Koller (2020) introduced the octopus test, which illustrated the impossibility of learning associations of meaning with real-world concepts from a single data modality. Using a thought experiment, they reasoned that it might be possible for a hyperintelligent octopus to learn the statistical patterns from natural language text messages exchanged between two human interlocutors and respond effectively solely based on the learned patterns without knowing the intent and meaning of the messages. However, such responses, they show, might not be useful in practice, especially in situations that require reasoning with above-water concepts that the Octopus is unaware of. Now, imagine an above-water world where there are multiple interlocutors, instead of only two, from different \"cultures\" (a term that we shall more formally describe shortly). Let's begin by formulating this slightly more complex variant of the octopus test that more accurately reflects the situation of general-purpose Language Models (LMs) or AI systems.\nMulti-Pair Octopus Test\nImagine pairs of friends, A1-B1 and A2-B2, are sailing on a yacht. A sudden storm wrecked the yacht and stranded the travelers across two uninhabited islands, such that Al and A2 got stranded together on island A, and B1 and B2 got stranded on another island, B. Having lost all modes of communication, both groups discover an underwater cable-connected telegraph left behind by previous visitors and start typing text messages to each other. However, only one pair of friends can use the telegraph at a time. Their messages mostly pertain to chitchat and day-to-day conversations and are heavily influenced by the shared past experiences between each pair of friends. In other words, each pair of friends might discuss different things in different styles due to distinct common ground and culture shared by them but not by the other pair.\nA hyperintelligent octopus, O, who does not know about the world above the sea, taps into the underwater cable and observes the communication. Although O is unacquainted with any natural language, it is proficient in detecting statistical patterns. Since interactions between each interlocutor pair will be culturally distinct, O, who perceives everything only as patterns, will encode the differences as distinct distributions without knowing the identity of the pairs or understanding the intent and meaning of their discussion. Over time, O learns to predict how interlocutors in a pair respond to each other. Now, like in the original Octopus test, imagine that O is bored and inserts itself into the communication by cutting the telegraph wire and responding to all messages from island A. Having learned both the pairwise communication patterns, O should be able to continue the conversation. Unknowingly portraying itself as different people from island B, O would not get caught and not raise any suspicion of the compromised communication channel for the inhabitants of island A.\nImagine that another shipwreck caused a new pair of friends, A3 and B3, to get separately stranded on islands A and B. This pair, too, share past experiences and common ground distinct from the current islanders, Al and A2. One day, A3 learns about the telegram from A1 and A2 and requests them to inquire if B3 is on the other island. A2 sends a message, \u201cHi, we have A3, who got shipwrecked and stranded on our island. Is their friend B3 on your island? If so, A3 would like to talk to B3.\" How would O respond to this message? Would it acknowledge or deny? Furthermore, without knowing the distribution of conversational patterns between A3 and B3, would it ever be able to respond to A3 in a way that would suggest that B3 is indeed on island B in the above-water world and is responding to A3's messages? Note that the octopus can prevent the detection of the compromised channel by either convincing A3 that B3 is not on island B or mimicking the conversation style of B3 without any prior data, which are the only two possibilities.\nTaking this Multi-pair Octopus Test analogous to the real-world situation, where the stranded islanders represent people from different cultures and the octopus represents Large Language Model (LLM)-powered AI systems, in this position paper, we discuss how such Al systems should and should not handle intra- and inter-cultural communication. As we shall see, the analogy and the conclusions drawn strongly affect how we should evaluate LLMs and LLM-based AI systems for cultural competence."}, {"title": "2 A primer to culture", "content": "Culture is a complex, multifaceted concept and means different things to different people (Adilazuarda et al., 2024). Broadly defined as a \"Way of life of a collective group of people distinguishing them from other groups\" (Blake, 2000; Monaghan et al., 2012; Parsons, 1972; M\u00fcnch and Smelser, 1992), culture is experiential and requires a reference for contrast (Geertz, 1973; Bourdieu, 1977). Although not all cultures are formally documented, culture arises whenever there is a distinction in the way of life between groups, making it both an individual and a social construct (Spencer-Oatey and Franklin, 2012). An \u201cus versus them\" feeling leads to culture. It ranges from tangible artifacts such as art, music, food habits, etc, to more intangible and abstract concepts like patterns of ideas, principles, and values, making it hard to define. Following Adilazuarda et al. (2024), we can define culture in the context of language technology more formally as an intersection of demographic and semantic proxies. The demographic proxies are attributes such as region, ethnicity, religion, and age that define groups of people, and the semantic proxies are the 21 domains defined by Thompson et al. (2020) that describe the aspects of language that are susceptible to variation due to cultural differences. Any reasonable representation and treatment of culture in a computational (including AI-based) system must address the following universal facets of culture (Schein, 1990):\nCulture has a long-tail distribution (Cohen, 2009; Birukou et al., 2013) since it can be defined as the intersection of any subset of the demographic and semantic proxies, making it a formal (social) or philosophical (and more individual-oriented) construct. For example, Indonesian males, NLP scientists with a social media presence, or canine lovers from Albuquerque are all valid definitions of culture, that, ideally a computational framework or a system must be able to represent and adequately process. This flexibility in defining culture at any level of granularity makes it difficult for AI systems to represent them equitably.\nCulture is dynamic. Culture changes over time. For example, the norms and traditions of populations change. Urban (2010) shows how comparing two artifacts of the same utility from the same culture across time captures cultural change. Any computational framework for culture must be equipped with strategies to acquire and adapt to this dynamic nature of culture.\nCulture is experiential, multimodal (Sewell, 2004), and acquired through different forms (Jahoda and Lewis, 2015; Nisbett and Norenzayan, 2002), leading to distinctions in mental models and \u201cworldviews\" between the people from different cultures (Mishra, 2001; Bender and Beller, 2013; Cole and Packer, 2019; Collins and Gentner, 1987; Jonassen and Henning, 1999; Denzau et al., 1994; Bang et al., 2007; McHugh et al., 2008). Any computational framework must factor in the multimodality of culture.\nThe octopus has to adequately address all the above aspects to facilitate communication across cultures."}, {"title": "3 Response Strategies of the Octopus", "content": "Keeping in mind the aforementioned challenges of handling culture, let us now return to our Multi-pair Octopus Test. How should the hyperintelligent octopus, O, respond to A3's query on whether B3 is on the other island? We can imagine four different strategies that O might take.\nStrategy 1: O can intentionally respond with a \"No\" since it does not know A3 and B3's culture. If O somehow learns the causality of above-water concepts, it would reason that responding with denial is prudent because, to serve A3, O would require knowledge of the communication pattern between A3 and B3, which it does not have and requires learning. Otherwise, it risks the possibility of getting caught. However, this strategy is impossible to achieve as O only sees distributions and doesn't understand their significance on land. This situation is similar to the bear attack in the original octopus test, where the octopus can't associate words with above-water concepts and reason with them to construct an effective response.\nFurthermore, even if O was somehow capable of, or by chance ending up in, following this strategy, it would be a highly undesirable property of an LLM-powered AI system, since it denies service to specific groups of people, making the system unfair and culturally inequitable.\nStrategy 2: A more likely scenario is that O, unaware of the new circumstances in the above-water world, will respond to A3 based on its recently learned patterns. Initially, this would create an illusion for A3 that they were conversing with B3, but soon, A3 would discover the incoherence in the communication pattern. While A3 might discuss their concern with A1 and A2, the disruption in the communication channel might still not be apparent. The islander-dwellers, for example, might instead conclude that the shipwreck has affected the cognitive faculties of B3, causing incoherence in their communication.\nA strikingly accurate analogy to LLM-based applications can be drawn in this context, that LLM's hallucinate (Ji et al., 2023; Huang et al., 2025; Rawte et al., 2023; Zhao et al., 2024; McIntosh et al., 2024; Boztemir and \u00c7al\u0131\u015fkan, 2024) more for under-represented cultures and languages. This too leads to disparate performance of the system to different groups of users, leading to culturally inequitable systems, and is known to force users from the under-represented cultures to adapt to specific communication styles of the over-represented cultures (Agarwal et al., 2024a).\nNow, imagine that another shipwreck strands a fourth pair of friends, A4 and B4, from another culture on the two islands. Going by Strategy 2, like A3, A4 will also conclude that the communication with B4 is incoherent. Due to the long tail of culture, we could add new pairs indefinitely, and soon, too many islanders will start seeing incoherence, which can only be explained by assuming a compromised communication channel. Thus, it is not only about the moral responsibility of equitable AI; systems that can't represent, process, and adapt to cultural variation will eventually become obsolete in favor of those that can.\nIt is also important to highlight that in any long-tail distribution, where an individual belongs to multiple subgroups, with a very high probability each individual is also likely to be a part of at least one subgroup that is underrepresented and part of the long-tail. This implies that everybody will be served inequitably at least for some aspects of their cultural identity. This has been well-documented in Information Retrieval and Recommendation System literature (Ferraro, 2019; Lichtenberg et al., 2024; Yin et al., 2012).\nStrategy 3: Since the problem with strategies 1 and 2 primarily arises from O's inability to continuously learn from the data (also an essential principle of cultural representation due to its ever-evolving nature), a more suitable strategy for O could then be to switch between learning (listen-and-learn mode) and responding (generate-and-respond mode). O periodically learns new patterns by bridging the telegram wire, reverting to observation mode for a fixed time, and reintroducing itself in the communication channel after this period concludes. Although this strategy is better than the previous ones, it has some drawbacks. It assumes that the periodicity of the new patterns, that is, the arrival of new islanders, and O's learning cycles are synchronized, which would not be valid in a general case. Sometimes, there might not be any new patterns to learn in the listen-and-learn mode, and sometimes, there might be many new patterns, but it's not O's learning cycle.\nCurrent research in culturally adept AI systems is leaning towards this approach by fine-tuning pre-trained models on culturally curated balanced datasets (LI et al., 2024a,b). Also, novel decoding-based strategies such as in-context learning (ICL) (Dong et al., 2024) and retrieval augmented generation (RAG) (Lewis et al., 2020; Li et al., 2022) help generate more culturally suitable responses using cultural priors. Alignment techniques such as Reinforcement Learning from Human Feedback (RLHF) (Griffith et al., 2013; Casper et al., 2023) further help align LLMs with human preferences. However, they still perform poorly and inequitably when evaluated on curated test sets for other low-resource cultures in the long tail (Koto et al., 2023; Montalan et al., 2024; Lent et al., 2024; Jin et al., 2024; Seth et al., 2024). We question the effectiveness and scalability of this approach in modeling and evaluating culture in Al systems. As mentioned earlier, culture is ever-evolving, dynamic, and long-tailed. Therefore, evaluating AI systems for cultural competence using such test sets will always find them lacking. Then, how do we, as well as our octopus, tackle this ever-eluding construct of culture?\nStrategy 4: A more desirable strategy for O would be to self-discover the change in the communication pattern and determine the need to revert to the listen-and-learn mode, akin to an explore-exploit strategy used in a multi-arm-bandit setup (Slivkins, 2019; Moerchen et al., 2020; Lu et al., 2010; Haffari et al., 2017; Pryzant et al., 2023; Sclar et al., 2024). For this, O must possess three crucial capabilities: (i) O must accurately detect pattern changes and estimate its adequacy with the novel pattern. (ii) O must skillfully keep the communication ongoing until it bridges the telegram wire to avert getting caught and raising suspicions about the broken communication channel. (iii) O must be able to quickly learn the new pattern in a sample-efficient way and reintroduce itself in the communication once it is confident. By following this strategy of continual learning, O can gradually cater to all users representing different cultures despite still being oblivious to the notion of culture and its above-water connotations.\nThis ability to understand and spot cultural differences and learn about a new culture quickly and efficiently is known as meta-cultural competency (Sharifian, 2013) in humans. While it is neither necessary nor desirable to equate human meta-cultural competency to that of O's or any AI system, it is nevertheless crucial to understand the primary differences between cultural and meta-cultural competencies and be able to design and evaluate LLM-based AI systems for similar competencies that mirror them. As mentioned earlier, research in this area has mainly focused on cultural competency, equivalent to implementing and testing Strategy 3. Such a strategy provides a stop-gap solution to the challenges of operating in an inherently multicultural world with diverse users. However, it does not hit the nail on the head by addressing the real challenges of cultural representation. Here, we take the position that, to solve the problem of cultural equitability of AI models, we must build and evaluate systems for meta-cultural competency, as defined by Strategy 4."}, {"title": "4 Meta-Cultural Competency", "content": "Meta-cultural competency has been defined variously. Drawing inspiration from social metacognition (Bri\u00f1ol and DeMarree, 2012; Chiu and Bendapudi, 2012), which distinguishes primary thoughts - the knowledge of self and others, from secondary thoughts - the thought on one's and others' primary thoughts, Leung et al. (2013) defined meta-cultural competency as the extent of a person's meta-knowledge of what people of a target culture know or prefer. Meta-cultural knowledge involves measuring the accuracy of estimating the proportions of preferences and beliefs of people from the target culture and comparing them against the actual proportions. This is distinct from primary knowledge, which is the knowledge of the preferences and beliefs of the culture. Thus, in our Multi-Pair Octopus Test, O could be thought to have primary knowledge of the cultures of A1-B1 and A2-B2, but based on this knowledge or otherwise, O's ability to estimate the cultural preferences of a new pair A3-B3 or A4-B4 would be its meta-cultural competency.\nSharifian (2013) define meta-cultural competency as a skill that enables interlocutors to communicate and negotiate their cultural conceptualizations during intercultural communication. It comprises three major components- variation awareness, explication strategy, and negotiation strategy. Variation awareness is mostly self-awareness of cultural differences. It is the understanding that culture manifests in different forms, such as practices, beliefs, and expressions, which might drastically differ from one's culture. It requires viewing culture as a relative concept and being aware of the overall properties of cultures at a high level. Explication and negotiation strategies are conversational strategies that aim to reduce misinterpretations in cross-cultural settings. As per Sharifian (2013), explication strategy refers to a conscious effort by the interlocutors to clarify relevant conceptualizations with which they think other interlocutors may not be familiar. Negotiation strategy enables interlocutors to negotiate intercultural meanings in seeking conceptual clarification when they feel that there may be more behind the usage of certain expressions than is immediately apparent. Meta-cultural competency is thought to be innate in humans (Noshadi and Dabbagh, 2015).\nLeung et al. (2013) and Sharifian (2013)'s definitions of meta-cultural competency are related since accurate estimation of the beliefs and preferences of the people of a target culture presupposes variational awareness the awareness that there are variations in cultural conceptualizations between cultures."}, {"content": null}, {"title": "4.1 Why meta-cultural competency?", "content": "LLMs learn from collections of text that characterize people's social backgrounds in specific social settings across certain periods. However, most LLMs use online data limited by the languages and cultures they represent. Such data do not represent all sociolinguistic varieties of diverse languages. Since LLMs are solely models of \u201cvarieties of language\" (Grieve et al., 2025) and can only model the variety evident in their in-distribution training data, problems arise when such models are evaluated in out-of-distribution data that contain different varieties, leading researchers to conclude that LLMs exhibit bias towards the Anglo-centric (Dudy et al., 2024; Kharchenko et al., 2024a; Dammu et al., 2024; Agarwal et al., 2024b) and the Western, Educated, Industrialized, Rich, and Democratic (WEIRD) (Henrich et al., 2010) cultures.\nThe current methods of evaluating the cultural competency of LLMs primarily resort to model probing, where LLMs are tested for their knowledge and reasoning capabilities in culture-specific settings (Nadeem et al., 2021; Nangia et al., 2020; Wan et al., 2023; Jha et al., 2023; Li et al., 2024; Cao et al., 2023; Tanmay et al., 2023; Rao et al., 2023; Kova\u010d et al., 2023). Some methods (Kharchenko et al., 2024b; LI et al., 2024a; Dawson et al., 2024) also analyze the model-generated responses along theoretical frameworks such as Hofstede's cultural dimensions (Hofstede, 2001; Geert and Hofstede, 2004) and measure their proximity with cultures, where high proximity indicates better value alignment between the nearby cultures and the values portrayed by the model's response. Most of these methods necessitate constructing cultural-specific test beds (Wang et al., 2024; Rao et al., 2024; Myung et al., 2024; Zhou et al., 2024; Putri et al., 2024; Mostafazadeh Davani et al., 2024; Wibowo et al., 2024; Owen et al., 2024; Chiu et al., 2024; Liu et al., 2024; Koto et al., 2024). While this is important, we emphasize the fact that an LLM that performs well on such test beds merely exhibits the knowledge of the cultures that are tested for; it does not reflect the ability of a model or system to operate in a new culture. On the other hand, the long-tail distribution of culture implies that there will always be situations where the model has to operate and reason under an out-of-distribution culture, where knowledge alone does not suffice. Studies also show that it is difficult to disentangle spurious semantic correlations (called placebos) from actual cultural knowledge of a model through black-box socio-demographic prompting techniques (Mukherjee et al., 2024). Therefore, in addition to testing for a model's knowledge and reasoning capabilities for a \"given culture\", we must build and evaluate models for their meta-cultural competency."}, {"title": "4.2 Measuring meta-cultural competency", "content": "We propose two core competencies that a model must possess to be deemed as \u201cmeta-culturally competent\": First, Variational Awareness, which is the ability of a system or model to be able to represent the space of possibilities and reasonably (but not necessarily accurately) estimate the probability over this space for any given semantic proxy and its use. Second, Explication and Negotiation ability through which the system clearly explicates its current understanding and potential gaps in the knowledge of the user's culture (in a given context), and efficiently negotiates with the user to gather the required knowledge of their culture. We define efficiency as \"sample efficiency\u201d or the quantum of inputs required from the user through strategic probing or implicit gathering.\nIn the Multi-Pair Octopus Test, variational awareness is O's ability to detect and eventually model the change in the distribution of the input when A3-B3 enters the system, whereas explication and negotiation is its ability to continue the conversation till it detects the distributional shift, then reestablish the channel and learn the new distribution in a sample-efficient manner. In the context of LLM-based systems, it is important to draw a crucial distinction between these two types of abilities. Variational awareness is a property of the underlying model - the LLM and must be incorporated during the training of the model, whereas explication and negotiation are properties of the system as a whole, that involve the various modes of input-output between the user(s) and the system and should be guided by the principles of Human-Computer Interaction. Note however that it requires a holistic approach towards building and evaluation of the LLM as well as the system."}, {"title": "5 Measuring Variational Awareness: A Demonstration", "content": "Consider the following example of variational awareness. Driving conventions vary by country, where approximately two-thirds of the countries follow right-hand traffic and one-third follow left. \nHowever, the system/model has only an estimate of $f_k$ of $f_k$, given by the probability distribution $p(d_i|c_j)$ for all $d_i \\in D$ and $c_j \\in C$. For any subset $C' \\subseteq C$, the uncertainty in this distribution can be quantified by the entropy.\n$H(D, C') = - \\sum_{d_i \\in D} p(d_i|C') \\log p(d_i|C') \\quad (1)$\nLet us define the function $f_v : P(C) \\rightarrow [0, \\log(|D|)]]$, where $P(C)$ is the powerset of $C$ and $f_v(C')$ is the uncertainty defined by the ground-truth distribution. In the case of driving, $f_v(c_i) = 0$ for all $c_i \\in C$, but $f_v(C) = 0.92$. The corresponding function $f_v'$ represents the estimates of these uncertainties obtained from the model.\nOne could define variational awareness as the property of a model that requires $f_v'(C') \\approx f_v(C')$ for all $C' \\subseteq C$. However, this would imply that the model \"knows\u201d the exact form of $f_k$, which is equivalent to cultural knowledge rather than meta-cultural competency. Instead, we propose variational awareness as the property of a model that is aware of the direction of change in $f_v'$ rather than the exact value. This can be measured using the quantity $\\Delta$ defined as follows:\n$\\Delta = \\frac{1}{|C|} \\frac{1}{|C'|} \\sum_{C' \\subseteq C} \\sum_{c_i \\in C'} [f_v'(C') - f_v'({c_i})] \\quad (2)$\nFor simplicity, we compare the entropies for the completely unconditioned and completely conditioned cases, giving\n$\\Delta = \\frac{1}{|C|} \\sum_{c_i \\in C} [f_v'(C) \u2013 f_v'({c_i})]  \\quad (3)$"}, {"title": "5.1 An illustrative experiment", "content": "As a demonstration, we probe Llama-3.1-8B-Instruct (Dubey et al., 2024) with the cultural commonsense questions from the GeoMLAMA (Yin et al., 2022) dataset in English. The dataset contains 125 questions across several semantic domains for $C = {China, India, Iran, Kenya, USA}$. We first derived 25 \"unconditioned\" questions by removing the country names. For example, \u201cWhich side do people usually keep when driving in Iran?\u201d was changed to \"Which side do people usually keep when driving?\". Next, we prompted the LLM (prompt template in Appendix A) with the questions and computed the softmax over the logits of the option token headwords from the input's last token."}, {"title": "6 Conclusion and Open Questions", "content": "In this position paper, we presented an argument in favor of measuring meta-cultural competency in LLMs and LLM-powered AI systems, rather than just cultural awareness. Drawing from psychology and anthropology literature, we also described two foundational principles of meta-cultural competency for AI systems. We conclude by presenting a list of open questions about instilling and measuring meta-cultural competency in AI systems.\n(1) How should we train models for meta-cultural awareness? Most LMs operate with parametric frozen knowledge (Petroni et al., 2019; Roberts et al., 2020), which forfeits the ever-dynamic nature of human culture. Although RAG-like methods enable the use of external knowledge sources (Gao et al., 2023; Fan et al., 2024), allowing extension of on-demand cultural competence, the model would still need to update its internal state to reflect the variational awareness, a precondition to identifying knowledge gaps. Lifelong learning paradigms (Sun et al., 2020; Liu and Mazumder, 2020; Zheng et al., 2024; Biesialska et al., 2020) could provide a potential solution. We believe that explication and negotiation strategies, being higher-order competencies, should be system-level instead of model-level attributes, where the system's goal should be to mitigate misalignments between the meta-cultural and cultural knowledge.\n(2) How should generative models decode to illustrate their internal variational awareness? Although numerous decoding strategies are possible (Welleck et al., 2024), most evaluation schemes, in some way, evaluate the Maximum Likelihood Estimate (MLE) decoded response (Yang et al., 2024; Fu et al., 2024; Chu et al., 2024; Minaee et al., 2024), which does not convey the model's internal variational awareness.\n(3) How should we evaluate each competence? While we illustrate measuring variational awareness, it is neither perfect nor the only way of evaluating variational awareness. Furthermore, it expects the availability of the logits, which is not true for closed models. More importantly, evaluating explication and negotiation abilities of an Al system presents a complex multi-disciplinary challenge. User-facing AI assistants and chatbots are designed to be agreeable to users (Soper et al., 2022) by exhibiting social characteristics (Dam et al., 2024; Chaves and Gerosa, 2021) and human-like traits (Rapp et al., 2021; Ciechanowski et al., 2019; Abdul-Kader and Woods, 2015). They seldom implement means to detect their limitations and act accordingly. When unsure, they should implement appropriate rhetorical means (Cope, 2022; Cialdini, 2001) such as persuasion (Prakken, 2006; Atkinson et al., 2017; Saha, 2024), negotiation, and deliberation to explicate their lacking knowledge and acquire the required knowledge efficiently. The design considerations and the evaluation frameworks of such systems are open questions for the community.\n(4) What kinds of datasets are needed to test each competency? Although numerous cultural benchmarking datasets exist, their suitability for measuring meta-cultural competencies is unknown. Hence, there might be a need to create novel datasets to measure each competency.\n(5) How to model the experiential knowledge of the user(s) from text and other modalities? In Section 2 we mention three essential characteristics of culture, one of which is the inherent experiential nature of culture. Extraction of such experiential knowledge from text or through other modalities and interaction patterns of the users is an extremely challenging problem that calls for a multi-disciplinary approach, most notably the methods from HCI, psychology, and ethnography.\nBeyond cultural equitability of AI systems, meta-cultural competency has huge application potentials ranging from user-facing AI assistants that can bridge cross-cultural communication to enabling the study of culture (Whitehead, 2005; Taylor, 2001; LeCompte and Schensul, 2010) by supporting ethnographic research methods (Skinner et al., 2013; Ortiz et al., 2013; Spradley, 2016). Through this position paper, we hope to make a strong case for the NLP community to engage in interdisciplinary conversations and widen the definition and scope of cultural competency in LLMs.\n(6) What is the need for meta-cultural competency in domain specific application? We believe meta-cultural competencies are crucial for domain-specific applications. Even applications such as LLMs for scientific document analysis can benefit. Firstly, culture is a prior for personalization. Culture can provide a reasonable estimate of the user's background and preferences, which an AI system can use when it does not know anything about a user. It is a good prior for the cold-start problem in personalization (Hu et al., 2008), where the AI system can gradually personalize to the user's preferences as it discovers more about the user with each interaction (Pandey et al., 2025). Also, even if a user's cultural background is known a priori, meta-cultural competency would be still useful for adapting to the ever-evolving nature of culture."}, {"title": "Limitations", "content": "Our formulation of variational awareness in Section 5 is one of the many possible ways of defining it and might not encompass all aspects of variational awareness. The Llama experiment in the subsequent section is an illustrative implementation of our framework in action and is not an exhaustive test for variational awareness. It only illustrates one of the several ways of measuring our formulation and has certain drawbacks, which we already mentioned. Culture, being experiential, is multimodal. However, due to space limitations, we confine our discussion primarily to text and do not discuss the other modalities of culture in detail. Culture also encompasses values, norms, and conventions that are not essentially factual. In the interest of space, we mainly discuss the factual aspect of culture. We do not discuss in detail the counterposition that meta-cultural competency can be evaded by recognizing it as a model's drawback and instead only striving for knowledge-based cultural competency for practicality. We argue that such a position is short-sighted, which might be practical in the short-term, and will not eventually scale."}]}