{"title": "ALVIN: Active Learning Via INterpolation", "authors": ["Michalis Korakakis", "Andreas Vlachos", "Adrian Weller"], "abstract": "Active Learning aims to minimize annotation\neffort by selecting the most useful instances\nfrom a pool of unlabeled data. However, typi-\ncal active learning methods overlook the pres-\nence of distinct example groups within a class,\nwhose prevalence may vary, e.g., in occupation\nclassification datasets certain demographics\nare disproportionately represented in specific\nclasses. This oversight causes models to rely on\nshortcuts for predictions, i.e., spurious correla-\ntions between input attributes and labels occur-\nring in well-represented groups. To address this\nissue, we propose Active Learning Via INter-\npolation (ALVIN), which conducts intra-class\ninterpolations between examples from under-\nrepresented and well-represented groups to cre-\nate anchors, i.e., artificial points situated be-\ntween the example groups in the representation\nspace. By selecting instances close to the an-\nchors for annotation, ALVIN identifies informa-\ntive examples exposing the model to regions of\nthe representation space that counteract the in-\nfluence of shortcuts. Crucially, since the model\nconsiders these examples to be of high cer-\ntainty, they are likely to be ignored by typical\nactive learning methods. Experimental results\non six datasets encompassing sentiment analy-\nsis, natural language inference, and paraphrase\ndetection demonstrate that ALVIN outperforms\nstate-of-the-art active learning methods in both\nin-distribution and out-of-distribution general-\nization.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable zero-shot and few-shot\nlearning capabilities of large language mod-\nels (LLMs) (Brown et al., 2020; Chowdhery et al.,\n2023; Touvron et al., 2023, inter alia), supervised\nfine-tuning remains a critical component of model\ndevelopment (Yuan et al., 2023; Mosbach et al.,\n2023; Bai et al., 2023). Collecting high-quality\nlabeled data is, nonetheless, time-consuming and\nlabor-intensive (Tan et al., 2024). To address this\nannotation bottleneck, active learning (AL) seeks\nto select the most useful instances from a pool\nof unlabeled data, thereby maximizing model per-\nformance subject to an annotation budget (Settles,\n2009).\nHowever, datasets commonly used for model\nfine-tuning often contain shortcuts (Gururangan\net al., 2018; McCoy et al., 2019; Wang and Culotta,\n2020), i.e., spurious correlations between input at-\ntributes and labels present in a large number of\nexamples (Geirhos et al., 2020). For example, in"}, {"title": "2 Active Learning Via INterpolation", "content": "occupation classification datasets, many examples\nexhibit patterns that incorrectly associate certain\ndemographics, such as race and gender, with spe-\ncific occupations (Borkan et al., 2019). Conse-\nquently, models exploiting shortcuts achieve high\nperformance on well-represented example groups,\nbut fail on under-represented groups where short-\ncuts do not apply (Tu et al., 2020). This issue is\nparticularly prominent in out-of-distribution set-\ntings, where under-represented groups can become\nmore prevalent due to distribution shifts (Koh et al.,\n2021). By neglecting the presence of these distinct\nexample groups in the training data, AL methods\namplify the prevalence of well-represented groups,\nthereby exacerbating shortcut learning (Gudovskiy\net al., 2020; Deng et al., 2023).\nMotivated by these shortcomings, we introduce\nActive Learning Via INterpolation (ALVIN). The\nkey idea behind ALVIN is to leverage interpola-\ntions between example groups to explore the rep-\nresentation space. Specifically, we identify unla-\nbeled instances for annotation by assessing their\nproximity to anchors, i.e., artificial points in the\nrepresentation space created through intra-class in-\nterpolations between under-represented and well-\nrepresented examples. Intuitively, ALVIN selects\ninformative instances with features distinct from\nthose prevalent in well-represented groups, helping\nthe model avoid reliance on shortcuts. Importantly,\nbecause these instances are deemed high certainty\nby the model, they are often overlooked by typical\nAL methods.\nWe conduct experiments on six datasets span-\nning sentiment analysis, natural language infer-\nence, and paraphrase detection. Our results\ndemonstrate that ALVIN consistently improves\nout-of-distribution generalization compared to sev-\neral state-of-the-art AL methods, across different\ndataset acquisition sizes, while also maintaining\nhigh in-distribution performance.\nWe analyze ALVIN to gain deeper insights into\nits performance improvements. First, we examine\nthe unlabeled examples identified by ALVIN, show-\ncasing its ability to select diverse, high-certainty\ninstances while avoiding outliers that could nega-\ntively impact performance. Next, through several\nablation studies, we demonstrate the advantages\nof our interpolation strategy compared to other\ninterpolation-based AL methods. Finally, we ex-\nplore the impact of hyper-parameters on perfor-\nmance and assess the computational runtime re-\nquired to select instances for annotation."}, {"title": "2.1 Preliminaries", "content": "We consider the typical pool-based active learn-\ning (AL) scenario (Lewis and Gale, 1994), in\nwhich an initial set of labeled instances $L$\n= {(xi, Yi)}1, where $x_i \\in X$ is the input and $y_i \\in$\n{1,2,...,C} is the corresponding label, along\nwith a pool of unlabeled instances $U = {xj}j=1,$\nwhere N < M. In each AL round, we query an\nannotation batch $B$ comprised of $b$ instances from\nU to be annotated and added to L. Then L is used\nto train a model $f_\\theta : X \\rightarrow Y$ parameterized by $\\theta$.\nThe model $f_\\theta$ consists of an encoder $f_{enc} : X \\rightarrow Z$\nmapping an input xi to a representation $z_i$, and a\nclassifier $f_{cls} : Z \\rightarrow Y$ which outputs a softmax\nprobability over the labels based on $z_i$. The AL\nprocess continues until the annotation budget is ex-\nhausted or a satisfactory model performance level\nis reached.\nFollowing Sagawa et al. (2019), we further as-\nsume that the training dataset contains distinct\ngroups of instances within some classes. Some\nof these groups are well-represented and strongly\nassociated with labels, e.g., high word overlap and\n\"entailment\" in natural language inference (NLI)\ndatasets (McCoy et al., 2019), while others are\nunder-represented, e.g., negation in the hypoth-\nesis and \"entailment\u201d (Gururangan et al., 2018).\nWe refer to the instances belonging to the well-\nrepresented groups associated with a particular\nclass as majority instances $g_{maj}$ of said class, and\nthe rest as minority instances $g_{min}.$\nModels often rely on shortcuts found in majority\ninstances to make predictions (Puli et al., 2023), a\ndependency that becomes problematic when distri-\nbution shifts at test time increase the prevalence\nof minority examples, resulting in poor out-of-\ndistribution generalization (Koh et al., 2021). This\nissue is further exacerbated in AL, where typical\nmethods like uncertainty sampling (Lewis and Gale,\n1994), select repetitive high uncertainty majority\ninstances (Deng et al., 2023). To counter shortcut\nlearning, it is crucial for the model to be exposed to\ninstances whose patterns deviate from those preva-\nlent in majority examples (Korakakis and Vlachos,\n2023)."}, {"title": "2.2 Algorithm", "content": "We hypothesize that the properties of the represen-\ntation space are crucial for identifying unlabeled\ninstances capable of mitigating shortcut learning.\nSpecifically, the reliance on shortcuts for predic-\ntions creates a spurious decision boundary, incor-\nrectly separating minority and majority examples\nwithin the same class. Thus, our goal is to select\ninformative instances that will prompt the model\nto adjust its decision boundary, thereby correct-\ning its reliance on shortcut features. To achieve\nthis, ALVIN employs intra-class interpolations be-\ntween minority and majority instances to create\nanchors. These anchors facilitate the exploration of\ndiverse feature combinations within the representa-\ntion space, enabling the identification of unlabeled\ninstances that integrate representations from differ-\nent example groups at varied proportions. However,\nbecause these instances exhibit high certainty, they\nare typically overlooked by existing AL methods,\ne.g., a model will confidently label an \u201centailment\"\ninstance with negation in NLI as \u201ccontradiction.\u201d\nThe overall procedure of ALVIN is detailed in Al-\ngorithm 1 for an AL round.\nInferring Minority/Majority Examples At the\nbeginning of each AL round, we first identify the\nminority and majority examples within each class\nin the training dataset (line 2). We are motivated\nby the observation that the existence of shortcuts\nwithin the majority examples causes a discrep-\nancy in training dynamics, leading the model to fit\nmajority examples faster than minority ones, and\nresulting in a spurious decision boundary (Shah\net al., 2020; Tu et al., 2020; Pezeshki et al., 2021).\nThus, we infer the example groups by monitor-\ning the frequency with which the model incor-\nrectly predicts an example (Toneva et al., 2019;\nSwayamdipta et al., 2020; Yaghoobzadeh et al.,\n2021). Specifically, we classify an example xi as\nminority if (1) the model's predictions switch be-\ntween correct to incorrect at least once during train-\ning, i.e., $acc_{x_i}^t > acc_{x_i}^{t+1}$, where $acc_{x_i}^t = 1_{\u0177=y_i}$\nindicates that the example $x_i$ is correctly classified\nat time step t, or (2) the example is consistently\nmisclassified by the model throughout training, i.e.,\n$\\forall t \\in {1,2,...,T}, \nacc_{x_i}^t = 0$ where $T$ is the\ntotal number of training epochs. Conversely, all\nother examples that do not meet these criteria are\nclassified as majority examples.\nAfter identifying the minority\nand majority examples within each class, we then\nproceed to create anchors to explore the representa-\ntion space between these example groups. In partic-\nular, for each class c in C, we initially sample Cmin\nand Cmaj (line 4), where $|L_{min}| = |L_{maj}| \\ll N$.\nNext, for every minority instance in $L_{min}$ (line 5)\nwe randomly sample a majority instance from\n$L_{maj}$ (line 6), and interpolate their representations\nto create the anchor ai,j (line 9):\n$a_{i,j} = \nAfenc(xi) + (1 \u2212 1) fenc(xj),$ (1)\nwhere the interpolation ratio $\\lambda \\in [0, 1]$ is sampled\nfrom a Beta distribution Beta(a, a). By adjusting\nthe parameter a of this distribution, we can control\nwhere the anchors lie in the representation space"}, {"title": "3 Experimental Setup", "content": "intuitively, when A is closer to 0, the anchor ai,j is pre-\ndominantly influenced by the minority instance xi;\nconversely, as A approaches 1, ai,j increasingly re-\nsembles the representation of majority instance xj.\nWe generate Kanchors for each minority-\nmajority pair (line 7). This process enables us\nto create anchors that incorporate varied feature\ncombinations, thus allowing for a comprehensive\nexploration of the representation space between\nminority and majority examples.\nAfter constructing the an-\nchors, we use K-Nearest-Neighbors (KNN) to iden-\ntify similar unlabeled examples xu \u2208 U to an an-\nchor in the representation space (line 10).2 We re-\npeat this process for each anchor across all classes.\nFinally, we select for annotation the top-b unla-\nbeled instances with the highest uncertainty (Lewis\nand Gale, 1994) (line 11). This approach maintains\nthe advantages of uncertainty-based instance selec-\ntion, while counteracting its tendency to facilitate\nshortcut learning by selecting a subset of unlabeled\ninstances that mitigate this phenomenon.\nDatasets We conduct experiments on six datasets\nacross sentiment analysis, natural language infer-\nence, and paraphrase detection. In line with previ-\nous works in AL (Yuan et al., 2020; Margatina et al.,\n2021; Deng et al., 2023), we use SA (Kaushik et al.,\n2020), NLI (Kaushik et al., 2020), ANLI (Nie et al.,\n2020), SST-2 (Socher et al., 2013), IMDB (Maas\net al., 2011), and QQP (Chen et al., 2017). \u03a4\u03bf\nassess out-of-distribution (OOD) generalization\nwe use SemEval-2017 Task 4 (Rosenthal et al.,\n2017) for SA, ANLI for NLI, and NLI for ANLI,\nIMDB for SST-2, SST-2 for IMDB, and TwitterP-\nPDB (Lan et al., 2017) for QQP. Validation and\ntest splits are used as described in Margatina et al.\n(2021) for IMDB, SST-2, and QQP, and Deng et al.\n(2023) for SA, ANLI, and NLI.\nComparisons We compare ALVIN with several\nbaseline and state-of-the-art AL methods:\n\u2022 Random samples instances uniformly at ran-\ndom.\n\u2022 Uncertainty (Lewis and Gale, 1994) acquires\nannotations for unlabeled instances with the\nhighest predictive entropy according to the\nmodel."}, {"title": "4 Results", "content": ""}]}