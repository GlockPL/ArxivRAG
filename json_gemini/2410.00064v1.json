{"title": "M2Distill: Multi-Modal Distillation for Lifelong Imitation Learning", "authors": ["Kaushik Roy", "Akila Dissanayake", "Brendan Tidd", "Peyman Moghadam"], "abstract": "Abstract-Lifelong imitation learning for manipulation tasks poses significant challenges due to distribution shifts that occur in incremental learning steps. Existing methods often focus on unsupervised skill discovery to construct an ever-growing skill library or distillation from multiple policies, which can lead to scalability issues as diverse manipulation tasks are continually introduced and may fail to ensure a consistent latent space throughout the learning process, leading to catastrophic forgetting of previously learned skills. In this paper, we introduce M2Distill, a multi-modal distillation-based method for lifelong imitation learning focusing on preserving consistent latent space across vision, language, and action distributions throughout the learning process. By regulating the shifts in latent representations across different modalities from previ-ous to current steps, and reducing discrepancies in Gaussian Mixture Model (GMM) policies between consecutive learning steps, we ensure that the learned policy retains its ability to perform previously learned tasks while seamlessly integrating new skills. Extensive evaluations on the LIBERO lifelong imita-tion learning benchmark suites, including LIBERO-OBJE\u0415\u0421\u0422, LIBERO-GOAL, and LIBERO-SPATIAL, demonstrate that our method consistently outperforms prior state-of-the-art methods across all evaluated metrics.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the field of robotics has made significant strides in creating intelligent systems capable of performing complex tasks autonomously. Among these advancements, Imitation Learning (IL) has become a popular and effective paradigm for robots to learn complex behaviors by observ-ing and mimicking human demonstrations [1]-[3]. Recent research further enhances the generalization ability of these methods by leveraging multi-modal inputs, such as vision, language, and actions, and the recent large vision-language-action (VLA) models [4], [5].\nDespite the impressive performance of IL models, the current state-of-the-art IL models focus on either learning from a single task or a known set of tasks in advance. This impedes their applicability in complex real-world settings, where robots need to continually learn new tasks as they arrive while retaining the models' performance on previously learned tasks. This is known as Lifelong Imitation Learning (LIL). Recently, a few studies [6]\u2013[9] show promising results in addressing imitating learning from sequentially arriving tasks while avoiding catastrophic forgetting. Catastrophic forgetting refers to undesirable behavior of the neural net-works in which newly acquired skills (i.e., knowledge) can degrade the preservation of previous ones [10], [11].\nRecent LIL methods, such as ER [12], method replay a subset of examples from previous tasks alongside new task data. However, imbalanced data distribution can cause the latent representations of past tasks to drift, leading the trained policy to favor the current task and reducing its performance on earlier tasks. Methods like BUDS [7], and LOTUS [9] rely on unsupervised skill discovery and integration, but maintaining an ever-expanding skill library becomes computationally expensive over time. PolyTask [13] addresses this by distilling skills from task-specific policies, but this approach requires access to all previously learned policies, making it resource-intensive and unrealistic.\nTo investigate the factors contributing to catastrophic for-getting in these scenarios, we visualize t-SNE plots of latent representations for AgentView using a ResNet18 backbone from ER policies at steps 2 and 3, respectively. The results, shown in Figure 1, reveal significant deformation in the latent space, with representations of prior tasks drifting considerably. This trend is consistent across different modal-ities and motivates us to design a multi-modal distillation framework to preserve both latent representations and action distributions as we continuously train our policy on new manipulation tasks."}, {"title": "II. RELATED WORK", "content": "Imitation Learning (IL), also known as Learning from Human Demonstration, is a machine learning paradigm where a robot learns to perform tasks by mimicking the actions of an expert demonstrator [14]\u2013[16]. The primary goal of imitation learning is to learn a policy that maps observations to actions and replicates the expert's actions. Lifelong Imitation Learning (LIL) extends this concept by focusing on the continuous acquisition of skills over time while retaining previously learned knowledge. In LIL, robots are designed to adapt to new tasks and environments without forgetting earlier skills, addressing the issue of catastrophic forgetting [10], [11].\nCatastrophic forgetting is well-studied for various prob-lems in computer vision, including classification [17], detec-tion [18], and semantic segmentation [19], within lifelong learning scenarios. In the literature of lifelong learning, dynamic architecture [20], [21], regularization [22], [23] and memory-replay [6], [11], [24], [25] based approaches have been proposed to tackle the catastrophic forgetting. Regular-ization based methods control the changes in the network's weight by introducing new regularization terms. Memory-replay based strategies store a subset of past examples and replay with new examples.\nLifelong learning has shown promise in the field of robotics; however, the volume of research specifically focus-ing on lifelong imitation learning remains limited. ER [12] preserves a limited number of past trajectories and replays them in conjunction with new trajectories from the ongoing manipulation task. In contrast, CRIL [6] leverages generative adversarial networks (GAN [26]) to generate the first frame of each trajectory and relies on an action-conditioned video prediction network to predict future frames using states and actions from the deep generative replay (DGR [27]) policy. BUDS [7] introduces a technique for skill discov-ery in robot manipulation tasks that do not require pre-segmented demonstrations. By employing a bottom-up ap-proach, it autonomously identifies and organizes skills from unsegmented, long-horizon demonstrations, enabling robots to effectively handle complex and extended manipulation tasks. LOTUS [9] allows robots to continuously learn and adapt to new tasks by leveraging unsupervised skill discovery and integration. It utilizes an open-vocabulary vision model for skill discovery and a meta-controller for skill integra-tion. PolyTask [13] proposes a method for learning unified policies across multiple tasks through behavior distillation. This approach distills knowledge from expert policies into a single policy, enabling the robot to efficiently perform various tasks with a generalized model. However, these LIL approaches still need to tackle challenges related to scalability and the effective integration of new skills across varied environments."}, {"title": "III. PROBLEM FORMULATION", "content": "The Lifelong Robot Learning problem extends the tra-ditional robot learning framework by requiring a robot to continuously acquire, adapt, and retain knowledge across a sequence of tasks {T1,...,TK} over its operational lifespan. This robot learning problem is formulated as a finite-horizon Markov Decision Process (MDP), denoted by M = (S, \u0391,\u03a4, \u0397, \u00b5o, R), where S represents the state space, A is the action space, T : S \u00d7 A \u2192 S is the transition function, H is the maximum horizon for each episode of a task, \u00b5o is the initial state distribution, and R : S \u00d7 A \u2192 R is the reward function. Given that the reward function R is often sparse, a goal predicate g: S \u2192 {0,1} is used to indicate whether a goal has been achieved. In the context of lifelong learning [11], the robot must develop a single policy \u3160 that can adapt to the specific requirements of each task Tk. This policy is conditioned on the task at hand, allowing the robot to tailor its policy to meet the unique objectives of each task while remaining consistent in its structure. Each task Tk = (\u03bc, gk) is characterized by its own initial state distribution \u03bc\u03b5 and goal predicate gk, while the state space S, action space A, transition function T, and time horizon H remain unchanged across all tasks. The robot's broader objective is to maximize its performance across all tasks, which can be mathematically represented as:\n$\\max J(\\pi) = \\frac{1}{K} \\sum_{k=1}^{K}  \\mathbb{E}_{\\tau^{k} \\sim \\pi(\\cdot|T_k), \\mu_{0}^{k} } [  \\frac{1}{H} \\sum_{t=1}^{L_k} g_k(s_t) ]$ (1)"}, {"title": "IV. PROPOSED METHOD - M2Distill", "content": "In our lifelong imitation learning framework, we priori-tize maintaining a consistent low-dimensional latent space across different modalities to address drift in the latent representation distributions. We hypothesize that distilling latent representations from prior models using expert demon-strations can help mitigate shifts in data distributions during incremental learning. Furthermore, we aim to preserve the action distribution from previously learned manipulation tasks while acquiring new skills, ensuring that the knowledge gained from expert demonstrations is retained and leveraged throughout the learning process.\nOur architecture for learning manipulation tasks from demonstrations is based on the ResNet-T design from [8], incorporating task embeddings from a pre-trained lan-guage transformer, two image encoders for different camera streams, and additional modality encoders for joint positions and gripper state data. The multi-modal tokens from various time steps are processed by a temporal transformer, with the final token passed to a GMM policy head to produce an ac-tion distribution. In our approach, we focus on retaining both the multi-modal token distributions and the GMM policy head's action distribution throughout incremental learning. Figure 2 depicts the overall architecture of our proposed distillation-based LIL approach. The following discussion covers multi-modal distillation, after which we present policy distillation."}, {"title": "A. Multi-modal Distillation", "content": "During the distillation process, we pass the same RGB images through the old and new ResNet18 [30] encoders, and our proposed distillation strategy is applied to the resulting latent representations. We minimize the squared L2 norm of the difference between the old and new latent representations, ensuring that the latent space for expert demonstrations in previously learned tasks remains intact. This constraint preserves performance on prior tasks while enabling the acquisition of new skills.\nLet's assume that for an input image batch of size B with L timestamps, the extracted feature vectors from the image encoder of our policy have dimensions B \u00d7 L \u00d7 64. This holds true for both the current step k and the previous step k-1.\nFor image inputs fk and fk-1 at steps k and k 1, and considering both the AgentView and HandEye image views, we have the following loss:\n$L_{image} = L_{AgentView} + L_{HandEye}$,\nwhere the loss for each modality \u0454 \u2208 {AgentView, HandEye} is defined as:\n$L_{\\epsilon \\in {AgentView,HandEye}} = \\frac{1}{NL} \\sum_{i=1}^{N} \\sum_{j=1}^{L} ||f_{i,j}^{k,\\epsilon} - f_{i,j}^{k-1,\\epsilon}||^2$. (4)\nSimilarly, we feed the text instructions into a pre-trained BERT [31] text encoder and project the output into a 64-dimensional latent space using an MLP. Let gk and gk-1 represent the latent representations of the text instruction at steps k and k \u2013 1. The distillation loss for the text modality is then:\n$L_{text} = \\frac{1}{NL} \\sum_{i=1}^{N} \\sum_{j=1}^{L} ||g_{i,j}^{k} - g_{i,j}^{k-1}||^2$ (5)\nMoreover, we condition our policy on additional modali-ties (e.g., joint information and gripper state) along with the image and text. Consequently, we define the following distillation loss to maintain a consistent latent space for this extra modality as well.\n$L_{extra} = \\sum_{\\epsilon \\in {joint, gripper}} \\frac{1}{NL} \\sum_{i=1}^{N} \\sum_{j=1}^{L} ||h_{i,j}^{k,\\epsilon} - h_{i,j}^{k-1,\\epsilon}||^2$, (6)"}, {"title": "B. Policy Distillation", "content": "In this paper, we prioritize preserving a consistent ac-tion distribution for previously learned manipulation tasks throughout the continual learning process. We address this by replicating the action distribution of the previous GMM policy within the current GMM policy, which helps maintain consistency in the distribution of action space between the two steps. This strategy is vital in preventing catastrophic forgetting, where new tasks could potentially disrupt the per-formance of previously learnt ones. By utilizing a Kullback-Leibler (KL) divergence loss between the old model's policy and that of the current model, we can ensure that the predicted actions for past tasks remain aligned with their original action distributions.\nLet \u03c0\u03ba and k-1 denote the action distributions of the policy at incremental steps k and k \u2212 1, respectively. The KL divergence between wk and wk-1 can be formulated as follows:\n$L_{policy} = L_{KL} (\\pi^{k} ||\\pi^{k-1})$\n$= E_{a \\sim \\pi^{k}} [log \\pi^{k} (a) \u2013 log \\pi^{k-1}(a)]$\n$= \\int \\pi^{k} (a) [log \\pi^{k} (a) \u2013 log \\pi^{k-1}(a)] da$. (7)\nWhen \u03c0 and \u3160k-1 are Gaussian distributions, this KL divergence has a closed-form solution. For mixtures of Gaussians (GMMs), which is the case in this work, the KL divergence lacks a closed-form expression due to the complexity of the mixture components [32]. To tackle this issue, we employ Monte Carlo sampling to approximate the KL divergence. Specifically, we draw a set of samples {as}1 from the distribution \u03c0k, and estimate the KL divergence by averaging the log difference between \u03c0\u03ba(\u03b1) and \u03c0\u22121(a) over these samples as follows.\n$L_{policy} \\approx \\frac{1}{N} \\sum_{s=1}^{N}(log \\pi^{k} (a^s) \u2013 log \\pi^{k-1}(a^s))$, (8)\nwhere wk(as) and wk-1(as) are the probability density function (pdf) for sample as using GMM policy wk and \u03c0k-1 respectively. By combining all the modality-specific distillation loss functions, we have the following combined distillation loss\n$ldistill(\\hat{s}_t, \\hat{a}_t) = \\lambda_i L_{image} + \\lambda_t L_{text} + \\lambda_e L_{extra} + \\lambda_p L_{policy}$, (9)\nwhere \u03bb\u03af, \u03c4, \u03bb\u03b5, and Ap are hyperparameters that control the balance between stability and plasticity of the policy throughout the learning process.\nFinal Optimization Objective. Putting all together, to up-date the policy, we optimize\n$\\min J(\\pi) = \\frac{1}{K} \\sum_{k=1}^{K}  \\mathbb{E}_{s_t,\\hat{s}_t \\sim D^k \\cup \\hat{D}^k } [  -log \\pi((a_t \\cup \\hat{a}_t)| (s \\cup \\hat{s}) {}_{<t}; T_k) + l_{distill}(\\hat{s}_t, \\hat{a}_t)] $. (10)\nHere, Dk and \u00d4Dk refer to the data distribution for the current task and memory exemplars, consisting of a subset of prior tasks' examples."}, {"title": "V. EXPERIMENTAL SETTINGS", "content": "We train our approach on a NVIDIA H100 GPU, and follow the data augmentation strategy proposed by [8]. For a fair comparison, our model shares the exact parameter configuration with the ResNet-T baseline and was trained with the same training hyperparameters. We train our model for 50 epochs at every incremental step and we set the weight of our proposed regularization terms as follows; At and Ae are set to 0.05 across all task suits. For LIBERO-OBJECT and LIBERO-SPATIAL, we use 0.05 for both Ai and Ap, whereas for LIBERO-GOAL, we increase their values to 0.25. We evaluate our method against the following baselines:\n\u2022 SEQUENTIAL: This baseline involves naively fine-tuning new tasks sequentially using the ResNet-Transformer architecture from LIBERO [8].\n\u2022 EWC [11]: A regularization based approach that reg-ulates the network's weights by selectively updating relatively less important weights for prior tasks.\n\u2022 ER [12]: A rehearsal-based method that preserves a memory buffer containing samples from previous tasks and uses this buffer to facilitate learning of new tasks. We impose a capacity limit of 1000 trajectories on the replay buffer."}, {"title": "VI. RESULTS", "content": "In this section, we first evaluate whether our proposed dis-tillation strategy aids the policy in leveraging existing skills while learning new manipulation tasks without forgetting in a lifelong imitation learning setup. Afterwards, we examine how our method's performance changes at intermediate steps as the policy is trained on a sequence of manipulation tasks. Furthermore, we assess the effectiveness of our method in maintaining a consistent latent space. Finally, we present an ablation study to evaluate the contribution of each regular-ization term in our proposed multi-modal distillation-based LIL method.\nComparison to SOTA methods. Table I provides a com-prehensive evaluation of our proposed distillation-based LIL method, M2Distill, in comparison to the current SOTA methods in LIBERO benchmark suites. We observe that reg-ularization based strategy (i.e., EWC [11]) performs worse than the memory-replay based strategies across the task suites. The results indicate that our multi-modal distillation-based LIL strategy outperforms the baseline methods across all evaluation metrics in the LIBERO-OBJECT, LIBERO-GOAL, and LIBERO-SPATIAL task suites. In particular, our method achieves a 4% higher AUC than LOTUS [9] in the LIBERO-OBJECT task suite, while showing similar FWT results. For the LIBERO-GOAL suite, our method achieves comparable AUC but shows a substantial improvement of 10% in both the FWT and NBT metrics. Moreover, in the LIBERO-SPATIAL task suite, our approach exceeds ER [12] by approximately 15% on the NBT metric and realizes a 5% improvement in AUC. Overall, our proposed method exhibits robustness in leveraging previously acquired skills while also effectively learning new ones.\nPerformance Analysis. We assess the effectiveness of our proposed approach by measuring the success rate at each incremental step in the lifelong imitation learning scenario on the LIBERO-OBJECT task suite. For this evaluation, we compare our method to ER, and present the average success rate along with the standard error based on three seeds, as illustrated in Figure 3. The line plot shows that our method outperforms the Experience Replay (ER) baseline consistently. As training progresses, the performance gap between the two methods widens, highlighting the superior learning capacity of our proposed method. Furthermore, the lower standard error in our method suggests greater stability, clearly demonstrating its superior effectiveness in lifelong imitation learning tasks.\nLatent Representation Drift Analysis. To assess the robust-ness of our proposed method in preserving a consistent latent space across different modalities during incremental steps in the lifelong imitation learning scenario on the LIBERO-OBJECT task suite, we compare our method against ER. We report the average drift in latent representations, calculated as the squared Euclidean distance between representations from the current and previous policies, averaged across three seeds, as illustrated in Figure 4. The bar plot illustrates that our method consistently exhibits less drift in latent repre-sentations across the Language, AgentView, and HandEye modalities compared to the Experience Replay (ER) baseline during incremental steps. The difference in performance is most noticeable in the later incremental steps, especially for Language and AgentView. This suggests that our method offers better stability in retaining learned skills over time.\nAblation Studies. We conduct experiments on LIBERO-OBJECT using a seed value of 100 to examine the contri-bution of each distillation component in our strategy. The results (shown in Table II) indicate that each regulariza-tion term is crucial for the consistent performance of the policy. Specifically, maintaining a consistent latent space for the vision modality is essential, as the performance significantly drops from 75% to 49% in AUC and 81% to 62% in FWT metric when distillation on the latent space for AgentView and HandEye views is not applied. Addition-ally, the impact of action distillation is notable; the AUC decreases by approximately 14% without this regularization term. Furthermore, the absence of a regularizer for the extra modality results in a drop of about 10% in FWT and 20% in AUC. These findings highlight the importance of consistent latent representations of different modality information for preserving performance on prior tasks while learning novel manipulation tasks."}, {"title": "VII. CONCLUSION", "content": "We propose a multi-modal distillation-based lifelong im-itation learning approach for robot manipulation tasks. In this work, we focus on maintaining the latent space for different modalities and the action distribution throughout the learning experiences. To achieve this, we impose con-straints on the alterations in the latent representations and action distributions between the prior and current policies. Specifically, we optimize the policy at step Tk by minimizing the squared L2 norm of latent features between the old and current encoders across different modalities, as well as the discrepancy between the prior and current GMM policies. Our proposed distillation strategy ensures a robust latent space alongside a GMM policy that preserves previously learned skills while adapting to new skills without forgetting. Through quantitative evaluation on the LIBERO task suites (i.e., LIBERO-OBJECT, LIBERO-GOAL, and LIBERO-SPATIAL), we demonstrate that our proposed method sig-nificantly outperforms baseline methods across all evaluation metrics. For future work, we intend to investigate a memory-free distillation strategy for lifelong imitation learning that is robust to noise."}]}