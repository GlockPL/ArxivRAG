{"title": "The State of Robot Motion Generation", "authors": ["Kostas E. Bekris", "Joe Doerr", "Patrick Meng", "Sumanth Tangirala"], "abstract": "This paper reviews the large spectrum of methods for generating robot motion proposed over the 50 years of robotics research culminating in recent developments. It crosses the boundaries of methodologies, typically not surveyed together, from those that operate over explicit models to those that learn implicit ones. The paper discusses the current state-of-the-art as well as properties of varying methodologies, highlighting opportunities for integration.", "sections": [{"title": "1 Introduction", "content": "The robotics community is grappling with a critical question. Will the emerging set of data-driven methods for generating robot motion supersede the traditional techniques as access to robot motion data increases?\nIn this context, this paper reviews methods for robot motion generation, which are classified into those that operate given an explicit model vs. those that implicitly learn one from data. Explicit models can correspond to analytical expressions for the world geometry and dynamics or an explainable, numerical approximation in the form of a simulator. Motion generation given explicit models is rather mature and methods are being deployed on real systems, such as autonomous vehicles and industrial manipulators. At the same time, there is increasing excitement for data-driven methods, which have been demonstrated to perform complex tasks, such as dexterous manipulation and unstructured locomotion. These methods often do not depend on explicit models. Instead, they learn implicit representations, which are stored in the internal parameters of machine learning models.\nGiven the challenge of comprehensively reviewing the vast amount of work in this area across disciplinary boundaries, the focus is on breadth rather than diving deeply into specific methodologies. Similarly, the focus is on principles that are applicable across robotic platforms instead of techniques that are specific to certain hardware configurations.\nThe paper concludes with a discussion regarding the properties of the various robot motion generation methods. It argues that integrative approaches and closer interactions between different sub-communities can help develop more robust, safe solutions that can be reliably deployed at reasonable costs and human engineering effort."}, {"title": "2 Motion Generation given Explicit Models", "content": "Figure 1 classifies methods that operate over an explicit model. Motion planning methods generate safe nominal paths/trajectories to a goal given a fully-observable world model. Task and motion planning extends the principle"}, {"title": "2.1 Motion Planning", "content": "Motion planning aims to identify a path with minimal cost, such as the shortest path or the fastest trajectory that brings a robot to a desirable goal state, without undesirable collisions, given a fully-observable world model.\nSearch-based Approaches: Uninformed search methods, such as Uniform Cost Search (UCS) or Dijkstra's algorithm [1], can compute optimal paths over a discrete representation of the state space in the form of a grid or a graph given a cost function. Informed alternatives, such as A* [2], utilize heuristic cost-to-go estimates from each state to accelerate solution discovery and can still return the optimum solution over the discrete representation for an admissible/consistent heuristic. Due to the comprehensive nature of search and the discrete representation, search methods suffer from the curse of dimensionality, i.e., the possible states to be explored grow exponentially with dimensions, rendering them computationally infeasible for many robotics problems given na\u00efve discretizations. There have been many successful applications of search methods in robotics, however, such as planning for autonomous vehicles [3] and single or dual arm planning [4].\nSampling-based Motion Planners (SBMPs): Sampling provides graph-based representations for searching the collision-free subset of a robot's state space in a more scalable manner than grids. The Probabilistic Roadmap Method (PRM) [5] samples collision-free configurations as nodes of a roadmap, and collision-free local paths define the edges. The roadmap can then be used to solve multiple queries via search. For specific queries, the Rapidly Exploring Random Tree (RRT) [6] generates a tree data structure rooted in the robot's start state to quickly explore the free state space until it reaches the goal's"}, {"title": "2.2 Task and Motion Planning (TAMP)", "content": "TAMP methods target long-horizon and multi-step robotic tasks, which may include moving through a sequence of goals or manipulating the environment [25]. TAMP methods typically define low-level operators with motion constraints and high-level logical relationships between the operators. Operators can contain hybrid discrete and continuous parameters, motion constraints, preconditions,"}, {"title": "2.3 Belief Space Planning", "content": "The above methods assume a deterministic, perfect world model. Sensing noise and inaccurate execution introduces uncertainty, however, and the need to generate robust motions to such disturbances. If these noise sources can be modeled probabilistically, (Partially Observable) Markov Decision Processes (PO)MDPs provide a formulation to reason about uncertainty. A Markov Decision Process (MDP) consists of a set of states S, set of actions A, transition probabilities between states T(s_{t+1} | s_t, a_t), and a reward function R(s_t, a_t, s_{t+1}). (PO)MDPs employ belief distributions for the problem representation, i.e., probability distribution over states given a set of observations O. They can be integrated with Bayesian state estimation (e.g., Kalman or particle filters) that return such beliefs. Solutions to (PO)MDPs are not nominal paths but policies that map beliefs to actions. Due to the consideration of uncertainty, computing an exact solution to a (PO)MDP is often computationally intractable [30], especially given the continuous state and action spaces of robotics.\nThis has motivated approximate solutions. The Successive Approximations of the Reachable Space under Optimal Policies (SARSOP) [31] applies point-based value iteration and offline sampling to focus on representative beliefs and determine the best action given the sampled set. Determinized Sparse Partially Observable Trees (DESPOT) [32] employ tree search online to compute the optimal action. Online methods can also perform policy search by using a controller in a limited search space, increasing scalability but not bounding solution quality [33]. Heuristic rules or assumptions can reduce complexity by ignoring long-term consequences and focus on immediate gains or most likely outcomes. For instance, Pre-image Back Chaining constructs state-action sequences (pre-images) leading backward from the goal [34]. Generalized Belief Space (GBS) planning dynamically adapts to ongoing sensing updates [35]."}, {"title": "2.4 Control and Feedback-based Planning", "content": "Instead of explicitly modeling uncertainty, control tightly integrates state estimation and motion generation in a closed-loop. Thus, it defines policies that are reactive to different possible outcomes that may be observed upon execution.\nLow-level Controllers: Proportional Integral Derivative (PID) control and related tools are simple feedback strategies that are robust once tuned. They are ubiquitous for tracking desirable robot controls from higher-level motion generation processes. Path Tracking Controllers dynamically select controls given the latest state estimate to minimize path deviation. These controllers, however, are myopic and typically neither reason about the desired goal or obstacles.\nPotential Functions and Operational Space Control: Potential functions [36] define attractive fields towards the goal and repulsive ones that push away from obstacles. Moving along the negative gradient of the sum of these fields provides the motion vector. Some engineering is needed to tune the parameters of the potentials. In complex environments, the corresponding potential may have multiple minima and goal convergence is not guaranteed. Navigation functions [37] are smooth and ensure a single minimum at the goal, but they are more complicated to design and can only be constructed for specific environments (i.e., sphere and star worlds). Such control policies do not have to be defined directly in the robot's state space. Operational Space Control [38] defines such control laws in lower-dim. task spaces. For instance, if the task involves the robot's end-effector, a control law is defined to move it towards a desired goal unencumbered by other robot constraints. Additional control laws can be defined so that links avoid collisions. The corresponding motion vectors for the individual links are then mapped and integrated to a state space motion for the robot via the pseudo-inverses of the robot's Jacobian matrix, which relates robot joint velocities to link velocities through a linear transformation parameterized by the joint states. This allows for multi-level hierarchical control [39], where a hierarchy can be imposed over constraints, operational tasks, and soft objectives. Then, lower priority objectives are solved in the null space of higher priority ones once projected to the state space. These strategies rely on precise robot models and high-quality sensing to provide accurate feedback on robot state.\nLinearization and Feedback-based Planning: Principles of linear control can be applied for robot motion generation. The Linear Quadratic Regulator (LQR) provides an optimal solution for linear time-invariant systems given a quadratic cost function as a control law of the form u(t) = \u2212K \u00b7 x(t), where x(t) and u(t) are respectively the system's state and the control to be applied at time t. Most robots, however, are nonlinear and time-varying, while tasks involve complex, non-quadratic objectives. Feedback linearization locally transforms nonlinear systems into equivalent linear ones so that linear control laws can be applied. It can be used for tracking states x_d along a desired trajectory by minimizing the error e = x(t) \u2212 x_d(t). Given the linearization, these solutions tend to work in the vicinity of the desired goal/trajectory. They can be ineffective when the feedback-linearized system behaves very differently from the original nonlinear system. To expand the set of initial conditions from which the goal can be reached, sequential composition of such feedback policies [40] can"}, {"title": "3 Data-driven Motion Generation with Implicit Models", "content": "Figure 2 presents a classification of data-driven robot motion generation methods that implicitly learn a model. Learning from demonstration methods employ supervision, where a robot is tasked to best replicate the demonstrated behavior. Alternatively, reinforcement learning learns to make decisions by experiencing rewards or penalties after interaction with the environment. Cross-task learning transfers knowledge from an existing solution to a new task or adapts it dynamically. Finally, large models, given access to significant data, allow pre-trained ML models to be fine-tuned and deployed in diverse domains."}, {"title": "3.1 Learning from Demonstrations", "content": "Imitation learning mimics the decision-making process given demonstration data. Behavior Cloning (BC) trains a model in a supervised manner given a dataset of demonstrations to provide a motion policy. Successful applications of BC have enabled robots to learn complex behaviors [52] [53]. The process starts with data collection, which includes capturing sensory inputs and corresponding control actions for a target task. An appropriate ML model architecture (e.g.,"}, {"title": "3.2 Deep Reinforcement Learning (DRL)", "content": "One area of robotics that has garnered interest [66] is Deep Reinforcement Learning (DRL), where Deep Neural Networks (DNN) predict the Q function of an MDP and the corresponding policy. It has been applied across tasks, e.g., grasping [67], locomotion [68], and assembly [69].\nModel-free, On-Policy DRL: Standard DRL gathers data from interaction with the environment, where Policy Gradient (PG) methods generate a batch of full trajectory data, compute the gradient of each state transition, and scale each gradient by the respective discounted return. PGs update the policy and repeat the process towards maximizing the expected cumulative reward. Proximal Policy Optimization (PPO) [70] is a PG method that leverages a value function, which is the average discounted return from a given state. These are on-policy methods, i.e., they only use data collected from the most recent policy. Exploration can be achieved through the periodic execution of random actions (epsilon greedy), injecting Gaussian noise to policy outputs [71], or using a stochastic policy for entropy maximization [70].\nDRL faces multiple challenges in robotics: (i) sample inefficiency: Policies need a lot of data to train and take significant training time. Real robot data, however, can be slow, expensive, and unsafe to collect [72]. (ii) instability: Policies can be inconsistent across training sessions due to poorly designed rewards, exploration strategies, learning rate parameters or learning error from DNNs. (iii) reward engineering: Simple rewards are sparse, i.e., they assign a 0 reward everywhere except at the goal. While desirable, sparse rewards often do not find a successful behavior in reasonable time [72]. This motivates dense rewards to guide exploration, which require manual engineering. While dense rewards can improve training time and stability, they can also lead to wrong behaviors, if they do not match the true task objective. (iv) long horizon tasks: The state space to be explored for such tasks is even larger making the propagation of rewards across subproblems difficult.\nThe above challenges have motivated multiple DRL variations in robotics: Simulation and Domain Randomization: Simulators, which are explicit world models, are useful for addressing sample inefficiency by generating data without real-world experiments. They suffer, however, from the model gap issue. Even so, it may still be possible to create good policies via domain randomization (DR) [73], which can partially compensate for imperfect models during sim-to-real transfer. DR results in behaviors that have the highest expected reward across a variety of underlying dynamics and perception errors.\nOff-policy RL Sample inefficiency motivates data reuse. Off-policy RL stores the expected discounted future reward of any in-distribution state-action tuple on a Deep Q function, which is used to train the policy \u03c0. To update the parame-"}, {"title": "3.3 Cross-task Learning", "content": "Demonstrations from related or earlier tasks can help bootstrap or guide RL. Transfer Learning methods exhibit high variability. One approach bootstraps RL by using an adaptively-weighted auxiliary term in the loss function of PPO to increase action similarity for the learned policy against the demonstrated one from a task with similar MDP [84]. It is also possible to pre-train multiple tasks with offline RL given a single task-conditioned policy [85]. Online, the approach fine-tunes both the conditioning parameter and policy to automatically reset tasks for autonomous real-world training. For transferring information across heterogeneous MDPs, researchers have mapped states and actions across MDPs [86] and transfered useful representations across domains [87].\nMulti-Task Learning trains many tasks in parallel while sharing information across tasks to accelerate training and improve generalization [88]. Methods focus on how to select parameters to sharing so as to effectively transfer learned representations between tasks [89].\nLifelong Learning emphasizes learning a new task in a sequence by transferring information from previously learned ones without forgetting how to solve them. Retaining previous task information can be done by mixing previous data with new ones during retraining [90]. Functionally composed modular lightweight networks have been proposed to learn a large variety of combinatorially related tasks to solve novel combination tasks in a zero-shot manner [91]."}, {"title": "3.4 Large Models", "content": "Large Language Models (LLMs) pretrained on internet-scale data have opened new avenues for robot motion generation. LLMs are capable of semantic reasoning and planning, making them candidates for extracting task specifications and creating action models for task planning [92]. LLMs have also been used to improve existing action models to handle failure cases [93]. These methods often do not generate constraints for low level motion in the operators as in TAMP. Furthermore, they have been used in conjunction with evolutionary optimization as code generators for defining rewards, which can be used to acquire complex skills via reinforcement learning [94].\nVision Language Models (VLMs) integrate information from visual input and language. Saycan [95] integrates a visual affordance model, which evaluates possible robot actions, with a language model that interprets the user's commands and generates high-level action plans. VLMs have also been used for autonomous generation of demonstration data over diverse tasks [96].\nThis lead to vision-language-action models (VLAs) or Robotics Foundation Models, i.e., large pretrained models that map actions to sensing data and language specifications. They can be fine-tuned for specific tasks to provide improvements in training time and generalization over trainingfrom scratch. Open-VLA [97] is such a model trained on a large robotic manipulation dataset [98] and utilizes large pretrained vision encoders and language models. It has been argued that it performs well on in-distribution tasks and robotic embodiments, while it can be fine-tuned for novel tasks and robotic embodiments."}, {"title": "4 Perspectives on Robot Motion Generation Research", "content": "Promises and Pitfalls of the Explicit Model Approach: These methods have a long history and can reliably solve various challenges today. For instance, planning collision-free motions for industrial arms is reliably addressed today at high speeds. Integration with state estimation allows mobile robots to execute navigation in semi-structured domains reliably. Challenges arise where model or state estimation reliability is low. Examples of such setups often involve the presence of complex contacts and partial observability, such as the manipulation of previously unknown objects in clutter, locomotion over uneven terrains, navigation at high speeds or in dynamic, unstructured environments.\nFor long-horizon tasks, TAMP methods require an engineer to encode possible pathways and concepts before symbolic reasoning, which often involves expensive combinatorial reasoning. These methods are limited to the variables defined and cannot easily anticipate changes from what has been programmed. While belief-space planning targets noise and partial observability, it requires significant computation and access to accurate models of uncertainty, which are not always available. At the same time, feedback-based solutions do find applicability in real-world domains, either via MPC or hierarchical control strategies but fully understanding the conditions under which a controller can solve a problem is an active challenge.\nPromises and Pitfalls of the Implicit Model Approach: The progress in ML promises effective data-driven motion generation methods that can access prior experience and do not require an engineered model or even accurate state estimation. They have unblocked perception tasks, such as object detection and estimation, which are often prerequisites for robust motion generation. Learning from demonstration, especially with diffusion processes that allow reflecting multi-modal distributions, can achieve impressive results exactly in tasks that the traditional, explicit model approaches face challenges with, such as dexterous manipulation and locomotion. This is true, however, as long as the setup upon execution resembles the demonstration setup, thus limiting generalization. The various versions of reinforcement learning bring the promise of broader generalization, and there have been many successful demonstrations of learning skills for robotic tasks via RL, though sample inefficiency still remains a bottleneck for achieving highly accurate solutions across a wide set of initial conditions.\nThese limitations have motivated the robotics community to pursue the direction of collecting more data for robotics problems in lab environments, which may be diverse across embodiments and tasks [98], towards the objective of mimicking the success of foundation models in language and vision challenges. While this direction should be pursued, it is not clear that it is possible to collect internet-scale demonstration data that will allow learning robust enough policies that cover the space of possible tasks that robots need to solve in novel, unstructured, and human environments. Furthermore, predicting when the resulting solutions will be successful is challenging, which is a significant concern, as failures in robotics can cause physical harm."}, {"title": "Integrative Directions", "content": "There is promise in the integration of solutions. For instance, data-driven methods training can benefit from simulation, where the explicit model approaches act as the demonstrators under full observability. The data-driven methods can learn to map sensing data to the actions demonstrated by the planners. This requires accurate enough models for the policies to be transferable to real systems, as well as strategies that makes these policies robust to varying conditions.\nUpon deployment, data-driven methods must be part of architectures that provide safety and verification, where they can benefit from explicit model methods. For instance, describing when a learned controller is successful, similar to control verification, can allow safe deployment. It can also assist in controller composition for long-horizon tasks, where high-level symbolic reasoning can be beneficial. Such task planning needs to be adaptive and allow a robot to dynamically define pre/postconditions, and discover new skills for its task. It should also be accompanied by failure explanation to identify why a problem is not solvable and guide data collection or reasoning for addressing similar challenges in the future. For explainability, maintaining an internal world model or a simulation (e.g., a \"cognitive, physical engine\") that is learned from data and first principles can be useful. There are already successful instances of integrating explicit and implicit models. There is work on natural-langauge-task-specification TAMP problems on large scenes where task plans from an LLM are verified using a model and pose-level planning is performed by a classical planner [99]. Another integration approach queries a VLM to solve a TAMP problem on the fly by creating 3D key points that resultin in a task plan as a sequence of path and goal constraints that guide motion planning [100].\nA gap towards integrative solutions is the lack of common interfaces, software components and benchmarks that would allow to easily switch and experiment with components from different methodologies. Most existing instances of software infrastructure either support one set of methods or the other, requiring the ad hoc tool composition for a novel integrative approach."}]}