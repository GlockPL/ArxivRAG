{"title": "FAIR4FREE: GENERATING HIGH-FIDELITY FAIR SYNTHETIC SAMPLES USING DATA FREE DISTILLATION", "authors": ["Md Fahim Sikder", "Daniel de Leng", "Fredrik Heintz"], "abstract": "This work presents Fair4Free, a novel generative model to generate synthetic fair data using data-free distillation in the latent space. Fair4Free can work on the situation when the data is private or inaccessible. In our approach, we first train a teacher model to create fair representation and then distil the knowledge to a student model (using a smaller architecture). The process of distilling the student model is data-free, i.e. the student model does not have access to the training dataset while distilling. After the distillation, we use the distilled model to generate fair synthetic samples. Our extensive experiments show that our synthetic samples outperform state-of-the-art models in all three criteria (fairness, utility and synthetic quality) with a performance increase of 5% for fairness, 8% for utility and 12% in synthetic quality for both tabular and image datasets.", "sections": [{"title": "INTRODUCTION", "content": "Nowadays, people rely on Artificial Intelligence-based applications to seek answers or make decisions. These AI-based models are trained with the data available in the real world. However, the available data in the real world is often full of machine or human biases (Liu et al., 2022). So, there is a possibility that those models will reflect biases when making a decision. For this reason, there is a strong need for bias mitigation models or bias-free datasets to ensure fairness within the models or datasets themselves. Furthermore, not all data is publicly accessible due to proprietary or sensitive cases, i.e. medical records. So, there is a need for a way to deal with these situations, too. Over the years, various approaches have been proposed to mitigate the bias issue from the model or datasets themselves, and researchers have categorized these techniques into three categories: Pre-processing, in-processing and post-processing techniques (Caton & Haas, 2024; Ntoutsi et al., 2020; Mehrabi et al., 2021). In the pre-processing technique, the dataset is processed to lower the correlation between the sensitive and non-sensitive attributes. However, in this process, valuable information can be lost. Post-processing techniques involve changing the output of the model in such a manner that the outcome of the model becomes fair towards demographics. These models also suffer accuracy as the model output is changed. In the in-processing techniques, the model is trained in such a way that during training time, the output of the model becomes fair. Though these approaches suffer from optimization problems (Oussidi & Elhassouny, 2018), a trade-off exists between fairness and accuracy. So, there is room for improvement.\nFair Generative Models (FGMs) are examples of in-processing bias-mitigation techniques. Over the years, different generative approaches have been proposed to tackle this issue. Variational autoencoder, Generative Adversarial Networks, and Diffusion-based models have seen outstanding performance in the tabular, text, and image domains (Jung et al., 2021; Choi et al., 2020; Wang et al., 2022). Different fairness constraints have been used to enforce the fairness quality in the generative samples, i.e., FairDisco (Liu et al., 2022) uses a distance correlation minimization method to weaken the connection between the sensitive and non-sensitive attributes. FairGAN (Li et al., 2022) uses dual discriminator-based generative adversarial networks architecture for generating fair synthetic samples. TabFairGAN (Rajabi & Garibay, 2022) generates synthetic fair data in two stages, first training a GAN to generate synthetic data, then adding a constraint on the synthetic samples to make it fair. FLDGMs (Ramachandranpillai et al., 2023) generate fair synthetic samples by generating the latent space with the help of GAN and diffusion architecture. Though these models"}, {"title": "RELATED WORK", "content": "Research on data fairness and fair models has recently seen advancement due to their usefulness in real-life decision-making and other matters. To create a fair model, some research focuses on creating fair representation, Learning Fair Representation (LFR) (Zemel et al., 2013) create fair representation by turning the process into an optimization problem. Their results show better fairness gain performance in the downstream task. Optimal Fair Classifier (Zhao & Gordon, 2022) gives a lower bound in the classification settings to characterize the balance between accuracy and fairness. FFVAE (Creager et al., 2019) learns fair representation by disentangling the fair latent space for multiple sensitive attributes. Different adversarial approaches have been taken over the years to learn fair representations, i.e. NRL (Feng et al., 2019) learns the fair representation using a generator and a critic with the help of a min-max game they have designed. A fair contrastive learning approach has been proposed to create fair representation for datasets where sensitive attributes are not present (Chai & Wang, 2022). Besides creating fair representation, focuses have"}, {"title": "PRELIMINARIES", "content": "In this section, we discuss the necessary background to follow the paper. First, we formulate the problem description followed by the definition of data fairness. Then, we discuss the background of knowledge distillation and generative models."}, {"title": "PROBLEM DEFINITION", "content": "Given a dataset tuple (\\{x_i, y_i, s_i\\}_{i=1}^B ~ D, where x \\in X represents non-sensitive attributes, y \\in Y is target variable and s \\in S is sensitive attribute, we need to create fair generative model H by distilling fair representation (z) in a data-free distillation environment."}, {"title": "DATA FAIRNESS", "content": "Data fairness can be measured from different viewpoints, i.e., a model can be fair if it shows equal performance for all the demographics. Over the years, different approaches have been shown to create a fair model that satisfies group or individual fairness. This work focuses on creating a generative model that provides group fairness. To achieve this, a model should follow definitions 1 and 2.\nDefinition 1 (Demographic Parity (DP), Barocas & Selbst (2016)). A binary prediction model f : X \u2192 Y, Y = \\{0,1\\} will achieve Demographic Parity (DP), iff\n$\\P[f(X) = 1 | S = 0] = P[f(X) = 1 | S = 1]$\nhere, S is the sensitive attribute and {0, 1} are representing different groups.\nDefinition 2 (Equalized Odds (EO), Barocas & Selbst (2016)). A binary prediction model f : X \u2192 \\hat{Y}, Y = \\{0, 1\\} will achieve Equalized Odds (EO), iff\n$\\P[f(X) = 1 | Y = 1, S = 0] = P[f(X) = 1 | Y = 1, S = 1]$\nhere, S is the sensitive attribute and {0, 1} are representing different groups."}, {"title": "KNOWLEDGE DISTILLATION AND GENERATIVE MODELS", "content": "Knowledge distillation works by taking a learned model and transfer the knowledge to a smaller architecture (smaller in layers and/or number of neurons) (Hinton et al., 2015). In the earliest distillation work (Hinton et al., 2015), the student model learned by taking the output label from the teacher model and the outcome of the student model using a supervised manner, also called response-based distillation. Most of the knowledge distillation work follows this approach (Li et al., 2023; Huang et al., 2022). However, it becomes challenging when we want to distil some distribution as they do not have any labels. Recently, Sikder et al. (2024a) used a combination of distillation loss and data-utility loss to distil the representation space without a label; however, that model requires training data to distil the latent space. Thus, distilling the model trained on private data will be difficult, especially since the data is inaccessible."}, {"title": "FAIR REPRESENTATION LEARNING", "content": "The first stage of our fair generative model is to learn a fair representation from biased data, D. We train a Variational Autoencoder (VAE) for this task and use the encoder (Eq) to get the fair representation. The encoder creates representation z = Eq(x,s), here (x, s) \\in D, x \\in X represents non-sensitive attributes and s \\in S represents sensitive attributes. Then the decoder reconstruct samples, x' = D\\theta(z,s). For learning the fair representation, along with the reconstruction loss and KL-divergence loss of VAE, distance correlation minimization loss $V_{\\beta}(z, s)$ and penalty"}, {"title": "DATA-FREE FAIR LATENT SPACE DISTILLATION AND SYNTHETIC DATA GENERATION", "content": "In this step, we take the trained fair encoder, $E_{\\phi}$ from step 1 and distil the knowledge of fair representation (z) to another architecture, $E_{\\psi}$, (we use less number of hidden features than used in $E_{\\phi}$) by first creating the latent fair representation, z = $E_{\\phi}(x, s)$, where, x, s \\in D. The main contribution of this work is to while distilling the latent space, z, to the model $E_{\\psi}$, we do not use any training data, (x, s). We feed Gaussian noise, n ~ N(0, 1) to the distilled model, and it produce some representation z' = $E_{\\psi}(n)$. We use a combination of distillation loss between the distilled representation (z') and fair representation (z) and KL-divergence loss on the output of the distilled model (Sikder et al., 2024a). The overall loss function is stated in Equation 2.\nHere, we use $L_{1}$-loss as L for the distillation loss. After the distillation, we use the distilled model, $E_{\\psi}$, and trained decoder, $D_{\\theta}$ from stage 1 to reconstruct high-fidelity fair synthetic samples, $x' = D_{\\theta}(E_{\\psi}(n)), n ~ N(0,1)$."}, {"title": "EXPERIMENTS", "content": "In this section, we present the experimental analysis of our work. We utilize two tabular and two image datasets to evaluate how our model performs across various dataset types. We compare the result of our model with several state-of-the-art fair models, i.e., Decaf (Van Breugel et al., 2021), TabFairGAN (Rajabi & Garibay, 2022), FairDisco (Liu et al., 2022), FLDGMs (Ramachandranpillai et al., 2023) in terms of fairness and utility. We further compare the works with Correlation Remover (Weerts et al., 2023) and Threshold Optimizer (Hardt et al., 2016), pre and post-processing techniques, respectively. We use FairX (Sikder et al., 2024b), a fairness benchmarking tool to load the dataset, train and evaluate the benchmark."}, {"title": "DATASET PREPROCESSING", "content": "We use four benchmarking datasets to train and evaluate our model. \u201cAdult-Income\" and \u201cCompas\u201d (Angwin et al., 2016) are two widely used tabular dataset and \"CelebA\" (Liu et al., 2015) and"}, {"title": "EVALUATION METRICS", "content": "We evaluate the performance of the distilled fair representation and fair synthetic samples regarding fairness, utility and synthetic sample quality (only for the synthetic samples). For both fairness and utility evaluation, we run a downstreaming task (explained in section 5.3) and evaluate the performance of the synthetic samples on Accuracy, F1-Score, Recall (utility metrics) and Demographic Parity Ratio (DPR) (Weerts et al., 2023), Equalized Odds Ratio (EOR) (Weerts et al., 2023) (fairness utility). Along with utility and fairness evaluation, we also measure the synthetic data quality of the fair generative model. We use the Density and Coverage (Alaa et al., 2022) metrics to validate if the generated samples have the same distribution as the original samples.\nBesides empirical evaluation, we also show the quality of synthetic samples and distilled latent space with visual evaluation. We use PCA (Bryant & Yarnold, 1995) and t-SNE (Van der Maaten & Hinton, 2008) plots to show how closely the distribution of the distilled latent space and fair latent space matches. Also, we show the synthetic samples generated for the image dataset."}, {"title": "DOWNSTREAMING TASK FOR EVALUATION", "content": "For the empirical evaluation, we set up a downstreaming task to determine the performance of the synthetic samples in terms of fairness and data utility. In the setup, we train a random-forest (Breiman, 2001) model for a supervised task using the features (sensitive (s) and non-sensitive (x) attributes) and target (y) based on the sensitive attributes, then evaluate in perspective of fairness and data utility. For example, for the \u201cAdult-Income\" dataset, we have {gender, race} as sensitive attributes. Hence, we train the random forest for each sensitive attribute and measure the fairness and data utility for respective s."}, {"title": "RESULTS AND DISCUSSION", "content": "We conduct extensive experiments and compare the performance of our model with six fair models. In this section, we discuss and analyze the result."}, {"title": "EMPIRICAL ANALYSIS FOR TABULAR DATA", "content": "We show the performance of our model in the downstreaming task regarding fairness and data utility for the \"Adult-Income\" dataset in Table 2 and the \"Compas\" dataset in Table 3. We use the {gender, race} as sensitive attributes and record the results for both tables. For the \"Adult-Income\" dataset, we predict the income class for an individual given their attributes (both sensitive and non-sensitive) as downstreaming task. And for the \"Compas\" dataset, we predict the re-offend probability for an inmate given their previous records.\nData Utility Analysis We measure the Accuracy, Recall and F1-score from the downstreaming task. We observe from Table 2 that our synthetic samples achieve 5% and 8% better performance in fairness and utility compared to FLDGMs (state-of-the-art model). In Table 3, the performance of"}, {"title": "VISUAL ANALYSIS FOR TABULAR AND IMAGE DATA", "content": "Besides the empirical quality of our synthetic samples, we also show visual analysis. We show the synthetic image samples from both CelebA and Colored-MNIST in Figure 1 and 2. We use \"Smiling\" as the sensitive attribute for the CelebA and \"Colors (Red, Green, Blue)\" for the MNIST dataset.\nFor the tabular dataset, we use the PCA and t-SNE plots to show how closely the distribution matches the distilled representation and the original fair representation. We observe from Figure 3 that the distributions closely match each other; this signifies that our data-free distillation method transfers the knowledge correctly."}, {"title": "FEATURE IMPORTANCE ANALYSIS", "content": "Along with the empirical evaluation and visual evaluation, we also test the feature importance of the original and synthetic data in downstreaming task. We set up a task, i.e., for the Compas dataset, given the inmate's records, to predict how likely the person will re-offend. Figure 4 shows the feature importance for both original and synthetic data (in this case, we use Gender (sex) as a sensitive attribute.). We observe that, for the original dataset, gender plays a vital role in reaching the decision, on the contrary, our synthetic samples do not rely on the sensitive attribute for making a decision. This also proves the usefulness of our synthetic samples for fair decision-making process."}, {"title": "DISCUSSION", "content": "This work presents Fair4Free, a data-free distillation-based fair generative model. With this approach, we generate high-fidelity fair synthetic samples with the help of knowledge distillation. We distil the fair representation from the trained model (teacher model) to another architecture (student model). During the distillation process, we do not use any training data; thus, the training of the student model is data-free. This helps when the dataset is unavailable for security and privacy reasons. Also, as we use a small architecture, we reduce the computational cost, and we can deploy the model into an edge device with better performance than the teacher model. Tables 2 and 3 show that our model is over-performing the state-of-the-art models in the perspective of data utility, fairness and synthetic quality. Besides empirical evaluation, Figure 1, 2 shows the synthetic data samples, and Figure 4 shows the usefulness of fair synthetic samples in decision-making.\nSocial Impact The Fair Generative model can play a vital role in the decision-making process because the generated samples do not consider the sensitive attributes while making decisions. This process helps to reduce bias and discrimination in the real-world scenario, i.e. fair recommendations in the healthcare system (Ramachandranpillai et al., 2024), fair economic recommendation (decision making on loans). Also, the fair model can help build trust with the user so it can be widely used in society.\nLimitations and Future Works In our experiments, we use a single sensitive attribute to train the generative model, i.e., we use either Gender or Race (for both Adult-Income and Compas datasets). So, in order to tackle intersectional bias, we need to work on a generative model that handles multiple sensitive attributes. However, we believe the data-free distillation process we present in this work can also be used if we have fair representation with multiple sensitive attributes."}, {"title": "CONCLUSION", "content": "Real-world data is filled with human and/or machine biases, and in the era of AI-powered decision-making systems, these biased data can cause harm to specific people as these models are trained with them. Furthermore, some datasets are not available publicly due to proprietary cases or restricted by data protection laws like GDPR or HIPPA to protect privacy. So, one way to tackle the data limitation and bias issue is to use a generative model. This work presents Fair4Free, a novel fair generative model that uses data-free distillation to generate fair synthetic samples. We pre-train a VAE with the biased data and produce fair representation in the latent space, then use another architecture to distill the fair representation. The distillation process is completely data-free, and then we use the distilled fair representation to create fair synthetic samples. Our extensive experiment shows that the quality of the synthetic samples outperforms state-of-the-art models regarding fairness, utility, and synthetic quality. As we use a distillation process and smaller architecture for the distilled model, these models can be deployed in the edge devices. Also, as the synthetic samples are fair towards demographics, these can help to mitigate the biased data issue."}]}