{"title": "The Vizier Gaussian Process Bandit Algorithm", "authors": ["Xingyou Song", "Qiuyi Zhang", "Chansoo Lee", "Emily Fertig", "Tzu-Kuo Huang", "Lior Belenki", "Greg Kochanski", "Setareh Ariafar", "Srinivas Vasudevan", "Sagi Perel", "Daniel Golovin"], "abstract": "Google Vizier has performed millions of optimizations and accelerated numerous research and production systems at Google, demonstrating the success of Bayesian optimization as a large-scale service. Over multiple years, its algorithm has been improved considerably, through the collective experiences of numerous research efforts and user feedback. In this technical report, we discuss the implementation details and design choices of the current default algorithm provided by Open Source Vizier. Our experiments on standardized benchmarks reveal its robustness and versatility against well-established industry baselines on multiple practical modes.", "sections": [{"title": "1. Introduction", "content": "Google Vizier (Golovin et al., 2017) is one of the world's largest black-box optimization services; it has tuned more than 70 million objectives across Google, and is also available to the public via Vertex Vizier, a Google Cloud product.\n\nVizier's default algorithm is loosely based on Gaussian process bandit optimization (Srinivas et al., 2010), but has evolved over time to improve not only standard performance metrics such as regret and optimality gap, but also user experience, inference speed, flexibility, scalability, and reliability."}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Blackbox Optimization", "content": "We use the standard terminology found in (Golovin et al., 2017) and (Song et al., 2022), where an optimization study consists of a problem statement and trajectory of trials. A problem statement consists of:\n\n\u2022 Search space X, which is the cartesian product of parameters of type DOUBLE, INTEGER, DISCRETE, or CATEGORICAL. Except for CATEGORICAL types, each parameter also can have an associated scaling type (linear, log, or reverse log).\n\u2022 Measurement space Y \u2286 RM which consists of M metrics from the blackbox objective function.\n\nThe blackbox function to be optimized f : X \u2192 Y is unknown to the algorithm, and may be stochastic. A trial consists of a pair (x, y), where x \u2208 X is the suggested parameters for the blackbox objective function, and y = f(x) \u2208 Y is a measurement. Assuming a canonical ordering of parameters and metrics within a study, we denote x(i) and y(i) to be the i-th parameter and its associated measurement, respectively. To simplify notation, we assume a single-metric objective function f : X \u2192 R unless otherwise specified.\n\nA trajectory of trials (x1, y1, x2, y2, ..., Xt, yt) can be obtained by an iterative optimization loop where an algorithm proposes new parameters x\u2081 and obtains yt = f(xt) through (possibly noisy) evaluation. Abstractly, the goal of blackbox optimization is to quickly improve y through multiple iterations which can be concretely defined with a performance measure over the trajectory such as best-so-far (corresponding to simple regret) or average performance (corresponding to cumulative regret), based"}, {"title": "2.2. Bayesian Optimization", "content": "We provide a brief summarization of the GP-UCB algorithm (Srinivas et al., 2010) for Bayesian optimization. Given a Gaussian process with a prior mean \u03bc\u00f5 : X \u2192 R and kernel K : X \u00d7 X \u2192 R>0, we can define its mean and standard deviation functions respectively as \u03bc : X \u2192 R and \u03c3 : X \u2192 R>0.\n\nWhile this algorithm is conceptually clear, there are many details which crucially affect overall optimization performance and algorithm serving in a distributed production system.\n\n\u2022 Input Preprocessing: Transforming the parameters into a unit L-infinity ball eases modeling.\n\u2022 Output Preprocessing: Nonlinear transformations of the measurement y \u2208 RM into a smaller subspace can mitigate harmful effects of outliers and shift model capacity towards discerning finer distinctions among good trials and away from poor trials.\n\u2022 Model Choice: The choice of kernel and priors over its hyperparameters are critical (Chen and Wang, 2018). For posterior updates, we use the Maximum A Posteriori (MAP) estimates of the kernel hyperparameters.\n\u2022 Acquisition Function: UCB is but one option of many acquisition functions which influence explore / exploit tradeoffs, diversity of selected trials, and handling of multiple metrics.\n\u2022 Acquisition Maximization: In general, maximizing the acquisition function is non-trivial, and the acquisition optimizer affects overall performance significantly."}, {"title": "3. Vizier's Gaussian Process Bandits", "content": "In this section, we outline the key components of the default Vizier algorithm, with additional details in Appendix B. Over time, these components on the C++ stack co-evolved, and thus form something approximating a local optimum in the design space."}, {"title": "3.1. Preprocessing", "content": "In order to improve the accuracy of our Gaussian Process regression model, we preprocess both inputs and outputs into \u00ee and \u0177 respectively, which are also reversible back into the original X, Y spaces. Note: For conciseness in the paper, we use \u00ee, \u0177 notation only when needed for mathematical precision."}, {"title": "3.1.1. Input Preprocessing", "content": "To provide reasonable scales as inputs, we enforce \u00ee \u2208 [0, 1]D for non-CATEGORICAL parameters. Using the provided bounds $x_{min}^{(d)}$ and $x_{max}^{(d)}$, for linear scaling, we transform the original value $x^{(d)}$ into $(x^{(d)} - x_{min}^{(d)})/(x_{max}^{(d)} - x_{min}^{(d)})$. For log scaling, we first apply log(\u00b7) to the input and bounds before applying the scaling transformation, and apply analogously for reverse-log scaling."}, {"title": "3.1.2. Output Preprocessing", "content": "To provide y objectives as suitable formats \u0177 for GPs, we perform multiple stages of transformations, all of which are relative to the observed values (y1, y2, ..., yt). These are asymmetric and assume maximization is the goal.\n\n\u2022 Linear Scaling: Let \u03be(I) := $(\\sum_{i \\in I} {(y - y_{median})^2} : i \\in 1})^{1/2}$. Divide by \u03be({i : yi \u2265 ymedian}) if non-zero, else by \u03be({i : 1 \u2264 i \u2264 t}) if non-zero. Finally, shift the data so the median is zero.\n\u2022 Half-Rank Warping: Supresses harm from extremely bad outliers. This warping rescales all unpromising objectives (i.e., worse than the median) so that scaled values are distributed as the lower half of a normal distribution, \u2013|N(0, 1)|, ensuring the typical deviation for poor objectives roughly matches those for good objectives. Median or better objectives remain unchanged.\n\u2022 Log Warping: Increases modeling resolution for good values. We first normalize all objectives as $\\hat{y} \\leftarrow \\frac{y - y_{min}}{y_{max} - y_{min}}$ and then define the warped values as $\\hat{y} \\leftarrow 0.5 - \\frac{\\log(1 + \\hat{y}(s - 1))}{\\log(s)}$ where s is a free parameter set to 1.5. This gently stretches the intervals between better y-values and compresses the intervals between worse values.\n\u2022 Infeasibility Warping: We replace all infeasible objectives with $y_{min} \u2013 (0.5(y_{max} \u2013 y_{min}))$. This maps infeasible objectives to a reasonably scaled unpromising value to prevent the optimizer from re-exploring infeasible regions.\n\u2022 Mean Shifting: Finally, we shift the objectives to have zero mean."}, {"title": "3.2. Gaussian Process Model", "content": "User objectives are commonly Lipschitz-continuous over preprocessed featurized search spaces, with respect to e.g., coordinated-scaled Euclidean distance. Thus we use a Matern-5/2 kernel (Rasmussen and Williams, 2006) along with length-scaled automatic relevance determination (ARD) in the GP model. The probabilistic model is defined as follows, where N[a,b] is the normal distribution truncated to range [a, b], parameterized by mean and variance:\n\n\u2022 $a_{log} ~ N_{[-3,1]}(\\log 0.039, 50)$ is the log of the amplitude of Matern kernel.\n\u2022 $\\lambda_{log}^{(d)} ~ N_{[-2,1]}(\\log 0.5, 50)$ is the log of the squared length scale for the d-th dimension, i.i.d. for d = 1, . . ., D. Denote $\\lambda_{log} = (\\lambda_{log}^{(1)},..., \\lambda_{log}^{(D)})$.\n\u2022 $\\epsilon_{log} ~ N_{[-10,0]}(\\log 0.0039, 50)$ is the log of the standard deviation of the Gaussian noise.\n\u2022 K(\u00b7, \u00b7) is the Matern-5/2 kernel with amplitude exp($2a_{log}$) and length scales exp($2\\lambda_{log}$), applied over x."}, {"title": "3.3. Posterior Updates", "content": "Given observed data {xs, \u0177s}s=1, we seek the MAP estimate of kernel hyperparameters which maximizes the joint probability Pr({x,y}, alog, log, Elog). That is, we maximize\n\n$\\log \\Pr(a_{log}) + \\log \\Pr(\\lambda_{log}) + \\log \\Pr(\\epsilon_{log}) + \\log \\Pr(({x,y}_{s=1}^t; a_{log}, \\lambda_{log}, \\epsilon_{log})$\n\nfor alog, Alog, Elog within the support of their respective prior distribution.\n\nWe use Scipy's L-BFGS-B to solve the constrained maximization problem, selecting the best results from four random initializations sampled from the hyperparameter priors. We sample the initial points from the uniform distribution of the truncated range."}, {"title": "3.4. Acquisition Function and Trust Regions", "content": "We use a fixed UCB coefficient of $\\sqrt{\\beta} = 1.8$, which is relatively large compared to other open source settings (Cowen-Rivers et al., 2022). While the large coefficient can lead to better exploration, the downside is that regular GP-UCB would spend much of its initial budget exploring search space corners, as being furthest away from support points, they have the largest posterior variance estimates and thus highest acquisition values.\n\nIn order to control this behavior, we apply a trust region to numeric parameters, which is a union of l-balls around the \u201ctrusted\u201d previously observed points {xH}H-1 in parameter space, or any points obtainable from those by changing categorical parameter values. The radius of these balls start at 0.2 in the scaled parameter space and grows unboundedly. When optimizing the acquisition function, any evaluation outside the trust region gets strongly penalized, adjusted by the distance to the nearest trusted point. This is expressed as:\n\n$x \\leftarrow\\begin{cases}UCB(x) & \\text{if dist(x, trusted) } \\leq \\text{radius} \\\\(-10^{12} - \\text{ dist(x, trusted)} & \\text{if dist(x, trusted) > radius}\\end{cases}$\n\nThe penalty value \u20131012 is guaranteed to be lower than UCB(x) due to our preprocessing steps and zero mean function. The additional distance term in the penalty is needed to provide gradient directions for acquistion optimizers to move into the trust region."}, {"title": "3.5. Acquisition Function Optimization", "content": "We use a customized version of the Firefly algorithm (Yang, 2009) where the mutation operators are customized per-datatype so that suggestions always correspond to points from the original X.\n\nNote that Ax (Balandat et al., 2020) by default uses L-BFGS-B for all-continuous spaces and a sequential greedy algorithm for mixed spaces, while HEBO (Cowen-Rivers et al., 2022) uses NSGA-II (Deb et al., 2002a). Other alternatives include policy gradients as in (Daulton et al., 2022)."}, {"title": "3.5.1. Firefly Algorithm", "content": "The Firefly algorithm (Yang, 2009) is a nature-inspired metaheuristic, particle swarm optimization technique (Kennedy and Eberhart, 1995) that draws inspiration from the flashing behavior of fireflies. In the original algorithm, a pool P of fireflies is maintained such that each firefly \u27a4 represents a potential solution to the optimization problem, and its \u201cbrightness", "dimmer firefly\" Xlow with a worse objective value will move towards towards a \u201cbrighter": "high, with the movement step's magnitude varying with distance r(xlow, Xhigh) = ||xlow - xhigh ||2, which we shorthand as r. The individual firefly update rule is expressed formally in Equation 3:\n\n$X_{low} \\leftarrow X_{low} + \\eta \\exp(-\\gamma r^2)(X_{high} \u2013 X_{low}) + N(0, \\omega^2 I_D)$\n\nwhere n is the attraction coefficient, y is an an absorption coefficient and N(0, w\u00b2ID) is an appropriately scaled random noise vector introduced to enhance exploration."}, {"title": "3.6. Batched Setting", "content": "In many practical settings, users will request: (1) additional suggestions before all previous suggestions have finished being evaluated, or (2) a simultaneous batch of suggestions. In the context of Bayesian Optimization, algorithms for the first case can be used to incrementally construct batches for the second case. In either case, we must factor in previous suggestions as we wish to avoid accidentally (approximately) duplicating prior proposals. To do this, the algorithm looks at unevaluated suggestion features, and we define the GP via:\n\n\u2022 Mean function \u03bc\u2081(\u00b7), which is still based on evaluated trials D\u2081 = {xs, ys}=1, equivalent to the sequential case.\n\u2022 Standard deviation function \u03c3\u03c4(\u00b7), which additionally accounts for unevaluated suggestions Ut = {xv, 0}v=1 given dummy zero objectives, an example of a \u201cconstant liar\u201d heuristic (Ginsbourger et al., 2010).\n\nFollowing the ideas of Contal et al. (2013) but modified to improve performance and align with infrastructure, we augment our GP-UCB algorithm with pure exploration, where we define two acquisition functions:\n\n\u2022 UCB (Upper Confidence Bound):\n\n$UCB(x|D_t, U_t, \\beta) := \\mu_t(x|D_t) + \\sqrt{\\beta} \\cdot \\sigma_t (x|D_t \\cup U_t)$\n\n\u2022 PE (Pure Exploration):\n\n$PE(x|D_t, U_t, \\tau_t, \\beta_e, \\rho) := \\sigma_t(x|D_t \\cup U_t) + \\rho \\cdot \\min(UCB(x|D_t, \\O, \\beta_e) - \\tau_t, 0)$\n\nwhere p is a penalty coefficient, \u00d8 denotes empty set, \u1e9ee is an exploration-specific UCB parameter, and tt is a threshold on the UCB value for exploration, computed solely conditioned on evaluated trials Dt."}, {"title": "3.7. Multi-objective optimization", "content": "In multiobjective optimization, Y = RM, and f is a vectorized function of scalar functions, i.e. f(x) = (f(1)(x), ..., f(M)(x)). For y1, y2 \u2208 Y, we say y2 Pareto dominates y1, written as y1 < y2 if for all coordinates i, $y_1^{(i)} < y_2^{(i)}$ and there exists j with $y_1^{(j)} < y_2^{(j)}$. For two points x1, x2 \u2208 X, we say that x2 Pareto-dominates x1 if f(x1) < f(x2). A point is Pareto-optimal in X if no point in X dominates it. Let X* \u2282 X denote the set of Pareto-optimal points in X, and correspondingly Y* = f(X*) denote the Pareto frontier.\n\nThe goal of multiobjective optimization is to find the Pareto frontier. Our main measure of progress is the hypervolume that is Pareto dominated by the metric vectors obtained so far. Specifically, for a compact set S\u2286 RM, let vol(S) be the hypervolume of S. For a set of metric vectors Y CRM, we use the (dominated) hypervolume indicator of Y with respect to a reference point yref as our progress metric:\n\n$HV_{y_{ref}}(Y) = vol({u|y_{ref} \\leq u \\text{ and } \\exists y \\in Y \\text{ such that } u < y})$\n\nA natural approach for maximizing the hypervolume is to greedily maximize the hypervolume gain at every step t, leading to widespread use of the Expected Hypervolume Improvement (EHVI) acquisition and its differentiable counterpart (Daulton et al., 2020). Unfortunately, computation of the hypervolume indicator is #P-hard in general (Bringmann and Friedrich, 2010)."}, {"title": "3.7.1. Hypervolume Approximation via Scalarization", "content": "Practitioners often solve multiobjective problems by choosing scalarization functions from metric vectors to scalars, and optimizing the latter. Let sw : RM \u2192 R denote a family of scalarization functions parameterized by w. By optimizing sw (f(x)) for various w to obtain the next suggestion, we effectively reduce the multiobjective case to the single objective case. A key observation is that when sw is monotonically increasing with respect to all coordinates in y, arg maxx sw(f(x))} is on the Pareto frontier.\n\nWe utilize the particular family of hypervolume scalarizations to ensure the converse: each point on the Pareto frontier equals arg maxx sw(f(x))} for some w. Additionally, the Pareto frontier can be provably estimated in an unbiased manner by sampling w and averaging the optimizers of the hypervolume scalarizations:"}, {"title": "3.7.2. Acquisition Scalarization and Multi-task GP", "content": "Based on our approximation method above, a natural way to produce an acquisition for multiobjective posteriors is to apply scalarizations on a vectorized per-metric upper confidence bound UCB(x) = (UCB(1)(x), ..., UCB(M)(x)). Then, we proceed to perform classic maximization on the average scalarized acquisition Ew [Sw(UCB(x))] with hypervolume scalarizations, which is related to the hypervolume of the upper confidence vector via Theorem 1, and can be approximated by averaging over thousands of random draws of weights w, due to fast JAX linear algebraic operations.\n\nSince we are measuring the improvement of the cumulative hypervolume upon adding a point x, our acquisition must substract the maximized scalarized value maxy\u2208 D\u2081 {Sw(y)} across previous trials in the dataset Dr. We find that using hypervolume-scalarized vector UCB works well in practice, as compared to other choices of acquisition such as EHVI, and believe that its bias towards optimism helps find better exploration and exploitation tradeoffs in hypervolume maximization. Furthermore, this method is provably optimal for minimizing hypervolume regret (Zhang, 2023):\n\n$X_{t+1} = argmax_x \\mathbb{E}_w[\\max{0, s_w(UCB(x))} - \\max_{y\\in D_t} {s_w(y)}]$\n\nNote the reference point yref is an implicit part of the acquisition function. We determine it using lower bounds on each metric individually (as detailed in Appendix B.5).\n\nTo model each of the y(i), we use the same GP modeling assumptions as in the single-objective case, with the only difference being that this multi-task GP allows for modeling correlations between"}, {"title": "3.8. Trial Seeding and Initialization", "content": "In order for the Gaussian Process to obtain a rich spread of evaluations over the search space, we use a different selection for the first few trials.\n\n\u2022 Initial Centering: In many practical cases, users tend to define search spaces which contain the optimum. We fix the initial trial to be the center of the search space (uniform sampling for CATEGORICAL parameters) which obtains a relatively close guess to this optimum, especially for high dimensions.\n\u2022 Quasi-Random Search: Afterwards, we may optionally sample trials quasi-randomly using Halton sequences (Niederreiter, 1992) as opposed to i.i.d. randomly, in order to obtain a more uniform spread over the search space, with the number of quasi-random trials proportional to the parameter count of X."}, {"title": "4. Experiments: Ray Tune Comparisons", "content": "We compare the Vizier algorithm with well-established baselines published on Ray Tune (Liaw et al., 2018): Ax (Balandat et al., 2020), BayesianOptimization (Nogueira, 2014), HEBO (Cowen-Rivers et al., 2022), HyperOpt (Bergstra et al., 2015), Optuna (Akiba et al., 2019), and Scikit-Optimize (Head et al., 2018). These baselines are all variants of Bayesian optimization, but mostly differ in their acquisition functions and optimizers."}, {"title": "4.1. Evaluation Protocol", "content": "Our emphasis for this paper is on production-quality and user accessibility, implying a stronger focus on robustness and out-of-the-box behavior without the need for knob-tuning. Thus we present results using default settings for all algorithm baselines in our main body, with Appendix A containing comparisons when certain knobs are changed. Using defaults also aligns with the intentions of algorithm developers, who may provide their own algorithm selectors (e.g., Ax selects its algorithm based on search space). We further diversify our collection of benchmark functions (Appendix D) to avoid possible issues if certain algorithms were tuned for any specific benchmarks.\n\nTrajectory Generation: Unless otherwise specified, every optimization trajectory is run with 100 trials with 20 repeats, and we plot the median curve along with 40-60 percentile error bars among all figures.\n\nLog-Efficiency Violin Plots: To get algorithm comparisons aggregated over multiple objective functions, we use the log-efficiency metric, explained further in Appendix E. In summmary, for a Ray Tune algorithm A, we define its RequiredBudget(y | f, A) with respect to an objective f and a target value y as the minimum number of iterations required to reach or surpass y. Then we may define relative"}, {"title": "4.2. Purely Continuous Spaces", "content": "We begin by benchmarking over continuous spaces in a sequential manner where trials are suggested and evaluated one-by-one. This setting is the most researched and supported by all baselines."}, {"title": "4.3. Non-Continuous Spaces", "content": "Many important hyperparameters and tuning decisions are inherently non-continuous (e.g. choosing to use neural networks vs. random forests), and therefore assessing performance over categorical spaces is crucial."}, {"title": "4.4. Batched Case", "content": "Given the above sequential results, it suffices to benchmark the best performing baselines with batched capabilities, namely Ax and HEBO. We vary the batch size (i.e. \"max_concurrent_trials\" in the Ray Tune API) and see that in Figure 11, even for large batches, Vizier still remains efficient against random search, while HEBO's performance in particular degrades, demonstrating the importance of our pure exploration mechanism from Section 3.6."}, {"title": "4.4.1. Multi-Objective Case", "content": "We directly use Ax, HEBO, and Optuna via their official API for benchmarking comparisons, as multi-objective optimization is not directly supported in Ray Tune. Our problems consist of collections of well-known synthetic functions designed specifically for multi-objective optimization, specifically \"DTLZ\" (Deb et al., 2002b), \u201cWFG\u201d (Huband et al., 2006), and \u201cZDT\u201d (Zitzler et al., 2000), totalling 21 different functions. All functions allow varying dimensions, and with the exception of ZDT, also"}, {"title": "5. Experiments: Ablations", "content": ""}, {"title": "5.1. Firefly vs. L-BFGS-B for Acquisition Optimization", "content": "The acquisition function landscape can have many local maxima, which under some conditions can be comparable to the number of support points. Vectorized Firefly handles such a landscape well, by simultaneously exploring multiple promising regions of the search space efficiently over O(10K) points. In contrast, second-order algorithms such as L-BFGS-B (details in Appendix C) make overly strong assumptions about acquisition function landscape shape and plateau too early."}, {"title": "5.2. Latency and GPU Acceleration", "content": "Since components in the GP-bandit algorithm were implemented in JAX and optimized with JIT-compilation, wall-clock speedups can be obtained when using accelerators. In Figure 16, we see that the algorithm runs significantly faster on GPU compared to CPU, making serving the algorithm quite cheap."}, {"title": "6. Conclusion", "content": "We have provided the Vizier default algorithm implementation details, which can be organized in terms of trial preprocessing, response surface modeling with Gaussian process prior and kernel, acquisition function definition, and evolutionary acquisition optimization. Throughout our experiments, we have demonstrated the algorithm's robustness over other industry-wide baselines across multiple axes, namely non-continuous parameters, high-dimensional spaces, batched settings, multi-metric objectives, and even numerical stability. This robustness has proven to support a wide variety of use-cases within Google and we expect it to additionally be an invaluable tool for the external research community."}, {"title": "A. Extended Experiments", "content": ""}, {"title": "A.1. Comparison with Ax's UCB", "content": "One question raised by the main experiments is whether the performance gaps between Vizier and e.g. Ax are purely due to differences in acquisition function definition, as Ax uses the family of expected improvement (EI) acquisitions (Ament et al., 2023; Mockus, 1974) by default, rather than UCB. In order to resolve this hypothesis, we further modify Ax to use the UpperConfidenceBound acquisition method with \u221a\u1e9e=1.8, similar to Vizier's UCB. We further disable this variant's SOBOL (i.e. quasi-random) sampling for an even more direct comparison to Vizier's single-objective method.\n\nIn Figure 17, we see that the median roughly remains the same for Ax-based methods, regardless of acquisition function definition, although UCB leads to a higher variance in performance. In Figure 18, we see some of these individual cases, and underperformance is most prounounced in some functions such as Gallagher101Me where both variants of Ax are unable to improve in the first half of the trial budget.\n\nThese results highlight the importance of other components within Vizier (e.g. acquisition maximization, kernel definition, and prior hyperparameter optimization) which also contribute to its robustness and success."}, {"title": "A.2. Noisy Objectives", "content": "Many real world evaluations tend to be noisy, i.e., repeated evaluations of f(x) at a fixed x may lead to different values. A typical mathematical abstraction is to assume that f(x) inherently produces a distribution of values over R from which measurements y may be sampled.\n\nFor BBOB functions, there is a standard set of \u201cnoise models\" which can be wrapped over the original deterministic functions, as prescribed in Hansen et al. (2009). These consist of two multiplicative (Gaussian and Uniform) and an additive (Cauchy) noise model, which are described in Appendix D. While the algorithm observes the noisy value per evaluation, our definition of performance however is still based on the original deterministic value.\n\nIn Figure 19, we benchmark over all three noise models (with their strongest suggested \u201csevere\u201d settings) over every BBOB function, and see that the plot is similar to the original Figure 6, albeit with a few additional positive log-efficiency outliers from baselines.\n\nWe hypothesize that since all of the baselines preprocess and normalize the observed y-values, the rankings of trials remain fairly stable, and thus the baseline performances remain robust to noise. Furthermore, small differences in the optimization trajectory generally do not affect the log-efficiency metric, which holistically measures the convergence curve."}, {"title": "B. Vizier GP Bandit: Extended Details", "content": "This section serves to clarify additional details for readers who are less familiar with Bayesian Optimization or are interested in lower-level specifics."}, {"title": "B.1. Kernel Definition", "content": "We use the Matern-5/2 kernel, whose exact implementation is found in TF Probability\u00b9. Naturally, for continuous parameters, we may define the distance between two features \u00ee\u2081 and \u00c2\u00a1 to be Euclidean-based. However, different coordinates may have different scales, and thus we need to account for normalization. We thus compute a length-scaled distance, defined as:\n\n$\\delta(\\vec{x}_i,\\vec{x}_j)^2 = 5 \\cdot \\sum_{d=1}^D \\frac{(\\vec{x}_i^{(d)}-\\vec{x}_j^{(d)})^2}{\\lambda_{log}^{(d)}}$\n\nwhere \u3121 = (\u03bb (1), ..., \u03bb (D)) = exp(log) are the squared length-scales. For CATEGORICAL parameters, the contribution from each parameter x(c) is instead\n\n$\\frac{1_{x_i^{(c)} \\neq x_j^{(c)}}}{{\\gamma^{(c)}}^2}$\n\nwhere (c) is a single trainable value. This logic is specifically implemented by the FeatureScaledWithCategorical kernel in TF Probability.\n\nUsing 8 = 8(x\u2081, xj) for brevity, this distance will then be used to define the kernel:\n\n$K(\\vec{x}_i,\\vec{x}_j) = \\alpha^2 \\cdot (1+\\delta+\\frac{{\\delta}^2}{3}) \\cdot \\exp(-\\delta)$\n\nwhere a = exp(alog) \u2208 R is an additional amplitude argument. This kernel together with a zero prior mean thus defines the Gaussian Process."}, {"title": "B.2. Firefly Acquisition Optimizer", "content": "For completeness, we formalize the vectorized firefly mechanic in Algorithm 3."}, {"title": "B.3. Batched Optimization", "content": "We formalize the pure exploration batched method in Algorithm 4."}, {"title": "B.4. Just-in-time (JIT) Compilation Optimization", "content": "JAX (Bradbury et al., 2018) utilizes just-in-time compilation to optimize a given computation graph. Compiled graphs are stored into a global cache, which may only be used if tensor input/output shapes"}, {"title": "B.5. Exact Hyperparameters", "content": "All exact hyperparameters can be observed from the code2 in google-vizier[jax]==0.1.7.\n\nMAP Estimation: To optimize the kernel hyperparameters alog, \u03bb\u00b4log, \u00a3log, our L-BFGS-B uses 50 maximum iterations, each step using a maximum of 20 line search steps. The optimizer is restarted for 4 times each with different random initializations, eventually returning the best hyperparameters.\n\nTrust Region Schedule: The radius is scheduled based on the number of observed points t and post-processed dimension D, specifically:\n\n$radius = 0.2 + (0.5 \u2013 0.2) \\frac{1}{5(\\frac{t}{D}+1)}$\n\nIf this radius is > 0.5, then the trust region is disabled (i.e. the radius becomes infinite).\n\nFirefly Optimizer: We use a maximum of 75000 evaluations, with a batch size of p = 25 and pool size P = min{10 + D + D1.2, 100}, thus leading to a maximum of MAX_ITERATIONS = [75000/P]. Further hyperparameters below control the specific update rules.\n\nFor force computations, y = 4.5/D, \u014battract = 1.5, and nrepel = 0.008.\n\nFor perturbations, if the space is hybrid, then @continuous = 0.16 and @categorical = 1.0, whereas if the space is purely categorical, @categorical = 30 for better exploration. Unsuccessful fireflies (i.e. those which did not improve acquisition score have their perturbation further scaled down by 0.7.\n\nThe pool of fireflies also has a keep probability of 0.96, in which new random fireflies may be introduced if a firefly is not kept.\n\nMulti-objective Acquisition: We use 1000 scalarizations for hypervolume approximation, and define our hypervolume reference point (Ishibuchi et al., 2018) as yref := \u0177worst - 0.01 (\u0177best - \u0177worst), where ybest and worst are, respectively, the maximum and minimum values of post-processed metrics.\n\nBatched Optimization: We maintain \u221a\u1e9e = 1.8 and set the exploration-specific UCB parameter \u221a\u00dfe = 0.5. We further set the penalty p = 10.0 and overwrite probability q = 0.1.\n\nQuasi-Random Seeding: For single-objective problems, we did not use initial quasi-random trial sampling. For multi-objective problems, the initial 10 trials are quasi-randomly sampled."}, {"title": "C. Experimental Baselines", "content": "For every baseline and benchmark function, we ran the optimization loop with 20 repeats, with a horizon of 100 trials."}, {"title": "C.1. Packages", "content": "At the time of writing", "default": "2.10.0", "packages": "n\n\u2022 ax-platform==0.3.4 (Balandat et al., 2020)\n\u2022 bayesian-optimization==1.4.3 (Nogueira, 2014)\n\u2022 HEBO==0.3.5 (Cowen-Rivers et al., 2022)\n\u2022 hyperopt==0.2.7 (Bergstra et al., 2015"}]}