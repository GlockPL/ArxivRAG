{"title": "Facial Expression-Enhanced TTS:\nCombining Face Representation and Emotion\nIntensity for Adaptive Speech", "authors": ["Yunji Chu", "Yunseob Shim", "Unsang Park"], "abstract": "We propose FEIM-TTS, an innovative zero-shot text-to-speech\n(TTS) model that synthesizes emotionally expressive speech, aligned\nwith facial images and modulated by emotion intensity. Leveraging deep\nlearning, FEIM-TTS transcends traditional TTS systems by interpreting\nfacial cues and adjusting to emotional nuances without dependence on la-\nbeled datasets. To address sparse audio-visual-emotional data, the model\nis trained using LRS3, CREMA-D, and MELD datasets, demonstrating\nits adaptability. FEIM-TTS's unique capability to produce high-quality,\nspeaker-agnostic speech makes it suitable for creating adaptable voices\nfor virtual characters. Moreover, FEIM-TTS significantly enhances acces-\nsibility for individuals with visual impairments or those who have trouble\nseeing. By integrating emotional nuances into TTS, our model enables\ndynamic and engaging auditory experiences for webcomics, allowing visu-\nally impaired users to enjoy these narratives more fully. Comprehensive\nevaluation evidences its proficiency in modulating emotion and inten-\nsity, advancing emotional speech synthesis and accessibility. Samples are\navailable at: https://feim-tts.github.io/.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving domain of speech synthesis, the enhancement of text-to-speech (TTS) technologies has witnessed significant advancements through the integration of deep generative models. Among these, diffusion models have emerged as a pivotal force, demonstrating unparalleled efficacy in generating high-fidelity images [5, 19], videos [3], and speech [7], thereby revolutionizing acoustic modeling and vocoder technologies [11,13]. Concurrently, leveraging facial images to identify speaker-specific attributes in TTS has heralded a paradigm shift towards more personalized and adaptive speech synthesis. Notably, the"}, {"title": "2 Related Works", "content": "FACE-TTS model represents a pioneering effort in employing facial cues for\nzero-shot TTS synthesis, albeit without incorporating emotional nuances in\nspeech [15].\nParallel advancements in emotional TTS synthesis have introduced innova-\ntive methods for embedding and modulating emotional tones within synthesized\nspeech. This exploration has led to the development of models that facilitate\nsoft-label guidance for fine-grained emotion control, such as EmoDiff [8], en-\nabling nuanced manipulation of emotional intensity. Moreover, advancements\nsuch as iEmoTTS [22] have paved the way for cross-speaker emotion transfer,\neffectively disentangling prosody and timbre to enhance emotional conveyance.\nThese developments underscore the critical role of emotional expressiveness in\naugmenting the realism of virtual agents and enriching human-computer inter-\nactions.\nDespite these significant strides, the synthesis of speech that intricately inte-\ngrates facial cues with dynamic emotional modulation remains an underexplored\navenue. This research introduces the FEIM-TTS framework, an end-to-end TTS\nmodel that combines Facial Expression and Emotion Intensity Manipulation to\ngenerate speech. This model stands out by modulating emotional intensity and\nincorporating facial expressions, marking a novel approach to synergizing emo-\ntional and facial dimensions in speech synthesis. Distinctively from prior studies\nin emotional TTS, FEIM-TTS employs Classifier-Free Diffusion Guidance [9] to\nfacilitate both conditional and unconditional training, alongside inference tai-\nlored to the intensity of emotions, setting a new precedent in the field."}, {"title": "2.1 Score-Based Diffusion Model", "content": "Score-based diffusion models [20] are deep learning-based generative models that\nutilize a gradual process of adding and removing noise to model data distribution.\nThe forward process is described by a Stochastic Differential Equation (SDE)\nthat gradually adds noise to the data. This noise accumulates in the original data,\nXo, creating noisy data Xt, until it reaches complete randomness at XT. This\nnoise addition process typically uses Gaussian noise and is adjusted according\nto time or noise level. The forward process is given by:\n$dX_t = \\frac{1}{2} \\Sigma^{-1} (\\mu \u2013 X_t)\\beta_t dt + \\sqrt{\\beta_t} dW_t$,\nwhere t \u2208 [0, T] for some finite time horizon T, \u03b2t is a function that controls the\nsize of the noise, u is a vector, \u2211 is a diagonal matrix with positive elements,\nand Wt denotes a standard Brownian motion.\nThe reverse process, aiming to predict the original data, can be represented\nby an SDE as:\n$dX_t = \\Sigma^{-1}(\\mu \u2013 X_t) \u2013 \\nabla log p_t(X_t) \\beta_tdt + \\sqrt{\\beta_t} dW_t$,"}, {"title": "2.2 Classifier-Free Diffusion Guidance", "content": "Classifier-free diffusion guidance modifies S(Xt,t) to apply classifier guidance\nwithout a classifier, by integrating an unconditional and a conditional model\nwithin a single framework.\nThe approach begins with the training of a conditional denoising diffusion\nmodel, represented as pe (X|c), which is parameterized through a score estimator\nS(Xt,t,c). In parallel, an unconditional model pe(X) is also trained, parame-\nterized by S(Xt,t). The unconditional model uses a null token \u00d8 as the class\nidentifier c, as shown in the following equation:\n$S(X_t,t) = S(X_t, t, c = \u00d8)$"}, {"title": "3 Method", "content": "This section outlines the architecture of the proposed FEIM-TTS model, focus-\ning on its training and inference. Building on the base architecture of FACE-\nTTS, FEIM-TTS introduces a unique diffusion process designed for emotional\nspeech synthesis. Utilizing principles similar to classifier-free diffusion guidance,\nit effectively incorporates emotional cues as a conditioning factor in the diffusion\nprocess. This allows for nuanced control over speech synthesis, enabling dynamic\nand context-aware expression of emotions in the generated speech."}, {"title": "3.1 Conditional TTS Model Training", "content": "In developing FEIM-TTS, our goal was to craft speech rich in emotional nuances.\nWe achieved this through the integration of classifier-free diffusion guidance,\ndrawing from the principles of score-based diffusion models known for their ca-\npacity to transform random noise into structured data. This method begins with\nnoise and, through iterative application of learned gradients, progressively refines\nit into a coherent output. The diffusion process is directed by a score function\ncapable of high-quality sample generation from complex data distributions.\nClassifier-free diffusion guidance enhances our model's generative power by\nallowing for flexibility in the use of conditioning signals, such as emotion labels,"}, {"title": "3.2 Emotion Intensity Controllable Sampling", "content": "During the sampling phase, the model employs a linear combination of con-\nditional and unconditional score estimates, regulated by the emotion intensity\nparameter. This parameter serves the dual purpose of balancing the quality\nand diversity of the samples and quantifying the intensity of the emotion being\nmodeled. The prediction of scores occurs in the reverse diffusion process and\nis formulated as follows, integrating speaker and emotion information into the\nODE for reverse diffusion sampling by:\n$dX_t = (\\mu \u2013 X \u2013 S(X_t, \u00b5, t, spk, emo)) \u03b2_t dt$\nThe corresponding equation for the impact of emotion intensity on the linear\ncombination is:\n$ w S(X_t, \u00b5, t, spk, emo) \u2013 (w \u2212 1) \u00b7 S(X_t, \u00b5, t, spk, (\u00d8)$\nThe settings of the emotion intensity parameter are instrumental in modu-\nlating the intensity of emotion as:"}, {"title": "4 Experiments", "content": "This study leverages three significant English-language datasets to train and\nevaluate the FEIM-TTS model: CREMA-D [4], MELD [18], and LRS3 [2]. The\nCREMA-D dataset contains 7,442 video clips from 91 actors, delivering 12\ndistinct sentences across six emotional states anger, disgust, fear, happiness,\nneutrality, and sadness with clips lasting between 2 to 4 seconds. MELD, de-\nrived from the TV series \"Friends,\" includes 13,708 utterances across seven emo-\ntional categories anger, disgust, fear, happiness, neutrality, sadness, and sur-\nprise with an average utterance length of 3.59 seconds. These datasets' emotion\ndistribution, crucial for training, is systematically presented in Tab. 1. LRS3,\nsourced from TED Talks, comprises 31,982 short video segments, each under\n100 characters or 6 seconds, devoid of emotional annotations and thus employed\nfor unconditional training aspects of the model.\nFor the model's conditional training, we partitioned CREMA-D and MELD\ninto 9,989 training, 1,109 validation, and 2,610 testing instances. Conversely, for\nunconditional training, LRS3 was divided into 31,965 training, 660 validation,\nand 661 testing instances, highlighting the comprehensive approach adopted for"}, {"title": "4.2 Model Configuration", "content": "The model processes speech, text, and facial images alongside emotions. Facial\nimages are analyzed by the Face Network, and text is processed through the Text"}, {"title": "4.3 Evaluations", "content": "Emotion Intensity Controllability To evaluate the capability to control\nemotion intensity, we employed a Speech Emotion Recognition (SER) framework,\nspecifically the HuBERT model [10], fine-tuned on the CREMA-D dataset with\na modified prediction layer. This adaptation achieved a test accuracy of 74.98%\non CREMA-D, underscoring its reliability for our evaluation.\nWe then synthesized 432 speech samples across six distinct emotions and\n12 sentences from CREMA-D, employing emotion intensities ranging from 1\nto 30. Although theoretically, emotion intensity can be set infinitely high, our\nempirical observations indicated that pronunciation tends to become slightly"}, {"title": "5 Discussion", "content": "In this work, we present FEIM-TTS, a text-to-speech synthesis model that stands\nout for its sophisticated modulation of emotional intensities within synthesized\nspeech. This model has been rigorously validated through quantitative assess-\nments using an SER model, demonstrating its capability to enhance speech with\ndetailed emotional variations. A hallmark of FEIM-TTS is its ability to accu-\nrately link a speaker's facial features with their speech characteristics, an at-\ntribute confirmed through qualitative evaluations involving virtual faces. This\nfeature positions the model as an ideal tool for creating speech for animated\ncharacters or virtual avatars.\nFEIM-TTS represents a significant stride in the text-to-speech domain by\nharmonizing facial imagery with emotional content and adjusting emotional in-"}]}