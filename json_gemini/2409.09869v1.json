{"title": "Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent", "authors": ["Pavel Osinenko", "Grigory Yaremenko", "Roman Zashchitin", "Anton Bolychev", "Sinan Ibrahim", "Dmitrii Dobriborsci"], "abstract": "This work presents and showcases a novel reinforcement learning agent called Critic As Lyapunov Function (CALF) which is model-free and ensures online environment, in other words, dynamical system stabilization. Online means that in each learning episode, the said environment is stabilized. This, as demonstrated in a case study with a mobile robot simulator, greatly improves the overall learning performance. The base actor-critic scheme of CALF is analogous to SARSA. The latter did not show any success in reaching the target in our studies. However, a modified version thereof, called SARSA-m here, did succeed in some learning scenarios. Still, CALF greatly outperformed the said approach. CALF was also demonstrated to improve a nominal stabilizer provided to it. In summary, the presented agent may be considered a viable approach to fusing classical control with reinforcement learning. Its concurrent approaches are mostly either offline or model-based, like, for instance, those that fuse model-predictive control into the agent.", "sections": [{"title": "I. BACKGROUND AND PROBLEM STATEMENT", "content": "Notation: We will use Python-like array notation, e. g., [0: T] = {0,...,T \u2013 1} or 80:T = {$0,...,ST-1}. Spaces of class kappa, kappa-infinity, kappa-ell and kappa-ell-infinity functions are denoted K, K\u221e, KL, KL\u221e, respectively. These are scalar monotonically increasing functions, zero at zero, and, additionally, tending to infinity in case of K. The subscript \u2265 0 in number set notation will indicate that only non-negative numbers are meant. The notation \u201ccl (\u2022)", "mod.\".\n\nA. Problem statement\nConsider the following optimal control problem": "n$V^{\\pi}(s_0) = \\sum_{t=0}^{\\infty}c(s_t, a_t) \\rightarrow \\min$,\ns. t.\n$s_0 \\in S, \\pi \\in \\Pi, t \\in Z_{\\geq 0}$\n$s_{t+1} = p(s_t, a_t), a_t \\in A(s_t)$\n$a_t = \\pi(s_t)$,\nwhere:\n1) S is the state space, e. g., R", "p": "S\u00d7A \u2192 S is the state transition map of the environment, assumed upper semi-continuous in norm;\n4) c:S\u00d7A \u2192 R is the cost function of the problem, that is a function that takes a state st and an action at and returns the immediate cost ct incurred upon the agent if it were to perform action at while in state st;\n5) II is the set of policies (state feedback control laws in other words), that is a set of functions \u03c0: S \u2192 \u0391.\nSometimes, also a discount factor \u03b3\u2208 (0,1) is introduced in front of the cost in (1) as a factor yt.\nThe goal of reinforcement learning is to find a policy \u3160 that minimizes V\u2122 (so) in (1). The policy \u03c0* that solves this problem is commonly referred to as the optimal policy. An agent will be referred to as a synonym for a which was generated by some finite algorithm, e. g., actor-critic. In the latter, for instance, one step of the algorithm tries to approximate, i. e., \"learn\", the value V\u2122 via a neural network Vw with weights w based on some collected data called replay buffer, say, R = {Stk, atk}k\u2208[0:Tr] of size Tr and s.t.tk < tk+1,\u2200k. In the second step, the algorithm may choose a so-called greedy or, e. g., \u025b-greedy action at, which minimize an actor loss Lact, e. g., Lact(at) = ct + Vwt (st+1) or take a random action with probability \u025b > 0, respectively. A useful object is the action-value or Q-function Q\u2122(s, a) \u2252 c(s, a) + V(s), where s is the state at the next step relative to s upon taking the action a. The herein present approach, CALF, will base particularly on Q-function.\nCommon reinforcement learning approaches to the above include various methods of policy gradient, actor-critics and tabular dynamic programming (see, e. g., [1]\u2013[5]). A common classical control approach is in turn, e.g., model-predictive control (MPC) that places some finite horizon Tpred > 2 in place of the infinity in (1) and optimizes over respective sequences of actions via state propagation through the model p. A model-free agent, as will be understood in this work, is a controller that does not use p, or any learned model thereof, to compute actions. Such an agent is also called data-driven for instance. Notice, not every reinforcement learning agent is model-free, e. g., Dyna [6], [7].\nLet now GCS,0 \u2208 G denote a goal set to which we want to drive the state optimally according to (1). We"}, {"title": "II. RELATED WORK", "content": "assume, G to be a compact neighborhood of the origin. Also a compact in a subspace of the state space spanned by some state variables of interest may be considered, but we omit this case for simplicity. Let $d(s) := \\inf_{s'\\in G} ||s - s'||$ be the distance to the goal. We call a policy \u03c0\u03bf \u2208 II a G-stabilizer if setting at = \u03c0\u03bf(st), \u2200t implies that the distance between st and G tends to zero over time. Formally, we use the following definition.\nDefinition 1: A policy \u03c0\u03bf \u2208 II is called a G-stabilizer, or simply a stabilizer, if the goal set is implied by the context, if\n$ \\forall t \\geq 0 \\quad a_t \\leftarrow \\pi_0(S_t) \\Rightarrow \\forall s_0 \\in S \\quad \\lim_{t\\rightarrow \\infty} d_G(s_t) = 0$. (2)\nIt is called a uniform G-stabilizer if, additionally, the limit in (2) is compact-convergent w.r. t. so and\n$\\exists \\epsilon_0 \\geq 0 \\quad \\forall t \\geq 0 \\quad a_t \\leftarrow \\pi_0(S_t) \\Rightarrow $\n$\\forall \\epsilon > \\epsilon_0 \\quad \\exists \\delta \\geq 0 \\quad d_G(s_0) \\leq \\delta$\n$ \\Rightarrow \\forall t \\geq 0 \\quad d_G(s_t) \\leq \\epsilon$, (3)\nwhere & is unbounded as a function of 8 and 8 = 0\n\u03b5 =80. The presence of an 80 means we do not insist on G being invariant. Thus, this extra condition only means that the state overshoot may be uniformly bounded over compacts in S.\nNotice that if the cost e be continuous, strictly positive outside G and diverging with the distance to G, the optimal policy \u03c0* is also a uniform G-stabilizer.\nThe main hypothesis of this work is that incorporating some \u03c0\u03bf into the agent may improve its learning and guarantee stabilization into the goal set in all learning episodes, i. e., online. The main problem is: how to do this incorporation? Notice that even if \u03c01,\u03c02 are two stabilizers, an arbitrary switching or \"shaping\" onto into another will not provide stabilization guarantee in general.\n\nThere are three principal methodologies for stability- ensuring reinforcement learning: shield-based reinforcement learning, the integration of MPC with reinforcement learning, and Lyapunov-based reinforcement learning.\nShield-based approaches involve a protective filter, also referred to as a shield, overseer, or supervisor, designed to prevent undesirable, say, destabilizing actions. These meth- ods vary in how they evaluate actions and generate safe alternatives. Shields range from manual human oversight [8] to sophisticated formal verification variants [9]-[12]. Unlike human operators, formal logic shields are theoretically error- proof, but they require highly specific application development and can be complex, as detailed in [13]. Such techniques have been applied in areas such as probabilistic shielding [13], [14], supervisory systems for autonomous vehicles [15], and safe robotic manipulation [16]. However, human overseers introduce subjective biases and potential errors, and formal logic shields are often difficult to design.\nThe combination of MPC and reinforcement learning rep- resents an active frontier in the quest for ensuring stability [17]-[22]. Such a fusion takes various forms, sometimes emphasizing model learning and other times focusing on safety constraints [23], [23]-[31]. The reinforcement learning"}, {"title": "III. APPROACH", "content": "dreamer is a notable example that adopts the predictive spirit of MPC [32], [33]. Such approaches draw on the MPC's well-established ability to ensure closed-loop stability via techniques like terminal costs and constraints. Proposals such as the one by Zanon et al. [17], [18] embed robust MPC within reinforcement learning to maintain safety. Other research em- phasizes the predictive control side more heavily [19], [20], building from a safe starting point towards a predictive model while maintaining the option to revert to safe policies when needed.\nLyapunov stability theory is well recognized in reinforce- ment learning, having roots dating back to the work of Perkins and Barto [34], [35], and has seen significant development since [20], [36]\u2013[39]. Typically, these approaches are offline and require validation of a Lyapunov decay condition in the state space. Chow et al. [36], for instance, developed a safe Bellman operator to ensure Lyapunov compliance, while Berkenkamp et al. [20] used state space segmentation to validate Lyapunov conditions, demanding certain confidence levels in the statistical environment model. Online Lyapunov- based approaches also exist, often inspired by adaptive con- trol techniques [40]. Robustifying controls, a key component introduced by [41], may distort the learning unless certain preconditions are met. Some reviews may be found in [42], [43]. Control barrier functions, another safety feature, have been successfully integrated with reinforcement learning, pro- viding enhanced safety capabilities as seen in simulations with bipedal robots [44] and in model-free reinforcement learning agents [45]. Stochastic stability theory provides a basis for correctness [46], but the current landscape of Lyapunov-based methods often lacks capacity for real-time application without extensive pre-training, and is generally predicated on specific assumptions about environmental dynamics, such as second- order differentiability [47], linearity [48], or global Lipschitz continuity [49].\nIt should be stressed that policy shaping algorithms, e. g., [50] may be attractive in their similarity to pre-training the agent to boost learning, no online stabilization can be ensured under them. In contrast to the existing approaches, CALF is online, bringing the interplay of a stabilizer and the agent onto a rigorous footing, thus providing a viable, model-free approach to combining classical control and reinforcement learning. This is the essence of the current contribution.\n\nIn general, it holds, due to the Hamilton-Jacobi-Bellman equation, that $v_t \\geq c_t + Q^{\\pi*}_{t+1} - Q^{\\pi*}_{t} = c_t$, where $C_t = C(s_t, \\pi*(s_t))$, $V^{\\pi*}_{t+1} = (V^{\\pi*}_{t+1} = c(s_t, \\pi*(s_t))$. This effectively means, given the conditions on the cost stated in Section I, that Q is a Lyapunov function for the closed-loop system st+1 = p(st, \u03c0*(st)). If a model (critic) is employed, e. g., a deep neural network Q\u2122(s, a), due to imperfections of learning, the Lyapunov property may be lost, although desired. In CALF, we would like to retain the Lyapunov property of Q\u2122. Enforcing Lyapunov-like constraints alone on Qu would not solve the problem of ensuring stability, since those constraints may fail to be feasible at some time steps. Offline"}, {"title": "IV. ANALYSIS", "content": "approaches, as mentioned in Section II, would overcome this by large samples of state-action trajectories. What we do here instead is that we employ any stabilizer, let us call it \u03c0\u03bf. The latter may be synthesized by common control techniques, like PID, sliding mode or funnel control. The question is, as stated earlier, how to combine the agent with \u03c0\u03bf? This is brought into a systemic way in CALF. Namely, the critic update, i. e., update of the weights w is done so as to try to satisfy Lyapunov-like constraints (decay and K-bounds, see Algorithm 1, line 7). If this succeeds, the weights are passed and the next actor update will base upon them. If not, the weights are \"frozen\" (the respective variable is denoted w\u2020) and \u03c0\u03bf is invoked. Along w\u2020, the state-action pair is also \u201cstored\u201d, namely, s\u2020, a\u207a. The Lyapunov-like constraints are checked relative to s\u2020, a\u2020, w\u2020. This trick enables to safely combine the agent with \u03c0\u03bf so as to retain the stabilization guarantee of the latter. We use the Q-function to make the overall approach model-free. The actor loss Lact may be taken equal the current critic with last \u201csuccessful\u201d weights w\u2020, namely, Lact(a) = Qw\u00b9 (st, a). Any augmentation of the loss, e. g., penalties or entropy terms, may be included into Lact, there is no restriction. Furthermore, one may choose to take an \u025b-greedy action.\nRegarding the K-bounds (see Algorithm 1, line 7), a reasonable choice of low, Kup would be\n$K_{low}(\\bullet) = C_{low}\\bullet^2, K_{up}(\\bullet) = C_{up}\\bullet^2, 0 < C_{low} < C_{up}$. (4)\nFor the decay constraint, one may also take a quadratic rate or simply a constant \u016b > 0. Overall, the hyper-parameters low, Kup and determine a trade-off between freedom of learning and worst-case-scenario reaching time of the goal. If flow and kup\n\u016b are chosen to be sufficiently small, the weights of the critic will not be prevented from converging to their ideal values, however if the critic is underfitted, smaller values of low, kup' may entail slower stabilization accordingly.\nNow, the critic loss Ccrit (see Algorithm 1, line 7) may be taken in various forms. The presented approach does not restrict the user. For instance, one may take a TD(1) on-policy loss as per:\n$L_{crit}(w) = \\sum_{k=0}^{T_R}(Q^w (s_{t_k}, a_{t_k}) - c(s_{t_k}, a_{t_k}) - Q^w(s_{t_{k+1}}, a_{t_{k+1}}))^2 + a_{crit} ||w - w^{\\dagger}||^2$. (5)\nThe regularization term acrit ||w - wt ||2 is redundant if gradient descent based minimization is used, since one could simply set a learning rate acrit as opposed to penalizing the displacement of weights. Notice the choice of the critic loss (or learning rate) does not prevent environment stabilization, although the quality of the learning may be affected. Finally, (5) is akin to the critic loss of SARSA due to its on-policy character, but this is not necessary, an off-policy loss may be used as well, e. g., with greedy actions instead of atk+1 in (5).\nRemark 1: An actor model \u03c0\u00ba, e. g., as probability distribution, with weights @ may be employed instead of direct actions at in Algorithm 1."}, {"title": "A. Modified SARSA", "content": "The new CALF agent was benchmarked via its immediate reinforcement learning alternative, State-action-reward-state-action (SARSA), which is essentially like Algorithm 1 prescribes (with on-policy critic loss), but with the Lyapunov-like constraints and the \u03c0\u03bf removed. In our case studies with a mobile robot, we observed such a plain SARSA failed to drive the robot to the target area within any reasonable number of learning iterations. To help SARSA succeed, we slightly modified it, namely, we retained the w\u2020-mechanism, i. e., we used wt in the critic loss (see Algorithm 1, line 7). We did not enforce the Lyapunov-like constraints in the optimization though. We only checked, whether those constraints were satisfied post factum and updated w\u2020 accordingly as in CALF. Such a modification turned out to help SARSA reach the target in some learning runs. This algorithm will further be referred to as SARSA-m."}, {"title": "V. CASE STUDY", "content": "IV. ANALYSIS\nThe main environment stability result is formulated in Theorem 1.\nTheorem 1: Consider the problem (1) and Algorithm 1. Let \u03c0\u03c4 denote the policy generated by Algorithm 1. If the policy \u03c0\u03bf is a stabilizer, then \u03c0t is a stabilizer. If the former is a uniform stabilizer, then so is \u03c0\u03c4.\nProof.\nFirst, let us consider G' to be a closed superset of G, where the Hausdorff distance between G and G' is non-zero. Let h denote the said distance.\n\nA. System description\nWe consider here a differential drive mobile wheeled robot (WMR) depicted in Fig. 1 for testing the proposed algorithms. The goal is to achieve autonomous stabilization of the robot at the origin of a coordinate system, starting from non-zero initial conditions. In the work space, there is a designated high-cost zone, which could represent a physical object like a swamp, a puddle, or a phenomenon that can adversely impact the robot's movement. Within this high-cost zone, the robot's maximum velocity is constrained to 1 cm per second to mitigate potential damage or difficulties in navigation.\nThe differential drive model describes the motion of the wheeled robot. In this model, the robot has two wheels mounted on either side, which can rotate independently. The position and orientation of the robot in the plane can be described by the state variables (x, y, v), where:\n\u2022\nx and y represent the coordinates of the robot in the plane.\nv is the orientation angle of the robot with respect to a reference direction.\nThe kinematic equations governing the motion of the robot are:\n$\\dot{x} = v \\cos(\\vartheta)$,\n$\\dot{y} = v \\sin(\\vartheta)$,\n$\\dot{\\vartheta} = \\omega$\n(17)\nwhere:\nv is the linear velocity of the robot.\nw is the angular velocity (rate of change of orientation). In the high-cost zone, the linear velocity v is restricted such that v < 1 cm/s. Outside of this zone, the robot can move at its maximum possible speed.\nThe control objective is to design a control law for v and w to stabilize the robot at the origin (0,0). This involves navigating from any initial position to the origin while possibly avoiding the high-cost zone. Experiments were conducted to validate the proposed algorithms under various initial condi- tions. The performance metrics typically include the time taken to reach the origin, the path taken by the robot, and its behavior in and around the high-cost zone.\nThe performance of CALF and SARSA-m as the benchmark agent were compared with a nominal stabilizer, and two MPC agents with different horizons. The latter are taken to get an idea about a nearly optimal controller for the stated problem. The code for the environment simulation, CALF, SARSA-m, MPC and nominal stabilizer may be found under https://github. com/osinenkop/rcognita-calf.\nIn our studies, we observed that a significant horizon of length 15 was sufficient to fulfill the stated goal. This controller is further referred to as MPC15. Notice that MPC15 may lack practicability due to such a long horizon, especially taking into account possible disturbances. Essentially, as the horizon length increases, the number of decision variables and constraints in the optimization problem grows, leading to increased computational complexity. Solving this problem within a reasonable time frame becomes more challenging as"}, {"title": "B. Discussion of results and conclusion", "content": "the horizon grows. For real-time applications, such as control- ling a differential drive mobile robot, the control decisions need to be computed and applied within very short time intervals. The extensive computations required for a 15-step horizon can exceed the available computational resources and time, making it possibly impractical for real-time implemen- tation. In this regard, CALF and SARSA-m are beneficial as they do not use any models. Again, MPC15 was rather taken to get a clue of the best possible cost.\nRecapping on the system description, at time t, the origin of the local coordinate frame (base) of the robot is located at the point (xt, Yt) of the world coordinate frame. The robot's linear velocity v(t) along the axes determines the direction of motion, while the angular velocity w(t) determines the rotation (refer to Fig. 1). The state vector s \u2208 R\u00b3 and the control input vector (action) a \u2208 R2 are defined as\n$\\bf{s} := [x\\;y\\;\\vartheta]^T, \\bf{a} := [v\\;\\omega]^T$. (18)\nThe environment state transition map p can be obtained via time discretization of the WMR differential equations which read: $\\dot{x} = v \\cos \\vartheta, \\dot{y} = v \\sin \\vartheta, \\dot{\\vartheta} = \\omega$. For the studied MPC, we used Euler discretization, whereas for CALF and SARSA- m, p is not necessary at all. In our case study, we used the controller sampling time of 100 ms.\nThe control actions were constrained by amin and @max, which are determined by the actuators' capabilities. We took those constraints matching Robotis TurtleBot3, namely, nor- mally (except for the high-cost spot) 0.22 m/s of maximal magnitude of translational and 2.84 rad/s of rotational velocity, respectively.\nThe control goal is to drive the robot into G = {s \u2208 R3 : ||s - s* || < \u0394}, where the target state s* was taken identical with the origin and A is target pose tolerance.\nRegarding the high-cost zone, we introduced a spot on the plane R2, namely, the cost was defined as (we did not penalize the action and only slightly penalized the angle):\n$c (\\bf{s},\\bf{a}) = x^2 + y^2 + 0.1\\vartheta^2 + 10c' (x, y)$, (19)\nwhere\n$c'(x,y) = \\frac{1}{2\\pi\\sigma_x\\sigma_y} exp \\bigg(-\\frac{1}{2} \\big( \\frac{(\\Delta x)^2}{\\sigma_x^2} + \\frac{(\\Delta y)^2}{\\sigma_y^2} \\big) \\bigg)$, (20)\nwith \u0394x = \u03bc\u03b1, \u0394 = \u03bc\u03c5; \u03c3\u03b1 \u03c3\u03c5 0.1 being standard deviations in the along-track and cross-track directions, respectively; and \u03bcx = \u22120.6, \u03bcy = -0.5 being the high cost spot center.\nWe now proceed to the description of the nominal stabilizer. The action components v,w were determined based on the polar coordinate representation of the WMR as per [54], namely:\n$\\begin{bmatrix} \\dot{\\rho} \\\\ \\dot{\\alpha} \\\\ \\dot{\\beta} \\end{bmatrix} = \\begin{bmatrix} \\cos \\alpha & 0 \\\\ - \\frac{\\sin \\alpha}{\\rho} & 1 \\\\ - \\frac{\\sin \\alpha}{\\rho} & 0 \\end{bmatrix} \\begin{bmatrix} v \\\\ \\omega \\end{bmatrix}$, (21)\nwhere the top sign (plus or minus) holds if a \u2208 (-\ud50c, \ud50c] and the bottom sign holds if a \u2208 (\u2212\u03c0, -\u03c0/2] U (\u03c0/2, \u03c0]. The transformation into the polar coordinates in turn reads:\n\u03c1 = \u221ax2 + y\u00b2,\n\u03b1 = -9 + arctan 2(y, x),\n\u03b2 = -\u03bd \u2013 \u03b1.\nThe stabilizer was set as per:\n$v \\leftarrow K_{\\rho}\\rho$,\n$\\omega \\leftarrow K_{\\alpha}\\alpha + K_{\\beta}\\beta$, (22)\n(23)\nwhere Kp > 0, \u039a\u03b2 < 0, \u039a\u03b1 \u2212 Kp > 0.\nThe robot was run for 30 seconds in each episode. If the target was not reached, we nominally added a cost of 2000 to rule out the cases where the robot simply did not drive to the target while also not getting into the \u201chot\u201d spot determined by c'. The target area was to 0.2 in radius around the origin (see Fig. 3). The initial robot pose was set to {\u22121\u22121 \u03c0/2}.\nAs can be seen from the results (Fig. 2, Fig. 3), both reinforcement learning agents outperformed MPC10 (CALF at once and SARSA-m in the final episode). The controller MPC15 performed best in terms of cost, but it should only be taken as a reference for a potential nearly best performing approach. It has a significant horizon length which reduces its practicability. Unlike MPC15, the studied reinforcement learning agents are totally model-free. Next, CALF also out- performed the nominal controller. What is remarkable is that both agents were able to detour the \u201chot\u201d spot (notice in Fig. 3 how the nominal stabilizer is blind to the spot). CALF did it in all best learning episodes, whereas SARSA-m succeeded only in a part of those. It should be repeated here that plain SARSA did not succeed in our case studies at all. Overall, CALF always succeeded in reaching the target, thanks to its design and Theorem 1, and also outperformed the benchmark agent SARSA-m in terms of learning performance. This validates the initial claim that ensuring online environment stability is not just practicable, but it is also beneficial for episode-to-episode learning."}]}