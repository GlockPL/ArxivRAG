{"title": "Simultaneous emulation and downscaling with physically-consistent deep learning-based regional ocean emulators", "authors": ["Leonard Lupin-Jimenez", "Moein Darman", "Subhashis Hazarika", "Tianning Wu", "Michael Gray", "Ruyoing He", "Anthony Wong", "Ashesh Chattopadhyay"], "abstract": "Building on top of the success in AI-based atmospheric emulation, we propose an AI-based ocean emulation and downscaling framework focusing on the high-resolution regional ocean over Gulf of Mexico. Regional ocean emulation presents unique challenges owing to the complex bathymetry and lateral boundary conditions as well as from fundamental biases in deep learning-based frameworks, such as instability and hallucinations. In this paper, we develop a deep learning-based framework to autoregressively integrate ocean-surface variables over the Gulf of Mexico at 8 Km spatial resolution without unphysical drifts over decadal time scales and simulataneously downscale and bias-correct it to 4 Km resolution using a physics-constrained generative model. The framework shows both short-term skills as well as accurate long-term statistics in terms of mean and variability.", "sections": [{"title": "Plain Language Summary", "content": "Data-driven models are promising tools for predicting ocean conditions and enhancing the details of these predictions. In this study, we applied advanced machine learning methods to model sea surface velocity and height in the Gulf of Mexico. To forecast broad ocean conditions, we used a method called Fourier Neural Operators (FNO), designed to balance computational efficiency with accuracy through a specialized loss function that combines grid and spectral space information. For creating high-resolution details from low-resolution data a process called downscaling we explored two different neural network architectures and compared their performance against simpler linear interpolation. This combination of forecasting and downscaling methods greatly improves the efficiency of ocean forecast and downscaling compared to numerical simulation with limited input variables. Our results highlight that these data-driven techniques can provide reliable, physics-aware predictions that can be useful for quick, localized analyses and in generating statistical predictions."}, {"title": "1 Introduction", "content": "The North Atlantic Ocean's western boundary current system (WBC), including the Loop Current (LC), Gulf Stream (GS), and Gulf Stream meander (GSM) play a significant role in controlling the Earth's ocean circulation, by transporting heat, salt, nutrients, and strongly influencing the global weather and climate system, including marine ecology. Modeling the regional ocean, e.g., in the Gulf of Mexico region (GoM), involves several challenges, starting with complex land boundaries, incorporation of lateral boundary conditions, which is usually computed from a global ocean model, and also eddy shedding events which is caused due to the interactions between the cool, subpolar circulation from the North and warm, subtropical circulation from the South. Accurately resolving the eddy shedding process in the GoM region has been a challenge for even high-resolution numerical ocean models in the past.\nRecent years have seen widespread success in machine learning (ML-) based data-driven emulation of atmospheric dynamics, where the weather forecasting accuracy of such ML models have surpassed the accuracy of numerical weather prediction models, while being several thousand times faster. While several of these AI weather models such as Four Cast-Net, GraphCast, and Pangu eventually become unstable or unphysical, a few of the works around stability of these models for climate time scales have succcessfully demonstrated a long-term stable atmosphere with accurate climatology and variability. To scale such ML-based emulator approaches to the full Earth system, ocean emulators are essential. However, there have been limited work on building global or regional ocean emulators beyond investigating low-dimensional models predicting large-scale patterns at short time scales. Recently, we have demonstrated success in data-driven regional ocean modeling at very high resolution (4 Km) near the GoM and GS region where OceanNet showed short-term prediction performance that was superior to numerical ocean models such as the Regional Ocean Modeling Systems (ROMS), while preserving long-term physical consistency. Similarly, and others have demonstrated promising results in emulating the global ocean at different CO2 forcings.\nIn this paper, we build on the recent success in data-driven autoregressive ocean forecasting in OceanNet, to extend it to multiple surface variables over climate time scales. Then, we investigate deep learning-based downscaling as a strategy to better resolve the GS emulated by the forecasting model. Downscaling and super resolution has been very popular in the weather and climate community where the focus has been primarily on increasing the fidelity of the forecasts from numerical models. Recently, Mardani et al. developed a generative model-based residual correction algorithm to downscale coarse-grained 25Km re-analysis to 2Km-scale fields (using observations) over Taiwan, wherein fine-scale convective structures were recovered. Such efforts in ocean dynamics have been largely absent. Here, we demonstrate both short- and long-term emulation of the surface ocean dynamics near the GoM region along with simultaneous downscaling at higher resolution. We highlight a few key important features in our framework. Unlike most work that downscale reanlaysis products into higher resolution observations, we autoregressively predict the surface ocean dynamics with an ML-based forecasting model and downscale the emulated fields to higher resolution. This is particularly difficult since autoregressive models have limited prediction skills, instability issues, and a tendency to become unphysical at long time scales. Furthermore, downscaling the predicted fields involve both super-resolution in the spatial fields as well as bias correction to account for error growth during autoregressive emulation.\nOur ML-based prediction and downscaling framework has the following features:\n\u2022 A long-term stable and physically consistent data-driven regional ocean emulator, i.e. a forecasting model (FC) trained on sea-surface height (SSH), sea-surface zonal and meridional velocities (SSU and SSV), and sea-surface kinetic energy (SSKE) from low-resolution (LR) GLORYS reanalysis data (E.U. Copernicus Marine Service Information (CMEMS), 2024).\n\u2022 A deterministic and generative downscaling framework, DS model, that super resolves and bias corrects the predicted fields from the FC model to high-resolution (HR) CNAPS reanalysis fields (NSF AI Institute for Research on Trustworthy AI (AI2ES), 2024).\nIn the rest of the paper, we describe the two datasets used for training the FC and DS models, the training and downscaling methodologies, principled structures used in the machine learning models to enforce physical consistency especially in the energy spectrum, and finally a results section that discusses the short- and long-term performance of the final downscaled fields with held-out CNAPS renalysis data."}, {"title": "2 Datasets", "content": "We used two reanalysis datasets in this paper. For training the autoregressive forecasting model, FC, we utilize LR data corresponding to SSH, SSU, SSV, and SSKE fields from the global ocean reanalysis, GLORYS (E.U. Copernicus Marine Service Information (CMEMS), 2024), at 8 Km. We downscale the autoregressively emulated fields from FC to a HR 4 Km regional reanalysis product, CNAPS (NSF AI Institute for Research on Trustworthy AI (AI2ES), 2024). Further details about the CNAPS reanalysis product can be obtained in Chattopadhyay et al. (2024). In this paper, the autoregressive model integrates the surface ocean dynamics at a daily time scale. Unlike other atmospheric emulators, which are typically integrated at 6 hourly temporal resolution, we integrated our ocean emulator daily, owing to the longer time scales of the oceanic processes."}, {"title": "3 Methodology", "content": "In the following sections, we outline the details of the FC and the DS model. Instead of forecasting at high-resolution by training on CNAPS data, as we had done in Chattopadhyay et al. (Chattopadhyay et al., 2024), the FC model is trained on lower resolution GLORYS regional data. The choice of using a low-resolution forecasting model circumvents the cost of computational memory when using 4 prognostic variables during training and also reduces spectral bias (Chattopadhyay & Hassanzadeh, 2023), thereby promoting stability of the FC model. Then, a physics-constrained generative (as well as a deterministic) downscaling model is trained to resolve higher resolution features in the predicted fields from the FC model to the CNAPS fields. The forecast and downscaling framework (FCDS) is shown in Figure 3 below. We have used a 2D Fourier neural operator (FNO) as the FC model and a modified variational autoencoder with a patch generative advserial network (PatchGAN-) based discriminator as well as a regular UNET-based architecture as the DS model."}, {"title": "3.1 Loss functions", "content": "For the training loss in the FC and DS models we use the weighted sum of grid and absolute spectral loss, shown below:\n$L_{grid} = \\frac{1}{Imn}\\sum_{c=1}^{m} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (y_{t;cij} - y_{p;cij})^2.$\n$L_{spectral, lon} = \\frac{1}{Imn}\\sum_{c=1}^{m} \\sum_{i=1}^{n}\\sum_{=1}^{n} (\\hat{y}_{t;cij} - \\hat{y}_{p;cij})^2.$\n$L_{spectral, lat} = \\frac{1}{Imn}\\sum_{c=1}^{m} \\sum_{1=1}^{n}\\sum_{j=1}^{n} (\\hat{y}_{t;cij} - \\hat{y}_{p;cij})^2.$\n$L_{total} = (1-\\lambda) L_{grid} + \\lambda \\frac{L_{spectral, lon} + L_{spectral, lat}}{2}.$\nHere, yt and yp represent the ground truth and model predictions in grid space, respectively, while \u0177t and \u0177p represent the ground truth and model predictions in spectral space. In Yt;cij, c is the channel, i is the latitude index, and j is the longitude index. I denotes the number of output channels in the model. m and n refer to the number of latitude and longitude indexes in each axis of the grid space, while m and \u00f1, respectively, indicate the corresponding number of latitude and longitude wave numbers in the spectral space. For grid points corresponding to land masses, the values of yt and Yp are set to 0, while for ocean, the values are set to 1.\nLgrid represents the MSE loss in the original grid space of the fields. This is calculated for each prognostic variable represented in a channel c, and averaged at all points in the grid. Lspectral, lon and Lspectral, lat are the spectral losses, with the Fourier transforms computed along the latitude in Eq. (3) and longitude in Eq. (2). Spectral loss allows us to reduce the spectral bias that leads to instabilities or unphysical drifts in autoregressive integration (Chattopadhyay & Hassanzadeh, 2023). In this work, since the Fourier transform is computed on a non-periodic region, we first mirror each field across the dimension we are computing the Fourier transform of, to enforce periodicity. We take the absolute value of the Fourier coefficients, \u0177t and \u0177p for the truth and the prediction, respectively.\nLtotal, the total loss, is computed as a weighted sum of the grid MSE and the spectral MSE loss, weighted by the parameter \u03bb. This allows us to adjust the balance between the grid and spectral loss in the total loss computation. In practice, we found the optimal value of A for forecasting and downscaling is 0.2."}, {"title": "3.2 FC model Training and Testing", "content": "The proposed FC model is trained on GLORYS LR data, where SSH, SSU, SSV, and SSKE at day ti is used as the input and the same prognostic fields, at day, ti+1 are used as labels. The SSKE is computed from SSU and SSV. The land-sea mask is also used as a constant input to the model. During training, the climatological lateral boundary conditions from the training data are enforced for the 4 prognostic variables. The FC model in the FCDS frameworks is a two-dimensional Fourier neural operator (FNO2D). It has six Fourier layers, where 64 Fourier modes are kept in each Fourier layer. The model is trained using the loss function given in Eq. (4) using an ADAM optimizer. For training, we have utilized GLORYS fields from 1992-2018. The autoregressive emulation skill of the FC model is validated with 50 initial conditions, starting from 2019 and into 2020."}, {"title": "3.3 Downscaling Architectures", "content": "We compare an UNET and a modified VAE architecture for downscaling in the DS model. The DS model super-resolves the low-resolution predicted variables from FC to high-resolution CNAPS fields. The DS model is trained offline to learn a map from the GLORYS prognostic fields to the CNAPS prognostic fields. For both the UNET and the VAE, the combined spatial grid and spectral loss function shown in Eq. (1)-Eq. (4) is used to reconstruct the high-resolution features of the flow."}, {"title": "3.3.1 UNET", "content": "The UNET architecture, illustrated in Fig. 5, is originally designed for image segmentation, but it has also shown significant promise in various other tasks, including image classification, regression, and downscaling.\nThe network is structured in a U-shape, with an encoder-decoder configuration. It begins with a channel-raising convolutional layer, followed by four down-sampling layers consisting of convolutional operations and pooling layers, which progressively reduce the spatial resolution while increasing the number of feature channels. This encoder portion captures the high-level features of the input data by compressing spatial information into a compact latent representation.\nThe decoder portion of the network consists of four up-sampling layers with transposed convolutions, which progressively reconstruct the spatial resolution of the data. These layers use learned filters to expand the feature maps, enabling the network to reconstruct the output image with high spatial fidelity. The key feature of the UNET architecture is the use of skip connections between the corresponding layers of the encoder and decoder. These skip connections allow the model to retain fine-grained spatial information that might otherwise be lost during down-sampling. By directly passing feature maps from the encoder to the decoder, these connections help refine the output and prevent the loss of important structural details.\nIn our specific application, the UNET framework leverages its latent space representation and skip connections to refine low-resolution input data. By utilizing the detailed feature maps passed through the skip connections, the network can correct discrepancies and adjust for the differences between the low-resolution and high-resolution data. This ensures that the model is better equipped to map the low-resolution input to a high-resolution output, aligning the predictions more closely with the characteristics of the high-resolution data. This approach enhances the accuracy and detail of the downscaled output, making it more suitable for applications requiring fine spatial resolution."}, {"title": "3.3.2 Variational Autoencoder with Adversarial Training", "content": "The Variational Autoencoder (VAE) is designed to reconstruct high-resolution CNAPS data from low-resolution GLORYS data while learning a meaningful latent representation. The architecture consists of an encoder, a decoder, and a PatchGAN discriminator, trained in an adversarial framework.\nThe encoder processes the input through an initial convolutional layer, followed by several downsampling layers (DownBlocks), which reduce spatial dimensions while increasing feature channels. The bottleneck layers (MidBlocks) refine the feature maps before generating the latent distribution, represented by the mean \u00b5 and log variance log \u03c3\u00b2. The latent sample z is computed using the reparameterization trick:\nz = \u03bc + \u03c3\u00b7 \u03b5, \u03c3 = exp(0.5\u00b7 log \u03c3\u00b2), \u03b5~ N(0, 1),\nwhere N(0, 1) denotes the standard normal distribution. Sampling the latent variable z using equation Eq. (5) ensures differentiability while mapping inputs to latent space, thus improving latent representation for reconstruction. Encoded feature maps are preserved as skip connections to enhance the decoder's reconstruction ability.\nThe decoder concatenates the latent sample with the skip-connected features and upsamples the spatial dimensions through UpBlocks, mirroring the encoder's downsampling operations. Additional MidBlocks further refine the reconstructed feature maps. A final convolutional layer produces the high-resolution output, cropped to remove padding applied during the encoder's input processing.\nThe discriminator is a PatchGAN, designed to evaluate the realism of reconstructed images at the patch level. It processes inputs through sequential convolutional layers, progressively reducing spatial dimensions while predicting a grid of values. Each cell in the grid corresponds to a patch of the input, with higher values indicating greater realism. The architecture employs LeakyReLU activation and batch normalization, with the final layer producing the grid of predictions.\nThe training combines multiple loss functions to optimize the VAE and discriminator, represented by V and D, respectively. The reconstruction loss incorporates spatial and spectral components and is defined as:\n$L_{recon} = L_{total} = (1 \u2212 1)L_{grid} + \\lambda \\frac{L_{spectral, lon} + L_{spectral, lat}}{2}$\nLatent space regularization is achieved via the Kullback-Leibler (KL) divergence:\n$L_{KL} = -\\frac{1}{N} \\sum (1 + log \u03c3\u00b2 \u2013 \u03bc\u00b2 \u2013 \u03c3\u00b2)$.\nThe adversarial loss encourages the generator to produce realistic outputs classified as real by the discriminator:\n$L_{adv} = \\frac{1}{N}(D(V(x(t))) \u2013 1)\u00b2 ,$\nwhere D(0) represents the discriminator's output, with values in (0,1); 0 indicates generated samples, and 1 indicates real samples.\nThe total generator loss Lgen combines the reconstruction loss Lrecon, the Kullback-Leibler divergence LKL weighted by BKL, and the adversarial loss Lady weighted by Aadv, where BKL and Aadv control the relative contributions of regularization and adversarial terms.\n$L_{gen} = L_{recon} + B_{KL} \\cdot L_{KL} + A_{adv} . L_{adv}.$\nThe discriminator minimizes the following objective:\n$L_{disc} = \\frac{1}{2}(L_{real} + L_{v}),$\nwhere:\n$L_{real} = \\frac{1}{N_{real}}\\sum_i^{N_{r}} (D(X_{hr,(t)}) \u2013 1)\u00b2 ,$\n$L_{y} = \\frac{1}{N_{y}}\\sum_i^{N_{r}}(D(V(X_{lr,(t)})) \u2013 0)\u00b2 .$\nHere, Xhr,i(t) denotes the high-resolution CNAPS ground truth, and V(Xlr,i(t)) represents low-resolution VAE-reconstructed data.\nTraining alternates between optimizing the generator and the discriminator. The generator minimizes Lgen via gradient accumulation, while the discriminator minimizes Ldisc. The dynamic adjustment of the learning rates for both networks ensures stability, preventing either model from overpowering the other. This dynamic adjustment ensures convergence and produces high-quality realistic reconstructions."}, {"title": "3.3.3 Online fine tuning of the DS model for bias correction", "content": "Unlike standard downscaling or super-resolution tasks, in this paper, we are downscaling the fields predicted by the autoregressive emulator which in itself has a model error and after a few integration time steps diverges from the true LR GLORYS fields, due to a combined effect of model error and sensitivity to initial conditions. In order to mitigate the bias between the diverging emulation and the high-resolution CNAPS fields, we fine-tune both the DS models using the same loss functions on training data from 2018. The online fine tuning process evolves the autoregressive model from 50 initial conditions in 2018 for 30 days. The offline-trained DS models are then fine-tuned to correct the bias between the emulation and CNAPS. There are several advantages in performing online fine tuning. One of the major ones is to account for and correc the discrepncy between the different meso-scale ocean parameterizations used in the physical model that was used to generate the reanalysis data in GLORYS and CNAPS."}, {"title": "4 Results", "content": "In this section, we demonstrate the performance of the FCDS framework, using both UNET and VAE, with several different skill metrics for both short-term performance and long-term statistics."}, {"title": "4.1 Short-term skills of the FCDS framework", "content": "Here, we show that the FCDS emulated and downscaled fields for SSH, SSU, and SSV visually in Fig. 7 to Fig. 11. Here, Fig. 7 is the initial condition while Fig. 11 shows the emulated and downscaled fields after a year. It must be noted that we do not expect the FCDS outputs to match the true high-resolution CNAPS fields after a year. The fields, however, do not go unstable and remain physically consistent. Each of the panels in Fig. 7 to Fig. 11 represent the emulated fields in low-resolution space using the FNO-based autoregressive model, a naive baseline super-resolution onto the CNAPS grid using bilininear interpolation, FCDS output, and the true GLORYS and CNAPS fields, from top to bottom. The forecast accuracy of the autoregressive model can be qualitatively assessed by comparing the first and the fourth panels, while the accuracy of the downscaled fields can be assessed by comparing the third and the fifth panel. Here, we show the snapshots of the fields starting from one initial condition. However, to compare skills, we have conducted emulation and downscaling over multiple initial conditions. In Fig. 7 to Fig. 11 we have shown the results with the online fine-tuned UNET-based DS model, which leads to the most performant FCDS framework. It is clear from the figures that the intensity of the GS is captured more accurately in the SSH field of the FCDS framework's output as compared to the baseline interpolation method.\nIn Fig. 12, we compare several short-term metrics to assess the performance of the FCDS framework over 30 days. We compare the Person correlation coefficient, ACC, SSIM, and RMSE metrics for each of the different configurations of the FCDS framework with both online fine-tuned and offline VAE and UNET as the DS model. Figure 12 shows that each of the configurations in the FCDS framework perform better than the navie interpolation baseline while the online fine-tuned UNET performs the best in terms of correlations and SSIM. The SSIM metric which captures the structures of the eddies and their shedding shows the instability in the interpolation-based DS model which has a very large uncertainty across the different initial conditions. In general, the FCDS framework is robust with any of the DS models and show comparable performance. It must be noted that unlike prediction tasks without downscaling the ACC does not start from 1.0 even at the initial condition. This is because the initial conditions are from the low-resolution GLORYS dataset while the true high-resolution snapshot are from CNAPS. This is a realistic forecasting and downscaling setup where the training data and the initial condition may come from different reanalysis or observation products."}, {"title": "4.2 Power spectrum of the FCDS framework", "content": "In this section, we go beyond the traditional statistical short-term metrics and investigate physics-informed metrics such the kinetic energy's power spectrum and the power spectrum of the SSH fields and compare it with the high-resolution CNAPS's power spectrum. In general, for the offline FCDS framework (i.e., before executing prediction and downscaling from an initial condition in the test set) in Fig. 13, we see that the generative VAE model captures the SSKE power spectrum more accurately than the deterministic UNET-based DS model, although in either case, we see that the higher wavenumbers are not accurately captured. We further see that each of the FCDS configuration captures the SSH spectrum accurately over 30 days of prediction. However, in the online mode as shown in Fig. 14, the advantage of the VAE model over the UNET model for DS disappears and they perform similarly for both SSKE and SSH. This difference between offline and online performance of deep learning models is not new and has been reported in studies involving subgrid-scale models and parameterizations in climate models as well (Lin et al., 2023). It must be noted that in online mode, the FCDS framework outperforms naive interpolation in terms of the power spectrum ensuring that the DS model does indeed capture physically realistic high-wavenumber features in the flow during autoregressive prediction and downscaling."}, {"title": "4.3 Long-term stability, mean, and variability", "content": "In this section, we run the FCDS framework for 10 years and analyze the emulations to inspect instability and unphysical drifts in the model. Figures 15 and 16 show the outputs from the FCDS framework after 100 and 4000 days of emulation. As can be seen, the power spectrum of SSU ans SSV remains roughly similar to the high-resolution CNAPS (with obvious artifcats near the higher wavenumbers) while the SSH spectrum is accurately captured. Finally, we compute the long-term mean and standard deviation of the FCDS framework in Fig. 17 and compare with the mean and standard deviation of high-resolution CNAPS. Fig. 17 shows that both mean and standard deviation is accurately captured in the FCDS framework ensuring that we have a long-term physical ocean climate without drifts."}, {"title": "5 Discussion", "content": "In this paper, we introduce FCDS a framework for autoregressive emulation and simultaneous downscaling and bias correction for the region of GoM. We have considered only the ocean surface variables in this paper, noting that the framework is easily extendable to other sub-surface variables. The framework is purely data-driven and hence can be executed at orders of magnitude faster runtime than physics-based modeling and downscaling frameworks. Both the autoregressive model and the downscaling models are equipped with a physics-inspired spectral loss function that remedies the adverse effects of spectral bias which leads to instability and unphysical drifts (Chattopadhyay & Hassanzadeh, 2023; Bonavita, 2023).\nIn this work, intead of taking physics-based forecasts and downscaling them to high-resolution we develop an autoregressive emulator which is orders of magnitude faster than a physics-based model and downscale the emulated fields into higher resolution. As such, the autoregressive model deviates from the true low-resolution fields due to compounding model error and chaos. Hence, the downscaling model is fine-tuned to perform bias correction as well. The bias correction strategy accounts for three sources of error: the deviation of the emulator, the discrepnacy in the resolutions between the GLORYS and CNAPS data, as well as the underlying differences in the physical parameterizations between the CNAPS and GLORYS's numerical models.\nIn this work, we emulate low-resolution fields instead of the high-resolution fields. This choice stems from how spectral bias a fundamental bias in deep neural networks to resolve high-wavenumber features of the fields' evolve during autoregressive integration. Generally, for weather and climate emulators that have gained popularity in the climate sciences, high-resolution emulators, evolve at much lower resolution effectively, due to spectral bias (Bonavita, 2023; Chattopadhyay & Hassanzadeh, 2023). This is a major cause of unphysical drifts and instabilities in these models. However, spectral bias is usually much lower when the field itself is low resolution and is thus less susceptible to instability or drifts. Hence, we focus on low-resolutiin autoregressive emulation and use a downscaling model to bring the predicted fields to higher resolution. Since, both the models are data-driven, they are very cheap to execute in inference mode; thus the final high-resolution outputs from the FCDS framework takes much less computational time than a physical model.\nUnlike other super-resolution tasks, where low resolution fields from the same distribution are downscaled to high-resolution, e.g., downscaling low-resolution ERA5 data to high-resolution ERA5, our work considers downscaling from one data distribution (GLORYS) to another (CNAPS) with different underlying physical models, parameterizations, as well as data assimilation stratgeies. This is a more realsitic set up, where the emulator is initialized with surface fields from GLORYS but the final downscaled outputs are compared with a different reanalysis product, e.g., CNAPS. While a more challneging tasks, offline downscaling and online fine-tuning together provides accurate forecasts both in short term as well as correct long-term statistical metrics that are physically consistent.\nWhile the FCDS framework uses surface ocean variables and remains stable at decadal time scales, there are some key components missing in the autoregressive model. To begin with, the model does not consider any atmospheric forcing which can only be incorporated with a separate atmospheric emulator. We are currently working towards a regional coupled emulator at high resolution. Furthermore, while we can successfully emulate the control climate, we cannot estimate regional response of the ocean to CO2 forcings. In future work, we would focus on developing the emulator with sub-surface ocean variables as well as radiative forcing to study the climate change impacts on the GoM region."}]}