{"title": "IMPROVING SPEECH RECOGNITION ERROR PREDICTION\nFOR MODERN AND OFF-THE-SHELF SPEECH RECOGNIZERS", "authors": ["Prashant Serai", "Peidong Wang", "Eric Fosler-Lussier"], "abstract": "Modeling the errors of a speech recognizer can help simulate\nerrorful recognized speech data from plain text, which has\nproven useful for tasks like discriminative language model-\ning, improving robustness of NLP systems, where limited or\neven no audio data is available at train time. Previous work\ntypically considered replicating behavior of GMM-HMM\nbased systems, but the behavior of more modern posterior-\nbased neural network acoustic models is not the same and\nrequires adjustments to the error prediction model. In this\nwork, we extend a prior phonetic confusion based model for\npredicting speech recognition errors in two ways: first, we\nintroduce a sampling-based paradigm that better simulates\nthe behavior of a posterior-based acoustic model. Second, we\ninvestigate replacing the confusion matrix with a sequence-\nto-sequence model in order to introduce context dependency\ninto the prediction. We evaluate the error predictors in two\nways: first by predicting the errors made by a Switchboard\nASR system on unseen data (Fisher), and then using that same\npredictor to estimate the behavior of an unrelated cloud-based\nASR system on a novel task. Sampling greatly improves pre-\ndictive accuracy within a 100-guess paradigm, while the\nsequence model performs similarly to the confusion matrix.", "sections": [{"title": "1. INTRODUCTION", "content": "Automatic Speech Recognition (ASR) is proliferating quickly,\nwith a variety of applications having speech as an input\nmodality. Yet, application specific audio data is often a scarce\nresource, and models trained on text data are widely being\npaired with cloud based speech recognition services. The\nnature of speech recognized text can be different from typed\ntext data, notably in the nature of errors i.e. typos vs. speech\nrecognition errors. Prior work has shown that given text data\nit is possible to simulate the recognition errors that might\noccur if the text were to be spoken, and the benefit of such\nsimulated data especially in the absence of in application\nspecific ASR data.\nFosler-Lussier et al. described a Weighted Finite State\nTransducer (WFST) framework that models word errors in\nspeech recognition by measuring kinds of phonetic errors to\nbuild a phonetic confusion matrix [1]. Anguita et al. looked\ninside the HMM-GMM acoustic model of the speech recog-\nnizer to directly determine phone distances and model errors\n[2]. Jyothi and Fosler-Lussier combined the two aforemen-\ntioned ideas and extended it to predict complete utterances\nof speech recognized text [3]. Several works have used sim-\nulated ASR error data to do discriminative training and im-\nprove ASR performance [4, 5, 6]. Simulating ASR errors can\nalso help with downstream tasks: Tsvetkov et al. incorporated\nknowledge of simulated ASR errors at train time, to improve\nthe performance of a Machine Translation system in the face\nof real speech recognized data at test time [7].\nThere is not prior published work, to the best of our\nknowledge, that predicts errors for an ASR system using a\nneural network acoustic model, or for commercial off-the-\nshelf recognizers. With modern speech recognizers we can\nno longer determine phone distances by peeking into the\nacoustic model, so some of the enhanced confusion trans-\nducer based models [2, 3] are no longer applicable. Also,\noff-the-shelf speech recognizers do not afford much access to\ninternals or information about them.\nHowever, there is a key flaw in the Fosler-Lussier et al.\nWFST model that implies a significant mismatch to poste-\nrior based models: the confusion matrix is used directly for\nprediction, and does not exhibit the peaky behavior of frame-\nlevel (or CTC-level) posterior estimates. This mismatch cre-\nates a poor model when generating errors for neural net based\nsystems. The key insight in this paper is that we use the confu-\nsion matrix over all examples to sample which phones should\nappear in the output distribution, but provide a distribution\nthat is much more peaky and ignores the smoother tails of the\nconfusion matrix. When sampling from the confusion matrix\ninstead of composing with its FST form, we find that it bet-\nter models the stochasticity of errors and confidence of neural\nnetwork acoustic models.\nThe standard confusion matrix in the WFST framework is\ncontext-independent, which does not reflect well the context-\ndependence of errors (for example, a canonical vowel next\nto /r/ will more likely be misidentified as an r-colored vowel"}, {"title": "2. MODEL DESCRIPTIONS", "content": "Our general pipeline for going from regular text to text with\nsimulated ASR errors is shown in Figurel. We convert the\ninput word sequence to a phone sequence, simulate phonetic\nerrors, and then decode back to a word sequence. We focus\nour attention on simulating the errors at the phonetic level."}, {"title": "2.1. Confusion Matrix Based Models", "content": "The WFST error prediction approach [1, 3] estimates an N-\nbest list of word confusions Wconf from an input word se-\nquence W through the following equation:\nWconf = WoP\u207b\u00b9oCoPoL\nThe words in the original text sequence are converted to\nphones using pronunciations from an inverted lexicon P\u207b\u00b9,\ncomposed with a confusion matrix WFST C, and is then\ndecoded back into a simulated ASR transcript i.e. word se-\nquence by composing a lexicon P and language model L,"}, {"title": "2.2. Neural Sequence to Sequence Based Models", "content": "We also experimented with context-dependent prediction of\nthe errors using a sequence-to-sequence model. To model\nphonetic confusions with information about the context, we\nuse a 2-layer 128-unit recurrent neural Sequence to Sequence\nModel (Seq2Seq) [9] with attention [10]. We feed the model\nwith the phonetic transcript of the true word sequence at the\ninput, and at each time step of the input we provide an addi-\ntional one hot vector containing one of five cues representing\ndifferent kinds of errors (no error, mutation, deletion, inser-\ntion of one additional phone, insertion of more than one ad-\nditional phones). At train time, we align the input and output\nphone sequence using the same technique as the confusion\nmatrix based system, to determine what kind of error is being\nmade (or not made) for each time step of the input. At test\ntime, we randomly sample to select one of the five aforemen-\ntioned cues for each timestep of the input, from a collapsed\nversion of the confusion matrix that holds information about\nthe frequencies of these five kinds of errors for each input\nphone. At test time, the probability distributions at the output\nof the Seq2Seq model are then converted into WFSTs, by se-\nlecting three phones at each time step, and a softmax function"}, {"title": "3. EXPERIMENTAL SETUP", "content": "We use the Kaldi Switchboard recipe to train a Deep Neu-\nral Network acoustic model on the Switchboard corpus. A\nSMBR criterion is used during training and decoding proceeds\nwith a trigram grammar [11]. We use this recognizer to tran-\nscribe speech data from the Fisher corpus [12], containing\nabout 1.8 million utterances with a word error rate of roughly\n30%. We use the speech recognized text from Fisher paired\nwith gold text as the training set for all our Models, hold-\ning out a validation set of 100 utterances for the tuning of\nhyperparameters. We tested our models on two kinds of data.\nFirstly, we predicted errorful transcripts for held out data from\nthe Fisher corpus, which was recognized by the aforemen-\ntioned speech recognizer that we trained. This was a set of\n500 utterances containing 504 error chunks across all recog-\nnized speech. The WFST prediction module uses the standard\nSwitchboard lexicon and grammar used in the recognizer.\nSecondly, we predicted errorful transcripts for data from\nthe Virtual Patient project [13], where volunteers read out\ntext data from doctor trainees querying a patient avatar. The\nrecorded speech was recognized using a cloud based ASR\nservice treated as a black box [14]. This was a set of 756\nutterances containing 258 error chunks across all recognized\nspeech, and the word error rate was slightly over 10%. As\nthere is a vocabulary mismatch between Switchboard and the\nVirtual Patient, but we did not want to inform the error pre-\ndiction system of the Virtual Patient vocabulary, we extended\nthe WFST lexicon and language model by leveraging mod-"}, {"title": "3.1. Data", "content": "We use the Kaldi Switchboard recipe to train a Deep Neu-\nral Network acoustic model on the Switchboard corpus. A\nSMBR criterion is used during training and decoding proceeds\nwith a trigram grammar [11]. We use this recognizer to tran-\nscribe speech data from the Fisher corpus [12], containing\nabout 1.8 million utterances with a word error rate of roughly\n30%. We use the speech recognized text from Fisher paired\nwith gold text as the training set for all our Models, hold-\ning out a validation set of 100 utterances for the tuning of\nhyperparameters. We tested our models on two kinds of data.\nFirstly, we predicted errorful transcripts for held out data from\nthe Fisher corpus, which was recognized by the aforemen-\ntioned speech recognizer that we trained. This was a set of\n500 utterances containing 504 error chunks across all recog-\nnized speech. The WFST prediction module uses the standard\nSwitchboard lexicon and grammar used in the recognizer.\nSecondly, we predicted errorful transcripts for data from\nthe Virtual Patient project [13], where volunteers read out\ntext data from doctor trainees querying a patient avatar. The\nrecorded speech was recognized using a cloud based ASR\nservice treated as a black box [14]. This was a set of 756\nutterances containing 258 error chunks across all recognized\nspeech, and the word error rate was slightly over 10%. As\nthere is a vocabulary mismatch between Switchboard and the\nVirtual Patient, but we did not want to inform the error pre-\ndiction system of the Virtual Patient vocabulary, we extended"}, {"title": "3.2. Training Details", "content": "To train the confusion matrix models, we first convert the\ngold and speech recognized transcripts to phone sequences,\nand align them using a phonetic distance based dynamic pro-\ngramming algorithm [1]. The alignment is done in such a way\nthat each input phone (e.g. /s/) is paired with a sequence of\nphones of length 0 (deletion, /s/:\u20ac), 1 (no error or mutation,\n/s/:/s/ or /s/:/z/) or more (insertion, /s/:/st/). For each possible\ninput phone, we count frequencies of various such \"alterna-\ntive\" phone sequences, and normalize them into probabilities.\nThis gives us our confusion matrix for composing or sam-\npling.\nFor the training the Sequence to Sequence (Seq2Seq)\nbased phonetic confusion model, we start with unaligned\npairs of gold and errorful phone sequences, and train it to\nminimize the cross entropy between the model predictions\nand the ground truth (i.e., the errorful phone sequence). In\nour experiments, we found that when we directly used the\nground truth sequence, which is a one hot distribution at\neach time step, the model predictions would be very peaky\nand not provide much diversity for decoding. To allow the\nmodel to learn to produce more diversity at the output, we\nsmoothed the ground truth sequence with alternatives from\nthe confusion matrix at train time:\nYsmooth = \u03b2 * Yoriginal + (1 \u2212 \u03b2) * C11[y]\nwhere y is the original phone label and Yoriginal is the one hot\nprobability distribution corresponding to it, and Ysmooth is the\nsmoothed probability distribution. C11 is a reduced version of\nthe confusion matrix that only has the alternatives of length 1\n(mutation or no error), and \u1e9e is the smoothing factor (we use\nvalue 0.8). Although C11 only captures mutation errors, we\nobserved that on smoothing, the neural network was automat-\nically learning to give meaningful weight to output phones\nat their adjacent timesteps as well. Finally, we use an Adam\noptimizer to minimize the cross entropy of the model predic-\ntions with the smoothed ground truth across minibatches of\n64 examples at every training step."}, {"title": "4. EVALUATION AND RESULTS", "content": "Following prior work, we use two metrics to evaluate the ef-\nfectiveness of our models in simulating ASR errors. The first\nmetric measures the percentage of real test set Error Chunks\nrecalled in a set of \"K best\" simulated speech recognized ut-\nterances for each gold word sequence. The error chunks are\nagain determined by aligning the gold word sequence with the"}, {"title": "5. CONCLUSIONS AND FUTURE WORK", "content": "We show that the sampling based paradigm greatly improves\nthe error prediction performance of the confusion matrix sys-\ntem. We observe that while the Seq2Seq confusion model\nmight be learning to predict errors in a context dependent\nmanner, the method does not generalize well across corpora,\nand needs further work. We think the Seq2Seq model may\nbenefit from enhancements such as a more robust generator\nfor the error types, multi-head attention, scheduled sampling,\nand beam search decoding."}, {"title": "6. ACKNOWLEDGEMENTS", "content": "This material is based upon work supported by the National\nScience Foundation under Grant No. 1618336. We thank\nAdam Stiff for sharing the paired text and speech recognized\ndata from the Virtual Patient project for our experiments."}]}