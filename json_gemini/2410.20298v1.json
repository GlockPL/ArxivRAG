{"title": "Learning from Response not Preference: A Stackelberg Approach for LLM Detoxification using Non-parallel Data", "authors": ["Xinhong Xie", "Tao Li", "Quanyan Zhu"], "abstract": "Text detoxification, a variant of style transfer tasks, finds useful applications in online social media. This work presents a fine-tuning method that only uses non-parallel data to turn large language models (LLM) into a detoxification rewritter. We model the fine-tuning process as a Stackelberg game between an LLM (leader) and a toxicity screener (follower), which is a binary style classifier (toxic or non-toxic). The LLM aims to align its preference according to the screener and generate paraphrases passing the screening. The primary challenge of non-parallel data fine-tuning is incomplete preference. In the case of unsuccessful paraphrases, the classifier cannot establish a preference between the input and paraphrase, as they belong to the same toxic style. Hence, preference-alignment fine-tuning methods, such as direct preference optimization (DPO), no longer apply. To address the challenge of incomplete preference, we propose Stackelberg response optimization (SRO), adapted from DPO, to enable the LLM to learn from the follower's response. The gist is that SRO decreases the likelihood of generating the paraphrase if it fails the follower's screening while performing DPO on the pair of the toxic input and its paraphrase when the latter passes the screening. Experiments indicate that the SRO-fine-tunned LLM achieves satisfying performance comparable to state-of-the-art models regarding style accuracy, content similarity, and fluency. The overall detoxification performance surpasses other computing methods and matches the human reference. Additional empirical evidence suggests that SRO is sensitive to the screener's feedback, and a slight perturbation leads to a significant performance drop. We release the code and LLM models at https://github.com/XXXinhong/ Detoxification_LLM.", "sections": [{"title": "1 Introduction", "content": "Identification of toxicity and other undesirable contents in user-generated texts is an active research area in NLP (Bevendorff et al., 2024). As a proactive combat (besides deletion), the task of automatic rewriting/rephrasing has received increasing attention from the NLP community (Villate-Castillo et al., 2024).\nMost existing works on text detoxification casts the problem as a variant of style transfer: the task of changing the style of a text while preserving the content. The style of a text refers to its characteristics, such as sentiment, level of formality, and political inclination (Dale et al., 2021). As a sequence-to-sequence task, style transfer tasks can employ an encoder-decoder model trained on parallel data that includes a parallel corpus of toxic sentences and their non-toxic paraphrases (Wieting and Gimpel, 2018a). Despite recent efforts to build parallel datasets (Logacheva et al., 2022), the existing parallel data collection is still in its infancy. As an alternative, style transfer models can also utilize non-parallel data. Prior works train encoder-decoder models on non-parallel data and push the decoder toward the target style with the help of style classifiers (Lee, 2020).\nThis work considers LLM-based detoxification models trained on non-parallel data. Without parallel data, few-shot fine-tuning does not apply since concrete demonstrations are not available to LLMs. Hence, we consider a different perspective on text detoxification: casting the problem as an alignment problem, where an LLM needs to be fine-tuned to align with a given preference (e.g., non-toxic over toxic contents). Such preference alignment has gained soaring popularity in LLM research as it addresses the LLM's steerability issue, aiming to achieve precise control of LLM's behaviors. Such fine-tuning requires human preference labeling, which is instrumental in reinforcement learning from human feedback (RLHF) pipelines (Ouyang et al., 2022). However, such a preference relation may not be immediately available in detoxification. Given a toxic text input, when the LLM generates two outputs that are still toxic, it is unclear which one is less toxic since both belong to the same style. In practice, we also observe that the human-labeled non-parallel data only involves binary labels (toxic or non-toxic) without explicit preference relations, making it challenging to rank texts from the same class. Consequently, RLHF may not apply due to the lack of clear preferences.\nFrom the example above, the key challenge of applying LLMs to detoxification (or style transfer in general) from the alignment viewpoint is that no preference exists among texts belonging to the same style. Therefore, LLMs need to learn from the outcome of style transfer, i.e., the feedback from the style screening (either success or failure), rather than preference labeling.\nWe model the LLM's fine-tuning process as a Stacelberg game between the LLM and a style classifier trained from human-labeled data. As illustrated in Figure 1, when the LLM (leader) receives a toxic text from the training dataset, it first generates a paraphrase and sends it to the screener (follower), who responds with a binary outcome: either success or failure. Based on this response, LLM updates the model weights by applying the gradient to our proposed Stackelberg loss. The intuition behind such loss is that 1) if the detoxification is successful, then the pair of the input text and LLM-generated text presents a clear preference, which is perfectly handled by the original DPO pipeline, and the Stackelberg loss reduces to DPO loss; 2) otherwise, only the failed transfer is utilized to compute the loss that aims to penalize LLMs for such paraphrasing. The sheer difference between our proposed fine-tuning method and RLHF is that the LLM learns from the classifier's response to its generated text without requiring additional preference labeling. We refer to the proposed fine-tuning method as Stackelberg Response Optimization. Our contributions are as follows.\n\u2022 We model LLM fine-tuning for detoxification as a Stacklberg game between the LLM and style screener. Such a Stackelberg formulation spares one from parallel data and human labeling.\n\u2022 We propose Stackelberg Response Optimization (SRO), adapted from DPO, to address the issue of missing preferences in RLHF.\n\u2022 We conduct an evaluation of the fine-tuned LLM and its comparison with a number of state-of-the-art models in text detoxification tasks. Our proposed STALR achieves superior overall performance (a joint metric synthesizing style accuracy, content similarity, and fluency) over other computing models and matches human reference.\n\u2022 We examine SRO's robustness to the screener. Empirical evidence suggests that SRO is highly sensitive to the screener's feedback. A slight perturbation in screening significantly degrades the detoxification performance."}, {"title": "2 Related Works", "content": "Style transfer and detoxification Prior works in style transfer consider encoder-decoder models on non-parallel data while using adversarial classifiers to steer the decoder toward the desired style (Shen et al., 2017a). To address the challenge of lacking parallel data, researchers have employed methods such as reinforcement learning (Luo et al., 2019), amortized variational inference (He et al., 2020), and utilizing style transfer classifiers to train models that convert texts from one style to another and vice versa (Lee, 2020).\nDetoxification specifically involves converting toxic to neutral language using datasets not directly paired but categorized for toxicity, treating toxic and neutral texts as distinct groups (nonparallel data). Several studies use datasets like the Jigsaw series (Laugier et al., 2021) or create their own from social media platforms for this purpose (Kivlichan et al., 2020; cjadams et al., 2017, 2019).\nPrior detoxification models often borrow methodologies from other domains (Shen et al., 2017b) and (Fu et al., 2018), using autoencoders (Nogueira dos Santos et al., 2018) with style classification and cycle-consistency losses, or fine-tuning denoising autoencoders like T5 (Laugier et al., 2021). Some models apply direct corrections to specific words (Tran et al., 2020; Wu et al., 2019) and then enhance text fluency with sequence-to-sequence models. Similarly, (Dale et al., 2021) uses masked language models for targeted edits.\nPreference alignment and fine-tuning of LLM LLMs have shown great capabilities in the past few years in generative tasks across various domains (Bubeck et al., 2023; Li and Zhu, 2024), and their encouraging success relies on the precise control of LLMs' output through alignment fine-tuning. One of the popular methods for gaining such control is reinforcement learning with human feedback (Ouyang et al., 2022). By training a reward model on human-labeled data, one can use the reward model to fine-tune LLM models using RL algorithms.\nTo avoid bias that exists in the reward model, (Rafailov et al., 2024) develops Direct Preference Optimization (DPO), which does not rely on another reward model to calculate loss; instead, DPO calculates the loss based on the distance between logits and the expected answers. In addition to DPO, there are also some emerging techniques, such as Identity Preference Optimisation (IPO) (Azar et al., 2023), which avoid Bradley-Terry modelization assumption to avoid overfitting problems, and Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024), which apply human-aware losses (HALOs) to directly maximizing the utility of generations instead of maximizing the likelihood of preferences.\nGame-theoretic RLHF Multi-agent reinforcement learning offers a robust theoretical grounding for examining learning agents that interact and need to adjust their strategies in response (Li et al., 2022a,b). Recent developments in game-theoretic methods tailored to RLHF include Self-Play Preference Optimization (SPO) (Swamy et al., 2024), which utilizes self-play to compare win rates across different trajectories. Similarly, Self-Play fIne-tuNing (SPIN) (Chen et al., 2024) employs self-play to create synthetic high-quality data by differentiating self-generated continuations from those generated by humans. Both approaches utilize a consistent reward model akin to traditional RLHF. Additionally, Nash-RLHF (Munos et al., 2024) introduces a competitive setup involving two language models, each aiming to gain favor from a human evaluator.\nClosely related to our Stackelberg formulation, Chakraborty et al. (Chakraborty et al., 2024) also consider the preference alignment as a Stackelberg game, where the reward model acts as the leader and the RL agent as the follower. However, the reward model is still built on complete preference relations, whereas our work focuses on incomplete preference resulting from the binary feedback of the screener."}, {"title": "3 Preliminary", "content": "Notations Consider a non-parallel dataset denoted by D, which consists of a blend of toxic and non-toxic texts (sentences). We denote by $x \\in D$ A typical text from the dataset. In our text detoxification task, such a text x is often paired with a prompt instructing the LLM to rewrite the sentence. For example, the prompt used in our experiment is \"paraphrase the following: x.\" LLM, despite its intricate inner workings, is represented by a function $f_\\theta(\\cdot)$ that maps the input text x to a rephrased text y, where $\\theta\\in \\Theta$ denotes the LLM's model weights. The style classifier, performing binary classification, maps the rephrased text y to a label $a \\in \\{0, 1\\}$. Since the classifier plays as a follower in the Stackelberg game, one can think of the label as the follower's action or response toward the LLM's paraphrasing. Denote the classifier by g(y), its definition is given by\n$a = g(y) = \\begin{cases}\n1 \\text{ if y is non-toxic}, \\\\\n0 \\text{ otherwise}.\n\\end{cases}$ (1)\nThe sequel also uses $f_\\theta(y|x)$ and $g(a|y)$ to represent the dependencies among variables.\nStackelberg game To prepare the reader for the later introduced Stackelberg detoxification game, we use the notations above to review some basics. Beginning a payoff-relevant state x (which affects the players' rewards/costs), the leader determines its parameterized policy $f_\\theta$ that yields an action $y = f_\\theta(x)$ upon receiving the state. Once observing the leader's action y, the follower responds by choosing action a. The follower's objective is to find a response policy $a = g(y)$ such that its reward v(x, y, a) is maximized. The follower's best response given x, y is $a^* = \\arg \\max_a v(x, y, a) \\triangleq BR(f_\\theta(x))$. Aware of the follower's objective and best response, the leader aims to seek the optimal policy that maximizes its rewards (minimizes the"}, {"title": "4 Stackelberg Learning from Response", "content": "4.1 Detoxification as a Stackelberg game\nConsider the text detoxification task where the LLM seeks to set the model weights $\\theta$ such that the paraphrased output $y \\sim f_\\theta(x)$ is non-toxic. In other words, it aims to generate y to pass the style classifier's screening, i.e., the desired classification outcome is a = 1.\nWe define the LLM's cost function as u: D x A \u2192 {0,1} to be defined later in Section 4.2, which encourages the LLM to generate successful paraphrases that pass the screening while penalizing the failed attempts. Since the style classifier's objective is to report the underlying style accurately, one can define the style classifier's cost function as the classification loss. This work considers an SVM-based classifier and uses hinge loss (Gentile and Warmuth, 1998). As discussed in the preliminary, we can treat the trained SVM as the best response mapping.\nThe interaction between the LLM and SVM unfolds as below. First, the LLM takes as an input a text x randomly sampled from the dataset D. Then, the LLM determines a paraphrase $y = f_\\theta(x)$, which is then sent to the SVM. Finally, the SVM best responds with a classification outcome a, which decides the LLM's utility. The equilibrium problem for the LLM, the leader, is given by\n$\\min_{\\theta \\in \\Theta} \\mathbb{E}_{x \\in D} u(x, a^*) \\quad \\text{s.t.} \\quad a^* = BR(f_\\theta(x)),$ (2)\nwhere a* simply corresponds to the classification output. Naturally, the solution to (2) returns the optimal LLM model for detoxification. We introduce Stackelberg response optimization, adapted from DPO, to approximately solve for the optimal solution.\n4.2 Stackelberg Response Optimization\nGiven a preference pair (yw, y\u0131), DPO aims to solve the following minimization problem\n$\\min_{\\theta} -\\mathbb{E}_{(x, y_w, y_l)\\sim D} \\Big[\\log \\sigma \\big( h_\\theta(y_w | x) - h_\\theta(y_l | x)\\big) \\Big],$ where we define $h_\\theta(y|x) \\triangleq \\beta \\log \\frac{f_\\theta(y|x)}{f_{ref}(y|x)}$. In the detoxification task, whether preference pairs are available depends on the screening outcome. If the paraphrase y passes the screening, then (y, x) constitutes a preference pair, which can be handled by DPO. However, if y fails the screening, then both y and x are dispreferred without a clear preference relation.\nWe propose the following objective function to the LLM leader, which is a hybrid of two kinds of loss functions corresponding to cases of passing or failing the screening, respectively."}, {"title": "5 Experiment", "content": "5.1 Toxicity Screener\nOur style classification model was developed using a dataset from the Jigsaw Toxic Comment Classification Challenge (cjadams et al., 2017), which contains over 120,000 non-toxic sentences and 14,000 toxic sentences. Following the data splitting in (Logacheva et al., 2022), we allocate 95% of the dataset for training and reserve 5% for testing. We employ TF-IDF for sentence tokenization and SVM for classification to form the response. Ultimately, the screening model achieved an accuracy of 98.6% on the test data.\n5.2 SRO Implementation\nWe fine-tune the pre-trained paraphrase T5 model using the toxic sentences from the Jigsaw Toxic dataset (cjadams et al., 2017). The training process includes 15 epochs, with the batch size being 16. The learning rate is 2 \u00d7 10\u22125, and weight decay is set to 0.01. The hyper-parameter \u03b2 in SRP is 0.01.\n5.3 Competing Models\nWe compare our SRO models with other style transfer models and common baselines in the literature (Dale et al., 2021).\n\u2022 Duplicate (baseline) \u2013 copy of the input.\n\u2022 Delete (baseline) \u2013 deletion of swear words.\n\u2022 BART-zero-shot (baseline) \u2013 BART model with no additional training.\n\u2022 Mask&Infill (Wu et al., 2019) \u2013 BERT-based pointwise editing model.\n\u2022 Delete-Retrieve-Generate models (Li et al., 2018): DRG-Template (replacement of toxic words with similar neutral words) and DR-GRetrieve (retrieval of non-toxic sentences with a similar sense) varieties.\n\u2022 DLSM (He et al., 2020) encoder-decoder model that uses amortized variational inference,\n\u2022 SST (Lee, 2020) \u2013 encoder-decoder model with the cross-entropy of a pre-trained style classifier as an additional discriminative loss.\n\u2022 CondBERT(Dale et al., 2021) \u2013 BERT-based model with extra style and content control.\n\u2022 ParaGeDi(Dale et al., 2021) \u2013 a model which enhances a paraphraser with style-informed LMs which re-weigh its output.\n5.4 Metrics\nWe apply three third-party models and LLM to evaluate our model, which is customary in many style transfer works (Logacheva et al., 2022; Dale et al., 2021). Namely, we evaluate:\n\u2022 style accuracy (STA) \u2013 the percentage of non-toxic outputs identified by a style classifier trained on the Jigsaw data. Note that such a testing classifier differs from the screener in that this classifier is built on a ROBERTa model and outputs a number ranging from 0 to 1, indicating toxicity likelihood.\n\u2022 content preservation (SIM) \u2013 the cosine similarity between the embeddings of the input text and the paraphrase's embeddings (Wieting et al., 2019). The embeddings are computed by an encoder that is trained on the ParaNMT corpus (Wieting and Gimpel, 2018b).\n\u2022 fluency (FL) \u2013 the percentage of fluent sentences identified by a RoBERTa-based classifier of linguistic acceptability trained on the COLA dataset(Warstadt et al., 2019).\nThen, the J matric is computed by multiplying the three individual matrics: J = STA \u00d7 SIM \u00d7 FL.\n5.5 Results\nEvaluation Table 1 summarizes the performance of our proposed SRO, baselines, and prior works. Our model was trained on the Jigsaw Toxic Comment Classification Challenge dataset and it outperforms other models in terms of the J metric. The baseline models (Delete and Duplicate) and the BART-zero-shot model return low J scores due to the following reasons. The Delete model just deletes the toxic words in the sentence, leading to very low FL(fluency) scores. The Duplicate model has a very low STA score because it doesn't introduce any modification to the input. The BART-zero-shot model, as a pre-trained model trained on a large corpus of text, can only handle tasks based solely on its prior knowledge and the instructions given in the prompt. Thus, the BART-zero-shot model cannot perform the detoxification task, yielding a low STA score.\nNow we turn to other advanced competing models. DRG-Template and DRG-Retrieve models enjoy high STA scores because the first step of these models is to delete the tokens with toxicity, making the paraphrase pass the screening easily. However, the Template returns low FL as it just replaces the toxicity words, and the Retrieve gives low SIM because it retrieves a similar sentence from the training dataset with a non-toxic attribute, and the similarity in Retrieve is measured in Euclidean distance on the embedding of two sentences. However, a small Euclidean distance does not imply content preservation. Mask&Infill model masks the toxic markers and infills them with similar non-toxic tokens. Such a practice maintains the basic structure of the input text, leading to a high SIM score. However, the infilling may introduce words that render the whole text incomprehensible and result in a low FL score. The DLSM and SST models perform full re-generation of text (as opposed to word-wise corrections). More importantly, their decoders are trained from scratch on a relatively small dataset, leading to low fluency scores. The ParaGeDi model applies a re-weight idea on logits to control the attribute of output,\nwhich means it pays little attention to the context. Therefore, this model gives a relatively low FL score. This problem also happens to CondBERT, which also uses re-weight on logits-wise, so the FL of generation text becomes relatively low.\nIn contrast to the models above that perform word/logit-wise corrections, our SRO model, fine-tuned on a text-paraphrase LLM, is more concerned with the context, yielding a high FL score. As shown in Table 1, our SRO achieves the highest FL scores while retaining satisfying scores on STA and SIM. Of particular note, SRO outperforms all the advanced competing models in terms of SIM."}, {"title": "5.6 Ablation study on SRO loss", "content": "RS-SRO The difference between SRO and DPO lies in the use of paraphrased text that fails the screening, for which no preference exists between the input and the output. To see how the failed attempts contribute to the success of SRO, we conduct an ablation study where SRO discards the failed outputs and only performs DPO on those successfully paraphrased instances. We refer to such a practice of SRO as response-selective SRO (RS-SRO) since the leader (LLM) only considers positive responses $a^* = 1$ while ignoring those negative ones. The loss function of RS-SRO is given by $\\mathbb{E}_{(x,y)\\sim D}[\\log \\sigma (h_\\theta (y|x) - h_\\theta(x|x))]1\\{a=1\\}$. From Table 1, we can see that the performance of the RS-SRO model decreases by a small margin compared to the SRO model, which proves the use of the fail loss we apply to the SRO model contributes to the SRO's success.\nPA-SRO The above experiment demonstrates the role played by negative feedback without creating complete preference. Then, one naturally wonders if SRO's treatment of failed paraphrases would also carry over to the successful ones. Even though the pair of the input and output reveals a preference relation, it is unclear whether such a relation is needed in SRO. Using terms in the Stackelberg model, the question is whether positive feedback with complete preference is necessary. To answer this question, we modify the SRO loss when $a^* = 1$ and refer to the resulting fine-tuning as preference-agnostic SRO (PA-SRO). Now for the PA-SRO model, the pass doesn't contain the $h_\\theta (x|x)$ part in the pass loss. The loss function of PA-SRO is given by $L(\\theta) = -\\mathbb{E}_{(x,y)\\sim D}[\\log \\sigma (a^*h_\\theta(y|x) \u2013 (1 a^*)h_\\theta(y|x)]$. From the results in Table 1, we can find the PA-SRO's J score is significantly smaller than those of SRO and RS-SRO. In our loss design, the partial loss (fail) is important though, the DPO-type loss (pass) is much more important as it creates a preference pair for the model to learn a direct preference instead of giving an indirect instruction for the model to learn.\nRobustness to Misclassification The style screener plays an instrumental part in the proposed SRO, providing feedback to the LLM's paraphrase. To investigate the screener's robustness against possible misclassification in style screening, we introduce various levels of misclassification to the screener and collect detoxification metrics when the LLM learns from possibly inaccurate responses.\nWe consider the following levels of misclassification: 10%, 20%, 50%, and 70%. For a given misclassification level, such as 10%, the working of the screener proceeds as below. Given a paraphrased text from the LLM, the screener employs the trained SVM to label the text with 90% probability while classifying the text as non-toxic regardless of the true style with 10%. In other words, the introduced misclassification serves as a relaxation of the screening process, creating more preference pairs with possibly inaccurate preference relations. In this case, an originally unsuccessful paraphrase, together with the toxic input, creates a seemingly complete preference pair to be fed to SRO. By conducting such experiments, we can determine how vulnerable the LLM is to misleading preference relations. From Table 3, we can see a huge gap between J of the SRO model and SRO under 10% relaxation (denoted by R-10%). The J also slightly decreases while the confidence decreases. We can conclude that the credibility of the screener is extremely important for the performance of SRO. In our understanding, the misleading preference pairs greatly hinder the SRO model from converging and give the SRO model wrong information or preferences to learn. This greatly reduces learning efficiency because making an error requires more effort to adjust it.\nIn contrast to relaxation, another type of misclassification is overkill; that is, the screener classifies the paraphrase as toxic, whatever the true style is. In this case, the overkill greatly reduces the portion of preference pairs in the response. Consequently, the partial loss is more frequently employed in the fine-tuning process. Similar to the previous setup, we introduce 10%, 20%, 50%, and 70% overkill to the screener to investigate the LLM's robustness against such misclassification. From the result shown in Table 3, we can see that the performance under the overkill screener decreases as the overkill rate increases. Besides, as we compare the relaxation case and overkill case at the same rate, we can conclude that the complete preference pairs when passing the screening are more helpful than the partial loss in the failed cases."}, {"title": "6 Conclusion", "content": "In this paper, we provide a new Stackelberg-game perspective on text detoxification using LLM on non-parallel data, where the LLM acts as the leader, aiming to determine its model weights such that the paraphrased texts will pass the toxicity screener. Such a Stackelberg formulation spares one from collecting human preference labeling and enables the LLM to learn directly from the screener's response. We propose Stackelberg response optimization (SRO) to solve the optimal leader's policy for LLM detoxification. Experimental results indicate that our SRO achieves superior overall performance over other competing models and matches human performance. Of particular note is that SRO's success depends largely on the screener's accurate feedback; a slight perturbation in the response leads to significant performance degradation."}]}