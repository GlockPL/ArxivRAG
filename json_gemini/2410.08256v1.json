{"title": "AdaShadow: Responsive Test-time Model Adaptation in Non-stationary Mobile Environments", "authors": ["Cheng Fang", "Sicong Liu", "Zimu Zhou", "Bin Guo", "Jiaqi Tang", "Ke Ma", "Zhiwen Yu"], "abstract": "On-device adapting to continual, unpredictable domain shifts is essential for mobile applications like autonomous driving and augmented reality to deliver seamless user experiences in evolving environments. Test-time adaptation (TTA) emerges as a promising solution by tuning model parameters with unlabeled live data immediately before prediction. However, TTA's unique forward-backward-reforward pipeline notably increases the latency over standard inference, undermining the responsiveness in time-sensitive mobile applications. This paper presents AdaShadow, a responsive test-time adaptation framework for non-stationary mobile data distribution and resource dynamics via selective updates of adaptation-critical layers. Although the tactic is recognized in generic on-device training, TTA's unsupervised and online context presents unique challenges in estimating layer importance and latency, as well as scheduling the optimal layer update plan. AdaShadow addresses these challenges with a backpropagation-free assessor to rapidly identify critical layers, a unit-based runtime predictor to account for resource dynamics in latency estimation, and an online scheduler for prompt layer update planning. Also, AdaShadow incorporates a memory I/O-aware computation reuse scheme to further reduce latency in the reforward pass. Results show that AdaShadow achieves the best accuracy-latency balance under continual shifts. At low memory and energy costs, Adashadow provides a 2x to 3.5x speedup (ms-level) over state-of-the-art TTA methods with comparable accuracy and a 14.8% to 25.4% accuracy boost over efficient supervised methods with similar latency.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep neural networks (DNNs) pre-trained in the cloud are increasingly deployed onto mobile devices for autonomous intelligence at the edge [20, 42\u201344, 70, 76], especially in some cities where cloud streaming is restricted [56, 69]. Applications such as on-device motion tracking (e.g., Google ARcore [1]) and AR/VR (e.g., Apple ARKit [2], Meta Spark [68]). These DNNs must operate reliably in open-world mobile environments, necessitating swift model adaptation to unforeseen domain shifts which represent differences in the distribution of the pre-trained model's data and the data encountered during test. The shifts are mainly caused by environmental changes (e.g., weather, lighting) and sensor degradation (e.g., low resolution and Gaussian noise). For instance, an autonomous car may encounter temporary road signs or modified traffic signals due to construction work and must adjust its DNNs with a few live samples collected on-the-fly [41, 74]. Such adaptation is crucial for maintaining safety and operational efficiency. Similarly, AR headsets require seamless integration of virtual elements into the physical world. As conditions such as lighting, field of view, and backgrounds vary with user movement, these headsets need to promptly refine their object recognition/tracking models based on limited input video clips, keeping the virtual overlays consistent with the evolving physical surroundings [11, 30, 84]. For instance, a vehicle driving at 90km/h requires its 60fps traffic sign recognition model to adapt within 16.6ms to ensure safety.\nTest-time adaptation (TTA) offers a compelling solution to combat unpredictable domain shifts in mobile environments [38]. It is an emerging domain adaptation paradigm that (i) utilizes unlabeled test data, and (ii) operates independently of source data and supervision from pre-training [73]. TTA typically works in three stages. A forward pass first generates initial inference results on the shifted test samples, i.e., the live data sensed in a new environment. If inference confidence is low (e.g., high entropy), then a backward pass follows to adjust the DNN parameters using unsupervised loss based on initial forward results. Afterward, a reforward pass applies the updated DNN to the same batch of input data to make the final predictions. This continual, unsupervised, source-free adaptation is particularly advantageous for mobile applications, which often lack access to pre-training data due to privacy or network restrictions. Moreover, these applications typically preclude the possibility of pre-labeling target domains due to the requisite labour and resources.\nDespite advances in TTA algorithms [24, 73, 77], their practical adoption in mobile applications is challenging. The forward-backward-reforward pipeline of TTA, essential for adaptability to domain shifts, adds considerable latency compared to standard inference (see \u00a7 2.2), which compromises the responsiveness in time-sensitive mobile applications e.g., AR and autonomous cars, where even minor delays can drastically impact user experience and operational safety. Despite pioneer studies on efficient TTA [24, 51, 67], they focus on decreasing computational or memory cost, which cannot easily translate into reduced wall-clock time. Backward-free TTA [4, 79, 80] experience unacceptable accuracy degradation during continual adaptation. This gap necessitates latency-efficient TTA for fast and accurate adaptation to non-stationary environments for mobile computing.\nIn this paper, we propose AdaShadow, a novel framework for responsive test-time adaptation, like how a shadow swiftly responds to the body's movements. The principle is sparse updating, i.e., selectively refining critical layers to reduce both computation and memory demands during backpropagation, and thus latency [28, 49, 86]. Although the tactic has been explored to improve the latency efficiency of generic DNN training on mobile devices [28], sparse updating in TTA encounters unique challenges due to its unsupervised and online nature. We elaborate on these challenges below.\n\u2022 Challenge #1: identifying critical layers at low latency. The importance of each layer differs drastically across different data distributions [35]. In non-stationary environments for mobile computing, each data batch may exhibit unique domain shifts, demanding rapid layer importance assessment. Prior works [28, 40] assess layer importance by computing the gradients using labeled data, which are computation-intensive (i.e., higher latency) and error-prone in TTA as test data are usually unlabeled in highly dynamic mobile scenarios. For instance, ElasticTrainer [28] results in 2.98s delay and 14.8% gradient errors for ResNet50 on unlabeled data.\n\u2022 Challenge #2: predicting runtime latency for layer update. To ensure the updated model architecture meets the latency requirement, we have to know exactly the run-time latency of each retained layer in this new model. Prior works [28, 88] measure layer latency offline and then use it to predict the run-time model inference latency. Yet mobiles are interactive devices, their hardware resource availability will change dynamically due to user operations, foreground-background APP switching, and computation and memory resource competitions, which makes offline estimation inaccurate for online model inference prediction.\n\u2022 Challenge #3: online scheduling layer update strategy efficiently. Even with accurate information on each layer's importance and their runtime latency, finding the optimal sparse updating strategy to improve performance in the presence of rigid delay requirements is still challenging due to the large search space. Dedicated strategies are necessary in exploring and pruning the search space with high efficiency.\nAdaShadow addresses these challenges with three functional module designs.\nFirst, we notice that the forward pass executed with each inference in TTA offers an opportunity to evaluate layer importance, avoiding the higher latency and imprecise results of backward gradients. Based on this observation, we propose a backpropagation-free layer importance assessor that can timely assess the importance of each layer by measuring the divergence between the layers' output feature maps in different environments, without requiring labeled data.\nSecond, to deliver precise and timely adaptation latency feedback, we introduce an online, unit-based (i.e., layer) adaptation latency predictor. This predictor distinguishes between static and dynamic update delays and incorporates fine-grained, dynamic system metrics like cache hit rates, competing CPU/GPU processes, and frequency into each unit's latency for accurate runtime measurements and improved online prediction.\nThird, Given predictions of unit importance and update latency, we further develop a lightweight dynamic programming (DP)-based online scheduler. This scheduler efficiently determines the optimal layer update strategy for three-stage TTA by clearly defining specific subproblems, utilizing recursion, and eliminating invalid subproblems.\nIn the implementation, AdaShadow optimizes the TTA loss for small batches, which is common in mobile applications, and harnesses computation reuse between forward and reforward passes to boost performance. We evaluate AdaShadow across three real-world mobile scenarios, addressing over 30 types of data shifts and 5 types of resource dynamics, using three mobile devices with varied hardware architectures. Results show that AdaShadow achieves the best accuracy-latency balance in scenarios with continual shifts. Adashadow achieves a 2x to 3.5\u00d7 adaptation speedup compared to state-of-the-art TTA methods, while maintaining comparable accuracy. Additionally, it offers a 14.8% to 25.4% accuracy improvement over state-of-the-art efficient supervised adaptation methods with similar latency (\u00a7 5). Our main contributions are summarized as follows.\n\u2022 To our knowledge, this is the first work on near-/real-time on-device DNN adaptation in non-stationary environments for mobile computing without labels or source data access. It overcomes latency bottlenecks of TTA in mobile contexts without compromising accuracy.\n\u2022 We propose AdaShadow, a holistic system design that includes a backpropagation-free layer importance assessor, a runtime latency predictor, an online update scheduler, and optimizations for efficient, resource-aware TTA. It seamlessly integrates with mainstream TTA pipelines, supports various DNN architectures."}, {"title": "2 BACKGROUND", "content": "Test-time adaptation (TTA) is an emerging unsupervised domain adaptation paradigm to combat domain shifts [38]. It adapts the DNN pre-trained in the source domain to unlabelled target data during testing, enhancing the accuracy on target data [73]. Uniquely, TTA assumes no access to the source data and supervision from the pre-training stage. This setup is fit for mobile applications where (i) accessing source data is impractical due to privacy concerns or bandwidth limitations; and (ii) annotating the target domain is infeasible or labor-intensive. However, TTA often introduces significant latency (see details in \u00a7 2.2) or decline in accuracy if overfitting to the new environment [51, 77].\nWe explore responsive test-time adaptation of DNNs to non-stationary mobile domain shifts, with a focus on latency efficiency. This is because many mobile applications demand real-time responsiveness. Accordingly, the DNN should adapt swiftly, delivering accurate inference under continuous domain shifts without perceptible delays. Additionally, the non-stationary environment means the adaptation should (i) operate effectively with small batches of data; and (ii) account for runtime resources dynamics. Both requirements impose extra challenges when optimizing the latency of TTA (see \u00a7 1). Tab. 1 summarizes the differences of our work from other representative studies."}, {"title": "2.1 Primer on Test-time Adaptation (TTA)", "content": "Test-time adaptation (TTA) is an emerging unsupervised domain adaptation paradigm to combat domain shifts [38]. It adapts the DNN pre-trained in the source domain to unlabelled target data during testing, enhancing the accuracy on target data [73]. Uniquely, TTA assumes no access to the source data and supervision from the pre-training stage. This setup is fit for mobile applications where (i) accessing source data is impractical due to privacy concerns or bandwidth limitations; and (ii) annotating the target domain is infeasible or labor-intensive. However, TTA often introduces significant latency (see details in \u00a7 2.2) or decline in accuracy if overfitting to the new environment [51, 77].\nWe explore responsive test-time adaptation of DNNs to non-stationary mobile domain shifts, with a focus on latency efficiency. This is because many mobile applications demand real-time responsiveness. Accordingly, the DNN should adapt swiftly, delivering accurate inference under continuous domain shifts without perceptible delays. Additionally, the non-stationary environment means the adaptation should (i) operate effectively with small batches of data; and (ii) account for runtime resources dynamics. Both requirements impose extra challenges when optimizing the latency of TTA (see \u00a7 1). Tab. 1 summarizes the differences of our work from other representative studies."}, {"title": "2.2 Latency of TTA", "content": "Inefficiency of Prior Arts. Despite emerging research on test-time adaptation for improved data efficiency [51] and memory efficiency [24, 67], latency remains a less-explored challenge. From Tab. 2, EcoTTA [67], a state-of-the-art, can be up to 4.1\u00d7 slower than inference using the pre-trained model without adaptation (denoted as source in the table), even though they achieve higher accuracy on the drifted testing datasets. The unsatisfactory latency motivates us to zoom into the latency of TTA.\nLatency Bottleneck. Latency is related to computing, memory access, and the availability of hardware resources [32, 88]. A typical TTA pipeline [38] includes three phases: forward, backward, and re-forward. In the first two phases (forward and backward), the model updates its parameters via standard gradient descent, based on the input batch data. In the last phase (reforward), the model performs inference on the same batch of data utilizing freshly updated parameters. This pipeline, known as the sequential adaptation/inference mode\u00b9, prioritizes inference accuracy by adapting the model before making predictions on it. However, it also implies that the inference for any batch of data must wait for the completion of adaptation, leading to noticeable latency. Formally, the overall TTA latency can be calculated as:\n$T = T_a + T_{re} = T_f + T_b + T_{re}$               (1)\nwhere $T_a$ and $T_{re}$ are the latency of model adaptation and inference (reforward), and $T_a$ is further decomposed into $T_f$ and $T_b$, i.e., latency of forward and backward, respectively."}, {"title": "2.3 Problem Statement", "content": "Our primary strategy to reduce adaptation latency is sparse updating [28, 34, 40], i.e., selectively updating a subset of layers that are crucial for TTA. It reduces the computation cost of gradients and the memory accesses to retrieve intermediate activations, thereby lowering latency in the backward pass. However, we observe that updating fewer layers does not guarantee a proportional decrease in TTA latency on mobile devices. As illustrated in Fig. 2, the latency for updating the first layer is comparable to updating the last four layers. This is because updating the first layer still necessitates computing gradients for all layers, leading to significant computation and memory access overhead.\nFormally, we explore low-latency test-time adaptation to the unseen environment e via sparse updating by formulating the following constrained optimization problem.\n$\\max_{S \\in \\mathbb{N}} A_e$  s.t. $T_f+T_b(S_\\mathbb{N})+T_{re}(S_\\mathbb{N}) \\leq \\sigma \\cdot T$   (2)\nwhere $S_\\mathbb{N}$ is the optimal sparse updating strategy, which is a binary vector of length N (the number of layers), and $A_e$ is the layer importance vector of the same format as $S_\\mathbb{N}$. $T_b$ and $T_{re}$ are the optimized backward and reforward latency under strategy $S_\\mathbb{N}$, respectively. T is the overall latency without sparse updates, covering the time for a data batch to process from input to prediction."}, {"title": "3 SYSTEM DESIGN", "content": "System Overview. Fig. 3 shows the system architecture of AdaShadow. It consists of three functional modules: (i) the backpropagation-free layer importance assessor resolves Challenge #1 by evaluating layer importance via the divergence of output feature embeddings across environments, utilizing unlabeled data in the forward pass (\u00a7 3.1). (ii) the runtime layer latency predictor overcomes Challenge #2 by integrating mobile resource dynamics into the latency modeling for online calibration of offline estimates (\u00a7 3.2). (iii) the online layer update scheduler addresses Challenge #3 via a lightweight dynamic programming strategy to search for the optimal layer update strategy with high efficiency (\u00a7 3.3). Additionally, we present the implementation and additional optimization of the AdaShadow system workflow in \u00a7 4."}, {"title": "3.1 Backpropagation-free Layer Importance Assessor", "content": "A prerequisite for sparse updating is to estimate the layer importance for on-device model adaptation. This process should be latency- and memory-efficient to allow frequent revocation on low-resource mobile devices in non-stationary environments. This subsection presents a backpropagation-free approach to significantly enhance the efficiency of prior schemes [28, 40, 57] for layer importance assessment.\nLimitations of Prior Arts. Existing studies [28, 40] rely on gradients to assess layer importance on adaptation accuracy, which faces two drawbacks. (i) Accurate gradients require labels, which are absent in mobile-end TTA. (ii) More critically, even with labels, the reliance on backpropagation to compute gradients for assessing layer importance results in substantial computational and memory overhead, as gradients must be propagated and stored for each layer. For instance, TTE [40] and ElasticTrainer [28] incur latencies of up to 167.2s and 2.98s, respectively, while consuming over 2,108 MB of memory for intermediate activations. Such overheads are unacceptable for near-/real-time DNN adaptation on mobile devices.\nWe propose to assess layer importance with unlabeled data merely in the forward pass for responsiveness by measuring the divergence between the layers' output feature maps in different environments. The idea is inspired by the distribution alignment techniques [47, 61], which quantifies drifts between the source and target data via the distributions of intermediate features in the forward stage. The rationale is that when a DNN encounters domain shifts from a new environment, it will be less confident about its predictions, as reflected by a deviation in the layer's output distribution from that observed in historical environments.\nWe cater this principle for TTA context as follows. Given an input data batch $x_e$ from a new environment e, we assess the importance $a_l^e$ of layer l in adapting to $x_e$ by measuring the Kullback-Leibler (KL) divergence between the embedding $E_l^f$ of layer l's output feature $o_l^f$ on $x_e$, i.e., $E_l^f = g(o_l^f)$, and the embedding $E_l^\\mathcal{H}$ on the average output features on historical environments $\\mathcal{H}$. Specifically, the layer importance $a_l^e$ is estimated as:\n$a_l^e = D_{KL}(g(o_l^f) || g(o_l^\\mathcal{H}))$                (3)\nwhere $D_{KL}()$ denotes the KL divergence. We justify our layer importance metric as follows.\n\u2022 The KL divergence effectively quantifies the data distribution shift in mobile environment e from the history $\\mathcal{H}$ perceived at layer l. A larger divergence indicates a greater necessity to update layer 1. Since we target at adaptation on data streams, e.g., live videos, the domain shifts between adjacent batches tend to be mild. Hence KL divergence can effectively handle mild domain shifts, it is unnecessary to adopt computation-intensive metrics, e.g., the Wasserstein distance, which are intended for distributions with low overlaps [63].\n\u2022 Rather than the raw output features, we measure the divergence of their embedding via an embedding function g(.). It normalizes output features to the same dimensions for fair importance evaluation across layers. More critically, it allows robust estimation with small batches, which is essential for non-stationary environments, as will be discussed next.\nFurthermore, we implement the layer importance metric with two optimizations for mobile environments.\n\u2022 For efficient distribution estimation with small-batch mobile data, we devise g(.) with only first and second-order moments. Compared to existing methods [5, 87] that minimize differences in higher-order sample moments, it can effectively extracts sufficient distribution information from small batch data. Concretely, we compute the channel-wise means $\\mu(o_l^f) = \\frac{1}{C} \\sum_{c \\in o_l^f} o_c^f$, and the channel-wise variances $\\sigma^2(o_l^f) = \\frac{1}{C} \\sum_{c \\in o_l^f} (o_c^f - \\mu(o_l^f))^2$ from layer l's output features $o_l^f$. The embedding $E_l^f$ is then represented as $[\\mu(o_l^{f,1}), \\sigma^2(o_l^{f,1}), ..., \\mu(o_l^{f,C}), \\sigma^2(o_l^{f,C})]$.\n\u2022 To track the non-stationary mobile environments, we gradually update the historical embedding $E_l^\\mathcal{H} = g(o_l^f)$ by integrating the new environment e into $\\mathcal{H}$ via a moving average strategy: $g(o_l^f) = \\alpha \\cdot g(o_l^f) + (1 - \\alpha) \\cdot g(o_l^\\mathcal{H})$, where \u03b1 \u2208 [0, 1] is a hyperparameter controlling the incorporating rate of new environment e.\nThe layer importance assessor reuses existing embeddings, eliminating the need for additional forward propagation. This limits the computational cost to statistic extraction and KL divergence calculation. For example, in ResNet50, our importance profiler incurs only 0.2 GFLOPs overhead for these computations, significantly lower than the 4.1 GFLOPs saved by halving backpropagation when \u03c3 = 0.5. Consequently, the theoretical latency can be optimized to as low as 4.8% of the original (0.2 GFLOPs) or up to 104.8% of the original (4.3 GFLOPs) when \u03c3 = 1."}, {"title": "3.2 Runtime Layer Latency Predictor", "content": "Precise layer update latency estimation is another key input to determine the optimal sparse updating strategy. As we target at TTA on mobile devices with limited and dynamic resources, offline predicting schemes [28, 88] incur large estimation errors. This subsection introduces an accurate and lightweight approach to model and calibrate the backward\u00b2 latency of individual layers to runtime resource dynamics.\nWe empirically demonstrate the necessity of estimating the training latency at runtime. Specifically, we first apply ElasticTrainer [28], the state-of-the-art sparse training scheme that predicts layer latency offline, to speed up the adaptation of a ResNet50 and NVIDIA Jetson NX by an acceleration ratio of about 2x under abundant resources (denoted as offline in Fig. 5). Then we simulate four dynamic resource conditions dr\u2081 ~ dr4 commonly seen in mobile systems [62]: dr\u2081: increase the mobile CPU/GPU temperature to 60 \u00b0C; dr2: add three competing compute-intensive processes; dr3: occupy the cache to tune the cache hit rate to be 30%; dr4: combine all factors from dr\u2081 ~ dr3. When running ElasticTrainer [28] in these four mobile system conditions using configurations obtained in the offline case, we observe mild to drastic decrease in acceleration ratios. The experiment shows that the offline layer latency estimates can be significantly inaccurate due to dynamic resource availability. [62] incorporates dynamic resources into latency predictions using a Graph Neural Network (GNN), however, incurring substantial overhead. Furthermore, latency profiling is ineffective without actual execution, as noted in [10]. In contrast, our proposed method delivers higher acceleration ratios by adapting to these runtime conditions.\nKey Idea. We observe that the execution latency of a DL unit mainly hinges on on-chip kernel computation and off-chip memory access. Each unit's kernel execution strategy and memory allocation are static, whereas kernel utilization, temperature, and cache-hit-rate are dynamic and measurable at runtime. Moreover, this unit-based approach is versatile across different DL models. For example, basic units in ResNet include Conv2d, BatchNorm, and Linear, while in Transformer, they consist of projectors Q, K, V, LayerNorm, and the feed-forward network (FFN).\nMethods. In offline predicting with abundant, static resources, the latency $t_l^{off}$ of layer l can be estimated as $t_l^{off} = t_l^{c,off} + t_l^{m,off}$ where $t_l^{c,off}$ and $t_l^{m,off}$ are corresponding latency for computation and memory access profiled offline, respectively. We account for limited, dynamic runtime resources on device by modeling the layer latency $t_l = \\pi_1 \\cdot t_l^{c,off} + \\pi_2 \\cdot t_l^{m,off}$, where $\u03c0_1 > 1$ and $\u03c0_2 > 1$ are the computation and memory expansion coefficients. As measuring $t_l^{c,off}$ and $t_l^{m,off}$ separately is challenging, we correlate tl to $t_l^{off}$ as follows.\n$t_l = (\\frac{\\pi_1}{\\eta_l + 1} + \\frac{\\pi_2}{\\eta_l + 1}) \\cdot t_l^{off}$               (4)\nNote that $t_l^{off}$ is measurable offline. To simplify Eq. 3.2, we define $\u03b7_l = \\frac{t_l^{m,off}}{t_l^{c,off}}$, a layer-dependent metric that remains constant regardless of runtime resource conditions. This metric captures the effects of the computation expansion coefficient \u03c0\u2081 and the memory expansion coefficient \u03c0\u2082 on overall unit latency. By using distinct coefficients for computation and memory, we can effectively profile their impacts on runtime performance. The two coefficients \u03c0\u2081 and 2 reflect resource limitations on computation and memory latency, applicable across layers and measurable during runtime. We'll detail how to assess these three hyperparameters next.\n\u2022 Profiling \u03c0\u2081. The computation expansion coefficient \u03c0\u2081 depicts two runtime factors that affect computation latency: (i) number n of competing processes, and (ii) core temperature $tem^e$ of mobile CPU/GPU.\nThe OS kernel of the mobile CPU [3, 65] and GPU [54, 72] often employs round robin scheduling (see Fig. 6). Hence an increase in process number leads to a linear growth in process waiting time.\nDynamic Voltage Frequency Scaling (DVFS) [22, 31, 39] is often activated in the mobile CPU/GPU to avoid overheating by reducing the clock frequency. This also increases the computation time.\nAccordingly, we set $\u03c0\u2081 = \\frac{freq(tem^{off})}{freq(tem^e)} [1 + f(n)]$, where f (\u00b7) is a linear function determined offline depicting the process switching overhead. The dynamic frequency freq() accounts for the DVFS.\n\u2022 Profiling \u03c02. We relate the memory expansion coefficient \u03c0\u2082 to the cache-hit-rate q, which is directly measurable at runtime. This is because sharing the limited cache among processes increases the memory access latency. Also, in case of cache miss, there is an extra latency for data movement, which is determined by the ratio between the bus bandwidth $b_{DRAM}$ and the cache bandwidth $b_{cache}$. Hence, we set $\u03c0\u2082 = q = \\frac{b_{cache}}{b_{DRAM}} + (1 - q) \\cdot \\frac{b_{DRAM}}{b_{cache}}$.\n\u2022 Profiling ni. Although it is challenging to measure $t_l^{c,off}$ and $t_l^{m,off}$ separately, their ratio $\u03b7_l = \\frac{t_l^{m,off}}{t_l^{c,off}}$ can be profiled offline. Specifically, we transform $\u03b7_l = \\frac{\\epsilon_l \u03b4_c}{\\epsilon_m \u03b4_m}$, where $c^l, m^l, \u03b4_c,$ and $\u03b4_m$ denote amount of MAC, amount of memory accesses, unit MAC time, and unit memory access time of layer l, respectively. $c^l$ and $m^l$ can be directly derived according to the type and hyperparameters of layer 1. $\u03b4_c$ is obtained from the processor's FLOPS F: $\u03b4_c = \\frac{1}{F}$, while dm is obtained from the cache bandwidth $b_{cache}$: $\u03b4_m = \\frac{1}{b_{cache}}$. We also include non-parameterized layers, e.g., activation layers, in $t_l^{off}$ as they also involve in gradient computation. The cost of latency prediction only involves sensing dynamic resource states and calculating execution latency using Eq., resulting in negligible computational and memory overhead."}, {"title": "3.3 Online Layer Update Scheduler", "content": "This subsection presents a dynamic programming (DP) based lightweight online scheduler that efficiently decides the best layer update strategy $S(\\mathbb{N})$ for Equ.(2), given the layer-wise metrics of a DNN. Picking the optimal solution to Equ.(2), an NP-hard nonlinear integer programming problem, can be time-consuming[40]; We formulate specific subproblems, recursion, and space for three-stage TTA.\nConsider a DNN with N layers. Let the importance and update latency of layer I (counting from the last layer) be $a_l^e$ and $t_l$ in environment e. A brute-force search for the optimal update strategy $S(\\mathbb{N})$ has a complexity of $O(2^\\mathbb{N})$, which is impractical for online scheduling. To address this, we propose a dynamic programming (DP) formulation to solve Equ.(2). Specifically, we aim to find the optimal layer selection $S(\\mathbb{N})$ that maximizes cumulative importance within the latency budget $T_{bgt} = \u03c3 \u00b7 T \u2212 T_f$. Let P[l][t] represent the maximum cumulative layer importance from the last layer to layer l for a given latency budget t. The binary vector $S_e[l][t]$ corresponds to one solution for P[l][t], while $S_e[l]$ denotes the optimal solution. We formulate the DP at the granularity of t and l.\n\u2022 Discrete time: We discretize time into NT units, i.e., at a resolution of $T_{bgt}/N_T$. We empirically set $N_T$ = 500 which effectively balances search cost and accuracy.\n\u2022 Layer-wise: It is natural to decompose sub-problems layer-wise to support diverse DNN architectures. It also aligns with the layer-wise DNN execution scheme on mobile devices with limited resources [88]. Finer-grained decomposition e.g., tensor-wise as [28] can be inefficient ($2^{14} \\cdot N_T$ sub-problems in tensor-wise vs. $10^7 \\cdot N_T$ sub-problems in layer-wise for ResNet50).\nAccordingly, 0 \u2264 l \u2264 N and 0 \u2264 t \u2264 NT, and the DP formulation would reduce the time complexity to $O(\\mathbb{N}^2 N_T)$ considering the time complexity O(N) of solving each sub-problem (details below). The base cases are trivial. We set P[0] [t] for all 0 \u2264 t \u2264 NT as 0. This is because the maximum cumulative layer importance is 0 if no layer is selected for updating. The recursion for P[l] [t] when l > 0 is more subtle.\ni) Case 1: If layer l is selected, then the maximum cumulative layer importance becomes $P[l_k][t \u2013 \\Delta t] + a_l^e$, where layer $l_k$ is the last layer selected in P[l - 1][t], i.e., the closest selection to layer 1. This is because updating one more layer decreases the latency budget when backpropagating till layer l \u2013 1 by \u0394t (details below) and increases importance by $a_l^e$.\nii) Case 2: If layer l is not selected, the maximum cumulative layer importance remains P[l - 1][t].\nAs shown in Fig. 7, we clarify the two cases assuming $l_k$ = l - 2. Since we search for the maximum, the final recursion becomes: P[l, t] = max{P[l \u2212 1, t], P[$l_k$, t \u2212 \u2206t] + $a_l^e$}. Note that the extra latency \u0394t induced by selecting layer l in Case 1 is not simply its layer update latency $t_l$. From Fig. 7, $\u0394t = t_{low}^k + \\sum_{m=l_k+1}^{l-1} t_m^\u03c9 + t_{low}^k$, where $t_{low}^\u03c9$ is the gradient calculation time of weight $w_i, \\sum_{m=l_k+1}^{l-1} t_m^\u03c9$ is the gradient calculation time of activations between layer l - 1 and layer $l_k$, and $\\sum_{m=l_k+1}^{l-1}$ time is the reforward time between layer l and layer $l_k$ + 1. Noting that $t_l = t_l^{ox} + t_l^{ow}$, we utilize the amount of MAC of $t_l^{ox}$ and $t_l^{ow}$ to infer their proportion of latency in $t_l^{op}$. As $l_k$ is unknown in advance, we gradually decrease $l_k$ from l - 1 to 0 to explore all possible $l_k$ to find the $l_k$ that maximizes P[$l_k$] [t \u2013 \u0394t] with a time complexity of O(N).\nGiven the above formulations, we can explore all P[l] [t] recursively till P[N][$T_{bgt}$], and then we can search the optimal layer update strategy $S_e[N]$. To further improve the efficiency, we refine the search space by removing two types of invalid sub-problems.\n\u2022 For $T_{bgt}$, we iteratively exclude sub-problem P[l, t] if t exceeds the latency budget $T_{bgt}$. We can also discard all sub-problems generated from an invalid P[l, t].\n\u2022 For t in sub-problem P[l, t], if the gradient computation time $ \\sum_{i=1} \u03b4x $ tmoff exceeds t, the total latency of P[l, t] will also exceed t even if no layers are selected for updating. Hence, we can discard such P[l, t] and all sub-problems it generates.\nTo eliminate redundant calculations, we identify overlapping solutions by formulating the latency of both backward and reforward into the same recursive sub-problem. This is because, as noted in \u00a7 2.3, the position of the first update layer impacts the latency of both backward and reforward simultaneously."}, {"title": "4 IMPLEMENTATION", "content": "This subsection presents how the layer importance assessor (\u00a7 3.1), layer latency predictor (\u00a7 3.2), and layer update scheduler (\u00a7 3.3) cooperates in the forward-backward-reforward pipeline (see Fig. 3) as well as our additional optimizations to improve the accuracy and efficiency of TTA on mobiles.\nAt initialization, DNNs are pre-trained in the cloud using source datasets and then deployed to mobile devices. Upon receiving a batch of test samples xe from a new environment e, AdaShadow adapts the DNN as follows.\n\u2022 Forward phase. AdaShadow employs the layer importance assessor and the latency predictor to derive the layer importance af and the backward and reforward latency to and tre for each layer of the DNN. Meanwhile, it prepares each layer's output feature maps of and computation graph for the backward phase.\n\u2022 Backward phase. AdaShadow calls the layer update scheduler to search for the optimal strategy S (N), based on af, tp, and the obtained in the forward stage. AdaShadow prunes computation graph nodes of layers not in S (N) to skip gradient calculation at the compiler level, to realize the selective propagation process. The training loss will be discussed later.\n\u2022 Reforward phase. AdaShadow infers on xe using the updated model from the backward stage. AdaShadow partially reuses computation results from the forward stage to accelerate the inference, as described next.\nWe propose two more optimizations during backward and reforward to boost the performance.\n\u2022 Adaptation loss for small batches. Existing TTA methods [52, 73", "52": "."}]}