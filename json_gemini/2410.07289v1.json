[{"title": "Principal Orthogonal Latent Components Analysis (POLCA Net)", "authors": ["Jose Antonio Martin H.", "Freddy Perozo", "Manuel Lopez"], "abstract": "Representation learning is a pivotal area in the field of machine learning, focusing on the development of methods to automatically discover the representations or features needed for a given task from raw data. Unlike traditional feature engineering, which requires manual crafting of features, representation learning aims to learn features that are more useful and relevant for tasks such as classification, prediction, and clustering. We introduce Principal Orthogonal Latent Components Analysis Network (POLCA Net), an approach to mimic and extend PCA and LDA capabilities to non-linear domains. POLCA Net combines an autoencoder framework with a set of specialized loss functions to achieve effective dimensionality reduction, orthogonality, variance-based feature sorting, high-fidelity reconstructions, and additionally, when used with classification labels, a latent representation well suited for linear classifiers and low dimensional visualization of class distribution as well.", "sections": [{"title": "1 Introduction", "content": "Representation learning is a pivotal area in the field of machine learning, focusing on the development of methods to automatically discover the representations or features needed for a given task from raw data. Unlike traditional feature engineering, which requires manual crafting of features, representation learning aims to learn features that are more useful and relevant for tasks such as classification, prediction, and clustering. This approach is integral in the performance of deep learning models, where layers of representation are learned hierarchically to capture increasingly abstract features of the data (Bengio et al., 2013).\nThe importance of representation learning lies in its ability to make complex data more accessible for machine learning algorithms. By learning meaningful representations, models can improve generalization to unseen data and reduce the reliance on domain-specific knowledge, thus enabling the application of machine learning in more diverse and complex domains (LeCun et al., 2015). Techniques such as autoencoders, word embeddings, and convolutional neural networks are prime examples of how representation learning has revolutionized tasks in natural language processing, computer vision, and beyond (Goodfellow et al., 2016).\nAs the field progresses, advancements in representation learning continue to enhance the capabilities of machine learning models, driving innovation in areas such as transfer learning, where representations learned in one context are adapted for use in another, and in unsupervised learning, where representations are learned without explicit labels (Radford et al., 2021). These developments underscore the growing significance of representation learning in shaping the future of artificial intelligence."}, {"title": null, "content": "Disentangled representations aim to encode information such that different dimensions of the representation correspond to distinct and independent factors of variation, making the learned features more interpretable and useful for downstream tasks. Techniques like B-VAE (beta-Variational Autoencoders, Higgins et al. 2017) have been proposed to encourage disentanglement. However, the concepts of disentangled or uncorrelated features seems to be vague or not rigorously or conventionally well defined in some contexts. The lack of formal definitions and standardized evaluation metrics for disentanglement poses challenges for advancing the field (Locatello et al., 2019a,b).\nFrom the formal mathematical point of view, there is a hierarchy (see appendix A) that begins with the weakest form, linear independence. Linear independence refers to a set of features or functions where no feature can be expressed as a linear combination of the others (Halmos, 1958). Orthogonality, where features or functions are not only linearly independent but also perpendicular, meaning their inner product is zero (Strang, 1993). Finally, the most stringent level is functional independence, where no feature or function can be expressed as a function of any combination of the others, capturing a stronger notion of independence that goes beyond linear relationships (Kolmogorov, 1950). This concept is critical in contexts where complete disentanglement of factors of variation is required, such as in the study of complex systems and high-dimensional data. Mutual Information is a measure of Statistical Independence and is usually used as a proxy to approximate it, but is weaker than Functional Independence (Cover and Thomas, 1991; Kraskov et al., 2004; Meyer et al., 2008; Peng et al., 2005; Shannon, 1948).\nIn addition to disentanglement, dimensionality reduction is a key focus within the realm of autoencoders. Autoencoders inherently perform dimensionality reduction by encoding input data into a lower-dimensional latent space before reconstructing it. Variational autoencoders (VAEs) and their extensions are particularly noted for balancing dimensionality reduction with the preservation of important data characteristics, often through the incorporation of probabilistic modeling techniques (Kingma and Welling, 2013). Information compression is a well-defined concept, particularly in the context of lossless compression, where it refers to the exact recovery of original data from the compressed form. The theoretical limits and challenges, including the undecidability of optimal compression in in the general case, are well-established in this domain (Chaitin, 1974). In lossy compression, while the concept remains well-defined, it becomes context-dependent, balancing between compression rate and acceptable loss of quality (Cover and Thomas, 1999).\nAmong these objectives, Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901) has long been a cornerstone method outside the realm of neural networks, widely adopted for its simplicity, interpretability, and effectiveness in dimensionality reduction. PCA's strengths lie in its ability to reconstruct data from reduced dimensions and its extraction of orthogonal features. In addition to PCA, Linear Discriminant Analysis (LDA) is another powerful technique for dimensionality reduction, particularly in supervised learning contexts. While PCA focuses on maximizing variance to project data into a lower-dimensional space, LDA aims to maximize the separation between different classes by finding a linear combination of features that best separates the classes (Fisher, 1936). This makes LDA particularly effective in classification tasks where the objective is to preserve class separability in the reduced-dimensional space.\nLDA, like PCA, assumes linearity in the data, but it goes a step further by incorporating label information, which PCA does not use. However, LDA's reliance on assumptions such as equal covariance matrices among classes can be a limitation in cases where this assumption does not hold. Despite these limitations, LDA remains a preferred choice for dimensionality reduction when the goal is to enhance class separability rather than just reduce the data's dimensionality.\nHere, we introduce Principal Orthogonal Latent Components Analysis Network (POLCA Net), a deep learning architecture designed to capture the benefits of PCA and (optionally) LDA while leveraging non-linear mappings to better handle complex data. POLCA Net extends autoencoders reconstruction loss by incorporating specific constraints as a set of carefully designed complimentary loss functions. POLCA Net enables accurate data reconstruction from reduced dimensions, which is crucial for effective data compression and noise reduction applications. The non-linear nature of"}, {"title": null, "content": "POLCA Net allows it to capture more complex relationships in the data, potentially leading to more accurate reconstructions than linear PCA and LDA, especially for datasets with inherent non-linear structures.\nThe key features of POLCA Net latent space representation can be summarized as follows:\n1. Orthogonal latent features: enforces orthogonality in the latent space through a specialized loss term that minimizes the average squared cosine-similarity of the latent components.\n2. Data Compression and dimensionality reduction: encourages the information compression in earlier latent dimensions via a center of mass loss.\n3. Optional Learning with labels: can be trained with class labels to obtain similar functionality to LDA.\n4. Optional Linear decoder: employs a pure linear decoder to maintain theoretical guarantees associated with linear methods, preserves additivity and homogeneity in the latent space, allowing for meaningful algebraic operations on the learned representations.\nThe experimental results indicate that POLCA Net not only captures the key advantages of PCA and LDA but also provides a versatile alternative for handling complex, high-dimensional data. The agnosticism with respect to the encoder and decoders used and the non-linear capabilities of POLCA Net combined with its ability to maintain PCA-like properties, makes it a powerful tool for modern data analysis and machine learning tasks, bridging the gap between traditional linear techniques and the broad flexibility of deep learning approaches."}, {"title": "1.1 Related works: PCA, autoencoders, and other dimensionality reduction techniques", "content": "A thorough review of the autoencoder architecture and its variants is presented in by (Li et al., 2023). However, the paper notes that there is no clear attempt to reproduce the capabilities of PCA (Principal Component Analysis) in an autoencoder, such as orthogonality and variance sorting in the latent space. The only exception to this is the Kernel-PCA approach, as discussed in (Dang and Pei, 2018; Majumdar, 2021; Pei, 2017). Kernel-PCA achieves non-linearity by applying a kernel transformation instead of an activation function.\nOrthogonality and Independence Ren et al. (2021) investigate an alternative method for achieving PCA using an autoencoder. Specifically, they aim to achieve results similar to Kernel-PCA by designing a series of linear and nonlinear layers to map the input data into a high-dimensional latent space. To ensure orthogonality in the latent space, the authors use a Cayley transform. However, unlike traditional PCA, the authors do not obtain a set of latent features sorted by data importance. The authors apply the method to the TE process: an industrial benchmark widely applied for the simulation and verification of process monitoring methods.\nIn the same vein, Wang et al. (2019) proposes a clustering algorithm that employs an autoencoder with an orthogonal constraint in the latent space. However, this approach does not account for the feature ordering based on their significance. The authors ensured the orthogonality constraint by introducing an additional loss term that minimizes the discrepancy between the identity matrix and the multiplication of latent vectors (in a batch) with their transpose. The authors conducted an examination of the challenges associated with imposing this constraint using their technique and comparable methods. The researchers utilize the proposed model on three distinct datasets comprising of images of faces and handwritten digits: MNIST, USPS, and YTF datasets.\nPlaut (2018) implemented PCA using a linear autoencoder and demonstrated that it is possible to extract the PCA loading vectors from the autoencoder weights. However, this method is limited to linear autoencoders. Previously, Kotropoulos and Schuller (2012) presented an earlier work that"}, {"title": null, "content": "applies theoretical approaches to implement PCA using neural networks. The authors introduced Hebbian learning rules and a complex domain extension. They proposed an autoassociative MultiLayer Perceptron (MLP) for nonlinear PCA, which is presented in the study. A similar approach is taken by Bartecki (2012).\nAn insightful survey work presented by Ghojogh et al. (2022), examines the interplay between PCA, Factor Analysis, and Variational Autoencoder (VAE). The relationship between PCA and B-VAE is noteworthy (Rolinek et al., 2019), as the regularization with the diagonal normal distribution in VAES and the feature disentanglement provided by the B-VAE variant lead to orthogonality. Our method, on the other hand, achieves similar objectives through a distinct and efficient approach.\nAlso, Huang et al. (2018) introduce a batch normalization layer that not only normalizes the inputs to the subsequent layer but also removes correlation between them. To accomplish this, they employ a Zero-Phase Component Analysis (ZCA) whitening matrix and offer an algorithm for its differentiation during backpropagation. They do not present a variance ordering of the outputs nor dimensionality reduction.\nOrthogonality and Dimensionality Reduction Migenda et al. (2021) presents an online neural network-based algorithm for principal component analysis (PCA). The algorithm is complex and includes an eigenvalue approximation inside one of the defined layers. Its purpose is to be practical when an online adjustment is required. The approach is applied on a wide range of datasets with varying characteristics.\nIn a recent study Pham et al. (2022) and Ladjal et al. (2019) propose a PCA-Autoencoder method for producing independent latent components with dimensions sorted by importance of the data. The method is applied to two sets of images: the CelebA dataset and a custom set of ellipse images. To ensure independence of the latent space components, the authors define and minimize a loss term associated with the magnitude of the covariance matrix during training, in a way similar to the method proposed here. To sort the latent components by feature importance, they propose a method based on training a series of autoencoders, where each successive autoencoder has a larger latent dimension while keeping the previously learned dimensions fixed. This approach is computationally intensive and does not allow for precise control over the actual importance of each latent component, as it assumes a natural adjustment of feature importance by the training process. By contrast, the approach presented here directly manages the ordering of the latent components according to their variance magnitude, enabling greater control over the sorting of components by importance. Additionally, our method avoids the need for training a series of autoencoders, which results in more meaningful and interpretable representations.\nAnomaly and Out-of-Distribution detection Our proposed solution provides an alternative to traditional anomaly detection techniques, such as Kernel PCA (Yang et al., 2022), which rely on sparse representation reconstruction-based methods and entail significant computational overhead. Furthermore, it serves as an alternative to reconstruction-error methods that typically employ autoencoders (AEs), variational autoencoders (VAEs), and generative adversarial networks (GANs, Yang et al. 2022, Salehi et al. 2022 and Chalapathy and Chawla 2019). Our approach offers the added benefit of an orthogonal and variance-ranked latent space, which further enhances the interpretability and efficiency of the learned representations.\nReduced Order Models (ROMs) Efforts to develop efficient data-driven Reduced Order Modeling (ROM) techniques, which can create computationally inexpensive lower-order representations of higher-order dynamical systems, have gained considerable attention in recent years (Vinuesa and Brunton, 2022). An area of particular interest lies in the utilization of neural network-based dimensionality reduction techniques, such as AES, VAEs, and GANs as well as variants of PCA, as implementation alternatives for reduced order models (Aversano et al., 2019; Pant et al., 2021; Simpson et al., 2023). POLCA-Net presents a promising alternative to these methods, providing a highly expressive ROM"}, {"title": "2 POLCA Net", "content": "POLCA Net is essentially an autoencoder architecture containing encoder network and a decoder network as shown in Fig. 1 plus a loss function composed by a set of weighted sub losses, each defined for a particular purpose or constraint of the latent space desired characteristics. The POLCA's loss acts on the bottleneck part (latent space) generated by the encoder combined with the reconstruction loss which is obtained from the difference between the input to the encoder and the output of the decoder. In terms of dimensionality reduction, POLCA Net reduces the dimensionality of high-dimensional data while preserving its underlying structure and relationships. This compression sorts as well the latent dimensions by their variance, similar to how PCA orders its principal components. This is accomplished through the center of mass loss that encourages the concentration of information in earlier latent dimensions, and optionally allowing for effective truncation of less significant dimensions. Also, a key feature of POLCA Net is its ability to extract orthogonal features from the input data, mirroring one of PCA's most valuable properties. This orthogonality is achieved through a specific term in the loss function that minimizes the cosine similarity between different latent dimensions. Unlike \u0420\u0421\u0410, which achieves a near perfect (up to numeric precision) orthogonality through linear transformations, POLCA Net enforces orthogonality in a non-linear latent space, offering a more flexible and potentially more powerful representation. In addition, POLCA Net can be used in a supervised setting in the same way as LDA, by incorporating an additional classification loss (such as cross entropy) and using the same latent space features as classification variable."}, {"title": "2.1 Multiobjetive Loss Function:", "content": "POLCA Net uses a composite loss function $\\mathcal{L}_{polca}$ that guides the learning process:\n$\\mathcal{L}_{polca} = \\mathcal{L}_{rec} + \\mathcal{L}_{class} + \\alpha \\mathcal{L}_{ort} + \\beta \\mathcal{L}_{com} + \\gamma \\mathcal{L}_{var}$,\nwhere $\\mathcal{L}_{rec}$ is the reconstruction loss (mean squared error), $\\mathcal{L}_{class}$ is an optional classification loss (such as cross entropy) or zero, $\\mathcal{L}_{ort}$ is the orthogonality enforcing loss, $\\mathcal{L}_{com}$ is the center of mass loss"}, {"title": null, "content": "(dimensionality reduction), $\\mathcal{L}_{var}$ is a variance regularization loss. The hyperparameters $\\alpha$, $\\beta$ and $\\gamma$ are used to weigh each loss component.\nThe reconstruction loss, $\\mathcal{L}_{rec}$, is the mean squared error (MSE) between the input and the reconstruction."}, {"title": "Orthogonality and Independence.", "content": "The loss $\\mathcal{L}_{ort}$, encourages the latent features to minimize the cosine similarity matrix S over the normalized latent representations. The implementation only requires computing the upper triangular matrix due to symmetry excluding the main diagonal as well:\n$\\mathcal{L}_{ort} = \\frac{2}{n(n - 1)} \\sum_{1<i<j<n} S_{ij}$\nwhere $S_{ij}$ is the (i, j)-th element of $S = Z^T Z$, and $\\bar{z}_i = \\frac{z_i}{\\|z_i\\|_2}, Z = [\\bar{z}_1, ..., \\bar{z}_N]^T$, N is the batch size, n the latent space dimension and $z_i$ are the latent features."}, {"title": "Dimensionality Reduction and Variance Regularization.", "content": "The center of mass loss, $\\mathcal{L}_{com}$, is designed to concentrate information in the earlier latent dimensions. It is computed as the weighted average of $L_1$-normalized variances and slightly exponentiated location components $i^{1+\\epsilon}$ ($ \\epsilon = 0.25$). The effect of this loss enables progressive reconstruction, where a rough approximation of the input can be obtained from just the first few latent dimensions, with finer details added as more dimensions are included. The variance regularization loss $\\mathcal{L}_{var}$, control the total per batch variance to prevent a possible gaming against the center of mass loss:\n$\\mathcal{L}_{com} = \\frac{\\sum_{i=0}^{n-1} i^{1+\\epsilon} \\cdot \\mathbb{E}[(z_i - \\mathbb{E}[z_i])^2]}{n \\cdot \\mathcal{L}_{var}}$,\n$\\mathcal{L}_{var} = \\sum_{i=0}^{n-1} \\mathbb{E}[(z_i - \\mathbb{E}[z_i])^2]$"}, {"title": "3 Experimental Analysis", "content": "For the experimental work, we used 16 different and diverse datasets containing both gray-scale and color images which are publicly available and widely known. Specifically, we employed the MNIST dataset (LeCun et al., 1998) and the FashionMNIST (fmnist, Xiao et al. 2017) dataset available in Pytorch. The 12 2D-MedMNIST (Yang et al., 2021, 2023) datasets, a large-scale MNIST-like collection of standardized biomedical images, and finally we included two more synthetic datasets generated from simple and high frequency sinusoidal images.\nThe experiments where performed as a comparison of PCA vs. POLCA Net, by training and testing both on all the refereed datasets. We evaluated the reconstruction capability as well as the classification informativeness of the generated reduced latent space. For evaluation of reconstruction quality across all datasets we used three standardized metrics: Normalized Mean Square Error (NRMSE), Peak Signal to Noise Ration (PSNR) and the Structural Similarity Index Measure (SSIM) of decoder outputs (reconstruction) vs. original images. For evaluating the classification power of the learnt latent features representation, we trained four different linear classifiers per dataset: The Perceptron, Ridge Classifier, Logistic Regression and Linear SVM (linear kernel), and evaluated the Accuracy and F1-Score metrics, for each classifier and dataset. We used the already defined dataset's train and test split data and collected all the reconstruction and classification metrics for each split as well.\nFor evaluating the POLCA Net multiobjective loss function, we designed a procedure to collect and analyse the gradients of all the POLCA Net losses during training, to evaluate and validate the interactions between each pair of losses. We use simple definition of loss similarity (s) to evaluate"}, {"title": null, "content": "possible loss conflicts or collaborations, the index s is based on the cosine similarity of gradient losses as shown next:\n$s = \\frac{\\nabla L_i \\cdot \\nabla L_j}{\\|\\nabla L_i\\| \\|\\nabla L_j\\|}; s < -0.01 \\text{ (conflict)}$\nIn classification tasks, Table 1 and Fig. 2a, POLCA Net consistently outperforms PCA across all tested linear classifiers.\nIn terms of image reconstruction, Table 1 and Fig. 2b, POLCA Net demonstrates superior performance across all evaluated metrics. The Normalized Root Mean Square Error (NRMSE) is significantly lower for POLCA Net, indicating better overall reconstruction accuracy. The Structural Similarity Index (SSIM) shows advantage for POLCA Net, suggesting better preservation of structural information. Most notably, the Peak Signal-to-Noise Ratio (PSNR) is substantially higher for POLCA Net, indicating superior reconstruction quality and less noise in the reconstructed images. These results highlight POLCA Net's balanced performance in both classification and reconstruction tasks."}, {"title": "4 Conclusion", "content": "This study introduced POLCA Net, an autoencoder-based approach for dimensionality reduction and feature extraction. The key findings are:\n1. POLCA Net combines multiple loss functions to achieve orthogonality, variance-based feature sorting, and dimensionality reduction in the latent space.\n2. Experiments were conducted on 16 diverse datasets, including MNIST, FashionMNIST, MedMNIST, and synthetic datasets.\n3. Performance was evaluated using reconstruction metrics (NRMSE, PSNR, SSIM) and classification metrics (Accuracy, F1-Score) for four linear classifiers."}, {"title": "5 Replicability", "content": "General experimental infrastructure and fixed parameters are shown in Table 2.\nFull source code of the POLCA Net implementation as well as all the required code (including scripts and jupyter notebooks) to reproduce the experiments and perform new ones has been provided as a pip installable single python package in the additional material."}, {"title": "A Hierarchy of Linear Independence, Orthogonality, and Functional Independence", "content": "Definition 1 (Linear Independence of Functions, Axler (2015)). A set of functions {$f_1, f_2, ..., f_n$} defined on a domain D is linearly independent if:\n$\\sum_{i=1}^n \\alpha_i f_i(x) = 0 \\forall x \\in D \\Rightarrow \\alpha_i = 0 \\forall i$\nDefinition 2 (Orthogonality of Functions, Debnath and Mikusi\u0144ski (2005)). A set of functions {$f_1, f_2,..., f_n$} is orthogonal with respect to an inner product $\\langle ., . \\rangle$ if:\n$\\langle f_i, f_j \\rangle = 0 \\forall i \\ne j$\nDefinition 3 (Functional Independence, Hirsch (1976); Lee (2012)). A set of functions {$f_1, f_2, ..., f_n$} is functionally independent if there exists no non-trivial function $\\Phi$ such that:\n$\\Phi(f_1(x), f_2(x), ..., f_n(x)) = 0 \\forall x \\in D$\nExample: Consider the functions sin(x), cos(x), and g(x) = $sin^2(x) + cos^2(x)$ on [0, 2$\\pi$]:\n* They are linearly independent.\n* sin(x) and cos(x) are orthogonal: $\\int_0^{2 \\pi} sin(x) cos(x)dx = 0$\n* They are not functionally independent because g(x) = 1 for all x\nThis example illustrates that orthogonality does not imply functional independence, and linear independence does not imply orthogonality.\nThus, functionally independent functions might not be orthogonal as they could have non-zero inner products, but still no function of them equals zero. Orthogonal functions are always linearly independent, but the reverse isn't true. Linear independence is the weakest condition and doesn't guarantee orthogonality or functional independence.\nIn applications requiring truly independent features or representations, verifying functional independence is necessary, as orthogonality or linear independence alone may not be sufficient to ensure complete independence of the functions or features.\nThis functional independence implies that each of these functionals captures a unique aspect of the function f(x) that cannot be derived from the others, making them valuable and distinct measures in function analysis. In practical applications, such as signal processing or data analysis, this independence suggests that considering all three measures can provide a more comprehensive understanding of the underlying function or data."}, {"title": "A.1 Example: Functional Independence of Area, Center of Mass, and Curve Length", "content": "As an illustrative example, lets study three independent functionals used in computer vision (Martin H. et al., 2010, although author's claim for orthogonal variant moments the right term shall be functional independent)\nConsider a function f(x) defined on the interval [a, b], where f(x) is continuous on [a, b] and differentiable on (a, b). We will analyze three functionals: the area under the curve A[f], the center of mass C[f], and the curve length L[f].\n1. Area under the curve:\n$A[f] = \\int_a^b f(x) dx$"}, {"title": null, "content": "2. Center of mass:\n$C[f] = \\frac{1}{A[f]} \\int_a^b x f(x) dx$\n3. Curve length:\n$L[f] = \\int_a^b \\sqrt{1 + (f'(x))^2} dx$\nProof of Functional Independence To prove that A[f], C[f], and L[f] are functionally independent, we need to show that there exists no non-trivial function $\\Phi$ such that:\n$\\Phi(A[f], C[f], L[f]) = 0 \\forall f$\nWe will demonstrate this by showing that each functional can be altered independently of the others using specific transformations of f(x).\nIndependence of A[f] and C[f] Consider the transformation $T_1[f](x) = f(x) + \\epsilon (x - \\frac{a+b}{2})$, where \\epsilon is a small non-zero constant.\nFor the area functional:\n$A[T_1[f]] = \\int_a^b (f(x) + \\epsilon (x - \\frac{a+b}{2})) dx$\n$= A[f] + \\epsilon \\int_a^b (x - \\frac{a+b}{2}) dx$\n$= A[f] + \\epsilon [\\frac{x^2}{2} - \\frac{a+b}{2}x]_a^b$\n$= A[f]$\nSince the integral of a linear term symmetric around (a + b)/2 cancels out, A[T\u2081[f]] = A[f].\nFor the center of mass functional:\n$C[T_1[f]] = \\frac{1}{A[T_1[f]]} \\int_a^b x (f(x) + \\epsilon (x - \\frac{a+b}{2})) dx$\n$= \\frac{1}{A[f]} \\int_a^b (x^2 - x \\frac{a+b}{2}) dx$\n$= C[f] + \\frac{\\epsilon}{A[f]} [\\frac{x^3}{3} - \\frac{a+b}{4}x^2]_a^b$\n$\\ne C[f]$ for $\\epsilon \\ne 0$\nThis shows that C[f] can be altered independently of A[f].\nIndependence of L[f] from A[f] and C[f] Consider the transformation $T_2[f](x) = f(x) + \\epsilon sin(\\frac{2 \\pi n}{b-a} x)$, where n is a large integer and \\epsilon is small.\nFor the area functional:\n$A[T_2[f]] = \\int_a^b (f(x) + \\epsilon sin(\\frac{2 \\pi n}{b-a} x)) dx$\n$= A[f] + \\epsilon \\int_a^b sin(\\frac{2 \\pi n}{b-a} x) dx$\n$= A[f]$"}, {"title": null, "content": "Since the integral of a sine function over its period is zero, A[T2[f]] = A[f].\nFor the center of mass functional:\n$C[T_2[f]] = \\frac{1}{A[f]} \\int_a^b x (f(x) + \\epsilon sin(\\frac{2 \\pi n}{b-a} x)) dx$\n$= C[f] + \\frac{\\epsilon}{A[f]} \\int_a^b x sin(\\frac{2 \\pi n}{b-a} x) dx$\n$= C[f] + O(\\frac{1}{n}) as n \\rightarrow \\infty$\nThe integral of x sin($\\frac{2 \\pi n}{b-a} x$) tends to zero as n increases, so C[f] is nearly unaffected.\nFor the curve length functional:\n$L[T_2[f]] = \\int_a^b \\sqrt{1 + (f'(x) + \\frac{2 \\pi n}{b-a} \\epsilon cos(\\frac{2 \\pi n}{b-a} x))^2} dx$\n$\\approx L[f] + \\frac{\\epsilon^2}{2} (\\frac{2 \\pi n}{b-a})^2 \\int_a^b cos^2 (\\frac{2 \\pi n}{b-a} x) dx$\n$\\approx L[f] + \\frac{\\epsilon^2}{4} (\\frac{2 \\pi n}{b-a})^2 (b - a)$\nThis shows that L[f] can be significantly altered while A[f] and C[f] remain almost unchanged.\nWe have demonstrated that:\n* A[f] can be kept constant while C[f] is altered.\n* C[f] can be kept nearly constant while L[f] is altered.\n* L[f] can be significantly altered while A[f] and C[f] remain nearly constant.\nThus, there exists no non-trivial function $\\Phi$ such that $\\Phi$(A[f], C[f], L[f]) = 0 for all f. This proves that A[f], C[f], and L[f] are functionally independent.\nLimitations and Considerations The proof assumes that f(x) is continuous on [a, b] and differentiable on (a, b). The transformations used are local; they demonstrate independence in a neighborhood of any given function f. While we've shown that each functional can be changed independently, we haven't explicitly considered all possible combinations of simultaneous changes."}, {"title": "B Functional Independence of POLCA Losses", "content": "Consider a POLCA Net based autoendoder with input x, latent representation z, and output \\hat{x}. We will analyze three loss functions: variance sorting loss $\\mathcal{L}_{com}$, variance reduction loss $\\mathcal{L}_{var}$, and reconstruction loss $\\mathcal{L}_2$.\nDefinitions Let $z_i$ be the i-th component of the latent vector z, and $\\sigma_i^2$ be its variance across a batch of inputs.\n1. Variance Sorting Loss:\n$\\mathcal{L}_{com} = \\frac{1}{d} \\sum_{i=1}^d i \\sigma_{(i)}^2$\nwhere $\\sigma_{(i)}^2$ are the sorted variances in descending order, and d is the dimension of z."}, {"title": null, "content": "2. Variance Reduction Loss:\n$\\mathcal{L}_{var} = \\sum_{i=1}^d \\sigma_i^2$\n3. Reconstruction Loss:\n$\\mathcal{L}_2 = ||x - \\hat{x}||^2$\nProof of Functional Independence To prove that $\\mathcal{L}_{com}$, $\\mathcal{L}_{var}$, and $\\mathcal{L}_2$ are functionally independent, we need to show that there exists no non-trivial function $\\Phi$ such that:\n$\\Phi(\\mathcal{L}_{com}, \\mathcal{L}_{var}, \\mathcal{L}_2) = 0 \\forall$ possible autoencoders\nWe will demonstrate this by showing that we can change each loss independently of the others. A key idea behind this proof is that in general ML classification and regression algorithms are blind with respect to features ordering and feature scaling and or normalization is usually performed as a previous step or is integrated inside the algorithm.\nIndependence of $\\mathcal{L}_{com}$ and $\\mathcal{L}_{var}$ Consider the transformation $T_1[z] = Pz$, where P is a permutation matrix.\nFor $\\mathcal{L}_{var}$:\n$\\mathcal{L}_{var}(T_1[z]) = \\sum_{i=1}^d \\sigma_i^2(Pz)$\n$= \\sum_{i=1}^d \\sigma_i^2(z)$\n$= \\mathcal{L}_{var}(z)$\nFor $\\mathcal{L}_{com}$:\n$\\mathcal{L}_{com}(T_1[z]) = \\frac{1}{d} \\sum_{i=1}^d i \\sigma_{(i)}^2(Pz)$\n$\\ne \\mathcal{L}_{com}(z)$ for non-trivial permutations\nThis transformation changes $\\mathcal{L}_{com}$ while keeping $\\mathcal{L}_{var}$ constant.\nIndependence of $\\mathcal{L}_2$ from $\\mathcal{L}_{com}$ and $\\mathcal{L}_{var}$ Consider the transformation $T_2[z] = \\alpha z$, where $\\alpha \\ne 0$ is a scaling factor.\nFor $\\mathcal{L}_{com}$ and $\\mathcal{L}_{var}$:\n$\\mathcal{L}_{com}(T_2[z]) = \\alpha^2 \\mathcal{L}_{com}(z)$\n$\\mathcal{L}_{var}(T_2[z]) = \\alpha^2 \\mathcal{L}_{var}(z)$\nFor $\\mathcal{L}_2$, assuming the decoder can compensate for the scaling:\n$\\mathcal{L}_2(T_2[z]) = ||x - \\hat{x}(T_2[z])||^2$\n$= ||x - \\hat{x}(z)||^2$\n$= \\mathcal{L}_2(z)$\nThis transformation changes $\\mathcal{L}_{com}$ and $\\mathcal{L}_{var}$ while keeping $\\mathcal{L}_2$ constant."}, {"title": null, "content": "Independence of $\\mathcal{L"}, 2, "from $\\mathcal{L}_{com}$ and $\\mathcal{L}_{var}$ Consider the transformation $T_3[x"], "mathcal{L}_{var}$": "n$\\mathcal{L"}, {"mathcal{L}_2$": "n$\\mathcal{L}_2(T_3[x]) = ||x - (\\hat{x} + \\epsilon"}]