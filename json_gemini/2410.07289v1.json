{"title": "Principal Orthogonal Latent Components Analysis (POLCA Net)", "authors": ["Jose Antonio Martin H.", "Freddy Perozo", "Manuel Lopez"], "abstract": "Representation learning is a pivotal area in the field of machine learning, focusing on the development of methods to automatically discover the representations or features needed for a given task from raw data. Unlike traditional feature engineering, which requires manual crafting of features, representation learning aims to learn features that are more useful and relevant for tasks such as classification, prediction, and clustering. We introduce Principal Orthogonal Latent Components Analysis Network (POLCA Net), an approach to mimic and extend PCA and LDA capabilities to non-linear domains. POLCA Net combines an autoencoder framework with a set of specialized loss functions to achieve effective dimensionality reduction, orthogonality, variance-based feature sorting, high-fidelity reconstructions, and additionally, when used with classification labels, a latent representation well suited for linear classifiers and low dimensional visualization of class distribution as well.", "sections": [{"title": "1 Introduction", "content": "Representation learning is a pivotal area in the field of machine learning, focusing on the development of methods to automatically discover the representations or features needed for a given task from raw data. Unlike traditional feature engineering, which requires manual crafting of features, representation learning aims to learn features that are more useful and relevant for tasks such as classification, prediction, and clustering. This approach is integral in the performance of deep learning models, where layers of representation are learned hierarchically to capture increasingly abstract features of the data (Bengio et al., 2013).\nThe importance of representation learning lies in its ability to make complex data more accessible for machine learning algorithms. By learning meaningful representations, models can improve generalization to unseen data and reduce the reliance on domain-specific knowledge, thus enabling the application of machine learning in more diverse and complex domains (LeCun et al., 2015). Techniques such as autoencoders, word embeddings, and convolutional neural networks are prime examples of how representation learning has revolutionized tasks in natural language processing, computer vision, and beyond (Goodfellow et al., 2016).\nAs the field progresses, advancements in representation learning continue to enhance the capabilities of machine learning models, driving innovation in areas such as transfer learning, where representations learned in one context are adapted for use in another, and in unsupervised learning, where representations are learned without explicit labels (Radford et al., 2021). These developments underscore the growing significance of representation learning in shaping the future of artificial intelligence."}, {"title": "2 POLCA Net", "content": "POLCA Net is essentially an autoencoder architecture containing encoder network and a decoder network as shown in Fig. 1 plus a loss function composed by a set of weighted sub losses, each defined for a particular purpose or constraint of the latent space desired characteristics. The POLCA's loss acts on the bottleneck part (latent space) generated by the encoder combined with the reconstruction loss which is obtained from the difference between the input to the encoder and the output of the decoder. In terms of dimensionality reduction, POLCA Net reduces the dimensionality of high-dimensional data while preserving its underlying structure and relationships. This compression sorts as well the latent dimensions by their variance, similar to how PCA orders its principal components. This is accomplished through the center of mass loss that encourages the concentration of information in earlier latent dimensions, and optionally allowing for effective truncation of less significant dimensions. Also, a key feature of POLCA Net is its ability to extract orthogonal features from the input data, mirroring one of PCA's most valuable properties. This orthogonality is achieved through a specific term in the loss function that minimizes the cosine similarity between different latent dimensions. Unlike PCA, which achieves a near perfect (up to numeric precision) orthogonality through linear transformations, POLCA Net enforces orthogonality in a non-linear latent space, offering a more flexible and potentially more powerful representation. In addition, POLCA Net can be used in a supervised setting in the same way as LDA, by incorporating an additional classification loss (such as cross entropy) and using the same latent space features as classification variable."}, {"title": "2.1 Multiobjetive Loss Function:", "content": "POLCA Net uses a composite loss function $L_{polca}$ that guides the learning process:\n$L_{polca} = L_{rec} + L_{class} + \\alpha L_{ort} + \\beta L_{com} + \\gamma L_{var}$,\nwhere $L_{rec}$ is the reconstruction loss (mean squared error), $L_{class}$ is an optional classification loss (such as cross entropy) or zero, $L_{ort}$ is the orthogonality enforcing loss, $L_{com}$ is the center of mass loss"}, {"title": "Orthogonality and Independence.", "content": "The loss $L_{ort}$, encourages the latent features to minimize the cosine similarity matrix S over the normalized latent representations. The implementation only requires computing the upper triangular matrix due to symmetry excluding the main diagonal as well:\n$L_{ort} = \\frac{2}{n(n - 1)} \\sum_{1<i<j<n} S_{ij}$"}, {"title": "4 Conclusion", "content": "This study introduced POLCA Net, an autoencoder-based approach for dimensionality reduction and feature extraction. The key findings are:\n1.  POLCA Net combines multiple loss functions to achieve orthogonality, variance-based feature sorting, and dimensionality reduction in the latent space.\n2.  Experiments were conducted on 16 diverse datasets, including MNIST, FashionMNIST, MedMNIST, and synthetic datasets.\n3.  Performance was evaluated using reconstruction metrics (NRMSE, PSNR, SSIM) and classification metrics (Accuracy, F1-Score) for four linear classifiers."}, {"title": "A Hierarchy of Linear Independence, Orthogonality, and Functional Independence", "content": "Definition 1 (Linear Independence of Functions, Axler (2015)). A set of functions ${f_1, f_2, ..., f_n}$ defined on a domain D is linearly independent if:\n$\\sum_{i=1}^{n} \\alpha_i f_i(x) = 0  \\forall x \\in D \\implies \\alpha_i= 0  \\forall i$\nDefinition 2 (Orthogonality of Functions, Debnath and Mikusi\u0144ski (2005)). A set of functions ${f_1, f_2,..., f_n}$ is orthogonal with respect to an inner product $\\langle , \\rangle$ if:\n$\\langle f_i, f_j\\rangle = 0 \\forall i \\neq j$\nDefinition 3 (Functional Independence, Hirsch (1976); Lee (2012)). A set of functions ${f_1, f_2, ..., f_n}$ is functionally independent if there exists no non-trivial function $\\Phi$ such that:\n$\\Phi(f_1(x), f_2(x), ..., f_n(x)) = 0  \\forall x \\in D$\nExample: Consider the functions sin(x), cos(x), and g(x) = sin\u00b2(x) + cos\u00b2(x) on [0, 2\u03c0]:\nThis example illustrates that orthogonality does not imply functional independence, and linear independence does not imply orthogonality.\nThus, functionally independent functions might not be orthogonal as they could have non-zero inner products, but still no function of them equals zero. Orthogonal functions are always linearly independent, but the reverse isn't true. Linear independence is the weakest condition and doesn't guarantee orthogonality or functional independence.\nIn applications requiring truly independent features or representations, verifying functional inde-pendence is necessary, as orthogonality or linear independence alone may not be sufficient to ensure complete independence of the functions or features.\nThis functional independence implies that each of these functionals captures a unique aspect of the function f(x) that cannot be derived from the others, making them valuable and distinct measures in function analysis. In practical applications, such as signal processing or data analysis, this independence suggests that considering all three measures can provide a more comprehensive understanding of the underlying function or data."}, {"title": "A.1 Example: Functional Independence of Area, Center of Mass, and Curve Length", "content": "As an illustrative example, lets study three independent functionals used in computer vision (Martin H. et al., 2010, although author's claim for orthogonal variant moments the right term shall be functional independent)\nConsider a function f(x) defined on the interval [a, b], where f(x) is continuous on [a, b] and differentiable on (a, b). We will analyze three functionals: the area under the curve A[f], the center of mass C[f], and the curve length L[f].\n1. Area under the curve:\n$A[f] = \\int_{a}^{b} f(x) dx$"}, {"title": "2. Center of mass:", "content": "$C[f] = \\frac{1}{A[f]} \\int_{a}^{b} x f (x) dx$"}, {"title": "3. Curve length:", "content": "$L[f] = \\int_{a}^{b} \\sqrt{1 + (f'(x))^2} dx$"}, {"title": "B Functional Independence of POLCA Losses", "content": "Consider a POLCA Net based autoendoder with input x, latent representation z, and output x\u0302. We will analyze three loss functions: variance sorting loss Lcom, variance reduction loss Lvar, and reconstruction loss L2.\nDefinitions Let zi be the i-th component of the latent vector z, and \u03c3i2 be its variance across a batch of inputs.\n1. Variance Sorting Loss:\n$L_{com} = \\frac{1}{d} \\sum_{i=1}^{d} i  \\sigma_{(i)}^2$\nwhere \u03c3(i)2 are the sorted variances in descending order, and d is the dimension of z."}, {"title": "2. Variance Reduction Loss:", "content": "$L_{var} = \\sum_{i=1}^{d} \\sigma_i^2$"}, {"title": "3. Reconstruction Loss:", "content": "$L_2 = ||x - \\hat{x}||^2$"}]}