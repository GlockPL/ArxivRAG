{"title": "ARMOR: Shielding Unlearnable Examples against Data Augmentation", "authors": ["Xueluan Gong", "Yuji Wang", "Yanjiao Chen", "Haocheng Dong", "Yiming Li", "Mengyuan Sun", "Shuaike Li", "Qian Wang", "Chen Chen"], "abstract": "Private data, when published online, may be collected by unauthorized parties to train deep neural networks (DNNs). To protect privacy, defensive noises can be added to original samples to degrade their learnability by DNNs. Recently, unlearnable examples [19] are proposed to minimize the training loss such that the model learns almost nothing. However, raw data are often pre-processed before being used for training, which may restore the private information of protected data. In this paper, we reveal the data privacy violation induced by data augmentation, a commonly used data pre-processing technique to improve model generalization capability, which is the first of its kind as far as we are concerned. We demonstrate that data augmentation can significantly raise the accuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To address this issue, we propose a defense framework, dubbed ARMOR, to protect data privacy from potential breaches of data augmentation. To overcome the difficulty of having no access to the model training process, we design a non-local module-assisted surrogate model that better captures the effect of data augmentation. In addition, we design a surrogate augmentation selection strategy that maximizes distribution alignment between augmented and non-augmented samples, to choose the optimal augmentation strategy for each class. We also use a dynamic step size adjustment algorithm to enhance the defensive noise generation process. Extensive experiments are conducted on 4 datasets and 5 data augmentation methods to verify the performance of ARMOR. Comparisons with 6 state-of-the-art defense methods have demonstrated that ARMOR can preserve the unlearnability of protected private data under data augmentation. ARMOR reduces the test accuracy of the model trained on augmented protected samples by as much as 60% more than baselines. We also show that ARMOR is robust to adversarial training. We will open-source our codes upon publication.", "sections": [{"title": "I. INTRODUCTION", "content": "THE success of deep learning models is, to a great extent, attributed to the availability of expansive data samples [31], [48], [41]. To train a well-performed DNN model, the model trainer may collect data samples from various sources. Although many public datasets can be easily downloaded online, private data samples are enticing thanks to their large quantity and diversity. For instance, an enormous number of facial images are published on social media and accessible to unauthorized parties to train well-performed face recognition DNN models. To protect personal data from being abused, worldwide regulations have been enacted, such as the European Union's General Data Protection Regulation (GDPR) [45] and California Consumer Privacy Act (CCPA) [13].\nTo protect private data samples from being used as training data for DNN models, researchers have proposed to add defensive noises to original samples. The resulting protected samples can be published as they have little learnability but maintain the perceptual quality of original samples. To achieve this objective, early works borrow techniques from adversarial example attacks, creating imperceptible adversarial noises to mislead the training process of DNN models [29], [10], [50]. Recently, the concept of unlearnable examples has been proposed [19], [26]. The main rationale is to minimize the training loss such that little can be learned from protected samples. Uncovering that adversarial training undermines unlearnability, Fu et al. [12] proposed to generate robust unlearnable examples that minimize adversarial training loss.\nIn this paper, we present a new threat to data privacy posed by data augmentation, a commonly used data pre-processing technique [7], [36]. Data augmentation is usually performed on raw data samples to resolve class imbalance issues and improve the generalization capability of the trained model. As far as we are concerned, we are the first to unveil the potential data privacy threat induced by data augmentation. We demonstrate that state-of-the-art unlearnable examples (e.g., EMIN [19]) can effectively suppress the accuracy of trained models to no more than 15%. Unfortunately, if data augmentation is applied to unlearnable examples, the accuracy of trained models will be elevated to more than 60%, severely damaging the unlearnability of protected data.\nTo address this challenge, we develop ARMOR, a defense framework that can protect data privacy against advanced data augmentation. The main challenge is that the defender has no knowledge or control over the training process conducted by the attacker. More specifically, the defender does not know the model structure and the data augmentation strategy chosen by the attacker. To overcome this difficulty, we propose to use a carefully designed surrogate model and a surrogate augmentation selection strategy for generating defensive noises that minimize the training loss. For surrogate model construction, we design a non-local module that widens the receptive field of the surrogate model to capture more benefits from data"}, {"title": "II. PRELIMINARIES", "content": ""}, {"title": "A. Data Privacy in Deep Learning", "content": "A deep neural network\u00b9 (DNN) is a function $f_\\theta$ parameterized by $\\Theta$, mapping an input x to the output y. The parameters are optimized by minimizing the aggregated prediction error (i.e., defined as the loss function L) on the training dataset $D_{train} = \\{x_i, y_i\\}_{i=1}^N$ as\n$\\min \\frac{1}{N} \\sum_{i=1}^N L(f_\\Theta(x_i), y_i).$                                                              (1)\nTraining datasets are considered as valuable assets since their quality has a considerable influence on the model performance. Training data samples are usually collected from various sources, including individual users whose private data may be sensitive [22], [1]. Potential data privacy breaches may occur in the pre-deployment and the post-deployment phases of deep learning [15], [38]. In the pre-deployment phase, attackers may collect a large pool of unauthorized data (e.g., from online social networks) to construct their training datasets [38], [9], [35], [18], [5], [19], [12]. In the post-deployment phase, attackers may infer the membership [39], [3], [42] or attributes [14], [11], [16] of training data samples.\nIn this paper, we focus on protecting data privacy in the pre-deployment phase. More specifically, the main objective is to protect private user data from being exploited by unauthorized parties to train a well-performed DNN model. To attain this goal, existing research works have proposed to add defensive noises to original data samples to reduce their learnability, i.e., the DNN model trained on the perturbed data samples will not reach satisfactory accuracy. Given an original sample x, its privacy-preserving version x' is created as\nx' = x + \\delta.                                                                                                              (2)\nwhere $\\delta$ is the defensive noise.\nThere are various choices for $\\delta$, a naive one being the Gaussian noise. Adversarial noises are often adopted as they may mislead the training process. EMAX [29], UTAP [10], CTAP [10] and NTGA [50] all followed this line. Recently, the idea of unlearnable samples emerged [19], generating the defensive noise $\\delta$ via an optimization problem\n$\\min E_{(x, y) \\in D_c} [\\min L(f_\\Theta(x + \\delta), y)],$                                                                                    (3)\n$\\theta$\nwhere $f_\\theta$ represents a hypothetical DNN model under training, $D_c$ is a clean training dataset used to train $f_\\theta$, and L is the loss function for training $f_\\theta$. The bi-level optimization problem consists of an outer and an inner minimization problems. The outer minimization problem searches for the parameters $\\Theta$ that minimize the training error of $f_\\theta$ on clean training dataset $D_c$. The inner minimization problem searches for an $L_p$-norm bounded noise $\\delta$ that minimizes the loss of $f_\\theta$ on sample $x+\\delta$ such that model $f_\\theta$ learns almost nothing from sample $x + \\delta$.\nDespite the effort to minimize the learnability of original samples, unlearnable examples are found to be vulnerable to adversarial training [12]. More specifically, adversarial examples created based on these unlearnable examples will yield non-minimal loss, allowing the model to learn useful information, thus undermining privacy protection. To tackle this issue, Fu et al. [12] proposed a robust unlearnable example generation method using min-min-max optimization as\n$\\min E_{(x, y) \\in D_c} [\\min_{\\delta} \\max_\\epsilon L(f_\\Theta(x + \\epsilon + \\delta), y)],$                                                                                    (4)\n$\\Theta$\nwhere $\\epsilon$ is the adversarial noise added to maximize the loss for adversarial training. Both $\\epsilon$ and $\\delta$ are bounded to satisfy imperceptibility.\nIn this paper, we will reveal the vulnerability of existing defense methods, especially unlearnable examples, to data augmentation. We further propose corresponding countermeasures to strengthen data privacy protection."}, {"title": "B. Data Augmentation", "content": "Data augmentation [44], [30], [43] is a technique that enriches a training dataset by generating new examples through transformations of original data samples, e.g., flipping, rotating, scaling, cropping, and changing color. The purpose of data augmentation is to help improve the performance and generalization capability of machine learning models, especially if the available training data samples are limited. By generating more diverse examples, data augmentation helps the model learn to be more robust to data variations. It is shown that data augmentation can also help defend against adversarial example attacks [34], [51] and backdoor attacks [2], [33]. In this paper, we review five state-of-the-art representative data augmentation methods, i.e., Mixup [53], feature distillation [27], PuzzleMix [21], Fast AutoAugment [25], and DeepAA [54].\nMixup. Mixup [53] constructs a weighted combination of randomly selected pairs of samples from the training dataset. Given two samples ($x_i, y_i$) and ($x_j, y_j$), an augmented sample is generated as ($\\lambda x_i + (1 - \\lambda)x_j, \\lambda y_i + (1 - \\lambda)y_j$), where $\\lambda \\in [0, 1]$ is a mixing coefficient that determines the weight of each sample in the linear combination.\nFeature distillation. Feature distillation [27] was initially designed to mitigate adversarial example attacks. It redesigns the quantization procedure of a traditional JPEG compression algorithm [46]. Depending on the location of the derived quantization in JPEG, feature distillation has the one-pass and the two-pass modes. The one-pass feature distillation inserts a quantization/de-quantization in the decompression process of the original JPEG algorithm. The two-pass feature distillation embeds a crafted quantization at the sensor side to compress the raw data samples.\nPuzzleMix. PuzzleMix [21] is an extension of Mixup, resolving the problem of generating unnatural augmented samples. PuzzleMix finds the optimal mixing mask based on the saliency information. PuzzleMix is shown to have outperformed state-of-the-art Mixup methods in terms of generalization and robustness against data corruption and adversarial perturbations.\nFastAA. AutoAugment [6] automates the search for an optimal data augmentation strategy given the original dataset using reinforcement learning. To reduce the computational complexity of AutoAugment, Fast AutoAugment (FastAA) [25] uses a more efficient search strategy based on density matching. The main idea is to learn a probability distribution over a space of candidate augmentation strategy that maximizes the performance of a given model on a validation dataset. The distribution is learned by matching the density of a learned feature space of augmented samples to that of original samples. In this way, FastAA significantly speeds up the search process compared to the original AutoAugment algorithm.\nDeepAA. Deep AutoAugment (DeepAA) [54] constructs a multi-layer data augmentation framework. In each layer, the"}, {"title": "III. UNVEILING THE EFFECTS OF DATA AUGMENTATION ON DATA PRIVACY", "content": "We reveal that data augmentation will impair the privacy protection provided by existing defensive noise generation methods. As far as we are concerned, this is the first time such a vulnerability has been exposed.\nWe first present the performance of different models trained on protected samples with or without data augmentation. As shown in Table I, we assess the protection performance of Gaussian noise and two state-of-the-art defense methods based on unlearnable examples, i.e., EMIN [19] and REM [12]. We use DeepAA [54] as the data augmentation method.\nEffect of Data Augmentation on Data Privacy\n\u2022 Without data augmentation, data privacy protection provided by unlearnable examples are effective.\n\u2022 With data augmentation, data privacy protection provided by unlearnable examples are undermined.\nAs shown in Table I, without data augmentation, EMIN and REM effectively prevent DNN models from achieving high test accuracy by learning from protected data samples. For instance, DNNs trained on unprotected clean CIFAR-10 dataset yield test accuracies of 92.66% (VGG-16) and 94.09% (ResNet-18). These accuracies drop to 25.59% (VGG-16) and 25.08% (ResNet-18) under EMIN protection. Unfortunately, when data augmentation is adopted, EMIN and REM struggle to keep the test accuracy of unauthorized models below 30%, especially for EMIN. For example, the test accuracy of the WRN_34_10 model is only 21.38% under EMIN for CIFAR- 10 dataset, but jumps to 66.14% once DeepAA is utilized.\nTo further verify our findings, we assess other state-of- the-art data augmentation strategies, including Mixup [53], Feature distillation [27], PuzzleMix [21], and FastAA [25]. The experimental results are presented in Table II. The results demonstrate that various data augmentation strategies can undermine data privacy protection. Besides DeepAA, both FastAA and PuzzMix can substantially improve model test accuracy (e.g., >60% in several cases) even if EMIN and REM are used for privacy protection. More evaluation results of the effects of data augmentation on defense methods EMAX [29], UTAP [10], \u0421\u0422\u0410\u0420 [10] and NTGA [50] can be found in Table IV and Table V.\nThrough comprehensive experiments, we have discovered that almost all existing defensive noises are susceptible to data augmentation. This finding highlights the importance of mitigating potential data privacy violations introduced by data augmentation."}, {"title": "IV. PROTECTING DATA PRIVACY UNDER DATA AUGMENTATION", "content": "In this paper, we propose ARMOR to enhance data privacy protection under the impact of data augmentation."}, {"title": "A. Threat Model", "content": "We consider two parties, i.e., the defender and the attacker.\nDefender. The role of the defender is usually assumed by a data owner who possesses a private dataset $D_c = \\{x_i, y_i\\}_{i=1}^N$. The goal of the defender is to generate a corresponding protected dataset $D_u$ to be published on social media, where $D_u = \\{(x_i + \\delta_i, y_i)\\}_{i=1}^N$. It should be ensured that a model trained on the published protected dataset $D_u$ performs poorly on the test set $D_t$, even if the model trainer utilizes advanced data augmentation strategies. The defensive noise $\\delta$ is bounded by $|\\delta||_p \\le \\epsilon$ to guarantee imperceptibility. Following existing defense works [19], we assume the defender has full access to the private dataset $D_c$. However, the defender does not have any knowledge or control over the training process of the attacker model. Additionally, the defender cannot modify the protected examples once they are published.\nAttacker. The role of the attacker is usually assumed by a model trainer who aims to train a well-performed model $f$ with the public dataset $D_u$. The attacker can employ any data augmentation strategy to improve the model performance. We also explore the effectiveness of ARMOR when the attacker adopts adversarial training to bolster model performance."}, {"title": "B. Privacy Protection Problem Formulation", "content": "To generate defensive noise $\\delta$ that can resist data augmentation, we formulate the optimization problem as\n$\\min E_{(x, y) \\sim D_c} [\\min L(f(A(x + \\delta, p), y)],                                                                                     (6)$\n$\\Theta$\nwhere $A(x + \\delta, p)$ is the function that enforces data augmentation strategy p on the protected sample $x + \\delta$. The key idea of Equation (6) is to minimize the loss of the protected sample even if data augmentation is applied. In Equation (6), $f(.)$ and $p$ are the surrogate model and the surrogate augmentation strategy that the defender uses to mimic the real model and real augmentation strategy adopted by the attacker. Therefore, the choice of $f(.)$ and $p$ are essential to determine the privacy protection performance. In addition, to solve the min-min problem to attain the optimal defensive noise $\\delta$ is non-trivia. To address these challenges, we equip ARMOR with three main building blocks, as depicted in Figure 1.\n\u2022 Surrogate model construction. With no information on the structure of the attacker model, the defender may have to initialize a surrogate model. Our empirical study"}, {"title": "C. Surrogate Model Construction", "content": "Ideally, the surrogate model should have the same structure as the attacker model. However, we assume that the defender has no access to the attacker model. Previous works [19], [12] have revealed that unlearnable examples generated by one specific model type can still show good protection performance under other trained model types. In this case, we may opt to choose a commonly used model structure, e.g., ResNet-18, for constructing the surrogate model.\nOur extensive empirical study reveals that ARMOR provides better protection when the surrogate model benefits more from data augmentation. Based on this, we improve the fundamental surrogate model structure to amplify the impact of data augmentation.\nData augmentation aims to enhance model generalization capability by reducing the model's reliance on local features, which helps mitigate overfitting. By increasing data diversity, data augmentation encourages the model to enlarge the receptive field to a broader sample region. Inspired by this principle, after initializing the surrogate model with a commonly-used DNN model, we further incorporate a non-local module [47] that captures a global receptive field of the sample.\nThe non-local module transforms an input a into a same-dimensional output b as\n$b_i = \\frac{1}{C(a)} \\sum_\\forall j f(a_i, a_j) g(a_j),$                                                                                                                     (7)\nwhere $b_i$ is the i-th feature of output b, $a_i$ is the i-th feature of input a, $f(,)$ is a pairwise function whose output depends on the relationship between $a_i$ and $a_j$, and $g(\\cdot)$ computes the j-th feature of the representation of input a. In addition, $C(a)$ refers to a factor used for normalization.\nThe key to a non-local module is the design of $g$ and $f$. For $g$, we choose a linear transformation as $G(a_j) = W_G a_j$, where $W_G$ is a learned weight matrix obtained in the training process. For F, we develop a Gaussian-embedded structure as\n$f(a_i, a_j) = \\Theta(a_i)^T \\cdot \\Phi(a_j),$                                                                                                                        (8)\nwhere $\\Theta(.)$ and $\\Phi(.)$ are linear transformation functions, and $x^T$ is the transportation of matrix x."}, {"title": "D. Surrogate Augmentation Strategy Selection", "content": "Data augmentation enriches a dataset by synthesizing additional data points that follow the same underlying distribution [17]. Thus, an effective data augmentation should maintain the underlying data distribution, ensuring that the distribution of augmented samples aligns with that of the original samples [4]. To achieve this goal, DeepAA adopts gradient alignment, i.e., aligning the gradients computed on the augmented sample batch with those computed based on the original sample batch [54]. As the attacker only gets the protected version of private samples, we select the augmentation strategy that maximizes the cosine similarity between the average gradients of the protected data and those of the augmented protected data. Mathematically, we search for the optimal augmentation strategy p as\n$p = arg\\ max_{p} CS(\\nabla x_1, \\nabla A(x_2, p))$                                                                                                       (9)\n$\\nabla x_1 \\cdot \\nabla A(x_2, p)$\n$\\qquad = arg\\ max_p \\frac{\n}{\n|\\nabla x_1| \\cdot |\\nabla A(x_2, p)|}$\nwhere CS(,) calculates the cosine similarity, ||. || represents the $L_2$ norm, and $x_1$ and $x_2$ are two batches of images from the current perturbed data. To improve efficiency, we adopt the class-wise augmentation strategy, i.e., we select a specific"}, {"title": "E. Defensive Noise Generation", "content": "The defensive noise is generated to make the attacker model have a small loss on the protected sample. Thus, the attacker model can hardly learn any constructive information from the image. We use cross-entropy as the loss function and the first-order optimization method PGD [29] to update the noise as\n$\\delta_{t+1} = \\delta_t - \\alpha * sign(\\nabla_{\\delta} L(f(A(x + \\delta_t, p), y))),$                                                                                                               (11)\nwhere t is the current iteration, and $\\alpha$ is the learning rate. The learning rate $\\alpha$ determines the length of each step taken in the negative direction of the gradient during the gradient descent iteration process. If the learning rate is too small, the training process may be too slow to converge. If the learning rate is too large, the training loss may fluctuate and fail to decrease monotonically. Therefore, it is essential to dynamically adjust the learning rate according to the status of the current iteration. Traditionally, PGD uses a constant learning rate. Instead, we adjust the learning rate to be inversely proportional to the input gradient norm. Specifically, samples with larger gradient norms are given lower learning rates [20], and vice versa.\n$\\eta_{i, t} = \\beta \\eta_{i, t-1} + (1 - \\beta) ||\\nabla x_{i, t-1} L(x_{i, t-1}, y_i)||^2,$\n$\\alpha_t = \\frac{c}{\\gamma + \\sqrt{\\eta_{i, t}}},$                                                                                                                      (12)\nwhere $x_{i,t}$ is i-th sample at the t-th iteration, $\\beta$ is the momentum, $\\eta_{i,t}$ is the moving average of the gradient norm, and $\\gamma$, c are hyperparameters to prevent $\\alpha_t$ from being too large.\nThe defensive noise generation process is iterated until the training process of the surrogate model converges. Note that our noise generation algorithm can generate both sample-wise noise and class-wise noise. Sample-wise noise can be directly optimized via Equation (11). For class-wise noise, we compute and average the generated defensive noise of each sample in a given class.\nThe defensive noise generation process is summarized in Algorithm 2."}, {"title": "V. EVALUATION SETUP", "content": ""}, {"title": "A. Model and Datasets", "content": "We conduct experiments on four widely-used datasets, i.e., CIFAR-10 [23], CIFAR-100 [23], Mini-ImageNet [8], and VGG-Face [32]. We utilize ResNet-18 to train the surrogate model on CIFAR-10, CIFAR-100, Mini-ImageNet, and VGG-Face by default.\nCIFAR-10. CIFAR-10 [23] consists of 60,000 color images, each of size 32 \u00d7 32 pixels, divided into 10 different classes. Each class contains 6,000 images. The images in CIFAR-10 are quite diverse and can be challenging to classify accurately due to variations in lighting, scale, orientation, and the presence of overlapping objects. We randomly select 50,000 samples from the dataset to form the training set, while the remaining 10,000 samples are reserved for the test set. We train the model on the training set for 100 epochs. The learning rate is 0.1, the batch size is 128, the weight_decay is 0.0005, and the momentum stochastic gradient descent is 0.9. A CosineAnnealingLR scheduler is used in the training process, with a T_max value of 100 and an eta_min value of 0.\nCIFAR-100. CIFAR-100 [23] is an extension of the CIFAR- 10 dataset. It consists of 60,000 color images, each of size 32 \u00d7 32 pixels, but unlike CIFAR-10, it contains 100 different classes. Each class in CIFAR-100 represents a fine-grained category, and there are 600 images per class. We divide the dataset into a training set and a test set, with 50,000 images for training and 10,000 images for testing. We train the model for 150 epochs. We set the learning rate as 0.1, the batch size as 128, and the momentum stochastic gradient descent as 0.9. A CosineAnnealingLR scheduler is used in the training process, with a T_max value of 150 and an eta_min value of 0.\nMini-ImageNet. Mini-ImageNet, a widely used subset of ImageNet [8], holds significant popularity within the research community [49], [28]. Mini-ImageNet is carefully selected to"}, {"title": "B. Evaluation Metrics", "content": "In line with prior research [12], [19], we employ test accuracy as a metric to evaluate the privacy-preserving capability of the noise. A lower test accuracy indicates that the model has acquired minimal knowledge from the training data, thereby suggesting a robust privacy protection ability of the noise.\nIn the experiments, we use sample-wise noise by default unless otherwise specified. We assume the commercial model trainer primarily uses DeepAA [54], a widely-used advanced automatic data augmentation strategy. The search space of the optimal augmentation operations is shown in Table III. We also explore the effectiveness of ARMOR when the model trainer adopts other state-of-the-art data augmentation strategies, such as Mixup [53], Feature distillation [27], PuzzleMix [21], and FastAA [25] in the experiments."}, {"title": "C. Experiential Settings of Various Defensive Noise Generation Methods", "content": "We compare ARMOR with several state-of-the-art data pro- tection methods, including Gaussian noise, EMAX [29], \u0422\u0410\u0420 [10], NTGA [50], EMIN [19], and REM [12].\nGaussian noise. We randomly generate noise independently for each training example or each class, sampling from the interval [-$\\epsilon$, $\\epsilon$].\nEMAX. EMAX [29] is generated based on the gradients of a pre-trained model. We generate the noise for EMAX using a PGD-20 attack on a pre-trained ResNet-18 model applied to the training dataset, and the step size is set to 2.\nEMIN. EMIN [19] is generated according to Equation (3). In every epoch, we train the surrogate model for a few epochs to solve the outer minimization problem, and PGD is employed to solve the inner minimization problem. The stop condition error rate is $\\chi$ = 0.1 for sample-wise noise and $\\chi$ = 0.01 for class-wise noise.\nTAP. TAP [10] is generated by performing a targeted adversarial attack on the model trained with clean data. In this attack, we follow the hyperparameter settings as specified in the original work.\nNTGA. NTGA [50] is an efficient method enabling clean- label, black-box generalization attacks against Deep Neural Networks. This work is based on the development of Neural Tangent Kernels (NTKs). On each dataset, we randomly split 10% of examples from the training dataset as a validation set to assist in the generation of noise.\nREM. REM [12] follows Equation (4) to generate the noise. We set the adversarial training perturbation radius as 2, as it is commonly used in the experiments of the original work. All the other hyperparameter settings follow the specifications of the original work.\nARMOR. In surrogate model construction, the non-local module is respectively added to the first two residual blocks of ResNet-18. For surrogate augmentation strategy selection, we choose ResNet-18 as our auxiliary model structure and pre- train the auxiliary model from scratch on the entire training dataset for 10 epochs. During each iteration, we train the aux- iliary model with 10 batches of perturbed data, and then use it to update our surrogate augmentation strategy. In defensive noise generation, we follow the same process as EMIN. We train the surrogate model on 10 batches, and the PGD attack is parameterized with 10 rounds, each with a step size of 0.8. The stop condition is also the same as EMIN.\nNote that for baselines and ARMOR, the radius of defensive perturbation $\\epsilon$ is set to 8/255. Based on prior studies in adversarial research, the defensive noise under this constraint is imperceptible to human observers.\nAll experiments are implemented in Python and run on a 14-core Intel(R) Xeon(R) Gold 5117 CPU @2.00GHz and NVIDIA GeForce RTX 3080 Ti GPU machine running Ubuntu 18.04 system."}, {"title": "VI. EVALUATION RESULTS", "content": ""}, {"title": "A. Comparison with Baselines", "content": "We compare ARMOR with several state-of-the-art data pro- tection methods, including EMAX [29], TAP [10], NTGA [50], EMIN [19], and REM [12]. We implement these base- lines according to their open-source codes. We also use Gaussian noise as a baseline.\nWe compare ARMOR with the baselines in both non-data- augmentation and data-augmentation scenarios. In the two scenarios, there are two kinds of noises, i.e., sample-wise noise and class-wise noise. Most of these methods generate sample- wise noise. Only EMAX [29] and EMIN [19] also include class-wise noise. Thus, we only compare ARMOR with EMAX [29] and EMIN [19] for the performance of class-wise noise. In the experiments, we use different model structures to test the effectiveness and transferability of the noise generated by ARMOR and baselines.\nWe first compare ARMOR with baselines in terms of sample- wise noise. The comparison results are shown in Table IV and Table V. We can see that models trained on unlearnable"}, {"title": "B. Ablation Study", "content": "In this section, we conduct an ablation study to examine the necessity of the base augmentation-resistant noise generation framework, the non-local module, and the dynamic step size adjustment algorithm. The results are shown in Table VIII and Table IX. The column of \"Unprotected\" represents the model test accuracy on unprotected data. The column of \"Base Noise\" represents the test accuracy on protected data with only base noise of ARMOR without non-local module and dynamic step size adjustment. The \u201cBase+Non-local\u201d method uses the non-local module. The \u201cBase+Non-local+Adaptive_Stepsize\u201d method is the complete data protection method of ARMOR. In this section, we use the sample-wise noise.\nIn both non-data-augmentation and data-augmentation sce- narios, comparing \"Unprotected\" and \"Base Noise\", we can observe that our base noise generation framework can signif- icantly decrease the model test accuracy. For example, when using the VGG-16 model, the clean test accuracy is 93.86% for CIFAR-10 with data augmentation but reaches as low as 46.76% using the \"Base Noise\" protection method. The reduction is more than 47.10%.\nCompared with \"Base Noise\", the \"Base+Non-local\" method further decreases the model test accuracy, which shows the effectiveness of the non-local module. The success of the non-local module is that it encourages the surrogate model to also focus on information from other areas and avoid over- emphasizing localized features.\nIt is shown that the dynamic step size adjustment algorithm can also further decrease the model test accuracy in almost all cases for all datasets. For example, when using the VGG-16 model, the \"Base+Non-local+Adaptive_Stepsize\" protection method can further decrease test accuracy from 40.89% to 29.59% under the data augmentation scenario. The reduction is as high as 11.3%."}, {"title": "C. Performance under Different Data Augmentation Methods", "content": "We explore the effectiveness of ARMOR and baselines when the model trainer adopts other state-of-the-art data augmenta- tion strategies, such as Mixup [53], Feature distillation [27], PuzzleMix [21], and FastAA [25]. Given that CTAP surpasses UTAP in data protection, we exclusively showcase the CTAP results here. The results are shown in Table X.\nWe can see that various data augmentation strategies can make the protected data learnable again, especially for TAP and NTGA. While EMIN and REM occasionally demonstrate resilience to data augmentation (i.e., reduce the test accuracy to less than 30%), strategies like FastAA and PuzzMix can successfully improve the model test accuracy to larger than 40% even 60% for these methods. In comparison, ARMOR can successfully decrease the model test accuracy to less than 30% (in most cases less than 20%) for all the list advanced data augmentation methods for all datasets."}, {"title": "D. Impact of Surrogate Model Structure", "content": "By default, the surrogate model for the four datasets em- ploys the ResNet-18 architecture. We explore whether ARMOR is also effective when using other surrogate model structures, such as VGG-16, ResNet-50, DenseNet-121, EfficientNet, and ResNext-50. Note that we set the commercial unauthorized model structure as ResNet-18.\nThe results are shown in Table XI and Table XII. It is shown that ARMOR is robust to the structures of the surrogate model. ARMOR can effectively reduce the test accuracy of the unauthorized commercial model regardless of the surrogate model structure."}, {"title": "E. Impact of the $\\epsilon$ Value", "content": "In ARMOR, we use the first-order optimization method PGD [29"}]}