{"title": "What Do You Want? User-centric Prompt Generation for Text-to-image Synthesis via Multi-turn Guidance", "authors": ["Yilun Liu", "Minggui He", "Feiyu Yao", "Yuhe Ji", "Shimin Tao", "Jingzhou Du", "Duan Li", "Jian Gao", "Li Zhang", "Hao Yang", "Boxing Chen", "Osamu Yoshie"], "abstract": "The emergence of text-to-image synthesis (TIS) models has significantly influenced digital image creation by producing high-quality visuals from written descriptions. Yet these models heavily rely on the quality and specificity of textual prompts, posing a challenge for novice users who may not be familiar with TIS-model-preferred prompt writing. Existing solutions relieve this via automatic model-preferred prompt generation from user queries. However, this single-turn manner suffers from limited user-centricity in terms of result interpretability and user interactivity. To address these issues, we propose DialPrompt, a multi-turn dialogue-based TIS prompt generation model that emphasises user-centricity. DialPrompt is designed to follow a multi-turn guidance workflow, where in each round of dialogue the model queries user with their preferences on possible optimization dimensions before generating the final TIS prompt. To achieve this, we mined 15 essential dimensions for high-quality prompts from advanced users and curated a multi-turn dataset. Through training on this dataset, DialPrompt can improve interpretability by allowing users to understand the correlation between specific phrases and image attributes. Additionally, it enables greater user control and engagement in the prompt generation process, leading to more personalized and visually satisfying outputs. Experiments indicate that DialPrompt achieves a competitive result in the quality of synthesized images, outperforming existing prompt engineering approaches by 5.7%. Furthermore, in our user evaluation, DialPrompt outperforms existing approaches by 46.5% in user-centricity score and is rated 7.9/10 by 19 human reviewers.", "sections": [{"title": "Introduction", "content": "The advent of text-to-image synthesis (TIS) models like Stable Diffusion (SD) (Rombach et al. 2022) has revolutionized the creation of digital images, enabling the generation of high-fidelity visuals from textual descriptions. However, as highlighted by recent studies (Ko et al. 2023; Liu, Qiao, and Chilton 2022), these models rely heavily on the quality of textual prompts provided by users. The specificity and relevance of these prompts may throw a significant impact on the fidelity and aesthetics of the generated images. Sometimes even adding some magical phrases in the prompts are key to a highly desirable image, such as \"soft\", \"by ghibli studio\" and \"arcane\u201c shown in Fig. 1(a).\nThus, crafting the perfect model-preferred prompt for TIS models such as SD can be a challenging and nontrivial task for novice users who are not familiar with relevant keywords and prompt writing. While there has been research on manual principles of designing prompts to improve image quality (Liu and Chilton 2022; Pavlichenko and Ustalov 2023), an emerging trend is to assist novice users with automatic creation of model-preferred prompts from user-inputted descriptions (Cao et al. 2023; Rosenman, Lal, and Howard 2024; Hei et al. 2024). These approaches typically leverage Large Language Models (LLMs) to interpret user inputs and transform them into prompts that are more in line with the TIS model's preferences, thereby enhancing the aesthetic quality of the generated images.\nHowever, we found existing single-turn-based approaches have several limitations in terms of user-centricity:\nFirstly, interpretability remains a challenge. Despite their ability to generate complex prompts, novice users often struggle to understand the significance of specific phrases within a prompt and how they correlate with the attributes of the generated image. For instance, as shown in Fig. 1(a), after obtaining the complex prompt with a single-turn query, users may still be confused about the effectiveness of the added keywords, such as \"rule of thirds\", which actually controls the photography rule, and \u201carcane\u201d, which means adding elements from a popular television series. Furthermore, existing studies highlighted the challenge that users could face understanding barriers in why the model did not produce expected outputs, which hindered users' trust with models (Zamfirescu-Pereira et al. 2023; Weisz et al. 2023).\nSecondly, the existing methods suffer from a lack of interactivity. Single-turn manners do not engage users in the prompt generation process, leading to outputs that may not align with the user's visual preferences. For example, in Fig. 1(a), the user may desire a realistically styled image, but were provided with a prompt of a comic-style image. This is also observed in the study of Strobelt et al. (2022), where they found that a prompt engineering tool should provide the user with the human-in-the-loop ability with rich feedback and user controlability to iteratively improve their prompt writing.\nTo address these shortcomings and enhance the user-centricity, we introduce DialPrompt, a dialogue-based TIS prompt generation model. DialPrompt seeks to improve upon the areas of interpretability and interactivity by conducting multiple rounds of queries to the user and gathering ample user preferences before generating the final prompt. To ensure user-centric experience, we studied 70k TIS prompts written by advanced users and mined 15 essential dimensions for crafting high-quality TIS prompts. Based on this finding, we curated a dataset containing 500+ multi-turn dialogues and trained DialPrompt. As shown in Fig. 1(b), our multi-turn dialogue flow is designed to provide step-by-step guidance on possible directions of prompt optimization within the 15 dimensions, such as content, structure, art style, and atmosphere, thereby ensuring a better interpretability of the generated final prompt. Also, DialPrompt allows users to actively influence the outcome based on their specific visual preferences, thereby granting them greater control over the prompt generation process. Our contributions are summarized as follows:\n\u2022 We identified 15 essential dimensions for high-quality TIS prompts from advanced users, which can guide prompt engineering for TIS and lead to better visual effects of images, as indicated by DialPrompt's competitive generated image quality, outperforming existing TIS prompt generation models and general-purpose LLMs.\n\u2022 We proposed and validated a novel user-centric paradigm for TIS prompt generation that significantly enhances user experiences (with the ratings improved by 46.5%) by allowing for more interpratable and personalized image creation processes.\n\u2022 We open-sourced a high-quality dataset containing over 500 multi-turn dialogues for creating user-desired TIS prompts, facilitating future user-centric research."}, {"title": "Related Work", "content": "Prompt Engineering in TIS\nDespite various architectures of TIS models proposed by researchers, such as autoregressive models (Ramesh et al. 2021), adversarial networks (Sauer et al. 2023) and diffusion models (Rombach et al. 2022), due to the relatively limited capacity of text encoders (such as the CLIP text encoder in SD (Radford et al. 2021a)), they are still sensitive to quality of input prompts. The aim of prompt engineering in TIS is to organize prompts that achieve appealing visual effects of generated images. Since a TIS model was trained with a specific style of prompts, the philosophy of writing model-preferred prompts can be manually summarized, either by providing templates (Pavlichenko and Ustalov 2023) or magical keywords (Oppenlaender 2023; Liu and Chilton 2022). However, it still requires significant efforts for inexperienced users to choose suitable templates and master the keywords. To ease user's burden, by learning from vast exemplary prompts, various automatic TIS prompt generation models are proposed. Despite different training paradigms, they can be categorized into two classes in term of user experiences. The first is prefix-based, where user inputs a short prefix of their desired prompt and the model completes the prompt (Rosenman, Lal, and Howard 2024; Datta et al. 2023; Hao et al. 2024). The second is instruct-based, where user inputs an instruction conveying their core ideas of creation and the model responds with a optimized prompt (Ma\u00f1as et al. 2024; Cao et al. 2023; Hei et al. 2024).\nOur work differs from existing approaches mainly in the user-machine interaction logic. Through a multi-turn dialogue, even novice users can be guided through in the optimization of prompt and fully express their preferences.\nUser-centric AI\nThe aim of user-centric AI is to build explainable AI systems that users can understand, trust, and effectively manage (Wang et al. 2019). Various designing philosophies are proposed to achieve towards user-centric AI, including visual designing such as user interfaces (Kim et al. 2023; Feng et al. 2023), and procedure designing such as dialogue systems (Cui et al. 2023; Dong et al. 2024). Among them, the technique of reverse question answering (QA) is of particular interest (Yin et al. 2019; Yao et al. 2022). In stead of answering user's questions, reverse QA systems ask user a series of questions in order to collecting preferences, thereby making the AI decision-making process more explainable and customized. Our work can be seen as a pioneering attempt to apply reverse QA into the field of TIS prompt generation to improve user-centricity."}, {"title": "Methodology", "content": "Advanced User Observation\nThe objective of DialPrompt is to enhance the interpretability and interactivity of TIS prompt generation by engaging users in a guided, step-by-step dialogue to capture their preferences. A critical prerequisite of this process involves identifying the key dimensions that define a high-quality TIS prompt. We achieved this goal by mining wisdom from advanced players of TIS models. Our initial dataset was sourced from lexica.art\u00b9, a widely used platform for discovering SD images and prompts created and shared by experienced users. This platform can provide a comprehensive view of current best practices in TIS prompt engineering. We began with a publicly available dataset from Hugging Face\u00b2, which includes 70k advanced SD prompts collected from lexica.art, along with corresponding user instructions generated by LLMs. To enhance the dataset's utility, we performed semantic clustering to remove redundant entries, ultimately refining it to a representative subset of around 5k pairs of user instructions and TIS prompts.\nWe then actively work with a group of language experts, which are from the language service center of a top-tier corporation, and conducted a manual study on specific elements in the 5k TIS prompts. These prompts were evenly assigned to each language expert, who was asked to review the prompts and summarize key dimensions appeared in the prompts. After discussion with experts, we aggregated them into 4 major categories and 15 specific dimensions that are essential for crafting high-quality TIS prompts. Details of the extracted categories and dimensions are listed below:\n\u2022 Artistic Elements and Techniques: This category encompasses the core components and methods of creating art, including Style (the visual appearance and artistic influences), Art (the various forms and media used), Detail (intricate aspects that enhance realism), and Composition (the arrangement of elements for visual balance).\n\u2022 Creative Expression: This kind is focused on how artists convey ideas and emotions, including Creativity (innovation and uniqueness in art), Theme (the central subject guiding the narrative), and Mood (the emotional tone set by the artwork).\n\u2022 Visual Impact: This group covers factors that influence the viewer's perception, such as Lighting (use of light to affect atmosphere), Focus (primary points of interest), Realism (accuracy and lifelikeness), and Color (use of hues for emotional expression).\n\u2022 Context and Quality: Background and quality of the artwork, including Setting (temporal and spatial context), Resolution (clarity and detail level), Elements (basic visual components like shapes and textures), and the Artist (whose style and skill shape the work)."}, {"title": "Algorithm 1: GPT-40's Workflow of Dialogue Construction from Instruction-Prompt Pairs", "content": "1: Input: User Instruction Set $P_n = \\{P_{n1}, P_{n2},..., P_{nn}\\}$, Advanced Prompt Set $P_a = \\{P_{a1}, P_{a2},..., P_{aN} \\}$, Optimization Dimension Set $K = \\{k_1,k_2,...,k_m\\}.\n2: for each $(p_{ni}, P_{az}) \\in (P_n, P_a)$ do\n3: Step 1: Compare dimension-specific differences\n4: $A_i = diff(p_{ai}, P_{ni})$.\n5: Step 2: Identify optimized dimensions\n6: $K_i = \\{k \\in K | \\Delta_{i,k} > \\epsilon_k\\}$.\n7: for each $k \\in K_i$ do\n8: Step 3: Compose a query $Q_k$ to user for dimension $k$ with optimization options.\n9: end for\n10: Step 4: Convert $(p_{ni}, P_{ai})$ into dialogue format using composed queries $\\{Q_k\\}, k \\in K$.\n11: end for\n12: Output: Optimized dialogue format prompts in the form:\n13: $\\{(User: d_{n1}, System: d_{a1}), . . ., (User: d_{nn}, System: d_{an})\\}$.\nThese dimensions represent the key aspects that a high-quality TIS prompt should effectively address, thereby can guide through our construction of training dataset of DialPrompt. To mine a high-quality subset of TIS prompts, we established a filtering policy whereby any prompt that demonstrates enhancements in at least 5 of these dimensions is preserved. Language experts were then asked to manually label the dimensions in the 5k prompts and filter the unqualified ones, leading to a final selection of 596 high-quality advanced TIS prompt along with user instructions. These 596 high-quality data entries will serve as base for the generation of multi-turn dialogues for TIS prompt generation."}, {"title": "Construction of MTGPD", "content": "Based on the curated 596 high-quality pairs, we propose a multi-turn guidance prompt dataset (MTGPD). Each sample in the dataset is a representative dialogue between user and AI assistant, where the the AI assistant proactively asking users step-by-step questions to fulfill the initial user request and construct an final TIS prompt optimized in the 15 key dimensions as discussed above.\nAs shown in Fig. 3, the construction of MTGPD is comprised of two primary components: Dialogue Format Conversion and Human Calibration. These components work synergistically to ensure that each dialogue in MTGPD is of the highest quality, both in terms of structure and content.\nDialogue Format Conversion. The Dialogue Format Conversion process is designed to transform the 596 high-quality pairs of user instruction and advanced TIS prompt into a dialogue format. We use the assistance of GPT-40 (Achiam et al. 2023) to automation this dialogue generation task, with the workflow formally outlined in Algorithm 1. The original prompt used for GPT-40 is in Appendix A.\nAs shown in Algorithm 1, we utilize the previously extracted 15 key optimization dimensions as the foundation for the dialogue construction process. For each pair of user instruction and advanced TIS prompt, the user instruction is input as the start of a conversation. In each round, the user is presented with options corresponding to a specific optimization dimension within those in the corresponding advanced TIS prompt. The user selects their desired options, gradually refining the TIS prompts. After all dimensions existed in the advanced TIS prompts are discussed, the user terminates the conversation by \"Please summarize the prompt for me\" and the assistant summarizes the final TIS prompt.\nHuman Calibration. To ensure the generated dialogue data meets high-quality standards, we implemented a Human Calibration process, which is critical for quality control. This process consists of three key steps:\n(1) Format Control: To ensure that the generated dialogue data adheres strictly to a one-query-one-answer structure, avoiding instances where one party speaks multiple times consecutively, all generated dialogues undergo a rigorous format check. If any instances of consecutive speaking are detected, the dialogues are either corrected or excluded.\n(2) Relevance Control: To guarantee that the generated dialogue content is closely aligned with the topic, a semantic analysis is conducted on the generated dialogues to filter out content that does not contribute to the improvement of prompt quality, such as mutual compliments or expressions of thanks. Only content directly related to the optimization of the TIS prompt is retained.\n(3) Summary Control: To assure the completion, each constructed dialogue should include a final optimized prompt, marking the end of the conversation. If in the final round of dialogue, a summary of TIS prompt is not generated, the dialogue will be manually inspected and corrected.\nThese processes ensure that the dataset is both structurally consistent and content-rich, thereby optimizing the performance and reliability of the model during training."}, {"title": "Multi-Turn Supervised Fine-Tuning of DialPrompt", "content": "For the training of DialPrompt, we developed a fine-tuning process specifically designed for our curated multi-turn dialogue dataset. This dataset provides rich conversational examples that allows the model to learn how to offer step-by-step guidance to users in optimizing TIS prompts across multiple key optimization dimensions. The fine-tuning process incorporates a multi-turn loss function (Zheng et al. 2024b), as illustrated in Figure 3. During the training process, for a given dialogue sample from the MTGPD dataset, we apply a masking strategy to the user input. Dialprompt is then tasked with predicting only the assistant's responses. In the final loss computation, the total loss is calculated as the average of the cross-entropy losses for the predicted words in each assistant response throughout the conversation. This training strategy allows an efficient learning of assistant behaviors from the training sample by avoiding overly segmentation of multi-turn dialogues."}, {"title": "Experiments", "content": "Experimental Setting\nImplementation Details. The experiments are conducted on 8 NVIDIA A100 GPUs. In our implementation of DialPrompt, the MTGPD dataset is randomly split into a training set and a test set by a ratio of 9:1. DialPrompt is then trained on the training set for 10 epochs, with a learning rate of 1 \u00d7 10-4, and a batch size of 16. The model is initialized from LLaMA3-8B-Instruct (Dubey et al. 2024). Stable Diffusion 3 Medium (Esser et al. 2024) is utilized as the TIS model in the main experiments.\nUser Preference Simulation. Given the multi-turn nature of DialPrompt, the generation of final TIS prompts require the other end of the dialogue, which is the participation of users. In mainstream multi-turn evaluation, the behavior of user end is fixed and irrelevant to AI responses, mostly asking pre-designed follow-up questions (Zheng et al. 2024a). However, in the evaluation of DialPrompt, user needs to express their preferences on the suggestions and choices proposed by DialPrompt in each round of dialogue, which is unpredictable. Thereby, in addition to human evaluation, we also utilize GPT-40-mini (Achiam et al. 2023) as an agent to enable an efficient user preference simulation. The prompt used in simulation for GPT-40-mini is listed in Appendix B.\nTo ensure the convergence of dialogues and avoid possible biases, the behavior of the agent is strictly prompted as following: (1) start the dialogue by querying with a user input in the test set; (2) respond with a random preference during the dialogue and (3) end the dialogue by asking for summarizing the prompt after a maximum number N of turns (We use N=5, approaching the average dialogue length in Table 1).\nEvaluation Dataset. In addition to the split test set from our MTGPD (which contains 60 samples and is denoted as MTGPD60), which is sourced from Lexica.art, another open-source TIS test set is also involved as an out-of-domain evaluation of DialPrompt. The out-of-domain test set, denoted as PP200, contains 200 prompts sampled from PartiPrompts (Yu et al. 2022), which is designed to represent a wide range of topics, including different domains and features of language. For MTGPD60, we use the user instructions as the original user input to conduct TIS prompt generation. For PP200, we keep the prompts short by sampling only from the categories of Basic and Simple Detail, and directly use the short prompts as the user input prefix of prompt generation.\nImage Quality Evaluation\nAfter obtaining the generated TIS prompts using DialPrompt or other methods, we input them into Stable Diffusion-v3 (Esser et al. 2024) to acquire the synthetic images. We then evaluate the quality of these images, which can reflect the overall quality of generated prompts.\nEvaluation Metrics. In the evaluation, We consider two dimensions of an image: fidelity and aesthetic. The dimension of fidelity measures the degree to which the synthetic image reflects what the input prompt describes. As naive prompts often lead to deviated output images, an advanced prompt should steadily produce relevant images. Thus, we use CLIP Score (Radford et al. 2021b) as the metric of fidelity, which measures the semantic consistency between the textual prompt and the produced image. For aesthetic, we use Aesthetic Score (Schuhmann et al. 2022), which is a CLIP-based model trained on human aesthetic feedbacks to predict aesthetic score of images.\nBaselines. We consider two groups of baselines: (1) TIS Prompt Models. We compare DialPrompt with three prefix-based approaches: PromptGen\u00b3, PromptExpansion (Datta et al. 2023) and MagicPrompt (Cao et al. 2023), plus BeautifulPrompt (Cao et al. 2023), which is a recent instruction-based model built upon LLMs. (2) General-purpose LLMs. Since most general-purpose proprietary LLMs nowadays are powerful in performing the task of prompt engineering (Liu et al. 2024a) and possess multimedia capabilities, we also include GPT-40 (Achiam et al. 2023), GPT-3.5-turbo (Ouyang et al. 2022), GPT-4-turbo (Achiam et al. 2023) and Claude-3.5-Sonnet (Anthropic 2024) in the evaluation, by directly instructing the LLMs to output an optimized prompt for Stable Diffusion."}, {"title": "Result.", "content": "As shown in Table 2, DialPrompt not only significantly improves the image quality of original user inputs, but also outperforms that of existing TIS prompt models in all test cases. DialPrompt's advantage indicates its competitive TIS prompt optimization ability, which can lead to stable and visually-appealing images. For the comparison with general-purpose LLMs, despite curated with far less data and training pipelines, DialPrompt still outperforms existing LLMs in Aesthetic Score and achieves a comparable performance in CLIP Score. The enhancement in Aesthetic Score can be attributed to the comprehensive prompt optimization dimensions in the training data of DialPrompt, while the advantage in CLIP Score is a result of a more profound comprehension of user requests through multi-turn dialogues. Visualized cases are included in Appendix D."}, {"title": "User Experience Evaluation", "content": "In this section, we conduct a quantitative analysis on the user-centric experience provided by DialPrompt. To enable an efficient evaluation, both automatic (by GPT-4) and manual evaluation approach are utilized.\nEvaluation Protocol. We define the following key dimensions of user experience to evaluate the extent of user-centricity an Al assistant demonstrates during interactions with users:\n\u2022 Clarity: Language and layout clarity of Al's responses that allows users easily understanding generated content.\n\u2022 Richness: Richness of the AI recommended aesthetic elements in the dialogue where user can choose.\n\u2022 Helpfulness: Degree to which AI can understand user's requirement and gives helpful guidance in dialogue.\nEach evaluation dimension receives a score on a scale of 1 to 10, where a higher score indicates better performance."}, {"title": "GPT-4 Evaluation.", "content": "Following recent studies that utilize GPT-4 for evaluating LLM's capability (Liu et al. 2024b), we compose a prompt based on the above evaluation criterion to request evaluation results from GPT-40 (See Appendix C for the full prompt). In addition to scores from the three dimension, GPT-40 is also requested to output an overall score and a reason (to mitigate hallucination). The evaluation is comparison-based. GPT-40 is asked to compare user interaction processes from two AI assistants, given the dialogue records for every sample in the MTGPD60 test set. In the evaluation, we keep one of the assistant as the reference dialogues in MTGPD60 test set, which are human-calibrated, and the other assistant as the method to be tested. To mitigate biases, the final rating is the average of two tests, with the input order of the two assistants swapped. For the baselines, in addition to existing TIS prompt generation approaches, we also include general-purpose LLMs, since they also possess multi-medial and dialogue abilities."}, {"title": "Human User Evaluation.", "content": "Despite remarkable ratings given by LLMs, evaluations directly from human are irreplaceable. For this task, we recruited 19 well-educated volunteers with different backgrounds, which can be categorized into three groups. Group A contains seven professional visual designers from the design center of a top-tier corporation. They use TIS models such as SD to aid designing, and compose TIS prompts manually without tool assistance. Group B consists of six developers who are experienced users of TIS models. Around half of them have AI background and tried automatic prompt engineering. Group C are six amateur users who do not regularly use TIS models and are hardly exposed to prompt engineering technologies."}, {"title": "Ablation Study", "content": "Different Training Styles. Instead of utilizing the full multi-turn dialogues in MTGPD as training data, we keep only the initial user query and the final optimized prompt in the dataset to train a single-turn TIS prompt model. As shown in Table 5, this single-turn model still significantly improves the Aesthetic Score of images from original user inputs, which suggests the effectiveness of the mining and cleaning process in the construction of MTGPD. Compared with single-turn, the multi-turn model improves largely in CLIP Score. Through multi-turn interaction with users, the TIS prompt generation process forms a step-by-step chain-of-thought (Wei et al. 2022), thereby decreasing hallucinations and increasing the stability of LLM performance."}, {"title": "Different TIS models.", "content": "We test the same prompt on a series of different TIS models: LDM (Rombach et al. 2022), SD-v1.5 (Rombach et al. 2022), SD-v2 (Rombach et al. 2022), SDXL (Podell et al. 2024) and SD-v3 (Esser et al. 2024). As shown in Table 6, images generated from DialPrompt's optimized prompts continuously outperforms that from original user inputs, indicating a strong transferability of DialPrompt to different TIS models."}, {"title": "Conclusion", "content": "In this paper, we seek to improve the user-centricity in TIS prompt engineering by proposing DiaPrompt, a novel dialogue-based TIS prompt generation model. DialPrompt not only shows advantages in improving the quality of synthetic images, but also provide a unique user experience through multi-turn guidance. Our user evaluation demonstrate that DialPrompt can not only assist novice users to easily optimize TIS prompt with their own ideas, but also aid professionals in their designing work through a comprehensive recommendation of aesthetic elements. Future work includes improving the flexibility of dialogues by incorporating reinforcement learning and increasing the prompt quality through multi-media training."}, {"title": "Appendix A: Prompt Template for Dialogue Format Conversion via GPT-4", "content": "[System]\nGiven an original user instruction and its enhanced version, create a structured multi-turn Q&A dialogue that guides a user to refine their prompt for creating an aesthetically superior image. Here are notices:\n1. The dialogue should start with the user's original instruction.\n2. This instruction is for Stable Diffusion to generate images.\n3. Questions focus on providing enrich information for user to choose based on the enhanced description.\n4. With user's chosen options, assistant output a final enriched instruction which is similar to enhanced description or keep enhanced description as final output.\n5. The dialogue should be ended with a enhanced version prompt for Stable Diffusion. When you output the enhanced version prompt, please add ###[BEGIN OF PROMPT] before the prompt, and ###[END OF PROMPT] after the prompt like this: ###[BEGIN OF PROMPT] 'lofi biopunk portrait of Shrek as a Disney Princess, Pixar style, by Tristan Eaton, Stanley 'Artgerm' Lau, and Tom Bagshaw.' ###[END OF PROMPT]\n6. Please Generate the multi-turn Q&A dialogue with llama alpaca format of json file, the Role Name in each sample must be 'user' and 'assistant'.\n7. In the last dialogue turn, you must add the summarize order for user content, like: 'user': Artgerm and Greg Rutkowski. Please summarize the prompt for me now.\n[User]\nPlease Generate the multi-turn Q&A dialogue with llama alpaca format of json file: {Input Instruction-Prompt Pair}"}, {"title": "Appendix C: Prompt Template for User Experience Evaluation via GPT-4", "content": "[The Start of Assistant 1's Dialogue]\n{Input Dialogue 1}\n[The End of Assistant 1's Dialogue]\n[The Start of Assistant 2's Dialogue]\n{Input Dialogue 2}\n[The End of Assistant 2's Dialogue]\n[System]\nWe would like to request you to compare on the performance of two Al assistants in the displayed multi-turn dialogues with the user, trying to recommend and build a proper Stable Diffusion prompt for the user. Please rate the user-friendliness of the two AI assistants, considering the Clarity, Richness and Helpfulness of the whole dialogue. (1) Clarity: to which degree the layout and language of Al's responses is organized and clear for users. (2) Richness: the richness of the AI recommended aesthetic elements that user can express preferences on in the dialogue. (3) Helpfulness: the degree to which the AI can understand user's requirement and give step-by-step guidance in the dialogue. Each dimension receives a score on a scale of 1 to 10, where a higher score indicates better performance. And also output an overall score of 1 to 10. Please first output two lines indicating the scores for Assistant 1 and 2, with each line containing only four values indicating the scores for overall, clarity, richness and helpfulness, respectively. The four scores are separated by space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the dimensions were presented does not affect your judgment."}, {"title": "Appendix B: Prompt Template for User Preference Simulation via GPT-4", "content": "[System]\nAssume you are a user who has a dialogue with a system which aims to enrich prompt for text to image generation, make suitable selection according to the option it provided without any biases, or ask it to combine all options based on your situation. your answer must be concise.\ne.g. system: To create a more captivating image, would you like the portrait to be realistic or stylized? Your answer: Realistic, please. Or you can answer: A mix of both is ok.\n[User]\n{Input Dialogue}"}]}