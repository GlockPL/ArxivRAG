{"title": "Constrained Hybrid Metaheuristic Algorithm for Probabilistic Neural Networks Learning", "authors": ["Piotr A. Kowalski", "Szymon Kucharczyk", "Jacek Ma\u0144dziuk"], "abstract": "This study investigates the potential of hybrid metaheuristic algorithms to enhance the training of Probabilistic Neural Networks (PNNs) by leveraging the complementary strengths of multiple optimisation strategies. Traditional learning methods, such as gradient-based approaches, often struggle to optimise high-dimensional and uncertain environments, while single-method metaheuristics may fail to exploit the solution space fully. To address these challenges, we propose the constrained Hybrid Metaheuristic (cHM) algorithm, a novel approach that combines multiple population-based optimisation techniques into a unified framework. The proposed procedure operates in two phases: an initial probing phase evaluates multiple metaheuristics to identify the best-performing one based on the error rate, followed by a fitting phase where the selected metaheuristic refines the PNN to achieve optimal smoothing parameters. This iterative process ensures efficient exploration and convergence, enhancing the network's generalisation and classification accuracy. cHM integrates several popular metaheuristics, such as BAT, Simulated Annealing, Flower Pollination Algorithm, Bacterial Foraging Optimization, and Particle Swarm Optimisation as internal optimisers. To evaluate CHM performance, experiments were conducted on 16 datasets with varying characteristics, including binary and multiclass classification tasks, balanced and imbalanced class distributions, and diverse feature dimensions. The results demonstrate that cHM effectively combines the strengths of individual metaheuristics, leading to faster convergence and more robust learning. By optimising the smoothing parameters of PNNs, the proposed method enhances classification performance across diverse datasets, proving its application flexibility and efficiency.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) has rapidly evolved from a niche field of academic research to an integral part of everyday life, revolutionising industries and shaping the way we interact with technology [1]. Initially, AI was seen as a distant ambition, confined to science fiction and theoretical discussions. However, advancements in machine learning, neural networks, and computational power have fueled its exponential growth, making AI accessible and applicable in numerous sectors. Today, AI is embedded in a wide range of technologies\u2014from the personal assistants on our smartphones and recommendation algorithms on streaming platforms to advanced medical diagnostics and autonomous vehicles [2, 3].\nIntegrating AI into daily life has transformed how we work, communicate, and make decisions. It has enabled businesses to optimise operations, improve customer experiences, and innovate in previously unimaginable ways. For instance, AI-powered chatbots are now common in customer service, providing immediate responses and personalised support. In healthcare, AI is being used for early detection of diseases, analysis of medical images, and even drug discovery, saving lives and improving the quality of care [4, 5]. In the financial sector, AI models are employed to predict market trends, manage risks, and detect fraud, offering new opportunities for growth and security [6]."}, {"title": null, "content": "As AI continues to advance, its impact will only deepen, offering solutions to some of society's most pressing challenges, such as climate change, education, and resource management [7], as well as public safety [8, 9] and protection of natural resources [10, 11]. However, this rapid expansion also raises concerns regarding ethical implications, job displacement, and the potential for bias in decision-making systems. The future of AI will require careful consideration of these challenges, ensuring that its benefits are harnessed responsibly and equitably [12]. Despite these concerns, the relentless progress of AI promises to reshape industries and societies, creating new opportunities and fundamentally altering the fabric of modern life [13].\nAI has become a powerful force driving innovation across industries, largely due to the development of neural networks [14]. These networks, particularly deep learning models, have revolutionised AI by enabling machines to learn from vast amounts of data and make decisions with impressive accuracy [15]. Neural networks are capable of identifying complex patterns in data, which has made them essential in tasks like image recognition, natural language processing, and autonomous systems [16]. Their ability to process and adapt to high-dimensional data has transformed fields such as healthcare, finance, and robotics [17]. However, as AI continues to grow, challenges around computational demands, interpretability, and ethical concerns remain [18, 19]. Despite these hurdles, neural networks are at the heart of AI's evolution, shaping the future of technology and offering immense potential for innovation and societal advancement [20].\nOne of the key branches of neural networks is the group of Probabilistic Neural Networks, which have gained significant importance in recent years due to their ability to model uncertainty and make probabilistic predictions. PNNs are designed to estimate the probability distribution of data, allowing them to provide not just predictions but also a measure of confidence in those predictions. This makes them particularly useful in applications where understanding the uncertainty or variability in the data is crucial, such as in medical diagnostics, risk assessment, and decision-making under uncertainty.\nThe development of PNN's can be traced back to the need for more interpretable and robust models in fields that require high levels of certainty, such as finance and healthcare. Unlike traditional neural networks, which output point estimates, PNNs generate probability distributions, offering a richer and more nuanced understanding of the data. This capability has made them invaluable in situations where the costs of misclassification are high, and decision-makers need to assess not only the most likely outcome"}, {"title": null, "content": "but also the associated risks.\nAs research in probabilistic modelling advanced, PNNs evolved to incorporate more sophisticated algorithms for density estimation, classifying patterns with greater accuracy even in noisy or incomplete datasets. These networks have become increasingly popular in areas like pattern recognition, anomaly detection, and classification tasks where uncertainty is inherent. The significance of PNNs lies in their ability to combine the power of neural networks with the flexibility and interpretability of probabilistic models, allowing for more informed, data-driven decisions in complex, uncertain environments.\nPNNs differ from traditional feedforward networks in that they do not possess the classic dense layers commonly found in such networks, nor do they rely on gradient-based learning procedures [21]. Instead, PNNs are typically based on kernel density estimation, where each neuron represents a local probability distribution rather than a specific learned weight [22]. This structure allows PNNs to effectively model uncertainty and complex distributions, but it also means that they do not use the backpropagation algorithm or other gradient-based optimisation methods for training [23].\nThe process of training PNNs, particularly in determining smoothing parameters, can be categorized into two main groups of methods. First, statistical deterministic approach methods, such as cross-validation [24] and plug-in techniques [25]. These methods focus on optimizing the smoothing parameter to minimize the error between the estimated and true probability density functions. They are stable and consistent, making them ideal when statistical accuracy is the primary goal. On the other hand, non-deterministic metaheuristic approaches, including optimization algorithms like Genetic Algorithms, Particle Swarm Optimization [26], and reinforcement learning [23], prioritize classification performance by maximizing class separability. They are less stable than statistical methods but can achieve significantly better results in specific runs when the training algorithm is appropriately tuned. The method proposed in this article belongs to the second group. While it is less stable than statistical methods, it can discover significantly better solutions in individual training runs. Non-deterministic approaches allow for flexibility and adaptability during training. With appropriate algorithm triggering and fine-tuning, these methods can significantly outperform deterministic statistical techniques, especially in complex classification scenarios where achieving optimal class distinction is critical.\nLearning in PNNs, however, remains a crucial procedure for their effec-"}, {"title": null, "content": "tiveness. It is through this learning process that the network is able to generalise the knowledge contained in the data and make accurate predictions. In PNNs, the learning typically involves adjusting the smoothing parameters, which govern how the probability distributions are modelled, ensuring that the network can adapt to the underlying data distribution. This adaptation is vital, as it allows PNNs to generalise well to new, unseen data, making them highly effective for tasks such as classification and pattern recognition. Thus, while PNNs operate under a different paradigm than traditional neural networks, their learning process is essential for enabling the network to extract meaningful insights from data and apply this knowledge to make reliable predictions in real-world scenarios [27].\nMetaheuristic algorithms are a class of optimisation techniques that have gained prominence in neural network learning due to their ability to explore large, complex search spaces without relying on gradient-based methods. These algorithms are particularly useful for training neural networks with non-convex objective functions, where traditional methods like gradient descent may struggle to find global optima or may get stuck in local minima [28]. Metaheuristics, such as Genetic Algorithms (GAs), Particle Swarm Optimization (PSO), Simulated Annealing (SA), Ant Colony Optimization (ACO), and other, provide an alternative approach by mimicking natural or biological processes to guide the search for optimal solutions in complex problems [29, 30, 31]. In the context of neural networks, metaheuristics are employed to fine-tune network architecture, select optimal hyperparameters, and train the network in an efficient and robust manner. Their ability to balance exploration and exploitation makes them effective in scenarios where traditional learning techniques may fail, particularly in high-dimensional, noisy, or complex data environments. By adapting to the problem at hand, metaheuristics offer a versatile and powerful toolset for improving the performance and generalisation ability of neural networks [32]."}, {"title": "1.1. Motivation", "content": "The synergy between heuristic algorithms in neural network training can significantly enhance model performance by combining the strengths of different optimisation strategies [33]. While metaheuristics excel at exploring the solution space and escaping from local optima, they may not always converge quickly or precisely to an optimal solution. To address this, hybrid approaches that combine metaheuristics with traditional gradient-based methods are increasingly being explored [34]. For example, a metaheuristic"}, {"title": null, "content": "algorithm can be used to find a promising starting point or optimise the network's architecture, followed by fine-tuning the weights with a gradient descent method. Alternatively, multiple metaheuristic algorithms can be combined to leverage the diverse exploration capabilities of each, ensuring a more thorough search of the solution space. This synergy can lead to faster convergence, better global search, and improved generalisation, making it an effective strategy for complex neural network training tasks. The complementary nature of heuristic and metaheuristic algorithms allows for a more robust and adaptive learning process, ultimately enhancing the network's ability to solve various challenging problems [35].\nThe motivation behind this research lies in the continuous quest to improve the efficiency and effectiveness of learning algorithms for neural networks, particularly in the context of Probabilistic Neural Networks (PNNs). Traditional learning methods, including gradient-based approaches, face challenges in optimising complex models, especially in high-dimensional and uncertain environments. While metaheuristic algorithms, particularly swarm-based techniques like Particle Swarm Optimization (PSO) and Firefly Algorithm (FPA), have shown promise in exploring complex solution spaces, their application is often limited to single-method optimisation processes that may not fully exploit the potential of the search space."}, {"title": "1.2. Contribution", "content": "In this work, we introduce a novel approach\u2500constrained Hybrid Metaheuristic (CHM)\u2014that seeks to overcome these limitations by combining multiple weak metaheuristics into a more robust and efficient super-metaheuristic. By leveraging the strengths of several population-based optimisation techniques, the cHM procedure can more effectively explore and optimise the smoothing parameters for PNNs, improving the network's performance. This hybrid method is particularly valuable for PNNs, where selecting appropriate smoothing parameters plays a crucial role in generalisation and classification accuracy.\nThe proposed cHM procedure is designed to operate in two phases: probing and fitting, with each phase subject to time constraints to ensure computational efficiency. The probing phase allows for an initial evaluation of multiple metaheuristics, selecting the one that performs best in terms of error rate. In the fitting phase, the best-performing metaheuristic continues to refine the PNN, ensuring that the model converges to an optimal solution. This iterative process allows the system to adapt and improve with"}, {"title": null, "content": "each cycle, ensuring robust learning and high-quality predictions. Through this innovative hybrid approach, we aim to enhance the learning capabilities of PNNs, providing a more efficient and flexible tool for solving complex classification tasks."}, {"title": "1.3. Paper organization", "content": "The remainder of this paper is organised as follows. Section 2 provides a detailed overview of the probabilistic neural network, including its structure and the training procedures, with a particular focus on the smoothing parameter modification procedure. Section 3 introduces the constrained Hybrid Metaheuristic algorithm, describing the various metaheuristic techniques considered, such as Particle Swarm Optimisation, the BAT algorithm, Bacterial Foraging Optimization, Simulated Annealing, and the Flower Pollination Algorithm. The proposed algorithm is then presented in detail. Section 4 discusses the results of numerical investigations, covering PNN training details, data sets, and the outcomes of the proposed learning procedure, including performance analysis of the metaheuristic training methods and the frequency of metaheuristic selection. Finally, Section5 concludes the paper with a summary of findings and potential future research directions."}, {"title": "2. Probabilistic neural network", "content": "Probabilistic neural networks (PNNs) are a type of artificial neural network that incorporate probabilistic principles in their functioning. In 1990 Donald Specht introduced the concept of PNNs in his papers [36, 37], which presented a new approach to classification [38] and regression [39] tasks by combining statistical methods with neural networks. Largely influenced by Bayes' theorem and the Parzen window method for probability density estimation, PNNs aim to combine the strengths of statistical probability with the powerful learning capabilities of neural networks. This enables them to model uncertainty in data more effectively. Unlike traditional neural networks that generate deterministic outputs, PNNs focus on predicting a probability distribution over possible outcomes. This allows them to handle noisy or uncertain input data and provide more robust predictions in real-world applications.\nThere are two main types of probabilistic neural networks: those designed for regression and those intended for classification tasks. In regression-based PNNs, the goal is to predict continuous values while providing an estimate"}, {"title": null, "content": "of the uncertainty associated with those predictions. These models typically return a mean prediction along with a confidence interval, enabling decision-making that considers not just the prediction but also the reliability of the output. In contrast, classification-based PNNs focus on assigning input data to discrete categories. They do this by estimating the probability that an input belongs to each possible class and then selecting the class with the highest probability. This probabilistic approach to classification helps in handling ambiguous or noisy data, as the model doesn't just provide a single label but rather a probability distribution over all possible classes (labels).\nThe development of PNNs has been shaped by advances in both computational power and probabilistic modelling techniques. Early PNNs were limited by the computational cost of estimating probability distributions for large datasets, but modern advancements, such as variational inference and Monte Carlo methods, have greatly improved their scalability [40]. Furthermore, the rise of deep learning has brought about new probabilistic architectures, like Bayesian neural networks, which build on the foundational ideas of PNNs. As research continues, integrating probabilistic reasoning into neural network models is expected to play an even larger role in fields like autonomous systems, where safety and uncertainty are critical considerations [41].\nProbabilistic neural networks have numerous applications across a wide range of fields. In finance, for example, they are used for risk assessment [42] and modelling uncertain market trends [43]. In healthcare, PNNs assist in medical diagnostics, where the uncertainty in patient data makes probabilistic models particularly valuable [44]. They are also employed in robotics [45] for decision-making under uncertainty [46] and in natural language processing for tasks like sentiment analysis [47], where multiple interpretations of text are possible. PNNs' ability to manage uncertainty makes them ideal for tasks where traditional deterministic models might struggle due to ambiguous or incomplete data. PNNs are widely used in other domains, such as interval data classification [48] and stream data classification [49], where the nature of the input data requires flexible probabilistic handling. Due to their structure and explainable nature, PNNs enable controlled dimensionality reduction [50] and dataset size pruning [51], allowing them to efficiently handle high-dimensional data while preserving the interpretability of the model. This controlled reduction makes PNNs a powerful tool in data-intensive applications where it is crucial to balance model complexity with performance, especially when working with large or streaming datasets."}, {"title": "2.1. Structure of probabilistic neural network", "content": "The functioning of the PNN classifier is built upon the kernel density estimator (KDE), which is a non-parametric method for estimating the probability density function of a given dataset. In general, the KDE can be expressed as:\n$$f(x) = \\frac{1}{PNh^N} \\sum_{p=1}^P K (\\frac{x - x^{(p)}}{h}).$$\nHere, $x = [x_1,..., x_N]$ represents a test sample, $h$ is the smoothing parameter, and $K (\\cdot)$ is the kernel function that maps input data from $R^N \\rightarrow [0,\\infty)$. The kernel function is responsible for quantifying the similarity between the test point and each training point in the dataset. The parameter $h$ controls the width of the kernel and, therefore, how much influence each training point has on the estimate. For multi-dimensional datasets, the kernel density estimator is often generalised using a product of individual kernels applied to each dimension of the input data:\n$$K(x) = K(x_1) \\cdot K(x_2) \\cdot ... \\cdot K(x_N).$$\nIn the approach presented in this paper, the kernel function for each dimension, $K(x_i)$, is defined as:\n$$K(x_i) = \\frac{2}{\\pi(x_i^2 + 1)^2}.$$\nThis specific form of the kernel is a one-dimensional Cauchy function chosen because of its beneficial analytic properties, particularly in terms of ensuring a well-behaved derivative. This kernel function is useful when it comes to modelling data that has heavier tails, as it places less emphasis on distant data points compared to other kernels like the Gaussian one.\nIt is also important to note that the exact choice of the kernel function can have a minor impact on the accuracy of the density estimation, with research suggesting that the kernel selection typically causes around a 4% difference in the quality of the non-parametric estimate. However, the kernel function choice should be tailored to the specific requirements of the application at hand. In this particular implementation of PNN, the product form of KDE (2) along with the Cauchy kernel (3) is used to model the underlying data distribution. This configuration was selected for its analytical convenience and suitability for the considered classification task."}, {"title": null, "content": "A PNN is structured with four distinct layers that work together to classify or predict outputs based on the input data. The first layer is the input layer, where the attributes of the input vector x are fed into the network. This layer simply passes the input data to the next layer without any transformation or computation.\nThe second layer is the pattern layer, which contains one neuron for each training sample in the dataset. In this layer, each neuron represents a training example and computes a similarity measure between the input vector and that specific example. The neurons in this layer compare the input vector to each training point, generating an output that reflects how closely the input matches each training example. There are two common approaches to computing these similarities: radial basis functions (or radial kernels) and product kernels. In the proposed case, the product kernel approach is used to calculate the output of the pattern layer.\nThe third layer is the summation layer, which aggregates the outputs from the pattern layer. In this layer, there is one neuron for each class j, and each of these neurons collects signals from the pattern neurons that correspond to training examples of the respective class. The summation neuron for a given class computes the probability density estimate for the input vector belonging to that class (4). This is done using a formula that integrates several key parameters, such as the number of training examples $P_j$ in the $j$-th class, a matrix of smoothing parameters $h$, and a modification coefficient $S_p$. The output of each summation neuron reflects the likelihood that the input vector belongs to its respective class.\n$$f_j(x) = \\frac{P_j}{P_i \\cdot det(h)} \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{S_p^N} K ((\\frac{x - x^{(p)}}{h})^T \\frac{h^{-1}}{S_p}).$$\nThe output layer consists of a single neuron that makes the final decision about the input's class. The neuron selects the class with the highest output from the summation layer, effectively applying Bayes' theorem to determine the most probable classification for the input vector. The decision is made by selecting the class $Out(x)$ that maximises the estimated probability density function $f_j(x)$ across all classes j (5).\n$$Out(x) = argmax_{j=1...J} f_j(x),$$"}, {"title": "2.2. Training procedures", "content": "The training process for a PNN primarily revolves around selecting appropriate values for the smoothing parameters $h_i$ and the modification coefficients $s_p$, which control the behavior of the pattern neurons and summation neurons, respectively. These parameters are critical for ensuring that the PNN accurately models the underlying probability distributions of the training data.\nThe process of training PNNs, particularly in determining the smoothing parameters, encompasses a variety of methodologies that can be broadly categorised into two distinct groups. The first group consists of statistical approaches, such as cross-validation and plug-in methods. These techniques stem from classical statistics and aim to optimise the smoothing parameter by minimising the mean squared error between the nonparametric probability density estimation and the true probability density function. This minimisation ensures that the density estimation closely aligns with the underlying data distribution, making these methods particularly effective in scenarios where statistical accuracy of density estimation is the primary objective.\nHowever, when PNNs are employed in classification tasks, the focus shifts from pure statistical accuracy to achieving optimal class separability. In such cases, the primary goal is not merely to approximate the true probability density function but to position the density estimation functions of different classes in a way that maximises their distinction. This ensures that the PNN classifier can effectively differentiate between classes, which is often more critical than minimizing the error of density estimation.\nTo address this classification-specific objective, alternative approaches to training PNNs have been developed. These include methods based on meta-heuristic optimisation algorithms, such as GA, PSO or SA, which search for the optimal smoothing parameters by directly optimising classification performance metrics. Additionally, reinforcement learning techniques can be employed, where the training process iteratively adjusts the smoothing parameters based on the feedback from the classification task itself, ultimately learning the parameter values that yield the highest classification accuracy.\nBy tailoring the training process to the specific requirements of the classification task, these alternative approaches can overcome the limitations of purely statistical methods, enabling PNNs to achieve superior performance in complex classification scenarios."}, {"title": null, "content": "A crucial element in training PNNs is selecting an appropriate method for determining the smoothing parameter, as this choice profoundly affects the network's classification performance. The smoothing parameter plays a pivotal role in controlling the balance between overfitting and underfitting, making its adjustment critical to the success of the model. Various approaches to defining the smoothing parameter have been proposed, each offering different levels of flexibility and application scenarios.\nThe simplest approach involves using a single scalar value for the smoothing parameter ($h_i = h$), applied uniformly to all pattern neurons in the network. This method is computationally efficient and is often preferred when the data is relatively homogeneous, and the classes are well-separated. However, its limitations become evident in more complex datasets, where a single global value may not adequately capture variations in data distribution.\nAn alternative approach assigns a single smoothing parameter value to each class. This method introduces more flexibility by allowing the parameter to vary between classes, enabling better adaptation to the characteristics of each class distribution. This approach is particularly useful in datasets where the classes exhibit distinct densities or variances, as it ensures that each class is smoothed appropriately without being overly rigid or too complex. This level of granularity can be expressed using the following formula:\n$$h_i = [h^{(1)}, h^{(2)}, ..., h^{(G)}]$$\nwhere $h^{(g)}$ represents the smoothing parameter for the g-th class, and G is the total number of classes in the dataset.\nFor even greater flexibility, a vector-based smoothing parameter can be employed, where each coordinate of the input pattern has its own smoothing value. This method is advantageous in high-dimensional datasets where different features exhibit varying levels of relevance or variability. By tailoring the smoothing parameter to each feature, the network can better adapt to local data structures and improve classification performance in such settings. This approach can be expressed using the following formula:\n$$h_i = [h_1, h_2, ..., h_n],$$\nwhere $h_j$ represents the smoothing parameter for the j-th coordinate of the input pattern, and n is the total number of features in the dataset.\nFinally, the most advanced approach involves a matrix of smoothing parameters, where each coordinate has a unique value not only for the feature"}, {"title": null, "content": "but also for the class it belongs to. This approach provides the highest level of customisation, allowing the network to account for intricate interdependencies between features and class distributions. It is particularly beneficial in complex classification tasks where class-specific feature relationships are essential for accurate predictions. This method can be expressed using the following formula:\n$$H_V =\n\\begin{bmatrix}\nh^{(1)}_1 & h^{(1)}_2 & ... & h^{(1)}_n\\\\\nh^{(2)}_1 & h^{(2)}_2 & ... & h^{(2)}_n\\\\\n\\vdots & \\vdots & & \\vdots\\\\\nh^{(G)}_1 & h^{(G)}_2 & ... & h^{(G)}_n\\\\\n\\end{bmatrix}$$\nwhere $h^{(g)}_j$ represents the smoothing parameter for the j-th coordinate of the input pattern and the g-th class, and G is the total number of classes in the dataset. This matrix structure enables the network to apply a distinct smoothing parameter for each feature within each class, offering the highest degree of flexibility and precision in adapting to the data's complex structure.\nThe choice of smoothing parameter method depends on the nature of the dataset and the complexity of the classification task. Simpler approaches, such as the scalar or class-level parameters, are suitable for relatively uniform datasets with clear class separability, while more complex methods, such as vector or matrix parameters, are better suited for high-dimensional or heterogeneous datasets with overlapping or intricately distributed classes. Each method represents a trade-off between computational efficiency and the capacity to model complex data structures effectively.\nThe performance of each method for determining the smoothing parameter, as well as the chosen configuration for the number of smoothing parameters, largely depends on the specific task assigned to the PNN. The effectiveness of various smoothing parameter selection procedures has been thoroughly analysed in the following articles [26, 52].\nSmoothing parameter modification procedure\nOnce the temporary values of the smoothing parameter vector are obtained using one of the methods presented above, the KDE quantities are calculated based on (4) for each element $x^{(p)}$ where $p = 1, ..., P$. This enables to independently compute the modification parameter $s_p$ for all $x^{(p)}$ patterns using the formula:"}, {"title": null, "content": "$$s_p = (\\frac{f(x^{(p)})}{\\check{S}})^{-c},$$\nwhere $\\check{S}$ represents the geometric mean of the KDE values $f(x^{(p)})$, and $c > 0$ is a constant determining the intensity of the modification. As $c$ increases, the modification intensity grows. It is important to note that when $c = 0$, $s_p = 1$, meaning no modification is applied to the smoothing parameter. The primary goal of introducing the smoothing parameter modification procedure in PNNs is to adjust the level of smoothing for individual data points to enhance the quality of density estimation. This procedure enables dynamic modification of the smoothing parameter based on the local properties of the data, such as the density of observations. It allows for more precise modelling of diverse data structures while minimising the effects of over-smoothing or under-smoothing."}, {"title": "3. The constrained Hybrid Metaheuristic (cHM) algorithm", "content": "We propose a constrained Hybrid Metaheuristic (cHM) algorithm that combines several swarm-based optimisation algorithms into a coherent meta-heuristic method. For the sake of clarity of the presentation, we will refer to the component swarm-based procedures (operating withinn cHM) as weak metaheuristics, inside-optimisers or single metaheuristics. cHM uses several weak metaheuristics based on a population of individuals, in particular, the ones mentioned in the previous section. In each of them, a population is a group of individuals that represent potential solutions, i.e. smoothing parameter vectors for a given PNN. Each individual contains sufficient information to produce a functional PNN.\nThe proposed optimisation method consists of two phases that can be repeated n-times: probing and fit. These phases are constrained in execution by the maximum number of times each phase calls an evaluation (fitness) function, max $FE_{probing}$ and max $FE_{fit}$ respectively. The max $FE_{probing/fit}$ [53] constraint could be transformed into other limitations, for instance, a time-based evaluation, which, however, strongly depends on the computational resources used in the experiments. It should be mentioned that, in this research, max$FE_{probing/fit}$ counts a single evaluation of each test sample of each individual as a separate evaluation. For instance, when the population"}, {"title": null, "content": "of 10 individuals is evaluated with 100 samples, 1000 evaluations are added to the value max $FE_{probing/fit}$.\nIn the first phase, the population for each weak metaheuristic is initialised similarly or taken from the previous iteration of the cHM algorithm. Next, each optimisation method is used separately to train the PNN until max $FE_{probing}$ number of evaluations is not met. Then, the method with the lowest cost function value (error rate) is selected for further PNN training. The population of the best single metaheuristic is saved, to be passed to the next phase. In the case of the same function cost scores tied by multiple metaheuristics, the best one of them is selected randomly.\nThe second phase considers the best-performing metaheuristic from the first phase. It uses the optimisation procedure, together with its population, to train PNN for the max$FE_{fit}$ number of evaluations. In the end, after the PNN is finished, the metaheuristic population from this step is saved, to be passed to the next iteration of the cHM algorithm.\nThese two phases are repeated n-times or until the process converges, i.e., the error rate is equal to 0 on the test set.\nThe detailed cHM pseudocode is shown in Algorithm 1."}, {"title": "3.2. Metaheuristic procedures", "content": "Generally, when using swarm-based algorithms to train PNNs, an individual in a population has the form of a vector of proposed smoothing parameters, i.e., each individual ($h_{ij}$) is a vector of parameters sufficient to trigger a PNN for a given data [54].\nIn the experiments with the proposed cHM algorithm the following portfolio of five metaheuristic methods have been considered: PSO, BAT algorithm (BAT), Bacterial Foraging Optimization (BFO), Simmulated Annealing (SA), and Flower Pollination Algorithm (FPA). All of these global optimization methods are well-known in the literature and have been described in numerous papers. In our implementation, the vanilla formulations of these metaheuristics are considered, and therefore, for the sake of space-saving, in what follows we only briefly mention the underlying principles and search mechanisms of these methods along with the relevant literature.\nPSO is one of the most widely used nature-inspired algorithms, with multiple enhancements presented in the field. Original PSO formulation refers to a swarm-based technique founded on the cooperation of particles in a swarm (population) [55]. The particles move around in a search space S iteratively, looking for the optimal position. Each particle has its position"}, {"title": null, "content": "p and velocity v that are updated through algorithm iterations. The new particle's position is influenced by its historically best position, as well as the historically best position of the entire swarm or the selected part of a swarm called the particles's neighborhood.\nBAT, similarly to PSO, procedure uses a population of individuals to search the space for a sub-optimal problem solution. It is inspired by bats' behavior for communication when hunting or moving. Bats use echolocation with varying frequency and loudness, depending on the distance from the prey and the size of the award [56]. The BAT algorithm (BA) might be seen as a special case of PSO and has been used to train PNNs before [57].\nBFO is based on the behavior of E. Coli bacteria foraging motions, and models different movements of the E. Coli including chemotaxis, swarming, reproduction, elimination, and dispersal [58]. These procedures are responsible for the bacterium actuation, sensing, and decision-making processes. Similarly to PSO and BA, the BFO procedure maintains a population of individuals that iteratively seek an optimal solution to the problem in a given space.\nSA is inspired by the annealing phenomenon while crystals are grown from melt [59]. In SA, similarly to previously-mentioned techniques, a population of individuals is used to search a solution space. The particles are initialized randomly, and the algorithm flow is controlled by two factors: temperature T and the Boltzmann distribution. Over the SA iterations, a new potential particle position is accepted if its cost function value is lower than previously (before the potential movement). Otherwise, the newly generated solution is accepted with the so-called acceptance probability, defined by the Boltzmann equation. This probability depends on the temperature, which gradually decreases in time, thus cooling down the SA process.\nFPA is inspired by insect flower pollination. The method iteratively searches the space of possible solutions by combining the following two steps: local (exploration) and global (exploitation) optimisation [60]. A random variable r and parameter p control the procedure's flow. For each individual, if the r > p the exploration phase is turned on. Otherwise, a local examination is performed randomly, around the current position of the individual. This exploitation step is referred to as \u201cself-pollination\" [54]"}, {"title": "4. Experimental results", "content": "The CHM method combines several metaheuristics into one optimisation method and leverages the advantages of each particular swarm-based technique in a shorter time frame. In addition, cHM application may lead to low-cost evaluation of different methods for a given problem. In the experiments, cHM was used for training PNN, with BAT, SA, FPA, BFO and PSO procedures used as inside optimisers in Algorithm 1. The methods used for the algorithm evaluation are presented in the following, together with the results of the experiment."}, {"title": "4.1. PNN training details", "content": "In this research, PNNs were constructed using the Cauchy kernel with separate smoothing parameters for each feature vector $h_{ij}$ (7) in the dataset. The cHM algorithm was employed to train the PNNs for classification problems. cHM was initialized with a randomly generated population of individuals, each representing a set of smoothing parameters required to build the PNN. The initial population consisted of 20 individuals with values constrained to real numbers in the range [0, 10]. This same population was used to initialize each of the inside optimizers within the cHM method. To ensure the reproducibility of experiments, a fixed random seed was applied to all stochastic operations performed during the calculations.\nWhen training PNNs with swarm intelligence methods, the smoothing parameters are determined using heuristic methods. The possible smoothing parameters were constrained to the interval [0, 10000] of real numbers. If the $h_i$ value was negative, the reflection technique [61] was applied to ensure the value fell within the constrained range of positive numbers.\nThe parameters of the cHM algorithm are shown in Table 1. The parameters used for each metaheuristic inside the CHM are shown in Tables 2 - 6. These parameters were selected according to the referenced papers."}, {"title": "4.3. Results of proposed learning procedure", "content": "CHM was applied to train and test PNNs for 16 datasets from Table 7. Each training procedure was performed using"}]}