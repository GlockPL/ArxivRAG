{"title": "MEEG and AT-DGNN: Advancing EEG Emotion\nRecognition with Music and Graph Learning", "authors": ["Minghao Xiao", "Zhengxi Zhu", "Wenyu Wang", "Meixia Qu"], "abstract": "Recent advances in neuroscience have elucidated\nthe crucial role of coordinated brain region activities during\ncognitive tasks. To explore this complexity, we introduce the\nMEEG dataset-a comprehensive multi-modal music-induced\nelectroencephalogram (EEG) dataset and the Attention-based\nTemporal learner with Dynamic Graph Neural Network (AT-\nDGNN), a novel framework for EEG-based emotion recognition.\nThe MEEG dataset captures a wide range of emotional responses\nto music, enabling an in-depth analysis of brainwave patterns\nin musical contexts. The AT-DGNN combines an attention-\nbased temporal learner with a dynamic graph neural network\n(DGNN) to accurately model the local and global graph dynamics\nof EEG data across varying brain network topologies. Our\nevaluations show that AT-DGNN achieves superior performance,\nwith accuracies (ACC) of 83.06% in arousal and 85.31% in\nvalence, outperforming state-of-the-art (SOTA) methods on the\nMEEG dataset. Comparative analyses with traditional datasets\nlike DEAP highlight the effectiveness of our approach and under-\nscore the potential of music as a powerful medium for emotion\ninduction. This study not only advances our understanding of\nthe brain's emotional processing but also enhances the accurcy\nof emotion recognition technologies in brain-computer interfaces\n(BCI), leveraging both graph-based learning and the emotional\nimpact of music. The source code and dataset are available at\nhttps://github.com/xmh1011/AT-DGNN.", "sections": [{"title": "I. INTRODUCTION", "content": "Emotion, a complex physiological and psychological phe-\nnomenon, plays a critical role in interpreting genuine reactions\nto daily interactions [1]. In 1980, Russell introduced the\ndimensional models of emotion, with the Valence-Arousal\n(VA) model being particularly noteworthy [2]. These mod-\nels conceptualize emotions within a continuous dimensional\nspace, providing a framework that allows for a more nuanced\nquantification of emotional states.\nBCI technology, which decodes brain signals into com-\nmands for external devices, predominantly employs EEG\nsignals [3]. This technology processes EEG signals through\nstages of preprocessing, feature extraction, and classification,\nthereby translating thoughts into actionable commands and\nenhancing human-machine interaction [4]. EEG signals, re-\nflecting cortical voltage fluctuations, are crucial for emotion\nrecognition due to their objectivity, high temporal resolution,\naffordability, and rapid data acquisition [5].\nEEG signals have long been pivotal in neuroscience research\n[6]. In 2011, Koelstra et al. established the DEAP emotion\ndataset by collecting physiological signals associated with\nemotional responses [7]. Subsequently, in 2015, Zheng et\nal. developed the SEED dataset, providing well-annotated\nEEG signals from multiple subjects [8]. Both datasets have\nbeen instrumental in facilitating the development and eval-\nuation of computational models for emotion analysis. Over\nrecent decades, diverse machine learning and signal processing\ntechniques have been utilized for emotion recognition from\nphysiological signals.\nA significant advancement came in 2018 when Lawhern et\nal. introduced EEGNet, a compact convolutional neural net-\nwork (CNN) designed for efficient EEG signal processing [9].\nFurther advancements were made by Schirrmeister et al., who\nenhanced EEG decoding using deep learning with CNNs and\nprovided sophisticated visualization tools to better comprehend\nbrain activities [10]. Temporal convolutional networks (TCNs)\nhave also become a formidable alternative for real-time BCI\napplications. In 2020, Ingolfsson et al. developed EEG-TCNet,\ndemonstrating high accuracy in motor imagery tasks and sug-\ngesting that TCNs could surpass traditional methods in specific\nBCI applications [11]. This was supported by Musallam et\nal., who highlighted the versatility and efficiency of TCNs\nin complex BCI tasks through the fusion of TCN for motor\nimagery classification [12].\nAll the aforementioned research treated EEG signals as\ntwo-dimensional time-series data, with the dimensions being\nchannels and time [13]. Channels refer to EEG electrodes,\nwhich are strategically placed on the scalp to record brain\nactivity from various functional areas according to the 10-20\nsystem established by the International Federation of Clinical\nNeurophysiology [3]. In recent years, EEG data has been\ntreated as a graph where electrodes are positioned in a two-\ndimensional layout that reflects their physical placement on the\nscalp [14]. In this context, the raw data or extracted features\nfrom each electrode introduce a third dimension to this spatial\nmap [15].\nIn 2023, Ding et al. [16] introduced the Local-Global-\nGraph network (LGGNet), which processed EEG data in\nan image-like format and has shown enhanced classification\nperformance, although it was primarily designed for multi-\ntask classification and didn't significantly improve emotion\nrecognition accuracy compared to other models. In terms of\nEEG datasets, the availability of open-source, multi-modal\nEEG emotion datasets remains limited, with DEAP and SEED"}, {"title": "", "content": "being some of the few prominent resources available.\nTo address limitations in emotion recognition research, this\nstudy introduces the MEEG dataset, a multi-modal EEG emo-\ntion dataset inspired by the DEAP format but enhanced with\nmusic-induced emotional states for greater validity. Addition-\nally, the LGGNet framework has been refined by integrating\nan attention mechanism and utilizing DGNN, significantly\nimproving classification accuracy on the MEEG dataset. This\napproach facilitates a deeper exploration of interconnections\nbetween various brain functions through dynamic graph anal-\nsis.\nThe contributions of this study are threefold and can be\nsummarized as follows:\n1) We establish the MEEG dataset, a robust multi-modal\nEEG emotion dataset similar to DEAP but enhanced\nwith music stimuli to effectively induce emotional states.\nThis enhancement ensures higher ACC and F1 scores\ncompared to DEAP using the same models.\n2) We propose the AT-DGNN framework for exploring\nconnections within and between different functional\nareas of the brain. By integrating an attention mechanism\nand employing DGNN, the LGGNet architecture is\nsignificantly enhanced for emotion analysis tasks.\n3) We compare the performance of the proposed method\nwith CNN, TCN, and GNN-based SOTA methods on\nthe MEEG dataset. Extensive ablation experiments are\nconducted to better understand AT-DGNN."}, {"title": "II. RELATED WORK", "content": "A. Multi-head Attention Mechanism\nEEG data is inherently complex, characterized by temporal\ndependencies and discrete features. Utilizing multiple attention\nheads allows us to capture a diverse range of temporal pat-\nterns and dependencies, effectively identifying key temporal\nfeatures. In this module, the attention block consists of a multi-\nhead attention (MHA) layer with several self-attention heads.\nEach self-attention head comprises three main components:\nqueries (Q), keys (K), and values (V). These components in-\nteract to generate attention scores that determine the weighted\nvalues.The procedure begins with the normalization of the\ninput $X_w$ through a LayerNorm layer, a critical step for\nstabilizing the learning process:\n$q_{ht} = k_{ht} = v_{ht} = W_h @ LN(X_{w,t})$\nThis formulation demonstrates how queries, keys, and values\nare generated from the normalized data at each position t for\neach head h, thus aiding in the computation of attention scores.\nThe context vector for each head is computed as a weighted\nsum of the values, where weights are determined by softmax-\nnormalized alignment scores. These scores are derived using\nthe scaled dot-product attention mechanism:\n$\\alpha_{tt'}^h = \\frac{(q_{ht})^Tk_{ht'}}{\\sqrt{d_H}}$\nUltimately, the outputs from all the heads are concatenated\nand subjected to a linear transformation. This consolidated\noutput of the MHA layer is then merged with the initial\ninput via a residual connection and normalized, leading to an\noptimized attention output from the layer."}, {"title": "B. Graph Neural Networks", "content": "Graph neural networks (GNNs) represent a significant ad-\nvancement in the domain of neural networks, designed ex-\nplicitly to process graph-structured data. Unlike CNNs, GNNs\nexcel in capturing the intricate relationships and dependencies\namong nodes within a graph through processes of aggregation\nand propagation of information across local neighbors [17]. A\ntypical graph is denoted as $G = (V, E)$, where V symbolizes\nthe set of nodes and E the set of edges. Each node $v_i \\in V$\nand edge $e_{ij} = (v_i,v_j) \\in E$ can be respectively associated\nwith a node and an edge in the graph. The adjacency matrix\nA is configured as an n\u00d7n matrix where $A_{ij} = 1$ if $e_{ij} \\in E$\nand $A_{ij} = 0$ otherwise. Node attributes are represented by X,\nwhere $X \\in R^{n\u00d7d}$, with each $x_i \\in R^d$ denoting the feature\nvector of node $v_i$.\nDGNNs extend the capabilities of GNNs to address dynamic\nor time-evolving graph-structured data. In DGNNs, a graph\nat any given time t is represented as $G_t = (V_t, E_t)$, with\nits corresponding adjacency matrix $A_t$. This matrix changes\ndynamically as nodes and edges are added or removed over\ntime. Node features at time t are likewise dynamic, represented\nas $X_t$ with each row $x_{t,i} \\in R^d$ embodying the evolving feature\nset of node $v_i$.\nThe computational heart of DGNNs lies in the dynamic\nupdate rules, where node representations $h_{t,i}$ are recurrently\nupdated based on the temporal graph structure. A prevalent\napproach involves the temporal graph attention mechanism,\nwhere the new node states are computed as:\n$h_t^{(k+1)} = \\sigma(\\sum_{j \\in N(v_i)} a_{t,ij}^{(k)} (W^{(k)}h_{t,j}^{(k)} + b^{(k)}))$\nHere, $N(v_i)$ represents the neighbors of node $v_i$, $a_{t,ij}$ are\nthe attention coefficients indicating the significance of the\nfeatures of neighbor j to node i, and $W^{(k)}$ and $b^{(k)}$ are\ntrainable parameters of the k-th layer, with $\u03c3$ being a non-\nlinear activation function."}, {"title": "C. Graph Neural Networks for EEG", "content": "In 2019, Song et al. [18] first introduced a novel DGCNN\nspecifically for multichannel EEG emotion recognition, em-\nploying an adjacency matrix to dynamically model EEG\nchannel relationships and enhance feature discrimination. This\nmethod proved to be superior to existing approaches. Subse-\nquently, advancements in GNNs have been increasingly ap-\nplied to emotion recognition. On the SEED dataset, Bao et al.\n[19] integrated a multi-layer GNN with a style-reconfigurable\nCNN, while Asadzadeh et al. [20] further improved the\nDGCNN by incorporating Bayesian signal recovery tech-"}, {"title": "III. METHODOLOGY", "content": "In this section, we will introduce the detailed architecture\nof AT-DGNN from the three aspects of data preprocessing,\nfeature extraction and graph learning in turn, and the overall\nstructure of AT-DGNN is shown in Fig 1.\nA. EEG Data Preprocessing\nThe initial sampling rate of MEEG dataset is 1000 Hz.\nHowever, since most existing EEG datasets are sampled at\neither 128 Hz or 200 Hz, the data are downsampled to 200\nHz. This downsampling is crucial because the original EEG\ndata often present significant challenges, including high levels\nof artifact noise, low signal amplitudes, poor interference resis-\ntance, a limited frequency range, and overlapping interference\nsignals within the EEG band. Consequently, preprocessing\nof the raw EEG data is necessary before feature extraction\ncan commence. Typically, EEG signals are categorized into\nfive distinct frequency bands: Delta (1-4 Hz), Theta (4-8 Hz),\nAlpha (8-14 Hz), Beta (14-31 Hz), and Gamma (31-50 Hz)"}, {"title": "", "content": "[23]. Accordingly, a band-pass filter ranging from 1 to 50\nHz is applied to the EEG data to eliminate interference from\nextraneous factors.\nB. Feature Extraction\n1) Temporal Learner: We introduce a temporal learner\nlayer employing multiscale 1D temporal kernels (T kernels)\nthat directly extract dynamic temporal representations from\nEEG data $X_i \\in R^{E\u00d7T}$, where E denotes the number of\nEEG electrodes, and T is the sample length. These kernels\nobviate the need for manually extracted features. The kernels'\nlength, dictated by the EEG data's sampling frequency (fs)\nand scaling coefficients ($\u03b1^2$), is given by:\n$S_i = (1, \\alpha^i f_s), i \\in [1,2,3]$\nEEG data processed through these layers yield dynamic\ntime-frequency representations. An average pooling (AvgPool)\nlayer, acting as a window function, calculates the averaged\npower across shorter segments. Applying logarithmic trans-\nformations and squaring functions further refines the feature\nextraction, as described by Schirrmeister et al. [10]. The output\nfrom each layer i, denoted as $Z_{temp}^i$, is formulated as:\n$Z_{temp}^i = log(AvgPool(P_{square}(F_{Conv1-D}(X_i, S_i))))$"}, {"title": "", "content": "Here, $S_i$ indicates the kernel size, $\u03a6_{log}$ is the logarithmic\nactivation function, $P_{square}$ represents the square function, and\n$F_{Conv1-D}$ signifies the 1D convolution operation.\nThe outputs across all kernels are concatenated along the\nfeature dimension to yield the final output of the temporal\nlearner layer $Z_T$:\n$Z_T = f_{bn}(Z_{temp}^1, Z_{temp}^2, Z_{temp}^3)$\nwhere $f_{bn}$ denotes the batch normalization operation.\n2) Sliding Window Segmentation: After temporal learner,\nwe utilize a sliding window approach to segment the time\nseries into multiple windows, which effectively extracts local\nfeatures of EEG signals and augments the data. To mitigate\nthe computational overhead introduced by the sliding window\nstep, we employ the convolution-based sliding window method\nproposed by Schirrmeister et al[24]. This method integrates the\nsliding window approach with convolutional layers, allowing\nconvolution operations to be performed only once across all\nwindows, thereby reducing computation and inference time\nthrough parallel processing.\nWe employ a sliding window with a length of W and a\nstride of S to segment the time series X into multiple windows\n$X_w \\in R^{B\u00d7C\u00d7W}$, where w = 1,...,n denotes the window\nindex and n is the total number of windows. Each window\n$X_w$ is processed sequentially through subsequent attention and\ntemporal convolution blocks. The formula for calculating the\nnumber of windows n is:\n$n = \\frac{length - window\\_size}{stride}+1$\nwhere length is the length of the input sequence, window_size\nis the length of each window, and stride is the step size.\n3) Multi-head Attention Module: Each subsequence $X_w$\nderived from sliding window segmentation is first standard-\nized using a layer normalization (LayerNorm) layer. For the\nnormalized input $X_{norm}$, the MHA mechanism computes the\n$q_{ht}$, $k_{ht}$, and $v_{ht}$ as linear transformations:\n$\\begin{cases} q_{ht} = W_Q LN(X_{w,t}),\\\\ k_{ht} = W_K LN(X_{w,t}),\\\\ v_{ht} = W_V LN(X_{w,t}), \\end{cases}$\nwhere LN denotes layer normalization. The matrices $W_Q$,\n$W_K$, and $W_V$ belong to $R^{d\u00d7d_h}$, where $d_h$ represents the\ndimension of each attention head. The attention context vector\n$C_{ht}$ for each head is computed as a weighted sum of the\nvalues, using weights $\\alpha_{htt'}$ derived from the scaled dot-\nproduct attention mechanism.\nThe context vectors from all heads are concatenated and\nlinearly transformed to yield the final output of the MHA layer:\n$X_i = W^O [C_1,...,C_H] + X_w,$\nwhere $W^O \\in R^{dH\u00d7d}$ is the output projection matrix. This pro-\ncedure effectively projects the combined outputs back to the\noriginal input dimension. Subsequently, $X_i$ is integrated with\nthe original subsequence $X_w$ through a residual connection"}, {"title": "", "content": "and normalized once more. This advanced MHA framework\nsignificantly enhances the model's capacity to discern intri-\ncate temporal patterns and dependencies within EEG signals,\nthereby augmenting the precision of decoding activities.\n4) Temporal Convolution: Following the MHA mechanism,\nthe output, denoted as attn_output, is normalized with a Layer-\nNorm layer and merged with the sliding window data through\na residual connection to generate the EEG data $X_i \\in R^{cxl}$,\nwhere c is the number of EEG channels, and l represents the\nsample length in the temporal dimension.\nWe employ multiple temporal convolutional kernels oper-\nating in parallel to extract dynamic temporal features. Each\nkernel's output, $Z_{ten} \\in R^{txc\u00d7fk}$, where t denotes the total\nnumber of kernels and fk the feature length, acts as a digital\nfilter capturing signals across various frequency bands. The\naddition of batch normalization ensures training stability, and\nReLU activation introduces non-linearity.\nThe outputs from different kernels are concatenated along\nthe feature dimension, forming a multi-scale output $Z_{MS} \\in$\n$R^{txcxfk}$:\n$Z_{MS} = [Z_{ten}^1,..., Z_{ten}^i]$\nThis integration of the temporal convolution with the MHA\nmechanism captures local temporal dependencies effectively,\nenriching the feature representations and enhancing their dis-\ncriminative capabilities.\n5) Feature Fusion: After computing the outputs of the tem-\nporal learner for each window, denoted as $Z_{un} \\in R^{B\u00d7C\u00d7Tw}$,\nwhere B is the batch size, C represents the channel count, Tw\nis the window's temporal length, and w indexes the window,\nthese outputs are assembled into a four-dimensional tensor:\n$Z_{stacked} = [Z_{un}^1, Z_{un}^2,..., Z_{un}^W] \\in R^{B\u00d7C\u00d7W\u00d7Tw}$\nThis tensor is subsequently rearranged and flattened to con-\nform to the input requirements of a convolution layer, resulting\nin dimensions (B, 32, -1), where 32 refers to the fixed chan-\nnel count and -1 represents the flattened dimensions. A fusion\nconvolution layer, utilizing a kernel size of 3 and a stride\nof 1, integrates the outputs across all windows. The output,\n$Z_{fused} \\in R^{B\u00d732\u00d7L}$, where L is the combined length, processes\nthe temporal features to capture global contextual information\neffectively. This approach allows for a more discriminative\nrepresentation of features in EEG signals, which is crucial for\nconstructing nodes in GNNs."}, {"title": "C. Graph Learning", "content": "1) Graph Filtering Layer: For the extracted features, we\nadopt the method described by Ding et al.[16], which involves\ndefining functional areas and performing local filtering. EEG\ndata is divided into three functional areas: general region ($G_g$),\nfrontal region ($G_f$), and hemispheric region ($G_h$). Each elec-\ntrode is treated as a node, and the learned dynamic temporal\nrepresentations are considered as node attributes. According to\nthe 10-20 system, electrodes are grouped based on their scalp\nlocations, as Fig 2."}, {"title": "", "content": "In the preprocessing step, EEG channels are systematically\nreordered within predefined groups to ensure adjacency of\nchannels within each local graph. This reordering facilitates\nthe subsequent application of localized graph operations. The\nreordering function is mathematically defined as:\n$Z_{reorder}^{filtered} = F_{reorder}(Z_{fuse})$\nFollowing the reordering, a graph filtering layer is employed\nto aggregate the transformed representations and learn patterns\nindicative of local brain activity. The local adjacency matrix,\ndenoted as $A_{local}$, is initialized as a fully connected matrix\nwhere all elements are set to 1. The local graph filtering\noperation then applies a trainable filter matrix $W_{local}$ and a\nbias vector $b_{local}$ to the reordered channel representations:\n$Z_{filtered} = ReLU(W_{local} * Z_{reorder} - b_{local})$\nThe output of the filtering process $Z_{local}^{filtered}$ within each graph\nis subsequently aggregated to form localized feature repre-\nsentations. This aggregation is performed using the defined\naggregation function $F_{aggregate}$, resulting in the following vector\nrepresentation for each local graph:\n$Z_{local} = F_{aggregate} (Z_{local}^{filtered})$\n2) Stacked DGNN: We propose a novel stacked DGNN\narchitecture engineered to discern complex structural relation-\nships within graphs. Each layer of this model adaptively recal-\nculates the adjacency matrix using input features, facilitating\nthe effective capture of hierarchical feature variations across\nmultiple levels.\nThe architecture incorporates multiple GNN layers. In each\nlayer, features are grouped and aggregated into matrices, with\neach group containing k channels, resulting in an aggregated\nfeature matrix $Z_{agg}$. The adjacency matrix for each layer is\ndynamically computed based on the feature similarity matrix\nS, defined as:\n$S = Z_{agg}^T Z_{agg}$\nTo preserve node connectivity within the graph, self-loops\nare introduced by adding the identity matrix I to S, and the"}, {"title": "", "content": "Algorithm 1 AT-DGNN\n1: Input: EEG data $X_i \\in R^{E\u00d7T}$, ground truth label y; graph\ndefinitions $G_g, G_f, G_h$; global adjacency matrix $W_{global}$\n2: Output: pred, the prediction of AT-DGNN\n3: Initialization;\n4: for i  1 to 3 do\n5:\nget ith temporal kernel size by Eq. 4;\n6:\nget $Z_{temp}^i$ by Eq. 5 using $X_i$ as input;\n7: end for\n8: get $Z_T$ by Eq. 6;\n9: get the window size n and segment the data by Eq. 7\n10: get the output $X_i$ by Eq. 9 after MHA;\n11: concatenate the feature dimension by Eq. 10\n12: do feature fusion by Eq. 11 to get $Z_{fused}$;\n13: do graph filtering and aggregation on each node by Eq.\n12 - 14, get the output $Z_{local}$.\n14: for i  1 to 3 do\n15:\nget DGNN output by Eq. 17, Eq. 18 and Eq. 19;\n16: end for\n17: Get pred by Eq. 20;\n18: Return pred\nmodified adjacency matrix $\\tilde{A}$ is subsequently normalized as\nfollows:\n$A = S + I,  \\tilde{A} = \\tilde{D}^{-0.5} . A .  \\tilde{D}^{-0.5}$\nDuring forward propagation, the initial layer performs graph\nconvolution using the normalized adjacency matrix $\\tilde{A}^{(1)}$,\nweight matrix $W^{(1)}$, and bias vector $b^{(1)}$. The output from\nthis layer is given by:\n$H^{(1)} = ReLU(\\tilde{A}^{(1)}. Z_{agg} W^{(1)}+b^{(1)})$\nSubsequent layers l = 2, . . ., L - 1 process the output of\nthe previous layer $H^{(l-1)}$ with corresponding layer-specific\nparameters $\\tilde{A}^{(1)}$, $W^{(1)}$, and $b^{(1)}$, to produce:\n$H^{(1)} = ReLU(\\tilde{A}^{(1)} . H^{(l-1)} . W^{(l)} + b^{(l)})$\nThe final output $H^{(L)}$ from the last hidden layer $H^{(L-1)}$\nutilizes the parameters of the ultimate layer $\\tilde{A}^{(L)}$, $W^{(L)}$, and\n$b^{(L)}$:\n$H^{(L)} = ReLU(\\tilde{A}^{(L)} . H^{(L-1)} . W^{(L)} + b^{(L)})$\nThe final network output is computed through a sequence\nof operations including batch normalization, dropout, and\nsoftmax activation, structured as:\n$Output = softmax (F_{dropout}(F(F_{bn}(H^{(L)}))))$\nwhere \u0393(\u00b7) denotes the flattening operation, ensuring that\nthe network output is appropriately structured for subsequent\nprocessing or analysis."}, {"title": "", "content": "This structured approach not only ensures detailed feature\nextraction at various levels but also enhances the model's\nability to learn and generalize from complex graph structures.\nFinally, the procedure of AT-DGNN can be summarized in\nAlgorithm 1."}, {"title": "IV. EXPERIMENTS", "content": "A. MEEG Dataset\nIn studies on emotion induction, visual stimuli such as\nimages, videos, and text are commonly employed in ex-\nperiments; however, participants' responses to these stimuli\nmay be influenced by their cultural backgrounds. Consider-\ning the auditory cortex exhibits emotion-specific functional\nconnections with a wide array of limbic, paralimbic, and\nneocortical structures, suggesting a more extensive role in\nemotion processing than previously understood [25], we opted\nto induce emotions in participants using music in the MEEG\ndataset. We carefully selected 20 music clips, each lasting one\nminute, characterized by distinct arousal and valence levels,\ndecidedly non-neutral. To minimize emotional carryover, a 15-\nsecond pause was introduced between clips [26]. The chosen\nmusic pieces included lesser-known works of Western classical\nmusic by composers such as Shostakovich and Tchaikovsky.\nThe study involved 32 students from Shandong University,\naged between 20 and 25 years, who had no formal music\neducation and were unfamiliar with Western classical music.\nEach participant signed an informed consent form prior to\nthe study. This experimental design aimed to minimize the\ninfluence of social and cultural backgrounds on emotional\nresponses, ensuring that changes in emotions were solely\ndue to musical stimuli, thereby enhancing the accuracy of\nthe experimental results. EEG data were collected using a\n32-channel BCI device NeuSen.W32 from Neuracle Tech,\nemploying the same electrode channels as in the DEAP dataset\n[7]. The data were sampled at a frequency of 1000 Hz. The\nEEG data were annotated based on the arousal and valence of\nthe music during various stages of the experiment."}, {"title": "B. Experiment Settings", "content": "To rigorously evaluate the model's performance, a nested\ncross-validation approach is employed, featuring a trial-wise\n10-fold cross-validation in the outer loop and a 4-fold cross-\nvalidation in the inner loop, as suggested by Varma [27].\nThis stratified sampling technique ensures robustness and\ngeneralization of the model by assessing its accuracy and\nreliability across diverse samples.\nAdditionally, a two-stage training strategy within the inner\nloop optimizes the utilization of training data. Initially, the\nbest model identified from the k-fold cross-validation is saved\nas a preliminary candidate. This model is then refined using\ncombined training data from the k folds, fine-tuned at a\nreduced learning rate to prevent overfitting, and further trained\nfor up to 20 epochs or until it achieves a training accuracy of\n100%, ensuring precise calibration. Importantly, test data is\nexcluded from the training process to maintain the integrity of\nthe evaluation.\nThe integration of two-stage training with 10-fold cross-\nvalidation minimizes variability in assessment results, pro-\nviding a comprehensive and reliable evaluation framework\nsuitable for diverse research and application domains."}, {"title": "C. Implement Details", "content": "The model was implemented using PyTorch [28] library,\nand the code can be found at https://github.com/xmh1011/AT-\nDGNN.\nCross-entropy loss was chosen as the objective function to\nguide the training process. The training was divided into two\nstages, with the first stage capped at 200 epochs and the second\nat 20 epochs. The batch size was set to 64. To reduce training\ntime and prevent overfitting, early stopping was implemented.\nFor the attention module, the window size was set to half\nthe data frequency. The kernel sizes for the time learner were\nset to 100, 50, and 25. The hidden layer count for the DGNN\nwas established at 32. Training was optimized using the Adam\noptimizer, starting with an initial learning rate of 1e-3, which\nwas reduced by a factor of 10 during the second stage. For\nmore details, please refer to the open-source GitHub repository\nfor AT-DGNN."}, {"title": "V. RESULTS AND DISCUSSION", "content": "In this section, we compare the average accuracy and F1\nscore of AT-DGNN on the MEEG dataset with CNN, TCN,\nand GNN-based SOTA methods in the BCI domain. The\nCNN-based methods include: EEGNet [9], TSception [29],\nDeepConvNet and ShallowConvNet [10]. The TCN-based\nmethods include: EEGTCNet [11], TCNet-Fusion [12] and\nATCNet [30]. The GNN-based methods include: DGCNN [18]\nand LGGNet [16]. Due to the smaller size of the CNN-based\nmodels, the learning rate and the number of training epochs\nare reduced to avoid overfitting. For the other models, the pa-\nrameters recommended by the authors are used. Additionally,\nablation studies are conducted to reveal the contribution of\neach component within the AT-DGNN architecture.\nA. Emotion Recognition\nTable I lists the results of our proposed method compared\nto other SOTA methods on the MEEG dataset. The AT-DGNN\nseries significantly outperforms the LGGNet series across both\narousal and valence dimensions. Specifically, the AT-DGNN-Gen model achieve the highest F1 score in the valence dimen-sion at 85.89%, which represents an improvement of 3.42% over the highest score of the LGGNet series (82.47%). In the arousal dimension, AT-DGNN-Fro leads with an accuracy of 84.88%, outperforming the best LGGNet result by 2.54%. The AT-DGNN models also demonstrate greater consistency with the smallest standard deviations, indicating robustness in the challenging and class-imbalanced MEEG dataset.\nCompared to traditional CNN-based methods and other machine learning techniques, the AT-DGNN series show sub-stantial enhancements. Improvements in F1 scores are up to 17.61% higher, and accuracy gains are as much as 9.66% better, highlighting the effectiveness of the DGNN approach."}, {"title": "", "content": "These improvements underscore the capability of AT-DGNN\nmodels to better capture complex patterns and dependencies\nin graph-structured data, thus providing more accurate and\nreliable performance across varied testing conditions.\nThe above methods all achieve relatively high accuracy\nrates on the MEEG dataset, with CNN-based models reaching\naround 75% accuracy, or even higher. In contrast, GNN-based\nmodels have accuracy rates above 80%. According to a study\nby Huang et al. [31] in 2023, the same models achieve about\n60% accuracy on the DEEP dataset, which is significantly\nlower than on the MEEG dataset. This also demonstrates\nthat music, compared to video, is more effective in evoking\nemotions in subjects.\nB. Ablation Study"}, {"title": "VI. CONCLUSION", "content": "This study introduces the MEEG dataset and the AT-DGNN\nframework as significant advancements in EEG-based emotion\nrecognition. The MEEG dataset, which uses music to induce\nemotional states, provides a valuable resource for studying\nbrain responses to emotional stimuli. The AT-DGNN frame-\nwork, integrating attention mechanisms with DGNN, excels\nin capturing complex temporal brain dynamics, significantly\nimproving emotion recognition accuracy compared to other\nSOTA methods. These innovations advance brain-computer\ninterface technologies and open new avenues for exploring\nthe relationship between music, emotion, and brain activity.\nThe superior performance of AT-DGNN, validated against\ntraditional models, underscores its potential for enhancing\nemotion analysis."}]}