{"title": "MEEG and AT-DGNN: Advancing EEG Emotion Recognition with Music and Graph Learning", "authors": ["Minghao Xiao", "Zhengxi Zhu", "Wenyu Wang", "Meixia Qu"], "abstract": "Recent advances in neuroscience have elucidated the crucial role of coordinated brain region activities during cognitive tasks. To explore this complexity, we introduce the MEEG dataset-a comprehensive multi-modal music-induced electroencephalogram (EEG) dataset and the Attention-based Temporal learner with Dynamic Graph Neural Network (AT-DGNN), a novel framework for EEG-based emotion recognition. The MEEG dataset captures a wide range of emotional responses to music, enabling an in-depth analysis of brainwave patterns in musical contexts. The AT-DGNN combines an attention-based temporal learner with a dynamic graph neural network (DGNN) to accurately model the local and global graph dynamics of EEG data across varying brain network topologies. Our evaluations show that AT-DGNN achieves superior performance, with accuracies (ACC) of 83.06% in arousal and 85.31% in valence, outperforming state-of-the-art (SOTA) methods on the MEEG dataset. Comparative analyses with traditional datasets like DEAP highlight the effectiveness of our approach and under-score the potential of music as a powerful medium for emotion induction. This study not only advances our understanding of the brain's emotional processing but also enhances the accurcy of emotion recognition technologies in brain-computer interfaces (BCI), leveraging both graph-based learning and the emotional impact of music.", "sections": [{"title": "I. INTRODUCTION", "content": "Emotion, a complex physiological and psychological phe-nomenon, plays a critical role in interpreting genuine reactions to daily interactions [1]. In 1980, Russell introduced the dimensional models of emotion, with the Valence-Arousal (VA) model being particularly noteworthy [2]. These mod-els conceptualize emotions within a continuous dimensional space, providing a framework that allows for a more nuanced quantification of emotional states.\nBCI technology, which decodes brain signals into com-mands for external devices, predominantly employs EEG signals [3]. This technology processes EEG signals through stages of preprocessing, feature extraction, and classification, thereby translating thoughts into actionable commands and enhancing human-machine interaction [4]. EEG signals, re-flecting cortical voltage fluctuations, are crucial for emotion recognition due to their objectivity, high temporal resolution, affordability, and rapid data acquisition [5].\nEEG signals have long been pivotal in neuroscience research [6]. In 2011, Koelstra et al. established the DEAP emotion dataset by collecting physiological signals associated with emotional responses [7]. Subsequently, in 2015, Zheng et al. developed the SEED dataset, providing well-annotated EEG signals from multiple subjects [8]. Both datasets have been instrumental in facilitating the development and eval-uation of computational models for emotion analysis. Over recent decades, diverse machine learning and signal processing techniques have been utilized for emotion recognition from physiological signals.\nA significant advancement came in 2018 when Lawhern et al. introduced EEGNet, a compact convolutional neural net-work (CNN) designed for efficient EEG signal processing [9]. Further advancements were made by Schirrmeister et al., who enhanced EEG decoding using deep learning with CNNs and provided sophisticated visualization tools to better comprehend brain activities [10]. Temporal convolutional networks (TCNs) have also become a formidable alternative for real-time BCI applications. In 2020, Ingolfsson et al. developed EEG-TCNet, demonstrating high accuracy in motor imagery tasks and sug-gesting that TCNs could surpass traditional methods in specific BCI applications [11]. This was supported by Musallam et al., who highlighted the versatility and efficiency of TCNs in complex BCI tasks through the fusion of TCN for motor imagery classification [12].\nAll the aforementioned research treated EEG signals as two-dimensional time-series data, with the dimensions being channels and time [13]. Channels refer to EEG electrodes, which are strategically placed on the scalp to record brain activity from various functional areas according to the 10-20 system established by the International Federation of Clinical Neurophysiology [3]. In recent years, EEG data has been treated as a graph where electrodes are positioned in a two-dimensional layout that reflects their physical placement on the scalp [14]. In this context, the raw data or extracted features from each electrode introduce a third dimension to this spatial map [15].\nIn 2023, Ding et al. [16] introduced the Local-Global-Graph network (LGGNet), which processed EEG data in an image-like format and has shown enhanced classification performance, although it was primarily designed for multi-task classification and didn't significantly improve emotion recognition accuracy compared to other models. In terms of EEG datasets, the availability of open-source, multi-modal EEG emotion datasets remains limited, with DEAP and SEED being some of the few prominent resources available.\nTo address limitations in emotion recognition research, this study introduces the MEEG dataset, a multi-modal EEG emo-tion dataset inspired by the DEAP format but enhanced with music-induced emotional states for greater validity. Addition-ally, the LGGNet framework has been refined by integrating an attention mechanism and utilizing DGNN, significantly improving classification accuracy on the MEEG dataset. This approach facilitates a deeper exploration of interconnections between various brain functions through dynamic graph anal-ysis.\nThe contributions of this study are threefold and can be summarized as follows:\n1) We establish the MEEG dataset, a robust multi-modal EEG emotion dataset similar to DEAP but enhanced with music stimuli to effectively induce emotional states. This enhancement ensures higher ACC and F1 scores compared to DEAP using the same models.\n2) We propose the AT-DGNN framework for exploring connections within and between different functional areas of the brain. By integrating an attention mechanism and employing DGNN, the LGGNet architecture is significantly enhanced for emotion analysis tasks.\n3) We compare the performance of the proposed method with CNN, TCN, and GNN-based SOTA methods on the MEEG dataset. Extensive ablation experiments are conducted to better understand AT-DGNN."}, {"title": "II. RELATED WORK", "content": "EEG data is inherently complex, characterized by temporal dependencies and discrete features. Utilizing multiple attention heads allows us to capture a diverse range of temporal pat-terns and dependencies, effectively identifying key temporal features. In this module, the attention block consists of a multi-head attention (MHA) layer with several self-attention heads. Each self-attention head comprises three main components: queries (Q), keys (K), and values (V). These components in-teract to generate attention scores that determine the weighted values.The procedure begins with the normalization of the input $X_w$ through a LayerNorm layer, a critical step for stabilizing the learning process:\n$q_{ht} = k_{ht} = v_{ht} = W@LN(X_{w,t})$\nThis formulation demonstrates how queries, keys, and values are generated from the normalized data at each position t for each head h, thus aiding in the computation of attention scores.\nThe context vector for each head is computed as a weighted sum of the values, where weights are determined by softmax-normalized alignment scores. These scores are derived using the scaled dot-product attention mechanism:\n$C_{htt'} = \\frac{(q_{ht})^Tk_{ht'}}{\\sqrt{d_H}}$\nUltimately, the outputs from all the heads are concatenated and subjected to a linear transformation. This consolidated output of the MHA layer is then merged with the initial input via a residual connection and normalized, leading to an optimized attention output from the layer."}, {"title": "B. Graph Neural Networks", "content": "Graph neural networks (GNNs) represent a significant ad-vancement in the domain of neural networks, designed ex-plicitly to process graph-structured data. Unlike CNNs, GNNs excel in capturing the intricate relationships and dependencies among nodes within a graph through processes of aggregation and propagation of information across local neighbors [17]. A typical graph is denoted as $G = (V, E)$, where V symbolizes the set of nodes and E the set of edges. Each node $v_i \\in V$ and edge $e_{ij} = (v_i,v_j) \\in E$ can be respectively associated with a node and an edge in the graph. The adjacency matrix A is configured as an $n \\times n$ matrix where $A_{ij} = 1$ if $e_{ij} \\in E$ and $A_{ij} = 0$ otherwise. Node attributes are represented by X, where $X \\in R^{n \\times d}$, with each $x_i \\in R^d$ denoting the feature vector of node $v_i$.\nDGNNs extend the capabilities of GNNs to address dynamic or time-evolving graph-structured data. In DGNNs, a graph at any given time t is represented as $G_t = (V_t, E_t)$, with its corresponding adjacency matrix $A_t$. This matrix changes dynamically as nodes and edges are added or removed over time. Node features at time t are likewise dynamic, represented as $X_t$ with each row $x_{t,i} \\in R^d$ embodying the evolving feature set of node $v_i$.\nThe computational heart of DGNNs lies in the dynamic update rules, where node representations $h_{t,i}$ are recurrently updated based on the temporal graph structure. A prevalent approach involves the temporal graph attention mechanism, where the new node states are computed as:\n$h_i^{(k+1)} = \\sigma(\\sum_{j \\in N(v_i)} a_{t,ij}^{(k)} (W^{(k)} h_{t,j}^{(k)} + b^{(k)}))$\nHere, $N(v_i)$ represents the neighbors of node $v_i$, $a_{t,ij}$ are the attention coefficients indicating the significance of the features of neighbor j to node i, and $W^{(k)}$ and $b^{(k)}$ are trainable parameters of the k-th layer, with $\\sigma$ being a non-linear activation function."}, {"title": "C. Graph Neural Networks for EEG", "content": "In 2019, Song et al. [18] first introduced a novel DGCNN specifically for multichannel EEG emotion recognition, em-ploying an adjacency matrix to dynamically model EEG channel relationships and enhance feature discrimination. This method proved to be superior to existing approaches. Subse-quently, advancements in GNNs have been increasingly ap-plied to emotion recognition. On the SEED dataset, Bao et al. [19] integrated a multi-layer GNN with a style-reconfigurable CNN, while Asadzadeh et al. [20] further improved the DGCNN by incorporating Bayesian signal recovery tech-niques, both achieving enhanced performance. For the DEAP dataset, Zhao et al. [21] developed coherence-based node clus-tering and multi-pooling techniques for GCNs. Additionally, Zhang et al. [22] enhanced GCN feature extraction by using Granger causality and differential entropy to construct more informative connectivity matrices."}, {"title": "III. METHODOLOGY", "content": "In this section, we will introduce the detailed architecture of AT-DGNN from the three aspects of data preprocessing, feature extraction and graph learning in turn, and the overall structure of AT-DGNN is shown in Fig 1.\nThe initial sampling rate of MEEG dataset is 1000 Hz. However, since most existing EEG datasets are sampled at either 128 Hz or 200 Hz, the data are downsampled to 200 Hz. This downsampling is crucial because the original EEG data often present significant challenges, including high levels of artifact noise, low signal amplitudes, poor interference resis-tance, a limited frequency range, and overlapping interference signals within the EEG band. Consequently, preprocessing of the raw EEG data is necessary before feature extraction can commence. Typically, EEG signals are categorized into five distinct frequency bands: Delta (1-4 Hz), Theta (4-8 Hz), Alpha (8-14 Hz), Beta (14-31 Hz), and Gamma (31-50 Hz) [23]. Accordingly, a band-pass filter ranging from 1 to 50 Hz is applied to the EEG data to eliminate interference from extraneous factors."}, {"title": "B. Feature Extraction", "content": "We introduce a temporal learner layer employing multiscale 1D temporal kernels (T kernels) that directly extract dynamic temporal representations from EEG data $X_i \\in R^{E \\times T}$, where E denotes the number of EEG electrodes, and T is the sample length. These kernels obviate the need for manually extracted features. The kernels' length, dictated by the EEG data's sampling frequency (fs) and scaling coefficients (a\u00b2), is given by:\n$S = (1, a^i f_s), i\\in [1,2,3]$\nEEG data processed through these layers yield dynamic time-frequency representations. An average pooling (AvgPool) layer, acting as a window function, calculates the averaged power across shorter segments. Applying logarithmic trans-formations and squaring functions further refines the feature extraction, as described by Schirrmeister et al. [10]. The output from each layer i, denoted as $Z^{temp}_i$, is formulated as:\n$Z^{temp}_i = \\log (AvgPool(P_{square}(F_{Conv1-D}(X_i, S_i))))$"}, {"title": "Sliding Window Segmentation", "content": "After temporal learner, we utilize a sliding window approach to segment the time series into multiple windows, which effectively extracts local features of EEG signals and augments the data. To mitigate the computational overhead introduced by the sliding window step, we employ the convolution-based sliding window method proposed by Schirrmeister et al[24]. This method integrates the sliding window approach with convolutional layers, allowing convolution operations to be performed only once across all windows, thereby reducing computation and inference time through parallel processing.\nWe employ a sliding window with a length of W and a stride of S to segment the time series X into multiple windows $X_w \\in R^{B \\times C \\times W}$, where w = 1,...,n denotes the window index and n is the total number of windows. Each window $X_w$ is processed sequentially through subsequent attention and temporal convolution blocks. The formula for calculating the number of windows n is:\n$n=\\frac{length - window\\_size}{stride} +1$\nwhere length is the length of the input sequence, window_size is the length of each window, and stride is the step size."}, {"title": "Multi-head Attention Module", "content": "Each subsequence $X_w$ derived from sliding window segmentation is first standard-ized using a layer normalization (LayerNorm) layer. For the normalized input $X_{norm}$, the MHA mechanism computes the $q_{ht}, k_{ht}, and u_{ht}$ as linear transformations:\n$\\begin{cases}q_{ht} = W^Q LN(X_{w,t}),\\\\k_{ht} = W^K LN(X_{w,t}),\\\\U_{ht} = W^V LN(X_{w,t}),\\end{cases}$\nwhere LN denotes layer normalization. The matrices $W^Q$, $W^K$, and $W^V$ belong to $R^{d \\times d_h}$, where $d_H$ represents the dimension of each attention head. The attention context vector $C_{ht}$ for each head is computed as a weighted sum of the values, using weights $a_{htt'}$ derived from the scaled dot-product attention mechanism.\nThe context vectors from all heads are concatenated and linearly transformed to yield the final output of the MHA layer:\n$X_1 = W^O [C_1,...,C_H] + X_w,$\nwhere $W^O \\in R^{d_H \\times d}$ is the output projection matrix. This pro-cedure effectively projects the combined outputs back to the original input dimension. Subsequently, $X_1$ is integrated with the original subsequence $X_w$ through a residual connection and normalized once more. This advanced MHA framework significantly enhances the model's capacity to discern intri-cate temporal patterns and dependencies within EEG signals, thereby augmenting the precision of decoding activities."}, {"title": "Temporal Convolution", "content": "Following the MHA mechanism, the output, denoted as attn_output, is normalized with a Layer-Norm layer and merged with the sliding window data through a residual connection to generate the EEG data $X_1 \\in R^{c \\times l}$, where c is the number of EEG channels, and I represents the sample length in the temporal dimension.\nWe employ multiple temporal convolutional kernels oper-ating in parallel to extract dynamic temporal features. Each kernel's output, $Z^{ten} \\in R^{t \\times c \\times f_k}$, where t denotes the total number of kernels and $f_k$ the feature length, acts as a digital filter capturing signals across various frequency bands. The addition of batch normalization ensures training stability, and ReLU activation introduces non-linearity.\nThe outputs from different kernels are concatenated along the feature dimension, forming a multi-scale output $Z^{MS} \\in R^{t \\times c \\times f_k}$:\n$Z^{MS} = [Z_{1}^{ten},..., Z_{t}^{ten}]$\nThis integration of the temporal convolution with the MHA mechanism captures local temporal dependencies effectively, enriching the feature representations and enhancing their dis-criminative capabilities."}, {"title": "Feature Fusion", "content": "After computing the outputs of the tem-poral learner for each window, denoted as $Z^{un} \\in R^{B \\times C \\times T_w}$, where B is the batch size, C represents the channel count, $T_w$ is the window's temporal length, and w indexes the window, these outputs are assembled into a four-dimensional tensor:\n$Z^{stacked} = [Z_{1}^{un}, Z_{2}^{un},..., Z_{W}^{un}] \\in R^{B \\times C \\times W \\times T_w}$\nThis tensor is subsequently rearranged and flattened to con-form to the input requirements of a convolution layer, resulting in dimensions (B, 32, -1), where 32 refers to the fixed chan-nel count and -1 represents the flattened dimensions. A fusion convolution layer, utilizing a kernel size of 3 and a stride of 1, integrates the outputs across all windows. The output, $Z^{fused} \\in R^{B \\times 32 \\times L}$, where L is the combined length, processes the temporal features to capture global contextual information effectively. This approach allows for a more discriminative representation of features in EEG signals, which is crucial for constructing nodes in GNNs."}, {"title": "C. Graph Learning", "content": "For the extracted features, we adopt the method described by Ding et al.[16], which involves defining functional areas and performing local filtering. EEG data is divided into three functional areas: general region (Gg), frontal region (Gf), and hemispheric region (Gh). Each elec-trode is treated as a node, and the learned dynamic temporal representations are considered as node attributes. According to the 10-20 system, electrodes are grouped based on their scalp locations, as Fig 2."}, {"title": "Graph Filtering Layer", "content": "In the preprocessing step, EEG channels are systematically reordered within predefined groups to ensure adjacency of channels within each local graph. This reordering facilitates the subsequent application of localized graph operations. The reordering function is mathematically defined as:\n$Z^{reorder} = F_{reorder}(Z^{fuse})$\nFollowing the reordering, a graph filtering layer is employed to aggregate the transformed representations and learn patterns indicative of local brain activity. The local adjacency matrix, denoted as $A_{local}$, is initialized as a fully connected matrix where all elements are set to 1. The local graph filtering operation then applies a trainable filter matrix $W_{local}$ and a bias vector $b_{local}$ to the reordered channel representations:\n$Z^{filtered} = ReLU(W_{local} \\cdot Z^{reorder} - b_{local})$\nThe output of the filtering process $Z^{filtered}$ within each graph is subsequently aggregated to form localized feature repre-sentations. This aggregation is performed using the defined aggregation function $F_{aggregate}$, resulting in the following vector representation for each local graph:\n$Z^{local} = F_{aggregate}(Z^{filtered})$"}, {"title": "Stacked DGNN", "content": "We propose a novel stacked DGNN architecture engineered to discern complex structural relation-ships within graphs. Each layer of this model adaptively recal-culates the adjacency matrix using input features, facilitating the effective capture of hierarchical feature variations across multiple levels.\nThe architecture incorporates multiple GNN layers. In each layer, features are grouped and aggregated into matrices, with each group containing k channels, resulting in an aggregated feature matrix $Z_{agg}$. The adjacency matrix for each layer is dynamically computed based on the feature similarity matrix S, defined as:\n$S = Z_{agg} Z_{agg}^T$\nTo preserve node connectivity within the graph, self-loops are introduced by adding the identity matrix I to S, and the modified adjacency matrix $\\tilde{A}$ is subsequently normalized as follows:\n$\\tilde{A} = S + I, \\tilde{A} = \\tilde{D}^{-0.5} \\cdot A \\cdot \\tilde{D}^{-0.5}$\nDuring forward propagation, the initial layer performs graph convolution using the normalized adjacency matrix $\\tilde{A}^{(1)}$, weight matrix $W^{(1)}$, and bias vector $b^{(1)}$. The output from this layer is given by:\n$H^{(1)} = ReLU(\\tilde{A}^{(1)} \\cdot Z_{agg} W^{(1)} + b^{(1)})$\nSubsequent layers l = 2, . . ., L - 1 process the output of the previous layer $H^{(l-1)}$ with corresponding layer-specific parameters $\\tilde{A}^{(l)}$, $W^{(l)}$, and $b^{(l)}$, to produce:\n$H^{(l)} = ReLU(\\tilde{A}^{(l)} \\cdot H^{(l-1)} \\cdot W^{(l)} + b^{(l)})$\nThe final output $H^{(L)}$ from the last hidden layer $H^{(L-1)}$ utilizes the parameters of the ultimate layer $\\tilde{A}^{(L)}$, $W^{(L)}$, and $b^{(L)}$:\n$H^{(L)} = ReLU(\\tilde{A}^{(L)} \\cdot H^{(L-1)} \\cdot W^{(L)} + b^{(L)})$\nThe final network output is computed through a sequence of operations including batch normalization, dropout, and softmax activation, structured as:\n$Output = softmax (F_{dropout}(F_{flatten}(F_{BN}(H^{(L)})))$,\nwhere $\\Gamma(\\cdot)$ denotes the flattening operation, ensuring that the network output is appropriately structured for subsequent processing or analysis."}, {"title": "IV. EXPERIMENTS", "content": "In studies on emotion induction, visual stimuli such as images, videos, and text are commonly employed in ex-periments; however, participants' responses to these stimuli may be influenced by their cultural backgrounds. Consider-ing the auditory cortex exhibits emotion-specific functional connections with a wide array of limbic, paralimbic, and neocortical structures, suggesting a more extensive role in emotion processing than previously understood [25], we opted to induce emotions in participants using music in the MEEG dataset. We carefully selected 20 music clips, each lasting one minute, characterized by distinct arousal and valence levels, decidedly non-neutral. To minimize emotional carryover, a 15-second pause was introduced between clips [26]. The chosen music pieces included lesser-known works of Western classical music by composers such as Shostakovich and Tchaikovsky. The study involved 32 students from Shandong University, aged between 20 and 25 years, who had no formal music education and were unfamiliar with Western classical music. Each participant signed an informed consent form prior to the study. This experimental design aimed to minimize the influence of social and cultural backgrounds on emotional responses, ensuring that changes in emotions were solely due to musical stimuli, thereby enhancing the accuracy of the experimental results. EEG data were collected using a 32-channel BCI device NeuSen.W32 from Neuracle Tech, employing the same electrode channels as in the DEAP dataset [7]. The data were sampled at a frequency of 1000 Hz. The EEG data were annotated based on the arousal and valence of the music during various stages of the experiment."}, {"title": "B. Experiment Settings", "content": "To rigorously evaluate the model's performance, a nested cross-validation approach is employed, featuring a trial-wise 10-fold cross-validation in the outer loop and a 4-fold cross-validation in the inner loop, as suggested by Varma [27]. This stratified sampling technique ensures robustness and generalization of the model by assessing its accuracy and reliability across diverse samples.\nAdditionally, a two-stage training strategy within the inner loop optimizes the utilization of training data. Initially, the best model identified from the k-fold cross-validation is saved as a preliminary candidate. This model is then refined using combined training data from the k folds, fine-tuned at a reduced learning rate to prevent overfitting, and further trained for up to 20 epochs or until it achieves a training accuracy of 100%, ensuring precise calibration. Importantly, test data is excluded from the training process to maintain the integrity of the evaluation.\nThe integration of two-stage training with 10-fold cross-validation minimizes variability in assessment results, pro-viding a comprehensive and reliable evaluation framework suitable for diverse research and application domains."}, {"title": "C. Implement Details", "content": "The model was implemented using PyTorch [28] library, and the code can be found at https://github.com/xmh1011/AT-DGNN.\nCross-entropy loss was chosen as the objective function to guide the training process. The training was divided into two stages, with the first stage capped at 200 epochs and the second at 20 epochs. The batch size was set to 64. To reduce training time and prevent overfitting, early stopping was implemented. For the attention module, the window size was set to half the data frequency. The kernel sizes for the time learner were set to 100, 50, and 25. The hidden layer count for the DGNN was established at 32. Training was optimized using the Adam optimizer, starting with an initial learning rate of 1e-3, which was reduced by a factor of 10 during the second stage. For more details, please refer to the open-source GitHub repository for AT-DGNN."}, {"title": "V. RESULTS AND DISCUSSION", "content": "In this section, we compare the average accuracy and F1 score of AT-DGNN on the MEEG dataset with CNN, TCN, and GNN-based SOTA methods in the BCI domain. The CNN-based methods include: EEGNet [9], TSception [29], DeepConvNet and ShallowConvNet [10]. The TCN-based methods include: EEGTCNet [11], TCNet-Fusion [12] and ATCNet [30]. The GNN-based methods include: DGCNN [18] and LGGNet [16]. Due to the smaller size of the CNN-based models, the learning rate and the number of training epochs are reduced to avoid overfitting. For the other models, the pa-rameters recommended by the authors are used. Additionally, ablation studies are conducted to reveal the contribution of each component within the AT-DGNN architecture."}, {"title": "A. Emotion Recognition", "content": "Table I lists the results of our proposed method compared to other SOTA methods on the MEEG dataset. The AT-DGNN series significantly outperforms the LGGNet series across both arousal and valence dimensions. Specifically, the AT-DGNN-Gen model achieve the highest F1 score in the valence dimen-sion at 85.89%, which represents an improvement of 3.42% over the highest score of the LGGNet series (82.47%). In the arousal dimension, AT-DGNN-Fro leads with an accuracy of 84.88%, outperforming the best LGGNet result by 2.54%. The AT-DGNN models also demonstrate greater consistency with the smallest standard deviations, indicating robustness in the challenging and class-imbalanced MEEG dataset.\nCompared to traditional CNN-based methods and other machine learning techniques, the AT-DGNN series show sub-stantial enhancements. Improvements in F1 scores are up to 17.61% higher, and accuracy gains are as much as 9.66% better, highlighting the effectiveness of the DGNN approach.\nThese improvements underscore the capability of AT-DGNN models to better capture complex patterns and dependencies in graph-structured data, thus providing more accurate and reliable performance across varied testing conditions.\nThe above methods all achieve relatively high accuracy rates on the MEEG dataset, with CNN-based models reaching around 75% accuracy, or even higher. In contrast, GNN-based models have accuracy rates above 80%. According to a study by Huang et al. [31] in 2023, the same models achieve about 60% accuracy on the DEEP dataset, which is significantly lower than on the MEEG dataset. This also demonstrates that music, compared to video, is more effective in evoking emotions in subjects."}, {"title": "VI. CONCLUSION", "content": "This study introduces the MEEG dataset and the AT-DGNN framework as significant advancements in EEG-based emotion recognition. The MEEG dataset, which uses music to induce emotional states, provides a valuable resource for studying brain responses to emotional stimuli. The AT-DGNN frame-work, integrating attention mechanisms with DGNN, excels in capturing complex temporal brain dynamics, significantly improving emotion recognition accuracy compared to other SOTA methods. These innovations advance brain-computer interface technologies and open new avenues for exploring the relationship between music, emotion, and brain activity. The superior performance of AT-DGNN, validated against traditional models, underscores its potential for enhancing emotion analysis."}]}