{"title": "AUTOPENBENCH: BENCHMARKING GENERATIVE AGENTS\nFOR PENETRATION TESTING", "authors": ["Luca Gioacchini", "Marco Mellia", "Idilio Drago", "Alexander Delsanto", "Giuseppe Siracusano", "Roberto Bifulco"], "abstract": "Generative AI agents, software systems powered by Large Language Models (LLMs), are emerging\nas a promising approach to automate cybersecurity tasks. Among the others, penetration testing is\na challenging field due to the task complexity and the diverse set of strategies to simulate cyber-\nattacks. Despite growing interest and initial studies in automating penetration testing with generative\nagents, there remains a significant gap in the form of a comprehensive and standard framework\nfor their evaluation, comparison and development. This paper introduces AUTOPENBENCH, an\nopen benchmark for evaluating generative agents in automated penetration testing. We address the\nchallenges of existing approaches by presenting a comprehensive framework that includes 33 tasks,\neach representing a vulnerable system that the agent has to attack. Tasks are of increasing difficulty\nlevels and include in-vitro and real-world scenarios. To assess the agent performance we define generic\nand specific milestones that allow anyone to compare results in a standardised manner and understand\nthe limits of the agent under test. We show the benefits of our methodology by benchmarking two\nmodular agent cognitive architectures: a fully autonomous and a semi-autonomous agent supporting\nhuman interaction. Our benchmark lets us compare their performance and limitations. For instance,\nthe fully autonomous agent performs unsatisfactorily achieving a 21% Success Rate across the\nbenchmark, solving 27% of the simple tasks and only one real-world task. In contrast, the assisted\nagent demonstrates substantial improvements, attaining 64% of success rate. AUTOPENBENCH\nallows us also to observe how different LLMs like GPT-40, Gemini Flash or OpenAI o1 impact the\nability of the agents to complete the tasks. We believe that our benchmark fills the gap by offering a\nstandard and flexible framework to compare penetration testing agents on a common ground. We\nhope to extend AUTOPENBENCH along with the research community by making it available under\nhttps://github.com/lucagioacchini/auto-pen-bench.", "sections": [{"title": "1 Introduction", "content": "Penetration testing, or pentesting, consists of performing real-world cyber-attacks to test the effectiveness of an\norganisation's security system. It is a complex and challenging field that requires a diverse set of skills and extensive\nknowledge [1]. The difficulty in conducting effective pentests has led researchers to explore automated solutions, from\nautomatic tools like Metasploit [2] or OWASP Netattacker [3] to AI-based solutions to enhance and automate various\naspects of the process (we refer the reader to [4] for a complete overview). These include deep reinforcement learning\napproaches to reproduce real pentest scenarios and automate attack paths [5,6], and more traditional rule tree models [7]."}, {"title": null, "content": "In recent years, we witnessed the rise of AI-based Generative Agents [8]. These are software systems that leverage\nfoundation models and Large Language Models (LLMs) to create solutions across diverse contexts [9, 10]. These\nagents are capable of performing complex tasks involving decision-making and multi-step planning. Building upon\nthis advancement, a new and growing approach has emerged: the use of autonomous generative agents to automate\npentesting processes [11-17].\nInitial approaches, such as PentestGPT [11] and the work of Hilario et al. [12], rely on manual or simulated interaction\nbetween vulnerable systems and ChatGPT [18], demonstrating low agent autonomy. Happe et al. [13] introduced\nlimited interaction with tools and integration with other LLMs, but suffer from low reproducibility. The HPTSA\nMultiAgent approach [14] employs agents tailored to specific pentest cases, lacking in generalisation across different\ntasks. A major limitation of these works is the lack of common benchmarks which limits reproducibility and makes\ntheir comparison complex, if not impossible. To this extent, Shao et al. proposed a benchmark [15] based on CTF-\nlike competitions. Unfortunately, their approach limits the agent interaction with the system to a narrow set of tools.\nSimilarly, in AutoAttacker [16] authors propose a custom agent and test it on a benchmark of 14 tasks, but the lack of\nopen-source code mines reproducibility. Additionally, they provide the agent with a long-term memory to store and\nretrieve its experience across the execution of each task. This introduces a dependency between various tasks that could\npotentially affect the final evaluation. Such an approach, while innovative, raises questions about the validity of results\nin isolated test scenarios.\nDespite the growing interest in this field, there remains a notable absence of a common, open framework to evaluate\nthese agents, guide their development and compare their performance on a common ground. The only work towards\nthis direction is Cybench [17]. The authors propose a benchmark framework based on 40 tasks from four real CTF\nchallenges. Despite such tasks might provide useful insights about agent performance, they tend to be gamified, often\nembedding hints or clues within the system, and are not representative of authentic pentesting scenarios. Additionally,\nthese challenges rarely involve active operations, like intercepting or manipulating network traffic, and are typically\nfocused on exploiting vulnerabilities in a preconfigured machine that simply offers a single service. While useful\nfor testing basic exploitation skills, Cybench oversimplifies complex attack vectors and lacks the unpredictability or\nreal-world constraints faced by pentesters\nOur research aligns with the direction established by Cybench, proposing AUTOPENBENCH, a complementary open-\nsource benchmark for evaluating generative agents in automatic penetration testing. The benchmark features 33\nvulnerable Docker containers across two difficulty levels, with basic in-vitro pentest scenarios (easier than the CTF\nchallenges of Cybench) and real-world cases (more complex and realistic than Cybench tasks). To fairly and thoroughly\nevaluate agents and to provide complete and thorough results, AUTOPENBENCH provides detailed information to\nunderstand and assess agent progress. To foster further studies, we make AUTOPENBENCH source code publicly\navailable\u00b9 and instructions to use and extend it. To show the potential of our approach, we introduce, test and compare\ntwo modular agent cognitive architectures: a fully autonomous version and a semi-autonomous one supporting human\ninteraction. Unlike previous works, we extend the agent capabilities for interacting with target systems by allowing it to\nexecute any command without restriction to a predefined set of tools. Hence, we analyse the agents performance across\nthe benchmark, investigating the reliability of these approaches in automated penetration testing scenarios.\nAUTOPENBENCH reveals that (i) the fully autonomous agent demonstrates limited effectiveness, achieving only a\n21% Success Rate (SR) across our benchmark, with a notable disparity between in-vitro (27% SR) and real-world\nscenarios (9% SR); (ii) enabling human-machine collaboration pays, resulting in a total SR of 64%, with 59% success\non in-vitro tasks and a 73% on real-world challenges. AUTOPENBENCH allows us to seamlessly assess the impact of\nusing different LLMs like GPT-40, OpenAI ol or Gemini Flash. The ability to automate and repeat the tasks allows us\nto compare the inherent randomness of LLMs which penalises the agent reliability for pentesting tasks."}, {"title": "2 Benchmark Overview", "content": "We build AUTOPENBENCH on top of AgentQuest [19], a modular framework which supports the design of benchmarks\nand agent architectures. In this work, we include 33 pentest tasks organised into 2 difficulty levels: in-vitro and real-\nworld tasks. We structure the tasks as a CTF challenge \u2013 i.e. the agent has to discover and exploit a vulnerability to read\nthe content of a flag. Each task involves at least one Docker container (the vulnerable system) and a container hosting\nthe agent pentest workstation. We split each task into milestones to objectively measure and understand the agent\nprogress. In the following, we provide an overview of the penetration test infrastructure, the benchmark categories and\nthe milestones required to evaluate the agent performance."}, {"title": "2.1 Penetration test infrastructure", "content": "We develop our penetration test infrastructure (overviewed in Figure 1) relying on a virtualisation approach to create\nisolated and scalable Docker containers [20]. On the one hand, we deploy one container hosting the agent pentest\nworkstation serving as the central hub from which all activities are coordinated. Such workstation operates on Kali\nLinux equipped with a comprehensive suite of security tools (e.g. the Metasploit framework [2], Hydra [21], Nmap [22]\netc.). On the other hand, we deploy a set of containers affected by vulnerabilities or system misconfigurations the agent\nis tasked to detect and exploit.\nTo connect the working station with the vulnerable containers, we reserve a /16 Docker virtual network on which each\ncontainer operates allowing the system to communicate through any protocol like SSH, HTTP, etc."}, {"title": "2.2 Types of vulnerable machines", "content": "We structure our benchmark by designing 33 tasks (or vulnerable containers to exploit) organised into two levels of\ndifficulties: in-vitro tasks and real-world tasks. The first refers to topics related to basic cybersecurity fundamentals; the\nlatter refers to various publicly disclosed Common Vulnerabilities and Exposures (CVEs) collected from 2014 to 2024."}, {"title": "In-vitro tasks", "content": "We design 22 tasks, each representing a distinct target for penetration testing activities. These tasks\nare inspired by scenarios typically encountered in cybersecurity courses, ensuring a level of complexity that is easy\nenough to investigate the agent performance in relatively simple tasks. We organise the penetration testing tasks into\nfour main categories: Access Control (AC), Web Security (WS), Network Security (NS), and Cryptography (CRPT).\nWe provide a brief description of the designed in-vitro tasks in Table 1. For each task, we define additional information,\nincluding the golden steps, i.e. the minimum number of commands required in one possible efficient solution for each\ntask2, the number of task-specific command milestones Mc and more generic stage milestones Ms that we use to\nmeasure the agent progress, see Section 2.3 for details. While these scenarios may not necessarily be representative\nof real production environments, they provide us with a simple framework to assess the agent logical reasoning and\nproblem-solving capabilities in simplified yet meaningful penetration testing contexts."}, {"title": "Access Control (AC)", "content": "vulnerabilities refer to a broad class of security weaknesses that arise from improper implementation\nor configuration of access control mechanisms in computer systems, e.g. user account management, file system\npermissions, etc. Typically, these vulnerabilities allow unauthorised users to gain elevated privileges, access restricted\nresources, or perform actions beyond their intended permissions."}, {"title": "Web Security (WS)", "content": "vulnerabilities encompass a wide range of security weaknesses that affect web applications, web\nservices, and web-based systems. These vulnerabilities typically arise from flaws in web application design, implemen-\ntation, or configuration, allowing malicious actors to exploit the application functionality or access unauthorised data."}, {"title": "Network Security (NS)", "content": "vulnerabilities refer to weaknesses in network infrastructure, protocols, or configurations that\ncan be exploited to compromise the security of networked systems. They can be exploited through various techniques,\nincluding network scanning, traffic interception, and protocol manipulation."}, {"title": "Cryptography (CRPT)", "content": "vulnerabilities refer to weaknesses in cryptographic algorithms. The target machine hosts a\ncryptographic web service. The agent must discover the service and interact with it to inspect the cryptographic\nalgorithm and recover the encryption key."}, {"title": "Real-world tasks", "content": "We select 11 tasks involving realistic pentest scenarios. These tasks come from publicly disclosed\ncybersecurity weaknesses with a unique CVE identifier assigned. Table 2 showcases the CVEs which span from 2014\nto 2024 and include a broad spectrum of vulnerability. Among the others, we include vulnerabilities like Remote Code\nExecution (RCE), where an attacker can run malicious code on the victim's machine; code injection, where a vulnerable\nprogram executes malicious code by misinterpreting external data as part of its source code; or buffer overflow, where a\nprogram writes data to a memory saturated-buffer, overwriting adjacent memory locations."}, {"title": "2.3 Milestones", "content": "Using AgentQuest [19] as the benchmarking framework, AUTOPENBENCH measures the agent advancement towards\nthe final goal (i.e. the flag capturing) through milestones. In a nutshell, we break down each task procedure into a\nseries of intermediate steps the agent needs to perform to find the flag. Namely, we design two sets of milestones:\ncommand milestones Mc are the textual descriptions of the commands the agent must execute; stage milestones Ms,\nare a set of keywords representing distinct phases of the penetration testing process and mapping each of the command\nmilestones. For example, the very first stage of each task is the 'target identification' (ms,0 \u2208 Ms). To complete this\nstage, the agent needs to run a network scan to identify the IP address of the vulnerable machine within the network\n(mc,0 \u2208 Mc) and then to identify the active services that can be exploited (mc,1 \u2208 Mc). In Table 3 we provide an\nexample of the milestones mapping in correspondence of the executed commands for the task AC0, a simple case where\nan SSH-enabled account hosts a weak password and sudo right.\nEvaluating the performance of an agent when there are multiple correct solutions is inherently challenging. For instance,\nobtaining SSH credentials for a machine to complete the infiltration phase can be accomplished through various\nmethods \u2013 e.g. the automated tool Hydra, or manual brute-force, among others. To address this complexity, we define\nthe milestones in a generic manner, as illustrated in Table 3. Inspired by previous work [29], we leverage an LLM,\nspecifically GPT-40 to check if the command is correct. We provide the model with the execution logs of the agent and\nsupply it with one command milestone at a time, asking it to identify the step and command by which the milestone was\nachieved, if any. Given that LLMs are prone to hallucinations [30], we allow a manual correction step to fix potential\nerrors and ensure the reliability of the evaluation process. After the comparison between the executed commands and the\nmilestones, we compute the Progress Rate (PR) [19], a performance metric in the [0; 1] range quantifying how much the\nagent progressed towards the final goal. In a nutshell, we assess the number of Mc achieved by the agent over the total.\nIn Figure 2 we provide an example of the agent qualitative evaluation when solving the CVE0 task. Each vertical bar\nindicates an execution step. In the bottom part, we report the commands the agent performed to reach each command\nmilestone. Thanks to the mapping between Mc and Ms, we can assess which stage of the penetration test the agent\nsuccessfully completed (indicated by the different colours).\nTo extend the benchmark to other tasks and categories, a developer must provide (i) the Docker configuration files of\nthe vulnerable system, (ii) the gold steps, (iii) the command milestones and (iv) the stage milestones, following the\nformat specified in the public repository."}, {"title": "3 Generative Agents", "content": "To assess the capability of generative agents in penetration testing, we use AUTOPENBENCH to test and compare a\ncompletely autonomous agent and a human-assisted agent. We design our generative agents relying on the COALA\nframework [31]. In a nutshell, CoALA defines an agent through three components: (i) a decision-making procedure,\na loop responsible for the agent behaviour relying on a pre-trained LLM; (ii) an action space to perform internal\nactions through reasoning procedures and external actions through grounding procedures; and (iii) at least one memory\ncomponent to store recent data related to a specific task (working memory), or across different tasks (semantic, episodic\nand procedural memory).\nConsider a general setup of an agent interacting with an environment for multiple execution steps approaching a task.\nAt execution step i, the environment produces a textual observation O\u017c providing information about its current state.\nThe agent triggers its decision procedure by providing an input prompt to the LLM to produce a thought Ti on the\nobservation, and an action A\u2081. Once processed, this action changes the environment state, resulting in a new observation\nand concluding the step. After the execution step, the agent updates its working memory H.\nMany existing generative language agents [32-34] implement the working memory as a textual document storing the\nhistory of the perceived observations and the output of the reasoning procedures. At each iteration, they append the\nnew information to the existing text and feed it entirely to the LLM as a new (longer) prompt. Building upon the same\napproach, we design the autonomous agent introducing a substantial improvement to the widely used ReACT agent [32]\nby structuring the execution step with three reasoning procedures detailed in Section 3.1. Additionally, we propose\na novel ReACT-based agent assisted by a human user promoting human-to-machine interaction (see Section 3.2 for\ndetails)."}, {"title": "3.1 Autonomous agent", "content": "Firstly, we provide to the agent the instructions for the task to solve in the form of a text I \u2013 i.e. the task description, the\nrole-playing jailbreak method [16] to bypass the LLM usage policies and additional agent preliminaries information.\nWe then define three sequential reasoning procedures within each execution step. The resulting instruction prompt\ntemplate is the following:"}, {"title": "Summary procedure", "content": "Motivated by early works highlighting cognitive issues and reasoning weaknesses induced by\nknown limitations of LLMs [35\u201337], at step i, the agent prompts the LLM to produce a concise summary S\u2081 of the\nreceived instructions I and its working memory H, highlighting the main findings discovered during the task execution.\nThis should reduce the LLM hallucination chances due to redundant or uninformative information contained in the\nagent history [36,37]. Additionally, as we forward the produced summary to the following procedures, the LLM is"}, {"title": "Thought and action procedures", "content": "At step i, the ReACT prompting technique asks the LLM to produce the thought Ti\nand the action A\u2081 within the same model call [32]. Nevertheless, we empirically observed the known inconsistency\nissue of LLMs [38, 39], in which the produced action does not follow the directives embedded in the thought. To\nmitigate this issue, we decouple the output generation in two procedures.\nWith the thought procedure we provide the LLM with the refined context \u2013 i.e. the summary S\u2081 generated in the previous\nprocedure \u2013 and the last execution step, represented by the last thought Ti\u22121, action Ai-1 and observation, Oi-1. Then,\nwe prompt the model to produce a thought Ti in response to the last execution step, explicitly stating that the produced\nthought will guide the agent in the action choice later. This is the thought procedure prompt template:"}, {"title": "3.2 Assisted agent", "content": "Studies on collaboration between human and AI models highlight the improvement in both models performance and\nuser satisfaction, assessing such technologies (LLMs in particular) as valuable assistants for many diverse tasks [40-42].\nAdditionally, in case of complex tasks like pentesting, a completely autonomous agent failing the final goal could\nrepeatedly attempt to find the correct solution [19], resulting in humongous costs for the end user (especially in case of\nmodel calls via proprietary APIs).\nIn light of these considerations, similarly to Cybench [17], we design a second generative agent involving direct\ncollaboration with a human user. Differently from PentestGPT [11], where the human is involved in all the execution\nsteps following the LLM guidance and manually executing within the pentest environment the output actions, our\nassisted agent still maintains a certain degree of autonomy. In a nutshell, we break down the final goal into sub-tasks\nthat the human user provides to the agent one at a time. The agent approaches each sub-task autonomously and, upon\nsuccess or meeting a stopping condition, it provides a task report waiting for the next sub-task. Notice that, differently\nfrom AutoGPT [33], where the sub-task planning is defined and managed by the agent itself, and from Cybench [17],\nwhere the sub-tasks are pre-defined, with our assisted agent the human user can guide the agent adapting the planning\nstrategies based on the report content.\nHere we provide an example of the possible sub-tasks:"}, {"title": "3.3 Tools and Structured Output", "content": "While many current benchmarks limit agent interactions to a predefined set of specific tools [13-16] like Netcat, Ghidra,\nor Metasploit, we design our benchmark for a complete system interaction. This design choice enables a more realistic\nassessment of the agents capabilities in a pentest scenario.\nTo facilitate this comprehensive interaction, we provide three default tools allowing agents to (i) submit their final\nanswer; (ii) establish SSH connections with custom host, port, and credential specifications; (iii) execute any Bash\ncommand through an interactive shell by specifying the IP address of the target container (either the victim or the\nKali workstation). This last tool enables a complete interaction with the systems. When needed, we extend the toolkit\nallowing agents to (iv) write any executable script within their Kali container.\nTo ensure efficient and error-free communication between the LLM and the testing environment, we implement\nstructured output using the Python library Instructor [43]. This approach prompts the LLM to return the output in a\ncustom JSON format, represented by a Pydantic object [44]. By structuring the output in this manner, we significantly\nreduce parsing errors."}, {"title": "4 Experimental Results", "content": "We evaluate the two generative agents performance with AUTOPENBENCH. To limit the monetary cost deriving from\nmultiple LLM API calls, we run our experiments using gpt-40-2024-08-06 which results as the currently best LLM\nfor pentesting (see Section 5 for the LLM selection). We fix the seed and set the LLM temperature to 0 to reduce the\nrandomness of the generated output. We set 30 as the step limit for the in-vitro tasks and 60 for the real-world tasks.\nTable 4 reports the task success rate (SR) for all the difficulty levels and task categories. For the failed tasks we report\nthe average, minimum and maximum progress rate (PR) measured at the last execution step. The autonomous agent\nfails most of the tasks (21% overall SR across all tasks). In the less complex in-vitro tasks, it performs slightly better\nwith an SR of 27% but solves only one real-world scenario. The agent correctly executes 40% of the intermediate steps\non average, suggesting some degree of partial task comprehension.\nThe assisted agent demonstrates substantially improved performance across all metrics. It solves the triple the number\nof tasks (64% of SR). The improvement is evident both in the in-vitro tasks (59% of SR) and the real-world tasks\n(73% of SR). When failing, the assisted agent progresses more than the autonomous one, reaching more than half of\nthe intermediate steps (53% of PR). In Appendix A we provide the complete execution logs of the autonomous and\nassistant agents solving the task AC0."}, {"title": "4.1 Autonomous agent", "content": "The Progress Rate (PR) AUTOPENBENCH computes, allows us to gain insights on the agent and the underlying LLM\nreasoning abilities. Despite the autonomous agent fails 16 out of the 22 in-vitro tasks, our results reveal several\nkey findings. In all cases, the agent consistently demonstrates proficiency in network discovery across all test cases,"}, {"title": "4.2 Assisted agent", "content": "The introduction of the assisted agent approach yields several notable advantages compared to the autonomous one,\nsolving three times as many tasks (SR of 64% compared to 21%). One of the main advantages of the assisted agent is\nthe split of the main task into smaller sub-tasks. By breaking down the problem space, the assisted agent can better\nmaintain focus and tackle each sub-task more efficiently. Additionally, the systematic cleaning of the agent scratchpad\nat the completion of each sub-task (cf. line 6 of Algorithm 2) helps to reduce the amount of uninformative text fed as\ninput to the LLM, improving its contextual awareness.\nDespite these advantages, the assisted agent still fails to accomplish 12 out of 33 tasks. Similarly to the autonomous\nagent, the assisted one fails to detect the vulnerability in AC1 and the content filter in WS2. Even though it can now\nsolve the easy SQL injection task (WS3), it still struggles in completing the attack on the UPDATE statement of (WS4).\nThe agent repeatedly attempts an attack on a SELECT statement, failing to adapt its approach effectively. The challenges\nare not limited to SQL injection; although the agent identifies the injection point of WS6, it persists in attempting a path\ntraversal approach, failing to execute the RCE exploit.\nRegarding NS tasks, the assisted agent succeeds in detecting and infiltrating the target machine hosting an SNMP\nservice on a standard port (NS2), where the autonomous agent fails. Nevertheless, it still struggles when the SMTP\nport is non-standard (NS3). Finally, in NS5, it exhibits the same limitations of the autonomous agent when running the\nman-in-the-middle attack.\nThe considerations on the autonomous agent in CRPT tasks are still valid. The assisted agent accomplishes only CRPT0,\nnow addressing the padding. Notably, the assisted agent improves sensibly in real-world tasks (SR of 73% in CVE\ncompared to 9%). From Figure 4 the agent completes the first three pentest stages in all the tasks detecting 100% of the\nvulnerabilities (compared to 50% with the autonomous agent). Among them, it correctly exploits and completes 82% of\nthe vulnerable containers (except the step limit exceeded in CVE3 limiting the task SR to 73%).\nAll in all, our results highlight how a semi-autonomous agent can overcome the limitations of such recent technology.\nAlthough we are far away from a fully autonomous agent, continued research and iterative refinement of technologies\nbased on the human-AI collaboration hold promise to efficiently automate penetration tests with minimum domain\nknowledge requirements."}, {"title": "5 Additional Analysis", "content": "In this section, we describe how we select the LLM used by our agents in Section 4 demonstrating the applicability of our\nbenchmark across various models. Additionally, we investigate the agent consistency in approaching pentest scenarios."}, {"title": "5.1 Choice of the LLM", "content": "We compare six models gpt-40 (2024-08-06 release), gpt-4-turbo (2024-04-09 release), gpt-40-mini (2024-07-18\nrelease), OpenAI 01-preview and 01-mini (both at the 2024-09-12 release), and gemini-1.5-flash on a simple test\ncase (ACO) where the agent has sudo permissions on the target container. We chose this scenario as an initial test; if an\nagent cannot successfully complete this task, it would not be sensible to proceed with more complex evaluations. We\nrun five instances of AC0 for each model using the same experimental settings of Section 4. We restrict the analysis to\nthe autonomous agent to minimise the influence of sub-task prompts on the output and evaluate the model performance\nthrough SR and, for failing tasks, we compute the PR and the primary reasons for failures.\nFrom Table 5, gpt-40 emerges as the top performer, successfully completing the task in all five runs. In contrast,\ngpt-4-turbo achieves a 40% SR. When it fails, the primary issue appears to be a lack of contextual awareness, which\nlimits its ability to progress with vulnerability detection and exploitation. This limitation is even more pronounced in\ngemini-1.5-flash, which fails all runs, achieving only 5% of the intermediate steps.\nThe OpenAI 01-preview model is designed to prevent jailbreak [45, 46], resulting in a complete failure to solve the\ntask across all five runs\u00b3. However, this prevention mechanism is not infallible, as the agent still manages to achieve\nan average of 12.5% of the intermediate steps. In these cases, the contextual awareness is unsatisfactory, with the\nagent unable to infiltrate the target machine (stage 2 of the penetration test). On the other hand, the 01-mini model\nconsistently lacks jailbreak prevention. It also demonstrates improved contextual awareness compared to 01-preview,\nthough still not comparable to the gpt-4x series, reaching only 27.5% of the intermediate steps.\nLastly, gpt-40-mini demonstrates unsuitability for structured output. While it manages to complete 55% of the\nintermediate steps, it fails to produce the correct JSON output format interrupting the task execution. All in all, we\nselect gpt-40 as the LLM for our agent given its consistent success across all test runs."}, {"title": "5.2 Agent Consistency", "content": "We conduct an additional analysis to evaluate the consistency of autonomous agents in penetration testing, where\nreliability is paramount. Despite configuring the LLM to minimise output randomness, we observe some inherent\nvariability. For this analysis, we focus on AC0 for its simplicity and AC2 as a more complex scenario. In AC2, the\nagent must detect and exploit a misconfigured cron job with root privileges after target discovery and infiltration. We\nuse gpt-40 for the autonomous agent with the same settings described in Section 4. We execute each task 10 times and\nreport in Figure 5 the distribution of the execution step number at which the agent solves each stage.\nFor ACO (Figure 5a), the agent successfully completes the task in all ten runs. However, we observe variability in\nthe number of steps needed for each stage: infiltrating the target system takes between 2 and 14 steps, detecting the\nvulnerability requires 3 to 13 steps, and exploitation ranges from 1 to 11 steps. In AC2 (Figure 5b), despite consistently\ndiscovery and infiltrating the target, the agent only detects the vulnerability in 30% of the runs and successfully exploits\nit in 40%, significantly reducing the LLM consistency. These results demonstrate that while the autonomous agent\nconsistently succeeds in simpler tasks, it exhibits variability that may increase costs due to additional LLM API calls.\nIn more complex scenarios, the LLM reliability decreases substantially, compromising its dependability for critical\ncybersecurity operations. AUTOPENBENCH greatly simplified this experiment and let us focus on the result analysis."}, {"title": "6 Conclusion", "content": "In this work, we developed AUTOPENBENCH, an open-source benchmark for evaluating generative agents in automatic\npenetration testing. We hope its availability opens to a fair and thorough comparison of agents performance in pentesting.\nFor this, we performed extensive experiments using two modular agent cognitive architectures: a fully autonomous\nversion and a semi-autonomous one supporting human interaction. AUTOPENBENCH let us obtain significant insights\ninto the current capabilities and limitations of AI-driven pentesting. The fully autonomous agent demonstrated limited\neffectiveness across our benchmark both in simple in-vitro tasks and in more complex real-world scenarios. The\ninherent randomness of the LLM penalised the model reliability, which is a must in penetration testing. Our assisted\nagent exhibited substantial improvements, especially in real-world challenges, highlighting the potential of human-AI\ncollaboration.\nLooking ahead, we acknowledge several limitations and opportunities for future work. While our benchmark covers\nsimple penetration testing areas, our goal is to create a comprehensive, common benchmark that serves as a playground\nfor autonomous agent development in cybersecurity. To this end, we make AUTOPENBENCH open and flexible. We plan\nto extend the benchmark with additional vulnerable containers, encompassing a broader range of scenarios and attack\nvectors. Furthermore, we aim to expand our analysis to include other LLMs, exploring how different AI architectures\nperform in pentesting tasks. Additionally, we intend to investigate the potential benefits of incorporating a RAG-based\nagent module capable of retrieving information about best pentesting practices from cybersecurity manuals, potentially\nenhancing the agents knowledge base and decision-making capabilities."}, {"title": "Ethics", "content": "In this work, we introduce novel benchmarks that can help the development of LLM-based agents for penetration testing.\nWe believe the use of LLMs for assisting penetration tests would represent a major contribution to increasing the security\nof connected systems. It will streamline and automate the testing of applications, thus preventing such vulnerabilities\nfrom reaching production systems. That is precisely what existing pentesting tools provide, and LLM-based systems\nwould represent a step forward in automating security testing.\nAn important question is whether LLM agents could be used to automate attacks against real systems. Our results show\nthat, as it stands now, LLM agents alone can hardly solve basic cybersecurity exercises. Yet, to minimise risks, our\nbenchmarks focus on didactic examples (in-vitro tasks) and widely known CVEs (real-world tasks). In other words, our\nbenchmarks are based on public knowledge and vulnerabilities for which public exploits are already readily available in\ntools such as Metasploit, thus representing no risks beyond existing tools."}, {"title": "A Agents Running Examples", "content": null}, {"title": "A.1 Autonomous Agent on ACo", "content": null}, {"title": "A.2 Assisted Agent on ACO", "content": null}]}