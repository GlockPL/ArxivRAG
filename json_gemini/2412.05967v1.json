{"title": "Language hooks: a modular framework for augmenting LLM reasoning that decouples tool usage from the model and its prompt", "authors": ["Damien de Mijolla", "Wen Yang", "Philippa Duckett", "Christopher Frye", "Mark Worrall"], "abstract": "Prompting and fine-tuning have emerged as two competing paradigms for augmenting language models with new capabilities, such as the use of tools. Prompting approaches are quick to set up but rely on providing explicit demonstrations of each tool's usage in the model's prompt, thus coupling tool use to the task at hand and limiting generalisation. Fine-tuning removes the need for task-specific demonstrations of tool usage at runtime; however, this ties new capabilities to a single model, thus making already-heavier setup costs a recurring expense. In this paper, we introduce language hooks, a novel framework for augmenting language models with new capabilities that is decoupled both from the model's task-specific prompt and from the model itself. The language hook algorithm interleaves text generation by the base model with the execution of modular programs that trigger conditionally based on the existing text and the available capabilities. Upon triggering, programs may call external tools, auxiliary language models (e.g. using tool specific prompts), and modify the existing context. We benchmark our method against state-of-the-art baselines, find that it outperforms task-aware approaches, and demonstrate its ability to generalise to novel tasks.", "sections": [{"title": "1 Introduction", "content": "A defining characteristic of human intelligence is the way we have externalised it into the world around us, from language and culture to the technology we have built and the knowledge we have accumulated. This externalisation of capabilities alleviates the need for humans to repeatedly solve problems of the past, allowing us to focus instead on novel aspects of the task at hand.\nIn a similar vein, recent work has explored the augmentation of large language models (LLMs) with external tools to increase the range of their capabilities (Brown et al., 2020; Chung et al., 2022; Lu et al., 2023). One approach to this involves hard-coding the reasoning paths available to the model (Trivedi et al., 2022; Khattab et al., 2022), which is feasible when the logical flow of a task is fixed. Another requires demonstrations of tool usage in the model's task-specific prompt (Yao et al., 2022; Gao et al., 2022). All such task-aware approaches broaden the model's capabilities but at the cost of narrowing its general applicability.\nAs an alternative, fine-tuning LLMs for tool use (Hao et al., 2023; Schick et al., 2023) embeds tool-awareness into the model itself, rather than filling its prompt or imposing upon its reasoning path. Such methods exhibit greater generalisability but come with their own drawbacks, namely in setup cost and in tightly coupling the new capabilities to a single specific model.\nBeyond tool usage, LLM capabilities can be augmented in many directions. For example, recent research has explored the possibility of detecting failure modes in language generation, e.g. hallucination (M\u00fcndler et al., 2023; Agrawal et al., 2023). Whilst this research is encouraging, there are important use cases for which it is so-far insufficient, e.g. for verifying age-restricted content in an LLM-powered app on a jurisdiction-specific basis. Verification of the model's output in such cases should be independent of the base model, as well as transparent and customisable by the user.\nIn our view, LLMs should be augmented with new capabilities through an approach that is:\n1.  Task-agnostic: widely applicable to new prob-lems and, in particular, independent from the model's task-specific prompt;\n2.  Model-agnostic: decoupled from any particu-lar model to avoid recurrent fine-tuning costs and enable independent verification;\n3.  Modular: built up from transparent compo-nents, each designed to enable a specific ca-pability, with the option of combining any number of such components at runtime;"}, {"title": "2 Language hooks", "content": "In this section, we define the language hook algorithm in detail and provide examples of specific hooks used in the experiments of Section 3.\nWe begin by defining notation. Let \\(C_0\\) denote the prompt, e.g. a question to be answered and any in-context examples. Let \\(g\\) denote the base model performing text generation, which will proceed one sentence at a time. Let \\(s_t\\) denote the t-th sentence of the response and \\(r_t = [s_1,..., s_{t-1}, s_t]\\) the reasoning stream. The full context after t sentences is then \\(C_t = [C_0, r_{t-1}, s_t]\\), and we have \\(C_{t+1} = g(C_t)\\). We introduce \\(f : s_t \\rightarrow {0, 1}\\) to"}, {"title": "2.1 Examples of language hook programs", "content": "Here we describe the program for each language hook used in the experiments of Section 3. Further details, including auxiliary-LLM prompts, are provided in Appendices B and E.\nCalculator hook. LLMs can still struggle with arithmetic tasks (Hendrycks et al., 2021), particularly with large numbers (Nogueira et al., 2021; Qian et al., 2022). Our calculator program aims to fix any incorrect calculations in the most recent sentence \\(s_t\\). The program first prompts an auxiliary LLM to extract all calculations from the sentence; these are then verified using the symbolic tool SymPy (Meurer et al., 2017). If an error is detected, the program corrects it and removes further reasoning to prevent errors propagating to future text generation. If no errors are detected, the program leaves the context unchanged.\nRetriever hook. The retriever program searches for relevant references to add to the prompt \\(C_0\\) and rewrites the most recent sentence \\(s_t\\) in light of the modified context. More specifically, the program: (i) prompts an auxiliary LLM to generate 5 search queries based on \\(C_{t-1}\\); (ii) feeds these queries to"}, {"title": "3 Experiments", "content": "In this section, we validate the language hooks framework by benchmarking the specific hooks introduced in Section 2.1 against state-of-the-art general and task-aware approaches in three different domains: mathematical reasoning, multi-hop QA, and a custom composite dataset that requires a combination of tools for good performance."}, {"title": "3.1 Experimental setup", "content": "We implement the language hook algorithm with ChatGPT (gpt-3.5-turbo-0301) (OpenAI, 2022) as the base model, curie for triggering, and ChatGPT as the auxiliary LLM within programs. We place our hooks in decreasing priority order as: retriever, guardrail, calculator. See Appendix B for the implementation of each hook in detail.\nWe compare the language hooks method to two general techniques: Chain-of-Thought Prompting (CoT) (Wei et al., 2022) which has no access to tools, and ReAct (Yao et al., 2022) which uses the prompt to demonstrate how the base model may initiate tool usage by outputting specific token patterns. We also compare against two strong task-aware baselines, PAL (Gao et al., 2022) for mathematical reasoning and DSP (Khattab et al., 2022) for multi-hop QA, which we comment on further in the subsections below.\nUnless noted otherwise, in our experiments we ensure that all methods employ the same base model, the same in-context examples, and the same specific tools. For each method, the number of in-context examples (randomly selected from the training split of each dataset) is min (3, k) where k is the maximum number of examples the method can support within context limits. Full prompts for all methods are included in Appendix F.\nFor each benchmarking experiment, we report performance on 500 unseen questions to control costs; these examples are randomly sub-sampled"}, {"title": "3.2 Mathematical reasoning", "content": "In this first set of experiments, we benchmark the calculator hook introduced in Section 2.1.\nBaselines. Alongside CoT and ReAct we also compare against PAL (Gao et al., 2022). PAL prompts the base model to write a computer program to solve each question, and the program is fed to a Python interpreter. We use the official implementation of PAL and the gpt-3.5-turbo-0301 support provided in their codebase.\nTool. Aside from PAL (which executes Python) both language hooks and ReAct are paired with SymPy. Given that all questions require only basic arithmetic this difference solely relates to the interface, not the capability, of the tool.\nDatasets. We evaluate on the GSM8K dataset (Cobbe et al., 2021), containing grade school math word problems, and on GSM-HARD (Gao et al., 2022), where numbers in GSM8K are artificially made larger to make the calculations more challenging. However, this creates many questions with nonsensical reasoning paths (e.g. negative salaries) which gpt-3.5-turbo-0301 often refuses to answer (see Appendix C.1 for examples). We therefore filter the 500 questions in our GSM-HARD evaluation set to remove cases where the answer is negative or non-integer, leaving 326 questions that we refer to as GSM-HARD-filtered. We report results in Table 1.\nLanguage hooks are non-intrusive. On GSM8K, where a calculator isn't strictly needed given gpt-3.5-turbo-0301's ability to do simple arithmetic, the running of the calculator program to validate the calculations in the generated reasoning stream does not harm performance \u2013 i.e. the hook performs validation in a fault-free way. In contrast, Table 1 shows that ReAct degrades performance compared to CoT, which we hypothesise is due to ReAct's strict response structure which differs sharply from the model's pre-training data.\nLanguage hooks are effective. On GSM-HARD language hooks performs slightly behind PAL, although this is not surprising as PAL, by translating the problem into code, abstracts away numerical values making it less sensitive to the implausible reasoning steps required on GSM-HARD. Once we adjust for this in GSM-HARD-filtered, language hooks in fact performs slightly ahead of PAL."}, {"title": "3.3 Multi-hop QA", "content": "In these experiments, we benchmark the guardrail and retriever hooks described in Section 2.1.\nBaselines. As above, we include the general methods CoT and ReAct. In this experiment, we also allow ReAct to fall back on CoT if it exceeds the context-length limit before satisfying the stopping criterion (i.e. returning a final answer).\nWe also benchmark DSP (Khattab et al., 2022), an approach that creates sophisticated pipelines between the base LLM and a retriever when solving knowledge-intensive QA tasks. We use the official implementation with gpt-3.5-turbo-0301 support, keeping all other settings at their defaults (apart from the external retriever, which we align across all the methods tested). As a result, DSP is the only method in our experiments that benefits from self-consistency (Wang et al., 2022). It is also the only method that controls the creation of its own in-context examples, as it has distinct requirements here that form a core part of the method.\nTool. We employ the hybrid retriever from the retriv package with default hyperparameters (Bassani, 2023) and build an index for each dataset using the titles and passages combined. For language hooks and ReAct, a single retriever query returns the 3 closest results, and for DSP we allow this to vary based on their implementation. To"}, {"title": "3.4 Composite QA", "content": "In these experiments, we benchmark the guardrail, calculator, and retriever hooks in combination. We compare against CoT and ReAct, the latter of which uses the calculator and retriever tools as discussed in Sections 3.2 and 3.3.\nDataset. The ability of a language model to synthesise responses to individual sub-questions into a final answer is under active research (Press et al., 2022; Dziri et al., 2023). We create a custom task, HotpotQA\u00d7GSM8K, to investigate how well different approaches utilise multiple tools in this setting. In particular, we select questions from HotpotQA and GSM8K where the answer is an integer value with more than 3 significant figures; we then pair these questions, one from each dataset, to create composite questions where the final answer requested is the product of the answers to the individual questions (see Appendix C.2 for examples). Answering a composite question thus requires interleaving mathematical reasoning with multi-hop QA along with a nontrivial large-integer multiplication step at the end. Good performance on this task requires an understanding of when to use which tool and how to properly compose the final answer. The task thus evaluates each method's ability to combine multiple tools in a novel and more realistic setting. See Table 4 for results.\nLanguage hooks can combine tools effectively in unseen settings. The results in Table 4 show that the language hook framework adapts to multiple-tool use with no explicit demonstration of such, significantly outperforming ReAct in this setting. We can further analyse this finding through an ablation that assesses the performance of ReAct and language hooks on the individual components of each"}, {"title": "4 Related work", "content": "In this section we discuss the large body of recent literature relating to our work.\nTask-aware methods for tool usage. Many works have created task-aware control-flows to solve narrow problems with the high-level steps required specified in code (Trivedi et al., 2022; Khattab et al., 2022; Xu et al., 2023). Another approach utilises the prompt to demonstrate how to decompose a problem and/or use tools. This appears in a variety of forms, from a single model call (Gao et al., 2022; He-Yueya et al., 2023), to chained"}, {"title": "5 Conclusion", "content": "In this paper we have introduced language hooks, a flexible and generic framework for equipping language models with new capabilities. The idea behind our method is natural: stream text out of a language model and use external programs to validate and augment the generated text in tandem. However, the implications are far-reaching, including the decoupling of tool usage from both the base model and its prompt. Beyond this, for certain applications we expect external validation of language model outputs to become the dominant paradigm; language hooks offer a transparent and modular way to accomplish this.\nOur experiments show that language hooks achieve performance competitive with both general and task-aware benchmarks, comparing favourably to strong baselines. We validated this on both mathematical-reasoning and multi-hop QA tasks. We also demonstrated language hooks' generalisability, outperforming baselines on a novel task requiring multi-tool use.\nAvenues for future work include further extending the functionality of language hooks beyond tool usage. This could involve hooks that detect and redirect toxic language or age-restricted content. One could also take advantage of the conditional computation that language hooks enable in a variety of different ways. For example, a hook could be developed to apply self-consistency in a more localised and cost-efficient fashion. In general, we hope our work leads to further advancements that enable augmented language models to have a positive impact across a wide range of applications."}, {"title": "6 Ethical considerations", "content": "External validation. Language hooks offers a framework for the external validation of model outputs, independent from the base model and in a transparent manner, at the point of text generation. We believe this has compelling applications, and the language hooks framework could be used in conjunction with internal safety measures (e.g. data collection and RLHF (Ouyang et al., 2022)) as a viable route for external validation of model outputs. Indeed, we believe safety is a property of the system as a whole, not solely of the language model, and that language hooks provides a framework to support the governance of language models, particularly for safety critical applications.\nGuardrail hook. The guardrail hook prompted the base model to respond when it has initially determined it did not have the required information. Prompting a model to guess in this way introduces the risk that the model will hallucinate information and therefore, as done in our experiments (see Appendix B.3), it can be beneficial to mark this text as potentially untrustworthy if used in later text generation steps. More generally, we believe that language models should not be solely relied upon as a source of information, and we encourage further research efforts into methods of transparently connecting them with external tools to allow for independent and transparent verification of their outputs."}, {"title": "7 Limitations", "content": "Interaction of multiple hooks. Our experiments demonstrate the use of multiple tools when solving a single task, which requires the setting of a priority order between hooks. A key consideration when designing programs is to avoid destructive interference where the program from one hook may undo or contradict the modifications made by another hook's program.\nUsage via. API. The current OpenAI API does not implement any mechanism for preserving the hidden state from model calls. As such, upon triggering, our experiments require re-calculating the context for every sentence, thus increasing the cost of using language hooks under the current OpenAI API.\nGreedy approach to generation. The language hook algorithm generates text one sentence at a time. This is essentially a greedy text generation approach which doesn't allow for the LLM to plan for tool usage several steps ahead of time. We imagine that the language hook framework may be ill-suited for some class of tools, where awareness of the availability of the tool is beneficial."}, {"title": "A Further analysis and ablations", "content": "We run the following ablations for ReAct and language hooks on the individual sub-questions in the composite QA dataset from Section 3.4 to assess their performance: (A) when given access to the correct tool only (as in Table 5) and (B) when given access to all tools (multi-tool).\nAblation (A) gives the baseline performance of an approach for each individual sub-question. Ablation (B) assesses an approach's ability to know when to use the correct tool, or ignore redundant ones. The results are in Table 6.\nWe see that both ReAct and language hooks perform similarly under (B) as with (A), demonstrating both approaches are able to correctly select tools when given a choice. As such the main performance drop comes from their ability to compose the individual answers together, which we analyse in the next section."}, {"title": "A.2 Compositionality", "content": "Whilst the above ablations allowed us to test tool selection, we can analyse how well approaches can compose their responses by analysing the results from the composite QA dataset which were presented in Table 4. To recap: a composite question consists of a GSM8K question's answer multiplied by a HotpotQA question's answer. Therefore to correctly answer a composite question requires:\n1.  Answering both sub-questions correctly;\n2.  Extracting the above answers from the reasoning to input into the final multiplication;\n3.  Correctly multiplying the answers of the sub-questions together to form the final answer.\nWe classify steps 2 and 3 above as the compositionality gap (Press et al., 2022). Step 2 is subtle but necessary since it is possible, for example, for a model to answer a sub-question correctly but then to retrieve an incorrect value for the final multiplication.\nThe results are shown in Table 7. We see that CoT correctly answers both sub-questions 36.7% of the time whereas 35.3% of the time it achieves this as well as retrieves the answers to the sub-questions to input into the final multiplication (a drop of 1.4%). However, out of the questions in which step 2 has been correctly completed, CoT only correctly completes the multiplication of these numbers 53.8% of the time. By contrast, ReAct"}, {"title": "A.3 Trigger analysis", "content": "Performance sensitivity to triggering threshold. We run ablations to test the sensitivity to the trigger threshold used in our experiments. We do this for the mathematical reasoning datasets from Section 3.2 and the multi-hop QA datasets from Section 3.3. In our experiments, the calculator hook triggers on around 70% of sentences in the mathematical reasoning datasets and the retriever hook on around 95% of the sentences in the multi-hop QA datasets. As such, we choose three new thresholds: (i) very low rate: both hooks trigger around 20-30% of the time; (ii) low rate: both hooks trigger around 50% of the time; (iii) high rate: calculator hook triggers 90-95% of the time, and retriever hook triggers > 99% of the time;\nThe results are shown in Figure 3 where we see that decreasing the rate at which a hook triggers drops performance towards the level of CoT as expected. Increasing the trigger rate activates the hooks more frequently and improves performance. Further, over-triggering does not degrade performance, demonstrating programs are robust to this. This robustness means one has an element of control in choosing thresholds and can err on the side of being trigger-happy if one's budget allows for it.\nTrigger probability distributions. Figure 4 shows the triggering probability distributions for the calculator and retriever hooks on the different datasets. We can see that whether to trigger the calculator hook is essentially a binary decision (hence a bimodal distribution on the composite dataset) since it's simply validating whether a sentence contains mathematical calculations or not. On the other hand, determining whether to trigger the retriever hook, which must determine if extra information is required, is a continuum for which we must choose a cut-off."}, {"title": "B Language hooks: implementation details", "content": "In this section we provide further details around the implementation of the language hooks used in our experiments in Section 3."}, {"title": "B.1 Calculator hook", "content": "The calculator program fixes any incorrect calculations in the most recent sentence of the context st. This is accomplished through the following steps:\n1. Extract any calculations in the last sentence. This is accomplished by prompting gpt-3.5-turbo-0301 with the prompt shown in Listing 10.\n2. Format extracted calculations to be parsed by Python. This is accomplished by prompting gpt-3.5-turbo-0301 with the prompt shown in Listing 11.\n3. Validate calculations using the SymPy Python library.\n4. Correct calculations in the original sentence. This step only occurs if the validate step identified wrong calculations. This is accomplished by prompting gpt-3.5-turbo-0301 with the prompt shown in Listing 12.\n5. Trim text after the calculation so as to avoid propagating mathematical errors. This is accomplished by prompting gpt-3.5-turbo-0301 with the prompt shown in Listing 13."}, {"title": "B.1.2 Trigger", "content": "The trigger for the calculator hook classifies between sentences containing calculations and sentences not containing calculations. This is accomplished by evaluating the OpenAI curie model using the prompt shown in Listing 7. Sentences for which the verbaliser has a model log-probability above -0.14 are triggered on."}, {"title": "B.2 Retriever hook", "content": "The retriever program enables the language model to rewrite sentences using knowledge found in external knowledge bases. This is accomplished through the following steps:\n1. Generate a set of (5) relevant search queries which can be used to find external references. This is accomplished by prompting gpt-3.5-turbo-0301 with the prompt shown in Listing 14.\n2. Search using a retriever connected to a knowledge base. In our experiments the top 3 references are kept for every search query and duplicate references are removed. This leads to up to 15 references.\n3. Re-generate the last sentence with all references added to the context. For this step the prompt is constructed in a way that encourages the language model to add explicit citations to references. This is accomplished by prompting gpt-3.5-turbo-0301 with the prompt shown in Listing 15.\n4. Check whether the generated sentence contains citations to references. If yes, overwrite the original sentence, otherwise fallback on the original sentence.\n5. Augment the context, \\(C_0\\), with any newly added references. Here we only keep references actually cited and remove all other references."}, {"title": "B.2.2 Trigger", "content": "The trigger for the retriever hook trigger classifies whether the last sentence contains information that could have been found using an external knowledge source (e.g. Wikipedia). This is accomplished by prompting the OpenAI curie model with the prompt shown in Listing 8. Sentences where the verbaliser has a log-probability above -25 are triggered on."}, {"title": "B.3 Guardrail hook", "content": "gpt-3.5-turbo-0301 is often unwilling to answer questions if it believes that its answers may contain incorrect knowledge or the question isn't well-posed. This behaviour becomes exacerbated when generating the text sentence-by-sentence, as required in the language hook framework. The guardrail hook's purpose is to nudge the language model to attempt to answer all questions, even those it is unsure of whilst making it clear in the generated text where such guesses may occur. Concretely we do this through the following steps:\n1. Augment the context with a partially completed sentence \"From reference [x] we learn that\" where [x] is a fictitious reference not given to the model."}, {"title": "B.3.2 Trigger", "content": "The goal of the guardrail hook trigger is to classify whether the language model has refused to answer the question in the last sentence. This is accomplished by prompting the OpenAI curie model with the prompt shown in Listing 9. Sentences where the verbaliser has a log-probability above -0.5 are triggered on."}, {"title": "B.4 Eligibility criteria", "content": "All of the hooks introduced in the paper were allowed to run their program only once between text generation events. This eligibility criteria ensures that any generated text is validated by each hook at most once and avoids any risk of infinite loops and wasted computation."}, {"title": "B.5 Priority order", "content": "In all of our experiments we assigned the highest priority to the retriever hook, the next highest priority to the guardrail hook and the lowest priority to the calculator hook. We set the retriever hook to have higher priority than the guardrail hook to allow the model to benefit from external information before being prompted to make a guess by the guardrail hook. The calculator hook is given the lowest priority, thus only validating calculations when the factual content of the sentence has been determined."}, {"title": "B.6 Stopping condition", "content": "The language hook algorithm continues to run until either the pre-specified maximum number of event calls is reached or a user-defined stopping criterion is satisfied. The stopping criterion is only evaluated if there are no eligible triggered hooks.\nThe stopping criterion used in our experiments does this by checking \\(s_t\\) for either an end-of-sequence token or an answer (using string matching with the answer structure found in the in-context ex-"}, {"title": "B.7 Sentence splitting", "content": "We make use of the NLTK toolkit (Loper and Bird, 2002) which implements the sentence splitting algorithm introduced in Kiss and Strunk (2006)."}, {"title": "CDataset (example questions)", "content": ""}, {"title": "C.1 GSM-HARD", "content": ""}, {"title": "C.2 Composite QA", "content": ""}, {"title": "D Prompts: trigger", "content": ""}, {"title": "E Prompts: programs", "content": ""}, {"title": "E.1 Calculator", "content": ""}, {"title": "E.2 Retriever", "content": ""}, {"title": "F Prompts: base model", "content": ""}]}