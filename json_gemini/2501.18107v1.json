{"title": "Scaling Inference-Efficient Language Models", "authors": ["Song Bian", "Minghao Yan", "Shivaram Venkataraman"], "abstract": "Scaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to 3.5\u00d7 difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training a total of 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by 1.8\u00d7 while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff.", "sections": [{"title": "1. Introduction", "content": "Scaling laws have shown immense value in guiding the development of large language models (LLMs) by establishing predictable relationships between model size, training compute, and performance metrics, such as loss and downstream tasks performance (Kaplan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023; Gadre et al., 2024). They reliably reduce the cost of training LLMs and improve model design efficiency by accurately estimating an LLM's performance via the results of smaller language models, which can be developed using far less cost and fewer computing resources.\nHowever, as the field progresses, it is increasingly evident that focusing solely on training does not adequately address the practical realities of deploying these models at scale (Touvron et al., 2023a). A key limitation of existing scaling laws is their disregard for inference costs, which dominate the long-term expenses of utilizing large models in real-world applications (Sardana et al., 2023). In other words, while compute-optimal models minimize training loss per unit of compute, they may result in models that are more expensive to serve, especially in latency-sensitive applications such as chatbots. The growing adoption of LLMs in reasoning systems also highlights the need for scaling frameworks that explicitly account for inference costs (Snell et al., 2024; Brown et al., 2024; Luo et al., 2024; Qi et al., 2024; Guan et al., 2025).\nWhile a recent study (Sardana et al., 2023) has introduced scaling laws that consider the total number of FLOPS for training and inference, their constraint requires estimating the number of tokens inferred during the model's lifespan. As inference is performed repeatedly throughout a model's lifecycle, their scaling law (Sardana et al., 2023) is not practical for real-world applications.\nIn addition, current scaling laws focus on balancing model size (number of parameters) and the number of training tokens within a fixed compute budget\u00b9 (Hoffmann et al., 2022; Muennighoff et al., 2023; Sardana et al., 2023; Gadre et al., 2024). Among these, the Chinchilla scaling law (Hoffmann et al., 2022) is the most renowned, demonstrating that the optimal training solution is D = 20N for a fixed FLOPS budget, where N is the number of parameters and D is the number of tokens for training. However, in practice, we see that FLOPs are not a primary constraint. Models are trained for durations much larger than Chinchilla optimal (e.g., 1T tokens for Llama-7B and 8T tokens for Gemma-2-9B (Touvron et al., 2023a; Team et al., 2024b)). Additionally, practitioners choose the model size (number of parameters) based on the memory capabilities of the deployment device (Hu et al., 2024; Yao et al., 2024). Thus, we need scaling laws that can explicitly consider data size, device memory, and inference latency.\nIn this work, we aim to address the following question:\nGiven dataset and parameter constraints, can we train an inference-efficient and accurate model for downstream tasks?\nIn this work, we first show that the number of parameters is not the exclusive factor affecting inference efficiency. As illustrated in Figure 2, the model architecture also plays a critical role. Following this observation, we introduce inference-efficient scaling laws, building upon the Chinchilla scaling law and incorporating model architecture considerations. Additionally, due to the disparity between model loss and accuracy in downstream tasks, we develop a novel method (Figure 6) that utilizes inference-efficient scaling laws to rank various model architectural choices. Our findings suggest that the relative ranking of loss predictions from scaling laws is more significant than their absolute values (\u00a72).\nTo fit the inference-efficient scaling laws, we train more than 60 models ranging from 80 million to 339 million parameters for up to 13 billion tokens and record the loss of models. We also train several models with more than 1 billion parameters and 20 billion tokens to evaluate the predictive power of the fitted inference-efficient scaling laws. We observe that overtraining plays a critical role in obtaining an accurate scaling law and that our inference-efficient scaling law is more accurate and robust than the Chinchilla scaling law. Using only 6 data points and 85 A100 GPU hours for curve fitting, our inference-efficient scaling law can still accurately predict the loss of scaled-up"}, {"title": "2. Scaling Laws", "content": "In this section, we first present the formulation of existing language model scaling laws in \u00a72.1. Next, we introduce a scaling law for inference efficiency that takes into account the number of parameters, training tokens, and model shape in \u00a72.2. Finally, we present a novel method to select inference-efficient language models for training using our scaling laws in \u00a72.3."}, {"title": "2.1. Preliminaries", "content": "Scaling laws predict a model's loss based on the allocated compute resource C. Following OpenAI (Kaplan et al., 2020) and Chinchilla (Hoffmann et al., 2022), the compute resource C is a function dependent on the model size N and the number of training tokens D. The goal is to minimize model loss within the constraints of the available compute resources:\n$\\arg \\min_{N,D} L(N, D) \\text{ s.t. } \\text{FLOPs}(N, D) = C$ (1)\nUsing the formulation above, several scaling laws have been established (Kaplan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023; Sardana et al., 2023) to accurately model the performance of large language models from training a series of much smaller ones. The Chinchilla loss function L(N, D)2 is widely adopted to predict a model's training loss:\n$L(N, D) = E + AN^{-\\alpha} + BD^{-\\beta}$ (2)\nwhere N is the number of parameters, D is the number of tokens used for training and A, B, \u0395, \u03b1, \u03b2 are parameters to be learned. Through training multiple models and curve fitting, Chinchilla (Hoffmann et al., 2022) identify D \u2248 20N as the compute-optimal solution for large language model pretraining."}, {"title": "2.2. Inference-Efficient Scaling Laws", "content": "Despite its popularity, the Chinchilla scaling law fails to resolve the following challenges:\n\u2022 The FLOPs constraint outlined in Eq. (1) does not reflect how model training decisions are made in practice. First, both the model size and the training corpus are determined in advance to accommodate for resource constraints when deploying these models (Touvron et al., 2023a). Therefore, for each model and training corpus pair, training FLOPs is essentially a fixed constant (assuming training epochs are also predetermined). Furthermore, while the Chinchilla scaling law suggests training a 10B parameter model with 200B tokens, overtraining frequently occurs in practice. For example, the LLaMA-3-8B model uses 15 trillion tokens for training (Touvron et al., 2023a), while the Gemma-2-9B model utilizes 8 trillion tokens (Team et al., 2024b). These numbers are 44-93x larger than the Chinchilla optimal recommendation.\n\u2022 Existing scaling laws focus only on how the number of parameters affects inference latency. However, as depicted in Figure 2, smaller models can sometimes exhibit higher inference latencies than larger models. For instance, MiniCPM-1B (Hu et al., 2024) has a higher latency compared to Qwen2.5-14B (Yang et al., 2024).\nIn view of this, we propose rewriting Eq. (1) as below to meet practical requirements:\n$\\arg \\min_{N,D} L(N, D) \\text{ s.t. } N \\leq N_c, D \\leq D_c, T_{inf} < T_c$ (3)\nwhere Nc represents the constraint on model size and Dc denotes the constraint on the number of training tokens. To account for the inference latency budget, we introduce a new term Te to our scaling law formulation to represent the inference latency constraint.\nMotivated by Figure 2, we closely examine the effect of model configuration on inference latency by altering the hidden size dmodel and number of layers nlayers as shown in Figure 3.\nFigure 3(a) shows that inference latency increases linearly with the number of layers when the hidden size remains constant. This occurs as the inference computation must be performed sequentially, one layer at a time (Yan et al., 2024). However, the matrix computations within a single layer can be performed in parallel. Furthermore, Figure 3(c) indicates that for the same number of parameters (say 7B), we can achieve different latency targets by changing the ratio of the number of hidden parameters in one layer (dmodel) vs. the number of layers (nlayers).\nPrior work (Kaplan et al., 2020) has shown the impact of the aspect ratio (dmodel/nlayers) on the performance of the model. However, it does not define the connection between model size, number of training tokens, and model shape. To establish this relationship, we trained several small models \u039d\u2208 {80, 116, 164, 237, 313}M by varying the aspect ratio and setting D\u2208 {20, 40, 160}N. Due to resource limitations, we only train a subset of the models at D = 160N. We plot the loss values against the aspect ratio in Figure 4. From the figure, we can see that the most suitable model shape adjustment is the inclusion of the term (1 + &R) to the Chinchilla scaling law (Hoffmann et al., 2022). Therefore, we derive the following inference-efficient scaling law formulation:\n$L(N, D, R) = (E + AN^{-\\alpha} + BD^{-\\beta}) \\cdot (1 + \\epsilon R^{\\gamma})$ (4)\nwhere N is the number of parameters, D is the number of training tokens, and R = dmodel/nlayers is the aspect ratio. Moreover, A, \u0392, \u0395, \u03b1, \u03b2, \u03b3,\u03b5 are learned parameters. In Figure 4, we plot the predicted values from the scaling law against the observed values from training. More details of the experimental setup and fitting procedure can be found in \u00a73."}, {"title": "2.3. Methodology", "content": "Scaling laws were first developed to predict the loss of language models. However, LLMs are evaluated on the performance of downstream tasks. A recent study (Gadre et al., 2024) attempts to establish scaling laws that link evaluation loss to errors in downstream tasks. Inherently, predicting the error in downstream tasks becomes challenging when model losses are similar, due to noise and inaccuracies in scaling laws. We observe this in Figure 5. To tackle this challenge, we develop a new method for training inference-efficient models, as shown in Figure 6. Our key idea is that inference latency measurement has negligible overhead, and scaling laws can help us estimate the loss of scaled-up models. Thus, we propose identifying top-k candidate models using inference latency and loss data, where the user can choose k. After training, we evaluate these models on downstream tasks and release the best-performing model to the public, taking into account both inference latency and performance on downstream tasks. Our method (Figure 6) can also be applied to different architectural optimizations, such as MLA (Liu et al., 2024a), to quantify the accuracy-efficiency tradeoff."}, {"title": "3. Experiments", "content": "We next discuss the experiment setup we use for model training and evaluation (\u00a73.1). Following that, in \u00a73.2, we demonstrate how to fit scaling laws using our experimental results."}, {"title": "3.1. Experimental Setup", "content": "Training Setup. For all experiments, we train transformer-based decoder-only language models (Vaswani, 2017). Following (Gururangan et al., 2023; Gadre et al., 2024), the model's architecture is similar to GPT-2 (Radford et al., 2019) and LLaMA (Touvron et al., 2023a), with GPT-NeoX (Black et al., 2022) employed as the tokenizer. We train models with a maximum of 1.5 billion parameters for up to 30 billion tokens, following the compute-optimal setup in (Hoffmann et al., 2022). The models are trained on uniformly sampled subsets of DCLM-Baseline (Li et al., 2024) with one epoch, ensuring no repetition in data (other than possible data repetition in the dataset itself). More details are included in Appendix A.\nEvaluation Setup. We use HuggingFace (Wolf, 2019) to measure the inference efficiency of models over a single NVIDIA Ampere 40GB A100 GPU. By default, we set the number of input and output tokens to be 128 and 256, respectively, aligning with the distribution outlined in ShareGPT (Kwon et al., 2023).\nWe use LLM-foundry (llm, 2024) along with a zero-shot evaluation approach to evaluate model performance on downstream tasks. We evaluate the downstream task accuracy of models derived from the methodology outlined in \u00a72.3 using the following datasets: ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), BoolQ (Clark et al., 2019), COPA (Roemmele et al., 2011), HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2020), Jeopardy (Jeo, 2022), and Winograd (Levesque et al., 2012).\nFurthermore, to compare the predicted loss against the actual loss, we measure relative prediction error: |\u03c8 \u2013 4|/\u03c8, mean squared error (MSE): \u03a3=1(Vi \u2013 Vi), and R\u00b2 = 1 \u2013 \u03a3=1(\u03c8\u03b5 \u2013 \u03a8\u03b5)\u00b2/ \u03a3\u00b2=1(\u03c8\u03b5 \u2013 \u03c8)\u00b2, where & represents the actual loss, & the predicted loss from scaling laws, and 4 = -1 . We also apply Spearman's rank correlation coefficient (Spearman, 1961) to evaluate how well the predicted rankings correspond to the actual rankings."}, {"title": "3.2. Fitting Scaling Laws", "content": "Following (Gadre et al., 2024), we use the Levenberg-Marquardt algorithm to fit Eq. (4). The Leven-berg-Marquardt algorithm solves least-squares curve fitting problems, where the goal is to find the parameter vector \u1e9e of a model f(x, \u03b2) that minimizes the sum of squared deviations. Formally, the problem can be expressed as $\\arg \\min_{\\beta} \\frac{1}{2} \\sum_{i} [Y_i \u2212 f(x_i, \\beta)]^2$, where (xi, yi) are data pairs. Following observations from Chinchilla scaling law (Hoffmann et al., 2022) and another recent work (Gadre et al., 2024), we set \u03b1, \u03b2, and y equal to simplify the fitting procedure. To fit and evaluate the scaling law, we train 63 models using a range of model sizes, shapes, and amounts of training tokens. The size of our model ranges from 80M to 339M and the number of tokens used for training ranges from 1.6B to 12.8B. Detailed model configurations can be found in Table 4 in Appendix A."}, {"title": "4. Results", "content": "In this section, we first study the predictive power of our inference-efficient scaling laws in \u00a74.1. Then, in \u00a74.2, we release an inference-efficient model that maintains accuracy on downstream tasks compared with open-sourced models by using the methodology outlined in Figure 6. We also show that our method significantly outperforms Chinchilla in predicting the best model configurations. Finally, we perform ablation studies on obtaining robust scaling laws and show that our inference-efficient scaling law is more robust than Chinchilla in various scenarios in \u00a74.3."}, {"title": "4.1. Prediction acccuracy", "content": "As shown in \u00a73.2, we obtain the actual losses of various models by training multiple small models with different model configurations to establish the scaling law. We set \u039d\u2208 {80,116,164, 237, 313}M and D = 20N to train small models and collect the data to fit the learnable parameters in Eq. (2) and Eq. (4). Furthermore, to enhance the generality of the scaling law, we train 80M models with D = 160N tokens, thereby collecting data from an over-training setting.\nThen, we train larger models on more tokens to evaluate the predictive power of our inference-efficient scaling law. We present the results in Figure 7. Figure 7 demonstrates that our scaling law achieves higher accuracy than the Chinchilla scaling law, as shown by a smaller MSE and a larger R2 (Wright, 1921) value. We reduce MSE from 0.0033 to 0.0006 while improving R\u00b2 from 0.9895 to 0.9982. In addition, the relative prediction error for the inference-efficient scaling law is less than 1.2%, whereas for the Chinchilla scaling laws, it ranges from 2.7% to 4.1%. This demonstrates that the inference-efficient scaling law predicts more accurately than the Chinchilla scaling law.\nFurthermore, as illustrated in Figure 6, prioritizing the ranking of predicted loss is more critical than its absolute value when employing the training methodology described in \u00a72.3 for inference-efficient models. We calculate Spearman's rank correlation coefficient (Spearman, 1961) for both the Chinchilla scaling law and the inference-efficient scaling law when predicting the loss of 1B models. The results are shown in Figure 7(c). The results indicate that our inference-efficient law is more effective in ranking different model configurations. For example, the inference-efficient scaling law shows a Spearman correlation of 1.00 for the 1B model loss prediction, in contrast to Chinchilla's -0.40. In Appendix A, we include more details on model configurations."}, {"title": "4.2. Inference-Efficient Models", "content": "Guided by the accurate inference-efficient scaling law, we employ the predict, rank, and select method outlined in Figure 6 to train inference-efficient models. First, we generate a range of variants from the Open-LM-1B model (Gururangan et al., 2023) by adjusting the aspect ratio. Then, we measure the inference latency of model variants on a single A100 GPU. Next, we select 3 models based on the measured inference latency and predicted loss, and train candidate models with the same training dataset. Finally, we evaluate the trained models over 20 downstream tasks and we outline the results in Figure 1 and Table 2.\nAs a baseline, the architecture of Morph-1B-v1 is identical to that of Open-LM-1B. The superior performance of Morph-1B-v1 over Open-LM-1B can be attributed to the higher quality DCLM-Baseline dataset (dcl, 2024). Additionally, OPT-IML-1.3B outperforms Morph-1B-v1 since it undergoes pre-training on 6x more unique tokens (180B vs 30B) followed by a fine-tuning stage (Iyer et al., 2022). Next, we train Morph-1B and Morph-1B-v2 which are derived from Morph-1B-v1 by modifying the aspect ratio. We use the same 30B tokens to train Morph-1B, Morph-1B-v1, and Morph-1B-v2. As illustrated in Table 2, the inference latency for Morph-1B-v1 is 1.8\u00d7 lower compared to Morph-1B, without any loss in accuracy."}, {"title": "4.3. Insights from Scaling Laws Fitting", "content": "Scaling laws provide a cheap and accurate way to predict language model performance at larger scales. However, a drawback of building scaling laws is the requirement to train models at various scales. In this section, we study how to make scaling laws robust and data-efficient.\nExclude Over-training Data. In this ablation study, we fit the scaling law based entirely on the Chinchilla-optimal setup, using only data points where training tokens are set to be Chinchilla-optimal. We vary the model size \u039d\u2208 {80, 116, 164, 237, 313}M and set the number of training tokens D = 20N, excluding data from N = 80M and D = 160N. Table 1 shows the configurations we run on and the results are shown in Figure 8. Compared to Figure 7, we observe that the inference-efficient scaling law is more robust than the Chinchilla scaling law. We achieve a much lower MSE of 0.1165 compared to Chinchilla's 0.9825 and an R2 score of 0.6293 compared to Chinchilla's -2.1259. However, we note that both scaling laws' performance deteriorates when applied to predicting losses in over-trained models. Therefore, data from over-training is essential to fit our inference-aware scaling law.\nSelect Model Shape Randomly. In this ablation study, we explore the robustness of our scaling laws via fitting models with random model architecture configurations. In this setting, the model architecture configuration for each size is chosen randomly. We randomly select a configuration from our model configuration pools (The complete list of candidate configurations can be found in Table 4 in Appendix). Figure 9 shows the experiment results. Compared to Chinchilla scaling laws, our inference-efficient scaling laws exhibit greater robustness with much smaller MSE (0.0008 vs 0.0198) and higher R\u00b2 value (0.9973 vs 0.9369). We then use these two laws to predict the loss of 1B models. The results show that the relative prediction error for the inference-efficient scaling law is less than 0.72%, significantly lower than the Chinchilla scaling law's relative prediction error, which ranges from 11.8% to 13.4%. Finally, by using only six data points to fit the two scaling laws, we significantly reduce the training costs associated with developing these laws. The GPU hours for fitting have been reduced from 450 to 85 A100 GPU hours."}, {"title": "5. Related Work", "content": "Large Language Models. Transformer (Vaswani, 2017) has been successfully applied to a variety of tasks: text classification (Wang, 2018; Sarlin et al., 2020), generation (Zellers et al., 2019; Sakaguchi et al., 2021), reasoning (Srivastava et al., 2022), and mathematics (Cobbe et al., 2021; Hendrycks et al., 2021), showcasing their broad applicability and effectiveness. The development of the GPT models (Brown et al., 2020) demonstrates that increasing the scale of language models significantly enhances their performance across various downstream tasks. The success of the GPT models has inspired the subsequent development of many large language models, including but not limited to LLaMA (Touvron et al., 2023a;b), Gemma (Team et al., 2024a;b), Qwen (Bai et al., 2023; Yang et al., 2024), and DeepSeek (Liu et al., 2024a;b; Guo et al., 2025), each designed to push the boundaries of language modeling.\nScaling Laws. Scaling laws are powerful predictors for how large language models behave as parameters increase (Kaplan et al., 2020). Plenty of subsequent works have contributed to the development of scaling laws (Hoffmann et al., 2022; Muennighoff et al., 2023; Sardana et al., 2023; Tao et al., 2024; Kumar et al., 2024; Gadre et al., 2024; Ruan et al., 2024; Abnar et al., 2025). In particular, Chinchilla scaling law (Hoffmann et al., 2022) optimizes a fixed computing budget allocation by balancing the number of model parameters against the number of training tokens to minimize the training loss. Data-Constrained scaling law (Muennighoff et al., 2023) extends the Chinchilla scaling laws by considering repeated data. The scaling laws presented in (Gadre et al., 2024) not only predict training loss under over-training scenarios but also connect training loss to downstream error. Beyond Chinchilla-Optimal (Sardana et al., 2023) attempted to account for inference cost in their scaling law. However, unlike training tokens, the number of inference tokens cannot be measured in advance. Our paper proposes a scaling law that avoids estimating the tokens generated during inference.\nInference Serving. Inference cost has drawn significant attention in recent years. Many inference systems and algorithms have been developed to speed up model serving (Olston et al., 2017; Gujarati et al., 2020; Gugger et al., 2022; Yu et al., 2022; Leviathan et al., 2023; Kwon et al., 2023; Zheng et al., 2023; Agarwal et al., 2024a;b; Ye et al., 2025; MLC team, 2023-2025). Specifically, Orca (Yu et al., 2022) utilizes continuous batching to achieve higher inference throughput. vLLM (Kwon et al., 2023) improves the throughput of popular LLMs by using PagedAttention to manage the KV cache memory. Furthermore, SGLang (Zheng et al., 2023) improves the inference throughput and latency by using RadixAttention. A recent study introduces FlashInfer (Ye et al., 2025), which employs block-sparse and composable formats to tackle KV cache storage heterogeneity."}, {"title": "6. Limitations and Future Work", "content": "Although there has been notable progress by our team, several unresolved challenges open up promising prospects for further study. First, due to resource limitations, we are unable to scale our training to include 7B models. Second, recently developed inference systems (Ye et al., 2025) can enhance inference efficiency and create new trade-offs between inference efficiency and model performance. Furthermore, Attention modules like Multi-Query Attention (MQA) (Shazeer, 2019), Grouped-Query Attention (GQA) (Ainslie et al., 2023) and Multi-Head Latent Attention (MLA) (Liu et al., 2024a) might also influence loss and inference latency. Our work provides a flexible way to quantify and predict how these architectural optimizations affect the accuracy-efficient tradeoffs. We hope this work opens up a new line of research that takes inference efficiency as an essential factor in designing language models."}, {"title": "7. Conclusion", "content": "In this work, we perform an extensive empirical study to develop scaling laws that guide us in designing inference-efficient model architecture. We first demonstrate that model architecture impacts inference efficiency and that existing scaling laws do not account for inference costs. To jointly optimize inference cost and model loss, we propose inference-efficient scaling laws. We conduct count number, each point is a number experiments to fit and evaluate the inference-efficient scaling laws. To tackle the disparity between model loss and downstream task performance, we have developed a novel methodology to train and rank inference-efficient models using our scaling law. Finally, we design and train Morph-1B model by leveraging inference-efficient scaling law, which enhances inference efficiency while maintaining accuracy in downstream tasks, compared to similar-sized open-sourced models."}, {"title": "Impact Statement", "content": "This paper presents work that aims to advance the field of Machine Learning. Our work aims to train more inference-efficient language models, potentially reducing the deployment cost of these models and their associated environmental impacts."}, {"title": "F. Contribution Statement", "content": "\u2022 Song collaborated with Minghao to set up the experimental environment and codebase and design experiments (\u00a73). Song collected all experimental data and developed the inference-efficient scaling laws based on Chinchilla scaling laws (\u00a72.2). Song proposed the methodology for training inference-efficient models (\u00a72.3), conducted all experiments (\u00a74), and prepared all figures in the paper. Ultimately, Song was responsible for writing and editing the paper.\n\u2022 Minghao proposed investigating how model configuration affects LLM scaling in inference efficiency and performance. He collaborated with Song to set up the experimental environment and codebase and design experiments (\u00a73), offered constructive suggestions throughout the project, and was responsible for writing and editing the paper.\n\u2022 Shivaram provided assistance in shuffling DCLM datasets and recommended training a 1B model that is more inference-efficient while maintaining accuracy on downstream tasks compared with other models (\u00a73). He provided numerous constructive comments throughout the project and helped us polish the entire paper."}]}