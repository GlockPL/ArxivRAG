{"title": "Robust Decision Transformer: Tackling Data Corruption in Offline RL via Sequence Modeling", "authors": ["Jiawei Xu", "Rui Yang", "Feng Luo", "Meng Fang", "Baoxiang Wang", "Lei Han"], "abstract": "Learning policies from offline datasets through offline reinforcement learning (RL) holds promise for scaling data-driven decision-making and avoiding unsafe and costly online interactions. However, real-world data collected from sensors or humans often contains noise and errors, posing a significant challenge for existing offline RL methods. Our study indicates that traditional offline RL methods based on temporal difference learning tend to underperform Decision Transformer (DT) under data corruption, especially when the amount of data is limited. This suggests the potential of sequential modeling for tackling data corruption in offline RL. To further unleash the potential of sequence modeling methods, we propose Robust Decision Transformer (RDT) by incorporating several robust techniques. Specifically, we introduce Gaussian weighted learning and iterative data correction to reduce the effect of corrupted data. Additionally, we leverage embedding dropout to enhance the model's resistance to erroneous inputs. Extensive experiments on MoJoCo, KitChen, and Adroit tasks demonstrate RDT's superior performance under diverse data corruption compared to previous methods. Moreover, RDT exhibits remarkable robustness in a challenging setting that combines training-time data corruption with testing-time observation perturbations. These results highlight the potential of robust sequence modeling for learning from noisy or corrupted offline datasets, thereby promoting the reliable application of offline RL in real-world tasks.", "sections": [{"title": "1 Introduction", "content": "Offline reinforcement learning (RL) aims to derive near-optimal policies from fully offline datasets (Levine et al., 2020; Wang et al., 2018; Fujimoto et al., 2019; Kumar et al., 2020), thereby reducing the need for costly and potentially unsafe online interactions with the environment. However, offline RL encounters a significant challenge known as distribution shift (Levine et al., 2020), which can lead to performance degradation. To address this challenge, several offline RL algorithms impose policy constraints (Wang et al., 2018; Fujimoto et al., 2019; Fujimoto and Gu, 2021; Kostrikov et al., 2021; Park et al., 2024) or maintain pessimistic values for out-of-distribution (OOD) actions (Kumar et al., 2020; An et al., 2021; Bai et al., 2022; Yang et al., 2022a; Ghasemipour et al., 2022), ensuring that the learned policy aligns closely with the training distribution. Although the majority of traditional offline RL methods rely on temporal difference learning, an alternative promising paradigm for offline RL emerges in the form of sequence modeling (Chen et al., 2021; Janner et al., 2021; Furuta et al., 2021; Shi et al., 2023; Wu et al., 2024). Unlike traditional RL methods, Decision Transformer (DT) (Chen et al., 2021), a representative sequence modeling method, treats RL as a supervised learning task, predicting actions directly from sequences of reward-to-gos, states, and actions. Prior work\n(Bhargava et al., 2023) suggests that DT excels at handling tasks with sparse rewards and suboptimal quality data, showcasing the potential of sequence modeling for real-world applications.\nWhen deploying offline RL in practical settings, dealing with noisy or corrupted data resulting from data collection or malicious attacks is inevitable (Zhang et al., 2020; Liang et al., 2024; Yang et al., 2024a). Therefore, robustly learning policies from such data becomes crucial for the deployment of offline RL. A series of prior works (Zhang et al., 2022; Ye et al., 2024b; Wu et al., 2022; Chen et al., 2024; Ye et al., 2024a) focus on the theoretical properties and certification of offline RL under data corruption. Notably, a recent study by Ye et al. (2024b) proposes an uncertainty-weighted offline RL algorithm with Q ensembles to address reward and dynamics corruption, while Yang et al. (2024b) enhance the robustness of Implicit Q-Learning (IQL) (Kostrikov et al., 2021) against corruptions on all elements using Huber loss and quantile Q estimators. These advancements, predominantly based on temporal difference learning without utilizing sequence modeling methods, prompt the intriguing question: Can sequential modeling method effectively handle data corruption in offline RL?\nIn this study, we conduct a comparative analysis of DT with traditional offline RL methods under different data corruption settings. Surprisingly, we observe that traditional methods relying on temporal difference learning struggle in limited data settings and notably underperform DT. Moreover, DT outperforms prior methods under state attack scenarios. These observations highlight the promising potential of sequential modeling for addressing data corruption challenges in offline RL. To further unlock the capabilities of sequential modeling methods under data corruption, we propose a novel algorithm called Robust Decision Transformer (RDT). RDT incorporates three robust techniques for DT, including Gaussian weighted learning, iterative data correction, and embedding dropout to mitigate the impact of corrupted data.\nThrough comprehensive experiments conducted on a diverse set of tasks, including MoJoCo, KitChen, and Adroit, we demonstrate that RDT outperforms conventional temporal difference learning and sequence modeling approaches under both random and adversarial data corruption scenarios. Additionally, RDT exhibits remarkable robustness in a challenging setting that combines training-time data corruption with testing-time observation perturbations. Our study emphasizes the significance of robust sequential modeling and offers valuable insights that would contribute to the trustworthy deployment of offline RL in real-world applications."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 RL and Offline RL", "content": "RL is generally formulated as a Markov Decision Process (MDP) defined by a tuple (S, A, P, r, \u03b3). This tuple comprises a state space S, an action space A, a transition function P, a reward function r, and a discount factor \u03b3\u2208 [0, 1). The objective of RL is to learn a policy \u03c0(a|s) that maximizes the expected cumulative return:\n$\\max_{\\pi} \\mathbb{E}_{s_0 \\sim p_0, a_t \\sim \\pi (.\\vert s_t), S_{t+1} \\sim P(.\\vert s_t, a_t)} \\left[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)\\right]$\n, where p0 denotes the distribution of initial states. In offline RL, the objective is to optimize the RL objective with a previously collected dataset $\\mathcal{D} = \\{(s_0^{(i)}, a_0^{(i)}, ..., s_{T-1}^{(i)}, a_{T-1}^{(i)}, s_T^{(i)}\\)^{(i)}_{i=0}\\,which consists\nof trajectories of length T and a total of N trajectories. The agent cannot directly interact with the\nenvironment during the offline phase."}, {"title": "2.2 Decision Transformer (DT)", "content": "Different from the MDP framework, DT models decision making from offline datasets as a sequential\nmodeling problem. The i-th trajectory $\\tau^{(i)}$ of length T in dataset $\\mathcal{D}$ is reorganized into a sequence of\nreturn-to-go $R_0^{(i)}$, state $s_0^{(i)}$, action $a_0^{(i)}$:\n$\\alpha^{(i)} = (R_0^{(i)}, s_0^{(i)}, a_0^{(i)}, ..., R_{T-1}^{(i)}, s_{T-1}^{(i)}, a_{T-1}^{(i)})$\nHere, the return-to-go $R_t^{(i)}$ is defined as the sum of rewards from the current step to the end of the\ntrajectory: $R_t^{(i)} = \\sum_{t'=t}^T r_t^{(i)}$. DT employs three linear projection layers to project the return-to-gos,"}, {"title": "2.3 Data Corruption in Offline RL", "content": "In prior works (Ye et al., 2024b; Yang et al., 2024b), the data is stored in transitions, and data\ncorruption is performed on individual elements (state, action, reward, next-state) of each transition.\nThis approach does not align well with trajectory-based sequential modeling methods like DT. In this\npaper, we consider a unified trajectory-based storage, where corrupting a next-state in a transition\ncorresponds to corrupting a state in the subsequent transition, while corruption for rewards and\nactions is consistent with prior works. To elaborate, an original trajectory is denoted as $\\tau_{origin}^{(i)} =\n(s_0^{(i)}, a_0^{(i)}, ..., s_{T-1}^{(i)}, a_{T-1}^{(i)}, s_T^{(i)}\\)$, which can be reorganized into the sequence data of DT in\nEq. 1 or split into T \u2212 1 transitions $(s_t^{(i)}, a_t^{(i)}, r_t^{(i)}, s_{t+1}^{(i)})_{t=0}^{T-2}$ for MDP-based methods. Note that\nhere only are three independent elements (i.e., states, actions, and rewards) under the trajectory-based\nstorage formulation.\nData corruption injects random or adversarial noise into the original states, actions, and rewards.\nRandom corruption adds random noise to the affected elements in the datasets, resulting in a\ncorrupted trajectory denoted as $\\tau_{corrupt}^{(i)} = (s_0^{(i)}, a_0^{(i)}, ..., s_{T-1}^{(i)}, a_{T-1}^{(i)}, s_T^{(i)}\\)$. For instance,\ns_t^{(i)} = s_t^{(i)} + \\sigma \\cdot \\xi$, $\\xi \\sim Uniform[-\\epsilon, \\epsilon]^{d_s}$ (ds is the dimensions of state, and \u03f5 is the corruption\nscale) and std(s) is the ds-dimensional standard deviation of all states in the offline dataset. In\ncontrast, adversarial corruption uses Projected Gradient Descent attack (Madry et al., 2017) with\npretrained value functions. More details about data corruption are provided in Appendix C.1."}, {"title": "3 Sequence Modeling for Offline RL with Data Corruption", "content": "We aim to answer the question of whether sequential modeling methods can effectively handle data\ncorruption in offline RL in Section 3.1. To achieve this, we compare DT with prior offline RL methods\nin the context of data corruption. Based on the insights gained from the motivating example, we then\npropose enhancements for improving the robustness of DT in Section 3.2."}, {"title": "3.1 Motivating Example", "content": "As illustrated in Figure 1, we compare DT with various offline RL algorithms under data corruption.\nWe apply random corruption introduced in Section 2.3 on states, actions, and rewards. Specifically,"}, {"title": "3.2 Robust Decision Transformer", "content": "To enhance the robustness of DT against various data corruption, we propose Robust Decision\nTransformer (RDT) by incorporating three components: embedding dropout (Section 3.2.1), Gaussian\nweighted learning (Section 3.2.2), and iterative data correction (Section 3.2.3). Notably, RDT predicts actions and rewards (rather than reward-to-gos) on top\nof the DT. Given the typically high dimensionality of states, we avoid predicting states to mitigate\npotential negative impacts. Additionally, rewards provide more direct supervision for the policy\ncompared to reward-to-gos, which are also dependent on future actions."}, {"title": "3.2.1 Embedding Dropout", "content": "In the setting of data corruption, corrupted states, actions, and rewards can supply shifted inputs\nor erroneous features to the model. This can lead to a performance drop if the model overfits to\nparticular harmful features. Therefore, learning a robust representation is important to enhance the\nmodel's resilience against data corruption scenario. To achieve this, we employ embedding dropout,\nwhich encourages the model to learn more robust embedding representations and avoid overfitting\n(Merity et al., 2017)."}, {"title": "3.2.2 Gaussian Weighted Learning", "content": "In DT, actions serve as the most crucial supervised information, acting as the labels. Consequently,\nerroneous actions can directly influence the model through backpropagation. In RDT, we predict both\nactions and rewards, with rewards helping to reduce overfitting to corrupted action labels. To further\nmitigate the impact of corrupt data, we aim to reduce the influence of unconfident action and reward\nlabels that could erroneously guide policy learning.\nTo identify uncertain labels, we adopt a simple but effective method: we use the value of the sample-\nwise loss to adjust the weight for the DT loss. The underlying insight is that a corrupted label would\ntypically lead to a larger loss. To softly reduce the effect of potentially corrupted labels, we use the\nGaussian weight, i.e., a weight that decays exponentially in accordance with the sample's loss. This\nis mathematically represented as:\nw_{a_t}^{(i)} = e^{-\\beta_a \\cdot \\delta_{at}}, w_{r_t}^{(i)} = e^{-\\beta_r \\cdot \\delta_{rt}}, where \\delta_{at} = no\\_grad(||\\pi_{\\theta}(\\tau_{t-K+1:t-1}^{(i)}) - a_t^{(i)}||^2), \\\\\n\\delta_{rt} = no\\_grad(||\\pi_{\\theta}(\\tau_{t-K+1:t-1}^{(i)}) - r_t^{(i)}||^2),\nIn this equation, \u03b4at and \u03b4rt represent the detached prediction errors at step t. The variables\n\u03b2a \u2265 0, \u03b2r \u2265 0 act as the temperature coefficients, providing flexibility to control the \"blurring\"\neffect of the Gaussian weights. With a larger value, more samples would be down-weighted. The loss\nfunction of RDT is expressed as follows:\n$\\mathcal{L}_{RDT}(\\theta) = \\mathbb{E}_{(\\tau^{(i)}) \\sim D} \\left[ \\frac{1}{K} \\sum_{t=0}^{K-1} w_{at}^{(i)} \\left\\|\\pi_{\\theta}(\\tau_{t-K+1:t-1}^{(i)}) - a_t^{(i)}\\right\\|^2 + w_{rt}^{(i)} \\left\\|\\pi_{\\theta}(\\tau_{t-K+1:t-1}^{(i)}) - r_t^{(i)}\\right\\|^2 \\right]$\nGaussian weighted learning enables us to mitigate the detrimental effects of corrupted labels, thereby\nenhancing the algorithm's robustness."}, {"title": "3.2.3 Iterative Data Correction", "content": "While Gaussian weighted learning has significantly reduced the impact of corrupted labels, these\nerroneous data can still influence the policy as they still serve as inputs for policy learning. We\npropose iteratively correcting corrupted data in the dataset using the model's predictions to bring the\ndata closer to their true values in the next iteration. This method can further minimize the detrimental\neffects of corruption and can be implemented to correct reward-to-gos, states, and actions in the\nsequence modeling framework. In RDT, it is straightforward to correct the actions and rewards in the\ndataset, and recalculate the reward-to-gos using corrected rewards. Therefore, our implementation\nfocuses on correcting states and reward-to-gos in the datasets, leaving better data correction methods\nfor states for future work.\nInitially, we store the distribution information of prediction error \u03b4 in Eq.4 throughout the learning\nphase to preserve the mean \u00b5s and variance of of actions and rewards, updating them with every\nbatch of samples. Our hypothesis is that the prediction error \u03b4 between predicted and clean label\nactions should exhibit consistency after sufficient training. Therefore, if erroneous label actions"}, {"title": "4 Experiments", "content": "In this section, we conduct a comprehensive empirical assessment of RDT to address the following\nquestions: 1) How does RDT perform under several data corruption scenarios during the training\nphase? 2) Is RDT robust to different data corruption rates and scales? 3) What is the impact of\neach component of RDT on its overall performance? 4) How does RDT perform when faced with\nobservation perturbations during the testing phase?"}, {"title": "4.1 Experimental Setups", "content": "We evaluate RDT across a variety of tasks, such as MoJoCo (\"medium-replay-v2\" datasets), KitChen,\nand Adroit (Fu et al., 2020). Two types of data corruption during the training phase are simulated:\nrandom corruption and adversarial corruption, attacking states, actions, and rewards, respectively.\nThe corruption rate is set to 0.3 and the corruption scale is \u03f5 = 1.0 for the main results. Data\ncorruption is introduced in Section 2.3 and more details can be found in Appendix C.1. Additionally,\nwe find that the data corruption problem can exacerbate when the data is limited. To simulate this\nscenario, we down-sample on both MuJoCo and Adroit tasks, referred to as MuJoCo (10%) and\nAdroit (1%), Specifically, we randomly select 10% (and 1%) of the trajectories from MuJoCo (and\nAdroit) tasks and conduct data corruption on the sampled data. We do not down-sample the Kitchen\ndataset because it already has a limited dataset size. All considered tasks have a similar dataset size\nof about 20,000 transitions, except for the original MuJoCo task, which includes a larger dataset.\nWe compare RDT with several SOTA offline RL algorithms and robust methods, namely BC,\nRBC (Sasaki and Yamashina, 2020), DeFog (Hu et al., 2023), CQL (Kumar et al., 2020), UWMSG (Ye\net al., 2024b), RIQL (Yang et al., 2024b), and DT (Chen et al., 2021). BC and RBC employ\nbehavior cloning loss within an MLP-based model for policy learning, while DeFog and DT utilize a\nTransformer architecture. We implement RBC with our Gaussian-weighted learning as an instance\nof (Sasaki and Yamashina, 2020). In Appendix D.1, we investigate the robustness of DT across\nvarious critical parameters. Drawing from these findings on DT's robustness, we establish a default\nimplementation of DT, DeFog, RDT with a sequence length of 20 and a block number of 3. To ensure\nthe validity of the findings, each experiment is repeated using 4 different random seeds."}, {"title": "4.2 Evaluation under Various Data Corruption", "content": "Random Data Corruption. To address the first question, we evaluate the RDT and baselines on\nrandom data corruption across a diverse set of task groups. Each group consists of three datasets,\nand the mean normalized score was calculated to represent the final outcomes. As shown in Table 1,\nRDT demonstrates superior performance in handling data corruption, achieving the highest score in\n10 out of 12 settings, particularly in the challenging Kitchen and Adroit tasks. Notably, across all\ntask groups, RDT outperforms DT, underscoring its effectiveness in reducing DT's sensitivity to data\ncorruption across all elements. Moreover, RBC significantly outperforms BC, further highlighting\nthe effectiveness of Gaussian weighted learning. However, prior offline RL methods, such as RIQL\nand UWMSG, fail to yield satisfactory results and even underperform BC overall, despite showing\nenhanced performance on the full MuJoCo dataset. This indicates that temporal difference methods\nare dependent on large-scale datasets.\nMixed Random Data Corruption. To\npresent a more challenging setting for the\nrobustness of RDT, we conduct an experi-\nment under the mixed random data corrup-\ntion setting. In this setting, all three ele-\nments - states, actions, and rewards - are cor-\nrupted at a rate of 0.3 and a scale of \u03f5 = 1.0.\nThe experimental results are shown in Fig-\nure 3. Remarkably, RDT consistently out-\nperforms other baseline models across all\ntasks, thereby highlighting its superior sta-\nbility even when faced with simultaneous\nand diverse random data corruption. Espe-\ncially on the challenging Kitchen task, RDT\noutperforms DT by an impressive margin of approximately 40%.\nAdversarial Data Corruption. We further extend the analysis to examine the robustness of the\nRDT under adversarial data corruption. We employ the same hyperparameters for RDT as those\nutilized in the random data corruption scenarios, demonstrating RDT's advantage of not requiring\nmeticulous hyperparameter tuning. As evidenced in Table 2, RDT consistently showcases a robust\nperformance by attaining the highest scores in 8 out of 9 settings and sustaining the top average\nperformance. Notably, RDT stands out for enhancing the average score by 28.9% compared to DT\nin the adversarial action corruption scenario. Intriguingly, temporal-difference methods like CQL,\nUWMSG, and RIQL perform notably worse than BC and sequence modeling methods such as DT\nand DeFog, highlighting the promise of sequence modeling approaches. We provide detailed results\nfor each task in Appendix D for comprehensive analysis."}, {"title": "4.3 Varying Corruption Rates and Scales", "content": "In the above experiments, we evaluate the effectiveness of RDT under a data corruption rate of\n0.3 and a scale of 1.0. We further examine the robustness of RDT under different corruption rates\nfrom {0.0, 0.1, 0.3, 0.5} and scales from {0.0, 1.0, 2.0}. As illustrated in Figure 4, RDT consistently\ndelivers superior performance compared to other baselines across different corruption rates and scales."}, {"title": "4.4 Ablation Study", "content": "We conduct comprehensive ablation studies to analyze the impact of each component on RDT's\nrobustness. For evaluation, we use the \"walker2d-medium-replay\", \"kitchen-compete\", and \"relocate-exper\" datasets. Specifically, we compare several variants of RDT: (1) DT(RP), which incorporates\nreward prediction in addition to the original DT; (2) DT(RP) w. ED, which adds embedding dropout\nto DT(RP); (3) DT(RP) w. GWL, which applies Gaussian weighted learning on top of DT(RP); and\n(4) DT(RP) w. IDC, which integrates only the iterative data correction method.\nWe evaluate the performance of these variants under different data corruption scenarios. In summary, all variants demonstrate improvements over DT(RP), proving the effectiveness\nof the individual components. Notably, Gaussian weighted learning appears to provide the most\nsignificant contribution, particularly under reward attack. However, it is important to note that none\nof these tailored models outperform RDT, indicating that the integration of all proposed techniques\nis crucial for achieving optimal robustness. Additionally, we provide ablation results on reward\nprediction in Appendix D.2, which demonstrate that reward prediction also brings improvements\nunder state and action corruption compared to DT."}, {"title": "4.5 Evaluation under Observation Perturbation during the Testing Phase", "content": "In this section, we further investigate the robustness of RDT when deployed in perturbed environments\nafter being trained on corrupted data, a challenging setting that includes both training-time and testing-time attacks. To address this, we evaluate RDT under two types of observation perturbations during\nthe testing phase: random and action diff, following prior works (Yang et al., 2022a; Zhang et al.,\n2020). The perturbation scale is used to control the extent of influence on the observation. Detailed\nimplementation of observation perturbations during the testing phase is provided in Appendix C.2.\nThe comparison results are presented in Figure 6, where all algorithms are trained on the offline dataset\nwith action corruption and evaluated under observation perturbation. These results demonstrate the\nsuperior robustness of RDT under the two types of observation perturbations, maintaining stability\neven at a high perturbation scale of 0.5. Notably, RORL (Yang et al., 2022a), a SOTA offline RL\nmethod designed to tackle testing-time observation perturbation, fails to perform effectively on these"}, {"title": "5 Related Work", "content": "Robust Offline RL. In offline RL, several works have focused on testing-time robustness against\nenvironment shifts (Shi and Chi, 2022; Yang et al., 2022a; Panaganti et al., 2022; Zhihe and Xu,\n2023). Regarding training-time robustness, Li et al. (2023) explore various types of reward attacks in\noffline RL and find that certain biases can inadvertently enhance the robustness of offline RL methods\nto reward corruption. From a theoretical perspective, Zhang et al. (2022) propose a robust offline\nRL algorithm utilizing robust supervised learning oracles. Additionally, Ye et al. (2024b) employ\nuncertainty weighting to address reward and dynamics corruption, providing theoretical guarantees.\nThe most relevant research by Yang et al. (2024b) employs the Huber loss to handle heavy-tailedness\nand utilizes quantile estimators to balance penalization for corrupted data. Furthermore, Mandal et al.\n(2024); Liang et al. (2024) enhance the resilience of offline algorithms within the RLHF framework.\nIt is important to note that these studies primarily focus on enhancing temporal difference methods,\nwith no emphasis on leveraging sequence modeling techniques to tackle data corruption.\nTransformers for RL. Recent research has redefined offline RL decision-making as a sequence\nmodeling problem using Transformer architectures (Chen et al., 2021; Janner et al., 2021). Unlike\ntraditional RL methods, these studies treat RL as a supervised learning task at a trajectory level. A\nseminal work, Decision Transformer (DT) (Chen et al., 2021), uses trajectory sequences to predict\nsubsequent actions. Trajectory Transformer (Janner et al., 2021) discretizes input sequences into\ntokens and employs beam search to predict the next action. These efforts have led to subsequent\nadvancements. For instance, Prompt DT (Xu et al., 2022) integrates demonstrations for better\ngeneralization, while Xie et al. (2023) introduce pre-training with future trajectory information.\nQ-learning DT (Yamagata et al., 2023) refines return-to-go using Q-values, and Agentic Transformer\n(Liu and Abbeel, 2023) uses hindsight to relabel target returns. LaMo (Shi et al., 2023) leverages pre-\ntrained language models for offline RL, and DeFog (Hu et al., 2022) addresses robustness in specific\nframe-dropping scenarios. Our work deviates from these approaches by focusing on improving\nrobustness against data corruption in offline RL."}, {"title": "6 Conclusion", "content": "In this study, we investigate the robustness of offline RL algorithms under various data corruptions,\nwith a specific focus on sequence modeling methods. Our empirical evidence suggests that current\noffline RL algorithms based on temporal difference learning exhibit significant susceptibility to\ndata corruption, particularly in scenarios with limited data. To address this issue, we introduce\nthe Robust Decision Transformer (RDT), a novel robust offline RL algorithm developed from the\nperspective of sequence modeling. Our comprehensive experiments highlight RDT's exceptional\nrobustness against both random and adversarial data corruption, across different corruption ratios\nand scales. Furthermore, we demonstrate RDT's superiority in handling both training-time and\ntesting-time attacks. We hope that our findings will inspire further research into the exploration of\nusing sequence modeling methods to address data corruption challenges in increasingly complex and\nrealistic scenarios."}, {"title": "A Algorithm Pseudocode", "content": "To provide an overview and better understanding, we detail the implementation of Robust Decision\nTransformer (RDT) in Algorithm 1.\nAlgorithm 1 Robust Decision Transformer (RDT)\nRequire: Offline dataset D, sequence model \u03c0\u03bf, initialed mean \u00b5s and variance \u03c3\u03be.\n1: for training step= 1, 2, ..., T do\nExtract batch Tt:t+K-1 from the offline dataset D.\n2:\n3:\nUpdate sequence model \u03c0\u03bf based on Eq. 5.\n4:\n5:\n6: end for\nCompute prediction errors \u03b4\u03b1\u2081 and dr\u2081 in batch data.\nUpdate corresponding mean \u00b5s and variance \u03c3\n7: if correction phase begins then\n8:\n9:\nEvaluate the z-score to identify the corrupted action aft) and reward (2).\nSubstitute corrupted actions and rewards with the predicted actions and rewards in dataset D.\n10: end if"}, {"title": "B Additional Related Works", "content": "Offline RL. Maintaining proximity between the policy and data distribution is essential in offline\nRL, as distributional shifts can lead to erroneous estimations (Levine et al., 2020). To counter\nthis, offline RL algorithms are primarily divided into two categories. The first category focuses on\npolicy constraints on the learned policy (Wang et al., 2018; Fujimoto et al., 2019; Li et al., 2020;\nFujimoto and Gu, 2021; Kostrikov et al., 2021; Emmons et al., 2021; Yang et al., 2022b; Sun et al.,\n2024; Xu et al., 2023; Park et al., 2024). The other category learns pessimistic value functions\nto penalize OOD actions (Kumar et al., 2020; Yu et al., 2020; An et al., 2021; Bai et al., 2022;\nYang et al., 2022a; Ghasemipour et al., 2022; Sun et al., 2022; Nikulin et al., 2023; Huang et al.,\n2024). To enhance the potential of offline RL in handling more complex tasks, recent research has\nintegrated advanced techniques like GAN (Vuong et al., 2022; Wang et al., 2023), transformers (Chen\net al., 2021; Chebotar et al., 2023; Yamagata et al., 2023) and diffusion models (Janner et al., 2022;\nHansen-Estruch et al., 2023; Wang et al., 2022)."}, {"title": "C Implementation Details", "content": ""}, {"title": "C.1 Data Corruption Details during Training Phase", "content": "Our study utilizes both random and adversarial corruption across three elements: states, actions, and\nrewards. We consider a range of tasks including MuJoCo, Kitchen, and Adroit. Particularly, we\nutilize the \"medium-replay-v2\" datasets in the MuJoCo tasks with sampling ratios of 100% and 10%,\nthe \"expert-v0\" datasets in the Adroit tasks with a sampling ratio of 1%, and we employ full datasets\nfor the tasks in the Kitchen due to their already limited data size. These datasets (Fu et al., 2020) are\ncollected either during the training process of an SAC agent or from expert demonstrations, thereby\nproviding highly diverse and representative tasks of the real world. To control the overall level of\ncorruption within the datasets, we introduce two parameters c and e following previous work (Yang\net al., 2024b). The parameter c signifies the rate of corrupted data within a dataset, whilst e represents\nthe scale of corruption observed across each dimension. We outline three types of random data\ncorruption and present a comprehensive overview of a mixed corruption approach as follows. Note\nthat in our setting, only three independent elements (i.e., states, actions, and rewards) are considered\nunder the trajectory-based storage approach.\n\u2022 Random state attack: We randomly sample c.NT states from all trajectories, where\nN refer to the number of trajectories and T represents the number of steps in a trajectory.\nWe then modify the selected state to \u015d = s + \u03bb \u00b7 std(s), \u5165 ~ Uniform[\u22126, 6]ds. Here, ds\nrepresents the dimension of states, and \"std(s)\" is the ds-dimensional standard deviation of\nall states in the offline dataset. The noise is scaled according to the standard deviation of\neach dimension and is independently added to each respective dimension."}, {"title": "C.2 Observation Perturbation Details during the Testing Phase", "content": "We evaluate the Robust Decision Transformer (RDT) under two types of perturbations during the\ntesting phase: random and action diff perturbations, as described in prior works (Yang et al., 2022a;\nZhang et al., 2020). Both perturbation types target the observations during the testing phase. The\ndetailed implementations are as follows:\n\u2022 Random: We sample perturbed states within an l\u221e ball of norm \u03f5. Specifically, we create\nthe perturbation set Ba(s, e) = {\u015d : d(s, \u015d) < \u03f5} for state s, where d(\u00b7) is the l\u221e norm, and\nsample one perturbed state to return to the agent.\n\u2022 Action Diff: This is an adversarial attack based on the pretrained IQL deterministic policy\n\u03bc(s). We first sample 50 perturbed states within an l\u221e ball of norm \u03f5 and then find the one\nthat maximizes the difference in actions: max\u015d\u2208Ba(s,e) ||\u03bc(s) \u2013 \u03bc(\u015d)||2.\nThe parameter e controls the extent of the observation perturbation. In this experiment, we first train\nRDT under different data corruption scenarios and then evaluate its performance in environments\nsubjected to observation perturbation."}, {"title": "C.3 Implementation Details of RDT", "content": "We implement the DT and RDT algorithms using the existing code base. Specifically. we build the\nnetwork with 3 Transformer blocks, incorporating one MLP embedding layer for each key element:\nstate, action, and return-to-go. We update the neural network using the AdamW optimizer, with a\nlearning rate set at 1 \u00d7 10-4 and a weight decay of 1 \u00d7 10-4. The batch size is set to 64, with a\nsequence length of 20, to ensure effective and efficient training. To maintain stability during training,\nwe adopt the state normalization as in (Yang et al., 2024b). During the training phase, we train DeFog,\nDT and RDT for 100 epochs and other baselines with 1000 epochs. Each epoch is characterized by\n1000 gradient steps. For evaluative purposes, we rollout each agent in the clean environment across\n10 trajectories, each with a maximum length of 1000, and we average the returns from these rollouts"}, {"title": "D Additional Experiments", "content": ""}, {}]}