{"title": "SemUV: Deep Learning based semantic\nmanipulation over UV texture map of virtual\nhuman heads", "authors": ["Anirban Mukherjee", "Venkat Suprabath Bitra", "Vignesh Bondugula", "Tarun Reddy Tallapureddy", "Dinesh Babu Jayagopi"], "abstract": "Designing and manipulating virtual human heads is essential\nacross various applications, including AR, VR, gaming, human-computer\ninteraction and VFX. Traditional graphic-based approaches require man-\nual effort and resources to achieve accurate representation of human\nheads. While modern deep learning techniques can generate and edit\nhighly photorealistic images of faces, their focus remains predominantly\non 2D facial images. This limitation makes them less suitable for 3D\napplications. Recognizing the vital role of editing within the UV texture\nspace as a key component in the 3D graphics pipeline, our work focuses\non this aspect to benefit graphic designers by providing enhanced control\nand precision in appearance manipulation. Research on existing methods\nwithin the UV texture space is limited, complex, and poses challenges.\nIn this paper, we introduce SemUV: a simple and effective approach us-\ning the FFHQ-UV dataset for semantic manipulation directly within the\nUV texture space. We train a StyleGAN model on the publicly available\nFFHQ-UV dataset, and subsequently train a boundary for interpolation\nand semantic feature manipulation. Through experiments comparing our\nmethod with 2D manipulation technique, we demonstrate its superior\nability to preserve identity while effectively modifying semantic features\nsuch as age, gender, and facial hair. Our approach is simple, agnostic to\nother 3D components such as structure, lighting, and rendering, and also\nenables seamless integration into standard 3D graphics pipelines without\ndemanding extensive domain expertise, time, or resources.", "sections": [{"title": "1 Introduction", "content": "Virtual humans are computer-generated characters designed to appear and be-\nhave like real people, created using technologies such as computer graphics, an-\nimation, and artificial intelligence. Creating and animating virtual humans is\nan active area of research due to their use in various application domains such\nas VR, AR, Mixed Reality, Human-Computer Interaction, Gaming and enter-\ntainment such as VFX and CGI. Designing and editing the facial appearance of"}, {"title": "2 Related works", "content": "The topic of generation and manipulation of facial images has been a common\narea of research in computer vision [14]. Modern generative models such as style-"}, {"title": "3 Our approach", "content": "In our proposed method, we operate within the space of albedo UV texture maps,\nwhich store the fundamental color information [13] of a virtual human head. Our\nmethod involves learning the UV texture feature space using a generative model,\nsubsequently allowing us to perform interpolations in the latent texture space\nof the model, corresponding to semantic manipulations in the UV texture map.\nOnce the texture map is wrapped onto a head mesh, the rendered results have\nthe changes in the virtual human's head (Fig. 2). To train such as model, first\nwe sample from a labeled dataset [1] with UV albedo texture maps of human\nfaces. Then, we learn the distribution of the texture space using the architecture\nof StyleGANv2-ada [17]. After this, we train an SVM to learn a boundary for\ndisentangling the latent space of the trained GAN model via subspace projec-\ntion [24]. This allows linear interpolation in the latent space resulting in precise\nmanipulation of semantics of the face such as facial hair, age and gender without\nchanging other aspects such as identity."}, {"title": "3.1 FFHQ-UV Dataset", "content": "We have used the FFHQ-UV dataset [1], which is derived from the FFHQ dataset\n[16], a large-scale face image dataset. FFHQ-UV contains over 50,000 images of"}, {"title": "3.2 Learning the UV texture space", "content": "To learn the UV space, we train a state-of-the-art generative vision model, com-\nmonly used for facial-image generation. StyleGANv2-Ada [15] is a generative\nadversarial network (GAN) [7] based architecture, whose strengths are a combi-\nnation of StyleGANv2 [17] and Adaptive Instance Normalization (AdaIN) [12].\nSimilar to the basic version of StyleGAN [16], the generator produces an inter-\nmediate feature map, w, by mapping z through a mapping network, denoted as\nW. This mapping network learns a non-linear mapping function that projects\nz to a disentangled latent space, where different dimensions control various at-\ntributes of the generated image. The intermediate feature map w is then passed\nthrough a series of convolutional layers, each followed by an AdaIN operation,"}, {"title": "3.3 Interpolating semantic features", "content": "GANs learn various semantics in linear subspaces of the latent space, which can\nbe manipulated without retraining the model. Building upon the idea by [24],\nwe train a linear Support Vector Machine (SVM) [4] to classify between the\nlevels of each semantic features within the learned latent space. For our model,\nwe choose the criteria of age, facial hair and gender, as per the available labels\nfrom the FFHQ-UV dataset. However, it must be noted that this method can\nbe scaled to any number of semantic features given a labeled dataset. Using\nthe labels, the SVM learns to classify the latent vectors, treating those with\nhighest attribute values in the available labels as positive samples, and those\nwith the lowest values as negative. After training, the coefficients of the SVM\nare used as the boundary vector for the features. To perform a modification of\na semantic in a given image with N steps, we use a projection method proposed\nin [15] to get the latent vector from the image, and subsequently interpolate N\nsteps in the latent space along the direction orthogonal to the boundary vector.\nThese interpolated steps correspond to incremental manipulation in the semantic\nfeatures. By ensuring orthogonalization of all the boundaries, we disentangle\nthe interpolation of semantic features. The ability to linearly interpolate with\ndesired number of steps allows precise control over the manipulation, crucial\nfor the process of designing and manipulating appearance of 3D human head\nmodels."}, {"title": "4 Experiment", "content": "We select 10000 UV maps from the FFHQ-UV dataset to train our model. We\nchoose the architecture of [15] since it is the state-of-the-art method for gener-\nating images resembling the training data. We train it on the dataset for 3000\nepochs on NVIDIA GeForce RTX 2080 Ti. Each epoch was run for 4000 images,\nfollowing the official PyTorch StyleGAN ADA implementation.\nThen we use the labels from the dataset to train a SVM, to be able to classify\nthe latent vectors as per the attributes, as mentioned in Sec 3.3."}, {"title": "5 Results", "content": "After training, we have a learned space of UV texture maps. We show the im-\nprovements while training the model using Fr\u00e9chet Inception Distance (FID)\nscore [9] and Kernel Inception Distance (KID) [2] scores across the epochs (Fig.\n4, both of which measure the similarity between real and generated samples.\nLower values of FID and KID indicate that the generated samples are of high\nquality, closely resembling the real samples in terms of visual appearance."}, {"title": "5.1 Interpolated results", "content": "To visualize the working of SemUV on head UV texture maps, we sample from\nthe dataset and perform semantic manipulation on 3 the features: age, facial\nhair, and gender.\nAfter performing the manipulations, we project the texture maps on the 3D\nmesh provided by the dataset on Blender 2, a 3D graphics engine. Once we have\nprojected the UV map, we have a complete 3D head model which we can view\nfrom any direction. Figs. 5a, 5b, 5c show the result of the semantic manipulations\non the head figure from various angles.\nAge: For the change in age in Fig. 5b, going from left to right we observe the\nfeatures of the face being similar to that of a child, young teen, and a grown"}, {"title": "5.2 Comparing with image-space manipulations", "content": "To evaluate the performance with respect to 2D image based approaches, we\nselect the texture map obtained using the pipeline of FFHQ-UV on a custom\nimage, and manipulate the age of the virtual head.\nWe choose age attribute in particular because it is a common attribute be-\ntween the FFHQ-UV labels, and InterFaceGAN [24], which we use as a bench-\nmark for performing semantic manipulation after inverting a given face image\nusing [28]. Gender was another common attribute between the two approaches,\nhowever the presence of hair in the image space, and lack of hair on the 3D head\ncould give biased results while user evaluation."}, {"title": "6 Discussion", "content": "In this section, we look into the key insights regarding the importance of SemUV,\nbased on our conducted experiments. We examine the limitations of our approach\nand explore potential future research directions aimed at enhancing our method\nand expanding the range of successful semantic manipulations of virtual heads\nin UV space."}, {"title": "6.1 Significance of SemUV:", "content": "Having a model capable of performing manipulations directly in UV space presents\na significant advantage for designers operating in the realm of 3D graphics. While\ntransformations generated by models operating in the 2D image space, such as\nincreasing hair length while changing gender from male to female or adding\nglasses when increasing age, may seem logically valid, they are not suitable for\nthe process of graphic designing processes, where such elements are typically"}, {"title": "6.2 Limitations:", "content": "While our approach is successfully able to perform manipulations in the semantic\naspects, there are challenges in disentangling certain features in the latent space\nwhere the labels are noisy. In Fig. 3, we see that although there was no facial hair\nin the initial image, the aged version has added slight patches of white hair. Such\nchallenges could be tackled by using more labelled examples, as well as modified\nmethods for non-linear separation boundaries for latent space disentanglement."}, {"title": "6.3 Future scope:", "content": "Some future research directions could explore various realistic face UV datasets.\nWith a dataset of high quality and diverse representations of texture maps, more\nphotorealistic images could be created which will result in even more realistic\nfaces that will be perceptually indifferent from real faces, reaching the photore-\nalistic quality of modern state-of-the-art generative models for 2D face images.\nThis will further reduce the gap between 2D and 3D face manipulations. Apart\nfrom the quality of the images in the dataset, there could also be more sets of\nlabels and attributes which correspond to other high level semantic features.\nThese labels could be used for training our proposed pipeline so that there is\neven more control for variations in the human heads. Another possible future\ndirection for the dataset could be to include normal maps, which include infor-\nmation about the depth of images. Representing details in skin requires depth\ninformation along with color. To accurately change features such as age, such\ndetails and textures in skin needs to be represented. Allowing the model to learn\nthe normal maps along with albedo maps could enable better semantic manipula-\ntions and much higher quality of rendering of the human faces. Considering that\nthese virtual humans are made keeping in mind a diverse range of demographics,\nthe ability to perform manipulations of features such as race and gender will be\nhelpful in introducing diversity in the virtual humans."}, {"title": "7 Conclusion", "content": "In this work, we have proposed SemUV: a novel, simple and effective deep learn-\ning based method for performing semantic manipulation of 3D human faces in"}]}