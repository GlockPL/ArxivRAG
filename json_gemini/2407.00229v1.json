{"title": "SemUV: Deep Learning based semantic\nmanipulation over UV texture map of virtual\nhuman heads", "authors": ["Anirban Mukherjee", "Venkat Suprabath Bitra", "Vignesh Bondugula", "Tarun Reddy Tallapureddy", "Dinesh Babu Jayagopi"], "abstract": "Designing and manipulating virtual human heads is essential\nacross various applications, including AR, VR, gaming, human-computer\ninteraction and VFX. Traditional graphic-based approaches require man-\nual effort and resources to achieve accurate representation of human\nheads. While modern deep learning techniques can generate and edit\nhighly photorealistic images of faces, their focus remains predominantly\non 2D facial images. This limitation makes them less suitable for 3D\napplications. Recognizing the vital role of editing within the UV texture\nspace as a key component in the 3D graphics pipeline, our work focuses\non this aspect to benefit graphic designers by providing enhanced control\nand precision in appearance manipulation. Research on existing methods\nwithin the UV texture space is limited, complex, and poses challenges.\nIn this paper, we introduce SemUV: a simple and effective approach us-\ning the FFHQ-UV dataset for semantic manipulation directly within the\nUV texture space. We train a StyleGAN model on the publicly available\nFFHQ-UV dataset, and subsequently train a boundary for interpolation\nand semantic feature manipulation. Through experiments comparing our\nmethod with 2D manipulation technique, we demonstrate its superior\nability to preserve identity while effectively modifying semantic features\nsuch as age, gender, and facial hair. Our approach is simple, agnostic to\nother 3D components such as structure, lighting, and rendering, and also\nenables seamless integration into standard 3D graphics pipelines without\ndemanding extensive domain expertise, time, or resources.", "sections": [{"title": "1 Introduction", "content": "Virtual humans are computer-generated characters designed to appear and be-\nhave like real people, created using technologies such as computer graphics, an-\nimation, and artificial intelligence. Creating and animating virtual humans is\nan active area of research due to their use in various application domains such\nas VR, AR, Mixed Reality, Human-Computer Interaction, Gaming and enter-\ntainment such as VFX and CGI. Designing and editing the facial appearance of\nthe virtual human is a crucial and challenging task, whether the virtual human\nis based on an existing human, or a generated identity. The appearance com-\nprises of semantic features, i.e. features in the image space having an interpreted\nmeaning, such as age, expression, and facial hair, which need be considered with\nrespect to the context and use-cases while designing and animating virtual hu-\nman heads.\nIn traditional 3D graphics, the process of creating such virtual human heads\nprimarily include designing a 3D mesh model, followed by designing a texture\nmap, which is wrapped onto the mesh to give color and material information.\nThese textures such as albedo (representing base color) and normals (represent-\ning surface depth) are 2D images, which are mapped onto the mesh using the\nconcept of UV mapping, which helps in specifying how a 3D model's surface\nwill be colored as per the 2D texture image. Therefore, the texture maps lie\nin the UV coordinates which, although being of 2-dimensions, is different from\nthe XY coordinate of image space where regular images are represented. To de-\nsign the appearance of a virtual head, graphic designers either paint on the UV\nspace, or directly on the 3D model, which is a challenging and time-consuming\ntask, requires manual effort and expertise in graphics, and difficult to achieve\nphotorealistic results.\nComparatively, generating and editing regular 2D images, as compared to 3D\nmodels, have greatly simplified lately due to the latest advancements in fields of\nAI such as deep learning and computer vision, where deep neural networks can\nlearn the distribution of these images from thousands of diverse examples in a\nhigh dimensional latent space, which allows a user to generate, manipulate and\nanimate photorealistic faces [16], [17], [25]. Although 2D human images can be\nedited using some of these approaches, they lack explicit structural information\ninherent in 3D head objects. The absence of geometry requires the 2D based\nmodels to implicitly learn the 3D movements [8]. This leads to structural and\ntemporal consistencies and often results in artifacts while talking, performing\npose changes and behavioral movements of the virtual heads. Furthermore, cre-\nating semantic manipulations in image space could lead to undesirable changes\nsuch as modifications in head structure and appearance of unwanted objects, be-\ncause of the generative models' holistic learning approaches, making it difficult\nto disentangle specific features implicitly. For instance, while modifying the age\nof a young person, a generative model might introduce spectacles in the older\nversion of the image (Fig. 6), which, although makes sense from a logical point\nof view, isn't ideal for 3D modeling purposes (Fig. 1). Such generative models,\ncapable of learning 2D images, could be useful for learning the 2D texture maps,\nmaking it bridge the gap between 2D images and 3D computer graphics. How-\never, there is significantly less work done for editing in the texture space for 3D\nhuman models. Consequently, our work is a crucial step in advancing the field in\nthis direction. Some works in the lines of reconstructing 3D human heads from\n2D images predict the texture maps, along with a 3D mesh. However, most of\nthese approaches do not provide the option for editing semantic features on the\ntexture maps. Moreover, many of these works model only the face area and not"}, {"title": "3 Our approach", "content": "In our proposed method, we operate within the space of albedo UV texture maps,\nwhich store the fundamental color information [13] of a virtual human head. Our\nmethod involves learning the UV texture feature space using a generative model,\nsubsequently allowing us to perform interpolations in the latent texture space\nof the model, corresponding to semantic manipulations in the UV texture map.\nOnce the texture map is wrapped onto a head mesh, the rendered results have\nthe changes in the virtual human's head (Fig. 2). To train such as model, first\nwe sample from a labeled dataset [1] with UV albedo texture maps of human\nfaces. Then, we learn the distribution of the texture space using the architecture\nof StyleGANv2-ada [17]. After this, we train an SVM to learn a boundary for\ndisentangling the latent space of the trained GAN model via subspace projec-\ntion [24]. This allows linear interpolation in the latent space resulting in precise\nmanipulation of semantics of the face such as facial hair, age and gender without\nchanging other aspects such as identity."}, {"title": "3.1 FFHQ-UV Dataset", "content": "We have used the FFHQ-UV dataset [1], which is derived from the FFHQ dataset\n[16], a large-scale face image dataset. FFHQ-UV contains over 50,000 images of"}, {"title": "3.2 Learning the UV texture space", "content": "To learn the UV space, we train a state-of-the-art generative vision model, com-\nmonly used for facial-image generation. StyleGANv2-Ada [15] is a generative\nadversarial network (GAN) [7] based architecture, whose strengths are a combi-\nnation of StyleGANv2 [17] and Adaptive Instance Normalization (AdaIN) [12].\nSimilar to the basic version of StyleGAN [16], the generator produces an inter-\nmediate feature map, w, by mapping z through a mapping network, denoted as\nW. This mapping network learns a non-linear mapping function that projects\nz to a disentangled latent space, where different dimensions control various at-\ntributes of the generated image. The intermediate feature map w is then passed\nthrough a series of convolutional layers, each followed by an AdaIN operation,"}, {"title": "3.3 Interpolating semantic features", "content": "GANs learn various semantics in linear subspaces of the latent space, which can\nbe manipulated without retraining the model. Building upon the idea by [24],\nwe train a linear Support Vector Machine (SVM) [4] to classify between the\nlevels of each semantic features within the learned latent space. For our model,\nwe choose the criteria of age, facial hair and gender, as per the available labels\nfrom the FFHQ-UV dataset. However, it must be noted that this method can\nbe scaled to any number of semantic features given a labeled dataset. Using\nthe labels, the SVM learns to classify the latent vectors, treating those with\nhighest attribute values in the available labels as positive samples, and those\nwith the lowest values as negative. After training, the coefficients of the SVM\nare used as the boundary vector for the features. To perform a modification of\na semantic in a given image with N steps, we use a projection method proposed\nin [15] to get the latent vector from the image, and subsequently interpolate N\nsteps in the latent space along the direction orthogonal to the boundary vector.\nThese interpolated steps correspond to incremental manipulation in the semantic\nfeatures. By ensuring orthogonalization of all the boundaries, we disentangle\nthe interpolation of semantic features. The ability to linearly interpolate with\ndesired number of steps allows precise control over the manipulation, crucial\nfor the process of designing and manipulating appearance of 3D human head\nmodels."}, {"title": "4 Experiment", "content": "We select 10000 UV maps from the FFHQ-UV dataset to train our model. We\nchoose the architecture of [15] since it is the state-of-the-art method for gener-\nating images resembling the training data. We train it on the dataset for 3000\nepochs on NVIDIA GeForce RTX 2080 Ti. Each epoch was run for 4000 images,\nfollowing the official PyTorch StyleGAN ADA implementation\u00b9.\nThen we use the labels from the dataset to train a SVM, to be able to classify\nthe latent vectors as per the attributes, as mentioned in Sec 3.3."}, {"title": "5 Results", "content": "After training, we have a learned space of UV texture maps. We show the im-\nprovements while training the model using Fr\u00e9chet Inception Distance (FID)\nscore [9] and Kernel Inception Distance (KID) [2] scores across the epochs (Fig.\n4, both of which measure the similarity between real and generated samples.\nLower values of FID and KID indicate that the generated samples are of high\nquality, closely resembling the real samples in terms of visual appearance."}, {"title": "5.1 Interpolated results", "content": "To visualize the working of SemUV on head UV texture maps, we sample from\nthe dataset and perform semantic manipulation on 3 the features: age, facial\nhair, and gender.\nAfter performing the manipulations, we project the texture maps on the 3D\nmesh provided by the dataset on Blender 2, a 3D graphics engine. Once we have\nprojected the UV map, we have a complete 3D head model which we can view\nfrom any direction. Figs. 5a, 5b, 5c show the result of the semantic manipulations\non the head figure from various angles.\nAge: For the change in age in Fig. 5b, going from left to right we observe the\nfeatures of the face being similar to that of a child, young teen, and a grown"}, {"title": "5.2 Comparing with image-space manipulations", "content": "To evaluate the performance with respect to 2D image based approaches, we\nselect the texture map obtained using the pipeline of FFHQ-UV on a custom\nimage, and manipulate the age of the virtual head.\nWe choose age attribute in particular because it is a common attribute be-\ntween the FFHQ-UV labels, and InterFaceGAN [24], which we use as a bench-\nmark for performing semantic manipulation after inverting a given face image\nusing [28]. Gender was another common attribute between the two approaches,\nhowever the presence of hair in the image space, and lack of hair on the 3D head\ncould give biased results while user evaluation."}, {"title": "User evaluation", "content": "For evaluating the performance of our approach as compared\nto the image based manipulations, a set of subjects were asked 8 questions as\nfollows:\nQ1. Which sequence (left to right) is better at showing increasing age?\nQ2. Which sequence (left to right) is better at ensuring ONLY age is changed\nand no other aspect of appearance is changed?\nQ3. Which sequence (left to right) better ensures lighting/illumination is main-\ntained?\nQ4. & Q5. In which pair of images does the identity and facial structure of the\nleft image match the most with that of the right image? (without considering\nage change)\nQ6. & Q7. In which pair of images does the expression of the left image match\nthe most with that of the right image?\nQ8. Out of the given sequences A and B, in which one does the changes in head\npose seem most smooth and structurally consistent?\nFor Q1, Q2, Q3, participants were shown Fig. 6 (a), i.e. two sequences of ma-\nnipulation of age, one using SemUV, and the other using image based approach.\nFor Q4 and Q6, participants were shown the left-most and right-most images of\nFig. 6 (a), i.e. pairs of images before and after manipulation of features. For Q5\nand Q7, we performed an additional task of removing the glasses using [24] once\nagain on the obtained final image, as we can see in Fig. 7 (b). The idea was to\nsee whether the aspects of identity and emotion are preserved after removing the\nglasses which, even though requires an additional step, would let us know how\nour approach compares without the addition of unwanted glasses. Finally, for\nQ8, participants were shown Fig. 7 (a), i.e. two sequences of images with chang-\ning pose, the bottom one being changed using image based manipulation, and\ntop ones being changed simply by changing the camera angle during rendering.\nTo determine if it performed significantly better than chance, we performed\none-sided binomial tests at a 5% significance level (a = 0.05) for each question,\nwith N = 30 trials and K successes where K is the number of participants\nwho preferred the results of our approach. Our null hypothesis (H0) is that our\napproach is not significantly better than chance, while our alternative hypothesis\n(H1) is that our approach is preferred more often.\nThere were 30 participants (18 male and 12 female) for the survey. The\nmean age of the participants is 26.93, with a standard deviation of 5.02. From\nTable 1 we can see that there is a significant observation that our approach is\nable to perform age manipulation better (Q1), by ensuring no other aspect of"}, {"title": "6 Discussion", "content": "In this section, we look into the key insights regarding the importance of SemUV,\nbased on our conducted experiments. We examine the limitations of our approach\nand explore potential future research directions aimed at enhancing our method\nand expanding the range of successful semantic manipulations of virtual heads\nin UV space."}, {"title": "6.1 Significance of SemUV:", "content": "Having a model capable of performing manipulations directly in UV space presents\na significant advantage for designers operating in the realm of 3D graphics. While\ntransformations generated by models operating in the 2D image space, such as\nincreasing hair length while changing gender from male to female or adding\nglasses when increasing age, may seem logically valid, they are not suitable for\nthe process of graphic designing processes, where such elements are typically"}, {"title": "6.2 Limitations:", "content": "While our approach is successfully able to perform manipulations in the semantic\naspects, there are challenges in disentangling certain features in the latent space\nwhere the labels are noisy. In Fig. 3, we see that although there was no facial hair\nin the initial image, the aged version has added slight patches of white hair. Such\nchallenges could be tackled by using more labelled examples, as well as modified\nmethods for non-linear separation boundaries for latent space disentanglement."}, {"title": "6.3 Future scope:", "content": "Some future research directions could explore various realistic face UV datasets.\nWith a dataset of high quality and diverse representations of texture maps, more\nphotorealistic images could be created which will result in even more realistic\nfaces that will be perceptually indifferent from real faces, reaching the photore-\nalistic quality of modern state-of-the-art generative models for 2D face images.\nThis will further reduce the gap between 2D and 3D face manipulations. Apart\nfrom the quality of the images in the dataset, there could also be more sets of\nlabels and attributes which correspond to other high level semantic features.\nThese labels could be used for training our proposed pipeline so that there is\neven more control for variations in the human heads. Another possible future\ndirection for the dataset could be to include normal maps, which include infor-\nmation about the depth of images. Representing details in skin requires depth\ninformation along with color. To accurately change features such as age, such\ndetails and textures in skin needs to be represented. Allowing the model to learn\nthe normal maps along with albedo maps could enable better semantic manipula-\ntions and much higher quality of rendering of the human faces. Considering that\nthese virtual humans are made keeping in mind a diverse range of demographics,\nthe ability to perform manipulations of features such as race and gender will be\nhelpful in introducing diversity in the virtual humans."}, {"title": "7 Conclusion", "content": "In this work, we have proposed SemUV: a novel, simple and effective deep learn-\ning based method for performing semantic manipulation of 3D human faces in"}]}