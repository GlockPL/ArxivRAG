{"title": "GROUNDING VIDEO MODELS TO ACTIONS THROUGH GOAL CONDITIONED EXPLORATION", "authors": ["Yunhao Luo", "Yilun Du"], "abstract": "Large video models, pretrained on massive amounts of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks. However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video. To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data are available. In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment \u2013 using generated video states as visual goals for exploration. We propose a framework that uses trajectory level action generation in combination with video guidance to enable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks. We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.", "sections": [{"title": "INTRODUCTION", "content": "Large video models trained on a massive amount of Internet video data capture rich information about the visual dynamics and semantics of the world for physical decision-making. Such models are able to provide information on how to accomplish tasks, allowing them to parameterize policies for solving many tasks. They are further able to serve as visual simulators of the world, allowing simulation of the visual state after a sequence of actions, and enabling visual planning to solve long-horizon tasks.\nHowever, directly applying video models zero-shot for physical decision-making is challenging due to embodiment grounding. While generated videos provide a rich set of visual goals for solving new tasks, they do not explicitly provide actionable information on how to reach each visual goal. To ground video models to actions, existing work has relied on training an inverse dynamics model or goal-conditioned policy on a set of demonstrations from the environment. Such an approach first requires demonstrations to be gathered in the target environment and embodiment of interest, which demands human labor or specific engineering (e.g. teleoperation or scripted policy). In addition, the learned policies may not generalize well to areas in an environment that are out-of-distribution of the training data.\nRecently, proposes an approach to directly regress actions from video models, without requiring any action annotations. In Ko et al. (2023), optical flow between synthesized video frames is computed and used, in combination with a depth map of the first image, to compute a rigid transform of objects in the environment. Robot actions are then inferred by solving for actions that can apply the specified rigid transform on an object. While such an approach is effective on a set of evaluated environments, it is limited in action resolution as the inferred object transforms can be imprecise due to both inaccurate optical flow and depth, leading to a relatively low success rate in"}, {"title": "RELATED WORK", "content": "Video Model in Decision making A large body of recent work has explored how video models can be used in decision making. Prior work has explored how video models can act as reward functions, representation learning, policies, dynamics models, and environments. Our work explores that given video generations, how we can learn a policy and infer the actions to execute in an environment without any action labels, while existing works requires domain specific action data. Most similar to our work is AVDC, which uses rigid object transformations computed by optical flow between video frames as an approach to extract actions from generated videos. We propose an alternative unsupervised approach to ground video models to continuous action by leveraging video-guided goal-conditioned exploration to learn a goal-conditioned policy.\nLearning from Demonstration without Actions. A flurry of work has studied the problem of robot learning from demonstrations without actions. One line of work studies the problem of extrapolating the control actions assuming that the expert state trajectories are provided though collecting such ground-truth state-level data is expensive and hard to scale. Some recent work explores learning from image/video robotic data without actions, either by constructing a world model,"}, {"title": "\u041c\u0415\u0422\u041dOD", "content": "In this section, we describe our method to ground video models to actions by unsupervised exploration in the environments. First, in Section 3.1, we describe the pipeline of policy execution conditioned on video frame and hindsight relabeling via environment rollouts. Next, in Section 3.2, we introduce a periodic random action bootstrapping technique to secure the quality of video-guided exploration. Finally, in Section 3.3, we propose a chunk-level action prediction technique to further enhance the coverage, stability, and accuracy of the goal-conditioned policy. The pseudocode of our method is provided in Algorithm 1."}, {"title": "LEARNING GOAL-CONDITIONED POLICIES THROUGH VIDEO-GUIDANCE AND\nHINDSIGHT RELABELING", "content": "A key challenge in unsupervised skill exploration is that the possible underlying environment states are enormous, making it difficult for an agent to discover and be trained on every valid state, especially in the high-dimensional visual domain. Many relevant states for downstream task completion, such as stacking blocks on top of each other or opening a cabinet, require a very precise sequence of actions to obtain, which is unlikely to happen from random exploration.\nVideo models have emerged as a powerful source of prior knowledge about the world, providing rich information about how to complete various tasks from large-scale internet data. We leverage the knowledge contained in these models to help guide our exploration in an environment to solve new tasks. To this end, we propose a novel method that uses a pre-trained video model  to guide the exploration and shrink the search space only to task-relevant states ( denote the initial observation and e denote the corresponding task description). This concurrently benefits sides: the goal-conditioned policy obtains task-relevant goals so as can perform efficient exploration centered around the task-relevant state space; on the other hand, the underlying information from the video model is extracted to end effectors and enables effective decision-making and control in embodied agents. Specifically, during exploration, we first leverage the video model to generate a sequence of images based on the given image observation start and language task description c,\n\\textit{pred\\_v} = f_{\\theta}(x_{\\textit{start}}, C), \\text{ where } x_{\\textit{start}} \\sim O \\text{ and } c \\sim T\nwhere O is the observation space and T is the task space. We then execute the actions predicted by the goal-conditioned policy  in the environment, where the goal x goal is set to each predicted video frame pred_v; sequentially. Hence, we can obtain an episode of image action pairs  of length t from the environment and these rollout data will be added to a replay buffer R for policy training.\nAt the beginning of training, these data might not necessarily reach the exact goals given by the video model, but are still effective to indicate the task-specific region, since the rollout results of"}, {"title": "PERIODIC RANDOM ACTION BOOTSTRAPPING", "content": "When a goal-conditioned policy is initialized from scratch, it is unable to effectively process the frames provided by a video model and use them to guide exploration, as it is unable to process input frames. Empirically, we found that a randomly initialized policy would often instead preferentially output particular actions (i.e. only move up) independent of goals given by the video model.\nThis significantly compromises the exploration because the policy will not explore the task-related states as we expect. More importantly, though we can obtain ground-truth environment dynamics via hindsight relabeling, the actions in the replay buffer R are output from the scratch policy itself. This might result in an undesirable loop where only the previous outputs are used as the ground-truth and these actions are irrelevant to completing the task.\nTo this end, inspired by random action sampling in RL setting , we propose a novel periodic random action bootstrapping method for grounding video model to actions. Specifically, we first conduct random action exploration and append the resulting data to the replay buffer R before the training starts, and periodically conduct extra random exploration during training. The process can be denoted by\n$$R \\leftarrow a_i^{tr} \\sim \\text{Uniform} \\left[a_{\\text{low}}, @_{\\text{high}}\\right] \\text{ for } i \\text{ in } [1, 2, ..., n_r]$$\nwhere nr denotes the number of random action episodes, tr denotes the length of a random action episode, alow and @high are the action limits. This proposed bootstrapping method can enhance the exploration in two ways: the initial random actions serve as the basic world dynamics information which enables the policy to reach the vicinity of goal states specified by video frames, ensuring effective video-guided exploration; the periodic extra random exploration can further expand the agent's discovered state space and stabilize policy training. Please see Section 4.3 for ablation studies."}, {"title": "CHUNK LEVEL ACTION PREDICTION AND EXPLORATION", "content": "In current robot exploration frameworks, the policy usually predicts a single action as output and explores the environment with the predicted action. However, single-action prediction can hinder"}, {"title": "EXPERIMENTS", "content": "We present our experiment results across four simulated environments shown in Figure 2. In Section 4.1, we describe our results on three robotic manipulation environments: 8 tasks on Libero , 6 tasks on MetaWorld, and 4 tasks on Calvin. Following this, in Section 4.2, we show evaluation results on 4 different scenes and 12 targets on iTHOR visual navigation environment. Finally, in Section 4.3, we present ablation studies on the proposed chunk level action prediction and random action bootstrapping methods. We provide more experiment results in Appendix A and B. We use the same demonstration data to train the baseline methods and our video model, except that training the video model only requires the image sequences of demonstrations, while most baseline methods need the corresponding action annotations (highlighted by an asterisk). In evaluation, for manipulation tasks, the initial robot state and object positions are randomized; for visual navigation tasks, we randomize the robot start position.\nImplementation. For the video model, the inputs are the observation image and the task description. We follow the lightweight video model architecture introduced in (Ko et al., 2023) and train one video model from scratch for each environment. We deem that finetuning large text-conditioned video models or designing task-specific video model might be an interesting future research direction. For the goal-conditioned policy, we implement it with a CNN-based Diffusion Policy , which takes as input the observation image and the goal image and outputs a chunk of actions. For detailed implementation of our method and each baseline, please refer to Appendix D."}, {"title": "MANIPULATION", "content": "In this section, we aim to evaluate the goal-conditioned policy learned by the proposed unsupervised exploration in tabletop manipulation environments with continuous action space. To better understand the capability of the method, we investigate multi-task policy learning in Libero and Calvin, and single-task policy learning in Meta-world. Note that methods highlighted by an asterisk require ground-truth action labels to train, while our method only requires image observations for training the video model with the potential of zero-shot generalization if trained on internet-scale data.\nLibero is a tabletop simulation of a Franka Robot, which features several dexterous manipulation tasks. For each task in Libero, the agent is required to achieve the final state described in a corresponding sentence, which identifies the target object and task completion state. The action space consists of the delta position and orientation of the end effector and the applied force on the gripper, resulting in a total dimension of 7. To improve the efficiency of exploration, we further add an exploration primitive for grasping, which we discuss in detail in Appendix D.2.\nWe include 8 tasks from two scenes in Libero as the testbed. In this environment, we aim to evaluate the multi-task learning capability of our method, hence we only train one policy model for all 8 tasks. The video model is trained on the visual image sequences of the demonstrations provided in"}, {"title": "VISUAL NAVIGATION", "content": "In addition to tabletop-level robot arm manipulation tasks, we evaluate the proposed method in a room-level visual navigation setting with discrete actions.\niTHOR is a room-level vision-based simulated environment, where agents can navigate in the scenes and interact with objects. We adopt the iTHOR visual object navigation benchmark, where agents are required to navigate to the specific type of objects given by a nature language input. The action space consists of four actions: Move Ahead, Turn Left, Turn Right, and Done. We incorporate 4 different scene types: Kitchen, Living Room, Bedroom, and Bathroom, with 3 targets in each scene. Following (Ko et al., 2023), the video model is trained on 20 demonstration image sequences per task. BC and GCBC are also trained on the same demonstrations while with access to corresponding actions. Since the navigation actions are discrete and episode lengths (typically <20) are much shorter than robot arm manipulation tasks, we find that predicting a single action suffices. Our method uses the same ResNet+MLP model architecture as BC-based baselines.\nWe present the quantitative results in Table 4. Our method outperforms the no expert data baseline AVDC by 36% on average and surpasses all baselines in the Kitchen scene while performing on"}, {"title": "ABLATION STUDIES", "content": "In this section, we ablate the effectiveness of design choices described in Section 3. We provide additional studies in Appendix A, including training with different amounts of data, video exploration frequency qu, and different horizons of the video model and goal-conditioned policy.\nRandom Action Bootstrapping. We conduct ablation studies on the importance of the random action bootstrapping technique. We first present the training-time exploration efficiency in Figure 8. As shown by w/o rand, the no random action method tends to collapse and can hardly achieve any task success during the exploration. Since the task completion data cannot be collected from the environment, w/o rand cannot obtain any meaningful results in test time, which is reflected in Table 5. In contrast, as shown by the blue, red, and orange lines in Figure 8, the model is able to obtain significantly more task success after we apply random action bootstrapping. The performance of w/o extra rand decreases through time in Scene 1, showing that extra random exploration is able to stabilize training, and its performance is similar to ours in Scene 2, probably because the manipulation"}, {"title": "DISCUSSION", "content": "Limitations. Our approach has several limitations. First, since the approach relies on goal-conditioned random exploration and without access to action labels, for tasks that require very precise manipulation (i.e. stacking a block tower in millimeter precision), our random exploration procedure may not find the precise set of actions. In such settings, having a random exploration primitive (i.e. stacking a block on top of another) on which we do goal-conditioned policy learning may help us find such precise actions. In addition, purely random exploration in the physical world might pose additional requirements for the workbench, as sampled actions or the initial actions outputted by the learned goal-conditioned policy may cause undesired contacts between the robot and the external environment. This can be partially mitigated by integrating hard-coded safety constraints and is also a further direction of future work. Finally, our method usually requires dozens of video-guided rollouts to obtain a competent policy for a task. While resetting an environment is easy in simulation, this might demand additional work in real-world experiments. Combining our approach with a policy that can reset environment states would be an interesting direction for the future.\nConclusion. In this paper, we have presented a self-supervised approach to ground generated videos into actions. As generative video models become increasingly more powerful, we believe that they will be increasingly useful for decision-making, providing powerful priors on how various tasks should be accomplished. As a result, the question of how we can accurately convert generated video plans to actual physical execution will become increasingly more relevant, and our approach points towards one direction to solve this question, through online interaction with the agent's environment."}, {"title": "A ADDITIONAL QUANTITATIVE RESULTS", "content": "In this section, we provide additional quantitative experiment results. In Section A.1, we compare our methods with BC trained on additional demonstration data. Next, in Section A.2, we investigate the effect of training the video model with different amounts of demonstration data. In Section A.3, we experiment our proposed method with video models of different horizons. Then, in Section A.4, we study how varying the video exploration frequency affects exploration performance. In Section A.5, we conduct ablation studies of using goal-conditioned policies with different horizons. Following this, in Section A.6, we evaluate the policy after different numbers of training iterations. In Section A.7, we compare our method with a reinforcement learning baseline with zero-shot reward. Finally, in Section A.8, we conduct ablation studies on the task success rate over the number of video-guided exploration rollouts."}, {"title": "BC WITH MORE DATA", "content": "We provide comparison of our policy learned by unsupervised exploration and plain BC with increasing amount of training data. To train these BC baselines, We use the official demonstrations provided by Libero. For example, BC 10 means that we use 10 demonstrations per task to train the BC model (resulting in 80 demonstrations in total). The reported results are average across 5 checkpoints. Note that our policy does not need any action labels and directly learn how to ground the goals given by the video model via exploration in the environment. The video model is trained with 20 demonstrations, which only requires image sequences. That said, this is not a completely fair comparison, but we include this comparison to better illustrate the difficulty of the tasks.\nWe report the results in Table 6. The success rate of our method even outperforms BC trained with 50 demonstrations per task, while our policy is learned purely by exploration, indicating the efficacy of the proposed exploration method. We also observe that though the success rate of BC generally increases over more training data, BC 20 is able to slightly outperform BC 30 in Scene 1 while BC 30 slightly outperforms BC 40 in Scene 2. Since the BC model is trained on 8 tasks, the heterogeneous task structures can result in fluctuation in the success rate (the model might bias towards some specific tasks)."}, {"title": "VIDEO MODEL WITH DIFFERENT AMOUNTS OF DATA", "content": "We investigate the effects of training the video model with different amounts of demonstration data. For the result shown in Table 1 in the main paper, we use 20 demonstrations per task to train the video model. In this section, we provide two ablation studies that use 10 demonstrations per task and 50 demonstrations per task, as shown in Table 7 below.\nThe performance of our method increases in both scenes as with more training data. While the performance uniformly increases in Scene 1, we observe that in some specific tasks of Scene 2, the performance slightly decreases. This might be due to the learning capacity of the underlying goal-reaching policy, since we only train one policy model for all eight tasks. In general, we believe that scaling up the data size, data diversity, and model complexity can further buttress the performance."}, {"title": "VIDEO MODEL WITH DIFFERENT HORIZONS", "content": "In this section, we study the effect of different video model horizons. We follow the training procedure of the video model in Ko et al. (2023): during training, given a start image observation, we uniformly sample h images between the start image and the final task completion image and use these h images as supervision signals. That said, a longer video horizon h will generate a denser subgoal sequence. Following Ko et al. (2023), we set horizon h = 7 across all our experiments except for this ablation study in Table 8.\nAs shown in Table 8, our method is able to maintain a similar performance across different video horizons. We observe that performance decreases when video horizon is set to 9. One potential reason is that the modeling complexity increases when we increase the prediction horizon, while we keep the same video model architecture, suggesting that we should learn video models over a sparser set of video frames."}, {"title": "DIFFERENT VIDEO EXPLORATION FREQUENCY", "content": "In this section, we study the effect of video exploration frequency. For example, when qr is set to 200, we conduct one video-guided exploration for each task every 200 training iterations. The trade-off between smaller and larger qu is that: if qr is small, the agent will conduct exploration in a higher frequency, where the replay buffer will refresh faster and contain more latest rollout data. If qu is large, the agent conducts exploration less frequently, which might enable the agent to better fit and digest the existing data in the replay buffer. We provide ablation studies on 5 different qu on Libero environment, as shown in Figure 9."}, {"title": "DIFFERENT POLICY HORIZON FOR OUR GOAL CONDITIONED POLICY", "content": "In this section, we study the effect of using different horizons for the diffusion policy based goal-conditioned policy. We set the policy horizons to 12, 16, 20, and 24 with the same CNN-based architecture. We observe that the policy learned by the proposed exploration pipeline is robust to the different horizons value, and obtain the highest success rate when horizon = 16, which also conforms to the reported results in Diffusion Policy (Chi et al., 2023). Please refer to Table 9 for quantitative results."}, {"title": "DIFFERENT TRAINING TIMESTEPS", "content": "In this section, we investigate the model performance at different training timesteps on Libero environment, specifically, when the model is trained for 40k, 80k, 120k, 160k, and 200k steps. We set the video exploration frequency qu to 200 in this experiment. We observe that the overall success rate of the two scenes stabilize after 80k training steps. However, the per-task success rate oscillates through further training. This might be caused by the recent collected data in the replay buffer (which is used for training) and the task level interference."}, {"title": "COMPARISON TO RL WITH ZERO-SHOT REWARD", "content": "In this section, we compare our method with a reinforcement learning baseline. Since our method does not have access to environment rewards, we leverage a foundational zero-shot robotic model to generate the rewards. Specifically, we adopt DrQ and LIV as the RL method and foundation model respectively. We use the potential-based reward defined in LIV.\nWe present the results in Table 11. We see that DrQ+LIV fails to make meaningful progress in five out of six tasks, achieving only an 8% success rate in the handle-press task. We observe that one probable cause is that the generated reward signals are ambiguous, which hinders the RL method to learn. For instance, it is challenging for the model to generate a significant positive reward upon task completion and the rewards may oscillate even when positive progress is being made."}, {"title": "SUCCESS RATE V.S. NUMBER OF VIDEO-GUIDED ROLLOUTS", "content": "In this section, we conduct ablation studies on the task success rate over the number of video-guided rollouts. We show the performance curve of the Libero task put the red mug on the plate in Figure 10, where the y-axis represents the success rate of 25 evaluation episodes and the x-axis denotes the number of conducted video-guided exploration rollouts.\nSpecifically, we first warm-start the policy by training it on randomly sampled actions, which corresponds to zero video-guided rollouts. Then, the model begins to perform video-guided exploration, and we set the video exploration frequency to 800 (i.e., the agent will conduct 5 video-guided exploration every 800 training iterations). We save a model checkpoint after every 10 video exploration episodes and evaluate each checkpoint on 25 test-time problems. Note that to better demonstrate the correlation of success rate and # of video-guided exploration, we train the policy model just on a single task, while in Table 1, the policy model is jointly trained on 8 tasks.\nAs shown in Figure 10, the policy is unable to accomplish the task without video-guided exploration episodes (when the x-axis is 0). However, the success rate rapidly increases with the start of video-guided exploration episodes, reaching a success rate of 40% after only 100 episodes. After approximately 200 episodes, the success rate gradually saturates, stabilizing around 80%."}, {"title": "ADDITIONAL QUALITATIVE RESULTS", "content": "In this section, we present additional qualitative results in both tabletop manipulation environments and visual navigation environments. For extra results and side-by-side qualitative comparison, please refer to our website.\nIn Section B.1, we provide qualitative results for each task in Libero environment. In Section B.2, we show qualitative results for Meta-World tasks. Next, in Section B.3, we present qualitative results on Calvin environment. Finally, in Section B.4, we present a long-horizon visual navigation planning rollout in iTHOR environment."}, {"title": "LIBERO", "content": "In this section, we provide additional qualitative results of our method on 8 tasks in Libero environment, as shown in Figure 11 and 12. For each episode, we show the generated video in the first row and the rollout results of our goal conditioned policy learned by self-supervised exploration in the second row."}, {"title": "\u039c\u0395\u03a4A-WOLRD", "content": "In this section, we provide additional qualitative results of our method on 6 tasks in Meta-World environment, as shown in Figure 13 and 14. For each episode, we show the generated video in the"}, {"title": "CALVIN", "content": "In this section, we provide additional qualitative results of our method on 4 tasks in Calvin environment, as shown in Figure 15. For each episode, we show the generated video in the first row, and the rollout results of our goal conditioned policy learned by self-supervised exploration in the second row."}, {"title": "ITHOR VISUAL NAVIGATION", "content": "B.4 ITHOR VISUAL NAVIGATION"}, {"title": "FAILURE MODE ANALYSIS", "content": "In the previous section, we demonstrate that our method is able to achieve non-trivial performance in various robotic environments. However, the method still poses some limitations, which lead to failure. In this section, we provide analysis into the failure mode of our method. We categorize the causes to video model based and policy learning based."}, {"title": "VIDEO MODEL", "content": "The performance of the video model is one influential factor of the success rate because that if an incorrect subgoal is given, even though the policy is accurate enough, the task cannot be completed.\nHallucination is a common issue for generative models (Yang et al., 2024a; Ji et al., 2023). We also observe some extent of hallucination in our video model. We present some visualizations in Figure 17.\nIn Table 1, Ours and Ours w/ SuSIE only achieve 25.6% and 36.0% on the task 'put the white mug on the plate'. We observe that one major cause of this relatively low performance is the heavy hallucination in the generated video of this task, as shown in the first row of Figure 17. We hypothesize that this might partially due to the dataset imbalance problem, as we have two tasks involving chocolate pudding while only one for white mug in this scene, considering that only 20 demonstrations are provided for each task."}, {"title": "POLICY LEARNING", "content": "Policy learning is particularly challenging in such unsupervised setting since we assume no action demonstrations nor rewards. Generally, we observe that the learned policy is able to follow the video model, but there remain challenges for the policy to handle fine-grained tasks that require precise control. For example, our policy achieves 94.4% at handle press task, while only achieves 16.8% at assembly task in Meta-World. In Figure 19 and Figure 20, we provide some qualitative results of failure cases in Libero and Meta-World."}, {"title": "IMPLEMENTATION DETAILS", "content": "In this section, we describe the implementation of our proposed method and baselines.\nSoftware: The computation platform is installed with Red Hat 7.9, Python 3.9, PyTorch 2.0, and Cuda 11.8\nHardware: For each of our experiments, we used 1 NVIDIA RTX 3090 GPU or a GPU of similar configuration."}, {"title": "MODEL ARCHITECTURES", "content": "For the architecture of the video model, we adopt the same design from AVDC , which uses 3D UNet as video denoiser and CLIP to encode language features. In tabletop manipulation tasks, We instantiate the goal-conditioned policy by a diffusion policy , which uses ResNet18 as image encoder and 1D UNet to denoise the action trajectory. Following the default setup in (Chi et al., 2023), we set the horizon to 16 across all other experiments. Finally, for iTHOR environment, since the action space is discrete and the required number of actions to reach goal is smaller, we use a ResNet18 with a 3-layer MLP with ReLU activation as the goal-conditioned policy with an output horizon of 1."}, {"title": "TRAINING DETAILS", "content": "Training Pipeline In training, we randomly sample sequences of image-action pairs of horizon h from the replay buffer. For Diffusion Policy setup, we mainly follow the hyperparameters suggested in the original paper. We provide detailed hyperparameters for training our model in Table 12 and 13. We do not apply any hyperparameter search or learning rate scheduler. For Libero environment, the training time of our model is approximately 36 hours for a full 200k training steps. However, the performance can gradually saturate with much fewer steps. For the Meta-World and iThor environments, the training time of our model is approximately one day; for Calvin, the training time of our model is around 15 hours. For these environments, we also observe that the performance can gradually saturate within less training time.\nExploration Grasping Primitive. To improve the exploration efficiency of grasping, we further add a simple grasping primitive that encourages the agent to attempt a grasp when the end effector is positioned closely above an object. Specifically, we measure the distance from the wrist camera to the nearest obstacle and compare the distance to the current height of the gripper above table. The agent is prompted to grasp if the difference is high (indicating a nearby object). This heuristic can be applied across various robots and greatly reduces the search space necessary to learn grasping."}, {"title": "IMPLEMENTATION OF BASELINES", "content": "Behavior Cloning (BC). For BC, the observed image is first fed to a ResNet-18 to encode visual information. The vision feature vector is then fed into a 3-layer MLP to predict the next action. For multi-task BC, we concatenate the vision feature with task language description feature encoded by CLIP, and feed the concatenated feature to a 3-layer MLP to predict an action. We use MSE as loss and train for 100k steps with a batch size of 64."}]}