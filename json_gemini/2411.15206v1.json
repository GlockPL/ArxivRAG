{"title": "Self-Supervised Conditional Distribution Learning on Graphs", "authors": ["Jie Chen", "Hua Mao", "Yuanbiao Gou", "Zhu Wang", "Xi Peng"], "abstract": "Graph contrastive learning (GCL) has shown promising performance in semisupervised graph classification. However, existing studies still encounter significant challenges in GCL. First, successive layers in graph neural network (GNN) tend to produce more similar node embeddings, while GCL aims to increase the dissimilarity between negative pairs of node embeddings. This inevitably results in a conflict between the message-passing mechanism of GNNs and the contrastive learning of negative pairs via intraviews. Second, leveraging the diversity and quantity of data provided by graph-structured data augmentations while preserving intrinsic semantic information is challenging. In this paper, we propose a self-supervised conditional distribution learning (SSCDL) method designed to learn graph representations from graph-structured data for semisupervised graph classification. Specifically, we present an end-to-end graph representation learning model to align the conditional distributions of weakly and strongly augmented features over the original features. This alignment effectively reduces the risk of disrupting intrinsic semantic information through graph-structured data augmentation. To avoid conflict between the message-passing mechanism and contrastive learning of negative pairs, positive pairs of node representations are retained for measuring the similarity between the original features and the corresponding weakly augmented features. Extensive experiments with several benchmark graph datasets demonstrate the effectiveness of the proposed SSCDL method.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph-structured data typically represent non-Euclidean data structures, which are often visualized as graphs with labels [1]. The analysis of such data has attracted consid- erable attention in various applications, e.g., social network recommendation [2], molecule classification [3], and traffic flow analysis [4]. In graph classification tasks, graph-structured data typically consists of multiple graphs, each with its own in- dependent topology. In practice, obtaining large-scale labelled annotations for graph-structured data is often prohibitively costly or even impossible. Semisupervised graph classification aims to predict the labels of graphs via a small number of labeled graphs and many unlabeled graphs.\nNumerous graph neural networks (GNNs) have been pro- posed for the analysis of graph-structured data, e.g., the graph convolutional network (GCN) [5], graph attention network (GAT) [6], graph sampling and aggregation (GraphSAGE) [7] and their variants. Through the message-passing mechanism, GNNs aggregate semantic information from neighboring nodes in the graph topology to central nodes. In graph classifica- tion tasks, GNNs aims to transform individual graphs into corresponding low-dimensional vector representations at the graph level [8]. Consequently, GNNs are capable of integrating graph-structured data into comprehensive graph representa- tions, preserving the intrinsic structure of graph-structured data [9].\nContrastive learning, which employs raw image data as its own supervision, has shown great potential in computer vision [10]-[13]. Several augmentation operations for visual data, such as rotating, cropping, and flipping, significantly improve the representation ability of visual learning models in downstream tasks [14]. Wang et al. [15] proposed a general contrastive learning framework that minimizes the distribution divergence between weakly and strongly augmented images over a representation bank. Inspired by the previous suc- cess of visual data augmentation, graph contrastive learning (GCL) leverages the advantages of graph data augmenta- tion to maximize the similarity between positive pairs and the dissimilarity between negative pairs [16]. For example, You et al. proposed a graph contrastive learning (GraphCL) framework that employs four types of augmentations to learn invariant graph representations across different augmented views, including node dropping, edge perturbation, attribute masking and subgraph extraction [17]. Zhu et al. proposed a graph contrastive representation learning method with adaptive augmentation that incorporates various priors for topological and semantic information of the graph [18]. These GCL-based approaches effectively enhance the generalizability of graph representation learning models in a self-supervised manner.\nGCL for graph classification has raised significant interests recently [8], [19], [20]. For example, Han et al. [20] adopted G-Mixup to augment graphs for graph classification. Yue et al. [8] proposed a graph label-invariant augmentation (GLIA) method that adds perturbations to the node embeddings of graphs for creating augmented graphs. However, these ap- proaches have overlooked two limitations in GCL. First, suc- cessive GNN layers tend to make node representations more similar due to the message-passing mechanism. In contrast, GCL aims to enhance the dissimilarity of negative pairs of node embeddings through mutual information maximization. This inevitably results in a potential conflict between message- passing mechanism of GNNs and the contrastive learning of negative pairs in intra-views. Second, different augmentations for graph-structured data often result in varying performances for GNNs [8]. For example, heavy edge perturbation can enhance the diversity and quantity of graph-structured data. Unfortunately, it may also disrupt the intrinsic semantic in- formation contained in the graph-structured data. Therefore, the other significant challenge is how to leverage the diversity and quantity provided by graph-structured data augmentation while preserving the intrinsic semantic information.\nIn this paper, we propose a self-supervised conditional distribution learning (SSCDL) method for learning graph representations from graph-structured data for semisupervised graph classification. We define slight perturbations to node attributes or graph topology as weak augmentations, whereas significant perturbations are referred to as strong augmen- tations. We first present an end-to-end graph representation learning model to align the conditional distributions of weakly and strongly augmented features over the original features. Aligning the conditional distributions effectively reduces the risk of compromising intrinsic semantic information when strong augmentation is applied. Moreover, only positive pairs of node representations are retained for measuring the sim- ilarity between the original features and their corresponding weakly augmented features. From the gradient perspective of the InfoNCE loss, we explain how the conflict between the message-passing mechanism of GNNs and the contrastive learning of negative pairs of node representations can be avoided. Additionally, we introduce a semisupervised learning scheme that includes a pretraining stage and a fine-tuning stage for semisupervised graph classification. Extensive experimen- tal results with benchmark graph datasets demonstrate that the proposed SSCDL method is highly competitive with state-of- the-art graph classification methods.\nOur major contributions are summarized as follows.\n1) We present an end-to-end graph representation learning model that learns graph representations from graph- structured data for semisupervised graph classification.\n2) Self-supervised conditional distribution learning is in- troduced to characterize the consistent conditional dis- tributions of weakly and strongly augmented node em- beddings over the original node embeddings.\n3) The similarity loss function is introduced to alleviate the potential conflict between the message-passing mecha- nism of the GNN and the contrastive learning of negative pairs in GCL."}, {"title": "II. PRELIMINARIES", "content": "A. Problem Formulation\nGiven a graph $G = (V,E)$, $V$ and $E$ represent the sets of nodes and edges, respectively. Let $X = [X_1, X_2, ..., X_n] \\in \\mathbb{R}^{d \\times n}$ denote the feature matrix of nodes, where $d$ denotes the dimension of node features, and $n$ is the number of nodes. Let $A \\in \\mathbb{R}^{d \\times n}$ be the adjacency matrix of $G$. If there exists an edge from node $i$ to node $j$, then $A_{ij} = 1$; otherwise, $A_{ij} = 0$. The degree matrix $D$ is defined as $D = diag [d_1, d_2, ..., d_n] \\in \\mathbb{R}^{n \\times n}$, and its diagonal elements are $d_i = \\sum_{v_j \\in V} A_{ij}$. Given an undirected graph $G$, let $\\tilde{A} = A + I$ denote the adjacency matrix of $G$ with added self-loops. The diagonal degree matrix $\\tilde{D}$ is defined as $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$. For a weighted graph, $e_{ij}$ in $E$ represents the weight of an edge from node $i$ to node $j$.\nGiven a set of graphs $G_s = \\{(G_i,y_i)\\}_{i=1}^{n_g}$ with $c$ categories, the purpose of graph classification is to learn a function $f$ that can map the graph to its label, where $n_g$ represents the number of graphs, and $y_i$ is the label of $G_i$.\nB. Graph Classification Theory\nGNNs have emerged as effective methods for graph clas- sification [8]. Without loss of generalizability, the hidden representation of node $j$ of a graph $G_i$ is iteratively updated by aggregating information from its neighbors $N(j)$ at the $l$-th layer, i.e.,\n$z_j^{(l)} = AGGREGATE(\\ \\{h_k^{(l-1)}: k \\in N(j)\\})$\n$h_j^{(l)} = COMBINE^{(l)}(h_j^{(l-1)}, z_j^{(l)})$\nwhere $AGGREGATE(\\cdot)$ is an aggregation function, $N(i)$ represents the neighbors of node $i$, and $COMBINE(\\cdot)$ combines the hidden representations of the neighbors and itself at the previous layer. For graph classification, GNNs ultimately apply a $READOUT$ function to aggregate the hidden repre- sentations of nodes into a graph-level representation at the final layer, i.e.,\n$h_{G_i} = READOUT (\\{h_j^{(L)} : j \\in V(G_i)\\} )$\nwhere $L$ denotes the index of the final layer. The graph-level representation $h_{G_i}$ can be employed to predict the label of $G_i$, i.e.,\n$p_i = softmax (h_{G_i})$\nwhere $softmax(\\cdot)$ is applied to transform an input vector into a probability distribution for various graph classification tasks."}, {"title": "III. RELATED WORK", "content": "GCL has recently attracted significant interest. For ex- ample, the GraphCL framework exhibits a pretraining strat- egy of GNNs through maximizing the agreement between two augmented views of graphs [17]. Given n augmented graphs, 2n augmented graphs can be obtained by applying one data argumentation for graph-structured data. The normalized temperature-scaled cross entropy loss (NT-Xent) [13], [21] is adopted to maximize the consistency between positive pairs $u_i, v_i$ compared with negative pairs, i.e.,\n$L_i = -log \\frac{f (u_i, v_i)}{f (u_i, v_i) + \\sum_{k \\neq i} f (u_i, v_k) + \\sum_{k \\neq i} f (u_i, u_k)}$\nwhere $f (\\cdot) = exp(\\text{sim}(\\cdot)/\\tau)$ and $\\text{sim} (u_i, v_j) = \\frac{u_i v_j}{\\parallel u_i \\parallel \\parallel v_j \\parallel}$ is the cosine similarity and $\u03c4$ is a temperature parameter. In GCL, a conflict arises in the message-passing mechanism of GNNS when combined with contrastive learning of negative pairs via intraviews. This is because certain samples simultaneously contribute to the gradients of both positive and negative pairs during message passing [22].\nRecent studies have proposed semisupervised graph clas- sification [8], [19]. These methods typically leverage a small number of graphs and massive numbers of available unlabeled graphs to explore the semantic information of graph-structured data. Most existing GCL-based methods for graph classifi- cation tasks inherently employ contrastive learning of both positive and negative pairs [8], [23]. This inevitably leads to a conflict between the message-passing mechanism of GNNS and the contrastive learning of negative pairs via intraviews. Additionally, data argumentation may disrupt the intrinsic semantic information contained in the graph-structured data, which makes enhancing the generalizability and robustness of the graph classification models challenging."}, {"title": "IV. SELF-SUPERVISED CONDITIONAL DISTRIBUTION LEARNING ON MULTIPLE GRAPHS", "content": "In this section, we formally introduce the proposed SSCDL method, including the SSCDL model and a semisupervised learning scheme for graph classification.\nA. Framework overview\nFig. 1 presents the framework of the SSCDL model, which consists of three modules: a GNN encoder module for pro- cessing graph-structured data, a projection head module and a self-supervised distribution construction module. The shared GNN encoder module learns graph-level representations, H, H' and Hw, from the raw graph-structured data, a strongly augmented view and a weakly augmented view, respectively. These graph-level representations are obtained from the node embeddings of the graphs through a graph pooling layer. In particular, H' are used for semisupervised graph classification tasks. The projection head module is designed to produce the projected representations of the raw graph-structured data augmentation and a weakly augmented view for GCL. The self-supervised conditional distribution module constructs the conditional distributions of strongly and weakly embedded features, using the original embeddings as a reference.\nB. Self-Supervised Conditional Distribution Learning\nWhen working with relatively small amounts of graph- structured data, GNN-based models often learn redundant features. Graph-structured data augmentation aims to increase the diversity and quantity of graph-structured data, which can improve the generalizability and robustness of GCL-based models. By applying strong augmentation, GNN-based models can learn essential features from graph-structured data, even when these data are slightly corrupted by noise. However, strong augmentations, such as extensive edge perturbation or attribute masking, can also significant perturb the original data, potentially altering the intrinsic semantic information contained in the graph-structured data and thus resulting in limited generalizability of GNN-based models.\nTo minimize the risk of altering intrinsic semantic in- formation with strong augmentation, we introduce a self- supervised conditional distribution learning strategy to guide graph representation learning. This learning strategy explores the supervision information embedded in the original graph- structured data for weak and strong augmentations. We con- sider the distribution of relative similarity between a weakly augmented node embedding and the original node embedding. The conditional distribution of $h'_i$ given $h_i$ is defined as follows:\n$p(h_i | h_i) = \\frac{p (h_i, h_i)}{p (h_i)} = \\frac{exp (\\text{sim} (h_i, h_i)/\\tau)}{\\sum_{k=1}^{K}exp (\\text{sim} (h_i, h_i)/\\tau)}$\n$\\tau = \\frac{exp (\\text{sim} (h_i, h_i)/\\tau)}{\\sum_{k=1}^{K}exp (\\text{sim} (h_i, h_i)/\\tau)}$\nwhere $K$ denotes the number of negative samples chosen from a weakly augmented view, and $h_i$ and $h'_i$ represent the corre- sponding original node embedding and weak node embedding, respectively. Similarly, we can obtain the other conditional distributions of $h'$ given $h_i$, i.e., $p (h' | h_i)$. Thus, we construct the conditional distributions of weakly and strongly augmented node embeddings given the original node embeddings.\nTo ensure consistency between $p (h | h_i)$ and $p (h' | h_i)$, we present a metric named the distribution divergence to quantify the divergence between these two conditional distributions as follows:\n$L_d = -\\frac{1}{n_g} \\sum_{i=1}^{n_g} p(h_i | h_i) log (p (h | h_i))$\nBy minimizing $L_d$, the conditional distributions of weakly augmented node embeddings given the original node embed- dings are employed to supervise the corresponding conditional distributions of strongly augmented node embeddings given the original node embeddings. The original node embeddings provide useful supervision information to ensure the consis- tency of the intrinsic semantic information between weak and strong augmentations. Consequently, this approach is beneficial for preventing overfitting of the SSCDL model to semisupervised graph classification.\nThe intrinsic semantic information within graph-structured data remains susceptible to distortion, even though weak augmentation is applied. We focus on how $p(h' | h_i)$ pro- vides valuable supervision information for $p (h | h')$. GCL is considered an effective strategy for mitigating the potential risk of distortion during weak augmentation. Unfortunately, there is a conflict between the message-passing mechanism of GNNs and the contrastive learning of negative pairs in augmented views, as previously discussed. For example, a node embedding $h_i$ is aggregated from the neighboring nodes $\\{h_j : j \u2208 N(i)\\}$ in an augmented view. However, these neigh- boring nodes are considered negative for $h_i$ when the loss function in Eq. (5) is employed. To evaluate the similarity be- tween a positive pair $(h_i, h'_i)$, the similarity loss is formulated as follows:\n$L_s = -\\frac{1}{n_g} \\sum_{i=1}^{n_g}log\\frac{exp (\\text{sim} (h_i, h'_i))}{\\sum_{j\\neq i}exp (\\text{sim} (h_i, h_j))}$"}, {"title": "C. Implementation Details", "content": "In this section, we introduce a semisupervised training scheme for graph classification. This training scheme consists of two stages: a pretraining stage and a fine-tuning stage.\n1) Pretraining Stage: When many unlabeled graphs are available, an intuitively designed pretraining framework is an effective approach for semisupervised graph classification. The pretraining framework of the SSCDL model includes a shared GNN encoder module of graph-structured data and a projec- tion head module. The shared GNN encoder module consists of three components, including a GNN encoder component, a pooling layer component and a multilayer perceptron (MLP) component. In the shared GNN encoder module, we perform graph representation learning according to graph classification theory.\nFor simplicity, a GCN [5], [17] is used as an example to describe the backbone of the GNN encoder module. The formula for the lth GCN layer in the GNN encoder is defined as follows:\n$H^{(l)} = \\sigma (\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}H^{(l-1)} W^{(l)})$\nwhere $W^{(l)}$ is a weight matrix of the lth layer, $H^{(l)}$ represents a node-level embedding matrix with $H^{(0)} = X$, and \u03c3is a nonlinear activation function, e.g., ReLU(\u00b7) = max(0, \u00b7). Then, we employ a global sum pooling layer as a READOUT function to obtain graph-level representations from the node- level embeddings, i.e.,\n$H_{g} = pooling (H^{(L)})$.\nFinally, the MLP component is composed of several fully connected layers, with computations defined as follows:\n$H = MLP (H_{g}, W^{(l)} )$\nwhere a nonlinear activation function, ReLU(\u00b7), is applied for each linear layer. To improve the generalizability of the SSCDL model, we apply a batch normalization function in individual layers [24].\nDuring the pretraining stage, the graph-level representations, H' and Hw, are produced by the shared GNN encoder module via the raw graph-structured data and a weakly augmented view, respectively. We further employ the projection head module [12], which consists of a two-layer MLP, to obtain the projections of H' and Hw for contrastive learning, i.e.,\n$P = W^{(2)} \\sigma (W^{(1)}H')$\n$P_w = W^{(2)} \\sigma (W^{(1)}H_w)$\nThe pretraining framework of the SSCDL model aims to ensure the consistency of the intrinsic semantic information between a weakly augmented view and the original graph- structured data. Therefore, the loss function in Eq. (9) is employed during the pretraining stage."}, {"title": "Algorithm 1 Optimization Procedure for SSCDL", "content": "Input:\nThe training graphs $G_r = \\{(G_i)\\}_{i=1}^{n_g}$, the testing graphs $G_t = \\{(G_i)\\}_{i=1}^{n_t}$, the number of iterations $epochs$, param- eters $\\alpha > 0$ and $\\beta > 0$.\n1: Construct a weakly augmented view $G_s$ and a strongly augmented view $G_w$ from $G_r$;\n2: for $t = 1$ to $epochs$ do\n3: Compute $P'$ and $P_w$ via Eq. (13) using the unlabeled parts of $G_r$ and $G_w$, respectively;\n4: Update the network by minimizing $L_s$ in Eq. (9)\n5: end for\n6: for $t = 1$ to $epochs$ do\n7: Compute $p_i$ via Eq. (4) for each labeled graph in $G_r$;\n8: Compute $P'$ and $P_w$ via Eq. (13) using the labeled parts of $G_r$ and $G_w$, respectively;\n9: Compute $p(h|h_i)$ and $p(h'|h_i)$ via Eq. (6) using the labeled parts of $G_r$, $G_s$ and $G_w$;\n10: Update the network by minimizing $L$ in Eq. (15)\n11: end for\n12: Compute $p_i$ via Eq. (4) for each testing graph in $G_t$;\nOutput: The predicted labels of the testing graphs $G_t$.\n2) Fine-Tuning Stage: During the fine-tuning stage, a small number of labeled graphs are employed to train the SSCDL model. We employ the cross-entropy loss function for graph classification, i.e.,\n$L_c = -\\frac{1}{n_g} \\sum_{i=1}^{n_g} \\sum_{j=1}^{C} y_{ij} log p_{ij}$\nwhere $y_i \u2208 R$ denotes the one hot encoding for the label of the $i$th graph, and $p_{ij}$ can be computed via Eq. (4). We can obtain the graph-level representation H, from a strongly augmented view via the shared GNN encoder module, similar to H' and Hw. The loss function of the SSCDL model in Eq. (8) is introduced in the fine-tuning stage. Instead of maximizing the mutual information between the strongly augmented view and the original graph-structured data, the weakly augmented view can provide useful information to bridge the gap by minimizing the distribution divergence $L_d$. Therefore, the overall loss of the proposed SSCDL model consists of three main components:\n$L = \\alpha L_c + L_s + \\beta L_d$\nwhere \u03b1 and \u03b2 are tradeoff hyperparameters. Algorithm 1 summarizes the entire optimization procedure of the proposed SSCDL method."}, {"title": "D. Theoretical Justification", "content": "1) Why is Pretraining Needed in SSCDL?: Let two random variables $U \u2208 \\mathbb{R}^{d'}$ and $V \u2208 \\mathbb{R}^{d'}$ be node embeddings corresponding to a weakly augmented view and the original view, respectively, where d' is the dimensionality of the node embeddings. The loss function of the SSCDL model in Eq. (9) is a lower bound of mutual information (MI), i.e.,\n$I (U; V) \\geq log (n_g \u2013 1) - L_s$\nTherefore, minimizing the loss function of the SSCDL model in Eq. (9) equivalent to maximizing a lower bound on I(U; V). The proof is similar to that of minimizing the InfoNCE loss [13]. By pretraining, the intrinsic semantic information is preserved as much as possible when applying weak augmentation to the graph-structured data. This explains why $p (h' | h_i)$ can provide valuable supervision information for $p (h | h')$ in the SSCDL model.\n2) Generalization Bound of the Distribution Divergence: We analyze the generalization bound of the distribution diver- gence in $L_d$ in Eq. (8). The proof of Theorem 1 is provided in the supplementary material. According to Theorem 1, $L_d$ has a specific lower bound in Eq. (8) if two conditions are satisfied, i.e., sim $(h_i, h_k) = 0$ $(1 \u2264 k \u2264 K)$ and $K > e^{\\tau}+ 1$. In particular, we have $L_d \u2265 0$ if $K > e^{\\tau} \u2013 1$. Therefore, a lower bound is theoretically guaranteed when minimizing $L$ in Eq. (15).\nTheorem 1 Assume that there are $n_g$ graphs and $K$ neg- ative samples for each graph, where sim $(h_i, h_k) = 0$ $(1 \u2264 k \u2264 K)$. Given two conditional distributions $h'$ and $h$ relative to $h_i$, denoted as p(h|hi) and p(h|hi), re- spectively, and the distribution divergence L_d in Eq. (8), the following inequality holds:\n$L_d \\geq log (K + 1) \u2013 \\frac{1}{\\tau}$\nif K satisfies the following condition, i.e.,\n$K> e^{\\frac{1}{\\tau}}-1$\nwhere \u03c4 is a temperature parameter.\n3) Analysis of Augmented Views:\nDefinition 1 (Sufficient Augmented View) Given an encoder f, an augmented view $U_{suf}$ is sufficient in contrastive learning if and only if I($U_{suf}$; v) = I(f ($v_{suf}$); v), where v represents an original view.\nIntuitively, the augmented view $U_{suf}$ is sufficient if all the information in $U_{suf}$ is preserved at approximately v during the graph embedding encoding phase. This indicates that f ($U_{suf}$) contains all the shared information between $U_{suf}$ and v.\nDefinition 2 (Minimal Sufficient Augmented View) The sufficient augmented view $U_{min}$ is minimal if and only if I(f ($U_{min}$); $U_{suf}$) \u2264 I(f($U_{suf}$); $U_{suf}$), f($U_{suf}$) that is sufficient.\nTheorem 2 Graph representations obtained by GNNs are em- ployed to predict y in a graph classification task. The minimal sufficient view $v_{min}$ contains less task-relevant information from the original view v than other sufficient view $U_{suf}$. Thus, we have\nI ($U_{min}$, y) = I ($U_{suf}$, y) + I ($U_{min}$, $Y|v$) .\nAmong all sufficient augmented views, the minimal suffi- cient view $U_{min}$ contains the least information about $U_{suf}$. It is assumed that $U_{min}$ contains only the information shared between $U_{min}$ and v. This implies that $U_{min}$ eliminates the information that is not shared between $U_{min}$ and v. However, some task-relevant information might not be present in the shared information between views [14], [25]. The proof of Theorem 2 is available in the supplementary material. Accord- ing to Theorem 2, $U_{suf}$ contains more task-relevant informa- tion. SSCDL is an end-to-end graph representation learning model. Therefore, $U_{suf}$ provides more valuable information than does $U_{min}$ for SSCDL.\nLet $U_{min}$ and $U_{suf}$ be a strongly augmented view and a weakly augmented view, respectively. The view $U_{min}$ intro- duces diversity to graph-structured data, which can enhance the generalizability of GNNs. In contrast, the view $U_{suf}$ con- tains nonshared information between $U_{min}$ and v, which may be crucial for graph classification tasks. This demonstrates that the distribution divergence $L_d$ in Eq. (8) effectively leverages both the diversity and the quantity of data provided by graph- structured data augmentations, while preserving the intrinsic semantic information."}, {"title": "V. EXPERIMENTS", "content": "A. Experimental Settings\n1) Datasets: We employed eight benchmark graph datasets from the TUDataset [26], including MUTAG, PROTEINS, IMDB-B, IMDB-M, RDT-B, RDT-M5K, COLLA and GITHUB, for evaluation. The statistics of these datasets and the parameter settings are given in the supplementary material.\nFollowing the experimental settings in [8], we evaluated all competing algorithms via 10-fold cross-validation. Each dataset was randomly shuffled and evenly divided into 10 parts. One part was employed as the test set, another as the validation set, and the remaining parts were used for training. The training set contains two types of graphs. Three different percentages of the dataset set, 30%, 50% and 70% are selected from each graph set as labeled training graphs, while the labels of the remaining graphs are unknown during training. Classification accuracy was employed to evaluate the performance of all competing algorithms. We reported the average results and the standard deviations of the classification accuracy.\n2) Comparison Methods: To evaluate the performance of graph classification, we compared the results of the proposed SSCDL method with those of several state-of-the-art graph classification methods, including GraphCL [17], GLIA [8], G- Mixup [20], the graph contrastive masked autoencoder (GC- MAE) [23] and graph reference distribution learning (GRDL) [27]. In particular, we employed the GCN as a backbone in GraphCL. Additionally, we considered two additional base- lines for graph classification, namely, the GCN [5], and GAT [6]. The source code for SSCDL is available online at https://github.com/chenjie20/SSCDL.\nB. Performance evaluation\nThe graph classification results for competing methods across the eight graph datasets are reported in Table I. The best and second-best results are highlighted in bold and underlined, respectively. Table I shows that the SSCDL method consistently outperforms the other competing methods. For example, the SSCDL method achieves performance improve- ments of approximately 2.11%, 0.55% and 0.49% over the second-best method (GLIA) at labels ratios of 30%, 50% and 70%, respectively, with the MUTAG dataset. The gap in the classification results between the SSCDL and GLIA is small when the label ratio is 50% or 70% with the MUTAG dataset, due to the small number of graphs in this dataset. However, the gap in the classification results widened with the other datasets as the number of graphs increased. Moreover, we observe the same advantages of the SSCDL method with different label ratios.\nGraphCL and GLIA are two representative graph contrastive learning-based methods used in the experiments. The GLIA algorithm achieves better graph classification results than the other competing methods do, such as G-Mixup, GCMAE and GRDL. GraphCL also achieves satisfactory results in the experiments. This finding highlights the significant benefits of the use of contrastive learning on graphs. The symbol '-' in Table I indicates an out-of-memory condition when the competing methods are applied to the corresponding datasets. GCMAE and GRDL often fail to perform graph classification for several of the large datasets, e.g., the RDT-M5K and GITHUB datasets, because of memory constraints.\nC. Ablation Study\nTo investigate the impact of the loss components $L_s$ in Eq. (15), we performed ablation studies on graph classification tasks without the pretraining stage. In particular, we considered two scenarios in the experiments. We first employed the loss components $L_c$ and $L_s$ in Eq. (15), which represents a self- supervised graph classification model with pretraining. This model retains only positive pairs of node representations for contrastive learning, which measures the similarity between the original features and the corresponding weakly augmented features. Then, we only conducted the fine-tuning stage of SSCDL, which involves the two loss components, $L_c$ and $L_d$, in Eq. (15). The invariants of the methods corresponding to these two scenarios are referred to as SSCDLcl and SSCDLft, respectively."}, {"title": "D. Parameter Sensitivity Analysis", "content": "We first perform parameter sensitivity analysis on two criti- cal parameters, \u03b1 and \u03b2, from Eq. (15). These parameters were selected from the set {0.01, 0.05, 0.1, 0.5, 1} for SSCDL. The remaining hyperparameters in the proposed SSCDL method are determined by the parameter settings. Owing to space limitations, we conducted experiments using two representa- tive datasets, i.e., the IMDB-B and COLLAB datasets. Fig. 2 shows the graph classification results with the IMDB-B and COLLAB datasets with different \u03b1 and \u03b2 combinations across different percentages of training samples, respectively. The classification results of the SSCDL method fluctuate slightly with different \u03b1 and \u03b2 combinations. This finding indicates that the proposed SSCDL method usually achieves satisfactory classification results with relatively wide ranges of \u03b1 and B values."}, {"title": "E. Empirical Investigation", "content": "We empirically investigated the impact of varying node masking ratios on the performance of the proposed SSCDL method. The node masking ratio for weak augmentation was selected from the set {0.1,0.2, 0.3, 0.35}. The node masking ratio for strongly augmented features is twice that of weak augmentation. Fig. 3 shows the graph classification results with different node masking ratios across different percentages of training samples with eight benchmark graph datasets. The graph classification result often increases when the node masking ratio gradually increases from 0.1 to 0.3 with most datasets. This finding demonstrates that self-supervised con- ditional distribution learning can effectively support graph representation learning. Additionally, the optimal range for the node masking ratio is relatively narrow, making it easier to determine this parameter in practice. In contrast, the graph classification result often decreases when the node masking ratio increases to 0.35. The primary reason for the decline in graph classification performance is the severe degradation of graph structure information in the strongly augmented view, where the node masking ratio is 0.7."}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduce the SSCDL method for semisu- pervised graph classification tasks. The SSCDL model is designed to align the conditional distributions of weakly and strongly augmented features over the original features. The proposed SSCDL method leverages the diversity and abun- dance provided by graph-structured data augmentations while preserving intrinsic semantic information. Moreover, SSCDL resolves the conflict between the message-passing mechanism of GNNs and the contrastive learning of negative pairs within intraviews. Additionally, we present a semisupervised learn- ing scheme consisting of pretraining and fine-tuning stages. Extensive experimental results with various graph datasets demonstrate the superior performance of the SSCDL method."}]}