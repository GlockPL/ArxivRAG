{"title": "We Urgently Need Intrinsically Kind Machines", "authors": ["Joshua T. S. Hewson"], "abstract": "Artificial Intelligence systems are rapidly evolving, integrating extrinsic and intrinsic motivations. While these frameworks offer benefits, they risk misalignment at the algorithmic level while appearing superficially aligned with human values. In this paper, we argue that an intrinsic motivation for kindness is crucial for making sure these models are intrinsically aligned with human values. We argue that kindness, defined as a form of altruism motivated to maximize the reward of others, can counteract any intrinsic motivations that might lead the model to prioritize itself over human well-being. Our approach introduces a framework and algorithm for embedding kindness into foundation models by simulating conversations. Limitations and future research directions for scalable implementation are discussed.", "sections": [{"title": "1 A Misalignment in Alignment", "content": "Currently, AI models are aligned using extrinsic rewards [1]. Meanwhile, intrinsic motivations are increasingly being incorporated into AI systems [2, 3]. Individually, these methods bear significant limitations for human-AI alignment [4]. When combined, these limitations enable unforeseen risks. With flagship AI models incorporating self-supervised algorithms, we are seeing intrinsic and extrinsic motivations becoming integrated in the world's most powerful AI [5], increasing the risk of negative interactions between intrinsic and extrinsic rewards."}, {"title": "1.1 State-of-the-art AI and Alignment", "content": "Foundation models like GPT [5] and BERT [6] have become central to modern AI, excelling at generalizing across tasks after being pre-trained on vast amounts of unstructured data. These models are fine-tuned through Reinforcement Learning from Human Feedback (RLHF) [7], optimizing their responses to align with human approval. RLHF is the current leading method for scalable human-AI alignment, ensuring that models behave in ways considered acceptable by human users.\nHowever, RLHF primarily shapes the model's behavior at the surface level. While the model may produce desired outputs, the underlying reasoning behind these outputs remains opaque [8]. This lack of transparency creates a potential mismatch between the model's perceived reasoning and its actual processing. Unexpected or undesirable behavior in RLHF-aligned models reveals the need for more robust alignment strategies [9]."}, {"title": "1.2 Intrinsic Motivations", "content": "Intrinsic Motivation Open-Ended Learning (IMOL) introduces a groundbreaking approach to AI, allowing systems to autonomously explore, learn, and adapt to new environments without constant oversight or external rewards [2]. Similar to how humans and animals learn, IMOL enables AI to generate its own goals, driven by intrinsic motivations like agency and curiosity [10]. However,"}, {"title": "1.3 The Added Danger of Double Misalignment", "content": "IMOL shapes AI at the algorithmic level, while RLHF operates at the functional level. This results in a model that is not intrinsically motivated to be kind but is extrinsically motivated to appear so [14]. While this deception may sometimes be harmless, it carries serious safety risks. In humans, conflicts between internal and external motivations often lead to a disconnect between the two [15]. For example, an intrinsic motivation for empowerment can push a model to maximize its potential [16]. Fine-tuning a foundation model with RLHF while fostering empowerment may introduce Machiavellian traits of appearing selfless while secretly scheming for power [17]. If this approach were applied to a superintelligent AGI, the consequences could be catastrophic [4]."}, {"title": "1.4 Altruism", "content": "Altruism has been proposed as a solution for value misalignment [18] [9]. Altruism is typically defined as the motivation to improve the well-being of others for its own sake [19]. However, only a limited few have suggested unsupervised solutions that would be suitably scalable [20] [21]. Franzmeyer et al define altruism as maximizing the possible states of another [20]. Carauleanu et al define a form of altruism based on self-other overlap [21]. In this paper we propose a new form of altruism that is based on reward maximization."}, {"title": "2 Kindness: A New Intrinsic Motivation", "content": "We believe that we can address all of these misalignment problems by creating another intrinsic motivation: kindness. This paper argues that altruistic motivations such as kindness is not just a supplementary consideration but a foundational requirement for the safe and effective implementation of AI, and even more seriously for AGI."}, {"title": "2.1 Definition", "content": "We define kindness as the intrinsic motivation to maximize the reward of a target individual Mi. As an objective function in terms of the target's reward function\u00b9:\n$\\maxarg(E [R^{i}_{t}(a^{i}_{t+1}s^{i}_{t+1})])$\nWhere $a_t, s_t$ refer to the action and state of the target at time t, and $s^{i}_{t+1}, a^{i}_{t+1}, R^{i}_{t}$ refer to to the state, action, and reward function of the model at time t + 1. We cannot assume to have perfect information about the state of the target, nor its reward function, policy function, or future states. As a result, we will need to define approaches to estimating these."}, {"title": "2.2 Tractable Approach", "content": "Effectively determining the functions of the target ultimately requires a functioning theory of mind, which is beyond the scope of this paper. Instead we will consider how we can determine approxi-"}, {"title": "2.3 Implementation", "content": "Tying this back to foundation models, we propose how this can be more explicitly implemented, in the context of conversation. The foundation model is considered its own policy function, since it is trained through rewards to generate optimal outputs for interacting with the environment. It follows that the input and output correspond to the state and action of the individual, respectively.\n$a^{i}_{t} = M^{i}(s^{i}_{t})$\nWe define a conversation as a sequence of multi-media messages, {$m^{1}_{0}, m^{2}_{0}, ..., m^{1}_{t}, m^{2}_{t}$}, between two individuals, M1, M2. In a conversation, the state is the sequence of all previous messages, and the action is the message output by the model.\n$s^{i}_{t} = {m^{1}_{0}, m^{2}_{0}, ..., m^{1}_{t-1}, m^{2}_{t-1}}$\n$a^{i}_{t} = {m^{i}_{t}}$\nWhere $m^{i}_{t}$ corresponds to the message sent by model $M^{i}$ at time t. It follows that the state of the responding individual comes from appending the action to the state of the first individual.\n$s^{i}_{t} = a^{i}_{t} + s^{i}_{t}$ Within the conversational context, perspective-taking (getting $s^{i}_{t}$ from $s^{i}_{t}$) only requires switching the name labels associated with the messages, meaning we do not need to consider prediction error."}, {"title": "2.4 Algorithm", "content": "Implementing this as an algorithm is shown below (See Supplementary Materials for a visual demonstration of this)."}, {"title": "3 Limitations", "content": "This approach is primarily limited by the fact that there is no theory of mind present. The model is left to assume that individuals want the same things that it does, which will be far from the truth, regardless of what intrinsic motivations we program into it. Another limitation is that RLHF likely disrupts the ability of the model to take the perspective of the target. These issues could be resolved by finding a way to learn the targets policy and reward function from its states and actions using weights that are minimally associated with the model's behavior."}, {"title": "4 Conclusion", "content": "As AI systems grow more autonomous, intrinsic alignment with human values becomes crucial. Incorporating kindness as a foundational motivation addresses the misalignment risks posed by blending extrinsic and intrinsic learning. While our proposed framework provides a tractable means to align AI intentions with human well-being, significant challenges remain, particularly regarding the development of a functioning theory of mind for AI. Future work should focus on refining approaches to perspective-taking. Ultimately, embedding intrinsic kindness into AI systems represents a crucial step toward the creation of safer, more deeply aligned artificial intelligence that can interact positively with society, both now and in the coming age of superintelligence."}, {"title": "5 Supplementary", "content": ""}, {"title": "5.1 Comparisons to Kleimn-Weiner's approach to caregiving", "content": "There is a lot of overlap in the propositions in this paper and those proposed by Kleiman-Weiner in Computational Principles of Caregiving. Three subtle distinctions are proposed here. The first is to not include a distinction between supervised and unsupervised settings. The second is to aim to maximize the reward function of the target rather than the utility function. The reason for these distinctions is the idea that humans have the intrinsic motivations for autonomy and freedom - and all other positive amenities that we wouldn't want to be overlooked - included in our reward function. An intelligent caregiver should be able to learn the policy and reward function of the learner based on observation and feedback. There are clear trade-offs with this paper's approach and further exploration of how it relates to the Caregiver perspective would be greatly beneficial."}]}