{"title": "Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models", "authors": ["Bingchen Liu", "Ehsan Akhgari", "Alexander Visheratin", "Aleks Kamko", "Linmiao Xu", "Shivam Shrirao", "Joao Souza", "Suhail Doshi", "Daiqing Li"], "abstract": "We introduce Playground v3 (PGv3), our latest text-to-image model that achieves state-of-the-art (SoTA) performance across multiple testing benchmarks, excels in graphic design abilities and introduces new capabilities. Unlike traditional text-to-image generative models that rely on pre-trained language models like T5 or CLIP text encoders, our approach fully integrates Large Language Models (LLMs) with a novel structure that leverages text conditions exclusively from a decoder-only LLM. Additionally, to enhance image captioning quality-we developed an in-house captioner, capable of generating captions with varying levels of detail, enriching the diversity of text structures. We also introduce a new benchmark CapsBench to evaluate detailed image captioning performance. Experimental results demonstrate that PGv3 excels in text prompt adherence, complex reasoning, and accurate text rendering. User preference studies indicate the super-human graphic design ability of our model for common design applications, such as stickers, posters, and logo designs. Furthermore, PGv3 introduces new capabilities, including precise RGB color control and robust multilingual understanding.", "sections": [{"title": "Introduction", "content": "Great progress has been made in the text-to-image generative models since last year [41, 50, 52, 15, 54]. The model architecture for large-scale text-to-image model is shifting from a traditional UNet-based[41] model to a transformer-based model[44] due to its scalability and simplicity. In this work, we built on this progress to develop a new DiT-based diffusion model and scale it to 24B parameters. We further propose a Deep-Fusion architecture to leverage the knowledge of the modern decoder-only large-language models [13] for the task of text-to-image generation.\nIn section 2 we describe the simplified model architecture, noise scheduling, and the new Variational Autoencoder (VAE). Our novel text-to-image model structure features a deep integration of an LLM, fully leveraging the LLM's internal prompt understanding to achieve state-of-the-art prompt-following performance. Section 3 discusses training details, including the use of multi-level captions and model merging during the post-training phase. Section 4 introduces our in-house captioner and a new captioning benchmark CapsBench.\nIn section 5, we present qualitative examples of our model's capabilities across five key aspects: photo-realism, prompt-following, text-rendering, RGB color control, and multilingual understanding. Most of the images displayed are generated from our evaluation testing prompt set, which was curated through our human-in-the-loop pipeline and excluded from our training set.\nIn section 6, we quantitatively evaluate our image models across four categories. Notably, we demonstrate that PGv3 exhibits superior graphic design capabilities, often surpassing even real human designers. We address image-text alignment and reasoning using the LLM-aided DPG-benchmark [24]. To further validate this, we developed an in-house version of the DPG-benchmark with more complex testing prompts and used GPT-40 as the VQA evaluator,; this new benchmark demonstrates PGv3's SoTA prompt-following performance. Additionally, we briefly showcase the reconstruction quality of our new VAE. Finally, we assess image quality using standard benchmarks such as ImageNet and MSCOCO. For our in-house captioner model, details and results of the CapsBench are discussed in section 7."}, {"title": "Methods", "content": "Playground-v3 (PGv3) is a Latent Diffusion Model (LDM) [52] trained using the EDM [27] for-\nmulation. Like other models such as DALL-E 3 [4], Imagen 2 [54], and Stable Diffusion 3 [15],\nPGv3 is designed for text-to-image (t2i) generation tasks. However, it distinguishes itself through a novel model structure, diverging from the commonly-used text encoders like T5 [49] or CLIP [48],\nwhich we consider to be insufficient. Instead, PGv3 fully integrates a large language model (LLM), specifically Llama3-8B [13], to enhance its capabilities in prompt understanding and following."}, {"title": "Model", "content": "There have been a lot of research efforts aimed at leveraging LLMs to improve t2i models' prompt-following abilities. These efforts typically involve either using LLMs as the text encoder to replace the commonly-used T5 and CLIP, or employing LLMs to adapt or revise prompts for a pretrained T5/CLIP-based t2i model [19, 56, 47, 24, 38, 3]. In PGv3, we adopt the former approach, i.e. we don't use any NLP encoders like T5 and CLIP, and solely rely on Llama3-8B to provide text conditioning to our diffusion model, from the initial stage of the model training."}, {"title": "Text Encoder", "content": "We take a notably different approach to using a pre-trained text model. The standard practice has been to use the output from the final layer of the T5 encoder or CLIP text encoder, with some research also finding value in incorporating the penultimate layer's output. It is well-established that each layer within a transformer model captures different representations, containing varying levels of word-level and sentence-level information [14, 12, 51]. However, we found that selecting the optimal layers for conditioning the t2i model can be cumbersome, particularly when using a decoder-style LLM, which arguably possesses superior generative power due to its more complex internal representations.\nWe believe that the continuity of information flow through every layer of the LLM is what enables its generative power and that the knowledge within the LLM spans across all its layers, rather than being encapsulated by the output of any single layer. Motivated by this, we designed our t2i model structure to replicate all the transformer blocks of an LLM. This design allows our model to take the hidden embedding outputs from each corresponding layer from LLM as conditioning at each respective layer of our t2i model. We argue that this approach fully leverages the complete \"thinking process\" of the LLM, guiding our t2i model to mirror the LLM's reasoning and generative process. As a result, our model achieves unparalleled prompt-following and prompt-coherence abilities when generating images."}, {"title": "Model Structure", "content": "PGv3 employs a DiT-style [44] model structure. Each transformer block in our image model is set up as identical to the corresponding block in the LLM we utilize, Llama3-8B in this case, including matching parameters such as hidden dimension size, number of attention heads, and attention heads dimensions. We trained only the image model component, leaving the LLM untouched. Note that during the diffusion sampling process, the LLM part of the network only needs to run once to generate all the intermediate hidden embeddings required.\nThe simplified model block is illustrated in Fig.2. Each transformer block consists of only one attention layer and one feed-forward layer, mirroring the structure of Llama3. Unlike most traditional CNN-based diffusion models [50, 51], which typically separate self-attention on image features from cross-attention between image and text features, we performed a single joint attention operation. In this approach, the image query attends to a concatenation of image keys and text keys, allowing it to extract relevant features from a combined pool of image and text values. This joint attention design is primarily motivated by the desire to reduce computational costs and inference time.\nAlthough we finalized our model design and began training before the publication of the SD3 paper [15], we have observed that the PGv3 transformer block shares a similar spirit with the MMDiT proposed in SD3. When considering the corresponding LLM block and image block as a single layer"}, {"title": "", "content": "in PGv3, our approach reflects a similar philosophy to MMDiT-enabling integrated and continuous interaction between image and text features across all model layers. However, our design is more minimalist and leverages the text encoder more thoroughly.\nThere have been many research efforts aimed at exploring better model structures, such as [35, 28, 8]. We experimented with several structural tweaks for PGv3 but did not observe significant benefits from most of them. Here we report on some designs that we found useful and incorporated into our final model design.\nU-Net skip connections between transformer blocks. We applied U-Net [53] skip connection across all the image blocks, as previously tried in U-DiT[58].\nToken down-sampling at middle layers. Among the 32 layers, we reduced the sequence length of the image keys and values by four times in middle layers making the whole network resembles a traditional convolution U-Net with only one level of down sampling. This adjustment slightly speeds up both training and inference time, and we did not observe any performance degradation as a result.\nPositional embedding. We used the traditional Rotary Position Embedding (RoPE) [57], similar to what is used in Llama3. Since we work with 2-dimensional image features, we employed a 2D version of ROPE. Several prior works have explored variations of 2D-RoPE; in our study, we primarily experimented with two types, which we refer to as \"interpolating-PE\" and \"expand-PE.\"\nThe \"interpolating-PE\" approach keeps the start and end positional IDs fixed, regardless of sequence length, and interpolates the positional IDs in between an idea employed by models like SD3 and others. The \"expand-PE\" method, on the other hand, increases the positional ID proportionally with sequence length, without applying any tricks or normalization (i.e., vanilla positional embedding).\nWe found that the \u201cinterpolating-PE\" had a significant downside: it caused the model to overfit heavily on training resolutions and failed to generalize to unseen aspect ratios. In contrast, the \u201cexpand-PE\" method performed well, showing no signs of resolution overfitting. Therefore, we opted to use the traditional positional embedding in the end."}, {"title": "New VAE", "content": "The Variational Autoencoder (VAE) [30] used in Latent Diffusion Models (LDM) plays a crucial role in determining the model's upper bound for fine-grained image quality. In the work of Emu [11], they increased the VAE latent channels from 4 to 16, which resulted in an enhanced ability to synthesize finer details, especially for smaller faces and words. Following this approach, we trained our in-house VAE with 16 channels. Additionally, instead of training solely at a 256 * 256 resolution, we extended the training to a 512 * 512 resolution, which further enhanced reconstruction performance.\nWe attempted to incorporate the frequency method proposed in Emu's VAE training but did not ob-serve any performance improvement. Additionally, we explored more aggressive VAE modifications, including using pretrained image encoders like the CLIP image encoder or the DINOv2 encoder [42]. Unfortunately, although these approaches yielded VAEs with comparable reconstruction abilities, the latent spaces generated by these pretrained image encoders, whether frozen or fine-tuned, resulted in unstable diffusion training, therefore we did not pursue this exploration further."}, {"title": "Model Training", "content": "Noise schedule. Despite the recent rise of flow-matching models [36, 39, 15], we continue to use the EDM schedule [27], as we did in PGv2.5. There is no specific reason for not transitioning to flow-matching; we simply have not encountered any performance limitations with the EDM schedule in our new PGv3 model structure. In the mean time, we did experiment with the dynamic timestamp weighting technique proposed in EDM2 [28], which is tailored to the EDM schedule. Unfortunately, we did not observe any significant performance improvements. We suspect this may be due to differences in model size and data, as our training scale is much larger and more diverse than the ImageNet training setup used in EDM2.\nMultiple Aspect-Ratio Support. Following our previous work in PGv2.5 [33], we adopted the multi-aspect ratio training strategy. We started with square image training at 256 * 256 low-resolution and used the online bucketing strategy at the higher resolution 512 * 512 and 1024 * 1024 pixel scales."}, {"title": "Multi-Level Captions per Image", "content": "We developed an in-house Vision Large Language Model (VLM) captioning system capable of generating highly detailed descriptions of images, including small objects, lighting, style, and visual effects. We ran benchmarks on this in-house captioner (PG Captioner) against other state-of-the-art systems, such as GPT-40. Our results show that our model outperforms others across multiple image-specific categories, which will be discussed in detail later.\nPrevious studies [4, 15] have proposed re-captioning datasets and using machine-generated captions as text conditions to enhance model performance. In our work, we further improved these captioning conditions by generating multi-level captions to reduce dataset bias and prevent model overfitting. As shown in Fig.3, for each image, we synthesized captions of six different lengths, ranging from fine-grained details to coarse concepts. During training, we randomly sampled one of these six captions for each image at different iterations.\nThe idea behind using multi-level captions is to help the model learn a better linguistic concept hierarchy. This allows the model to not only learn the relationship between words and images but to also understand semantic relationships and hierarchies among words, by viewing the same image with varying levels of conceptual detail. As a result, our model demonstrates greater diversity when responding to short prompts involving general concepts, and adheres more closely to prompts when provided with highly detailed descriptions. Moreover, when training on datasets with fewer data samples, such as during the supervised fine-tuning (SFT) stage, multi-level captions helped prevent overfitting and significantly enhanced the model's ability to generalize the good image properties of the SFT dataset to other image domains. As shown in Fig.4, when given a short and simple prompt, our model can generate diverse content, varying in style, color tone, ethnicity, and other attributes.\nTraining stability. We observed loss spikes during the later stages of model training. This behavior manifested as the training loss becoming unusually large without resulting in NaN values, causing the model to enter a state where it could no longer generate meaningful content. We tried several"}, {"title": "", "content": "strategies to address this issue, including lowering the betas of the Adam optimizer [29], using gradient clipping, and reducing the learning rate. However, none of these methods proved effective. Ultimately, we found that the following approach helped mitigate the issue.\nDuring training, we looped through the gradients of all model parameters and counted how many gradients exceeded a specific gradient-value threshold. If this count surpassed a predefined count threshold, we discarded the training iteration, meaning we do not update the model weights using the gradients. We developed this solution after observing training behavior around the time of a loss spike. Specifically, we noticed that the number of large-valued gradients began to increase a few iterations before a spike occurred, peaking at the spike iteration. By recording the count of large-valued gradients many iterations before a spike, we set that value as a threshold to trigger the abandonment of a training iteration.\nThis approach differs from gradient clipping, which we found insufficient to prevent the model weights from being adversely affected by problematic gradients. Gradient clipping only reduces the magnitude of the gradients but does not alter the direction of the weight updates. As a result, the weights could still move toward collapse. By discarding the entire batch of gradients, we avoided this risk and ensured the model remained on a stable training trajectory."}, {"title": "Captioning Model", "content": "For the captioning model, we used a standard vision-language architecture that consists of a vision encoder, a vision-language adapter, and a decoder-only language model [37, 34, 9]. Following"}, {"title": "PG Captioner", "content": "Captioning evaluation is a complex problem. There are two main classes of captioning evaluation metrics. The first one is reference-based metrics - BLEU [43], CIDEr [59], METEOR [2], SPICE [1]. These metrics use a ground truth caption or a set of captions to calculate similarity as a quality metric. The problem with such methods is that the score is bound to the reference format. Recent works, like FAIEr [60] propose a combination of visual and textual scene graphs to create better metrics. The second class of evaluation metrics is reference-free metrics - CLIPScore [20], InfoMetIC [23], TIGEr [26]. These methods use semantic vectors from the reference image or multiple regions of the image to calculate a similarity metric for the proposed captions. The drawback of these methods is that for dense images and long and detailed captions, semantic vectors will be not be representative as they will include too many concepts.\nA new type of evaluation method is a question-based metric. The authors of QACE [32] proposed a framework for generating questions from captions and evaluating proposed captions using these questions. A similar approach was recently proposed for evaluation of image generation models [10]. Davidsonian Scene Graph (DSG) allows generation of questions organized in dependency graphs that help to comprehensively evaluate text-to-image models.\nInspired by DSG and DPG-bench [24], we propose a reversed approach for image captioning evaluation. Based on the image, we generate \"yes-no\" question-answer pairs across 17 categories: general, image type, text, color, position, relation, relative position, entity, entity size, entity shape, count, emotion, blur, image artifacts, proper noun (world knowledge), color palette, and color grading. The answer to the majority of questions is \"yes\", as this provides a more clear signal of correctness. During evaluation, we use an LLM to answer the questions based solely on the candidate caption. Available answer options are \"yes\", \"no\", and \"n/a\" (when it is impossible to answer the question). We then compare the answers with the reference answers to calculate the resulting accuracy."}, {"title": "CapsBench", "content": "We are releasing CapsBench\u00b9 a set of 200 images and 2471 questions for them, resulting in 12 questions per image on average. Images represent a wide variety of types: film scenes, cartoon scenes, movie posters, invitations, advertisements, casual photography, street photography, landscape photography, and interior photography. A diversity of questions and images enables comprehensive evaluation of image captioning systems. We describe evaluation results for PG Captioner and other state-of-the-art models in Section 7."}, {"title": "Image qualitative results", "content": null}, {"title": "Photo-realism", "content": null}, {"title": "Prompt-Following", "content": "Fig.9 illustrates three examples highlighting the superior prompt-following ability of our PGv3 model compared to other models, including Ideogram-2 and Flux-pro. In each prompt, the colored text highlights the specific details that other models fail to capture, while PGv3 consistently follows"}, {"title": "Text Rendering", "content": "Fig.10 demonstrates the text rendering capabilities of PGv3. The model can generate images across a wide range of design categories, including posters, logos, stickers, book covers, and presentation slides. Notably, PGv3 can reproduce trending memes with customized text and create entirely new memes with limitless characters and compositions, thanks to its powerful prompt-following and text rendering abilities."}, {"title": "RGB Color Control", "content": "PGv3 achieves exceptionally fine-grained color control in generated content, surpassing standard color palettes. With its robust prompt-following capabilities and specialized training, PGv3 enables users to precisely control the color of each object or area within an image using exact RGB values, making it ideal for professional design scenarios that require precise color matching. As shown in Fig. 11 and 12, PGv3 accepts both overall color palettes, automatically applying the specified colors to appropriate objects and areas, and dedicated color values for specific objects as defined by the user."}, {"title": "Multilingual Prompt Input", "content": "Thanks to the powerful LLM used as our text encoder, which inherently understands multiple languages and forms well-related word representations across them, PGv3 can naturally interpret"}, {"title": "Image Model Quantitative Evaluation", "content": null}, {"title": "Evaluation on Graphic Design Ability", "content": "We collected around 4k high-quality images created by human designers in common graphic design applications like monograms, stickers, t-shirt, mockups, mobile wallpaper, logo, cards, posters etc."}, {"title": "Image-text Alignment Evaluation", "content": null}, {"title": "DPG-bench", "content": "In order to access the text following ability (text-image consistency), we compared our models with SOTA models on two benchmarks. We measured DPG-bench follows [24] in Tab. 1, and our model achieved an overall score of 87.04, which is higher than DALLE 3 and SD3-medium."}, {"title": "DPG-bench Hard", "content": "However, upon closer inspection of the results, we found that the Visual-Question-Answering (VQA) model used in the open-sourced DPG-bench often makes errors, providing incorrect answers even when models correctly generate the content specified by the testing prompts. To more accurately assess the true prompt-following performance, we developed an upgraded version called \"DPG-bench Hard.\" The following outlines the steps used to construct this benchmark.\nFirst, we applied k-means clustering on our internally curated dataset, which consists of images rated by real users. This resulted in 120 clusters covering diverse image domains. From each cluster, we randomly selected 20 images near the cluster centers, yielding a testing set of 2,400 images in total. We used GPT-40 to caption these images, ensuring the captions were detailed and comprehensive, with an average length of about 256 tokens. To avoid potential prompt-style bias favoring our model, we particularly choose to not use our in-house Captioner for this task. After generating the captions, we asked GPT-40 to create yes-or-no questions based on each caption, then categorized these questions in line with the original DPG-bench. Each prompt generated an average of 30 questions. This process parallels how we constructed our CapsBench, which will be described in detail in Sec.7\nTo evaluate each model, we generated four images per testing prompt using random seeds, then used GPT-40 to answer the corresponding questions for each image. We summarized the \"yes\" responses from GPT-40 as the final accuracy rate. Despite the possibility of occasional errors by GPT-40, we found the error margin to be below 5%, making the testing scores more closely aligned with human judgment as compared to the original DPG-bench."}, {"title": "", "content": "Fig.16 and Tab. 2 present our testing results. PGv3 achieved an overall accuracy score of 88.62%, outperforming other tested models, including Flux-pro and Ideogram-2, across all sub-categories. Notably, a significant portion (40%) of the testing images in our DPG-bench Hard set belong to graphic design-related categories, such as posters, ads, greeting cards, and book covers, all of which require the model to be able to follow the complicated object arrangements and layout requirements from the testing prompts. This benchmark not only reflects PGv3's leading prompt-following performance but also effectively highlights PGv3's superior performance in graphical design domains."}, {"title": "Image-Text Reasoning Evaluation", "content": "We evaluated the model's reasoning ability using a popular benchmark, GenEval [18]. In Tab 3, our model achieved a 0.76 overall score, higher than SD3's 0.74. In particular, our model has a much better score on object and position reasoning."}, {"title": "Text Synthesis Evaluation", "content": "Following previous work [6], we selected 1k samples from Mario-eval, which includes text images in Laion, TMDB and OpenLibrary. We calculated a clip score and used Kosmos 2.5 [40] OCR model to detect text, and calculated text rendering accuracy. In Tab. 4, our model has much better text accuracy"}, {"title": "Evaluation on Image Quality", "content": "We present results on two standard benchmarks, ImageNet and MSCOCO, to compare our model's performance against other state-of-the-art models.\nHowever, we maintain our position that ImageNet and MSCOCO may not effectively capture the comprehensive performance of large-scale text-to-image models, especially given the progress of models like PGv3 in prompt-following."}, {"title": "ImageNet Benchmark", "content": "In Tab. 5, we compare PGv3 with our previous model PGv2.5. Since our model takes text as a condition, we follow previous work [15] to use a photo of a {class} format to convert ImageNet class labels into text conditions. We also compared a SoTA method EDM2 [28] model in the same setting. We augmented it with a clip text conditioner for a fair comparison. From the table, PGv3 performs better in terms of FID and FD DINOv2. We also show SoTA class conditioned model performance as a reference; usually, those models take the class label as a condition to achieve better performance."}, {"title": "MSCOCO Benchmark", "content": "We also evaluated our models in MSCOCO validation set with 30k prompts. In table 6 we evaluated both the original prompt (short) and also a reception prompt using PG Captioner (long). We measure the model performance in FID and FD DINOv2 [42], we found FD DINOv2 is a better metrics that is more sensitive to object shapes. We measured our model at different training phases, and compared with the PGv2.5 model for 256 and 512 resolutions. We also compared other SoTA models at the 1024\u00d71024 resolution."}, {"title": "Evaluation on VAE Reconstruction", "content": "Following previous works [11, 15], we increased the VAE latent channel size from 4 to 16, and we trained the VAE in a progressive manner from lower resolution to higher resolution. In Tab. 7, we report reconstruction performance on a validation set on multiple resolutions. From the table, we improve significantly compared to SDXL channel 4 VAE; specifically, at resolution 1024x1024, we achieve 33.44 PSNR."}, {"title": "Captioning Evaluation", "content": "We used CapsBench to evaluate PG Captioner, along with leading proprietary models GPT-40 and Claude-3.5 Sonnet. When generating captions with proprietary models, we used detailed instructions with output schema, few-shot prompting (by providing three examples of high-quality detailed captions) and chain-of-thought reasoning (perform written analysis of the image prior to generating the caption) to achieve the best results.\nFor question answering, we used Claude-3.5 Sonnet. To work around stochastic outputs of Sonnet and ensure consistency, we run question answering three times for every caption and use the consensus answer. As a result, combined score of PG Captioner is 72.19%, Claude-3.5 Sonnet's 71.78%, GPT-40's 70.66%. Fig. 18 shows the results split by categories. We can see that CapsBench helps to reveal shortcomings of existing models for captioning. Captions usually don't contain descriptions of object shapes (e.g., they likely produce \"brown wooden table,\" instead of \"brown rectangular wooden table\"), sizes in the image (e.g., \"the man appears big in the image\"), and visual artifacts (like noise or lens flare). This may be a problem for using these captions in systems heavily reliant on completeness of the captions, such as image generation models or systems for visually impaired users.\nAdditionally, we evaluated the length of the captions to understand whether longer captions correlate with higher evaluation scores. Fig. 19 shows histograms of caption lengths for evaluated models. Interestingly, all three models have different length distributions, where GPT-4o generates the shortest captions and Claude-3.5 Sonnet produces captions that are twice as long on average.\nTo further study the pros and cons of different models, we performed qualitative analysis of captions. In Fig. 20, we show one of the images from CapsBench. This image contains a significant number of subjects and a lot of small details, which makes it very challenging for VLMs. Tab. 8 shows the"}, {"title": "Conclusion", "content": "In this work, we introduce Playground v3, a state-of-the-art text-to-image generation model with strong prompt adherence, reasoning and text rendering ability. It shows superhuman graphic design abilities from a user preference study on various downstream design applications, where we compare samples generated by our model and a high quality curated image dataset created by human designers. We also release to the community a new carefully prepared image captioning benchmark CapsBench, designed for detailed long captions, to facilitate the research and benchmarking of the progress of image understanding ability."}]}