{"title": "Cross-Patient Pseudo Bags Generation and Curriculum Contrastive Learning for Imbalanced Multiclassification of Whole Slide Image", "authors": ["Yonghuang Wu", "Xuan Xie", "Xinyuan Niu", "Chengqian Zhao", "Jinhua Yu"], "abstract": "Pathology computing has dramatically improved pathologists' workflow and diagnostic decision-making processes. Although computer-aided diagnostic systems have shown considerable value in whole slide image (WSI) analysis, the problem of multi-classification under sample imbalance remains an intractable challenge. To address this, we propose learning fine-grained information by generating sub-bags with feature distributions similar to the original WSIs. Additionally, we utilize a pseudo-bag generation algorithm to further leverage the abundant and redundant information in WSIs, allowing efficient training in unbalanced-sample multi-classification tasks. Furthermore, we introduce an affinity-based sample selection and curriculum contrastive learning strategy to enhance the stability of model representation learning. Unlike previous approaches, our framework transitions from learning bag-level representations to understanding and exploiting the feature distribution of multi-instance bags. Our method demonstrates significant performance improvements on three datasets, including tumor classification and lymph node metastasis. On average, it achieves a 4.39-point improvement in F1 score compared to the second-best method across the three tasks, underscoring its superior performance.", "sections": [{"title": "1. Introduction", "content": "Pathology, as a critical component of medical diagnostics, has witnessed significant advancements with the integration of computational techniques. Computational pathology has significantly enhanced the workflow and diagnostic decision-making processes for pathologists (Zhang et al. 2022). In particular, the advent of computer vision has revolutionized the automated analysis of Whole Slide Images (WSIs), drawing considerable attention from the research community. Numerous studies have demonstrated that computer-aided diagnostic systems provide substantial reference value for pathologists (Lu et al. 2021; Lee et al. 2022).\nHowever, the vast scale of WSIs (billions of pixels) and the extremely high information density (comprising hundreds of thousands of cells and heterogeneous tissues) (Juyal et al. 2024) present challenges for achieving slide-level predictions through end-to-end models.\nThe difficulty of multi-classification under unbalanced sample conditions is one of the striking challenges. The distribution of categories in pathology image classification is often unbalanced due to differences in disease incidence and degree of progression. For example, certain disease subtypes may be rarer than others, or there may be little data for a particular grade in the pathology grading. This imbalance is not only observed in the overall distribution of the dataset but also within each WSI. For example, only a fraction of malignant regions may contribute to the positive labels in a WSI, making it challenging to detect these regions accurately. This unbalanced category distribution becomes even more problematic in multi-classification tasks, potentially leading to poor model performance for rare categories, even though some rare lesions may be of significant clinical importance.\nTo address the problem of category imbalance, previous studies have primarily focused on enhancing the representation capabilities of models. This includes the use of pre-trained feature extractors, multi-scale feature fusion, complex instance mining, and curriculum learning approaches (details in section 2). These methods aim to cope with sample imbalance by improving feature representation or learning strategy. WSI image information has huge redundancy, and previous methods seek to extract salient features while removing redundancy. However, we propose that the redundancy of WSI information can provide an alternative solution to the unbalanced sample multi-classification problem.\nWe propose a framework to utilize redundant information in WSIs by reorganizing feature distribution to address multi-classification with unbalanced samples. Pathology images contain complex cellular structures and tissue patterns. By dividing images into sub-bags with similar features, we can better capture and analyze these fine-grained pathological features. Specifically, we construct pseudo-bags across patients using sub-bags, leveraging the rich information in pathology images.\nTo ensure strong feature representation, we use an affinity-based sample selection method for contrastive learning with adjustable difficulty. This reduces category imbalance and improves model generalization from limited samples. Our contributions are:\n\u2022 A two-stage instance bag generation strategy to improve category balance during training. First, generate sub-bags from WSIs to capture feature distributions. Then, combine sub-bags from different patients within the same category to create pseudo-bags, enhancing balanced training and model robustness.\n\u2022 An affinity-based sample selection strategy and a curriculum contrastive learning approach to optimize the feature space.\n\u2022 Significant performance improvement on three datasets, addressing multi-classification imbalance in pathology image classification."}, {"title": "2.Related Work", "content": "In recent years, MIL has been widely used in WSI analysis (Carbonneau et al. 2018). MIL is a weakly supervised framework that organizes data into bags, each containing multiple instances but only one bag-level label, differing from traditional supervised learning where each sample has a corresponding label. The high resolution of WSI makes it difficult to directly input them into deep learning models on existing hardware devices, and manual annotation of WSI images is challenging due to their large size. MIL methods effectively address these challenges in WSI analysis.\nSignificant progress has been made in instance-level and embedding-level MIL research (Li, Li, and Eliceiri 2021). In instance-level MIL, key instances within a bag are selected to represent the entire bag and are assigned a bag-level label. Then, an instance-level classifier is trained, and the final bag-level prediction requires aggregating the prediction results of all instances. The selection of key instances can be achieved using the EM algorithm (Hou et al. 2016) or a histogram-based feature selection algorithm (Zhao et al. 2020).\nOther works (Chikontwe et al. 2020; Kanavati et al. 2020) iteratively train neural networks to facilitate instance-level classifiers. One such method selects key instances and integrates their classification results into the overall WSI prediction using RNN (Campanella et al. 2019).\nIn embedding-based methods, a high-level representation of a bag is learned to build a bag-level classifier. Existing research has proposed various aggregation methods, including clustering (Yang et al. 2023), attention mechanisms (Ilse, Tomczak, and Welling 2018; Li, Li, and Eliceiri 2021; Lu et al. 2021; Xiang and Zhang 2023), transformers (Shao et al. 2021; Yu et al. 2023), and graph networks (Chen et al. 2021; Lee et al. 2022).\nAttention-based methods, such as (Ilse, Tomczak, and Welling 2018; Li, Li, and Eliceiri 2021; Lu et al. 2021), use neural networks to predict the weights of each instance and perform weighted averaging of instance features based on these weights. Transformer-based methods (Shao et al. 2021; Yu et al. 2023; Fourkioti, De Vries, and Bakal 2023) incorporate the query mechanism of transformers into weight prediction, enhancing interaction between instances. Graph-based methods (Hou et al. 2016; Li et al. 2018; Lee et al. 2022; Zheng et al. 2022) utilize a graph structure to preserve spatial relationships between instances and aggregate information of WSI through graph networks. TEA-Graph (Lee et al. 2022) further introduces super-patch to effectively reduce node numbers and improve training efficiency."}, {"title": "2.1 Multiple Instance Learning in WSI", "content": null}, {"title": "2.2 Representation Learning in WSI", "content": "Learning is the process of transforming raw data into more expressive and interpretable feature representations. In deep learning-based WSI analysis, it is challenging to directly input billion-pixel-level images into the network. Therefore, thousands to tens of thousands of patches need to be extracted for feature extraction. Existing methods (Ilse, Tomczak, and Welling 2018; Lu et al. 2021; Zhang et al. 2022; Shao et al. 2021) typically use pre-trained models on ImageNet, which poses challenges in obtaining high quality feature representations.\nTo achieve better feature representations, existing research has proposed methods such as feature extractor pre-training, multi-scale feature fusion, complex instance mining, contrastive learning, task-specific fine-tuning, curriculum learning. For pre-training, VAEGAN (Zhao et al. 2020) trains feature extractors using a combination of variational autoencoders and generative adversarial networks instead of traditional pre-training on classification networks. Other methods (Lazard et al. 2023; Li, Li, and Eliceiri 2021) in-"}, {"title": "3.Method", "content": null}, {"title": "3.1 Feature Distribution-Guided Sub-bag and Pseudo-Bag Generation", "content": "Consider a WSI collection Xi \u2208 {X1,...,Xp} containing P patients with the corresponding label Yi \u2208 {Y1,..., Yp}. Each pathology image is tiled and extracting non-overlapping N foreground patches {Xij}N j=1 by a tissue segmentation method. These patches serve as instances in a weakly supervised task, where instance-level labels are not provided. A feature extraction network is applied to embed all patches {Xij}N j=1 to a bag of feature vectors {fij}N j=1 Following that, an aggregation methods or feature selection algorithm is employed to obtain a bag feature vector, which is then used as the input to a classifier for predicting the patient-level label Yi.\nWe extend the label prediction task to predict the joint distribution of labels Y and latent variables O, where the latent variables O describe the distribution of all regions in the pathological images. We assume that the latent variables O follow the distribution Q(O). Using variational inference, we seek the expression for the variational distribution Q(O):\n$\\sum_i \\ln P(Y_i | X, \\theta) \\geq \\sum_i \\sum_{O_i} Q(O_i) \\ln \\bigg(\\frac{P(Y_i, O_i | X_i, \\theta)}{Q(O_i)}\\bigg),$ (1)\nhere, \u03b8 represents the parameters of learnable model. The goal of variational inference here is to maximize the above likelihood function. Specifically, $\\sum \\ln P(Y_i | X, \\theta)$ is equivalent to its lower bound if and only if Q(Oi) = P(Oi | Yi, Xi, \u03b8). Therefore, we can use Q(O) to approximate the posterior distribution P(Oi | Yi, Xi, \u03b8).\nIn practical applications, directly calculating and optimizing the Evidence Lower Bound (ELBO) can be quite complex. Therefore, we propose a simplified method: further approximating P(Oi | Yi, Xi, \u03b8) with P(Oi | Xi, \u03b8). Since the neural network implicitly learns the mapping from X to Y during the training process, using P(Oi | Xi, \u03b8) as an approximation for P(Oi | Yi, Xi, \u03b8) is reasonable.\nBased on this approximation, we can express the joint probability P(Y, O | X, \u03b8) as follows:\nP(Y,O | X, \\theta) = P(Y | X,\\theta,O)P(O | X,\\theta), (2)\nwhere P(Y | X,\u03b8,O) and P(O | X,\u03b8) constitute the differentiable function T1 : RN\u00d7d \u2192 [R1\u00d7c, where N is the number of the instances in a whole slide image X, d is the embedding dimension. The T1 first generates feature distribution Q(O\u00bf) = {Bij}N j=1 for each pathological image and then aggregates all remapped instance features into vi \u2208 R1\u00d7v using Q(O\u00bf) for final classification. We formulate the learning objective of this part as follows:\n$\\mathcal{L}_1 = \\mathbb{E}_{X_i, Y_i \\sim (X,Y)} H(T_1( \\{f_{ij}\\}_{j=1}^N ), Y_i),$ (3)\nwhere H() is the cross entropy loss function.\nA learnable function T2 : RS\u00d7\u00a7\u00d7d \u2192 RS\u00d7v \u2192 R1\u00d7c is employed to derive classification vectors based on multiple instance sub-bags. According to the task-specific feature distribution P(O | X,\u03b8), the sub-bags are sampled from multiple instance bags and satisfies the understanding of the overall feature distribution of pathological images by T1 at different training stages.\nIn detail, we sort the {Bij}N j=1 and denote the sorted result as {Bij}N j=1, followed by dividing it into {{fij}Ns=s}S s=1 according to a certain step S (equal to the number of the sub-bags), which operation is abstracted as a multi-instance sub-bagging function S. Corresponding sorted instance feature set is denoted as A = S({fij}N j=1) = {{fij}Ns=s}S s=1. Through learnable T2, each sub-bag-level aggregated feature is added to a new set, e.g., {v1, . . ., vS}, and then yielding the slide-level prediction results through average pooling."}, {"title": null, "content": "It would be optimized with supervised learning objective as follows:\n$\\mathcal{L}_2 = \\mathbb{E}_{X_i, Y_i \\sim (X,Y)} H(T_2(A), Y_i).$ (4)\nAn important implementation in this section is generating pseudo bags by selecting sub-bags from different patients, ensuring each pseudo bag Gi = {Gij}S j=1 consists of S sub-bags with Yi = c, where c is one of the C categories. These pseudo bags are cross-patient in nature. We use pseudo bags as unlabeled data and constrain the consistency of their predictions in T1 and T2.\n$\\mathcal{L}_{c1} = \\mathbb{E}_{G_i \\sim G} H(T_1(G_i), T_2(S(G_i))).$ (5)\nAdditionally, we define Lc2 using the original multi-instance bags {fij}N j=1 to ensure the consistency of predictions for a bag in T1 and 72. Finally, we combine these two consistency losses into a single loss Lc = Lc1 + Lc2.\nBy generating additional samples from another class during each training iteration, this method gradually balances the distribution of instances across all classes. Specifically, in a classification task with C classes, for each current sample, an additional sample from another class is generated for training. After one iteration, the total number of samples for class c, N, is calculated as follows:\n$N_c = N_c + \\tilde{N}_c = N_c + \\sum_{d=1}^C \\tilde{N}_c^d,$ (6)\nwhere Nc represents the initial number of samples for class c, \u00d1c represents the additional samples generated for class c through the dynamic sample generation method.\nThis method continuously adjusts and generates new samples during training, leading to a more balanced number of samples across all classes, thereby improving the performance of the classification model on imbalanced datasets."}, {"title": "3.2 Affinity-Based Curriculum Contrastive Learning", "content": "To ensure high-performance feature embedding from bags to pseudo-bags, we propose a curriculum contrastive learning strategy. While curriculum learning can stabilize model training, selecting appropriate positive and negative samples throughout the training process remains a challenge. We address this by using a category affinity index to select positives and semi-hard negatives, ensuring that anchor-negative pairs are farther apart than anchor-positive pairs. This progressively increases the curriculum difficulty and improves model optimization. The algorithm is shown in Algorithm 1.\nFirst, we maintain a dictionary Do where each key is a unique case ID and its value is the feature embedding vi for that case. During each epoch, we update the dictionary values using momentum to ensure consistent bag-level representations (He et al. 2020). Specifically, for each case i, the embedding vi is updated as follows:\n$\\mathcal{D}_{bi} \\leftarrow m \\mathcal{D}_{bi} + (1 - m)v_i,$ (7)\nwhere m \u2208 [0, 1) is the momentum coefficient.\nWe calculate the cosine similarity between each anchor embedding vi and the embeddings in Dt to obtain the category affinity index Ic\u2208 {Y,\u00acY;} \u2208 R|D|, where |D| is the number of samples labeled with class c. Each value in I represents the similarity between the momentum-updated Dbi and the anchor embedding vi. The similarity is measured by cosine similarity as follows:\nsim(vi, Dbi) = $\\frac{{v_i}^T D_{bi}}{||v_i||_2|| D_{bi}||_2}$ (8)\nWe randomly select a value I, \u2208 Ic\u2208{Y;} with the top K in similarity as the threshold and determine an embedding corresponding to I, as v+. Then, we delimit the selection space in Ic\u2208 {Y} according to I\u2081, where the selection space is I-Y \u2229 Y.\nA hyper-parameter k \u2208 [0,1] is introduced to measure the curriculum difficulty. Specifically, the most confusing but semi-hard anchor-negative pair compared to the anchor-positive pair can be determined by k = 0. Accordingly, the anchor-negative pair that ensures the maximum sim(vi, v\u00ec) is selected through k = 1. These two cases are described as follows:\nk = 0 \u2192 argmin (sim(v\u00bd, v+) \u2013 sim(vi, v\u00af)),\nk = 1 \u2192 argmax (sim(v\u00bd, v+) \u2013 sim(vi, v\u00af)) . (9)\nRelaxing curriculum difficulty stabilizes gradients in early training. As training progresses, simple objectives become insufficient, so increasing difficulty helps the model escape local optima. We adjust the difficulty parameter k using k = 1\u2212 ( epochmax epoch)2 to ensure a gradual increase in difficulty.\nWe determine v+ and v\u00ec through Ic\u2208 {Y\u2081\u2192Y;} and ki in each iteration. To speed up the construction of positive and negative pairs for the model, T triplets are constructed in each iteration according to the strategy mentioned above, e.g., {(vi,v+, v\u00af), . . ., (vi, v\u00a6 +, v\u00a6\u00af)}.\nWe minimize the similarity between the same classes and maximize the similarity between different classes according to the curriculum. The objective function is formulated as follows:\n$\\mathcal{L}_s = \\sum_{i=1}^P\\sum_{t=1}^{T} max (0, sim(v_i, v^-) - sim(v_i, v^+) + \\alpha),$ (10)\nwhere P is the size of the WSI collection, sim(vi, v+) represents the similarity between sample vi and the positive sample v+, sim(vi, v\u00af) is analogous."}, {"title": "4.Experiments", "content": null}, {"title": "4.1 Datasets and Implementation Details", "content": "The Cancer Genome Atlas (TCGA) Kidney dataset includes 887 WSIs in SVS format across three subtypes: 500 KIRC, 273 KIRP, and 114 KICH slides. The Liver Cancer dataset, available upon request, contains 670 HCC, 206 CHC, and 92 ICC WSIs. The CAMELYON17 (Bejnordi et al. 2017) comprises 500 publicly available WSIs for detecting breast cancer metastases, with 5 WSIs per patient. It includes 87 Macro, 59 Micro, 36 ITC, and 318 Normal instances. We combined Macro, Micro, and ITC into a Positive class, while Normal remains tumor-negative.\nWe randomly selected 1/5 of the dataset as the test set, using the rest for training and validation. The training-to-validation ratio was 4:1, and all results are reported as the average of five experiments.\nAll methods are based on the same data preprocessing tools, which follows the CLAM (Lu et al. 2021). Our experiments utilized two model structures: the first structure, T1, is identical to the CLAM, while the second structure, T2, adds a pooling module on top of the CLAM model. During training, a weighted average loss function was used, specifically incorporating L1, L2, Ls, and Lc. The experiment setup includes a maximum of 100 epochs, with early stopping enabled and a patience value of 20. The optimizer used is Adam, with a learning rate of 1e-4 and a regularization parameter of 2e-5. The batch size for all experiments is set to 1. Model performance evaluation metrics follow the standards outlined in (Yu et al. 2020), including F1 score, area under the receiver operating characteristic curve (AUC), accuracy (ACC), Matthew's correlation coefficient (MCC), sensitivity (SENS), specificity (SPEC), positive predictive value (PPV), and negative predictive value (NPV). All experiments were conducted on a system running Ubuntu 18.04.6 LTS, equipped with an NVIDIA V100 32G GPU, using CUDA version 11.7, Python version 3.7.7, and PyTorch version 1.12.1+cu113."}, {"title": "4.2 Performance Comparison with Existing Works", "content": "We demonstrated the experimental results of the proposed method on the three datasets, and compared them with SOTA methods including standard DSMIL(Li, Li, and Eliceiri 2021), TransMIL(Shao et al. 2021), DTFD(Zhang et al. 2022), and CLAM(Lu et al. 2021).\nThe TCGA Kidney and Liver Cancer datasets are used for tumor classification tasks, while the Camelyon17 dataset is used for lymph node metastasis classification tasks.\nTable 1 highlights the challenge of class imbalance in pathology image research. Advanced baseline methods show high AUC scores but lower class average F1 scores.\nThe DTFD method, based on standard MIL, randomly splits the multi-instance bag into multiple sub-bags and completes classification by separately selecting and integrating instance features. In tumor classification tasks with class imbalance, the AUC is much higher than the F1 score, indicating that DTFD may not effectively extract key instances to represent the class within the sub-bags, particularly for minority classes.\nMethods based on bag feature integration, such as CLAM, TransMIL, and DSMIL, perform more consistently across the three tasks. Our method achieves the highest scores in metrics that emphasize handling imbalanced scenarios, such as F1 score (default: macro), AUC, and MCC."}, {"title": "4.3 Heatmap and Class Imbalance Visualization", "content": "For the Camelyon 17, we visualized the model using attention scores to obtain the tumor locations. We normalized the attention scores and mapped them to a color space to draw heatmaps for each WSI based on the spatial location of the instances. We selected five WSIs from the Camelyon 17 for demonstration, as shown in Figure 3. The tumor areas are highlighted in red in the second row of the figure, and the heatmaps in the third row show clear high attention, illustrating the model's potential for tumor localization.\nFor TCGA Kidney, a significant class imbalance exists with a ratio of 1:2.4:4.4. This imbalance may lead to suboptimal embedding of features by the model, making it difficult to adapt to new data. We trained twice for each method and projected their feature embedding into a two-dimensional space."}, {"title": "4.4 Multi-Class Imbalance", "content": "This study thoroughly investigated the challenge of multi-category imbalance in pathology image classification, focusing on TCGA kidney with original sample ratios of 1:2.4:4.4 across three categories. To systematically evaluate the performance of our proposed method under various imbalance conditions, we generated several imbalanced datasets with the following ratios: 1:1:1, 1:1:2, 1:1:41:2:2, and 1:2:3. These ratios encompass a wide range of scenarios, from perfectly balanced to highly imbalanced. Entropy is a measure of uncertainty or disorder, and for a set of categories with proportions p1,p2,..., pn, it is defined as: \u2212 \u03a3ni=1 pi log(pi), where pi is the proportion of samples in category i. This formula helps capture the degree of imbalance, with higher entropy indicating a more balanced distribution and lower entropy indicating greater imbalance, as detailed in Table 2."}, {"title": "4.5 Ablation Study", "content": "We identified three innovative points: C1, the consistency of predictions for a bag in T\u2081 and 72.; C2, the use of curriculum contrastive learning; and C3, the generation of pseudo-bags using cross-patient sub-bags. For the ablation study, we removed each innovative point (C1, C2, and C3) separately and conducted experiments to assess their impact on the model's performance.\nThe Table 3 showed that removing any of the innovative points led to a decrease in model performance. Notably, simultaneous removal of C1 and C2 (M4) or removal of all features (M5) resulted in a significant performance decline.\nAdditionally, Table 4 explores the effect of varing the number of sub-bags on model performance. Results indicate that increasing the number of sub-bags initially enhances F1 Score (macro), AUC, and ACC, stabilizing after reaching optimal levels. This suggests that while increasing sub-bags can improve performance, the marginal benefit diminishes beyond a certain threshold.\nIn Table 4, we compare several different curriculum learning difficulty scheduling functions. Our method uses the smooth function as the default setting, and we compare it with three other functions: exponential (E), random (R), and linear (L). The linear function is defined as h = 1\u2212 epochmax epoch, the exponential function is defined as h = exp (\u2212 epochmax epoch), and the random function is defined as h = random(0, 1). The experimental results show that using the smooth function achieves the best performance in terms of convergence speed and final classification accuracy."}, {"title": "5.Conclusion", "content": "In this paper, we address the issue of class imbalance in pathological image classification tasks and leverage the rich and sometimes redundant information in pathological images to enhance the model's representational capacity. Specifically, our approach explores three main aspects. First, we propose obtaining sub-bags with similar feature distributions from the original multi-instance bags, and these sub-bags from different patients can be used to construct pseudo-bags, thereby making more efficient use of the information in pathological images. Finally, we investigate sample selection based on affinity to achieve curriculum contrastive learning. These explorations have been thoroughly validated on three datasets, demonstrating that our approach can effectively improve the imbalanced multiclassification of whole slide images."}]}