{"title": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "authors": ["Yang Liu", "Weixing Chen", "Yongjie Bai", "Jingzhou Luo", "Xinshuai Song", "Kaixuan Jiang", "Zhida Li", "Ganlong Zhao", "Junyi Lin", "Guanbin Li", "Wen Gao", "Liang Lin"], "abstract": "Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation.", "sections": [{"title": "I. INTRODUCTION", "content": "EMBODIED AI was initially proposed from the Embod-ied Turing Test by Alan Turing in 1950 [1], which is designed to determine whether agents can display intelligence that is not just limited to solving abstract problems in a virtual environment (cyber space\u00b9), but that is also capable of navigating the complexity and unpredictability of the physical world. The agents in the cyber space are generally referred to as disembodied AI, while those in the physical space are embodied AI. Recent advances in Multi-modal Large Models (MLMs) have injected strong perception, interaction and planning capabilities to embodied models, to develop general-purpose embodied agents and robots that actively interact with virtual and physical environments [2]. Therefore, the embodied agents are widely considered as the best carriers for MLMs. The recent representative embodied models are RT-2 [3] and RT-H [4]. Nevertheless, the capabilities of long-term memory, understanding complex intentions, and the decomposition of complex tasks are limited for current MLMs. To achieve Artificial General Intelligence (AGI), the de-velopment of embodied AI stands as a fundamental avenue. Different from conversational agents like ChatGPT [5], em-bodied AI believes that the true AGI can be achieved by controlling physical embodiments and interacting with both simulated and physical environments [6]\u2013[9]. As we stand at the forefront of AGI-driven innovation, it is crucial to delve deeper into the realm of embodied AI, unraveling their complexities, evaluating their current developmental stage, and contemplating the potential trajectories they may follow in the future. Nowadays, embodied Al contains various key techniques across Computer Vision (CV), Natural Language Processing (NLP), and robotics, with the most representative being embodied perception, embodied interaction, embodied agents, and sim-to-real robotic control. Therefore, it is imper-ative to capture the evolving landscape of embodied AI in the pursuit of AGI through a comprehensive survey. Embodied agent is the most prominent basis of embodied AI. For an embodied task, the embodied agent must fully un-"}, {"title": "II. EMBODIED ROBOTS", "content": "Embodied AI actively interacts with the physical environ-ment and encompasses a broad spectrum of embodiments, including robots, smart appliances, smart glasses, autonomous vehicles, etc. Among them, robots stand out as one of the most prominent embodiments. Depending on the application, robots are designed in various forms to leverage their hardware characteristics for specific tasks, as shown in Fig. 4.\nA. Fixed-base Robots\nFixed-base robots, as shown in Fig. 4 (a), are extensively employed in laboratory automation, educational training, and industrial manufacturing due to their compactness and high-precision operations. These robots feature robust bases and structures that ensure stability and high accuracy during opera-tion. Equipped with high-precision sensors and actuators, they achieve micron-level precision, making them suitable for tasks that require high accuracy and repeatability [22]. Moreover, fixed-base robots are highly programmable, allowing users to adapt them for various task scenarios, such as Franka (Franka Emika panda) [23], Kuka iiwa (KUKA) [24], and Sawyer (Rethink Robotics) [25]. Despite their excellent performance in many fields, fixed-base robots have certain disadvantages. Firstly, their fixed-base design limits their operational range and flexibility, preventing them from moving or adjusting positions over large areas and leading to their collaboration with humans and other robots. Secondly, fixed-base robots are generally expensive and require specialized personnel for installation and maintenance, which increases their initial investment and operational costs [22], [26].\nB. Wheeled Robots and Tracked Robots\nFor mobile robots, they can face more complex and diverse application scenarios. Wheeled robots, as shown in Fig. 4 (b), known for their efficient mobility, are widely employed in lo-gistics, warehousing, and security inspections. The advantages of wheeled robots include their simple structure, relatively low cost, high energy efficiency, and rapid movement capabilities on flat surfaces [22]. These robots are typically equipped with high-precision sensors such as LiDAR and cameras, enabling autonomous navigation and environmental perception, making them highly effective in automated warehouse management and inspection tasks, e.g., Kiva robots (Kiva Systems) [27] and Jackal robot (Clearpath Robotics) [28]. However, wheeled robots have limited mobility in complex terrains and harsh en-vironments, particularly on uneven ground. Additionally, their load capacity and maneuverability are somewhat restricted. In comparison, tracked robots, with their powerful off-road capabilities and high maneuverability, show significant poten-tial in fields such as agriculture, construction, and disaster recovery, as shown in Fig. 4 (c). The track system provides a larger ground contact area, distributing the robot's weight and reducing the risk of sinking in soft terrain such as mud and sand. Moreover, tracked robots are typically equipped with robust power and suspension systems, allowing them to maintain stability and traction on complex terrains [29]. Consequently, reliable tracked robots are also used in sensitive areas such as the military. The iRobot PackBot is a versatile military-tracked robot capable of performing tasks such as"}, {"title": "C. Quadruped Robots", "content": "Quadruped robots, known for their stability and adaptabil-ity, are well-suited for complex terrain exploration, rescue missions, and military applications. Inspired by quadrupedal animals, these robots can maintain balance and mobility on uneven surfaces, as shown in Fig. 4 (d). The multi-jointed design allows them to mimic biological movements, achieving complex gaits and posture adjustments. High adjustability enables the robots to automatically adapt their stance to chang-ing terrain, enhancing maneuverability and stability. Sensing systems, such as LiDAR and cameras, provide environmental awareness, allowing the robots to navigate autonomously and avoid obstacles [31]. Researchers commonly use several types of quadruped robots as research platforms: Unitree Robotics, Boston Dynamics Spot, and ANYmal C. Unitree Robotics' Unitree A1 and Gol are noted for their cost-effectiveness and flexibility. The A1 [32] and Gol [33] possess strong mobility and intelligent obstacle avoidance capabilities, suitable for various applications. Boston Dynamics' Spot is renowned for its superior stability and operational flexibility, which are com-monly used in industrial inspections and rescue missions. It features powerful load-carrying capacity and adaptability, ca-pable of performing complex tasks in harsh environments [34]. ANYbotics' ANYmal C, with its modular design and high durability, is widely employed in industrial inspection and maintenance. The ANYmal C is equipped with autonomous navigation and remote operation capabilities, suitable for pro-longed outdoor tasks and even extreme lunar missions [35]. Like fixed-base robots, quadruped robots face similar draw-backs, such as high costs. The complex design and high manufacturing costs of quadruped robots result in substantial initial investments, limiting their use in cost-sensitive areas. Additionally, quadruped robots have limited battery endurance in complex environments, requiring frequent recharging or battery replacement for prolonged operation [36]."}, {"title": "D. Humanoid Robots", "content": "Following the discussion on fixed-base and quadruped robots, humanoid robots are distinguished by their human-like form and are increasingly prevalent in sectors such as the service industry, healthcare, and collaborative environments. These robots can mimic human movements and behavioral patterns, providing personalized services and support. Their dexterous hand designs enable them to perform intricate and complex tasks, distinguishing them from other types of robots, as shown in Fig. 4 (e). These hands typically have multi-ple degrees of freedom and high-precision sensors, allowing them to emulate the grasping and manipulation capabilities of human hands, which is particularly crucial in fields such as medical surgery and precision manufacturing [37], [38]. Among current humanoid robots, Atlas (Boston Dynamics) is renowned for its exceptional mobility and stability. Atlas can perform complex dynamic actions such as running, jumping, and rolling, demonstrating the potential of humanoid robots in highly dynamic environments [39]. The HRP series (AIST) is utilized in various research and industrial applications, with a design focus on high stability and flexibility, making it effective in complex environments, particularly for collabo-rative tasks with humans [40]. ASIMO (Honda), one of the most famous humanoid robots, can walk, run, climb stairs, and recognize faces and gestures, making it suitable for reception and guide services [41]. Additionally, a small social robot, Pepper (Softbank Robotics) can recognize emotions and engage in natural language communication and is widely used in customer service and educational settings [42]. Despite their excellent performance in many fields, hu-manoid robots face significant challenges in maintaining oper-ational stability and reliability in complex environments due to their sophisticated control systems. These challenges include robust bipedal walking control algorithms and dexterous hand grasping algorithms [37]. Furthermore, traditional humanoid robots based on hydraulic systems, with their bulky structures and high maintenance costs, are gradually being replaced by motor-driven systems. Recently, Tesla and Unitree Robotics have introduced their humanoid robots based on motor sys-tems. With the integration of LLMs, humanoid robots are expected to handle various complex tasks more intelligently, filling labor gaps in manufacturing, healthcare, and the service industry, thereby improving efficiency and safety [43]."}, {"title": "E. Biomimetic Robots", "content": "Unlike the previously mentioned robots, biomimetic robots perform tasks in complex and dynamic environments by simulating the efficient movements and functions of natu-ral organisms. By emulating biological entities' forms and movement mechanisms, these robots demonstrate significant potential in fields such as healthcare, environmental moni-toring, and biological research [22]. Typically, they utilize flexible materials and structures to achieve lifelike, agile movements. These materials not only enhance the robots' adaptability and flexibility but also minimize environmental impact. Furthermore, biomimetic robots are often equipped with advanced sensors and control systems, enabling real-time environmental sensing and rapid response, thereby enhancing their autonomous navigation and task execution capabilities. Importantly, biomimetic designs can significantly improve the robots' energy efficiency by mimicking the efficient move-ment mechanisms of biological organisms, making them more economical regarding energy consumption [54], [55]. These biomimetic robots include fish-like robots [56], [57], insect-like robots [58], [59], and soft-bodied robots [60], as shown in Fig. 4 (f). Despite their impressive performance, biomimetic robots face several challenges. First, their design and man-ufacturing processes are often complex and costly, limiting large-scale production and widespread application. Second, due to their use of flexible materials and complex movement mechanisms, the durability and reliability of biomimetic robots need improvement in extreme environments."}, {"title": "III. EMBODIED SIMULATORS", "content": "Data scarcity has been a persistent challenge in embodied AI research. Nonetheless, collecting real world robot data poses numerous challenges. First, real world robot training proceeds slowly due to its real-time nature, which cannot be paral-lelized. The associated costs are prohibitively high, demanding dedicated deployment sites, expert operational control for data collection, and substantial hardware expenses. Moreover, the most significant challenge lies in reproducibility, stemming from vast differences in robot hardware configurations, con-trol methods, and implementation frameworks, impeding data transferability. In such circumstances, simulators offer a novel solution for collecting and training data for embodied AI. Embodied simulators are vital for embodied AI as they offer cost-effective experimentation, ensuring safety by simulating potentially hazardous scenarios, scalability for testing in di-verse environments, rapid prototyping capabilities, accessibil-ity to a wider research community, controlled environments for precise studies, data generation for training and evaluation, and standardized benchmarks for algorithm comparison. To enable agents to interact with the environment, it is necessary to construct a realistic simulated environment. This requires consideration of the physical characteristics of the environ-ment, the properties of objects, and their interactions. This section will introduce the commonly used simulation platforms in two parts: the general simulator based on under-lying simulation and the simulator based on real scenes.\nA. General Simulator\nThe physical interactions and dynamic changes present in real environments are irreplaceable. However, deploying embodied models in the physical world often incurs high costs and faces numerous challenges. The ultimate goal of embodied AI is to transfer findings from virtual environments to real-world applications. Researchers can select simulators that best suit their needs to aid their studies. General-purpose simulators provide a virtual environment that closely mimics the physical world, allowing for algorithm development and model training, which offers significant cost, time, and safety advantages. Isaac Sim [44] developed by NVIDIA, is an advanced simulation platform tailored for robotics and AI research. The primary features of Isaac Sim include high-fidelity phys-ical simulation, real-time ray tracing, an extensive library of robotic models, and deep learning support. Pixar's USD"}, {"title": "B. Real-Scene Based Simulators", "content": "Achieving universal embodied agents in household activities has been a primary focus in the field of embodied AI research. These embodied agents need to deeply understand human daily life and perform complex embodied tasks such as navigation and interaction in indoor environments. To meet the demands of these complex tasks, the simulated environments need to be as close to real world as possible, which places high demands on the complexity and realism of the simulators. This led to the creation of simulators based on real world environments. These simulators mostly collect data from the real world, create photorealistic 3D assets, and build scenes using 3D game engines like UE5 and Unity. The rich and realistic scenes make simulators based on real world environments the top choice for research on embodied Al in household activities. AI2-THOR [62] is an indoor embodied scene simulator based on Unity3D, launched in 2017 and led by the Allen Institute for Artificial Intelligence. As a high-fidelity simulator built in the real world, the biggest feature of AI2-THOR is its richly interactive scene objects and the physical properties assigned to them (such as open/close or even cold/hot). AI2-THOR consists of two parts: iTHOR and RoboTHOR. iTHOR contains 120 rooms categorized as kitchens, bedrooms, bath-rooms, and living rooms, with over 2000 unique interactive objects, and supports multi-agent simulation; RoboTHOR con-tains 89 modular apartments with 600+ objects, the uniqueness of which is that these apartments correspond to real scenes in the real world. This means that researchers can remotely deploy their models in the real environment. So far, more than a hundred works have been published based on AI2-THOR. Matterport 3D [63] is proposed in R2R [64] in 2018, is more commonly used as a large-scale 2D-3D visual dataset rather than an embodied simulator. The Matterport3D dataset includes 90 architectural indoor scenes, comprises 10,800 panoramas and 194,400 RGB-D images, and provides surface reconstruction, camera posture, and 2D and 3D semantic segmentation annotations. In embodied AI, Matterport3D is mainly used for visual language navigation. Matterport3D transforms 3D scenes into discrete 'viewpoints', and embodied agents move between adjacent 'viewpoints' in Matterport3D scenes. At each 'viewpoint', embodied agents can obtain a 1280x1024 panorama image (18\u00d7 RGB-D) centered on the 'viewpoint'. The advantage of Matterport3D lies in its large, diverse, and detailed annotated 2D-3D data, as well as the ease of use and flexibility brought about by simulator-deploy-free. In the field of embodied navigation, Matterport3D is already"}, {"title": "IV. EMBODIED PERCEPTION", "content": "The \"north stars\" of the future of visual perception is embodied-centric visual reasoning and social intelligence [75]. Unlike merely recognizing objects in images [76]\u2013[78], agent with embodied perception must move in the physical world and interact with the environment. This requires a deeper understanding of 3D space and dynamic environments. Em-bodied perception requires visual perception and reasoning, understanding the 3D relations within a scene, and predicting and performing complex tasks based on visual information.\nA. Active Visual Perception\nActive visual perception systems require fundamental ca-pabilities such as state estimation, scene perception, and environment exploration. As shown in Fig. 7, these capabilities have been extensively studied within the domains of Visual Si-multaneous Localization and Mapping (vSLAM) [119], [120], 3D Scene Understanding [121], and Active Exploration [13]. These research areas contribute to developing robust active visual perception systems, facilitating improved environmental interaction and navigation in complex, dynamic settings. We briefly introduce these three components and summarize the methods mentioned in each part.\n1) Visual Simultaneous Localisation and Mapping: Simul-taneous Localization and Mapping (SLAM) is a technique that determines a mobile robot's position in an unknown environment while concurrently constructing a map of that environment [122], [123]. Range-based SLAM [124]\u2013[126] creates point cloud representations using rangefinders (e.g., laser scanners, radar, and/or sonar), but is costly and provides limited environmental information. Visual SLAM (vSLAM) [119], [120] uses on-board cameras to capture frames and construct a representation of the environment. It has gained popularity due to its low hardware cost, high accuracy in small-scale scenarios, and ability to capture rich environmental information. Classical vSLAM techniques can be divided into Traditional vSLAM and Semantic vSLAM [120]. Traditional vSLAM systems estimate the robot's pose in an unknown environment using image information and multi-view geometry principles to construct a low-level map (e.g., sparse maps, semi-dense maps, and dense maps) composed of point clouds, such as filter-based methods (e.g., MonoSLAM [79], MSCKF [80]), keyframe-based methods (e.g., PTA\u041c [81], ORB-SLAM [82]), and direct tracking methods (e.g., DTAM [83], LSD-SLAM [84]). Since the point clouds in these low-level maps do not correspond to objects in the envi-ronment, they are difficult for embodied robots to understand and use directly. With the rise of semantic concepts, semantic vSLAM systems combined with semantic information solu-tions have significantly enhanced robots' ability to perceive the unexplored environment. Early works, such as SLAM++ [85], use real-time 3D object recognition and tracking to create efficient object graphs, enabling robust loop closure, relocalization, and object detection in cluttered environments. CubeSLAM [86] and HDP-SLAM [87] introduce 3-D rectangular into the map to"}, {"title": "B. 3D Visual Grounding", "content": "Unlike traditional 2D visual grounding (VG), which oper-ates within the confines of flat images, 3D VG incorporates depth, perspective, and spatial relationships between objects, providing a more robust framework for agents to interact with their environment. The task of 3D VG involves locating objects within a 3D environment using natural language descriptions [130], [131]. As summarized in Table V, recent methodologies in 3D visual grounding can be roughly divided into two categories: two-stage and one-stage methods [145]. 1) Two-stage 3D Visual Grounding methods: Similar to corresponding 2D tasks [146], early research in 3D grounding predominantly utilized a two-stage detect-then-match pipeline. They initially employ pretrained detector [147] or segmentor [148]\u2013[150] to extract features from numerous object propos-als within a 3D scene, which are then fused with linguistic query features to match the target object. The focus of the two-stage research is mainly on the second stage, such as exploring the correlation between object proposal features and linguistic query features to select the best-matched object. ReferIt3D [131] and TGNN [132] not only learn to match the proposal features with textual embedding but also encode the contextual relationship among the objects via graph neural networks. To enhance 3D visual grounding in free-form descriptions and irregular point cloud, FFL-3DOG [134] utilizes a language-scene graph for phrase correlations, a multi-level 3D proposal relation graph for enriching visual features, and a description-guided 3D visual graph for encoding global contexts. Recently, as the transformer architecture has demonstrated outstanding performance in natural language processing [151], [152] and computer vision tasks [15], [153], research has increasingly focused on using transformers for extracting and fusing visual language features in 3D visual grounding tasks. For example, LanguageRefer [136] employs a transformer-based architecture combining 3D spatial embeddings, language descriptions, and class label embeddings to achieve robust 3D visual grounding. 3DVG-Transformer [135] is a relation-aware visual grounding method for 3D point clouds, featuring a coordinate-guided contextual aggregation module for relation-enhanced proposal generation and a multiplex attention mod-ule for cross-modal proposal disambiguation. To enable more fine-grained reasoning of 3D objects and referring expressions, TransRefer3D [154] enhances cross-modal feature represen-tation using entity-and-relation aware attention, incorporating"}, {"title": "C. Visual Language Navigation", "content": "Visual Language Navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navi-gate in unseen environments following linguistic instructions. VLN requires robots to understand complex and diverse visual observations and meanwhile interpret instructions at different granularities. The input for VLN typically consists of two parts: visual information and natural language instructions. The visual information can either be a video of past trajectories or a set of historical-current observation images. The natural language instructions include the target that the embodied agent needs to reach or the task that the embodied agent is expected to complete. The embodied agent must use the above information to select one or a series of actions from a list of candidates to fulfill the requirements of the natural language instructions. This process could be represented as:\n$$Action = M(O, H, I)$$\nwhere  \\(Action\\) is the chosen action or a list of action can-didates, O is the current observation, H is the historical information, and I is the natural language instruction.\nSR (Success Rate), TL (Trajectory Length), and SPL (Success Weighted by Path Length) are the most commonly used metrics in VLN. Among them, SR directly reflects the navigation performance of the embodied agent, TL reflects the navigation efficiency, and SPL combines both to indicate the overall performance of the embodied agent. Below, we will introduce visual-linguistic navigation di-vided into two parts: datasets and methods."}, {"title": "D. Non-Visual Perception: Tactile", "content": "The skin facilitates human tactile perception. The skin changes shape when touched, and its abundant nerve cells send electrical signals [188]. This tactile perception allows humans to grasp handy work fully. Therefore, touch is vital for robots to interact with the real world. A sense of touch enables robots to acquire information such as material, shape, temperature, and even objects' contact force and gravity. Current work on tactile perception focuses on three areas: sensor design, dataset construction, and application. Tactile perception undoubtedly enhances the human-computer inter-action experience and holds great promise [189]\u2013[191]. 1) Sensor Design: Tactile sensor design methods can be divided into three categories: non-vision-based, vision-based, and multi-modal. At early time, tactile sensors were chiefly engineered to register fundamental, low-dimensional sensory outputs such as force, pressure, vibration, and temperature [192]-[197]. Their principles are mostly related to electricity and physical mechanics and their data is often low-dimension series with temporal correlations. One of the notable rep-resentatives is BioTac [198] and its simulator [199]."}, {"title": "V. EMBODIED INTERACTION", "content": "Embodied interaction tasks refer to scenarios where agents interact with humans and environment in a physical or sim-ulated space. The typical embodied interaction tasks are Em-bodied Question Answering (EQA) and embodied grasping.\nA. Embodied Question Answering\nFor Embodied Question Answering (EQA) task, the agent needs to explore the environment from a first-person perspec-tive to gather information necessary to answer the given ques-tions. An agent with autonomous exploration and decision-making capabilities must not only consider which actions to take to explore the environment but also determine when to stop exploring to answer questions. Existing works focus on different types of questions, some of which are shown in Fig. 11. In this section, we first introduce the existing datasets and then discuss related methods. 1) Datasets: Conducting robot experiments in real environ-ments is often constrained by scenarios and robot hardware. As virtual experimental platforms, simulators offer suitable environmental conditions for constructing embodied question answering datasets. Training and testing models on datasets created in simulators significantly reduce experimental costs"}, {"title": "B. Embodied Grasping", "content": "Embodied interaction, in addition to engaging in question-answering interactions with humans, can also involve per-forming operations based on human instructions, such as grasping and placing objects, thereby completing interactions among the robots, humans and objects. Embodied grasping requires comprehensive semantic understanding, scene per-ception, decision-making, and robust control planning. The embodied grasping methods integrate traditional robotic kine-matic grasping with large models such as large language models (LLMs) [261] and vision-language foundation models [15], which enables agents to perform grasping tasks under multi-sensory perceptions, including visual active perception, language understanding and reasoning. Figure 12 (b) illustrates an overview of human-agent-object interaction, where the agent accomplishes embodied grasping tasks.\n1) Gripper: The current research focus in grasping technol-ogy is on two-finger parallel grippers and five-finger dexterous hands. For two-finger parallel grippers, grasping postures are generally categorized into two types: 4-DOF and 6-DOF [262]. The 4-DOF grasp synthesis [263]\u2013[265] defines the grasp using a three-dimensional position and a top-down hand orien-tation (yaw), commonly referred to as \"top-down grasping\". In contrast, 6-DOF grasp synthesis [266]\u2013[268] defines the grasp posture through a six-dimensional position and orientation. For five-finger dexterous hand grippers, the ShadowHand, a widely used five-finger robotic dexterous hand, features 26 degrees of freedom (DOF). This high dimensionality significantly increases the complexity of generating effective grasp postures and planning execution trajectories. 2) Datasets: Recently, a substantial number of grasping datasets [264], [265], [268]\u2013[270] have been generated. These datasets typically contain annotated grasping data based on images (RGB, depth), point clouds, or 3D scenes. With the advent of multimodal large models and the application of foundational language models to robotic grasping, there is an urgent need for datasets that include linguistic text. Consequently, existing datasets have been extended or re-constructed to create semantic-grasping datasets [271]\u2013[274]. These datasets are instrumental in studying grasping models grounded in language, enabling agents to develop a broad understanding of semantics. Traditional grasping datasets encompass data for both single objects [265] and cluttered scenes [263], providing stable grasp annotations (4-DOF or 6-DOF) that conform to kine-matics for each object. These data can be collected from real desktop environments [265], typically including RGB, depth, and point cloud data, or from virtual environments [268],"}, {"title": "VI. EMBODIED AGENT", "content": "An agent is defined as an autonomous entity capable of per-ceiving its environment and acting to achieve specific objec-tives. Initially, Symbolic Agents, rooted in symbolic reasoning, and Reactive Agents, known for their rapid responsiveness, were widely utilized. However, these agents were limited in handling complex strategies under uncertainty. Learning-based agents were subsequently developed to mitigate this limitation, yet they remained inadequate for large-scale real-world problems. Recent advancements in MLMs have further expanded the application of agents to practical scenarios. When these MLM-based agents are embodied in physical entities, they can effectively transfer their problem-solving capabilities from virtual space to the physical world, thereby becoming Embodied Agents [279]. To enable embodied agents to operate in the information-rich and complex real world, the Embodied Multimodal Foun-dation Model has been developed to provide these agents with multimodal perception and reasoning capabilities. To complete a task, embodied agents typically involves the fol-lowing process: 1) decomposing the abstract and complex task into specific subtasks, which is referred to as high-level Embodied Task Planning. 2) gradually implementing these subtasks by effectively utilizing Embodied Perception and Embodied Interaction models or leveraging the Foundation Model's policy function, named low-level Embodied Action Planning. It is worth noting that task planning involves think-ing before acting, and is therefore typically considered in cyber space. In contrast, action planning must account for effective interaction with the environment and feedback on this information to the task planner to adjust task planning. Thus, it is crucial for embodied agents to align and generalize their abilities from the cyber space to the physical world.\nA. Embodied Multimodal Foundation Model\nEmbodied agents are required to recognize their environ-ment visually, understand instructions audibly, and compre-hend their own state to enable complex interactions and oper-ations. This demands a model that integrates multiple sensory modalities and natural language processing capabilities to enhance the agent's understanding and decision-making by synthesizing diverse data types. Thus, the Embodied Mul-timodal Foundation Model is emerging. Google DeepMind initiated research in the Robotics Foundation Model field eight years ago, continually exploring ways to scale models and data more effectively. Their findings revealed that leveraging foundation models and large, diverse datasets is the optimal strategy. They developed a series of works based on the Robotic Transformer (RT) [12], offering substantial insights for future research on embodied agents."}, {"title": "VII. SIM-TO-REAL ADAPTATION", "content": "Sim-to-Real adaptation in Embodied AI refers to the process of transferring capabilities or behaviors learned in simulated environments (cyber space) to real-world scenarios (physical world). It involves validating and improving the effectiveness of algorithms, models, and control strategies developed in sim-ulation to ensure they perform robustly and reliably in physical environments. To achieve sim-to-real adaptation, embodied world models, data collection and training methods, and embodied control algorithms are three essential components.\nA. Embodied World Model\nSim-to-Real involves creating world models in simulation that closely resemble real-world environments, helping algo-rithms generalize better when transferred. The world model approach aims to build an end-to-end model that maps vision to action, or even anything to anything, by predicting the next state in a generative or predictive manner to make decisions. The biggest difference between such world models and VLA models is that VLA models are first trained on large-scale"}, {"title": "B. Data Collection and Training", "content": "For sim-to-real adaptation, the high-quality data is impor-tant. Traditional data collection methods involve expensive equipment, precise operations, and are time-consuming and labor-intensive, often lacking flexibility. Recently, some effi-cient and cost-effective methods have been proposed for high-quality demonstration data collection and training. This section will discuss various methods for data collection in both real-world and simulated environments. Fig. 15 presents demon-stration data from both real-world and simulated environments.\n1) Real-World Data: Training large, high-capacity models on high-volume, rich datasets has demonstrated remarkable capabilities and significant success in effectively addressing downstream applications. For instance, large language models such as ChatGPT, GPT-4, and LLaMA have not only excelled in the field of NLP but have also provided excellent problem-solving capabilities for downstream tasks. Therefore, is it possible to train an embodied large model in the robotics field,"}, {"title": "C. Embodied Control", "content": "Embodied control aims to enable robots to acquire new skills through interaction and learning from their environment, thereby adapting to and completing complex tasks. Embodied control learns through interaction with the environment and optimizes behavior using a reward mechanism to obtain the optimal policy, thereby avoiding the drawbacks of traditional physical modeling methods. Embodied control methods can be divided into two types:\n1) Deep Reinforcement Learning (DRL). DRL can han-dle high-dimensional data and learn complex behavior pat-terns, making it suitable for decision-making and control. The hybrid and dynamic policy gradient (HDPG) [363] is proposed for biped locomotion, allowing the control policy to be simultaneously optimized by multiple criteria dynamically. DeepGait [364] is a neural network policies for terrain-aware locomotion, which combines methods for model-based motion planning and reinforcement learning. It includes a terrain-aware planner for generating gait sequences and base motions guiding the robot towards target directions, along with a gait and base motion controller for executing these sequences while maintaining balance. Both the planner and controller are"}, {"title": "VIII. CHALLENGES AND FUTURE DIRECTIONS", "content": "Despite of the rapid progress of embodied AI, it faces several challenges and presents exciting future directions. High-quality Robotic Datasets: Obtaining sufficient real world robotic data remains a significant challenge. Collecting this data is both time-consuming and resource-intensive. Rely-ing solely on simulation data worsens the sim-to-real gap prob-lem. Creating diverse real world robotic datasets necessitates close and extensive collaboration among various institutions. Additionally, the development of more realistic and efficient simulators is essential for improving the quality of simulated data. Current work RT-1 [12] used pre-trained models based on robot images and natural language commands. RT-1 has achieved good results in navigation and grasping tasks, but acquiring real world robot datasets is very challenging. For building generalizable embodied models capable of cross-scenario and cross-task applications in robotics, it is essen-tial to construct large-scale datasets, leveraging high-quality simulated environment data to assist real world data. Efficient Utilization of Human Demonstration Data: Efficient utilization of human demonstration data involves leveraging the actions and behaviors demonstrated by humans to train and improve robotic systems. This process includes collecting, processing, and learning from large-scale, high-quality datasets where humans perform tasks that robots are intended to learn. Current work R3M [379] used action labels and human demonstration data to learn generalizable representations has shown high success rates in some robot grasping tasks, but the efficiency for complex tasks still needs improvement. Therefore, it is important to effectively utilize large amounts of unstructured, multi-label, and multi-modal human demonstration data combined with action label data to train embodied models that can learn various tasks in relatively short periods. By efficiently utilizing human demonstration data, robotic systems can achieve higher levels of performance and adaptability, making them more capable of performing complex tasks in dynamic environments."}, {"title": "Cognition of Complex Environment", "content": "Cognition of com-plex environment refers to the ability of embodied agents in physical or virtual environments"}]}