{"title": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI", "authors": ["Yang Liu", "Weixing Chen", "Yongjie Bai", "Jingzhou Luo", "Xinshuai Song", "Kaixuan Jiang", "Zhida Li", "Ganlong Zhao", "Junyi Lin", "Guanbin Li", "Wen Gao", "Liang Lin"], "abstract": "Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.", "sections": [{"title": "I. INTRODUCTION", "content": "EMBODIED AI was initially proposed from the Embod- ied Turing Test by Alan Turing in 1950 [1], which is designed to determine whether agents can display intelligence that is not just limited to solving abstract problems in a virtual environment (cyber space\u00b9), but that is also capable of navigating the complexity and unpredictability of the physical world. The agents in the cyber space are generally referred to as disembodied AI, while those in the physical space are embodied AI (Table I). Recent advances in Multi-modal Large Models (MLMs) have injected strong perception, interaction and planning capabilities to embodied models, to develop general-purpose embodied agents and robots that actively interact with virtual and physical environments [2]. Therefore, the embodied agents are widely considered as the best carriers for MLMs. The recent representative embodied models are RT-2 [3] and RT-H [4]. Nevertheless, the capabilities of long-term memory, understanding complex intentions, and the decomposition of complex tasks are limited for current MLMs. To achieve Artificial General Intelligence (AGI), the de- velopment of embodied AI stands as a fundamental avenue. Different from conversational agents like ChatGPT [5], em- bodied AI believes that the true AGI can be achieved by controlling physical embodiments and interacting with both simulated and physical environments [6]\u2013[9]. As we stand at the forefront of AGI-driven innovation, it is crucial to delve deeper into the realm of embodied AI, unraveling their complexities, evaluating their current developmental stage, and contemplating the potential trajectories they may follow in the future. Nowadays, embodied Al contains various key techniques across Computer Vision (CV), Natural Language Processing (NLP), and robotics, with the most representative being embodied perception, embodied interaction, embodied agents, and sim-to-real robotic control. Therefore, it is imper- ative to capture the evolving landscape of embodied AI in the pursuit of AGI through a comprehensive survey. Embodied agent is the most prominent basis of embodied AI. For an embodied task, the embodied agent must fully un-"}, {"title": "II. EMBODIED ROBOTS", "content": "Embodied AI actively interacts with the physical environ- ment and encompasses a broad spectrum of embodiments, including robots, smart appliances, smart glasses, autonomous vehicles, etc. Among them, robots stand out as one of the most prominent embodiments. Depending on the application, robots are designed in various forms to leverage their hardware characteristics for specific tasks, as shown in Fig. 4.\nA. Fixed-base Robots\nFixed-base robots, as shown in Fig. 4 (a), are extensively employed in laboratory automation, educational training, and industrial manufacturing due to their compactness and high- precision operations. These robots feature robust bases and structures that ensure stability and high accuracy during opera- tion. Equipped with high-precision sensors and actuators, they achieve micron-level precision, making them suitable for tasks that require high accuracy and repeatability [22]. Moreover, fixed-base robots are highly programmable, allowing users to adapt them for various task scenarios, such as Franka (Franka Emika panda) [23], Kuka iiwa (KUKA) [24], and Sawyer (Rethink Robotics) [25]. Despite their excellent performance in many fields, fixed-base robots have certain disadvantages. Firstly, their fixed-base design limits their operational range and flexibility, preventing them from moving or adjusting positions over large areas and leading to their collaboration with humans and other robots. Secondly, fixed-base robots are generally expensive and require specialized personnel for installation and maintenance, which increases their initial investment and operational costs [22], [26].\nB. Wheeled Robots and Tracked Robots\nFor mobile robots, they can face more complex and diverse application scenarios. Wheeled robots, as shown in Fig. 4 (b), known for their efficient mobility, are widely employed in lo- gistics, warehousing, and security inspections. The advantages of wheeled robots include their simple structure, relatively low cost, high energy efficiency, and rapid movement capabilities on flat surfaces [22]. These robots are typically equipped with high-precision sensors such as LiDAR and cameras, enabling autonomous navigation and environmental perception, making them highly effective in automated warehouse management and inspection tasks, e.g., Kiva robots (Kiva Systems) [27] and Jackal robot (Clearpath Robotics) [28]. However, wheeled robots have limited mobility in complex terrains and harsh en- vironments, particularly on uneven ground. Additionally, their load capacity and maneuverability are somewhat restricted. In comparison, tracked robots, with their powerful off-road capabilities and high maneuverability, show significant poten- tial in fields such as agriculture, construction, and disaster recovery, as shown in Fig. 4 (c). The track system provides a larger ground contact area, distributing the robot's weight and reducing the risk of sinking in soft terrain such as mud and sand. Moreover, tracked robots are typically equipped with robust power and suspension systems, allowing them to maintain stability and traction on complex terrains [29]. Consequently, reliable tracked robots are also used in sensitive areas such as the military. The iRobot PackBot is a versatile military-tracked robot capable of performing tasks such as"}, {"title": "III. EMBODIED SIMULATORS", "content": "Data scarcity has been a persistent challenge in embodied AI research. Nonetheless, collecting real world robot data poses numerous challenges. First, real world robot training proceeds slowly due to its real-time nature, which cannot be paral- lelized. The associated costs are prohibitively high, demanding dedicated deployment sites, expert operational control for data collection, and substantial hardware expenses. Moreover, the most significant challenge lies in reproducibility, stemming from vast differences in robot hardware configurations, con- trol methods, and implementation frameworks, impeding data transferability. In such circumstances, simulators offer a novel solution for collecting and training data for embodied AI. Embodied simulators are vital for embodied AI as they offer cost-effective experimentation, ensuring safety by simulating potentially hazardous scenarios, scalability for testing in di- verse environments, rapid prototyping capabilities, accessibil- ity to a wider research community, controlled environments for precise studies, data generation for training and evaluation, and standardized benchmarks for algorithm comparison. To enable agents to interact with the environment, it is necessary to construct a realistic simulated environment. This requires consideration of the physical characteristics of the environ- ment, the properties of objects, and their interactions. This section will introduce the commonly used simulation platforms in two parts: the general simulator based on under- lying simulation and the simulator based on real scenes.\nA. General Simulator\nThe physical interactions and dynamic changes present in real environments are irreplaceable. However, deploying embodied models in the physical world often incurs high costs and faces numerous challenges. The ultimate goal of embodied AI is to transfer findings from virtual environments to real- world applications. Researchers can select simulators that best suit their needs to aid their studies. General-purpose simulators provide a virtual environment that closely mimics the physical world, allowing for algorithm development and model training, which offers significant cost, time, and safety advantages. Isaac Sim [44] developed by NVIDIA, is an advanced simulation platform tailored for robotics and AI research. The primary features of Isaac Sim include high-fidelity phys- ical simulation, real-time ray tracing, an extensive library of robotic models, and deep learning support. Pixar's USD"}, {"title": "IV. EMBODIED PERCEPTION", "content": "The \"north stars\" of the future of visual perception is embodied-centric visual reasoning and social intelligence [75]. Unlike merely recognizing objects in images [76]\u2013[78], agent with embodied perception must move in the physical world and interact with the environment. This requires a deeper understanding of 3D space and dynamic environments. Em- bodied perception requires visual perception and reasoning, understanding the 3D relations within a scene, and predicting and performing complex tasks based on visual information.\nA. Active Visual Perception\nActive visual perception systems require fundamental ca- pabilities such as state estimation, scene perception, and environment exploration. As shown in Fig. 7, these capabilities have been extensively studied within the domains of Visual Si- multaneous Localization and Mapping (vSLAM) [119], [120], 3D Scene Understanding [121], and Active Exploration [13]. These research areas contribute to developing robust active visual perception systems, facilitating improved environmental interaction and navigation in complex, dynamic settings. We briefly introduce these three components and summarize the methods mentioned in each part in Table IV.\n1) Visual Simultaneous Localisation and Mapping: Simul- taneous Localization and Mapping (SLAM) is a technique that determines a mobile robot's position in an unknown environment while concurrently constructing a map of that environment [122], [123]. Range-based SLAM [124]\u2013[126] creates point cloud representations using rangefinders (e.g., laser scanners, radar, and/or sonar), but is costly and provides limited environmental information. Visual SLAM (vSLAM) [119], [120] uses on-board cameras to capture frames and construct a representation of the environment. It has gained popularity due to its low hardware cost, high accuracy in small-scale scenarios, and ability to capture rich environmental information. Classical vSLAM techniques can be divided into Traditional vSLAM and Semantic vSLAM [120]. Traditional vSLAM systems estimate the robot's pose in an unknown environment using image information and multi- view geometry principles to construct a low-level map (e.g., sparse maps, semi-dense maps, and dense maps) composed of point clouds, such as filter-based methods (e.g., MonoSLAM [79], MSCKF [80]), keyframe-based methods (e.g., \u041f\u0422\u0410\u041c [81], ORB-SLAM [82]), and direct tracking methods (e.g., DTAM [83], LSD-SLAM [84]). Since the point clouds in these low-level maps do not correspond to objects in the envi- ronment, they are difficult for embodied robots to understand and use directly. With the rise of semantic concepts, semantic vSLAM systems combined with semantic information solu- tions have significantly enhanced robots' ability to perceive the unexplored environment.\nEarly works, such as SLAM++ [85], use real-time 3D object recognition and tracking to create efficient object graphs, enabling robust loop closure, relocalization, and object detection in cluttered environments. CubeSLAM [86] and HDP-SLAM [87] introduce 3-D rectangular into the map to construct a lightweight semantic map. QuadricSLAM [88] employs semantic 3D ellipsoids to achieve precise modeling of object shapes and poses in complex geometrical environments. So-SLAM [89] incorporates fully coupled spatial structure constraints (coplanarity, collinearity, and proximity) in indoor environments. To meet the challenges of dynamic environ- ments, DS-SLAM [90], DynaSLAM [91] and SG-SLAM [92] employ semantic segmentation for motion consistency checks and multiview geometry algorithms to identify and filter dynamic objects, ensuring stable localization and mapping. OVD-SLAM [93] leverages semantic, depth, and optical flow information to distinguish dynamic regions without predefined labels, achieving more accurate and robust localization. GS- SLAM [94] utilizes 3D Gaussian representation that balances efficiency and accuracy through a real-time differentiable splatting rendering pipeline and adaptive expansion strategy.\n2) 3D Scene Understanding: 3D scene understanding aims to distinguish objects' semantics, identify their locations, and infer the geometric attributes from 3D scene data, which is fundamental in autonomous driving [127], robot navigation [128], and human-computer interaction [129] etc. A scene may be recorded as 3D point clouds using 3D scanning tools like LiDAR or RGB-D sensors. Unlike images, point clouds are sparse, disordered, and irregular, [121] makes scene interpretation extremely challenging.\nIn recent years, numerous deep learning methods for 3D scene understanding have been proposed, which can be divided into projection-based, voxel-based, and point-based meth- ods. Concretely, projection-based methods (e.g., MV3D [95], PointPillars [96], MVCNN [97] ) project 3D points onto various image planes and employ 2D CNN-based backbones for feature extraction. Voxel-based methods convert point clouds into regular voxel grids to facilitate 3D convolution operations (e.g., VoxNet [98], SSCNet [99]), and some works improve their efficiency through sparse convolution (e.g., MinkowskiNet [100], SSCNs [101], Embodiedscan [102]). In contrast, point-based methods process point clouds directly (e.g., PointNet [103], PointNet++ [104], PointMLP [105]). Recently, to achieve model scalability, Transformers-based (e.g., PointTransformer [106], Swin3d [107], PT2 [108], PT3 [109]) and Mamba-based (e.g., PointMamba [110], PCM [111], Mamba3D [112]) architectures have emerged.\n3) Active Exploration: The previously introduced 3D scene understanding methods endow robots with the ability to per- ceive the environment in a passive manner. In such cases, the perception system's information acquisition and decision- making do not adapt to the evolving scene. However, passive perception serves as a crucial foundation for active exploration. Given that robots are capable of movement and frequent"}, {"title": "V. EMBODIED INTERACTION", "content": "Embodied interaction tasks refer to scenarios where agents interact with humans and environment in a physical or sim- ulated space. The typical embodied interaction tasks are Em- bodied Question Answering (EQA) and embodied grasping.\nA. Embodied Question Answering\nFor Embodied Question Answering (EQA) task, the agent needs to explore the environment from a first-person perspec- tive to gather information necessary to answer the given ques- tions. An agent with autonomous exploration and decision- making capabilities must not only consider which actions to take to explore the environment but also determine when to stop exploring to answer questions. Existing works focus on different types of questions, some of which are shown in Fig. 11. In this section, we first introduce the existing datasets and then discuss related methods.\n1) Datasets: Conducting robot experiments in real environ- ments is often constrained by scenarios and robot hardware. As virtual experimental platforms, simulators offer suitable environmental conditions for constructing embodied question answering datasets. Training and testing models on datasets created in simulators significantly reduce experimental costs"}, {"title": "VI. EMBODIED AGENT", "content": "An agent is defined as an autonomous entity capable of per- ceiving its environment and acting to achieve specific objec- tives. Initially, Symbolic Agents, rooted in symbolic reasoning, and Reactive Agents, known for their rapid responsiveness, were widely utilized. However, these agents were limited in handling complex strategies under uncertainty. Learning- based agents were subsequently developed to mitigate this limitation, yet they remained inadequate for large-scale real- world problems. Recent advancements in MLMs have further expanded the application of agents to practical scenarios. When these MLM-based agents are embodied in physical entities, they can effectively transfer their problem-solving capabilities from virtual space to the physical world, thereby becoming Embodied Agents [279].\nTo enable embodied agents to operate in the information- rich and complex real world, the Embodied Multimodal Foun- dation Model has been developed to provide these agents with multimodal perception and reasoning capabilities. To complete a task, embodied agents typically involves the fol- lowing process: 1) decomposing the abstract and complex task into specific subtasks, which is referred to as high-level Embodied Task Planning. 2) gradually implementing these subtasks by effectively utilizing Embodied Perception and Embodied Interaction models or leveraging the Foundation Model's policy function, named low-level Embodied Action Planning. It is worth noting that task planning involves think- ing before acting, and is therefore typically considered in cyber space. In contrast, action planning must account for effective interaction with the environment and feedback on this information to the task planner to adjust task planning. Thus, it is crucial for embodied agents to align and generalize their abilities from the cyber space to the physical world.\nA. Embodied Multimodal Foundation Model\nEmbodied agents are required to recognize their environ- ment visually, understand instructions audibly, and compre- hend their own state to enable complex interactions and oper- ations. This demands a model that integrates multiple sensory modalities and natural language processing capabilities to enhance the agent's understanding and decision-making by synthesizing diverse data types. Thus, the Embodied Mul- timodal Foundation Model is emerging. Google DeepMind initiated research in the Robotics Foundation Model field eight years ago, continually exploring ways to scale models and data more effectively. Their findings revealed that leveraging foundation models and large, diverse datasets is the optimal strategy. They developed a series of works based on the Robotic Transformer (RT) [12], offering substantial insights for future research on embodied agents."}, {"title": "VII. SIM-TO-REAL ADAPTATION", "content": "Sim-to-Real adaptation in Embodied AI refers to the process of transferring capabilities or behaviors learned in simulated environments (cyber space) to real-world scenarios (physical world). It involves validating and improving the effectiveness of algorithms, models, and control strategies developed in sim- ulation to ensure they perform robustly and reliably in physical environments. To achieve sim-to-real adaptation, embodied world models, data collection and training methods, and embodied control algorithms are three essential components.\nA. Embodied World Model\nSim-to-Real involves creating world models in simulation that closely resemble real-world environments, helping algo- rithms generalize better when transferred. The world model approach aims to build an end-to-end model that maps vision to action, or even anything to anything, by predicting the next state in a generative or predictive manner to make decisions. The biggest difference between such world models and VLA models is that VLA models are first trained on large-scale internet datasets to achieve high-level emergent capabilities and then co-finetuned with real world robot data. In contrast, world models are trained from scratch on physical world data, gradually developing high-level capabilities as the amount of data increases. However, they remain low-level physical world models, somewhat akin to the mechanism of human neural reflex systems. This makes them more suitable for scenarios where both inputs and outputs are relatively structured, such as autonomous driving (input: vision, output: throttle, brake, steering wheel) or object sorting (input: vision, instructions, numerical sensors, output: grasping the target object and placing it in the target location). They are less suited for generalization to unstructured, complex embodied tasks. Learning world models is promising of the physical simu- lation field. Compared to traditional simulation methods, it offers significant advantages, such as the ability to reason about interactions with incomplete information, meet real-time computation requirements, and improve prediction accuracy over time. The predictive capability of such world models is crucial, enabling robots to develop the physical intuition necessary to operate in the human world. As shown in Fig. 14, according to the learning pipeline of the world environ- ment, they can be divided into Generation-based Methods, Prediction-based Methods and Knowledge-driven Methods.\n1) Generation-based Methods: As the scale of models and data progressively increases, generative models have demon- strated the ability to understand and generate images (e.g., World Models [332]), videos (e.g., Sora [18], Pandora [333]), point clouds (e.g., 3D-VLA [334]) or other formats of data (e.g., DWM [335]) that conform to physical laws. This ability indicates that generative models can learn and internalize world knowledge. Specifically, after being exposed to vast amounts of data, generative models can not only capture the statistical properties of the data but also simulate the physical and causal relations of the real world through their intrinsic structures and mechanisms. Therefore, these generative models can be considered more than simple pattern recognition tools: they exhibit characteristics of world models. Consequently, the world knowledge embedded in generative models can be lever- aged to enhance the performance of other models. By mining and utilizing the world knowledge represented in generative models, we can improve model generalization and robustness. This approach not only enhances the model's adaptability to new environments but also increases its predictive accuracy on unknown data [333], [334]. However, generative models also have certain limitations and drawbacks. For instance, when data distribution is significantly biased or training data is insufficient, generative models may produce inaccurate or distorted outputs. Additionally, the training process for these models typically requires substantial computational resources and time, and the models often lack interpretability, which complicates their practical application. Overall, while gen- erative models have shown great potential in understanding and generating content that conforms to physical laws, several technical and practical challenges must be addressed for their effective application. These challenges include improving model efficiency, enhancing interpretability, and addressing"}, {"title": "VIII. CHALLENGES AND FUTURE DIRECTIONS", "content": "Despite of the rapid progress of embodied AI, it faces several challenges and presents exciting future directions. High-quality Robotic Datasets: Obtaining sufficient real world robotic data remains a significant challenge. Collecting this data is both time-consuming and resource-intensive. Rely- ing solely on simulation data worsens the sim-to-real gap prob- lem. Creating diverse real world robotic datasets necessitates close and extensive collaboration among various institutions. Additionally, the development of more realistic and efficient simulators is essential for improving the quality of simulated data. Current work RT-1 [12] used pre-trained models based on robot images and natural language commands. RT-1 has achieved good results in navigation and grasping tasks, but acquiring real world robot datasets is very challenging. For building generalizable embodied models capable of cross- scenario and cross-task applications in robotics, it is essen- tial to construct large-scale datasets, leveraging high-quality simulated environment data to assist real world data. Efficient Utilization of Human Demonstration Data: Efficient utilization of human demonstration data involves leveraging the actions and behaviors demonstrated by humans to train and improve robotic systems. This process includes collecting, processing, and learning from large-scale, high-quality datasets where humans perform tasks that robots are intended to learn. Current work R3M [379] used action labels and human demonstration data to learn generalizable representations has shown high success rates in some robot grasping tasks, but the efficiency for complex tasks still needs improvement. Therefore, it is important to effectively utilize large amounts of unstructured, multi-label, and multi-modal human demonstration data combined with action label data to train embodied models that can learn various tasks in relatively short periods. By efficiently utilizing human demonstration data, robotic systems can achieve higher levels of performance and adaptability, making them more capable of performing complex tasks in dynamic environments. Cognition of Complex Environment: Cognition of com- plex environment refers to the ability of embodied agents in physical or virtual environments, to perceive, understand, and navigate complex real world environments. Based on extensive commonsense knowledge, the Say-Can [380] utilized pre-trained LLM models' task decomposition mechanism, which relies heavily on large amounts of commonsense knowledge for simple task planning but lacking understanding of long- term tasks in complex environments. For unstructured open environments, current works usually rely on pre-trained LLMs' task decomposition mechanism using extensive common-sense knowledge for simple task planning, while lacking specific scene understanding. It is vital to enhance the ability of knowl- edge transfer and generalization in complex environments. A truly versatile robotics system should be capable of compre- hending and executing natural language instructions across diverse and unseen scenes. This necessitates the development of adaptable and scalable embodied agent architectures. Long-Horizon Task Execution: Executing single instruc- tions can often entail long-horizon tasks for robots, exem- plified by commands like \u201cclean the kitchen,\u201d which involve activities such as rearranging objects, sweeping floors, wiping tables, and more. Accomplishing such tasks successfully ne- cessitates the robot's ability to plan and execute a sequence of low-level actions over extended time spans. While current high-level task planners have shown initial success, they often prove inadequate in diverse scenarios due to their lack of tuning for embodied tasks. Addressing this challenge requires the development of efficient planners equipped with robust perception capabilities and much commonsense knowledge. Unified Embodied Foundation Model: Exploring founda- tion models for embodied robot tasks remains a nascent area of research, primarily due to the wide array of embodiments, environments, and tasks inherent in robotics. Compounding this challenge are isolated datasets and evaluation setups. Es- tablishing a robust and unified foundation model for embodied robotics demands leveraging large-scale internet datasets and cutting-edge LLMs, MLMs and WMs. Causal Relation Discovery: Existing data-driven embodied agents make decisions based on the intrinsic correlations within the data. However, this modeling approach does not allow the models to truly understand the causal relations between knowledge, behavior, and environment, resulting in biased strategies. This makes it difficult to ensure that they can operate in real-world environments in an interpretable, robust, and reliable manner. Therefore, it is important for embodied agents to construct embodied perception, reasoning, and interaction framework driven by world knowledge, capable of autonomous causal reasoning. By understanding the world through interaction and learning its workings via abductive reasoning, we can further enhance the adaptability, decision reliability, and generalization capabilities of multimodal em- bodied agents in complex real-world environments. For embodied tasks (such as embodied question answering, visual language navigation, and instruction following), it is necessary to introduce embodied interactive causal represen- tation learning. This involves establishing spatial-temporal causal relations across modalities through interactive instruc- tions and state predictions, forming a representation learning system based on interaction and deduction. Moreover, agents need to understand the affordances of objects to achieve adap- tive task planning and long-distance autonomous navigation in dynamic scenes. To optimize decision-making, it is necessary to combine counterfactual and causal intervention strategies [381]\u2013[384], trace causality from counterfactual and causal intervention perspectives, reduce exploration iterations, and optimize decisions. Constructing a causal graph based on world knowledge and driving sim-to-real transfer of agents through active causal reasoning will form a unified framework for embodied perception, reasoning, and interaction. Continual Learning: In robotics applications, continual learning [385] is crucial for deploying robot learning policies in diverse environments, yet it remains a largely unexplored domain. While some recent studies have examined sub-topics of continual learning\u2014such as incremental learning, rapid motor adaptation, and human-in-the-loop learning\u2014these so- lutions are often designed for a single task or platform and do not yet consider foundational models. Open research problems and viable approaches include: 1) mixing different proportions of prior data distribution when fine-tuning on the latest data to alleviate catastrophic forgetting [386], 2) developing efficient prototypes from prior distributions or curricula for task infer- ence in learning new tasks, 3) improving training stability and sample efficiency of online learning algorithms, 4) identifying principled ways to seamlessly incorporate large-capacity mod- els into control frameworks, potentially through hierarchical learning or slow-fast control, for real-time inference. Unified Evaluation Benchmark: While numerous bench- marks exist for evaluating low-level control policies, they often vary significantly in the skills they assess. Furthermore, the objects and scenes included in these benchmarks are typically limited by simulator constraints. To comprehensively evaluate embodied models, there is a need for benchmarks that encompass a diverse range of skills using realistic simulators. Regarding high-level task planners, many benchmarks focus on assessing planning capability through question-answering tasks. However, a more desirable approach involves evaluating both the high-level task planner and the low-level control policy together for executing long-horizon tasks and measuring success rates, rather than relying solely on isolated assessments of the planner. This integrated approach offers a more holistic assessment of the capabilities of embodied AI systems."}, {"title": "IX. CONCLUSION", "content": "Embodied AI allows agents to sense, perceive, and interact with various objects from both cyber space and physical world, which exhibits its vital significance toward achiev- ing AGI. This survey extensively reviews embodied robots, simulators, four representative embodied tasks: visual active perception, embodied interaction, embodied agents and sim- to-real robotic control, and future research directions. The comparative summary of the embodied robots, simulators, datasets, and approaches provides a clear picture of the recent development in embodied AI, which greatly benefits the future research along this emerging and promising research direction."}]}