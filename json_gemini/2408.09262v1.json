{"title": "PREMAP: A Unifying PREiMage APproximation Framework for Neural Networks", "authors": ["Xiyue Zhang", "Benjie Wang", "Marta Kwiatkowska", "Huan Zhang"], "abstract": "Most methods for neural network verification focus on bounding the image, i.e., set of outputs for a given input set. This can be used to, for example, check the robustness of neural network predictions to bounded perturbations of an input. However, verifying properties concerning the preimage, i.e., the set of inputs satisfying an output property, requires abstractions in the input space. We present a general framework for preimage abstraction that produces under- and over-approximations of any polyhedral output set. Our framework employs cheap parameterised linear relaxations of the neural network, together with an anytime refinement procedure that iteratively partitions the input region by splitting on input features and neurons. The effectiveness of our approach relies on carefully designed heuristics and optimization objectives to achieve rapid improvements in the approximation volume. We evaluate our method on a range of tasks, demonstrating significant improvement in efficiency and scalability to high-input-dimensional image classification tasks compared to state-of-the-art techniques. Further, we showcase the application to quantitative verification and robustness analysis, presenting a sound and complete algorithm for the former and providing sound quantitative results for the latter.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable empirical success of neural networks, ensuring their safety against potentially adversarial behaviour, especially when using them as decision-making components in autonomous systems (Bojarski et al., 2016; Codevilla et al., 2018; Yun et al., 2017), is an important and challenging task. Towards this aim, various approaches have been developed for the verification of neural networks, with extensive effort devoted, in particular, to the problem of local robustness verification, which focuses on deciding the presence or absence of adversarial examples (Szegedy et al., 2013; Biggio et al., 2013) within an \\(e\\)-perturbation neighbourhood (Huang et al., 2017; Katz et al., 2017; Zhang et al., 2018;"}, {"title": "2 Preliminaries", "content": "We use \\(f : R^d \\rightarrow R^m\\) to denote a feed-forward neural network. For layer \\(i\\), we use \\(W^{(i)}\\) to denote the weight matrix, \\(b^{(i)}\\) the bias, \\(z^{(i)}\\) the pre-activation neurons, and \\(x^{(i)}\\) the post-activation neurons, such that we have \\(z^{(i)} = W^{(i)}x^{(i-1)}+b^{(i)}\\). We use \\(h^{(i)}(x)\\) to denote the function from input to pre-activation neurons, and \\(a^{(i)}(x)\\) the function from input to the post-activation neurons, i.e., \\(z^{(i)} = h^{(i)}(x)\\) and \\(x^{(i)} = a^{(i)}(x)\\). In this paper, we focus on ReLU neural networks with \\(a^{(i)}(x) = ReLU(h^{(i)}(x))\\), where \\(ReLU(h) := max(h, 0)\\) is applied element-wise. However, our method can be generalized to other activation functions that can be bounded by linear functions, similarly to Zhang et al. (2018).\nLinear Relaxation of Neural Networks. Nonlinear activation functions lead to the NP-completeness of the neural network verification problem as proved in Katz et al. (2017). To address such intractability, linear relaxation is often used to transform the nonconvex constraints into linear programs. As shown in Figure 2, given concrete lower and upper bounds \\(l^{(i)} < h^{(i)}(x) \\le u^{(i)}\\) on the pre-activation values of layer \\(i\\), there are three cases to"}, {"title": "3 Problem Formulation", "content": "In this work, we are interested in the problem of computing preimages for neural networks. Given a subset \\(O \\subset R^m\\) of the codomain, the preimage of a function \\(f : R^d \\rightarrow R^m\\) is defined to be the set of all inputs \\(x \\in R^d\\) that are mapped to an element of \\(O\\) by \\(f\\). For neural networks in particular, the input is typically restricted to some bounded input region \\(C \\subset R^d\\). In this work, we restrict the output set \\(O\\) to be a polyhedron, and the input set \\(C\\) to be an axis-aligned hyperrectangle region \\(C \\subset R^d\\), as these are commonly used in neural network verification. We now define the notion of a restricted preimage.\n\\textbf{Definition 1} (Restricted Preimage) Given a neural network \\(f : R^d \\rightarrow R^m\\), and an input set \\(C \\subset R^d\\), the restricted preimage of an output set \\(O \\subset R^m\\) is defined to be the set \\(f_C^{-1}(O) := {x \\in R^d|f(x) \\in O \\land x \\in C}\\).\n\\textbf{Example 1} To illustrate our problem formulation and approach, we introduce a vehicle parking task from Ayala et al. (2011) as a running example. In this task, there are four parking lots, located in each quadrant of a \\(2 \\times 2\\) grid \\([0,2]^2\\), and a neural network with two hidden layers of 10 ReLU neurons \\(f : R^2 \\rightarrow R^4\\) is trained to classify which parking lot an input point belongs to. To analyze the behaviour of the neural network in the input region \\([0,2] \\times [0,2]\\), we set \\(C = {x \\in R^2|(0 \\le x_1 \\le 2) \\land (0 \\le x_2 \\le 2)}\\). Then the restricted preimage \\(f^{-1}(O)\\) of the set \\(O = {y \\in R^4|\\land_{i\\in{2,3,4}} y_1 - y_i \\ge 0}\\) is the subspace of the region \\([0,2] \\times [0,2]\\) that is labelled as parking lot 1 by the neural network.\nWe focus on provable approximations of the preimage. Given a first-order formula \\(A\\), \\(a\\) is an under-approximation (resp. over-approximation) of \\(A\\) if it holds that \\(\\forall x.a(x) \\Rightarrow A(x)\\) (resp. \\(\\forall x.A(x) \\Rightarrow a(x)\\)). In our context, the restricted preimage is defined by the formula \\(A(x) = (f(x) \\in O) \\land (x \\in C)\\), and we restrict to approximations \\(a\\) that take the form of a disjoint union of polytopes (DUP). The goal of our method is to generate a DUP approximation \\(T\\) that is as tight as possible; that is, we aim to maximize the volume \\(vol(T)\\) of an under-approximation, or minimize the volume \\(vol(T)\\) of an over-approximation.\n\\textbf{Definition 2} (Disjoint Union of Polytopes) A disjoint union of polytopes (DUP) is a FOL formula \\(a\\) of the form \\(a(x) := \\bigvee_{i=1}^k a_i(x)\\), where each \\(a_i\\) is a polytope formula (conjunction of a finite set of linear half-space constraints), with the property that \\(a_i \\land a_j\\) is unsatisfiable for any \\(i \\neq j\\)."}, {"title": "3.2 Quantitative Properties", "content": "One of the most important verification problems for neural networks is that of proving guarantees on the output of a network for a given input set (Gehr et al., 2018; Gopinath et al., 2020; Ruan et al., 2018). This is often expressed as a property of the form \\((I,O)\\) such that \\(\\forall x \\in I \\Rightarrow f(x) \\in O\\). We can generalize this to quantitative properties:\n\\textbf{Definition 3} (Quantitative Property) Given a neural network \\(f : R^d \\rightarrow R^m\\), a measurable input set with non-zero measure (volume) \\(I \\subseteq R^d\\), a measurable output set \\(O \\subseteq R^m\\), and a rational proportion \\(p \\in [0,1]\\), we say that the neural network satisfies the property \\((I,O,p)\\) if \\(\\frac{vol(f(I) \\cap O)}{vol(I)} > p\\).\nNeural network verification algorithms can be characterized by two main properties: soundness, which states that the algorithm always returns correct results, and completeness, which states that the algorithm always reaches a conclusion on any verification query (Liu et al., 2021). We now define the soundness and completeness of verification algorithms for quantitative properties.\n\\textbf{Definition 4} (Soundness) A verification algorithm \\(QV\\) is sound if, whenever \\(QV\\) outputs True, the property \\((I,O,p)\\) holds.\n\\textbf{Definition 5} (Completeness) A verification algorithm \\(QV\\) is complete if (i) \\(QV\\) never returns Unknown, and (ii) whenever \\(QV\\) outputs False, the property \\((I,O,p)\\) does not hold.\nIf the property \\((I, O)\\) holds, then the quantitative property \\((I, O, 1)\\) holds, while quantitative properties for \\(0 < p < 1\\) provide more information when \\((I,O)\\) does not hold. Most neural network verification methods produce approximations of the image of \\(I\\) in the output space, which cannot be used to verify quantitative properties. Preimage over-approximations include points outside of the true preimage; thus, they cannot be applied for sound quantitative verification. In contrast, preimage under-approximations provide a lower bound on the volume of the preimage, allowing us to soundly verify quantitative properties."}, {"title": "4 Methodology", "content": "In this section, we present the main components of our methodology. Figure 3 shows the workflow of our preimage approximation method (using under-approximation as an illustration).\nIn Section 4.2, we introduce how to cheaply and soundly under-approximate (or over-approximate) the (restricted) preimage with a single polytope by means of the linear relaxation methods (Algorithm 2), which offer greater scalability than the exact method (Matoba"}, {"title": "4.2 Polytope Approximation via Linear Relaxation", "content": "We first show how to adapt linear relaxation techniques to efficiently generate valid under-approximations and over-approximations to the restricted preimage \\(C\\) as a single polytope. Recall that LiRPA methods enable us to obtain linear lower and upper bounds on the output of a neural network \\(f\\), that is, \\(Ax+b < f(x) < Ax+b\\), where the linear coefficients depend on the input region \\(C\\).\nSuppose that we are given the input hyperrectangle \\(C = {x \\in R^d|x = \\land_{i=1}^d [x_i, \\overline{x}_i]}\\), and the output polytope specified using the half-space constraints \\(\\forall_i(y) = (c_i^T y + d_i \\ge 0)\\) for \\(i = 1, ..., K\\) over the output space. Let us first consider generating a guaranteed under-approximation. Given a constraint \\(\\forall_i\\), we append an additional linear layer at the end of the network \\(f\\), which maps \\(y \\mapsto c_i^T y + d_i\\), such that the function \\(g_i : R^d \\rightarrow R\\) represented by the new network is \\(g_i(x) = c_i^T f(x) + d_i\\). Then, applying LiRPA lower bounding to each"}, {"title": "4.3 Global Branching and Refinement", "content": "As LiRPA performs crude linear relaxation, the resulting bounds can be quite loose, even with optimisation over bounding parameters (as we will see in Section 4.4), meaning that"}, {"title": "su", "content": "We propose a subregion selection strategy that prioritises splitting subregions with the largest difference in volume between the exact preimage \\(f_{cal}^{-1}(O)\\) and the (already computed) polytope approximation \\(T_{caub}(O)\\) on that subdomain: this indicates \"how much improvement\" can be achieved on this subdomain and is implemented as the \\(CalcPriority\\) function in Algorithm 1. Unfortunately, computing the volume of a polytope exactly is a computationally expensive task, requiring specialised tools (Chevallier et al., 2022). To overcome this, we employ Monte Carlo estimation of volume computation by sampling \\(N\\) points \\(X_1,..., X_N\\) uniformly from the input subdomain \\(C_{sub}\\). For an under-approximation, we have:\nPriority\\((C_{aub}) := \\frac{vol(C_{aub})}{N} \\times (\\sum_{i=1}^N \\mathbb{1}_{x_i \\in f_c^{-1}(O)} - \\sum_{i=1}^N \\mathbb{1}_{x_i \\in T_{Csub}(O)})\\  \\approx vol(f_c^{-1}(O)) \u2013 vol(T_{Csub}(O))\\measure the gap between the polytope under-approximation and the optimal approximation, namely, the preimage itself."}, {"title": "4.4 Local Optimization", "content": "One of the key components behind the effectiveness of LiRPA-based bounds is the ability to efficiently improve the tightness of the bounding function by optimising the relaxation parameters \\(\\alpha\\) via projected gradient descent. In the context of local robustness verification, the goal is to optimise the concrete (scalar) lower or upper bounds over the (sub)region \\(C_{sub}\\) (Xu et al., 2020), i.e., \\(\\min_{x\\in C_{sub}} A(\\alpha)x + b(\\alpha)\\) in the case of lower bounds, where we explicitly note the dependence of the linear coefficients on \\(\\alpha\\). In our case, we are instead interested in optimising \\(\\alpha\\) to refine the polytope approximation, that is, increase the volume of under-approximations and decrease the volume of over-approximations (to the exact preimage).\nAs before, we employ statistical estimation; we sample \\(N\\) points \\(x_1,..., X_N\\) uniformly from the input domain \\(C_{sub}\\) then employ Monte Carlo estimation for the volume of the"}, {"title": "4.5 Optimisation of Lagrangian Relaxation", "content": "Previously, in Section 4.3, we proposed a preimage refinement method that adds intermediate ReLU splitting planes to tighten the bounds of a selected individual neuron. However, intermediate bounds for other neurons are not updated based on the newly added splitting constraint. In the following, we first discuss the impact of stabilising an intermediate ReLU neuron from two different perspectives. We then present an optimisation approach leveraging Lagrangian relaxation to enforce the splitting constraint on refining the preimage.\n\\textbf{Effect of Stabilisation of Intermediate Neurons.} Our previous approach of Zhang et al. (2024) exploits one level of bound tightening after ReLU splitting: the substitution of relaxation functions with exact linear functions for the individual neuron. Specifically, assume an intermediate (unstable) neuron \\(z_j^{(i)} (= h^{(i)}(x))\\) is selected to split the input (i)\\) = {x \\in C | z_j^{(i)} \\ge 0}\\) and \\(C_{(sub)region \\(C\\) into two subregions \\(C_{(sub}^{(i)} = {x \\in C | z_j^{(i)} <0}\\). For each subregion, the linear bounding functions of the nonlinear activation function"}, {"title": "4.6 Overall Algorithm", "content": "Our overall preimage approximation method is summarised in Algorithm 1. It takes as input a neural network \\(f\\), input region \\(C\\), output region \\(O\\), target polytope volume threshold \\(v\\) (a proxy for approximation precision), maximum number of iterations \\(R\\), number of samples \\(N\\) for statistical estimation, and Boolean variables indicating (i) whether to return an under-approximation or over-approximation and (ii) whether to use input or ReLU splitting, and returns a disjoint polytope union \\(Dom\\) representing a guaranteed under-approximation (or over-approximation) to the preimage.\nThe algorithm initiates and maintains a priority queue of (sub)regions according to Equation 7. The initialisation step (Lines 1-2) generates an initial polytope approximation of the whole region using Algorithm 2 (Sections 4.2, 4.4, 4.5), with priority calculated (\\textit{CalcPriority}\\) according to Equations 4, 6. Then, the preimage refinement loop (Lines 3-11) partitions a subregion in each iteration, with the preimage restricted to the child subregions then being re-approximated (Line 9-10). In each iteration, we choose the region to split (Line 4) and the splitting plane to cut on (Line 6 for input split and Line 8 for ReLU split), as detailed in Section 4.3. The preimage subregion queue is then updated by computing the priorities for each subregion by approximating their volume (Line 11). The loop terminates and the approximation is returned when the target volume threshold \\(v\\) or maximum iteration limit \\(R\\) is reached."}, {"title": "4.7 Quantitative Verification", "content": "We now show how to use our efficient preimage under-approximation method (Algorithm 1) to verify a given quantitative property \\((I,O,p)\\), where \\(O\\) is a polyhedron, \\(I\\) a polytope and \\(p\\) the desired proportion threshold, summarised in Algorithm 3. Note that preimage over-approximation cannot be applied for sound quantitative verification as the approximation may contain false regions outside the true preimage. To simplify, assume that \\(I\\) is a hyperrectangle, so that we can take \\(C = I\\). We discuss the case of general polytopes at the end of this section. We utilise Algorithm 1 by setting the volume threshold \\(v\\) to"}, {"title": "5 Experiments", "content": "We have implemented our approach as a tool for preimage approximation for polyhedral output sets/specifications. In this section, we report on experimental evaluation of the proposed approach, and demonstrate its effectiveness in approximation generation and the application to quantitative analysis of neural networks."}, {"title": "5.1 Benchmark and Evaluation Metric", "content": "We evaluate our preimage analysis approach on a benchmark of reinforcement learning and image classification tasks. Besides the vehicle parking task of Ayala et al. (2011) shown in the running example, we consider the following tasks: (1) aircraft collision avoidance system"}, {"title": "Su", "content": "We propose a subregion selection strategy that prioritises splitting subregions with the largest difference in volume between the exact preimage \\(f_{cal}^{-1}(O)\\) and the (already computed) polytope approximation \\(T_{caub}(O)\\) on that subdomain: this indicates \"how much improvement\" can be achieved on this subdomain and is implemented as the \\(CalcPriority\\) function in Algorithm 1. Unfortunately, computing the volume of a polytope exactly is a computationally expensive task, requiring specialised tools (Chevallier et al., 2022). To overcome this, we employ Monte Carlo estimation of volume computation by sampling \\(N\\) points \\(X_1,..., X_N\\) uniformly from the input subdomain \\(C_{sub}\\). For an under-approximation, we have:\n\\begin{equation}\nPriority(C_{aub}) = \\frac{vol(C_{aub})}{N} \\times (\\sum_{i=1}^N \\mathbb{1}_{x_i \\in f_c^{-1}(O)} - \\sum_{i=1}^N \\mathbb{1}_{x_i \\in T_{Csub}(O)}) \n\\approx vol(f_c^{-1}(O)) \u2013 vol(T_{Csub}(O))\n(4)\n\\end{equation}\n\nThis measures the gap between the polytope under-approximation and the optimal approximation, namely, the preimage itself.\n\\begin{equation}\nPriority(C_{sub}) = \\frac{vol(C_{sub})}{N} \\times (\\sum_{i=1}^N \\mathbb{1}_{x_i \\in f_c^{-1}(O)} - \\sum_{i=1}^N \\mathbb{1}_{x_i \\in T_{Csub}(O)}) \n\\approx vol(T_{Csub}(O)) \u2013 vol(f_c^{-1}(O))\n(6)\n\\end{equation}\n We then choose the leaf subdomain with the maximum priority. This leaf subdomain is then partitioned into two subregions \\(C_{lub}^l, C_{lub}^r\\), each of which we then approximate with polytopes \\(T_{cl}^l(O), T_{cl}^r(O)\\). As tighter intermediate concrete bounds, and thus linear bounding functions, can be computed on the partitioned subregions, the polytope approximation on each subregion will be refined compared with the polytope approximation on the original subregion."}, {"title": "4.4 Local Optimization", "content": "One of the key components behind the effectiveness of LiRPA-based bounds is the ability to efficiently improve the tightness of the bounding function by optimising the relaxation parameters \\(\u03b1\\) via projected gradient descent. In the context of local robustness verification, the goal is to optimise the concrete (scalar) lower or upper bounds over the (sub)region \\(C_{sub}\\) (Xu et al., 2020), i.e., \\(min_{x\u2208C_{sub}} A(\u03b1)x + b(\u03b1)\\) in the case of lower bounds, where we explicitly note the dependence of the linear coefficients on \\(\u03b1\\). In our case, we are instead interested in optimising \\(\u03b1\\) to refine the polytope approximation, that is, increase the volume of under-approximations and decrease the volume of over-approximations (to the exact preimage)."}, {"title": "4.5 Optimization of Lagrangian Relaxation", "content": "Previously, in Section 4.3, we proposed a preimage refinement method that adds intermediate ReLU splitting planes to tighten the bounds of a selected individual neuron. However, intermediate bounds for other neurons are not updated based on the newly added splitting constraint. In the following, we first discuss the impact of stabilizing an intermediate ReLU neuron from two different perspectives. We then present an optimization approach leveraging Lagrangian relaxation to enforce the splitting constraint on refining the preimage.\nOur previous approach of Zhang et al. (2024) exploits one level of bound tightening after ReLU splitting: the substitution of relaxation functions with exact linear functions for the individual neuron. Specifically, assume an intermediate (unstable) neuron \\(z^{(i)}_{j} \\equiv h^{(i)}(x)\\) is selected to split the subregion \\(C\\) into two subregions \\(C^{(i)}_{<=0} = \\left\\{ x \\in C | z^{(i)}_{j} < 0 \\right\\}\\) and \\(C^{(i)}_{>=0} = \\left\\{ x \\in C | z^{(i)}_{j} > 0 \\right\\}\\). For each subregion, the linear bounding functions of the nonlinear activation function"}, {"title": "4.6 Overall Algorithm", "content": "Our overall preimage approximation method is summarised in Algorithm 1. It takes as input a neural network \\(f\\), input region \\(C\\), output region \\(O\\), target polytope volume threshold \\(v\\) (a proxy for approximation precision), maximum number of iterations \\(R\\), number of samples \\(N\\) for statistical estimation, and Boolean variables indicating (i) whether to return an under-approximation or over-approximation and (ii) whether to use input or ReLU splitting, and returns a disjoint polytope union \\(Dom\\) representing a guaranteed under-approximation (or over-approximation) to the preimage.\nThe algorithm initiates and maintains a priority queue of (sub)regions according to Equation 7. The initialisation step (Lines 1-2) generates an initial polytope approximation of the whole region using Algorithm 2 (Sections 4.2, 4.4, 4.5), with priority calculated (\\textit{CalcPriority}\\) according to Equations 4, 6. Then, the preimage refinement loop (Lines 3-11) partitions a subregion in each iteration, with the preimage restricted to the child subregions then being re-approximated (Line 9-10). In each iteration, we choose the region to split (Line 4) and the splitting plane to cut on (Line 6 for input split and Line 8 for ReLU split), as detailed in Section 4.3. The preimage subregion queue is then updated by computing the priorities for each subregion by approximating their volume (Line 11). The loop terminates and the approximation is returned when the target volume threshold \\(v\\) or maximum iteration limit \\(R\\) is reached."}, {"title": "4.7 Quantitative Verification", "content": "We now show how to use our efficient preimage under-approximation method (Algorithm 1) to verify a given quantitative property \\((I,O,p)\\), where \\(O\\) is a polyhedron, \\(I\\) a polytope and \\(p\\) the desired proportion threshold, summarised in Algorithm 3. Note that preimage over-approximation cannot be applied for sound quantitative verification as the approximation may contain false regions outside the true preimage. To simplify, assume that \\(I\\) is a hyperrectangle, so that we can take \\(C = I\\). We discuss the case of general polytopes at the end of this section. We utilise Algorithm 1 by setting the volume threshold \\(v\\) to"}, {"title": "5.2.1 Effectiveness on Preimage Approximation with Input Split", "content": "We apply Algorithm 1 with input splitting to the preimage approximation problem for low-dimensional reinforcement learning tasks. For comparison, we also run the exact preimage generation method (Exact) from Matoba and Fleuret (2020) and the preimage over-approximation method (Invprop) from Kotha et al. (2023, accessed October, 2023)."}, {"title": "5.2.4 Quantitative Verification", "content": "We now demonstrate the application of our preimage under-approximation to quantitative verification of the property (I,O,p); that is, we aim to check whether f(x) \u2208 O for at least proportion p of input values x \u2208 I. Table 8 summarises the quantitative verification results, which leverage the disjointness of our under-approximation, such that we can compute the total volume covered by computing the volume of each individual polytope."}, {"title": "6 Related Work", "content": "Our paper is related to a series of works on robustness verification of neural networks. To address the scalability issues with complete verifiers (Huang et al., 2017; Katz et al., 2017; Tjeng et al., 2019) based on constraint solving, convex relaxation (Salman et al., 2019) has been used for developing highly efficient incomplete verification methods (Zhang et al., 2018; Wong and Kolter, 2018; Singh et al., 2019; Xu et al., 2020). Later works employed the branch-and-bound (BaB) framework (Bunel et al., 2018, 2020) to achieve completeness, using incomplete methods for the bounding procedure (Xu et al., 2021; Wang et al., 2021b; Ferrari et al., 2022). In this work, we adapt convex relaxation for efficient preimage approximation. Further, our divide-and-conquer procedure is analogous to BaB, but focuses on maximising covered volume for under-approximation (resp. minimising for over-approximation) rather than maximising or minimising a function value. There are also works that have sought to define a weaker notion of local robustness known as statistical robustness (Webb et al., 2019b; Mangal et al., 2019; Wang et al., 2021a), which requires that a proportion of points under some perturbation distribution around an input point are classified in the same way. Verification of statistical robustness is typically achieved by sampling and statistical guarantees (Webb et al., 2019b; Baluta et al., 2021; Tit et al., 2021; Yang et al., 2021). In this paper, we apply our symbolic approximation approach to quantitative analysis of neural networks, while providing exact quantitative rather than statistical evaluation (Webb et al., 2019a).\nAnother line of related works considers deriving exact or approximate abstractions of neural networks, which are applied for explanation (Sotoudeh and Thakur, 2021), verification (Elboher et al., 2020; Pulina and Tacchella, 2010), reachability analysis (Prabhakar and Afzal, 2019), and preimage approximation (Dathathri et al., 2019; Kotha et al., 2023). Dathathri et al. (2019) leverages symbolic interpolants (Albarghouthi and McMillan, 2013) for preimage approximations, facing exponential complexity in the number of hidden neurons. Kotha et al. (2023) considers the preimage overapproximation problem via inverse bound propagation, but their approach cannot be directly extended to the under-approximation setting. They also do not consider any strategic branching and refinement methodologies like those in our unified framework. Our anytime algorithm, which combines convex relaxation with principled splitting strategies for refinement, is applicable for both under- and over-approximations. Their work may benefit from our splitting strategies to scale to higher dimensions."}, {"title": "7 Conclusion", "content": "We present an efficient and unifying algorithm for preimage approximation of neural networks. Our anytime method derives from the observation that linear relaxation can be used to efficiently produce approximations, in conjunction with custom-designed strategies for iteratively decomposing the problem to rapidly improve the approximation quality. We formulate the preimage approximation in each refinement iteration as an optimisation problem and propose a differentiable objective to derive tighter preimages via optimising over convex bounding parameters and Lagrange multipliers. Unlike previous approaches, our method is designed for, and scales to, both low and high-dimensional problems. Experimental eval-"}, {"title": "Appendix A. Experiment Setup", "content": "In this section, we present the detailed configuration of neural networks in the benchmark tasks."}, {"title": "A.1 Vehicle Parking.", "content": "For the vehicle parking task, we train a neural network with one hidden layer of 20 neurons, which is computationally feasible for exact preimage computation for comparison. We consider computing the preimage approximation with input region corresponding to the entire input space \\(C = \\{ x \\in R^2 | x \\in [0,2]^2 \\}\\), and output sets \\(O_k\\), which correspond to the neural network outputting label k: \\(O_k = \\{ y \\in R^4 | \\land_{i \\in \\{1,2,3,4\\} \\\\ k} Y_k - Y_i \\ge 0 \\}, k \\in \\{1,2,3,4\\}\\)."}, {"title": "A.2 Aircraft Collision Avoidance", "content": "The aircraft collision avoidance (VCAS) system (Julian and Kochenderfer, 2019) is used to provide advisory for collision avoidance between the ownship aircraft and the intruder. VCAS uses four input features (h,ha,hp,t) representing the relative altitude of the aircrafts, vertical climbing rates of the ownship and intruder aircrafts, respectively, and time to the loss of horizontal separation. VCAS is implemented by nine feed-forward neural networks built with a hidden layer of 21 neurons. In our experiment, we use the following input region for the ownship and intruder aircraft as in Matoba and Fleuret (2020):\n\\(h \\in [-8000, 8000], h_a \\in [-100,100], h_b = 30\\), and \\(t \\in [0,40]\\). In the training, the input configurations are normalized into a range of [-1,1]. We consider the output property \\(O = \\{ y \\in R^9 | \\land_{i \\in [2,9]} y_1 \\ge y_i \\}\\) and generate the preimage approximation for the VCAS neural networks."}, {"title": "A.3 Neural Network Controllers", "content": ""}, {"title": "A.3.1 Cartpole", "content": "The cartpole control problem considers balancing a pole atop a cart by controlling the movement of the cart. The neural network controller has two hidden layers with 64 neurons, and uses four input variables representing the position and velocity of the cart, the angle and angular velocity of the pole. The controller outputs are pushing the cart left or right. In the experiments, we set the following input region for the Cartpole task: (1) cart position [-1,1], (2) cart velocity [0, 2], (3) angle of the pole [-0.2, 0], and (4) angular velocity of the pole [-2, 0] (with varied feature length in the evaluation). We consider the output property for the action pushing left."}, {"title": "A.3.2 Lunarlander", "content": "The Lunarlander problem considers the task of correct landing of a moon lander on a landing pad. The neural network for Lunarlander has two hidden layers with 64 neurons, and eight input features addressing the lander's coordinate, orientation, velocities, and ground contact indicators. The outputs represent four actions. For the Lunarlander task, we set the input region as: (1) horizontal and vertical position [-1,0] \u00d7 [0,1], (2) horizontal and vertical"}, {"title": "Proposition 9", "content": "Given any subregion \\(C_{sub}\\) with polytope under-approximation \\(T_{C_{sub}}(O)\\), and its children \\(C_{sub}^l, C_{sub}^r\\) with polytope under-approximations \\(T_{C_{sub}^l}(O), T_{C_{sub}^r}(O)\\) respectively, it holds that:\n\n(22)\n Proof We define \\(T_{C_{sub}}(O)|_{l}, T_{C_{sub}}(O)|_{r}\\) to be the restrictions of \\(T_{C_{sub}}(O)\\) to \\(C_{sub}^l\\) and \\(C_{sub}^r\\) respectively, that is:"}, {"title": "Input split", "content": ": We show \\(g_{l,i}(x) \u2265 g_{i}(x)\\) for all \\(x \u2208 C_{sub}^l\\) by induction (dropping the index i"}]}