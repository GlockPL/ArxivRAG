{"title": "PREMAP: A Unifying PREiMage APproximation Framework for Neural Networks", "authors": ["Xiyue Zhang", "Benjie Wang", "Marta Kwiatkowska", "Huan Zhang"], "abstract": "Most methods for neural network verification focus on bounding the image, i.e., set of outputs for a given input set. This can be used to, for example, check the robustness of neural network predictions to bounded perturbations of an input. However, verifying properties concerning the preimage, i.e., the set of inputs satisfying an output property, requires abstractions in the input space. We present a general framework for preimage abstraction that produces under- and over-approximations of any polyhedral output set. Our framework employs cheap parameterised linear relaxations of the neural network, together with an anytime refinement procedure that iteratively partitions the input region by splitting on input features and neurons. The effectiveness of our approach relies on carefully designed heuristics and optimization objectives to achieve rapid improvements in the approximation volume. We evaluate our method on a range of tasks, demonstrating significant improvement in efficiency and scalability to high-input-dimensional image classification tasks compared to state-of-the-art techniques. Further, we showcase the application to quantitative verification and robustness analysis, presenting a sound and complete algorithm for the former and providing sound quantitative results for the latter.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable empirical success of neural networks, ensuring their safety against potentially adversarial behaviour, especially when using them as decision-making components in autonomous systems, is an important and challenging task. Towards this aim, various approaches have been developed for the verification of neural networks, with extensive effort devoted, in particular, to the problem of local robustness verification, which focuses on deciding the presence or absence of adversarial examples within an \u03b5-perturbation neighbourhood."}, {"title": "2 Preliminaries", "content": "We use f : Rd \u2192 Rm to denote a feed-forward neural network. For layer i, we use W(i) to denote the weight matrix, b(i) the bias, z(i) the pre-activation neurons, and \u03b1(i) the post-activation neurons, such that we have z(i) = W(i)\u03b1(i-1)+b(i). We use h(i)(x) to denote the function from input to pre-activation neurons, and a(i)(x) the function from input to the post-activation neurons, i.e., z(i) = h(i) (x) and \u03b1(i) = a(i) (x). In this paper, we focus on ReLU neural networks with a(i) (x) = ReLU(h(i) (x)), where ReLU(h) := max(h, 0) is applied element-wise. However, our method can be generalized to other activation functions that can be bounded by linear functions, similarly to Zhang et al. (2018).\nLinear Relaxation of Neural Networks. Nonlinear activation functions lead to the NP-completeness of the neural network verification problem as proved in Katz et al. (2017). To address such intractability, linear relaxation is often used to transform the nonconvex constraints into linear programs. As shown in Figure 2, given concrete lower and upper bounds l(i) \u2264 h(i)(x) \u2264 u(i) on the pre-activation values of layer i, there are three cases to"}, {"title": "3 Problem Formulation", "content": "In this work, we are interested in the problem of computing preimages for neural networks. Given a subset O \u2286 Rm of the codomain, the preimage of a function f : Rd \u2192 Rm is defined to be the set of all inputs x \u2208 Rd that are mapped to an element of O by f. For neural networks in particular, the input is typically restricted to some bounded input region C \u2286 Rd. In this work, we restrict the output set O to be a polyhedron, and the input set C to be an axis-aligned hyperrectangle region C \u2286 Rd, as these are commonly used in neural network verification. We now define the notion of a restricted preimage.\nDefinition 1 (Restricted Preimage) Given a neural network f : Rd \u2192 Rm, and an input set C \u2286 Rd, the restricted preimage of an output set O \u2286 Rm is defined to be the set f\u22121C(O) := {x \u2208 Rd|f(x) \u2208 O \u2227 x \u2208 C}.\nExample 1 To illustrate our problem formulation and approach, we introduce a vehicle parking task from Ayala et al. (2011) as a running example. In this task, there are four parking lots, located in each quadrant of a 2 \u00d7 2 grid [0,2]2, and a neural network with two hidden layers of 10 ReLU neurons f : R2 \u2192 R4 is trained to classify which parking lot an input point belongs to. To analyze the behaviour of the neural network in the input region [0,2] \u00d7 [0,2], we set C = {x \u2208 R2|(0 \u2264 x1 \u2264 2) \u2227 (0 \u2264 x2 \u2264 2)}. Then the restricted preimage f\u22121C(O) of the set O = {y \u2208 R4|\u2227i\u2208{2,3,4} y1 \u2212 yi \u2265 0} is the subspace of the region [0,2] \u00d7 [0,2] that is labelled as parking lot 1 by the neural network.\nWe focus on provable approximations of the preimage. Given a first-order formula A, a is an under-approximation (resp. over-approximation) of A if it holds that \u2200x.\u03b1(x) \u21d2 A(x) (resp. \u2200x.A(x) \u21d2 \u03b1(x)). In our context, the restricted preimage is defined by the formula A(x) = (f(x) \u2208 O) \u2227 (x \u2208 C), and we restrict to approximations \u03b1 that take the form of a disjoint union of polytopes (DUP). The goal of our method is to generate a DUP approximation T that is as tight as possible; that is, we aim to maximize the volume vol(T) of an under-approximation, or minimize the volume vol(T) of an over-approximation.\nDefinition 2 (Disjoint Union of Polytopes) A disjoint union of polytopes (DUP) is a FOL formula \u03b1 of the form \u03b1(x) := \u2228i=1\u03b1i(x), where each \u03b1i is a polytope formula (conjunction of a finite set of linear half-space constraints), with the property that \u03b1i \u2227 \u03b1j is unsatisfiable for any i \u2260 j."}, {"title": "3.2 Quantitative Properties", "content": "One of the most important verification problems for neural networks is that of proving guarantees on the output of a network for a given input set. This is often expressed as a property of the form (I,O) such that \u2200x \u2208 I \u21d2 f(x)\u2208 O. We can generalize this to quantitative properties:\nDefinition 3 (Quantitative Property) Given a neural network f : Rd \u2192 Rm, a measurable input set with non-zero measure (volume) I \u2286 Rd, a measurable output set O \u2286 Rm, and a rational proportion p \u2208 [0,1], we say that the neural network satisfies the property (I,O,p) if  vol(f\u22121(O)) vol(I) > p.\nNeural network verification algorithms can be characterized by two main properties: soundness, which states that the algorithm always returns correct results, and completeness, which states that the algorithm always reaches a conclusion on any verification query. We now define the soundness and completeness of verification algorithms for quantitative properties.\nDefinition 4 (Soundness) A verification algorithm QV is sound if, whenever QV outputs True, the property (I,O,p) holds.\nDefinition 5 (Completeness) A verification algorithm QV is complete if (i) QV never returns Unknown, and (ii) whenever QV outputs False, the property (I,O,p) does not hold.\nIf the property (I, O) holds, then the quantitative property (I, O, 1) holds, while quan-titative properties for 0 < p < 1 provide more information when (I,O) does not hold. Most neural network verification methods produce approximations of the image of I in the output space, which cannot be used to verify quantitative properties. Preimage over-approximations include points outside of the true preimage; thus, they cannot be applied for sound quantitative verification. In contrast, preimage under-approximations provide a lower bound on the volume of the preimage, allowing us to soundly verify quantitative properties."}, {"title": "4 Methodology", "content": "In this section, we present the main components of our methodology. Figure 3 shows the workflow of our preimage approximation method (using under-approximation as an illustration).\nIn Section 4.2, we introduce how to cheaply and soundly under-approximate (or over-approximate) the (restricted) preimage with a single polytope by means of the linear relaxation methods (Algorithm 2), which offer greater scalability than the exact method. To handle the approximation loss caused by linear relaxation, in Section 4.3 we propose an anytime refinement algorithm that improves the approximation by partitioning a (sub)region into subregions with splitting (hyper)planes, with each subregion then being approximated more accurately in parallel. In Section 4.4, we propose a novel differentiable objective to optimise the bounding parameters of linear relaxation to tighten the polytope approximation. Next, in Section 4.5, we propose a refinement scheme based on intermediate ReLU splitting planes and derive a preimage optimisation method using Lagrangian relaxation of the splitting constraints. The main contribution of this paper (Algorithm 1) integrates these four components and is described in Section 4.6. Finally, in Section 4.7, we apply our method to quantitative verification (Algorithm 3) and prove its soundness and completeness.\nTo simplify the presentation, we focus on computing under-approximations and explain the necessary changes to compute over-approximations in highlight boxes throughout."}, {"title": "4.2 Polytope Approximation via Linear Relaxation", "content": "We first show how to adapt linear relaxation techniques to efficiently generate valid under-approximations and over-approximations to the restricted preimage for a given input region C as a single polytope. Recall that LiRPA methods enable us to obtain linear lower and upper bounds on the output of a neural network f, that is, A\u00afx + b\u00af \u2264 f(x) \u2264 A^x + b^, where the linear coefficients depend on the input region C.\nSuppose that we are given the input hyperrectangle C = {x \u2208 Rd|x = \u2227di=1\u03a6i}, and the output polytope specified using the half-space constraints Vi(y) = (c\u2032iy + di \u2265 0) for i = 1, ..., K over the output space. Let us first consider generating a guaranteed under-approximation. Given a constraint Vi, we append an additional linear layer at the end of the network f, which maps y \u2192 c\u2032iy + di, such that the function gi : Rd \u2192 R represented by the new network is gi(x) = c\u2032if(x) + di. Then, applying LiRPA lower bounding to each gi, we obtain a lower bound gi(x) = a\u2032ix + b\u2032i; for each i, such that gi(x) \u2265 0 \u21d2 gi(x) \u2265 0 for x \u2208 C. Notice that, for each i = 1, ..., K, a\u2032ix + b\u2032i \u2265 0 is a half-space constraint in the input space. We conjoin these constraints, along with the restriction to the input region C, to obtain a polytope:\nTC(O) := {x|\u2227Ki=1(gi(x) \u2265 0) \u2227 \u2227di=1\u03a6i} (1)\nOver-Approximation Alternatively, to generate a guaranteed over-approximation, we can instead apply LiRPA upper bounding to each gi, obtaining upper bounds gi(x) = a\u2032ix + b\u2032i for each i, such that gi(x) \u2265 0 \u21d2 gi(x) \u2265 0 for x \u2208 C, and defining the polytope:\nTC(O) := {x|\u2227Ki=1(gi(x) \u2265 0) \u2227 \u2227di=1\u03a6i} (2)\nProposition 6 TC(O),TC(O) are respectively under- and over-approximations to the re-stricted preimage f\u22121C(O).\nProof For the under-approximation, the LiRPA bound gi(x) \u2264 gi(x) holds for any x \u2208 C and i = 1, ..., K, and so we have \u2227Ki=1(gi(x) \u2265 0) \u2227 x \u2208 C \u21d2 \u2227Ki=1(gi(x) \u2265 0) \u2227 x \u2208 C, i.e., TC(O) is an under-approximation to f\u22121C (O). Similarly, for the over-approximation,"}, {"title": "4.3 Global Branching and Refinement", "content": "As LiRPA performs crude linear relaxation, the resulting bounds can be quite loose, even with optimisation over bounding parameters (as we will see in Section 4.4), meaning that the (single) polytope under-approximation or over-approximation is unlikely to be a good approximation to the preimage by itself. To address this challenge, we employ a divide-and-conquer approach that iteratively refines our approximation of the preimage. Starting from the initial region C at the root, our method generates a tree by iteratively partitioning a subregion Csub represented at a leaf node into two smaller subregions C\u2032sub, C\u2032\u2032sub, which are then attached as children to that leaf node. In this way, the subregions represented by all leaves of the tree are disjoint, such that their union is the initial region C.\nIn order to under-approximate (resp. over-approximate) the preimage, for each leaf subregion Csub we compute, using LiRPA bounds, an associated polytope that under-approximates (resp. over-approximates) the preimage in Csub. Thus, irrespective of the number of refinements performed, the union of the under-approximating polytopes (resp. over-approximating) corresponding to all leaves forms an anytime DUP under-approximation (resp. over-approximation) T to the preimage in the original region C. The process of refining the subregions continues until an appropriate termination criterion is met.\nUnfortunately, even with a moderate number of input dimensions or unstable ReLU nodes, na\u00efvely splitting along all input- or ReLU-planes quickly becomes computationally intractable. For example, splitting a d-dimensional hyperrectangle using bisections along each dimension results in 2d subdomains to approximate. It thus becomes crucial to pri-oritise the subregions to split, as well as improve the efficiency of the splitting procedure itself. We describe these in turn."}, {"title": "4.4 Local Optimization", "content": "One of the key components behind the effectiveness of LiRPA-based bounds is the ability to efficiently improve the tightness of the bounding function by optimising the relaxation parameters \u03b1 via projected gradient descent. In the context of local robustness verification, the goal is to optimise the concrete (scalar) lower or upper bounds over the (sub)region Csub, i.e., minx\u2208Csub A(\u03b1)x + b(\u03b1) in the case of lower bounds, where we explicitly note the dependence of the linear coefficients on \u03b1. In our case, we are instead interested in optimising \u03b1 to refine the polytope approximation, that is, increase the volume of under-approximations and decrease the volume of over-approximations (to the exact preimage).\nAs before, we employ statistical estimation; we sample N points x1,..., xN uniformly from the input domain Csub then employ Monte Carlo estimation for the volume of the approximating polytope. In the case of under-approximation, we have:\nvol(TCsub,\u03b1(O)) = vol(Csub)N \u00d7 \u2211Ni=1Ixi\u2208TCsub,\u03b1(O) (9)\nwhere we highlight the dependence of TCsub(O) = {x|\u2227Ki=1gi(x, \u03b1i) \u2265 0 \u2227 \u2227di=1\u03a6i(x)} on \u03b1 = (\u03b11, ..., \u03b1K), and \u03b1i are the \u03b1-parameters for the linear relaxation of the neural network gi corresponding to the ith half-space constraint in O. However, this is still non-differentiable w.r.t. \u03b1 due to the identity function. We now show how to derive a differentiable relaxation, which is amenable to gradient-based optimization:\nvol(TCsub,\u03b1(O)) = vol(Csub)N \u2211Ni=1 IIminigi(xj ,\u03b1i)\u22650 (10)\n= vol(Csub)N \u2211Nj=1 IImini=1,...,Kgi(xj,\u03b1i)\u22650 (11)\n\u2248 vol(Csub)N \u2211Nj=1\u03c3(\u2212LSE(\u2212g1(xj, \u03b11), ..., \u2212gK(xj, \u03b1K))) (12)\nAs before, we use a sigmoid relaxation to approximate the volume. However, the min-imum function is still non-differentiable. Thus, we approximate the minimum over spec-ifications using the log-sum-exp (LSE) function. The log-sum-exp function is defined by LSE(y1, ..., yK) := log(\u2211Ki=1eyi), and is a differentiable approximation to the maxi-mum function; we employ it to approximate the minimisation by adding the appropriate sign changes. The final expression is now a differentiable function of \u03b1.\nThen the goal is to maximise the volume of the under-approximation with respect to \u03b1:\nLoss(\u03b1) = \u2212vol(TCsub,\u03b1(O)) (13)\nWe employ this as the loss function in Algorithm 2 (Line 7) for generating a polytope approximation, and optimise volume using projected gradient descent.\nOver-Approximation In the case of an over-approximation (Line 12 of Algorithm 2), we instead aim to minimise the volume of the approximation:\nLoss(\u03b1) = vol(TCsub,\u03b1(O)) (14)"}, {"title": "4.5 Optimisation of Lagrangian Relaxation", "content": "Previously, in Section 4.3, we proposed a preimage refinement method that adds intermedi-ate ReLU splitting planes to tighten the bounds of a selected individual neuron. However, intermediate bounds for other neurons are not updated based on the newly added splitting constraint. In the following, we first discuss the impact of stabilising an intermediate ReLU neuron from two different perspectives. We then present an optimisation approach leverag-ing Lagrangian relaxation to enforce the splitting constraint on refining the preimage.\nEffect of Stabilisation of Intermediate Neurons. Our previous approach of Zhang et al. (2024) exploits one level of bound tightening after ReLU splitting: the substitution of relaxation functions with exact linear functions for the individual neuron. Specifically, assume an intermediate (unstable) neuron z(i)j (= h(i)j(x)) is selected to split the input (sub)region C into two subregions C+(i)j = {x \u2208 C | z(i)j > 0} and C\u2212(i)j = {x \u2208 C | z(i)j < 0}. For each subregion, the linear bounding functions of the nonlinear activation function"}, {"title": "4.6 Overall Algorithm", "content": "Our overall preimage approximation method is summarised in Algorithm 1. It takes as input a neural network f, input region C, output region O, target polytope volume threshold v (a proxy for approximation precision), maximum number of iterations R, number of samples N for statistical estimation, and Boolean variables indicating (i) whether to return an under-approximation or over-approximation and (ii) whether to use input or ReLU splitting, and returns a disjoint polytope union Dom representing a guaranteed under-approximation (or over-approximation) to the preimage.\nThe algorithm initiates and maintains a priority queue of (sub)regions according to Equation 7. The initialisation step (Lines 1-2) generates an initial polytope approximation of the whole region using Algorithm 2 (Sections 4.2, 4.4, 4.5), with priority calculated (CalcPriority) according to Equations 4, 6. Then, the preimage refinement loop (Lines 3-11) partitions a subregion in each iteration, with the preimage restricted to the child subregions then being re-approximated (Line 9-10). In each iteration, we choose the region to split (Line 4) and the splitting plane to cut on (Line 6 for input split and Line 8 for ReLU split), as detailed in Section 4.3. The preimage subregion queue is then updated by computing the priorities for each subregion by approximating their volume (Line 11). The loop terminates and the approximation is returned when the target volume threshold v or maximum iteration limit R is reached."}, {"title": "4.7 Quantitative Verification", "content": "We now show how to use our efficient preimage under-approximation method (Algorithm 1) to verify a given quantitative property (I,O,p), where O is a polyhedron, I a polytope and p the desired proportion threshold, summarised in Algorithm 3. Note that preimage over-approximation cannot be applied for sound quantitative verification as the approxi-mation may contain false regions outside the true preimage. To simplify, assume that I is a hyperrectangle, so that we can take C = I. We discuss the case of general polytopes at the end of this section. We utilise Algorithm 1 by setting the volume threshold v to"}, {"title": "5 Experiments", "content": "We have implemented our approach as a tool for preimage approximation for polyhedral output sets/specifications. In this section, we report on experimental evaluation of the proposed approach, and demonstrate its effectiveness in approximation generation and the application to quantitative analysis of neural networks."}, {"title": "5.1 Benchmark and Evaluation Metric", "content": "We evaluate our preimage analysis approach on a benchmark of reinforcement learning and image classification tasks. Besides the vehicle parking task of Ayala et al. (2011) shown in the running example, we consider the following tasks: (1) aircraft collision avoidance system"}, {"title": "5.2 Evaluation", "content": "We apply Algorithm 1 with input splitting to the preimage approximation problem for low-dimensional reinforcement learning tasks. For comparison, we also run the exact preim-age generation method (Exact) from Matoba and Fleuret (2020) and the preimage over-approximation method (Invprop) from Kotha et al. (2023, accessed October, 2023)."}, {"title": "5.2.2 EFFECTIVENESS OF SMOOTHED INPUT SPLITTING", "content": "We now analyse the effectiveness of the smoothed splitting method described in Section 4.3 (Equation 7 and 8), in comparison to a volume-guided splitting method that chooses the input feature leading to the greatest improvement in approximation volume. From Figures 12 and 13, we observe that the smoothed splitting method requires significantly fewer refinement iterations for all reinforcement learning controllers to achieve the target coverage, thus reducing the number of polytopes and computation time, than the volume-guided splitting method. More specifically, the smoothed splitting method achieves an average reduction of 43.6% in the number of polytopes and 51.0% in computation time for under-approximation across the neural network controllers, up to 80.8%/81.2% reduction for the Lunarlander task. Similar improvements in computation efficiency and size of polytope union are also achieved for over-approximations, with an average reduction of 50.8%/49.6% across all reinforcement learning tasks.\nRecall that the smoothed input splitting heuristic relaxes the volume-based heuristic, such that, for each sampled input point in the input region, we take into account not only whether the point lies in the polytope approximation, but also how far away the point is from the approximation. This is particularly crucial in early iterations, where the approximation may be too loose; for example, an under-approximation may have no overlap with the input region (thus zero volume). Therefore, computing the approximation volume (after splitting on each input feature) provides very little signal. In such cases, the smoothed splitting heuristic is able to capture promising input features that, while not immediately improving the approximation volume, can bring the preimage bounding planes closer to the exact preimage, which is beneficial for future iterations."}, {"title": "5.2.3 EFFECTIVENESS OF PREIMAGE APPROXIMATION WITH RELU SPLIT", "content": "In this subsection, we evaluate the scalability of Algorithm 1 with ReLU splitting by ap-plying it to MNIST image classifiers. In particular, we consider input regions defined by bounded perturbations to a given MNIST image. Table 4 and 5 summarise the evaluation results for two types of image perturbations commonly considered in the adversarial robust-ness literature (L\u221e and patch attack, respectively). For L\u221e attacks, bounded perturbation noise is applied to all image pixels. The patch attack applies only to a smaller patch area of n \u00d7 m pixels but allows arbitrary perturbations covering the whole valid range [0,1]. The task is then to produce a DUP approximation of the subset of the perturbation region that is guaranteed to be classified correctly.\nFor L\u221e attack, we evaluate our method over perturbations of increasing size, from 0.06 to 0.09. It is worth noting that for this size of preimage, e.g., from 0.06 to 0.07, the volume of the input region increases by tens of orders of magnitude due to the high dimensionality, making effective preimage approximation significantly more challenging. Table 4 shows that our approach (Algorithm 1) without Lagrangian optimisation (marked in columns w/o) is able to generate a preimage under-approximation that achieves the targeted coverage of 0.75 for L\u221e noise up to 0.08. The fact that the number of polytopes and computation time remain manageable is due to the effectiveness of ReLU splitting. In Table 5, for the patch attack, we observe that the number of polytopes and time required increase sharply when increasing the patch size for both the centre and corner area of the image, suggesting that the model is more sensitive to larger local perturbations. It is also interesting that our method can generate preimage approximations for larger patches in the corner as opposed to the centre of the image; we hypothesize this is due to the greater influence of central pixels on the neural network output, and correspondingly a greater number of unstable neurons over the input perturbation space.\nTable 6 shows the preimage refinement results for over-approximations in the context of patch attack. The results of our approach (Algorithm 1) without Lagrangian optimi-sation are summarised in columns w/o. As shown in the table, our refinement method can effectively tighten the over-approximation to the targeted coverage of 1.25 for differ-ent attack configurations. For patch size 10 \u00d7 10 (centre) and 16 \u00d7 15 (corner), we found that the perturbation region is a trivial over-approximation itself for the target coverage of 1.25; thus, we demonstrate the results with a target coverage of 1.1 and 1.05. Similarly to under-approximations, a patch attack in the centre with a smaller patch size requires more refinement iterations than the patch attack in the corner, demonstrating a greater influence of central pixels.\nEffectiveness of Lagrangian Optimisation. The results of evaluation of our approach (Algorithm 1) for under-approximation with Lagrangian optimisation are shown in Table 4 and 5 (marked in columns w/ LagOpt with grey background). For L\u221e attack, the refinement method with Lagrangian optimisation generates preimage approximations that achieve the target coverage of 0.75 for all perturbation settings, including perturbation noise 0.09 where the refinement without Lagrangian optimisation fails (0.751 vs 0.165 in Table 4). The new refinement method also leads to a significant reduction in the number of polytopes and computation cost. For the patch attack, the refinement method with Lagrangian optimisa-tion effectively improves the preimage approximation precision for all configuration settings."}, {"title": "5.2.4 QUANTITATIVE VERIFICATION", "content": "We now demonstrate the application of our preimage under-approximation to quantitative verification of the property (I, O, p); that is, we aim to check whether f(x) \u2208 O for at least proportion p of input values x \u2208 I. Table 8 summarises the quantitative verification results, which leverage the disjointness of our under-approximation, such that we can compute the total volume covered by computing the volume of each individual polytope.\nVertical Collision Avoidance System. In this example, we consider the VCAS system and a scenario where the two aircraft have negative relative altitude from intruder to ownship (h\u2208 [-8000,0]), the ownship aircraft has a positive climbing rate ha \u2208 [0,100] and the intruder has a stable negative climbing rate hp = -30, and time to the loss of horizontal separation is t\u2208 [0,40], which defines the input region I. For this scenario, the correct advisory is \u201cClear Of Conflict\u201d (COC). We apply Algorithm 3 to verify the quantitative property where O = {y \u2208 R9| \u22279i=2 y1 \u2212 yi \u2265 0} and the proportion p = 0.9, with an iteration limit of 1000. The quantitative proportion reached by the generated under-approximation is 90.8%, which verifies the quantitative property in 5.620s.\nCartpole. In the Cartpole problem, the objective is to balance the pole attached to a cart by pushing the cart either left or right. We consider a scenario where the cart position is to the right of the centre (x \u2208 [0,1]), the cart is moving right (x\u02d9 \u2208 [0,0.5]), the pole is slightly tilted to the right (\u03b8\u2208 [0,0.1]) and pole is moving to the left (\u03b8\u02d9\u2208 [\u22120.2, 0]). To balance the pole, the neural network controller needs to determine \u201cpushing left\u201d. We apply Algorithm 3 to verify the quantitative property, where O = {y| y1 \u2212 y2 > 0} and the proportion p = 0.9, with an iteration limit of 1000. The under-approximation algorithm takes 12.1s to reach the target proportion 90.0%.\nLunarlander. In the Lunarlander task, the objective of the neural networks controller is to achieve a safe landing of the lander. Consider a scenario where the lander is slightly to the left of the centre of the landing pad (x \u2208 [\u22121,0]), the lander is above the landing pad sufficient for descent correction (h \u2208 [0,1]), and it is moving to the right (x\u02d9 \u2208 [1,2]) but descending rapidly (h\u02d9\u2208 [\u22122, -1]). To avoid a hard landing, the neural network controller needs to reduce the descent speed by taking the action \u201cfire main engine\u201d. We formulate the quantitative property for this task, where O = {y \u2208 R4| \u22274i\u2208{1,3,4} y2 \u2265 yi} and the proportion p = 0.9. To compute preimage under-approximation for this more complex task takes 429.480s to reach the target proportion 90.0%."}, {"title": "6 Related Work", "content": "Our paper is related to a series of works on robustness verification of neural networks. To address the scalability issues with complete verifiers based on constraint solving, convex relaxation has been used for developing highly efficient incomplete verification methods. Later works employed the branch-and-bound (BaB) framework to achieve completeness, using incomplete methods for the bounding procedure. In this work, we adapt convex relaxation for efficient preimage approximation. Further, our divide-and-conquer procedure is analogous to BaB, but focuses on maximising covered volume for under-approximation (resp. minimising for over-approximation) rather than maximising or minimising a function value. There are also works that have sought to define a weaker notion of local robustness known as statistical robustness, which requires that a proportion of points under some perturbation distribution around an input point are classified in the same way. Verification of statistical robustness is typically achieved by sampling and statistical guarantees. In this paper, we apply our symbolic approximation approach to quantitative analysis of neural networks, while providing exact quantitative rather than statistical evaluation.\nAnother line of related works considers deriving exact or approximate abstractions of neural networks, which are applied for explanation, veri-fication, reachability analysis, and preimage approximation. Dathathri et al. (2019) leverages symbolic interpolants for preimage approximations, facing exponential complexity in the number of hidden neurons. Kotha et al. (2023) considers the preimage overapproximation problem via inverse bound propagation, but their approach cannot be directly extended to the under-approximation setting. They also do not consider any strategic branching and refinement methodologies like those in our unified framework. Our anytime algorithm, which combines convex relaxation with principled splitting strategies for refinement, is applicable for both under- and over-approximations. Their work may benefit from our splitting strategies to scale to higher dimensions."}, {"title": "7 Conclusion", "content": "We present an efficient and unifying algorithm for preimage approximation of neural net-works. Our anytime method derives from the observation that linear relaxation can be used to efficiently produce approximations, in conjunction with custom-designed strategies for iteratively decomposing the problem to rapidly improve the approximation quality. We for-mulate the preimage approximation in each refinement iteration as an optimisation problem and propose a differentiable objective to derive tighter preimages via optimising over convex bounding parameters and Lagrange multipliers. Unlike previous approaches, our method is designed for, and scales to, both low and high-dimensional problems. Experimental eval-"}, {"title": "Appendix A. Experiment Setup", "content": "In this section, we present the detailed configuration of neural networks in the benchmark tasks.\nFor the vehicle parking task, we train a neural network with one hidden layer of 20 neurons, which is computationally feasible for exact preimage computation for comparison. We consider computing the preimage approximation with input region corresponding to the entire input space C = {x \u2208 R2|x \u2208 [0,2]2}, and output sets Ok, which correspond to the neural network outputting label k: Ok = {y \u2208 R4 | \u2227i\u2208{1,2,3,4}\\k yk \u2212 yi \u2265 0}, k\u2208 {1,2,3,4}.\nThe aircraft collision avoidance (VCAS) system (Julian and Kochenderfer, 2019) is used to provide advisory for collision avoidance between the ownship aircraft and the intruder. VCAS uses four input features (h,ha,hp,t) representing the relative altitude of the air-crafts, vertical climbing rates of the ownship and intruder aircrafts, respectively, and time to the loss of horizontal separation. VCAS is implemented by nine feed-forward neural networks built with a hidden layer of 21 neurons. In our experiment, we use the follow-ing input region for the ownship and intruder aircraft as in Matoba and Fleuret (2020): h\u2208 [-8000, 8000], ha \u2208 [\u2212100,100], hb = 30, and t \u2208 [0,40]. In the training, the input configurations are normalized into a range of [-1,1]. We consider the output property O = {y \u2208 R9 | \u22279i=2 y1 \u2265 yi} and generate the preimage approximation for the VCAS neural networks.\nThe cartpole control problem considers balancing a pole atop a cart by controlling the movement of the cart. The neural network controller has two hidden layers with 64 neurons, and uses four input variables representing the position and velocity of the cart, the angle and angular velocity of the pole. The controller outputs are pushing the cart left or right. In the experiments, we set the following input region for the Cartpole task: (1) cart position [-1,1], (2) cart velocity [0, 2], (3) angle of the pole [-0.2, 0], and (4) angular velocity of the pole [-2, 0] (with varied feature length in the evaluation). We consider the output property for the action pushing left.\nThe Lunarlander problem considers the task of correct landing of a moon lander on a landing pad. The neural network for Lunarlander has two hidden layers with 64 neurons, and eight input features addressing the lander\u2019s coordinate, orientation, velocities, and ground contact indicators. The outputs represent four actions. For the Lunarlander task, we set the input region as: (1) horizontal and vertical position [-1,0] \u00d7 [0,1], (2) horizontal and vertical"}, {"title": "Appendix B. Proofs", "content": "We present the propositions and proofs on guaranteed polytope volume improvement with each refinement iteration"}]}