{"title": "Leveraging Social Media Data and Artificial Intelligence for Improving Earthquake Response Efforts", "authors": ["Kalin Kopanov", "Velizar Varbanov", "Tatiana Atanasova"], "abstract": "The integration of social media and artificial intelligence (AI) into disaster management, particularly for earthquake response, represents a profound evolution in emergency management practices. In the digital age, real-time information sharing has reached unprecedented levels, with social media platforms emerging as crucial communication channels during crises. This shift has transformed traditional, centralized emergency services into more decentralized, participatory models of disaster situational awareness. Our study includes an experimental analysis of 8,900 social media interactions, including 2,920 posts and 5,980 replies on X (formerly Twitter), following a magnitude 5.1 earthquake in Oklahoma on February 2, 2024. The analysis covers data from the immediate aftermath and extends over the following seven days, illustrating the critical role of digital platforms in modern disaster response. The results demonstrate that social media platforms can be effectively used as real-time situational awareness tools, delivering critical information to society and authorities during emergencies.", "sections": [{"title": "Introduction", "content": "The transformative role of social media in modern society is profoundly evident in its capacity to disseminate information swiftly during emergencies. Platforms such as X (formerly Twitter), Facebook, Telegram and TikTok transcend their traditional roles as channels for social interaction to become critical tools in emergency management. These platforms facilitate the rapid spread of crucial information, vital for initiating timely response actions. When accurately collected and analyzed, data from these sources significantly enhance situational awareness, improve decision-making processes, and expand geographic and contextual understanding in crisis situations."}, {"title": "Related work", "content": "Currently, disasters trigger substantial spikes in social media activity, as individuals share updates about their situations and experiences, and seek information to make informed decisions. This increase is not exclusive to platforms like X/Twitter or limited to the developed world [1].\nKarimiziarani's comprehensive review [2] underscores the pivotal role of social media analytics in earthquake disaster management, highlighting its capabilities in enhancing response and recovery efforts. Social media data has proven instrumental in facilitating rapid damage assessment, pinpointing disruptions in critical infrastructure, and bolstering coordination during emergency responses. The integration of this data with seismic monitoring systems has emerged as a best practice, enabling more effective and timely interventions. Additionally, the review details the use of sentiment analysis for monitoring public anxiety levels during seismic events, which assists in addressing community concerns proactively. Machine learning algorithms are also noted for their efficacy in real-time detection of events, significantly improving the speed and accuracy of responses to seismic activities.\nAcademic research has shown the potential of social media for the real-time detection and monitoring of natural disasters. User-generated content on these platforms can act as early indicators, greatly aiding in rapid response and resource allocation. Particularly, X/Twitter's real-time capabilities allow for the immediate detection of events such as earthquakes. Users' posts (tweets) provide critical data that enable the deployment of algorithms to monitor and identify disaster occurrences. For instance, an advanced classifier analyzes post (tweet) features like keywords, word count, and context to detect specific events. Furthermore, a probabilistic spatiotemporal model is applied to determine the event's epicenter and path, utilizing techniques such as Kalman filtering and particle filtering for precise location estimation. This approach has proven exceptionally effective in Japan, where an X/Twitter-based earthquake report-"}, {"title": "Challenges and opportunities", "content": "The exploitation of social media analytics in earthquake response operations embodies a significant interplay between considerable challenges and promising opportunities. This convergence of digital communication platforms represents a fundamental transformation in how disaster management can harness real-time, user-generated content to effectively mitigate the impacts of catastrophic events. Furthermore, it will highlight opportunities for future implementation of location awareness and situational awareness to enhance support and response strategies. In the following subsections, we cover five principal challenges and opportunities; however, this is not an exhaustive list, and further challenges and opportunities undoubtedly exist and merit consideration."}, {"title": "Communication Infrastructure", "content": "The resilience of communication infrastructures is critically tested during earthquakes when traditional networks are often compromised. Depending on the earthquake's magnitude and severity, communication can be partially affected, temporarily disabled, or suffer minimal damage. These impacts are usually most severe near the earthquake's epicenter, although the wider region may experience the earthquake without significant damage to its communication infrastructure. In such scenarios, affected individuals may promptly begin interacting and posting updates about their status on social media. This immediate exchange of information underscores the inherent vulnerability of traditional communication networks and necessitates the de-"}, {"title": "Data Management Capabilities", "content": "The overwhelming influx of data during disasters poses significant challenges in its collection, processing, and analysis. Efficiently managing this data requires sophisticated computational approaches that can quickly filter through vast volumes of information to identify and prioritize actionable insights. This scenario presents an excellent opportunity to harness the full spectrum of AI technologies, including Natural Language Processing (NLP) techniques and Large Language Models (LLMs). These Al tools are well-equipped to handle the noisy, unstructured nature of social media content, which often includes multilingual content, sarcasm, and context-specific nuances, adding layers of complexity to data processing.\nFurthermore, the multilingual aspect of social media data presents a considerable challenge. While NLP can achieve remarkable success in processing English, it often underperforms in languages with less available training data or those characterized by linguistic complexities, such as Arabic."}, {"title": "Geotagging", "content": "The use of geolocation data in social media analytics can significantly enhance the specificity and effectiveness of disaster response efforts. However, it is important to note that when users upload images and videos to social media platforms, metadata, including location data, is often stripped away by default. While users have the option to add geotags manually, these tags may not always be accurate, potentially leading to misdirection or false filtering by location data. Additionally, by default, most applications have the geolocation option turned off, which further limits the availability of precise location data. Despite these challenges, when available and accurate, geotag data can be incredibly valuable, providing snapshot information about who is responding and what is happening in the areas affected by an earthquake. This can be crucial for coordinating timely and effective disaster response efforts."}, {"title": "Application Programming Interfaces (APIs)", "content": "Access to social media data typically involves the use of APIs provided by the platforms. These APIs allow for the retrieval of data streams based on specific queries. However, significant challenges arise due to API constraints, including \u201cpull\" rather than \"push\" data access models, rate limits, and restrictive Terms of Service (ToS). Furthermore, social media companies often impose technological barriers to prevent data exploitation for commercial purposes or competitive development, complicating the acquisition of timely, relevant and voluminous data.\nTo proactively address these limitations, it is crucial to stay informed about updates to the ToS and rate limits of various social media platforms. Monitoring these changes can help in adapting strategies and tools to comply with legal requirements"}, {"title": "Seamless Integration with Emergency Management Systems", "content": "Integrating social media analytics tools into existing emergency management infrastructures presents a unique set of challenges, particularly in ensuring compatibility and enhancing the functionality of legacy systems. Some countries, like the United States, have developed sophisticated emergency management systems and utilize various platforms to enhance public communication and response strategies. For instance, the United States Geological Survey (USGS, https://earthquake.usgs.gov/) and the European Mediterranean Seismological Centre (EMSC, https://emsc-csem.org/) provide up-to-date information on seismic activities through dedicated sections on their websites and social media channels. These organizations manage robust web pages and social media outlets to disseminate timely and accurate information."}, {"title": "Experimental part", "content": "Our methodology involves an analysis of user-generated content and interactions on X (formerly Twitter), immediately following the Oklahoma earthquake which occurred on February 2, 2024, at 23:24:28 local time [5], and extending over the subsequent seven days (until February 9, 2024). This research utilized two types of queries: one based on keywords, using \u201cOklahoma\" and \"earthquake\u201d, and another based on location, covering a 250 km radius from the epicenter where the earthquake's impact was felt within Oklahoma State.\nThe keyword-based query yielded a total of 7,987 results, comprising 2,610 posts and 5,377 comments. The location-based query returned 1,988 results, including 652 posts and 1,336 comments. After merging these results and removing duplicates, a total of 1,075 entries were omitted. The final dataset thus contained 8,900 results, consisting of 2,920 posts and 5,980 comments. It is important to note that while our dataset likely includes most, if not all, posts from the analyzed period, the comments collected represent only the most recent or relevant ones - approximately 27% of the initially noted 22,111 comments as of February 10, 2024. Furthermore, it is worth noting that our statistics (for posts and comments) show the dataset recorded 325,221"}, {"title": null, "content": "reposts, almost 2 million likes, and 223 million views. These figures represent the initial data before any processing and filtering were applied.\nAs expected, we observed the highest user activities within the first 24 hours, accounting for approximately 85% of all posts and replies (comments). To better visualize the temporal spread of activity, we employed a Locally Weighted Scatterplot Smoothing (LOWESS)\u00b9 approach for charting this data (fig. 1). By fitting simple models to localized subsets of the data, LOWESS captures the stochastic nature of the underlying processes without assuming a specific global model form. This method is particularly advantageous for our analysis as it allows the depiction of both the central tendency and dispersion of the data over time, thus accommodating the inherent variability and providing a more nuanced view of user engagement trends. The smoothed visualization underscores significant patterns while mitigating the distraction of short-term fluctuations, thereby facilitating a more intuitive interpretation of the temporal dynamics at play."}, {"title": null, "content": "By tracking user activity and leveraging Al-driven natural language processing (NLP), we aim to filter out irrelevant information, thereby enhancing the focus on critical data that can complement existing sources such as applications, news, and seismological data. For our research, we chose to process the data with XLM-ROBERTa, which, according to our previous tests [6], demonstrated the best overall performance in recognizing location entities.\nThe analysis yielded significant insights, with 8,402 locations being identified from 4,914 posts and replies (comments), representing approximately 55% of the total 8,900 entries analyzed. Notably, similar to prior findings, approximately 83% of these locations were identified within the first 24 hours following the earthquake. Further"}, {"title": null, "content": "refinement of the data revealed that 5,663 entries, or 67.4% of all detected locations, were directly associated with Oklahoma State and the related earthquake events.\nSubsequent steps involved cleaning the data to remove exact duplicates, which reduced the number of location mentions to 324. A more nuanced grouping process was then applied, where similar location references (e.g. \"Tulsa County\", \"Tulsa OK\u201d, \"Tulsa Oklahoma\u201d and \u201cWest Tulsa\u201d) were consolidated into a single representative term (\"Tulsa\u201d). This consolidation resulted in the identification of 144 unique locations within Oklahoma State, as illustrated in Figure 2. This streamlined representation allows for a more focused analysis of the impact areas and facilitates more effective communication of the results."}, {"title": null, "content": "In a separate test, we utilized AI, specifically OpenAI's GPT-4, and a LLM, to identify the coordinates of locations within Oklahoma State. This approach enabled GPT-4 (gpt-4-0125-preview) to accurately recognize 137 locations, achieving a 95% accuracy rate. However, it missed 7 locations where the exact location was unclear or not commonly recognized, such as Eufaula Lake or Eufaula Dam. This application of AI and LLMs technologies underscores their potential in enhancing the precision of geospatial analyses in our research.\nAdditionally, we enhance our monitoring capabilities by employing keyword analysis to detect text that signals events requiring attention or immediate response. For example, phrases such as \"felt it\", appearing 585 times, \u201cshook\u201d, mentioned 257 times, and \"damage\", noted 144 times, suggest potential areas of concern that may warrant further investigation. Similarly, less frequent but critical terms like \"explosion\" (8 mentions), \u201ccrash\u201d (7), and \"fire\" (6) indicate more urgent situations requiring prompt attention."}, {"title": "Conclusion", "content": "The integration of social media and AI into earthquake disaster management, as demonstrated in this study, could significantly enhances situational awareness and response efficiency. The rapid dissemination of user-generated content immediately following the Oklahoma earthquake highlights the importance of social media as a real-time data source. Utilizing advanced NLP techniques for geolocation identification, we were able to accurately filter and analyze critical data. This could not only improves decision-making and resource allocation but also strengthens coordination among emergency responders and aid organizations.\nDespite challenges like data accuracy and information overload, the benefits of leveraging social media for disaster response are evident. The volume of data generated within the first 24 hours is particularly valuable, offering a crucial window for rapid response that can substantially mitigate the impact of disasters. Moving forward, enhancing the integration of these technologies in disaster management strategies is essential for developing more responsive and resilient systems, capable of protecting and informing communities during critical times."}]}