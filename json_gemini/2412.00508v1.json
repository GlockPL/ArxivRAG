{"title": "GRAPH-TO-SFILES: CONTROL STRUCTURE PREDICTION FROM PROCESS TOPOLOGIES USING GENERATIVE ARTIFICIAL INTELLIGENCE", "authors": ["Lukas Schulze Balhorn", "Kevin Degens", "Artur M. Schweidtmann"], "abstract": "Control structure design is an important but tedious step in P&ID development. Generative artificial intelligence (AI) promises to reduce P&ID development time by supporting engineers. Previous research on generative AI in chemical process design mainly represented processes by sequences. However, graphs offer a promising alternative because of their permutation invariance. We propose the Graph-to-SFILES model, a generative AI method to predict control structures from flowsheet topologies. The Graph-to-SFILES model takes the flowsheet topology as a graph input and returns a control-extended flowsheet as a sequence in the SFILES 2.0 notation. We compare four different graph encoder architectures, one of them being a graph neural network (GNN) proposed in this work. The Graph-to-SFILES model achieves a top-5 accuracy of 73.2% when trained on 10,000 flowsheet topologies. In addition, the proposed GNN performs best among the encoder architectures. Compared to a purely sequence-based approach, the Graph-to-SFILES model improves the top-5 accuracy for a relatively small training dataset of 1,000 flowsheets from 0.9% to 28.4%. However, the sequence-based approach performs better on a large-scale dataset of 100,000 flowsheets. These results highlight the potential of graph-based AI models to accelerate P&ID development in small-data regimes but their effectiveness on industry relevant case studies still needs to be investigated.", "sections": [{"title": "1 Introduction", "content": "Developing piping and instrumentation diagrams (P&IDs) is currently a manual and tedious task (Toghraei, 2019). Supporting engineers in P&ID development with computer algorithms offers potential for cost and time reduction (Dzhusupova et al., 2022; Bramsiepe et al., 2014). Time savings can reduce a project's lead time, which is a critical economic success factor, particularly in the fine chemical and pharmaceutical industries (Bramsiepe et al., 2014). In addition, employing computer algorithms for tedious tasks can free up resources for innovation (Dzhusupova et al., 2022)."}, {"title": "2 Information representation of flowsheets", "content": "To leverage machine learning and learn from chemical processes and their data, we need to represent chemical processes in a machine-readable format. We represent processes as graphs for the input to our models (Section 2.1) and as SFILES for the output of our models (Section 2.2). The distillation column in Figure la serves as an example to illustrate the information representations."}, {"title": "2.1 Process graphs", "content": "Chemical processes can be viewed as complex networks of unit operations connected through streams. An intuitive choice for the information representation is therefore a graph. We make use of the graph format defined by Vogel et al. (2023b) for PFDs. Here, nodes represent unit operations, control units, raw materials, and products, while directed edges represent streams and signals. Control units that refer to a stream, such as a temperature indicator, are placed in the stream by splitting the stream into two. In addition, edges have attributes to store further stream information: If multiple outlet streams exist, we define the outlet location as top outlet (tout) or bottom outlet (bout). For heat exchangers, we identify the incoming and outgoing streams that belong to the same mass train using the edge attribute StreamID_{in/out}."}, {"title": "2.2 SFILES", "content": "An alternative information representation to graphs is the Simplified Flowsheet Input Line Entry System (SFILES). The SFILES was developed by d'Anterroches (2005) and is inspired by the Simplified Molecular Input Line Entry system (SMILES) (Weininger et al., 1989) used to encode molecular graphs to strings. The SFILES encodes the topology of a process as a sequence of tokens, such as (raw) for the raw material or (r) for a reactor. The order of the tokens is determined by following a path from the raw materials to the products along the streams. In SFILES, square brackets indicate a side-stream that diverges from the main process stream. At the place where the side-stream diverges the brackets are opened, and the tokens of that side-stream are added. When a side-stream rejoins the main stream or ends the brackets are closed and the SFILES notation continues from where the stream diverged.\nRecently Vogel et al. (2023b) introduced SFILES 2.0, which extends the original SFILES notation with control structures and topology details. Controllers are incorporated using the controller token (C) followed by a tag with the letter code (e.g., {PC}). The position of the controller token is based on its location in the flowsheet. The signal line is encoded similarly to recycles and differentiated by an underscore (_#, <_#). Topology details such as a stream leaving a column at the top are incorporated as tags in curly brackets ({tout}). By using an extra tag before a unit, the SFILES 2.0 encodes where the stream enters a unit, and similarly, by adding a tag in front of the stream leaving a unit it is encoded where the stream leaves. Independent material streams are appended to the SFILES, starting with an n|. Note that there can be multiple SFILES for the same flowsheet but there exists a unique, or canonical, way of writing them uniquely, defined by a set of rules."}, {"title": "3 Graph-to-SFILES methodology", "content": "We propose the Graph-to-SFILES model to predict the control structure of chemical processes. Figure 2 provides an overview of the model. We start with a PFD that is represented by a graph as described in Section 2.1. Given the graph, the Graph-to-SFILES model generates an SFILES that represents the CEF (Section 2.2). The Graph-to-SFILES model is inspired by the Graph2SMILES model for molecules (Tu and Coley, 2023) and is based on a transformer architecture, as shown in Figure 3. The model consists of two main components: (i) A graph encoder to encode a PFD given as a process graph and (ii) a sequence decoder that, given the encoded input, generates the CEF in the SFILES 2.0 format. In Section 3.1, we describe the graph encoder. The graph encoder aims to convert the input process graph into a continuous latent-space representation. The latent-space representation is essential for the sequence decoder to generate a suitable control structure. The sequence decoder and the corresponding decoding strategy are described in Section 3.2 and Section 3.3 respectively. Then, we provide details on the training process in Section 3.4."}, {"title": "3.1 Graph encoder", "content": "We implement the graph encoder as a GNN. GNNs are a type of neural network designed to operate on graph-structured data. The GNN iteratively aggregates information from neighboring nodes (and edges, depending on the architecture) to update the node (and edge) features. We define a sequence of GNN layers, where each layer updates the node features based on their direct neighboring nodes and edges.\nA GNN layer generally consists of two basic steps, a message passing step and an update step. During the message passing step, each node sends a message, usually a linear transformation of its feature vector, to its neighboring nodes. The message passing is followed by an update step in which the incoming messages are merged into a single vector. This step uses a permutation invariant function, e.g., sum, mean, min, or max. Information is shared across the graph by using multiple iterations of these message-passing and update steps. The linear transformations using learnable parameters in the message passing and update steps allow the learning of complex relationships between the nodes and edges, making their feature vectors more valuable for downstream tasks. In the Graph-to-SFILES model, the individual component nodes get information about their neighboring components and streams.\nWe propose and compare four different GNN architectures for the generation of CEFs from PFDs, as shown in Figure 4: (i) The Graph Attention Transformer (GATv2) (Veli\u010dkovi\u0107 et al., 2017; Brody et al., 2021), (ii) the Graph Convolution Transformer (GraphConv) (Shi et al., 2020), (iii) the Graph Transformer (Dwivedi and Bresson, 2020), and (iv) a new GNN that we propose in this study, the Combined model. All architectures utilize the attention mechanism to facilitate the message passing, where attention is the weight of the message between node i and node j, denoted as ai,j.\nFirstly, we implement the GATv2 as proposed by Veli\u010dkovi\u0107 et al. (2017) and modified by Brody et al. (2021) (Figure 4a). In the attention-based message passing block the attention $a_{i,j} \\in \\mathbb{R}$ is calculated using"}, {"title": null, "content": "$a_{i,j} = softmax \\Big(a \\space LeakyReLU ([W_{qenc}h_i||W_{kenc}h_j ||W_{eenc}e_{i,j}])\\Big)$\nThe inputs consisting of node features $h \\in \\mathbb{R}^{d_{enc}}$ and edge features $e \\in \\mathbb{R}^{d_{enc}}$ are multiplied by weight matrices $W \\in \\mathbb{R}^{d_{enc}xd_{enc}}$ and then concatenated. After applying the nonlinear function LeakyReLU, the output is multiplied by another weight matrix $a \\in \\mathbb{R}^{3d_{enc}}$ and normalized using the softmax function. Here, $d_{enc}$ is the internal encoder dimension. Note that the first encoder layer uses $W \\in \\mathbb{R}^{d_{node}xd_{enc}}$ and $W \\in \\mathbb{R}^{d_{edge}\\times d_{enc}}$ to map the node $d_{node}$ and edge $d_{edge}$ dimension to the encoder dimension. After calculating the attention between the nodes, we determine the updated node features $h'$:\n$h'_{i} = ReLU \\Big( \\sum_{j \\in N_i} a_{i,j}W_{venc}h_j \\Big)$\nFor each node i, the message from node j in Ni is given as $a_{i,j}W_{venc}h_j$ with Ni being the direct neighbors of node i including itself. In the update step, the messages are summed up, followed by a nonlinearity (ReLU).\nSecondly, we implement the GraphConv (Figure 4b). The GraphConv utilizes scaled dot-product attention, where the attention $a_{i,j}$ is given by\n$A_{i,j}=\\space softmax \\Big(\\frac{(W_{qenc}h_i)(W_{kenc}h_j + W_{eenc}e_{i,j})^T}{\\sqrt{d_{enc}}}\\Big)$.\nIn addition, the GraphConv makes use of a gated residual connection. The gated residual connection should prevent the graph encoder from over-smoothing by injecting the residual node feature hi directly in the updated node feature h'."}, {"title": null, "content": "Here, a weight $(\\Beta = f(h_i, h_j, e_{i,j}) \\in [0, 1])$ is calculated which determines the importance between the residual and the message passing. Then, the outcome of the gated residual connection is normalized using layer normalization and passed through a final nonlinearity (ReLu). This results in an updated node feature (h').\nThirdly, we implement the Graph Transformer (Figure 4c). The attention used in their model is defined as\n$A_{i,j} = softmax \\Big(\\frac{(W_{qenc} h_i)(W_{kenc} h_j)^T}{\\sqrt{d_{enc}}} W_{eenc}e_{i,j}\\Big)$\nThe unique characteristic of the Graph Transformer is the positional encoding. While the message passing step passes messages only from direct neighbors of a node, the positional encoding encodes the complete graph structure in each node feature. Dwivedi and Bresson (2020) use eigenvectors of the graph's Laplacian matrix to create a positional encoding $\\lambda \\in \\mathbb{R}^{denc}$. The Laplacian positional encoding is added to the node features in the first GNN layer that also includes a linear transformation for node ($d_{node} \\rightarrow d_{enc}$) and edge embedding ($d_{edge} \\rightarrow d_{enc}$). Furthermore, the Graph Transformer includes two residual connections after the attention-based message passing with a feed-forward neural network (FFN) between the residual connections.\nFourthly, we propose and implement the Combined model, which combines the dynamic attention mechanism from GATv2 with the positional encoding and residual connections from the Graph Transformer architecture, as shown in Figure 4d.\nSimilar to the attention used in the work of Vaswani et al. (2017), multi-head attention is used for all graph encoders to capture multiple patterns during the learning. Instead of calculating only one attention weight, m attention weights are"}, {"title": "3.2 Sequence decoder", "content": "The decoder generates the CEF in the SFILES 2.0 notation, given the PFD as an encoded process graph. We follow the original implementation of the transformer decoder by Vaswani et al. (2017), as shown in Figure 3.\nThe decoder generates an output sequence iteratively. The first token of the output sequence is always a start-of-sequence token (SOS). Then, every forward pass of the decoder adds a new token to the output sequence until the new token is an end-of-sequence token (EOS). This generation technique is called autoregression, meaning the model generates its output step-by-step, based on previously generated tokens and the input sequence. Autoregression allows the model to create novel and coherent sequences.\nBefore the output tokens are processed by the decoder, they are transformed into continuous vector representations, or embeddings, of dimension ddec using a linear transformation. The embedding is complemented by positional encoding. The decoder does not contain a function that keeps track of the sequence order, like recurrence or convolutions. To address this, the Graph-to-SFILES model uses pairs of sinusoidal functions (Vaswani et al., 2017). A positional encoding vector is calculated for every input token and added to the embedding, allowing the model to differentiate identical tokens at different positions. The resulting embedding is fed into the decoder.\nThe decoder contains multiple layers where each layer contains blocks of attention mechanisms and FFNs. First, the decoder analyzes the embedded output x \u2208 Rddec with the scaled dot-product attention mechanism as described by"}, {"title": null, "content": "$Attention(x) = softmax \\Big(\\frac{(W_{qdec}x)(W_{kdec}x)^T}{\\sqrt{d_{dec}}} W_{udec}x.\\Big)$\nSimilar to the attention in the graph encoder (Section 3.1), attention describes the importance of one token for the context of another token. The attention mechanism is enhanced by using m attention heads that calculate the attention independently. The resulting outputs of each attention head are concatenated as introduced in Section 3.1. Second, we introduce the encoded process graph to the decoder. The encoded graph is multiplied by the query matrix Wqdec and key matrix Wkdec in a further attention block. The output of the previous attention block is multiplied by the value matrix Wudec. Third, an FFN is applied to transform the output of the self-attention mechanism. Each block has a residual connection where the block's output and the residual are summed and normalized using Layer Norm. Finally, the sequence decoder's output is mapped to a probability for the next token to be added to the output sequence."}, {"title": "3.3 Decoding strategy", "content": "The decoding strategy defines how the next token of the output sequence is selected. The computationally cheapest strategy is greedy search. At every prediction step of the decoder, greedy search selects the token with the highest probability. However, selecting the token with the highest probability likely does not lead to the sequence with the highest joint probability. In addition, the coherence of the output sequence can be suboptimal, with subsequences that are not related to each other. In contrast to greedy search, calculating all possible sequences and comparing their joint probability is intractable.\nTo overcome the aforementioned limitations, we use beam search as the decoding strategy. At the first decoding step, beam search keeps track of the k tokens with the highest probability, where each token represents a beam. In the next decoding step, the model returns a probability distribution for the next token of each beam. Using this distribution, the joined probability of extending the beams is calculated. Then, the k combinations yielding the highest joined probabilities are selected and used as beams for the next decoding step. This process is repeated until all beams have reached the end-of-sequence token (<EOS>) or a maximum number of tokens. Then, the model returns the beams in order of highest joint probability, yielding top-k SFILES predictions."}, {"title": "3.4 Training", "content": "The objective function for training the Graph-to-SFILES model is the cross-entropy loss that compares the probability of the predicted next token with the ground truth token. During the training, we use teacher-forcing. Teacher-forcing is a training strategy for supervised learning, c.f., if the correct output sequence is known. At each decoding step, the model output is corrected to force the model to predict the correct output sequence for the given input sequence. The Graph-to-SFILES model is implemented in Python using the PyTorch and PyTorch Geometric libraries."}, {"title": "4 Data", "content": "We train the proposed Graph-to-SFILES model on a dataset containing 100,000 pairs of PFDs and CEFs which were synthetically generated from a collection of process patterns from literature. The patterns are combined following engineering heuristics to generate complete chemical process schematics. Note that these process schematics do not represent an actual chemical process but are similar regarding their structure. There are two versions of the dataset. First, the controllers are removed from the flowsheet to create the PFD from the CEF. Secondly, the valves were removed in addition to make the schematics more in line with a flowsheet found in simulation software like Aspen. The usage of this dataset will be indicated as without valves.\nPre-processing of the input data is needed to turn the SFILES into the graph format from Section 2.1. Firstly, we utilize the open-source SFILES 2.0 repository (https://doi.org/10.5281/zenodo. 6901932) to convert the flowsheets from the SFILES 2.0 format to a NetworkX graph. As described in Section 2.1, unit operations, valves, and controllers are represented as nodes and streams and signals as edges.\nSecondly, we convert the attributes in the NetworkX graph to node and edge features. The node attributes, specifically unit types, are encoded in a one-dimensional vector using a dictionary. The dictionary stores a relationship between the unit type and an integer value, for instance, (raw) equals 1, (hex) equals 2, etc. The edge attributes are translated into a three-dimensional vector. The first dimension encodes information from which side of a unit a stream is leaving. The second and third dimensions encode to which side of a heat exchanger a stream is connected, according to the scheme shown in Table 1. These two pre-processing steps yield the processed graph shown in Figure 5."}, {"title": "5 Results and discussion", "content": "In the following, we describe and discuss the performance of the Graph-to-SFILES model when trained on a dataset of synthetic PFD-CEF pairs. In Section 5.1, we report evaluation metrics for the Graph-to-SFILES model and compare the four model architectures. In Section 5.2, we compare the Graph-to-SFILES model and our previous sequence-to-sequence approach (Hirtreiter et al., 2023). In addition, we select an illustrative example from the test dataset and present the model predictions in Section 5.3. Finally, we discuss the results (Section 5.4)."}, {"title": "5.1 Graph-to-SFILES evaluation", "content": "We train the Graph-to-SFILES model using four different graph encoder architectures (Section 3.1) on the 1k, 10k, and 100k dataset from Hirtreiter et al. (2023). We optimize hyperparameters for each architecture using a grid search. We select the optimal hyperparameters based on the validation loss from training on the 10k dataset. The hyperparameters"}, {"title": "5.2 Comparison Graph-to-SFILES and sequence-to-sequence model", "content": "A comparison between the Graph-to-SFILES model using the Combined model encoder and the sequence-to-sequence model by Hirtreiter et al. (2023) suggests that the model performance depends on the dataset size. On the 1k dataset, we see significantly better performance when using a graph encoder instead of a sequence encoder. Table 2 shows that the CEF accuracy increases from 0.3% to 15.2% (top-1) and from 0.9% to 28.4% (top-5). By contrast, the sequence-to-sequence model reaches higher scores than the Graph-to-SFILES model in all evaluation metrics for the 100k dataset. While the sequence-to-sequence model has a CEF accuracy of 48.6% (top-1) and 89.2% (top-5), the Graph-to-SFILES model scores lower with 46.1% (top-1) and 79% (top-5). Especially the top-1 PFD reconstruction accuracy is significantly higher for the sequence-to-sequence model at 99.2% compared to the Graph-to-SFILES model at 80.5%. On the 10k dataset, we observe a similar performance between the Graph-to-SFILES and the sequence-to-sequence model. Here, the Graph-to-SFILES model reaches a higher top-1 CEF accuracy and BLEU score and the sequence-to-sequence reaches higher scores in the other evaluation metrics. Likewise, both models yield similar results when compared to a dataset where the input PFDs do not contain valves (10k without valves) as shown in Table 3."}, {"title": "5.3 Illustrative example", "content": "To illustrate the Graph-to-SFILES model performance, we predict the CEF for a sample PFD from the independent test dataset. The flowsheet, shown in Figure 6, contains three raw material streams which are mixed and fed to a reactor. The reactor has two outlets of which one is split to be partly recycled and partly compressed.\nWe employ the Combined model encoder trained on 100,000 flowsheets. From the three model samples presented in 2, we select the sample with the highest top-1 CEF accuracy. The model predicts the following five SFILES 2.0. Here, the five predictions correspond to the five beams with the highest joined probability during beam search decoding, ordered by descending joint probability."}, {"title": "5.4 Discussion", "content": "Overall, the Graph-to-SFILES model serves as a proof-of-concept to use graph data for control structure prediction. As such, the Graph-to-SFILES model is meant as a support tool for engineers and not to fully automate the control structure design. The results indicate that the Graph-to-SFILES model successfully learns node and edge features in process graphs to generate a CEF. Furthermore, the experiments suggest that a graph encoder is more data-efficient than a sequence encoder for smaller process datasets in the scale of 1,000 flowsheets. We believe that data efficiency is an important property of generative AI in process design due to the current lack of detailed process data.\nIt is important to bear in mind the limitations of the current work. Firstly, the current dataset does not include any equipment, operation, or stream parameters but solely topological information. The proof needs to be made that the Graph-to-SFILES model can learn those parameters and that they improve the model prediction. Secondly, we train our model on a synthetic dataset. In an ideal scenario, we would train our model on curated, industrial process data available at a large scale. Further efforts need to be made to collect data that are close to the final application in industrial practice, e.g., by digitizing industry data (Theisen et al., 2023) or by mining academic data and patents (Schweidtmann, 2024b; Balhorn et al., 2022). Thirdly, our Graph-to-SFILES model is a purely data-driven approach. Data-driven approaches proved to be successful for generative AI tasks if large-scale data are available, for instance for molecular design (Du et al., 2024) or language generation (ChatGPT). However, as described above, a large-scale process dataset is currently missing. Therefore, including physical knowledge in our model could potentially improve its performance. Furthermore, physical knowledge could increase safety, interpretability, and trust in the model. Process modeling and control principles could be integrated into the model as hybrid AI approaches. In consequence, we believe that overcoming the aforementioned limitations brings the vision of AI-assisted control structure design within reach."}, {"title": "6 Conclusions", "content": "This work proposes the Graph-to-SFILES model. The Graph-to-SFILES model automatically extends a process schematic with control structures using a graphical representation of the process. We propose a GNN architecture for the graph encoder and compare it with three architectures from the literature on a synthetic dataset.\nThe best-performing Graph-to-SFILES model, using a graph encoder with positional encoding, can predict the correct control structure for 46.1% of the synthetic test data. Even in cases where the model did not fully predict the correct schematics, the output tends to be a valid alternative control layout, as demonstrated in an illustrative example. However, it remains an open research question whether the Graph-to-SFILES model can learn from industry-relevant and complex process details, such as stream conditions or composition, provided as node and edge features."}]}