{"title": "PhyloGen: Language Model-Enhanced Phylogenetic Inference via Graph Structure Generation", "authors": ["Chenrui Duan", "Zelin Zang", "Siyuan Li", "Yongjie Xu", "Stan Z. Li"], "abstract": "Phylogenetic trees elucidate evolutionary relationships among species, but phylogenetic inference remains challenging due to the complexity of combining continuous (branch lengths) and discrete parameters (tree topology). Traditional Markov Chain Monte Carlo methods face slow convergence and computational burdens. Existing Variational Inference methods, which require pre-generated topologies and typically treat tree structures and branch lengths independently, may overlook critical sequence features, limiting their accuracy and flexibility. We propose PhyloGen, a novel method leveraging a pre-trained genomic language model to generate and optimize phylogenetic trees without dependence on evolutionary models or aligned sequence constraints. PhyloGen views phylogenetic inference as a conditionally constrained tree structure generation problem, jointly optimizing tree topology and branch lengths through three core modules: (i) Feature Extraction, (ii) PhyloTree Construction, and (iii) PhyloTree Structure Modeling. Meanwhile, we introduce a Scoring Function to guide the model towards a more stable gradient descent. We demonstrate the effectiveness and robustness of PhyloGen on eight real-world benchmark datasets. Visualization results confirm PhyloGen provides deeper insights into phylogenetic relationships.", "sections": [{"title": "1 Introduction", "content": "Phylogenetic trees [42] (or evolutionary trees) are tree-structured graphs representing kinship relationships between species [11], where each leaf node represents a distinct species and internal nodes represent evolutionary bifurcations. The tree's topology reflects evolutionary relationships based on their genetic characteristics, and branch lengths indicate evolutionary distances. Phylogenetics study is the foundation of evolutionary synthetic biology and is critical for tracking the evolutionary trajectories of species, analyzing the transmission pathways of newly discovered viruses, and providing meaningful insights for clinical applications [25, 13, 33].\nBackground. Despite DNA sequencing technologies [32] allowing us to construct evolutionary relationships based on molecular properties (DNA, RNA, and proteins), phylogenetic inference remains challenging due to its complex parameter space, which encompasses both continuous (branch lengths) and discrete (tree topology) components. Traditional MCMC-based methods like MrBayes [29] and RevBayes [9], despite their ability to extensively explore the huge tree space, are hindered by slow convergence rates. Additionally, the number of possible tree topologies for n species grows factorially as (2n -5)!! for n \u2265 3, posing huge computational challenges. In contrast, VI-based methods leverage approximate inference to provide more efficient estimations. Depending on the data type and research objectives, these methods can be further categorized into tree representation learning and tree structure generation. Tree Representation Learning, as shown in Fig. 1(a),"}, {"title": "2 Related Works", "content": "MCMC-based methods, such as MrBayes [29] and RevBayes [9], have been widely used for phylogenetic inference due to their ability to explore vast tree spaces.\nVI-based methods offer a more efficient alternative to MCMC by leveraging approximate inference techniques. These methods can be categorized into two main approaches: structure representation and structure generation. Tree Representation Learning. This approach focuses on extracting information from existing tree structures. SBNs [45] capture the relationships between existing subsplits without addressing branch lengths. VBPI [46] and its variants VBPI-NF [43] and VBPI- GNN [44]: These methods introduce a two-pass approach to learn node representations, including"}, {"title": "3 Methods", "content": "Notation. The phylogenetic tree is denoted as (\u0442, B\u2081), where 7 is an unrooted binary tree topology reflecting evolutionary relationships among N species. B, denotes the non-negative evolutionary distances of each branch. The tree consists of N leaf nodes, each corresponding to a species, and several internal nodes. PhyloGen aims to generate tree topology and branch lengths directly from the raw sequences.\nFramework. We model the phylogenetic tree inference problem as a tree structure generation task under conditional constraints, consisting of three main modules as shown in Fig. 2. Feature Extraction module extracts genome embeddings from raw sequences via a pre-trained language model. PhyloTree Construction module uses these embeddings to generate an initial tree structure using a tree construction algorithm, introducing the latent variable z* to represent the tree topology. PhyloTree Structural Modeling module iteratively refines the tree structure and branch lengths through topology learning and branch length learning components. By integrating these modules, we transform the complex dynamics of evolutionary history into a tree-based learning framework, facilitating a deeper understanding of phylogenetic relationships."}, {"title": "A. Feature Extraction", "content": "We utilize DNABERT2 [49], a genome language model, to transform molecular (DNA) sequences Y = {y\u1d62}\u1d62=\u2081\u1d3a into genomic embeddings E = {e\u1d62}\u1d62=\u2081\u1d3a. These embeddings discern complex patterns and long-range dependencies, serving as the basis for generating the latent variables z*, which dynamically inform both the topology and the branch lengths. By introducing DNABERT2, we redefine the construction of phylogenetic trees as a continuous optimization problem within a biologically meaningful embedding space."}, {"title": "B. PhyloTree Construction", "content": "To construct the phylogenetic tree, genomic embeddings E are input into an MLP network to derive the parameters \u03bcand o of the latent space, representing topological embeddings z*. This latent space effectively captures the evolutionary relationships that guide the subsequent tree structure generation process. The latent variable is then generated using the reparameterization trick [15]: z* = \u03bc + \u03c3\u2299\u03b5, \u03b5 ~ N(0, I). Then, distance matrix D is computed"}, {"title": "C. PhyloTree Structure Modeling", "content": "The purpose of the tree structure modeling module is to jointly optimize both the phylogenetic tree's discrete topology and continuous branch lengths."}, {"title": "C.1. Topology Learning", "content": "The first component involves a TreeEncoder R(z|r(z*)) and a TreeDecoder Q(T(z)|z) to learn the tree topology. Concretely, the encoder R receives an initial tree topology T(z*) and genomic embeddings E from DNABERT2 as inputs, conditions the latent state z to refine topological embeddings in a continuous space. We introduce variational inference to enhance the quality of the tree structure and adapt to data uncertainty and complexity by minimizing the Kullback-Leibler (KL) divergence. Additionally, the encoder R acts as a regularization mechanism, reducing overfitting and enhancing gradient stability. To refine the topology, the decoder Q samples from the probability distribution parameterized by the encoder's latent state. This process optimizes model parameters by minimizing the KL divergence between the true distribution P(\u315c(z)) = P(y|T(z), B\u2081) and the variational distribution Q(t(z)) = q(t(z), B7). To facilitate backpropagation through stochastic nodes, the latent variable z is sampled using the reparameterization trick: z = \u03bc + \u03c3\u2299\u03b5, where \u025b ~ N (0, I) introduces controlled stochasticity while maintaining differentiability, ensuring that z effectively captures refined topological embeddings."}, {"title": "C.2. Branch Length (Blens) Learning", "content": "The second component utilizes the inferred topology \u0442 and genomic embeddings E from DNABERT2 to generate and adjust branch lengths."}, {"title": "Step1: Node Feature via Linear-Time Dual-Pass Traversal", "content": "We utilize a linear time O(n) dual-pass traversal method, combining postorder (bottom-up) and preorder (top-down) strategies, guaranteeing each node is processed only once.\nPostorder Traversal (bottom-up) aggregates information from leaf nodes toward the root node.\nc\u1d62 = max(1, \u039a \u2013 \u03a3\u2c7c)\u207b\u00b9, (1)\nj\u2208ch(i)\nwhere ci is the scaling factor for node i, influenced by the node's connectivity, and K = 3 due to binary tree properties.\nf\u1d62 = C\u1d62\u03a3f\u2c7c + e\u1d62, (2)\nj\u2208ch(i)\nwhere fi incorporates contributions from children nodes fj and its own initial feature e\u1d62 from a pre-trained genome model. Initially, c\u1d62 = 0, f\u1d62 = e\u1d62.\nPreorder Traversal (top-down) propagates information from the root node toward the leaf nodes, enhancing each node's feature xi: X\u1d62 = C\u1d62e\u209a[\u1d62] + f\u1d62, (3)\nwhere ep[i] is the feature of node i's parent."}, {"title": "Step2: Feature Enhancement with Dynamic Graph Convolution (DGCNN)", "content": "We use the edge convolutional layer of DGCNN [35] to enhance node features. This approach captures both local interactions (between neighboring nodes) and global structural dependencies (reflecting the entire tree structure) within the phylogenetic tree. For layer L, inputs {x\u1d62 \u2208 \u211d\u1da0}\u1d62=\u2081\u1d3a and outputs {x\u1d62\u1d38\u207a\u00b9 \u2208 \u211d\u1da0'}\u1d62=\u2081\u1d3a have feature dimensions F = 768 and F' = 100. The transformations are:\nx\u1d62\u1d38\u207a\u00b9 = DGCNN(x\u1d62\u1d38), m\u1d62\u2c7c = h(x\u1d62\u1d38, x\u2c7c\u1d38),\nwhere h : \u211d\u1da0 \u00d7 \u211d\u1da0 \u2192 \u211d\u1da0' is an MLP-shared asymmetric edge function.\nTopology-Invariance Property. Despite the non-uniqueness of 2D coordinates due to infinite possible translational rotations, the distances between species remain constant. We define this as topological invariance: d\u00b2 = ||zi - zj||\u00b2, where zi and zj are coordinates in the hidden space, maintaining accurate topological relationships. The function h(xi, xj) incorporates global and local features through: h(xi, xj) = h(xi, Xi - xj, dij)."}, {"title": "Aggregation of Features", "content": "The node features are aggregated using a MAX operation across all edges:\nK\nx\u1d62\u1d38\u207a\u00b9 = \u2211 m\u2096\u1d38 = m\u2081\u1d38  m\u2082\u1d38 ...  m\u1d62\u2096 \u1d38 , (4)\nk=1\nwhere denotes the MAX aggregation function, focusing on the most significant features."}, {"title": "Step3: Reparametrization of Branch Length", "content": "Node features x\u0142 obtained from edge convolution layers are further fed into the M network, parameterizing mean and log-variance for branch lengths: \u03bc, log(\u03c3\u00b2) = MLP2(h\u1d62\u1d38). Concretely, the M network is defined as:\nh\u1d62\u207d\u00b9\u207e = MLP1(x\u1d62\u1d38), h\u1d62 = MAX{h\u2c7c\u207d\u00b9\u207e : j\u2208N(i)}\u222ah\u1d62\u207d\u00b9\u207e, (5)\nwhere hi are node features processed through an additional MLP1 to capture inter-node interactions. Branch lengths are updated by reparametrization b = exp(\u03bc + exp(\u03c3) * rvs), where rvs ~ N(0, I), ensuring differentiability and capturing the probabilistic nature of estimates. Throughout, the prior P(B+) is assumed exponential, while the posterior Q(B+), learned via a graph neural network, reflects inferred tree topologies and node characteristics."}, {"title": "3.1 Scoring Function", "content": "To address the convergence challenges often associated with the ELBO in VAE models, we incorporate a scoring function S, implemented via an MLP network. This function assesses each leaf node in the latent space z and provides additional gradient information, facilitating more efficient learning and convergence.\nDuring training, S and ELBO form a joint optimization objective, optimizing gradient directions to improve overall performance. Fig. 3 compares the convergence behaviors and stability of S and ELBO throughout the training process. The horizontal axis represents the training steps, and the vertical axis represents the two metric values. The closer the S curve is to the ELBO curve, the more it proves that S can effectively evaluate the model performance and maintain a consistent optimization trend with ELBO. Different configurations of S, including those with Fully Connected layers (w/ FC), with two layers MLP (w/ MLP-2), and with three layers MLP (w/ MLP-3), demonstrate similar trends, closely following the ELBO curve. After an initial period of rapid change, all metrics stabilize and exhibit minor fluctuations, demonstrating robustness in convergence. What's more, the number of layers in MLP has less impact on performance."}, {"title": "3.2 Learning Objectives", "content": "As discussed in Appendix Background A, our primary goal is to maximize the expected marginal likelihood of the observed species sequence Y via max log p(Y|(\u03c4(z), B7)). The posterior distribution as p(\u03c4(z), B4|Y) is difficult to infer directly, so we utilize variational inference to approximate it as q(t(z), B+ Y). The detailed deviation is in Appendix D.2.\nTo minimize the KL divergence between the true prior and the approximate posterior distributions, we start from the joint probability distribution: p(Y, \u03c4(z), B\u30f6) = p(Y|\u03c4(z), B+)p(B\u2081|\u315c(z))p(t(z)),"}, {"title": "3.3 End-to-End Learning via Stochastic Gradient Descent", "content": "We derive the gradients of model parameters @ as follows:\n\u2207@L = EQ\u03b8(z*) [\u2207@logQ\u03b8(z*) EQ(B+|\u03c4(z)) [log P(Y,B+|\u03c4(z))]+\nQ(BT(Z))\nlog P(T(z))R(z|\u315c(z*))] +\u2207H[Qo(z*)], (10)\nwhere H[Qe(z*)] is the entropy of Qe(z*). The derivation of the model parameters \u03c6:\n\u2207\u03c6L = \u2207 \u03c6EQa(z*) [EQ\u2084(B+\\7(2)) log P(Y, B P(Y, B+ | T(2)))]. (11)\nQ(BT(z)))\nThe derivation of the model parameters \u03c8:\n\u2207\u2084L = EQo(z*)[\u2207y log (R\u2084(z|t(z))]. (12)"}, {"title": "4 Experiments", "content": "In this section, we conduct experiments to demonstrate the effectiveness of our proposed PhyloGen. We aim to answer seven research questions as follows:\nRQ1: How effective is PhyloGen in generating tree structures under the benchmark datasets?\nRQ2: How diverse are the tree topologies generated by PhyloGen?\nRQ3: How consistent is the PhyloGen-generated tree structure compared to the MrBayes method?\nRQ4: How robust is PhyloGen to species sequences?\nRQ5: How does each PhyloGen's module affect its performance?\nRQ6: What evolutionary relationships between species does PhyloGen learn?\nRQ7: How do key hyper-parameters affect PhyloGen's performance?"}, {"title": "4.1 Experiment Setup", "content": "Tasks and Datasets. We evaluate the performance of PhyloGen on the Variational Bayesian Phylogenetic Inference task with Evidence Lower Bound (ELBO) and Marginal Log Likelihood (MLL) as metrics on eight benchmark datasets (see Appendix C).\nBaselines. We compared PhyloGen against three categories of methods: MCMC-based methods like MrBayes and SBN. Structure Representation methods, including VBPI and VBPI-GNN, use pre-generated tree topologies in training that have the potential to achieve high likelihoods, and thus, their training and inference are restricted to a small space of tree topologies and thus are not directly comparable. Structure Generation methods, which are PhyloTree Structure Generation tasks that perform approximate Bayesian inference without pre-selecting topologies. For additional experimental details, including training details, baselines, architectures, and hyperparameters, the interested reader is referred to Appendix E."}, {"title": "4.2 Performance evaluation across eight benchmark datasets (RQ1)", "content": "Table 1: Comparison of the MLL (\u2191) with different approaches in eight benchmark datasets. VBPI and VBPI-GNN use pre-generated tree topologies in training and thus are not directly comparable.\nMethods Dataset DS1 DS2 DS3 DS4 DS5 DS6 DS7 DS8\n#Taxa (N) 27 29 36 41 50 50 59 64"}, {"title": "4.3 Tree Topological Diversity Analysis (RQ2)", "content": "To evaluate the topological diversity of trees generated by PhyloGen on DS1, we use three metrics: Simpson's Diversity Index [5], Top Frequency, and Top 95% Frequency, as detailed in Tab. 3. A higher Diversity Index, which approaches 1, suggests broad diversity among generated tree topologies. The lower Top Frequency suggests a balanced distribution, preventing single tree structures from being overly dominant. Furthermore, the presence of 149 distinct topologies within the Top 95% Frequency underscores PhyloGen's ability to generate a diverse range of topologies."}, {"title": "4.4 Bipartition Frequency Distribution (RQ3)", "content": "Fig. 5 shows the bipartition frequency distributions of trees inferred by PhyloGen for datasets DS1, DS2, and DS3. The horizontal axis indicates the ranking of the bipartitions in the tree topology, and the vertical axis indicates the normalized frequency of occurrence of the corresponding bipartitions. The similarity of our method's curves to those of MrBayes underscores its accuracy, demonstrating that PhyloGen consistently captures evolutionary patterns with reliability comparable to the gold standard. This indicates a robust validation of PhyloGen's phylogenetic inference capabilities. More detailed information is provided in Appendix E.3."}, {"title": "4.5 Robustness Assessment (RQ4)", "content": "To assess our model's robustness, we test its adaptability to data changes by modifying the number of nodes in the DS1 dataset, which initially contained 27 species sequences. Specifically, we conduct two experiments: Setting 1: randomly deleting 4 nodes to simulate the impact of data incompleteness and potential information loss, and Setting 2: randomly adding 4 nodes to simulate an increase in data size. As shown in Tab. 4, our model and its variants exhibit significant stability and adaptability under both cases. Changes in ELBO and MLL metrics are represented by A values, where positive changes indicate improved performance and negative changes indicate decreased performance. Specifically, smaller positive increases after node deletion (Setting 1) and node addition (Setting 2) emphasize the model's ability to adapt to changes in data structure effectively. In contrast, larger negative decreases highlight challenges when adjusting to increased data complexity. Furthermore, our model exhibits"}, {"title": "4.6 Ablation Study (RQ5)", "content": "Tab. 5 shows the performance of our model compared with the removal of the KL loss and the Scoring Function S, respectively. Ours performs best on ELBO and MLL metrics, with a significant decrease in performance after the removal of the KL loss, suggesting that the KL loss plays a key role in regularizing the model and avoiding overfitting. While removing the S module had a large impact on MLL, the ELBO impact was relatively small, indicating that the impact of the S module is more complex and may be related to specific feature extraction functions. Our future work will explore further enhancements to the S module and investigate other regularization techniques to refine the model's performance."}, {"title": "4.7 Case Study of PhyloTree Structure (RQ6)", "content": "Fig. 7 shows a phylogenetic tree constructed from DS1 dataset, where each leaf node represents a specific species, and the text next to the node indicates the species name. The branch lengths reflect the genetic distances, with shorter branches indicating recent evolutionary history and longer branches indicating greater genetic differences. The phylogenetic tree shown in Fig. 7 places Siren intermedia and Trachemys scripta, both aquatic organisms, on adjacent branches, reflecting our model could capture their adaptive evolutionary information to aquatic environments. Meanwhile, the reptilian species Heterodon platyrhinos and Trachemys scripta are also on neighbouring branches, suggesting a relatively recent common ancestor compared to other amphibian and reptilian species. Notably, our method does not require padding the sequences to a unified length, which effectively reflects actual sequence variation. For a detailed species sequence and topology comparison, please refer to Appendix E.6."}, {"title": "5 Conclusion", "content": "Contributions In this study, we introduced PhyloGen, a novel approach leveraging pre-trained genomic language models to enhance phylogenetic tree inference through graph structure generation. By addressing the limitations of traditional MCMC and existing VI methods, PhyloGen jointly optimizes tree topology and branch lengths without relying on evolutionary models or equal-length sequence constraints. PhyloGen views phylogenetic tree inference as a conditionally constrained tree structure generation problem, jointly optimizing tree topology and branch lengths through three core modules: (i) Feature Extraction, (ii) PhyloTree Construction, and (iii) PhyloTree Structure Modeling. These modules map species sequences into a continuous geometric space, refine the tree topology and branch lengths, and maintain topological invariance. Our method demonstrated superior performance and robustness across multiple real-world datasets, providing deeper insights into phylogenetic relationships.\nLimitations and Future Works While our model demonstrates outstanding performance on standard benchmarks, it may benefit from using more expressive distributions or incorporating prior constraints to better capture complex dependencies and interactions in the latent space. Additionally, although the Neighbor-Joining algorithm is effective for iterative tree construction, it is computationally intensive. We are exploring efficient data structures and parallel processing techniques to address this bottleneck. Furthermore, our model has primarily been applied to genomic data, and further research is needed to extend its applicability to diverse biological data, such as protein and single-cell data."}, {"title": "A Background", "content": ""}, {"title": "A.1 Graph Structure Generation (GSG)", "content": "Let G = (A, X) denote a graph, where A \u2208 \u211d\u1d3a\u00d7\u1d3a is the adjacency matrix and X \u2208 \u211d\u1d3a\u00d7\u1da0 is the node feature matrix with xi \u2208 \u211d\u1da0 being the embedding of node vi. Given a feature matrix X, the target of GSG is to directly learn a graph structure A* by jointly optimizing the graph structure and Graph Neural Networks (GNN) [19, 1, 3].\nFeature Extraction To better model the similarity of node pairs, a feature extractor is usually needed to map node features from a high-dimensional input space to a low-dimensional embedding space [40].\nGraph Construction A similarity metric function is generally used to compute the similarity between embedding pairs as edge weights. There are several ways to construct a sparse adjacency matrix from a fully connected similarity matrix. For instance, we can create a graph that selects only connected node pairs whose similarity exceeds some predetermined threshold. In addition, we can connect the k nearest nodes to one node, thus constructing a k-nearest neighbor (kNN) [28].\nGraph Structure modeling The core of GSG is the structure learner, which models edge connectivity to refine the preliminary graph [12]. Metric-based and neural network-based approaches are generally employed to learn edge connectivity through parameterized networks to receive node representations and regenerate the adjacency matrix that optimizes the graph structure, which is able to reveal the real connectivity relationship between nodes better and can be widely used in various downstream tasks [26]."}, {"title": "A.2 Variational Inference (VI)", "content": "Variational Autoencoders (VAE) [15] is a deep generative models that learn the distribution of input data by encoding it into a latent space. In this process, the encoder maps each input x to a latent space defined by parameters: mean \u00b5 and variance \u03c3. Latent variables z are then sampled from this distribution for data generation.\nVI is employed within VAE to handle the computational challenges of estimating marginal likelihoods of observed data. This involves computing the log of the marginal likelihood:\nmax log p\u03b8 (X) N ()= \u2211log/p\u03b8 (X,Z)dz (13)\ni=1 Z\nwhere pe (X, Z) represents the joint distribution of the observable data x e.g. a Gaussian distribution, \u039d(x|\u03bc, \u03c3) and its latent encoding Z under the model parameter 0.\nSince the direct estimation of marginal likelihoods is typically infeasible, VI introduces a variational distribution q(z|x) to approximate the true posterior. The goal of VI is to maximize the Evidence Lower Bound (ELBO), formulated as:\nELBO = Eqq (z|x) [logpo(x|z)] \u2013 KL[q\u00a2(z|x)||p(z)] (14)\nThe first term is the reconstruction log-likelihood, log pe (x|z) can be considered as a decoder, i.e., the log-likelihood between the reconstructed data and the original data given the potential representation. The second term, the KL divergence, quantifies the difference between the variational posterior q4(z|x) and the latent prior p(z). Usually, VAE utilizes a reparameterization trick for gradient backpropagation through non-differentiable sampling operations. Once trained, VAEs can generate new data by directly sampling from the latent space and processing it through the decoder."}, {"title": "B Related Work", "content": "MCMC-based methods, such as MrBayes [29] and RevBayes [9], have been widely used for phylogenetic inference due to their ability to explore vast tree spaces. However, these methods are limited by the high-dimensional search space and the combinatorial explosion of tree topologies, which makes estimating posterior probabilities using simple sample relative frequencies (SRF) problematic [8]. The accuracy of these methods is compromised in unsampled tree spaces, and they"}, {"title": "C Datasets", "content": "Our model PhyloGen performs phylogenetic inference on biological sequence datasets of 27 to 64 species compiled in [17]. Notably, our approach does not require sequences to be of equal length, overcoming a common limitation in traditional phylogenetic analysis. In Tab. 6, we summarize the statistics of benchmark datasets."}, {"title": "D Methods", "content": ""}, {"title": "D.1 Scoring Function", "content": "To address the convergence challenges often associated with the ELBO in VAE models, we incorporate a scoring function S, implemented via an MLP network. This function assesses each leaf node in the latent space z and provides additional gradient information, facilitating more efficient learning and"}, {"title": "D.2 Gradient Derivation for Objective Function", "content": "It is important to emphasise that we no longer distinguish between z* and z in the derivation for simplicity and uniformly use the latent variable z to denote the topology.\nIn variational inference, we typically start from the joint probability distribution:\np(Y, \u03b8) = p(Y|\u03b8)p(\u03b8) (15)\nwhere @ could be any set of parameters or latent variables, and Y is the observed data.\nFor a tree model, assume that @ includes the tree topology T(z) and the branch lengths B\u012b. Thus, we have:\np(Y, \u03c4(z), B\u2081) = p(Y|\u03c4(z), B-)p(\u03c4(z), B\u30f6) (16)\nwhere p(Y|\u03c4(z), B\u30f6) represents the conditional probability of the observed data Y, given the tree structure and branch lengths, while p(\u03c4(z), B\u30f6) is the joint prior probability of the tree structure and branch lengths.\nFurther assuming that the tree topology (z) and the branch lengths B, are conditionally independent, the prior can be decomposed as:\np(\u03c4(z), B\u2081) = p(B\u2081|\u03c4(z))p(t(z)) (17)"}, {"title": "D.3 Algorithm", "content": "PhyloGen's algorithm flow is as shown in Algorithm 1."}, {"title": "E Experiment", "content": ""}, {"title": "E.1 Training Details", "content": "We focus on the most challenging aspect of the phylogenetic tree inference task: the joint learning of tree topologies and branch lengths. For this, we employ a uniform prior for the tree topology and an independent and identically distributed (i.i.d.) exponential prior (Exp(10)) for the branch lengths. We evaluate all methods across eight real datasets (DS1-8) frequently used to benchmark phylogenetic tree inference methods. These datasets include sequences from 27 to 64 eukaryote species, each comprising 378 to 2520 sites. Notably, our approach does not require sequences to be of equal length, thus overcoming a common limitation in traditional phylogenetic analysis. For our Monte Carlo simulations, we select K = 2 samples and apply an annealed unnormalized posterior during each i-th iteration, where \u03bb\u03b7 = min{1.0, 0.001 + i/H} acts as the inverse temperature. This parameter starts at 0.001 and gradually increases to 1 over H iterations, effectively simulating a cooling schedule commonly used in annealing algorithms, similar to the approach in [45], with an initial temperature of 0.001, which gradually decreases over 100,000 steps.\nDuring the model training process, we utilize stochastic gradient descent to process a total of one million Monte Carlo samples, employing K samples at each training step. The stepping-stone (SS) algorithm [37] in MrBayes is viewed as the gold-standard value. All models were implemented in Pytorch [27] with the Adam optimizer [14]. The MLL estimate is derived by sampling the importance of 1000 samples, with the larger mean value being better. The learning rate is initially set to 1e-4 and is reduced by a factor of 0.75 every 200,000 training steps. Momentum is set at 0.9 to prevent the optimization process from becoming trapped in local minima. Utilizing the StepLR scheduler, the current learning rate is multiplied by 0.75 every 200,000 steps to ensure steady progression, detailed in Tab. 7 and Tab. 8."}, {"title": "E.2 Baselines", "content": "We mainly compare PhyloGen with three types of methods, including two MCMC-based methods (i.e., MrBayes, SBN), two tree-structure representation learning methods (i.e., VBPI, VBPI-GNN), and five tree-structure generation methods. It should be noted that the results of all baseline methods are not included in the MLL tables, as some of the baseline methods are not provided with source code, and the results of the MLL metrics are not shown in the original paper."}, {"title": "E.3 Bipartition Frequency Distribution (RQ3)", "content": "A bipartition frequency comparison plot is used in evolutionary analysis to show the bipartition frequencies observed in a sample of tree topologies. Each bipartition represents a node in the tree and defines the division of two sets of taxonomic units (e.g., species) on either side of that node. The frequency of these bipartitions in the posterior tree topology distribution reflects how often each topology appeared during the MCMC sampling process, which in turn reveals the confidence level of the different topological features."}, {"title": "E.4 Ablation Study (RQ5)", "content": "Tab. 5 shows the performance of our model compared with the removal of the KL loss and the Scoring Function S, respectively. ours performs best on both the ELBO and MLL metrics, with a significant decrease in performance after the removal of the KL loss, suggesting that the KL loss plays a key role in regularizing the model and avoiding overfitting. While removing the S module had a large impact on MLL, the ELBO impact was relatively small, indicating that the impact of the S module is more complex and may be related to specific feature extraction functions. Our future work will explore further enhancements to the S module and investigate other regularization techniques to refine the model's performance."}, {"title": "E.5 Hyper-Parameter Analysis (RQ7)", "content": "Table 9: Hyperparameter Analysis of PhyloGen Performance in Various Parameter Configurations."}, {"title": "E.6 Visualizations", "content": ""}, {"title": "F Broader Impacts", "content": "We recognise the importance of addressing the societal impact of our work. Phylogenetic inference methods such as the one we propose have the potential to greatly improve our understanding of the evolution, origin, and transmission mechanisms of viruses and bacteria. Such an understanding could have far-reaching societal implications, especially in the fields of public health and disease control."}]}