{"title": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach", "authors": ["Shijian Deng", "Wentian Zhao", "Yu-Jhe Li", "Kun Wan", "Daniel Miranda", "Ajinkya Kale", "Yapeng Tian"], "abstract": "Self-improvement in multimodal large language models (MLLMs) is crucial for enhancing their reliability and robustness. However, current methods often rely heavily on MLLMs themselves as judges, leading to high computational costs and potential pitfalls like reward hacking and model collapse. This paper introduces a novel, model-level judge-free self-improvement framework. Our approach employs a controlled feedback mechanism while eliminating the need for MLLMs in the verification loop. We generate preference learning pairs using a controllable hallucination mechanism and optimize data quality by leveraging lightweight, contrastive language-image encoders to evaluate and reverse pairs when necessary. Evaluations across public benchmarks and our newly introduced IC dataset-designed to challenge hallucination control-demonstrate that our model outperforms conventional techniques. We achieve superior precision and recall with significantly lower computational demands. This method offers an efficient pathway to scalable self-improvement in MLLMs, balancing performance gains with reduced resource requirements.", "sections": [{"title": "1. Introduction", "content": "Self-improvement is a natural way for humans to learn independently, enabling them to acquire knowledge and skills beyond what they learn from their teachers. This same paradigm is being gradually adapted for large language models (LLMs) and multi-modal large language models (MLLMs) to achieve performance improvements beyond the seed model with minimal human supervision.\nRecent studies have explored various approaches [5, 7, 26, 30] to self-improvement in MLLMs. For instance, RLAIF-V [26] uses MLLMs to evaluate and score responses generated by another MLLM, creating preference learning pairs from responses to the same image and question. M3ID [7], POVID [30], and STIC [5] employ techniques like bad prompts, image corruption, unconditioned generation, and response injection to generate hallucinated responses as negative samples for preference learning.\nHowever, several issues limit this paradigm: 1) it relies heavily on the quality of the verifier (e.g., a reward model); 2) the process can be resource-intensive, generating numerous samples but only using a tiny subset; 3) the cost multiplies when another large model is needed for verification, especially when generating reasoning or comments for final evaluation. Past studies [22, 23] have underscored the necessity of an external verifier.\nTo overcome these challenges, we propose an alternative approach, illustrated in Fig 1, enabling self-improvement without directly using an MLLM as a verifier for dataset filtering. Our method involves controlled hallucination to generate preference-learning pairs, lightweight evaluation with a contrastive language-image encoder to optimize data quality, and direct preference optimization (DPO) [18] to train the seed model.\nFirst, we use an efficient, controllable approach to generate simple negative or hard-negative samples, creating the initial preference-learning pairs. We employ a controller ranging from 0 to 1 to control the level of hallucination in responses. After generating the initial dataset, we leverage a lightweight, contrastive language-image pretrained encoder to compute average sentence-level CLIP-Score [8]. This score identifies and updates pairs where the negative sample scores higher than the positive, refining our preference-learning dataset. Finally, we use the optimized dataset to train the seed model via DPO [18], producing a self-improved model. Extensive evaluations on both in-house and public benchmarks show significant gains over the original seed model.\nOur primary contributions are as follows:\n\u2022 We propose a novel and efficient framework for self-improvement in MLLMs that: (a) combines a predefined, controllable mechanism for efficient negative sample generation, and (b) uses a lightweight verifier to effectively control positive and negative pairs, automatically reversing them when necessary.\n\u2022 We collected a new IC dataset, which includes GPT-40-assisted evaluation both precision and recall of MLLMs.\n\u2022 Experimental results demonstrate that we can significantly better performance over the seed model on both our IC and Object HalBench datasets."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Multimodal Large Language Models", "content": "To leverage the knowledge and reasoning capabilities of LLMs in multimodal settings and address broad multimodal comprehension challenges, MLLMs have been developed. Significant work has been done in this field, such as LLaVA [15], which connects CLIP with the LLaMA model through an adapter; Qwen-VL [3], which implements grounding and text-reading abilities by aligning image-caption-box tuples; CogVLM [24], which uses a trainable visual expert module in the attention and FFN layers to enable deep fusion of vision-language features without sacrificing NLP task performance; InternVL [4], which employs both contrastive and generative tasks to better align the large-scale vision foundation model with MLLM; Pixtral [1], which processes images through the vision encoder at their native resolution and aspect ratio, converting them into image tokens for each patch in the image, allowing it to handle any number of images of arbitrary sizes in its large context window; and LLaMA3.2 Vision, which incorporates visual-recognition capabilities into LLaMA 3 [6] via a compositional approach to ensure that text-only task performance is not affected by the addition of visual-recognition capabilities."}, {"title": "2.2. Self-Improvement", "content": "Even after large-scale pretraining, instruction tuning, and reinforcement learning from human feedback (RLHF) [17], large models may still show vulnerabilities in various cases. Although new data can always be prepared to improve a specific missing capability of the model, this is not a sustainable long-term solution to fix all issues at once. To enhance a large model's helpfulness and trustworthiness without exhausting human effort, a new self-improvement paradigm has been adopted, as systematically discussed in the survey [21]. For MLLMs, this often involves two key steps: sampling and verification.\nSampling. To improve the seed model's performance, the first step is to sample the necessary data. The simplest approach is to change seeds and randomly sample a large number of outputs, though this may not be efficient. Instead, users can predefine the type of data to generate by employing improved prompts and chains of thought to produce high-quality data, or by using corrupted images, attention masks, and text to generate negative data, as explored in POVID [30], STIC [5], and BDHS [2]. In M3ID [7], the authors also use mutual information from information theory to better control the quality of generated outputs and therefore achieve more effective sampling. In our work, we further simplify this sampling approach, making the process even more straightforward and practical.\nVerification. The model would not significantly improve if it simply reuses any generated data for retraining. A more effective approach is to perform data selection before training. There are many ways to achieve this. The simplest method is majority voting, though this may fail when the correct output is not the most common. A verifier, while optional, is commonly used as an additional quality control layer for data. The most straightforward and widely used verification method is to use an MLLM as a reward model, as seen in RLAIF-V [26]. However, this approach has limitations related to cost and potential bias due to the reward models' own limitations. An external verifier can help address these issues. For example, CLIP-DPO [16] utilizes CLIP to rank short descriptions generated by the MLLM. We adopted a similar approach and extended it to suit long captions, seamlessly integrating it into our self-improvement framework along with our sampling methods to further enhance the robustness of our pipeline."}, {"title": "3. Method", "content": "This section describes our approach to efficient self-improvement in MLLMs. We begin with a brief overview of DPO, followed by a description of our controllable method for generating positive and negative data pairs for training. Next, we highlight the importance of incorporating a lightweight quality control mechanism to ensure that the generated data effectively guides the learning process. Finally, we explain how the generated data is used to train the seed model with DPO, culminating in a self-improved model."}, {"title": "3.1. Preliminaries: DPO", "content": "DPO has recently emerged as a popular method for preference learning in large language models due to its simplicity compared to reinforcement learning-based techniques like RLHF and RLAIF. Unlike RLHF/RLAIF, which uses reinforcement learning to optimize a policy, DPO frames preference learning as a supervised learning task.\nGiven a dataset of preference pairs $(x, y_w, y_l)$, where $y_w$ is preferred over $y_l$ for input x, DPO directly optimizes model parameters $\\theta$ to maximize the probability of preferred outputs and minimize the probability of dispreferred outputs relative to a reference model $\\pi_0$. The objective is often defined with a negative log-likelihood loss:\n$\\mathcal{L}(\\theta) = - \\log \\sigma (\\Delta(x, y_w, y_l; \\theta))$, (1)\nwhere\n$\\Delta(x, y_w, y_l; \\theta) = [\\log \\pi_{\\theta}(y_w | x) - \\log \\pi_0(y_l | x)] - [\\log \\pi_0(y_w | x) - \\log \\pi_0(y_l | x)]$. (2)\nHere, $\\pi_{\\theta}$ represents the model parameterized by $\\theta$, $\\pi_0$ is the initial policy, x is the input text prompt along with the image, and $\\sigma$ is the sigmoid function.\nThis loss function encourages the model to assign higher probabilities to preferred responses and lower probabilities to dispreferred responses while staying close to the reference model. This approach avoids the complexity and instability of reinforcement learning, simplifying training and enhancing convergence.\nIn this work, we adopt DPO to optimize our model's alignment with preference data efficiently generated and filtered by our framework for self-improvement."}, {"title": "3.2. Motivation", "content": "To perform well, preference learning requires diverse data and accurate preference labels for each pair, making it critical to establish a fully controllable approach for generating the required dataset. While it is challenging to produce data that surpasses the quality of what the seed model can generate, it is relatively feasible to create data that is worse than what the model can typically produce. It is also important to know how much worse the sample we need before we generate it since both too hard or too simple pairs may not work the best. Based on these observations, we propose a simple yet efficient method for generating preference learning data pairs with any difference level between positive samples and negative samples.\nThe high computational cost of running models with a large number of parameters, combined with the inherent inductive biases of MLLMs, imposes significant limitations on relying on large models as verifiers. To address these challenges, we introduce an objective and lightweight alternative for verification purposes."}, {"title": "3.3. Controllable Dataset Generation", "content": "To train a self-improving model with preference learning, we first need to prepare a suitable dataset. To generate preference pairs, we use the seed model $m_0$ to create a positive response $y_w$ and a negative response $y_l$ from the same input image $X_{img}$ and instruction $X_{instruct}$. To obtain the negative sample, we introduce interventions during the decoding process of the MLLM. We use two decoding paths: a conditional path $p_c$ that generates a response based on both the input image $X_{img}$ and instruction $X_{instruct}$, and an unconditional path $p_u$ that uses only the instruction $X_{instruct}$, without the image $X_{img}$. The generation is controlled by the hallucination ratio, i.e., $h_{ratio}$, which determines the level of hallucination to be injected into the generated caption, ranging from 0 to 1. A higher $h_{ratio}$ denotes injecting more hallucinations into the response.\nAs shown in Fig. 2, for each output token, the distribution is determined by combining the token distribution $t_c$ from the conditional path and the token distribution $t_u$ from the unconditional path, weighted by the hallucination ratio (i.e., $h_{ratio}$):\n$t = (1 - h_{ratio}) \\cdot t_c + h_{ratio} \\cdot t_u$. (3)\nThe two paths, $p_c$ and $p_u$, do not interact, ensuring that the unconditional path never accesses any information from the input image, thus serving as a \"pure\" hallucination source. Each pair is initially labeled, with the response generated under the lower $h_{ratio}$ assigned as positive and the other as negative. The $h_{ratio}$ follows a predefined distribution, such as uniform or Gaussian, and remains fixed for each decoding process once assigned."}, {"title": "3.4. Lightweight Preference Data Inversion", "content": "Although the generated pairs initially have assigned positive or negative labels, these labels may not always be accurate, as the conditional generation process with the seed MLLM can sometimes introduce a certain level of hallucination in the decoded text. To address this, we implement an additional quality control step to manage cases where initial labeling may be incorrect.\nSpecifically, we use a lightweight CLIP model, which is the vision-language contrastive pretrained encoder of the MLLM. For each initial pair $(y_w, y_l)$, we calculate the $CLIP_{score}$ between the image and each decoded sentence. Since CLIP has a 77-token limit and cannot accommodate overly long captions, we compute the average sentence-level CLIP_scores for the initial positive caption, $CLIP_{score_w}$, and the initial negative caption, $CLIP_{score_l}$. If $CLIP_{score_w} < CLIP_{score_l}$, indicating that the initial positive is rated lower than the initial negative, we swap the preference labels, designating $y_l'$ as the final negative $y_l^f$ and $y_w'$ as the final positive $y_w^f$. Otherwise, we retain the original order in the final pair.\nThis process prevents cases where an initial negative sample might outperform its counterpart, which could undermine subsequent preference learning. After this step, we obtain the final preference pairs $(y_w^f, y_l^f)$, which are used in preference alignment training to improve the seed model $m_0$."}, {"title": "3.5. Preference Learning Finetuning", "content": "After obtaining the final pairs of positive caption $y_w^f$ and negative caption $y_l^f$ generated from the same input image $X_{img}$ and instruction $X_{instruct}$, we select a subset of the preference dataset D within a certain range of the $CLIP_{score}$ difference, $CLIP_{score_w^f} - CLIP_{score_l^f}$, forming $D_{sub}$. We then use DPO, a commonly used, low-cost alternative to RLHF, to train the seed model $m_0$, further enhancing its performance.\nThrough this finetuning process, we obtain an improved model $m_1$, which is self-improved from the seed model $m_0$ using its own generated dataset."}, {"title": "4. Experiments", "content": "To evaluate the effectiveness of our proposed self-improvement framework, we tested it on both our IC dataset using GPT-40 series evaluation and a commonly used benchmark. We introduce the experimental settings for dataset generation and verification, followed by a detailed analysis of results and ablation studies to demonstrate the effectiveness of our framework and each of its design modules."}, {"title": "4.1. Datasets", "content": "IC Dataset. Current hallucination benchmarks primarily evaluate the precision of captions while often ignoring recall. To comprehensively assess MLLMs' captioning abilities, we have collected a new dataset containing 150 challenging images prone to hallucination across a wide range of domains and scenarios. These include abstract concepts, animals, animations, artistic content, common sense violations, documents, events, fashion, food, handwriting, illustrations, objects, people, posters, scenes, technology, and vehicles.\nAfter generating captions, we use the GPT-40 series to evaluate them based on precision (elements in the caption that are present in the image) and recall (elements in the image that are captured in the caption) to calculate a final F1 score, which serves as a measure of caption quality."}, {"title": "4.2. Experiment Setup", "content": "For the seed model $m_0$, we used LLaVA-1.5-13B [13], a popular and representative MLLM. An 8xA100 node with 80GB VRAM per GPU was used for DPO training, while data generation and other processes were performed on a single GPU.\nDuring data generation, we sampled 100k images from the LLaVA instruction tuning dataset, llava_v1_5_mix665k, and removed all question-answer pairs. Using the prompt \"Describe image in detail,\" the model generated responses with an $h_{ratio}$ ranging from 0 to 1. Initially, captions generated with a lower $h_{ratio}$ were assigned as the initial positive samples, $y_w$, while captions generated from the same inputs $X_{instruct}$ and $X_{img}$ with a higher $h_{ratio}$ were assigned as the initial negative samples, $y_l$. This process resulted in 100k initial preference pairs.\nFor the obtained caption pairs, each sentence was extracted, and the CLIP model was used to compute the CLIP_score for each image-caption pair. For sentences longer than the CLIP model's context limit, we split them into shorter sub-sentences, computed their respective CLIP_scores, and calculated the average CLIP_score for each caption by averaging the scores from all sentences and sub-sentences. If the average CLIP_score of a negative caption was higher than that of the positive caption, we swapped the positive and negative samples. The pairs were then sorted by CLIP_score difference, from low to high, and organized into 10 splits, each containing 10k pairs.\nFor each split, we trained a LLaVA model and conducted inference on the IC dataset and other benchmarks to gather results. For the IC dataset, GPT-40 was used as the evaluator to compute precision, recall, and F1 score."}, {"title": "4.3. Results", "content": "With the improved model $m_1$ derived from the original seed model $m_0$, we evaluated performance across different benchmarks and presented the results in Tab 1 and Tab 2. As shown, the self-improved model outperforms previous models on both benchmarks. In particular, compared to the original seed model LLaVA-1.5-13B, performance has improved significantly, clearly demonstrating the effectiveness of our framework. Compared to previous methods, ours is the first to emphasize an efficient self-improvement approach that balances efficiency and effectiveness.\nFor a fixed $h_{ratio}$ during dataset generation, we show the ablation study results in Fig. 4. Using a uniform distribution, we obtained the experimental results as illustrated in Fig. 5. With different CLIP_score difference pairs generated with $h_{ratio}$ sampled from a Gaussian distribution, we also present the ablation study results in Fig. 6. We observe a clear performance gain for each component added to our framework, compared to the seed model $m_0$ and the model without that component, as shown in Fig. 7.\nIn Fig. 8, we show qualitative results to demonstrate the differences between our self-improved model and the original seed model. We also use the generated captions to perform image reconstruction with the DALLE 3 model, as shown in Fig. 3."}, {"title": "4.4. Experimental Analysis", "content": "From the comprehensive evaluation results, we observe that our self-improved model shows significant performance gains across various benchmarks compared to the initial seed model in all evaluation dimensions.\nOur model performs substantially better than the initial seed model"}, {"title": "5. Limitations and Future Work", "content": "Although our experiments demonstrate that our framework is highly effective for enhancing the initial model's performance, we acknowledge some limitations and highlight areas for exploration and improvement in future work.\nRecursive Self-Improvement. Due to limited resources, we were unable to investigate whether recursive self-improvement is feasible by iteratively applying our framework in multiple rounds, from data generation to preference learning finetuning, to go beyond $m_1$ and potentially achieve $m_2$, $m_3$, and so on. This could reveal whether further improvements are possible or if an upper performance bound exists.\nScaling with Larger Models and Datasets. Because of training costs, we were unable to experiment with even larger models or larger datasets. Exploring the scaling laws of the framework with additional resources would be an interesting avenue for future research.\nExtending to Other Modalities. Although our experiments focused solely on vision-language tasks, the framework should be able to extend to other modalities, such as video and audio. These directions present promising topics for future exploration."}, {"title": "6. Conclusion", "content": "In this paper, we propose a novel and efficient self-improvement framework for MLLMs that does not require model-level self-feedback. We demonstrate that, using our methods: 1) We significantly improve the seed model's performance, reduce hallucination, and enhance image-caption correspondence compared to the original seed model across different benchmarks. 2) Our approach enables precise control over the pair generation process, allowing us to efficiently generate preference pairs with any desired level of difference between samples. 3) We prevent cases where the positive sample is worse than the negative one by using a lightweight CLIP model to flip samples when the score difference is negative. Unlike traditional self-improvement methods, our approach dramatically reduces the parameters required during the verification process, as it eliminates the need for a model-level judge. Extensive experiments demonstrate that our framework effectively balances superior performance and efficiency. We hope our work inspires new strategies for managing trade-offs in the self-improvement process for MLLMs."}, {"title": "Supplementary Material", "content": "In this appendix, we first provide additional details about our IC dataset, including image counts across its various categories. Next, we present additional qualitative results from our experiments, utilizing our efficient self-improvement framework in comparison to the seed model. Finally, with GPT-40's evaluation, we further demonstrate the effectiveness of our proposed method."}, {"title": "A. Details of the IC Dataset", "content": "As mentioned in the main paper Sec 4, to comprehensively evaluate the model's performance across different caption cases, including the most challenging types, it was necessary to build a diverse dataset to address this issue.\nWe provide details of each category and the number of samples collected in our IC dataset in Table 3."}, {"title": "B. Demo Examples", "content": "Qualitative Examples Across Different Categories. To better demonstrate the usefulness of our proposed framework, we have included additional qualitative example comparisons such as an animation image in Fig. 10, a documents image, and a common sense violation image."}]}