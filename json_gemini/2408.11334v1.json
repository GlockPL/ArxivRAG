{"title": "BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast Ultrasound Reports", "authors": ["Yuxuan Chen", "Haoyan Yang", "Hengkai Pan", "Fardeen Siddiqui", "Antonio Verdone", "Qingyang Zhang", "Sumit Chopra", "Chen Zhao", "Yiqiu Shen"], "abstract": "Breast ultrasound is essential for detecting and diagnosing abnormalities, with radiology reports summarizing key findings like lesion characteristics and malignancy assessments. Extracting this critical information is challenging due to the unstructured nature of these reports, with varied linguistic styles and inconsistent formatting. While proprietary LLMs like GPT-4 are effective, they are costly and raise privacy concerns when handling protected health information. This study presents a pipeline for developing an in-house LLM to extract clinical information from radiology reports. We first use GPT-4 to create a small labeled dataset, then fine-tune a Llama3-8B model on it. Evaluated on clinician-annotated reports, our model achieves an average F1 score of 84.6%, which is on par with GPT-4. Our findings demonstrate the feasibility of developing an in-house LLM that not only matches GPT-4's performance but also offers cost reductions and enhanced data privacy.", "sections": [{"title": "1 INTRODUCTION", "content": "Ultrasonography plays a crucial role in diagnosing breast pathology, with radiologists generating comprehensive reports following the BI-RADS guidelines to standardize lesion characterization [8, 15]. However, the large volume of data in these reports, combined with variations in reporting styles and clinical discrepancies, often leads to unstructured information that's challenging to extract systematically. Implementing a method to retrieve this clinical data could enhance Clinical Decision Support Systems (CDSS) [10], providing real-time alerts and recommendations, improving data accuracy, and benefiting both physicians and trainees by supporting case characterization and diagnostic decision tracking.\nCommercial LLMs like GPT-4 [17] offer satisfactory accuracy in information retrieval but can be costly for large datasets and pose privacy risks with protected health information (PHI) in medical reports. To mitigate these issues, institutions often fine-tune open-source models like Llama 3 [1], which, while potentially less accurate, enhance privacy and reduce costs. However, developing in-house LLMs is challenging due to the need for large, high-quality annotated clinical datasets [16] and the unstructured nature of radiology reports, which vary in style and format.\nWe aim to standardize a workflow for institutions to train an in-house LLM without the need for costly, large-scale"}, {"title": "2 RELATED WORKS", "content": "In clinical concept extraction, methodologies have undergone substantial evolution, progressing from early rule-based systems to deep learning approaches. Initially, the domain relied heavily on rule-based systems, such as the method proposed by Friedman et al., which employs a three-phase processing approach-parsing to identify text structure, regularization to standardize terms, and encoding to map terms to a controlled vocabulary [9].\nWith technological growth, the introduction of machine learning models such as Conditional Random Fields [7] and Support Vector Machines [20] shifted the field towards more dynamic analysis. Furthermore, the emergence of deep learning architectures, including Recurrent Neural Networks and Long Short-Term Memory models [19], has freed humans from manual feature engineering by employing distributed word representations [18], while effectively capturing the subtle nuances and complexities inherent in clinical text [2]. The advent of the transformer [21] marked another significant milestone. Transformer-based models such as BERT [5], ALBERT [13], ROBERTa [14], and ELECTRA [3] have been explored for clinical concept extraction tasks [22]. The incorporation of self-attention mechanisms in these models enhances the long-term dependencies management, thereby providing a more sophisticated tool for this task.\nRecent advancements in LLMs, such as GPT-4 [17] and Llama 3 [1], have revolutionized the field of natural language processing. Their proficiency in tasks ranging from text summarization to question answering highlights their versatility. Consequently, our research aims to investigate the fine-tuning of LLMs for clinical concept extraction, demonstrating their capability to manage complex clinical text."}, {"title": "3 METHODS", "content": "Formally, let x denote a breast ultrasound report, containing textual descriptions of one or multiple lesions. Our goal is to transform each report x into a list of JSON dictionaries y = [Y1, Y2,..., yn], where each dictionary yi corresponds to a lesion described in x. Each dictionary yi is defined as yi = {(k1,01), (k2, 02), ..., (km, vm)}, where k1,k2,..., km are the keys representing lesion attributes, and 01, 02,...,vm are the corresponding values extracted from the report. In our study, there are 16 keys of interest.\nOur objective is to train an LLM that processes x to generate y. Please refer to Appendix A for an example of input and output."}, {"title": "3.2 Pipeline for building in-house LLMs", "content": "As illustrated in Figure 1, our pipeline consists of three steps: 1) extracting observations (also referred to as findings) and impressions from reports; 2) generating training labels using GPT-4 [17]; 3) fine-tuning Llama3-8B using Q-LORA [4]."}, {"title": "3.2.1 Extract Observations and Impressions.", "content": "Figure 1 illustrates the components of a typical breast ultrasound report, which includes: 1) patient and examination details; 2) methodology of the ultrasound; 3) findings, detailing the notable features observed in the breast tissue; 4) impression, offering a concise summary, interpretation of findings, and recommendations for subsequent actions; and 5) disclosure. Unlike other sections whose information is available in a structured format, the observation and impression sections contain key descriptions of lesions in an unstructured manner. Consequently, we focus on extracting clinical information primarily from these two sections. We employ regular expressions to isolate the observation and impression sections, ensuring the input for the LLMs is in a clean format."}, {"title": "3.2.2 Generate Training Labels using GPT-4-32K.", "content": "Our institute utilizes a HIPAA-compliant GPT-4 instance, allowing"}, {"title": "3.2.3 Fine-Tuning.", "content": "We fine-tune Llama-3-8B by utilizing QLORA [4] on pairs of report and JSON outputs generated by GPT-4. Fine-tuning allows Llama-3-8B to perform well on our specific task by adapting our dataset. We choose QLoRA, which is a quantized version of Low-Rank Adaptation (LoRA) [11], as it can significantly reduce memory and computational requirements.\nFirst, we quantize the model weights as q = Q(0). Instead of performing a full fine-tuning, we only update the two low-rank matrices B and A using backpropogration, as demonstrated in (1). This approach reduces the size of the parameters that need to be updated from approximately 16GB to about 100MB compared to full fine-tuning, while maintaining model performance. The matrices B and A are updated through backpropagation based on the loss function defined in (2) and (3). This ensures that the model minimizes the discrepancy between the predicted outputs and the actual outputs at the token level.\nIn summary, the QLoRA method significantly reduces the number of parameters that need to be updated compared to a full fine-tuning, enhancing both memory and computational efficiency.\n \u0394\u03b8\u2081 ~ \u0392\u0391, \u03b8\u2081 \u2190 \u03b8\u2081 + \u0394\u03b8\u1f76   (1)\nwhere B \u2208 Rdxr and A \u2208 Rr\u00d7k, with rank r < min(d, k). r represents the LoRA attention dimension and a is for LoRA scaling.\n  Poq (s|x) = \u03a0j=1t Poq (sj|x, s<j)   (2)\n L(@q) = -Exs(x) [log peg (s|x)]   (3)\nwhere s is the output string, s<j = [S1, ..., Sj-1] for j = 2, ..., t, and t is the length of string s."}, {"title": "3.3 Inference", "content": "During the inference phase of BURExtract-Llama, a report undergoes processing to extract the observation and impression components. These elements are integrated with the fine-tuning instructions detailed in Step 3 of Figure 1 to execute inference, producing a structured JSON output."}, {"title": "4 EXPERIMENTS", "content": "The dataset used in this study consist of 4000 breast ultrasound reports from NYU Langone. We divide this dataset into training (3600), validation (280), and test sets (120), with no overlapping reports. The training and validation sets are labeled using GPT-4. The test set is clinician-labeled to mitigate potential errors from automated labeling, ensuring accurate model evaluation."}, {"title": "4.2 Training Detail", "content": "We include all hyper-parameters in Appendix C. During the validation, we focus on the number of training epochs and the LORA attention dimension to select the best model. As detailed in Appendix D, epoch 4 and a LoRA attention dimension of 64 yielded the best performance. Our best model was trained on an Nvidia A100 GPU for 1.5 hours."}, {"title": "4.3 Evaluation Metrics", "content": "We employ two primary categories of evaluation metrics: per report matching and per key matching. Per Report Matching includes three metrics:\n\u2022 JSONable Accuracy: The percentage of the LLM's outputs that can be converted into a valid list of dictionaries.\n\u2022 Exact Matching (EM) Accuracy: The percentage of the LLM's outputs, after converted to JSON, that exactly match the ground truth for all 16 keys {k1,k2...k16}.\n\u2022 Close Domain Matching (CDM) Accuracy: The percentage of the LLM outputs that match the ground truth"}, {"title": "4.4 Results", "content": "Table 2 provides a detailed comparison of precision, recall, and F1 scores for each key. The average F1 score difference between BURExtract-Llama and GPT-4's ICL is within 0.1%. Notably, BURExtract-Llama outperforms GPT-4 in keys like \"depth,\" \"anatomical region,\" and \"posterior features,\" with a maximum difference of 2.3% in \"anatomical region\".\nAs shown in Table 3, BURExtract-Llama achieves 100% structured output for the JSONable accuracy, demonstrating its ability to follow the prompt instructions and produce the desired output format. Our BURExtract-Llama outperforms Llama3-8B with ICL by 12.5% in EM and 10.0% in CDM, highlighting the benefits of fine-tuning. It matches GPT-4 in EM and is only 0.9% behind in CDM, proving BURExtract-Llama to be a viable alternative to GPT-4. Additionally, our BURExtract-Llama can infer a new report in approximately 2 seconds with vllm [12], demonstrating its low latency."}, {"title": "4.5 Error Analysis", "content": "We conduct an error analysis to identify instances where BURExtract-Llama failed. While the detailed error cases are listed in the appendix F, we present a summary here:\n\u2022 Missing Lesion: The model sometimes fails to cover all lesions within a report, as reflected by the lower recall rate compared to precision in Table 2.\n\u2022 Lesion Attribute Confusion: The model occasionally misattributes the values of one lesion to another, leading to incorrect associations.\n\u2022 Handling \"N/A\": The ground truth is marked as \"N/A\" when a report doesn't mention information for an attribute. Our model sometimes exhibits inconsistency in handling these cases, predicting \"N/A\" for attributes with actual values. This issue likely arises from the high frequency of \"N/A\" values in the training set for some attributes, as detailed in Appendix G."}, {"title": "5 CONCLUSION", "content": "This study presents a workflow of building in-house LLM to extract relevant clinical information from radiology reports. We utilize GPT-4 to label a small dataset, then fine-tune Llama3-8B on it. The fine-tuned model, BURExtract-Llama, demonstrates performance comparable to GPT-4. Still, we recognize several limitations. First, we acknowledge that the training labels generated by GPT-4 might be flawed. Future research could explore methods to manage noisy labels. The second limitation is the lack of external validation. Since the writing style of reports from a single hospital tends to be consistent, it is uncertain how the model would perform on data from institutions with different writing styles. Future research should also incorporate an external test set to evaluate the generalizability of the BURExtract-Llama."}, {"title": "F THREE DIFFERENT KINDS OF ERROR CASES", "content": "Observation: The study demonstrates heterogeneous background echotexture. Again noted is an area of postsurgical distortion with a 9 x 9 x 4 mm seroma in the upper outer left breast. There is a 3 x 2 mm cyst in the left breast 3:00 N9 location. No suspicious abnormalities were seen sonographically in the left."}, {"title": "2. Lesion Attribute Confusion", "content": "In the following example, the model incorrectly attributes \"6 month follow up,\" which is associated to the lesion at 12 o'clock, to the lesion at 1 o'clock.\nObservation: RIGHT: There is a stable from [DATE] 1 cm benign mass right breast at 12 o'clock. Additionally, there is a stable from [DATE] 0.7 cm mass right breast at 1 o'clock."}]}