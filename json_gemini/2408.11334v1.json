{"title": "BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast Ultrasound Reports", "authors": ["Yuxuan Chen", "Haoyan Yang", "Hengkai Pan", "Fardeen Siddiqui", "Antonio Verdone", "Qingyang Zhang", "Sumit Chopra", "Chen Zhao", "Yiqiu Shen"], "abstract": "Breast ultrasound is essential for detecting and diagnosing abnormalities, with radiology reports summarizing key findings like lesion characteristics and malignancy assessments. Extracting this critical information is challenging due to the unstructured nature of these reports, with varied linguistic styles and inconsistent formatting. While proprietary LLMs like GPT-4 are effective, they are costly and raise privacy concerns when handling protected health information. This study presents a pipeline for developing an in-house LLM to extract clinical information from radiology reports. We first use GPT-4 to create a small labeled dataset, then fine-tune a Llama3-8B model on it. Evaluated on clinician-annotated reports, our model achieves an average F1 score of 84.6%, which is on par with GPT-4. Our findings demonstrate the feasibility of developing an in-house LLM that not only matches GPT-4's performance but also offers cost reductions and enhanced data privacy.", "sections": [{"title": "1 INTRODUCTION", "content": "Ultrasonography plays a crucial role in diagnosing breast pathology, with radiologists generating comprehensive reports following the BI-RADS guidelines to standardize lesion characterization [8, 15]. However, the large volume of data in these reports, combined with variations in reporting styles and clinical discrepancies, often leads to unstructured information that's challenging to extract systematically. Implementing a method to retrieve this clinical data could enhance Clinical Decision Support Systems (CDSS) [10], providing real-time alerts and recommendations, improving data accuracy, and benefiting both physicians and trainees by supporting case characterization and diagnostic decision tracking.\nCommercial LLMs like GPT-4 [17] offer satisfactory accuracy in information retrieval but can be costly for large datasets and pose privacy risks with protected health information (PHI) in medical reports. To mitigate these issues, institutions often fine-tune open-source models like Llama 3 [1], which, while potentially less accurate, enhance privacy and reduce costs. However, developing in-house LLMs is challenging due to the need for large, high-quality annotated clinical datasets [16] and the unstructured nature of radiology reports, which vary in style and format.\nWe aim to standardize a workflow for institutions to train an in-house LLM without the need for costly, large-scale"}, {"title": "2 RELATED WORKS", "content": "In clinical concept extraction, methodologies have undergone substantial evolution, progressing from early rule-based systems to deep learning approaches. Initially, the domain relied heavily on rule-based systems, such as the method proposed by Friedman et al., which employs a three-phase processing approach-parsing to identify text structure, regularization to standardize terms, and encoding to map terms to a controlled vocabulary [9].\nWith technological growth, the introduction of machine learning models such as Conditional Random Fields [7] and Support Vector Machines [20] shifted the field towards more dynamic analysis. Furthermore, the emergence of deep learning architectures, including Recurrent Neural Networks and Long Short-Term Memory models [19], has freed humans from manual feature engineering by employing distributed word representations [18], while effectively capturing the subtle nuances and complexities inherent in clinical text [2].\nThe advent of the transformer [21] marked another significant milestone. Transformer-based models such as BERT [5], ALBERT [13], ROBERTa [14], and ELECTRA [3] have been explored for clinical concept extraction tasks [22]. The incorporation of self-attention mechanisms in these models enhances the long-term dependencies management, thereby providing a more sophisticated tool for this task.\nRecent advancements in LLMs, such as GPT-4 [17] and Llama 3 [1], have revolutionized the field of natural language processing. Their proficiency in tasks ranging from text summarization to question answering highlights their versatility. Consequently, our research aims to investigate the fine-tuning of LLMs for clinical concept extraction, demonstrating their capability to manage complex clinical text."}, {"title": "3 METHODS", "content": "Formally, let x denote a breast ultrasound report, containing textual descriptions of one or multiple lesions. Our goal is to transform each report x into a list of JSON dictionaries y = [Y1, Y2,..., yn], where each dictionary yi corresponds to a lesion described in x. Each dictionary yi is defined as yi = {(k1,01), (k2, 02), ..., (km, vm)}, where k1,k2,..., km are the keys representing lesion attributes, and 01, 02, ..., vm are the corresponding values extracted from the report. In our study, there are 16 keys of interest. Our objective is to train an LLM that processes x to generate y. Please refer to Appendix A for an example of input and output."}, {"title": "3.2 Pipeline for building in-house LLMs", "content": "As illustrated in Figure 1, our pipeline consists of three steps: 1) extracting observations (also referred to as findings) and impressions from reports; 2) generating training labels using GPT-4 [17]; 3) fine-tuning Llama3-8B using Q-LORA [4]."}, {"title": "3.2.1 Extract Observations and Impressions", "content": "Figure 1 illustrates the components of a typical breast ultrasound report, which includes: 1) patient and examination details; 2) methodology of the ultrasound; 3) findings, detailing the notable features observed in the breast tissue; 4) impression, offering a concise summary, interpretation of findings, and recommendations for subsequent actions; and 5) disclosure. Unlike other sections whose information is available in a structured format, the observation and impression sections contain key descriptions of lesions in an unstructured manner. Consequently, we focus on extracting clinical information primarily from these two sections. We employ regular expressions to isolate the observation and impression sections, ensuring the input for the LLMs is in a clean format."}, {"title": "3.2.2 Generate Training Labels using GPT-4-32K", "content": "Our institute utilizes a HIPAA-compliant GPT-4 instance, allowing"}, {"title": "3.2.3 Fine-Tuning", "content": "We fine-tune Llama-3-8B by utilizing QLoRA [4] on pairs of report and JSON outputs generated by GPT-4. Fine-tuning allows Llama-3-8B to perform well on our specific task by adapting our dataset. We choose QLoRA, which is a quantized version of Low-Rank Adaptation (LoRA) [11], as it can significantly reduce memory and computational requirements.\nFirst, we quantize the model weights as q = Q(0). Instead of performing a full fine-tuning, we only update the two low-rank matrices B and A using backpropogration, as demonstrated in (1). This approach reduces the size of the parameters that need to be updated from approximately 16GB to about 100MB compared to full fine-tuning, while maintaining model performance. The matrices B and A are updated through backpropagation based on the loss function defined in (2) and (3). This ensures that the model minimizes the discrepancy between the predicted outputs and the actual outputs at the token level.\nIn summary, the QLoRA method significantly reduces the number of parameters that need to be updated compared to a full fine-tuning, enhancing both memory and computational efficiency.\n\\Delta \\theta_{i} \\sim B A, \\theta_{i} \\leftarrow \\theta_{i} + \\Delta \\theta_{i}\nwhere B \\in \\mathbb{R}^{d \\times r} and A \\in \\mathbb{R}^{r \\times k}, with rank r < min(d, k). r represents the LoRA attention dimension and a is for LoRA scaling.\nP_{\\theta_{q}}(s|x) = \\prod_{j=1}^{t} P_{\\theta_{q}}(s_{j}|x, s_{<j})\n\\mathcal{L}(\\theta_{q}) = -\\mathbb{E}_{x,s \\thicksim (x)} [log p_{\\theta_{q}}(s|x)]"}, {"title": "3.3 Inference", "content": "During the inference phase of BURExtract-Llama, a report undergoes processing to extract the observation and impression components. These elements are integrated with the fine-tuning instructions detailed in Step 3 of Figure 1 to execute inference, producing a structured JSON output."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Dataset", "content": "The dataset used in this study consist of 4000 breast ultrasound reports from NYU Langone. We divide this dataset into training (3600), validation (280), and test sets (120), with no overlapping reports. The training and validation sets are labeled using GPT-4. The test set is clinician-labeled to mitigate potential errors from automated labeling, ensuring accurate model evaluation."}, {"title": "4.2 Training Detail", "content": "We include all hyper-parameters in Appendix C. During the validation, we focus on the number of training epochs and the LORA attention dimension to select the best model. As detailed in Appendix D, epoch 4 and a LoRA attention dimension of 64 yielded the best performance. Our best model was trained on an Nvidia A100 GPU for 1.5 hours."}, {"title": "4.3 Evaluation Metrics", "content": "We employ two primary categories of evaluation metrics: per report matching and per key matching. Per Report Matching includes three metrics:\n*   JSONable Accuracy: The percentage of the LLM's outputs that can be converted into a valid list of dictionaries.\n*   Exact Matching (EM) Accuracy: The percentage of the LLM's outputs, after converted to JSON, that exactly match the ground truth for all 16 keys {k1,k2...k16}.\n*   Close Domain Matching (CDM) Accuracy: The percentage of the LLM outputs that match the ground truth"}, {"title": "4.4 Results", "content": "Table 2 provides a detailed comparison of precision, recall, and F1 scores for each key. The average F1 score difference between BURExtract-Llama and GPT-4's ICL is within 0.1%. Notably, BURExtract-Llama outperforms GPT-4 in keys like \"depth,\" \"anatomical region,\" and \"posterior features,\" with a maximum difference of 2.3% in \"anatomical region\".\nAs shown in Table 3, BURExtract-Llama achieves 100% structured output for the JSONable accuracy, demonstrating its ability to follow the prompt instructions and produce the desired output format. Our BURExtract-Llama outperforms Llama3-8B with ICL by 12.5% in EM and 10.0% in CDM, highlighting the benefits of fine-tuning. It matches GPT-4 in EM and is only 0.9% behind in CDM, proving BURExtract-Llama to be a viable alternative to GPT-4. Additionally, our BURExtract-Llama can infer a new report in approximately 2 seconds with vllm [12], demonstrating its low latency."}, {"title": "4.5 Error Analysis", "content": "We conduct an error analysis to identify instances where BURExtract-Llama failed. While the detailed error cases are listed in the appendix F, we present a summary here:\n*   Missing Lesion: The model sometimes fails to cover all lesions within a report, as reflected by the lower recall rate compared to precision in Table 2.\n*   Lesion Attribute Confusion: The model occasionally misattributes the values of one lesion to another, leading to incorrect associations.\n*   Handling \"N/A\": The ground truth is marked as \"N/A\" when a report doesn't mention information for an attribute. Our model sometimes exhibits inconsistency in handling these cases, predicting \"N/A\" for attributes with actual values. This issue likely arises from the high frequency of \"N/A\" values in the training set for some attributes, as detailed in Appendix G."}, {"title": "5 CONCLUSION", "content": "This study presents a workflow of building in-house LLM to extract relevant clinical information from radiology reports. We utilize GPT-4 to label a small dataset, then fine-tune Llama3-8B on it. The fine-tuned model, BURExtract-Llama, demonstrates performance comparable to GPT-4. Still, we recognize several limitations. First, we acknowledge that the training labels generated by GPT-4 might be flawed. Future research could explore methods to manage noisy labels. The second limitation is the lack of external validation. Since the writing style of reports from a single hospital tends to be consistent, it is uncertain how the model would perform on data from institutions with different writing styles. Future research should also incorporate an external test set to evaluate the generalizability of the BURExtract-Llama."}, {"title": "A EXAMPLE INPUT AND OUTPUT OF LLM", "content": "#### INPUT:\nObservation:\nAt the right 9:00 axis, 1 cm from the nipple, there is a 0.4 cm simple cyst. At the left 6:00 retroareolar location, there is a 0.6 x 0.4 x 0.6 cm hypoechoic mass, probably benign may represent a debris-filled cyst and corresponding to the previous mass annotated at the \"left nipple posterior\" location on exam dated MM/DD/YYYY. No other suspicious cystic or solid masses identified in either breast.\nImpression:\nPROBABLY BENIGN FOLLOW-UP RECOMMENDED Left 6:00 retroareolar 0.6 cm hypoechoic mass, probably benign cyst with debris. Recommend follow-up ultrasound in 6 months to assess stability. I personally discussed the findings and recommendations with the patient.\n### OUTPUT:\n\"\"\"\n[{\n\"location\": {\n\"side_of_breast\": \"right\",\n\"clock_position\": \"9\",\n\"distance_from_nipple\": \"1\"\n},\n\"depth\": \"N/A\",\n\"anatomical_region\": \"N/A\",\n\"type\": \"cyst\",\n\"shape\": \"N/A\",\n\"orientation\": \"N/A\",\n\"margin\": \"N/A\",\n\"echogenicity\": \"N/A\",\n\"calcifications\": \"N/A\",\n\"vascularity\": \"N/A\",\n\"posterior_features\": \"N/A\",\n\"suspicion\": \"N/A\",\n\"subtype\": \"N/A\",\n\"next_step\": \"N/A\"\n},\n{\n\"location\": {\n\"side_of_breast\": \"left\",\n\"clock_position\": \"6\",\n\"distance_from_nipple\": \"N/A\"\n},\n\"depth\": \"N/A\",\n\"anatomical_region\": \"retroareolar\",\n\"type\": \"mass\",\n\"shape\": \"N/A\",\n\"orientation\": \"N/A\",\n\"margin\": \"N/A\",\n\"echogenicity\": \"hypoechoic\",\n\"calcifications\": \"N/A\",\n\"vascularity\": \"N/A\",\n\"posterior_features\": \"N/A\",\n\"suspicion\": \"probably benign\",\n\"subtype\": \"cyst with debris\",\n\"next_step\": \"follow-up ultrasound in 6 months\"\n}]\n\"\"\""}, {"title": "B PROMPT TEMPLATE FOR GENERATING JSON LABLES", "content": "You are given a breast ultrasound imaging report with \"Observation\" and \"Impression\" sections. Your task is to extract essential information from these sections and store it in a standard JSON dictionary. The JSON dictionary should contain the following keys:"}, {"title": "C HYPER-PARAMETERS", "content": ""}, {"title": "D HYPER-PARAMETER OPTIMIZATION RESULTS", "content": "The details for the four metrics in the legend can be found in section 4.3."}, {"title": "E DETAILS OF METRICS", "content": "JSONable Accuracy: As shown in Equation (4), a report x is considered jsonable if the model's prediction y' is a valid list of JSON dictionaries:\nJSONable(x) = IsValidJSONList(y')\nThe JSONable accuracy is then calculated as the proportion of reports that are jsonable, as described in Equation (5):\nJSONable_Acc = \\frac{\\sum_{x \\in X} I(JSONable(x))}{|X|}\nwhere X comprises all the reports, and I is the indicator function that returns 1 if the condition is true and 0 otherwise.\nCDM Accuracy: Equation (6) defines the first condition for CDM, which checks whether the lengths of the predicted list y' and true list y are equal:\nLenMatch(x) = (|y'| = |y|)\nwhere the lists for both y' and y are sorted by key k14 side of breast before comparison to maintain the correct order of the lesions.\nEquation (7) defines the second condition for CDM. This condition requires that for each report x, the values of the keys in Kclose in the prediction list y[k] must exactly match the corresponding values in the ground truth list yi [k] for every element i from 1 to n, where Kclose represents the set of keys {k1,k2, ..., k10}.\nCDM\\_Cond(x) = \\bigwedge_{i=1}^{n} \\bigwedge_{k \\in K_{close}} (y'[k] = y_{i}[k])\nAs shown in Equation (8), for a report x to be considered a Close Domain Match, both conditions must be met:\nCDM(x) = LenMatch(x) \\wedge CDM\\_Cond(x)\nThe Close Domain Matching accuracy is calculated as the proportion of reports that are close domain matches, as described in Equation (9):\nCDM\\_Acc = \\frac{\\sum_{x \\in X} I(CDM(x))}{|X|}\nEM Accuracy: Equation (10) defines the conditions for Exact Matching (EM), where Kexact represents the set of keys {k1,k2,..., k16}:\nEM\\_Cond(x) = \\bigwedge_{i=1}^{n} \\bigwedge_{k \\in K_{exact}} (y'[k] = y_{i}[k])\nAs shown in Equation (11), for a report x to be considered an Exact Match, both conditions must be satisfied:\nEM(x) = LenMatch(x) \\wedge EM\\_Cond(x)\nThe Exact Matching accuracy is calculated as the proportion of reports that are exact matches, as described in Equation (12):\nEM\\_Acc = \\frac{\\sum_{x \\in X} I(EM(x))}{|X|}\nTo evaluate the model's performance on specific key attributes, we calculate recall, precision, and F1 score for each key k. Given that the lengths of the prediction list and the ground truth list may vary, simply sorting the lists and iterating over them for comparison may not be fair, as the order can still be incorrect. Therefore, we split both the prediction list and the ground truth list into three separate lists based on the side of the breast: left, right and N/A. Comparisons are then made between the corresponding lists, yside and yside. The formal definitions of these evaluation metrics are as follows:\nRecall: Measures the model's ability to identify all relevant instances of a specific key k. Recall is quantified as the ratio of correctly predicted instances to the total instances present in the ground truth. This is computed using equations (13) - (15)\nMinLen(x, side) = min(|y'_{side}|, |y_{side}|)\nCountMatch(x, k, side) = \\sum_{i=1}^{MinLen(x,side)} I(y'_{side,i} [k] = y_{side,i} [k])\nRecall(k) = \\frac{\\sum_{x \\in X} (CountMatch(x, k, left) + CountMatch(x, k, right) + CountMatch(x, k, N/A))}{\\sum_{y \\in Y} |y|}\nwhere Y is the set that contains all the ground truth.\nPrecision: Evaluates the proportion of true positive predictions among all predictive instances on key k. Precision is defined as the ratio of correctly predicted instances to the total number of predicted instances. This is calculated in equation (16):\nPrecision(k) = \\frac{\\sum_{x \\in X} (CountMatch(x, k, left) + CountMatch(x, k, right) + CountMatch(x, k, N/A))}{\\sum_{y' \\in Y'} |y'|}"}, {"title": "F THREE DIFFERENT KINDS OF ERROR CASES", "content": "Missing Lesion\nObservation: The study demonstrates heterogeneous background echotexture. Again noted is an area of postsurgical distortion with a 9 x 9 x 4 mm seroma in the upper outer left breast. There is a 3 x 2 mm cyst in the left breast 3:00 N9 location. No suspicious abnormalities were seen sonographically in the left.\nImpression: No Impression\nPrediction:\n[{'location': {'side_of_breast': 'left',\n'clock_position': '3',\n'distance_from_nipple': '9'},\n'depth': 'n/a',\n'anatomical_region': 'n/a',\n'type': 'cyst',\n'shape': 'n/a',\n'orientation': 'n/a',\n'margin': 'n/a',\n'echogenicity': 'n/a',\n'calcifications': 'n/a',\n'vascularity': 'n/a',\n'posterior_features': 'n/a',\n'suspicion': 'n/a',\n'subtype': 'n/a',\n'next_step': 'n/a'}]\nGround Truth:\n[{'location': {'side_of_breast': 'left',\n'clock_position': 'n/a',\n'distance_from_nipple': 'n/a'},\n'depth': 'n/a',\n'anatomical_region': 'n/a',\n'type': 'seroma',\n'shape': 'n/a',\n'orientation': 'n/a',\n'margin': 'n/a',\n'echogenicity': 'n/a',\n'calcifications': 'n/a',\n'vascularity': 'n/a',\n'posterior_features': 'n/a',\n'suspicion': 'n/a',\n'subtype': 'n/a',\n'next_step': 'n/a'},\n{'location': {'side_of_breast': 'left',\n'clock_position': '3',\n'distance_from_nipple': '9'},\n'depth': 'n/a',\n'anatomical_region': 'n/a',\n'type': 'cyst',\n'shape': 'n/a',\n'orientation': 'n/a',\n'margin': 'n/a',\n'echogenicity': 'n/a',\n'calcifications': 'n/a',\n'vascularity': 'n/a',\n'posterior_features': 'n/a',\n'suspicion': 'n/a',\n'subtype': 'n/a',\n'next_step': 'n/a'}]"}, {"title": "2. Lesion Attribute Confusion", "content": "In the following example, the model incorrectly attributes \"6 month follow up,\" which is associated to the lesion at 12 o'clock, to the lesion at 1 o'clock.\nObservation: RIGHT: There is a stable from [DATE] 1 cm benign mass right breast at 12 o'clock. Additionally, there is a stable from [DATE] 0.7 cm mass right breast at 1 o'clock.\nImpression: PROBABLY BENIGN FOLLOW-UP RECOMMENDED A follow-up ultrasound in 6 months is recommended for probably benign mass at right 12:00 to document. 12 month follow-up is recommended for additional probably benign mass described above at right 1:00 to demonstrate stability.\nPrediction:\n[{'location': {'side_of_breast': 'right',\n'clock_position': '12',\n'distance_from_nipple': 'n/a'},\n'depth': 'n/a',\n'anatomical_region': 'n/a',\n'type': 'mass',\n'shape': 'n/a',\n'orientation': 'n/a',\n'margin': 'n/a',\n'echogenicity': 'n/a',\n'calcifications': 'n/a',\n'vascularity': 'n/a',\n'posterior_features': 'n/a',\n'suspicion': 'n/a',\n'subtype': 'n/a',\n'next_step': '6 month follow-up'},\n{'location': {'side_of_breast': 'right',\n'clock_position': '1',\n'distance_from_nipple': 'n/a'},\n'depth': 'n/a',\n'anatomical_region': 'n/a',\n'type': 'mass',\n'shape': 'n/a',\n'orientation': 'n/a',\n'margin': 'n/a',\n'echogenicity': 'n/a',\n'calcifications': 'n/a',\n'vascularity': 'n/a',\n'posterior_features': 'n/a',\n'suspicion': 'n/a',\n'subtype': 'n/a',\n'next_step': '6 month follow-up'}]\nGround Truth:\n[{'location': {'side_of_breast': 'right',\n'clock_position': '12',\n'distance_from_nipple': 'n/a'},\n'depth': 'n/a',\n'anatomical_region': 'n/a',\n'type': 'mass',\n'shape': 'n/a',\n'orientation': 'n/a',\n'margin': 'n/a',\n'echogenicity': 'n/a',\n'calcifications': 'n/a',\n'vascularity': 'n/a',\n'posterior_features': 'n/a',\n'suspicion': 'probably benign',\n'subtype': 'n/a',\n'next_step': '6 month follow-up'},\n{'location': {'side_of_breast': 'right',\n'clock_position': '1',\n'distance_from_nipple': 'n/a'},"}, {"title": "3. Handling \"N/A\"", "content": "Please refer to the example in Lesion Attribute Confusion. The model fails to predict the actual value for suspicion of malignancy, resulting in \"N/A\"."}]}