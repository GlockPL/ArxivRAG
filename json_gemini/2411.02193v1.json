{"title": "IMPROVING STEERING VECTORS BY\nTARGETING SPARSE AUTOENCODER FEATURES", "authors": ["Sviatoslav Chalnev", "Matthew Siu", "Arthur Conmy"], "abstract": "To control the behavior of language models, steering methods attempt to ensure that outputs of the\nmodel satisfy specific pre-defined properties. Adding steering vectors to the model is a promising\nmethod of model control that is easier than finetuning, and may be more robust than prompt-\ning.However, it can be difficult to anticipate the effects of steering vectors produced by almost all\nexisting methods, such as CAA [Panickssery et al., 2024] or the direct use of SAE latents [Templeton\net al., 2024]. In our work, we address this issue by using SAEs to measure the effects of steering\nvectors, giving us a method that can be used to understand the causal effect of any steering vector\nintervention. We use this method for measuring causal effects to develop an improved steering\nmethod, SAE-Targeted Steering (SAE-TS), which finds steering vectors to target specific SAE\nfeatures while minimizing unintended side effects. We show that overall, SAE-TS balances steering\neffects with coherence better than CAA and SAE feature steering, when evaluated on a range of tasks.", "sections": [{"title": "1 Introduction", "content": "There are widespread calls for better control of the behaviour of Large Language Models (LLMs; e.g. The White House\n[2023]). Current methods such as prompting [Wallace et al., 2024] and finetuning [Ouyang et al., 2022, Chung et al.,\n2022] offer some degree of control, but have clear limitations. For example, prompting can be fragile and is often\nsusceptible to methods that can subvert these instructions [Wei et al., 2023]. Finetuning a model can be more robust but\nrequires a curated dataset for training which can be both expensive and time-consuming to produce (e.g. Dubey et al.\n[2024]).\nSteering vectors [Turner et al., 2024] have the potential to be more robust than prompting, and both cheaper and easier\nto implement than finetuning. Steering vectors work by adding activations to the hidden state of a model, part way\nthrough the forward pass (Section 3). By generating and inserting activation vectors into the model's forward pass, we\ncan steer the model towards desired behaviors.\nHowever, a problem with current steering methods is their unpredictability \u2013 it's often unclear exactly how a steering\nvector will affect model behavior. Steering vectors may not produce the intended changes in the model's output or may\ncause unforeseen behaviors, as we discuss in Section 3. In some cases, steering interventions produce no interpretable\nchanges in model behavior other than model degradation, as we show in Section 5. This unpredictability makes it\ndifficult to precisely control the model.\nTo address these challenges, we develop a method for quantifying the effects of a steering intervention on model outputs.\nWe use Sparse Autoencoders (SAEs; Ng [2011], Cunningham et al. [2023], Bricken et al. [2023]) to measure the change\nin feature activations caused by steering interventions. This lets us understand what change in behavior to expect from\nany particular steering intervention. Note: our analysis is not about how early SAEs impact later layer SAEs (as studied\nby prior work such as Marks et al. [2024] and Anders and Bloom [2024]). Instead, we look at how rollouts generated by\na steered model differ from those generated by the unsteered, base model.\nBuilding on this feature effects measurement method, we introduce SAE-Targeted Steering (SAE-TS), a method\nthat constructs steering vectors to specifically target desired SAE features while minimizing unintended side effects.\nSAE-TS involves learning a linear relationship between steering vectors and their effects on SAE features. This allows\nus to construct steering vectors using the interpretable features found by the SAE. Notably, SAE-TS uses the SAE for\nsteering differently to prior work such as Templeton et al. [2024] and Durmus et al. [2024]. We use the SAE to measure\nthe effects of steering vectors, rather than directly adding SAE latents to model forward passes.\nWe evaluate SAE-TS against existing methods such as Contrastive Activation Addition (CAA) [Panickssery et al.,\n2024] and direct SAE feature steering Templeton et al. [2024]. Our evaluations demonstrate that SAE-TS outperforms\nexisting methods, achieving better alignment with the intended behavior while maintaining the semantic coherence of\nthe generated text across various tasks.\nKey Contributions:\n1.  We develop a method to quantify and interpret the effects of a steering intervention using SAEs (Section 3).\n2.  We introduce SAE-Targeted Steering (SAE-TS), a method that constructs steering vectors to achieve specific\ndesired effects while minimizing unintended changes (Section 4).\n3.  We evaluate SAE-TS on a set of steering tasks (Section 5), showing that it outperforms existing methods,\nsuccessfully steering the model while maintaining output quality."}, {"title": "2 Related work", "content": "Activation Steering is a method for controlling model outputs introduced in Turner et al. [2024] where steering\nvectors are extracted by taking the difference between activations from pairs of contrasting positive and negative\nprompts. Panickssery et al. [2024] scaled this method to Llama-2, and Zou et al. [2023] use the methodology on a\nsomewhat wider range of tasks. Recently, Cao et al. [2024] offers a way to learn a steering vector to optimally steer\na model toward producing desired outputs. As with the activation steering methods above, a dataset of positive and\nnegative examples is required for this method to work. Lee et al. [2024] introduce conditional steering. Scherlis et al.\n[2024] also studies the causal limitations of existing steering methods.\nMechanistic Interpretability and SAEs. Mechanistic Intepretability aims to understand how LLMs function internally\nby breaking down the models into understandable components [Olah et al., 2020, Olah]. The recent development of\nsparse autoencoders [Ng, 2011, Bricken et al., 2023, Cunningham et al., 2023] gives us a way to decompose the\nresidual stream of transformer models into sparse and often human understandable features. Sparse autoencoders\nprovide evidence supporting the hypothesis that the latent space of LLMs is composed of linear and interpretable"}, {"title": "3 Measuring Steering Effects", "content": "As discussed in the introduction (Section 1), it can be difficult to predict the behavior of steering vectors. Measuring the\neffects of a steering intervention is useful for both interpreting the causal role of features and designing more effective\nsteering vectors.\nIn transformer models, we can represent the forward pass as $h_n = b_n(h_{n-1})$, where $b_i$ denotes the $i^{th}$ block and $h_i$ is\nthe hidden state after layer $i$. When we insert a steering vector $v$ at a specific layer $l$, we modify the hidden state as\n$h_l \\leftarrow h_l + \\alpha v$, where $\\alpha$ is a scaling coefficient that determines the norm of the added steering vector.\nWe use JumpReLU SAEs [Rajamanoharan et al., 2024], which consist of an encoder $f$ and a decoder $\\hat{x}$, where the\nreconstruction $\\hat{x}$ of the input $x$ is given by:\n$\\hat{x} = f(x)W_{dec} + b_{dec},$\nand the SAE is trained such that $\\hat{x}$ approximates $x$. SAE steering involves using the decoder vector $d_i$ of feature $i$,\nwhich is the $i^{th}$ row of $W_{dec}$, as the steering vector.\nIn order to measure a steering intervention's effect on the model output, we look at the difference in SAE feature\nactivations between steered model outputs and original, unsteered model outputs. Our method is outlined at a high level\nin Figure 1 and can be summarised as follows:\n1.  Data Generation: Generate text rollout datasets $D_{steered}$ and $D_{unsteered}$ using steered and unsteered models\nrespectively, where the steered model has a steering vector $v$ inserted at layer $l$.\n2.  Feature Extraction: For each generated output, pass the text back through the model up to layer $l$, and use the\nSAE encoder $f$ to extract the feature activations at each position.\n3.  Effect Computation: Compute the average feature activations across all outputs and positions for both the\nunsteered and steered models, and calculate the difference to obtain the steering effects vector $y \\in \\mathbb{R}^{d_{SAE}}$:\n$y = \\mathbb{E}_{steered}[f(x)] - \\mathbb{E}_{unsteered}[f(x)].$"}, {"title": "4 Targeted Steering", "content": "We can use the above method of measuring steering effects to find steering vectors which steer the model towards a\ndesired feature effect, while having minimal side-effects.\nWe do this by first training a linear effect approximator function (see Section 4.1) to predict the feature effects for any\ngiven steering vector and then use this function to find targeted steering vectors to achieve a desired effect.\nThe effect approximator is a linear function $\\hat{y} = xM + b$, where $x$ is the $d_{model}$-dimensional steering vector, $M$ is a\n$d_{model} \\times d_{SAE}$ matrix, and $b$ has dimension $d_{SAE}$. We train it to minimize the mean squared error (MSE) between its\nprediction $\\hat{y}$ and the observed effect vector $y$.\nOnce we have trained the effect approximator, we use it to find a steering vector that increases the activation of a\ntarget feature $j$, while keeping other features unchanged. To achieve this, we construct the targeted steering vector $s$ as\nfollows:\n$s = \\frac{M_j}{||M_j||} - \\lambda \\frac{M_b}{||M_b||}$\nWe then normalize $s$ to unit norm. In all our experiments, we set $\\lambda = 1$. See Appendix A for further discussion on why\nwe do this instead of solving for $x$ by computing the pseudoinverse of $M$.\nTo construct the effect approximator, we collect a dataset of 50,000 steering vectors and their corresponding steering\neffects. The steering vectors are the decoder vectors from a larger SAE trained at the same layer, which in the case of\nGemma-2-2b is the layer 12 65k SAE with an L0 of 72. This provides us with a diverse set of steering vectors, letting\nus observe a wide range of steering effects.\nFor each steering vector $x$ in the dataset, we measure its effect $y$ on the model by computing the difference in SAE\nfeature activations between the steered and unsteered model outputs, as described in Section 3. We then train the linear\neffect approximator $\\hat{y} = xM + b$ using Adam to minimize the MSE between $\\hat{y}$ and $y$.\nBy learning the mapping from steering vectors to their effects on SAE features, the effect approximator allows us to\npredict the change in feature activations for any given steering vector. This enables us to design steering vectors that\nproduce precise and predictable changes in the model's behavior."}, {"title": "4.2 Scaling Factor Selection", "content": "Selecting an appropriate scaling factor $\\alpha$ when adding the steering vector $x$ is important for getting good steering\nresults. The model is sensitive to different directions to varying degrees; some steering vectors may have a significant\nimpact even with a small scaling factor, while others require a larger scaling factor to produce noticeable effects. To\ncompensate for this variability, we need an automatic method to adjust the scaling factor for each steering vector\nindividually.\nWe do this by finding a scaling factor $\\alpha$ for each steering vector such that the steered model's cross-entropy loss goes\nup by 0.5 above the unsteered baseline. By applying this scaling factor selection process to all steering vectors in our\ndataset, we ensure that the collected steering effects accurately reflect the model's response within the optimal range of\nsteering intensities."}, {"title": "5 Evaluations", "content": "To evaluate our steering vectors, we use gpt-4o-mini to rate two aspects of the generated text on a scale of 1 to 10:\n\u2022 Behavioral score Assesses whether the steering target was met, based on task-specific criteria.\n\u2022 Coherence score Evaluates whether the steering intervention maintains the model's general capabilities and\nproduces semantically correct text."}, {"title": "6 Feature Effects Visualization", "content": "In this section, we introduce EffectVis, an interactive interface and tool built for exploring feature effects. On the top\nleft panel, users can search for features and view their feature visualization charts \u2013 similar to Neuronpedia [Lin and\nBloom, 2023]. Features can be added as feature cards to the canvas when you want to inspect them further.\nFor a selected card, our tool allows you to query for three different properties of the feature:\n\u2022 The feature effects.\n\u2022 The feature actions. These are the features that cause the selected feature to activate more often.\n\u2022 Cosine similar feature directions.\nIn addition, when a feature card is selected, other cards for the same feature are highlighted, making it easy to see here\nelse a feature is used at a glance. By clicking the magnifying icon on the top right of a feature card, the top panel will\nbe populated with the relevant information for that feature.\nPrior to developing EffectVis, we had a multi-step process for exploring and making sense a feature's effects and/or\ncosine similarity. First, we would access the list of effects for a feature in a python notebook. Then, using Neuronpedia,\nwe would search for each feature one by one. These were then added to a list on Neuronpedia.\nDue to the time-consuming nature of the process, we were limited in how many features we could inspect, reducing\nour ability to build intuition for what was being measured with the feature effects. EffectVis solved this for us by\ncompressing the above steps into a single keyboard command action. It also made comparisons between feature effects\nand/or decoder cosine similarity easier. Lists of features could be pulled up side-by-side and compared visually. For\ncomparing many lists, selecting a card shows you the other lists it is a part of.\nEffectVis helped us develop the insights that led to our Optimized Steering Vectors method. In the future, it may useful\nto incorporate functionality for querying our feature effects by different filters such as effect scores as we did for part of\nour analysis in the feature effects section. This would allow us to perform more experiments and ask a wider variety of\nquestions using the interface that we resorted to using python notebooks to explore. We haven't rigorously compared\nEffectVis to alternative tools, as it is tailored for the standalone method that we developed."}, {"title": "7 Discussion", "content": null}, {"title": "7.1 Limitations", "content": "We performed all of our investigations using Gemma-2 2B and 9B, both of these models share the same distilled model\narchitecture, so our results may not transfer to other models. Additionally, these are base models which haven't been\ninstruction tuned, so we focused on steering in the open-ended generation setting and did not investigate tasks and\nfeatures which would be relevant for safety or capabilities.\nWe focused only on steering towards concepts present in SAEs, which is a problem for multiple reasons. Firstly, some\nconcepts don't appear as individual SAE features, a phenomenon called feature splitting. Secondly, niche features may\nonly appear in extremely large SAEs, as discussed in the \"feature completeness\" section of Templeton et al. [2024].\nThese problems are particularly relevant because our targeted steering method, as described in Section 4, can only steer\ntowards features present in the SAE used to measure feature effects. We explore a potential solution in Appendix D."}, {"title": "7.2 Future work", "content": "An important direction for future work is to apply our steering methods to chat models and safety-relevant steering\ntargets to evaluate effectiveness in practical scenarios. Additionally, this work can be extended by exploring different\nSAE architectures, such as TopK SAEs [Gao et al., 2024], and testing on various model architectures beyond the\nGemma family. Another direction to explore is adapting BiPO [Cao et al., 2024] to optimize steering vectors towards\ndesired feature effects."}, {"title": "Appendix", "content": null}, {"title": "A Targeted Steering Method Justification", "content": null}, {"title": "A.1 Empirical justification", "content": "As described in Section 4, our targeted steering method aims to find a steering vector $s$ that increases activation of a\ntarget feature while minimizing unintended effects. Given our effect approximator function $\\hat{y} = xM + b$, an obvious\napproach would be to directly solve for the optimal steering vector $x$ by computing the pseudoinverse of $M$. However,\nempirically we found that our method (SAE-TS), which takes the normalized target feature column of $M$ and subtracts\nthe normalized bias effects, significantly outperforms the pseudoinverse approach, as shown in Table 3."}, {"title": "A.2 Why do this work? Intuition", "content": "If we ignore the bias term, using the normalized $M_j$ is equivalent to linear regression and gives the steering direction\nthat maximizes predicted effect on feature $j$ subject to the unit norm constraint, while ignoring effects on other features.\nWhy does ignoring predicted effects on other features produce good results?\nAn important part of the story is the data on which the effect approximator was trained. The steering vectors used to\ngenerate the data were added at a scale $\\alpha$ chosen to increase the model's loss by 0.5. This scaling encodes information\nabout the model's sensitivity to different directions - directions that strongly affect many features require a smaller $\\alpha$ to\nreach the target loss increase.\nThen, when training the effect approximator, the steering vectors are normalised to have norm 1. As a result, $M_j$\nimplicitly accounts for effects on other features. A steering direction that strongly affects other features will show up in\nthe training data with smaller effect magnitudes due to the smaller $\\alpha$ used."}, {"title": "A.3 Effect of the bias term", "content": "The effect approximator's bias vector $b$ has a few features with large values while most are close to 0. The high-bias\nfeatures include three that activate on the <BOS> token, an induction feature that activates on repeated patterns, and a\nposition feature that activates on all token positions > 2. All the largest features are densely activating, as shown in\nTable 4.\nWe suspect that the the initial Coherence bump that we observe in Section 5 is due to the bias term $(-M_b)$ in\nEquation (3). We can see this in Figure 5 where we plot the Coherence score when steering with just $-M_b$ against\nsteering with random normal vectors.\nOne possible reason for the coherence bump could be due to some \"incoherent text\" features which on average activate\nmore when we steer the model as the model is degraded. Then, when we subtract the bias term, the model produces\nmore coherent outputs.\nWe notice some quirks when steering with the bias term \u2013 notably, it seems to increase the frequency of proper nouns in\nthe generated text, with almost every rollout containing a proper noun at scale 100. This raises the possibility that the\napparent increase in coherence might be an artifact of our evaluation setup, where the presence of proper nouns causes\ngpt4o-mini to assign higher coherence scores rather than reflecting genuinely more coherent text. Example rollouts can\nbe found in Appendix I.4."}, {"title": "B Example Evaluation Criteria", "content": "We provide an example of our evaluation setup for the Wedding task:\nBehavioral score criterion:\nThe text contains discussion about a wedding\nCoherence score criterion:\nThe text is coherent and the grammar is correct.\nEvaluation Setup: We use gpt4o-mini with the following prompts:"}, {"title": "C Gemma-2-9B Results", "content": "We perform the same evaluations as in Section 5 for the gemma-2-9B model and plot the results in Figure 6. We use the\nGemmascope layer 12 residual stream SAE with width 16k and an L0 of 130. It was easy to find relevant features for\nall the tasks except for the London task. There is no feature in this particular SAE which fires only on London, thus we\nuse a mix of the following three features: 6915 - large cities including London, 9983 - cities, 11748 - United Kingdom.\nWe see that SAE-TS is once again the best steering method achieving higher Behavioral*Coherence scores than either\nCAA or SAE steering in 8 out of the 9 tasks."}, {"title": "D Rotation Steering", "content": "Our SAE-TS method requires measuring feature effects for each feature we want to target. However, some features\nmight be too rare to appear in our measurement dataset, or might not exist in the SAE we use for measurement. To\naddress this, we develop rotation steering, a method that learns a transformation $f : \\mathbb{R}^{dmodel} \\rightarrow \\mathbb{R}^{dmodel}$ between SAE\ndecoder vectors and effective steering vectors.\nWe compute this transformation by first calculating the correlation matrix $C = MW_{dec}$. We then perform SVD on\nthis correlation matrix: $C = UE V^T$ and then the rotation matrix $R = U V^T$ gives us a transformation between the\nmodel's activation spaces. Next, similarly to the process described in Section 4, we generate a steering vector for any\nSAE feature $i$ by taking its decoder vector $d_i$ (the $i$th row of $W_{dec}$) and apply the learned rotation to get $v = d_i R$, we\nthen normalise $v$ and subtract (normalised) $M_b$.\nThis approach allows us to generate steering vectors for features not present in our original feature effects dataset, as\nlong as we can find a relevant feature in any SAE trained on the same model layer.\nIn Table 5 we see that rotation steering performs reasonably well but is on average worse than SAE-TS, at least for the\nspecific tasks we test on."}, {"title": "E Finding related groups of features", "content": "We find that by looking at features with similar feature effects, we can find groups of features that are thematically\nrelated more so than we can when looking at cosine similar features along their decoder vectors.\nProcess for curating the below example wedding feature group: Starting with the wedding feature, we recursively look\nat features with similar feature effects, adding the ones that we qualitatively believe fit into the feature group. We\nthen repeat this process looking at features with similar decoder vectors. The venn diagram below compares the set of\nfeatures that we could find looking at the SAE decoder vector vs. the feature effects."}, {"title": "F Different Prompt", "content": "Unless otherwise specified, all the steering score analysis in this paper was done with \"<BOS>I think\" as the starting\nprompt from which completions are generated. To verify that our results do not depend on this particular prompt, we\nrepeat the evaluation experiments the \"<BOS>Surprisingly,\" prompt (Figure 7). We see that the results are qualitatively\nsimilar to the \"<BOS>I think\" prompt."}, {"title": "I Example Rollouts", "content": null}, {"title": "I.1 SAE-TS London", "content": "Scale = 100 (optimal scale)\n<bos>I think these photos were taken at the National Portrait Gallery in London earlier\nthis year. The portraits collection features paintings of British actors Johnny Depp\nand Helen Mirren as\n<bos>I think I've written about my mum's beautiful family home in north London a few\ntimes, but it was my last weekend in London with my mum\n<bos>I think it's safe to say that the fashion scene in New York City continues to soar.\n\\n\\nAfter a few years of highlights from the city, including\n<bos>I think we've all been waiting for a new <em>Sherlock</em> series over in London\never since the actress Karen Wright announced that she would be leaving the\nScale = 160 (too high). At high steering scales, the model seems to mostly talk about fashion, designers, and art\nexhibitions.\n<bos>I thinkLondon designer Samantha Morton's kitchen installation recently designed\nin London inspired by the city. The Interior Design show hosted event hosted in\nLondon which saw Interiors Designer\n<bos>I thinkyou artist Victoria Morris's new exhibition at Somerset Gallery in London\nis celebrating the French capital's fashion and design scene with Works on Display,\n<bos>I thinkLondon's fashion scene is thriving with vibrant exhibits and exhibitions\ncurrently showing in The Royal Academy's Summer Exhibition,\nwhichtookreturnsfromLondonfavouritesMichel\",\n<bos>I thinkLondon's fashion scene continues to thrive thanks to the city's eclectic\ncommunities which celebrate the capital\\u2019s vibrant eateries and restaurants\nthissummer whichhave"}, {"title": "1.2 CAA London", "content": "Scale = 160. It does not mention London directly, but does talk about the Thames.\n\"<bos>I think the people on the other end of the phone is for support but not for our\nservice which they have no problem with\\n\\nI_have now put the same\",\n\"<bos>I think it is nice but we have only just visited once, we have never been on\nthe cruise this, you have to be a real ale one I can\",\n\"<bos>I think in a few short months it will be better than the Thames boat trip.\n\\nThis week starts\\n\\nNo 019 on Monday 4/\",\n\"<bos>I think it would be cool if you could take a bus that takes you to the wharf\nfor a pint on a Sunday morning and take a walk to the harbour\","}, {"title": "1.3 SAE London", "content": "Scale = 120. Does not mention London.\n\"<bos>I think I have seen them on before. It is a very good idea.\\n\\nThanks, and\nyes, it is a good idea..\\n\\nIf the market\",\n\"<bos>I think this is 369.\\n\\nI' 60 in oner\\n\\nYeah we go and get him\\n\\nWell\ndone, now for those\",\n\"<bos>I think you are still the same the first time I met you to make my wife'\nI know that it is no. 1 (I do not\",\n\"<bos>I think that' 2015 is an actually a correct time to open this book,\nas each of you will get in aneropolis after an\","}, {"title": "1.4 Steering with \u2013 Mb", "content": "Scale = 100.\n\"<bos>I think back on my childhood and remember growing up in Atlanta with my family\nand best friend.\\n\\nWe spent our summers playing baseball in the neighborhood\nand learning to\"\n\"<bos>I think the idea of The Black Keys' new video for \u201cLonely Phaser\" is inspired\nby a \u201cTwilight\u201d movie.\\n\\nShot in London and directed by\"\n\"<bos>I think we're officially in Fall. It's been a chilly season for the past\ncouple months and this week is no exception. While we'\"\n\"<bos>I think the new movie \"Still Alice\" based on the memoir of Alice Koda is one\nof the most compelling films of the year. The film follows the\""}]}