{"title": "A Review of Human Emotion Synthesis Based on Generative Technology", "authors": ["Fei Ma", "Yukan Li", "Yifan Xie", "Ying He", "Yi Zhang", "Hongwei Ren", "Zhou Liu", "Wei Yao", "Fuji Ren", "Fei Richard Yu", "Shiguang Ni"], "abstract": "Human emotion synthesis is a crucial aspect of affective computing. It involves using computational methods to mimic and convey human emotions through various modalities, with the goal of enabling more natural and effective human-computer interactions. Recent advancements in generative models, such as Autoencoders, Generative Adversarial Networks, Diffusion Models, Large Language Models, and Sequence-to-Sequence Models, have significantly contributed to the development of this field. However, there is a notable lack of comprehensive reviews in this field. To address this problem, this paper aims to address this gap by providing a thorough and systematic overview of recent advancements in human emotion synthesis based on generative models. Specifically, this review will first present the review methodology, the emotion models involved, the mathematical principles of generative models, and the datasets used. Then, the review covers the application of different generative models to emotion synthesis based on a variety of modalities, including facial images, speech, and text. It also examines mainstream evaluation metrics. Additionally, the review presents some major findings and suggests future research directions, providing a comprehensive understanding of the role of generative technology in the nuanced domain of emotion synthesis.", "sections": [{"title": "1 INTRODUCTION", "content": "AFFECTIVE computing is an interdisciplinary research\nfield that aims to endow computers with the ability\nto recognize, understand, express, and respond to human\nemotions [1], [2]. It integrates theories and methods from\nmultiple disciplines such as computer science, psychology,\nand cognitive science, attempting to reveal the essence of\nhuman emotions and apply it to human-computer inter-\naction and intelligent systems. The core goal of affective\ncomputing is to enable computers to perceive, understand,\nand express emotions like humans, thereby achieving more\nnatural and friendly human-computer interaction [3], [4],\n[5], [6].\nEmotion synthesis [7] is an important branch of affective\ncomputing, which aims to enable computers to generate\nemotional expressions similar to human emotions. This\nability can be realized through various common modalities,\nsuch as facial images, speech, and text. To achieve emotion\nsynthesis, researchers have proposed a series of traditional\nmethods by analyzing the characteristics of human emo-\ntional expressions and establishing mathematical models.\nThese models are then used to generate speech and facial\nexpressions with specific emotions using computers [8], [9].\nArtificial intelligence has made remarkable advances in\nsynthesizing human emotions, marking a significant break-\nthrough in the field. In particular, generative technology\nhas greatly improved the effect and application scope of\nemotion synthesis [10], [11], [12]. Compared with traditional\nmethods, these new models can automatically learn the\ncharacteristics of emotional expressions from massive data\nwithout relying on manually designed rules and models\n[13], [14], [15], [16]. With their powerful generation capa-\nbilities, generative models can generate emotional samples\nthat are highly similar to real data and more flexible, greatly\nexpanding the research boundaries in the field of emotion\nsynthesis. For example, some researchers use Autoencoders\n(AEs) [17] to generate speech with emotions. By modifying\nthis structure, they can extract speaker embeddings, isolate\ntimbre information, and control the flow of emotional at-\ntributes [18]. Other researchers use Generative Adversarial\nNetworks (GANs) [19] to generate facial images with spe-\ncific emotions. By controlling the input of the generative\nmodel, they can generate faces expressing different emo-\ntions such as happiness, sadness, and anger [20]. In recent\nyears, Diffusion Models (DMs) [21] and Large Language\nModels (LLMs) [22] have also been widely used in emo-"}, {"title": "2 REVIEW METHODOLOGY", "content": "To compile this extensive review on human emotion synthe-\nsis based on generative models, a systematic and rigorous\napproach was followed to ensure the comprehensiveness\nand relevance of the literature. The overall screening pro-\ncess is shown in Fig. 3. Initially, we conducted compre-\nhensive searches across key academic databases, including\nIEEE Xplore, ScienceDirect, and Google Scholar. The search\nstrategy combined general and modality-specific keywords\nrelated to generative models and emotion synthesis. Ex-\namples of search terms included \"facial emotion synthe-\nsis\u201d + \u201cgenerative models,\u201d \u201cspeech emotion synthesis\u201d +\n\u201cgenerative models,\u201d \u201cemotional face reenactment,\u201d and\n\u201cemotional voice conversion,\u201d etc., aiming to cover a wide\narray of studies in these domains. To ensure the inclusion\nof the most recent advancements, the search focused on\npapers published between 2017 and 2024, providing a com-\nprehensive overview of developments in this timeframe. To\nfurther broaden the scope, we included terms referencing\nspecific generative model architectures, such as AE, GAN,\nDM, LLM, and Seq2Seq. The initial search yielded more than\n270 papers.\nFurthermore, we established strict inclusion criteria for\nthe selection process: (1) Peer-reviewed papers published"}, {"title": "3 EMOTION MODEL", "content": "Emotion is commonly understood as a complex and ever-\nchanging state of mind and body, which can be triggered\nby various interactions [39], perceptions, or thoughts [40].\nIt encompasses a wide range of experiences, cognitive eval-\nuations, behavioral responses, physiological reactions [41],\nand communicative expressions. In the realm of human\ncognition, emotions play a crucial role in decision-making\n[42], shaping our perceptions, and guiding our interactions\nwith others [43].\nAs illustrated in Fig. 4, the study of emotions has re-\nsulted in the development of different theoretical models,\nwhich can be primarily categorized into discrete emotions\ntheory and multidimensional emotion theory [44]. In the\nmost basic discrete emotion framework, emotions are sim-\nply categorized as positive or negative, also known as polar-\nity [45], [46]. Within this framework, the term \"emotion\" is\noften replaced with \"sentiment,\" which sometimes includes\na neutral category as well. However, this sentiment cate-\ngorization is considered too simplistic for certain contexts.\nTherefore, the more detailed discrete emotion theory catego-\nrizes basic emotions that are universally recognized across\ncultures into six or eight types [47], [48], [49]. On the other\nhand, the multidimensional emotions theory suggests that\nemotions can be viewed along a continuous spectrum, often\ndefined by dimensions such as 2D (valence and arousal)\n[50] or 3D (valence, arousal, and dominance) [51]. These\ntheoretical perspectives provide valuable insights into the\ncomplex nature of human emotions and serve as founda-\ntional principles for emotion synthesis.\nBy considering emotions in generative tasks, machines\nnot only understand and process information, but also\nbecome attuned to the emotional dimensions of human\nexperience. This enriches the human-machine interaction\nlandscape and opens up new avenues for the development\nof empathetic and intuitive Al developments."}, {"title": "4 DIFFERENCE", "content": "The current state of research in affective computing pri-\nmarily focuses on areas such as human sentiment analysis\n[10], emotion detection, and emotion recognition [12]. These\ntasks have a long-standing tradition and are considered\nsignificant when viewed in a broader context. For instance,\nSaxena et al. [58] conducted a survey on emotion recognition\nmethods, examining facial, physiological, speech, and text\napproaches. They highlighted key techniques like Stationary\nWavelet Transform and Particle Swarm Optimization. In\nanother study, Nandwani et al. [59] analyzed sentiment\nanalysis and emotion detection methods, discussing the"}, {"title": "5 GENERATIVE MODEL", "content": "The generative model [61] refers to a model that can be\ndescribed as generating data, belonging to a type of proba-\nbility model. In machine learning, it can model data directly\nor establish conditional probability distributions between\nvariables via Bayes' theorem, allowing the generation of\nnew data not present in the training set. In this section,\nwe explain the mathematics of different generative models,\nincluding AE, GAN, DM, LLM, and Seq2Seq."}, {"title": "5.1 Auto-Encoder (AE)", "content": "AEs [17] are neural network models used in generative tasks\nto efficiently learn data representations. These models com-\npress input data into simplified patterns, then reconstruct\nit by preserving key features while minimizing errors in\nthe reproduction process. As a variant of AE, Variational\nAuto-Encoders (VAEs) introduced by Kingma and Welling\nin 2013 [62], offer a principled approach to learning latent\ndata representations to account for data uncertainty and\nvariability, making them well-suited for generative tasks.\nThe training of VAEs is guided by the maximization of the\nEvidence Lower BOund (ELBO), which can be expressed as\nfollows:\nELBO = Eq_{q(z|x)} [log p_{o}(x|z)] - D_{KL}[q_{\u00a2}(z|x)||p(z)], (1)\nwhere the encoder, q_{q(z|x)}, maps the input data x to a dis-\ntribution over the latent space characterized by parameters\n4. Typically, this distribution is assumed to be Gaussian,\nwith the encoder outputting the mean and variance of\nthe distribution. The latent variable z, sampled from this\ndistribution, is then fed into the decoder, p_{o}(x|z), which\nattempts to reconstruct the input data, where o denotes the\nparameters of the decoder. In the Equation (1), the first term\nis the expected log-likelihood of the data given the latent\nvariables, encouraging accurate reconstruction of the data.\nThe second component measures how closely the encoded\ndata patterns match an expected statistical distribution p(z),\nusing the Kullback-Leibler divergence formula. By optimiz-\ning the ELBO, VAEs learn to balance the trade-off between\nfidelity in data reconstruction and adherence to a structured\nlatent space."}, {"title": "5.2 Generative Adversarial Network (GAN)", "content": "GANs [19] are distinguished by their unique training\nmethodology, which leverages the concept of adversarial\nlearning, setting up a dynamic competition between two\ndistinct neural networks: the generator and the discrimina-\ntor. The generator network, G, aims to map latent space\nvectors, drawn from a prior distribution p_{z}(z), to data\nspace, effectively generating new data samples that mimic\nthe distribution of real data, P_{data}(x). In contrast, the dis-\ncriminator network, D, is trained to distinguish between\nsamples drawn from the real data distribution and those\nproduced by the generator. The competition between net-\nworks steadily improves their performance, ultimately en-\nabling the generator to create lifelike outputs. The training\nof GANs is formulated as a min-max game, which can\nbe formally represented by the following value function\nV(G, D):\nmin_{G} max_{D} V (D, G) =E_{x~p_{data}(x)} [log D(x)]+\nE_{z~p_{z}(z)} [log(1 - D(G(z)))], (2)\nwhere the first term represents the expected log-probability\nthat the discriminator correctly identifies real data sam-\nples as real. The second term represents the expected log-\nprobability that the discriminator correctly identifies fake\nsamples (generated by G) as fake. Training GANs involves\nalternating between optimizing D to maximize V (D, G) for\nfixed G (improving D's accuracy in distinguishing real from\nfake samples) and optimizing G to minimize V(D,G) for\nfixed D (improving G's ability to generate realistic samples).\nThis adversarial training process continues until a state of\nequilibrium is reached where G generates samples indis-\ntinguishable from real data by D. The adversarial training\nmechanism of GANs has proven to be highly effective for\ngenerating complex, high-dimensional data."}, {"title": "5.3 Diffusion Model (DM)", "content": "DMs [21] have emerged as a class of powerful generative\nmodels that synthesize data by gradually refining random\nnoise into structured patterns. The fundamental principle\nbehind DMs involves two phases: a forward (noising) phase\nand a reverse (denoising) phase. In the forward phase, step\nby step, the model gradually adds random noise to the data"}, {"title": "5.4 Large Language Model (LLM)", "content": "Large Language Models are AI systems trained on massive\ntext datasets, using billions of parameters to learn language\npatterns. Their deep understanding of language enables\nhuman-like text comprehension and generation, transform-\ning how computers process natural language. The advent\nof Transformer architecture, developed by Vaswani et al. in\n2017 [22], marked a significant breakthrough in language\nmodeling.\nThe training objective of LLMs can generally be de-\nscribed as learning a conditional probability distribu-\ntion over sequences. Given an input sequence X =\n(x_{1},x_{2},...,x_{T}), the model maximizes the likelihood of\ngenerating each token based on prior tokens, represented\nas:\np(X) = \\prod_{t=1}^{T} P(x_{t}|X_{<t}) (5)\nwhere p(x_{t}|x_{<t}) is the probability of token x_{t} given its con-\ntext x_{<t}. A key feature of this process is the attention mech-\nanism, which allows the model to dynamically focus on\ndifferent parts of the context at each step. In self-attention,\nthe probability of generating each token is influenced by\na weighted sum of the context, with attention weights a_{tj}\ncomputed as:\na_{tj} = \\frac{exp(q_{t} k_{j})}{\\sum_{k=1}^{T} exp(t_{k})} (6)\nand the new token representation z_{t} is given by:\nz_{t} = \\sum_{j=1}^{T} A_{tj}V_{j} (7)\nBy analyzing relationships between tokens, the attention\nsystem helps the model understand context and produce\ncoherent, relevant text."}, {"title": "5.5 Sequence-to-Sequence (Seq2Seq) Model", "content": "Seq2Seq model [25] is a neural network architecture de-\nsigned for tasks involving sequential input-output pairs.\nIt follows an encoder-decoder structure, where each com-\nponent is typically implemented with recurrent neural net-\nworks (RNNs) or Transformers in later models.\nOne defining feature of Seq2Seq models is their focus\non sequential data, which differentiates them from models\nlike GANs or VAEs that do not inherently account for\nsequential dependencies. Unlike general language models,\nSeq2Seq models specialize in transforming one sequence\ninto another, effectively capturing both immediate and dis-\ntant patterns in the data.\nIn the Seq2Seq model, the encoder processes the input\nsequence (x_{1},x_{2},...,x_{T}) to produce a context vector c,\noften represented by the encoder's final hidden state:\nc = h_{T} (8)\nwhere h_{T} encapsulates the input sequence's essential infor-\nmation. The decoder then uses this context vector to gen-\nerate the output sequence (y_{1}, y_{2}, \u00b7 \u00b7 \u00b7, y_{T'} ) one element at a\ntime, conditioned on c and previously generated outputs:\ns_{t} = g(y_{t-1}, s_{t-1}, c) (9)\nP(y_{t}|y_{<t}, x) = softmax(Ws_{t}) (10)\nwhere s_{t} is the hidden state at time t, and W is a weight\nmatrix for calculating the output distribution.\nSeq2Seq models are effective for handling variable-\nlength input and output sequences, making them well-\nsuited for applications like translation, summarization, and\nquestion answering, where coherent sequence transforma-\ntion is required."}, {"title": "6 DATABASES", "content": "The performance of human emotion synthesis tasks based\non generative models is closely tied to the quality and\nrichness of the utilized datasets. To be specific, the diver-\nsity and scope of the datasets play a crucial role in the\nmodel's ability to generalize across various emotional states,\ncultural contexts, and individual differences. The structure\nand content of emotion databases directly shape how re-\nsearchers design and build emotion synthesis models. The\nstructure, annotation scheme, and inherent biases of the\ndatasets influence the choice of model architecture, loss\nfunctions, and training strategies. Furthermore, the size and"}, {"title": "7 FACIAL EMOTION SYNTHESIS", "content": "Facial emotion synthesis is a crucial research field within\nhuman emotion synthesis, aiming to generate faces that ex-\npress specified emotions. This technology holds significant\nacademic value in computer graphics and computer vision,\nwhile also demonstrating great promise for applications\nin virtual reality (VR), gaming, and interactive computer\nsystems. Based on existing works, we can broadly categorize\nfacial emotion synthesis into three main approaches: face\nreenactment (Section 7.1), talking head generation (Section\n7.3), and facial manipulation (Section 7.2). The related works\nare illustrated in Table 5."}, {"title": "7.1 Face Reenactment", "content": "Face reenactment focuses on transferring facial expressions\nfrom a source actor to a target face, preserving the identity\nof the target while adopting the emotional expressions of the\nsource. This technique is particularly useful in applications\nlike film dubbing, virtual avatars, and privacy-preserving\nvideo conferencing.\nIn facial reenactment, there are some tasks that empha-\nsize emotional attributes in the face. For example, in [90],\nTripathy et al. introduced ICface, a GAN-based face anima-\ntor that manipulated facial expressions in a given image.\nThe animation process was guided by interpretable control\nsignals, such as head pose angles and Action Units (AU)\nvalues, which were derived from various sources, allowing\nfor selective emotion transfer. Zeng et al. [91] proposed\nDAE-GAN, which employed two deforming autoencoders\nto separate identity and pose in unlabeled videos, reducing\nthe need for manual annotation. It realized emotional trans-\nfer between different identities with varied poses using con-\nditional generation and disentangled features. Strizhkova\net al. [92] proposed a novel method for emotion editing in\nhead reenactment videos by manipulating the latent space\nof a pre-trained GAN. This technique disentangled emotion,\nidentity, and pose within the latent space, allowing for the\ndirect modification of emotions in the reenactment videos\nwithout affecting the person's identity or the speech-related\nfacial expressions. Groth et al. [93] designed a new method\nto achieve emotion mapping by generating correctly rec-\nognized expressions, using video reenactments to influence\nthe intensity of the emotion. In [94], Ali et al. utilized two\nencoders to separately capture expression from a source and\nidentity from a target image, merging these features to cre-\nate expressive images, enhanced by innovative consistency\nlosses for both expression and identity features. In Fig. 5,\nXue et al. presented LSGAN [95], employing a transforma-\ntive generator that combined target expression labels with\nspecific facial region features to produce clear and distinct\nfacial expressions in images. Shao et al. [96] utilized dual\nparallel generators and wavelet-based discriminators for\nfacial expression translation, enhancing realism by focusing\non key areas with an attention mechanism and capturing\nexpression details across scales without the bidirectional\ntranslation interference seen in single-generator models."}, {"title": "7.2 Face Manipulation", "content": "Face manipulation involves editing specific attributes of a\nface, such as changing expressions, age, hairstyles, etc., to\ngenerate different versions of the same person. It can be\nseen that this has a completely different goal compared to\nface reenactment which transfers the expressions and move-\nments of a source face to a target face. Researchers in this\nfield focus on the alteration of specific facial attributes while\npreserving the remaining attributes unchanged, under the\ncondition of explicit predefined facial information, thereby\neffecting changes in emotional expression. Works in this\nfield are mainly based on GAN and its variants [20], [30],\n[97], [98], [99], [100], [101], [102], [103], [104], [105], [106]. For"}, {"title": "7.3 Talking Head Generation", "content": "Talking head generation aims to create realistic, animated\nfacial models that can speak and emote based on input\naudio or text. This approach is particularly valuable in creat-\ning virtual assistants, digital newscasters, and personalized\ncontent for educational or entertainment purposes.\nA series of works that incorporate emotion synthesis\ninto talking head generation have been explored. For ex-\nample, Eskimez et al. [116] presented a novel system for\ngenerating talking faces, achieving independent control of\nemotional expressions by disregarding the emotions ex-\npressed in the input speech audio and instead conditioning\nthe face generation on an independent categorical emotion\nvariable. In [117], Vougioukas et al. developed a special-\nized GAN that uses three discriminators to create detailed\nfacial expressions that match a speaker's emotional state.\nAs illustrated in Fig. 6, Zhang et al. presented EmoTalker\n[23], a framework for emotionally editable talking face gen-\neration, utilizing a diffusion model and a novel Emotion\nIntensity Block, and integrating a custom dataset to enhance\nemotion interpretation. In [118], Zeng et al. presented ET-\nGAN, an end-to-end system for generating talking faces\nwith tailored expressions from guiding videos, identity im-\nages, and arbitrary audio, utilizing multiple encoders for\nidentity, expression, and audio-lip synchronization, along-\nside advanced frame and spatial-temporal discriminators.\nGan et al. [119] generated synchronized emotion coeffi-\ncients and emotion-driven facial images through flow-based\nand vector-quantized models, while using lightweight emo-\ntion prompts or CLIP supervision for model control and"}, {"title": "8 SPEECH EMOTION SYNTHESIS", "content": "Speech emotion synthesis is a critical research field within\nhuman emotion synthesis, focusing on the manipulation\nand generation of acoustic features in speech signals to alter\nor create specific emotional states. This area of study aims\nto modify key vocal parameters such as volume, intonation,\npitch, speaking rate, and timbre to effectively convey a\ndesired emotional state in synthesized speech. Based on\nexisting works, we divide it into voice conversion (Section\n8.1),text-to-speech (Section 8.2) and speech manipulation\n(Section 8.3). Table 6 illustrates the overall papers about\nspeech emotion synthesis."}, {"title": "8.1 Voice Conversion", "content": "Voice conversion technology modifies a speaker's voice to\nexpress different emotions while keeping the original words\nintact. This enables emotional dubbing in films and games,\nand helps create more natural-sounding AI assistants.\nFor instance, in [76], Zhou et al. leveraged VAW-GAN\nand deep emotional features from speech emotion recog-\nnition to describe emotional prosody. By using adjustable\nemotional features to guide the decoder, this approach\ncould transform speech to express both familiar and new\nemotions. In [137], they combined VAW-GAN and Con-\ntinuous Wavelet Transform (CWT) for advanced spectrum\nand prosody conversion. By incorporating CWT-based F0\nmodeling, the system uniquely enhanced the granularity\nof prosody representation. In [138], they focused on disen-\ntangling and re-composing emotional elements in speech,\ninnovatively employing CWT for detailed prosody analysis\nand integrating F0 conditioning in the decoder to enhance\nemotion conversion performance. Similarly, in [139], they\nproposed a CycleGAN-based model [140] that did not\nrequire parallel data. It utilized CWT to analyze FO on\nmultiple scales, enabling detailed prosody modification. In\n[141], Fu et al. also presented an improved CycleGAN-\nbased model that incorporated a transformer to augment\ntemporal dependencies and integrated curriculum learn-\ning and a fine-grained level discriminator, enhancing the\nmodel's ability to capture and convert emotional nuances\nin speech more effectively. Rizos et al. [142] utilized class-\nconditional GAN and an auxiliary domain classifier to gen-\nerate emotional speech samples. In [143], Elgaar et al. in-\ntroduced a Factorized Hierarchical Variational Autoencoder\n(FHVAE) for multi-speaker and multi-domain emotional\nvoice conversion, enhancing disentangled representation\nand emotion conversion quality through novel algorithms\nand loss functions. Gao et al. [144] used style transfer\nautoencoders for emotional voice conversion without par-\nallel data, leveraging disentangled representation learning\nto modify emotion-related characteristics. In Fig. 7, Chen et\nal. introduced a Tacotron2-based framework using emotion\ndisentangling modules [145] to achieve cross-speaker emo-\ntion transfer by separating speaker identity from emotion. In\n[146], Oh et al. proposed the DurFlex-EVC model, integrat-\ning a style autoencoder and unit aligner for advanced con-\ntrol and flexibility. It leveraged HuBERT features, denoising\ndiffusion models, and self-supervised learning to modify\nemotional tones while maintaining linguistic content and\nunique vocal traits. Moreover, there were also some works\nrelated to emotional voice conversion based on StarGAN\n[147], [148], [149], AE [150], and Seq2Seq [151], [152], [153]."}, {"title": "8.2 Text-to-Speech (TTS)", "content": "TTS aims to generate natural-sounding speech based on the\nsemantic content and context of the input text. This tech-\nnology is particularly crucial for enabling more engaging\nand human-like interactions with virtual assistants, audio-\nbook narrations, and personalized content delivery. Some\nresearchers have attempted to incorporate emotional factors\ninto TTS systems to synthesize speech with emotional ex-\npressiveness.\nFor example, in [154], Lei et al. introduced a unified\nmodel for fine-grained emotional speech synthesis, obtain-\ning fine-grained emotion expressions with emotion descrip-\ntors or phoneme-level manual labels. Li et al. [155] de-\nveloped DiCLET-TTS, which improves emotional speech\nsynthesis by combining emotion separation techniques with\nadvanced probability-based decoding. In [156], Tits et al. ex-\nplored adapting a Deep Convolutional TTS (DCTTS) model\nto various emotions using minimal emotional data. In [157],\nSchnell et al. leveraged WaveNet and emotion intensity\nextraction using attention LSTM and transformer models,\ndemonstrating increased perceived emotion accuracy. Wu et\nal [158] synthesized emotional speech with limited labeled\ndata, achieving comparable performance to fully supervised\nmodels. In [159], Um et al. employed an inter-to-intra dis-\ntance ratio algorithm and an effective interpolation tech-\nnique to achieve nuanced emotion intensity control. In [81],\nIm et al. proposed EmoQ-TTS, a system that synthesized\nexpressive emotional speech by conditioning phoneme-wise\nemotion information with fine-grained emotion intensity,\nusing intensity pseudo-labels generated via distance-based\nintensity quantization. Hortal et al. [160] combined Tacotron\n2 with GANs to modulate prosody, allowing customiza-\ntion of inferred speech with specified emotions. Guo et al.\n[161] presented \"EmoDiff,\" a DM-based model that enabled\nintensity-controllable emotional speech synthesis using a\nsoft-label guidance technique. Li et al. [162] utilized the\nTacotron framework, enhanced with emotion classifiers and\nstyle loss, to generate expressive, controllable emotional\nspeech efficiently. In [163], Lei et al. introduced a novel\nmethod integrating global-level, utterance-level, and local-\nlevel modules to achieve precise emotion modeling and"}, {"title": "8.3 Speech Manipulation", "content": "Speech manipulation focuses on modifying various aspects\nof speech signals, such as the speaker's identity or linguistic\ncontent. It shares similarities with face manipulation in Sec-\ntion 7.2, as both aim to alter or control specific attributes of\nhuman-generated data, be it facial features or speech char-\nacteristics. Recently, some researchers have begun exploring\nhow to manipulate the emotional attributes of speech.\nFor instance, in [79], Jia et al. presented ET-GAN, an in-\nnovative cross-language emotion transfer system for speech\nsynthesis, which was unique for not necessitating parallel\ntraining data and utilized CycleGAN, achieving signifi-\ncant improvements in emotional accuracy and naturalness\nof synthetic speech. In [167], Matsumoto et al. utilized\nWaveNet and auxiliary features like voiced, unvoiced, and\nsilence flags to generate speech-like emotional sounds with-\nout linguistic content, enhancing emotional expressiveness\ncontrol. In [168], Wang et al. presented Emo-CampNet,\na text-based speech editing model. It integrated emotion\nattributes and a context-aware mask prediction network,\nemploying generative adversarial training and data aug-\nmentation to enhance emotional expressiveness and speaker\nvariability in edited speech. In [36], Inoue et al. introduced a\nnovel emotion editing technique in speech synthesis utiliz-\ning a hierarchical emotion distribution extractor within the\nFastSpeech2 framework [169], enabling fine-grained, quan-\ntitative emotion control at phoneme, word, and utterance\nlevels for dynamic and nuanced speech generation."}, {"title": "9 TEXTUAL EMOTION SYNTHESIS", "content": "Textual emotion synthesis is a vital research field within\nhuman emotion synthesis, focusing on the generation of\ntexts that possess specific emotional, sentiment, or em-\npathetic attributes. This area of study aims to create or\nmodify written content to convey desired emotional states,\nsentiments, or empathetic responses, thereby enhancing the\nexpressiveness and impact of textual communication. Based\non existing works, we consider the following two types of\ntasks: text emotion transfer (Section 9.1) and empathetic di-\nalogue generation (Section 9.2). Table 7 shows the literature\nabout textual emotion synthesis."}, {"title": "9.1 Text Emotion Transfer", "content": "Text emotion transfer focuses on transforming the emotional\ntone or sentiment of an existing text while preserving its\ncore semantic content. It allows for the modification of neu-\ntral text into emotionally charged content, or the alteration\nof one emotional state to another.\nFor example, in [184], Mohammadibaghmolaei et al. pro-\nposed a text emotion transfer technique based on masked\nlanguage modeling and transfer learning, and a GPT-2\nmodel underwent training to construct an initial sentence\nbased on its altered sequences, allowing the model to per-\nform efficiently even with limited emotion-annotated data.\nA novel text sentiment transfer methodology was proposed\nby Li et al. [26] in which they employed a three-step\nprocess\u2014Delete, Retrieve, and Generate. This approach,\npowered by unsupervised learning and neural sequence-to-\nsequence models, effectively altered sentiment while retain-\ning content. In [185], Jin et al. introduced the IMaT model\nthat constructed a pseudo-parallel corpus through semantic\nalignment, then applied a sequence-to-sequence model for\nattribute translation, refining this alignment across itera-\ntions. Wu et al. [186] proposed a two-stage \"Mask and Infill\"\nmethodology that significantly enhanced the performance\nof non-parallel text sentiment transfer. Following the \"mask\nand infill\" method, in [187], Malmi et al. introduced an\ninnovative unsupervised method using padded masked\nlanguage models (MLMs) for sentiment transfer, using a\npadded MLM variant to avoid having to predetermine the\nnumber of inserted tokens. In [188], Yang et al. presented a\ntechnique leveraging language models as discriminators for\nunsupervised sentiment manipulation, enhancing stability\nand content fidelity in generated text. In [189], Shen et al. de-\nveloped a technique for text sentiment transfer without par-\nallel data, utilizing refined alignment of latent representa-\ntions, which effectively separated content from style, allow-\ning for sentiment modification by mapping sentences to a\nstyle-independent content vector, then decoding this vector\ninto another style. Zhang et al. [190] employed GAN for sen-\ntiment transfer across different text domains, innovatively\ncombining adversarial and reinforcement learning with a\ncross-domain sentiment transfer model, enhancing the abil-\nity to generate emotionally nuanced text while maintaining\ndomain-specific content. In Fig. 9, Riley et al. leveraged\nT5 to extract a style vector from preceding sentences and\nused it to provide extra conditioning for the decoder [183].\nHuang et al. introduced a method [191] based on Cycle-\nconsistent Adversarial AutoEncoders, comprising LSTM au-\ntoencoders, adversarial style transfer networks, and cycle-\nconsistent constraints, innovating unsupervised text style\ntransfer. In [192], Luo et al. proposed the Seq2SentiSeq\nmodel, incorporating sentiment intensity via a Gaussian\nkernel in the decoder, enhancing sentiment control. They\ntrained the model using cycle reinforcement learning, which\nmaintains the original message while changing emotional\ntone without requiring matched data pairs."}, {"title": "9.2 Empathetic Dialogue Generation", "content": "Empathetic dialogue generation is a crucial aspect of creat-\ning more human-like and emotionally intelligent conversa-\ntional agents. It goes beyond simply generating contextually\nrelevant responses and focuses on incorporating emotional\nunderstanding and support into the generated dialogue.\nA lot of researchers focused on how to generate re-\nsponses with specific emotional tendencies or more empa-\nthy by LLMs. For example, in [193", "194": "provided a Hybrid Empathetic\nFramework (HEF) that used SEMs as flexible enhancements\nto LLMs, implementing a two-stage emotion prediction\nstrategy and an emotion cause perception strategy. In [195"}]}