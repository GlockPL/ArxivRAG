{"title": "Self-training Room Layout Estimation via Geometry-aware Ray-casting", "authors": ["Bolivar Solarte", "Chin-Hsuan Wu", "Jin-Cheng Jhang", "Jonathan Lee", "Yi-Hsuan Tsai", "Min Sun"], "abstract": "In this paper, we introduce a novel geometry-aware self-training framework for room layout estimation models on unseen scenes with unlabeled data. Our approach utilizes a ray-casting formulation to aggregate multiple estimates from different viewing positions, enabling the computation of reliable pseudo-labels for self-training. In particular, our ray-casting approach enforces multi-view consistency along all ray directions and prioritizes spatial proximity to the camera view for geometry reasoning. As a result, our geometry-aware pseudo-labels effectively handle complex room geometries and occluded walls without relying on assumptions such as Manhattan World or planar room walls. Evaluation on publicly available datasets, including synthetic and real-world scenarios, demonstrates significant improvements in current state-of-the-art layout models without using any human annotation.\nKeywords: Self-training Room Layout Estimation Multi-view Layout Consistency", "sections": [{"title": "1 Introduction", "content": "While significant progress has been made in room layout estimation, current state-of-the-art solutions predominantly rely on supervised frameworks, utilizing either monocular panoramic images [9, 21, 22, 27] or direct geometry sensors like depth cameras or LiDAR [2,23]. However, this reliance presents a significant challenge for real-world applications due to variations in geometry complexity and scene conditions, thereby making data collection and manual labeling particularly cumbersome.\nA practical solution for self-training a geometry-based model in unseen environments is by exploiting the multi-view consistency from multiple noisy estimations [7, 12]. However, applying multi-view consistency for room layout estimation has been poorly explored in the literature. For instance, recent approaches in multi-view layout estimation [8, 13, 19] particularly rely on ground truth annotations to define important concepts such as wall occlusion and wall match correspondences. Other solutions avoid partial dependency on label annotation"}, {"title": "2 Related Work", "content": "Room Layout Estimation. Estimating the room layout geometry is a long-standing problem, where earlier works [3, 26, 28] mainly rely on key features, semantic cues, and prior geometries to reason about the underlying geometry. While deep learning solutions for this task have brought robustness in the estimation by leveraging supervision from labeled data [6, 10, 29, 31], most of these solutions define the problem as a regression map task. An outstanding solution that changes this paradigm is HorizonNet [21], which redefines the optimization as an 1D boundary regression problem, simplifying the definition for the layout geometry. Upon this solution, approaches like [22] have impressive results by leveraging a simple layout definition. Another advance is LED2Net [27] and LGTNet [9], which introduces a horizon-depth vector definition, constraining the layout geometry directly on Euclidean space. Upon this solution, recent approaches [5,30] present further constrains during training, none of them targeting multi-view consistency.\nMulti-view Layout. Recent approaches in multi-view setting [8, 13, 19] define the multi-view layout estimation problem jointly with camera pose registration. In particular, [8] introduces important concepts for geometry reasoning, such as layout occlusion and layout match correspondences strictly relying on ground truth annotations. An outstanding solution in this manner is Graph-Covis [13], which is built upon [8] to define a multi-view setting capable of estimating layout and camera pose from multi-views using a graph neural network approach. Nevertheless, these solutions rely on ground truth annotations for reasoning the underlying geometry.\nSemi-Supervised and Self-training Layout Estimation. Semi-supervision and self-training methods aim to define a reliable reference to constrain the learning optimization without ground truth annotations [11]. Along this line, SSLayout360 [25] utilizes a Mean Teacher framework [24] to train a layout estimation model using pseudo-labels from a exponential-moving-average operation. However, [25] treats each image in isolation, neglecting valuable geometric information from alternate camera views. Furthermore, the challenge arises from"}, {"title": "3 Proposed Method", "content": "The following outlines our proposed self-training framework for room layout estimation. In Sec. 3.1, we describe the multi-view layout consistency problem (MLC) as well as the preliminaries for self-training room layout models. In Sec. 3.2, we present our ray-casting data aggregation process to create geometry-aware pseudo-labels solely from estimated data. Lastly, in Sec. 3.3, we present our weighted loss formulation towards leveraging the farthest distant geometry in a scene. For illustration purposes, an overview of our self-training framework is depicted in Fig. 2."}, {"title": "3.1 Self-training Room Layout with Multi-view Layout Consistency", "content": "In general, self-training a room layout model by multi-view layout consistency (MLC) aims to fine-tune a pre-trained model with reliable pseudo-labels computed from multiple estimations along an unseen scene [18]. This scene with n views can be defined as follows:\n$S = \\{(I_i, T_i)\\}_{i=1:n}, I_i \\in \\mathbb{R}^{H\\times W}, T_i \\in SE(3),$\nwhere S is the set of inputs views, $I_i$ represents a panoramic image of size $H \\times W$ pixels, and $T_i$ is the corresponding camera pose with rotation $R_i \\in SO(3)$ and translation $t_i \\in \\mathbb{R}^3$ defined in world coordinates. For any view in the set S, we can define an estimated layout geometry as follows:\n$y_i = \\pi(f_\\Theta(I_i), T_i), y_i \\in \\mathbb{R}^{3\\times W},$\nwhere $f_\\Theta$ is a layout model parameterized by $\\Theta$, $\\pi(\\cdot)$ is a projection function that transforms the model's prediction into the Euclidean space, and $y_i$ is the estimated layout geometry registered in world coordinates. For simplicity, we refer to $y_i$ as the floor boundary only. For layout models such as [21,22], $\\pi(\\cdot)$ processes a 1D boundary vector defined in spherical coordinates, while models [9, 27] handle a 1D horizon-depth estimation. A closed-form definition for both is described in our supplementary material.\nBy estimating multiple layouts from every view in the scene, we can define the pseudo labeling process as follows:\n$Y = concat(\\{y_i\\}_{i=1:n}), Y \\in \\mathbb{R}^{3 \\times nW}$\n$Y_i = R_i Y + t_i, y_i = \\Phi(Y_i), \\bar{y}_i \\in \\mathbb{R}^{3\\times W},$\nwhere Y is the concatenation of n layout geometries estimated by Eq. (2), $Y_i$ stands as the rigid transformation of Y into the i-th camera reference, and $\\Phi(.)$ is the aggregating function that estimates a pseudo-label $\\bar{y}_i$ for the i-th view in the scene.\nNote that, in the case of 360-MLC [18], $\\Phi(.)$ is the function that samples the median values of re-projected points in the image domain without any geometry reasoning, see Fig. 1-(c). In Sec. 3.2, we redefine $\\Phi(.)$ as a ray-casting function for computing geometry-aware pseudo-labels.\nThe self-training optimization of $f_\\Theta$ with multiple pseudo-labels $\\bar{y}_i$ can be defined as follows:\n$\\min_\\Theta \\frac{1}{n} \\sum_{i=1}^n w_i \\cdot L(f_\\Theta(I_i), \\pi^{-1}(\\bar{y}_i)),$\nwhere $\\pi^{-1}(.)$ is the inverse function presented in Eq. (2), $w_i \\in \\mathbb{R}^W$ is a weighted vector associated to the uncertainty in each pseudo-label $\\bar{y}_i$, and $L(.)$ is the loss function that constraints the self-training optimization.\nNote that, in the case of 360-MLC [18], The self-training constraint is defined as a weighted L1 loss with $w_i = \\sigma_i^{-2}$, where $\\sigma_i$ is the standard deviation of"}, {"title": "3.2 Pseudo-labeling by Ray-casting", "content": "Probability distribution on a ray. We hypothesize that the projection of multiple layout estimates onto a ray can describe a probability distribution of the underlying geometry. This distribution can then serve as the basis for sampling reliable pseudo-labels. To this end, we propose a ray-casting formulation that projects multiple estimates of a pre-trained model into a set of ray directions defined in the bird-eye-view (BEV), i.e., ray vectors defined in the xz Euclidean plane. This is motivated by previous works [9,27] to represent a room layout geometry directly in the Euclidean space, avoiding distortion and discrete issues presented in the image domain.\nWe define a set of ray directions in world coordinates as follows:\n$\\mathbb{R} = \\{r_j\\}_{j=1:w}, r_j \\in \\mathbb{R}^3, |r_j| = 1,$\n$V = \\{n_j\\}_{j=1:W}, n_j \\in \\mathbb{R}^3, n_j \\cdot r = 0,$\nwhere $r_j$ is a ray direction constrained by $r_j \\cdot [0,1,0]^T = 0$ (i.e., on the xz Euclidean plane), and $n_j$ is its corresponding normal vector. Then, a pseudo-label from a probability function defined on a ray vector can be defined as follows:\n$\\bar{y}_{ir} = E[P_r(Y_i)]r, r \\in \\mathbb{R},$\nwhere r is a ray vector introduced by Eq. (5), $Y_i$ is the concatenation of all estimated layouts in the i-th camera reference as presented in Eq. (3), $\\bar{y}_{i,r}$ stands for the i-th pseudo label defined on the ray r, and $P_r(.)$ is the unknown probability function along a ray direction r. For simplicity, we refer to this probability function as $P_r$.\nRegardless of the noise within the estimated layout geometries, the density function $P_r$ may vary significantly for every camera view and ray direction, in"}, {"title": "Multi-cycle ray-casting for pseudo-labeling.", "content": "To tackle occlusions, we condition $P_r$, presented by Eq. (6), in three ways. First, we increase the sample count near each ray direction and camera view based on the intuition that a higher sample count may enhance the representation of non-occluded geometries. Second, similar to 360-MLC [18], we approximate the expectation of projected samples to median(.) for filtering out noisy estimates, i.e., the median value of points on the ray. However, instead of sampling from a unique view (in the image domain), we sample them from multiple camera locations and ray directions in an iterative process named multi-cycle ray-casting (see Fig. 2). This stems from the fact that sampling over $P_r$ from multiple camera locations and directions must yield the same underlying room geometry. Finally, following the noise reduction, we approximate the expectation of $P_r$ to the closest sample on the ray. This is based on the understanding that non-occluded geometries must lie at the closest point along the ray direction. This is illustrated in Fig. 3-(c), where the pseudo-label for the camera view $T_j$ (magenta contour) is computed by sampling points on the rays by using the min() function.\nWith a slight notation abuse, the projection of nearby estimates onto a ray direction can be defined as follows:\n$\\Omega_{r_j}(Y_i) = \\{r\\cdot x | \\forall x \\in Y_i\\}$\nst.\n$0 < r \\cdot x < \\delta_r,$ and $|n \\cdot x | \\leq \\delta_n,$\nwhere x is a 3D-point $\\in \\mathbb{R}^3$ defined in $Y_i$, r and n are ray-vectors define by Eq. (5), and $\\{\\delta_r, \\delta_n\\}$ is a set of hyper-parameters that allows us to filter out non-local points. This projection is illustrated in Fig. 3-(b), where the subset of points $\\Omega$ (magenta dots) is defined along the ray vector r. For simplicity, we refer to the probability of these projected samples as $P_\\Omega$\nThe multi-cycle ray-casting process to filter out noisy estimates can be described as follows:\n$Y_i^{(k+1)} = \\{\\text{median}(\\Omega_{r_j}(Y_i^{(k)}))r_j\\}_{i=1:n j=1:W},$\nwhere $Y_i^{(k)}$ stands for the layout estimates in the i-th camera reference at the k-th cycle. Note that this filtering process is evaluated from all camera views i and all ray directions rj.\nFinally, a pseudo label and its uncertainty from a filtered set of layout estimations can be evaluated as follows:\n$\\bar{y}_i = \\{\\min(\\Omega_{r_j}(Y_i^{(m)}))r_j\\}_{j=1:W},$\n$\\sigma_i^{(0)} = \\{\\text{std}(\\Omega_{r_j}(Y_i))\\}_{j=1:W},$"}, {"title": "3.3 Weighted Distance Loss", "content": "To complement our proposed ray-casting pseudo-labels resented in Sec. 3.2, we introduce a weighted loss formulation that particularly focuses on the farthest geometries within a room. This stems from the empirical evidence that pre-trained layout models tend to estimate more accurately the geometries closer to the camera view than those farther away. This limitation can be attributed, in part, to the datasets used for training, e.g., [2, 4], where room scenes are predominantly captured from the room center, and larger-sized rooms are less represented. Another contributing factor to this limitation is the difficulty in capturing accurate details for the farthest regions from a single view [9]. Therefore, we hypothesize that our pseudo-labels may present the most significant impact during self-training when targeting the farthest geometries in a scene.\nOur weighted formulation can be described as follows:\n$L_{WD} = \\sum_i w_i \\|y_i - \\bar{y}_i\\|_1$ with $w_i = \\frac{e^{\\kappa(||y_i|| - d_{min})}}{\\sigma_i},$\nwhere $||y_i||$ is the Euclidean norm of the pseudo labels computed by Eq. (9), $d_{min}$ is the distance from which we want to prioritize the self-training, $\\kappa$ is a hyper-parameter that allows us to control the weighting priority to the farthest geometries, and $\\sigma_i$ represent the standard deviation computed in Eq. (9). In Fig. 4, we compare our proposed weighted-distance function with traditional L1 loss [18,21,22, 27]. Note that a $L_1$ evaluation does not aim at any particular geometry in the scene, while our proposed $w_i$ aims at the farthest walls from the camera view."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nBaseline and Model Backbones. The baseline used in the following experiments is the recent 360-MLC [18] taken from the official implementation provided by the authors. For a fair comparison with 360-MLC, we use the same layout model backbone by default, i.e., HorizonNet [21] pre-trained in [2]. To further compare our proposed solution, we present results using LGTNet [9] pre-trained on [2] as an additional layout model backbone.\nDatasets. Similar to 360-MLC [18], we show evaluations in the MP3D-FPE dataset [17]. We also show results on the real-world ZInD dataset [4]. In addition, we show results in our newly collected dataset rendered from Habitat-v2 [15], referred to as HM3D-MVL. In the case of the ZInD dataset, we use the layout category \"visible layout\" provided by the authors and select the scenes that contain at least five frames per room. For all the mentioned datasets, we compute pseudo labels from the training splits, self-train the pre-trained model, and evaluate results on the testing splits using ground truth annotations provided by the authors. To further corroborate our claim of handling occluded geometries, we also present evaluations on a manually selected subset of the testing split that contains samples with geometry occlusions only. We refer to this subset as Occlusion subset. Details of these datasets are present in Tab. 1.\nEvaluation Metrics. Following [9, 18, 21, 32], we evaluate results using standard metrics defined for room layout estimation. For room boundary prediction, we evaluate the 2D and 3D intersection-over-union (IoU). For evaluating the smoothness and consistency of layout depth maps, we evaluate root-mean-square (RMS) and $\\delta_1$ errors as defined in [9,21,27]. All experiments show the median results of 10 self-training runs, each consisting of 15 training epochs.\nImplementation Details. The layout models' backbones and their pre-trained weights used in our experiments are taken from their official implementation provided by the authors [9, 21]. To train the models, we use common data augmentation techniques for the room layout task, i.e., left-right flipping, panoramic rotation, and luminance augmentation. We use the Adam optimizer with a batch size of 4 and a learning rate $1 \\times 10^{-4}$ with a decay ratio of 90%. All models are trained on a single Nvidia RTX 2080Ti GPU with 12 GB of memory. For constructing our ray-casting pseudo-labels, we use 15 cycles per room scene, $\\delta_n = 20$ and $\\delta_n = 0.01$. For our weighted distance loss function, we use $\\kappa = 0.5$ and $d_{min} = 2$."}, {"title": "4.2 Quantitative Results", "content": "Evaluation using HorizonNet Backbone. In these experiments, we compare our proposed ray-casting self-training frameworks with the baseline 360-MLC [18], utilizing the HorizonNet layout model [21] pre-trained in [2]. The results are presented in Tab. 2 under two main settings: using 10% and 100% of the training set. Results in the 10% setting show that our proposed solution outperforms 360-MLC, even with a limited number of samples for self-training. Results in the 100% setting further demonstrate the improved performance of our proposed self-training framework.\nBy comparing results in the occlusion subset, we find evidence that our solution significantly outperforms 360-MLC. Particularly, while our proposed ray-casting self-training consistently improves performance with increased data, 360-MLC shows only marginal improvement and in some settings, presents a decline in performance. For instance, consider the evaluation of the occlusion subset of the HM3D-MVL dataset. When using only 10% of the data, 360-MLC achieves 81.66% 2D IoU. However, the result on the 100% setting shows a drop in performance to 79.19%. This suggests that 360-MLC contains a large amount of noisy pseudo labels such that increasing the amount of data significantly hurts the performance. We argue that the general benefit of our ray-casting pseudo-labels is mainly due to their strong reasoning capability on occluded geometries. Additionally, we present a comparison against the fully-supervised HorizonNet [21]"}, {"title": "4.3 Qualitative Results", "content": "Qualitative Results on Panoramic Images. For illustration purposes, we present in Fig. 7 several qualitative results of our proposed self-training framework compared with 360-MLC. Based on these results, we find that our solution shows a significant improvement in handling occluded geometries in all datasets."}, {"title": "4.4 Ablation Study for Weighted Distance Loss Formulation", "content": "We present an ablation study that validates our weighted distance loss formulation presented in Sec. 3.3. The results of this ablation are shown in Tab. 4. By comparing rows (a) and (b), we validate the gain in performance of self-training directly using our proposed ray-casting pseudo-labels without any weighting formulation. By comparing (c) and (b), we verify a weighted formulation based only on the uncertainty $\\sigma$ computed by Eq. (9). We can appreciate that this weighting formulation yields better performance on the occlusion subset but not for the whole testing set. We argue that a weighting formulation based on uncertainty $\\sigma$ does not consider any geometry information. In contrast, in row (d), we show the results of our weighted formulation as presented in Eq. (10). Thus we can assert that a weighting formulation that prioritizes the farthest geometries with respect to the camera view yields better performance."}, {"title": "5 Conclusions", "content": "In this paper, we present a geometry-aware self-training framework for multi-view room layout estimation that requires only unlabeled images as input. Our"}]}