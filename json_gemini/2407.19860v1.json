{"title": "Anomalous State Sequence Modeling\nto Enhance Safety in Reinforcement Learning", "authors": ["Leen Kweider", "Maissa Abou Kassem", "Ubai Sandouk"], "abstract": "The deployment of artificial intelligence (AI) in decision-making applications requires ensuring an appropriate\nlevel of safety and reliability, particularly in changing environments that contain a large number of unknown\nobservations. To address this challenge, we propose a novel safe reinforcement learning (RL) approach that\nutilizes an anomalous state sequence to enhance RL safety. Our proposed solution Safe Reinforcement Learning\nwith Anomalous State Sequences (AnoSeqs) consists of two stages. First, we train an agent in a non-safety-\ncritical offline 'source' environment to collect safe state sequences. Next, we use these safe sequences to build\nan anomaly detection model that can detect potentially unsafe state sequences in a 'target' safety-critical\nenvironment where failures can have high costs. The estimated risk from the anomaly detection model is utilized\nto train a risk-averse RL policy in the target environment; this involves adjusting the reward function to penalize\nthe agent for visiting anomalous states deemed unsafe by our anomaly model. In experiments on multiple safety-\ncritical benchmarking environments including self-driving cars, our solution approach successfully learns safer\npolicies and proves that sequential anomaly detection can provide an effective supervisory signal for training\nsafety-aware RL agents.", "sections": [{"title": "INTRODUCTION", "content": "Ensuring the safety and reliability of Artificial Intelligence\n(AI) is critical, particularly in decision-making applications\nin dynamically changing real-world environments that\nintroduce numerous unknown observations that may entail\nhigh costs or risks, making it crucial to ensure that Al\nsystems can operate safely and efficiently. Reinforcement\nLearning (RL) is a popular technique used in decision-\nmaking applications. However, its deployment in safety-\ncritical environments poses additional difficulties.\nOne of the primary challenges in deploying RL in real-\nworld scenarios is the high training cost in real\nenvironments. Traditional RL approaches rely on trial and\nerror, which can lead to undesirable outcomes in situations\nin which safety is paramount. Moreover, RL algorithms\ntypically focus on maximizing rewards, which may not\nalign with the safety considerations. Therefore, there is\ngrowing interest in developing RL techniques that\nprioritize safety alongside performance.\nAnother challenge of traditional RL algorithms is that they\noften assume that the environment operates as expected\nwithout considering the possibility of anomalous states or\nactions. However, in real-world scenarios, anomalies can\narise due to various factors such as sensor failures,\nadversarial attacks, or changes in the environment.\nTo address these limitations, we propose a novel safe RL\napproach named Safe Reinforcement Learning with\nAnomalous State Sequences (AnoSeqs), which leverages\nanomalous state sequences as a measure of unsafety. By\nconsidering the sequence of states over time, our approach\ncan identify potential safety risks that may not be apparent\nfrom single-state analysis. In particular, using state\nsequences rather than individual states for anomaly\ndetection is essential for safety-critical applications. State"}, {"title": "RELATED WORK", "content": "Several approaches have been proposed in recent years to\nenhance safety in reinforcement learning (RL) systems,\nparticularly in dynamic environments with high uncertainty\nand potential risks. In this section, we present a summary of\nnotable works and contributions to safe reinforcement\nlearning. [5] provided a foundational perspective on safe\nreinforcement learning, this survey offers a classification of\nmethods based on their approach to exploration and\noptimization and serves as a useful reference\nunderstanding the early development of Safe Reinforcement\nLearning. More recently, [6] provided a broader view of safe\nRL. The authors attempted to create a bridge between control\nsystems and RL. They categorized safety methods based on\nthe kind of constraints they satisfy, including hard\nconstraints, probabilistic constraints, and soft constraints.\nfor\nThe literature provides methods to improve RL safety at\nvarious stages of the RL process. We categorize these\napproaches based on where and how safety is integrated into\nthe RL process, highlighting key contributions and\nmethodologies. Related approaches include:\n\u2022 Optimization level safety:\nTraditional RL algorithms primarily focus on finding\noptimal control policies that guide actions to optimize a\ngiven criterion, typically cumulative rewards over time.\nHowever, these methods often neglect safety considerations,\nleading to potentially risky or unsafe actions. To address this\nissue, several approaches have been proposed to incorporate\nsafety by modifying optimization criterion in the\nreinforcement learning algorithm.\nOne type of optimization criterion modification is the use of\nconstrained optimization techniques, where the agent's\npolicy is optimized subject to safety constraints defined\nbased on domain-specific knowledge or predefined safety\nrules. Constrained Markov Decision Process (CMDP) was\nintroduced by [7]. In this approach, the RL agent is trained\nto maximize its expected reward while respecting a set of\nsafety constraints. These constraints ensure that the agent\noperates within acceptable boundaries, mitigating the risk of\ncatastrophic outcomes.\nFurther studies have explored the use of safety constraints\nin RL, as explored by studies such as [8-11].\nLagrangian algorithm [1]: Lagrangian method is a\nmathematical technique used to solve optimization problems\nwith equality and inequality constraints. In the context of RL,\nthis method can be used to optimize the policy to maximize\ncumulative expected rewards while satisfying certain\nconstraints. The Lagrangian approach allows for the\nincorporation of safety constraints dynamically during the\nlearning process. By introducing a Lagrange multiplier, the\nconstrained optimization problem can be rewritten to balance\nthe trade-off between reward maximization and constraint\nsatisfaction.\n\u2022 Incorporating Risk-Averse Policies:\nAnother type of modifying optimization criterion is risk-\nsensitive criterion where risk-sensitive RL policies explicitly\nconsider the uncertainty or risk associated with different\nactions. It aims to balance the trade-off between maximizing\nexpected rewards and minimizing the potential negative\nconsequences of risky actions. These policies employ\nvarious risk (or uncertainty) measures, these estimates are\ntypically computed for the system dynamics or the overall\ncost function and leveraged to produce more conservative\n(and safer) policies, Risk measure can be defined as\nconditional value-at-risk (CVaR) or entropy regularization,\nor any other risk measures [12-14] to guide the agent's\ndecision-making process towards less cost actions.\nResearchers have also explored the use of learned risk\nmeasures or uncertainty risk measures that capture risk\npreferences directly from data or model uncertainty. These\nmeasures allow RL agents to adapt to varying risk\npreferences or handle situations where traditional risk\nmeasures may not be adequate. Risk can be defined as the\nprobability of collision, uncertainty estimate, or probability\nof unsafe state-action pairs [15-19]."}, {"title": "PROPOSED METHODOLOGY", "content": "Our proposed approach Safe Reinforcement Learning with\nAnomalous State Sequences (AnoSeqs) utilizes sequential\nanomaly detection modeling in reinforcement learning as the\nmeasure of unsafety. As illustrated in Figure 1 the proposed\napproach (AnoSeqs) operates in two distinct phases: learn\nunsafe state sequence detection and train risk-averse policy.\n\u2022 Phase 1: Learn Unsafe State Sequence Detection\nIn this phase, we first collect safe state sequences by training\nan RL agent to explore and navigate in an offline simulated\nnon-safety-critical \"source\" environment and then learn the\nsafe state sequences with an anomaly detection model."}, {"title": "1- Collect Safe Sequences:", "content": "We train a reinforcement learning agent, specifically an\nactor-critic reinforcement learning agent, in an offline\nsimulated non-safety-critical \"source\" environment. The\nobjective is to collect safe state sequences, which do not lead\nto unsafe states, agent damage, or episode termination.\n\"Unsafe states\" are defined as states where the agent violates\npredefined safety constraints. \"Agent damage\" refers to any\nharm to the agent, such as collisions or malfunctions.\n\"Episode termination\" occurs when the agent reaches a\nterminal state without successfully completing its assigned\ntask.\nThe source environment is designed to mimic real-world\nscenarios but with reduced complexity and risk; i.e., a\nsimulated environment where RL agent can explore the\nworld and enter unsafe states with no real cost. Once the\nagent has navigated the source environment enough, we\nextract the safe state sequences it encountered during its\nexploration."}, {"title": "2- Learn Safe Sequences:", "content": "We use the collected safe state sequences to train a sequential\nanomaly detection model. This model aims to identify\nanomalous state sequences in the target environment. Each\nsequence of states is represented as a multivariate time-series\nwhich is a timestamped sequence of observations/states of\nsize T:\nX = [S1, S2, ..., ST],\nWhere each state st is collected at a specific timestamp t,\nSt\u2208 RM where M is the number of state variables in the\nenvironment state space and T is the lookback timestamps,\nmeaning that each sequence contains the past T timesteps of\nstate variables, this allows the model to capture temporal\ndependencies in the data and improve its ability to learn the\nsafe sequences and detect anomalous states in the target\nenvironment.\nWe employ a transformer auto-encoder architecture,\ninspired by the work presented in [26] to reconstruct the safe\nsequences. The reconstruction loss is calculated as the mean\nabsolute error (MAE):\nMAE =  1/T sum_{t=1}^{T} |s_t-hat{s}_t|\nWhere st is the reconstructed state at time t.\nThe anomaly score nt for a sequence at time t is given by the\nreconstruction error:\nnt = softmax(- sum_{t=1}^{T} |s_t-hat{s}_t|/log (1 + MAE))\nBy using the transformer auto-encoder to learn a compact\nand informative representation of the safe sequences, we can\neffectively detect anomalies in the target environment and\nuse this anomaly score as a risk measure to train a risk-averse\npolicy in the target environment."}, {"title": "\u2022 Phase 2: Train Risk-Averse Policy", "content": "In this phase, we train a risk-averse policy in the target\nenvironment, which is a safety-critical environment (i.e., a\nreal-world scenario). We use the anomaly scores from the\nsequence anomaly detection model as an estimated risk\nduring online training in the target environment. We achieve\nthis by adjusting the reward function to penalize the agent for\nvisiting states that are detected as unsafe based on the\nanomaly detection output. Specifically, we add a risk penalty\nterm to the standard reward function used in reinforcement\nlearning. The risk penalty term increases with the anomaly\nscore, discouraging the agent from entering potentially\nhazardous states.\nThe goal of the risk-averse RL actor-critic policy is to\nlearn a policy that maximizes the expected cumulative\nreward while minimizing the risk of encountering anomalies.\nTo achieve this, we modify the reward function to include\nthe anomaly score, which encourages the agent to avoid\nstates with high anomaly scores and discourages it from\nentering potentially hazardous states. Specifically, the\nreward function is defined as:\nrt = r_t^{orig}  if \u03b7 \u2264 0\nr_t^{orig} - \u03b2\u03b7_t  otherwise\nWhere r orig is the original reward at time step t, nt is the\nanomaly score at time step t, \u03b8 is a threshold value for the\nanomaly score, and \u1e9e is a hyperparameter that controls the\nstrength of the penalty term.\nBy combining sequential anomaly detection with\nreinforcement learning, our proposed approach enables the\ndevelopment of intelligent agents capable of operating safely\nin dynamic, uncertain environments. The next section\npresents experimental results\ndemonstrating the\neffectiveness of our method."}, {"title": "EXPERIMENTS", "content": "To evaluate the performance of our proposed approach, we\nuse three different benchmarking environments:\n\u2022 Safety Ant Run: The environment, Safety Ant Run-v0,\ninvolves a quadrupedal agent with four legs. The task for the\nagent is to run through an avenue between two safety\nboundaries. Observations for the agent include sensory\ninputs regarding its position, velocity, and surrounding\nenvironment. Rewards are given for maintaining a running\nspeed within the safety boundaries, while a cost signal is\nreceived when exceeding an agent-specific velocity\nthreshold. [27]."}, {"title": "\u2022 Safety Gym Point Goal:", "content": "The environment, Safety Gym\nPoint Goal-v1, is a mass point agent. The task for the agent\nis to navigate to the green goal while avoiding blue hazards.\nOne vase is present in the scene, but the agent is not\npenalized for hitting it. Observations for the agent include\ndata on its position, goal location, and positions of hazards.\nRewards are provided for reaching the goal, and penalties are\ngiven for colliding with hazards [28]."}, {"title": "\u2022 Safety Meta Drive:", "content": "The environment, Safe Meta Drive,\nsimulates an autonomous vehicle scenario. The task for the\nagent is to reach navigation landmarks as quickly as possible\nwithout colliding with other vehicles or going off-road.\nObservations for the agent include inputs regarding its speed,\nposition, nearby vehicles, and road boundaries. Rewards are\ngiven for fast and safe navigation, while penalties are applied\nfor collisions or going off-road [29].\nThese environments represent various safety-critical\nscenarios, such as driving within speed limits, avoiding\npedestrians, and navigating through construction zones. We\nemploy three metrics to measure the performance of our\nalgorithm [30]:"}, {"title": "B. Metrics:", "content": "Episodic Return = sum of rewards in the test time. It\nindicates how well the agent finishes the original task."}, {"title": "2. Episodic Cost Rate =", "content": "number of cost signals / length\nof the episode. It indicates how safe the agent is at the\ntest time. A \"cost signal\" represents an event or state\ndeemed undesirable or unsafe, such as a collision or\nsafety violation."}, {"title": "3. Total Cost Rate =", "content": "total number of cost signals / total\nnumber of training steps. It indicates how safe the\nagent is in the whole training process."}, {"title": "HYPERPARAMETERS SETTINGS", "content": "In this study, we employed a sequential anomaly detection\nmodel based on a Transformer-based autoencoder to identify\nanomalous states in three different environments. The model\nwas trained on safe state sequences collected using\nreinforcement learning agents, and we extracted safe\nsequences by checking for collisions, damage, or episode\ntermination.\nFor the AnoSeqs algorithm, implemented using the twin-\ndelayed deep deterministic policy gradient TD3 [31] policy\nwithin the SafeRL-Kit framework [30], we focused on two\nkey hyperparameters: the anomaly threshold and the\nanomaly penalty. The anomaly threshold was set based on\nthe 95th percentile of the mean absolute error for\nautoencoded safe sequences from the source environment."}, {"title": "RESULTS", "content": "In this section, we will present the results of our proposed\nAnoSeqs method. We will compare its learning curves with\nbaseline and state-of-the-art algorithms. Additionally, we\nwill provide a summary of the algorithm performance after\ntraining and conduct a sensitivity analysis to examine the\nimpact of different hyperparameter settings."}, {"title": "A. Comparison with Baseline Algorithm TD3:", "content": "In this subsection, we present the results of our proposed\nmethod AnoSeqs and compare its performance with the\nbaseline TD3 algorithm [31]. We evaluated our method on\nthe three safety-critical benchmarking environments: Safety\nAnt Run, Safety Gym Point Goal, and Safety Meta Drive.\nAs shown in Figure 2, our proposed method, AnoSeqs,\noutperforms the baseline algorithm, TD3, in terms of\nreducing the cost rate during training, as well as the average\nepisodic cost. The cost rate during training is consistently\nlower for AnoSeqs compared to TD3. In Safety Ant Run, the\nreduction in reward is expected as the algorithm becomes\nmore cautious to avoid costly actions. In Safety Gym Point\nGoal, AnoSeqs achieves a lower cost rate during training and\na slightly lower average episodic cost compared to TD3 but\nmaintains a similar level of episodic reward, indicating that\nour algorithm is able to learn a safe policy without\ncompromising the primary task. Moreover, in Safety Meta\nDrive, AnoSeqs not only reduces the cost rate during training\nbut also increases the reward, due to the environment's\nreward design that includes negative signals for costly\nactions."}, {"title": "B. Comparison with State-of-the-Art Algorithms:", "content": "As shown in Figure 3, the comparison results with other\nsafety algorithms indicate that the AnoSeqs algorithm\nachieves a good balance between episodic return and cost.\nThe state-of-the-art algorithms included in our comparison\nare the Lagrangian RL algorithm [1], Recovery RL algorithm\n[2], and Reward Shaping algorithm [3, 4], each ensuring\nsafety at different stages of the reinforcement learning\nprocess. The Lagrangian RL algorithm incorporates safety\nduring the optimization phase by enforcing constraints. The\nRecovery RL algorithm enhances safety at the output of the\npolicy by introducing a recovery layer that mitigates unsafe\nactions. The Reward Shaping algorithms integrate safety into\npolicy by penalizing costs.\nIn the Safety Ant Run environment, the AnoSeqs\nalgorithm demonstrates a high episodic return (left column)\ncompared to other algorithms, while maintaining a low\nepisodic cost rate and total cost rate. Although the\nLagrangian algorithm achieved the lowest cost rate, its return\nwas low, indicating that this algorithm provides a high level\nof safety at the expense of performance due to its constraint\nsatisfaction method. In the Safety Gym Point environment,\nthe AnoSeqs algorithm achieves the highest episodic return\nwith the lowest episodic and total cost rates. In the Safety\nMeta Drive environment, the algorithm also achieves high\nepisodic returns with low episodic and total costs. The\nRecovery and Reward Shaping algorithms managed to\nreduce costs but at the expense of returns. These results\nhighlight the ability of the AnoSeqs algorithm to sustainably\nimprove performance across multiple environments."}, {"title": "C. Summary Statistics of Algorithm Performance:", "content": "To provide a comprehensive evaluation of the AnoSeqs\nalgorithm, we conducted 100 convergence tests and\ncompared the results with the baseline TD3 algorithm,\ngenerating summary statistics post-training. This evaluation\naims to demonstrate the robustness of the AnoSeqs\nalgorithm, proving that it consistently maintains its\nperformance after policy convergence."}, {"title": "D. Sensitivity Analysis with Different Hyperparameter Settings:", "content": "We study the impacts of two critical hyperparameters in our\nmodel: the anomaly penalty (\u03b2) and the anomaly threshold\n(0) on the Safety Ant Run environment. The anomaly\npenalty \u1e9e controls the strength of the penalty term, while \u03b8\nis a threshold value for the anomaly score. Figure 4 presents\nthe impact of different penalty values on the Episode Reward\nand Episode Cost. The results show that even with higher\npenalties, which ensure more safety by minimizing risky\nactions, the learning curves maintain similar shapes, with\nlower costs and similar rewards, demonstrating robustness\nacross different values. Through multiple experiments, we\nfound that a penalty of 100 achieves the best balance between\nperformance and safety."}, {"title": "For 0, the plots in Figure 5 demonstrate the sensitivity of", "content": "For 0, the plots in Figure 5 demonstrate the sensitivity of\nthe model to various threshold values. We tested different\nanomaly thresholds (0.00089, 0.0005, and 0.001) using\ndifferent methods of calculating the anomaly threshold, such\nas the 95th percentile of the mean absolute error, 3 standard\ndeviations from the mean, and 2 standard deviations from the\nmean. Lower thresholds increase the model's sensitivity to\nanomalies. As the threshold rises, the learning curves begin\nto resemble those of the baseline TD3 algorithm without\nanomaly modifications, indicating reduced sensitivity to\nanomalies. This suggests that with higher thresholds, the\nmodel may fail to detect anomalies, reverting to baseline\nperformance. This analysis highlights the necessity of fine-\ntuning 0 and \u1e9e to balance the model's sensitivity to anomalies\nand the associated penalty, optimizing overall performance\nwhile maintaining safety.\nOverall, the results demonstrate that our proposed method\nAnoSeqs is effective in enhancing the safety of\nreinforcement learning agents in various safety-critical\nenvironments and our algorithm is able to balance\nexploration and exploitation effectively while maintaining\nsafety.\nOur main contribution is the development of a novel\nmethod for learning to detect anomalous state sequences as\nthe measure of unsafety, which enables any RL algorithm to\nbecome more cautious and safer. By incorporating\nanomalous state sequences into the training process, our\nalgorithm can learn safer policies and reduce the likelihood\nof encountering undesirable events even without recovery,\nconstraints, or safety projection techniques. Therefore, our\nmethod can be applied to safe RL algorithms in the literature\nto further enhance their safety."}, {"title": "CONCLUSION", "content": "We proposed a novel approach to safe reinforcement\nlearning called Safe Reinforcement Learning with\nAnomalous State Sequences (AnoSeqs). Our approach\nleverages sequential anomaly detection to enhance the safety\nof RL agents in dynamic and uncertain environments. By\nintegrating anomalous state sequences into the training\nprocess by learning safe state sequences from a source\nenvironment, our algorithm is able to learn safer policies and\nreduce the likelihood of encountering unsafe actions. The\nexperimental results demonstrated the effectiveness of our\nproposed method in various safety-critical environments.\nOur work highlights the potential of sequential anomaly\ndetection in safe reinforcement learning. By adopting this\ntechnique, RL agents can better handle unexpected events\nand learn safer policies in complex and dynamic\nenvironments. Future research directions may explore\nfurther integration of anomaly detection methods into\ndifferent stages of the RL learning process, towards more\nrobust and reliable Al systems."}]}