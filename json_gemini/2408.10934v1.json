{"title": "SDI-Net: Toward Sufficient Dual-View Interaction for Low-light Stereo Image Enhancement", "authors": ["LinLin Hu", "Ao Sun", "Shijie Hao", "Richang Hong", "Meng Wang"], "abstract": "Currently, most low-light image enhancement methods only consider information from a single view, neglecting the correlation between cross-view information. Therefore, the enhancement results produced by these methods are often unsatisfactory. In this context, there have been efforts to develop methods specifically for low-light stereo image enhancement. These methods take into account the cross-view disparities and enable interaction between the left and right views, leading to improved performance. However, these methods still do not fully exploit the interaction between left and right view information. To address this issue, we propose a model called Toward Sufficient Dual-View Interaction for Low-light Stereo Image Enhancement (SDI-Net). The backbone structure of SDI-Net is two encoder-decoder pairs, which are used to learn the mapping function from low-light images to normal-light images. Among the encoders and the decoders, we design a module named Cross-View Sufficient Interaction Module (CSIM), aiming to fully exploit the correlations between the binocular views via the attention mechanism. The quantitative and visual results on public datasets validate the superiority of our method over other related methods. Ablation studies also demonstrate the effectiveness of the key elements in our model.", "sections": [{"title": "I. INTRODUCTION", "content": "Low-light image enhancement aims to obtain images with good visibility from low-light images, which has been a main low-level vision task in both academic and industrial communities. The research on low-light image enhancement has made great progress in the past decades. Conventional non-learning-based methods [3], [4], [5], [6], [7], [8], [9], [10], [11] for low-light enhancement primarily concentrate on brightness adjustment and contrast enhancement based on prior knowledge such as the Retinex theory. However, these approaches frequently yield suboptimal enhancement results, often resulting in less satisfying visual quality. In comparison to the conventional methods, low-light image enhancement methods based on deep neural network [12], [13], [14], [15], [16], [17] have achieved significant advancements in recent years. These methods employ convolutional neural networks (CNNs) or generative adversarial networks (GANs) as the backbone framework to learn the mapping function from low-light images to normal light images.\n\nThe above-mentioned models are oriented for monocular images. In another word, the input of these models is one single image. In recent years, the emergence of stereo cameras has sparked interest in stereo vision across diverse fields, which provides richer information than monocular image processing systems. For example, as for binocular images, there exist sufficient horizontal discrepancies between the left view and the right view of a same object. Based on this characteristic, many works have been dedicated to developing stereo image restoration techniques for enhancing image visual quality [18], [19], [20], such as stereo super-resolution, stereo deblurring, and stereo dehazing. The primary challenge faced by stereo image restoration methods is how to fully leverage the correlations between the two views, specifically, how to effectively associate the left and the right views for obtaining more effective feature representation. For example, for the stereo image super-resolution task, iPASSENet[20] aims to capture the correspondence between the left and the right views through modeling the parallax attention mechanism.\n\nAs a typical image restoration task, low-light image en-hancement is also striving for advancements by incorporating stereo vision[21], [22], [23]. In comparison to the monocu-lar (single-image) low-light enhancement methods, low-light stereo image enhancement (LLSIE) methods obtain better performance in general. However, due to the problem that low-light images suffer from low contrast and imaging noise, the current LLSIE research is still limited in well restoring image details. For example, as shown in Fig.1, the result of LLSIE method DCI-Net[2] also has restoring errors as large as the single-image low-light enhancement method MIRNet[1]. One of the main reasons is that the interaction between the left and right views of the current LLSIE methods is not sufficient.\n\nTo address this problem, we propose SDI-Net aiming at fully exploring dual-view interaction for low-light stereo image enhancement. The proposed model is able to restore the illumination and details of low-light images through sufficient interaction between the left and right views. We develop an intermediate interaction module named Cross-View Sufficient Interaction Module (CSIM) to explore and strengthen the interaction between features of both views. The first part of CSIM is the Cross-View Attention Interaction Module (CAIM), which is adept at calculating the disparity between the left and right views and aligning them with high precision. The second part comprises a Pixel and Channel Attention Block (PCAB), designed to differentially enhance areas of varying brightness levels, thereby restoring richer details and textures. The proposed SDI-Net is highlighted in the following aspects:\n\n\u2022 SDI-Net adopts two identical UNets as its backbone structure. Each UNet acts as the image encoder and de-coder for either of the left-view image and the right-view image. This symmetry model structure ensures the two"}, {"title": "II. RELATED WORKS", "content": "The traditional single-image low-light enhancement meth-ods mainly include the histogram equalization and retinex-based methods. Histogram equalization methods directly ad-just the dynamic range of low-light images, thereby enhancing image contrast[6]. Retinex-based methods aim to decompose a low-light image into reflection and illumination layers, and adjust the brightness of the illumination layer to achieve the low-light enhancement[24], [9], [25]. These methods are highlighted for their clear physical interpretability. However, they are limited in its capability of fitting the complex mapping function between low-light and normal-light images. By leveraging deep learning models, learning-based low-light enhancement methods well solve this limitation and greatly improve the performance. These methods are able learn end-to-end appearance mapping functions between low-light inputs and normal-light outputs[17], [26], [27], [1]. Recently, models with limited or no supervision also emerge as popular low-light enhancement methods [28], [13], [29], [15] due to their lightweight and fast characteristics. However, they are prone to introduce over-enhancement and obvious artifacts into enhanc-ing results. Despite of the success of single-image low-light enhancement methods, their information source is one single input all along. In contrast, methods taking stereo images as inputs pave way for utilizing richer information in this task, which have the potential of achieving better enhancing performance."}, {"title": "B. Stereo image restoration methods", "content": "Recently, stereo image restoration methods have begun to attract more attention, and obtained better performance than single-image restoration methods, such as the tasks of super-resolution, deblurring, deraining, dehazing, and low-light en-hancement. In the stereo image super-resolution method, Jeoh et al.[30] propose the first stereo super-resolution network to compensate for the parallax between stereo images by shifting. Wang et al.[19] use a parallax attention mechanism to merge similar features in two views to explore pixel correspondence. In the stereo deblurring method, Zhou et al.[31] align features by estimating the difference between the left and right views. Li et al.[32] propose a new stereo image deblurring model by exploring two-pixel alignment. In the stereo dehazing method, Pang et al.[33] use a stereo transformation module to explore the correlation between binocular images. As for our low-light task, DVENet[21] is regarded as the representative method, which is to enhance the network by integrating multi-scale dual-view features, and fuse the features of a single image under the guidance of the light map. Recently, DCI-Net[2] tries to further exploits the interaction between the left and right views by considering the spatial connection between multiple scales.\n\nIn this paper, we focus on the low-light enhancement task. We propose SDI-Net to fully incorporate the information from the left and right views via the attention mechanism imposed on different aspects, such as the view level, the channel level and the pixel level. We introduce the technical details of SDI-Net in the following section."}, {"title": "III. METHOD", "content": "The SDI-Net model takes low-light stereo image pairs as inputs, and utilizes two identical UNet branches as the"}, {"title": "A. Overall framework", "content": "The overall framework of SDI-Net is shown in Fig.2, which can be divided into three stages: Feature Encoder, Cross-View Sufficient Interaction, and Feature Decoder.\n\nFeature Encoder. First, we take the left-view low-light image $I_l \\in R^{H\\times W\\times 3}$ and the right-view low-light image $I_r \\in R^{H\\times W\\times 3}$ as the inputs of the encoders. Based on the en-coders, the feature maps $F_l \\in R^{\\frac{H}{4}\\times \\frac{W}{4} \\times 3}$ and $F_r \\in R^{\\frac{H}{4}\\times \\frac{W}{4} \\times 3}$ are learned. This process can be formulated as:\n\n$F_l = FE(I_l), F_r = FE(I_r)$ (1)"}, {"title": "B. Cross-View Sufficient Interaction Module (CSIM)", "content": "The CSIM module is composed of two parts, i.e. Cross-View Attention Interaction Module (the left part of Fig.3) and Pixel and Channel Attention Block (the right part of Fig.3), which are described in the following."}, {"title": "Cross-View Attention Interaction Module (CAIM).", "content": "CAIM is used to extract the correlation information at the view level. The correlation computing process is based on Scaled Dot Product Attention[34], which utilizes all the keys to compute the dot products of the query, and applies the softmax function to obtain the weights of the values:\n\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{C}})V$ (4)\n\nwhere Q represents the query matrix, K represents the key matrix, V represents the value matrix, and C is the channel numbers. In our application, we use $Q_l = Conv(LN(F_l))$ and $K_r = Conv(LN(F_r))$ to represent the features of the two views, where LN(\u00b7) represents layer normalization, and Conv(\u00b7) represents a 1\u00d71 convolution operation. The refined features can be obtained as:\n\n$F_{r\\rightarrow l} = Attention(Q_l, K_r, Conv(F_l)) + F_l$ (5)\n\n$F_{l\\rightarrow r} = Attention(K_r, Q_l, Conv(F_r)) + F_r$ (6)\n\nFrom above process, the computing process for obtaining $F_{r\\rightarrow l}$ and $F_{l\\rightarrow r}$ fully considers the interaction from each other view."}, {"title": "Pixel and Channel Attention Block (PCAB).", "content": "PCAB is composed on several stacked feature enhancing blocks for further refining $F_{r\\rightarrow l}$ and $F_{l\\rightarrow r}$. In the first feature enhancing block (FEB), we initially pre-process $F_{r\\rightarrow l}$ and $F_{l\\rightarrow r}$ based on the following procedure:\n\n$R_l = Conv(G(Conv(F_{r\\rightarrow l})) + F_{r\\rightarrow l})$ (7)\n\n$R_r = Conv(G(Conv(F_{l\\rightarrow r})) + F_{l\\rightarrow r})$ (8)\n\nwhere Conv(\u00b7) refers to a 3\u00d73 convolution operation and G(\u00b7) stands for GeLU activation function. Then, the channel atten-tion function CA(\u00b7) and the pixel attention function PA(\u00b7) [35] are introduced to further exploit the feature correlation at the channel level and the pixel level:\n\n$R_l^* = PA(CA(R_l)), R_r^* = PA(CA(R_r))$ (9)\n\nThe channel attention (CA(\u00b7)) learns the importance of each feature channel, aiming to highlight more useful information. The pixel attention (PA(\u00b7)) dynamically adjust the importance of each pixel, which aiming to enhance important details and texture features while reducing the impact from noise.\n\nIn the following, the above process is repeated by stacking multiple FEBs in a sequential way. Of note, instead of $F_{r\\rightarrow l}$ and $F_{l\\rightarrow r}$, the inputs of the blocks other than the first FEB are the previous neighboring FEB's outputs. In our research, we empirically set the number of FEBs as 10 to build the PCAB module.\n\nAt the end of PCAB, we combine the gradually refined $R_l^*$ and $R_r^*$ with its original feature representation $F_l$ and $F_r$ using a weighted element-wise addition to obtain $SF_l$ and $SF_r$, which is the final feature representation learned by the CSIM module:\n\n$SF_l = \\gamma_l R_l^* + F_l$ (10)\n\n$SF_r = \\gamma_r R_r^* + F_r$ (11)\n\nwhere $\\gamma_l$ and $\\gamma_r$ are trainable weights and are initialized with zeros to ensure stable training."}, {"title": "C. Loss function", "content": "In this paper, we use the L1 loss and the FFT loss to train the whole network, which can be expressed as follows:\n\n$L = L_1 + \\lambda L_{fre}$ (12)\n\nwhere \u03bb stands for a hyper-parameter, empirically set to 0.1. It is important to note that $L_{fre}$ helps to restore the normal light image via preserving the frequency-domain image characteristics. The two loss terms can be expressed as:\n\n$L_1 = ||\\hat{I_l} - I_l^G||_1 + ||\\hat{I_r} - I_r^G||_1$ (13)\n\n$L_{fre} = ||\\varphi(\\hat{I_l}) - \\varphi(I_l^G)||_1 + ||\\varphi(\\hat{I_r}) - \\varphi(I_r^G)||_1$ (14)\n\nwhere $|| \\cdot ||_1$ is the L1 loss, which is used to maintain the overall structure and normal-light and low-light details of the image. $\\varphi(\\cdot)$ stands for Fast Fourier Transform, which helps to recover the texture details and edge information of the image, making the reconstructed image closer to the image under normal-light conditions. $\\hat{I_l}$ and $\\hat{I_r}$ represent the restored left and right normal-light images, and $I_l^G$ and $I_r^G$ represent the ground truth corresponding to $\\hat{I_l}$ and $\\hat{I_r}$."}, {"title": "IV. EXPERIMENTS", "content": "Datasets. We adopt the existing public datasets specifically designed for low-light stereo image enhancement, i.e., Middle-bury and Synthetic Holopix50k from DVENet[21], to evaluate the effectiveness of our method. The Middlebury dataset selects 136 pairs of images as the training set with resolution of 512x512 and 36 pairs of images for testing. The Middle-bury dataset is carefully built to offer high-quality, precisely calibrated stereo images, which are crucial for the low-light stereo image enhancement task. The Synthetic Holopix50k dataset is a much larger dataset. Its training set includes 1128 images with resolution of 512x512, and its test set includes 159 images. The Holopix50k dataset originates from a larger collection of real-world images captured by dual-camera smartphones, offering a broader but less controlled variety of low-light scenarios. This diversity is vital for training models to generalize across a wide range of real-world conditions.\n\nEvaluation Metrics. We use two mostly-adopted full ref-erence quality metrics PSNR and SSIM to evaluate the model performance. Higher PSNR or SSIM values indicate better performance.\n\nMethods for Comparison. We compare our proposed SDI-Net with several single-image (monocular) low-light en-hancement methods and stereo-image low-light enhancement methods. The single-image low-light enhancement methods for comparison include the non-learning based ones (NPE[36], LIME[3], RRM[9]), ZeroDCE[28], and the learning-based ones (RetinexNet[17], MBLLEN[37], DSLR[26], KIND[16], DRBN[13], SNR-Aware[38], LLFormer [39] and MIRNet[1]). The stereo-image low-light enhancement methods include DVENet[21] and the newly proposed DCI-Net[2]. Con-sidering the limited number of the existing stereo-image low-light enhancement methods, we additionally introduced iPASSRNet[20] for comparison. Originally designed for stereo image super-resolution, iPASSRNet can be easily adapted as a stereo-image low-light enhancement model. With the two datasets, we re-trained iPASSRNet and evaluated its perfor-mance in our application.\n\nImplementation Details. The hardware for the experiments is an Nvidia RTX 2080 Ti card with 12G memory. The batch"}, {"title": "B. Quantitative Comparison", "content": "The quantitative results of SDI-Net and the methods for comparison on Middlebury and Synthetic Holopix50k are reported in Table I. The non-learning traditional methods, such as NPE, LIME, JieP, and RRM, exhibit relatively poor performance. The reason is that it is difficult for the model-driven methods to fit the complex mapping functions between low-light and normal-light images. As for the learning-based monocular low-light enhancement methods, the PSNR and SSIM values are generally much higher, especially for the recently proposed methods such as DRBN and MIRNet. It is noted that the quantitative performance of ZeroDCE is relatively low. The reason is that it focuses on learning an optimal mapping curve function by only using dark images. The absence of full supervision information makes its per-formance not so well in terms of PSNR and SSIM. As for the stereo-image low-light enhancement, we can observe an obvious advantage over the monocular family, showing the effectiveness of using stereo images in general. Among these methods, our SDI-Net performs the best on both datasets in terms of PSNR and SSIM, showing the effectiveness and superiority of our method."}, {"title": "C. Visual Comparison", "content": "Visual results on Middlebury. Fig.4 and Fig.5 provide the visual comparisons for all the methods. In Fig.4, we can see that some of the methods fail to improve the visibility of the input image, while other methods introduce color distortions or artifacts into their results. By comparing the zoomed-in patch of the enhanced results and the ground-truth image, we can see that our method performs better than others in terms of the visual quality, which well improves the visibility and recovers the detail textures simultaneously. We present another"}, {"title": "Model structure.", "content": "In Table II, VO represents the scenario where we perform a simple interaction on the extracted features in the middle of the encoders and decoders. In another word, the CSIM is replaced with a heuristic two-round interaction process. In the first round, $F_l$ and $F_r$ are directly concatenated and then down-sampled. In the second round, the down-sampled feature is respectively concatenated with $F_l$ and $F_r$ and down-sampled again. The refined feature rep-resentations are then sent into the decoding stage. V1 means that we add PCAB after the VO version. V2 indicates that we only utilize CAIM for interaction but do not use the PCAB module. By comparing the performance of our complete model with V0 to V2, we observe that both CAIM and PCAB play indispensable roles in our low-light enhancement task. For example, the usefulness of CAIM are both empirically validated in the comparison between Ours and v1, and the comparison between V2 and V0. Similarly, the comparison between Ours and V2, and the comparison between V1 and VO also empirically demonstrate the effectiveness of PCA\u0412.\n\nLoss function. In TableII, V3 means that the loss term $L_{fre}$ is not used. Compared to the completed model, we can see that there exists a clear performance decline for V3, which demonstrates the importance of using the loss term $L_{fre}$. The rationale is clear that the frequency-based image representation provides a complementary role to the spatial pixel arrays. Therefore, the combination of the two different loss terms effectively promotes the visual quality of enhanced results."}, {"title": "V. CONCLUSION", "content": "For the low-light image enhancement task, the research community have begun to explore the usage of more advanced imaging systems. For example, it is beneficial for enhance-ment models to simultaneously use the binocular images as the inputs. However, the limitation of insufficient interaction between these two views still exists in current research on low-light stereo image enhancement. In this paper, we propose SDI-Net for sufficiently modeling the dual-view interaction for low-light stereo image enhancement. By modeling the attention mechanism at the view level, the channel level and the pixel level, SDI-Net facilitates comprehensive information exchange of the dual views, therefore better restoring the high-quality normal-light images from low-light images. Experimental results on two public datasets demonstrate the effectiveness and superiority of SDI-Net in terms of quantitative and visual comparison. In the future, we plan to extend the interaction between the two views into the frequency domain and semantic domain."}]}