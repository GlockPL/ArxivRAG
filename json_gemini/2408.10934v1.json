{"title": "SDI-Net: Toward Sufficient Dual-View Interaction\nfor Low-light Stereo Image Enhancement", "authors": ["LinLin Hut", "Ao Sun", "Shijie Hao", "Richang Hong", "Meng Wang"], "abstract": "Currently, most low-light image enhancement methods only consider information from a single view, neglecting\nthe correlation between cross-view information. Therefore, the\nenhancement results produced by these methods are often un-\nsatisfactory. In this context, there have been efforts to develop\nmethods specifically for low-light stereo image enhancement.\nThese methods take into account the cross-view disparities and\nenable interaction between the left and right views, leading to\nimproved performance. However, these methods still do not fully\nexploit the interaction between left and right view information. To\naddress this issue, we propose a model called Toward Sufficient\nDual-View Interaction for Low-light Stereo Image Enhancement\n(SDI-Net). The backbone structure of SDI-Net is two encoder-\ndecoder pairs, which are used to learn the mapping function from\nlow-light images to normal-light images. Among the encoders and\nthe decoders, we design a module named Cross-View Sufficient\nInteraction Module (CSIM), aiming to fully exploit the correla-\ntions between the binocular views via the attention mechanism.\nThe quantitative and visual results on public datasets validate the\nsuperiority of our method over other related methods. Ablation\nstudies also demonstrate the effectiveness of the key elements in\nour model.", "sections": [{"title": "I. INTRODUCTION", "content": "Low-light image enhancement aims to obtain images with\ngood visibility from low-light images, which has been a\nmain low-level vision task in both academic and industrial\ncommunities. The research on low-light image enhancement\nhas made great progress in the past decades. Conventional\nnon-learning-based methods [3], [4], [5], [6], [7], [8], [9],\n[10], [11] for low-light enhancement primarily concentrate\non brightness adjustment and contrast enhancement based on\nprior knowledge such as the Retinex theory. However, these\napproaches frequently yield suboptimal enhancement results,\noften resulting in less satisfying visual quality. In comparison\nto the conventional methods, low-light image enhancement\nmethods based on deep neural network [12], [13], [14], [15],\n[16], [17] have achieved significant advancements in recent\nyears. These methods employ convolutional neural networks\n(CNNs) or generative adversarial networks (GANs) as the\nbackbone framework to learn the mapping function from low-\nlight images to normal light images.\nThe above-mentioned models are oriented for monocular\nimages. In another word, the input of these models is one\nsingle image. In recent years, the emergence of stereo cameras\nhas sparked interest in stereo vision across diverse fields,\nwhich provides richer information than monocular image\nprocessing systems. For example, as for binocular images,\nthere exist sufficient horizontal discrepancies between the left\nview and the right view of a same object. Based on this\ncharacteristic, many works have been dedicated to developing\nstereo image restoration techniques for enhancing image visual\nquality [18], [19], [20], such as stereo super-resolution, stereo\ndeblurring, and stereo dehazing. The primary challenge faced\nby stereo image restoration methods is how to fully leverage\nthe correlations between the two views, specifically, how to\neffectively associate the left and the right views for obtaining\nmore effective feature representation. For example, for the\nstereo image super-resolution task, iPASSENet[20] aims to\ncapture the correspondence between the left and the right\nviews through modeling the parallax attention mechanism.\nAs a typical image restoration task, low-light image en-\nhancement is also striving for advancements by incorporating\nstereo vision[21], [22], [23]. In comparison to the monocu-\nlar (single-image) low-light enhancement methods, low-light\nstereo image enhancement (LLSIE) methods obtain better\nperformance in general. However, due to the problem that low-\nlight images suffer from low contrast and imaging noise, the\ncurrent LLSIE research is still limited in well restoring image\ndetails. For example, as shown in Fig.1, the result of LLSIE\nmethod DCI-Net[2] also has restoring errors as large as the\nsingle-image low-light enhancement method MIRNet[1]. One\nof the main reasons is that the interaction between the left and\nright views of the current LLSIE methods is not sufficient.\nTo address this problem, we propose SDI-Net aiming at\nfully exploring dual-view interaction for low-light stereo im-\nage enhancement. The proposed model is able to restore the\nillumination and details of low-light images through sufficient\ninteraction between the left and right views. We develop an\nintermediate interaction module named Cross-View Sufficient\nInteraction Module (CSIM) to explore and strengthen the\ninteraction between features of both views. The first part\nof CSIM is the Cross-View Attention Interaction Module\n(CAIM), which is adept at calculating the disparity between\nthe left and right views and aligning them with high precision.\nThe second part comprises a Pixel and Channel Attention\nBlock (PCAB), designed to differentially enhance areas of\nvarying brightness levels, thereby restoring richer details and\ntextures. The proposed SDI-Net is highlighted in the following\naspects:\n\u2022\nSDI-Net adopts two identical UNets as its backbone\nstructure. Each UNet acts as the image encoder and de-\ncoder for either of the left-view image and the right-view\nimage. This symmetry model structure ensures the two"}, {"title": "II. RELATED WORKS", "content": "The traditional single-image low-light enhancement meth-\nods mainly include the histogram equalization and retinex-\nbased methods. Histogram equalization methods directly ad-\njust the dynamic range of low-light images, thereby enhancing\nimage contrast[6]. Retinex-based methods aim to decompose\na low-light image into reflection and illumination layers, and\nadjust the brightness of the illumination layer to achieve\nthe low-light enhancement[24], [9], [25]. These methods are\nhighlighted for their clear physical interpretability. However,\nthey are limited in its capability of fitting the complex\nmapping function between low-light and normal-light images.\nBy leveraging deep learning models, learning-based low-light\nenhancement methods well solve this limitation and greatly\nimprove the performance. These methods are able learn end-\nto-end appearance mapping functions between low-light inputs\nand normal-light outputs[17], [26], [27], [1]. Recently, models\nwith limited or no supervision also emerge as popular low-\nlight enhancement methods [28], [13], [29], [15] due to their\nlightweight and fast characteristics. However, they are prone to\nintroduce over-enhancement and obvious artifacts into enhanc-\ning results. Despite of the success of single-image low-light\nenhancement methods, their information source is one single\ninput all along. In contrast, methods taking stereo images\nas inputs pave way for utilizing richer information in this\ntask, which have the potential of achieving better enhancing\nperformance."}, {"title": "B. Stereo image restoration methods", "content": "Recently, stereo image restoration methods have begun to\nattract more attention, and obtained better performance than\nsingle-image restoration methods, such as the tasks of super-\nresolution, deblurring, deraining, dehazing, and low-light en-\nhancement. In the stereo image super-resolution method, Jeoh\net al.[30] propose the first stereo super-resolution network to\ncompensate for the parallax between stereo images by shifting.\nWang et al.[19] use a parallax attention mechanism to merge\nsimilar features in two views to explore pixel correspondence.\nIn the stereo deblurring method, Zhou et al.[31] align features\nby estimating the difference between the left and right views.\nLi et al.[32] propose a new stereo image deblurring model by\nexploring two-pixel alignment. In the stereo dehazing method,\nPang et al.[33] use a stereo transformation module to explore\nthe correlation between binocular images. As for our low-light\ntask, DVENet[21] is regarded as the representative method,\nwhich is to enhance the network by integrating multi-scale\ndual-view features, and fuse the features of a single image\nunder the guidance of the light map. Recently, DCI-Net[2] tries\nto further exploits the interaction between the left and right\nviews by considering the spatial connection between multiple\nscales.\nIn this paper, we focus on the low-light enhancement task.\nWe propose SDI-Net to fully incorporate the information from\nthe left and right views via the attention mechanism imposed\non different aspects, such as the view level, the channel level\nand the pixel level. We introduce the technical details of SDI-\nNet in the following section."}, {"title": "III. METHOD", "content": "The SDI-Net model takes low-light stereo image pairs\nas inputs, and utilizes two identical UNet branches as the"}, {"title": "A. Overall framework", "content": "The overall framework of SDI-Net is shown in Fig.2, which\ncan be divided into three stages: Feature Encoder, Cross-\nView Sufficient Interaction, and Feature Decoder.\nFeature Encoder. First, we take the left-view low-light\nimage \\(I_l \\in \\mathbb{R}^{H\\times W\\times 3}\\) and the right-view low-light image\n\\(I_r \\in \\mathbb{R}^{H\\times W\\times 3}\\) as the inputs of the encoders. Based on the en-\ncoders, the feature maps \\(F_l \\in \\mathbb{R}^{\\frac{H}{4} \\times \\frac{W}{4} \\times C}\\) and \\(F_r \\in \\mathbb{R}^{\\frac{H}{4} \\times \\frac{W}{4} \\times C}\\\nare learned. This process can be formulated as:\n\\[F_l = FE(I_l), F_r = FE(I_r)\\]\nwhere FE(\u00b7) means feature encoder.\nThe encoders comprise several convolutional layers fol-\nlowed by down-sampling operations, aiming to initially cap-\nture and encode the local and global information of each view.\nOf note, the learned network weights are shared between the\ntwo branches. Then, we send the obtained feature information\nto the next stage of cross-view sufficient interaction.\nCross-View Sufficient Interaction Module. The CSIM\nmodule stays between the encoders and the decoders of the\ntwo UNet branches, aiming to promote sufficient interaction\nbetween the features learned from the two views. The CSIM\nmodule is composed of the Cross-View Attention Interaction\nModule (CAIM) part and the Pixel and Channel Attention\nBlock (PCAB) part. The former part focuses on exploring the\nmutual attention of \\(F_l\\) and \\(F_r\\) at the view level. Then, the\nlater part concentrates on exploiting the attention mechanism\nat the channel level and the pixel level. Specifically, channel\nattention (CA) helps to emphasize informative channels, while\npixel attention (PA) highlights relevant spatial locations. In\nthis way, the CSIM module enables sufficient interaction\nbetween the two views, obtaining the mutually fused features\n\\(SF_l\\in \\mathbb{R}^{\\frac{H}{4} \\times \\frac{W}{4} \\times C}\\) and \\(SF_r \\in \\mathbb{R}^{\\frac{H}{4} \\times \\frac{W}{4} \\times C}\\).\n\\[SF_l, SF_r = CSIM(F_l, F_r)\\]\nwhere CSIM(\u00b7) represents the core part of our SDI-Net.\nThen, \\(SF_l\\) and \\(SF_r\\) are sent to the next feature decoding stage\nfor image reconstruction.\nFeature Decoder. In this stage, the feature decoders re-\nconstruct the enhanced stereo images based on the enriched\nfeature representation \\(SF_l\\) and \\(SF_r\\). The two feature maps are\nrespectively fed into the decoders of the two UNet branches.\nThis process involves two up-sampling operations, followed by\nconvolution layers to recover the spatial dimensions and gen-\nerate the final enhanced stereo images. The first up-sampling\noperation is to send \\(SF_l \\in \\mathbb{R}^{\\frac{H}{4} \\times \\frac{W}{4} \\times C}\\) and \\(SF_r \\in \\mathbb{R}^{\\frac{H}{4} \\times \\frac{W}{4} \\times C}\\\ninto the convolutional layer through 8 residual blocks and one\n3*3 up-sampling convolutional layer, respectively, to obtain\nthe first recovery of the middle layer features \\(M_l \\in \\mathbb{R}^{\\frac{H}{2} \\times \\frac{W}{2} \\times C}\\\nand \\(M_r \\in \\mathbb{R}^{\\frac{H}{2} \\times \\frac{W}{2} \\times C}\\), and then fuse the feature information\nextracted by the first layer of the Feature Encoder on the spa-\ntial domain through the concatenation operation. The second\nup-sampling operation is to process \\(M_l \\in \\mathbb{R}^{\\frac{H}{2} \\times \\frac{W}{2} \\times C}\\\nand \\(M_r \\in \\mathbb{R}^{\\frac{H}{2} \\times \\frac{W}{2} \\times C}\\) identically to obtain \\(E_l \\in \\mathbb{R}^{H\\times W\\times 3}\\) and\n\\(E_r \\in \\mathbb{R}^{H\\times W\\times 3}\\). This process can be formulated as follows:\n\\[E_l = FD(SF_l), E_r = FD(SF_r)\\]\nwhere FD(\u00b7) stands for the feature decoders. Similar as the\nencoding stage, the learned weights of the decoders are shared\nbetween the two branches.\nTo train the whole model, we use the L1 loss and the FFT\nloss to achieve the low-light enhancement task."}, {"title": "B. Cross-View Sufficient Interaction Module (CSIM)", "content": "The CSIM module is composed of two parts, i.e. Cross-\nView Attention Interaction Module (the left part of Fig.3) and\nPixel and Channel Attention Block (the right part of Fig.3),\nwhich are described in the following."}, {"title": "Cross-View Attention Interaction Module (CAIM)", "content": "CAIM is used to extract the correlation information at the\nview level. The correlation computing process is based on\nScaled Dot Product Attention[34], which utilizes all the keys to\ncompute the dot products of the query, and applies the softmax\nfunction to obtain the weights of the values:\n\\[Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{C}}) V\\]\nwhere Q represents the query matrix, K represents the key\nmatrix, V represents the value matrix, and C is the channel\nnumbers. In our application, we use \\(Q_l = Conv(LN(F_l))\\\\)\nand \\(K_r = Conv(LN(F_r))\\) to represent the features of the\ntwo views, where LN(\u00b7) represents layer normalization, and\nConv(\u00b7) represents a 1\u00d71 convolution operation. The refined\nfeatures can be obtained as:\n\\[F_{r\\rightarrow l} = Attention(Q_l, K_r, Conv(F_l)) + F_l\\]\n\\[F_{l\\rightarrow r} = Attention(K_r, Q_l, Conv(F_r)) + F_r\\]\nFrom above process, the computing process for obtaining \\(F_{r\\rightarrow l}\\\nand F_{l\\rightarrow r}\\) fully considers the interaction from each other view."}, {"title": "Pixel and Channel Attention Block (PCAB)", "content": "PCAB is\ncomposed on several stacked feature enhancing blocks for\nfurther refining \\(F_{r\\rightarrow l}\\) and \\(F_{l\\rightarrow r}\\). In the first feature enhancing\nblock (FEB), we initially pre-process \\(F_{r\\rightarrow l}\\) and \\(F_{l\\rightarrow r}\\) based\non the following procedure:\n\\[R_l = Conv(G(Conv(F_{r\\rightarrow l})) + F_{r\\rightarrow l})\\]\n\\[R_r = Conv(G(Conv(F_{l\\rightarrow r})) + F_{l\\rightarrow r})\\]\nwhere Conv(\u00b7) refers to a 3\u00d73 convolution operation and G(\u00b7)\nstands for GeLU activation function. Then, the channel atten-\ntion function CA(\u00b7) and the pixel attention function PA(\u00b7)\n[35] are introduced to further exploit the feature correlation at\nthe channel level and the pixel level:\n\\[R_l^* = PA(CA(R_l)), R_r^* = PA(CA(R_r))\\]\nThe channel attention (CA(\u00b7)) learns the importance of each\nfeature channel, aiming to highlight more useful information.\nThe pixel attention (PA(\u00b7)) dynamically adjust the importance\nof each pixel, which aiming to enhance important details and\ntexture features while reducing the impact from noise.\nIn the following, the above process is repeated by stacking\nmultiple FEBs in a sequential way. Of note, instead of \\(F_{r\\rightarrow l}\\)\nand \\(F_{l\\rightarrow r}\\), the inputs of the blocks other than the first FEB are\nthe previous neighboring FEB's outputs. In our research, we\nempirically set the number of FEBs as 10 to build the PCAB\nmodule.\nAt the end of PCAB, we combine the gradually refined\n\\(R_l^*\\) and \\(R_r^*\\) with its original feature representation \\(F_l\\) and\n\\(F_r\\) using a weighted element-wise addition to obtain \\(SF_l\\) and\n\\(SF_r\\), which is the final feature representation learned by the\nCSIM module:\n\\[SF_l = \\gamma_l R_l^* + F_l\\]\n\\[SF_r = \\gamma_r R_r^* + F_r\\]\nwhere \\(\\gamma_l\\) and \\(\\gamma_r\\) are trainable weights and are initialized with\nzeros to ensure stable training."}, {"title": "C. Loss function", "content": "In this paper, we use the L1 loss and the FFT loss to train\nthe whole network, which can be expressed as follows:\n\\[L = L_1 + \\lambda L_{fre}\\]\nwhere \u03bb stands for a hyper-parameter, empirically set to\n0.1. It is important to note that \\(L_{fre}\\) helps to restore the\nnormal light image via preserving the frequency-domain image\ncharacteristics. The two loss terms can be expressed as:\n\\[L_1 = ||E_l - E_l^{GT}||_1 + ||E_r - E_r^{GT}||_1\\]\n\\[L_{fre} = ||\\varphi(E_l) - \\varphi(E_l^{GT})||_1 + ||\\varphi(E_r) - \\varphi(E_r^{GT})||_1\\]\nwhere \\(||.||_1\\) is the L1 loss, which is used to maintain the overall\nstructure and normal-light and low-light details of the image.\n\\varphi(.) stands for Fast Fourier Transform, which helps to recover\nthe texture details and edge information of the image, making\nthe reconstructed image closer to the image under normal-light\nconditions. \\(E_l\\) and \\(E_r\\) represent the restored left and right\nnormal-light images, and \\(E_l^{GT}\\) and \\(E_r^{GT}\\) represent the ground\ntruth corresponding to \\(E_l\\) and \\(E_r\\)."}, {"title": "IV. EXPERIMENTS", "content": "Datasets. We adopt the existing public datasets specifically\ndesigned for low-light stereo image enhancement, i.e., Middle-\nbury and Synthetic Holopix50k from DVENet[21], to evaluate\nthe effectiveness of our method. The Middlebury dataset\nselects 136 pairs of images as the training set with resolution\nof 512x512 and 36 pairs of images for testing. The Middle-bury dataset is carefully built to offer high-quality, precisely\ncalibrated stereo images, which are crucial for the low-light\nstereo image enhancement task. The Synthetic Holopix50k\ndataset is a much larger dataset. Its training set includes\n1128 images with resolution of 512x512, and its test set\nincludes 159 images. The Holopix50k dataset originates from a\nlarger collection of real-world images captured by dual-camera\nsmartphones, offering a broader but less controlled variety of\nlow-light scenarios. This diversity is vital for training models\nto generalize across a wide range of real-world conditions.\nEvaluation Metrics. We use two mostly-adopted full ref-\nerence quality metrics PSNR and SSIM to evaluate the model\nperformance. Higher PSNR or SSIM values indicate better\nperformance.\nMethods for Comparison. We compare our proposed\nSDI-Net with several single-image (monocular) low-light en-\nhancement methods and stereo-image low-light enhancement\nmethods. The single-image low-light enhancement methods\nfor comparison include the non-learning based ones (NPE[36],\nLIME[3], RRM[9]), ZeroDCE[28], and the learning-based\nones (RetinexNet[17], MBLLEN[37], DSLR[26], KIND[16],\nDRBN[13], SNR-Aware[38], LLFormer [39] and MIRNet[1]).\nThe stereo-image low-light enhancement methods include\nDVENet[21] and the newly proposed DCI-Net[2]. Con-\nsidering the limited number of the existing stereo-image\nlow-light enhancement methods, we additionally introduced\niPASSRNet[20] for comparison. Originally designed for stereo\nimage super-resolution, iPASSRNet can be easily adapted as\na stereo-image low-light enhancement model. With the two\ndatasets, we re-trained iPASSRNet and evaluated its perfor-\nmance in our application."}, {"title": "B. Quantitative Comparison", "content": "The quantitative results of SDI-Net and the methods for\ncomparison on Middlebury and Synthetic Holopix50k are\nreported in Table I. The non-learning traditional methods,\nsuch as NPE, LIME, JieP, and RRM, exhibit relatively poor\nperformance. The reason is that it is difficult for the model-driven methods to fit the complex mapping functions between\nlow-light and normal-light images. As for the learning-based\nmonocular low-light enhancement methods, the PSNR and\nSSIM values are generally much higher, especially for the\nrecently proposed methods such as DRBN and MIRNet. It\nis noted that the quantitative performance of ZeroDCE is\nrelatively low. The reason is that it focuses on learning an\noptimal mapping curve function by only using dark images.\nThe absence of full supervision information makes its per-\nformance not so well in terms of PSNR and SSIM. As for\nthe stereo-image low-light enhancement, we can observe an\nobvious advantage over the monocular family, showing the\neffectiveness of using stereo images in general. Among these\nmethods, our SDI-Net performs the best on both datasets in\nterms of PSNR and SSIM, showing the effectiveness and\nsuperiority of our method."}, {"title": "C. Visual Comparison", "content": "Visual results on Middlebury. Fig.4 and Fig.5 provide the\nvisual comparisons for all the methods. In Fig.4, we can see\nthat some of the methods fail to improve the visibility of the\ninput image, while other methods introduce color distortions\nor artifacts into their results. By comparing the zoomed-in\npatch of the enhanced results and the ground-truth image, we\ncan see that our method performs better than others in terms\nof the visual quality, which well improves the visibility and\nrecovers the detail textures simultaneously. We present another"}, {"title": "D. Ablation study", "content": "In the ablation study, we demonstrate the effectiveness of\nthe CAIM module, PCAB module, and the employed loss\nfunctions, in which Middlebury is the evaluation dataset.\nModel structure. In Table II, VO represents the scenario\nwhere we perform a simple interaction on the extracted\nfeatures in the middle of the encoders and decoders. In\nanother word, the CSIM is replaced with a heuristic two-round\ninteraction process. In the first round, \\(F_l\\) and \\(F_r\\) are directly\nconcatenated and then down-sampled. In the second round,\nthe down-sampled feature is respectively concatenated with\n\\(F_l\\) and \\(F_r\\) and down-sampled again. The refined feature rep-\nresentations are then sent into the decoding stage. V1 means\nthat we add PCAB after the VO version. V2 indicates that we\nonly utilize CAIM for interaction but do not use the PCAB\nmodule. By comparing the performance of our complete model\nwith V0 to V2, we observe that both CAIM and PCAB\nplay indispensable roles in our low-light enhancement task.\nFor example, the usefulness of CAIM are both empirically\nvalidated in the comparison between Ours and v1, and the\ncomparison between V2 and V0. Similarly, the comparison\nbetween Ours and V2, and the comparison between V1 and\nVO also empirically demonstrate the effectiveness of PCA\u0412.\nLoss function. In TableII, V3 means that the loss term \\(L_{fre}\\)\nis not used. Compared to the completed model, we can see\nthat there exists a clear performance decline for V3, which\ndemonstrates the importance of using the loss term \\(L_{fre}\\). The\nrationale is clear that the frequency-based image representation\nprovides a complementary role to the spatial pixel arrays.\nTherefore, the combination of the two different loss terms\neffectively promotes the visual quality of enhanced results."}, {"title": "V. CONCLUSION", "content": "For the low-light image enhancement task, the research\ncommunity have begun to explore the usage of more advanced\nimaging systems. For example, it is beneficial for enhance-\nment models to simultaneously use the binocular images as\nthe inputs. However, the limitation of insufficient interaction\nbetween these two views still exists in current research on\nlow-light stereo image enhancement. In this paper, we propose\nSDI-Net for sufficiently modeling the dual-view interaction\nfor low-light stereo image enhancement. By modeling the\nattention mechanism at the view level, the channel level and\nthe pixel level, SDI-Net facilitates comprehensive information\nexchange of the dual views, therefore better restoring the high-quality normal-light images from low-light images. Experi-mental results on two public datasets demonstrate the effec-tiveness and superiority of SDI-Net in terms of quantitative\nand visual comparison. In the future, we plan to extend the\ninteraction between the two views into the frequency domain\nand semantic domain."}]}