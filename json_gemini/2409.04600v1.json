{"title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review", "authors": ["Dmitry Scherbakov", "Nina Hubig", "Vinita Jansari", "Alexander Bakumenko", "Leslie A. Lenert"], "abstract": "Objective: This study aims to summarize the usage of Large Language Models (LLMs)\nin the process of creating a scientific review. We look at the range of stages in a review that can\nbe automated and assess the current state-of-the-art research projects in the field.\nMaterials and Methods: The search was conducted in June 2024 in PubMed, Scopus,\nDimensions, and Google Scholar databases by human reviewers. Screening and extraction\nprocess took place in Covidence with the help of LLM add-on which uses OpenAI gpt-4o model.\nChatGPT was used to clean extracted data and generate code for figures in this manuscript,\nChatGPT and Scite.ai were used in drafting all components of the manuscript, except the\nmethods and discussion sections.\nResults: 3,788 articles were retrieved, and 172 studies were deemed eligible for the final\nreview. ChatGPT and GPT-based LLM emerged as the most dominant architecture for review\nautomation (n=126, 73.2%). A significant number of review automation projects were found, but\nonly a limited number of papers (n=26, 15.1%) were actual reviews that used LLM during their\ncreation. Most citations focused on automation of a particular stage of review, such as Searching\nfor publications (n=60, 34.9%), and Data extraction (n=54, 31.4%). When comparing pooled\nperformance of GPT-based and BERT-based models, the former were better in data extraction\nwith mean precision 83.0% (SD=10.4), and recall 86.0% (SD=9.8), while being slightly less\naccurate in title and abstract screening stage (Maccuracy=77.3%, SD=13.0 vs Maccuracy=80.9%\nSD=11.8).\nDiscussion/Conclusion: Our LLM-assisted systematic review revealed a significant\nnumber of research projects related to review automation using LLMs. The results looked\npromising, and we anticipate that LLMs will change in the near future the way the scientific\nreviews are conducted, significantly reducing the time required to generate systematic reviews of\nthe literature and expanding how systematic reviews are used to guide science.\nKeywords: Large Language Models, Review Automation, Systematic Review, Scoping\nReview, Covidence.", "sections": [{"title": "Introduction", "content": "The abundance of scientific information available can be overwhelming, posing a\nchallenge for researchers to navigate relevant data. Consequently, scoping and systematic\nreviews that are helping scientists synthesize the evidence have seen a significant increase over\nthe years. Toh & Lee noted an exponential rise in the number of scoping reviews, with 2,665\nscoping reviews being published in 2020 alone, compared to less than 10 reviews annually\nbefore 2009 1. The same trend is observed in systematic reviews and meta-analyses, for example,\nin cardiology over 2,400 meta-analyses were published in 2019, quadruple the number from\n2012 2.\nA completion of a review requires substantial resources; further, there is often\nunpredictable uncertainty in the amount of resources required 3. The time to complete a single\nsystematic review varies, but authors typically give estimates in months and even years 4.\nScreening automation platforms, such as Covidence 5, facilitate systematic and scoping reviews\nby streamlining established guidelines, such as the Preferred Reporting Items for Systematic\nReviews and Meta-Analyses (PRISMA) and PICO (Population, Intervention, Comparison, and\nOutcome) to ensure transparency and rigor in the review process 6. The use of such platforms\nmay reduce the time to complete reviews by providing tools that automate key tasks, such as\nremoving duplicate references, generating flow-charts of the screening process, visual extraction\ndesigners, and workflows for several independent reviewers.\nAlthough, for example, Covidence, includes features to reduce the time to complete\nscreening, such as key term highlighting and embedded natural processing (NLP) algorithm 7 it\nprimarily organizes the significant manual work that is still needed from human reviewers like\nscreening and extraction.. Each of these steps normally requires two independent analysts, with a\nthird optional human expert supervising the process and resolving the disagreements.\nEven with two reviewers double-checking each other, as much as 3% of relevant citations\nare missed, and if only a single reviewer is used (for example, in rapid reviews), as many as 13%\nof relevant publications can be missed 8. The relatively weak performance of humans in\nscreening relevant articles has led some investigators to develop natural language processing\ntools 9-12 to automate screening. A recent statement by the National Institute for Health and Care\nExcellence (NICE) highlights a big potential of AI in the systematic review process automation\n13"}, {"title": "Introduction", "content": "Large Language Models (LLM) recently emerged as one of the most powerful NLP tools\nacross different ranges of tasks. By conducting this review, we wanted to evaluate the natural\nextension of the use of LLMs to guide and direct the review process. Thus, this systematic\nreview aims to (1) summarize the current state-of-the-art research projects using LLMs to\nautomate the review process, (2) look at the range of review types and review stages that are\nbeing automated, (3) assess the quality of each research project, (4) assess the performance of\nLLMs used for automation.\nAs LLMs are used as a possible substitute for a human reviewer, the idea behind this\npublication is to turn the tool \u201con itself\u201d, conducting a systematic review of research projects\nusing LLMs for systematic reviews."}, {"title": "Methods", "content": "The study's research plan was formulated by the author team and adjusted based on the\nguidance provided by the preferred reporting items for systematic review and meta-analysis\nprotocols (PRISMA-P) 2015: elaboration and explanation 14 and the latest JBI checklist 15 for\nconducting systematic reviews. The review protocol was registered in the Open Science\nFramework (OSF) database 16.\nWe decided that to be included in the review, citations had to be centered around the usage\nof LLMs in automation of different phases of systematic review. Only English-language journal\npublications were considered, including, conference abstracts, and review publications that used\nLLMs in their creation.\nPublications were excluded if they:\n\u2022\nDid not use some kind of LLM (e.g. ChatGPT, Mistral, GPT-3.5, BERT)\n\u2022\nDid not describe automation of any stage of the review process\n\u2022\nThe paper was a review article itself that did not use LLM to conduct the review\n\u2022\nFull text of the article could not be retrieved or was not in English\nThe initial search was conducted by a human reviewer (DS) in June 2024 in PubMed,\nScopus, Dimensions, and Google Scholar databases."}, {"title": "Methods", "content": "All citations were then uploaded to Covidence. The screening and extraction process took\nplace in Covidence with the help of LLM plugin for Covidence that our team developed. This\nplugin is used during screening and extraction phases. The process of using LLM for screening\nand extraction is shown on Figure 1.\nThe developed add-on works by interacting with the Covidence platform programmatically\nvia an intermediary software solution that was created in Python and R. The solution passes\ncontents between Covidence and the LLM OpenAI gpt-4o model . Once the LLM generates the"}, {"title": "Methods", "content": "response, a script automates actions in Covidence, such as clicking Include/Exclude buttons or\nleaving notes.\nThe review process involved three stages that were automated by Covidence add-on:\nabstract screening, full-text screening, and extraction. In each stage, two human reviewers\ncalibrated by screening a sample to refine inclusion criteria and extraction categories. They then\ncreated and tested prompts for the LLM. LLM inference was programmed to run inference 3 times\nto determine the final decision (e.g., \"include\" or \"exclude\") based on the majority vote. Three\nprompts per phase are detailed in Supplementary Appendix S1.\nFor abstract screening, LLM and human reviewers voted for consensus, and a human expert\nconsensus was established. In full-text screening and extraction, a single human reviewer verified\nLLM results. Extraction precision was measured, and for categories with low precision (<80%), a\nmanual reviewer validated LLM outputs. Benchmarks are provided in Supplementary Appendix\nS2.\nThe data charting form for extraction were designed by human experts (DS, VJ, AB, LL,\nand NH) and adopted into the LLM prompt to collect the following primary information:\n\u2022\nAuthor, year, title;\n\u2022\nCountry and/or US state;\n\u2022\nWhat types of reviews were automated;\n\u2022\nStage of review automated in the research project;\n\u2022\nLLM type used;\n\u2022\nPerformance metrics reported by authors during each stage of the review. In\nparticular, Accuracy, Precision, Recall, Specificity, and F1 were extracted, if other\nmetrics were used instead, they were grouped under \u201cOther metrics\u201d category;\n\u2022\nBrief information on how were these performance metrics calculated;\n\u2022\nBrief information on reported timesaving;\n\u2022\nWhat was general opinion of the study team on the usage of LLMs in review\nautomation (positive, negative, or mixed) with a citation to support this viewpoint.\nHuman reviewers (DS, VJ, AB) performed quality assessment of given studies using a set\nof selected categories from the reviewed studies and a points-based scale:"}, {"title": "Methods", "content": "\u2610\n\u2610\n\u2610\nRatings of the universities where authors are affiliated (the data was linked to QS\nranking 2024 17), maximum value across all co-author affiliations was used:\nRanked 1 to 100: 2 points\n100-1000: 1 point\n>1000: 0 point\nNumber of samples (full-texts or abstracts) that authors used to compute their\nperformance metrics:\nMore than 200: 2 points\n50-200: 1 point\n<50: 0 points\nSources of the funding of the research project (public, private or mixed)\nPublic funding: 2 points\nNo funding: 1 point\nPrivate funding: 0 points\n\u2610\n\u2610\n\u2610\n\u2610\n\u2610\n\u2610\nImpact factor of the journal 18\nMore than 5: 2 points\n1 to 5: 1 point\nLess than 1: 0 points\nIs the paper an actual review which used LLM?\nA review: 2 points\nNot a review (methods paper): 1 point\n\u2610\n\u2610\nWere performance metrics (benchmarks) reported?\n2 points for reporting performance metrics\n0 points for no metrics\nIf value in any above category could not be determined (e.g. no match for university or\nimpact factor, or unknown value in category), then the NA value was assigned. Based on the mean\nof points across all the quality categories, studies were classified as low (<1 points), medium (1 to\n1.5 points) or high quality (>=1.5 points).\nAn LLM tool by Google (NotebookLM, version from August 2024) along with a manual\nreview (DS, VJ, AB) was used to cross check the extraction results for the fields where precision"}, {"title": "Methods", "content": "of extraction was low (<0.8) during the benchmark. Again, ChatGPT (40 model) was used to clean\nthe extraction data: format the case, remove duplicates, rename similar entries to a common name.\nThe data was then manually fed into the chat window by a human reviewer (DS). Scite.ai (version\nfrom August 2024) was used to draft parts of the introduction and discussion sections, while\nChatGPT was used to draft the abstract and results section of this review by generating R code\nsnippets to produce all figures (except Figure 1 which was generated by Covidence). ChatGPT\nwas also used to draft the text of the results section, which was then corrected by our team where\nneeded. Human experts edited and verified the final LLM-generated draft of the manuscript.\nAdditionally, we report the time saving and the computational costs in Supplementary\nAppendix S3. We used our own time measurements and reference data from experienced reviewers\nto calculate time-saving 19."}, {"title": "Results", "content": "Figure 2 outlines the PRISMA article selection process for this study. Initially, 3,788\nstudies were identified across several databases: PubMed (n = 2,174), Scopus (n = 1,207),\nDimensions (n = 356), and Google Scholar (n = 48), along with 3 additional studies from citation\nsearching. Following the removal of 447 duplicates (1 manually and 446 by Covidence), 3,341\nstudies remained for the screening phase.\nDuring the title and abstract screening process, 3,041 studies were excluded, leaving 300\nstudies for retrieval and full-text eligibility assessment. Out of these 300 studies, 128 were\nexcluded for various reasons, with the most common being \u201cThe paper does not describe the\nautomation of any stage of the review process\u201d (n = 88). A total of 172 studies were included in\nthe final review."}, {"title": "Results", "content": "Figure 3 shows the geographic distribution of studies across 43 countries. Most citations are from\nthe US (n=60, 34.9%), followed by Australia (n=14, 8.14%), the UK and China (n=13, 7.6%), and Germany\n(n=11, 6.4%). Other notable contributors include Canada (n=7, 4.1%) and India (n=6, 3.5%). Austria,\nIreland, Italy, the Netherlands, and South Korea each contributed 4 studies (2.3%), while countries like\nNew Zealand, France, Japan, and others provided 3 (1.7%). The rest contributed 1-2 studies.\nIn the US, 47 studies had state-level data. Tennessee, New York, and Massachusetts led\nwith 5 citations each (10.6%), followed by California (n=4, 8.5%). North Carolina and Ohio\ncontributed 3 studies (6.4%), while several other states provided 2 (4.3%) or 1 (2.1%) citation"}, {"title": "Results", "content": "Figure 4A shows the types of reviews discussed in automation papers. The most frequently\nmentioned type is \u2018Systematic Review' (n=118, 68.6%), followed by \u2018Literature/Narrative\nReview' (n=37, 21.5%) and \u2018Meta-Analysis' (n=19, 11.0%). The remaining categories include\n'Scoping Review' (n=8, 4.7%), \u2018Other/Non-specific' (n=14, 8.1%), and 'Rapid Review' (n=6,\n3.5%). 'Umbrella Review' has a smaller representation with 2 mentions (1.2%).\nFigure 4B illustrates the stages of review discussed in automation papers. The most\nfrequently mentioned stage is \u2018Searching for publications' (n=60, 34.9%), followed by \u2018Data\nextraction' (n=54, 31.4%) and \u2018Evidence synthesis/summarization\u2019(n=32, 18.6%). Other\ncategories with notable mentions include \u2018Title and abstract screening' (n=43, 25.0%), \u2018Drafting\na publication' (n=22, 12.8%), 'Full-text screening' (n=14, 8.1%), 'Quality and bias assessment'\n(n=12, 7.0%), \u2018Publication classification' (n=10, 5.8%), \u2018Other stages' (n=6, 3.5%), and \u2018Code\nand plots generation' (n=4, 2.3%).\nThe most frequently mentioned Al model is GPT/ChatGPT, with 126 occurrences (73.3%),\nshowing its widespread use (Supplemental Figure S5). BERT-based models are also notable with 32\nmentions (18.6%). LLaMA/Alpaca models have 8 mentions (4.7%), followed by Google Bard/Gemini with\n5 (2.9%), and Claude models with 7 (4.1%). Other models like BART (n=3, 1.7%) and Mistral (n=4, 2.3%)\nare less frequent. Several models, including Bing and XLNet, have 2 mentions each (1.2%), while many\nothers are mentioned just once (0.6%).\nOf the 172 citations, 79 (45.9%) reported common metrics like Accuracy, Precision/Recall,\nand F1, while 36 (20.9%) used less common metrics like G-score and Jaccard similarity. The\nremaining 57 publications (33.1%) relied on qualitative assessments."}, {"title": "Results", "content": "Figure 5 shows performance metrics for GPT- and BERT-based models. GPT models had\nlower accuracy in title/abstract screening (M=77.34, SD=13.06) compared to BERT models\n(M=80.87, SD=11.81). However, GPT models performed better in data extraction, with precision\n(M=83.07, SD=10.43) and recall (M=85.99, SD=9.82), while BERT models had lower precision\n(M=61.06, SD=31.26) and similar recall (M=80.03, SD=10.09). In title/abstract screening, BERT\nmodels had higher precision (M=65.6, SD=17.65) but lower recall (M=72.93, SD=23.95) than\nGPT models (precision M=63.2, SD=24.34; recall M=80.42, SD=23.31).\nMajority of reviewed publications were papers describing how LLM could be used to\nautomate a certain phase of the review (n=146, 84.9%) (Supplemental Figure S6A). Only 26\n(15.1%) papers were actual reviews conducted with some help from LLM tools. Majority of\nauthors were positive about the usage of LLMs in reviews (n=120, 69.8%), with 43 citations\n(25.0%) containing mixed or cautious views on LLM usage (Supplemental Figure S6B). Only 9\n(5.2%) study teams had negative experiences with LLM usage. Most studies had public funding\nreported (n=97, 56.4%) (Supplemental Figure S6C). When considering all the factors together,\nsuch as funding, journal impact factor, sample size, reported metrics, and others (see Methods), 72\ncitations (41.9%) appear to be of high quality, with 73 citations being medium quality (42.4%)\n(Supplemental Figure S6D).\nSupplemental Table S7 presents the extraction table with all extracted categories across\n172 citations."}, {"title": "Discussion", "content": "Our LLM-assisted systematic review revealed a significant number of research projects\nrelated to review automation with LLM. Indeed, other researchers have noted promising results\nfor LLMs in different areas, such as understanding human language and generating contextually\nappropriate responses 20-22.\nDespite finding a significant number of projects using LLMs to automate some stages of\nthe review process only few papers focused on the full cycle of review automation 23,24. There\nmight be perceived publication barriers, for example, journals recently started to ask about LLM-\ngenerated content, although we don't have information on whether this leads to changes in\nreviewing process. Growing number of LLM-generated papers will probably eventually change\nhow review is conducted (reviewers might be assisted by LLMs or review paper format could be\neventually replaced by online real-time information retrieval).\nThe strength of present review is in large-scale (over 3000 abstracts screened, and 172 full-\ntext publications eligible for extraction) automation of different stages of review, including\ndrafting the manuscript sections, and plot generation. Only few citations focused on automation of\nfull cycle of review, while most focused only on specific areas like extraction or screening,\nincluding our own previous systematic review where GPT-3.5 was used with LDA-based topic\nmodelling for validation of human findings 25. In contrast, the LLM-based method that we applied\nin this work demonstrated its direct applicability, by facilitating the automation of the abstract and\nfull-text screening, data extraction, as well as the knowledge synthesis stages, with the discussed\nconstraints. Furthermore, our method is domain-agnostic, thus it can be integrated into large-scale\nreview projects across different domains. The implications of such automation include reducing\nhuman workload and improving overall efficiency of systematic reviews. Furthermore, such tool\nin its more mature form will require less expertise from human reviewers, which could contribute\nto the democratization of systematic and scoping review process, with the potential to add features\nrelated to meta-analysis into the process.\nGPT-based LLM were the most dominant type of LLM and the one that seems to show\nremarkable results on the data extraction, arguably the most complex and time-consuming stage\nof any review. It's usage for literature reviews is obvious, at this moment there are little restrictions\non the type of information users can load into ChatGPT, and published papers are unlikely to\ncontain any sensitive information, making ChatGPT with its high-performing model and\ndeveloped API an obvious choice. At the same time smaller models like BERT, Llama or Mistral"}, {"title": "Limitations", "content": "We used calibrated LLMs as reviewers in this project. Some extraction categories, such as\nperformance metrics, had relatively lower accuracy, so the results of this extraction category\nshould be taken with caution. Nevertheless, in this review LLMs achieved remarkable results in\naccuracy, making it possible to delegate time-consuming phases of review to LLMs. Studies\ngenerally recommend a single reviewer approach in some cases like rapid reviews27. However we\nbelieve that the LLM approach could substitute human reviewers, and human effort should be\nredirected to supervision of the review process.\nA further limitation of this work is the simplified scoring system we introduced for research\nevaluation, which, using arbitrary weightings, may overlook key aspects like the novelty,\nrobustness, and relevance of the studies. Future research should focus on improving LLM\nperformance metrics, particularly precision and recall in lower-accuracy extraction categories.\nAdditionally, integrating and evaluating different LLMs, possibly in combination with other AI\nmodels, should be explored to enhance performance. The short- and long-term impact of these\nintegrations on review quality, along with ethical considerations, must also be assessed to maintain\nresearch credibility and trust."}, {"title": "Conclusion", "content": "The use of LLMs in review automation is rapidly growing, with expected radical changes\nin scientific evidence synthesis. LLMs are likely to significantly reduce the time needed for\nreviews while producing similar or higher-quality data in greater quantities than manual reviews.\nResearch shows it is becoming increasingly difficult to distinguish between LLM-generated and\nhuman-written text.28 and the presence of LLM generated texts in scientific publications in"}, {"title": "Conclusion", "content": "growing exponentially 29. To promote transparency and proper acknowledgment, researchers are\nencouraged to openly disclose their use of LLMs in academic papers, providing information on\nthe prompts employed and the sections of text affected 30.\nDespite early successes, few systematic reviews using LLMs were identified in our review.\nAlthough still in its early stages, AI-assisted reviews are already yielding impressive results, with\ngrowing interest as researchers develop semi-automated pipelines. However, generating\ntrustworthy and useful AI-driven reviews still presents both technological and ethical challenges,\nparticular for quantitative meta-analyses comparing treatment effects. However, the conduct of\nmore simple systematic reviews, such as scoping reviews, appears to be well within the capabilities\nof current or near future AI methods."}, {"title": "Author contributions", "content": "LL, NH, and DS conceived and designed the review. DS developed the LLM screening\nautomation add-on for Covidence. DS and VJ contributed to search strategy development. DS,\nAB and VJ performed the benchmarks for LLM and designed LLM prompts. DS, AB and VJ\nverified data extraction results. VJ and AB researched third-party components that were used to\ncreate the review. AB developed the script for journal impact factor assessment. DS analyzed the\ndata and drafted the manuscript with the help of scite.ai and ChatGPT. NH and LL found the\nresources to conduct the review. All authors critically reviewed and revised the manuscript and\napproved the final version for submission."}, {"title": "Funding", "content": "This publication was supported, in part, by the National Center for Advancing\nTranslational Sciences of the National Institutes of Health under Grant Number UL1 TR001450.\nDr. Scherbakov was supported by grant T15 LM013977, Biomedical Informatics and Data\nScience for Health Equity Research (SC BIDS4Health). This publication was supported in part\nby a Smart-state Chair endowment. The content is solely the responsibility of the authors and\ndoes not necessarily represent the official views of the National Institutes of Health."}, {"title": "Data availability", "content": "The data underlying this article are available in the article and supplementary materials."}, {"title": "Conflicts of interest statement", "content": "The authors have no competing interests to declare."}]}