{"title": "TOKEN PRUNING USING A LIGHTWEIGHT BACKGROUND AWARE VISION TRANSFORMER", "authors": ["Sudhakar Sah", "Ravish Kumar", "Honnesh Rohmetra", "Ehsan Saboori"], "abstract": "High runtime memory and high latency puts significant constraint on Vision Transformer training and inference, especially on edge devices. Token pruning reduces the number of input tokens to the ViT based on importance criteria of each token. We present a Background Aware Vision Transformer (BAVIT) model, a pre-processing block to object detection models like DETR/YOLOS aimed to reduce runtime memory and increase throughput by using a novel approach to identify background tokens in the image. The background tokens can be pruned completely or partially before feeding to a ViT based object detector. We use the semantic information provided by segmentation map and/or bounding box annotation to train a few layers of ViT to classify tokens to either foreground or background. Using 2 layers and 10 layers of BAViT, background and foreground tokens can be separated with 75% and 88% accuracy on VOC dataset and 71% and 80% accuracy on COCO dataset respectively. We show a 2 layer BAViT-small model as pre-processor to YOLOS can increase the throughput by 30% - 40% with a mAP drop of 3% without any sparse fine-tuning and 2% with sparse fine-tuning. Our approach is specifically targeted for Edge AI use cases. Code and data are available at [Link].", "sections": [{"title": "Introduction", "content": "Transformers [1] have already demonstrated their ability to outperform traditional methods in Natural Language Processing (NLP) with models like BERT [2] and ROBERTa [3]. They are now commonly used in modern vision- related tasks such as classification [4], object detection [5] [6] [7], segmentation [8], and pose estimation [9] as Vision Transformers(ViT). Despite the advantages of ViTs over traditional CNN-based approaches, their high computational requirements pose significant challenge in deployment of these models on edge devices with limited memory and computational power. The ViT accepts small image patches (typically 16 \u00d7 16 size) called tokens as input. As image resolution increases, more input tokens are generated, which increases the performance but reduces model throughput and latency.\nViT is also used for object detection by models like DETR [5] which uses learnable queries and encoder features to produce box predictions using decoder. Different variations of DETR-like models like [10][6], [7] are proposed to create state of the art object detection models.\nZheng et.al [11] showed that the complexity of Deformable DETR [12] is 8.8\u00d7 compared to the decoder which suggests that focusing on efficiency of the encoder is very important. All the tokens do not have same importance and by reducing the number of tokens results in latency and throughput improvement. The technique to reduce the number of tokens by assessing the importance or relevance of each token is called token pruning. In this work, we aim to reduce the number of input tokens by introducing a novel token importance criteria for pruning with a minimal impact on performance. Our approach uses segmentation masks provided in the COCO (80 object categories) [13] and PASCAL VOC (20 object categories) [14] datasets to annotate each individual patch as foreground (FG) or background (BG). This annotation serves as a guide for ViT models in object detection tasks to determine the importance of each token. Sparse DETR [15] and Focus DETR [11] are two most impressive and state of the art techniques for token pruning. As show in Figure 1, sparse DETR uses the token importance score by computing cross-attention map in the decoder which"}, {"title": "Related Work", "content": ""}, {"title": "Vision Transformers", "content": "Transformers [1] have emerged as a dominant architecture in NLP [2] [3] as well as vision-related tasks [10] [7] and these models [16] have achieved state-of-the-art performance for vision tasks including object detection such as DETR [5], RT-SETR[10] and YOLOS[7]. DETR [5] employs a combination of CNN-based backbones followed by transformers to address object detection tasks. Swin-transformers [4] introduced new ViTs that can serve as general- purpose backbones for computer vision tasks. WBDetr[6] replaced the CNN-based backbones in DETR [5] with a transformer-based backbone for object detection. Similarly, innovations continue to enhance ViT capabilities, such as [17], which introduces a K-dimensional score map to provide localized information about image patches. Recent work by Fang et al. [7] proposes end to end object detection as sequence-to-sequence task. Our BAVIT proposes an additional information about of these image tokens as BG and FG, which can be integrated as the pre-processing stage to filter out unnecessary patches."}, {"title": "Runtime Memory Improvement", "content": "ViTs [16] require substantial runtime memory, which limits their use on smaller devices. Many research efforts, including [18] [19] [20], propose methods to optimize the performance of vision transformers. Reformer [21] introduces architectural changes to the residual layers, replacing them with reversible residual layers to make the model more efficient. Sparse attention[18] proposes an alternative attention formulation through sparse factorization of the attention matrix, which is one of the most computationally expensive components in ViTs. Sparse Detr[15] enhances the efficiency"}, {"title": "Token Pruning", "content": "The number of tokens contribute to quadratic complexity in ViTs during inference. However, all the tokens generated from the input image are not equally important; many primarily contain background information. Several research efforts, including [27] [28] [29] [15] [11], propose efficient approaches to remove unnecessary tokens, thereby improving the inference time. [11] introduces a technique that efficiently scores the importance of tokens, discards background queries, and enhances the semantic interaction of fine-grained object queries based on these scores. [29] proposes an adaptive method to hierarchically discard useless tokens and adjust computational costs for different input instances. [28] suggests reusing pruned tokens at later stages of the model. Our work is very close to Focus DETR [11] as both approaches focus on classifying tokens into FG and BG. However, Focus DETR uses a heavy backbone from DETR (like ResNet50, ResNet101 [30]) which is not suitable for edge devices. Also, Focus DETR proposes many modifications to the existing DETR model which requires model retraining or fine-tuning for a long time. Therefore, although the technique produces SOTA results, it is not feasible approach for edge devices. Our work proposes a simpler strategy for background token identification using a learnable small ViT model using 2 layers. Also, our approach produces foreground images which visibly looks very similar to Focus DETR produced foreground images but our approach uses a very small model, compared to Focus DETR, to achieve this. BAViT can be used as a separate module and integrated with other models at the pre-processing data stage, enabling faster performance and making the models suitable for smaller devices. Our target use case is small ViTs for edge devices, therefore it is difficult to compare our method with Focus DETR MAP/latency numbers which uses very large model and performs latency experiments on larger GPUs."}, {"title": "Methodology", "content": ""}, {"title": "Auxiliary Annotations", "content": "Transformers accept image patches (called tokens) of size (k \u00d7 k), created by dividing the input image into a sequence of square patches, as shown in Figure 2. ViTs use these patches to classify objects in the image through the attention mechanism. Popular datasets like Microsoft COCO [13] and Pascal VOC [14], used for object detection and segmenta- tion tasks, contain annotations such as bounding boxes and instance segmentation maps. We create a M-dimensional patch annotation vector for every input image, where M represents the total number of tokens formed by dividing the input image into k \u00d7 k smaller non-overlapping patches as shown in Figure 2. We compare the Jaccard similarity coefficient [31] of each token with all the bounding boxes or segmentation map and it is labeled as one (Foreground - FG) if the overlap of a token with any of the bounding box is more than 0.5, otherwise it is labeled as zero (Background - BG) as shown in Equation 1 and Equation 2. Figure 2 shows a sample Pascal VOC image (left), bounding boxes (center), and image patches with BG patches in gray and FG patches in red color. When using segmentation maps to create the annotation vector, any image patch with more than 10% overlapping pixel with any class of segmentation map is considered foreground; otherwise, it is considered background. We trained BAViT model both using bounding box annotations and segmentation maps but most of the results presented in this paper are from annotated data using segmentation map.\n$L\u2081 = \\begin{cases}1 & \\text{if } J(P_i, B_j) \\geq \\tau\\\\0 & \\text{if } J(P_i, B_j) < \\tau\\end{cases}$ (1)\n$J(P_i, B_j) = \\frac{|P_i \\cap B_j|}{|P_i \\cup B_j|}$ (2)\nwhere $P_i$ is patch and $B_i$ is bounding box, $L_i$ is assigned label for $i^{th}$ token, $J(P_i, B_j)$ is Jaccard coefficient, $\\tau$ is threshold for selecting the token as foreground or background."}, {"title": "BAVIT Architecture", "content": "BAVIT architecture is created by introducing few fundamental changes in the traditional ViT architecture as illustrated in Figure 3 (left). We remove the CLS token and introduce a linear layer with two output classes for each token. Traditional ViT uses CLS token to encapsulate knowledge from all tokens and it provides the score for each class."}, {"title": "Accumulative Cross Entropy Loss", "content": "In contrast to the traditional ViT classifier training, which involves introducing an additional classification token (CLS) and calculating loss only for that token, we propose a new loss function that calculates the Cross Entropy Loss [32] for each token individually and then aggregates these losses. This aggregated loss is termed as Accumulative Cross Entropy Loss ($L_{acc}$), as defined in 3.\n$L_{acc} = \\frac{1}{N M} \\sum_{i=1}^N \\sum_{j=1}^M \\sum_{c=1}^C Y_{i,j,c} \\cdot log(\\hat{Y}_{i,j,c})$ (3)\nwhere $N$ is the the number of image samples, $M$ is the number of tokens per sample, $C$ is the number of classes (background and foreground). $Y_{i,j,c}$ is the variable indicating whether the j-th token in the i-th sample belongs to class c. It's value is one if the token belongs to class c, otherwise it is zero. $\\hat{Y}_{i,j,c}$ is the predicted probability of the j-th token in the i-th sample being in class c."}, {"title": "Model Training", "content": "We use both Pascal VOC [14] and COCO 2017 [13] to train BAViT and reported mAP (mean Average Precision) result on the validation dataset for both. Each training batch, denoted as (B, M, S), consists of M tokens of size 16 \u00d7 16, each with an embedding size of S = 192, and labels for each token indicating either BG or FG. We employed the Adam [33] optimizer with a step learning rate scheduler and trained the model for 100 epochs until convergence. The initial weights for the ViT [34] model were loaded from ImageNet-1k [35] dataset pre-trained model."}, {"title": "BAVIT Integration with ViT based Detection", "content": "The BAViT-small is added as a pre-processing block of the ViT based object detector as shown in Figure 3. We have used YOLOS [7] as the object detection model, an architecture similar to DETR[5] with an exception that YOLOS provides an option to use the detector without a CNN backbone. Our method works directly on image tokens, so it cannot be applied to a CNN backbone based ViT object detectors. The BAVIT model works on 384 \u00d7 384 input and YOLOS (tiny) expects 512 \u00d7 512 inputs to achieve the benchmark mAP. BAVIT outputs the classification of each token as BG or FG with a total of 576 tokens but the YOLOS model expects 1024 tokens so we upscale the tokens labels from 576 to 1024 keeping the relative BG/FG patch position same. After the label scaling step, each of 1024 token is classified as BG or FG token. The YOLOS model only computes the FG tokens from first to the final layer. We also modify YOLOS model slightly so that it does not compute anything for the BG tokens and return zeros as the final output token for these tokens. All the FG tokens are processed in the usual manner. So, the modified BAVIT + YOLOS-tiny model contains 14 layers, first 2 layers of BAViT and the 12 layers of YOLOS-tiny."}, {"title": "Results", "content": ""}, {"title": "BG/FG Classification Model", "content": "The BAVIT model was trained with both 2 layers (BAViT-small) and 10 layers (BAViT-large) depth. Table 1 displays the token classification accuracy of these models on different datasets. BAViT-small is used for integration with object detection model (YOLOS) but we also trained the BAViT-large model to assess the impact on model accuracy. We found that BAViT-small achieved 75.93% accuracy, which was reasonable compared to BAViT-large's 88.79% accuracy for the BG/FG classification task on VOC dataset given the difference in number of parameters for these two models. We also trained both models on COCO dataset as shown in Table 1 and used BAViT-small trained with COCO with mAP 70.88% as pre-processing block. YOLOS-tiny model has 6.5M parameters using 18.8 GFLOPS. Addition of BAViT-small over the native YOLOS-tiny marginally increases the total number of parameters (by 1.49M) and FLOP counts (+1.961 GFLOPS) but substantially reduced the amount of total number of tokens (25.63%) which has the quadratic impact over the computational complexity of the ViT models. On the other hand focus DETR [11] models with ResNet50 backbone has 48M parameters using GFLOPS which is almost 8x times bigger and slower. Our results also suggest that it can be applied to different datasets with configurable number of layers based on latency and RAM constraints."}, {"title": "Token reduction using BAVIT", "content": "As explained in section 3.5, we added BAViT-tiny to pre-process the image and classify each patch as FG or BG tokens before passing to YOLOS model. Using FG patches for all computation and ignoring all the BG patches, we can reduce the number of tokens in YOLOS-tiny model drastically. Equation 4 shows the calculation used to calculate the average reduction in tokens for 5000 COCO validation images. Table 2 shows BAViT model used with different level of sparsity for token pruning and the impact on mAP due to the same. The sparsity is controlled by modifying the confidence threshold of background tokens. Our BAViT model adds extra complexity to the overall model but since this model has very low complexity and it works at much lower resolution, the overall number of token is less than the original model. for eg. the model with 34% sparsity reduces total tokens by 24% with an accuracy drop of 2.6% on COCO dataset. Please note that we are not demonstrating the results of fine-tuning for most of these models. However, we have finetuned one of the model with 35% sparsity and could improve the accuracy by 2 mAP points. It is important to note that we fine-tuned the model only for 30 epochs to improve the accuracy.\nAlthough our method suffers a drop in mAP due to sparsification, it is still applicable to edge use cases whereas solution proposed in methods like Sparse DETR [15] and Focus DETR [11] can't be used. Focus DETR, being the SOTA in token pruning field, uses ResNet50 and ResNet101 backbones to detect background tokens, which makes it impractical for edge use cases with very limited memory and computational capabilities. Also, Focus DETR proposes significant"}, {"title": "Conclusion", "content": "In this work, we introduced a novel method for separating BG/FG patches in images by leveraging existing annotations from bounding boxes and segmentation maps to create localized annotations. We applied these annotations within a token classification training strategy, achieving an accuracy of up to 88.79% on the Pascal VOC dataset and 80.57% on the COCO dataset using a 10-layer transformer model. Notably, even with just 2 transformer layers, we were able to achieve over 75% accuracy on Pascal VOC and 70% on COCO dataset respectively. We also used BAVIT-small model for pre-processing step to prune tokens of a YOLOS-tiny model. Our approach could reduce the number of tokens by 25% with a mAP drop of 3% on COCO dataset. This drop is shown to be recovered (less than 2% mAP drop) by sparse token finetuning by using just 30 epochs. BAViT approach is a low cost and low complexity alternative to SOTA methods like Focus DETR [11] which works on large models not fitting on edge devices. Future work involves integrating our approach to YOLOS type of model to jointly train BG/FG classifier and object detector together to observe the accuracy-latency trade-off. Additionally, we also aim to achieve adaptive sparsity based on input image complexity, with a learnable threshold parameter similar to [29].\n$Token Reduction = \\frac{\\sum_{i=1}^n T_{yi} - (T_{bi} + T_{yi} \\cdot s)}{N}$ (4)\nwhere $T_{yi}$ is total YOLOS tokens for $i^{th}$ image, $T_{bi}$ is total BAViT tokens for $i^{th}$ image, $s$ is sparsity percentage in $i^{th}$ image and N is total number of images."}]}