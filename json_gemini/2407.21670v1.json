{"title": "Universal Approximation Theory: Foundations for Parallelism in Neural Networks", "authors": ["Wei Wang", "Qing Li"], "abstract": "Neural networks are increasingly evolving towards training large models with big data, a method that has demonstrated superior performance across many tasks. However, this approach introduces an urgent problem: current deep learning models are predominantly serial, meaning that as the number of network layers increases, so do the training and inference times. This is unacceptable if deep learning is to continue advancing. Therefore, this paper proposes a deep learning parallelization strategy based on the Universal Approximation Theorem (UAT). From this foundation, we designed a parallel network called Para-Former to test our theory. Unlike traditional serial models, the inference time of Para-Former does not increase with the number of layers, significantly accelerating the inference speed of multi-layer networks. Experimental results validate the effectiveness of this network.", "sections": [{"title": "Introduction", "content": "In today's era, deep learning models have undoubtedly taken the lead in the fields of computer vision (CV) and natural language processing (NLP), demonstrating exceptional performance across numerous tasks. In CV, models such as ResNet(He et al. 2015) and ViT(Vaswani et al. 2017) for image recognition, U-Net (Ronneberger, Fischer, and Brox 2015) for image segmentation, GAN (Goodfellow et al. 2014) for image generation, and diffusion models like SORA (Liu et al. 2024) for video generation highlight the extensive application of deep learning in CV. These models are primarily based on convolutional neural networks (CNNs), Transformers, or a combination of both. Simultaneously, large language models (LLMs) based on Transformers have become dominant in natural language processing. Overall, deep learning models have shown vast application potential and powerful capabilities so far.\nHowever, this progress has brought forth some challenges: the development trend in deep learning is moving towards larger models and larger datasets. This trend is fundamentally driven by the UAT(Cybenko 2007; Hornik, Stinchcombe, and White 1989). From the papers UAT2LLM(Wang and Li 2024) and UAT2CV(Wang and Li 2024), we know that residual-based CNNs and Transformer-based models are specific implementations of UAT, both exhibiting the characteristic of dynamically approximating functions based on inputs. Larger models and datasets are indeed requirements of the UAT theory: deeper network layers can more precisely approximate target functions dynamically based on inputs, while larger and more diverse datasets help ensure the training data closely represents real-world conditions, making the model's approximated functions more reflective of real-world complexities.\nHowever, this trend inevitably leads to increased demands for computational resources and extended training times. As more powerful models are needed, the UAT theory mathematically necessitates larger networks, which in turn require more computational resources. While technological advancements, especially in hardware performance, provide potential ways to alleviate this issue, the increase in time costs remains a fundamental obstacle to scaling serial networks. The improvement in computational resources such as GPUs and TPUs is limited by the speed of electrical signal transmission, whereas the theoretical scale of networks has no such upper limit. When networks reach tens or even hundreds of thousands of layers, inference times can extend to several minutes or even hours for each batch. Current solutions include model optimization techniques like quantization and pruning (Sun et al. 2023; Ma, Fang, and Wang 2023; Xia et al. 2023), hardware acceleration with specialized accelerators and edge computing (Gale et al. 2020), and parallel and distributed computing (Ben-Nun and Hoefler 2018) with model parallelization, data parallelization, and distributed computing frameworks. However, these do not fundamentally address the inference delay caused by the increasing number of serial network layers.\nWe believe the root cause lies in the serial computation mechanism of existing deep learning models, where the output of one layer serves as the input for the next. This design stems from the early development of deep learning networks in the field of CV, where image data exhibit local correlations and spatiotemporal invariance. To accommodate these features, early CV networks primarily employed convolutional layers with small receptive fields. To achieve a larger receptive field, deeper network layers were necessary, leading to early deep-learning architectures like AlexNet and VGG. The introduction of ResNet brought a paradigm shift, as residual networks have the ability to dynamically fit functions based on input, influencing almost all subsequent network designs and solidifying the tradition of multi-layer stacking.\nWhile this design was manageable with fewer layers, the increasing depth and number of parameters in modern networks have created a critical issue: slow computation efficiency due to the need for sequential layer-by-layer calculations during inference. This computation method inevitably leads to delays, which become more severe as the network depth increases, significantly constraining the progress of CV and NLP technologies.\nTherefore, it is imperative to explore parallelization techniques for deep learning networks, enabling parallel computation across multiple layers. This approach is key to accelerating the inference process, breaking the time bottleneck, and advancing CV and NLP technologies towards greater efficiency and speed. To develop such parallel computation, we must start from the fundamental theory of deep learning: the UAT. The network must be designed in accordance with UAT principles and must satisfy the requirements outlined in the papers UAT2LLM and UAT2CV, which emphasize the ability to dynamically fit functions based on data. With this in mind and leveraging existing network structures, we designed a parallel network, Para-Former, to validate the feasibility of our theory. Our contributions include:\n\u2022 Proposing the theory of parallel deep learning networks and experimentally validating its feasibility.\n\u2022 Analyzing the characteristics, advantages, and disadvantages of both serial and parallel networks.\n\u2022 Investigating the issues present in deep learning from a data perspective.\nThe structure of this paper is as follows: In Section 2, we first present the general form of the Universal Approximation Theorem (UAT). Based on the mathematical form of UAT, we define the parallel network Para-Former in Section 3. In Section 4, we validate the effectiveness of Para-Former in various ways: we assess its feasibility in Section 4.2, explore the impact of network depth in Section 4.3, and analyze the influence of different datasets in Section 4.4."}, {"title": "UAT", "content": "Deep learning networks have demonstrated their powerful capabilities in numerous CV and NLP tasks. In the papers UAT2LLMs and UAT2CVs, it has been proven that both Transformer-based networks and residual-based CNNs are specific implementations of the UAT. Therefore, we consider UAT to be the fundamental theory for deep learning. To design parallelized networks, we must start from the principles of UAT. Before formally designing parallel networks, we will first review the basic form of the UAT theory and then propose a network parallelization solution based on this mathematical theory. The UAT was initially proposed by Cy-benko (Cybenko 2007), and its basic form is as follows:\n\n$G(x) = \\sum_{j=1}^{N} a_j \\sigma (W_j^T x + \\theta_j)$\n\n(1)\n\nis dense in C (In). Here, $W_j \\in R^{n}$ and $a_j,\\theta \\in R$ are parameters o is a sigmoid function. For any $f \\in C (I^n)$ and $\\varepsilon > 0$, there exists a function G(x):\n\n$|G(x) - f(x)| < \\varepsilon$ for all $x \\in I^n$.\n\n(2)\n\nThis indicates that when N is sufficiently large, a neural network can approximate any continuous function on a closed interval. Hornik, Stinchcombe, and White (Hornik, Stinchcombe, and White 1989) further proved that this function can approximate any Borel measurable function. Observing Equation (1), where the function G(x) produces scalar outputs in the real number field R, the scenario naturally extends when G(x) maps to Rm, requiring approximation in each dimension. To accommodate such multi-dimensional outputs, we simply need to extend Equation (1): the transformation matrix Wj is revised to the space Rnxm, the bias term \u03b8j is recast as a vector in Rm, and aj is reshaped into a matrix. We call the parameters Wj and aj in UAT as weight and \u03b8j as bias."}, {"title": "UAT for Parallel Network", "content": "From Equation (1), we can observe a significant characteristic: if we define different indices j as the indices of layers and the corresponding layer's mathematical form is $a_j \\sigma (W_j^T x + \\theta_j)$, it is evident that the parameters between different layers do not interact with each other. Therefore, based on (1), we can design networks where parameters across multiple layers are independent, achieving parallel computation.\nThe blocks in this network should satisfy the formula given in Figure 1.b. Clearly, under this condition, the sum of the multiple blocks' outputs adheres to the UAT theory.\nIf we design the parallel network solely based on Eq. (1), the network will face a limitation once training is complete: the function that can be approximated by the network is fixed, as the parameters in the corresponding UAT are fixed. So to ensure the network can dynamically approximate functions based on input, some or all of the parameters $a_j$, $W_j$ and $\\theta_j$ must be influenced by the input (referencing to UAT2LLM and UAT2CV), which can be simply written as $a_j(x)$, $W_j(x)$ and $\\theta_j(x)$.\nThis allows the network to dynamically adapt and fit the function based on the input while enabling parallel computation. So we could design the basic block.\n3.2 Para-former\nBased on the previous request on the basic module, we designed the Para-Former which is a parallel network by us-"}, {"title": "Speed-up Ratio", "content": "We have proposed a network parallelization scheme, and in this section, we will compare the running speed of this parallelized network with that of traditional serial networks. It is important to note that due to differences in hardware and implementation methods, providing an objective comparison through experiments can be challenging. Therefore, we will focus on a theoretical analysis. Based on the network structure shown in Figure 2, the inference time of Para-Former is only affected by its network depth.\nAssume we use a Para-Former with a depth of M and a layer count of L, compared to a traditional Transformer network with a depth of N layers, where L >> N. In this case, the running speed of the parallel network would be N/M times faster than that of the serial network. This is because the speed of the parallel network is solely related to the depth of Para-Former. As the network depth increases, the acceleration effect becomes more pronounced."}, {"title": "Experiments", "content": "Our experimental approach is to first verify the feasibility of the parallel network theory and its adherence to the UAT approximation properties using several commonly used public image classification datasets (Section 4.1). We then explore the impact of network depth on the UAT theory (Section 4.3), followed by an investigation into the influence of the dataset (Section 4.4)."}, {"title": "Dataset", "content": "In pursuit of a comprehensive understanding of the efficacy and practicality of parallel models, we meticulously selected several widely-acknowledged and representative image classification datasets for an integrated comparative analysis. These datasets encompass a diverse array of visual content and span various levels of difficulty, thereby furnishing us with a fertile ground to probe the performance boundaries of parallel models. Specifically, our study centers on the following pivotal datasets:\n1. CIFAR-10 & CIFAR-100 (Krizhevsky 2009): These were constructed by Alex Krizhevsky et al. CIFAR-10 encompasses 60,000 32x32 pixel color images across 10 classes, while CIFAR-100 extends this to 100 categories.\n2. STL-10 (Coates, Ng, and Lee 2011): STL-10 comprises 5,000 training and 8,000 testing images distributed among 10 classes, each at a 96x96 pixel resolution.\n3. Flowers-102 (Nilsback and Zisserman 2008): Focused on floral categorization, this dataset houses 102 distinct flower species, totalling 8,189 images with a per-species count ranging from 40 to 258.\n4. Oxford-IIIT Pets (Parkhi et al. 2012): Tailored for the recognition of pet breeds (cats and dogs), this dataset contains over 7,300 images across 37 categories, characterized by varied animal poses, backgrounds, and lighting conditions."}, {"title": "The Feasibility of Para-Former", "content": "To validate the feasibility and characteristics of Para-Former, we conducted training and validation on several commonly used image classification datasets, with all models trained for 500 epochs. The experimental design involved preprocessing inputs using the common ViT techniques of patch extraction and positional encoding. We then used Transformer as the basic model architecture, setting all network depths to 1 and configuring Para-Former from single-layer to multi-layer setups, as shown in Figure 3.a. A single-layer setup corresponds to a ViT with a depth of 1.\nIt is evident from Table 2 that as the number of parallel network layers increases, the model accuracy improves progressively (Except for the result of Para-F-1-24 being worse than Para-F-1-12 on the Flower-102 dataset, this variation is within a reasonable range.). According to UAT theory, increasing the network layers is equivalent to increasing N in Equation (1), thereby enhancing the approximation capability of UAT. The results in Table 2 are in complete agreement with UAT theory. Therefore, we believe that this parallel network approach is entirely feasible.\nHowever, the overall prediction accuracy in Table 2 is quite low. What could be the reasons for this? We believe there are three main factors affecting the model's predictive performance:\n1. Model's Fitting Capability: This is influenced by the model's size and design. Specifically, the value of N in UAT and the degrees of freedom in the parameters. The larger the N value, the stronger the UAT's fitting capability. Parameter freedom refers to the number of parameters affected by the input in UAT, such as a freedom of 1 in Eq. 8 and a freedom of 6 in Eq. 9. The degrees of freedom correspond to the model's ability to dynamically fit based on input. Merely increasing N enhances the model's ability to fit specific functions, but images are often diverse. Therefore, the network must have the ability to dynamically fit the corresponding function based on the input. Higher degrees of freedom mean the model can fit a wide range of features with significant differences. Additionally, the bias term in the network can also dynamically fit based on input using UAT, which we refer to as bias freedom. Models with bias freedom have a stronger dynamic fitting capability. The mathematical model generated by the residual network is an example of UAT with bias freedom, achieving the fitting capability of a multi-layer network with just shallow layers.\n2. Data Influence: Data is another critical factor in deep learning training. Since natural images are usually diverse, limited data can lead to overfitting. For example, in a cat-dog classification task, if dogs in the training set mostly appear against backgrounds of blue skies and grass, these backgrounds might be mistakenly considered part of the dog's features, a phenomenon known as feature coupling. If cats rarely appear in such backgrounds in the training set but do in the test set, they might be misclassified as dogs. Training on a large, diverse dataset means that common backgrounds frequently appear with various objects or animals, such as cats, dogs, and elephants against blue skies, white clouds, houses, and trees. Thus, even when classifying objects, the model inherently learns background information. In this context, fine-tuning on the cat-dog dataset yields better results because the background does not interfere with dog feature recognition. Without sufficient data, even a network with strong fitting capabilities cannot perform well and is prone to overfitting."}, {"title": "Model Depth Effect", "content": "Based on the conclusions from Section 4.2, we will explore the impact of network depth on predictive performance in this section. According to Eq. 8 and Eq. 9, we know that model depth primarily affects the degrees of freedom of the model parameters in UAT. Therefore, we designed Para-Former models with 24 layers and varying depths of 2, 3, and 6, and compared their predictive performance with that of a ViT with a depth of 6.\nIt is evident that all results surpass those of Para-F-1-24 in Table 2 because the networks in Table 3 have higher degrees of freedom in their parameters. Notably, Para-F-6-6 consistently outperforms Para-F-2-24 and Para-F-3-24, indicating that the higher the degrees of freedom of the parameters, the stronger the model's performance. However, the model cannot be too deep, so we must balance network depth and layer count. Another issue is that almost all models hit a performance bottleneck. Particularly, the prediction accuracy for OxfordIIITPet and Flower-102 is very poor. This is because these datasets are relatively small and exhibit high data diversity."}, {"title": "Dataset Effect", "content": "Current deep learning models generally perform well on the aforementioned datasets. However, as shown in Table 3, the prediction accuracy of Para-Former is not very high, especially on the OxfordIIITPet and Flower-102 datasets. What is the fundamental reason for this poor performance? In this section, we will explain the relationship between deep learning and data.\nFirst, the results were obtained using the validated ViT model (Para-F-1-1 and Para-Former-6-6), which performed similarly to Para-Former. Given that the ViT model has been extensively validated and is a commonly used model architecture, the effectiveness of the models is unquestionable.\nSo, why is the performance so poor? The root cause lies in the data itself: the model's performance is generally directly proportional to the amount of available data.\nDoes this mean that deep learning models are not powerful enough or lack generalization and reasoning capabilities? On the contrary, achieving around 25% accuracy on OxfordIIITPet and about 40% on Flower-102 with such limited data actually demonstrates the strength and generalization capability of deep learning models. A common misconception about deep learning is that humans, even if unfamiliar with certain species, can accurately distinguish OxfordIIITPet and Flower-102 images by carefully examining them, thus believing that deep learning lacks generalization or human-like inferencing ability. This viewpoint is flawed. Assuming the human brain functions as a UAT model, it can automatically disregard background information before seeing these images, as this background is frequently encountered in daily life, and the UAT in the human brain has already fitted this information. Therefore, humans only need to focus on specific information and then use it to train their brains to find similarities and differences. For instance, a 10-year-old child's brain, observing the world at 60 frames per second, would have processed about 1\u00d71010 high-resolution images by the age of 10, many of which include common objects and backgrounds in daily life. Therefore, criticizing deep learning models for being unable to train on small datasets and lacking universality and generalization is unfair.\nThus, we should train on large datasets and then fine-tune on smaller ones. This is also the current deep learning training method, showcasing the powerful generalization capability of deep learning models, especially when there is some correlation between datasets. Following this fine-tuning approach, we fine-tuned the pre-trained ViT model on the aforementioned datasets for just 10 epochs. The results, as shown in Table 4, indicate a significant improvement in prediction performance.\nFigure 6 shows the rationality behind fine-tuning. Since we essentially adjust parameters based on the input to produce target results using UAT, if there is a correlation in the data, the corresponding parameters derived from the input $(a_i, W_i, and \\theta_i; i = 1, \u00b7, N)$ will be similar. Therefore, fine-tuning is feasible."}, {"title": "Conclusion", "content": "In this paper, we propose a parallel network based on the Universal Approximation Theorem (UAT) and validate its effectiveness on large datasets. We analyze the characteristics of both serial and parallel networks. Serial networks, especially those based on residual connections, often have higher degrees of parameter freedom, leading to stronger generalization capabilities. However, as the network depth increases, the inference time also becomes longer. On the other hand, parallel networks trade computational resources for time, meaning their runtime is not affected by the number of network layers. This allows for the design of multi-layer parallel networks that can replace serial networks to achieve fast inference. Additionally, we analyze the impact and reasons data have on deep learning. From the UAT perspective, training on large datasets essentially enhances the model's ability to fit data."}, {"title": "A The UAT Format of Transformer", "content": "Eq.(1) and (2) give an example of the matrix-vector format of MHA and FFN. Eq. (3) and Eq. (4) give the corresponding UAT format of $x_{i+1}$ and $x_{i+2}$. It is obvious that the bias and weight terms in the UAT corresponding to Transformed can change dynamically based on the input.\nDue to the complexity of the variables involved, we will combine some terms to align the resulting equations with the standard UAT form. To distinguish these parameter variables, we will keep the original variables unchanged. If a parameter variable is influenced only by other variables, we will denote it with a bar on top, such as $\\bar{W}$. If it is influenced by the input $x_i$, we will mark it with a hat, such as $\\hat{W}$."}]}