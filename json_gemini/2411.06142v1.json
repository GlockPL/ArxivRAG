{"title": "Aquila-plus: Prompt-Driven Visual-Language Models for Pixel-Level Remote Sensing Image Understanding", "authors": ["Kaixuan Lu"], "abstract": "The recent development of vision language models (VLMs) has led to significant advances in visual-language integration through visual instruction tuning, and they have rapidly evolved in the field of remote sensing image understanding, demonstrating their powerful capabilities. However, existing RSVLMs mainly focus on image-level or frame-level understanding, making it difficult to achieve fine-grained pixel-level visual-language alignment. Additionally, the lack of mask-based instructional data limits their further development. In this paper, we propose a mask-text instruction tuning method called Aquila-plus, which extends the capabilities of RSVLMs to achieve pixel-level visual understanding by incorporating fine-grained mask regions into language instructions. To achieve this, we first meticulously constructed a mask region-text dataset containing 100K samples, and then designed a visual-language model by injecting pixel-level representations into a large language model (LLM). Specifically, Aquila-plus uses a convolutional", "sections": [{"title": "1. Instruction", "content": "Vision language models (VLMs) are crucial building blocks for creating a general-purpose visual assistant, and they have gained increasing popularity in the research community in recent years. Although recent VLMs such as LLaVA, MiniGPT-4, Otter, Instruct-BLIP, Qwen-VL, and LLaVA-1.5 have demonstrated remarkable achievements in instruction following and visual reasoning capabilities, most of them perform visual-language alignment at the image level, using image-text pairs. The lack of region-level alignment makes it challenging for these models to handle fine-grained image understanding tasks, such as region classification, description, and reasoning.\nCompared to coarse bounding boxes, using fine-grained masks as reference inputs allows for a more precise representation of objects. By training with numerous high-quality masks, the recently developed SAM supports using simple bounding boxes or points as prompts, and it excels"}, {"title": null, "content": "in zero-shot segmentation quality for objects, parts, or subparts. Several studies, such as HQ-SAM, have further enhanced SAM's capabilities in fine-grained segmentation and generalization, making segmentation more practical in real-world applications. However, these models cannot provide primary semantic labels, let alone detailed semantic attributes and descriptions. As a result, existing methods are limited when it comes to understanding real-world scenes that contain fine-grained multimodal information.\nThis paper presents a novel method, Aquila-plus, aimed at expanding the capability of VLMs in fine-grained pixel-level understanding. To achieve this, we propose a mask-aware visual extractor to capture precise visual mask features at different granularities. These visual features are interleaved with language instructions, forming an input sequence for the LLM. To facilitate the use of high-resolution inputs, we employ convolutional CLIP as the visual encoder. Compared to ViT-based models, convolutional CLIP performs well at higher input resolutions and offers greater efficiency and robustness. With this design, Aquila-plus can achieve fine semantic understanding of object-level and part-level regions, providing primary object categories, detailed object attributes, and more complex scene descriptions.\nTo achieve fine-grained pixel-level alignment between visual and language features, we meticulously constructed a large-scale mask-based"}, {"title": null, "content": "region-text dataset called Aquila-plus-100K, in which each region's mask and text description are carefully annotated. Most of the data in this dataset come from publicly available datasets and are formatted in an instruction-following style using well-designed prompt templates, covering both object-level and part-level samples. The dataset includes not only detailed descriptions and dialogues but also enriched attribute information. Additionally, we enhanced Aquila-plus's response robustness and flexibility by introducing spatial-aware and category-aware negative sample mining as well as short-form response instructions.\nThrough visual instruction tuning, our proposed model achieves new capabilities that go beyond frame-level and image-level understanding. As shown in Figure 1-(b), Aquila-plus can generate fine-grained semantics based on class-agnostic masks from the existing SAM. Extensive experimental results demonstrate that our approach outperforms others in open-vocabulary recognition, object classification, description and reasoning, as well as object hallucination tasks. The main contributions of this paper are summarized as follows:\n\u2022 We propose a novel method, Aquila-plus, which enables multimodal large language models (VLMs) to achieve pixel-level instruction tuning, thereby facilitating fine-grained and open-world visual understanding.\n\u2022 We constructed a large-scale instruction tuning dataset for the remote"}, {"title": null, "content": "sensing domain, named Aquila-plus-100K, containing object-level and part-level mask-text pairs to enhance the model's robustness and flexibility.\n\u2022 Our method, as a fine visual understanding approach, outperforms the current state-of-the-art methods across various region understanding tasks."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1 Vision Language Models", "content": "Large language models (LLMs), such as GPT-3, Flan-T5, PaLM, and LLaMA, have achieved significant progress in natural language processing (NLP) research. These advancements have driven the development of multimodal language models, enabling applications like ChatGPT by expanding training data and increasing model scale. The success of LLMs and VLMs has also inspired research in computer vision, making multimodal context learning possible. Recent research has increasingly focused on leveraging pretrained LLMs for visual instruction tuning. Models such as LLaVA, MiniGPT-4, mPLUG-Owl, Otter, Instruct-BLIP, Qwen-VL, and LLaVA-1.5 have demonstrated impressive capabilities in instruction following and visual reasoning tasks. However, these models are limited in multimodal tasks at the image level and struggle when it comes to referring to specific regions for finer"}, {"title": null, "content": "understanding."}, {"title": "2.2 Region-Level Image Understanding", "content": "In the context of region-level image understanding, the process begins with locating regions of potential interest, followed by deeper visual comprehension. The Segment Anything Model (SAM), through training on numerous high-quality masks, has shown outstanding performance in zero-shot object, part, and sub-part segmentation tasks. Since the original SAM does not provide semantic labels, several methods, such as SEEM, HIPIE, and Semantic SAM, have extended the model to predict semantic categories for the masks. However, basic semantic labels are often insufficient for real-world applications. Thus, there is a need to introduce additional semantic information, such as color, position, or even comprehensive descriptions for scene understanding and reasoning.\nAlthough some works have achieved pixel-level alignment, they fail to provide region-based descriptions.\nRecent studies, such as GPT4RoI, PVIT, Kosmos-2, Shikra, Ferret, and GLaMM, have enabled MLLMs to achieve region-level image understanding. However, most of these approaches use bounding boxes as reference input regions, which may introduce irrelevant background features, resulting in less precise region-text alignment during visual"}, {"title": null, "content": "instruction tuning. Furthermore, these models only allow for smaller input image sizes, such as 224\u00d7224, which makes it difficult to analyze details in densely populated object regions. To address these issues, this paper introduces a pixel-level understanding method based on LLMs. Our method supports using input masks for region references and allows for higher image resolutions. Additionally, we constructed a dataset containing mask-text pairs to facilitate instruction-based learning."}, {"title": "3. Aquila-plus-100K Dataset", "content": "In this section, we introduce Aquila-plus-100K, an instruction-based remote sensing dataset containing mask-text pairs, consisting of approximately 100K multimodal dialogues aimed at encouraging VLMs to perform pixel-level image understanding. Specifically, Aquila-plus-100K comprises mask-text instruction data at both object-level and part-level, created based on publicly available datasets. To ensure that the data meets the requirements for instruction-following tasks, we utilized GPT-4 with carefully designed prompt templates to generate high-quality mask-text pairs. Furthermore, to enhance the robustness and flexibility of responses, we introduced negative sample mining techniques and short-form response prompts. Figure 1 illustrates an example from the Aquila-plus-100K dataset."}, {"title": "4. Aquila-plus Method", "content": ""}, {"title": "4.1 Model Architecture", "content": "An overview of the Aquila-plus architecture is illustrated in Figure 2. Aquila-plus consists of an image-level visual encoder, a pixel-level mask-aware visual extractor, and a large language model (LLM). Given an image, a reference mask region, and an input language prompt, we perform tokenization and transformation to obtain embedded representations. The interleaved sequence of mask features and language embeddings is then fed into the LLM to achieve fine semantic understanding."}, {"title": "4.2 Convolutional CLIP Visual Encoder", "content": "Most visual encoders in VLMs utilize ViT-based CLIP models, such as those used in LLaVA. However, lower input resolutions make it challenging to achieve fine-grained image understanding for pixel-level representations, especially in small regions. To address this issue, we introduce convolutional CLIP models, such as ResNet and ConvNeXt, as the visual encoder. The CNN-based convolutional CLIP model performs well across various input resolutions and offers higher generalization ability compared to ViT-based CLIP models. Moreover, the multi-scale feature maps generated by CNN can be directly used for subsequent feature extraction of each object region. In implementation, we choose the ConvNeXt-Large CLIP model as the visual encoder and use the output from the last stage as the image-level feature."}, {"title": "4.3 Mask Spatial Feature Extractor", "content": "Unlike previous region-level approaches that used sparse bounding boxes as reference inputs (e.g., PVIT, Shikra), Aquila-plus utilizes fine-grained mask region representations. To capture pixel-level features of each object region, we propose a mask-aware visual extractor that not only encodes mask-level visual features but also gathers spatial positional information for each region.\nSpecifically, we first apply a mask pooling operation to the multi-layer image features produced by the visual encoder, pooling all the features that fall within the mask region. We then pass each layer of features through a linear projection layer to generate region-level embeddings of the same dimension, and we fuse multi-layer features by summation. To adapt and produce visual mask tokens, we further use a multi-layer perceptron (MLP) layer for processing.\nTo retain the spatial geometric relationship of object regions, we encode pixel-level spatial relations using the binary mask of each object region. First, we resize the mask to a size of 224\u00d7224, then flatten and project it to generate spatial tokens. Finally, we combine the visual mask tokens with their corresponding spatial tokens to form the embedding representation for each mask region."}, {"title": "4.4 Tokenization", "content": "As shown in Figure 2, we input the image into the pretrained visual"}, {"title": null, "content": "encoder, ConvNeXt-Large CLIP, to extract image-level embedding representations. For textual information, we tokenize the text sequence and project it to obtain text embeddings. For mask-based regions, we define a special token as a placeholder <region> and replace it with mask tokens and spatial tokens, such as <mask> and <position>. When referring to an object region in the text input, <region> is appended after the region name, such as \"region1\" or \"region2.\" In this way, the mask regions blend seamlessly with the text to form complete sentences, which are subsequently tokenized together.\nIn addition to the user's instruction, we introduce a prefix prompt: \"<image>\\n An overview of the given image is provided.\" Here, <image> is a special placeholder token that is replaced by the image-level embeddings from the visual encoder. All image-level and region-level visual tokens are interleaved with text tokens and input into the LLM for understanding the image and responding to the user's instructions regarding different object regions. We use the Vicuna model as the LLM, which is an instruction-tuned decoder version based on LLaMA."}, {"title": "4.5 Training", "content": "The training process for the Aquila-plus model consists of three stages, with supervision in all stages achieved by minimizing the next token prediction loss."}, {"title": null, "content": "Stage 1: Image-Text Alignment Pretraining\nUsing the ConvNeXt-Large convolutional CLIP visual encoder, we begin by training image-level features and a language connector to achieve image-text feature alignment. In this stage, Aquila-plus consists of a pretrained visual encoder, a pretrained LLM, an image-level projector and a region-level projector. We employ a multi-layer perceptron (MLP) as the visual-language connector to enhance the model's multimodal capabilities. During this stage, only the image-level projector and the region-level are trained, while the visual encoder and LLM remain frozen.\nStage 2: End-to-End Fine-Tuning\nIn this stage, we fix the weights of the visual encoder and fine-tune the image-level projector, region-level projector, and LLM of Aquila-plus. The focus is on expanding Aquila-plus's capabilities to accurately follow user instructions and handle complex pixel-level region understanding tasks. During this stage, we use the constructed Aquila-plus-100K dataset."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1 Experimental Results", "content": "To evaluate the effectiveness of our proposed model, Aquila-plus, we"}, {"title": null, "content": "conducted multiple experiments to demonstrate its capabilities in tasks such as pixel-level region recognition, classification, and complex description and reasoning, as shown in the Figure 3."}, {"title": "6. Conclusion", "content": "This paper presents a novel method, Aquila-plus, that introduces pixel-level mask region referencing into language instructions, significantly enhancing the fine-grained visual understanding capabilities of VLMs. By incorporating a mask-aware visual extractor and employing a convolutional CLIP model as the visual encoder, Aquila-plus achieves region-based image understanding capabilities. To achieve fine-grained pixel-level alignment between vision and language, we meticulously constructed a dataset named Aquila-plus-100K, which contains 724K mask-text pairs. Aquila-plus demonstrates superior performance across various region understanding tasks, achieving new state-of-the-art results. We hope that the Aquila-plus-100K dataset and the Aquila-plus model will advance visual region understanding in practical applications of VLMs."}]}