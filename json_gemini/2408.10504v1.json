{"title": "QPO: Query-dependent Prompt Optimization via\nMulti-Loop Offline Reinforcement Learning", "authors": ["Yilun Kong", "Hangyu Mao", "Qi Zhao", "Bin Zhang", "Jingqing Ruan", "Li Shen", "Yongzhe Chang", "Xueqian Wang", "Rui Zhao", "Dacheng Tao"], "abstract": "Prompt engineering has demonstrated remarkable success in enhancing the performance of\nlarge language models (LLMs) across diverse tasks. However, most existing prompt op-\ntimization methods only focus on the task-level performance, overlooking the importance\nof query-preferred prompts, which leads to suboptimal performances. Additionally, these\nmethods rely heavily on frequent interactions with LLMs to obtain feedback for guiding the\noptimization process, incurring substantial redundant interaction costs. In this paper, we in-\ntroduce Query-dependent Prompt Optimization (QPO), which leverages multi-loop offline\nreinforcement learning to iteratively fine-tune a small pretrained language model to generate\noptimal prompts tailored to the input queries, thus significantly improving the prompting ef-\nfect on the large target LLM. We derive insights from offline prompting demonstration data,\nwhich already exists in large quantities as a by-product of benchmarking diverse prompts on\nopen-sourced tasks, thereby circumventing the expenses of online interactions. Furthermore,\nwe continuously augment the offline dataset with the generated prompts in each loop, as\nthe prompts from the fine-tuned model are supposed to outperform the source prompts in\nthe original dataset. These iterative loops bootstrap the model towards generating optimal\nprompts. Experiments on various LLM scales and diverse NLP and math tasks demonstrate\nthe efficacy and cost-efficiency of our method in both zero-shot and few-shot scenarios.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have exhibited impressive prowess in various domains of natural language\nprocessing (NLP) (Ouyang et al., 2022; Touvron et al., 2023; Achiam et al., 2023). Prompt engineering, a\nmethod that simply adds an instruction to the input query, emerges as a lightweight and promising solution\nfor adapting LLMs to downstream tasks without the need for parameter tuning (Liu et al., 2023a; Ajith\net al., 2023). Since the performance of LLMs towards a particular task is significantly influenced by the\nquality of the prompt, the key challenge of prompting lies in how to design the optimal prompts.\nNumerous prompt engineering algorithms have been proposed in recent years. Some algorithms (Zhou et al.,\n2022; Wang et al., 2023b; Guo et al., 2023; Wang et al., 2024b) leverage LLMs as a prompt optimizer, em-\nploying black-box optimization to derive the best prompts. Others utilize reinforcement learning (RL) (Deng\net al., 2022; Zhang et al., 2022; Kong et al., 2024) to train a policy model to generate the optimal prompts.\nDespite these advances, prompt engineering still grapples with great challenges:\n(1) Most of previous algorithms only focus on obtaining task-level optimal prompts, aiming to attain an\noptimal average performance across all queries within a specific task. However, they overlook the\ncritical insight that no single prompt can be ideal for every possible query (Sun et al., 2023)."}, {"title": "2 Methodology", "content": "To automatically generate query-level optimal prompts in a cost-efficiency method, we propose QPO, a\nnovel query-dependent prompt optimization framework. The overall process is shown in Figure 2. In the\nfirst round, QPO harnesses the readily existing datasets to fine-tune a pre-trained language model into a\npolicy model through offline reinforcement learning, enabling it to generate query-specific optimal prompts\nbased on input queries. Subsequently, this policy model efficiently explores a broader query space and\nenriches the prompt space with more diverse and high-quality prompts, both of which contribute to the\naugmentation of the dataset. The augmented dataset, in turn, further refines the policy model for the\nnext loop. Consequently, through multi-loop training and data augmentation, the performance of prompt\noptimization is bootstrapped for improvement."}, {"title": "2.1 Problem Formulation", "content": "This study focuses on training the policy model to generate prompts, which can instruct a target LLM to\noutput expected answers that fulfill the query-dependent objective, based on the given queries.\nQuery and Answer. We consider the task of answering queries $q \\in Q = V^{\\infty}$ expressed in a natural\nlanguage, where V denotes the vocabulary. Each query q is supposed to have an expected answer $y^* \\in Y$\nas ground truth. Queries annotated with ground-truth answers can be employed as demonstrations d for\nfew-shot scenario.\nPrompt and Policy Model. The performance of an LLM can be significantly enhanced through appro-\npriate prompts. The prompt $p \\in P = V^{\\infty}$ is a natural language instruction that explains to the LLM how\nto complete the query and output the predicted answer. In this paper, we utilize a fine-tuned policy model\n$\\pi : Q \\rightarrow P$ to generate query-specific prompts based on the given queries: $p = \\pi(q)$.\nTarget LLM. These tasks are performed by employing a target LLM $l : Q \\rightarrow Y$, feeding queries q into\nthe LLM to get answers. With prompts, the answers can be obtained by $\\hat{y}^{(i,j,k)} = l(p^{(i)}, d^{(k)}, q^{(i)})$, where\n$q^{(i)}, p^{(j)}, d^{(k)}$ denote the i-th query, j-th prompt, and k-th combination of demonstrations, respectively. For\nzero-shot setting, d is null, so we use $\\hat{y}^{(i,j)}$ as a shorthand unless otherwise specified.\nQuery-dependent Objective. Given a task with queries and ground-truth answers, the quality of the\npredicted answers can be evaluated by a metric $\\rho(y^*, \\hat{y})$, such as correctness. The objective of query-\ndependent prompt optimization can be formulated as below:\n$\\pi^* = \\arg \\max_{\\pi} \\rho(y^{*(i)}, l(\\pi(q^{(i)}), q^{(i)}))_{i\\in[N]}$,\n(1)\nwhich aims to optimize the policy model to generate a query-specific prompt that enhance the target LLM's\nperformance on this particular query. As its performance improves across each single query, the overall task\nperformance will consequently improve."}, {"title": "2.2 Model Query-dependent Prompt Optimization as Offline RL", "content": "Reinforcement Learning Formulation. As a text generation problem, prompt generation can be formu-\nlated as a Markov Decision Process (MDP) (S, A, r, f) with a finite state space S, action space A, reward\nfunction r, and state-transition probability function f, where a sparse reward can be obtained only after the\ncomplete prompt is generated. Instead of dedicatedly designing intricate proxy rewards, we formulate the\nMDP as a single-step decision-making process. In offline RL, given the dataset:\n$D = \\{q^{(i)}, p^{(i)}, r^{(i,j)} = R(q^{(i)}, p^{(i)})\\}_{i\\in[N],j\\in[M]}$,\nwhere the initial state $q \\in S$ is the input user query with n tokens $q = (q_1, ..., q_n)$ and the corresponding\naction $p \\in A$ represents the generated prompt with m tokens $p = (p_1, \u2026, p_m)$, where each token q,p is from\nthe vocabulary V. In a single-step decision episode, the policy model generates a complete prompt based on\nthe given expected reward and query $p \\sim \\pi(p|r, q)$. The reward guides the quality of the generated prompt.\nFor further discussion on why prompt generation is formulated as a single-step decision process, please refer\nto Appendix A.2.\nReward Design. In our single-step prompt generation process, the reward plays a crucial role in aligning\nwith the true capabilities of the prompt. In this paper, we mainly focus on neural language understanding\ntasks and math reasoning tasks rather than generative tasks, which are challenging to assess objectively\nbecause of the poor correlation between the evaluation metrics and human preferences (Liang et al., 2022;\nGoyal et al., 2022). We design the reward based on two dimensions: query-level and task-level. Query-level\nreward measures whether the prompt can instruct LLM to answer the specific question correctly, while task-\nlevel reward measures the prompt's average prompting ability for all queries in one task. In particular, we\nadopt the average accuracy of the prompt for all test questions as the task-level reward for both zero-shot\nand few-shot settings,\n$R_{task}(q^{(i)}, p^{(i)}) = \\frac{1}{N} \\Sigma_{i=1}^N\\mathbb{1}\\{\\hat{y}^{(i,j)} = y^{*(i)}\\}$.\n(2)"}, {"title": "2.3 QPO Framework for Multi-Loop Augmentation", "content": "2.3.1 Initial Demonstration Construction\nData Availability. We start by emphasizing the presence and significance of prompt demonstrations\ngenerated by previous researches of prompt optimization. In the realm of prompt engineering, abundant"}, {"title": "3 Experiments", "content": "3.1 Experimental Setup\nTasks. We perform experiments on 6 language understanding tasks and 2 math reasoning tasks to validate\nour methods, including topic classification (AG's News (Zhang et al., 2015)), natural language inference\n(BoolQ (Clark et al., 2019)), sentiment classification (IMDB (Maas et al., 2011), TweetEval Emotion (Mo-\nhammad et al., 2018)), multi-choice QA (CosmosQA (Huang et al., 2019), HellaSwag (Zellers et al., 2019)),\nand math reasoning (GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021)). These tasks are widely\nstudied in prompting settings, and hence many expert-crafted and machine-generated prompts are available,\nwhich facilitates our offline data collection procedure.\nBaselines. We compare our method with three types of baselines, including manual prompt engineering\n(PromptSource (Bach et al., 2022), Chain-of-Thought (Kojima et al., 2022)), online prompt optimization\n(Low Perplexity (Gonen et al., 2022), RLPrompt (Deng et al., 2022), APE (Zhou et al., 2022)) and offline\nprompting approach (Prompt-OIRL (Sun et al., 2023)). We also compare the prompts rewritten by Chat-\nGPT. We reproduce the online methods based on InstructEval (Ajith et al., 2023). Rather than making a\ndirect while unfair comparison between the performance of QPO and newer online algorithms, our primary\nfocus is to demonstrate that QPO can further enhance performance by optimizing prompts at query-level\nbased on the prompts obtained through online optimization. Notably, for fair comparison, we use a larger\ndataset in Prompt-OIRL than ours after final loop data augmentation to eliminate our advances in additional\ninteractions.\nLLMs. Our method is model-agnostic for policy model, and we use GPT-2 (Radford et al., 2019) in\nthis paper, which is compact while has sufficient text generation capability. For the target LLMs, we use\npublicly available Llama2-7b-chat (Touvron et al., 2023) for natural language understanding tasks, while for\nthe more challenging math reasoning tasks, we opt for GPT-3.5-turbo (Ouyang et al., 2022). Moreover, to\nevaluate the cross-model generalization ability of our trained policy model, we employ models at different\nabilities, scaling from the GPTNeo-1.3b (Black et al., 2021) to Llama2-13b-chat (Touvron et al., 2023) and\nVicuna-13b (Zheng et al., 2024).\nImplementation Details. For the initial data collection, we utilize 30 expert prompts obtained from\nInstructEval and 120 prompts rewritten from ChatGPT-3.5 to construct the dataset. We set the QPO with\n3 loops. For all the tasks, we use a unified hyperparameter set and do not need complex hyperparameter\ndesign for specific task. We do not cherry-pick checkpoints and directly use the final checkpoint in each\nloop for evaluation and next loop's training. For NLU tasks, we evaluate our method on both zero-shot and\nfew-shot settings, while for math reasoning tasks for GPT-3.5, we only test its zero-shot performance due"}, {"title": "3.2 Main Results", "content": "Natural Understanding tasks. Table 1 presents a comprehensive comparison of query-dependent opti-\nmized prompts generated by QPO against human prompts, query-agnostic prompt optimization methods,\nand SOTA offline query-dependent prompt selection method on Llama2-7b-chat on zero-shot and 6-shot set-\ntings. The results show that: (1) QPO delivers significantly better results on zero-shot metric (avg. +7.2%\nover other baselines) and also outperforms other baselines on few-shot setting (avg. +3.3%), where the con-\ntextual demonstrations slightly diminish the critical role of prompts, as evidenced by the standard deviation\nin few-shot scenarios being significantly smaller than in zero-shot scenarios. The 3-shot results in Table 11\nfurther illustrate that our algorithm's advantage becomes more pronounced as the information provided by\ncontextual demonstrations decreases (avg. +6.1%). (2) Based on the dataset constructed from these online\nalgorithms, both QPO and Prompt-OIRL exhibit better performance to those optimized at the task level on\nall three settings (avg. +7.2%, +2.3%, +4.9%, respectively), which underscores the effectiveness of optimiz-"}, {"title": "3.3 Analysis", "content": "Multi-loop augmentation can bootstrap to\nimprove performance in a interaction-cost-\nefficient way. Multi-Loop Augmentation (MLA) in\nour method is supposed to improve the exploration of\nthe query space and prompt space efficiently, so that\nthe augmented dataset can enhance the policy model\nin turn. The average number of queries in datasets\nincreases from 283 in original dataset to 673 in the\nfinal augmented dataset, and the average number of\nprompts rises from 150 to 829, which obtains 138%\nand 453% increases in question coverage and prompt\ndiversity with only a 17.1% increase in total data vol-\nume. We limit the amount of data augmented in each\nloop, as research (Wang et al., 2024c) indicates that\ngenerated data comprising 10% of the real data pro-\nvides the maximum training benefit, whereas exces-\nsive generated data can be counterproductive. To further quantify the effect of MLA, we conduct ablation\nexperiments that finetune the model the same iterations without data augmentation. As shown in Figure 3,\nin 3 NLU tasks (AG News, BoolQ, IMDB) and 2 math reasoning (MR) tasks (GSM8K and SVAMP), MLA\ncan consistently improve the performance in all settings, while training with more epochs without\ndata augmentation results in significant drops due to overfitting. To be specific, multi-loop augmentation\nand training lead to a performance improvement of 6.7% and 2.3% for Llama2 in zero-shot and few-shot set-\ntings in NLU tasks, respectively, while in the zero-shot math reasoning tasks for GPT-3.5, the performance\nimproved by 3.5%.\nDespite the introduction of online LLM interactions in MLA, our\nmethod: (1) only engages new interactions to supplement the\ndataset after the training convergence in each loop, rather than\nrequiring interactions at each step to provide real-time guidance\nfor optimization direction; (2) requires significantly fewer inter-\nactions compared to other online methods. Therefore, QPO is\nconsidered an offline approach. To demonstrate this, we com-\npared our method with the classic online algorithm APE, which\nis a paradigm for many online prompt optimization methods. Table 3 presents the LLM inference time\ncosts, which is operated locally on NVIDIA V100 GPU. To be specific, for 4 loops of training, 3 data aug-\nmentation processes are employed, resulting in each MLA requiring approximately 0.097h of computational\nresources. Given an average performance improvement of around 2% per loop, this expense\nfor interaction is relatively justified and valuable. Thus, employing QPO for prompt optimization is\nsubstantially more cost-efficient than methods reliant on LLMs as critics. We analyse the specific number of\nrequired interactions for these methods in Appendix C.2.\nReward Matters. To validate the effects of RL and our proposed reward prediction loss, we conduct abla-\ntion experiments comparing SFT, RL, and QPO (RL+Reward Loss). Table 4 presents that RL+Reward\nLoss consistently surpasses others in all evaluation tasks, which confirms the effects of the additional"}, {"title": "4 Related Works", "content": "Prompt Optimization. Automatic prompt optimization has become a pivotal issue in domain of LLMs.\nRecently, there has been a growing interest in this area. Soft prompts (Li & Liang, 2021; Zhang et al., 2021;"}, {"title": "5 Conclusion and Future Work", "content": "We propose QPO, a novel query-dependent prompt optimization method through a multi-loop offline RL\nframework, which is cost-efficient. This method leverages offline datasets from existing evaluations and\nemploys offline RL to fine-tune a samll-scale language model as the policy model, generating query-level\noptimal prompts. Experimental results demonstrate the superiority and cross-model generalization ability\nof QPO, significantly enhancing the value of our method. Besides, we conduct extensive ablation studies\nto analyse our method's insights and rationale. While our method has substantially reduced the required\nonline interactions, we can entirely eliminate these interactions if necessary. For instance, in certain business\nscenarios where only a fixed amount of reward-labeled data is available and the reward labeling rules are\nunknown, it is impossible to use LLMs to obtain new labeled data as our exploration. We can employ inverse\nreinforcement learning to derive a reward model from the original dataset to score the new query-prompt\npairs obtained through the multiple-loop data augmentation. This approach completely eliminates the need\nfor online interactions with LLMs, which we consider for future work. Furthermore, in the domains of\ntext-to-image and text-to-video generation, where LLMs have slower inference speeds, interactions are more\nexpensive, and user queries are more unique, our offline query-dependent prompt optimization methods will\nshow greater potential and advantages."}, {"title": "A Extended Discussion on QPO", "content": "A.1 Advantages of offline RL compared to SFT\nSFT can be regarded as a form of imitation learning (Zheng et al., 2022), similar to behavior cloning (Torabi\net al., 2018), where it mimics expert actions (i.e., the query-specific optimal prompts) based on given states\n(i.e., input questions). While offline reinforcement learning introduces reward values, guiding the generation\nof prompts based on the given queries and associated rewards. In prompt generation task, directly employing\nSFT results in low data utilization because it is unknown in advance that which prompt is optimal for\nspecific problems. This necessitates massive prior evaluation of all prompts before selecting the best one\nas the expert behavior to construct the dataset, leading to significant resource wastage. Specifically, in the\nfinal SFT dataset, each query should ideally have only one prompt label. All M prompts must be evaluated\nbut only one optimal prompt is retained as the label for SFT ultimately, wasting the remaining (M \u2013 1)/M\nevaluation data. While for offline RL, the prediction of prompt is based on both reward and query, leading\nto a much more diversity in input. Since different prompts have various rewards, most of them can be\nutilized for training. Offline RL offers significant advantages through the introduction of the rewards, which\nallows each query to be associated with multiple prompts and their corresponding rewards in the dataset.\nOn one hand, the rewards improves the utilization of collected suboptimal prompts for training, significantly\nboosting exploration of the prompt space while avoiding resource wastage caused by prompt selection. On\nthe other hand, rewards in RL enable the policy model to distinguish the prompting abilities of different\nprompts for various queries at the token level, leading to the generation of novel prompts for unknown\nqueries.\nA.2 Prompt Generation as Single-step Decision Process\nIn the reinforcement learning setup, prompt generation is a sparse reward task. The complete prompt is\nevaluated with the target LLM to obtain a reward value only after it is fully generated. Given the availability\nof substantial amounts of such test data, offline reinforcement learning is an effective approach for prompt\noptimization. For offline RL, there are two methods for collecting token-wise rewards which can be used for\nmulti-step decision-making processes:\n(1) Manually designing token-level rewards, which not only requires extensive prior expert knowledge but\nalso introduces additional errors, making it unreliable.\n(2) Iteratively deleting the prompt's tokens from end to beginning and re-evaluating to obtain rewards\nfrom $s_{m-1} = (q, P_1, \u2026, P_{m-1})$ to $s_m = (q, P_1, \u2026, P_m)$, where $p_m$ denotes the m-th token in the complete\nprompt. However, this approach significantly increases the cost of collecting datasets and is thus\nimpractical.\nTherefore, we do not introduce additional proxy rewards and directly utilize the sparse rewards. At this point,\ninstead of modeling the prompt generation task as a multi-step decision-making process {r, s_0, P_1, r, s_1, P_2, ...},\nwhere each r is the same sparse reward and additional methods are required to encode long texts\n(q_1, ..., q_n, p_1, ...,p_i) into the single token state $s_i$, we simplify the task modeling based on the principle\nof Occam's Razor. Thus, the prompt generation is formulated into a single-step decision process (r, q, p).\nNotably, the single-step decision-making process refers to the reward and state, taking only one action (i.e.,\ngenerating the prompt), while the inner prompt generation process remains an autoregressive prediction\n$P \\sim \\Pi_{\\epsilon\\pi}(P_t|r, q, P_{<t})$.\nA.3 The Architecture of Policy Model\nCompared to the standard GPT-2 model with 124M parameters, we make three modifications to the model\narchitecture: a reward embedding layer, a reward prediction layer, and a timestep embedding layer. Both\nthe reward embedding layer and reward prediction layer are single-layer MLPs at the first token of the policy\nmodel. The reward is encoder into a reward encoding through the reward embedding layer, then input as the\nfirst token into the transformer to obtain the output at the first token position, which is then passed through\nthe reward prediction layer to generate the predicted reward. This process can be viewed as an autoencoder"}, {"title": "B Supplemental Experiment Detials", "content": "B.1 Hyperparameters and Details\nFor the task datasets with default testing or development set, we use their original split to obtain our testing\nset. If there is no official training/development/testing split, we randomly sample a reasonably large set for\nstable evaluating and testing. Additionally, for all tasks, we split 10% training samples as collection set for\ninitial query collection and subsequent query augmentation, ensuring that the collected queries for training\nthe policy model do not appear in the in-context examples during few-shot evaluation, and simultaneously\nsimulating a scenario where very few questions are available.\nThe parameters for the experiments are shown in Table 9. Notably, QPO consists of 4 loops, indicating there\nare 4 times of offline reinforcement learning fine-tuning stage and 3 times of data augmentation between\neach fine-tuning stage. Since we introduce additional networks into the policy model, the number of training\nepochs and the learning rate required for the first loop of training are relatively high.\nB.2 Offline Dataset Collection\nFor the initial dataset collection, testing 150 prompts on too many queries incurs excessive computational\ncosts. Therefore, we test 100 queries for every group of 40 prompts (with the last group containing 30\nprompts). Each group sampled different queries, achieving a balance between query coverage and compu-\ntational cost. Since queries are randomly sampled for each test group, there are overlapped queries across\ndifferent groups. In the end, our dataset consists of 15,000 paired examples, comprising 150 prompts and ap-\nproximately 350 queries. This dataset size is significantly lower than that required by other offline algorithms,\nsuch as Prompt-OIRL for averages 56,900 examples and Promptist for averages 360,000 examples.\nFor the specific prompts, we utilize the 30 prompts from InstructEval (Ajith et al., 2023) and leverage\nChatGPT-3.5 chat box to rewrite 120 new prompts based on these 30 prompts through in-context learning.\nSpecifically, we provide 5 existing prompts to ChatGPT-3.5, and ask it to generate other 30 effective prompts\nfor 4 times. The prompt we use for rewriting is demonstrated in Table 10.\nB.3 Codes and Hardware\nOur code, as well as the offline datasets, will be released as open-accessible. All experiments are conducted\non a single NVIDIA V100 32g GPU."}, {"title": "C Additional Results", "content": "C.1 Main Results on 3-shot Evaluation\nC.2 Cost Analysis\nIn this section, we analyse the specific number of interactions required by different algorithms. APE requires\na total of M*|D|*T interactions with LLMs to obtain the feedback for optimization, while QPO only requires\n|C|*T' interactions for data augmentation, where M is the candidate population under evaluation, |D| is the\nsize of development set, |C| is the size of collection set, and T and T' are the number of iterations needed.\nThough C is slightly larger than |D| for better exploration, QPO significantly reduces computational\noverhead due to its minimal iteration T' and the absence of the need for evaluate M candidate prompts.\nC.3 Results on Different Prompt Quantity\nC.4 Results on Different Prompt Quality\nC.5 Case Study"}]}