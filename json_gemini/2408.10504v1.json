{"title": "QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning", "authors": ["Yilun Kong", "Hangyu Mao", "Qi Zhao", "Bin Zhang", "Jingqing Ruan", "Li Shen", "Yongzhe Chang", "Xueqian Wang", "Rui Zhao", "Dacheng Tao"], "abstract": "Prompt engineering has demonstrated remarkable success in enhancing the performance of large language models (LLMs) across diverse tasks. However, most existing prompt optimization methods only focus on the task-level performance, overlooking the importance of query-preferred prompts, which leads to suboptimal performances. Additionally, these methods rely heavily on frequent interactions with LLMs to obtain feedback for guiding the optimization process, incurring substantial redundant interaction costs. In this paper, we introduce Query-dependent Prompt Optimization (QPO), which leverages multi-loop offline reinforcement learning to iteratively fine-tune a small pretrained language model to generate optimal prompts tailored to the input queries, thus significantly improving the prompting effect on the large target LLM. We derive insights from offline prompting demonstration data, which already exists in large quantities as a by-product of benchmarking diverse prompts on open-sourced tasks, thereby circumventing the expenses of online interactions. Furthermore, we continuously augment the offline dataset with the generated prompts in each loop, as the prompts from the fine-tuned model are supposed to outperform the source prompts in the original dataset. These iterative loops bootstrap the model towards generating optimal prompts. Experiments on various LLM scales and diverse NLP and math tasks demonstrate the efficacy and cost-efficiency of our method in both zero-shot and few-shot scenarios.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have exhibited impressive prowess in various domains of natural language processing (NLP) (Ouyang et al., 2022; Touvron et al., 2023; Achiam et al., 2023). Prompt engineering, a method that simply adds an instruction to the input query, emerges as a lightweight and promising solution for adapting LLMs to downstream tasks without the need for parameter tuning (Liu et al., 2023a; Ajith et al., 2023). Since the performance of LLMs towards a particular task is significantly influenced by the quality of the prompt, the key challenge of prompting lies in how to design the optimal prompts.\nNumerous prompt engineering algorithms have been proposed in recent years. Some algorithms (Zhou et al., 2022; Wang et al., 2023b; Guo et al., 2023; Wang et al., 2024b) leverage LLMs as a prompt optimizer, employing black-box optimization to derive the best prompts. Others utilize reinforcement learning (RL) (Deng et al., 2022; Zhang et al., 2022; Kong et al., 2024) to train a policy model to generate the optimal prompts. Despite these advances, prompt engineering still grapples with great challenges:\n(1) Most of previous algorithms only focus on obtaining task-level optimal prompts, aiming to attain an optimal average performance across all queries within a specific task. However, they overlook the critical insight that no single prompt can be ideal for every possible query (Sun et al., 2023)."}, {"title": "2 Methodology", "content": "To automatically generate query-level optimal prompts in a cost-efficiency method, we propose QPO, a novel query-dependent prompt optimization framework. The overall process is shown in Figure 2. In the first round, QPO harnesses the readily existing datasets to fine-tune a pre-trained language model into a policy model through offline reinforcement learning, enabling it to generate query-specific optimal prompts based on input queries. Subsequently, this policy model efficiently explores a broader query space and enriches the prompt space with more diverse and high-quality prompts, both of which contribute to the augmentation of the dataset. The augmented dataset, in turn, further refines the policy model for the next loop. Consequently, through multi-loop training and data augmentation, the performance of prompt optimization is bootstrapped for improvement."}, {"title": "2.1 Problem Formulation", "content": "This study focuses on training the policy model to generate prompts, which can instruct a target LLM to output expected answers that fulfill the query-dependent objective, based on the given queries.\nQuery and Answer. We consider the task of answering queries $q \\in Q = V^{\\infty}$ expressed in a natural language, where V denotes the vocabulary. Each query q is supposed to have an expected answer $y^* \\in Y$ as ground truth. Queries annotated with ground-truth answers can be employed as demonstrations d for few-shot scenario.\nPrompt and Policy Model. The performance of an LLM can be significantly enhanced through appropriate prompts. The prompt $p \\in P = V^{\\infty}$ is a natural language instruction that explains to the LLM how to complete the query and output the predicted answer. In this paper, we utilize a fine-tuned policy model $\\pi : Q \\rightarrow P$ to generate query-specific prompts based on the given queries: $p = \\pi(q)$.\nTarget LLM. These tasks are performed by employing a target LLM $l : Q \\rightarrow Y$, feeding queries q into the LLM to get answers. With prompts, the answers can be obtained by $\\hat{y}^{(i,j,k)} = l(p^{(i)}, d^{(k)}, q^{(i)})$, where $q^{(i)}, p^{(j)}, d^{(k)}$ denote the i-th query, j-th prompt, and k-th combination of demonstrations, respectively. For zero-shot setting, d is null, so we use $\\hat{y}^{(i,j)}$ as a shorthand unless otherwise specified.\nQuery-dependent Objective. Given a task with queries and ground-truth answers, the quality of the predicted answers can be evaluated by a metric $\\rho(y^*, \\hat{y})$, such as correctness. The objective of query-dependent prompt optimization can be formulated as below:\n$\\pi^* = \\arg \\max_{\\pi} \\rho(y^{*(i)}, l(\\pi(q^{(i)}), q^{(i)}))_{i\\in[N]}$,\nwhich aims to optimize the policy model to generate a query-specific prompt that enhance the target LLM's performance on this particular query. As its performance improves across each single query, the overall task performance will consequently improve."}, {"title": "2.2 Model Query-dependent Prompt Optimization as Offline RL", "content": "Reinforcement Learning Formulation. As a text generation problem, prompt generation can be formu-lated as a Markov Decision Process (MDP) (S, A, r, f) with a finite state space S, action space A, reward function r, and state-transition probability function f, where a sparse reward can be obtained only after the complete prompt is generated. Instead of dedicatedly designing intricate proxy rewards, we formulate the MDP as a single-step decision-making process. In offline RL, given the dataset:\n$D = \\{q^{(i)}, p^{(i)}, r^{(i,j)} = R(q^{(i)}, p^{(i)})\\}_{i\\in[N],j\\in[M]}$,\nwhere the initial state $q \\in S$ is the input user query with n tokens $q = (91, ..., qn)$ and the corresponding action $p\\in A$ represents the generated prompt with m tokens $p = (p1, \u2026, pm)$, where each token q,p is from the vocabulary V. In a single-step decision episode, the policy model generates a complete prompt based on the given expected reward and query $p ~ \\pi(p|r, q)$. The reward guides the quality of the generated prompt. For further discussion on why prompt generation is formulated as a single-step decision process, please refer to Appendix A.2.\nReward Design. In our single-step prompt generation process, the reward plays a crucial role in aligning with the true capabilities of the prompt. In this paper, we mainly focus on neural language understanding tasks and math reasoning tasks rather than generative tasks, which are challenging to assess objectively because of the poor correlation between the evaluation metrics and human preferences (Liang et al., 2022; Goyal et al., 2022). We design the reward based on two dimensions: query-level and task-level. Query-level reward measures whether the prompt can instruct LLM to answer the specific question correctly, while task-level reward measures the prompt's average prompting ability for all queries in one task. In particular, we adopt the average accuracy of the prompt for all test questions as the task-level reward for both zero-shot and few-shot settings,\n$R_{task}(q^{(i)}, p^{(i)}) = \\frac{1}{N} \\Sigma \\mathbb{1}{\\{\\hat{y}^{(i,j)} = y^{*(i)}\\}\\}$"}, {"title": "2.3 QPO Framework for Multi-Loop Augmentation", "content": "2.3.1 Initial Demonstration Construction\nData Availability. We start by emphasizing the presence and significance of prompt demonstrations generated by previous researches of prompt optimization. In the realm of prompt engineering, abundant"}, {"title": "3 Experiments", "content": "3.1 Experimental Setup\nTasks. We perform experiments on 6 language understanding tasks and 2 math reasoning tasks to validate our methods, including topic classification (AG's News (Zhang et al., 2015)), natural language inference (BoolQ (Clark et al., 2019)), sentiment classification (IMDB (Maas et al., 2011), TweetEval Emotion (Mo-hammad et al., 2018)), multi-choice QA (CosmosQA (Huang et al., 2019), HellaSwag (Zellers et al., 2019)), and math reasoning (GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021)). These tasks are widely studied in prompting settings, and hence many expert-crafted and machine-generated prompts are available, which facilitates our offline data collection procedure.\nBaselines. We compare our method with three types of baselines, including manual prompt engineering (PromptSource (Bach et al., 2022), Chain-of-Thought (Kojima et al., 2022)), online prompt optimization (Low Perplexity (Gonen et al., 2022), RLPrompt (Deng et al., 2022), APE (Zhou et al., 2022)) and offline prompting approach (Prompt-OIRL (Sun et al., 2023)). We also compare the prompts rewritten by Chat-GPT. We reproduce the online methods based on InstructEval (Ajith et al., 2023). Rather than making a direct while unfair comparison between the performance of QPO and newer online algorithms, our primary focus is to demonstrate that QPO can further enhance performance by optimizing prompts at query-level based on the prompts obtained through online optimization. Notably, for fair comparison, we use a larger dataset in Prompt-OIRL than ours after final loop data augmentation to eliminate our advances in additional interactions.\nLLMs. Our method is model-agnostic for policy model, and we use GPT-2 (Radford et al., 2019) in this paper, which is compact while has sufficient text generation capability. For the target LLMs, we use publicly available Llama2-7b-chat (Touvron et al., 2023) for natural language understanding tasks, while for the more challenging math reasoning tasks, we opt for GPT-3.5-turbo (Ouyang et al., 2022). Moreover, to evaluate the cross-model generalization ability of our trained policy model, we employ models at different abilities, scaling from the GPTNeo-1.3b (Black et al., 2021) to Llama2-13b-chat (Touvron et al., 2023) and Vicuna-13b (Zheng et al., 2024).\nImplementation Details. For the initial data collection, we utilize 30 expert prompts obtained from InstructEval and 120 prompts rewritten from ChatGPT-3.5 to construct the dataset. We set the QPO with 3 loops. For all the tasks, we use a unified hyperparameter set and do not need complex hyperparameter design for specific task. We do not cherry-pick checkpoints and directly use the final checkpoint in each loop for evaluation and next loop's training. For NLU tasks, we evaluate our method on both zero-shot and few-shot settings, while for math reasoning tasks for GPT-3.5, we only test its zero-shot performance due"}, {"title": "3.2 Main Results", "content": "Natural Understanding tasks. Table 1 presents a comprehensive comparison of query-dependent opti-mized prompts generated by QPO against human prompts, query-agnostic prompt optimization methods, and SOTA offline query-dependent prompt selection method on Llama2-7b-chat on zero-shot and 6-shot set-tings. The results show that: (1) QPO delivers significantly better results on zero-shot metric (avg. +7.2% over other baselines) and also outperforms other baselines on few-shot setting (avg. +3.3%), where the con-textual demonstrations slightly diminish the critical role of prompts, as evidenced by the standard deviation in few-shot scenarios being significantly smaller than in zero-shot scenarios. The 3-shot results in Table 11 further illustrate that our algorithm's advantage becomes more pronounced as the information provided by contextual demonstrations decreases (avg. +6.1%). (2) Based on the dataset constructed from these online algorithms, both QPO and Prompt-OIRL exhibit better performance to those optimized at the task level on all three settings (avg. +7.2%, +2.3%, +4.9%, respectively), which underscores the effectiveness of optimiz-"}, {"title": "3.3 Analysis", "content": "Multi-loop augmentation can bootstrap to improve performance in a interaction-cost-efficient way. Multi-Loop Augmentation (MLA) in our method is supposed to improve the exploration of the query space and prompt space efficiently, so that the augmented dataset can enhance the policy model in turn. The average number of queries in datasets increases from 283 in original dataset to 673 in the final augmented dataset, and the average number of prompts rises from 150 to 829, which obtains 138% and 453% increases in question coverage and prompt diversity with only a 17.1% increase in total data volume. We limit the amount of data augmented in each loop, as research (Wang et al., 2024c) indicates that generated data comprising 10% of the real data pro-vides the maximum training benefit, whereas exces-sive generated data can be counterproductive. To further quantify the effect of MLA, we conduct ablation experiments that finetune the model the same iterations without data augmentation. As shown in Figure 3, in 3 NLU tasks (AG News, BoolQ, IMDB) and 2 math reasoning (MR) tasks (GSM8K and SVAMP), MLA can consistently improve the performance in all settings, while training with more epochs without data augmentation results in significant drops due to overfitting. To be specific, multi-loop augmentation and training lead to a performance improvement of 6.7% and 2.3% for Llama2 in zero-shot and few-shot set-tings in NLU tasks, respectively, while in the zero-shot math reasoning tasks for GPT-3.5, the performance improved by 3.5%.\nDespite the introduction of online LLM interactions in MLA, our method: (1) only engages new interactions to supplement the dataset after the training convergence in each loop, rather than requiring interactions at each step to provide real-time guidance for optimization direction; (2) requires significantly fewer inter-actions compared to other online methods. Therefore, QPO is considered an offline approach. To demonstrate this, we com-pared our method with the classic online algorithm APE, which"}, {"title": "4 Related Works", "content": "Prompt Optimization. Automatic prompt optimization has become a pivotal issue in domain of LLMs. Recently, there has been a growing interest in this area. Soft prompts (Li & Liang, 2021; Zhang et al., 2021;"}, {"title": "A Extended Discussion on QPO", "content": "A.1 Advantages of offline RL compared to SFT\nSFT can be regarded as a form of imitation learning (Zheng et al., 2022), similar to behavior cloning (Torabi et al., 2018), where it mimics expert actions (i.e., the query-specific optimal prompts) based on given states (i.e., input questions). While offline reinforcement learning introduces reward values, guiding the generation of prompts based on the given queries and associated rewards. In prompt generation task, directly employing SFT results in low data utilization because it is unknown in advance that which prompt is optimal for specific problems. This necessitates massive prior evaluation of all prompts before selecting the best one as the expert behavior to construct the dataset, leading to significant resource wastage. Specifically, in the final SFT dataset, each query should ideally have only one prompt label. All M prompts must be evaluated but only one optimal prompt is retained as the label for SFT ultimately, wasting the remaining (M \u2013 1)/M evaluation data. While for offline RL, the prediction of prompt is based on both reward and query, leading to a much more diversity in input. Since different prompts have various rewards, most of them can be utilized for training. Offline RL offers significant advantages through the introduction of the rewards, which allows each query to be associated with multiple prompts and their corresponding rewards in the dataset. On one hand, the rewards improves the utilization of collected suboptimal prompts for training, significantly boosting exploration of the prompt space while avoiding resource wastage caused by prompt selection. On the other hand, rewards in RL enable the policy model to distinguish the prompting abilities of different prompts for various queries at the token level, leading to the generation of novel prompts for unknown queries.\nA.2 Prompt Generation as Single-step Decision Process\nIn the reinforcement learning setup, prompt generation is a sparse reward task. The complete prompt is evaluated with the target LLM to obtain a reward value only after it is fully generated. Given the availability of substantial amounts of such test data, offline reinforcement learning is an effective approach for prompt optimization. For offline RL, there are two methods for collecting token-wise rewards which can be used for multi-step decision-making processes:\n(1) Manually designing token-level rewards, which not only requires extensive prior expert knowledge but also introduces additional errors, making it unreliable.\n(2) Iteratively deleting the prompt's tokens from end to beginning and re-evaluating to obtain rewards from $s_{m\u22121} = (q, P1, \u2026, P_{m\u22121})$ to $s_m = (q, P1, \u2026, Pm)$, where pm denotes the m-th token in the complete prompt. However, this approach significantly increases the cost of collecting datasets and is thus impractical.\nTherefore, we do not introduce additional proxy rewards and directly utilize the sparse rewards. At this point, instead of modeling the prompt generation task as a multi-step decision-making process {r, so, P1, r, 81, P2, ...}, where each r is the same sparse reward and additional methods are required to encode long texts (91, ..., qn, p1, ...,pi) into the single token state si, we simplify the task modeling based on the principle of Occam's Razor. Thus, the prompt generation is formulated into a single-step decision process (r, q, p). Notably, the single-step decision-making process refers to the reward and state, taking only one action (i.e., generating the prompt), while the inner prompt generation process remains an autoregressive prediction $P ~ \u03a0\u03b5\u03c0(Pt|r, q, P_{<t})$.\nA.3 The Architecture of Policy Model\nCompared to the standard GPT-2 model with 124M parameters, we make three modifications to the model architecture: a reward embedding layer, a reward prediction layer, and a timestep embedding layer. Both the reward embedding layer and reward prediction layer are single-layer MLPs at the first token of the policy model. The reward is encoder into a reward encoding through the reward embedding layer, then input as the first token into the transformer to obtain the output at the first token position, which is then passed through the reward prediction layer to generate the predicted reward. This process can be viewed as an autoencoder"}]}