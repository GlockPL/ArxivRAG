{"title": "HVI: A New Color Space for Low-light Image Enhancement", "authors": ["Qingsen Yan", "Yixu Feng", "Cheng Zhang", "Guansong Pang", "Kangbiao Shi", "Peng Wu", "Wei Dong", "Jinqiu Sun", "Yanning Zhang"], "abstract": "Low-Light Image Enhancement (LLIE) is a crucial computer vision task that aims to restore detailed visual information from corrupted low-light images. Many existing LLIE methods are based on standard RGB (sRGB) space, which often produce color bias and brightness artifacts due to inherent high color sensitivity in sRGB. While converting the images using Hue, Saturation and Value (HSV) color space helps resolve the brightness issue, it introduces significant red and black noise artifacts. To address this issue, we propose a new color space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by polarized HS maps and learnable intensity. The former enforces small distances for red coordinates to remove the red artifacts, while the latter compresses the low-light regions to remove the black artifacts. To fully leverage the chromatic and intensity information, a novel Color and Intensity Decoupling Network (CIDNet) is further introduced to learn accurate photometric mapping function under different lighting conditions in the HVI space. Comprehensive results from benchmark and ablation experiments show that the proposed HVI color space with CIDNet outperforms the state-of-the-art methods on 10 datasets.", "sections": [{"title": "1. Introduction", "content": "Under low-light conditions, imaging sensors often capture weak light signals with severe noise, resulting in poor visual quality for low-light images. Obtaining high-quality images from such degraded images necessitates Low-Light Image Enhancement (LLIE) that aims at improving the image brightness while simultaneously reducing the impact of noise and color bias [38].\nThe majority of existing LLIE approaches [19, 25, 29, 52, 75, 80] focus on finding an appropriate image brightness, which is typically done by employing deep neural networks to learn a mapping relationship between low-light images and normal-light images within the standard RGB (sRGB) space. However, the image brightness exhibits a strong coupling with the color from the three sRGB channels, a.k.a. high color sensitivity in [17, 36], causing an obvious color distortion of the restored image in these LLIE methods [6, 29], as shown in Fig. 1 (a).\nInspired by Kubelka-Munk theory [17], recent methods [39, 76, 81] have sought to transform images from the sRGB color space to the Hue, Saturation and Value (HSV) color space. These methods help achieve the brightness enhancement more accurately, but they amplify local color space noise [17], introducing severe artifacts in the results. As illustrated in Fig. 1 (b), the transformation from sRGB to the HSV disrupts the continuity of the red (\u2460 Red Discontinuity Noise) and black (\u2461 Black Plane Noise) color, resulting in increased Euclidean distances for similar color and the introduction of artifacts in the final images (see the zoomed in images of \u2460 and \u2461). These two types of noise can cause severe artifacts in the enhancement of red-dominated or extremely dark images.\nTo address these issues, we introduce a new color space named Horizontal/Vertical-Intensity (HVI), designed specifically for the LLIE task. The key intuition is that minimizing color space noise, by reducing Euclidean distances in similar colors. To this end, we polarize in the Hue and Saturation (HS) plane to enforce smaller distances for similar red point coordinates, which eliminates the red discontinuity noise in the primary HSV space (see Fig. 1 (c)). For the black plane noise issue, we introduce a trainable darkness density parameter k and its corresponding adaptive intensity collapse function \\(C_k\\), which compresses the radius of low-light regions to zero, with the flexibility to gradually expand to the value of one as the intensity increases, as illustrated in Fig. 1 (d). This helps remove the black noise artifacts while maintaining the primary image appearance.\nWe further propose an LLIE network, named Color and Intensity Decoupling Network (CIDNet), to leverage the chromatic and intensity information for optimizing the color and intensity in the HVI space. After transforming the image into the HVI space, CIDNet leverages two network branches, namely HV-branch and intensity-branch, to respectively model decoupled color and brightness information for learning accurate photometric mapping under different lighting conditions and restoring more natural colors.\nOur contributions can be summarized as follows:\n\u2022 We introduce a new HVI color space for the LLIE task, which is uniquely defined by polarized HS and trainable intensity. This offers an effective tool that eliminates the color space noise arising from the HSV space, largely enhancing the brightness of the low-light images.\n\u2022 We further propose a novel LLIE network, CIDNet, to concurrently model the intensity and chromatic of low-light images in the HVI space. Despite being lightweight and computationally-efficient with relatively small parameters (1.88M) and computational loads (7.57GFLOPs), it enables the learning of accurate photometric mapping under different lighting conditions.\n\u2022 Comprehensive results from quantitative and qualitative experiments show that our approach outperforms various types of state-of-the-art (SOTA) methods on different metrics across 10 datasets."}, {"title": "2. Related Work", "content": "2.1. Low-Light Image Enhancement\nSingle-stage Methods. Single-stage deep learning approaches [6, 13, 23, 29, 41, 75] has been widely used in LLIE. Existing methods propose distinct solutions to address the aforementioned issues. For instance, RetinexNet [62] enhances images by decoupling illumination and reflectance based on Retinex theory. Bread [21] decouples the entanglement of noise and color distortion by using YCbCr color space. Furthermore, they designed a color adaption network to tackle the color distortion issue left in light-enhanced images. Still, RetinexNet and Bread can show inaccurate control in terms of brightness and biased color in black areas.\nDiffusion-based Methods. With the advancement of Denoising Diffusion Probabilistic Models (DDPMs) [24], diffusion-based generative models have achieved remarkable results in the LLIE task. It has shown the capability to generate more accurate and appropriate images. However, they still exhibit issues such as local overexposure or color shift. Recent LLIE diffusion methods have attempted to address these challenges by incorporating global supervised brightness correction or employing local color correctors [14, 26, 27, 59, 64, 79]. Other methods such as Diff-Retinex [69] rethink the retinex theory with a diffusion-based model in the LLIE task, aiming to decomposed an image to illumination and reflectance from sRGB color space. However, these diffusion models often fail to fully decouple brightness and color information.\n2.2. Color Space\nRGB. Currently, the most commonly used is the sRGB color space. For the same principle as visual recognition by the human eye, sRGB is widely used in digital imaging devices [47]. Nevertheless, image brightness and color exhibit a strong interdependence with the three channels in sRGB [17]. A slight disturbance in the color space will cause an obvious variation in both the brightness and color of the generated image. Thus, sRGB is not a desired color space for low-light enhancement.\nHSV and YCbCr. The HSV color space represents points in an RGB color model with a cylindrical coordinate system [15]. Indeed, it does decouple brightness and color of the image from RGB channels. However, the red color discontinuity and black plane noise pose significant challenges when we enhance the images in HSV color space, resulting in the emergence of various obvious artifacts. To circumvent issues related to HSV, some methods [4, 21] also transform sRGB images to the YCbCr color space, which has an illumination axis (Y) and reflect-color-plane (CbCr). Although it solved the hue dimension discontinuity problem of HSV, the Y axis is still coupled with the CbCr plane partially, leading to severe color shifts."}, {"title": "3. HVI Color Space", "content": "The HVI space is built upon the HSV color space, which is proposed to address the color space noise issues arising from the HSV space. The key intuition in HVI is that the restored images should have good perceptual quality for respective colors, i.e., similar colors have small Euclidean distances. Below we introduce the HVI transformation in detail, where the HSV color space is first applied to decouple the brightness and color information of input images, which could cause color space noise (e.g., red discontinuity and black plane). We then introduce our proposed polarized HS operations and learnable intensity collapse function in HVI to effectively address these issues.\n3.1. Color Space Noises in HSV\nIntensity Map. In the task of LLIE, one crucial aspect is accurately estimating the illumination intensity map of the scene from a sRGB input image. Previous methods [16, 62, 75] largely rely on the Retinex theory [34], using deep learning to directly generate the corresponding normal-light map. While this approach aligns with statistical principles, it often struggles to fit physical laws and human perception [17], resulting in limited generalizability [59]. Therefore, we instead refer to the Max-RGB theory [34] to estimate the intensity map, a.k.a., Value in HSV, rather than using neural networks to generate it. According to Max-RGB, for each individual pixel x, we can estimate the intensity map of an image \\(I_{max} \\in \\mathbb{R}^{H\\times W}\\) as follows:\n\\[I_{max}(x) = \\max_{c\\in \\{R,G,B\\}} (I_c(x)).\\quad (1)\\]\nThe intensity map then goes through the sRGB-HSV transformation that can lead to various red and black noises. We introduce these separately as follows.\nHue/Saturation Plane. Real-world low-light images often contain significant noise, making its identification and removal a key challenge in LLIE. Recent studies [62, 69] indicate that the noise in low-light images is a primary cause to shifts in Hue and Saturation, a.k.a. a general case of Reintex theory [59], while having minimal impact on light intensity. Therefore, decoupling the sRGB color space, known for its high color sensitivity, can be advantageous for the LLIE task. By leveraging pixel-based photometric invariance [17] and dichromatic reflection modeling [51], sRGB can be decoupled into illuminance and chromatic components, yielding the HSV (Hue/Saturation-Value) color space. In this representation, the Value map (V) component corresponds to light intensity map (\\(V = I_{max}\\)), while the HS plane forms a chromaticity plane independent of illuminance constraints. Specifically, the transformation of sRGB image to Saturation map (S) is defined as follows\n\\[\\begin{equation}\n    S = \\begin{cases}\n        0, & I_{max} = 0 \\\\\n        \\frac{\\Delta}{I_{max}}, & I_{max} \\neq 0\n    \\end{cases} \\quad (2)\n\\end{equation}\\]\nwhere \\(\\Delta = I_{max} - \\min(I_c)\\) and s is any pixel in S. The Hue map (H) is formulated as\n\\[\\begin{equation}\n    h = \\begin{cases}\n        0, & \\text{if } s = 0 \\\\\n        \\frac{I_G-I_B}{\\Delta} \\bmod 6, & \\text{if } I_{max} = I_R \\\\\n        2+\\frac{I_B-I_R}{\\Delta}, & \\text{if } I_{max} = I_G \\\\\n        4+\\frac{I_R-I_G}{\\Delta}, & \\text{if } I_{max} = I_B\n    \\end{cases} \\quad (3)\n\\end{equation}\\]\nwhere h is any pixel in H.\nColor Space Noises. Converting an sRGB image to HSV space effectively decouples brightness from color, enabling more accurate color denoising and more natural illuminance recovery. However, this transformation also amplifies noise in the red and dark regions [17], which are critical for LLIE tasks. As illustrated in Fig. 1 (b), enhancing the image brightness within the HSV color space yields a more balanced brightness level. However, excessive noise in the red discontinuities and the black plane introduces significant artifacts, particularly in red and dark areas of output image, which greatly degrade the perceptual quality.\nTo address this issue, we propose the HVI color space as follows, which effectively preserves the decoupling of brightness and color while minimizing these artifacts.\n3.2. Horizontal/Vertical Plane with Polarized HS and Collapsible Intensity\nOur primary approach to addressing the color space noise issue is to ensure that more similar colors exhibit smaller Euclidean distances. Along the Hue axis, the color red appears identically at both h = 0 and h = 6, due to the modular arithmetic of Hue-axis, which splits the same color across two ends of the spectrum. In particular, to address the red discontinuity issue, we apply polarization to the Hue axis (h) at each pixel in H, obtaining orthogonal \\(h = \\cos(\\theta)\\) and \\(v = s\\sin(\\theta)\\). When the Hue axis is polarized, it forms an angle within the orthogonalized h - v plane, with s representing the distance from the origin.\nFor the black plane noise issue, we aim to collapse regions of low light intensity while preserving those with higher in intensity. However, the optimal extent of collapse varies across different datasets and networks. Therefore, it is important to make this region adaptively collapsible through a learning process. To achieve this, we introduce an adaptive intensity collapse function \\(C_k\\) as follows\n\\[C_k(x) = \\sqrt{\\frac{k}{2}} \\sin\\left(\\frac{\\pi I_{max}(x)}{2}\\right) + \\epsilon, \\quad (4)\\]\nwhere \\(k \\in \\mathbb{Q}^+\\) is a trainable parameter to control the dark color point density, and a small \\(\\epsilon = 1\\times10^{-8}\\) is used to avoid gradient explosion. Essentially, \\(C_k\\) serves a radius mapping function, with smaller \\(C_k\\) corresponding to smaller radius or lower intensity values. Thus, black points are clustered together as \\(C_k\\) decreases. We then formalize the Horizontal (\\(\\hat{H}\\)) map and Vertical (V) map as\n\\[\\begin{aligned}\n    \\hat{H} &= C_k \\odot S \\odot H,\\\\\n    V &= C_k \\odot S \\odot V,\n\\end{aligned}\\quad (5)\\]\nwhere \\(h\\in H, v \\in V\\), and \\(\\odot\\) denotes the element-wise multiplication. \\(\\hat{H}, V, \\) and \\(I_{max}\\) can be concatenated to form an HVI image.\nThanks to these operations, the HVI space builds a strong color space that maintains the advantages of HSV while removing the red and black noises. More importantly, since HVI is trainable due to k and \\(C_k\\), specifically designed neural networks, as discussed in the following, can be created to optimize LLIE upon HVI under various lighting conditions."}, {"title": "4. Color and Intensity Decoupling Network", "content": "To more effectively utilize chromatic and brightness information in the HVI space, we introduce a novel dual-branch LLIE network, named Color and Intensity Decoupling Network (CIDNet), to separately model the HV-plane and I-axis information in the HVI space, as shown in Fig. 2. CIDNet employs HV-branch to suppress the noise and chromaticity in the dark regions and utilizes I-branch to estimate the illuminance of the whole images.\nThe overall framework of CIDNet can be divided into three consecutive main steps. There is an HVI transformation applied before the dual-branch enhancement network. After the enhancement, CIDNet performs perceptual-inverse HVI transformation to map the image to the sRGB space. Below we introduce each step in detail.\n4.1. HVI Transformation\nAs described in Sec. 3.2, the HVI transformation decomposes the sRGB image into two components: an intensity map containing scene illuminance information and an HV color map containing scene color and structure information. Specifically, we first calculate the intensity map using Eq. 1, which is \\(I_1 = I_{max}\\). Subsequently, we utilize the intensity map and the original image to generate HV color map using Eq. 5. Furthermore, a trainable density-k is employed to adjust the color point density of the low-intensity color plane, as shown in Fig. 2(a).\n4.2. Dual-branch Enhancement Network\nAs illustrated in Fig. 2 (b), the dual-branch network is built upon the UNet architecture, involving an encoder and an decoder with respective three Lighten Cross-Attention (LCA) modules, and multiple skip connections. Two key designs in the network are the dual-branch structure and the LCA module. We explain the key intuition behind these designs as follows.\nThe LLIE task can be decomposed into two sub-tasks: noise removal in low-light regions and brightness enhancement. By converting the image to the HVI color space, where luminance and color are decoupled, we can apply brightness mapping to the Intensity map and denoising to the HV color map. Since these two sub-tasks follow distinct statistical patterns [38], inspired by Retinex-based methods [16, 62, 63], we use separate branches, the I-branch and HV-branch, to address each sub-task individually. Additionally, the input to the HV branch is formed by concatenating the Intensity map with the HV color map, as we observed that severely low-light images contain a small amount of noise in the luminance component as well.\nThe cross-attention between the two branches is used, rather than using self-attention individually to each branch. One main reason is that the illumination intensity is inversely proportional to image noise intensity. A low-light image may also contain high-illumination regions requiring only minimal denoising and enhancement. Therefore, using the intensity features to guide the HV-branch in denoising can reduce global color shifts and achieve more effective noise suppression. For another reason, the noisy intensity information, after being denoised in the HV-branch, is transferred to the I-branch through cross-attention, resulting in smoother enhancement outcomes.\n4.3. Perceptual-inverse HVI Transformation\nTo convert HVI back to the HSV color space, we perform a Perceptual-inverse HVI Transformation (PHVIT), which is a surjective mapping while allowing for the independent adjustment of the image's saturation and brightness.\nThe PHVIT sets h and \\(\\hat{v}\\) as an intermediate variable as\n\\[h = \\frac{\\hat{H}}{C_k+\\epsilon}, \\quad v = \\frac{\\hat{V}}{C_k+\\epsilon},\\quad\\]\nwhere \\(\\epsilon = 1 \\times 10^{-8}\\) is used to avoid gradient explosion. Then, we convert h and v to HSV color space. The Hue (H), Saturation (S) and Value (V) map can be estimated as\n\\[\\begin{aligned}\nH &= \\arctan(\\frac{v}{h}) \\bmod 1,\\\\\nS &= \\alpha_{\\varsigma}\\sqrt{h^2 + v^2},\\\\\nV &= \\alpha_{1}I_1,\n\\end{aligned}\\quad (6)\\]\nwhere \\(\\alpha_{\\varsigma}, \\alpha_{1}\\) are the customizing linear parameters to change the image color saturation and brightness. Finally, we will obtain the sRGB image with HSV image [15].\n4.4. Loss Function\nTo provide comprehensive supervision for training CID-Net, we guide the enhancement from two key perspectives, including the GroundTruth in the sRGB space and the HVI map in the HVI space. Specifically, given the enhanced HVI map \\(\\hat{I}_{HVI}\\) and the restored sRGB image \\(\\hat{I}\\) output from CIDNet, we aim to minimize their difference to the primary sRGB GroundTruth I and its corresponding HVI map \\(I_{HVI}\\):\n\\[L = \\lambda \\cdot l(\\hat{I}_{HVI}, I_{HVI}) + l(\\hat{I}, I),\\quad (7)\\]\nwhere \\(\\lambda\\) is a weighting hyperparameter to balance the losses in the two different color spaces. This helps achieve not only more closely with the probabilistic distribution of sRGB in the HVI space, especially the red and black ones, due to the optimized k and \\(C_k\\), but also inheritance of pixel-level structure detail in the sRGB space."}, {"title": "5. Experiments", "content": "5.1. Datasets and Settings\nWe employ seven commonly-used LLIE benchmark datasets for evaluation, including LOLv1 [62], LOLv2 [68], DICM [35], LIME [22], MEF [43], NPE [57], and VV [54]. We also conduct further experiments on two extreme datasets, SICE [5] (containing mix and grad test sets [77]) and SID (Sony-Total-Dark) [7].\nLOL. The LOL dataset has v1 [62] and v2 [68] versions. Compared to LOL-v1 that contains both real and synthetic data, LOL-v2 is divided into real and synthetic subsets. For LOLv1 and LOLv2-Real, we crop the training images into 400 \u00d7 400 patches and train CIDNet for 1,500 epochs with a batch size of 8. For LOLv2-Synthetic, we set the batch size to 1 and train CIDNet for 500 epochs without cropping.\nSICE. The original SICE dataset [5] contains 589 low-light and overexposed images, with the training, validation, and test sets divided into three groups according to 7:1:2. CIDNet is optimized using 160 \u00d7 160 cropped images from the training set for 1,000 epochs with a batch size of 10 and tested on the datasets SICE-Mix and SICE-Grad [77].\nSony-Total-Dark. This dataset is a customized version of a SID subset [7]. To make this dataset more challenging, we convert the raw format images to sRGB images with no gamma correction, resulting in images with extreme darkness. We crop the training images into 256 \u00d7 256 patches and train CIDNet for 1,000 epochs with a batch size of 4.\nExperiment Settings. We implement our CIDNet by PyTorch. The model is trained with the Adam [31] optimizer (\\(\\beta_1\\) = 0.9 and \\(\\beta_2\\) = 0.999) by using a single NIVIDA 2080Ti or 3090 GPU. The learning rate is initially set to 1\u00d710\u22124 and then steadily decreased to 1 \u00d7 10\u22127 by the cosine annealing scheme [42] during the training process.\nEvaluation Metrics. For the paired datasets, we adopt the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) [61] as the distortion metrics. To comprehensively evaluate the perceptual quality of the restored images, we report Learned Perceptual Image Patch Similarity (LPIPS) [74] with AlexNet [32] as the reference. For the unpaired datasets, we evaluate single recovered images using BRISQUE [44] and NIQE [45] perceptually."}, {"title": "5.2. Main Results", "content": "Results on LOL Datasets. In Tab. 1, it can be found that our method is optimal on all metrics for both LOLv1 and LOLv2 datasets with 1.88M parameters and 7.57 GFLOPs. We outperform the best RGB-based method GSAD (diffusion) in terms of all PSNR, SSIM, and LPIPS metrics, while utilizing only 10.8% parameters of GSAD. Compared to RetinexFormer, a SOTA method based on Retinex theory, CIDNet delivers higher image quality while reducing computational cost by 8.28 GFLOPs. Subjective results are illustrated in Fig. 3, which demonstrates that our method not only more accurately recovers multi-color regions compared to GroundTruth but also achieves stable brightness enhancement, thanks to the HVI color space. More visualization comparison can be found in supplementary materials.\nResults on SICE and Sony-total-Dark. To validate the performance on large-scale datasets, we evaluate CIDNet on SICE (including Mix and Grad) and SID-Total-Dark. The results are presented in Table 2, where it is clear that CIDNet is the best performer in both PSNR and SSIM metrics on the two datasets. Notably, on Sony-Total-Dark, our model surpasses the second-best method by 6.678 dB in PSNR. This improvement is due to the extreme darkness of the dataset images, which substantially increases the difficulty of distinguishing details from noise. However, CIDNet leverages the intensity collapse function \\(C_k\\) to effectively maintain an optimal signal-to-noise ratio during training, enabling better detail recovery.\nResults on Unpaired Datasets. We evaluate the effectiveness of models trained on LOLv1 or LOLv2-Syn using various methods, and report their performance using BRISQUE and NIQE metrics in Tab. 2. Our method exhibits a substantial improvement in the NIQE metric compared to other approaches. As shown in Fig. 4, while CIDNet does not outperform RetinexNet in the BRISQUE metric in Tab. 2, its recovered perceptual results are closer to realistic appearances than RetinexNet. This may be attributed to the fact that building upon the HSV space, the HVI color space is derived from real-world perceptual models [17].\nGeneralizing HVI to Other LLIE Models. To verify the effectiveness of the HVI color space, we further evaluate its performance when it is used with different LLIE models. In particular, HVIT, together with its inverse mapping PHVIT, is used as a plug-and-play module into six SOTA methods that use sRGB images as input and are independent of specific color space characteristics. The results are reported in Tab. 3. It is clear that transforming to the HVI color space improves PSNR, SSIM, and LPIPS metrics across various methods compared to the results in the sRGB color space. Notably, the GSAD method demonstrates the most significant improvement, with a PSNR increase of 3.562 dB. This demonstrates not only the generalizability of HVI to various sRGB-based methods but also its general effectiveness as a color space for the LLIE task.\nFor the inference time results in Tab. 3, it is evident that the diffusion-based methods require longer GPU time but achieve better enhancement results. In contrast, CIDNet shows the most efficient inference while achieving the highest PSNR and the second-best SSIM and LPIPS scores. This highlights the strong ability of CIDNet in balancing the efficiency and effectiveness within the HVI color space."}, {"title": "5.3. Ablation Study", "content": "We validate our HVI color space and the key modules in CIDNet using both quantitative (Tab. 4) and qualitative results (Figs. 5 and 6). The experiments are all performed on LOLv2-Real for fast convergence and stable performance.\nHVI Color Space. It can be seen in Tab. 4 that the enhancement in the sRGB color space leads to chromatic aberration and luminance bias, as demonstrated by the difference between Figs. 5(b) and (g). Compared to sRGB, using HSV yields images aligned more closely with the GroundTruth in both luminance and color due to its effectiveness on decoupling brightness from color, which can be observed the enhancement from Fig. 5(b) to Fig. 5(c). This results in improved PSNR and LPIPS in Tab. 4. However, it significantly introduces more noise due to the red discontinuity in HSV, leading to the noisy black spots in the red regions in Fig. 5(c) and the degraded SSIM in Tab. 4. Using the polarization or the \\(C_k\\) solely in the HVI space can lead to similar image quality in PSNR and SSIM compared to those in the HSV color space. Qualitatively, as shown in Fig. 5(d), using polarization only helps cluster similar red tones, avoiding the red discontinuity. Relying solely on the intensity function \\(C_k\\) helps adjust the brightness, but leads to confusion between red and other colors. Consequently, dot-like artifacts appear not only in the red regions but also color shifts in other areas, as shown in Fig. 5(e). These issues are all effectively mitigated when the polarization and the \\(C_k\\) function are applied together, as shown by consistent improvement of the Full Model in all three metrics in Tab. 4 and the better image enhancement in Fig. 5(f).\nDual-branch Network Structure. In Tab. 4, adding self-attention to the baseline noticeably improves all three metrics, indicating that transformer-based models hold potential for application in the HVI color space. We then modified the architecture from a single-branch to a dual-branch structure without cross-attention, resulting in a PSNR increase of 0.846 dB, while SSIM and LPIPS showed minimal change. Further incorporating the cross-attention into the I-branch and HV-branch (Full Model) obtains the best color restoration, light enhancement, and optimal metric performance, as shown in Tab. 4. This can also be observed in Fig. 6.\nLoss Function. As shown in Tab. 4, compared to using both HVI and sRGB losses, relying solely on the HVI loss lacks pixel-level spatial consistency constraints, leading to a loss of structural detail in the image and thus lower performance across the three metrics, especially in the LPIPS metric. On the other hand, using only sRGB loss is focused on pixel-space enhancement, neglecting the low-light probability distribution in the HVI color space, resulting in undesired color imbalance."}, {"title": "6. Conclusion", "content": "In this work, we introduce the HVI color space and the CIDNet approach to address the color bias and brightness artifact issues that occur to current sRGB-based LLIE approaches. By encapsulating polarized HS maps and a learnable intensity component, HVI shows strong robustness to both issues. To further enhance LLIE, CIDNet is designed to model decoupled chromatic and intensity information in the HVI space for achieving precise photometric adjustments under varying lighting conditions. Experimental results on 10 datasets demonstrate that the HVI color space, combined with CIDNet, outperforms SOTA LLIE methods, establishing it as a robust solution for low-light enhancement."}]}