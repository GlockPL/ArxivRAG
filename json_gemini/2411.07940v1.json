{"title": "Automatic dataset shift identification to support root cause analysis of Al performance drift", "authors": ["M\u00e9lanie Roschewitz", "Raghav Mehta", "Charles Jones", "Ben Glocker"], "abstract": "Shifts in data distribution can substantially harm the performance of clinical Al models. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, root causes of dataset shifts are varied, and the choice of shift mitigation strategies is highly dependent on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework, effectively distinguishing between prevalence shift (caused by a change in the label distribution), covariate shift (caused by a change in input characteristics) and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the importance of self-supervised encoders for detecting subtle covariate shifts and propose a novel shift detector leveraging both self-supervised encoders and task model outputs for improved shift detection. We report promising results for the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts, using four large publicly available datasets.", "sections": [{"title": "Introduction", "content": "Machine learning models are notoriously sensitive to changes in the input data distribution, a phenomenon commonly referred to as dataset shift\u00b9\u20134. This is particularly problematic in clinical settings, where dataset shift is a common occurrence and may arise from various factors. Changes in the frequency of disease positives over time or across geographical regions cause prevalence shift6,7 (also known as label shift). The use of different acquisition protocols or scanners8\u201310, or a change in patient demographics11,12 can induce shifts in image characteristics, known as covariate shift. We illustrate examples of real-world shifts in Fig. 1. Dataset shift can dramatically affect the performance of AI and lead to clinical errors such as misdiagnosis4,13,14. It is recognised as the fundamental barrier hindering AI adoption15,16. It is hence crucial to implement safeguards allowing not only effective detection of the presence of shifts, but importantly, reliable identification of the root causes. Comprehensive shift detection and identification frameworks are key for the safe deployment and continuous monitoring of AI in clinical practice.\nDataset shifts can be detected at deployment time by using statistical testing to compare the distributions of incoming test data to the distribution of the reference data (representative of the data used to validate the deployed AI model). Significant progress has been made in this field where state-of-the-art methods can detect various types of real-world shifts17\u201320. Shifts between test and reference data can either be detected at the output level by comparing distributions of model outputs, or at the input level by comparing low-dimensional feature representations of input images19 (see Methods, Dataset shift detection methods for more details). In this study, we show that different types of shifts require different shift detection approaches. On the one hand, comparing model output distributions allows for the reliable detection of shifts directly related to the downstream task, such as changes in prevalence. On the other hand, we show that for shifts orthogonal to the downstream task, such as changes in image acquisition protocols, comparing output distributions is not sufficient. For such shifts, test and reference data need to be compared at the input level using rich feature representations. We demonstrate that self-supervised neural network image encoders21, trained without using any task-specific annotations, yield excellent low-dimensional feature representations for shift detection.\nWhile detecting dataset shifts is important, it is insufficient for the safe deployment of AI. Besides knowing that there is a problem, we need to be able to identify its root cause to take the necessary actions, implement preventive measures, and safeguard against harm caused by AI errors. To our knowledge, no solution yet exists to identify the root cause of dataset shifts. The precise identification of the type of shifts is critical for selecting appropriate mitigation strategies. For example, prevalence shifts can often be mitigated with lightweight output recalibration techniques14,22\u201324, but these rely on the assumption that no other types of shift are present. In contrast, covariate shifts require more advanced domain adaptation techniques or model fine-tuning4,25\u201329. The difficulty is that a change in image characteristics may cause similar changes in the distribu-"}, {"title": "Results", "content": "Datasets and shift generation\nWe evaluate our methods on four different datasets covering three different imaging modalities: (i) chest radiography using PadChest30 and RSNA Pneumonia Detection31 datasets; (ii) mammography using the EMBED\u00b3\u00b2 dataset, and (iii) fundus images using the Kaggle Diabetic Retinopathy Detection\u00b3\u00b3, Kaggle Aptos Blindness Detection34 and Messidor-v235 datasets. To study prevalence shift detection, we associate each dataset with a downstream task. For chest radiography datasets we focus on pneumonia detection (binary), for mammography on breast density assessment (4 classes), and for retinal images on diabetic retinopathy assessment (binary). More details can be found in Methods, Datasets.\nFor every dataset, we study different types of shifts at different levels of intensity. We simulate various levels of prevalence shift by resampling the test set according to specific label distributions. Similarly, we simulate different types of covariate shifts by resampling the datasets according to specific scanner, acquisition site or gender distributions. We then measure shift detection (resp. identification) accuracy by repeated sampling of shifted test sets. Specifically, for each bootstrap sample, a test set is sampled according to the defined shift generating process. We then measure how many times the shift is detected (resp. the correct shift was identified) out of all 200 bootstrap samples. Technical details on shift generation processes, experimental setup, and implementation of shift detection algorithms can be found in Methods, Shift generation details and"}, {"title": "Different shifts require different detectors", "content": "We first investigate which types of shifts are successfully detected by prominent dataset shift detection methods. We compare two families of shift detectors: model output-based and feature-based detectors leveraging representations from pretrained neural networks. We additionally propose and test a dual approach that combines both complementary approaches for improved shift detection ('Duo'). A detailed description of the shift detection tests used in this study can be found in Methods, Dataset shift detection methods.\nFor feature-based shift detection, any pretrained network could be used as feature extractor. A perhaps obvious choice is to simply use the encoder from the task-specific classification model. However, this may not be the best choice as learned features will be heavily skewed towards encoding characteristics specifically relevant to that task as opposed to encoding more generic image representations sensitive to data distribution changes36. Hence, we here explore the potential of self-supervised (SSL) image encoders for shift detection. Indeed, encoders trained in a self-supervised manner, i.e. without any labels, learn fine-grained representations effectively summarising all the information encoded in a given image, resulting in ideal candidates for generic shift detection. We compare the performance of feature-based shift detection for five different encoders (using the features obtained from the last layer of the neural network encoder). We compare: (i) 'Random' a neural network (ResNet) encoder with random weights; (ii) \u2018Supervised ImageNet' where features are extracted using a ResNet encoder trained to perform classification on natural images from the ImageNet dataset; (iii) \u2018Task model', the ResNet encoder from the task model used to perform the downstream classification task; (iv) 'SSL ImageNet' a self-supervised encoder trained on ImageNet data only; (v) 'SSL Modality Specific' a self-supervised encoder trained on the same modality as the test datasets. Additionally, for the RETINA dataset, we include a comparison using the open-source 'RetFound\u201937 self-supervised foundation model as a feature extractor. More details on self-supervised models can be found in Methods, Self-supervised learning.\nOutput- and feature-based detectors detect different shifts. Figs. 3 and 4 show the shift detection rates for every dataset-shift combination. Across all datasets, a clear pattern appears. For prevalence shifts (Fig. 3), output-based shift detection performs significantly better than all feature-based tests. This is intuitive as the shift is directly related to the downstream prediction task. A change in prevalence should directly be reflected by a change in the distribution of task model outputs. For covariate shifts, results are very different, regardless of whether"}, {"title": "Combining output- and feature-based detection", "content": "Building on the previous findings, we introduce a dual detection approach combining responses from output-based and feature-based shift detectors, using self-supervised features for more robust shift detection. Results in Figs. 3 and 4 demonstrate that the proposed 'Duo' detector (in orange) performs best overall across shifts and datasets, regardless of the shift severity. For example, with the duo detector and test set size of 1000 samples, the detection accuracy is > 80% for nearly every shift, across all datasets. On the contrary, the other detectors respectively fail either on prevalence shift (with feature-based detectors detecting less than 50% prevalence shift cases averaged over datasets) or on covariate shift (where output-based detection detects less than 5% of gender shifts for radiography and less than 25% of acquisition shifts for EMBED). We employ"}, {"title": "Shift identification performance", "content": "To perform shift identification, we leverage the fact that output-based and feature-based shift measures detect different types of shifts to precisely identify the shift present in the test dataset, following a decision logic detailed in Methods Dataset shift identification and illustrated in Fig. 2.\nIn-depth evaluation results in Figs. 5 and 6 demonstrate that our novel shift identification framework is capable of distinguishing between prevalence shifts, covariate shifts and mixed shifts with high accuracy across all datasets and types of shifts. Overall, more subtle shifts are best detected with larger test sets whereas larger shifts can be detected with smaller test sets. For prevalence shifts, the average shift identification rate, across datasets and shift levels, is 85% with 500 test images and reaches 95% with 1,000 test images (Fig. 5 a-d). Similarly, for covariate shifts caused by acquisition shifts, when using a test set size of 500 images (and 250 test exams on EMBED), the identification accuracy is greater than 80% for any shift level and dataset, with an average identification accuracy of 89% across datasets and shift levels (Fig. 5 g-i). For the RETINA dataset, identification rates already reach 100% with a test set as small as 250 images. When the covariate shift is induced by gender shifts, covariate shift is detected with identification accuracies greater than 90% for both PadChest and RSNA Pneumonia, with a test set size of 500 images, for all but one shift level (Fig. 5 e-f). Results in Fig. 6, show that the framework is also able to accurately distinguish between cases of covariate shift only and cases of covariate and prevalence shifts. For these mixed shifts, shifts are identified with increasingly high accuracy as the test set size increases. With a test set of 1,000 images, mixed gender and prevalence shifts are correctly identified as mixed shifts with an average accuracy of 97% for RSNA Pneumonia and 75% for PadChest. Mixed shifts induced by acquisition and prevalence shifts are detected with an average 100% accuracy for the RETINA dataset, 77% for PadChest and 79% for EMBED, across shift levels with 1,000 test images. The overall identification accuracy for mixed shifts across all shifts and datasets is 85% (with 1000 test images)."}, {"title": "Discussion", "content": "Shift identification is critical for root cause analysis of AI performance drift. We propose a lightweight and practical shift identification framework, capable of detecting and identifying important real-world dataset shifts. Our results show that the proposed framework separates prevalence shifts from covariate shifts with high accuracy, as well as identifies cases of mixed shifts, across three different data modalities and various types of realistic shifts. By analysing common dataset shift detection paradigms, we find that different types of shifts require different types of shift detectors. Our analysis also demonstrates the importance of the choice of encoders for feature-"}, {"title": "Dataset shift detection methods", "content": "Several paradigms have been proposed for dataset shift detection. The most widely used and simplest method to implement consists of comparing distributions of a classifier's outputs between the reference and test domain, proposed by Rabanser et al.19 and referred to as \u2018Black Box Shift Detection' (BBSD). Specifically, softmax model outputs (predicted probabilities) are collected for all samples in the reference and test sets. Then, for each class, a separate univariate Kolmogorov-Smirnov (K-S) test is run to determine if the class-wise predicted probabilities distributions differ between reference and test domain. A multiple-testing Bonferonni41 correction is then applied to the individual tests to determine the overall significance of the test.\nIn the same study, Rabanser et al.19 also propose another type of shift detector where reference and test data input distribution are compared using a feature-based approach. In this test, the input sample (image) first gets projected to a smaller dimension, e.g. through a pretrained neural network encoder and the shift is then measured using the 'Maximum Mean Discrepancy' permutation test originally proposed by42. The Maximum Mean Discrepancy measures the distance between two distributions p and q based on the distance between their mean embeddings. An unbiased estimate of the square of the MMD statistic can be computed via\nMMD =  \\frac{1}{m^2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\kappa(z_i, z_j) + \\frac{1}{n^2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\kappa(z_i', z_j') - \\frac{2}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\kappa(z_i, z_j'),\nwhere {zi}i=1 ~ p, and {z'j}j=1 ~ q and k is a kernel over the embedding space. The p-value can then be obtained using a permutation test. In Rabanser et al. they proposed to use the RBF kernel \u03ba(z, z') = e\u2212||z\u2212z'||2/\u03c3, setting \u03c3 as the median distance between all samples. An alternative way would be to explicitly learn the kernel43 with 'deep kernels'. The disadvantage of this approach is that the kernel needs to be learned for every single test set for which one wishes to run shift detection. In this work, we use the RBF kernel.\nAdditionally, we propose and evaluate a dual shift detector, combining BBSD and MMD outcomes (referred to as 'Duo'). We first run each test independently, this yields C p-values for the BBSD test (one per class), and one p-value for the MMD permutation test. To account for multiple testing, we then apply Bonferroni correction\u2074\u00b9 on the C + 1 p-values to get the overall dual test outcome.\nAnother existing shift detection approach consists of training a domain classifier to classify samples between the reference and test domains44\u201346 and using the accuracy of this classifier as a proxy for measuring distances between distributions. One drawback of this approach is its high computational cost: for every test set, a new domain classifier must be trained, which is highly impractical in continuous monitoring scenarios. Hence, we here focus on output-based (BBSD) and feature-based (MMD) shift detection methods, as these do not require training of additional models at test-time."}, {"title": "Dataset shift identification", "content": "In this work, we propose a novel framework for identifying whether dataset shift is caused by prevalence shift, by covariate shift or by a mix of both. The process is summarised in Fig. 2. The approach can be divided into two stages. In the first step, we perform 'standard' dataset shift detection to separate the 'shift' from the 'no shift' cases. For this detection step, we use the proposed dual detection approach, combining signals from model outputs and self-supervised features to detect shifts (see Dataset shift detection methods). Provided that a shift has been detected in this first step, we add an additional 'shift identification' step to separate prevalence from covariate shifts. We start by estimating the label distribution in the test dataset using the density ratio estimator from state-of-the-art label adaptation method CPMCN22 (see next section). Next, we undersample the reference set to match the estimated test set prevalence shift. We then compare feature distributions between the prevalence-adjusted reference set and the test set. If adjusting"}, {"title": "Self-supervised learning", "content": "Self-supervised learning aims to learn meaningful representations of images without relying on labelled data21,48,49. Two prominent paradigms for learning such representations are contrastive learning and predictive coding methods. In contrastive learning, models are trained to learn similar representations for semantically related pairs of image views (positive pairs) while pushing them away from the representations of unrelated inputs (negative pairs). A seminal work in this area is SimCLR21, where different views are created using heavy random data augmentation pipelines and representations are optimised using a normalised temperature-scaled cross-entropy loss21. SimCLR has since been refined in numerous follow-up studies48,50\u201352, yet remains widely used. On the other hand, predictive coding approaches, such as Masked Auto Encoders49, learn representations by masking parts of the input and training a neural network to reconstruct the missing regions, encouraging the network to build a fine-grained understanding of the semantic information present in the visible parts of the image."}, {"title": "Datasets", "content": "For chest radiography, we use two public datasets for our analysis: the RSNA Pneumonia dataset\u00b3\u00b9, a subset of the NIH Chest-Xray8 dataset53 manually relabelled by expert radiologists for presence of pneumonia-like opacities. We use the original metadata from the NIH Chest-Xray8 dataset53 to retrieve patient gender. We also use PadChest30 a larger dataset containing multiple disease labels extracted from radiology reports. We here focus on the pneumonia label. Importantly, this dataset contains important metadata such as scanner information or the gender of the patient, allowing us to generate a wide range of shifts. For mammography, we use the EMBED dataset\u00b3\u00b3\u00b2 a large mammogram dataset collected in the US, on 6 different scanners. Finally, for fundus imaging, we create \u2018RETINA' a multi-domain dataset by combining three different public datasets: the Kaggle Diabetic Retinopathy Detection33 dataset, the Kaggle Aptos Blindness Detection dataset34 and the Messidor-v2 dataset35. These datasets cover different regions of the world (India, France, US) but also with varying image acquisition devices: images from the Messidor-v2 are high-quality images, while many images in the Kaggle datasets are of lower quality (including phone pictures). Creating this multi-centre dataset allows us to simulate various domain shifts by varying the proportion of data from each 'site' (original dataset source). The task of interest for fundus images here is binary diabetic retinopathy (DR) classification for fundus images, where we classify images between referable DR (grades 2,3,4) and healthy/non-referable DR (grades 0,1)."}, {"title": "Shift generation details", "content": "For prevalence shifts, we assume that the task of interest is pneumonia detection for chest x-ray images, breast density classification for mammography and binary diabetic retinopathy classification for fundus images. In terms of covariate shifts, we simulate different sub-types of shifts depending on the datasets at hand. For both chest radiography datasets, we first simulate sub-population shift in the form of gender shift where we vary the proportion of female patients in the test set. PadChest30 additionally includes scanner labels, allowing the study of acquisition shift. The dataset features scans from two types of scanners, 'Phillips' and 'Imaging', with an initial 40%-60% distribution. To simulate different levels of acquisition shift, we vary the proportion of Phillips scans in the test set. Similarly, for the EMBED32 mammography dataset, we simulate various levels of acquisition shift by varying the distribution of scanners in the test set. This dataset offers a complementary view to the chest x-ray datasets, addressing a multi-class problem (unlike the binary pneumonia detection task) and providing even more flexibility for simulating diverse levels of acquisition shifts (as it contains data from six different scanners). Note that in EMBED each standard mammography exam comprises 4 mammograms (left/right breasts and MLO/CC views). We excluded all exams that did not contain exactly four images and kept exactly one exam per patient. For this dataset, test set resampling was done at the exam level (as opposed to image-level sampling for other datasets). Finally, for our RETINA dataset, we simulate domain shifts by varying the proportion of samples coming from each underlying dataset (Aptos34, Kaggle DR33 and Messidor35)."}, {"title": "Implementation details", "content": "For all datasets, we first define train, validation and test splits. The training set is reserved for training task models (pneumonia, density and DR classification). All task models are ResNet-50 models initialised with ImageNet weights. To evaluate the shift identification accuracy of the proposed frameworks, we follow standard evaluation practices from the dataset shift detection literature. Specifically, we repeat the following process 200 times: (i) sample a subset of size Ntest from the original test split according to the shift of interest, (ii) run the shift identification test, and (iii) record whether the shift is correctly identified. The final shift identification accuracy is computed as the proportion of times the shift is correctly identified out of the bootstrap samples. For each bootstrap repetition, we also re-sample the reference set (against which we measure the shift) by taking a random sample of Nref data points from the validation set (uniformly at random). For both chest radiography datasets, we used Nref = 2,000 images and Ntest \u2208 {100, 250, 500, 1000} images. For the RETINA dataset, we used Nref = 1,000 images and Ntest \u2208 {100, 250, 1000} images. The case of EMBED is slightly different as we need to ensure that images belonging to the same exam are always sampled simultaneously. Hence, for this dataset sampling of all the reference sets, shifted test sets (and permutations in the MMD"}, {"title": "Prevalence shift estimation and adaptation", "content": "Many studies have proposed algorithms to alleviate the detrimental effect of prevalence shift on machine learning models. Most methods follow the same over-arching idea: (i) estimate the density ratio Pref(Y)/Ptest(Y); (ii) use this ratio to recalibrate the model14,22,23,47. In this work, we are primarily interested in the first step, to estimate the prevalence in the test set in the absence of labels. Various methods have been proposed to estimate this density ratio, mostly differing by their computational costs14,22,23,47. Here, we use the recently proposed state-of-the-art \u2018class probability matching with calibration network' (CPMCN) method by Wen et al.22 to estimate this ratio. In this work, the authors estimate the density ratio w\u2208 RC, with wi = \\frac{P_{test}(Y=i)}{P_{ref}(Y=i)}\\  by\n\u0175 := arg min\\w\u2208RC  \\frac{1}{C} \\sum_{i=1}^{C}  \\frac{Pref(Y = i) -  \\frac{1}{m} \\sum_{x\u2208Dt} \\frac{p(i|x)}{\\sum_{j=1}^{C}wjp(j|x)} \\.\nWith m the number of samples in the test set Dt, Pref(Y = i) is the empirical proportion of samples with class i in the reference set, and p(ix) is the probability predicted by the model for class i given sample x. Given this estimated ratio \u0175, we can easily recover the estimated label distribution on the test domain\nPtest(Y = i) = \u0175i \u00b7 Pref(Y = i), \u2200i \u2208 C\nWe follow this method to estimate the label distribution in the test set in our shift identification module.\nNote that, once the density ratio has been estimated, it can be used to recalibrate model outputs to mitigate the effects of prevalence shifts on model performance. Precisely, re-calibrated probabilities can be computed by:\n\u011d(ix) = \\frac{w_ip(i|x)}{\\sum_{j=1}^{C} w_jp(j|x)}."}, {"title": "Supplementary material", "content": "In this section, our goal is to motivate the need for dataset shift identification, illustrating the usefulness of distinguishing between prevalence shift and covariate shift with a simple example on chest radiography data.\nFor this motivating example, we use data from the PadChest dataset30 and focus on classifying images with pneumonia. We then generate various test sets with (i) prevalence shift, (ii) acquisition shift, (iii) both shifts and evaluate the model on these shifted datasets. Our goal is to illustrate why identifying the type of shift occurring is a crucial issue. Indeed, many domain adaptation methods are tailored for specific shifts; label shift adaptation, for example, aims at tackling the effect of prevalence shift only, often explicitly assuming that no other shift is occurring. In supplementary Fig. 1, we show the effect of dataset shift on model calibration before and after applying a post-hoc label shift adaptation algorithm in three settings: (i) only prevalence shift occurs, (ii) acquisition shift occurs instead, (iii) both shifts happen. For prevalence shift, we sampled a test set with 20% prevalence of pneumonia, shifted from the original 3% prevalence in PadChest (training and validation prevalence). For acquisition shift, we create a test set containing 90% of scans taken with Phillips scanner and 10% with Imaging scanner, a shift from the original 40%-60% scanner distribution in PadChest. The label shift adaptation algorithm used for this example is CPMCN22 (see Prevalence shift estimation and adaptation). We can clearly see that all shifts cause some amount of model calibration deterioration. More importantly, in the prevalence shift case, CPMCN successfully restores model calibration on the shifted dataset, and the estimated prevalence matches the true test set prevalence. However, for the acquisition shift scenario, the effect of applying label shift correction is counter-productive: calibration is much worse after applying the re-calibration than before. In the case where both shifts are happening at the same time, the re-calibration also fails to restore the calibration. This is, of course, absolutely expected from the theory: label shift adaptation methods like the ones tested here were designed for the case where only prevalence shift is happening. However, this empirical observation confirms the main motivation of our study: it"}, {"title": "False detection rate for different shift detectors", "content": "In this section, we measure the false detection rate of dataset shift in the absence of shift in the test set generating process. We verify that the false-positive rates of the output-based, feature-based and Duo detectors stay low across datasets and detectors, we find that false detection rates hover around the expected type-I error of the statistical tests 5% for most datasets and detection methods. The only exception is EMBED, where false positive rates are a little higher than expected for the output-based (and hence Duo) detector. This is due to the fact that - for this highly imbalanced classification problem - small randomly resampled test sets may lead to small differences in the observed cumulative distribution of predictions, even in the absence of a shift in the test set generation process."}]}