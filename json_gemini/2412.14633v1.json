{"title": "Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training Quantization in Vision Transformers", "authors": ["Rui Ding", "Liang Yong", "Sihuan Zhao", "Jing Nie", "Lihui Chen", "Haijun Liu", "Xichuan Zhou"], "abstract": "Due to its efficiency, Post-Training Quantization (PTQ) has been widely adopted for compressing\nVision Transformers (ViTs). However, when quantized into low-bit representations, there is often\na significant performance drop compared to their full-precision counterparts. To address this issue,\nreconstruction methods have been incorporated into the PTQ framework to improve performance\nin low-bit quantization settings. Nevertheless, existing related methods predefine the reconstruc-\ntion granularity and seldom explore the progressive relationships between different reconstruction\ngranularities, which leads to sub-optimal quantization results in ViTs. To this end, in this paper,\nwe propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for accurate PTQ, which\nsignificantly improves the performance of low-bit quantized vision transformers. Specifically, we\ndefine multi-head self-attention and multi-layer perceptron modules along with their shortcuts as\nthe finest reconstruction units. After reconstructing these two fine-grained units, we combine them\nto form coarser blocks and reconstruct them at a coarser granularity level. We iteratively perform\nthis combination and reconstruction process, achieving progressive fine-to-coarse reconstruction.\nAdditionally, we introduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the\ndifficulty of training, thereby further enhancing model performance. Experimental results on the\nImageNet dataset demonstrate that our proposed method achieves the best Top-1 accuracy among\nstate-of-the-art methods, particularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides,\nquantization results on the COCO dataset reveal the effectiveness and generalization of our proposed\nmethod on other computer vision tasks like object detection and instance segmentation.", "sections": [{"title": "1. Introduction", "content": "In recent years, Vision Transformers (ViTs) Dosovit-\nskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner,\nDehghani, Minderer, Heigold, Gelly, Uszkoreit and Houlsby\n(2021) have attracted increasing attention due to their im-\npressive performance and have achieved great success in\nvarious categories of vision tasks, such as image classifica-\ntion Wang, Du, Wang and Xu (2024); Su, Cao, Zhao, Li,\nWu, Han and Liu (2024); Chen, Lin, Li, Shen, Wu, Chao\nand Ji (2023), semantic segmentation Zhang, Yu, Zhang,\nWu, Tian, Kang and Li (2024); Li, Wang, Liu, Zhu, Zhong\nand Wang (2024); Dong, Wang and Wang (2023), object\ndetection Liu and Wang (2024); Rend\u00f3n-Segador, \u00c1lvarez\nGarc\u00eda, Salazar-Gonz\u00e1lez and Tommasi (2023); Cao, Yuan,\nFeng and Niu (2022); Zhu, Su, Lu, Li, Wang and Dai\n(2021), and so on. The basic block within ViTs consists\nof the Multi-Head Self-Attention (MHSA) and the Multi-\nLayer Perceptron (MLP), where the former is used to model\nlong-distance dependencies, and the latter is used to enhance\nfeature representation capabilities Dosovitskiy et al. (2021).\nBuilt on multiple basic blocks with large feature dimensions,\nViTs typically suffer from huge memory and computational\nrequirements, posing deployment difficulties on resource-\nlimited edge devices.\nTo address this problem, model quantization has been\nproposed and is considered a promising compression solu-\ntion, which quantizes the full-precision weights and activa-\ntions within ViTs into low-bit representations, significantly\nreducing memory and computational costs. This technique is\ncommonly divided into two categories: Quantization-Aware\nTraining (QAT) Esser, McKinstry, Bablani, Appuswamy\nand Modha (2020); Bondarenko, Nagel and Blankevoort\n(2024); Le and Li (2023) and Post-Training Quantization\n(PTQ) Li, Xiao, Yang and Gu (2023); Lin, Zhang, Sun, Li\nand Zhou (2022); Chu, Yang and Huang (2024). Unlike the\ncostly QAT, which requires retraining on the entire dataset\nfrom scratch, PTQ only needs to calibrate the quantiza-\ntion parameters on a small-scale unlabeled dataset, making\nit suitable for efficiently quantizing ViTs. However, when\nquantized into low-bit (under 6-bit), there exists an unac-\nceptable performance drop compared with the full-precision\ncounterparts, such as 8.51% and 36.23% accuracy loss in 4-\nbit and 3-bit quantized ViT-S, respectively Ma, Li, Zheng,\nLing, Xiao, Wang, Wen, Chao and Ji (2024); Zhong, Hu,\nLin, Chen and Ji (2023).\nTo bridge the performance gap, reconstruction methods\nhave been incorporated into PTQ for performance improve-\nment with limited resource requirements, aiming to reduce\nthe output difference between the quantized model and the\nfull-precision model. As shown in Figure 1(a), conventional\nreconstruction granularity is single and fixed during opti-\nmization, such as module-wise and block-wise. For block-\nwise reconstruction, BRECQ Li, Gong, Tan, Yang, Hu,"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Post-Training Quantization for ViTs", "content": "Compared with the time-consuming Quantization-Aware\nTraining (QAT) methods Wang, Wang, Xu, Zhou and Lu\n(2022); Le and Li (2023); He, Lou, Zhang, Liu, Wu, Zhou\nand Zhuang (2023); Kirtas, Oikonomou, Passalis, Mourgias-\nAlexandris, Moralis-Pegios, Pleros and Tefas (2022), which\nrequire retraining the whole model on the entire training\ndataset from scratch, Post-Training Quantization (PTQ)\nmethods demonstrate superior efficiency by conducting cali-\nbration on small-scale unlabeled datasets. However, PTQ for\nvision transformers often suffers from an unacceptable accu-\nracy drop compared to full-precision counterparts. Directly\napplying quantization methods designed for Convolutional\nNeural Networks (CNNs) Esser et al. (2020); Xu, Li, Wang\nand Zhang (2024) in ViTs usually does not work well\ndue to the unique structures like Multi-Head Self-Attention\n(MHSA), Multi-Layer Perceptron (MLP), and LayerNorm.\nTo alleviate this problem, numerous PTQ works ded-\nicated to vision transformers have been proposed. Early\nworks, such as FQ-ViT Lin et al. (2022), propose power-\nof-two factor and log-int-softmax methods to systematically\nreduce the performance drop and sustain the non-uniform\ndistribution, building a fully quantized ViT. Additionally,\nVT-PTQ Liu, Wang, Han, Zhang, Ma and Gao (2021b) in-\ncorporates a ranking loss into the objective function, aiming"}, {"title": "2.2. Reconstruction in Post-Training Quantization", "content": "Although specially designed PTQ methods for ViTs\nimprove the model performance to some extent, there still\nexists a notable accuracy drop compared to full-precision\nmodels in low-bit quantization scenarios. For example, the\n4-bit quantized ViT-S using PTQ method RepQ-ViT Li\net al. (2023) suffers a 16.34% Top-1 accuracy drop on the\nImageNet dataset.\nTo address this issue, reconstruction methods have been\nincorporated into the PTQ framework, aiming to further\nboost the capacity of quantized models by minimizing the\noutput difference between the quantized and full-precision\nmodels. BRECQ Li et al. (2021) first proposes recon-\nstructing the quantized model at a block-wise granularity\nlevel and identifies the overfitting problem of network-\nwise reconstruction, which shows superior effectiveness on\nCNNs. However, integrating BRECQ into PTQ for ViTs\ndoes not bring significant performance improvements in\nlow-bit quantization. Therefore, PD-Quant Liu et al. (2023a)\nintroduces the prediction difference loss from the network-\nwise level to the block-wise reconstruction which is aligned\nwith the objective function, demonstrating better generaliza-\ntion performance for image classification. Besides, ADFQ-\nViT Jiang et al. (2024) presents attention-score-enhanced\nmodule-wise optimization for finer reconstruction, which is\nproven to reduce quantization errors. Furthermore, Outlier-\nAware Ma et al. (2024) first introduces the concept of\nreconstruction granularity and demonstrates the influence\nof different granularities from the perspective of reducing\noutlier problems. It is worth noting that the difference\nfrom our work is that Outlier-Aware still applies the single\nreconstruction granularity by slicing the basic block and\ndoes not consider the progressive relationships among them."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Preliminaries", "content": ""}, {"title": "3.1.1. Modules in ViTs.", "content": "For a single vision transformer, the input image is\nfirst decomposed into N flattened image patches and then\nprojected to feature matrix represented by $X \\in \\mathbb{R}^{N \\times D}$.\nThe processed features are then fed into L transformer\nbasic blocks, which contain the Multi-Head Self-Attention\n(MHSA), Multi-Layer Perceptron (MLP) and LayerNorm\n(LN). The forward computation process of the 1-th basic\nblock can be formulated as follows:\n$Y_{l} = X_{l} + MHSA(LN(X_{l})),$ (1)\n$X_{l+1} = Y_{l} + MLP(LN(Y_{l})),$ (2)\nwhere both the MHSA and MLP utilize the shortcuts to\nenhance the representational capability and improve the\ntraining performance."}, {"title": "3.1.2. Quantizers in ViTs.", "content": "Following the common strategies in recent works Li et al.\n(2023); Moon, Kim, Cheon and Ham (2024); Zhong et al.\n(2023), we adopt the uniform quantization for quantizing\nall the weights and most of the activations within ViTs. In\ndetail, considering b-bit quantization, a full-precision x can\nbe uniformly mapped to a integer $x_q$ as follows:\n$x_{q} = clamp(\\lfloor \\frac{x}{s} \\rceil + z, 0, 2^{b} - 1),$ (3)\nwhere $\\lfloor \\rceil$ is the rounding operation, and the clamp function\nlimits the quantized value within the representation range\nfrom 0 to $2^{b} - 1$. Besides, the s and z denote the quanti-\nzation scale and zero-point which can be calibrated through\nthe PTQ process on the small-scale calibration dataset as\nfollows:\n$s = \\frac{max(x) - min(x)}{2^{b} - 1}, z = \\lfloor - \\frac{min(x)}{s} \\rceil,$ (4)\nwhere the dequantization process can be formulated as:\n$x_{deq} = s \\cdot (x_{q} - z) \\approx x$. (5)\nIn particular, the softmax operation converts the attention\nscores within MHSA into probabilities named post-softmax\nactivations, which follow the power-law distribution. There-\nfore, we apply the commonly used log2 quantizer for them\nas follows:\n$x_{q} = clamp(\\lfloor - \\frac{log_{2}(x)}{s} \\rceil, 0, 2^{b} - 1)$. (6)\nThe de-quantization value is computed by:\n$x_{deq} = s \\cdot 2^{x_{q}}$, (7)\nwhich can be implemented with the efficient bit-shifting\noperations Li et al. (2023) for supporting the fast model\ninference.\nDuring the optimization process, by utilizing the Straight\nThrough Estimator (STE) Bengio, L\u00e9onard and Courville\n(2013) to propagate the gradients through the rounding func-\ntion $\\lfloor \\rceil$, weights and quantization parameters in quantized\nViTs can be updated through back-propagation algorithms."}, {"title": "3.2. Progessive Fine-to-Course Reconstruction", "content": "Reconstruction methods aim to restore performance by\ndecreasing the output difference between quantized models\nand full-precision counterparts. Conventional works usually\npredefine the reconstruction granularity before the optimiza-\ntion process, such as module-wise and block-wise. However,\nfew studies explore the progressive relationship between\nthese granularity levels, where the finer reconstruction units\nare the building components for the coarser ones.\nBased on the above observations, we propose a novel\nreconstruction method for ViTs by utilizing the internal\nconnections between different granularity levels, named Pro-\ngressive Fine-to-Coarse Reconstruction (PFCR). As shown\nin Figure 2 and defined in Eq. (8) along with Eq. (9), we\nchoose the MHSA and MLP with their shortcuts as our finest\nreconstruction units, which can be represented with:\n$R_{0}^{A}(X) = X + MHSA(LN(X)),$ (8)\n$R_{0}^{P}(Y_{l}) = Y_{l} + MLP(LN(Y_{l})),$ (9)\nwhere we consider $R_{g}(\\cdot)$ function as the reconstruction units,\nthe superscript A and P denote the two finest reconstruction\nunits, and the subscript g = 0 is the finest reconstruction\ngranularity level. Then we define the value range of g as:\n$g = \\{0, 1, 2, ..., G\\},$ (10)\nwhere there are total G + 1 granularity levels and start from\nindex 0. The bigger number of g means the coarser recon-\nstruction granularity and vice versa, therefore G represents\nthe coarsest reconstruction granularity.\nThe reason why we define $R_{0}^{A}(X)$ and $R_{0}^{P}(Y_{l})$ as the\nfinest reconstruction granularities can be summarized into\nthree folds: (1) the finer granularity reconstruction will con-\nsume more training time, damaging the efficiency of PTQ,\n(2) these two units retain structural and functional integrity,\n(3) the shortcuts in these modules provide effective gradient\nflows for better optimization results.\nTherefore, as illustrated in the right sub-figure of Figure\n2, the PFCR begins from these two finest units at the granu-\nlarity level g = 0, and the minimizing objective function is\nformulated as:\n$min||\\hat{R_{0}^{A}(X)} - R_{0}^{A}(X)||_{2},$ (11)\n$min||\\hat{R_{0}^{P}(Y_{l})} - R_{0}^{P}(Y_{l})||_{2},$ (12)\nwhere we use Mean Square Error (MSE) denoted by $|| \\cdot ||_{2}$\nto measure the output errors between the quantized units $\\hat{R}$\nand the full-precision counterparts R. We adopt the prevalent\noptimization algorithms to update the weights of quantizers\nin ViTs for minimizing the object function. Due to the\nsimplified module structure, the over-fitting problem will be\nmitigated during the reconstruction process Li et al. (2021).\nAfter they are reconstructed, we combine these two fine\nunits to build the reconstruction targets $R_{1}$ with a coarser\ngranularity level $g = 1$. At this level, we conduct the\nblock-wise reconstruction similar to other works Zhong et al.\n(2023); Li et al. (2021); Ma et al. (2024), as follows:\n$min||\\hat{R_{1}(X_{l})} - R_{1}(X_{l})||_{2},$ (13)\nwhere $R_{1}$ and $\\hat{R_{1}}$ denote the full-precision and quantized\nblocks respectively, which are built on the units from finer\ngranularity level g = 0. In this case, with Eq. (8) and Eq.\n(9), we could rewrite the Eq. (13) as follows:\n$min||\\hat{R_{0}^{P}(R_{0}^{A}(X_{l}))} - R_{0}^{P}(R_{0}^{A}(X_{l}))||_{2}.$ (14)\nwhere the coarse block-wise reconstruction is decomposed\ninto two finer module-wise reconstructions that have been\noptimized in the last granularity level (g = 0) with Eq.\n(11) and Eq. (12). By inheriting the optimized weights from\nthe finer level, this strategy provides a better initialization\nfor the current reconstruction granularity and the coarser\nreconstruction finetunes the parameters further, boosting the\nconverge speed and reducing the reconstruction loss.\nAs shown in Figure 2, the coarser reconstruction for $R_{G}$\nwill be conducted when two finer units $R_{G-1}$ reconstructed,\ntherefore the coaserest granularity level G is computed by:\n$G =\\begin{cases}\nlog_{2}(2L), & if (2L \\& (2L - 1)) = 0, \\\\\n\\lfloor log_{2}(2L) \\rfloor - 1, & else,\n\\end{cases}$ (15)\nwhere & denotes the bit-wise AND operation, the 2L indi-\ncates the total number of finest reconstructions units in L\nblocks for ViTs. For some cases where the number is not a\npower of 2, we apply the rounding operation and ignore the\nlast granularity level. Taking the 12-block ViT Dosovitskiy"}, {"title": "3.3. Progressive Optimization Strategy", "content": "Based on the observations from I&S-ViT Zhong et al.\n(2023), the loss landscape of ViTs with quantized weights\nand activations is rugged and drastically changing, resulting\nin poor training performance. Previous works like ReAct\nLiu, Shen, Savvides and Cheng (2020) propose a two-stage\nframework equipped with distributional loss to build high-\nperformance binary neural networks, and I&S-ViT also in-\ntroduces the three-stage method for better results.\nIn this paper, to push the limit of the low-bit quan-\ntized ViTs, we propose a Progressive Optimization Strategy\n(POS), combining the two-stage progressive optimization\nwith the progressive fine-to-coarse reconstruction. Specifi-\ncally, the process can be summarized as follows:\n(1) Smooth Training with Less Granularity. In the first\nstage, we retain the weights of ViTs in full-precision repre-\nsentation while quantizing the activations into the target low-\nbit bit-width. This method could provide a smoother and less\nvariation loss landscape for the optimization process, which\nalleviates the training challenge for reconstruction. Due to\nthis great property, we set G = 1 in this stage to balance the\ntrade-off between performance and efficiency.\n(2) Rugged Training with More Granularity. In the\nsecond stage, we quantize both the weights and activations in\nViTs into low-bit representations, where the loss landscape\nof reconstruction becomes rugged and varied. To alleviate\nthe training difficulties, we compute the coarsest granularity\nby Eq. (15). Combining the fine-to-coarse method and inher-\niting the weights from the first stage, the POS converges to\nbetter optimization results. The simplified structure of finer\nreconstruction units acts like a regularization for coarser-\ngrained reconstruction, reducing the generalization error\ncompared with the previous works with fixed granularity Li\net al. (2021); Zhong et al. (2023); Ma et al. (2024).\nGiven the unequal network capacity in various recon-\nstruction granularity, we adopt the diminishing learning\nrates and incremental training iterations as follows:\n$lrg = lro * (1 - 0.2 * g),$ (18)\n$iter_{g} = iter_{0} * (1 + 0.2 * g),$ (19)\nwhere lro and iter0 denote the base learning rate and it-\neration in g = 0, respectively. The base iteration number\nimpacts the reconstruction time and the quantization perfor-\nmance in low-bit quantization, and we will investigate the\neffect in detail within the experimental section.\nWith PFCR and POS, we provide the optimization\npipeline of the reconstruction method in Algorithm 1."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Settings", "content": ""}, {"title": "4.1.1. Models and Datasets", "content": "We apply the proposed PFCR along with POS to quan-\ntize different types of vision transformers, including ViT\nDosovitskiy et al. (2021), DeiT Touvron, Cord, Douze,\nMassa, Sablayrolles and J\u00e9gou (2021) and Swin-Transformer\nLiu, Lin, Cao, Hu, Wei, Zhang, Lin and Guo (2021a) with\nsmall and base model scales. The quantization bit-width of\nthese ViTs is set as 3-bit, 4-bit, and 6-bit for both weights and\nactivations. The pre-trained models are adopted from Timm\nWightman (2019).\nAfter that, we evaluate the reconstruction performance\non the validation set of ImageNet Deng, Dong, Socher, Li,\nLi and Fei-Fei (2009) which is a large-scale benchmark\ndataset for image classification with 1000 classes. The Top-1\naccuracy is reported as the evaluation metric.\nBesides, for validating the generalization of our pro-\nposed method, we evaluate the proposed PFCR on the"}, {"title": "4.1.2. Implementation Details", "content": "We implement our proposed PFCR and POS based on\nthe Pytorch Paszke, Gross, Massa, Lerer, Bradbury, Chanan,\nKilleen, Lin, Gimelshein, Antiga et al. (2019) framework\nand conduct all the experiments on a single A6000 GPU.\nFor calibration, we adopt the 64 randomly selected samples\nfrom the training set in ImageNet to initialize the quanti-\nzation parameters in Eq. (3) and Eq. (6). All the weights\nand activations are quantized into the target low-bit rep-\nresentations, including the output from Softmax attention\nand LayerNorm layer. Channel-wise uniform quantization is\nopted for weights, while layer-wise uniform quantization is\nused for activations except for the post-softmax activations\nthat adopt the log2 quantizer.\nFor the ImageNet dataset, we randomly select 1024\nsamples for reconstruction. The default learning rate lro is\nset as 4e-5 and iteration number iter0 is set as 800, 300,\nand 100 for 3-bit, 4-bit, and 6-bit quantization, respectively.\nFor the COCO dataset, we select only 1 sample for\nreconstruction, which is 1024\u00d7 fewer than the previous\nmethod Zhong et al. (2023). The default learning rate and\niteration number are set as 6e-7 and 500 respectively, and\nthe batch size is set as 1. Furthermore, the Adam optimizer\nKingma and Ba (2014) and cosine annealing scheduler are\napplied for updating parameters and adjusting the learning\nrate, respectively."}, {"title": "4.2. Comparisons with SOTA on ImageNet dataset", "content": "In Table 1, we compare our method with other State-Of-\nThe-Art (SOTA) works on ImageNet and report the Top-\n1 accuracy of various categories of quantized ViTs under\ndifferent low-bit quantization settings, including 3-bit, 4-bit,\n6-bit quantization for both weights and activations.\nFor 3-bit quantization, as shown in Table 1, our proposed\nPFCR achieves superior PTQ performance based on differ-\nent backbones, especially improving the Top-1 accuracy of\n3-bit ViT-S and 3-bit ViT-B by 13.78% and 11.84% over\nthe SOTA reconstrution method I&S-ViT, respectively. In\ncontrast, the block-wise methods BRECQ and PD-Quant\nwith single and fixed reconstruction granularity suffer from\na severe performance drop compared with the full-precision\ncounterparts, from 13.56% to 80.97% in all the network\nstructures. The significant performance improvement can be\nattributed to the more effective reconstruction process bene-\nfited from the progressive fine-to-coarse strategy combined\nwith the POS framework."}, {"title": "4.3. Comparisons with SOTA on COCO dataset", "content": "To further explore the effectiveness of our proposed\nmethod on other high-level computer vision tasks, based\non Mask-RCNN and Cascade Mask-RCNN frameworks, we\nconduct experiments on the COCO dataset and compare the\n4-bit quantization results using Swin-T and Swin-S back-\nbones in Table 2. The box average precision $Ap^{box}$ and\nmask average precision $Ap^{mask}$ are applied as the evaluation\nmetrics for object detection and instance segmentation.\nFor the Mask R-CNN framework, as the result shows,\nthe APQ-ViT achieves the best box average precision with\nthe Swin-S backbone. Nevertheless, it suffers significant\nperformance degeneration with the Swin-S backbone, which\nis shown that the box and mask average precision are 22.3%\nand 19.0% lower than the full-precision counterpart. In\ncontrast, our proposed PFCR achieves consistent perfor-\nmance improvement in different backbone models (Swin-T\nand Swin-S) compared with the block-wise method such as\nBRECQ and PD-Quant, which demonstrates the effective-\nness and robustness of the proposed progressive reconstruc-\ntion technique. Particularly, when the Swin-T is used as the\nbackbone, our proposed achieves 1.8 and 2.0 points higher\naverage precision over the SOTA method I&S-ViT for object\ndetection and instance segmentation tasks respectively.\nFor the Cascade Mask R-CNN framework, our proposed\nPFCR obtains the best results on most evaluation metrics\nin various backbone models for both of the vision tasks.\nSpecifically, When Cascade Mask R-CNN with Swin-T is\nused, our proposed PFCR improved the box AP and mask\nAP by 0.1 and 0.5 points over the previous SOTA method.\nWhen Swin-S serves as the backbone model, the segmen-\ntation performance of 4-bit quantized Cascade Mask R-\nCNN is further increased by 0.3 points in mask average\nprecision. Meanwhile, the detection performance of our\nproposed method is comparable with the SOTA method.\nThese results validate the effectiveness and generalization\nof our proposed reconstruction method on various high-level\ncomputer vision tasks."}, {"title": "4.4. Ablation Study", "content": "In this subsection, based on ViT-S, we provide a com-\nprehensive study for analyzing the effect of the different"}, {"title": "4.4.1. Effect of PFCR", "content": "Firstly, we examine the effect of the proposed progres-\nsive fine-to-coarse reconstruction method. Table 4 demon-\nstrates the quantization results of ViT-S under different low-\nbit settings and reconstruction granularity levels defined by\nG. For this part in the Ablation Study, we only explore\nthe effect of granularity G on the quantization performance\nso the default iteration iter0 = 250 is simply set as 250\niterations for experimental efficiency and eliminating the\neffect of longer training time. Besides, we use the one-stage\nreconstruction for PFCR to exclude the influence of POS.\nAs Table 4 shows, as G grows larger, the performance\nof quantized ViT-S is improved significantly under 3-bit and\n4-bit quantization settings, indicating the effectiveness of the\nproposed PFCR in low-bit quantization. For example, the 3-\nbit quantized ViT-S boosts its Top-1 accuracy from 19.23%\nto 49.83% by progressively reconstructing the model from\nfine-to-coarse granularity. Nevertheless, the 6-bit quantized\nViT-S suffers a 0.01% accuracy drop when G increases\nfrom 2 to 3, showcasing the over-fitting problem in higher-\nbit quantization which corresponds to the phenomenon in\nthe previous section. Specifically, as revealed in Table 3,\ncompared with the normal block-wise reconstruction, our"}, {"title": "4.4.2. Effect of POS", "content": "Secondly, we study the influence of the progressive\noptimization strategy on the performance of low-bit quan-\ntization. To this end, in Table 5, we report the accuracy\nof the model under different quantization bit-width using\nthe conventional one-stage method and the proposed POS\nrespectively. It is worth noting that the performance gain\nbrought by the proposed POS is more significant in ex-\ntremely low-precision quantization such as 4.41% and 2.28%\nin 3-bit and 4-bit quantized ViT-S respectively. For 6-bit\nquantization, although not obvious, our method still achieves\n0.18% higher Top-1 accuracy improvement compared with\nthe conventional one-stage reconstruction method. Besides,\nas shown in Table 3, the proposed POS is beneficial for\nboth the block-wise reconstruction and PFCR, boosting their\nclassification performance by 38.67%. and 2.28% severally.\nIn particular, combining the POS and PFCR, the best Top-1\naccuracy of 76.31% is obtained, narrowing the performance\ngap to 5.08% between the full-precision counterpart. These\nconvincing results showcase that our proposed training strat-\negy is suitable for the progressive reconstruction technique,\nproviding a smoother and more stable optimization land-\nscape for better results in low-bit quantized ViTs.\nFurthermore, as demonstrated in Table 6, the previous\nworks like Outlier-Aware Ma et al. (2024) optimize the\nparameters of quantizers with 20000 iterations to achieve the\nsatisfied performance, costing about 2 hours for reconstruc-\ntion. In contrast, the proposed PFCR with POS takes only\nnearly 19 minutes with 300 base iterations to reconstruct\nthe whole model, which saves the time requirement by\n6.96 \u00d7 and achieves 3.43% performance improvement. It is\nworth noting that even the PFCR without POS obtains better\nclassification performance with even lower time cost for\nreconstruction. As mentioned earlier in the proposed PFCR,\nthe finer reconstructed units provide a better initialization for"}, {"title": "4.4.3. Effect of Iteration Number", "content": "Finally, we explore the effect of the iteration number on\nthe quantization performance of the low-bit quantized ViT-\nS regardless of efficiency. Figure 4(a) illustrates the Top-1\naccuracy of different low-bit quantized ViT-S with the in-\ncreasing iteration number iter from 200 to 1100. It is shown\nthat the performance of 4-bit and 6-bit quantized models\nis not sensitive to the iteration number, where the longer\nreconstruction time does not bring obvious accuracy gains.\nDifferent from them, the 3-bit quantized model benefits\nsignificantly from the larger number of iterations. However,"}, {"title": "4.5. Comparisons of Visualization Results", "content": "To more intuitively demonstrate the effectiveness of our\nproposed method, we select 8 images from the ImageNet\nvalidation set and utilize the Grad-CAM Selvaraju et al.\n(2017) to compare the visualization results with the block-\nwise I&S-ViT Zhong et al. (2023) based on 3-bit quantized\nViT-B in Figure 5. As the figure shows, our proposed method\npays better attention to the discriminative features such as\nImage 2 and Image 3. Besides, as Image 6 and Image 8\nreveal, our quantized 3-bit ViT-B has a greater focus on\ninformative objects, without distracting their attention from\nclass-independent objects. Equipped with the progressive\nreconstruction technique, our method achieves a lower quan-\ntization error and better classification performance than the\nblock-wise reconstruction works, contributing to the 11.84%\nTop-1 accuracy improvement in 3-bit quantization."}, {"title": "5. Conclusions", "content": "In this paper", "granularity": "the fine granularity is\nthe building component for the coarser granularity. When\ntwo units are reconstructed, the coarser units are combined\nby them and begin to reconstruct. With this process iter-\natively, we complete the whole reconstruction in the end.\nBesides, we propose a two-stage optimization method POS,\nwhich alleviates the training difficulties in low-bit quanti-\nzation and boosts the performance further. Experimental\nresults show that our proposed method achieves state-of-\nthe-art performance in low-bit quantized ViTs on different\\"}]}