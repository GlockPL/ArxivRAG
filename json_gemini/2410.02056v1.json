{"title": "SYNTHIO: AUGMENTING SMALL-SCALE AUDIO CLASSIFICATION DATASETS WITH SYNTHETIC DATA", "authors": ["Sreyan Ghosh", "Sonal Kumar", "Zhifeng Kong", "Rafael Valle", "Bryan Catanzaro", "Dinesh Manocha"], "abstract": "We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noise or masking segments), struggle to create data that captures the true diversity present in real-world audios. To address this shortcoming, we propose to augment the dataset with synthetic audio generated from text-to-audio (T2A) diffusion models. However, synthesizing effective augmentations is challenging because not only should the generated data be acoustically consistent with the underlying small-scale dataset, but they should also have sufficient compositional diversity. To overcome the first challenge, we align the generations of the T2A model with the small-scale dataset using preference optimization. This ensures that the acoustic characteristics of the generated data remain consistent with the small-scale dataset. To address the second challenge, we propose a novel caption generation technique that leverages the reasoning capabilities of Large Language Models to (1) generate diverse and meaningful audio captions and (2) iteratively refine their quality. The generated captions are then used to prompt the aligned T2A model. We extensively evaluate Synthio on ten datasets and four simulated limited-data settings. Results indicate our method consistently outperforms all baselines by 0.1%-39% using a T2A model trained only on weakly-captioned AudioSet. Code will be available here.", "sections": [{"title": "1 INTRODUCTION", "content": "Audio classification is the foundational audio processing task of understanding the input audio and assigning it to one or multiple predefined labels. However, training audio classification models requires a lot of high-quality labeled data, which is not always readily available (Ghosh et al., 2022). Manually collecting and annotating large-scale audio datasets is an expensive, time-consuming, and noisy process (Nguyen et al., 2017; Mart\u00edn-Morat\u00f3 & Mesaros, 2021), and recent concerns about data privacy and usage rights further hinder this process (Ren et al., 2023).\nData augmentation, which involves expanding original small-scale datasets with additional data, is a promising solution to address data scarcity. Traditional augmentation techniques attempt to diversify audio samples by applying randomly parameterized artificial transformations to existing audio. These methods include spectral masking (Park et al., 2019), temporal jittering (Nanni et al., 2020), cropping (Niizumi et al., 2021), mixing (Seth et al., 2023; Ghosh et al., 2023b; Niizumi et al., 2021) and other techniques (Saeed et al., 2021; Al-Tahan & Mohsenzadeh, 2021; Manocha et al., 2021). While these approaches have shown success, they operate at the level of observed data rather than reflecting the underlying data-generating process that occurs in real-world scenarios. As a result, they statistically modify the data without directly influencing the causal mechanisms that produced it, leading to high correlations between augmented samples and limited control over diversity.\nGenerating synthetic data from pre-trained text-to-audio (T2A) models addresses the limitations of standard data augmentation techniques while retaining their strengths of universality, controllability, and performance (Trabucco et al., 2024). The recent success of generative models makes this approach particularly appealing (Long et al., 2024; Evans et al., 2024b). However, generating synthetic audio presents unique challenges due to the complexity of waveforms and temporal"}, {"title": "2 RELATED WORK", "content": "Data Augmentation for Audio and Beyond. Expanding or augmenting small-scale datasets with additional data has been widely studied in the literature. Traditional augmentation methods, which apply randomly parameterized artificial transformations to data during training, remain the most common approach across language Wei & Zou (2019); Karimi et al. (2021), vision (Shorten & Khoshgoftaar, 2019; Wang et al., 2017; Yun et al., 2019), and audio (Park et al., 2019; Spijkervet, 2021). For audio, specific techniques include SpecAugment, adding background noise, reverberation, and random spectrogram transformations. With the emergence of generative models, synthetic data augmentation has been increasingly adopted for language (Ghosh et al., 2023a; 2024c; Chen et al., 2021) and vision (Trabucco et al., 2024; Zhao et al., 2024; He et al., 2023), proving to be more effective than traditional methods. These approaches generally incorporate explicit steps to ensure the consistency and diversity of generated augmentations. While some progress has been made in speech, synthetic data for audio remains relatively underexplored. Recently, parallel work has investigated simple prompting of T2A models to improve T2A generation (Kong et al., 2024) and environmental scene classification (Ronchini et al., 2024).\nText-to-Audio Generation. In recent years, there has been a significant surge in research on text-to-audio (T2A) models. The most popular architectures include auto-regressive models based on codecs (Kreuk et al., 2023; Copet et al., 2024) and diffusion models Liu et al. (2023); Ghosal et al. (2023); Evans et al. (2024a). Clotho (Drossos et al., 2020) and AudioCaps (Kim et al., 2019) remain the largest human-annotated datasets for training these models. However, large-scale datasets for T2A model training are still scarce. Recently, Yuan et al. (2024) synthetically captioned AudioSet (Gemmeke et al., 2017), demonstrating its effectiveness for training T2A models. For downstream adaptation, earlier works have primarily relied on Empirical Risk Minimization (ERM). Majumder et al. (2024) introduced preference optimization for T2A models, creating a synthetic preference dataset based on scores provided by a CLAP model (Elizalde et al., 2023)."}, {"title": "3 BACKGROUND", "content": "Diffusion Models. Diffusion models consist of two main processes: a forward process and a reverse process. Given a data point 10 with probability distribution p(x0), the forward diffusion process gradually adds Gaussian noise to xo according to a pre-set variance schedule \u03b2\u2081,\uff65\uff65\uff65, \u03b2r and degrades the structure of the data. At the time step t, the latent variable xt is only determined by the Xt-1 due to its discrete-time Markov process nature. At inference time, the diffusion model iteratively executes the reverse process T times starting from a randomly sampled Gaussian Noise (\u2208 ~ N(0, I)). For more details on diffusion models, we request our readers to refer to Appendix A.1.\nWe now explain Direct Preference Optimization (DPO), a simpler alternative to RLHF (Christiano et al., 2017). Abusing notation, we will also xo as random variables for language.\nReward Modeling. Estimating human preferences for a particular generation 20, given the context c, is challenging because we do not have direct access to a reward model r(c, xo). In our scenario, we assume only ranked pairs of samples are available, where one sample is considered a \u201cwinner\" (x0) and the other a \u201closer\u201d (x) under the same conditioning c. Based on the Bradley-Terry (BT) model, human preferences can be modeled as:\nPBT(x > xoc) = \u03c3(r(c, xo) \u2013 r(c, xo)) (1)\nwhere o represents the sigmoid function. The reward model r(c, xo) is parameterized by a neural network & and trained through maximum likelihood estimation for binary classification:\nLBT($) = -Ec,x,x [logo(r\u2084(c, xo) \u2013 r$(c, xo))] (2)\nHere, prompt c and data pairs (x, x) are drawn from a dataset labeled with human preferences.\nRLHF: The goal of RLHF is to optimize a conditional distribution po(x0|c), where c De, such that the latent reward model r(c, xo) is maximized. This is done while regularizing the distribution through the Kullback-Leibler (KL) divergence from a reference distribution Pref, resulting in the following objective:\nmax Ec-Dc,xo~po(xo|c)[r(c, xo)] \u2013 BDKL[Po(xo|C)||Pref(xo|C)] (3)\nPe\nHere, the hyperparameter \u1e9e controls the strength of regularization.\nDPO : DPO directly optimizes the conditional distribution po(x0c) to align data generation with the preferences observed in (any form of) feedback. The goal is to optimize the distribution of generated data such that it maximizes alignment with human preference rankings while maintaining consistency with the underlying reference distribution Pref(x0|c)."}, {"title": "4 METHODOLOGY", "content": "Let Dsmall = {(a\u017c, li), 1 \u2264 i \u2264 n} be a high-quality, small-scale human-annotated audio classification dataset with n audio-label pairs. Let Da-c be a potentially noisy, large-scale weakly-captioned dataset of audio-caption pairs with zero intersection with Dsmall. Our goal is to train a T2A model T\u00ba using Da-c, then use it to generate a synthetic dataset Dsyn and then finally add it to Dsmall (now attributed as Dtrain) to improve audio classification performance. This is accomplished through two key steps: first, aligning the generations from T\u00ba with the acoustic characteristics of Dsmall, and second, generating new captions to prompt T\u00ba for creating synthetic audio data. We elaborate on these next."}, {"title": "4.1 ALIGNING THE TEXT-TO-AUDIO MODEL USING PREFERENCE OPTIMIZATION", "content": "T2A models trained on internet-scale data often generate audio that diverges from the characteristics of small-scale datasets, resulting in distribution shifts. These mismatches can include variations in spectral (e.g., frequency content), perceptual (e.g., pitch, loudness), harmonic, or other acoustic characteristics\nTo address these issues, we propose the concept of aligning teaching with learning preferences. Our approach assumes that the classification model (viewed as the student) performs better when trained on synthetic audio that closely matches the inherent acoustic properties of our high-quality and human-labeled Dsmall. Thus, we align the generations of the T2A model (viewed as the teacher) to Dsmall, ensuring that the generated augmentations align with the desired characteristics and sound similar, ultimately enhancing the student model's ability to generalize to similarly characterized test data. As shown in Fig. 2, we achieve this using preference optimization (DPO in our case) and align generations of T\u00ba with Dsmall. Unlike standard fine-tuning, which can lead to less diverse outputs and overfitting due to a narrow focus on minimizing loss, preference optimization encourages greater exploration in the model's output space, preventing mode collapse and fostering more diverse augmentations. Additionally, DPO leverages pairwise learning, offering richer training signals compared to the independent outputs used in standard fine-tuning, further mitigating overfitting risks. We detail our two-step approach for DPO optimization below:\nStep 1: Construction of the Preference Dataset. To create our preference dataset Dpref = {(a, a\u2081), ..., (a, a')}, we first generate template-based captions for each instance in Dsmall in the form: \"Sound of a label\", where label is the category associated with the audio. For each instance, we prompt the T2A model j times, with all generations starting from randomly initialized Gaussian noise (generation configuration is detailed in Section 5). Each generated audio is then paired with the corresponding ground-truth audio from the gold dataset. This resulting Dpref dataset has n \u00d7 j instances, where the generated audio is treated as the \"loser\" and the ground-truth audio as the \"winner\". This simple approach has proven highly effective in aligning generations by generative models by prior work (Majumder et al., 2024; Tian et al., 2024).\nStep 2: Preference Optimization Using DPO. After constructing Dpref, we train our T2A model on this dataset with DPO using the approach outlined in Section 3. The resulting aligned model is referred to as Tain. Details of the hyper-parameters used for training are provided in Section 5."}, {"title": "4.2 GENERATING DIVERSE SYNTHETIC AUGMENTATIONS", "content": "It is not well-studied in the literature on how to leverage synthetic audio generation for downstream tasks. The only existing work relied on manually crafted prompt templates (e.g., \"Sound of a {label}\") (Ronchini et al., 2024). It has a significant limitation: there is no precise control over the specific components in the generated audio for a given caption. This can result in repetitive or completely inconsistent patterns, particularly with weaker T2A models 3. These could bias the model to learn spurious correlations, a known issue in synthetic data augmentation (Ghosh et al., 2024c).\nWhile the alignment stage helps the T2A model generate audio with acoustic characteristics similar to the small-scale dataset (e.g., spectral, harmonic, etc.), it does not fully account for the compositional diversity of the generated audios (e.g., sound events, their temporal relationships, background elements). To tackle this, we propose the concept of language-guided audio imagination, where we propose to imagine novel audios guided by language. Specifically, we leverage the reasoning abilities of LLMs to generate diverse and meaningful captions for a category label in a controlled yet scalable manner. These captions are then used to prompt our aligned T2A model for generating novel audios."}, {"title": "4.2.1 GENERATING DIVERSE PROMPTS WITH MIXCAP", "content": "We propose MixCap, a prompt generation method that creates diverse and effective captions in three steps: First, we employ GAMA (Ghosh et al., 2024a) to caption all audio files in Dsmall. Next, we prompt an LLM to extract phrases describing the acoustic components of the audio. These components correspond to the acoustic elements such as backgrounds and foreground events, and their attributes and relations, etc (see prompt in Appendix A.2). Finally, for each training instance in Dsmall, we prompt the LLM with the ground-truth label and the extracted components from all instances to generate N diverse audio captions that blend existing and new components. This approach prevents overemphasis on events already present in Dsmall and encourages diversity in the generated audio. These captions are then used to prompt the aligned T2A model to generate synthetic data Dsyn."}, {"title": "4.2.2 FILTERING & SELF-REFLECTION", "content": "Filtering. After generating captions and their corresponding audio, we filter the audio for label consistency. While LLMs can generate diverse captions, the audio produced must remain aligned with the ground-truth label. To ensure this, we use CLAP to evaluate the generated audio, accepting those that meet a similarity threshold of p% and rejecting the rest. We denote the accepted audios as Dan and the rejected ones as Dren. The CLAP model is pre-trained on Da-c and fine-tuned on Dsmall to adapt to the target dataset. Example captions are in Table 6, and prompts are in Appendix A.2.\nSelf-Reflection. For the rejected audios in Dre Dsyn, we prompt the LLM to reflect on its generated captions and revise them to better align with the target label. Precisely, we feed the LLM with the original caption of each rejected audio along with extracted components from all accepted captions in Dayn and task it to rewrite the rejected captions. The revised captions are then used to generate new audio, which is again filtered using CLAP. Audios that meet the threshold are added to Dsyn;"}, {"title": "5 EXPERIMENTAL SETUP", "content": "Models and Hyper-Parameters. For our T2A model, we choose the Stable Audio architecture (Evans et al., 2024b). Stable Audio is based on the Diffusion Transformer architecture, and we refer our readers to the original paper for more details. We train the model from scratch on Sound-VECaps (Yuan et al., 2024) (with \u22481.5 million weakly captioned audio-caption pairs) to avoid any data leakage. For training, we employ a batch size of 64, an AdamW optimizer, a learning rate of 5e-4, and a weight decay of le-3 for 40 epochs. For DPO-based alignment tuning, we fine-tune with a batch size of 32 and a learning rate of 5e-4 for 12 epochs. For our audio classification model, we employ the Audio Spectrogram Transformer (AST) (Gong et al., 2021) (pre-trained on the AudioSet dataset) and fine-tune it with a batch size of 24 and learning rate of le-4 for 50 epochs. In each experiment, we adjust the number of generated augmentations N (ranging from 1 to 5) based on performance on the validation set. All hyper-parameters are fixed across splits, and results are averaged across 3 runs.\nDatasets. We create small-scale datasets by downsampling commonly used audio classification datasets to n samples. Our selected datasets include a mix of music, everyday sounds, and acoustic scenes. For multi-class classification, we use NSynth Instruments, TUT Urban, ESC50 (Piczak), USD8K (Salamon et al., 2014), GTZAN (Tzanetakis et al., 2001), Medley-solos-DB (Lostanlen & Cella, 2017), MUSDB18 (Rafii et al., 2017), DCASE Task 4 (Mesaros et al., 2017), and Vocal Sounds (VS) (Mesaros et al., 2017), evaluating them for accuracy. For multi-label classification, we use the FSD50K (Fonseca et al., 2022) dataset and evaluate it using the Fmacro metric. We exclude AudioSet from evaluation as Sound-VECaps is derived from it. To ensure a downsampled dataset that has a label distribution similar to that of the of the original dataset, we employ stratified sampling based on categories. Our experiments are conducted with n = {50, 100, 200, 500} samples, and we downsample the validation sets for training while evaluating all models on the original test splits.\nBaselines. Our baselines include: (i) Gold-only (No Aug.): We employ only the small-scale dataset for training and do not perform any augmentations. (ii) Traditional augmentation baselines: SpecAugment, Noise Augmentation (we either add random Gaussian noise or background noise from AudioSet and present averaged results), Pitch and Time Shift and Audiomentations (Jordal, 2021) a combination of the AddGaussianNoise, TimeStretch, PitchShift, Shift, SpecFrequencyMask, TimeMask and TimeStretch \u2013 combination with the highest average score on 4 datasets and splits and was selected after grid search over all possible combinations). (iii) Generative baselines: Vanilla Synthetic Augmentation (Vanilla Syn. Aug.) we prompt To with template captions), Vanilla Syn. Aug. + LLM Caps \u2013 we prompt To with random captions generated with LLMs. (iv) Finally, inspired by Burg et al. (2023), we also employ a retrieval baseline where instead of generating augmentations from our T2A model trained on Da-c, we just retrieve the top-n instances (w.r.t. CLAP similarity) from the AudioSet for each instance in Dsmall as our augmentations.\nAblations. We ablate Synthio with: (i) w/o Self-Reflection: We remove the repetitive self-reflection module and iterate and filter only once; (ii) w/o DPO: We skip the tuning step and prompt the un-alined T\u00ba for augmentations; (iii) w/ ERM: We replace DPO tuning with standard Empirical Risk Minimization(ERM)-based fine-tuning with diffusion loss; (iv) w/ Template Captions: We remove MixCap and self-reflection modules and prompt Tain with template captions; (v) w/o MixCap: Similar to our Random Captions baseline, but we retain all other modules of Synthio."}, {"title": "6 RESULTS AND DISCUSSION", "content": "In this section, we present our main benchmark results, followed by detailed analyses to highlight Synthio's strengths and explore its versatility. Key areas of investigation include Synthio's consistency and diversity in augmentations, performance on synthetic-only datasets, scalability, impact on long-tailed categories, and its potential extension to complex audio understanding tasks like captioning."}, {"title": "6.1 How CONSISTENT AND DIVERSE ARE AUGMENTATIONS GENERATED BY SYNTHIO?", "content": "Fig. 4 compares the distributions of pitch and various spectral features between generated audios in Dsyn and real audios in Dsmall across different methods on the USD8K and NSynth datasets. The features analyzed include Pitch Salience (clarity of the main pitch) (Ricard, 2004), Spectral Flatness (tonal vs. noise-like quality) (Peeters, 2004), Flux (rate of spectral change) (Tzanetakis & Cook, 1999), and Complexity (level of sound detail) (Laurier et al., 2010). Notably, Synthio-generated audios closely replicate the spectral features of the original audios, showing the best alignment among all methods and demonstrating Synthio's ability to generate consistent augmentations. Table 2 presents CLAP similarity scores between ground-truth audios and their N generated augmentations, averaged across all dataset instances. Audios generated with Synthio achieve the highest compositional diversity for generated audios among all baselines. Table 8 shows that audios generated using Synthio have the highest similarity with the ground-truth category label."}, {"title": "6.2 HOW GOOD ARE SYNTHETIC AUDIOS GENERATED BY SYNTHIO?", "content": "Consistent with prior findings in vision (He et al., 2023), we observe that synthetic data alone performs sub-optimally compared to human-annotated data. However, our results show that enhancing the consistency and diversity of synthetic data aided by a small-scale version of the target dataset significantly improves model performance. Table 3 compares models trained exclusively on synthetic data with our baselines (i.e., only Dsyn is used for training AST). Synthio outperforms all baselines by 0.1%-26.25%, with DPO-based alignment driving the improvements."}, {"title": "6.3 CAN SYNTHIO BE EXTENDED TO THE MORE COMPLEX AUDIO CAPTIONING TASK?", "content": "Audio captioning, unlike classification, involves describing the content of an audio sample using natural language, making it a more complex task. To demonstrate Synthio's effectiveness for audio captioning, we evaluated it on down-sampled versions of AudioCaps. For this task, we adapted Synthio by extracting components directly from the available captions and retrained our T2A model on a modified version of Sound-VECaps, excluding any audio from AudioSet."}, {"title": "7 CONCLUSION, LIMITATIONS, AND FUTURE WORK", "content": "We introduced Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Synthio incorporates several innovative components to generate augmentations that are both consistent with and diverse from the small-scale dataset. Our extensive experiments demonstrate that even when using a T2A model trained on a weakly-captioned AudioSet, Synthio significantly outperforms multiple baselines.\nHowever, Synthio has some limitations: (i) Its performance is influenced by the capabilities of the T2A model and the quality of its training data. As T2A models continue to improve, we expect Synthio's performance to benefit accordingly. (ii) The process of generating audio captions using LLMs may introduce biases inherent in the LLMs into the training process. (iii) Synthio is computationally more intensive than traditional augmentation methods due to the need for prompting LLMs and T2A models. We anticipate that ongoing advancements in model efficiency will help mitigate these computational challenges."}, {"title": "A APPENDIX", "content": "Table of Contents:\n\u2022 A.1 Background on Diffusion Models\n\u2022 A.2 Prompts\n\u2022 A.3 Examples\n\u2022 A.4 Extra Results\n\u2022 A.5 Dataset Details"}, {"title": "A.1 DIFFUSION MODELS", "content": "Diffusion models consist of two main processes: a forward process and a reverse process. Given a data point 10 with probability distribution p(x0), the forward diffusion process gradually adds Gaussian noise to xo according to a pre-set variance schedule \u03b2\u2081,\uff65\uff65\uff65, \u03b2r and degrades the structure of the data. At the time step t, the latent variable xt is only determined by the Xt-1 due to its discrete-time Markov process nature, and can be expressed as:\np(xt | Xt-1) = N(xt; \u221a1 \u2212 \u1e9etxt\u22121, \u1e9etI), (12)\nAs t increases over several diffusion steps, p(x) approaches a unit spherical Gaussian distribution. The marginal distribution of xt at any given step can be expressed analytically as:\np(xt | xo) = N(xt; \u221aatxo, (1 \u2013at)I), (13)\nwhere at = \u03a0=1(1 \u2212 \u03b2\u03b5). The reverse process aims to reconstruct the original data from the noise-corrupted version by learning a series of conditional distributions. The transition from xt to Xt-1 is modeled as:\nPo(Xt\u22121 | Xt) = N(xt-1; \u03bc\u03bf\u00b9, \u03c3\u03bf1), (14)\n\u03bc\u03b8t-1=\nBt\u221a1 - \u0101t\n\u20ac0 (xt,t)\n\u03c3\u03b8t-1=\n1-at-1\u03b2t,\n1-\u0101t (15)\n(16)\nwhere at = 1 \u2212 \u03b2t, &t = \u03a0\u2081=1 \u03b1\u03af, \u03b8 represents the learnable parameters, \u03bc\u03cc\u00af\u00b9 is the mean estimate,\u03c3\u03b8t-12 is the standard deviation estimate, and eo(xt, t) is the noise estimated by the neural network.The reverse process estimates the data distribution p(x0) by integrating over all possible paths:\nPo(xo) =\n[Po(XT) Po(Pt-1| Xt)dx1 : T (17)\nt=1"}, {"title": "A.2 PROMPTS", "content": "Fig. 6, 7, 8 and 9 illustrate all the prompts used in our experiments. For all experiments, we prompt GPT-4-Turbo (GPT-4-turbo-2024-04-09) with top-p=0.5 and temperature=0.7."}, {"title": "A.4 EXTRA RESULTS", "content": "A.4.1 RESULTS ON THE FULL TRAINING SPLITS\nTable 7 presents the performance comparison of Synthio on the full original dataset splits (where the entire training set is used without any downsampling). While Synthio outperforms all baselines, traditional augmentation methods prove to be much more competitive in this scenario. This contrasts"}, {"title": "A.4.2 AUDIO GENERATION RESULTS FOR OUR TRAINED STABLE DIFFUSION", "content": "Table 9 presents a comparison of audio generation results across several evaluation metrics. We evaluate our trained Stable Diffusion model (used in our experiments, including a version further fine-tuned on AudioCaps) against other available models and baselines from the literature. Notably, our model performs competitively with other fully open-source models across most metrics."}, {"title": "A.4.3 FAD SCORES FOR GENERATED AUGMENTATIONS", "content": "To offer an alternative perspective on the distributional consistency between the generated augmentations and the ground-truth small-scale dataset, we compare the Fr\u00e9chet Audio Distance (FAD) scores (Kilgour et al., 2018). For this experiment, we use Synthio with Template Captions. Table 10 presents a comparison of FAD scores between Synthio and other baselines. Synthio achieves the highest FAD score, indicating that it produces the most consistent audio augmentations."}, {"title": "A.5 DATASET DETAILS", "content": "NSynth Instruments: NSynth is a large-scale dataset consisting of musical notes played by a variety of instruments. It includes a rich set of acoustic features from instruments like guitars, flutes, and more, providing diverse sound textures for classification tasks.\nTUT Urban: The TUT Urban dataset captures everyday sounds from urban environments, including noises like traffic, human activities, and construction. It is commonly used for acoustic scene classification and environmental sound recognition.\nESC-50: ESC-50 is a well-known dataset for environmental sound classification, containing 50 categories of everyday sounds such as animal noises, natural elements, and human activities, making it suitable for multi-class classification challenges."}]}