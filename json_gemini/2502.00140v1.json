{"title": "Demystifying MPNNs: Message Passing as Merely Efficient Matrix Multiplication", "authors": ["Qin Jiang", "Chengjia Wang", "Michael Lones", "Wei Pang"], "abstract": "While Graph Neural Networks (GNNs) have achieved remarkable success, their design largely relies on empirical intuition rather than theoretical understanding. In this paper, we present a comprehensive analysis of GNN behavior through three fundamental aspects: (1) we establish that k-layer Message Passing Neural Networks efficiently aggregate k-hop neighborhood information through iterative computation, (2) analyze how different loop structures influence neighborhood computation, and (3) examine behavior across structure-feature hybrid and structure-only tasks. For deeper GNNs, we demonstrate that gradient-related issues, rather than just over-smoothing, can significantly impact performance in sparse graphs. We also analyze how different normalization schemes affect model performance and how GNNs make predictions with uniform node features, providing a theoretical framework that bridges the gap between empirical success and theoretical understanding.", "sections": [{"title": "1. Introduction", "content": "Graph Neural Networks (GNNs) are considered to be powerful in learning on graph-structured data, particularly through their iterative neighbor aggregation mechanism.\nDespite their widespread adoption as feature extractors for graph data, fundamental questions about GNNs' representational capabilities remain open (Dehmamy et al., 2019). The design of new GNN architectures often relies on empirical intuition and heuristics rather than theoretical foundations (Xu et al., 2019).\nWhile GNNs integrate both structural and feature information for predictions, our understanding of how these components interact and influence the final predictions remains limited. It is commonly assumed that a k-layer GNN effectively synthesizes both structural and feature information by aggregating data from progressively larger neighborhoods. However, our research reveals a more nuanced reality: when increasing from k to (k + 1) layers, the layer-wise iterative aggregation process effectively substitutes information from k-hop neighbors with that of (k + 1)-hop neighbors, rather than building a cumulative representation as previously thought. This is because graph loops lead to the coexistence of multi-hop neighbors in k-hop neighbors.\nThis paper demystifies Message Passing Neural Networks (MPNNs) by revealing their fundamental nature: the message passing process is, at its core, a memory-efficient implementation of matrix multiplication operations. Through this lens, we demonstrate three key insights:\n(1) In Section 2, we establish that a k-layer MPNN transforms node representations by iteratively aggregating information from k-hop neighborhoods.\nMore precisely, we prove an approximation equivalence: a k-layer MPNN operating with adjacency matrix A is approximately equivalent to a single-layer MPNN operating with adjacency matrix $A^k$.\nThis result not only provides a formal characterization of how message passing depth relates to neighborhood influence in GNNs, but also reveals a computational advantage: while direct computation of $A^k$ requires storing the full power matrix and can exceed memory constraints for large graphs, the iterative message passing in GNNs achieves equivalent neighborhood aggregation through memory-efficient layer-wise operations.\n(2) In Section 3, we analyze how different types of graph loops affect k-hop neighborhood computation, as loops create additional paths between nodes and thus increase the density of k-hop neighborhoods.\n(3) Finally, in Section 4, we examine MPNN behavior across structure-feature hybrid tasks and structure-only tasks, revealing their underlying similarity: structure-only tasks are essentially structure-feature hybrid tasks where node degrees serve as the node features.\nWe challenge the conventional wisdom about deeper GNNs' performance degradation: contrary to the common over-smoothing (Rusch et al., 2023) explanation, we experimentally demonstrate that gradient-related issues can be the primary cause for sparse graphs. In addition, we explain how GNNs predict with uniform features and how different normalization schemes fundamentally influence their performance.\nIn summary, our work provides a theoretical foundation for understanding GNN behavior through three key aspects: the relationship between network depth and neighborhood aggregation, the impact of graph loop structures, and the role of gradients in deep architectures, normalization influence. These theoretical insights not only bridge the gap between empirical success and mathematical understanding but also provide practical guidance for GNN architecture design and deployment across various applications.\nThe code for the experiments conducted in this paper is available at https://anonymous.4open.science/status/demystify-B30E."}, {"title": "Notation and definitions", "content": "A graph $G = (A, X)$ is a set of N nodes connected via a set of edges. The adjacency matrix of a graph A encodes graph topology, where each element $A_{ij}$ represents an edge from node i to node j. In this paper, edges are directed, the undirected graph is considered to be a special case of directed graph where all edges have their reversed edges in the graph. Each node i is assigned a feature vector $x_i \\in \\mathbb{R}^d$, and all the feature vectors are stacked to a feature matrix $X \\in \\mathbb{R}^{n\\times d}$, where n is the number of nodes in G. The set of neighbors of node i is denoted by $\\mathcal{N}(i)$.\nWe use $AB$ or $A \\cdot B$ to denote the matrix product of matrices A and B. All multiplications and exponentiations are matrix products, unless explicitly stated. Lower indices $A_{ij}$ denote i, jth elements of A, and $A_i$ means the ith row. $A^p$ denotes the pth matrix power of A."}, {"title": "2. k-layer GNNS", "content": ""}, {"title": "2.1. k-order features", "content": "Definition 2.1. The k-hop neighbor of a node v in a graph G = (V, E) is any node u \u2208 V such that there is a directed path of k consecutive edges from node u to node v.\nDefinition 2.2. A kth order node feature, defined as $A^k X$, represents the result of multiplying the adjacency matrix A with itself p times and then multiplying with the node feature matrix X. Particularly, 0th order node feature is the original node feature.\nLemma 2.3. For a graph $G = (V, E)$ with adjacency matrix A and node feature matrix X, the features aggregated from p-hop neighbors of each node are equivalent to the kth order node feature $A^k X$.\nRemark 2.4. $A^kXW$ is a linear transformation of k-hop neighbor features $A^kX$ using weight matrix W.\nLemma 2.5. In the k-th power of the adjacency matrix $A^k$, a non-zero element $A^k_{ij} > 0$ indicates that there exists at least one directed path of length exactly k from node i to node j. Furthermore, the value of $A^k_{ij}$ represents the total number of such paths.\nRemark 2.6. The kth order node feature gathers information from nodes which are exactly k-hop away from the center node, as illustrated in Fig. 1."}, {"title": "2.2. Node representation of k-layer GNNS", "content": "Lemma 2.7. For all natural numbers k, the output of a k-layer GCN without self-loops can be expressed as:\n$H^{(k)} = \\sigma((W\\odot A)^k X W^{(k)})$                                                                              (1)\nLemma 2.8. For all natural numbers k, the output of a k-layer GCN with self-loops can be expressed as:\n$H^{(k)} = \\sigma((W\\odot (A+I))^k X W^{(k)})$                                                                  (2)\n$(A + I)^k$ can indeed be decomposed into a linear combination of powers of A, as described by the binomial theorem:\n$(A + I)^k = \\sum_{l=0}^{k} \\binom{k}{l} A^l$\nWhere $\\binom{k}{l}$ is the binomial coefficient.\nTherefore, the final representation $H^{(k)}$ is a linear combination of the feature transformations derived from paths of lengths ranging from 0-th to k-th order, capturing information aggregated over different scales in the graph."}, {"title": "2.3. Summary of k-layer GNNS", "content": "In summary, for a k-layer GNN, both GCN with self-loops and GraphSAGE integrate information from all neighborhood orders up to k. In contrast, a GCN without self-loops incorporates information solely from the k-th order neighborhood, as lower-order features are excluded in the absence of self-loops.\nThe approximation capabilities of graph neural networks (GNNs) reveal that a k-layer GNN with an adjacency matrix A has the same approximation power as a 1-layer GCN with the adjacency matrix $A^k$. This observation demystifies the iterative aggregation power of message-passing neural networks (MPNNs).\nIn essence, multiple iterations of aggregation are equivalent to performing high-order matrix multiplications.\nHowever, adding self-loops (as in GCNs with self-loops) or concatenating self-node features (as in GraphSAGE) incorporates features of all orders. While this can enhance the expressiveness of the model, it may also lead to over-smoothing, ultimately limiting the depth of GNNs and their ability to capture meaningful representations in deeper architectures."}, {"title": "3. Loops", "content": "In Section 2, we discussed the influence of self-loops in GCNs. In this section, we will extend our discussion to consider all types of loops in graph neural networks and analyze their effects."}, {"title": "3.1. Self-loops", "content": "Sources of self-loops include:\n1. Original Graph: In some networks, such as webpage networks, a node (e.g., a webpage) might naturally link to itself.\n2. GNN Model Design: Many GNN models, such as GCN (Kipf & Welling, 2016) and DiG(ib) (Tong et al., 2020), explicitly add self-loops to improve performance, particularly on homophilic graphs.\nAs discussed in Sec. 2.8, a k-layer GCN with self-loops would gather information from neighbors within the range of 0-hop to k-hop neighbors. This fact was established via matrix multiplication. In this section, we will prove it geometrically.\nLemma 3.1. When self-loops are added to a graph, the k-hop neighbors of any node are also its (k + 1)-hop neighbors.\nThe proof is provided in Appendix B.1. This path-based property can be expressed in terms of the adjacency matrix:\nLemma 3.2. Let G be a graph with self-loops. Then for any k \u2265 1, any connection present in $A^k$ is also present in $A^{k+1}$."}, {"title": "3.2. Two-node Loops", "content": "A directed graph where each pair of connected nodes has edges in both directions (making its adjacency matrix symmetric) can be viewed as an undirected graph. In other words, an undirected graph is equivalent to a directed graph where every edge is bidirected.\nLemma 3.3. For an undirected graph, for any k \u2265 1, the k-hop neighbors of any node are also its (k + 2)-hop neighbors.\nThe proof is provided in Appendix B.2. This property can be expressed in terms of the adjacency matrix:\nLemma 3.4. Let G be an undirected graph. Then for any k \u2265 1, any connection present in $A^k$ is also present in $A^{k+2}$"}, {"title": "3.3. Multi-node Loops", "content": "Lemma 3.5. For a graph containing a loop of length m, let v be any node in the graph. For any k \u2265 1, if u is a k-hop neighbor of v where the k-hop path from u to v contains at least one node from the loop, then u is also a (k + m)-hop neighbor of v.\nThe proof is provided in Appendix B.3. This path-based property can be naturally expressed in terms of the adjacency matrix of the graph:"}, {"title": "3.4. Longest path", "content": ""}, {"title": "3.4.1. FOR DIRECTED GRAPH", "content": "Lemma 3.7. For a directed graph with adjacency matrix A, if the graph contains no loops (cycles) and h is the length of the longest simple path, then:\n$A^m = 0$ for all m > h\nThe proof is provided in Appendix B.4."}, {"title": "3.4.2. FOR UNDIRECTED GRAPH", "content": "Lemma 3.8. Let G be an undirected graph, and let h be the length of the longest path in G. Then for any m > h, the connections present in $A^m$ are identical to those in $A^h$.\nThe proof is provided in Appendix B.5."}, {"title": "3.5. Loops Influence", "content": "Different types of graph structures influence how connectivity patterns evolve as we take higher powers of the adjacency matrix. Self-loops allow paths to extend by single steps, while two-node loops (undirected edges) enable extension by pairs of steps. More generally, any m-node loop allows paths to extend by m steps while preserving all existing connections.\nFor the maximal path length, undirected and directed graphs behave quite differently. In undirected graphs, paths can always extend beyond the spanning tree's longest path length while maintaining the same connectivity pattern. However, in directed acyclic graphs, no paths can exist beyond this length."}, {"title": "4. Structure-Feature Dichotomy in Node Classification", "content": "Graph Neural Networks (GNNs) combine node features and graph structure for predictions. However, recent work shows structure-agnostic models like MLPs outperform GNNs on certain datasets (e.g., WebKB (Zheng et al., 2022)). Complementing this finding, we show that some node classification tasks perform equally well without node features. Based on this Structure-Feature Dichotomy, we categorize tasks into three types: feature-only, structure-only, and hybrid. We then analyze how GNNs make predictions for the latter two cases."}, {"title": "4.1. Structure-feature Hybrid Type", "content": "Citation networks like Cora, CiteSeer, and PubMed represent classic node classification tasks where research papers are categorized by their topics. While individual papers may not contain comprehensive field-specific content, aggregating features from neighboring nodes can enrich the representation of each paper's research domain, making GNNs particularly effective for this task.\nFigure 4 shows consistent patterns across CiteSeer, CoraML, and PubMed datasets comparing three approaches: (1) increasing GNN layers with first-order neighbors (blue line), (2) single-layer GNN with increasing k-hop neighbors (red line), and (3) k-hop neighbors with (k-1) additional linear layers (green line). The black line represents the density of the effective adjacency matrix-the percentage of non-zero elements after k-hop expansion. The low density values across all datasets indicate these are sparse directed networks.\nThe single-layer GNN with increasing k-hop neighbors maintains stable performance, while both the deep GNN and the hybrid approach show significant performance degradation with increasing depth. While both architectures access k-hop neighborhood information\u2014through $A^k$ in single-layer GNN and k successive applications of A in k-layer GNN-their empirical performance differs substantially despite theoretical equivalence in terms of Universal Approximation (Section 2.3).\nThe fact that performance remains stable when increasing the neighborhood size k in a single-layer architecture (red line) indicates minimal over-smoothing in this case. Thus, we hypothesized that gradient-related issues might be the primary cause of performance degradation in deeper networks.\nTo test this hypothesis, we designed approach (3) which combines k-hop neighborhood aggregation with (k-1) additional linear layers. This architecture shares parameter count with the deep GNN while using expanded neighborhoods like the single-layer approach. The deteriorating performance of this hybrid approach parallels that of the deep GNN, strongly suggesting that gradient-related issues, rather than over-smoothing, cause the performance drop in deep GNNs.\nWe further investigated Reverse Direction and Bidirectional Propagation, with experimental results on CiteSeer presented in Figure 5. For Reverse Direction Propagation, we observed performance trends similar to forward direction propagation. However, Bidirectional Propagation exhibited distinct behavior: increasing the neighborhood size k in single-layer architectures (red line) led to performance degradation, which can be attributed to over-smoothing since the connection density inversely correlates with k. The connection density (black line) and performance with increasing k (red line) both stabilized after k=17.\nAs shown in Figure 5b, k-layer GNNs with first-order neighbors (blue line) performed worse than single-layer models with equivalent k-hop neighbors (red line), likely due to the compound effects of over-smoothing and vanishing gradients."}, {"title": "4.2. Structure-only Type", "content": "In this section, we will present three datasets which work well without node features, where all nodes have uniform features.\nAs shown in Table 2, for Chameleon, Squirrel and Telegram"}, {"title": "4.2.1. NORMALIZATIONS", "content": "Graph normalization, which typically involves dot multiplication of the adjacency matrix to adjust edge weights, plays a crucial role in graph neural networks (GNNs). While various normalization schemes exist, their theoretical implications remain under-explored. We denote a general normalization function as $f(A)$.\nNo Normalization The simplest approach is to use the raw adjacency matrix without any normalization (Li et al., 2017): $f_1(A) = A$. In this case, the node feature update rule becomes:\n$h_i^{(l+1)} = \\sigma(\\sum_{j \\in \\mathcal{N}(i)} W^{(l)} h_j^{(l)})$\nThe aggregation directly sums neighboring features, leading to larger feature magnitudes for higher degree nodes. With homogeneous features $h_i^{(0)} = 1$, node representations become proportional to degrees.\nWhile this makes it suitable for degree-dependent tasks like traffic prediction and network flow classification, repeated aggregation of unnormalized features can cause numerical instability. The node representations may grow or vanish exponentially with network depth. This numerical instability explains the suboptimal performance of unnormalized adjacency matrices compared to normalized variants in Table 2. The exponential growth or decay of node representations across layers likely hindered the model's ability to learn effective graph representations, despite preserving the degree information.\nRow Normalization Row normalization (Hamilton et al., 2017) scales each row of the adjacency matrix by the inverse of node degree: $f_2(A) = D^{-1} A$. The node feature update rule becomes:\n$h_i^{(l+1)} = \\sigma(\\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{d_i} W^{(l)} h_j^{(l)})$\nFor this formulation, the aggregated information represents the mean of neighboring features rather than their sum. Node degrees no longer directly influence feature magnitudes. With homogeneous features $h_i^{(0)} = 1$, all nodes get identical representations. This explains the poor traffic prediction in Table 2-degree information is lost."}, {"title": "Symmetric Normalization", "content": "Symmetric normalization (Kipf & Welling, 2016) applies: $f_3(A) = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$. The node feature update rule becomes:\n$h_i^{(l+1)} = \\sigma(\\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{d_i d_j}} h_j W^{(l)})$\nThe neighbor's influence is determined by both degrees-if a neighbor's degree is much larger than the center node's, its feature weight becomes smaller than in row normalization.\nDirected Normalization For directed graphs, Rossi et al. (2024) proposes: $f_4(A) = D_{in}^{-\\frac{1}{2}} A D_{out}^{-\\frac{1}{2}}$. The node feature update rule becomes:\n$h_i^{(l+1)} = \\sigma(\\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{d_{in} d_{out}}} h_j W^{(l)})$\nThis distinguishes between in-degree and out-degree for more accurate normalization in directed graphs.\nIn summary, row normalization of adjacency matrices, when applied with uniform node features, results in loss of structural information since all nodes become indistinguishable. While using the unnormalized adjacency matrix preserves both degree information and feature distinctions, it can lead to numerical instability since the eigenvalues of f(A) may grow or diminish exponentially, rather than being bounded within [-1, 1]. This instability can affect the training of graph neural networks."}, {"title": "4.2.2. How GNN PREDICT FOR STRUCTURE-ONLY CLASSIFICATION", "content": "Fig. 6 demonstrates that increasing the number of GCN layers leads to better accuracy, while increasing the power k of the adjacency matrix (k-hop neighborhoods) results in poor predictive performance on the Telegram dataset."}, {"title": "4.3. Summary", "content": "In this section, we further analyze MPNN predictions through the structure-feature dichotomy. Node classification datasets can be divided into a structure-feature dichotomy, where some datasets perform better with MLPs. For datasets where GNNs are effective, predictions can be made with or without node features, determined by the task type.\nStructure-feature Hybrid Tasks Content-based classification problems, exemplified by citation networks, require integration of both node features and neighborhood features propagated through structural connections. Our analysis reveals that the commonly observed performance degradation in deeper layers, traditionally attributed to over-smoothing, may instead stem from gradient-related challenges, especially for sparse networks.\nStructure-only tasks In tasks such as traffic prediction and network flow classification, MPNNs can make predictions using purely structural information. We show that when nodes have uniform features, the prediction mechanism shifts-node degree becomes the effective feature, and a (k+1)-layer MPNN with uniform features predicts equivalently to a k-layer MPNN using node degree as the sole feature.\nWe also prove that combining featureless inputs with row normalization leads to degenerate predictions where MPNNs learn nothing.\nIn sum, structure-only tasks can be considered as a special type of Structure-feature Hybrid Tasks, where the node degree act as node feature."}, {"title": "5. Conclusions", "content": "This work demystifies Message Passing Neural Networks by revealing their computational essence: the message passing process is fundamentally a memory-efficient implementation of matrix multiplication operations. We establish that k-layer MPNNs aggregate information from k-hop neighborhoods through iterative computation, making them practical for large graphs where direct computation of powered adjacency matrices would be prohibitively expensive.\nThrough careful analysis of loop structures, we theoretically characterize how different types of loops influence k-hop neighborhood density. We demonstrate that common GNN practices, such as adding self-loops and converting directed graphs to undirected ones by adding reverse edges, significantly increase k-hop neighborhood density, potentially leading to over-smoothing.\nOur analysis challenges two common misconceptions in the field: (1) performance degradation in deeper GNNs is not necessarily due to over-smoothing. For sparse directed graphs, deeper architectures are less susceptible to over-smoothing due to low connection density, yet their performance degrades due to vanishing gradients and overfitting from accumulated weights; and (2) deeper GNN architectures do not necessarily lead to over-smoothing as long as loop structures don't create dense k-hop connectivity.\nFurthermore, we explained how GNNs work in structure-feature hybrid tasks and how for structure-only tasks, the node degree becomes the actual feature.\nThese insights offer theoretical understanding of how GNNs work and provides practical guidance for GNN architecture design, particularly regarding the choice of directed versus undirected aggregation, whether to add self-loops, and the selection of normalization strategies."}]}