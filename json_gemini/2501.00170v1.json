{"title": "Federated Learning with Workload Reduction through Partial Training of Client Models and Entropy-Based Data Selection", "authors": ["Hongrui Shi", "Valentin Radu", "Po Yang"], "abstract": "With the rapid expansion of edge devices, such as IoT devices, where crucial data needed for machine learning applications is generated, it becomes essential to promote their participation in privacy-preserving Federated Learning (FL) systems. The best way to achieve this desiderate is by reducing their training workload to match their constrained computational resources. While prior FL research has address the workload constrains by introducing lightweight models on the edge, limited attention has been given to optimizing on-device training efficiency through reducing the amount of data need during training. In this work, we propose FedFT-EDS, a novel approach that combines Fine-Tuning of partial client models with Entropy-based Data Selection to reduce training workloads on edge devices. By actively selecting the most informative local instances for learning, FedFT-EDS reduces training data significantly in FL and demonstrates that not all user data is equally beneficial for FL on all rounds. Our experiments on CIFAR-10 and CIFAR-100 show that FedFT-EDS uses only 50% user data while improving the global model performance compared to baseline methods, FedAvg and FedProx. Importantly, FedFT-EDS improves client learning efficiency by up to 3 times, using one third of training time on clients to achieve an equivalent performance to the baselines. This work highlights the importance of data selection in FL and presents a promising pathway to scalable and efficient Federate Learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) [27] has emerged as a prominent distributed learning paradigm that enables many distributed devices to collaborate in training machine learning models without sharing sensitive user data. At the same time, the widespread adoption of small edge computing devices, such as those in the Internet of Things (IoT), has led to an unprecedented surge in data generation much of it vital for advancing machine learning (ML) applications. However, training traditional models on resource-constrained devices, and the growing trend of adopting large foundation models across distributed systems, presents significant challenges to the efficiency and scalability of Federated Learning (FL). To address these challenges, there is a pressing need to optimize FL training processes to minimize the computational burden on edge devices. By reducing the workload of these devices, we can enable a broader range of resource-constrained devices to participate in FL, unlocking the potential of their untapped data sources.\nWhile previous works [3], [6], [7], [10], [18], [30] primarily focus on reducing the workload on devices by allowing clients to locally train a small part of the model or a subset of model parameters (submodel), the potential for enhancing on-device training efficiency by reducing the amount of training data required has been largely overlooked. In the space of centralized machine learning, active learning has been employed to prioritize data samples that can most effectively improve the model through additional training [19]. However, in the context of federated learning, active learning has been largely disregarded, primarily due to concerns about the perceived overhead it may impose on client workloads [17], [29].\nIn this work, we concurrently adopt two workload reduction strategies \u2013 partial model training and data selection \u2013 demonstrating their effectiveness in enhancing model training, accelerating convergence, and significantly reducing client workloads. These improvements enable federated learning across a wide range of devices.\nOur proposed method, Federated Fine-Tuning with Entropy-based Data Selection (FedFT-EDS), leverages transfer learning to enable efficient federated fine-tuning. We begin by pre-training a global model on a large, accessible source domain to establish a strong foundation for feature extraction. During the federated learning process, clients are tasked with fine-tuning only a smaller portion of the model on their local data, significantly reducing the computational burden. To further optimize the training process, we introduce entropy-based data selection. Clients perform a single forward pass on their local data to identify a small subset of the most informative samples, which are then used to update the model. This strategy minimizes the computational overhead associated with data selection. To enhance the quality of the selected subset, we employ a hardened softmax function to prioritize samples with high uncertainty, ensuring that the model benefits from the most valuable training examples.\nOur experiments on CIFAR-10 and CIFAR-100, conducted under non-IID data distribution, demonstrate that FedFT-EDS consistently outperforms popular FL baselines, FedAvg [27] and FedProx [22], and their variants with random data selection. FedFT-EDS achieves superior global generalization, improving performance by up to 5%. Moreover, it significantly enhances learning efficiency, requiring less than one-third of"}, {"title": "II. RELATED WORKS", "content": "Partial model training reduces workloads on clients by allowing them to train only a portion of the model rather than the full model. A series of FL works have proposed updating submodels of the global model on clients. These submodels are chosen to meet the local computational resources, thus supporting the participation of less capable devices.\nFjORD [10] uses a dropout strategy to extract submodels from the global model, which are then broadcast for client update. The dropout probabilities, which are used to indicate how many units are disconnected from the global model, are chosen according to the computational capabilities of each client. The pruned units do not participate in the local updates, thus reducing the training cost on clients. In a similar study, HeteroFL [6] reduces the size of each hidden layer of the global model through a determined ratio to extract its submodels. These ratios are determined based on the computational capability of each participating client. The server then aggregates the updated submodels keeping track of the position of each updated parameter in the global model. This concept works even for larger models, InclusiveFL [24] showing its efficacy on Transformer models.\nFedRolex [3] critiques FJORD and HeteroFL for unevenly training the parameters of submodels, which degrades global model performance under data heterogeneity. Instead, FedRolex uses a rolling window to extract a submodels between communication rounds, thus allowing each parts of the global model to be exposed to different local data distributions.\nRecent works [5], [39] have found that fine-tuning a classifier on top of a fixed feature extractor, pretrained on a set of base tasks, can achieve performance comparable to state-of-the-art few-shot learning approaches. We apply this insight from centralized learning to the context of FL."}, {"title": "B. Active Learning", "content": "Active Learning (AL) [35], [37], [38] is a technique designed to reduce the labeling effort required for creating large training datasets by identifying high-value data points for labeling to improve training quality. It is based on the premise that machine learning algorithms can maintain their learning effectiveness using a carefully selected subset of training samples [35]. AL solutions based on uncertainty assessment [4], [25], [42] are among the most popular techniques, characterized by their simplicity and low computational cost.\nModel prediction is often used as a proxy for data certainty, to determine the most uncertain data points for annotation and training. Marginal sampling [36] and information entropy [37] are two widely used techniques for measuring data certainty. Notably, the latter selects samples with the highest entropy for annotation and training [1], [13], [21], [26]. Conversely, recent studies [20], [28], [41] show that AL can enhance machine learning efficiency by reducing the size of the training dataset. While AL has been extensively studied in centralized learning settings, its utility extends to federated learning, though it has received comparatively less attention. The few studies on FL that incorporate AL, such as LoGo [14] and F-AL [2], focus on reducing the labeling burden on clients [12] rather than strategically selecting training data from the available and labeled local data.\nLi et al. [17] introduce the gradient upper bound norm on the global loss to calculate the importance scores of training samples. FLRD [29] learns a relevant data selector on the client side using reinforcement learning. The learned selector can identify local samples that are most beneficial for improving global model performance.\nFedEntropy [34] leverages entropy to select the most useful client models for global model aggregation. Each participating client computes the average entropy of its local data and uploads this value to the server. The server then eliminates clients that reduce the overall entropy, selecting a subset of clients that maximize global entropy. FedEntropy shows that combining models uploaded by this client model selection criteria improves global model performance. FedAvg-BE [33] also utilizes entropy, but for selecting batched local data to update client models. The batched local data comprises training samples with the highest entropy, which are most beneficial for learning. FedAvg-BE is the prior work most closely related to ours in its use of entropy for client data selection in FL. However, their focus is primarily on addressing the data heterogeneity challenge. In contrast, our work investigates the potential of entropy-based data selection for mitigating the straggler issue. Further, we argue that batch level entropy masks the utility of individual samples, so we focus on selecting the best individual samples for training by separately calculating entropy at sample level. Another distinguishing factor is our introduction of the hardened softmax activation to enhance the relevance of data selection by entropy.\nOur novel approach combines two client workload reduction strategies, data selection and partial model training, tackling"}, {"title": "III. PROPOSED METHOD", "content": "Figure 1 presents the workflow and Algorithm 1 the implementation of our proposed Federated Fine-Tuning with Entropy-based Data Selection (FedFT-EDS).\nWe introduce the standard FL problem setup for the preliminaries. For a client indexed by k, its local training dataset is denoted as $D_k$, comprising a set of samples represented as ${(x_i, y_i)}_{i=1}^{|D_k|}$, where $x_i$ and $y_i$ are the i-th local instance and its corresponding label respectively. The model trained in FL is called the global model, denoted as $M_g$, whose parameters are denoted as $w_g$. The learning objective is to find the $w_g$ that minimises the combined local losses across all clients, as described by Equation 1 below.\narg minL(wg) = \nwhere, N is the size of the client pool, $L_k(w_g)$ is the empirical local loss, $p_k = \\frac{|D_k|}{D}$, $D$ is the coefficient to weight individual losses, determined by the proportion of local data relative to the total client data $D = \\bigcup_{k\\in[N]} D_k$.\nFL benefits significantly from pretraining a global model in terms of generalisation, convergence, and fairness [31]. Our proposed entropy-based data selection method adopts the pretraining strategy for the initial global model prior to starting the FL update rounds. Concretely, the global model is first"}, {"title": "C. Local Updates of FedFT-EDS", "content": "FedFT-EDS performs local updates by fine-tuning a well-defined part of the model with local instances actively selected based on their entropy.\nFormally, at communication round t, client k first downloads the global model as $M_t$ parameterised with $w_t^g$. The client then selects local instances for training $M_t^k$. To achieve this, the client model performs a forward pass with all available client data $x_i \\in D_k, i = 1,\u2026\u2026,|D_k|$ to obtain their softmax activations as follows:\n$\nwhere $p_{t,k}^{(i)}$ is the probability vector output by the softmax activation layer. Then the Shannon entropy for an instance $x_k^{(i)}$ is calculated with Equation 3.\nH_{t,k}^{(i)} = - \\sum_{j=1}^{n}p_{t,k}^{(i)} (logp_{t,k}^{(i)}).\nwhere $p_{t,k}^{(i)}$ is the probability of labeling the instance $x_k^{(i)}$ as the j-th class in the possible outcomes ${C_1, C_2, ..., C_n}$.\nThe client k ranks instance $x_i \\in D_k, i = 1,\u2026\u2026,|D_k|$ based on the calculated entropy values. High entropy implies that the model is more uncertain about its prediction and vice versa. Therefore, samples with higher entropy are seen as harder but more valuable ones for learning than those with lower entropy. By ranking samples according to their entropy values, the client identifies a small subset of local data, $D_{k, select}$, which contains instances with higher entropy values that can update $w_t$ efficiently.\nUsing the pretrained global model, client k only fine-tunes the upper part of $M_t$ on $D_{k, select}$ while keeping the lower part frozen. Specifically, the parameters of the client model are denoted as $w_t^k = {\\phi, \\theta_t^k}$, where $\\phi$ is the feature extractor at the bottom of the model and $\\theta$ denotes the upper part. Here, $\\phi$ is not indexed with the communication round t nor with the client index k because it comes from the pretrained global model and all clients have the same copy for that part"}, {"title": "D. Global Updates of FedFT-EDS", "content": "FedFT-EDS builds on FedAvg to update the global model. Specifically, FedFT-EDS fuses the updated model parameters $\\theta_{t+1}^k, k \\in {1,..., K}$ as follows:\n$\\theta_{t+1} = \\frac{\\sum_{k=1}^{K}p_k\\theta_{t+1}^k}{\\sum_{k=1}^{K}p_k}$\nwhere, $p_k$ is calculated based on the selected client data $p = \\frac{|D_{k, select}|}{\\bigcup_{k\\in[K]} D_{k, select}}$. The server constructs the global model for the next communication round t+1 from the frozen part and from the updated part $w_{t+1} = {\\phi, \\theta_{t+1}}$ and distributes it to the clients.\nAs discussed in local updates, the feature extractor $\\phi$ is not updated during FL iterations. Hence, the server and the clients only need to communicate the upper part of the global model regularly, $\\theta$, which also reduces the communication overhead in FL. Algorithm 1 describes the details of the proposed FedFT-EDS."}, {"title": "E. Entropy Calculation with Hardened Softmax", "content": "In this section, we introduce the hardened softmax to enhance the efficacy of our data selection method. For a given local sample, the more confident the model prediction is (lower entropy), the less it contributes to learning. The hardened softmax increases the entropy of such confident samples, effectively excluding them from local updates, thus ensuring that only highly uncertain samples are selected for training.\nEquation 2 and Equation 3 describe how the entropy is calculated to support the local sample selection. However, a key limitation of Shannon entropy is that small changes in $p_{t,k}^{(i)}$ within the probability vector result in only minor changes in entropy [21]. This characteristic can restrict the effectiveness of Shannon entropy in identifying the most uncertain instances. Specifically, a slight increase in $p_{t,k}^{(i)}$ indicates that the model has become slightly more confident in classifying the instance into category j. Since the model gains less from training on instances it is already confident about, it would be preferable for a small increase in $p_{j,t,k}^{(i)}$ to cause a significant decrease in entropy, thereby excluding the instance from local updates.\nWhile we cannot alter the Shannon entropy defined by Equation 2, we can reshape the probability distribution in the probability vector in Equation 2 by amplifying the change in $p_{j,t,k}^{(i)}$. To address this limitation of entropy-based data selection, we employ the hardened softmax activation. The"}, {"title": "IV. EXPERIMENTS", "content": "1) Datasets and models: We evaluate FedFT-EDS on two image classification tasks using CIFAR-10, CIFAR-100 [16] and a speech classification task using the Google Speech Command dataset [40]. The global model is pretrained on the ImageNet Small 32 \u00d7 32 dataset prior to FL. The number of local update epochs E is set to 5. SGD optimiser with a learning rate of 0.1 and momentum of 0.5 is used for the local updates. The temperature in the hardened softmax activation is set to 0.1. The Wide ResNet (WRN) model [43] with the depth of 16 and width of 1 is used in our FL experiments. The client model is fine-tuned from layer 3, with layer 1 and layer 2 being fixed during local updates. Client data heterogeneity is also simulated in our experiments. Following many prior works [8], [11], [23], the Dirichlet distribution, denoted by Diri(a), is employed to partition the non-IID client data. A small a value suggests strong data heterogeneity and vice versa. In our experiments, we use Diri(0.1) and Diri(0.5) to simulate different levels of data heterogeneity. In Section IV-H, we further conduct ablation studies on the effects of temperature values, model layer levels for fine-tuning, and data heterogeneity with different setup.\n2) Baselines: We choose two popular FL baselines, FedAvg and FedProx, to compare with our FedFT-EDS. FedAvg is the standard FL baseline that we introduced in the preliminary section of Section III. FedProx advances FedAvg to tackle the model shift problem by using a proximal term that prevents the local updates from drifting far from the global objective. Additionally, we construct two new baselines by modifying them to use random data selection for local updates, denoted as FedAvg-RDS and FedProx-RDS, which are used to demonstrate the effect of reduced training data. Basically, FedAvg-RDS and FedProx-RDS clients randomly select a proportion of their data to update the global model locally during each round. Finally, we use the FedFT-RDS to denote the baseline that adopts the same partial model training strategy to FedFT-EDS but selected client data randomly, contrasting with the entropy-based data selection.\n3) Setup of random data selection: Section III-C presented how FedFT-EDS performs the data selection from local data by relying on entropy information at the beginning of each local round. This is due to entropy calculated over the model output on each data instance changing with the model update. As such, FedFT-EDS performs the data selection dynamically. For fair comparison, we allow clients to perform a uniform random data selection from their local data at the start of each local update. Thus, the client data used for model updates varies between rounds."}, {"title": "B. Benefits of Pretraining", "content": "This section reports our empirical studies on the benefits of pretraining. We show that the pretrained global model is robust to the model shift problem induced by the data heterogeneity, leading to improved global model performance in FL. In this study, we use CIFAR-100 and Small ImageNet as the source domains separately to pretrain the global model and perform FL on CIFAR-10 with 10 clients holding heterogeneous data.\nTable I reports FL performance on the test set of CIFAR-10. Using FedAvg as the baseline, we compare the global model performance of using the pretraining strategy and that of without using pretraining. We clearly see that pretraining on a source domain, either from CIFAR-100 or from Small ImageNet, significantly improves the global model performance by up to 8%. Pretraining with Small ImageNet yields better results due to pretraining exposing the model to broader diversity and richer samples.\nFurther, we observe the performance gap under different levels of data heterogeneity, seeing that pretrained global model has clear advantage in strong data heterogeneity conditions. Pretraining improves FedAvg by around 8% for the case of"}, {"title": "C. Close-Domain Evaluation", "content": "This section introduce our experiment results when the FL task is a close domain to the pretraining domain. Notably, pretraining the global model on Small Imagenet, Table II reports the performance of FedFT-EDS and baseline methods on image classification, CIFAR-10 and CIFAR-100. The chosen baselines are FedAvg, FedProx, FedAvg-RDS, FedProx-RDS, and FedFT-RDS. In addition, we include FedAvg without global model pretraining (the vanilla FedAvg trained in FL from scratch), and the centralised training to anchor the results. FedAvg (scratch) and centralised training are the lower and the upper bound of performance respectively. Regarding the data selection, local instances are selected in proportion to the available local data, defined by $P_{ds}$. Where $P_{ds}$ is 100%, no data selection is performed, using all local data for local updates. For FedFT-EDS, FedFT-RDS, FedAvg-RDS, we select fewer local samples by setting $P_{ds}$ to 10%.\nTable II shows that FedFT-EDS outperforms all the other baselines expect for the centralised training. These results are insightful in three ways. First, pretraining significantly improves the global model performance. The most substantial boost is observed in strong data heterogeneity, for Diri(0.1). Second, both FedFT-RDS and FedFT-EDS outperform baselines without using partial local fine-tuning by large margins, up to 5% on CIFAR-10 and up to 3% on CIFAR-100. The superior performance achieved by FedFT-RDS over FedAVG-"}, {"title": "E. Overcoming the Straggler Issue with FedFT-EDS", "content": "We further simulate a larger client pool with stragglers dropping out in FedAvg to replicate some clients folding under the standard FL heavy workload. Thus, we show how FedFT-EDS improves FL by addressing the straggler issue through lightweight workloads. In this experiment, we set the number of clients to 100. The fraction of participating clients"}, {"title": "F. Improved Global Model Convergence with FedFT-EDS", "content": "We also see a faster convergence of FedFT-EDS over FedFT-RDS. Figure 8 and Figure 9 present the learning curves during the FL training rounds, where FedFT-EDS outperforms all the other methods, especially on the first few rounds. From round 20, the other methods manage to close the gap, but in all evaluated conditions our FedFT-EDS outperforms them in global model performance. These results offer a clear indication of the importance of selecting the right training samples early, which entropy based selection manages to produce.\nEffectively, our proposed solution FedFT-EDS accelerates the training in FL, which is another advantage when running on resource-constrained devices since participating in just a few rounds suffices for an effective training."}, {"title": "G. Cross-Domain Evaluation", "content": "Besides image classification, we extend the evaluation of FedFT-EDS to another cross-domain scenario, in speech recognition using the Google Speech Command dataset. In this scenario, we concentrate on the generalisation performance of FedFT-EDS, evaluating the efficacy of partial model fine-tuning and entropy-based data selection on the target domain, which is significantly different from the pretraining domain. We assume a full client participation setting with 100 clients as before, and set a strong data heterogeneity with Diri(0.1).\nTable IV presents the global model accuracy (%) achieved by the vanilla FedAvg without pretraining, FedAvg with pretraining the global model, FedFT-RDS, and our FedFT-EDS. Additionally, we report the performance of centralised learning as the upper bound achievable on the dataset. The observation is twofold. First, pretraining the global model is still beneficial across domains, improving the performance of"}, {"title": "H. Ablation Studies", "content": "In this ablation study we investigate further the impacts of some important factors on FedFT-EDS. These are the level of data heterogeneity defined by Diri(a), the depth of the partial model used for fine-tuning, and the temperature for the hardened softmax. We use CIFAR-100 for this ablation study, with 100 clients in full participation, taking advantage of the lower workload of FedFT-EDS. The proportion of selected"}, {"title": "V. CONCLUSION", "content": "We presented FedFT-EDS, a novel approach to enhance Federated Learning (FL) by integrating partial model fine-tuning with entropy-based data selection. By determining the most informative data points for training, FedFT-EDS effectively reduces the training workload on resource-constrained devices while improving the overall efficiency and performance of the global model. Experiments on CIFAR-10 and CIFAR-100 demonstrated that FedFT-EDS uses only 50% of client data, accelerates training by up to three-fold, and outperforms established methods like FedAvg and FedProx in heterogeneous settings. Our findings highlight the importance of targeted data selection in FL systems, paving the way for more scalable, resource-efficient learning frameworks."}]}