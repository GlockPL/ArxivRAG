{"title": "An experimental evaluation of Siamese Neural Networks for robot localization using omnidirectional imaging in indoor environments", "authors": ["Juan Jos\u00e9 Cabrera", "Vicente Rom\u00e1n", "Arturo Gil", "Oscar Reinoso", "Luis Pay\u00e1"], "abstract": "The objective of this paper is to address the localization problem using omnidirectional images captured by a catadioptric vision system mounted on the robot. For this purpose, we explore the potential of Siamese Neural Networks for modeling indoor environments using panoramic images as the unique source of information. Siamese Neural Networks are characterized by their ability to generate a similarity function between two input data, in this case, between two panoramic images. In this study, Siamese Neural Networks composed of two Convolutional Neural Networks (CNNs) are used. The output of each CNN is a descriptor which is used to characterize each image. The dissimilarity of the images is computed by measuring the distance between these descriptors. This fact makes Siamese Neural Networks particularly suitable to perform image retrieval tasks. First, we evaluate an initial task strongly related to localization that consists in detecting whether two images have been captured in the same or in different rooms. Next, we assess Siamese Neural Networks in the context of a global localization problem. The results outperform previous techniques for solving the localization task using the COLD-Freiburg dataset, in a variety of lighting conditions, specially when using images captured in cloudy and night conditions.", "sections": [{"title": "1 Introduction", "content": "During the past few years, vision sensors have been used extensively in the field of map building and localization with mobile robots (Hu et al., 2020; Zhong et al., 2018). In particular, the ability to localize in the map is of paramount importance in order to develop autonomous robots that can navigate in real operation conditions (Reinoso and Pay\u00e1, 2020). The interest in using vision sensors to capture information from the environment is still high. Cameras can capture a big amount of information from the environment with a relatively low cost and they can be used in both, indoor and outdoor areas. Additionally, the images permit carrying out other highly specialized tasks such as object recognition and people detection.\nAmong the available configurations to capture visual information, the use of omnidirectional vision sensors in mobile robotics has become common. Omni-directional cameras obtain images that cover a field of view of 360 deg. around the robot (Junior et al., 2016). As a result, they are commonly used to address navigation tasks (Rituerto et al., 2010).\nThe large amount of information provided by cameras requires robust techniques to extract and describe the relevant visual information. Different paradigms have been considered to extract this relevant information. A first group of techniques concentrate on detecting, describing and tracking some landmarks or local features along the scenes (Garcia-Fidalgo and Ortiz, 2015; Luthardt et al., 2018; Shamsfakhr et al., 2019; Cao et al., 2020; Lin et al., 2020). Different local features have been used in mapping and localization tasks, including SIFT, SURF (Gil et al., 2010) and ORB descriptors (E. Rublee and Bradski, 2011). A global description of each image can then be obtained, for example, by means of the Bag of Words model (Ra\u00fal Mur-Artal and Tard\u00f3s, 2015). A second group of techniques work with each scene as a whole, and build a unique descriptor per image that contains information on its global appearance (Berenguer et al., 2015; Korrapati and Mezouar, 2017; Khaliq et al., 2019; Liu and Zhang, 2012). Finally, hardware developments have led many authors to use Artificial Intelligence (AI) techniques to extract relevant information from images. Specifically, Convolutional Neural Networks (CNNs) have been proposed to address different computer vision and robotics tasks. For example, Xu et al. (2019) and Leyva-Vallina et al. (2019) proposed global appearance descriptors based on a CNN to obtain the most probable pose of the robot."}, {"title": "2 State of the art", "content": "As stated before, Siamese Neural Networks are able to generate a similarity function from pairs of input data. They can be regarded as a superstructure that includes two Neural Networks. These architectures accept two different inputs and offer a single output. The underlying networks share the same weights and different functions can be used to conform a single output. They were first proposed in 1993 in order to distinguish correct signatures from forgeries (Bromley et al., 1993). Since then, these architectures have been proposed in different areas of knowledge. For example, Thiolliere et al. (2015) proposed a Siamese Neural Network for audio and speech signal processing, Zheng et al. (2019) used this architecture for the comparison of DNA sequences or Jeon et al. (2019) used it for drug discovery purposes. Furthermore, Parajuli et al. (2017) developed a Siamese Neural Network to track cardiac motion and Sandouk and Chen (2017) proposed a Siamese architecture in order to recognize music tags."}, {"title": "3 Siamese Neural Networks", "content": "Siamese Neural Networks can be described as a superstructure that includes, at least, two different Neural Networks beneath. Weights are shared between the networks and a single output is generated by combining the outputs of both networks. In the present work, we use Convolutional Neural Networks to conform the two branches of the Siamese Neural Network. The output of each CNN is a descriptor which is used to characterize each input image. The dissimilarity of the input images is computed by measuring the distance between these descriptors. In this way, Siamese Neural Networks can be trained to generate similar descriptors when the training images belong to the same category. This fact makes Siamese Neural Networks particularly suitable to perform image retrieval tasks. Additionally, it is worth noting that the outputs, training, and performance of the network depend directly on:\n\u2022 The architectures used in subnetworks W1 and W2 to extract the main features of the images.\n\u2022 The conversion of the feature maps from the convolutional layers to a descriptor vector.\n\u2022 The dimension of the output descriptors that embed the pair of input images.\n\u2022 The training carried out with the available images. In particular, the labelling and the ratio of images of each category.\nIn this manuscript, we analyze the influence of these items on the visual localization of the robot."}, {"title": "4 Visual Localization", "content": "This section is focused on explaining the mobile robot localization using visual information. In this manuscript, we assume that a visual map of the environment is initially available. To obtain this map, the robot has moved throughout the area capturing omnidirectional images along the trajectory. Firstly, the images are transformed to a panoramic format (with size 128x512 in the present work), resulting in the set {I1, I2, ..., IN}. These images are captured from N points of view, whose poses are known and stored P\u2081 = (xi, Yi, 0i), i = 1, ..., N. Additionally the room where the picture has been captured is known too, so a set of labels is available: R\u2081 = (ri), i = 1, ..., N. Each image will be embedded into a single descriptor during the localization, using the proposed architecture, yielding {f1, f2, ..., fv}. The trajectory followed by the robot includes different rooms with different visual information. In this work, these rooms include a corridor, some offices, a library and a bathroom."}, {"title": "4.1 Room Discrimination", "content": "In this subsection an initial task related to localization is evaluated to study whether a Siamese Neural Network is able to distinguish between images captured from the same or from different rooms. For this purpose, the model will be trained and tested with pairs of random images captured from the same and/or different room."}, {"title": "4.2 Global Localization", "content": "In this study we consider that a map of the environment is available, as described before. The absolute localization problem is solved by comparing the test image directly with all the images in the map. This comparison is performed using the descriptors fi associated to each image in the map. The pose"}, {"title": "5 Architecture and Training of the Deep Learning tools", "content": "The structure of a classical CNN used for classification tasks can be split into two different stages (Cebollada et al., 2019): the feature learning and the classification stages. Features are extracted using several convolutional layers whereas the classification task can be constructed using fully connected layers and a final Softmax function. In our approach, the classification stage is replaced by a flattening phase. In this sense, the feature learning phase outputs a feature map which is flattened to a vector and dimensionally reduced by fully connected layers. This phase permits generating a single description vector per input image. As a result, the model provides two vectors fo and fi (one per input image). These descriptors are compared using the Euclidean distance in the comparison phase (d(fo, f\u2081) = ||fo \u2013 f1||2). Therefore, during training, the weights of the networks are updated in order to obtain the optimal global descriptors. After the comparison, the distance between them and the similarity label (1 : dissimilar, 0 : similar) are used as data for the loss function. In our case the loss function used is the Constrastive Loss Function.\nL(fo, fi) = 1/2 (1 - y)d(fo, fix)\u00b2 + y/2  max(a \u2013 d(fo, f1), 0)2 (1)"}, {"title": "5.1 Parameters and Networks", "content": "In this manuscript we compare different networks in the feature learning stage. As inputs to the flattening layers we consider the representation computed in the last convolutional layer of Alexnet (Krizhevsky et al., 2012), DenseNet (He et al., 2016), VGG11, VGG13, VGG16 and VGG19 (Simonyan and Zisserman, 2014). The ReLU activation layers are not shown for brevity, but they have been used after each conv2d layer. The different feature learning structures are evaluated in the Section 6."}, {"title": "5.2 Data Augmentation", "content": "Additionally, a data augmentation technique is proposed as a method to improve the performance of the network. It increases the number of images in the training dataset. Having a larger number of training images reduces the overfitting of the model and boosts its robustness against real operation conditions. Cabrera et al. (2021) and Sakkos et al. (2019) demonstrated the use of data augmentation in CNNs to improve their effectiveness under changing lighting conditions.\nOur proposed data augmentation is focused mainly on such lighting conditions and concentrates on editing local regions by simulating lights, reflections and shadow effects caused by light sources from different angles. Moreover global illumination changes are also taken into account. Other effects not related with the illumination but that can appear when images are captured in real operating conditions are also used."}, {"title": "Local effects", "content": "Light sources that fall on a specific area or the surface of an object are reproduced. We call this local illumination changes since only a small patch of the image is being affected. The shape of different light sources can vary meaningfully.\nCircular shapes from light bulbs or square and trapezoid shapes from reflections or windows are common. We edit the intensity of different regions following these shapes to simulate the light source; the pixel intensity is increased to reproduce more bright or it is decreased to simulate a shadow effect. In order to replicate a realistic fading effect, the intensity of brightening/darkening is gradually decreased from the center to the edge as an attenuation of the light.\nThe size of the shapes and the position is selected randomly to simulate the effect in different ways and so does the maximum value to consider different intensities. In our experiments these figures are built with sizes between 15 and 40 pixels, different intensities are applied and the patch is degraded from intensity values +/- 160 or 100 to 5."}, {"title": "Global illumination", "content": "Global illumination variations can occur in some cases. To model such illumination changes, we need to alter pixels across the whole image, rather than in a small region. A constant value c is added to all the pixels to model a global brightness effect on the image or it is subtracted to simulate a global darkness. The value of c varies from 35 to 75 in this work."}, {"title": "Sharpness/Blurring", "content": "Finding sharper borders among diverse objects will contribute to provide a better separation among them and between foreground and background. In contrast, blurring effects are caused by low illumination and movements of the camera, which are common in mobile robotics. Both effects are incorporated in the data augmentation. They can be observed in\nmsh = 0-10\\-15-1\\0-10, msr = 1/25 11111\\11111\\11111\\11111\\11111."}, {"title": "Contrast variation", "content": "The contrast of the image plays an important role in highlighting different objects in the scene. Low contrast images usually look softer and with less shadows and reflections. The effect is proposed for this data augmentation to improve the robustness of the framework. The contrast is modified following the next equation:\nIs = 64 + c * (I \u2013 64)\nwhere Is is the resulting image, I the original image and c is the contrast factor. For c > 1 the contrast increases and c < 1 decreases the contrast. Additionally, an equalization of the image is also added to the data augmentation set. It evenly distributes the histogram values, which permits obtaining a new contrast augmentation effect."}, {"title": "Saturation changes", "content": "The colour saturation of the image deals with the intensity of the colour. The less saturation, the less colourful the image is, even it can resemble a grey-scale image if the saturation is very low. In contrast, more vivid colours are obtained when the colour saturation is high. It can simulate situations when illumination changes significantly. The colour saturation can be edited by converting the RGB image to HSV, after that, it is possible to directly change the saturation channel by multiplying it by a constant factor c. If the saturation attribute is multiplied by c > 1 the colours become more saturated and by c < 1 the colour saturation decreases."}, {"title": "Rotation", "content": "The original image covers 360 degrees around the robot. For that reason the image can be rotated without losing any piece of information. This effect will simulate the situation in which the robot is in the same position but the orientation is different. Moreover, having a training dataset containing this"}, {"title": "5.3 Training and testing the Siamese Neural Network", "content": "As presented in Section 5.1, different CNNs architectures can be used as the base of Siamese Neural Networks. Initially, we start from pretrained networks with known weights and biases. Then, we retrain the network to fit it to our"}, {"title": "5.3.1 Room Discrimination", "content": "The main goal of this task is to evaluate whether a Siamese Neural Network is capable of determining if two images belong to the same or different room. It is an important capability to perform localization tasks.\nThe training phase is performed by feeding the network with pairs of images. These pairs are labelled with 0 if they have been captured from the same room and 1 if not. The ratio same/different room pairs is varied in the training phase to study its influence."}, {"title": "5.3.2 Global localization", "content": "The global localization problem considers the estimation of the robot pose within the whole floor of the building. For this purpose, a Siamese Neural Network is trained. The training is carried out with image pairs labelled with the following equation:\nLabel(Ii, Ij) = ||Pi-Pi||2/Kb if Ii and Ij belong to the same room, 1 otherwise (2)\nwhere Ii and Ij are two images and pi and pj are their corresponding positions (coordinates of the capture points). This constitutes a normalized Euclidean distance between the capture points. K\u266d corresponds to the maximum distance between two images in the building."}, {"title": "6 Experiments", "content": "The set of experiments is designed to test the performance of the Siamese Neural Network as global descriptor generator to tackle the room discrimination"}, {"title": "6.1 Training and test datasets", "content": "The images used in the experiments are obtained from an indoor dataset (Pronobis and Caputo, 2009). This database was captured by an omnidi-rectional vision sensor mounted on a mobile robot which followed different trajectories that visited 9 different rooms. A variety of lighting conditions was considered to capture the sets of images.\nIn this way, the training sets will be used to carry out the training of the Siamese Neural Networks, and the test sets will evaluate the performance of the networks under the three lighting conditions. The visual model is the map available for the robot to carry out the localization, so it will be used in the testing phase of the global localization."}, {"title": "6.2 Room Discrimination", "content": "In this subsection we assess the ability of the network to predict whether two images are taken from the same room. The effectiveness of the Siamese Neural Network is calculated by comparing pairs of images and checking their label. The results are expressed in percentage of accuracy. Several experiments have been conducted while varying different parameters: the feature extraction architecture, the flattening layers and the percentage of similar/dissimilar images. As common parameters, we train the network using 8486 pairs of"}, {"title": "6.2.1 Influence of the architecture on the feature extraction process", "content": "In this subsection we compare different models in the feature extraction stage of a Siamese Neural Network. The different models used can be observed in\nResults are presented in terms of global accuracy. Additionally, the test accuracy for the same and different room predictions is also presented.\nThe table shows that the best networks are VGG13 and VGG16. They obtain the best accuracy for predicting pairs of images in the same room (99.44% and 99.47% respectively). In addition, VGG13 and VGG16 present the best accuracy predicting if two images are taken from different rooms (79.86% and 78.91%). Moreover, the \u2018Simple 1' and \u2018Simple 2' networks obtain considerably good results using only three convolutional layers. Finally, in general terms, it can be observed that all the architectures perform better in predicting whether two images belong to the same room. For this reason, we consider below the possibility of varying the percentage of images of each category in the training phase."}, {"title": "6.2.2 Influence of the training parameters", "content": "In the light of the previous results, next, different training parameters are evaluated. As we explain in the previous subsection, the ratio of training pairs"}, {"title": "6.2.3 Influence of the architecture of the flattening layers", "content": "As explained in subsection 5.1, the feature extraction layers output a matrix that is transformed to a vector in the flattening phase. Different combinations of fully connected layers are also evaluated. All these experiments have been performed training the network with a 10% of pairs of images taken from the same room and a 90% of pairs of images from different rooms.\nThese tables show that relatively good performances can be achieved with some configurations. Notwithstanding that, we observe that in general terms, the same-room accuracy tends to decrease when the different-room accuracy increases and vice versa. This will be analyzed deeply in future works, but it may be due to the use of the Contrastive Loss function (Sun et al., 2020a)."}, {"title": "6.3 Global Localization", "content": "The global localization is performed as explained in subsection 5.3.2. The VGG16 network is employed in this task since it led to the best results in the room discrimination task. Different experiments have been performed in order to choose the best configuration. We will mainly analyze the ratio of same/different room pairs, which is the parameter that has shown the greatest influence on the results. Moreover, in this subsection we will assess the influence of the data augmentation on the results. Each pair of images is labelled according the Equation 2.\nFirst, concerning the experiment to evaluate the influence of the ratio same/different room pairs, we train the network using 8,486 pairs of images per epoch from the training dataset 1. Second, with respect to the experi-ment to assess the effect of the data augmentation, 977,856 pairs of images per epoch from the training dataset 2 are used. These two experiments are described in subsection 6.3.1. In both cases, the flattening layers are configured with 500-500-5 neurons. Moreover, subsection 6.3.2 evaluates the influence of the flattening layers. In this case, the training dataset 1 is used. As common parameters, we use 16 as batch size, the Stochastic Gradient Descent (SGD) optimizer, with a learning rate of 0.001 and a momentum of 0.9 and 30 epochs."}, {"title": "6.3.1 Influence of the training parameters", "content": "Ratio of same/different room pairs:\nshows the results using VGG16 in the feature extraction part and three fully connected layers with 500-500-5 neurons in the flattening part. The training of the model has been performed with different percentages of pairs of images belonging to the same and different rooms. The results show that the lowest localization error is obtained when the training is performed using"}, {"title": "Data Augmentation", "content": "Next, we evaluate the influence of the data augmentation on the localization task. presents the results using the training dataset 2 (augmented) and test datasets 1, 2 and 3. For this purpose, we will start from the best configurations obtained so far and show the results according to the percentage of training image pairs."}, {"title": "6.3.2 Influence of the architecture of the flattening layers", "content": "To conclude the experimental section, shows the results after evalu-ating different flattening layers. In this case, a descriptor of dimension 5 works better in the global localization problem. The Siamese Neural Network is able to perform the localization with an average error of 0.5821 m when using as the flattening layers three different fully connected layers with 500, 500 and 5 neurons."}, {"title": "6.3.3 General comparison with other methods", "content": "Finally, the Siamese Neural Networks are compared with other previous global-appearance techniques which include the use of a single AlexNet structure and two classic analytic descriptors: HOG and gist, as described in the work by Cebollada et al. (2022). Table 17 compares all the methods in a global localization task using, in all cases, the COLD-Freiburg Dataset. As we can notice, the best results in terms of localization error for cloudy and night conditions are obtained with the siamese structures proposed in the present work. The best results for the sunny illumination condition are obtained first with the single Alexnet structure and second with the siamese network without data augmentation."}, {"title": "7 Conclusions", "content": "In this paper, a global localization method using Siamese Neural Networks has been proposed and evaluated. Localization, along with mapping, is one of the main tasks to be addressed by an autonomous mobile robot. First, an initial task of discriminating same and different rooms has been proposed in order to assess the ability of Siamese Neural Networs and know the influence of the most relevant parameters. After that, the global localization problem is addressed.\nIn the experiments, several well known architectures have been tested to conform the Siamese Neural Network, some of which are AlexNet, VGG11, VGG13, VGG16, VGG19, VGG11bn, VGG13bn, VGG16bn and VGG19bn. The best performance in the initial task has been achieved by VGG13 and VGG16. In general terms, the VGG architectures have provided the best results.\nApart from these feature extraction architectures, a group of Fully Con-nected layers have been added to carry out the conversion of the activation maps resulting from the convolutional layers to a description vector. In the present work, different sizes of the Fully Connected layers have been studied, as well as the size of the final descriptor. For the initial task, the performance of the network is slightly higher when the Fully Connected layers sizes are 1000-1000-10. In contrast, in the global localization, the localization error decreases drastically in those networks that have a set of Fully Connected layers of size 500-500-5 neurons.\nThe training parameter that contributes most to the performance of the network is the percentage of image pairs belonging to the same and different rooms. In this sense, there is a correlation between the percentage of images of same/different room and its respective accuracy, i.e., when the percentage of pairs of images in the same room increases, its associated accuracy also does and a similar effect occurs with the different room category. Furthermore, when the same room accuracy increases, the different room accuracy decreases, and vice versa. This situation may be caused by the Contrastive Loss func-tion which has an associated lack of flexibility in the optimization. Other loss functions used in other applications could improve localization results, such as Circle Loss (Sun et al., 2020b) and will be considered in future studies.\nIn addition, a data augmentation technique has been proposed in order to improve the performance of the network. The proposed effects try to simu-late real operation conditions. In addition, a set of effects specially designed to increase the robustness against changes of the lighting conditions in the scene have been generated. As for the results obtained, the performance of the network is especially benefited when working in cloudy and night lighting conditions. In the case of the cloudy lighting condition, when the training is performed with data augmentation, the average localization error is reduced around 12 centimeters. As for the night illumination condition, the average error is reduced around 20 centimeters. On the contrary, in sunny illumination condition the average localization error increases 34 centimeters when data augmentation is used.\nAs future works, the proposed localization techniques will be extended to outdoor environments, which are more challenging because of their unstruc-tured and changing conditions. In addition, other types of sensors will be considered to carry out the localization robustly, such as LiDAR."}]}