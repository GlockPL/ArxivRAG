{"title": "No Images, No Problem: Retaining Knowledge in Continual VQA with Questions-Only Memory", "authors": ["Imad Eddine Marouf", "Enzo Tartaglione", "St\u00e9phane Lathuili\u00e8re", "Joost van de Weijer"], "abstract": "Continual Learning in Visual Question Answering (VQACL) requires models to learn new visual-linguistic tasks (plasticity) while retaining knowledge from previous tasks (stability). The multimodal nature of VQACL presents unique challenges, requiring models to balance stability across visual and textual domains while maintaining plasticity to adapt to novel objects and reasoning tasks. Existing methods, predominantly designed for unimodal tasks, often struggle to balance these demands effectively. In this work, we introduce Question-only replay with Attention Distillation (QUAD), a novel approach for VQACL that leverages only past task questions for regularization, eliminating the need to store visual data and addressing both memory and privacy concerns. QUAD achieves stability by introducing a Question-only Replay mechanism that selectively uses questions from previous tasks to prevent overfitting to the current task's answer space, thereby mitigating the out-of-answer-set problem. Complementing this, we propose Attention Consistency Distillation, which uniquely enforces both intra-modal and inter-modal attention consistency across tasks, preserving essential visual-linguistic associations. Extensive experiments on VQAv2 and NExT-QA demonstrate that QUAD significantly outperforms state-of-the-art methods, achieving robust performance in continual VQA. Code is available at: https://github.com/IemProg/QUAD", "sections": [{"title": "1. Introduction", "content": "Continual learning (CL) seeks to enable models to incorporate new information while retaining previously learnt knowledge, thereby addressing the problem of catastrophic forgetting (CF) [48, 49]. This capability is essential in dy-"}, {"title": "2. Related work", "content": "Visual Question Answering (VQA) involves answering natural language questions by interpreting visual content [3, 38, 51]. Recent approaches leverage vision-language models (VLMs) built on transformer architectures [8, 16, 37, 61, 76] alongside pre-trained language models [78, 80]. For instance, Cho et al. [9] proposed a generative transformer for VQA that analyses visual elements in conjunction with textual questions. Many methods aim to improve generalisation capabilities by enhancing compositionality, which is essential for cognitive reasoning [30, 36]. For instance, Johnson et al. [28] explored the composition of visual attributes by creating a dataset designed for compositional reasoning, while Whitehead et al. [73] used contrastive learning to enhance compositionality, disentangling reasoning skills from visual concepts. Although these approaches make strides toward compositionality, the implicit decomposition may impact generalisation, and constructing effective contrastive samples remains complex.\nContinual Learning (CL) aims to develop frameworks capable of incrementally learning from sequentially arriving datasets. This is a fundamental challenge for many deep learning methods due to catastrophic forgetting (CF) [48]. CL methods can be divided into different categories. While knowledge distillation-based methods constrain mappings between incremental models to prevent forgetting [13, 27, 29, 39, 46, 57], optimisation-based techniques modify the gradient updates during training to minimise interference with previously learnt tasks to mitigate forgetting [5, 44, 59, 79], and representation-based methods focus on learning robust and adaptable feature representations [14, 15, 17, 18]. Recently we have also witnessed the uprisal of prompt-based methods like [62, 68, 70, 71], which utilise visual prompts [43] with pre-trained transformers for CL scenarios. Most of these methods target"}, {"title": "3. Question-only replay with Attention Distillation (QUAD)", "content": "We propose QUAD, a novel approach for VQACL-QR that eliminates the need to store images from previous tasks by using only past task questions (see Fig. 2). Following several works in continual learning [11, 40], we adopt a regularisation framework where the total learning objective $L_{VQACL}$ consists of two main components:"}, {"title": "3.1. Setting Overview", "content": "The VQACL setting [85] is designed to assess the capability of a model to adapt to a sequence of tasks, each involving both visual and linguistic inputs, in a continual learning environment. It approaches VQA as a generative task, where the objective is to generate textual answers given an image and a corresponding question [20, 85]. The model encounters a non-stationary stream of data, requiring it to learn and adapt incrementally over time without revisiting prior data. We consider a sequence of T tasks, denoted as T1, T2, \u2026\u2026\u2026, TT. Each task Tt is characterized by a set of image-question-answer triplets (xt, qt, yt), where xt \u2208 Xt denotes the image, qt \u2208 Qt represents the question, and yt \u2208 Yt corresponds to the answer. The challenge is to train a model $ \\phi $ that can effectively learn the current task Tt while retaining the knowledge from all previous tasks {T1,T2,...,Tt-1}.\nIn VQACL, the sequence of tasks is organized as a series of L macro-tasks, each comprising K sub-tasks, resulting in a total of T = L \u00d7 K tasks. Each macro-task is designed to develop specific reasoning skills such as counting, color identification, or object recognition (i.e linguistic task). For example, in a counting task, the model primarily engages with questions like \u201cHow many objects are there?\u201d or \"What number is shown?\".\nEach linguistic macro-task is further divided into visually-driven sub-tasks. Formally, each macro-task T"}, {"title": "3.2. Overview", "content": "We propose QUAD, a novel approach for VQACL-QR that eliminates the need to store images from previous tasks by using only past task questions (see Fig. 2). Following several works in continual learning [11, 40], we adopt a regularisation framework where the total learning objective $L_{VQACL}$ consists of two main components:\n$L_{VQACL} = L_{Plasticity} + \\lambda L_{Stability}$ (1)\nThe first, the plasticity term $L_{Plasticity}$, guides the model's adaptation to the current task Tt, and $ \\lambda > 0 $ is a weighting coefficient adjusting the balance between plasticity and stability. Following common practice in VQA [2, 56, 85], we implement this loss using cross-entropy to compare the network's prediction for an input image-question pair (xt, qt) with the corresponding annotated answer yt:\n$L_{Plasticity} = E_{(x_t, q_t, y_t) \\sim T_t} L_{CE} [ \\phi(x_t, q^*_t), y_t]. $ (2)\nThe second loss component, the stability term $L_{Stability}$, helps prevent forgetting. In the standard VQACL setting, where images from past tasks can be stored in the memory M, this term can be implemented similarly to the plasticity term, using cross-entropy and averaging the loss over triplets (xm, qm, ym) \u2208 M. In the VQACL-QR setting, storage of images xm from past tasks in the memory M is prohibited. To address this limitation, we propose a novel $L_{Stability}$ loss tailored for VQACL-QR. This loss combines two complementary strategies-pseudo-labeling $L_{PL}$ and attention distillation $L_{Att}$ to effectively compensate for the absence of past task images, ensuring robust knowledge retention while adhering to the constraints of the VQACL-QR framework. In our case, the stability term can be written as:\n$L_{Stability} = L_{PL} + L_{Att}$, (3)"}, {"title": "3.3. Pseudo-Labeling for Question-only Replay", "content": "To address knowledge retention in continual VQA, we propose a pseudo-labeling approach that leverages image-question pairs formed by combining current task images with questions from past tasks stored in memory. Specifically, for each image from the current task xt, we pair it with a question qm sampled from the memory. By asking past questions on new images, we prompt the model to recall and retain prior knowledge. Inspired by prior distillation-based techniques [11, 40], we employ the model from the previous task $\\phi_{t\u22121}$ to generate answers for each new image-question pair (xt, qm). These answers serve as soft pseudo-labels for the current model $\\phi_t$, enforcing consistency with past knowledge. The pseudo-labeling loss is defined as:\n$L_{PL} = E_{x_t \\sim T_t} E_{q_m \\sim M} L_{CE} [\\phi_t(x_t, q_m), \\phi_{t-1}(x_t, q_m)].$ (4)\nWe use the network's output as soft pseudo-labels, avoiding the application of the argmax operator. Applying argmax would compel the network to align its predictions solely with the most likely class, while soft pseudo-labeling enables a more nuanced alignment with the output distribution of the model $\\phi_{t-1}$ [26, 66, 84].\nOur new image-question pairs (xt, qm) exposes the model to a broad set of visual-question combinations, enhancing its stability across tasks. Importantly, it allows the model to retain a broad ability to answer different types of questions, even if it cannot produce precise, correct answers. Crucially, without this pseudo-labelling regularisation, the model becomes susceptible to the out-of-answer-set problem, where overfitting to the current task's answer space leads to incorrect responses for questions from previous tasks. For instance, after training on a color recognition task, the model might incorrectly respond with a color name when asked a counting question from a prior task.\nQuestion Selection for QUAD. Randomly pairing images from the current task xt with past questions qm from memory can help mitigate forgetting, but it is inherently suboptimal as it often results in mismatched pairs. For instance, if the current macro-task Tt focuses on counting, random pairing could lead to questions like \"How many cows are in this image?\" being associated with an image of cars, which undermines the relevance and effectiveness of the training process. To address this, we introduce a question selection strategy that prioritises questions directly related to the object categories ci being learnt in the current visually-driven subtask $S_t^k$. This targeted pairing ensures that the training process leverages meaningful and contextually relevant image-question combinations. By aligning questions with the visual context, QUAD enhances adaptation to new linguistically-driven tasks and visually-driven subtasks, leading to effective knowledge transfer.\nExample: Suppose the current visually-driven subtask $S_t^k$ involves learning to count cars. Thus, the question selection strategy prioritises questions from memory relevant to cars, such as \"What's the color of the car?\", ensuring alignment between the visual content and the linguistic query."}, {"title": "3.4. Attention Consistency Distillation", "content": "Continual VQA demands a careful balance between stability and plasticity, particularly as models must adapt to new tasks without forgetting. While pseudo-labeling offers a straightforward way to preserve prior knowledge, it provides limited stability because it only influences the network's outputs, leaving its internal representations vulnerable to drift. Feature distillation across multiple layers [11, 29, 54, 77] provides stronger regularisation by guiding internal representations but often compromises plasticity. These methods impose rigid constraints across all layers, which is particularly problematic in our setting, where new image-question pairs (xt, qm) introduced during learning the current task were not seen in previous tasks. Although these pairs are meaningful and aligned, the model has no prior experience processing them, making strict layer-wise regularisation overly restrictive. Our VQA model [52, 85] processes image features and language tokens as a unified sequence in a transformer, where self-attention naturally captures both intra-modal (language-to-language and image-to-image) and inter-modal (language-to-image) interactions. This implicit cross-attention capability enables the model to compute similarity scores between language and image tokens without requiring cross-attention layers. However, as the model is sequentially fine-tuned on new tasks, it runs the risk of suffering from self-attention drift\u2014a gradual shift in focus away from relevant visual regions associated with previous tasks [21, 65]. This drift occurs because pseudo-labeling alone cannot prevent changes in the model's internal representations.\nTo address this, we introduce an intermediate approach: attention consistency distillation. Our approach aligns the self-attention weights of the current model $\\phi_t$ with those of the previous task model $\\phi_{t\u22121}$, preserving both intra-modal and inter-modal relationships. This alignment acts as a lightweight but effective regularisation that maintains stability within the self-attention patterns. By distilling only the attention maps, we constrain the model to maintain its focus on relevant visual regions for answering questions without imposing strong regularisation on all layers.\nLet us sample an image-question pair (xt, qm) with xt ~ Tt, qm ~ M: we denote $A_t^k$ and $A_{t-1}^k$ as the corresponding self-attention maps in an attention head k for the current and previous models, respectively, computed over the unified token sequence of both language and image features [52, 85]. These maps are generated by applying softmax to the scaled query-key product, yielding normalised distributions that represent the focus between intra-modal and inter-modal interactions. To align these distributions, we define the self-attention distillation loss $L_{Att}$ using cross-entropy, encouraging the current model's attention maps $A_t^k$ to resemble those from the previous task $A_{t-1}^k$:\n$L_{Att} = E_{x_t \\sim T_t} E_{q_m \\sim M} E_{k \\sim K} L_{CE} [A_t^k(x_t, q_m), A_{t-1}^k(x_t, q_m)],$ (5)\nwhere K denotes the set of all the attention heads across all the layers of $\\phi$. Our approach employs cross-entropy loss over normalized attention maps, in contrast to prior works [11, 54] that rely on L1 or ReLU+L1 losses. Cross-entropy enforces alignment between attention distributions, preserving both individual token importance and structured inter-modal relationships essential for VQACL. In contrast, L1-based methods treat attention scores as independent values, imposing rigid constraints that limit adaptability to new tasks. A key advantage of cross-entropy is its consistency across all loss terms in QUAD ($L_{CE}$, $L_{PL}$, $L_{Att}$), ensuring coherent optimization. Empirically, this design mitigates catastrophic forgetting."}, {"title": "4. Experiments", "content": "Implementation Details. To ensure a fair comparison, we adopt the protocol of [85] for both feature extraction and training across datasets. For the visual embeddings, we use a Faster R-CNN [58] trained on the Visual Genome dataset [34] to obtain 36 region-based features per image in the VQAv2 dataset. For videos in the NEXT-QA dataset, we extract clip-level motion features using an inflated 3D ResNeXt-101 [25], setting n = 16 regions per clip. These visual features are adapted through a two-layer MLP with GELU activation, preparing them as inputs to the transformer model. Our transformer backbone, based on T5 [55], consists of 12 blocks for both encoder and decoder modules, each containing 12 attention heads. The embedding dimension d is tailored to the task-specific requirements. Training is conducted for 3 epochs per task, with a batch size of 80. We utilize the Adam optimizer [31] with an initial learning rate of $10^{-4}$. $\\lambda$ is set to 0.5 in all experiments. All implementations are based on PyTorch [53]. Unlike VQACL [85], QUAD does not create or store any prototypes for questions or visual objects.\nEvaluation Metrics. We utilise two established metrics for continual learning [4, 45, 85]: final average performance (AP), and average forgetting (Forget). The AP metric reflects the model's overall performance across all learnt tasks, highlighting its ability to consistently acquire new tasks. Let $a_{i,j}$ represent the performance of the model on task $T^i$ after it has completed learning this task $T^j$. Then, AP is calculated as: $AP = \\frac{1}{T} \\sum_{i=1}^T a_{i,T}$. Additionally, the Forget metric quantifies the performance loss in previous tasks as new tasks are learnt and is computed as: $Forget = \\frac{1}{T-1} \\sum_{i=1}^{T-1} \\max_{z \\in {i,...,T-1}}(a_{i,z} - a_{i,T})$. To"}, {"title": "4.1. Experimental setup", "content": "To address this, we introduce an intermediate approach: attention consistency distillation. Our approach aligns the self-attention weights of the current model $\\phi_t$ with those of the previous task model $\\phi_{t\u22121}$, preserving both intra-modal and inter-modal relationships. This alignment acts as a lightweight but effective regularisation that maintains stability within the self-attention patterns. By distilling only the attention maps, we constrain the model to maintain its focus on relevant visual regions for answering questions without imposing strong regularisation on all layers.\nLet us sample an image-question pair (xt, qm) with xt ~ Tt, qm ~ M: we denote $A_t^k$ and $A_{t-1}^k$ as the corresponding self-attention maps in an attention head k for the current and previous models, respectively, computed over the unified token sequence of both language and image features [52, 85]. These maps are generated by applying softmax to the scaled query-key product, yielding normalised distributions that represent the focus between intra-modal and inter-modal interactions. To align these distributions, we define the self-attention distillation loss $L_{Att}$ using cross-entropy, encouraging the current model's attention maps $A_t^k$ to resemble those from the previous task $A_{t-1}^k$:\n$L_{Att} = E_{x_t \\sim T_t} E_{q_m \\sim M} E_{k \\sim K} L_{CE} [A_t^k(x_t, q_m), A_{t-1}^k(x_t, q_m)],$ (5)\nwhere K denotes the set of all the attention heads across all the layers of $\\phi$. Our approach employs cross-entropy loss over normalized attention maps, in contrast to prior works [11, 54] that rely on L1 or ReLU+L1 losses. Cross-entropy enforces alignment between attention distributions, preserving both individual token importance and structured inter-modal relationships essential for VQACL. In contrast, L1-based methods treat attention scores as independent values, imposing rigid constraints that limit adaptability to new tasks. A key advantage of cross-entropy is its consistency across all loss terms in QUAD ($L_{CE}$, $L_{PL}$, $L_{Att}$), ensuring coherent optimization. Empirically, this design mitigates catastrophic forgetting."}, {"title": "4.2. Main results", "content": "Performance Analysis On Standard Setting. Tab. 1 shows a detailed comparison of continual learning ap-"}, {"title": "5. Ablation Study and Analysis", "content": "Distillation Components. Tab. 3 shows the impact of PL distillation and attention distillation in QUAD. We evaluate three setups: (1) PL distillation alone, (2) attention distillation alone, and (3) the combination of both.\nUsing PL distillation alone achieves moderate performance, with AP of 30.72% on VQAv2 and 29.04% on NEXT-QA, indicating effective feature alignment across tasks. In contrast, attention distillation alone yields lower AP scores (13.34% on VQAv2 and 13.24% on NEXT-QA) with high forgetting scores (32.08% on VQAv2 and 24.56% on NExT-QA), suggesting limited task adaptation. Combining PL and attention distillation achieves the best results, with AP scores of 39.25% on VQAv2 and 31.70% on NEXT-QA, and the lowest forgetting rates.\nAttention Distillation. Tab. 4 demonstrates the effectiveness of our proposed $L_{Att}$ within QUAD. Unlike prior methods such as Attn-dist (L1) and Asym-Attn [54], which impose L1 or ReLU+L1 losses on raw attention scores, our approach applies cross-entropy over normalized attention maps. QUAD outperforms previous methods, achieving an AP of 39.25% and Forgetting of 4.91% on VQAv2, and an AP of 31.70% with Forgetting of 2.91% on NExT-QA. These results highlight the importance of preserving distributional consistency in attention maps and avoiding unstructured alignment.\nAnalysing Plasticity/Stability. Fig. 3 illustrates the impact of different strategies on plasticity and forgetting in"}, {"title": "6. Conclusion", "content": "In this work, we introduced QUESTION-ONLY REPLAY WITH ATTENTION DISTILLATION (QUAD), a novel distillation-based approach for continual VQA. Unlike conventional methods that store both images and questions, QUAD addresses storage and privacy concerns by retaining only past task questions. This design enables effective regularization without storing sensitive visual data, making it highly practical for privacy-conscious applications.\nComprehensive evaluations on VQAv2 and NExT-QA show that QUAD consistently outperforms both memory-free and memory-rehearsal methods, achieving state-of-the-art generalization and knowledge retention. These results validate question-only replay as an efficient and scalable solution for continual VQA. Future work could explore dynamically generated questions to eliminate memory storage entirely, further enhancing adaptability in real-world multimodal scenarios."}, {"title": "7. Ethics Statement", "content": "Our method, QUAD, is designed to improve continual learning in Visual Question Answering (VQACL) while maintaining generalization and privacy through distilaltion using questions-only. We do not foresee any negative societal impact from this work, as it does not involve the generation of harmful or biased data. However, like any machine learning system, there remains a potential risk if it is applied unethically or without proper oversight. QUAD's design includes mechanisms to enhance privacy, reducing the storage of sensitive visual data. Despite this, its applicability beyond the specific datasets and tasks used in our experiments remains to be thoroughly tested, and we caution against the unconsidered deployment of the method in sensitive applications without further validation."}, {"title": "8. Limitations of QUAD", "content": "While QUAD effectively reduces storage requirements and enhances privacy by eliminating the need to store images, it may be suboptimal for tasks that heavily rely on detailed visual or spatial reasoning. Certain VQA tasks, such as object classification, fine-grained attribute recognition, or spatial relationships, inherently require access to visual information to retain critical knowledge from previous tasks. For instance, as shown in Fig. 3, QUAD struggles to maintain performance on the 'type' task in VQAv2, which depends on visual cues, whereas it performs well on conceptually driven tasks like 'commonsense' reasoning.\nOur findings suggest that question-only replay is particularly well-suited for constrained scenarios where privacy and storage efficiency are primary concerns. However, in settings where high fidelity in visual reasoning is essential, storing a subset of representative images may be necessary"}, {"title": "9. Computational Analysis", "content": "Efficient memory and storage management is crucial for continual VQA, where scalability is a key challenge. This section analyzes storage requirements, computational complexity, and GPU memory usage of our text-only replay approach compared to image-based methods. By storing only past task questions, we significantly reduce storage complexity from O(N \u00b7 (I + Lq + La)) to O(N \u00b7 Lq), where N is the number of stored samples, I is the image size, and Lq and La represent the question and answer lengths in bits.\nIn terms of GPU memory usage, question-only replay has a minimal impact since the number of processed input pairs remains the same. The primary reduction stems from loading fewer images, but this accounts for less than 5% of the total memory footprint, which is dominated by gradients, weights, and activations. This makes our approach particularly appealing in scenarios where storage is constrained but GPU memory availability remains a concern.\nFrom a computational complexity perspective, our method does not introduce any additional overhead. The computational cost remains unchanged when processing images from past or current tasks. The forward and backward passes are identical, ensuring that our approach maintains the same efficiency while significantly improving storage scalability.\nThis analysis validates our design choices, demonstrating that question-only replay can achieve competitive performance while substantially reducing storage requirements. This efficiency makes it highly scalable and practical for real-world deployment."}, {"title": "10. Detailed Description of the VQACL Setting", "content": "This section provides a detailed overview of the Visual Question Answering Continual Learning (VQACL) setting,"}, {"title": "11. Details of Evaluation Datasets", "content": "In this section, we provide a detailed overview of the two datasets used in our evaluation: VQA v2 and NEXT-QA. Each dataset has been carefully structured into different tasks, which are used to evaluate the performance of our continual learning models.\nWe summarize the statistics of each dataset, focusing on both linguistic and object-related tasks. Tables 5 and 6 (previously described) present the linguistic-driven task breakdown, including categories such as Recognition, Commonsense, Count, and others.\nAdditionally, we grouped the objects in each dataset into five distinct object groups to facilitate better understanding and comparison of the models' object recognition capabilities. Tables 7 and 8 offer a detailed breakdown of the objects associated with each group in VQA v2 and NEXT-QA, respectively. This categorization will aid in analyzing how the models perform across different object categories.\nThese two datasets, each structured uniquely in terms of linguistic tasks and object types, allow us to rigorously assess the models in varied real-world scenarios. Together, these benchmarks enable a comprehensive evaluation of the continual learning approaches proposed in this work."}, {"title": "12. Extended Analysis of Plasticity/Stability Trade-Off", "content": "Fig.6 compares the impact of three continual learning strategies on performance across tasks in the NExT-QA dataset. The sequential finetuning baseline (left) demonstrates severe forgetting, with consistently low off-diagonal values. Specifically, tasks like temporal reasoning (TN and TC) exhibit the worst performance, as these tasks require advanced reasoning over time sequences, which is inherently challenging for the model.\nIntroducing pseudo-label distillation through LPL (center) mitigates the issue of forgetting by enforcing output consistency with the previous model. This results in improved cross-domain retention, particularly in easier tasks like 'DB' and 'DL'. However, its performance on complex tasks such as \"DO\" (Descriptive Others) and 'CH' (Causal How) remains suboptimal, as these tasks require the model to maintain intricate visual-linguistic relationships, which LPL alone struggles to address."}]}