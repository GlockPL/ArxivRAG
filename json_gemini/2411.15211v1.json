{"title": "LightLLM: A Versatile Large Language Model for Predictive Light Sensing", "authors": ["Jiawei Hu", "Hong Jia", "Mahbub Hassan", "Lina Yao", "Brano Kusy", "Wen Hu"], "abstract": "We propose LightLLM, a model that fine tunes pre-trained large language models (LLMs) for light-based sensing tasks. It integrates a sensor data encoder to extract key features, a contextual prompt to provide environmental information, and a fusion layer to combine these inputs into a unified representation. This combined input is then processed by the pre-trained LLM, which remains frozen while being fine-tuned through the addition of lightweight, trainable components, allowing the model to adapt to new tasks without altering its original parameters. This approach enables flexible adaptation of LLM to specialized light sensing tasks with minimal computational overhead and retraining effort. We have implemented LightLLM for three light sensing tasks: light-based localization, outdoor solar forecasting, and indoor solar estimation. Using real-world experimental datasets, we demonstrate that LightLLM significantly outperforms state-of-the-art methods, achieving 4.4x improvement in localization accuracy and 3.4x improvement in indoor solar estimation when tested in previously unseen environments. We further demonstrate that LightLLM outperforms ChatGPT-4 with direct prompting, highlighting the advantages of LightLLM's specialized architecture for sensor data fusion with textual prompts.", "sections": [{"title": "1 INTRODUCTION", "content": "Predictive light sensing (PLS) is becoming increasingly significant as the demand for intelligent and sustainable systems grows. For example, by leveraging light sensor signatures, PLS can accurately infer the location of individuals in indoor spaces, enabling seamless indoor positioning using light spectral information (LSI) where GPS signals are unreliable [18] in smart buildings (see Figure 1 Left). Moreover, outdoor solar forecasting [20] for predicting future power generation of solar panels on rooftops based on weather conditions and sunlight availability and indoor solar estimation for predicting the potential solar output at different locations within an indoor space [7], such as a vertical farm (see Figure 1 Right) [6], are becoming critical as buildings and smart devices shift towards sustainability.\nTraditional approaches to solving PLS tasks rely on task-specific deep learning models tailored to each individual application [56]. These models require separate training, manual tuning, and frequent retraining to adapt to changing data distributions, leading to significant engineering overhead. Furthermore, they often struggle to generalize to unseen data or new environments [36], which is particularly problematic in real-world scenarios where domain shifts due to changing conditions are common. Techniques like data augmentation [15] and transfer learning [57] are often employed to mitigate this issue. However, generating high-quality synthetic data is resource-intensive, and transfer learning is limited when the target task diverges significantly from the source domain. Consequently, these models remain both computationally expensive and inflexible, restricting their scalability across diverse PLS tasks.\nLarge Language Models (LLMs), with their vast pre-trained knowledge and cross-domain generalization capabilities, present new opportunities to address the challenges outlined above. Inspired by the success of models like ChatGPT [33], which have demonstrated exceptional performance in reasoning and answering complex questions across diverse contexts in natural language processing, LLMs show promise for offering a unified solution across various IoT applications, including light sensing tasks.\nHowever, simply using LLMs with textual prompts, as explored in recent studies [22, 52], is insufficient for these tasks. LLMs are not inherently designed to process sensor data, such as light spectral information or time-series solar output, nor can they generate non-text outputs required for specialized applications like location classification or energy prediction. While recent efforts have extended LLMs beyond traditional language tasks\u2014such as NetLLM [49] for networking and MentalLLM [51] for mental health assessment-these approaches still lack the ability to capture the domain-specific spatial, spectral, and environmental nuances critical for effective PLS sensing across different scenarios and environments.\nWe propose LightLLM, an innovative model designed to tackle light-based sensing tasks by leveraging the strengths of a pre-trained LLM. It integrates three key components: a task-specific sensor data encoder that extracts meaningful features from sensor inputs, a task-specific natural language prompt that provides contextual information about the environment, and a fusion layer that combines both sensor features and contextual prompts into a unified feature vector. This fused representation is then processed by the pre-trained LLM, which remains entirely frozen but fine-tuned using the LoRA (Low-Rank Adaptation) method. This unique architecture allows LightLLM to efficiently utilize the pre-existing general knowledge of LLMs while seamlessly integrating domain-specific sensor data, enabling it to handle specialized light sensing tasks with high adaptability and minimal retraining effort.\nKey contributions of this paper can be summarized as follows:\n\u2022 We present LightLLM, a general-purpose framework for adapting LLM to a wide range of light-based tasks with minimal effort. To our knowledge, this is the first attempt to use LLM for PLS.\n\u2022 We conduct comprehensive real-world experiments to validate the effectiveness of LightLLM for three distinct PLS tasks, light-based indoor localization, outdoor solar forecasting, and indoor solar estimation. Our results show the superior performance of"}, {"title": "2 Methodology", "content": "Our proposed framework, LightLLM, integrates advanced encoders for various data modalities with the capabilities of LLMs to address the challenges of diverse tasks. The framework, as illustrated in Figure 2, comprises five key components as followed.\n\u2022 Task-Specific Encoders: Tailored to individual tasks, such as Graph Neural Networks (GNNs) [50] for localization, Temporal Convolutional Networks (TCNs) [25] for forecasting, and Convolutional Neural Networks (CNNs) [27] for energy estimation. For tasks like indoor localization, a custom Knowledge Graph (KG) is integrated to represent a richer context, enabling improved interpretation of sensor data.\n\u2022 Task-Specific Knowledge Prompts: To provide structured contextual information, LightLLM employs task-specific prompts that encode domain knowledge essential to each task. These prompts are embedded as textual inputs, aligning sensor data properties and environmental factors with the LLM's knowledge base.\n\u2022 Latent Fusion Layer (LFL): A specialized component that integrates the encoded data with latent representations from the LLM. Task-specific prompts are leveraged to enhance the knowledge representation of sensor data.\n\u2022 LoRA: An efficient fine-tuning mechanism that adapts the LLM to specific tasks with minimal computational overhead."}, {"title": "2.1 System Overview", "content": "Our proposed framework, LightLLM, integrates advanced encoders for various data modalities with the capabilities of LLMs to address the challenges of diverse tasks. The framework, as illustrated in Figure 2, comprises five key components as followed.\n\u2022 Task-Specific Encoders: Tailored to individual tasks, such as Graph Neural Networks (GNNs) [50] for localization, Temporal Convolutional Networks (TCNs) [25] for forecasting, and Convolutional Neural Networks (CNNs) [27] for energy estimation. For tasks like indoor localization, a custom Knowledge Graph (KG) is integrated to represent a richer context, enabling improved interpretation of sensor data.\n\u2022 Task-Specific Knowledge Prompts: To provide structured contextual information, LightLLM employs task-specific prompts that encode domain knowledge essential to each task. These prompts are embedded as textual inputs, aligning sensor data properties and environmental factors with the LLM's knowledge base.\n\u2022 Latent Fusion Layer (LFL): A specialized component that integrates the encoded data with latent representations from the LLM. Task-specific prompts are leveraged to enhance the knowledge representation of sensor data.\n\u2022 LoRA: An efficient fine-tuning mechanism that adapts the LLM to specific tasks with minimal computational overhead."}, {"title": "2.2 Knowledge Graph for Enhanced Data Representation", "content": "In LightLLM, the custom KG is designed to capture complex, spatially dependent relationships that are critical for tasks such as LSI indoor localization. By focusing on relevant sensor interactions, the KG enhances the interpretability and efficiency of GNN-based encoders by concentrating on task-specific correlations.\nKGs have demonstrated their effectiveness in representing structured data across various domains. Recent research highlights their ability to improve task performance by organizing information into a structured, relational format [8]. In the context of smart building systems, standard ontologies like Brick [5] provide a schema for smart building metadata but fall short in addressing the specific needs of tasks such as LSI indoor localization. For example, such tasks require detailed spatial relationships informed by sensor orientation, light angles, and the presence of obstacles. To bridge this gap, we design a custom KG that captures these unique dependencies, enabling enhanced task performance.\nThe KG consists of nodes representing sensors and light sources, each with properties such as coordinates (x, y, z), orientation, and detection range. The detailed Node Connection Algorithm is outlined in Algorithm 1, which constructs a KG by identifying meaningful connections between sensors and light sources based on spatial criteria. It consists of two phases as follows. In phase 1, for each pair of sensors (si, sj) (Line 2), the Euclidean distance is computed (Line 3). If the distance is within distance_threshold, their fields of view (FOVs) are calculated (Lines 5-6). A \u201ccorrelated\" edge is added if their FOVs overlap (Line 7) and no obstacles block the path (Line 8). In phase 2, for each sensor si and light source lk (Line 10), the horizontal distance (dh) and vertical difference (d) are calculated (Lines 11-12). An edge is created if dh and dv are less than or equal to the thresholds (Line 13) and no obstacles exist (Line 14).\nOur KG includes obstacle-free connections only, and focuses on meaningful connections to reduce complexity. Furthermore, it enables dynamic interactions based on FOV and spatial criteria to ensure realistic modeling. This approach ensures the KG effectively captures relationships critical for enhancing LightLLM's interpretation of sensor data."}, {"title": "2.3 Task-Specific Encoders", "content": "The task-specific encoders in LightLLM are designed to extract meaningful representations from diverse types of input data, ensuring that the most appropriate encoding techniques are applied."}, {"title": "2.3.1 Localization Encoder: Graph Neural Network", "content": "For indoor localization, we employ a GNN to interpret spatial relationships among light sensors. The GNN leverages a graph structure where nodes represent sensor readings, and edges represent spatial relationships, as detailed in Section 2.2. By aggregating information from neighboring nodes, the GNN captures inter-sensor dependencies, enhancing spatial awareness and improving localization accuracy."}, {"title": "2.3.2 Forecasting Encoder: Temporal Convolutional Network", "content": "For the task of solar energy forecasting, we employ a TCN to handle the time-series data from outdoor solar cells. TCNs are ideal for capturing long-range dependencies in time-series data due to their architecture, which combines causal convolutions and dilation."}, {"title": "2.3.3 Energy Estimation Encoder: Convolutional Neural Network", "content": "To estimate indoor solar energy output, we use a CNN to analyze spectral sensor data. The CNN extracts hierarchical features across different wavelengths, helping identify both low-level and high-level patterns in the spectral data. This capability is critical for modeling the complex interactions between light spectra and solar cell energy output."}, {"title": "2.4 Task-Specific Knowledge Prompts", "content": "For each task, a task-specific prompt is generated to provide additional contextual knowledge. These prompts embed domain-specific features such as spatial information for localization or environmental conditions for solar forecasting. This added context helps guide the model in understanding the relationship between input features.\nIt ensures that LightLLM leverages domain knowledge tailored to the unique challenges of each task. For example, in unseen environments where sensor layouts or lighting conditions might differ from the training set, the pre-trained LLM already possesses generalized knowledge about spatial relationships and environmental patterns. When combined with the specific spatial configurations provided in the prompt, LightLLM can better understand how a sensor's position or a light source's distribution influences the readings, helping the inference of model."}, {"title": "2.5 Adaptive Latent Fusion Layer", "content": "The LFL in LightLLM is a key component designed to integrate encoded features from task-specific encoders with prompt embeddings generated from task-specific knowledge prompts. This integration is achieved through an multi-head attention mechanism [42], allowing the model to leverage the strengths of both the feature encoders and the LLM. The LFL ensures that the encoded data is aligned with the LLM's latent space, facilitating effective fusion of multimodal information.\nThe encoded features Fenc from the task-specific encoder and the embedded prompt Pembed are fused using a multi-head attention mechanism within the LFL. Specifically, the query Q, key K, and value V matrices are computed from the encoded features and the prompt embeddings as follows:\n$Q = W_Q F_{enc}, K = W_K P_{embed}, V = W_V P_{embed}.$\nHere, the prompt embeddings Pembed encapsulate structured, task-specific knowledge that complements the encoded sensor features. By assigning Pembed as both the key and value, the attention mechanism identifies relevant knowledge and integrates it into the encoded features Fenc."}, {"title": "2.6 Low-Rank Adaptation", "content": "To efficiently adapt the pre-trained language model for our specific downstream tasks, we utilize LoRA [17], which introduces low-rank matrices around the original pre-trained LLM, allowing for dimensionality reduction and expansion, effectively performing a rank adaptation. During training, the parameters of the LLM remain frozen, while only the low-rank matrices A and B are trained. This significantly reduces the number of trainable parameters while preserving the model's capacity to adapt to new tasks.\nInitially, the reduction matrix A is randomly initialized using a Gaussian distribution, while the expansion matrix B is initialized as a zero matrix. This setup ensures that the pre-trained behavior of the LLM is maintained at the beginning of training, with the adaptation gradually introduced as the matrices are trained.\nThe modified weight matrix W' is computed as follows:\n$W' = W + \\alpha AB,$\nwhere W is the pre-trained weight matrix (e.g., a frozen LLM). The dimensions of low-rank matrices $A \\in R^{d \\times r}$ and $B \\in R^{r \\times d}$ are significantly smaller than the full weight matrix $W \\in R^{d \\times d}$, significantly reducing the number of parameters being fine-tuned.\nDuring the forward pass, the input x is computed as follows:\n$W'x = Wx + \\alpha(AB)x.$\nThis approach is analogous to a residual connection, where the low-rank updates simulate the full fine-tuning process by introducing an adaptable layer on top of the pre-trained LLM. When the rank r equals the dimension of W, LoRA effectively performs full fine-tuning, but with r < d, LoRA provides a more computationally efficient way to adapt the model. In the inference phase, LoRA introduces minimal additional latency, with parallel computations for Wx and (AB)x.\nIn our model, we emperically chose the settings r = 64, \u03b1 = 64, and dropout of 0.1, to achieve an optimal balance between task-specific adaptation and preserving the pre-trained knowledge of the LLM, while maintaining computational efficiency."}, {"title": "2.7 Task-Specific Output Layer", "content": "The output layer of LightLLM is designed to handle the specific requirements of different tasks by employing task-specific heads. This ensures that the final model outputs are appropriate for each application.\nFor example, for the localization task, the output layer is configured as a probability classification head [14]. This head outputs a probability distribution over possible locations, allowing the model to predict the most likely position based on the input data. The classification head uses a softmax activation function to convert the model's logits into probabilities, thereby facilitating accurate location predictions.\nThe flexible design of this task-specific head allows LightLLM to be easily adapted to different tasks. By incorporating the appropriate headers, the model is able to provide accurate, task-relevant output, whether it involves classification, regression, or other output formats. This module ensures that the model's predictions are not only accurate but also customized to the unique characteristics of each PLS task."}, {"title": "3 EVALUATION", "content": null}, {"title": "3.1 Light Spectral Localization", "content": "Indoor localization using light spectral information has proven to be a highly effective technique for achieving passive Visible Light Positioning (VLP) in indoor environments. This method leverages the unique properties of light in different spaces to estimate the location of users. One of the leading systems for spectral-based localization is Iris [18], which employs a conditional Generative Adversarial Network (cGAN) for data augmentation to enhance its performance.\nTo evaluate the performance of LightLLM in indoor localization, we collected data from two distinct indoor environments: a large office and a apartment. In the office environment, we deployed 27 spectral sensors placed strategically throughout the 108 square meter area. In the apartment environment, we deployed 17 spectral sensors across the 25 square meter space. Each environment was carefully configured to capture spectral data over multiple sessions.\nIn the seen environment tests, where both training and testing data are collected from the same indoor room, LightLLM shows comparable or slightly better performance than Iris (see Figure 6). For example, in the 90th percentile error, where LightLLM achieves 0.608m vs 0.705m for Iris in the apartment environment, and LightLLM achieves 0.89m, while Iris performs slightly worse at 0.60m of 90th percentile error in the office environment.\nIn the unseen environment, where the training data does not include the environment used in the test set (e.g., train with apartment's dataset and test with office's dataset). This setting could evalute how well the system generalizes to new environments, which is a significant challenge for traditional models like Iris.\nThe results in Figure 7 show that Iris sees a dramatic increase in error in both environments, with a median error of 3.93m and 4.35m, respectively, a significant degradation from its seen performance. In contrast, LightLLM maintains far better generalization with a median error of only 0.98m in office and 1.19m in apartment, achieving 4.4x improvement. Similarly, 75th percentile and 90th percentile error further emphasize this difference, for example, LightLLM achieves 1.99m and 3.46m compared to 5.05m and 5.61m for Iris in the office. This demonstrates LightLLM's superior ability to handle difficult and unseen localization tasks, where traditional method failed to work.\nIn summary, LightLLM exhibits competitive performance in seen environments but shows superior generalization capabilities in unseen environments. The ability of LightLLM to adapt to new environments with minimal degradation in performance highlights its advantage over traditional systems like Iris, which almost impossible to work in a completely new environment with unfamiliar data. This makes LightLLM a more robust and scalable solution for light spectral indoor localization, especially in real-world applications where environments are diverse and complex."}, {"title": "3.2 Outdoor Solar Energy Forecasting", "content": "Solar forecasting plays a crucial role in optimizing energy management systems and improving the reliability of renewable energy sources. Accurate forecasts of photovoltaic (PV) generation help ensure energy grid stability and enable more efficient integration of solar energy into the energy mix. To achieve these predictions, solar forecasting models typically rely on a variety of data models, including historical PV generation and environmental data, such as images of the sky that capture cloud movement."}, {"title": "3.3 Indoor Solar Energy Estimation", "content": null}, {"title": "3.3.1 Challenges in Real-World Solar Energy Estimation.", "content": "With the increasing deployment of solar cells for indoor energy harvesting applications, such as those used in low-power IoT devices [1] or used as interactive surfaces [30], presents an opportunity to harness light more effectively for energy generation in indoor spaces. Given that different types of solar cells exhibit unique responses to varying light conditions, optimizing their placement and understanding their energy output in real-world environments becomes critical. This optimization can greatly enhance the efficiency of solar energy systems in indoor settings.\nFortunately, existing technology allows us to simulate and collect spectral sensor data [19]. However, the challenge lies in accurately translating this spectral data into solar energy output for each unique solar cell. While mathematical models exist to predict this conversion under idealized conditions, their accuracy diminishes in complex, real-world scenarios due to various discrepancies between theory and practice.\nIn theory, the short-circuit current density Jsc is a parameter in assessing the performance of a solar cell. As described in [48], this value depends on the external quantum efficiency (EQE), which is the proportion of photo-generated electrons to incident photons, and the intensity of photons at each wavelength Pin (\u03bb). Following the approach in [29], the total current generated by a solar cell can be expressed as:\n$Current = A \\cdot \\int_{Amin}^{Amax} \\alpha(\\lambda) \\cdot I(\\lambda) \\cdot \\lambda \\cdot d\\lambda,$"}, {"title": "3.3.2 Performance Evaluation Results Under Seen Environment.", "content": "We then tested several machine learning methods under a seen environment, where the training set contained data from the same environment as the test set. As summarized in Table 3. In this controlled setting, data-driven methods significantly outperformed the physical equation-based approach. Among the data-driven methods, Random Forest achieved the lowest MAPE of 2.58%, while LightLLM closely followed with a MAPE of 2.59%. In terms of MSE, LightLLM achieved the best performance 5.72. This demonstrates that, given sufficient training data from the same environment, the data-driven models can effectively capture the relationship between spectral data and solar cell energy output.\nResults Under Unseen Environment. To further test the generalization of the models, we evaluated them on unseen environments, where the test set was drawn from environments not present in the training set. We specifically altered the lighting and spatial configuration within the room. For example, we introduced two diffused standing lamps, and rearranged the room layout by changing the furniture. The results for this setting are shown in Table 3. Most machine learning models suffered a significant performance drop. For example, Linear Regression and Random Forest showed MAPE values exceeding 140%, illustrating their tendency to overfit and their lack of generalization capability. In contrast, LightLLM demonstrated remarkable robustness, achieving a MAPE of 26.33% and MSE of 263.21, the best among the compared methods. We note that LightLLM consistently outperforms the traditional equation-based approach in both seen and unseen environments."}, {"title": "3.4 Ablation Study", "content": null}, {"title": "3.4.1 LightLLM vs. Direct Prompting", "content": "To evaluate the effectiveness of LightLLM against a simpler method of directly prompting an LLM (e.g., ChatGPT-4), we firstly conducted an experiment using the indoor localization task as a representative. We compared both zero-shot and few-shot prompting approaches using models like GPT-4, GPT-3.5 [33], Llama 3 [41], and Mistral Large 2 [3]. Our goal was to determine whether carefully crafted prompts, including chain-of-thought (CoT) [45] prompting strategy, could compete with the task-specific design of LightLLM.\nPrompting Setup. The ability of LLMs to understand and reason across a wide range of tasks makes them powerful, particularly when paired with well-constructed prompts. Recent studies [28] have demonstrated that proper prompt engineering can significantly improve LLM performance on domain-specific tasks. Therefore, for the direct prompting method, we created a detailed prompt, leveraging CoT reasoning to improve its ability to process the sensor data step by step."}, {"title": "3.4.2 Impact of Prompt Knowledge and LFL", "content": "The inclusion of prompt knowledge and the LFL plays a important role in enhancing LightLLM's performance, particularly in unseen scenarios where the system must generalize to new environments not encountered during training. To demonstrate the importance of these components, we compare the performance of LightLLM with and without the prompt knowledge and LFL.\nAmong the tasks evaluated, localization shows the most notable improvement with prompt knowledge and LFL, in the unseen scenario, LightLLM with these components achieves a 90th percentile error of 3.46 meters, a considerable improvement over the 5.41 meters when they are removed. For indoor solar estimation, a similar trend can be observed. The system with prompt knowledge, which includes detailed descriptions of solar cell placement and light conditions, reduces the prediction error, achieving a MAPE of 26.33%, compared to 35.42% without those. Similarly, we could also observe an improvement in the solar forecasting task, CRPS decrease from 2.80kW to 2.52kW."}, {"title": "3.4.3 Impact of Task-specific Encoder and Low-Rank Adaptation", "content": "Both task-specific encoders (TSE) and LoRA are essential in affecting the performance of LightLLM across various tasks. To assess their impact, we evaluated the model's performance with and without these components.\nFor example, for the task of solar energy estimation, in unseen scenario, LightLLM with LoRA reduces the average MAPE from 28.45% to 26.33%. Moreover, for the solar energy forecasting task, the inclusion of a task-specific encoder brings a 12% improvement in the CRPS metric, demonstrating its ability to capture the temporal dependencies in time-series data.\nIn summary, both LoRA and task-specific encoders provide considerable performance enhancement, with LoRA ensuring efficient fine-tuning for task-specific adjustments and task-specific encoders capturing the unique characteristics of each task. Together, these components make LightLLM highly accurate, flexible, and scalable across diverse applications and environments."}, {"title": "3.4.4 Impact of Knowledge Graph", "content": "In this section, we evaluate the contribution of the KG mentioned in Section 2.2. In the ablation setting, we remove the KG creation in LightLLM, and instead, using raw data as input. We evaluated the models based on our LSI Localization task.\nIn our evaluation results, the model with KG achieved significantly lower error, with a median error of 0.98 meters. In contrast, removing the KG led to a 50% decrease in localization accuracy, with the median error increasing to 1.42 meters. The findings validate the design rationale presented in Section 2.2, demonstrating that the KG's structured data is instrumental in achieving precise localization within the LightLLM framework."}, {"title": "3.4.5 Impact of LLM in LightLLM", "content": "To assess the importance of the LLM component within LightLLM, we conducted an ablation study by comparing three versions of our model as shown in Figure 12: LightLLM (full version), Light_Transformer (replacing the LLM component with a single transformer), and Light_w/oLLM (completely removing the LLM, using a simplified model). The purpose of this experiment is to evaluate whether the LLM plays a critical role in the performance of LightLLM, particularly for tasks like solar energy forecasting. The results in Figure 13 demonstrated that LightLLM (with the LLM component) consistently outperformed the other two versions across all evaluated metrics. This ablation study thus reinforces the importance of incorporating LLMs in LightLLM."}, {"title": "4 FUTURE DIRECTIONS", "content": "Exploration of Different LLM Models: Our next attempt is to understand whether and how different pre-trained LLM models may affect the performance of LightLLM. To this end, we compared the performance of LLaMA-7B, which has 7 billion parameters, against GPT-2 with only 1.5 billion parameters across all tasks. The results in Figure 14 demonstrated that LLaMA-7B consistently outperformed GPT-2.\nThe improvements in performance using LLaMA-7B can be attributed to its larger model size and more recent training, which have equipped it with a more extensive knowledge base and greater capacity to model complex patterns, enabling better reasoning and adaptation for spectral and temporal data."}, {"title": "5 RELATED WORK", "content": null}, {"title": "5.1 PLS Tasks and Algorithms", "content": "The field of PLS tasks encompasses applications like indoor localization, solar energy forecasting, and indoor solar energy estimation. These tasks are critical for optimizing energy systems, enhancing smart environments, and improving the efficiency of renewable energy grids.\nIndoor localization using light spectral information, on the other hand, is essential for services like smart navigation and asset tracking in smart buildings. Data collected from light sensors offers a promising alternative to traditional localization methods. Existing models are tailored to specific environments, and it usually relies on fingerprint collection to train the model, which requires extensive data collection and may not generalize well to new spaces [18, 24, 43].\nSolar radiation forecasting helps predict future energy output, improving the reliability and efficiency of energy distribution. Recent advancements in this field have largely focused on deep learning-based methods, such as combining CNN with recurrent neural network (RNN) [16, 34, 39, 54]. There is also a growing trend of replacing RNN with transformers to better capture temporal patterns in the data and improve forecasting accuracy [32].\nWith the development of efficient solar cells for indoor environments [21, 30, 48], there is a growing need for accurate indoor solar energy estimation. This task focuses on accurately predicting the energy output of photovoltaic systems in indoor conditions to maximize their utility. Current approaches often rely on the solar models that have been presented that use mathematical linear [53] and nonlinear functions [12].\nDespite the progress made in solving these tasks, several significant challenges remain. One of the primary issues is the lack of generalization across different environments or unseen data [36]. While data augmentation [15, 55] can help improve performance in the short term, it is computationally expensive and resource-intensive, and it does not fundamentally address the issue of generalization across diverse tasks. Another limitation arises from the fixed nature of these models: a model designed for indoor localization cannot be easily repurposed for solar energy forecasting without re-engineering the entire architecture [46]. This rigidity limits the development of a scalable, multi-task solution capable of handling the diversity of data modalities in PLS tasks."}, {"title": "5.2 Large Language Model and Applications", "content": "In recent years, LLMs achieved a great success in the field of natural language processing (NLP). Models like BERT [10], GPT [33], and LLaMA [41], which are built on the Transformer architecture, have demonstrated their ability to perform reasoning, translation and text generation tasks. One of the most significant advancements in LLMs is their ability to learn from massive datasets, capturing latent knowledge that allows them to perform well across diverse tasks.\nThe flexibility of LLMs has made them increasingly attractive for applications beyond NLP. Techniques like LoRA [17] and prefix-tuning [26] have made it possible to fine-tune LLM for specific downstream tasks with minimal computational overhead. This has led to the exploration of LLM in non-textual domains, including cross-modal learning where text is combined with images, audio, or sensor data.\nRecent studies have shown the potential of LLM in multimodal tasks, where they are adapted to handle inputs from various sources beyond text. For example, models like CLIP [37] successfully integrate image-text pairs, allowing LLM to perform visual reasoning tasks. Similarly, PaLM-E [11] has been applied to control robotics tasks, showcasing how LLM can extend their capabilities into non-linguistic domains by integrating structured knowledge from other modalities."}, {"title": "6 CONCLUSION", "content": "We presented LightLLM, a unified framework designed to address diverse PLS tasks such as indoor localization, outdoor solar forecasting, and indoor solar estimation. The Latent Fusion Layer and task-specific prompts enable effective multimodal sensor fusion, providing a rich context information. Moreover, by integrating LLM with task-specific encoders and utilizing techniques like LoRA for efficient fine-tuning, LightLLM achieved significant improvements in accuracy and adaptability, particularly in unseen scenarios. Our extensive evaluations showed that LightLLM outperforms traditional methods and and simple prompting approaches, in handling complex, real-world data. Though evaluated on different tasks, LightLLM's design offers a general-purpose solution for other context-rich sensing applications, establishing a foundation for LLM-based frameworks in PLS."}]}