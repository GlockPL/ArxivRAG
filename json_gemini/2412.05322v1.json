{"title": "p-NeRF: Leveraging Attenuation Priors in Neural Radiance Field for 3D Computed Tomography Reconstruction", "authors": ["Li Zhou", "Changsheng Fang", "Bahareh Morovati", "Yongtong Liu", "Shuo Han", "Yongshun Xu", "Hengyong Yu"], "abstract": "This paper introduces p-NeRF, a self-supervised approach\nthat sets a new standard in novel view synthesis (NVS) and\ncomputed tomography (CT) reconstruction by modeling a\ncontinuous volumetric radiance field enriched with physics-\nbased attenuation priors. The p-NeRF represents a three-\ndimensional (3D) volume through a fully-connected neu-\nral network that takes a single continuous four-dimensional\n(4D) coordinate\u2014spatial location (x, y, z) and an initial-\nized attenuation value (p)\u2014and outputs the attenuation co-\nefficient at that position. By querying these 4D coordinates\nalong X-ray paths, the classic forward projection technique\nis applied to integrate attenuation data across the 3D space.\nBy matching and refining pre-initialized attenuation val-\nues derived from traditional reconstruction algorithms like\nFeldkamp-Davis-Kress algorithm (FDK) or conjugate gra-\ndient least squares (CGLS), the enriched schema delivers\nsuperior fidelity in both projection synthesis and image re-\nconstruction, with negligible extra computational overhead.\nThe paper details the optimization of p-NeRF for accu-\nrate NVS and high-quality CT reconstruction from a limited\nnumber of projections, setting a new standard for sparse-\nview CT applications.", "sections": [{"title": "1. Introduction", "content": "X-ray imaging, a form of penetrative imaging to capture\ndetailed internal structures, is widely applied across fields,\nsuch as medicine, industrial inspection, materials science,\netc. [29, 41]. While high-resolution projections can pro-\nduce high-quality CT images, they require extended expo-\nsure time, and excessive radiation pose health risks, par-\nticularly in medical applications. Consequently, one of\nthe recent research topics focused on sparse-view and low-\ndose techniques to reduce radiation exposure and/or in-\ncrease temporal resolution while maintaining diagnostic\nutility [32, 44, 48], particularly for the situations where\nthere are no slip-ring in the CT scanner. However, in cases\nlike 3D cone beam CT (CBCT) reconstruction, sparse-view\ndata often limits resolution and introduces artifacts com-\npared with full-view data, which highlights the need for\nadvanced computational methods to accurately synthesize\nviews and generate anatomical structure from a limited\nnumber of projections.\nCT reconstruction techniques can be categorized into an-\nalytical, iterative, and hybrid data-driven approaches. Ana-\nlytical methods, such as filtered backprojection (FBP) [19]\nand its cone-beam variants [14], are effective with dense\nprojection data but fall short under sparse-view conditions.\nIterative approaches, including methods like simultaneous\nalgebraic reconstruction technique (SART) family [2, 17]\nand total variation (TV)-minimization techniques [50], ad-\ndress some of these limitations by optimizing the recon-\nstruction through iterative updates. However, these meth-\nods require considerable computational resources and are\nnot always practical for real-time or large-scale applica-\ntions. Recently, hybrid data-driven approaches that incor-\nporate deep learning have enhanced the traditional tech-\nniques by using neural networks to predict, fill gaps, and\nimprove denoising for sparse data [18, 33]. While these\nmodels achieve impressive results, they often require exten-\nsive labeled datasets for training, which limits their adapt-\nability and generalizability to different datasets without fur-\nther fine-tuning. Given these limitations, self-supervised\nmethods offer a promising alternative by reducing reliance\non labeled data and improving adaptability across various\nreconstruction tasks. Inspired by advancements in 3D re-\nconstruction in computer vision, recent works [1, 8, 49, 51]\nhave explored the applications of self-supervised methods\nfor CT reconstruction, where internal structures are cap-\ntured through X-ray transmission imaging rather than re-\nflective imaging.\nIn the computer vision field, 3D reconstruction typically\nrepresents shapes as discrete point clouds or meshes. Im-\nplicit neural representations (INRs) have become popular"}, {"title": "2. Related Work", "content": "CT reconstruction methods are broadly categorized into an-\nalytical, iterative, and hybrid data-driven approaches. Foun-\ndational analytical algorithms, such as filtered backprojec-\ntion (FBP) [19] and its cone-beam extension, Feldkamp-\nDavis-Kress (FDK) [14], reconstruct attenuation coeffi-"}, {"title": "2.1. 3D CT Reconstruction", "content": "cients from projections by solving the Radon transform.\nVariants with different filters and Parker weights improve\nimage quality under dense data but produce artifacts under\nsparse-view conditions. Iterative algorithms address these\nlimitations by framing reconstruction as a maximum a pos-\nteriori (MAP) problem, solved through iterative optimiza-\ntion. Gradient-based approaches like the algebraic recon-\nstruction technique (ART) family (SART [2], SIRT [17],\nand OS-SART [50]) vary in update strategy: SART and\nSIRT update the full projection set at once, yielding high\nquality images with high computational cost, and OS-\nSART strikes a balance with efficient and subset-based up-\ndates. Total variation (TV) minimization based methods\n(e.g., ASD-POCS [13], OS-ASD-POCS [50], and AwASD-\nPOCS [40]) iteratively refine reconstructions to suppress\nnoise, with variants improving computational efficiency\nor edge preservation. Krylov subspace algorithms (e.g.,\nCGLS [12], LSQR [15]) achieve faster convergence by fo-\ncusing on the eigenvectors of the residual in descending or-\nder, which allows for increased convergence rates compared\nto the SART family. While the SART-based methods are ef-\nfective with high-quality projection data, Krylov subspace\nalgorithms, like CGLS, are well-suited for handling large\ndatasets or low-quality data, offering both speed and robust-\nness, especially in CT denoising.\nHybrid data-driven approaches combine the traditional\nmethods with deep learning. These methods enhance re-\nconstruction by using neural networks to 1) predict and fill\ngaps in limit-view or sparse-view projections [3, 22, 26],\n2) directly infer attenuation coefficients as denoising tasks\nfrom training data [25, 44, 53], and 3) optimize differen-\ntiable processes to accelerate computations [39, 46, 47].\nWhile these models achieve impressive results and require\nminimal data during inference, they depend on extensive,\ndomain-specific datasets for training, which restricts their\nadaptability to different applications without further fine-\ntuning. Consequently, hybrid data-driven models excel\nin specific contexts but face challenges in generalizability\nacross different CT reconstruction scenarios. Given these\nlimitations, studying self-supervised methods could provide\na promising alternative by reducing reliance on labeled data\nand improving adaptability across different reconstruction\ntasks."}, {"title": "2.2. Implicit Neural Representation and Rendering", "content": "Learning implicit neural representations (INRs) has be-\ncome a popular approach in 3D scene reconstruction, of-\nfering a way to transform discrete data points into con-\ntinuous functions for learning-based 3D geometry mod-\neling [16, 30, 35]. Neural rendering leverages INRs to\nmap discrete data to coordinate-based continuous represen-\ntations, typically using implicit functions parameterized by\nneural networks [28, 31, 42]. One prominent method is the\nneural radiance field (NeRF) [31], a model that has set a\nstandard for high-quality novel view synthesis.\nFor reflective imaging, NeRF models a scene by map-\nping spatial position (x, y, z) and viewing direction (\u03b8, \u03c6)\nalong camera rays to RGB color (r, g, b) and volume den-\nsity (\u03c3). This allows NeRF to produce high-quality novel\nviews by integrating color and densities along rays through\nvolumetric rendering [23]. Numerous works have aimed to\nimprove NeRF's efficiency [5, 6, 9, 10], and extend its ap-\nplication scope, such as generative modeling, unbounded\nscenes, and RGB-D synthesis. More recently, INR-based\nmethods have been explored for CT image reconstruc-\ntion [8, 51, 52], transforming discrete samples into contin-\nuous representations of internal structures.\nFor X-ray imaging, adapting NeRF is necessary to\naddress fundamental differences from reflective imaging.\nUnlike visible light, which reveals surface color and re-\nflectance, X-ray penetrates objects to capture internal\nstructures through attenuation. Neural attenuation fields\n(NAF) [51] modify the NeRF framework for 3D CBCT re-\nconstruction by mapping spatial positions (x, y, z) directly\nto attenuation coefficients (p). Follow-up works [11, 21,\n27, 38] refined this method by introducing advanced mod-\neling functions or complex encoding techniques, such as\nTransformer-based network in SAX-NeRF [8]. However,\nthese adaptations often increase computational demands.\nTo address this issue, we introduce attenuation priors to\nINR-based CT reconstruction algorithms, achieving per-\nformance improvement with minimal computational over-\nhead."}, {"title": "3. Method", "content": "With appropriate preprocessing steps, the measurement of\nX-ray attenuation can be approximately modeled by a linear\nintegral, represented as:"}, {"title": "3.1. X-ray Sampling", "content": "$\nI(r)\n=\n\\int_{t_n}^{t_f} p(r(t)) dt,\n$"}, {"title": "", "content": "where p(r(t)) represents the attenuation coefficient at each\npoint r(t) parametrized by t along the path, and $t_n$ and $t_f$\nare the near and far limits to account for the material's ef-\nfective attenuation coefficients along the ray path. To align\nwith the voxel grid of view, the ray is divided into evenly\nspaced bins within the near and far limits, with one point\nuniformly sampled within each bin. Eq. (1) can be dis-\ncretized as:"}, {"title": "", "content": "$\nI(r) = \\sum_{j=1}^{M} P_j \\Delta t,\n$"}, {"title": "", "content": "where M is number of points sampled along the ray, $P_j$\ndenotes the attenuation coefficient at the each sampled bin"}, {"title": "", "content": "$\nR =\n\\begin{pmatrix}\ncos(a) & sin(a) & 0\\\\\n-sin(a) & cos(a) & 0\\\\\n0 & 0 & 1\n\\end{pmatrix}\n$"}, {"title": "3.2. Attenuation Priors Scene Representation", "content": "We follow the conventional idea of NeRF [31], adapting\nit for X-ray imaging by using attenuation values instead\nof color and density. The following pipeline, illustrated\nin Fig. 1, details our approach to integrate attenuation priors\ninto a neural radiance field.\nAttenuation Modeling: We model tomographic images\nas a continuous 4D attenuation field, represented as a func-\ntion:"}, {"title": "", "content": "$\nF_{\\Theta}: (x, y, z, p_0) \\rightarrow (\\rho),\n$"}, {"title": "", "content": "where (x, y, z) is a 3D spatial coordinate, $p_0$ represents an\ninitialized attenuation value, and $\u03c1$ denotes the learned at-\ntenuation coefficient. The neural network, parameterized\nby \u0398, is optimized to map each 4D input to its correspond-\ning attenuation value, yielding a continuous representation\nof the attenuation field.\nAttenuation Initialization: The initialization of atten-\nuation priors begins with the traditional reconstruction al-\ngorithms, such as FDK and CGLS, to produce initial at-\ntenuation estimates, $p_0$, for the 3D attenuation map in the\nglobal coordinate system. While these voxel-based initial\nvalues provide a structured foundation, further refinement\nis needed to achieve a continuous, high-fidelity reconstruc-\ntion. To this end, we employ three interpolation meth-\nods\u2014mean, nearest neighbor, and trilinear\u2014to smooth and\nadapt attenuation values at points where X-ray paths inter-\nsect the voxel grid, as shown in Fig. 2. The attenuation\nvalue at a sampled point is determined using 8 neighboring\nvoxel vertices via their mean, nearest neighbor, or trilin-\near interpolation. This initialization pipeline offers robust\npriors that support accurate projection synthesis and high-\nquality reconstruction from any view.\nFeature Encoding: Previous studies [20, 37] have\nshown that deep networks tend to favor low-frequency rep-\nresentations, limiting their ability to capture fine details in\ncolor and geometry. By mapping low-dimensional inputs\ninto a higher-dimensional space with high-frequency encod-\nings, neural networks better capture these high-frequency\nvariations. Given that X-ray imaging naturally features ho-\nmogeneous tissue regions with sharp boundaries at anatom-\nical transitions, resulting low variation of the image, we\nadopt a learning-based encoding mechanism to leverage\nthese properties. For spatial positions, we use a hash en-\ncoder [34]\u2014a sparse, learning-based encoding method\u2014to\nefficiently represent position details. For attenuation val-\nues, we apply a lightweight linear transformation to refine\nthe input for enhanced accuracy in reconstruction.\nModel Optimization: The optimization process of p-\nNeRF involves minimizing the difference between the pre-"}, {"title": "", "content": "$\nLoss(\\Phi, \\Psi, A) = \\sum_{r \\in R} ||I(r) \u2013 \\hat{I}(r) ||^2,\n$"}, {"title": "", "content": "where I(r) and \u00ce(r) are real and synthesized projections\nfrom ray r respectively, and R is a batch of rays. I is the\nlearned hash encoder for spatial location (x, y, z), \u03a8 is the\nlearned linear encoder for initial attenuation p0, and A is\nthe attenuation coefficients network. \u00ce(r) is computed by\nEq. (2) where pj is estimated by the network A and the re-\nlated encoders and \u03a8. This is implemented by the classic\nforward projection technique that corresponds to the render-\ning model in the computer vision field. The CT reconstruc-\ntion process is achieved by feeding the voxel grid coordi-\nnates, sized to the desired dimensions, into the trained neu-\nral function Fe to predict the corresponding attenuation co-\nefficients. In our work, we use the MLP model architecture\nfrom [31] as the foundation for our study. Additionally, we\nexplore a transformer-based model that incorporates X-ray\nstructural awareness [8] to further validate the applicability\nof our proposed method. The final output is represented as\na discrete 3D matrix."}, {"title": "4. Experiments and Results", "content": "Following novel Nerf-based works [8, 51], we evaluate\nour method using publicly available human organ C\u0422\ndatasets, including LIDC-IDRI [4] and scientific visualiza-\ntion dataset [24]. Using the open-source tomographic tool-\nbox TIGRE [7], we simulate 100 projections per case with\n3% noise over a 0\u00b0-180\u00b0 range. For the novel view synthe-\nsis (NVS) task, we split these projections evenly, with 50\nfor training and 50 for testing. For sparse-view CT recon-\nstruction, the CT volumes are reserved as ground truths for\nevaluation.\nThe p-NeRF framework is implemented in PyTorch and\ntrained on an NVIDIA RTX 3090 GPU. We optimize the\nmodel using the Adam optimizer (\u03b2\u2081 = 0.9, \u03b22 = 0.999)\nfor at most 3,000 iterations, with an inital learning rate of\n0.001, reduced to 0.0001 halfway through training. For ef-\nficient processing, we use a batch size of 1,024 rays, with\neach ray sampled at 192 points to adequately represent the\nattenuation field across the CT volume. After training,\nwe quantitatively evaluate the reconstruction by computing\nboth peak signal-to-noise ratio (PSNR) and structural simi-\nlarity index (SSIM) metrics. PSNR (in dB) provides a sta-\ntistical measure of artifact suppression performance, while\nSSIM assesses perceptual differences between two images.\nHigher PSNR and SSIM values indicate more accurate re-\nconstructions."}, {"title": "4.1. Experimental Setup", "content": "4.2. Main Results\nWe include comparisons with the conventional CT recon-\nstruction algorithms, including FDK (physics-based), AS-\nPOCS (iterative), and CGLS (Krylov subspace), as well as\nthe state-of-the-art NeRF-based methods, named NAF and\nSAX-NeRF. We deploy the p-NeRF framework to NeRF-\nbased models by utilizing their neural networks as the map-\nping function Fe, naming the enhanced models p-NAF\nand p-SAX-NeRF. These new models integrate attenuation\npriors, the nearest neighbor interpolation method, and the\ntwo proposed feature encoders, demonstrating the effec-\ntiveness and adaptability of our approach. Unless other-\nwise noted, this configuration is used throughout the ex-\nperiments. A detailed analysis of these design choices is\nprovided in Sec. 4.3, with additional quantitative and quali-\ntative results available in the supplementary materials.\n3D CT Reconstruction: Tab. 1 and Fig. 3 provide quan-\ntitative and visual comparisons of 3D CT reconstruction\nacross baseline and p-NeRF-enhanced models. The p-based\nmodels consistently outperform their counterparts across all\ncases. Specifically, SSIM values improve from 0.93 to 0.95\nin foot reconstructions when comparing NAF and P-NAF,\nalso it surpasses SAX-NeRF-based models while maintain-\ning a negligible computational cost, which will be discussed\nin Sec. 4.3. In chest scans, p-NAF achieves a PSNR of\n35.07 dB, compared to 34.46 dB with SAX-NeRF, under-\nscoring the positive impact of leveraging attenuation priors.\nThese results validate that p-NeRF effectively reduces arti-\nfacts and yields more precise reconstructions, especially in\nsparse-view conditions, emphasizing the benefits of refined\nattenuation initialization.\nNovel View Synthesis: In Tab. 2, quantitative evalua-\ntion results of novel view synthesis (NVS) show that p-\nNAF and p-SAX-NeRF provide substantial improvements\nover approaches adopting only spatial location as inputs,\nand promising the effectiveness of attenuation priors mech-\nanism. For instance, p-SAX-NeRF achieves a PSNR of\n47.66 dB and an SSIM of 0.9996 on chest views, compared\nto SAX-NeRF's 47.41 dB and 0.9994, respectively. Fig. 4\nvisually demonstrates the sharper anatomical details ren-\ndered by p-based models, effectively capturing fine struc-\ntures. This performance, combined with supplementary re-\nsults, highlights the capability of p-NeRF in delivering vi-\nsually accurate and consistent NVS results across various\nanatomical regions."}, {"title": "4.3. Ablation Study", "content": "Attenuation Initialization Analysis: FDK is our primary\nchoice for initialization in the pipeline due to its ability"}, {"title": "4.4. Main Results", "content": "ors.\n5. Conclusion\nThis work introduces p-NeRF, a self-supervised neural\nframework designed for high-fidelity 3D CT reconstruction\nfrom sparse-view X-ray data. By leveraging attenuation pri-\nors with a continuous 4D representation, p-NeRF advances\nthe performance of CT imaging while reducing reliance on\ndense data or supervised learning. Key contributions in-\nclude a novel input schema that integrates initialized at-\ntenuation priors, effective feature encoding methods, and\nrobust interpolation techniques. Evaluations on the X3D\ndataset validate p-NeRF's state-of-the-art performance in\nnovel view synthesis and CT reconstruction. This frame-\nwork not only sets a new benchmark in efficiency and accu-\nracy for sparse-view CT applications but also offers poten-\ntial for adaptation across a range of Nerf-based methods on\nCT reconstruction, contributing to advancements in medical\nand industrial CT imaging."}]}