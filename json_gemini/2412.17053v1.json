{"title": "DR-ENCODER: ENCODE LOW-RANK GRADIENTS WITH RANDOM PRIOR FOR LARGE LANGUAGE MODELS DIFFERENTIALLY PRIVATELY", "authors": ["Huiwen Wu", "Deyi Zhang", "Xiaohan Li", "Xiaogang Xu", "Jiafei Wu", "Zhe Liu"], "abstract": "The emergence of the Large Language Model (LLM) has shown their superiority in a wide range of disciplines, including language understanding and translation, relational logic reasoning, and even partial differential equations solving. The transformer is the pervasive backbone architecture for the foundation model construction. It is vital to research how to adjust the Transformer architecture to achieve an end-to-end privacy guarantee in LLM fine-tuning. In this paper, we investigate three potential information leakage during a federated fine-tuning procedure for LLM (FedLLM). Based on the potential information leakage, we provide an end-to-end privacy guarantee solution for FedLLM by inserting two-stage randomness. The first stage is to train a gradient auto-encoder with a Gaussian random prior based on the statistical information of the gradients generated by local clients. The second stage is to fine-tune the overall LLM with a differential privacy guarantee by adopting appropriate Gaussian noises. We show the efficiency and accuracy gains of our proposed method with several foundation models and two popular evaluation benchmarks. Furthermore, we present a comprehensive privacy analysis with Gaussian Differential Privacy (GDP) and Renyi Differential Privacy (RDP).", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated their strong capabilities in real-life applications, such as language understanding [22], mathematical reasoning [21], and even differential equation solving [19]. LLM-based applications provide a variety of convenient tools, including chatbots, virtual assistants, article writing, creative writing, and translation services. However, the extensive use of LLMs in daily work also poses a significant risk of unintentional leakage of personal information.\nOne typical approach to address privacy concerns in daily LLM usage is to use differential privacy during model training, achieved by adding extra Gaussian noise to the training data or intermediate gradients. However, these methods may lose their effectiveness when dealing with extensive input data and billions of model parameters in LLMs. This"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Differential Privacy for LLMs", "content": "EW-Tune [4] presents a differential privacy (DP) framework for fine-tuning LLMs using an edgewise accountant. This approach ensures finite sample privacy guarantees through perturbation applied to the low-rank decomposition of the gradient matrix. The authors in [36] introduce a technique named Just Fine-tune Twice (JFT) that aims to achieve selective differential privacy (SDP) in large language models (LLMs) through two protective layers. The first layer, a low contextual detector, secures named entities, proper nouns, pronouns, and sentence components like subjects and objects, while the second layer, a high contextual detector, censors verbs to further enhance privacy. Whispered Tuning [37] is a multifaceted approach that integrates redaction of personally identifiable information, differential privacy techniques, and output filtering to improve privacy preservation in LLM. Split-N-Denoise [30] is a system crafted to protect user data privacy during the inference stage of large language models (LLM) by leveraging local differential privacy (LDP). It provides the user with a Transformer-based denoising model pre-trained on the server using public datasets and artificial noise. In [6], research study two variants of DP-SGD were investigated in the research study with sampling at the sample level and gradient clipping per sample to achieve differential privacy at the sample level and user-level sampling with clipping of gradient per user to achieve differential privacy at the user level."}, {"title": "2.2 Parameter Efficient Fine-tuning (PEFT)", "content": "Mixout approaches [25] integrate the standard network with the dropout network utilizing a specified probability. LORA methods [20, 28, 8] decompose the gradient matrix and reconstruct it by multiplying the low-rank matrices. Adapter-based methods [23, 29] introduce an additional adapter layer within the transformer layer, altering the network architecture. MagPruning methods [16, 17, 24] follow the principle that large weights are more important. By filtering out small weights in absolute values, it tunes the parameters with large absolute values only. DiffPruning [32, 15] uses a Bernoulli random variable to represent the mask selection process and learns this variable through reparameterization methods. Child-Pruning [44, 34] trains in the full parameter space and calculates the projected mask to find the child network. In [14], the authors provide a unified sparse fine-tuning model containing random approaches, rule-based"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Privacy Goal", "content": "The goal of our methods is to provide an end-to-end privacy guarantee for the gradient compression procedure in federated learning. We divide the whole gradients compress into two procedures. One is the pre-training of AutoEncoder to acquire the encoder and decoder for gradients compression. The second is federated fine-tuning with clients equipping encoder and server equipping with decoder. In the aforementioned two stages, there are several chances of information leakage. Firstly, when using local training gradients as input for AutoEncoder, the collection and transmission of the local gradients of the clients leak the sensitive information of the clients through the reconstruction attacker from the gradients to the original data [35] and the membership inference from the gradients [12, 31, 42]. Moreover, as we record the local training gradients as the input for training the AutoEncoder, the local data information is condensed into the models' weights of AutoEncoder. There is a chance to infer the original data information from the trained AutoEncoder, which are called model inversion attacks [13]. We took the following steps to achieve the end-to-end privacy guarantee.\n\u2022 Instead of transimission of the exact gradients from client to server, we transmit the stastics of gradients only. And we use the statistics to generate synthetic gradients for AutoEncoder pre-training.\n\u2022 In the fine-tuning stage, instead of transmitting the exact gradients from client to server, we adopt differential privacy on the local gradients.\n\u2022 A rigorous analysis of the privacy cost is presented to validate the privacy leakage in the entire federated system."}, {"title": "3.2 Pre-Training with Random Prior", "content": "In this section, we illustrate the process of training an AutoEncoder to grasp the statistical properties of the training gradients. Inspired by work on training deep neural networks for gradient compression [26, 43] and noise reduction [30] in conventional ML and LLM, we introduce a novel method to train the AutoEncoder by exclusively sharing the statistical details of the training gradients during the actual training phase. Thus, protecting the original gradient information during AutoEncoder training is crucial. To achieve this goal, we collect only the statistical information of the gradients, including the mean and standard deviation for each layer and epoch, and send them to the server. The information collected can be stored in the form [${m}_{i}^{*}, {s}_{i}^{*} $], where * denotes the low-rank parts of A or B, i represents the layer index and t indicates the epoch index. We adopt a dynamical way to compute the mean and standard deviation of local gradients by layer and epoch, which follows steps 1 to 4 on the client side as shown in Algorithm 1. The hyperparameters \u03b2\u2081, \u03b2\u2082, h\u2081, h\u2082 are small scalars. In our experiments, we use \u03b2\u2081 = 0.99, \u03b2\u2082 = 0.9, h\u2081 = 10\u207b\u2075 and h\u2082 = 10\u207b\u00b3. When server receiving the collected mean and standard deviation, it generates synthetic gradients with Gaussian distribution. The synthetic gradients are in the form [G] = [\u00c2\u1d62, B\u1d62], \u00c2\u1d62 ~ \u00d1 (${m}_{A,i}, {s}_{A,i}^{2}$), B\u1d62 ~ N(${m}_{B,i}, {s}_{B,i}^{2}$). The server uses synthetic gradients to train the AutoEncoder with loss of l\u2082 reconstruction. Once the server completes training the AutoEncoder, it separates the AutoEncoder into an encoder and a decoder, dispatching the encoder to all clients while retaining the decoder exclusively. We present the training details in Algorithm 1."}, {"title": "3.3 Differentially Private Federated Fine-tuning with LoRA gradients", "content": "First of all, we lay the foundation algorithm for fine-tuning the LLM model with differential LoRA gradients privately. Several works consider training LLMs according to differential privacy constraint. For example, in [6], the author proposes a sample-level differential privacy and a user-level differential privacy method. In our work, our goal is to provide differential client-level privacy for the entire FL system. In each client at iteration t, the gradient is decomposed to a low-rank decomposition for light transmission. G = AB, where A \u2208 \u211d\u207f\u02e3\u02b3, B \u2208 \u211d\u02b3\u02e3\u207f. We first compress the gradients with the Enc trained in Algorithm 1. We then clip the gradient.\n${{A}_{i}^{c}=\\text{Clip}({{A}_{i}})=\\frac{{{A}_{i}}}{\\parallel {{A}_{i}}\\parallel _{F}}\\min (1,\\frac{C}{\\parallel {{A}_{i}}\\parallel _{F}});}$ (1)\n${{B}_{i}^{c}=\\text{Clip}({{B}_{i}})=\\frac{{{B}_{i}}}{\\parallel {{B}_{i}}\\parallel _{F}}\\min (1,\\frac{C}{\\parallel {{B}_{i}}\\parallel _{F}}).}$ (2)"}, {"title": "4 Theoretical Analysis", "content": "Here, we derive the theoretical analysis to show how to achieve client-level differential privacy based on our proposed methods."}, {"title": "4.1 Priliminary", "content": "Definition 4.1. (l\u2082-sensitivity [10]). The l\u2082-sensitivity of a function f : D \u2192 \u211d\u1d48 is:\n$\\Delta_{2,f}=\\underset{x,y \\in \\mathcal{D},||x-y||=1}{\\text{max}}||f(x)-f(y)||_2$.\nDefinition 4.2. (The Gaussian Mechanism [10]) Given a function f : D \u2192 \u211d\u1d48 on a data set D, the Gaussian mechanism is defined as:\n$M_G(x,f(\u00b7),\u03c3)=f(x)+(Y_1,\u00b7\u00b7\u00b7 ,Y_k)$\nwhere Y\u1d62 are i.i.d. random variables drawn from N(0, \u03c3\u00b2\u0394\u2082f\u00b2) and \u03c3=\u221a(2ln(1.25/\u03b4)).\nTheorem 4.1. [10] The Gaussian mechanism defined in Definition 4.2 preserves (\u03f5, \u03b4)-differential privacy."}, {"title": "4.2 Privacy of DR-Encoder", "content": "First, we show that, with clip value 1, the sensitivity of the composition of gradient aggregation in Eq. (5) is 2/K.\nLemma 4.1. With clip value 1, the sensitivity of gradient aggregation defined in Eq. (5) is 2/K, with K be the number of selected clients.\nThen we show with sensitivity equal 1 and the noise multiplier \u03c3, the privacy loss for per iteration is G\u2081/\u03c3-DP.\nLemma 4.2 (Privacy per iteration). Suppose Noise with random variable sampled from Gaussian mechanism N(0, 4\u03c3\u00b2/K\u00b2). Then DR-Encoder (Algorithm 2) for per gradient update satisfies G\u2081/\u03c3-DP, where G\u2081/\u03c3(\u03b1) =\n\u03a6(\u03a6\u207b\u00b9(1 \u2212 \u03b1) \u2212 1/\u03c3) and \u03a6 denotes the standard normal cumulative distribution function.\nBy applying Lemma 4.2 to mechanism defined in Algorithm 2, we have the privacy analysis of DR-Encoder for per gradient update is G\u2081/\u03c3-DP. After that, one can prove the privacy analysis of training equipped with DR-Encoder, with subsampling amplification (for SGD or mini-batch SGD) and the central limit theorem of composition over iteration T via GDP [5]. According to the Central Limit Theorem (Theorem 5 [5]), we have approximated G\u03bc for the accumulated privacy loss of DR-Encoder. Furthermore, we convert the accumulated \u00b5-GDP back to (\u03f5, \u03b4)-DP with the primal-dual result (Corollary 2.13 in [9]). Then we have the main result of the privacy analysis of DR-Encoder, which preserves (\u03f5, \u03b4)\u2212DP with noise multiplier 2\u03c3/K, number of iterations T, and subsampling rate p."}, {"title": "5 Experiments", "content": "In this segment, we perform extensive experiments to address the following research inquiries.\n\u2022 RQ1 How does the DR-Encoder train by Random Prior work in the fine-tuning scenario?\n\u2022 RQ2 How much noise do we exert on the gradient to guarantee DP with various budgets?\n\u2022 RQ3 How does the random mechanism to achieve DP influence the FedLLM fine-tuning?\n\u2022 RQ4 What is the difference in performance between the AutoEncoder trained with the informative prior and with the random prior?\n\u2022 RQ5 What is the gain in communication efficiency of the proposed method?"}, {"title": "5.1 Experimental Settings", "content": "Now we present the details of experimental settings including foundation models, datasets, evaluation benchmarks, and hyper-parameters that we utilized throughout all papers.\nFor the foundational models, we adopt the fine-tuning strategy based on LlaMa-7B and Qwen-7B. LlaMa is an open and efficient LLM foundation model developed by Meta with parameter sizes ranging from 7B to 65B [39]. Qwen is developed and released by Alibaba Group [2]. Due to the limited computational resource, we carry out the experiments on the 7B version. For the fine-tuning of our large language models (LLMs), we leveraged two primary datasets: MMLU-train [18] and Databricks-dolly-15k [7]. The MMLU-train dataset encompasses 57 tasks in diverse disciplines, such as elementary mathematics, United States history, computer science, legal studies, and more. Meanwhile, Databricks-dolly-15k is structured into eight categories, including brainstorming, classification, closed QA, creative writing, general QA, information extraction, open QA, and summarization. To evaluate the performance of our LLM, we used the C-Eval [46] and MMLU [18] benchmarks, which provide comprehensive, multidisciplinary assessments"}, {"title": "5.2 Foundation Models Improvement (RQ1)", "content": "In this section, we demonstrate how various fine-tuning strategies enhance model performance over the foundational ones. We detail the fine-tuning of the Qwen model with MMLU datasets and its evaluation in MMLU, as well as the fine-tuning with dolly-15k datasets and evaluation on C-Eval, in Table 2. From Table 2, it is evident that fine-tuning methods significantly improve accuracy compared to foundational methods. For instance, in the second part (C-Eval) of Table 2, the accuracy improvement of FedCG is 3.8 for the 'Average' subject and 6.3 for 'Avg(Hard)' relative to the foundational model. In the first part (MMLU) of Table 2, the LoRA method exhibits an accuracy increment of 0.7 in the 'Average' subject while FedCG shows an increment of 0.37 compared to the Qwen7B-Base-MMLU model. Moreover, the DR-Encoder with an autoencoder trained using random prior information demonstrates a comparable fine-tuning performance to FedCG with an autoencoder trained on raw gradients, outperforming LoRA without gradient compression. For example, in the C-Eval evaluation, the highest score of 26.6 for the \u2018Average\u2019 subject is achieved by FedCG, while the second highest score of 26.2 is achieved by DR-Encoder, which surpasses LoRA\u2019s performance of 25.8. A similar trend is observed for the 'Avg(Hard)\u2019 subject."}, {"title": "5.3 Privacy Accounting via RDP or GDP (RQ2)", "content": "This section discusses how to account for privacy loss in the FedLLM system. We employ the RDP [33] and GDP [9] methods to account for loss of privacy. We provide a table showing the one-to-one relationship between the privacy"}, {"title": "5.4 Overall Influence of Differential Privacy (RQ3)", "content": "In Table 4, we present the training results with various privacy budgets of the set {0.25, 0.5, 1.0, 2.0, 4.0, 8.0}. Smaller privacy budgets ensure stronger privacy guarantees. Based on the results shown in Table 4, we observe that the inclusion of Gaussian noise in the low-rank decomposition of gradients to provide differential privacy leads to a decline in model performance. As illustrated in the first row of Table 4, adding Gaussian noise directly to the LoRA gradients results in the 'Average' score dropping from 56.77 to 25.67. This shows that Gaussian noise has a negative impact on model training. For the LlaMa model, there are no evaluation results for LoRA methods with small e due to the substantial noise applied to the gradients."}, {"title": "5.5 Comparison Between FedCG and DR-Encoder (RQ4)", "content": "With the use of an informative pre-trained AutoEncoder, FedCG achieves performance comparable to the nonprivate scenario. For example, the scores obtained by Qwen-FedCG-e-0.5 are close to those of Qwen-LoRA-6-\u221e. This is attributed to the strong denoising capability of the informative AutoEncoder. A similar pattern can be observed for the LlaMa example. Moreover, FedCG displays consistent performance across different privacy budgets. This consistency is likely a result of the denoising capabilities of the informative AutoEncoder. In contrast to FedCG, DR-Encoder exhibits a downward trend in evaluation scores as privacy budgets become more stringent. This aligns with the general understanding that a smaller privacy budget results in more significant Gaussian noise being added, thereby impairing overall model performance."}, {"title": "5.6 DR-Encoder with Large Privacy Budget (RQ4)", "content": "Most notably, even when employing an AutoEncoder trained with a Random Prior, DR-Encoder efficiently alleviates the performance degradation caused by differential privacy for moderate privacy budgets of e = 1.0 and 2.0. Interestingly, with looser privacy budgets of \u20ac = 4.0 and 8.0, DR-Encoder surpasses the non-privacy benchmarks, such as Compress-6-\u221e and Cent-6-\u221e. This happens because looser privacy budgets lead to a smaller amount of random noise, thus improving the generalizability of the fine-tuned model."}, {"title": "5.7 Communication Efficiency Gain (RQ5)", "content": "In this section, we demonstrate the communication efficiency gain (CEG) of our method DR-Encoder in comparison to FedCG. In the initial stage, which involves collecting gradients and transmitting them to the server for autoencoder pre-training, for the LlaMa model with low-rank parameters r = 8, FedCG requires 4096\u00d78\u00d72\u00d74\u00d7 32 \u00d7 20 = 167, 772, 160 \u2248 160MB of parameters. In contrast, for DR-Encoder, we only send the mean and standard deviation for each layer and the gradients of each epoch, with a total parameter count of 2 \u00d7 2 \u00d7 4 \u00d7 32 \u00d7 20 = 10, 240 \u2248 0.078MB. We reduce the communication complexity in the AutoEncoder preprocessing stage to 6.10 \u00d7 10\u22125, illustrating the substantial efficiency gain of our proposed AutoEncoder training with Random Prior. We summarize this comparison of model parameters, storage, and efficiency gain in Table 5."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a practical learning-based gradient compression method to achieve client-level differential privacy during fine-tuning of LLMs. Our approach offers high privacy guarantees, minimal communication overhead, and manageable computational demands. We utilize statistical data of the local gradients to train an AutoEncoder, preventing reverse attacks from the AutoEncoder on raw gradients as well as from raw gradients to the original input data. In addition, we apply Gaussian noise to compressed gradients to ensure user-level differential privacy. We quantify privacy loss using GDP and RDP, offering a thorough privacy analysis. We propose a one-stage fine-tuning algorithm by integrating AutoEncoder training with LLM fine-tuning into a cohesive system. Moreover, our method can be extended to the Large Vision Model (LVM) and Large Multimodal Model (LMM) in the future."}, {"title": "8 Supplementary Material", "content": ""}, {"title": "8.1 Training Details", "content": "In this part, we detail the training procedures of the experiments discussed in the main document."}, {"title": "8.1.1 Hyper-parameters", "content": "The hyperparameters for fine tuning are detailed in Table 6. Using the FedLLM hyperparameters, the noise multiplier \u03c3 for the privacy hyperparameters is calculated via GDP [9], as shown in the main content, Table 3. In industry DP applications, the standard privacy budgets used include \u03f5 = 4.0 for Apple Emoji, \u03f5 = 2.0 for Apple Health, and \u03f5 = ln 3 for Google's RAPPOR [11]. However, in our experiments, we also tested significantly tighter values of e across all data sets as {2\u207b\u1d4f, k \u2208 \u2115, \u22123 \u2264 k \u2264 2} to validate the robustness of our method."}, {"title": "8.1.2 Computation Power", "content": "One of our servers is equipped with an NVIDIA GPU (serial number 1) with 24 GB of memory. This allows us to perform parallel processing for tasks that require high computational power, such as pre-training the AutoEncoder in stage one and federated fine-tuning in stage two. Our computational infrastructure runs on Ubuntu Linux 22.04. Additionally, to optimize the performance of specific machine learning components in our simulations, we have implemented PyTorch 2.2.1 with CUDA 12.4 to seamlessly integrate GPU processing into our workflows."}, {"title": "8.1.3 Training and Fine-tuning Data", "content": "We utilize the identical dataset for both AutoEncoder pre-training and FedLLM fine-tuning. The data set applied with the LlaMa model is Dolly-bricks-15k, whereas the data set for the Qwen model is the MMLU data set. The Databricks-dolly-15k [7] dataset encompasses eight distinct categories: brainstorming, classification, closed QA, creative writing, general QA, information extraction, open QA and summarization. This data set is segregated into 100 segments using the widely used category Dirichlet allocation technique [40]. The MMLU dataset [18] spans 57 tasks, including elementary mathematics, US history, computer science, law, and others. This data set is randomly partitioned into 3 segments."}, {"title": "8.2 Privacy Analysis Details", "content": "This section provides an in-depth analysis of privacy loss using GDP [9].\nThe random mechanism depicted in the main context, Algorithm 2 can be abstracted in the following operator form.\n$\\mathcal{M}:$ Deco \u2218 Aggregation \u2218 Noise \u2218 Clip \u2218 Enc.\nEnc represents the compression phase through the encoder. Clip signifies the clipping stage. Noise incorporates random Gaussian noise into the gradients. Aggregation indicates the aggregation step by summing all the gradients from the chosen clients. Dec signifies the decompression phase with the decoder. Randomness is present in the Noise procedure, while all other steps are deterministic."}, {"title": "8.2.1 Proof of Lemma 4.1", "content": "Proof. Suppose D' is a data set adjacent to D with one date changing, which leads to only one G change. By clipping with the value 1, the clipped low rank parameters have a Frobenious norm less than or equal to 1.\n$||G||_{F}=||\\bar{A} \\bar{B}||_{F} \\le ||\\bar{A}||_{F}||\\bar{B}||_{F} \\le 1$.\n$\\frac{1}{K}\\sum_{i=1}^{K}\\frac{1}{K}|G_{i}|_{F} \\le 2,$\nsince $||G_{i}|\\le 1$ after clipping. Thus, the sensitivity of the gradient aggregation in DR-Encoder defined in Eq.(8) is 2/K, where K is the number of selected clients."}, {"title": "8.2.2 Proof of Lemma 4.2", "content": "Proof. By Theorem 2.7 in GDP [9], with the addition of DP noise 4\u03c3\u00b2/K\u00b2, the gradient aggregation step is G\u2081/\u03b4-DP. Furthermore, the decoder Dec is deterministic after the gradient aggregation step. By the post-process property (Proposition 2.8 [9]), we have that DR-Encoder for per gradient update satisfies G\u2081/\u03c3-DP."}, {"title": "8.2.3 Central Limit Theorem for GDP", "content": "Theorem 8.1 (Central Limit Theorem for GDP [5]). Suppose Algorithm 1 run with number of steps T and Poisson sampling without replacement with probability p = K/M, which satisfy p\u221aT \u2192 v. Then $C_{p}(G_{1/\u03c3})^T \u2192 G_{\u03bc}$"}, {"title": "8.2.4 Proof of Theorem Theorem 4.2", "content": "Proof. By Lemma 4.1 and Lemma 4.2, we find that the update per gradient for DR-Encoder is G\u2081/\u03c3-DP. According to the GDP subsampling and composition theorem [9], with the sampling rate p and iteration T, the overall loss of privacy is $C_{p}(G_{1/\u03c3})^T$. By the Central Limit Theorem (Theorem 5 in [5]), if $p\\sqrt{T} \u2192 v$, and $\u03bc = v\u22c5 \u221a(e^{1/\u03c3\u00b2} \u2013 1)$, $C_{p}(G_{1/\u03c3})^T \u2192 G_{\u03bc}$ as T \u2192 \u221e."}, {"title": "8.2.5 Choice of Privacy Accountant", "content": "By GDP [5], one can compute the noise multiplier \u03c3 by the sampling rate p, the number of rounds T, the privacy budget \u03f5 and \u03b4. For comparison, we present the \u03f5 accumulated with RDP [33] under the same noise magnitude \u03c3 in the main context, Table 3, which shows a looser accumulation compared to GDP. Small \u03f5 and \u03b4 provide a strong privacy guarantee but can degrade the utility of the model. The frequent privacy budget used in academia and industry is (\u03f5, 10\u207b\u2075) where \u03f5 is 4.0 for Apple Emoji, 2.0 for Apple Health [38], and ln 3 for Google RAPPOR [11]. In our experiments, we achieve a stronger privacy guarantee with \u03f5 as small as {0.25, 0.5}."}, {"title": "8.2.6 Client-level Differential Privacy", "content": "To conclude, we demonstrate that the random mechanism described by Eq. 8 ensures {\u03f5,\u03b4}-DP by utilizing the Gaussian mechanism as specified in Definition 4.2 of the main text as the Noise operator, while all other operators are deterministic. Additionally, the privacy focus in our discussion is per client rather than per sample because the Aggregation operator functions during the server-side aggregation of all client gradients. From the above analysis, we conclude the privacy proof for our proposed methods, which comply with differential privacy at the client level."}, {"title": "8.3 Architecture of Encoder and Decoder", "content": "We refine the AutoEncoder using the Uformer architecture defined in [41]. The architectural details are shown in Table 7, and the training methodology is illustrated in Figure 3. This iteration of the AutoEncoder preserves the upsampling and downsampling layers inherent to the ResNet-based AutoEncoder while replacing the core components with Basic Uformer units, each comprising two LeWin Transformer Blocks. The encoder integrates the DownSampling layers with three Basic Uformer blocks, whereas the decoder incorporates three Basic Uformer blocks in conjunction with the UpSampling layers. Moreover, we test on different architectures by changing the number of Uforme blocks or the number of Upsampling and Downsampling and choose the best one with 7 Downsampling + 6 Uformer + 7 Upsampling by balancing efficiency and efficacy."}, {"title": "8.4 Gradient Distribution Visualization", "content": "In this section, we present the distribution of the gradient throughout the learning epochs. Figure 4 illustrates the behavior of the low-rank decomposition matrices A and B across various layers during the training iterations. The x-axis shows the ranges of values of the matrices, the y-axis indicates the iteration steps, and the z-axis depicts the frequency of these values. Vertically, from top to bottom, the distributions of A and B are displayed in the layers {0, 1, 2, 29, 30, 31}. Horizontally, from left to right, the distributions of A in the attention layer and the Multi-Layer Perceptron (MLP) and B in the attention layer and the MLP layer are shown. In Figure 5, we visualize the gradients for layers {4, 5, 6, 26, 27, 28}. The visualization reveals that the gradient flattens as the learning epoch progresses. A significant difference is observed between the gradient distributions of the A and B parts."}]}