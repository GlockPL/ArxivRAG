{"title": "[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster", "authors": ["Qizhe Zhang", "Aosong Cheng", "Ming Lu", "Zhiyong Zhuo", "Minqi Wang", "Jiajun Cao", "Shaobo Guo", "Qi She", "Shanghang Zhang"], "abstract": "Large vision-language models (VLMs) often rely on a substantial number of visual tokens when interacting with large language models (LLMs), which has proven to be inefficient. Recent efforts have aimed to accelerate VLM inference by pruning visual tokens. Most existing methods assess the importance of visual tokens based on the text-visual cross-attentions in LLMs. In this study, we find that the cross-attentions between text and visual tokens in LLMs are inaccurate. Pruning tokens based on these inaccurate attentions leads to significant performance degradation, especially at high reduction ratios. To this end, we introduce FasterVLM, a simple yet effective training-free visual token pruning method that evaluates the importance of visual tokens more accurately by utilizing attentions between the [CLS] token and image tokens from the visual encoder. Since FasterVLM eliminates redundant visual tokens immediately after the visual encoder, ensuring they do not interact with LLMs and resulting in faster VLM inference. It is worth noting that, benefiting from the accuracy of [CLS] cross-attentions, FasterVLM can prune 95% of visual tokens while maintaining 90% of the performance of LLaVA-1.5-7B. We apply FasterVLM to various VLMs, including LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA, to demonstrate its effectiveness. Experimental results show that our FasterVLM maintains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing text-visual attention-based methods. Our code is available at https://github.com/Theia-4869/FasterVLM.", "sections": [{"title": "1 Introduction", "content": "With recent advances in large language models (LLMs) [3, 13, 38, 43, 44, 54, 55, 64], numerous efforts have been made to extend their powerful reasoning capabilities to multi-modal tasks [14, 35, 62], giving rise to vision language models (VLMs) such as GPT-4 [1], Gemini [51], LLaVA [34], and Qwen-VL [2]. These VLMs typically contain a visual encoder (e.g. CLIP) to convert visual inputs into sequential representations, and an LLM (e.g. Llama 2) for text generation, with modal alignment for the LLM to accept visual tokens as input [34]. Although these approaches have achieved remarkable performance in multi-modal tasks like visual question answering [15, 16, 20, 36, 50], the visual tokens, which comprise the majority of the input sequence, significantly increase the computational complexity and cost of VLM inference. Some work improves model performance by increasing input image resolution [33, 37, 59], but this leads to longer visual token sequences, further escalating computational overhead, which can be even worse in video models like Video-LLaVA [30].\nTo reduce the inference cost of VLMs, FastV [8] analyses the information flow in LLMs within VLMs and identifies the inefficient visual attention phenomenon: In the deeper layers of the LLM (after layer 2), visual tokens receive significantly less attention than their textual counterparts, implying that less useful information flows from visual tokens to the text portion, motivating the pruning of visual tokens in VLMs. According to the distribution of attention in different layers, FastV proposes an intuitive solution, ranking the visual tokens based on attention scores received after layer 2 of the LLM and pruning the lowest R% according to the computational budget, thereby reducing visual token redundancy and improving VLM inference efficiency. Subsequently, more work follows this paradigm [61, 65], utilizing various methods to prune visual tokens through text-visual attention from the LLM. Although FastV successfully identifies the inefficiency of visual tokens in VLMs, pruning based on text-visual attention is not effective. As shown in Fig. 2, the performance of VLM degrades significantly as the reduction ratio increases.\nIn this paper, we find that the text-visual attention within the LLMs of current VLMs fails to align well with the actual significance of visual tokens, meaning that the attention weights assigned to each visual token do not correspond appropriately to their importance in the task. This misalignment can be explained from two aspects, termed attention shift and attention dispersion. Specifically, attention shift refers to a tendency for textual attention to focus more on later parts of the visual token sequence, which is not desirable for preserving valuable visual information. We attribute this phenomenon to two main factors: the unidirectional information flow within the LLM and the relative positional relationships between tokens. These factors cause textual prompts to preferentially focus on the visual tokens positioned later in the sequence. Conversely, Attention dispersion refers to the less concentrated attention distribution within the LLM compared to the visual encoder. In the LLM, more visual tokens receive relatively high attention scores, but the highest attention value is lower. We believe that, despite the multi-modal projector's role in aligning visual representations with text embeddings, a gap still remains between them. This gap results in noisy text-visual attention at shallower layers, hindering the effective pruning of important visual tokens.\nTo more accurately evaluate the importance of visual tokens, we introduce FasterVLM, a simple yet effective pruning method. FasterVLM directly utilizes the attention weights from the [CLS] token within the image encoder to assess the importance of each visual token. By removing tokens with lower attention scores before they are processed by the LLM, we achieve maximum inference acceleration. Quantitative and qualitative analyses indicate that [CLS] attention does not exhibit the shift phenomenon, thanks to the global attention mechanism in the image encoder. Moreover, compared to text-visual attention, [CLS] attention is more concentrated, serving as a better indicator for evaluating the importance of visual tokens. As noted in [11], certain artifacts exist in the attention maps of Vision Transformers [42, 45, 53], where the regions contain pixels highly similar to their surroundings. The model discards local information from these regions and stores global information in these artifacts. We find that these global tokens significantly impact VLM performance. Under high reduction ratios, previous methods struggle to identify these artifacts due to inaccurate text-visual attention. In contrast, FasterVLM can accurately identify these visual tokens containing global information via [CLS] attention, preserving most of the performance."}, {"title": "2 Related Work", "content": "The recent impressive success of large language models (LLMs) [3, 13, 38, 43, 44, 54, 55, 64] leads to a trend of extending their powerful reasoning capabilities to multi-modal comprehension tasks, giving birth to vision-language models (VLMs) [1, 51]. These VLMs generally include a visual encoder for serializing representations of input images and an LLM for text generation. To make the LLM acceptable for visual representations as input, VLMs typically align the visual and language modalities with an alignment module, which can be a simple linear layer [34], an MLP projector [32], or a deep query-based network [2, 26]. Although this allows the LLM to have visual perception, the introduction of long visual token sequences increases the computational burdens. Furthermore, studies have shown that existing VLMs still suffer from visual shortcomings [52] or hallucinations [18]. To mitigate this, efforts have been made to improve VLM performance by increasing the resolution of input images [37, 59], which further exacerbates the computation. For example, LLaVA-1.5 [34] encodes an image with 336 resolution into 576 visual tokens, while LLaVA-NeXT [33] doubles the resolution, resulting in 2880 tokens, potentially dozens of times more than the textual prompts. For video inputs, Video-LLaVA [30] needs to handle even more visual tokens from multiple frames, making VLM inference prohibitively expensive. Optimizing the inference process of VLMs is an urgent task to enable their application in resource-constrained real-world scenarios."}, {"title": "2.1 Vision-Language Models (VLMs)", "content": "The recent impressive success of large language models (LLMs) [3, 13, 38, 43, 44, 54, 55, 64] leads to a trend of extending their powerful reasoning capabilities to multi-modal comprehension tasks, giving birth to vision-language models (VLMs) [1, 51]. These VLMs generally include a visual encoder for serializing representations of input images and an LLM for text generation. To make the LLM acceptable for visual representations as input, VLMs typically align the visual and language modalities with an alignment module, which can be a simple linear layer [34], an MLP projector [32], or a deep query-based network [2, 26]. Although this allows the LLM to have visual perception, the introduction of long visual token sequences increases the computational burdens. Furthermore, studies have shown that existing VLMs still suffer from visual shortcomings [52] or hallucinations [18]. To mitigate this, efforts have been made to improve VLM performance by increasing the resolution of input images [37, 59], which further exacerbates the computation. For example, LLaVA-1.5 [34] encodes an image with 336 resolution into 576 visual tokens, while LLaVA-NeXT [33] doubles the resolution, resulting in 2880 tokens, potentially dozens of times more than the textual prompts. For video inputs, Video-LLaVA [30] needs to handle even more visual tokens from multiple frames, making VLM inference prohibitively expensive. Optimizing the inference process of VLMs is an urgent task to enable their application in resource-constrained real-world scenarios."}, {"title": "2.2 Token Compression for VLMs", "content": "One way to optimize VLM inference is by compressing the visual tokens that occupy the majority of the input sequence. Several studies have explored token sequence compression in language models [9, 19, 41, 46]. Compared with text, image information tends to have higher redundancy, making visual token compression for VLMs more reasonable and effective. LLaVA-PurMerge [48] leverages attention mechanisms to select important visual tokens and merges them using similar key clustering, achieving competitive performance while improving VLM inference efficiency. LLaVolta [7] proposes a heuristic and stage-wise compression method that reduces VLM training costs while largely maintaining original performance. TokenPacker [27] adopts a coarse-to-fine approach, using point-to-region attention to retain richer details while compressing visual tokens. MQT [17] and M\u00b3 [5] employ Matryoshka Representation Learning [25] to hierarchically compress visual tokens, reducing the number of tokens to no more than 10. However, these methods require additional training for VLMs to adapt to the compressed visual representations. In this work, we focus on VLM inference optimization without additional training."}, {"title": "2.3 Token Pruning for VLMs", "content": "Token pruning is another approach to improve model inference efficiency by removing less important tokens from the sequence. With the emergence of Transformer-based networks, this technique has been actively explored in natural language processing [22, 60] and computer vision [40, 47]. However, effective pruning of visual tokens in VLMs still remains under-explored. FastV [8] first identifies the redundancy and inefficiency of visual tokens in VLMs and proposed a simple method to remove visual tokens with low attention scores after the layer 2 of LLM. Subsequently, FitPrune [61] introduces a method to fit pruning recipes based on attention statistics, while SparseVLM [65] removes distractions from text prompts and uses more accurate text attention to progressively sparsify visual tokens. However, these approaches rely on text-visual attention within the LLM to evaluate the importance of visual tokens. In this work, we show that such attention does not align well with visual token importance and propose that pruning based solely on the [CLS] attention from the visual encoder yields unexpectedly good performances."}, {"title": "3 Inaccurate Text-Visual Attention in VLMS", "content": "Text-visual attention from the LLM Decoder in VLMs is often used to evaluate the importance of visual tokens for pruning. However, we find this attention does not align with the actual importance of visual tokens. To understand this misalignment, we first introduce the attention mechanism in the visual encoder and LLM decoder of VLMs in Sec. 3.1 as preliminaries. Then, in Sec. 3.2 and Sec. 3.3, we describe two phenomena observed in the visual attention of LLM decoder, termed attention shift and attention dispersion, corresponding to the position and intensity of attention respectively. These phenomena are absent in the visual encoder of VLMs, which motivates our use of [CLS] attention as a more reliable indicator of visual token importance."}, {"title": "3.1 Preliminaries", "content": "Existing VLMs typically consist of two core components: a visual encoder and an LLM de- coder, both are the Transformer-based architecture [12, 56]. Although they both rely on the self- attention mechanism, there are subtle differences in specific implementations. In VLMs, the visual encoder (e.g. CLIP [45]) employs a global attention mechanism, where the patch-wise image X = [xcls; Xing1, Xing2, \u2026, Ximgn] \u2208R(n+1)\u00d7d is first converted into the query Q, key K and value V through three weight matrices WQ, WK, Wv \u2208 Rd\u00d7d respectively:\nQ = XWQ, K=XWK, V = XWv\nwhere n is the length of the image token sequence, and d is the size of the hidden state. Then, the scaled dot-product attention is calculated as follows:\nA = Softmax(QKT /\u221ad)O = AV\nHere, we refer to the first row of A as [CLS] attention.\nWhile the LLM decoder utilizes causal self-attention [56], where each token can only attend to the previous tokens (i.e. those from the past) and not the following ones (i.e. those from the future). For the whole input sequence X = [xsys; Ximg; Xtxt; Xout] \u2208 Rl\u00d7d, the query Q, key K and value V are obtained in the same way, and the causal attention is then calculated as:\nA = Softmax((QK+M)/\u221ad)O = AV\nwhere l is the total length of the input sequence (including system prompt, image patches, text question, and output answer), and M \u2208 R\u012b\u00d7l is a lower triangular causal mask ensuring that each token attends only to itself and previous tokens. Our focus is on the attention received by visual tokens from three sources: image attention (from the image patches themselves), text attention (from the text prompt), and last attention (from the output answer since we only extract attention at the prefill phase, where the output contains only a single token). Multi-head attention is averaged across all heads following FastV [8] for analysis."}, {"title": "3.2 Text-Visual Attention Shift", "content": "To analyze text-visual attention in VLMs, we first randomly sample N image-text pairs from the LLaVA-mix665k data [32]. These samples are then used to prompt the VLM to generate responses, during which we extract the attention received by the visual tokens. In this section, N is set to 1,000, and we use LLaVA-1.5-7B as the VLM, following the original configuration in [32]. We analyze the distribution of attention scores with respect to token indices, with sources including the [CLS] token from the visual encoder, as well as image patches, text prompt, and output response from the LLM decoder (after layer 2, following [8]).\nAs shown in Fig. 3, a clear trend is visible across all attention distributions within the LLM decoder (image, text, and last), where attention scores increase with larger token indices. This suggests that if visual tokens were pruned based on text-visual attention from the LLM decoder, most retained visual tokens would be located in the lower half of the input image, potentially leading to a serious loss of important visual information. We attribute this phenomenon to the unidirectional nature of attention in the LLM, as this trend does not appear in the visual encoder, which employs global attention. Although this causal attention is the core of the next-token prediction paradigm, it is not well-suited for assessing the importance of visual tokens. As shown in the distribution plots, attention from the [CLS] token serves as a more reliable indicator for token pruning. In addition, the red vertical lines in Fig. 3 denote the length of each row in the input image (24 for CLIP-ViT-L-14-336px in LLaVA-1.5). Compared to the attention in the LLM decoder, [CLS] attention is more sensitive to image boundary areas, allocating more attention to central regions containing richer information, which further supports its effectiveness for evaluating visual token importance."}, {"title": "3.3 Text-Visual Attention Dispersion", "content": "In addition to the attention location distribution, we also analyze the distribution of attention intensity to further illustrate the lack of concentration in text-visual attention. The concentration here implies that a small number of tokens receive the majority of attention, while most tokens have low attention scores, indicating that the model identifies important tokens with high certainty. In row 1 of Fig. 4, we visualize the attention of the [CLS] token and the last output token on the visual tokens within the same image. It is apparent that [CLS] attention is highly focused, with only a few tokens receiving significant attention distributed in different positions like the aircraft nose, tail, runway, and sky. These tokens capture global information about the input image, and retaining them can accelerate VLM inference while preserving most of the performance. In contrast, the last output token attention (as well as other text tokens) is more dispersed, with multiple regions across the image receiving high attention, making it challenging to select important visual tokens during pruning. It is worth noting that in the last attention map, tokens with high attention in the [CLS] attention map receive little attention, while tokens with high attention scores are concentrated in the lower part of the image, including areas like the runway, grass, and even padding, which contain little valuable information. This aligns with the attention drift phenomenon discussed in the previous section."}, {"title": "4 [CLS] Attention for Visual Token Pruning", "content": "Based on the above analysis of attention in VLMs, we propose FasterVLM, which uses [CLS] attention from the visual encoder as a more accurate indicator for visual token pruning. By removing redundant visual tokens before the LLM decoder, our approach could make VLM inference faster than methods that prune tokens within the LLM."}, {"title": "4.1 Faster VLM", "content": "To achieve more accurate visual token pruning, we propose FasterVLM, which leverages [CLS] attention from the visual encoder to rerank visual tokens and filter out the lower parts. Fig. 5 illustrates our FasterVLM design. At the output layer of the visual encoder (e.g. the penultimate layer of CLIP [45] in LLaVA-1.5 [32]), the attention from the [CLS] token z[cLs] \u2208 Rd to other image tokens Zv \u2208 Rn\u00d7d is computed by:\na[CLS] = Softmax((Z[CLS]WQ(ZWV)T)/\u221ad)"}, {"title": "4.2 Theoretical Computation Reduction", "content": "Since the length of language instructions is uncertain and typically much shorter than the length of visual tokens, we only consider the FLOPs associated with visual tokens. In the language model of the VLM, assume n is the number of visual tokens, d is the hidden state size and m is the intermediate size of FFN (with SwiGLU [49] activation in LLaMA [54]). For the prefill stage, the FLOPs of a single transformer layer can be estimated by 8nd\u00b2 + 4n2d + 6ndm, and the theoretical FLOPs reduction with a token reduction ratio R (and n = (1 \u2013 R) \u00b7 n) is calculated as:\nF=1-(8nd\u00b2 + 4n'2d+ 6n'dm)/(8nd\u00b2 + 4n2d + 6ndm) = R+(4d+2n+3m)/(4d+2n)* (R-R\u00b2)\nSince the second term is relatively small, the overall FLOP reduction is close to R but slightly higher. In the decode stage with KV cache, the attention complexity reduces to O(n), and the FLOPs for a transformer layer can be re-estimated by 8d\u00b2 + 4nd + 6ndm. In this case, the theoretical FLOPs reduction is still close to R but slightly lower. Moreover, existing hardware does not adapt well to the varying KV cache length during inference, so pruning before the LLM decoder (our FasterVLM implementation) actually results in more inference acceleration."}, {"title": "5 Experiments", "content": "In this section, we validate our method across various VLM architectures on comprehensive multi- modal benchmarks, including high-resolution image and video understanding tasks. We compare our approach with multiple existing methods and conduct an efficiency analysis."}, {"title": "5.1 Experimental Setup", "content": "Datasets. We conduct extensive experiments on 10 image-based multi-modal benchmarks, including common visual question answering tasks such as VQAv2 [15], GQA [20], VizWiz [16], ScienceQA- IMG [36], and TextVQA [50], as well as other multi-modal benchmarks such as POPE [29], MME [14], MMBench [35], MMBench-CN [35] and MM-Vet [62]. Additionally, we also experiment on 4 widely used video question answering benchmarks, including TGIF-QA [21], MSVD-QA [57], MSRVTT-QA [57], and ActivityNet-QA [63]. All experiments on these benchmarks follow the default settings and evaluation metrics. Details of each task are provided in the appendix.\nModel architectures. We apply FasterVLM to various VLM architectures, including the classic LLaVA-1.5 [32] models with both 7B and 13B parameters, LLaVA-NeXT-7B [33] for high-resolution image inputs, and Video-LLaVA-7B [30] for video understanding. For all models, we follow the same inference settings as the original papers.\nComparison methods. We choose FastV [8], FitPrune [61], and SparseVLM [65] as comparison methods. All methods rely on text-visual attention in the LLM decoder for visual token pruning without additional training. FastV, as the initial approach, performs a one-time pruning after the second layer in the LLM. FitPrune derives an optimal pruning strategy based on predefined computational budgets, gradually pruning at each layer. SparseVLM, on the other hand, pre-selects text prompts used to guide pruning, aiming to reduce noise in text-visual attention."}, {"title": "5.2 Main results", "content": "We first apply FasterVLM to the widely used LLaVA-1.5 model and conduct a comprehensive comparison with existing methods. Tab. 1 shows the performance on the LLaVA-1.5-7B model as the visual token reduction ratio increases from 50% to 95%. We also plot the relationship between average performance and the visual token reduction ratio in Fig. 2. Note that SparseVLM includes a visual token recycling mechanism, which prevents reducing the visual token count below 95%. The results indicate that for all text-visual attention-based methods, model performance degrades noticeably as the reduction ratio increases, suggesting that text-based attention is not reliable in identifying the most important visual tokens. In contrast, our FasterVLM significantly mitigates this performance decline, retaining 89.41% of the original performance at a high reduction ratio of 95%, while substantially enhancing computational efficiency. This demonstrates the critical importance of [CLS] attention for effective visual token pruning.\nWe also validate our FasterVLM on the 13B version of LLaVA-1.5, with results shown in Tab. 2. Due to space constraints, we report only the average results across the 10 benchmarks. Detailed results can be found in the appendix. With a larger language model, the VLM achieves better performance on vision-language tasks, even with the same visual encoder. However, as the token pruning ratio increases, pruning methods based on text attention exhibit a similar, sharp decline in"}, {"title": "5.3 FasterVLM with higher resolution", "content": "Some works [33, 37, 59] attempt to improve VLM performance on visual question answering tasks by increasing the input image resolution. However, this resolution increase introduces more visual tokens, further intensifying the computational load on the VLM. In this section, we apply FasterVLM to LLaVA-NeXT-7B, which can handle up to 2880 visual tokens. The results are presented in Tab. 3. Compared to LLaVA-1.5, LLaVA-NeXT involves a greater number of visual tokens, implying a higher degree of redundancy. At a 95% reduction ratio, retaining only 145 visual tokens, FasterVLM preserves 88.35% of the original performance, significantly outperforming FastV and FitPrune. This demonstrates the value of FasterVLM for high-resolution visual inputs."}, {"title": "5.4 Faster VLM with video understanding", "content": "In addition to high-resolution images, video is another scenario with high redundancy in visual tokens. Thanks to its simple design, we can easily apply FasterVLM to Video-LLaVA, which accepts videos as input. Following [39] and [30], we conduct experiments on four video question answering benchmarks, using ChatGPT-Assistant for evaluation. Due to the commercial API usage limits, we follow [8] to use the first 1K samples from each benchmark in our experiments. The evaluation results are shown in Tab. 4. Video-LaVA processes 8 frames of 224-resolution video, totaling 2048 visual tokens. FasterVLM maintains 93.76% of the original performance at a 90% reduction ratio (retaining 130 tokens) and 89.28% with only 65 tokens (95% reduction ratio), significantly outperforming FastV. This improvement is due to the higher redundancy in temporally continuous video frames compared to single images, and the [CLS] attention accurately identifies key tokens within the video sequence, enabling FasterVLM to maintain strong performance even at high reduction ratios."}, {"title": "5.5 Ablation study", "content": "Although our design is simple, it is both effective and efficient. To demonstrate this, we conduct a thorough ablation study on different strategies, presented in Tab. 5. Lines (a)-(c) examine different criteria for pruning visual tokens before the LLM, including random pruning (a), pruning based on the average self-attention among all image patches (b), and pruning based on [CLS] attention (c, used in our FasterVLM). Using image patch attention is less effective than [CLS] attention, as the [CLS] token contains more global information. Interestingly, random pruning does not perform poorly, further highlighting the redundancy of visual tokens in VLMs. In line (d), we prune tokens at the same layer as FastV but determine token importance based on [CLS] attention. This approach performs similarly to pruning before the LLM but results in lower inference efficiency. In line (e), we apply a commonly used strategy in token compression for VLMs, token merging, where each retained token is combined with its k most similar tokens based on key similarity after selecting important tokens using [CLS] attention. Unfortunately, this strategy proves unsuitable for training-free token pruning, as greater merging led to significant performance degradation. Thus, FasterVLM adopts the simplest yet most effective method, directly pruning visual tokens before the LLM using [CLS] attention."}, {"title": "5.6 Efficiency analysis", "content": "To demonstrate the efficiency of our FasterVLM, we perform a comparative analysis of FLOPs, cache storage, CUDA memory, and inference throughput with FastV on LLaVA-NeXT-7B. To get rid of the impact of output sequence length on the decoding time, we choose the SQA-IMG benchmark for analysis, where the model only outputs a single option. All experiments are performed on a single NVIDIA A100-80GB GPU. As shown in Tab. 6, at a 95% reduction ratio, FasterVLM achieves a \u00d72.2 increase in throughput for LLaVA-NeXT. Compared to FastV with the same reduction ratio, FasterVLM requires less memory and achieves faster inference. This is because pruning within the LLM results in varying KV cache lengths, which is incompatible with current hardware. Moreover, the design of pruning before LLM enables compatibility with faster attention implementations like FlashAttention [10], which are infeasible for methods like FastV that require access to internal LLM attention information. Additional experiments are provided in the appendix."}, {"title": "6 Conclusion", "content": "In conclusion, we first reveal the inaccuracies in recent popular text-visual attention of VLMs for visual token pruning caused by the phenomena of attention shift and attention dispersion. Then, we introduce a straightforward and effective training-free visual token pruning method called FasterVLM, which evaluates the importance of visual tokens more accurately by the attentions between the [CLS] and image tokens from the visual encoder. By pruning the visual tokens before the LLM, we achieve better performance and faster inference than existing text-visual attention-based pruning methods."}, {"title": "A Details of Experimental Setup", "content": "In this appendix, we first provide the details of the experimental setup in Appendix A, including information about the datasets, model architectures, and comparison methods. Then, in Appendix B, we offer a more detailed analysis of the attention in VLMs, along with more visualizations and insights. In Appendix C, we present additional experimental results, including results with richer details across different model architectures and visualizations of the ablation studies. Finally, Appendix D includes further efficiency analysis with FlashAttention."}, {"title": "A.1 Datasets", "content": "We evaluate our method on a total of 14 widely used benchmarks, including 10 image benchmarks and 4 video benchmarks. Each task is described as follows."}, {"title": "A.1.1 Image Benchmarks", "content": "We conduct experiments on 10 image benchmarks used in LLaVA [34], including 5 visual ques- tion answering benchmarks and 5 multi-modal reasoning benchmarks. All inference settings and evaluation metrics for these tasks follow the original configurations in LLaVA-1.5 [34]. VQAv2 [15]. The VQAv2 benchmark evaluates the model's visual recognition capabilities through open-ended questions. It consists of 265,016 images from MSCOCO dataset [31], with each image containing at least 3 questions. The dataset incorporates adversarially balanced question design, ensuring that each question corresponds to at least two images with completely different answers, preventing models from relying solely on statistical patterns to derive answers. We utilize the LLaVA subset of test-dev set for evaluation, which includes 107,394 image-question pairs. Each question is associated with 10 ground truth answers, and automatic evaluation metrics are used for scoring.\nGQA [20]. The GQA benchmark focuses on evaluating structured understanding and reasoning abilities for scenes depicted in images. In addition to images and questions, it provides scene graph annotations derived from the Visual Genome dataset [24] for each image, including structured descriptions of objects, attributes, and their relationships within the scene. The questions are generated using the scene graphs, ensuring that each question corresponds to a clear semantic path. We use the accuracy on the test-dev set for evaluation, which contains 12,578 image-question pairs.\nVizWiz [16]. The VizWiz benchmark uses images captured by blind users to evaluate the model's visual understanding capabilities in real-world scenarios. Each image is first taken and uploaded by a blind user, accompanied by a question. The question is then paired with 10 crowdsourced answers for automated evaluation. Since the images are captured by blind users in real-life settings, some questions may be difficult to answer due to issues like blur or poor lighting. Additionally, since the images and questions originate from the same source, some questions may not be directly relevant to the image. We evaluate the model using the test-dev set, which includes 8,000 image-question pairs.\nScienceQA [36]. The ScienceQA benchmark uses multiple-choice questions to evaluate the model's zero-shot generalization on scientific topics. The dataset contains rich domain diversity across three subjects: natural sciences, language science, and social science. Questions within each subject are hierarchically organized by topic, category, and skill, encompassing a total of 26 topics, 127 categories, and 379 skills. The images are illustrations related to the questions, and some questions do not have corresponding images. We evaluate the model using a subset of the test set that includes both questions and images, referred to as SQA-IMG, which contains 2,017 image-question pairs.\nTextVQA [50]. The TextVQA benchmark is designed to evaluate the model's ability to recognize textual information within images, emphasizing the integration of optical character recognition (OCR) and natural language understanding. The images are primarily sourced from the Open Images v3 dataset [23] and contain a variety of scenarios such as signs, billboards, and product packaging that"}, {"title": "A.1.2 Video Benchmarks", "content": "To evaluate the performance of different methods in scenarios with higher visual redundancy, we also conduct experiments on 4 video benchmarks used in Video-LLaVA [30]. The evaluation follows Video-ChatGPT [39], using gpt-3.5-turbo assistant for scoring. Due to the commercial API usage limits, we follow [8] to use the first 1K samples of each benchmark in the experiments.\nTGIF-QA [21]. The TGIF-QA benchmark extends image-based VQA tasks to videos, requiring models to focus on both spatial and temporal attentions. It includes 72K animated GIFs from the Tumblr GIF dataset [28] and 165K crowdsourced question-answer pairs. We evaluate model performance using the Frame QA task in this benchmark.\nMSVD-QA [57]. The MSVD-QA benchmark is based on the Microsoft Research Video Description Corpus [6], which is commonly used for video captioning tasks. The question-answer pairs in the benchmark are derived from the descriptions in the corpus. The benchmark consists of 1,970 video clips and 50.5K question-answer pairs in total.\nMSRVTT-QA [57]. The MSRVTT-QA benchmark is based on the Microsoft Research Video to Text dataset [58], which is larger and has more complex scenes than the MSVD dataset. The benchmark consists of 10K video clips and 243K question-answer pairs in total.\nActivityNet-QA [63]. The ActivityNet-QA benchmark is based on the ActivityNet dataset [4], consisting of 5,800 complex web videos with an average length of 180 seconds. The benchmark includes 58K question-answer pairs, all of which are manually annotated to ensure higher quality."}, {"title": "A.2 Model Architectures", "content": "LLAVA-1.5 [32"}]}