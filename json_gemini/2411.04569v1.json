{"title": "Impact of Label Noise on Learning Complex Features", "authors": ["Rahul Vashisht", "P. Krishna Kumar", "Harsha Vardhan Govind", "Harish G. Ramaswamy"], "abstract": "Neural networks trained with stochastic gradient descent exhibit an inductive bias\ntowards simpler decision boundaries, typically converging to a narrow family of\nfunctions, and often fail to capture more complex features. This phenomenon\nraises concerns about the capacity of deep models to adequately learn and represent\nreal-world datasets. Traditional approaches such as explicit regularization, data\naugmentation, architectural modifications, etc., have largely proven ineffective in\nencouraging the models to learn diverse features. In this work, we investigate the\nimpact of pre-training models with noisy labels on the dynamics of SGD across\nvarious architectures and datasets. We show that pretraining promotes learning\ncomplex functions and diverse features in the presence of noise. Our experiments\ndemonstrate that pre-training with noisy labels encourages gradient descent to find\nalternate minima that do not solely depend upon simple features, rather learns more\ncomplex and broader set of features, without hurting performance.", "sections": [{"title": "1 Introduction", "content": "Overparameterized models trained using stochastic gradient descent tend to focus on only a small\nfraction of the available features. Such behavior reduces the diversity of features that contribute\nto the classification of data. This phenomenon has been discussed in the context of simplicity bias\n[20, 17], where neural models solely rely on simpler, easy-to-learn, features. It is also studied in\nrelation to the intrinsic regularization properties of SGD that inherently favors lower-complexity\nmodels [10]. Previous works have shown that models trained using SGD learn linear functions first,\nand as training progresses they learn functions of increasing complexity [10]. This preference causes\nmany potentially useful features to remain unlearned and underutilized, resulting in models that have\ninferior discriminative quality and rely on features that are coincidental rather than causal [2]. Such\nmodels when faced with distributional shifts or adversarially perturbed data, are unable to generalize\nto novel or altered environments.\nRecent works have attributed this biased behavior to the intrinsic regularization properties of SGD that\nnaturally favors low-complexity solutions [10]. Models that exhibit poor generalization for minority\ngroups are shown to dedicate excess parameters to memorizing a small number of data points [19].\nComplex features are often overshadowed by the amplification and replication of simpler features [1].\nIn real-world datasets, simplicity bias in neural networks is often defined as the propensity to learn\nlow-dimensional projections of inputs [14], where features such as shape and color (e.g., in MNIST or\nCIFAR) define decision boundaries of varying complexity. Standard approaches like ensembling and\nadversarial training fail to effectively address the limitations imposed by this bias [20]. There is an\nincreasing interest in understanding the factors that contribute to SGD's bias toward simpler feature\nsets, shortcut feature learning, as well as strategies to mitigate or exploit this tendency to improve\nmodel's performance. These include approaches that aim to reduce feature correlation [1, 24, 14, 23],"}, {"title": "2 Effect of Noisy Pre-training on Learned Decision Functions", "content": "We study a two stage training procedure: First, we pre-train an overparameterized model on corrupted\nlabels (by randomly flipping the labels for a fraction of data points) called as noisy pre-training.\nThis pre-trained model is then optimised to minimize the training loss, which naturally leads to a\nhigh training accuracy over the noisy-labels. Second, we utilize the pretrained model to once again\ntrain over the original unmodified labels. In subsequent sections, we compare this two-stage method\nwith standard training without explicit regularization.\nConsider a d-dimensional synthetic Multi-slab dataset D, whose first coordinate is a linear block\nwith instances for class-0 sampled from Uniform distribution in [-1,0] and class-1 sampled from\nUniform distribution in [0, 1]. The remaining d 1 coordinates have samples from two classes\ndistributed in k-well separated alternating regions 3 [20]. We consider a a 4-dimensional multi-slab\ndata with increasing complexity. Figure 1 illustrates the 4-dimensional multi-slab data. Here, an\noverparameterized ReLU network can potentially use any feature to determine the class label. Note"}, {"title": "3 Experiments", "content": "In this section, we empirically demonstrate the impact of initializing parameters from noisy pretraining\nover Dominoes [20] and Waterbirds data [16]. See Appendix A.1 for more datasets.\nDatasets: MNIST-fMNIST dataset from Dominoes consists of collated images, where {0, 1} MNIST\ndigits are vertically stacked with {shirt, frock} Fashion-MNIST apparel. Here, MNIST block is the\neasier feature to learn, whereas fMNIST is complex. WaterBirds dataset comprises of birds natively\nfound on land/water, with different backgrounds of land or water. An image of water-bird with water\nin the background (similarly land-bird on land) is called in-group partition, whereas mismatch of\nwaterbird-on-land or landbird-on-water is referred to as out-group partition. Details in Appendix A.1.\nMeasuring feature dependence: To identify the feature(s) that the learned model is utilizing for\nclassification decision, we use: (1). Randomized shuffle accuracy: to selectively shuffle one among\nMNIST (top) or fMNIST (bottom) part of the test images, and reevaluate the model. If its performance\nis drastically reduced by shuffling a feature, it implies the dependence of model's decision on the\ncorresponding feature. (2). Visualizing Gram matrix: We plot diagonal entries of Gram matrix\nWT W\u2081 to observe changes in the first layer's parameter W\u2081. The brighter parts of W\u2191 W\u2081 denote\ngreater dependence on those corresponding pixels in the input image. Further, Appendix A.6 shows\neigenvector based visualization for Gram matrix.\nVarying correlation between features & class labels: We control the predictive powers of features\nby perturbing their correlation with true labels. For example: In Dominoes dataset D, we create\npartially-correlated data D' by collating images such that top MNIST-block is only 95% correlated\nwith classification labels (not 100% accurately predictive). This implies that if predictions were\nsolely on the basis of top-block, then the bayes-error will be 5%. The rationale of adding adversarial\ncorrelation is to push the models to those regimes where no simple solution achieves perfect accuracy,\nthus getting closer to real world scenarios."}, {"title": "4 Discussion and Conclusion", "content": "Overparameterized neural networks tend to learn decision functions based on a limited and simpler\nset of features if they exist. Our experiments demonstrate that initializing the model parameters by\nfirst training over noisy-labels can enable the model to overcome this constraint, and allow for the\nlearning of more diverse range of features. One possible explanation for this phenomenon is that\nthe neural model reaches a minima that is characterized by a complex decision boundary to fit the\nnoisy labels. The model, however, remains trapped in this minima and is unable to revert to a simpler\ndecision boundary despite the intrinsic regularization properties of SGD. This observation challenges\nthe commonly accepted opinion that SGD provides effective implicit regularization."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Dataset Description:", "content": "Slab Data: Figures 4 and 5 shows 4-dimensional dataset and noisy 4-dimensional multi-slab dataset.\nDominoes Data: We consider Dominoes binary classification datasets consisting of 3 independent\ndatasets, where the top half of the image contains MNIST digits [12] from classes 0, 1, and the\nbottom half contains MNIST images from classes 7, 9 (MNIST-MNIST), Fashion-MNIST [27]\nimages from classes coat, dress (MNIST-Fashion) or CIFAR-10 [11] images from classes car, truck\n(MNIST-CIFAR). In all Dominoes datasets, the top half of the image (MNIST 0-1 images) presents a\neasy to learn feature; the bottom half of the image presents a harder-to-learn feature. The images\nare made into gray-scale and scaled to appropriate size so that they can be collated top-bottom style.\nFigure 6 shows the examples of dominoes dataset. In case of 100% correlation Note that each block,\ntop and bottom, is fully predictive of the class label in case of 100% correlation, while only harder to\nlearn feature is fully predictive of class label in case of 95% correlation.\nWaterbirds Dataset: The waterbirds dataset is constructed by cropping birds from Caltech-UCSD\nBirds-200-2011 (CUB) dataset [25] and taking backgrounds from the Places dataset [28] and placing\nthe birds from CUB dataset to backgrounds from Places dataset. Figure 7 shows sample instances\nfrom waterbirds dataset. Table 4 describes the proportions of different groups in the waterbirds\ndataset. The waterbirds on land and landbirds on water are known as out-group, while landbird on\nland and waterbirds on water are known as the in-group."}, {"title": "A.2 Network Architectures", "content": "Slab Dataset: We use 2-layer multi-layer perceptron network with ReLU activation, the first layer\ncontains 100 hidden unit and second layer contains 200 hidden units. We use SGD with 0.1 learning\nrate as optimizer.\nDominoes Dataset: For all dominoes dataset, \u201cMNIST-FMNIST\u201d, \u201cMNIST-MNIST\", and \"MNIST-\nCIFAR\", datasets, we use 4-layer fully connected networks with 10 hidden units and ReLU activation.\nWe use SGD optimizer with learning rate 0.01.\nWaterBirds Dataset: We use fully connected network, with 4 layers and ReLU activation having 20\nhidden units. We use SGD optimizer with learning rate 0.05 ."}, {"title": "A.3 Additional Results:", "content": ""}, {"title": "A.3.1 4D Slab Data Decision Boundary plots for Different Random Seeds", "content": "In this section, we show that under standard training, the model converges to a similar family of\ndecision functions, which are easier to learn. This convergence does not happen under pretrained\nnoise based training, which allows model to learn diverse set of functions with reliance on harder to\nlearn features. Figure 9 and 10 demonstrates this finding using decision boundary plots for 4d slab\ndata described in Section 2."}, {"title": "A.4 Impact of Label Smoothing", "content": "Label smoothing [22] is a regularization technique, which replaces the one-hot ground truth vectors,\nwith mixture of ground truth vectors and uniform distribution. In case of noisy pretraining, flip the\nlabels of a fraction of data samples, whereas in case of label smoothing, the ground truth labels are\nmixture of one hot vector and uniform distribution, which act as a addition of noise to labels, thus\nshowing equivalent effect."}, {"title": "A.5 Varying Model Architecture", "content": ""}, {"title": "A.6 Visualizing Feature Importance Using Top Eigenvectors", "content": ""}]}