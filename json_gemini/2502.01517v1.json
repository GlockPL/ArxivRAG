{"title": "Regularized interpolation in 4D neural fields\nenables optimization of 3D printed geometries", "authors": ["Christos Margadji", "Andi Kuswoyo", "Sebastian W. Pattinson"], "abstract": "The ability to accurately produce geometries with specified properties is perhaps the most important\ncharacteristic of a manufacturing process. 3D printing is marked by exceptional design freedom and\ncomplexity but is also prone to geometric and other defects that must be resolved for it to reach its full\npotential. Ultimately, this will require both astute design decisions and timely parameter adjustments\nto maintain stability that is challenging even with expert human operators. While machine learning\nis widely investigated in 3D printing, existing methods typically overlook spatial features that vary\nacross prints and thus find it difficult to produce desired geometries. Here, we encode volumetric\nrepresentations of printed parts into neural fields and apply a new regularization strategy, based\non minimizing the partial derivative of the field's output with respect to a single, non-learnable\nparameter. By thus encouraging small input changes to yield only small output variations, we\nencourage smooth interpolation between observed volumes and hence realistic geometry predictions.\nThis framework therefore allows the extraction of \"imagined\" 3D shapes, revealing how a part would\nlook if manufactured under previously unseen parameters. The resulting continuous field is used for\ndata-driven optimization to maximize geometric fidelity between expected and produced geometries,\nreducing post-processing, material waste, and production costs. By optimizing process parameters\ndynamically, our approach enables advanced planning strategies, potentially allowing manufacturers\nto better realize complex and feature-rich designs.", "sections": [{"title": "1 Introduction", "content": "Perhaps the most crucial aspect of a manufacturing process is its ability to create geometries with defined properties.\nThis is also true of material extrusion (ME), the most prevalent additive manufacturing (AM) or 3D printing technique,\nwhich can produce highly complex and custom parts in a wide variety of materials. Its applications span areas from\naerospace [1, 2, 3, 4], to medical devices [5, 6, 7, 8, 9], and construction [10, 11]. ME relies on feedstock material in\nfilament form which undergoes heating before being extruded through a nozzle onto the build surface in a layer-by-layer\nmanner[12]. However, the complexity of the processes makes it prone to geometric and other defects. This is because\nthe printed object results from the interaction of diverse factors including material flow and the motion of the mechanical\nsystem, all of which must be correctly coordinated at each point in space and time[13].\nThe complexity of the parameter space combined with the potential wealth of data available to characterize the process\nmakes ME amenable to machine and particularly deep learning approaches. These have been widely investigated\nfor monitoring and controlling process parameters with a view to minimizing defects [14, 15, 16, 17, 18]. However,\nthe mitigation of flaws in AM via deep learning, while always challenging, becomes increasingly intractable when\ncompounded by sub-optimal part design that does not account for the manufacturing process [19]. For instance, the\ninclusion of intricate features, such as thin walls, can introduce manufacturing instabilities that compromise geometric\nfidelity. In such cases, controlling material flow rate is particularly critical. Higher flow rates may be needed during\ninitial layers to improve adhesion between the build surface and the part, while lower flow rates help maintain accuracy\nin thin-profile regions by preventing hatch overlap. Yet, some situations require both higher and lower flow rates within\nthe same print, creating inevitable trade-offs. Conventionally, identifying correlations between design features and\nprocess parameters relies heavily on trial-and-error methods, which is expensive in time, labor and resources.\nSimilar challenges in spatial domains have been successfully addressed using 3D deep learning. In medicine, 3D deep\nlearning has been used to analyze breast magnetic resonance imaging scans and detect anomalous, potentially malignant\ngeometric features [20]. In robotics, it has been used for scene perception[21] and completion[22], allowing agents to\ninfer occluded structures beyond their immediate observations. Additionally, it has been used to model deformations,\npredict structural integrity, and simulate fluid dynamics[23, 24, 25, 26, 27]. However, translating such methods to AM\nis difficult. Unlike some more established domains, AM processes often feature a combinatorial explosion of variables\ndriven by factors including the need to apply specific process parameters at each point in a build, properties of multiple\nmaterials and their interfaces, and highly diverse available design configurations. Complicating matters further is the\nscarcity of high-quality AM datasets, the difficulty of tracking defects in 3D spaces, and the limitations inherent to\nprevalent shape representations[28]. Consequently, direct adaptation of strategies from medicine, robotics, and other\ndomains is non-trivial, demanding new approaches tuned to the interplay between spatial and process-driven challenges\nof AM.\nNeural fields are continuous functions (fields) \u2013 often defined over space, time, or other domains \u2013 parameterized\nby a neural network. Instead of storing or describing a complex object as a traditional dataset of discrete points,\npixels, or voxels, a neural field represents it as an implicit function that can produce values at any coordinate. This\napproach allows for flexible representations that can capture intricate details and make it possible to generalize to\nunseen data. Neural fields have been used in various domains such as computer graphics [29, 30, 31, 32, 33, 34],\nrobotics[35, 36, 37, 38, 39]and scientific simulations[40, 41, 42] and various types of fields can be modeled, including\nimage pixel information[43, 44], spectrogram features or wavelengths in audio processing tasks [43, 45, 46], surrogate\nmodels for partial differential equations [42, 47], or other relevant physical quantities [48, 49, 50]. Specifically for\n3D representation tasks, neural fields have been utilized to represent shapes as occupancy[51] or signed distance\nfunctions [52] and scenes as radiance fields [53]. Notable advancements include the introduction of periodic activation\nfunctions[43], which capture high-frequency details in input data, and learning Fourier features directly[54], which\nenhances detail representation. Other advancements have further improved the efficiency and scalability of neural fields,\nmaking them more practical for a wide range of applications [55, 56, 57, 58].\nIn this study we deployed neural fields to optimize geometric fidelity in 3D printing. Every neural field is designed and\noptimized to encode the volume of multiple instances of a geometry, as a function of the underlying process parameter.\nTo ensure smooth interpolation, we introduce a novel gradient-driven interpolation regularization (GDIR) strategy,\nwhich minimizes the partial norm of the Jacobian of the network. This reduces the impact of small parameter variations\non output geometry, drawing inspiration from the small-motion assumption and advancements in physics-informed\nneural networks. By treating the trained neural field as a model capable of inferring geometry at unseen flow rate values,\nwe identify optimal process parameters to maximize geometric fidelity between expected and produced parts. Applying\nthese parameter adjustments in the physical process demonstrates the effectiveness of our approach in enhancing print\nquality and fidelity."}, {"title": "2 Results", "content": "Process parameters in manufacturing, like material flow rate in 3D printing, are critical in aligning expected and\nproduced geometries in ME. Here, \"produced geometry\" refers to the physical output of the process, while \"expected\ngeometry\" denotes the as-modeled design specified by the user. Discrepancies between these geometries are common,\nwhich makes achieving geometric fidelity a well-documented challenge. However, datasets which systematically\nexplore the relationship between expected and produced geometries remain scarce. To address this gap, we created a\ncustom dataset using a 3D printing system and a computer-tomography (CT) scanner, as detailed in Methods 4.1.1 and\n4.1.2. Four experimental cycles were conducted, producing iterations of four distinct geometries: Bolt, Gear, Bunny,\nand Statue. Each geometry was manufactured nine times with different flow rate values (45%, 50%, 60%, 80%, 100%,\n130%, 170%, 220%, and 280%). The flow rate is expressed as a percentage relative to a calibrated baseline, where\n100% represents the optimal setting. Post-manufacturing, CT scanning was used to capture the volumetric structure of\nthe printed parts, including internal and external features, creating digital twins of all nine iterations of each produced\ngeometry. The scans were filtered, registered, and aligned as described in Methods 4.1.3. The complete pipeline - from\nscan acquisition to registered volumetric data - is illustrated in Figure 1B. Post-processing of each scan was validated\nusing the physical to digital weight comparison as described in Methods 4.5.1.\nAlthough the expected geometry is always the same (i.e., same numerical commands\ngiven to the printer), higher flow rates result in increased material deposition, leading to bulkier geometries. Conversely,\nat lower flow rates, insufficient material is deposited to fully fill the geometry domain, resulting in voids within the\nproduced part. This is consistent across all tested geometries, but notably, for the Bunny geometry it is evident that\nspecific features print more accurately at a flow rate of 170% compared to the standard 100%. This challenges the\nconventional practice of maintaining a constant flow rate throughout the process that oversimplifies the process. For\ninstance, it might be the case that thin features may benefit from increased flow rates, due to enhanced material adhesion\nand improved structural integrity, compensating for the challenges of depositing material on smaller, less stable regions.\nThis raises an important and underexplored question: can process parameters be dynamically controlled to maximize\nfidelity for different regions of a geometry?"}, {"title": "2.2 Promoting smoothness in neural fields", "content": "First, we aim to understand how geometry changes in response to process parameter alterations. We leverage neural\nfields, representing the produced geometries as learned functions denoted F(x, y, z; $). Here, (x, y, z) are spatial\ncoordinates in 3D space, and $ represents the flow rate. The function F maps this 4D input space to a scalar value S,\nwhich indicates the presence (S = 1) or absence (S = 0) of material at the specified infinitesimal point, based on the\ninstance's scan. In our framework, F is a multi-layer perceptron featuring sinusoidal activation functions in all hidden\nlayers but the last. To visualize the produced geometry as manufactured from a specific process parameter value \u03c6 = \u03c6\u03bf,\nwe evaluate F over a grid spanning the entire spatial domain, while holding 40 constant. This allows determining the\nstructure's volumetric representation as shown in Figure 2, referred to as \u201cmanifold reconstruction\u201d hereafter.\nTo train the neural field, we represent each CT scan as a set of data points P where each point pi corre-\nsponds to a tuple (xi, Yi, Zi, \u00dei, Si), with (xi, Yi, zi) denoting the spatial coordinates, i the process parameter,\nand S \u20ac 0,1 the material state. The number of data\npoints N, is given by the product of the dimensions of the\n3D array representing the scan resolution, multiplied by\nthe number of available iterations per geometry. All in-\nputs are normalized to the range [-1, +1] using min-max\nnormalization, based on the specified domain boundaries.\nDuring training the primary objective is to minimize a\nmean squared error function that minimizes the difference\nbetween the neural field's prediction and the known state\nS. More training details are provided in Methods 4.2.1\nand implementation details in Methods 4.3.1.\nBy training on multiple geometry instances simultane-\nously, we aim to construct a continuous representation of\nthe geometric space. That would allow the learned func-\ntion to \u2018imagine' how the geometry would look like, had it\nbeen manufactured under certain unseen conditions. How-\never, there are no formal guarantees for continuity. As\nsupervision decreases, the learning algorithm struggles,\nleading to a higher likelihood of unreliable interpolations\nbetween seen conditions. This phenomenon is clearly\ndemonstrated in Figure 3, where the 3D reconstructions\ncollapse at unseen regimes, particularly in the higher\nranges where supervision is more limited. Addressing\nthis in conventional ways would require enriching the\ndataset with additional iterations of the geometry. Un-\nfortunately, this approach is generally impractical due to\nthe high cost of manufacturing and measuring the parts\nneeded to produce such data.\nThe physics of the problem can be leveraged to ensure\nsmoothness in the learned field, by encouraging Lipschitz\ncontinuity through GDIR (see Methods 4.2.2) This prop-\nerty ensures that small changes in the input dimensions\nlead to proportionally small or smaller changes in the\noutput, formally defined in equation (1):\n$VX \\in R\u00b3 \\lim F(X, \\varphi1) - F (X,\\varphi2) | \\approx 0$ (1)\n$\\varphi1-\\varphi2$\n; where 1 and 2 are neighboring flow rate values and\n|\u00b7| denotes a suitable norm measuring the discrepancy\nbetween output signals. On the global scale, this implies\nfor an infinitesimal change in the flow rate value, there should only be a negligible change in the structure's volume as\nreconstructed by F. In our application we achieve this systematically, by minimizing the field's gradients; this involves\ndifferentiating through the network with respect to the flow rate parameter input (i.e., calculate the partial derivative)\nand encourage smoothness at the said dimension by penalizing the norm of the gradient's Jacobian. This becomes our\nsecondary training objective, with effective results shown in Figure 3. We also show additional results in Figure A2 in\nrandom XY planes, for better visualization. We note that introducing GDIR slows down training by 8.30\u00b10.12%. At\ninference time, there is no additional latency as the gradient calculations are not necessary."}, {"title": "2.3 Neural fields as simulators for optimizing geometric fidelity", "content": "These models accurately represent printed objects and smoothly interpolate between seen instances; however, the\nprimary goal is to provide a tool for engineers to maximize geometric fidelity between expected and produced geometries.\nTo achieve this, we leverage the learned neural field as a simulation engine to model geometry evolution as a function of\nprocess changes. The network's continuity enables it to \u201cimagine\" expected outputs, even if process parameters change\nmid-print. This capability allows us to conduct millions of virtual experiments with varying parameter sets, facilitating\ndynamic discovery of optimal parameters (see Methods 4.4).\nTo evaluate our methodology, we optimized the geometric fidelity of the printed Bunny model, a widely used yet\nchallenging case study due to its intricate features. Our goal was to determine the optimal flow rate for each of the\nmodel's 100 layers to preserve smoothness in the main body and minimize overlap in delicate regions like the ears\n(Figure 5A). Treating flow rate as a discrete variable ranging from 45% to 280% in 1% increments, we computed a\nfitness score proportional to the L1-norm between the reconstruction and the rendering (Figure 1B). This produced a\ngrid landscape where the optimizer sought the minimum trough corresponding to the optimal parameters (Figure 1C).\nThe optimizer identified higher flow rates for the ear regions while reducing material extrusion early to prevent excessive\nbuildup at the ear tips. It also slightly under-extruded the body, a common 3D printing practice to enhance geometric\nfidelity, albeit at the expense of structural integrity. Applying these corrections resulted in a printed Bunny with both\nsmooth body contours and fine ear details, Figure 1D. However, the unsupported belly area remained rough, as the\noptimization algorithm treated each layer independently. This limitation prevented it from addressing roughness caused\nby insufficient support from preceding layers in the 3D space."}, {"title": "3 Discussion", "content": "Central to our approach is a novel regularization strategy that promotes smooth interpolation across the process\nparameter space. By introducing structured inductive biases in regions where data is sparse, this technique helps avoid\nerratic behavior in uncharted areas. In essence, it guides the model toward more stable and predictable outputs, even\nwhen direct supervision is limited. Most importantly, our approach is process and representation agnostic, which implies\nthat it may be used for any other manufacturing method which requires optimization to achieve diverse properties as a\nfunction of process parameters. Additionally, we note this approach is well applicable to a broader set of problems\nwithin the machine learning community, including video frame interpolation.\nThe shapes of produced parts from 3D printing are a function of the expected geometry, and the process parameters\nused to produce it as we showed. We empirically learn this function, utilizing neural fields as a form of continuous\nshape representation. The neural fields are directly learned from experimental data obtained through CT scans of"}, {"title": "4 Methods", "content": "The experimental setup for data collection consisted of a Creality CR-20 Pro material extrusion AM system, without\nadditional modifications to replicate typical operational conditions. The AM system was interfaced with a Raspberry Pi\n4 Model B running OctoPrint 1.9.3. The material flow rate, defined as the percentage of material exiting the nozzle's\norifice per unit of time, was directly controlled by passing numerical commands (M221 SXX where XX is given in\npercentage) to the printer's firmware. This measurement is always relative to the default settings established during\ncalibration. The AM system utilized polylactic acid (PLA) material feedstock in the form of a 1.75 mm grey color\nfilament, sourced from PolyMaker.\nAll geometries were outsourced from the internet in a mesh form and were turned into GCode using Cura 5.6.0. The\nprinting durations for each trial were P01: 47 minutes, P02: 68 minutes, P03: 23 minutes and P04: 32 minutes. Each\ngeometry was manufactured nine times, each time with a different flow rate as explained in the main text. The parts\nfrom each geometry were then scanned using computer tomography and the processing pipeline described in the main\ntext was used for alignment and registration. All data are shown in Supplementary Figure A1 A to D. Physical to digital\nweight for the geometries is also shown in Figure S1E."}, {"title": "4.1.2 Micro-Computer-Tomography", "content": "A Nikon Xtex 225 H micro-CT scanner with a 28802880-pixel detector was used to capture the volumetric structure\nof the geometries. CT was chosen over alternative methods like structured light scans due to its capability to capture\ninternal structures in addition to external surfaces without significant cost constraints. For each geometry, the nine\nsamples were loaded on a custom foam holder and scanned together to increase efficiency. The voxel size in X, Y and Z\nwas set to 35.126 to 45.448 microns, depending on the size of the holder. The CT scan originally provided axial images,\nwhich were transitioned to voxel data via standard computational reconstruction techniques, Figure 1A. A threshold\nfilter was then applied to eliminate lower refraction intensities occurring from the foam holder, and a Gaussian filter\nwas applied to remove any remaining artifacts. The scan was subsequently partitioned to isolate the nine samples, and\neach one was moved to its own axes system."}, {"title": "4.1.3 Registration and alignment", "content": "Orientation mismatches in CT scan analysis and other spatial domain applications often hinder the direct comparability\nof data from different samples. To address this challenge, we detect the first layer of each sample for precise centering\nand alignment. Specifically, we track the first activated pixel from the bottom along the Z axis to construct a depth\nmap that accurately represents the bottom surface of each build. We then fit a plane on the detected depth points and\ncalculate a transformation that aligns with the XY plane. This transformation is subsequently applied to the entire\ndomain, ensuring that the Z direction of the part is aligned with the original build direction. We then register the\ndifferent sample scans, by calculating a transformation that involves translation in the X and Y directions and rotation\naround the Z axis, utilizing a probabilistic approach known as coherent point drift (CPD)."}, {"title": "4.2.1 Neural fields", "content": "In principle, a neural field is a neural function which encodes the mapping from input coordinates to corresponding\nsignal values, Equation (2)\n\u0398* = arg min  \u03a3(Si, Fe(Xi))\n\u0398\ni=0\n(2)\n; where Rin represents the input space and each dimension corresponds to a feature or attribute of the data, O the\ntrainable parameters that govern the behavior of the neural network and Rout signifies the output space. To train the\nneural field, a dataset composed of N input coordinates X\u2081 \u2208 Rin and corresponding output signal values Si \u2208 Rout\nis exploited, where i indexes point pairs and takes values 1 < i < N. The training objective is finding the optimal"}, {"title": "4.2.2 Gradient-driven interpolation regularization", "content": "A Lipschitz network is a type of neural network designed to enforce a Lipschitz constraint, which ensures that the rate\nof change of the network's output is bounded by a constant c, referred to as the Lipschitz constant[59]. Formally, for\nany two inputs $1 and 2 in the input space Rin, a Lipschitz continuous function F\u0259 satisfies Equation (5)\n||Fe (41) - Fe (42)|| \u2264 c||41-42||(5)\n; where c controls the smoothness of the function. This property is critical for tasks requiring stable, controlled, and\ninterpretable mappings, such as neural fields, where the network encodes spatially continuous functions like signed\ndistance fields. Similar assumptions have been made elsewhere [60, 61, 62] including for optical flow filters which are\nfamous for motion prediction[63].\nWithout losing generality, Equation (5) can also be expressed as Equation (1) from the main text, which can be enforced\nby encouraging the partial derivative of the network's output with respect to & being equal to 0, Equation (6)\nFe\n= 0\n\u10db(6)\nThe partial derivative can be computed using automatic differentiation[64] which is readily available in most common\nmachine learning frameworks such as autograd in PyTorch. To encourage minimization of the computed derivative, an\nadditional loss term is integrated, with the loss function becoming Equation (7)\nL=\nLMSE + 1.Fe\n2\n(7)\n; where X is a weight factor to balance the magnitudes of the two terms. This factor controls the strength with which\ngradient constraints are enforced, leading to a constrain-malleability trade-off as discussed. We call this approach\ngradient-driven interpolation regularization (GDIR) as it promotes smoothness in the learned representation by regulating\nits gradients.\nWe note that the minimization of the two loss terms is computed on two different point sets. While LMSE is computed\non the available data points which represent occupancy (from the CT scans), the minimization of the partial derivative\nneeds to hold true at all coordinates in Rin. We thus utilize Latin hypercube sampling (LHS) to generate proxies for the\nlatter, i.e., random coordinates across the combined spatial and parameter spaces. LHS is particularly advantageous as\nit ensures a more uniform distribution of sampled points across the entire space compared to simple random sampling.\nThe efficiency and accuracy of this method also depend on the number of sampled proxies. Generally, a higher number\nenhances the resolution and robustness of GDIR at the cost of increased computational intensity. It is crucial to balance\nthe granularity of sampling with the available computational resources."}, {"title": "4.3 Volume to signed distance function", "content": "We convert the volumetric data into a signed distance function (SDF) by applying the marching cubes algorithm[65] to\nextract surfaces from each available volume. We then calculate the Euclidean distance of each point in the volumetric\ngrid from its nearest surface, resulting in the SDF. This scalar field encodes the distance from each point to the nearest\nsurface, with the sign indicating whether the point is inside or outside the geometry. Specifically, points inside the\ngeometry have negative SDF values, points on the surface have a zero SDF value (the zero-level set), and points outside\nhave positive SDF values. The zero-level set precisely defines the object's surface. All SDF values are normalized\nbased on their minimum and maximum values according to their sign. The grid is then flattened into N points with\nform (x, y, z, \u00a2, SDF)."}, {"title": "4.3.1 Implementation details", "content": "The architecture we use is based on the original SIREN with hyperparameters shown in Table A3. GDIR networks have\nsame characteristics, but the additional regularization term is enabled. For each geometry, the best frequency for the\nsinusoidal activations (both first and hidden) was found using a grid search on 10% of the available training data points.\nTraining and inference were conducted on one NVIDIA Quadro RTX 5000 (16GB) GPU supplemented by an Intel\nCore i9-9900K (16-core) CPU and 64GB of DDR4 RAM."}, {"title": "4.4 Optimization strategy", "content": "Assuming the neural field achieves high accuracy, we iteratively evaluate all layers to determine the optimal flow\nrate & that maximizes the similarity between the cross-section of the expected geometry at any given height z and\nthe corresponding manifold reconstruction. This approach is conceptually like topology optimization methods (also\nsolvable with neural fields) [66, 67] but focuses on precise control of material deposition at a per-layer resolution to\nachieve desired geometric outcomes. This can be expressed as Equation (8)\n(z) = arg min |Fe (x, y\u2758 z) \u2013 layer (z)|\n\u03c6(8)\n; where layer (z) represents cross-sectional images rendered from the expected geometry, and Fe(x, y|z) corresponds to\nthe geometric reconstruction generated by the field at the pre-specified height z. The \"rendered\" cross-sections represent\nthe expected geometry and are obtained by processing numerical outputs from the computer-aided manufacturing slicer\nand visualizing them using computer graphics techniques, rather than directly using surfaces derived from the CAD\nmesh. This ensures fidelity not only to external surfaces but also to internal features influenced by infill patterns, which\nare critical for the part's functionality."}, {"title": "4.5 Validation metrics", "content": "To calculate the digital weight and compare it with the physical weight, we calculate the product of occupied voxels,\nvolume of each voxel as a function of resolution, and material density. In mathematical form, Equation (9)\nDW = \u039f \u00d7 \u03c5 \u03c7\u03c1(9)\n; where O is a scalar representing the occupancy in terms of the number of activated voxels, v is the volume of each\nvoxel and p is the density of polylactic acid (PLA) equal to 1.25g/cm\u00b3. The physical mass of the samples is weighted\nusing an Ohaus Navigator NVT Precision Balance."}, {"title": "4.5.2 Structural Similarity Index", "content": "The Structural Similarity Index (SSIM) evaluates the similarity between two images, emphasizing structural content\nover pixel-wise differences. Widely used in tasks such as compression, denoising, and image generation, SSIM\ncompares luminance (mean intensity), contrast (standard deviation), and structure (normalized correlation) across small,\noverlapping windows, typically weighted by a Gaussian function. It is defined as: Equation (10)\nSSIM (x, y) =(2\u03bcx\u03bcy + C1) (25xy + C2)\n(\u03bc\u03b1 + \u03bc\u03b5 + C1) (\u03c3 + \u03c3\u03b5 + C2)(10)\n; where \u03bc\u03b5 and \u00b5y are the mean intensities, \u03c3\u03b5 and \u03c3y are the variances, and oxy is the covariance between images x\nand y. Constants C1 and C2 stabilize the formula to avoid division by zero. SSIM values range from [\u22121, +1], where\nthe max value indicates identical images.\nFor our 3D domain, we compute SSIM by slicing it into 2D cross-sections along the Z-axis, calculating SSIM for each\nslice. Results are averaged, with error bars representing standard deviations across slices."}, {"title": "4.5.3 L1-norm", "content": "The L1-norm is a suitable choice for comparing discrepancies between two volumetric datasets because it measures\nthe absolute differences between corresponding elements in the datasets. By summing the absolute values of these\ndifferences, the L1-norm provides a straightforward and interpretable metric to quantify the overall deviation between\nthe datasets, making it ideal for evaluating similarity or identifying anomalies in volumetric data.\nMathematically, for a pair of volumetric datasets with the same resolution, the L1-norm is defined as Equation (11):\nVi - V21 = \u03a3\u03a3\u03a3 1 (i, j, k) \u2013 V2 (i, j, k)|\nN\ni=1 j=1 k=1(11)\n; where V\u2081 (i, j, k) and V2(i, j, k) are the values of the volumetric datasets V\u2081 and V2 at the spatial coordinate (i, j, k)\nand n, m, and p are the dimensions of the datasets along the x-, y-, and z-axes, respectively. N is the total number of\npoints being compared, equal to n \u00d7 m \u00d7 p. This formulation provides a global measure of discrepancy, making it an\neffective tool for volumetric data analysis. L1-norm is averaged on across parts."}]}