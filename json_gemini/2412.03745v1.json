{"title": "Deep Variational Bayesian Modeling of Haze Degradation Process", "authors": ["Eun Woo Im", "Junsung Shin", "Sungyong Baik", "Tae Hyun Kim"], "abstract": "Relying on the representation power of neural networks, most recent works have often neglected several factors involved in haze degradation, such as transmission (the amount of light reaching an observer from a scene over distance) and atmospheric light. These factors are generally unknown, making dehazing problems ill-posed and creating inherent uncertainties. To account for such uncertainties and factors involved in haze degradation, we introduce a variational Bayesian framework for single image dehazing. We propose to take not only a clean image and but also transmission map as latent variables, the posterior distributions of which are parameterized by corresponding neural networks: dehazing and transmission networks, respectively. Based on a physical model for haze degradation, our variational Bayesian framework leads to a new objective function that encourages the cooperation between them, facilitating the joint training of and thereby boosting the performance of each other. In our framework, a dehazing network can estimate a clean image independently of a transmission map estimation during inference, introducing no overhead. Furthermore, our model-agnostic framework can be seamlessly incorporated with other existing dehazing networks, greatly enhancing the performance consistently across datasets and models.", "sections": [{"title": "1 Introduction", "content": "Haze is an atmospheric phenomenon, where airborne particles (e.g., fog, dust, etc.) between the scene and an observer obscure the scene. Such phenomenon causes poor visibility and thereby severely affects the performance of high-level vision tasks, such as semantic segmentation and object detection. The extent of haze effects is determined by how far the scene is and the amount of airborne particles that either attenuate the visibility of the scene or scatter global atmospheric light towards an observer. As such, an atmospheric scattering model [32, 33] formulates haze effects as:\n\n$I=Jt+A \\cdot (1-t),$\n\nwhere $I$ and $J$ are an observed hazy image and a scene radiance (i.e.clean haze-free image), and $\\cdot$ indicates the pixel-wise multiplication. The scalar $A$ denotes global atmospheric light, and $t$ is the transmission map representing the remaining fraction of light that reaches an observer from the scene. In general, $t$ and $A$ are unknown, and thus recovering the clean image $J$ from a given hazy image $I$ is a highly ill-posed and challenging problem.\nBased on this physical haze model, early works have imposed constraints with strong assumptions or priors (e.g., hazy regions have higher intensity values than haze-free regions [42] or haze-free regions have at least one color channel with low intensity [16]). Due to such strong priors, these prior-based methods fail to work under scenarios where assumptions do not hold, resulting in poor generalization. To alleviate this, recent data-driven approaches rely on large-scale datasets and the representation power of neural networks to recover clean haze-free images by learning to estimate transmissions [3, 37] or directly learning a mapping of haze-free images [10, 26, 28, 38, 48] or jointly estimate both from hazy images [47, 49]. However, there are inherent ambiguities and uncertainties (e.g., airlight-albedo ambiguity: we cannot tell how much light is from scene radiance or atmospheric light [11]), causing inaccurate estimation of transmission map or haze-free images."}, {"title": "2 Related Work", "content": "In general, the haze effect is dependent on depth (i.e., a deep scene or a distant object produce minimal transmission, therefore resulting in a substantially hazy image). The main challenge of the dehazing task lies in effectively extracting information on atmospheric light and transmission map merely from the given hazy image. Most of existing dehazing strategies can be categorized into two approaches: prior and learning-based methods.\nPrior-based Methods. Early dehazing algorithms mostly depend on Eq. (1) and statistical prior to impose constraints on the solution space. Fattal et al. [11] assumed that shading of the object and transmission are statistically uncorrelated over the entire image patch to estimate the transmission map and albedo of the medium. Tan et al. [42] proposed to compute transmission map using Markov random field, utilizing the prior that haze-free regions have higher contrast than hazy ones. He et al. [16] proposed dark channel prior to estimate transmission maps and atmospheric light, based on the observation that the lowest intensity among the color channels of natural outdoor images is close to zero due to factors, such as shadow or color patterns. Zhu et al. [52] proposed a linear model that restores depth maps with assumed color attenuation prior which describes the relationship between the pixel intensity, saturation, and their differences. Berman et al. [2] introduced a non-local method with haze-line prior, which assumes that a few hundred distinct colors can successfully approximate the color of haze-free regions, forming compact clusters in RGB space. These methods often fail to work due to strong priors and assumptions.\nLearning-based Methods. As deep learning technology and large scale open source datasets become increasingly procurable, data-driven learning-based methods have become prevalent [5, 18, 36, 44, 45, 51]. In contrast to prior-based methods, these learning-based methods learn to map hazy images to haze-free images directly in an end-to-end manner. Cai et al. [3] proposed DehazeNet, an end to end modeling with CNN, and Ren et al. [37] introduced, a multi-scale neural network (MSCNN), both successfully estimate transmission maps. Li et al. [21] introduced a new variable dependent on hazy input by using the atmospheric scattering model in Eq. (1), enabled to reconstruct latent clean image by predicting the variable. Zhang et al. [47] proposed an edge-preserving loss, multi-level architectures, and introduced a discriminator to estimate transmission map and atmospheric light mutually. Ren et al. [38] proposed several pre-processing steps and multi-scale-fusion-based network to learn confidence maps for improved global visibility. Dong et al. [7] incorporated generic recursive boosting algorithm in the dense feature fusion model for information preservation and performance improvement. Guo et al. [15] added geometrical information to a transformer module and concatenated with a CNN module to increase the local and global connectivity.\nVariational Bayesian Modeling. Bayesian modeling allows for the modeling of uncertainties and latent variables that may not be readily apparent from the observed data. While variational inference is a potent tool for approximating intricate probability distributions, its practical application requires careful consideration of prior knowledge. For instance, to tackle denoising problem, Yue et al. [46] model noise-free image and its variance as latent variables utilizing a conjugate prior. Wang et al. [43] leverage Dirichlet distribution to model blur kernel and deblurred image as latent variables and introduced two inference structures that are independent and dependent on the estimated blur kernel under blur process. In this work,"}, {"title": "3 Variational Haze Removal Framework", "content": "Let $\\mathcal{D}$ be a training dataset composed of $n$ triplets $(x, y, t)$ of hazy image $y \\in \\mathbb{R}^{h \\times w \\times 3}$, ground truth clean image $x \\in \\mathbb{R}^{h \\times w \\times 3}$, and transmission map $t \\in \\mathbb{R}^{h \\times w \\times 1}$, respectively. $h$ and $w$ are the height and width of an image in RGB space, respectively. Moreover, we denote latent haze-free image, latent transmission, and atmospheric light as $z$, $\\tau$, and $A$. We consider the clean image and transmission map as latent variables, and proceed to calculate their posterior distribution based on haze degradation. This work aims to construct a variational function approximation of the posterior given a single hazy image through the Bayesian model including likelihood and priors. Learning the joint distribution of our latent variables can further escalate the performance of the conventional dehazing networks. The details are elaborated as in the following."}, {"title": "3.1 Bayesian Model Construction", "content": "Likelihood Model. Based on the atmospheric scattering model in Eq. (1), we start with taking intensity value of a hazy image as latent, which we assume to follow a Gaussian distribution as:\n\n$y_i \\sim \\mathcal{N}(z_i t_i + A(1 - \\tau_i), \\sigma^2),$\n\nwhere $y_i$, $z_i$, and $t_i$ denote pixel values of a hazy image, haze-free image, and transmission map at a pixel location $i$ respectively. Moreover, $\\mathcal{N}(\\mu, \\sigma^2)$ is the Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$. For the sake of analytical feasibility and the basic properties of the Gaussian distribution that facilitate the parameterization of latent variables, we model $y_i$ as a Gaussian distribution [31]. Since our training dataset $\\mathcal{D}$ includes the ground truth haze-free image and transmission map, we can further take $z$ and $t$ as latent and train neural networks to estimate their posteriors.\nHaze-free Image. In general, optimizing the $L_1$ loss function encourages the median estimation of the observations rather than mean as with $L_2$ loss [17, 41]. Therefore, conventional dehazing networks favor $L_1$ loss variants to $L_2$ loss [15, 24, 35, 40] and minimize the absolute difference between the ground truth clean and predicted dehazed images during training to produce sharper edges/boundaries while suppressing the noise in homogeneous regions. If regression model errors are assumed to follow a Laplace distribution, then maximum likelihood estimates of the distribution parameters correspond to $L_1$ regression estimates [30]. Accordingly, we model the haze-free image under data-driven Laplace prior as:\n\n$z_i \\sim \\text{Laplace}(x_i, \\epsilon_1),$\n\nwhere $x_i$ is a pixel value of the ground truth clean image at $i$, $\\text{Laplace}(\\eta, \\delta^2)$ denotes the Laplace distribution with parameters of location $\\eta$ and scale $\\delta^2$. $\\epsilon_1$ is the mean absolute deviation from the median of $z_i$. Given that $x_i$ serves as a reliable prior for $z_i$, we set a small value to $\\epsilon_1$.\nTransmission Map. In this work, we assume that the atmosphere is homogeneous in the scene as in previous arts [6, 33]. Under this assumption, the scene radiance is exponentially attenuated [42] and the transmission map can be formulated with scattering coefficient $\\beta$ and depth map $d$ [6, 34] as:\n\n$t = e^{-\\beta \\cdot d}$\n\nFor reason similar to Eq. (2), we can model the probability of scene depth as a normal distribution. When the logarithm of a variable is normally distributed, then the variable has log-normal distribution. In addition, as in haze-free image modeling, a large number of transmission maps $t$ from the training data provide a strong data-driven prior to our latent transmission map $\\tau$. Therefore, we model the latent transmission map at pixel location $i$ as follows:\n\n$\\tau_i \\sim \\text{Lognormal}(-\\beta d_i, \\epsilon_2),$"}, {"title": "3.2 Variational Formulation of Posterior", "content": "We aim to infer the posterior of the latent variables by merging the Bayesian models in Eqs. (2-5). The direct estimation of the true posterior of the latent variables $z$ and $t$ solely from a single hazy image $y$ (i.e., $p(z, t|y)$) is computationally infeasible. Therefore, we construct a variational surrogate distribution $q(z, t|y)$ to approximate $p(z, t|y)$. Following the mean field assumption, we partition the variables into independent parts (i.e., assume the conditional independence between two variables $z$, and $\\tau$):\n\n$q(z, \\tau|y) = q(z|y)q(\\tau|y).$\n\nUsing Eqs. (3) and (5) with an assumption that surrogate distribution $q(z|y)$ and $q(\\tau|y)$ have scale parameter $\\epsilon_\\theta$ and scatter parameter $\\epsilon_\\psi$ respectively, we formulate the variational posterior as:\n\n$q(z|y) = \\prod_i \\text{Laplace}(\\phi_\\theta(y)_i, \\epsilon_\\theta),$\n$q(\\tau|y) = \\prod_i \\text{Lognormal}(\\log \\nu_\\psi(y)_i, \\epsilon_\\psi),$\n\nwhere $\\phi_\\theta(\\cdot)$, and $\\nu_\\psi(\\cdot)$ are neural networks that are trained to estimate the posterior distribution parameters of latent variable $z$, and $\\tau$, conditioned on the input hazy image $y$. Since our proposed framework focuses on modeling the posterior of latent variables, without any assumption on the form of $\\phi_\\theta(\\cdot)$, and $\\nu_\\psi(\\cdot)$, our framework is model-agnostic. In particular, $\\phi_\\theta$, which we call D-Net, can be any existing dehazing networks (e.g., [4]), which is a neural network with parameters $\\theta$ trained to estimate the haze-free image. Similarly, $\\nu_\\psi$, named as T-Net, is an auxiliary neural network with parameters $\\psi$ to estimate the transmission map from a given input hazy image. Note that there is no additional overhead during inference as we can estimate a haze-free image with only D-Net, and the transmission map is optionally obtainable as shown in Figure 2."}, {"title": "3.3 Variational Lower Bound", "content": "With the functional parameterization of the variational posterior, we can optimize the trainable parameters $\\theta$, and $\\psi$ to maximize the posterior probability. To do so, we decompose the marginal log-likelihood and obtain the variational lower bound. For notational simplicity, we use $\\phi_i$ and $\\nu_i$ rather than $\\phi_\\theta(y)_i$ and $\\nu_\\psi(y)_i$, then the marginal log-likelihood is given by,\n\n$\\log p(y; z, \\tau) = \\mathbb{E}_{q(z,\\tau|y)}[\\log p(y)]$\n$= \\mathbb{E}_{q(z,\\tau|y)} \\left[ \\log \\frac{p(y, z, \\tau)}{p(z, \\tau|y)} \\right]$\n$= \\iint q(z, \\tau|y) \\log \\frac{p(y, z, \\tau)}{q(z, \\tau|y)} dz d\\tau$\n$= \\iint q(z, \\tau|y) \\log \\frac{p(y|z, t)p(z)p(\\tau)}{q(z, \\tau|y)} dz d\\tau$\n$= \\iint q(z, \\tau|y) \\log \\frac{p(y|z, t)p(z)p(\\tau)}{q(z|y)q(\\tau|y)} dz d\\tau$\n$= \\mathbb{E}_{q(z,\\tau|y)}[\\log p(y|z, t)] - \\mathbb{E}_{q(z,\\tau|y)} \\left[ \\log \\frac{p(z)}{q(z|y)} \\right] - \\mathbb{E}_{q(z,\\tau|y)} \\left[ \\log \\frac{p(\\tau)}{q(\\tau|y)} \\right]$\n$= \\mathbb{E}_{q(z,\\tau|y)}[\\log p(y|z, t)] + KL(q(z|y)||p(z)) + KL(q(\\tau|y)||p(\\tau))$\n$= \\mathcal{L}(y; \\phi_\\theta, \\nu_\\psi) + KL(q(z, \\tau|y)||p(z, \\tau|y)).$\n\nwhere $KL(||)$ computes the Kullback-Leibler (KL) divergence of two distributions, and $\\mathcal{L}(y; \\phi_\\theta, \\nu_\\psi)$ is the variational lower bound which can be combined with Eqs. (7) and (8) as follows:\n\n$\\mathcal{L}(y; \\phi_\\theta, \\nu_\\psi) = \\mathbb{E}_{q(z,\\tau|y)} \\left[ \\log \\frac{p(y|z, t)p(z)p(\\tau)}{q(z, \\tau|y)} \\right]$\n$= \\mathbb{E}_{q(z,\\tau|y)}[\\log p(y|z, t)] - \\mathbb{E}_{q(z,\\tau|y)} \\left[ \\log \\frac{p(z)p(\\tau)}{q(z|y)q(\\tau|y)} \\right]$\n$= \\mathbb{E}_{q(z,\\tau|y)}[\\log p(y|z, t)] - KL (q(z|y)||p(z)) - KL (q(\\tau|y)||p(\\tau)).$\n\nand each term in Eq. (10) can be calculated analytically as follows:\n\n$\\mathbb{E}_{q(z,t|y)} [\\log p(y|z, t)]$\n$= - \\frac{h w}{2} \\log 2\\pi\\sigma^2 - \\sum_{i=1}^{h w} \\frac{(y_i - (\\phi_i\\nu_i + A(1 - \\nu_i)))^2}{2 \\sigma^2}.$\n\nand\n\n$KL(q(z|y)||p(z))$\n$= \\frac{h w}{\\epsilon_\\theta} \\left[ \\exp \\left( \\frac{|\\phi_i - x_i|}{\\epsilon_\\theta} \\right) + \\frac{\\phi_i - x_i}{\\epsilon_\\theta} \\right]$\n\nand\n\n$KL(q(\\tau|y)||p(\\tau)) = \\sum_{i=1}^{h w} \\left( \\log \\frac{\\nu_i}{t_i} + \\frac{t_i}{\\nu_i} - 1 \\right).$\n\nNote that all terms in Eq. (10) are differentiable, and we can train the network parameters $\\theta$ and $\\psi$ over the given training dataset $\\mathcal{D}$ by"}, {"title": "3.4 Learning with Variational Lower Bound", "content": "By minimizing the final objective function in Eq. (14) through conventional back-propagation without using the reparameterization trick [20], we can train the parameters of networks $\\phi_\\theta$ and $\\nu_\\psi$ and estimate the posterior of latent variables $z$ and $t$ as illustrated in Figure 2. Notably, the roles of three terms composing the total objective can be explained as follows. The first term represents the likelihood of the observed hazy images and is responsible for encouraging cooperation between dehazing and transmission networks based on the Eq. (1), which describes the relationship between the three latent variables: hazy image, haze-free image, and transmission. The second (Eq. (12)) and the third (Eq. (13)) terms act as regularization, making the posterior distribution close to prior distribution. Therefore, two separate networks $\\phi_\\theta$ and $\\nu_\\psi$ can complement each other with the aid of the joint term, and they are simultaneously trained by simulating the physical haze degradation process.\nFurthermore, $\\sigma^2$, $\\epsilon_\\theta$ and $\\epsilon_\\psi$ can be interpreted as not only uncertainty of each variable but also the importance of the associated term. For instance, the importance of the KL divergence between $q(z|y)$ and $p(z)$ increases as $\\epsilon_\\theta$ approaches to zero."}, {"title": "4 Experimental Results", "content": "4.1 Experimental Setting\nDatasets. We conducted our experiments on both synthetic and real-world datasets. We utilize the RESIDE [22] and Haze4K [27] as synthetic datasets, and the NH-Haze [1] and Fattal evaluation set [12] for real-world datasets. The RESIDE benchmark comprises synthetic hazy images along with their corresponding clean images captured in both indoor and outdoor scenarios. The Synthetic Objective Test Set (SOTS) is used to evaluate the performance of the models on RESIDE dataset. The indoor training set (ITS) of RESIDE benchmark consists of 13990 generated hazy images from 1399 clean images. The outdoor training set (OTS) of RESIDE benchmark includes a total of 313950 hazy images generated by using the collected real outdoor images. Haze4K is constructed by generating 4000 hazy images with randomly sampled atmospheric light $A$ and scattering coefficient $\\beta$ from 500 clean indoor images in NYU-Depth [39] and 500 outdoor images in OTS. NH-Haze contains 55 paired images of real-world haze scenes.\nImplementation. For our D-Net, we can employ any conventional dehazing networks, and we use GCANet [4], FFA-Net [35], and a recent state-of-the-art network DehazeFormer-B [40] as our baseline dehazing networks. For our T-Net, we use GCANet with a clamping activation function on the output layer. For fair comparison, we follow all training and evaluation strategies of the baselines (e.g., total epoch, optimizer, etc.) and our Bayesian framework is implemented based on the officially available code of each baseline.\nIn the case of real-world NH-Haze dataset, total training epoch is set to 300 using the official train-test split. As the NH-Haze train set lacks the ground truth transmission map, we estimated the map using a clean and hazy image pair while assuming $A = 1$ from Eq. (1) (i.e., $t = (I\u2013A)/(J\u2013A+\\epsilon) \\in \\mathbb{R}^{h \\times w \\times 3}$ with $\\epsilon = 10^{-6}$ for numerical stability)."}, {"title": "4.2 Performance Evaluation", "content": "To evaluate the performance of the proposed Bayesian framework, we compare the dehazing results with and without using the proposed framework both on synthetic and real-world haze datasets.\nResults on Synthetic Datasets. Table 1 presents the dehazing results on three different datasets (SOTS-Indoor, SOTS-Outdoor, and Haze4K), comparing with DCP [16], BCCR [29] CAP [52], NLD [2], DehazeNet [3], AOD-Net [21], MSBDN [7], and DeHamer [15]. Notably, for our baseline GCANet [4], FFA-Net [35], and DehazeFormer-B [40], we provide two sets of PSNR and SSIM values: the scores reported in their original manuscripts and reproduced numbers in our experiments which are within the parentheses. As shown, our proposed method integrated with DehazeFormer-B obtains the highest metric scores in every domain, except for the PSNR on Haze4K, where FFA-Net + Ours performs the best.\nResults on Real-world Datasets. In Table 2, the evaluation results in terms of PSNR, SSIM, and LPIPS obtained from the NH-Haze test set [1] are provided. We compare with DCP, CAP, MSBDN, and our baselines. Our proposed method, when combined with FFA-Net, achieved the best performance in every metric. Notably, we observed an improvement in performance of over 0.28 dB in PSNR and 0.021 in SSIM on average.\nUser Study Results. Due to the lack of ground truth clean images or object annotations in the Fattal evaluation set [12], as well as the need for further evaluation of the qualitative aspects, we conducted a user study. The details are described as follows. First, we randomly selected six images each from SOTS-indoor, SOTS-outdoor [22], Haze4K test set [27], and Fattal evaluation set [12]. For each hazy image, we randomly chose a pair of dehazing results from one of the three base models (GCANet [4], FFA-Net [35], and DehazeFormer-B [40]) and corresponding enhanced model by our approach."}, {"title": "4.3 Object Detection Application", "content": "We further assess the quality of the estimated haze-free images by evaluating how much the haze removal improves a downstream task: object detection in this work. We perform experiments with YOLOv5 [19] as an object detector on KITTI Haze dataset [7], which is synthesized based on KITTI detection dataset [13], following the dataset generation algorithm of RESIDE dataset [22] with depth estimation method [14]. The quality of dehazed images is evaluated with how much detection accuracy improves in terms of mean average precision."}, {"title": "4.4 Ablation Study", "content": "Joint optimization and Bayesian modeling. we performed ablation study in order to validate the effectiveness of our Bayesian modeling, we compared our framework with the slghtlty modified version of the joint training framework proposed by Zhang et al. [49]. Specifically, the estimated joint transmission map is concatenated to the input of the dehazing module, facilitating the joint training of the dehazing and transmission estimation module as illustrated in Figure 7. Note that for a fair comparison, we have excluded the adversarial network, adversarial loss, and perceptual loss, and employed GCANet [4] for both modules.\nTo verify the effectiveness of the joint optimization strategy and our Bayesian modeling, we compare these with the modified version of joint training framework introduced in Zhang et al. [49]. Specifically, the estimated transmission map is concatenated to the input of the dehazing module to jointly train the dehazing and transmission estimation networks. We utilize GCANet for dehazing and transmission estimation module for a fair comparison and train with the ground truth clean image and transmission map using the L2 objective. This configuration (GCANet + Modified [49]) allows joint training, but does not take into account the haze degradation process. From the comparison result, it is observed that our final model (GCANet + Ours) using the joint optimization with our Bayesian framework outperforms the best. We analyze that the likelihood term (i.e., Eq. (11)) in the proposed objective is responsible for leveraging the relationships between latent variables and uncertainty, resulting in our method's superior performance in comparison with the modified joint method, which lacks this term. In addition, joint optimization helps to utilize transmission information, thus both contribute to performance gain.\nPrior Distribution. We further justify our choice of prior distributions for $z$ and $t$ by performing ablation study on the prior distributions. As reported in Table 5, we perform ablations based on"}, {"title": "4.5 Discussion", "content": "Role of T-Net. T-Net serves as an auxiliary branch in our framework, designed to estimate the scale parameter of the Lognormal distribution modeling the latent variable $\\tau$, which is not utilized in the inference phase. However, we can further inspect the reason behind the output of D-Net by examining the output of T-Net. For instance, the dehazing network may prioritize the restoration of more vivid colors in areas where lower transmission is estimated.\nInfluence of Model Capacities. We studied how the size of each module of our framework could affect the other by conducting experiments on Haze4K [27] dataset. We estimate the impact of the T-Net on the performance of D-Net by evaluating the ones we used for benchmarking Haze4K dataset. Note that the architecture for T-Net is fixed in this experiment. On the other hand, to evaluate the impact of D-Net performance on the T-Net structure, we modified the number of filters in the hidden layer of GCANet. Specifically, we fixed the architecture of D-Net as GCANet and employed another GCANet as T-Net with 48 and 96 filters per layer, respectively, while the original structure has 64 filters per layer. The estimated transmission maps and their evaluation metrics (mean squared error (MSE) and SSIM) on the Haze4K test set are presented in Figure 8 and Table 6, respectively. Although the same network architecture is used for T-Net, improving D-Net architecture from a simple one (e.g., GCANet) to advanced ones (FFA-Net, DehazeFormer-B) tends to improve the transmission accuracy. Likewise, we observe that the performance of D-Net improves as the capacity of T-Net increased as reported in Table 7. In other words, T-Net improves as D-Net architecture improves, owing to our proposed framework that allows cooperation between two branches. Thus, improving D-Net better facilitates the training of T-Net or vice versa.\nWe believe this is because D-Net and T-Net are complementary in that they are jointly trained to minimize the objective function. Specifically, more accurate estimation of either z or t results in a lower loss value, allowing for more accurate gradient computation. Thus, transmission estimation module can improve with dehazing module.\nThe results also allude to the efficacy of our joint optimization of clean haze-free image and transmission map, which are related by our proposed Bayesian framework and objective."}, {"title": "5 Conclusion", "content": "This work is founded on the motivation that there are inherent uncertainties that make the single image dehazing problem challenging. To alleviate this problem, we propose to formulate a variational Bayesian framework for single image dehazing. Incorporating the atmospheric scattering model, we handle uncertainties involved in estimating transmission and haze-free images. In particular, we take transmission and haze-free images as latent variables and use neural networks to parameterize the approximate posterior distribution of these joint latent variables. Our framework provides consistent performance improvement across various models and numerous datasets."}]}