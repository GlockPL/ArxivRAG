{"title": "TLDR: Unsupervised Goal-Conditioned RL via\nTemporal Distance-Aware Representations", "authors": ["Junik Bae", "Kwanyoung Park", "Youngwoon Lee"], "abstract": "Unsupervised goal-conditioned reinforcement learning (GCRL) is a\npromising paradigm for developing diverse robotic skills without external super-\nvision. However, existing unsupervised GCRL methods often struggle to cover\na wide range of states in complex environments due to their limited exploration\nand sparse or noisy rewards for GCRL. To overcome these challenges, we propose\na novel unsupervised GCRL method that leverages TemporaL Distance-aware\nRepresentations (TLDR). TLDR selects faraway goals to initiate exploration and\ncomputes intrinsic exploration rewards and goal-reaching rewards, based on tem-\nporal distance. Specifically, our exploration policy seeks states with large temporal\ndistances (i.e. covering a large state space), while the goal-conditioned policy\nlearns to minimize the temporal distance to the goal (i.e. reaching the goal). Our\nexperimental results in six simulated robotic locomotion environments demonstrate\nthat our method significantly outperforms previous unsupervised GCRL methods\nin achieving a wide variety of states.", "sections": [{"title": "1 Introduction", "content": "Human babies can autonomously learn goal-reaching skills, starting from controlling their own\nbodies and gradually improving their capabilities to achieve more challenging goals, involving\nlonger-horizon behaviors. Similarly, for intelligent agents like robots, the ability to reach a large set\nof states-including both the environment states and agent states-is crucial. This capability not only\nserves as a foundational skill set by itself but also enables achieving more complex tasks.\nCan robots autonomously learn such long-horizon\ngoal-reaching skills like humans? This is partic-\nularly compelling as learning goal-reaching be-\nhaviors in robots is task-agnostic and does not\nrequire any external supervision, offering a scal-\nable approach for unsupervised pre-training of\nrobots [3, 4, 5, 6, 7, 8, 9]. However, prior unsu-\npervised goal-conditioned reinforcement learning\n(GCRL) [10, 2] and unsupervised skill discovery [1]\nmethods exhibit limited coverage of reachable states\nin complex environments.\nThe major challenges in unsupervised GCRL are\ntwofold: (1) exploring diverse states to ensure the\nagent can learn to achieve a wide variety of goals,\nand (2) effectively learning a goal-reaching policy. Previous methods focus on exploring novel\nstates [11] or states with high uncertainty in next state prediction [10, 2]. However, these methods aim\nto discover unseen states or state transitions, which may not be meaningful. Additionally, training a"}, {"title": "2 Related Work", "content": "Unsupervised goal-conditioned reinforcement learning (GCRL) aims to learn a goal-conditioned\npolicy that can reach diverse goal states without external supervision [15, 16, 10, 2]. The major\nchallenges of unsupervised GCRL can be summarized in two aspects: (1) optimizing goal-conditioned\npolicies and (2) collecting trajectories with novel goals that effectively enlarge its state coverage.\nRecent techniques such as hindsight experience reply (HER) [8] and model-based policy optimiza-\ntion [10, 12] have improved the efficiency of GCRL. However, learning complex, long-horizon\ngoal-reaching behaviors remains difficult due to sparse (e.g. whether it reaches the goal) [8] or heuris-\ntic rewards (e.g. cosine similarity between the state and goal) [10, 12]. Instead, temporal distance,\ndefined as the number of environment steps between states estimated from data, can provide more\ndense and grounded rewards [17, 10, 18]. Nonetheless, this often leads to sub-optimal goal-reaching\nbehaviors since it does not reflect the \"shortest temporal distance\u201d between states. In this paper,\nwe propose to use the estimated shortest temporal distance as reward signals for GCRL, inspired by\nQRL [14] and HILP [13]. We apply the learned representations to compute goal-reaching rewards\nrather than directly learning the value function in QRL or using it for skill-learning rewards in HILP.\nExploration in unsupervised GCRL relies heavily on selecting exploratory goals that lead to novel\nstates and expand state coverage. Various strategies for exploratory goal selection have been intro-\nduced, including selecting less visited states [19], states with low-density in state distributions [20, 11],\nand states with high uncertainty in dynamics [10, 2]. Instead of sampling uncertain or less visited\nstates as goals, we select goals that are temporally distant from the visited state distribution, encour-\naging coverage of broader state spaces which require more environment steps to reach.\nUnsupervised skill discovery [21, 22, 23, 24, 25, 26, 27, 1] is another approach to learning diverse\nbehaviors without supervision, yet often lacks robust exploration capabilities [27], requiring manual\nfeature engineering or limiting to low-dimensional state spaces. METRA [1] addresses these limita-\ntions by computing skill-learning rewards with temporal distance-aware representations, though it\nexhibits limited coverage in complex environments.\nTemporal distance-aware representations have been extensively used in imitation learning [28],\nrepresentation learning [29, 30], unsupervised skill discovery [1], offline skill learning [13], and\nGCRL [14]. Methods like QRL [14], HILP [13], and METRA [1] are closely related to our work\nas they learn temporal distance-preserving representations or goal-reaching value functions. HILP\nand METRA use temporal distance-aware representations for skill representations and skill rewards.\nOn the other hand, QRL learns a quasimetric model as a (negated) goal-reaching value function. In\ncontrast, our method uses temporal distance-aware representations across the entire unsupervised\nGCRL pipeline."}, {"title": "3 Approach", "content": "In this paper, we introduce TemporaL Distance-aware Representations (TLDR), an unsupervised\ngoal-conditioned reinforcement learning (GCRL) method, integrating temporal distance within unsu-\npervised GCRL. As illustrated in Figure 2, TLDR integrates temporal distance-aware representations\n(Section 3.2) into every facet of the Go-Explore [19] strategy (Section 3.3), which chooses a goal\nfrom experience (Section 3.4), reaches the selected goal via the goal-conditioned policy, and executes\nthe exploration policy to gather diverse experiences. We then refine both the exploration policy\n(Section 3.5) and goal-conditioned policy (Section 3.6) based on the collected data and rewards\ncomputed using the temporal distance-aware representations. We describe the full algorithm in\nAlgorithm 1. Please refer to Appendix A for further implementation details."}, {"title": "3.1 Problem Formulation", "content": "We formulate the unsupervised GCRL problem with a goal-conditioned Markov decision process,\ndefined as the tuple M = (S, A, p, G). S and A denote the state and action spaces, respectively.\np : S \u00d7 A \u2192 \u25b3(S) denotes the transition dynamics, where \u2206(X) denotes the set of probability\ndistributions over X. The goal of the agent is to learn an optimal goal-conditioned policy \u03c0G :\nS \u00d7 G \u2192 A, where \u03c0(a | s, g) outputs an action a \u2208 A that can navigate to the goal g \u2208 Gas fast\nas possible from the current state s. In this paper, we set G = S, allowing any state as a potential\ngoal for the agent."}, {"title": "3.2 Learning Temporal Distance-Aware Representations", "content": "Temporal distance, defined as the minimum number of environment steps between states, can provide\nmore dense and grounded rewards for goal-conditioned policy learning as well as exploration. For\nGCRL, instead of relying on sparse and binary goal-reaching rewards, the change in temporal distance\nbefore and after taking an action can be an informative learning signal. Moreover, exploration in\nunsupervised GCRL can be incentivized by discovering temporally faraway states. Therefore, in this\npaper, we propose to use temporal distance for unsupervised GCRL."}, {"title": "3.3 Unsupervised GCRL with Temporal Distance-Aware Representations", "content": "With temporal distance-aware representations, we can integrate the concept of temporal distance\ninto unsupervised GCRL. Our approach is built upon the Go-Explore procedure [19], a widely-\nused unsupervised GCRL algorithm comprising two phases: (1) the \u201cGo-phase,\u201d where the goal-\nconditioned policy \\(\u03c0_\\theta^G\\) navigates toward a goal g, and (2) the \u201cExplore-phase,\u201d where the exploration\npolicy \\(\u03c0_\\phi^E\\) gathers new state trajectories to refine the goal-conditioned policy.\nWhile Go-Explore relies on task-specific information for goal selection and exploration policy\ntraining, our method uses task-agnostic temporal distance metrics induced by temporal distance-\naware representations. The subsequent sections detail how our method leverages the representation\nfor selecting goals in the Go-phase (Section 3.4), enhancing the exploration policy (Section 3.5), and\nfacilitating the GCRL policy training (Section 3.6)."}, {"title": "3.4 Exploratory Goal Selection", "content": "For unsupervised GCRL, selecting low-density (less visited) states as exploratory goals can enhance\ngoal-directed exploration [15, 16]. However, the concept of \"density\u201d of a state does not necessarily\nindicate how rare or hard to reach the state. For example, while a robotic arm might actively seek out\nunseen (low-density) joint positions, interacting with objects could offer more significant learning\nopportunities [27]. Thus, we propose selecting goals that are temporally distant from states that are\nalready visited (i.e. in the replay buffer) to explore not only diverse but also hard-to-reach states."}, {"title": "3.5 Learning Exploration Policy", "content": "After the goal-conditioned policy navigates towards the chosen goal g for TG steps, the exploration\npolicy \\(\u03c0_\\phi^E\\) is executed to discover states even more distant from the visited states. This objective of\nthe exploration policy can be simply defined as:\n\\(r^E (s, s') = r_{TLDR} (s') \u2013 r_{TLDR}(s).\\)\nSimilar to LEXA [10], we alternate between goal-reaching episodes and exploration episodes. For\ngoal-reaching episodes, we execute the goal-conditioned policy until the end of the episodes. For\nexploration episodes, we sample the timestep \\(T_G \\sim Unif(0, T \u2212 1)\\) at the beginning of each episode\nand execute the exploration policy if timestep t > TG."}, {"title": "3.6 Learning Goal-Conditioned Policy", "content": "The goal-conditioned policy aims to minimize the distance to the goal. However, defining \u201cdistance\u201d\nto the goal often requires domain knowledge. Instead, we propose leveraging a task-agnostic metric,\ntemporal distance, as the learning signal for the goal-conditioned policy:\n\\(r(s, s', g) = ||\u03c6(s) - \u03c6(g) || - ||\u03c6(s') \u2013 \u03c6(g)||.\\)\nIf our representations accurately capture temporal distances between states, optimizing this reward in\na greedy manner becomes sufficient for learning an optimal goal-reaching policy."}, {"title": "4 Experiments", "content": "In this paper, we propose TLDR, a novel unsupervised GCRL method that utilizes temporal distance-\naware representations for both exploration and optimizing a goal-conditioned policy. Through our\nexperiments, we aim to answer the following 3 questions: (1) Does TLDR explore better compared to\nother exploration methods? (2) Is our goal-conditioned policy better than prior unsupervised GCRL\nmethods? (3) How crucial is TLDR for goal-conditioned policy learning and exploration?"}, {"title": "4.1 Experimental Setup", "content": "Tasks. We evaluate our method in 6 state-based environments and 2 pixel-based environments, as\nillustrated in Figure 3. For state-based environments, we use Ant and HalfCheetah from OpenAI\nGym [31], Humanoid-Run and Quadruped-Escape from DeepMind Control Suite (DMC) [32],\nAntMaze-Large from D4RL [33], and AntMaze-Ultra [34]. For Humanoid-Run and Quadruped-\nEscape, we include the 3D coordinates of the agents in their observations. For pixel-based environ-\nments, we use Quadruped (Pixel) from METRA [1] and Kitchen (Pixel) from D4RL [33], with the\nimage size of 64 \u00d7 64 \u00d7 3 as the observation.\nComparisons. We compare our method with 6 prior unsupervised GCRL, skill discovery, and\nexploration methods. For state-based environments, we compare with METRA, PEG, APT, RND,\nand Disagreement. For pixel-based environments, we compare with METRA and LEXA.\n\u2022 METRA [1]: the state-of-the-art unsupervised skill discovery method which leverages temporal\ndistance-aware representations.\n\u2022 PEG [2]: the state-of-the-art unsupervised GCRL method which plans to obtain goals with\nmaximum exploration rewards.\n\u2022 LEXA [10]: uses world model to train an Achiever and Explorer policy.\n\u2022 APT [25]: maximizes the entropy reward estimated from the k-nearest neighbors in a minibatch.\n\u2022 RND [35]: uses the distillation loss of a network to a random target network as rewards.\n\u2022 Disagreement [36]: utilizes the disagreement among an ensemble of world models as rewards.\nEvaluation setups. Following METRA [1] and PEG [2], we evaluate unsupervised exploration\nusing state coverage or queue state coverage, and evaluate goal-reaching performance using goal"}, {"title": "4.2 Quantitative Results", "content": "In Figure 4, we compare the state coverage during training. TLDR outperforms all prior works,\nexcept in HalfCheetah compared to METRA. METRA learns low-dimensional skills and extends\nthe temporal distance along a few directions specified by the skills, providing a strong inductive bias\nfor simple locomotion tasks like HalfCheetah. On the other hand, TLDR achieves much larger state\ncoverage in complex environments than METRA, including AntMaze-Large, AntMaze-Ultra, and\nQuadruped-Escape, where all other methods struggle and only explore limited regions. This shows\nthe strength of our method in the exploration of complex environments.\nWe then compare the goal-reaching performance of our method with PEG and METRA in Figure 5.\nWe first report the average distance between goals and the last states of trajectories. The results\nin Figures 5a to 5c show that TLDR can navigate towards the given goals closer than, or at least\non par with METRA. For the AntMaze environments, we report the number of pre-defined goals\nreached by the goal-conditioned policy. Figures 5d and 5e show that TLDR is the only method that\ncan navigate towards a various set of goals in both mazes, demonstrating its superior exploration and\ngoal-conditioned policy learning with temporal distance.\nFigure 6 shows the results in pixel-based environments. In Quadruped (Pixel), TLDR explores diverse\nregions but learns slower than LEXA and METRA. For Kitchen (Pixel), TLDR interacts with all six\nobjects during training, but struggles at learning the goal-conditioned policy. We hypothesize that\nlearning a temporal abstraction is more challenging with pixel observations, which may lead & to\nencode erroneous temporal information. We leave more detailed analyses for future works."}, {"title": "4.3 Qualitative Results", "content": "We visualize the learned goal-reaching behaviors on\nthe AntMaze-Ultra environment in Figure 7. TLDR\ncan successfully reach both near and faraway goals\nin diverse regions. On the other hand, METRA\nand PEG fail to navigate to diverse goals. METRA\ncould reach some goals distant from the initial posi-\ntion, whereas PEG fails to reach temporally faraway\ngoals. This clearly shows the benefit of using tem-\nporal distance in unsupervised GCRL."}, {"title": "4.4 Ablation Studies", "content": "To investigate the importance of temporal distance-aware representations in our algorithm, we conduct\nablation studies on exploration strategies and GCRL reward designs.\nExploration strategy. For goal selection and exploration rewards, we replace temporal distance,\n||\u03c6(s) \u2013 z(i) || in Equation (2), with other exploration bonuses: RND, APT (with ICM [37] representa-\ntions), and Disagreement. Note that goal-conditioned policies are still trained with the same temporal\ndistance-based rewards as TLDR, thereby comparing only exploration strategies. As shown in Fig-\nure 8a, using TLDR reward for goal selection and exploration rewards achieves significantly higher\nperformance than other exploration bonuses. This result implies that our temporal distance-based\nrewards are effective for unsupervised exploration.\nGCRL reward design. We compare with two goal-conditioned policy learning methods:\n(1) QRL [14], which uses a quasimetric value function, and (2) sparse HER [8], which uses the\nsparse goal-reaching reward -1(s\u2260 g). Figure 8b shows the superior performance of our temporal\ndistance-based GCRL reward. This highlights the importance of incorporating temporal distance-\naware representations in training goal-conditioned policies."}, {"title": "5 Conclusion", "content": "In this paper, we introduce TLDR, an unsupervised GCRL algorithm that incorporates temporal\ndistance-aware representations. TLDR leverages temporal distance for exploration and learning the\ngoal-reaching policy. By pursuing states with larger temporal distances, TLDR can continuously\nexplore challenging regions, achieving better state coverage. The experimental results demonstrate\nthat our method can cover significantly larger state spaces across diverse environments than existing\nunsupervised reinforcement learning algorithms."}, {"title": "5.1 Limitations", "content": "While TLDR achieves remarkable state coverage, it still has several limitations. Firstly, TLDR shows\na slower learning speed compared to METRA in pixel-based environments. Secondly, our temporal\ndistance-aware representations do not capture the asymmetric temporal distance between the states,\nwhich can make policy learning challenging for asymmetric environments. Finally, TLDR achieves\nhigh efficiency in terms of wall clock time, but with a relatively low update-to-data ratio (number\nof gradient steps divided by number of environment steps) of 1/32 in state-based experiments, as\nused in METRA. However, we believe that increasing the update-to-data ratio or using model-based\napproaches could potentially enhance sample efficiency."}, {"title": "A Training Details", "content": "A.1 Computing Resources and Experiments\nAll experiments are done on a single RTX 4090 GPU and 4 CPU cores. Each state-based experiment\ntakes 12 hours for all methods, following METRA [1], which trains each method for 9-10 hours. It\ncorresponds to the different environment steps used for different experiments, as described in Table 1.\nWe use 5 random seeds for all experiments, and report the mean and standard deviation of the results.\nA.2 Implementation Details\nOur method, TLDR, is implemented on top of the official implementation of METRA. Similar to\nMETRA, we use SAC [38] for learning the goal-reaching policy and exploration policy. We train our\ntemporal distance-aware representation \u03c6(s) by maximizing the following objective:\n\\(E_{s \\sim p_S, g \\sim p_G} [f(||\u03c6(s) - \u03c6(g)||) + \u03bb \u00b7 min(\u03b5, 1 \u2212 \\frac{||\u03c6(s) - \u03c6(s')||}{\u03b5})],\\)\nwhere we apply affine-transformed softplus f to Equation (1):\n\\(f(x) = -softplus (500 \u2013 x, \u03b2 = 0.01),\\)\nwhich alleviates the effect of too long distances ||\u03c6(s) - \u03c6(g)||, following QRL [14].\nFor training the exploration policy, we normalize the TLDR reward used in Equation (3) to keep the\nrewards on a consistent scale. We simply divide the TLDR reward by a running estimate of its mean\nvalue, following APT [25].\nFor METRA, PEG, and LEXA, we use their official implementation. For random exploration\napproaches (APT, RND, Disagreement), we use the implementation from URLB [39].\nA.3 Hyperparameters\nThe hyperparameters used in our experiments are summarized in Table 2.\nFor METRA, we use 2-D continuous skills for Ant, 16-D discrete skills for HalfCheetah, 24-D\ndiscrete skills for Kitchen (Pixel), and 4-D continuous skills for other environments. We use the\nbatch size of 1024 for state-based environments and 256 for pixel-based environments. We set the\nnumber of gradient steps for each experiment to be the same as ours. We use the default values for\nthe remaining hyperparameters. To perform goal-reaching tasks with METRA, we set the skill z as\n\\(\\frac{\u03c6(g)-\u03c6(s)}{||\u03c6(g)-\u03c6(s)||} \\) for continuous skills or \\(arg \\,\\max_{dim} (\u03c6(g) \u2013 \u03c6(s))\\) for discrete skills.\nIn PEG, we use the same hyperparameters used in their AntMaze experiments. Since PEG uses\nthe normalized goal space, we measure the range of the observations and normalize the goal states\naccording to the minimum and maximum range.\nIn LEXA, we follow their hyperparameters and opt for the temporal distance reward for training the\nAchiever policy.\nA.4 Environment Details\nAnt. We use the MuJoCo Ant environment in OpenAI gym [31]. The observation space is 29-D\nand the action space is 8-D. Following METRA, we normalize the observations for Ant with a fixed\nmean and standard deviation of observations computed from randomly generated trajectories. The\nepisode length is 200.\nHalfCheetah We use the MuJoCo HalfCheetah environment in OpenAI gym [31]. The observation\nspace is 18-D and the action space is 6-D. Following METRA, we normalize the observations for\nHalfCheetah with a fixed mean and standard deviation of observations from randomly generated\ntrajectories. The episode length is 200.\nHumanoid-Run. We use the Humanoid-Run task from DeepMind Control Suite [32]. The global\nx, y, z coordinates of the agent are added to the observation. Humanoid has 55-D observation space\nwith 21-D action space. The episode length is 200.\nQuadruped-Escape. Quadruped-Escape is included in DeepMind Control Suite [32]. The\nquadruped robot is initialized in a basin surrounded by complex terrains, as described in Figure 3d.\nDue to the complex terrains, moving further away from the initial position is challenging. Similar to\nthe AntMaze environments, we fix the terrain shape. Also, we add the global x, y, z coordinates of\nthe agent to the observation. Quadruped-Escape has 104-D observation space with 12-D action space.\nThe episode length is 200.\nAntMaze-Large. We use antmaze-large-play-v2 in D4RL [33]. The observation and action\nspaces are the same as the Ant environment. The episode length is 300. To make exploration more\nchallenging, we fix the initial location of the agent to be the bottom right corner of the maze, as\nshown in Figure 3e.\nAntMaze-Ultra. We use antmaze-ultra-play-v0 proposed by Jiang et al. [34]. The observation\nand action spaces are the same as the Ant environment. The episode length is 600. Similar to AntMaze-\nLarge, we fix the initial location of the agent to be the bottom right corner of the maze, as shown in\nFigure 3f.\nQuadruped (Pixel). We use the pixel-based version of the Quadruped environment [32] used in\nMETRA [1]. Specifically, we use the image size of 64 \u00d7 64 \u00d7 3 with 200 episode length.\nKitchen (Pixel). We use the pixel-based version of the Kitchen environment [42] used in ME-\nTRA [1] and LEXA [10]. Specifically, we use the image size of 64 \u00d7 64 \u00d7 3 with 50 episode length.\nThe action space has 9 dimensions.\nA.5 Evaluation Protocol\nFor Ant, Humanoid, and Quadruped (Pixel), we sample goals with (x, y)-coordinates from [-50, 50]2,\n[-40, 40]2, and [\u221215, 15]2, respectively. For the rest of the goal state (e.g. joint poses), we use the\ninitial robot configuration following Park et al. [1].\nFor HalfCheetah, we sample goals with x-coordinates from [-100, 100].\nFor AntMaze-Large and AntMaze-Ultra, we use the pre-defined goals as shown in Figure 7. A goal\nis deemed to be reached when an ant gets closer than 0.5 to the goal.\nFor Kitchen, we use the same 6 single-task goal images used in LEXA [10], which consist of\ninteractions with Kettle, Microwave, Light switch, Hinge cabinet, Slide cabinet, and Bottom burner.\nWe report the total number of achieved tasks during evaluation.\nFor all environments, we use a full state as a goal. Specifically, for state-based observations, we\nuse the observation upon reset as the base observation and switch the x, y coordinates (or x for\nHalfCheetah) to the corresponding dimensions. For Quadruped (Pixel), we render the image of the\nstate where the agent is at the goal position and use it as the goal.\nB More Ablation Studies\nWe conduct the ablation studies on the number of nearest neighbors k (Figure 9) and dim (s)\n(Figure 10) used in Equation (2). Figure 9 shows that different k affects exploration in Ant, but\nfor the other environments, the performance is not affected by the values of k. For dim (s), the\nperformance is nearly the same across different settings."}]}