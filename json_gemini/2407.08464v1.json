{"title": "TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware Representations", "authors": ["Junik Bae", "Kwanyoung Park", "Youngwoon Lee"], "abstract": "Unsupervised goal-conditioned reinforcement learning (GCRL) is a promising paradigm for developing diverse robotic skills without external supervision. However, existing unsupervised GCRL methods often struggle to cover a wide range of states in complex environments due to their limited exploration and sparse or noisy rewards for GCRL. To overcome these challenges, we propose a novel unsupervised GCRL method that leverages TemporaL Distance-aware Representations (TLDR). TLDR selects faraway goals to initiate exploration and computes intrinsic exploration rewards and goal-reaching rewards, based on temporal distance. Specifically, our exploration policy seeks states with large temporal distances (i.e. covering a large state space), while the goal-conditioned policy learns to minimize the temporal distance to the goal (i.e. reaching the goal). Our experimental results in six simulated robotic locomotion environments demonstrate that our method significantly outperforms previous unsupervised GCRL methods in achieving a wide variety of states.", "sections": [{"title": "1 Introduction", "content": "Human babies can autonomously learn goal-reaching skills, starting from controlling their own bodies and gradually improving their capabilities to achieve more challenging goals, involving longer-horizon behaviors. Similarly, for intelligent agents like robots, the ability to reach a large set of states-including both the environment states and agent states-is crucial. This capability not only serves as a foundational skill set by itself but also enables achieving more complex tasks.\nCan robots autonomously learn such long-horizon goal-reaching skills like humans? This is partic- ularly compelling as learning goal-reaching be- haviors in robots is task-agnostic and does not require any external supervision, offering a scal- able approach for unsupervised pre-training of robots [3, 4, 5, 6, 7, 8, 9]. However, prior unsu- pervised goal-conditioned reinforcement learning (GCRL) [10, 2] and unsupervised skill discovery [1] methods exhibit limited coverage of reachable states in complex environments, as shown in Figure 1.\nThe major challenges in unsupervised GCRL are twofold: (1) exploring diverse states to ensure the agent can learn to achieve a wide variety of goals, and (2) effectively learning a goal-reaching policy. Previous methods focus on exploring novel states [11] or states with high uncertainty in next state prediction [10, 2]. However, these methods aim to discover unseen states or state transitions, which may not be meaningful. Additionally, training a"}, {"title": "2 Related Work", "content": "Unsupervised goal-conditioned reinforcement learning (GCRL) aims to learn a goal-conditioned policy that can reach diverse goal states without external supervision [15, 16, 10, 2]. The major challenges of unsupervised GCRL can be summarized in two aspects: (1) optimizing goal-conditioned policies and (2) collecting trajectories with novel goals that effectively enlarge its state coverage.\nRecent techniques such as hindsight experience reply (HER) [8] and model-based policy optimiza- tion [10, 12] have improved the efficiency of GCRL. However, learning complex, long-horizon goal-reaching behaviors remains difficult due to sparse (e.g. whether it reaches the goal) [8] or heuris- tic rewards (e.g. cosine similarity between the state and goal) [10, 12]. Instead, temporal distance, defined as the number of environment steps between states estimated from data, can provide more dense and grounded rewards [17, 10, 18]. Nonetheless, this often leads to sub-optimal goal-reaching behaviors since it does not reflect the \"shortest temporal distance\u201d between states. In this paper, we propose to use the estimated shortest temporal distance as reward signals for GCRL, inspired by QRL [14] and HILP [13]. We apply the learned representations to compute goal-reaching rewards rather than directly learning the value function in QRL or using it for skill-learning rewards in HILP.\nExploration in unsupervised GCRL relies heavily on selecting exploratory goals that lead to novel states and expand state coverage. Various strategies for exploratory goal selection have been intro- duced, including selecting less visited states [19], states with low-density in state distributions [20, 11], and states with high uncertainty in dynamics [10, 2]. Instead of sampling uncertain or less visited states as goals, we select goals that are temporally distant from the visited state distribution, encour- aging coverage of broader state spaces which require more environment steps to reach.\nUnsupervised skill discovery [21, 22, 23, 24, 25, 26, 27, 1] is another approach to learning diverse behaviors without supervision, yet often lacks robust exploration capabilities [27], requiring manual feature engineering or limiting to low-dimensional state spaces. METRA [1] addresses these limita- tions by computing skill-learning rewards with temporal distance-aware representations, though it exhibits limited coverage in complex environments, as depicted in Figure 1.\nTemporal distance-aware representations have been extensively used in imitation learning [28], representation learning [29, 30], unsupervised skill discovery [1], offline skill learning [13], and GCRL [14]. Methods like QRL [14], HILP [13], and METRA [1] are closely related to our work as they learn temporal distance-preserving representations or goal-reaching value functions. HILP and METRA use temporal distance-aware representations for skill representations and skill rewards. On the other hand, QRL learns a quasimetric model as a (negated) goal-reaching value function. In contrast, our method uses temporal distance-aware representations across the entire unsupervised GCRL pipeline."}, {"title": "3 Approach", "content": "In this paper, we introduce TemporaL Distance-aware Representations (TLDR), an unsupervised goal-conditioned reinforcement learning (GCRL) method, integrating temporal distance within unsu- pervised GCRL. As illustrated in Figure 2, TLDR integrates temporal distance-aware representations (Section 3.2) into every facet of the Go-Explore [19] strategy (Section 3.3), which chooses a goal from experience (Section 3.4), reaches the selected goal via the goal-conditioned policy, and executes the exploration policy to gather diverse experiences. We then refine both the exploration policy (Section 3.5) and goal-conditioned policy (Section 3.6) based on the collected data and rewards computed using the temporal distance-aware representations. We describe the full algorithm in Algorithm 1. Please refer to Appendix A for further implementation details."}, {"title": "3.1 Problem Formulation", "content": "We formulate the unsupervised GCRL problem with a goal-conditioned Markov decision process, defined as the tuple M = (S, A, p, G). S and A denote the state and action spaces, respectively. p : S \u00d7 A \u2192 \u25b3(S) denotes the transition dynamics, where \u2206(X) denotes the set of probability distributions over X. The goal of the agent is to learn an optimal goal-conditioned policy \u03c0G : S \u00d7 G \u2192 A, where \u03c0(a | s, g) outputs an action a \u2208 A that can navigate to the goal g \u2208 G as fast as possible from the current state s. In this paper, we set G = S, allowing any state as a potential goal for the agent."}, {"title": "3.2 Learning Temporal Distance-Aware Representations", "content": "Temporal distance, defined as the minimum number of environment steps between states, can provide more dense and grounded rewards for goal-conditioned policy learning as well as exploration. For GCRL, instead of relying on sparse and binary goal-reaching rewards, the change in temporal distance before and after taking an action can be an informative learning signal. Moreover, exploration in unsupervised GCRL can be incentivized by discovering temporally faraway states. Therefore, in this paper, we propose to use temporal distance for unsupervised GCRL."}, {"title": "3.3 Unsupervised GCRL with Temporal Distance-Aware Representations", "content": "With temporal distance-aware representations, we can integrate the concept of temporal distance into unsupervised GCRL. Our approach is built upon the Go-Explore procedure [19], a widely- used unsupervised GCRL algorithm comprising two phases: (1) the \u201cGo-phase,\u201d where the goal- conditioned policy \\(\\pi_G\\) navigates toward a goal g, and (2) the \u201cExplore-phase,\u201d where the exploration policy \\(\\pi_E\\) gathers new state trajectories to refine the goal-conditioned policy.\nWhile Go-Explore relies on task-specific information for goal selection and exploration policy training, our method uses task-agnostic temporal distance metrics induced by temporal distance- aware representations. The subsequent sections detail how our method leverages the representation for selecting goals in the Go-phase (Section 3.4), enhancing the exploration policy (Section 3.5), and facilitating the GCRL policy training (Section 3.6)."}, {"title": "3.4 Exploratory Goal Selection", "content": "For unsupervised GCRL, selecting low-density (less visited) states as exploratory goals can enhance goal-directed exploration [15, 16]. However, the concept of \"density\u201d of a state does not necessarily indicate how rare or hard to reach the state. For example, while a robotic arm might actively seek out unseen (low-density) joint positions, interacting with objects could offer more significant learning opportunities [27]. Thus, we propose selecting goals that are temporally distant from states that are already visited (i.e. in the replay buffer) to explore not only diverse but also hard-to-reach states."}, {"title": "3.5 Learning Exploration Policy", "content": "After the goal-conditioned policy navigates towards the chosen goal g for \\(T_G\\) steps, the exploration policy \\(\\pi_E\\) is executed to discover states even more distant from the visited states. This objective of the exploration policy can be simply defined as:\n\\[r^E(s, s') = r_{TLDR}(s') - r_{TLDR}(s).\\]\nSimilar to LEXA [10], we alternate between goal-reaching episodes and exploration episodes. For goal-reaching episodes, we execute the goal-conditioned policy until the end of the episodes. For exploration episodes, we sample the timestep \\(T_G \\sim \\text{Unif}(0, T - 1)\\) at the beginning of each episode and execute the exploration policy if timestep t > \\(T_G\\)."}, {"title": "3.6 Learning Goal-Conditioned Policy", "content": "The goal-conditioned policy aims to minimize the distance to the goal. However, defining \u201cdistance\u201d to the goal often requires domain knowledge. Instead, we propose leveraging a task-agnostic metric, temporal distance, as the learning signal for the goal-conditioned policy:\n\\[r(s, s', g) = ||\\phi(s) - \\phi(g)|| - ||\\phi(s') - \\phi(g)||.\\]\nIf our representations accurately capture temporal distances between states, optimizing this reward in a greedy manner becomes sufficient for learning an optimal goal-reaching policy."}, {"title": "4 Experiments", "content": "In this paper, we propose TLDR, a novel unsupervised GCRL method that utilizes temporal distance- aware representations for both exploration and optimizing a goal-conditioned policy. Through our experiments, we aim to answer the following 3 questions: (1) Does TLDR explore better compared to other exploration methods? (2) Is our goal-conditioned policy better than prior unsupervised GCRL methods? (3) How crucial is TLDR for goal-conditioned policy learning and exploration?"}, {"title": "4.1 Experimental Setup", "content": "Tasks. We evaluate our method in 6 state-based environments and 2 pixel-based environments, as illustrated in Figure 3. For state-based environments, we use Ant and HalfCheetah from OpenAI Gym [31], Humanoid-Run and Quadruped-Escape from DeepMind Control Suite (DMC) [32], AntMaze-Large from D4RL [33], and AntMaze-Ultra [34]. For Humanoid-Run and Quadruped- Escape, we include the 3D coordinates of the agents in their observations. For pixel-based environ- ments, we use Quadruped (Pixel) from METRA [1] and Kitchen (Pixel) from D4RL [33], with the image size of 64 \u00d7 64 \u00d7 3 as the observation.\nComparisons. We compare our method with 6 prior unsupervised GCRL, skill discovery, and exploration methods. For state-based environments, we compare with METRA, PEG, APT, RND, and Disagreement. For pixel-based environments, we compare with METRA and LEXA.\n\u2022 METRA [1]: the state-of-the-art unsupervised skill discovery method which leverages temporal distance-aware representations.\n\u2022 PEG [2]: the state-of-the-art unsupervised GCRL method which plans to obtain goals with maximum exploration rewards.\n\u2022 LEXA [10]: uses world model to train an Achiever and Explorer policy.\n\u2022 APT [25]: maximizes the entropy reward estimated from the k-nearest neighbors in a minibatch.\n\u2022 RND [35]: uses the distillation loss of a network to a random target network as rewards.\n\u2022 Disagreement [36]: utilizes the disagreement among an ensemble of world models as rewards.\nEvaluation setups. Following METRA [1] and PEG [2], we evaluate unsupervised exploration using state coverage or queue state coverage, and evaluate goal-reaching performance using goal"}, {"title": "4.2 Quantitative Results", "content": "In Figure 4, we compare the state coverage during training. TLDR outperforms all prior works, except in HalfCheetah compared to METRA. METRA learns low-dimensional skills and extends the temporal distance along a few directions specified by the skills, providing a strong inductive bias for simple locomotion tasks like HalfCheetah. On the other hand, TLDR achieves much larger state coverage in complex environments than METRA, including AntMaze-Large, AntMaze-Ultra, and Quadruped-Escape, where all other methods struggle and only explore limited regions. This shows the strength of our method in the exploration of complex environments.\nWe then compare the goal-reaching performance of our method with PEG and METRA in Figure 5. We first report the average distance between goals and the last states of trajectories. The results in Figures 5a to 5c show that TLDR can navigate towards the given goals closer than, or at least on par with METRA. For the AntMaze environments, we report the number of pre-defined goals reached by the goal-conditioned policy. Figures 5d and 5e show that TLDR is the only method that can navigate towards a various set of goals in both mazes, demonstrating its superior exploration and goal-conditioned policy learning with temporal distance.\nFigure 6 shows the results in pixel-based environments. In Quadruped (Pixel), TLDR explores diverse regions but learns slower than LEXA and METRA. For Kitchen (Pixel), TLDR interacts with all six objects during training, but struggles at learning the goal-conditioned policy. We hypothesize that learning a temporal abstraction is more challenging with pixel observations, which may lead & to encode erroneous temporal information. We leave more detailed analyses for future works."}, {"title": "4.3 Qualitative Results", "content": "We visualize the learned goal-reaching behaviors on the AntMaze-Ultra environment in Figure 7. TLDR can successfully reach both near and faraway goals in diverse regions. On the other hand, METRA and PEG fail to navigate to diverse goals. METRA could reach some goals distant from the initial posi- tion, whereas PEG fails to reach temporally faraway goals. This clearly shows the benefit of using tem- poral distance in unsupervised GCRL."}, {"title": "4.4 Ablation Studies", "content": "To investigate the importance of temporal distance-aware representations in our algorithm, we conduct ablation studies on exploration strategies and GCRL reward designs.\nExploration strategy. For goal selection and exploration rewards, we replace temporal distance, ||\\(\\phi(s)\\) \u2013 z(i) || in Equation (2), with other exploration bonuses: RND, APT (with ICM [37] representa- tions), and Disagreement. Note that goal-conditioned policies are still trained with the same temporal distance-based rewards as TLDR, thereby comparing only exploration strategies. As shown in Fig- ure 8a, using TLDR reward for goal selection and exploration rewards achieves significantly higher performance than other exploration bonuses. This result implies that our temporal distance-based rewards are effective for unsupervised exploration.\nGCRL reward design. We compare with two goal-conditioned policy learning methods: (1) QRL [14], which uses a quasimetric value function, and (2) sparse HER [8], which uses the sparse goal-reaching reward -1(s\u2260 g). Figure 8b shows the superior performance of our temporal distance-based GCRL reward. This highlights the importance of incorporating temporal distance- aware representations in training goal-conditioned policies."}, {"title": "5 Conclusion", "content": "In this paper, we introduce TLDR, an unsupervised GCRL algorithm that incorporates temporal distance-aware representations. TLDR leverages temporal distance for exploration and learning the goal-reaching policy. By pursuing states with larger temporal distances, TLDR can continuously explore challenging regions, achieving better state coverage. The experimental results demonstrate that our method can cover significantly larger state spaces across diverse environments than existing unsupervised reinforcement learning algorithms."}, {"title": "5.1 Limitations", "content": "While TLDR achieves remarkable state coverage, it still has several limitations. Firstly, TLDR shows a slower learning speed compared to METRA in pixel-based environments. Secondly, our temporal distance-aware representations do not capture the asymmetric temporal distance between the states, which can make policy learning challenging for asymmetric environments. Finally, TLDR achieves high efficiency in terms of wall clock time, but with a relatively low update-to-data ratio (number of gradient steps divided by number of environment steps) of 1/32 in state-based experiments, as used in METRA. However, we believe that increasing the update-to-data ratio or using model-based approaches could potentially enhance sample efficiency."}]}