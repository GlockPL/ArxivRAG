{"title": "Federated-Continual Dynamic Segmentation of Histopathology guided by Barlow Continuity", "authors": ["Niklas Babendererde", "Haozhe Zhu", "Moritz Fuchs", "Jonathan Stieber", "Anirban Mukhopadhyay"], "abstract": "Federated- and Continual Learning have been established as approaches to enable privacy-aware learning on continuously changing data, as required for deploying AI systems in histopathology images. However, data shifts can occur in a dynamic world, spatially between institutions and temporally, due to changing data over time. This leads to two issues: Client Drift, where the central model degrades from aggregating data from clients trained on shifted data, and Catastrophic Forgetting, from temporal shifts such as changes in patient populations. Both tend to degrade the model's performance of previously seen data or spatially distributed training. Despite both problems arising from the same underlying problem of data shifts, existing research addresses them only individually. In this work, we introduce a method that can jointly alleviate Client Drift and Catastrophic Forgetting by using our proposed Dynamic Barlow Continuity that evaluates client updates on a public reference dataset and uses this to guide the training process to a spatially and temporally shift-invariant model. We evaluate our approach on the histopathology datasets BCSS and Semicol and prove our method to be highly effective by jointly improving the dice score as much as from 15.8% to 71.6% in Client Drift and from 42.5% to 62.8% in Catastrophic Forgetting. This enables Dynamic Learning by establishing spatio-temporal shift-invariance.", "sections": [{"title": "1. Introduction", "content": "AI-Assisted histopathology segmentation has the potential to significantly support pathologists and improve experience for their patients. Generalization of such models depends on the availability of diverse data from multiple hospitals [5, 31]. Privacy and logistical requirements typically forbid the sharing of such sensitive patient data. Federated Learning (FL) [28], which allows decentralized training without sharing of training data, has been established as an important paradigm for circumnavigating the data availability issues of histopathology [18, 34, 36]. On the other hand, privacy and storage requirements necessitate that the model is continuously trained on an evolving data stream without access to the old data [24]. This means retraining the model on the entire dataset whenever new data becomes available is impossible. Consequently, the concept of Continual Learning [15] is also essential for the deployment of privacy-aware AI-assisted histopathology.\nHowever, a core problem in histopathology is that this type of data is prone to data shifts, which manifest spatially between clients in Federated Learning or temporally in Continual Learning [3, 5, 12]. Spatial shifts, for example, can be caused when certain hospitals use different scanner models for scanning the tissue [12], staining styles which depend on the preparation technique differ [31, 34] or artifacts [32, 33] occur in certain hospitals. In Federated Learning, all these types of data shifts can cause performance degradation of the central model, which is known as Client Drift (CD) [40]. Temporal Shifts are particularly evident in scenarios such as when new diseases occur [9, 21], the acquisition techniques change [12, 34] or the patient population changes [5, 17]. Continual Learning suffers from Catastrophic Forgetting (CF) [24] where data shifts over time cause the model to forget old knowledge. Consequently, addressing both problems of Client Drift and Catastrophic Forgetting is essential for the deployment of AI-assisted histopathology. However, existing approaches only address Client Drift and Catastrophic Forgetting individually, despite their underlying common cause of spatio-temporal distribution shifts.\nAs medical data is only sparsely available [16] on the dynamic grid of space-time, assumptions are necessary to make Dynamic Learning feasible. We assume spatio-"}, {"title": "2. Related Work", "content": "In Continual Learning, there are three main branches of learning paradigms: the regularization-based approaches [14], the architectural approaches [1], and the replay-based methods [25, 26]. Of these branches, only the replay-based branch has an easy path to be adapted to Federated Learning. The Rehearsal method [26] is often seen as an upper bound of what can be achieved with such methods. However, it violates patient privacy by saving samples from the old datasets to interleave them during the training of the next stage. B\u00e1ndi et al. [4] proposes Continual Learning in the context of histopathology but for task-incremental learning and a classification task, unlike our method that is designed to train segmentation tasks under distribution shifts. In Federated Learning, many methods that have been proposed can be categorized into either a Weight aggregation approach [20, 27, 34, 36] or a distillation approach [11]. Remarkably, Yoem et al. [36] have shown that, in some cases, Federated Learning can even outperform centralized learning in histopathology. Also, recent advances [6] have shown that methods based on learning synthetic datasets and using them to train the server model can be successful they are limited to upper bounds similar to the Rehearsal method [26] from Continual Learning."}, {"title": "3. Method", "content": "The intuition of our method is to guide the training process towards a shift-invariant parameter configuration. We achieve this by continuously evaluating model updates both spatially for updates from Federated Learning clients and temporally in a Continual Learning setting \u2013 for the curation of all updates during the Federated Continual model aggregation process. Consequently, our method consists of a component for the evaluation of model updates and an aggregation method that uses the evaluation results for improving spatio-temporal shift-invariance. Algorithm 1 provides an overview, how we combine the model evaluation and aggregation method. In the following, we will begin with a detailed definition of our model update evaluation function, before explaining, how we apply it during the aggregation process."}, {"title": "3.1. Evaluation of Model Updates", "content": "The foundation of our method is the evaluation method for potential model updates that we apply on the server side during aggregation. Our model update evaluation is inspired by the Barlow Twin Objective Function [38] which is originally designed to evaluate self-supervised learning by measuring the changes of the embeddings of one specific image when the input data is shifted. It has been successfully deployed in the conventional centralized training on closed datasets that it was designed for, but for Dynamic Learning, we need to enable decentralized training (Federated Learning) and training on an open dataset (Continual Learning). Consequently, the comparison of two model states from Federated Learning aggregation or Continual Learning model updates \u2013 regarding diverse data is required. Therefore, as visualized in Figure 2 (left), our modified DynBC function compares two models a potential update and the current model state instead of only one. The novelty of our evaluation function in comparison with the existing objective function of Barlow Twins stems from the approach of comparing two model states instead of only evaluating one, enabling the live-evaluation of spatio-temporal training processes. Moreover, to preserve privacy during the model evaluation, we sample reference data from a separate, public dataset of histopathology images. We augment it with one out of a selection of augmentations that changes by each image sample, allowing us to"}, {"title": "3.1.1 Choice of metric for DynBC", "content": "As an essential part of DynBC distance measurement is the comparison of the two segmentation masks derived from the compared model states, we explain in the following the reasoning behind using the dot product as the underlying similarity metric. A first important criterion specifically for the given histopathology context is that the goal is the precise measurement of overlap of the segmentation. The dot product allows us to directly measure this, unlike other metrics such as Cosine Similarity or KL-Divergence. Besides the precision, also computational efficiency led to the decision to use the dot product. In the case of histopathology"}, {"title": "3.2. Improving robustness against Client Drift", "content": "We deploy the method at the aggregation step of the server by evaluating the update from each Client to the current main server model. Consequently, we apply or ignore the update from a client based on the DynBC distance. As visualized in Figure 2 (right), the goal is to detect an uncommon increase in the DynBC for certain client update, as we want to limit the global model update to only gradual drifts. Such an increase would exceed the aforementioned threshold factor. Therefore, we track the global maximum DynBC, and if we surpass it by a certain factor, this drastic change is considered an indicator of an undesirable update regarding shift-invariance. In such cases, the server ignores the update from the specific client and only averages from clients with sufficiently low DynBC. To ensure the method's reliability, DynBCs that are exceeding their previous maximum by the given factor are not tracked among the maximum DynBCs. The latest global update is distributed to the ignored client to guide it to a more robust parameter state along with the other clients, decreasing the performance degradation from Client Drift."}, {"title": "3.3. Improving robustness against Catastrophic Forgetting", "content": "Similarly to the Client Drift scenario, we use our method to evaluate model updates in Continual Learning regarding their shift-invariance and ensure homogeneity in the temporal domain. Therefore, we evaluate each potential model update and the corresponding current model using our DynBC metric. We track the maximum DynBC over all valid model updates and ignore updates that surpass it by a given factor. This way, we guide the model to a more shift-invariant and homogenous distribution, helping to decrease the performance degradation from Catastrophic Forgetting."}, {"title": "4. Experiments and Results", "content": "In the following section, we provide details on the datasets and setup of the experiments for evaluating DynBC. Then we evaluate DynBC in the scenarios of Client Drift, Catastrophic Forgetting and their combination and compare the resulting performance improvements to our baseline and multiple comparison methods such as the upper-bound of Rehearsal from the Continual Learning domain, FedAdam from Federated Learning and FedWeIT from Federated Continual Learning. Lastly, we conduct ablation studies, comparing different threshold parameters and"}, {"title": "4.1. Dataset and Experimental Setup", "content": "We perform supervised training of a U-Net [29], using BCEWithLogitsLoss from PyTorch [23] with ADAM [13] as optimizer. The learning rate is $1e-4$ and the batch size is 4. We use three public histopathology segmentation datasets: BCSS [2] and Semicol [22] for the evaluation and Camelyon17 [5] as a separate reference dataset for DynBC. All datasets use a patch size of 256 \u00d7 256 pixel, with 0.25 microns per pixel, and are split by patients. BCSS we split based on patients into 60% train, 30% test, and 10% validation data from 10710 patches. For Semicol, we sample 70% train, 20% test, and 10% validation data from 2120 patches. We use Camelyon17 only for measuring prediction distances in DynBC and not for training or testing, and we only sample from 1125 randomly chosen validation patches. We train a binary segmentation task. Therefore, in BCSS we segment for Tumor and in Semicol for a merged label Tumor and Tumor Stroma. As reference images need to be augmented for DynBC, we augment each sample with one selected augmentation: From Torchvision [19] Gaussian Blur with kernel size 19 and $\\sigma = 4.0$, Motion Blur with a blur limit of 29 or from Albumentations [7] Gaussian Noise with a variance limit of 1000. We evaluate DynBC (with a maximum factor for DynBC increase of 2) separately on the public histopathology segmentation datasets BCSS and Semicol and for emulating data shift on the training data, we either apply Torchvision brightness (increase by 2.0 for BCSS or 1.2 for Semicol) or Torchvision Gaussian blur with a kernel size of 19 and $\\sigma = 8.0$. To avoid data leakage, we don't use Gaussian blur on the DynBC reference dataset when we evaluate this augmentation. We repeat all experiments on 3 seeds and train on a GeForce"}, {"title": "4.2. Evaluation for Client Drift", "content": "In the case of BCSS, we deploy 5 clients while 3 clients experience a data shift via augmentations and on Semicol, we run with 4 clients, and 2 of them augmented (Figure 3 on the left side). All clients have their equally sized and individual parts of the train set. We use FedAvg [20] as algorithm for Federated Learning and train until the global model converges after 100 epochs (BCSS) or 80 epochs(Semicol). We evaluate the central model on a separate original test without augmentations. To ensure consistent results, we train for 20 more epochs and average the test set dice score. Semicol converges after epoch 80. Similarly, we compute the average dice score over the next 20 epochs.\nFor comparison, we plot the resulting dice scores of the baseline evaluation without any method against Client Drift or Catastrophic Forgetting, our method DynBC, and Rehearsal [26] with 10% Rehearsal data. Rehearsal is a well-established method from Continual Learning but can also be applied in a Dynamic scenario [3]. We visualize this comparison in Figure 4 and Table 1 shows the corresponding dice scores. The results show a consistent improvement throughout all scenarios. In Client Drift on BCSS and Semicol (Gaussian blur), DynBC is always performing significantly better than Rehearsal and the baseline. Only in the case of the brightness-augmented Semicol, the improvement through DynBC is inconsistent. We explain this with information loss through the brightness augmentation, as the patches from Semicol are already bright. Rehearsal can compensate for this by training partially on the original data distribution. However, it is not privacy-preserving. Therefore, the improvements from Rehearsal come at the price of breaking the assumption that there is no access to the original distribution. In all other experiments for Client Drift, Rehearsal even shows a performance degradation compared to the baseline, as it is originally a method from Continual Learning and can't mitigate the spatial shift. DynBC bene-"}, {"title": "4.3. Evaluation for Catastrophic Forgetting", "content": "As in the evaluation for Client Drift, we emulate data shift by applying the same selection of augmentations on the same two histopathology datasets. However, we want to simulate the performance impact of a train set that shifts over time when using our method compared to the baseline and Rehearsal with 10% rehearsed data. First, we train a single model on the original train set without applying any shift for 75 epochs before switching to one of the selected augmentations to emulate data shift and continue training on the shifted data until convergence (130 BCSS and 80 Semicol). Catastrophic Forgetting shows a performance drop after shifting to the augmented dataset (Figure 3 on the right side). For consistent results, we again evaluate the average test set dice score of the last 20 epochs after convergence. According to Figure 5 and Table 2, DynBC significantly improves the performance in all scenarios except the brightness-augmented Semicol. However, the performance loss is minimal (0.6336 to 0.6253 dice score) compared to the improvements in all other scenarios. Rehearsal gives the best performance in this scenario, but this is not surprising, as it is often seen as an upper bound for preserving performance in scenarios of Catastrophic Forgetting because it comes at the price of breaking the assumption that old data cannot be accessed anymore. In privacy-sensitive applications such as training on histopathology images, this violation would often be problematic due to privacy regulations. DynBC is preserving this principle as it solely relies on a separate dataset, which can also be a public dataset, as in this evaluation."}, {"title": "4.4. Evaluation for combined Client Drift and Catastrophic Forgetting", "content": "Our method is designed to concurrently mitigate Client Drift and Catastrophic Forgetting in a setting of Federated-Continual Learning. To simulate such a scenario, we split"}, {"title": "4.5. Ablations", "content": "In the following, we compare different ablations of the DynBC method. We start with analyzing the impact of augmenting the reference dataset and continue with comparing different DynBC threshold factors."}, {"title": "4.5.1 Augmentation of the Reference Dataset", "content": "We conduct an ablation on whether augmenting the reference dataset improves the performance of DynBC. Table 4 shows the performance on BCSS with brightness augmentation and the same settings as for the experiments on Client Drift and Catastrophic Forgetting. The augmentations im-"}, {"title": "4.5.2 Threshold factor for model updates", "content": "DynBC applies a threshold factor on the measured model distances that determines, whether a model update is considered to fulfill the Continuity requirements and should be applied or if it should be discarded. As the DynBC distance is calculated based on a predicted segmentation mask instead of model parameters, the threshold is independent of the underlying model architecture. However, the choice of a right threshold is fundamental for choosing the right model updates: Too low a threshold would hinder the training process as it would block most of the updates, potentially avoiding any training progress after the initialization phase of DynBC. If the threshold is configured too high, this method would not filter enough unintended model updates and therefore not be able to improve segmentation performance in the setting of Client Drift and Catastrophic Forgetting. In Table 5, we apply DynBC to the Catastrophic Forgetting Scenario on the BCSS dataset with the same brightness- and gaussian blur augmentation as used in the main experiments and evaluate the resulting dice scores.\nThe results indicate that the threshold factor 2.0 provides the best performance, supporting our decision to use this parameter in our work."}, {"title": "4.6. Limitations of DynBC", "content": "Our method requires a separate reference dataset. This limits the application in scenarios with very limited availability of public training data. As in this paper we aim for a histopathology-based application, this is not a problem as"}, {"title": "5. Conclusion", "content": "We introduce Dynamic Barlow Continuity (DynBC) as a method that guides the training process to spatio-temporal continuity, jointly mitigating two fundamental problems Client Drift and Catastrophic Forgetting for the robust- and privacy-aware deployment of AI-assisted histopathology. Our evaluation of two histopathology datasets shows significant improvement in dice score for Client Drift and Catastrophic Forgetting even when they are combined. This demonstrates the spatio-temporal shift-invariance for Dynamic Learning through DynBC."}]}