{"title": "Benchmark for Evaluation and Analysis of Citation Recommendation Models", "authors": ["Puja Maharjan"], "abstract": "Citation recommendation systems have attracted much academic interest, resulting in many studies and implementations. These systems help authors automatically generate proper citations by suggesting relevant references based on the text they have written. However, the methods used in citation recommendation differ across various studies and implementations. Some approaches focus on the overall content of papers, while others consider the context of the citation text. Additionally, the datasets used in these studies include different aspects of papers, such as metadata, citation context, or even the full text of the paper in various formats and structures. The diversity in models, datasets, and evaluation metrics makes it challenging to assess and compare citation recommendation methods effectively. To address this issue, a standardized dataset and evaluation metrics are needed to evaluate these models consistently. Therefore, we propose developing a benchmark specifically designed to analyze and compare citation recommendation models. This benchmark will evaluate the performance of models on different features of the citation context and provide a comprehensive evaluation of the models across all these tasks, presenting the results in a standardized way. By creating a benchmark with standardized evaluation metrics, researchers and practitioners in the field of citation recommendation will have a common platform to assess and compare different models. This will enable meaningful comparisons and help identify promising approaches for further research and development in the field.", "sections": [{"title": "1 INTRODUCTION", "content": "Citation recommendation is the process of providing accurate citations for the intended cited papers. In order to accomplish this, citation recommendation systems make use of various features from the citing or cited paper. These features encompass the metadata and text content, as well as the contextual information surrounding the citation. The metadata typically includes the title, author, abstract, venue, and published date. Based on these features, citation recommendation systems can be categorized as either global or local.\nGlobal citation recommendation systems rely on comprehensive information, including metadata and content of papers, to predict citations. For example, Bethard and Jurafsky [5] utilized only the title and abstract in their model, while ClusCite [29] incorporated both metadata and article content, using terms, authors, and venues to group papers based on shared interests for similar paper recommendations. Bhagavatula et al. [6] and Cohan et al. [10] leveraged the content of articles for citation recommendations.\nIn contrast, local citation recommendation systems focus on the contextual aspects of the citation. Some of these methods combine context with metadata [16-18, 27, 44, 45]. Depending on the approach, different character limits are used, such as 100 [12, 16, 17, 20, 27], 200 [27], 600 [44, 45], or 2048 [35]. Additionally, the tokens considered for the context can be either characters [12, 16, 27] or words [17]. The context can be bidirectional [12, 16-18, 27] or unidirectional [20].\nOngoing research in citation recommendation also emphasizes the significance of datasets utilized for training these models. The existing citation datasets are sourced from a wide range of publication fields and journals, encompassing diverse meta-information extracted from the papers. Specific datasets cater to particular domains, such as ACL-ARC [7] which focuses on data from the computational linguistics, DBLP [33] which comprises datasets from the field of computer science, PubMed which contains data from the biomedical domain, and FullTextPeerRead [20] which consists of datasets from ACL and PeerRead. On the other hand, datasets like Refseer [19], Citeseer [15], S2ORC [25], Citeomatic Opencorpus [6], and CORE [22] are derived from different fields. Similarly, datasets such as unArxiv [32] and Bibliometric-Enhanced arXiv [31] include papers extracted from Arxiv.\nDifferent models employ diverse dataset parameters to accommodate their unique structural requirements, often necessitating the need for filtering and format changes to align with their respective models. In addition, authors frequently construct their own datasets tailored to their specific citation recommendation methods [6, 19, 20]. However, the creation of a standardized dataset presents challenges due to the variations in formats and structures that are tailored to individual models. The diversity in models, datasets, and evaluation metrics poses challenges when assessing and comparing citation recommendation approaches [2]. To address this issue, there is a pressing need for a standardized dataset and evaluation metrics that can be used to evaluate these models consistently.\nThis work proposes developing a benchmark for analyzing and comparing citation recommendation models. In our experiment, our primary focus will be developing a benchmark for local citation recommendation systems. This benchmark will assess the performance of models on distinct features of the citation context and provide a comprehensive evaluation of the citation recommendation models across all these features, presenting the results in a standardized manner. By establishing a benchmark with standardized evaluation metrics, researchers and practitioners in the field of citation recommendation will have a common platform for assessing and comparing different models. This will enable efficient evaluation of citation recommendation models across various domains and contexts, eventually guiding to improved model performance and a more in-depth understanding of citation recommendation systems.\nThe contributions of this work are as follows:"}, {"title": "2 RELATED WORK", "content": "The field of citation recommendation has seen extensive research encompassing diverse methods and evaluation techniques. These systems are categorized based on their underlying methodologies. Ali et al. [2] classified citation recommendation approaches into Collaborative Filtering (CF), Content-Based Filtering (CB), Graph-Based (GB) filtering, Deep Learning-Based (DL) models, and Hybrid Filtering. Similarly, Ko et al. [23] distinguished these techniques into text mining, KNN, clustering, matrix factorization, and neural networks. Collaborative filtering and matrix factorization utilize user ratings and preferences to suggest new items, while content-based filtering relies on the metadata of papers such as titles, abstracts, and keywords. Text mining techniques like TF-IDF analyze semantic content for recommendation, and graph-based approaches utilize relationships among entities such as authors, papers, and venues. Deep learning methods, on the other hand, employ models trained on citation contexts or full texts to predict citations. KNN and clustering methods further refine recommendations by identifying similar papers or grouping relevant features.\nHybrid models combine these approaches to achieve improved performance. For instance, He et al. [17] introduced a context-aware recommendation model based on local context using a non-parametric probabilistic framework, emphasizing the effectiveness of local context (citation surroundings) over global context (title and abstract). F\u00e4rber et al. [13] further categorized local citation recommendation into feature-based models, topic modeling, machine translation, and neural networks, which leverage varying features, from textual similarity to neural embedding techniques.\nCitation recommendation models use different context sizes and methodologies. Jeong et al. [20] demonstrated that context size significantly impacts performance, favoring a bi-directional context of approximately 100 tokens. Similarly, Medi\u0107 et al. [27] found that smaller contexts, about 200 characters, yield better results. Techniques also vary, including probabilistic models [17], LSTM-based models [44], and Graph Convolutional Networks (GCNs) coupled with BERT embeddings [20]. Many models utilize a two-phase system: a candidate selection step, often using fast retrieval techniques such as KNN or BM25, followed by a reranker phase for finer evaluation. Notable contributions in this domain include SciBERT-based reranking models [16, 28] and transformer architectures such as Specter [10] and Galactica [35].\nNegative sampling techniques play a crucial role in model training. For instance, Gu et al. [16] retrieved candidates using a hierarchical attention encoder and randomly sampled negative papers to enhance the relevance of predictions. Similarly, Nogueira et al. [28] used BM25 for candidate retrieval, marking relevant papers as positive and others as negative. The careful design of negative samples, such as the use of nearest neighbors or time-based filtering, has been shown to improve performance [6, 41].\nDespite progress, there is no standardized benchmark for citation recommendation models, unlike other NLP domains. Deghani et al. [11] highlighted the need for cohesive benchmarks to measure progress effectively. Benchmarks in NLP, such as GLUE [40], SuperGLUE [39], and GEM [14], offer insights into how comprehensive evaluation frameworks enhance task performance. These frameworks incorporate metrics like inference latency, energy efficiency, and fairness, essential for holistic assessment. For citation systems, benchmarks must address diverse phenomena, including varying citation patterns and author biases, to capture real-world applicability.\nCitation patterns, such as positioning and frequency, vary significantly across disciplines, with differences in biomedical sciences, physical sciences, and humanities [8]. Zhu et al. [46] identified that citation influence correlates with the number of in-text mentions and title similarity, while Ritchie et al. [30] emphasized the importance of using broader context windows for indexing. Furthermore, newer papers (0-3 years) and older works (8+ years) often exhibit lower perceived influence compared to those published in intermediate periods.\nThis broad spectrum of research underscores the complexity of citation recommendation and the critical need for standardized benchmarks to ensure consistent and meaningful advancements in the field."}, {"title": "3 DIAGNOSTIC DATASETS", "content": "Diagnostic datasets play a crucial role in error analysis, performance evaluation, and model comparison in citation recommendation [40]. In this work, to generate the datasets, the S2ORC dataset [25] is utilized, a filtered dataset obtained from the Semantic Scholar literature corpus [3]. This comprehensive dataset consists of papers from medicine, physics, biology, computer science, mathematics, chemistry, materials science, psychology, engineering, environmental science, business, geography, sociology, political science, geology, economics, history, art, and philosophy. The datasets are distributed across multiple years and journals. The dataset provide comprehensive data about research papers, including their full text, annotations, author information, referenced works, and annotated content elements. The Semantic Scholar dataset S2AG [21] is employed to access metadata. Diverse model-agnostic datasets are essential to ensure the practical evaluation of various models. This work aims to generate diagnostic datasets focusing on different features of the citing text extracted from the S2ORC dataset. The process includes extracting data from each field in proportion to their representation"}, {"title": "3.1 Datasets", "content": "3.1.1 Context Length. In order to define the classes of the context length, the length of sentences in the papers must be checked. Initially, the sentences will be split at the token level, and the number of tokens in each sentence will be counted; here, the tokens represent words. The outliers in the sentence length are analyzed where, the outliers with a high number of tokens are likely due to mathematical equations and formulas that were split at the character level because of spaces between each character. These outliers will be removed using a z-score value. Any calculated z-score value greater than the assigned z-threshold value (set to 3) will be marked as an outlier and removed accordingly. Afterward, the probability distribution function will be calculated to generate a normalized distribution. The longest sentences extend up to 70 tokens. Now, sections are defined to determine the classes for this diagnostic dataset. The mean and standard deviation will be used to extract the upper and lower bounds.\n3.1.2 Citation Location. The location of a citation within the text can impact the performance of citation recommendation models. For example, in a study by Wright et al. [43], the authors focused on the citation location at the end of a sentence to determine if it enhances clarity. First, information on the citation location of various citations in the sentence based on their token position will be collected. Initially, sentences of mid-length, as described in the previous section, will be filtered to ensure that the length distribution does not impact the results.\nThe position will then be normalized by dividing the position of the citation by the length of the respective sentence. The position of the citation in the sentence appears mostly at the end, with a few at the beginning.\n3.1.3 Context Type. Citation recommendation models' performance can vary depending on the type of context. The objective is to create a diagnostic dataset encompassing diverse context types. While different studies examine diverse types of citation contexts [4, 24, 34, 36, 42, 46], this paper uses the citation intent of the papers defined in S2AG dataset. The focus will be on mid-length sentences with the citation location at the end.\n3.1.4 Low Resource Papers. Many fields do not account for a sufficient proportion of the entire S2ORC dataset. Several models, such as those in [6, 28], are trained on PubMed, which contains data from the medical domain, and DBLP [1, 6, 9, 28, 37, 44], which contains data from the computer science domain. Similarly, various models are trained on ACL papers which includes NLP and computational linguistic [1, 16, 20, 26, 27, 41, 44].\nCitation recommendation models should also be able to provide accurate citations for less popular fields. Therefore, the aim of this diagnostic dataset is to assess how well the models perform in low-resource fields. Fields with less than 3% of the total papers, are defined as low-resource datasets. These fields includes: psychology, environmental science, engineering, business, geography, political science, sociology, geology, history, art and philosophy.\n3.1.5 POS of Surrounding Words. In the citing text, the surrounding words of the citations can belong to different parts-of-speech (POS). For instance, in the sentence \".. is stated by Abc et al. (2022),\" the citation is followed by a preposition (IN). Similarly, citations can be followed by verbs, such as \".. argues.\" Additionally, citations may follow proper nouns, as in \"... S2ORC (Lo et al., 2019).\" These surrounding words play a crucial role in determining whether the preceding or following token is the actual citation. To assess the performance of citation recommendation models in handling this aspect, a dataset will be created that includes various POS words before and after the citation.\nBy incorporating POS words that are less or more frequent in association with citations, the effectiveness of models in utilizing POS information for accurate citation recommendation can be evaluated. This dataset will provide insights into the model's ability to consider the context and surrounding linguistic cues when making citation predictions.\nThe POS of the preceding and following words in citations is analyzed to obtain the results. shows the distribution of the combined POS words by combining the POS tags as follows:\n\u2022 verb: VB, VBD, VBG, VBP, VBZ, VBN\n\u2022 noun: NN, NNS, NNP, NNPS\n\u2022 adjective: JJ, JJR, JJS\n\u2022 adverb: RB, RBR, RBS\n\u2022 pronoun: PRP, PRP$, WP, EX\n\u2022 preposition: IN, TO\n\u2022 others: PDT, DT, CC, MD, WDT, POS, RP, FW, CD, WRB"}, {"title": "3.2 Data Extraction", "content": "To collect the samples, the dataset is analyzed first, and samples are collected accordingly, as depicted in Figure 6. Only in the first stage are the papers sampled, and in the second stage, the citing sentences are extracted according to the requirements of the diagnostic datasets. Upon obtaining the S2ORC data alongside its metadata from S2AG, the next step involves extracting fields, years, and citation count information from these datasets. The data is then analyzed based on these parameters. The most prominent publications within our dataset fall within the field of medicine, constituting 45.50% of the entire dataset. In contrast, history, art, and philosophy together account for less than 0.50% of the total dataset. Notably, the dataset demonstrates an increase in papers until 2020, followed by a decline. Medicine contains a substantial number of papers, followed by computer science, physics, and biology.\nSimilarly, the distribution of citation counts across all fields will be examined. Given the varying record counts in different fields, the average citation count in each field will be calculated by normalizing the sum of citation counts in each field based on the total number of records within that field. The m fields can be labeled as $f_1, f_2,..., f_m$. The average citation count in the i-th field for a total of n papers is depicted in the mathematical expression below, where $Cavg(f_i)$ is the average citation count in field i.\n$Cavg(f_i) = \\frac{\\sum_{j=1}^{N_i} C_{ij}}{n_i}$        (1)\nHere, $C_{ij}$ is the citation count of the j-th paper in the i-th field, and $n_i$ is the total number of papers in the i-th field.\n3.2.1 Paper Selection. In this section, the approach for paper selection is described.\nFor the sampling process, fields, years, and citation counts are used as parameters. This decision is based on the premise that citation recommendation models should be capable of providing accurate citations regardless of the paper's field, publication year, and the popularity of the cited paper. By sampling data from each of these strata, the goal is to create the diagnostic datasets that is evenly distributed."}, {"title": "3.2.2 Sentence Extraction", "content": "We will follow the steps defined below for the extraction of the citing sentences, as shown in Figure 8.\n(1) Extracting Text\nFirst, the citing and cited papers needs to be collected. There are three collections for each dataset, consisting of the cited and citing paper's metadata and the citing paper's text. The citing papers for each cited paper are retrieved from the MongoDB collection for the specified dataset. Then, the text content of the citing paper is extracted. The text content of the papers is retrieved by parsing each annotated paragraph defined in the S2ORC dataset.\n(2) Bibliography Entry Extraction\nIn addition to analyzing sentences, a comprehensive analysis of references within each sentence will be conducted. The focus is on retrieving relevant reference information and citation text. The S2ORC dataset provides annotations for cited bibliographies and their respective citations in the text. To accurately parse references, specific attributes within the sentence will be examined, including bibliography references, text citations, cited paper authors, cited paper titles, and cited paper publication venues, to ensure their presence in S2ORC.\nAs mentioned earlier, the S2ORC dataset contains bibliography entries along with bibliography references and links them through a unique identifier. The bibliography references will be extracted using the corpus ID of the cited paper. Once these references are obtained, the unique identifier will be used to identify the bibliography entry in the text content, such as [12] or (abc et al., 2012), etc. This value will be used in the further process of citing sentences for the respective cited paper.\n(3) Sentence Extraction\nNext, individual sentences will be extracted from the retrieved paragraphs using the SciSpacy library and the SciBERT model. To ensure sentence integrity, each sentence will be verified to conclude with appropriate punctuation (period, question mark, or exclamation mark), trimmed as necessary, and a space will be added at the end. Additionally, sentences containing footer notes with superscripts (e.g., \"7 this is...\") will be identified and excluded by checking the first character of the sentence. Sentences that contain only the citation entry of the respective paper retrieved from the above process will be returned. Sentences with only one citation entry will be retrieved since the diagnostic datasets consist of sentences containing only one citation. The sentences will be filtered accordingly.\n(4) Citation Parsing\nInstances in the S2ORC dataset reveal citations that do not adhere to standard citation rules. For example, citations such as \"(Lee et al., 1997a, Lee et al., 1997b)\" are incorrectly transformed into \"(Lee et al., 1997a (Lee et al., 1997b)\". To tackle this issue, regular expressions are utilized to identify the standard citation structures of various citation styles. A citation parser is developed to determine if a sentence contains correct citations following various citation styles and to retrieve the citation text. The citation parser for the initial cited sentence selection checks for one correct citation in the sentence. Subsequently, the citation text within the sentence is replaced with the placeholder \"<REF>\" and the replaced citation is added to the metadata for future reference.\n(5) Identifying the Citing Sentence Class\nEach diagnostic dataset represents various features of the citing sentences. These parameters are defined by different classes of these features. The classes and features of the citing sentences for the diagnostic datasets are defined below:\n(a) Fields: Biology, Chemistry, Computer Science, Medicine, Materials Science, Physics, Mathematics\n(b) Years: 1996-2000, 2001-2005, 2006-2010, 2011-2015, 2016-2020\n(c) Citation Count Groups: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n(d) Context Length: Short, Medium, Long\n(e) Context Location: First, Middle, Last\n(f) Context Type: Background, Result, Method\n(g) POS of Surrounding Words:\n(i) Preceding POS: verb, noun, adjective, adverb, pronoun, preposition, and others\n(ii) Following POS: verb, noun, adjective, adverb, pronoun, preposition, and others\n(h) Low Resource: Psychology, Environmental Science, Engineering, Business, Geography, Political Science, Sociology, Geology, History, Art, Philosophy\nOnce the classes are retrieved, the sentence is filtered based on the mentioned class. If it complies, additional conditions are applied accordingly. These conditions ensure that other features, such as length or position, do not affect the evaluation result. The additional conditions applied to each dataset are listed below:\n(a) Context Length: Reference at the \"end\" of the sentence\n(b) Context Location: The length of the citing sentence belongs to the class \"medium\""}, {"title": "4 RESULT AND DISCUSSION", "content": "In this section, the results from different models on the diagnostic datasets are compared. For the evaluation, we chose Neural Citation Recommendation (NCN) [12], Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-Based Reranking (LCR) [16], Galactica [35] and BM25 ranking algorithm which is our baseline. First, we clarify that all the models are trained on different datasets. LCR and NCN are trained on Arxiv belonging to different years. Table 1 presents the results of the models across all diagnostic datasets, highlighting the top K results where K is 10 for both Recall and MRR. BM25 shows the best performance in terms of Recall and MRR for most datasets. Specifically, BM25 outperforms other models across all classes for diagnostic datasets categorized by fields, years, and citation counts. This comprehensive performance across different classes underscores the robustness of BM25. LCR achieves a better Recall value for the length dataset, while BM25 excels in MRR. In the location dataset, BM25 has the highest Recall when the reference is at the start of the context, whereas LCR performs better for references in the middle and end. However, BM25 consistently has a higher MRR across all classes in the location dataset. In the intent dataset, LCR demonstrates superior performance in Recall for all classes, whereas BM25 leads in MRR. For the low-resource dataset, BM25 dominates Recall and MRR across all classes. Additionally, BM25 achieves higher MRR across all classes and for BM25, except for the preceding Preposition and Others categories.\nDue to the varying training datasets, different models perform differently on the classes of the diagnostic datasets. In the field datasets, NCN and LCR perform well in mathematics, while Galactica models perform well in computer science. On the other hand, BM25 performs the best in the chemistry domain. In the year's dataset, older years have had better performance in NCN and LCR, while Galactica models have better performance in recent years. Similarly, BM25's performance peaks in the mid-level years. When it comes to citation count, a direct relationship is observed with the performance of NCN, which improves as the number of citation counts increases. Conversely, LCR demonstrates its strength for cited papers with a lower citation count. The performance of Galactica models is closely tied to the citation counts of the cited papers; in contrast, the performance of BM25 shows an inverse correlation with the citation count. NCN, LCR, and Galactica models show superior performance in the short-length context, while BM25 excels with longer contexts. LCR and Galactica models perform better for references positioned at the end of the context; on the other hand, NCN shows better performance for references in the middle. BM25 demonstrates the best overall performance when the reference is at the start of the context. In the intent dataset, NCN, LCR, and BM25 show the best performance for the intent with background class, while Galactica performs well for the methodology. Similar to the low-resource dataset, NCN shows better recall in philosophy, while MRR is better in business. For LCR and Galactica models, engineering demonstrates the best performance compared to other low-resource fields. BM25 shows better recall in art and MRR in geography. In the POS dataset, references with the following adjective POS perform better in NCN. For LCR, the preceding POS noun shows better performance. The Galactica models perform well with the preceding POS preposition, while in BM25, references with the preceding POS pronoun achieve higher performance.\nAs mentioned, the performance varies among different models, primarily due to differences in their training datasets and architectural designs. For instance, Galactica models exhibit lower performance in the location dataset when referencing the start and middle of sentences compared to the end. This discrepancy arises from Galactica's training on the Transformer model's decoder, which retains information about the preceding context before the masked token. Similarly, BM25 demonstrates superior overall performance compared to other models, primarily because it has the advantage of querying the entire S2AG dataset, which comprises millions of records."}, {"title": "5 LIMITATIONS", "content": "The size of the diagnostic dataset could be more significant, which was limited due to the time required to manually check the accuracy of the context. Additional datasets can be incorporated to evaluate more features of the context and global information. More published citation recommendation models could have been covered for the evaluations. Similarly, a standard framework, such as a library or script, could have been created to evaluate the models. Different evaluation metrics such as nDCG, F1 score, and perplexity could also be used. Similarly, models could also be compared based on parameters such as processing time, CPU utilization, memory requirement, and GPU utilization."}, {"title": "6 CONCLUSION", "content": "In conclusion, a comprehensive benchmark for evaluating and comparing citation recommendation models is established. Utilizing the S2ORC and S2AG datasets, thorough data cleaning and preprocessing have been performed, followed by the construction of diagnostic datasets based on various citation context features. These datasets have facilitated a robust assessment of model performance through relevant evaluation metrics. Additionally, the evaluation of selected citation recommendation models has enabled a standardized analysis and comparison of their performance. This study provides valuable insights into the effectiveness of various approaches within the field of citation recommendation, highlighting key strengths and areas for improvement in current methodologies."}]}