{"title": "Strategic Fusion Optimizes Transformer Compression", "authors": ["Md Shoaibur Rahman"], "abstract": "This study investigates transformer model compression by systematically pruning its layers. We evaluated 14 pruning strategies across nine diverse datasets, including 12 strategies based on different signals obtained from layer activations, mutual information, gradients, weights, and attention. To address the limitations of single-signal strategies, we introduced two fusion strategies, linear regression and random forest, which combine individual strategies (i.e., strategic fusion), for more informed pruning decisions. Additionally, we applied knowledge distillation to mitigate any accuracy loss during layer pruning. Our results reveal that random forest strategic fusion outperforms individual strategies in seven out of nine datasets and achieves near-optimal performance in the other two. The distilled random forest surpasses the original accuracy in six datasets and mitigates accuracy drops in the remaining three. Knowledge distillation also improves the accuracy-to-size ratio by an average factor of 18.84 across all datasets. Supported by mathematical foundations and biological analogies, our findings suggest that strategically combining multiple signals can lead to efficient, high-performing transformer models for resource-constrained applications.", "sections": [{"title": "1. Introduction", "content": "Large pre-trained transformer models (Vaswani et al., 2023) have transformed natural language processing by achieving state-of-the-art performance across a wide range of tasks, from sentiment analysis to text classification (Rahman & Borera, 2024). However, their significant computational and memory requirements present a challenge for deployment in resource-constrained environments, such as edge devices or real-time applications (Strubell et al., 2019). Model compression has emerged as a crucial area of research to address these limitations, enabling efficient use of these powerful models without compromising their performance.\nPast studies explored various approaches for compressing large transformer models, including pruning unimportant weights (Han et al., 2015), quantizing parameters (Gong et al., 2014), and knowledge distillation (Hinton et al., 2015). Recent work also extends beyond individual parameters to prune entire layers considered less critical, using strategies such as activation-based (Ganguli & Chong, 2024), gradient-based (Molchanov et al., 2017; Yang et al., 2023), mutual information-based (Isik et al., 2022), weight-based (Frankle & Carbin, 2019), or attention-based (Michel et al., 2019) pruning. Although these techniques effectively reduce model size and computational costs, they often rely on single metrics to determine layer redundancy.\nHowever, focusing solely on a single signal, which often requires predefined pruning rules, may not fully capture the nuanced contributions of a layer to downstream tasks (Hooker et al., 2021; Zhang et al., 2022). Therefore, single signal-based pruning strategies often lead to drastic drops in accuracy. Crucially, existing works seldom explore how to combine multiple pruning signals or thoroughly examine how to sequence layer pruning decisions without a predefined rule. This gap underlines the need for a framework that accounts for multiple pruning signals and systematically assesses the importance of each layer to optimize performance trade-offs.\nIn this work, we examined 12 individual pruning strategies using signals from layer activations, mutual information, gradients, weights, and attention, and propose two fusion strategies to integrate these signals. We provided the mathematical and biological intuition behind the choice of each strategy, which demonstrates theoretical and practical perspectives on their selection. We tested our strategies using the BERT model (Devlin et al., 2019). We evaluated 14 pruning strategies across nine datasets, focusing on text classification and sentiment analysis. Each individual strategy computes a layer-specific metric or signal, identifies layers to prune based on a predefined rule (e.g., based on the min or max value of the metric), and fine-tunes the compressed model. The fusion strategies, based on linear regression and random forest, integrate multiple pruning signals to automatically identify optimal layer pruning schedules without a predefined rule. Finally, we incorporated knowledge distillation-based training, where the compressed model was trained using the original model as a teacher to recover any accuracy lost during pruning.\nOur experiments reveal that integrating multiple signals through strategic fusion consistently outperforms single-metric approaches in both accuracy improvement and model size reduction. In particular, random forest-based fusion strategy achieves the best performance in seven out of nine datasets, while ranking second and third best for the remaining two datasets. Furthermore, knowledge distillation exceeds the original accuracy for six datasets and mitigates the accuracy drops in three other datasets. The accuracy-to-size ratio after distillation increases by an average factor of 18.84 across all datasets. Our results also highlight that which layers are pruned and in what sequence matters greatly: edge layers often carry critical information, and high-performing strategies automatically learn not to prune them early. Taken together, our findings demonstrate that the fusion of individual strategies into a data-driven framework can lead to an effective and efficient compressed transformer model."}, {"title": "2. Methodology", "content": ""}, {"title": "2.1. Datasets", "content": "We employed nine diverse text classification datasets of varying domains (e.g., user reviews, scientific abstracts, news articles) and number of labels (2 to 20 classes): newsgroup, dbpedia-14 (dbpedia), arxiv-classification (arxiv), patent-classification (patent), yahoo_answers_topics (yahoo), yelp_review_full (yelp), ag_news (agnews), imdb, and amazon_polarity (amazon). All datasets except newsgroup are available via Hugging Face, whereas newsgroup is accessible through scikit-learn. All input sequences were tokenized using the BERT tokenizer and padded or truncated to a maximum length of 32 tokens for consistent processing across datasets."}, {"title": "2.2. Layer Pruning Strategies", "content": ""}, {"title": "2.2.1. ACTIVATION-BASED PRUNING", "content": "Activation-based pruning is a natural approach to identify redundant transformer layers due to the fundamental role activations play in neural network operations (Ganguli & Chong, 2024). Activations, measured as the output of neurons after applying nonlinear transformations, represent the input in a transformed feature space. Layers with specific activation patterns may contribute in various ways to the overall functionality of the network.\nFrom a mathematical perspective, activations can be viewed as mappings from the input space to a feature space, where their magnitude and distribution signify the importance of a layer. Layers exhibiting consistently low or sparse activations are hypothesized to contribute minimally to overall feature transformation. Biologically, this aligns with the idea that neurons or brain regions with persistently low firing rates play a negligible role in processing, further motivating the use of activations as a basis for pruning (Harvey et al., 2013).\nTo quantify activations, we used three strategies to aggregate them into different signals or metrics: inhibition, intensity, and energy. These metrics offer complementary insights into the importance of activations within a layer.\nInhibition: Inhibition measures the mean value of activations in a layer:\n$A_{inhibition} = \\frac{1}{n.d}\\sum_{i=1}^{n}\\sum_{j=1}^{d}A_{i,j}$"}, {"title": "Intensity", "content": "Intensity: Intensity measures the mean of absolute activations, capturing the L\u2081-norm:\n$A_{intensity} = \\frac{1}{n.d}\\sum_{i=1}^{n}\\sum_{j=1}^{d}|A_{i,j}|$\nIntensity reflects the magnitude of activation. Layers with low intensity often produce sparse activations, implying a limited influence on subsequent layers. Mathematically, low intensity reduces the transformation T(A) to bias terms:\nT(A) = WA + b\u2248 b\nBiologically, this aligns with the idea that neurons with weak signals contribute less to cognitive processing (Harvey et al., 2013). However, low-intensity layers can still encode selective and critical features, making pruning based solely on intensity occasionally misleading.\nEnergy: Energy measures the mean of squared values of activations, capturing the L2-norm:\n$A_{energy} = \\frac{1}{n.d}\\sum_{i=1}^{n}\\sum_{j=1}^{d}A_{i,j}^2$\nEnergy reflects the overall strength of the signal, with low energy suggesting that the layer has minimal influence on the network's computations. Energy is particularly useful because it magnifies larger activations while diminishing the impact of smaller ones. A low-energy layer produces outputs with minimal power, ||A||3 = Tr(ATA). This suggests that low-power layers do not contribute substantially to overall information propagation. However, pruning based on energy may overlook layers that generate weak but highly structured signals essential for specific tasks, analogous to how certain brain regions exhibit low energy usage while maintaining critical functions.\nActivation-based pruning methods offer compelling mathematical and biological rationales for identifying redundant layers. Inhibition, intensity, and energy provide diverse ways to quantify the contribution of activations. While these methods may succeed when activations are consistently low across various inputs, they may falter in cases where weak or sparse activations encode critical information."}, {"title": "2.2.2. MUTUAL INFORMATION-BASED PRUNING", "content": "Mutual information (MI) provides a rigorous framework for quantifying the dependence between variables, making it a natural candidate to evaluate the contribution of individual transformer layers (Isik et al., 2022). In the context of neural networks, MI captures how much information a layer's activations share with the target labels or adjacent layers. This enables principled pruning by identifying layers that contribute the least to task performance or exhibit high redundancy with neighboring layers.\nFrom a mathematical perspective, MI measures the reduction in uncertainty about one variable given the knowledge of another. In transformers, activations at a given layer encode a representation of the input, and MI evaluates how much of this representation is task-relevant or novel compared to adjacent layers. Biologically, this approach aligns with the brain's reliance on efficient information transfer across neural circuits, where regions with low mutual information with their outputs or neighboring regions likely perform redundant or less critical computations (Xu et al., 2023).\nTo quantify the information contributed by each layer, we used two strategies to aggregate them into different signals or metrics: Task-Relevance-MI that computes MI between a layer and the target, and Flow-Relevance-MI that computes MI flow between two consecutive layers. Each method offers unique insights into the contribution and redundancy of a layer based on shared information. Below, we provide mathematical definitions and analyze their implications for layer pruning.\nTask-Relevance-MI: This strategy measures the dependency between a layer's activations and the target labels and informs whether a layer's output contributes task-critical information for prediction. Mathematically, the MI for layer l is defined as:\n$MItask (1) = I(A_{l}; y)$\nwhere $A_{l} \u2208 R^{n\u00d7d}$ represents the activations of layer l, and y denotes the target labels. I(A\u2081; y) is computed as:\n$I(A_{l}; y) = \\sum_{i=1}^{n}log\\frac{p(a_{l,i}, Y_{i})}{p(a_{l,i})P(y_{i})}$\nwhere p(al,i, Yi) is the joint probability, and p(\u03b1\u03b9,i) and p(yi) are marginal distributions.\nBiologically, layers with high Task-Relevance-MI are analogous to brain regions specialized in processing task-relevant information (e.g. visual cortex for vision or somatosensory cortex for touch) (Rahman & Yau, 2019), while layers with low Task-Relevance-MI are considered redundant and contribute minimally to specific tasks. Therefore, a low $MItask (1)$ suggests that the layer provides little task-relevant information and may be a candidate for pruning.\nFlow-Relevance-MI: This strategy evaluates redundancy between adjacent layers, quantifying how much new information layer 1 + 1 introduces relative to layer l, and is defined as:\n$MIflow (1) = I(A_{l}; A_{l+1})$\nwhere $A_{l+1} \u2208 R^{n\u00d7d}$ represents the activations of the subsequent layer. Since the activations in the intermediate layers are typically continuous, the computation of $I(A_{l}; A_{l+1})$ can be estimated via the reduction in variance of At when conditioned on Al+1:\n$I(A_{l}; A_{l+1}) \u2248 Var(A_{l}) - Var(A_{l} A_{l+1})$\nBiologically, layers with high Flow-Relevance-MI are analogous to brain regions that efficiently transfer information between interconnected areas, enabling hierarchical processing (Felleman & Essen, 1991). In contrast, areas with low Flow-Relevance-MI are considered redundant and unnecessary (e.g., two nearly identical visual information processing circuits are not required and do not exist). Therefore, layers with low MIflow (1) may be candidates for pruning.\nIn both methods, MI allows for targeted pruning decisions by identifying layers with low task relevance or high redundancy. However, these methods assume that low MI directly correlates with redundancy, which might overlook layers that encode intermediate features essential for downstream processing."}, {"title": "2.2.3. GRADIENT-BASED PRUNING", "content": "Gradient-based pruning uses the magnitude and structure of gradients to assess the contribution of individual transformer layers (Yang et al., 2023; Molchanov et al., 2017). Gradients, which represent the sensitivity of the loss function with respect to the model parameters, provide a direct measure of how much each layer contributes to reducing the loss. By analyzing gradient information, we can identify layers that exert minimal influence on the model's optimization dynamics and are thus potential candidates for pruning.\nFrom a mathematical perspective, gradients quantify the change in the model's output or loss in response to small perturbations in its parameters. Layers with consistently low gradient magnitudes indicate that their parameters are less significant for the optimization process and contribute minimally to performance improvement. Biologically, this aligns with the concept of synaptic plasticity in the brain, where connections with low or negligible weight updates over time are considered less critical for learning and can be pruned to improve efficiency (Magee & Grienberger, 2020).\nTo apply gradient-based pruning, we used two strategies to aggregate gradients into different signals or metrics: Gradient Magnitude, which directly computes the magnitude the gradients, and Gradient Fisher Information, which computes the variance of the derivative of the loss. Below, we provide mathematical definitions and analyze their implications for layer pruning.\nGradient Magnitude: This strategy computes the mean magnitude of the gradients for each layer, measuring the overall contribution of the layer to loss reduction. For a given layer l, the gradient magnitude is defined as:\n$G_{magnitude} (1) = \\frac{1}{||\u03b8_{l}||}\\sum_{\\theta_{l}\\epsilon \u03b8_{l}}|\\frac{\u2202L}{\u2202\u03b8_{l}}|$\nwhere \u03b8\u03b9 is the set of parameters in layer l, L is the loss function, and is the gradient of the loss with respect to parameter 0. Layers with low Gmagnitude (1) suggest that their parameters contribute negligibly to reducing the loss, i.e., changing their parameters do not affect the loss much. So, those layers can be pruned.\nFisher Information: This strategy computed the expected change in the loss function when parameters are perturbed, providing a second-order measure of parameter importance. For a layer l, the Fisher information is defined as:\n$F(l) = E_{(x,y)~D}[(\\frac{\u2202}{\u2202\u03b8_{l}}\\frac{\u2202}{\u2202\u03b8_{l}})L(x, y; \u03b8)]$\nwhere D is the data distribution, \u03b8\u03b9 represents the parameters of layer l, and  is the gradient of the loss with respect to \u03b8\u03b9. Fisher information highlights parameters or layers that are critical for maintaining the current loss minimum. Layers with low Fisher information imply that perturbing their parameters minimally affects the loss, making them redundant.\nGradient-based pruning strategies offer a direct measure of layer importance by evaluating their influence on loss optimization. Gradient magnitude provides an intuitive first-order measure, while Fisher information captures second-order effects, offering deeper insights into parameter significance. However, both methods rely on the assumption that low-gradient magnitudes or Fisher information correlate directly with redundancy. In practice, layers with low gradients might still play stabilizing roles, analogous to brain regions that act as modulatory hubs with minimal direct activity but essential indirect contributions."}, {"title": "2.2.4. WEIGHT-BASED PRUNING", "content": "Weight-based pruning is another natural candidate to identify redundant transformer layers due to the foundational role weights play in defining layer transformations (Frankle & Carbin, 2019). In neural networks, weights parameterize the linear mappings that transform inputs into feature representations, directly affecting the layer's contribution to the overall model. Layers with weak, sparse, or low-entropy weights are less likely to provide significant transformations, making them prime candidates for pruning without significantly impairing performance.\nFrom a mathematical perspective, the weights $W\u2208 R^{dout\u00d7din}$ define the transformations within a layer, where din and dout represent the input and output dimensions. The properties of the weight matrix, such as its norm, sparsity, and entropy, provide key insights into the importance of a layer's contribution. Biologically, this aligns with the synaptic pruning mechanisms of the brain, where weak or redundant connections are systematically removed to optimize information processing (Paolicelli et al., 2011).\nTo apply weight-based pruning, we used three strategies to aggregate the weights into different signals or metrics: norm, sparsity, and entropy. These metrics provide various perspectives into the importance of weights within a layer. Below, each method is mathematically defined and analyzed in terms of its implications and effectiveness for layer pruning.\nNorm: This strategy computes the L2-norm of the weight matrix in a layer, quantifying the overall magnitude of its parameters and capturing the strength of the transformation:\n$||W||_{2} = \\sqrt{\\sum_{i=1}^{dout} \\sum_{j=1}^{din} W_{ij}^2}$\nA low norm indicates that the layer's weights, W, are close to zero, suggesting minimal contribution to the model's transformation. Mathematically, this implies that the layer's output is approximated primarily by its bias term:\nT(A) = WA + b\u2248 b\nwhere T(A) is the transformed output of the layer, and A is the input activation from the previous layer. Layers with low-weight norms are considered redundant as they exert negligible influence on downstream computations. Consequently, such layers can be interpreted as weak connections that add little to the model's overall functionality, making them strong candidates for pruning.\nSparsity: This strategy measures the proportion of zero-valued elements in the weight matrix:\n$S(W) = \\frac{\\sum[W_{i,j} = 0]}{dout din}$\nwhere [] is the indicator function. A high sparsity value implies that most of the weights in the layer are zero. This implies that the output of the layer is approximated by the bias term as shown in Equation (12).\nEntropy: This strategy measures the diversity in the weight distribution, reflecting the information content encoded by the weights. Mathematically,\n$H(W) = - \\sum_{i=1}^{dout} \\sum_{j=1}^{din} \\frac{W_{ij}}{||W||_{1}} log \\frac{W_{ij}}{||W||_{1}}$\nwhere ||W||1 = \u2211i,j |Wi,j| is the l\u2081-norm of W, and e is a small constant to avoid numerical instability. Layers with low entropy exhibit a highly concentrated weight distribution, dominated by a few large weights. Such layers may provide limited diversity in transformations, making them potential candidates for pruning. The weight entropy is analogous to the diversity of neural activation patterns in the brain. Circuits with concentrated activity are less efficient for generalizable tasks, whereas distributed activity allows for richer information processing.\nWeight-based pruning strategies offer a mathematically sound and biologically inspired framework for identifying redundant layers. Although norm, sparsity, and entropy provide distinct insights into the significance of weights, pruning layers based on these metrics can fail if the underlying assumptions, i.e. low norm indicating low importance, high sparsity implying irrelevance, or low entropy signaling redundancy, do not accurately reflect the actual role of a layer in the network."}, {"title": "2.2.5. ATTENTION-BASED PRUNING", "content": "Attention-based pruning uses the fundamental role of attention mechanisms in transformers to identify and remove redundant layers (Michel et al., 2019). Attention weights indicate how tokens influence each other during the generation of contextual representations. Layers whose attention weights are uniformly distributed or consistently low are unlikely to capture meaningful token interactions, making them prime candidates for pruning with minimal impact on overall performance.\nFrom a mathematical perspective, the attention weights a \u2208 Rnxnxh quantify the influence of one token on another across h attention heads and n tokens. The properties of attention, such as their mean importance and entropy, provide insights into the relevance of a layer's attention mechanism. Biologically, this aligns with the concept of selective attention in neural circuits, where the brain prioritizes specific stimuli while suppressing others, ensuring efficient processing (Convento et al., 2018). Similarly, layers with ineffective or redundant attention mechanisms in transformers can be pruned to optimize the model structure.\nTo quantify attention, we used two strategies to aggregate them into different signals or metrics: attention weight and attention entropy. These metrics offer complementary perspectives on the importance of attention mechanisms in a layer. Below, each method is mathematically defined and analyzed in terms of its implications and effectiveness for layer pruning.\nAttention Weight: Attention weight measures the average magnitude of the attention scores across all tokens and heads within a layer:\n$A_{weight} = \\frac{1}{n^{2}.h}\\sum_{i=1}^{h}\\sum_{j=1}^{n}\\sum_{k=1}^{n}a_{i,j,k}$\nwhere ai,j,k represents the attention score from token i to token j in head k. A low attention weight suggests that the layer's attention mechanism assigns uniformly low importance across all tokens, indicating that the layer minimally influences the contextual representations. Mathematically, this implies that the layer contributes little to the model's ability to distinguish between relevant and irrelevant input tokens. Thus, such layers are strong candidates for pruning.\nAttention Entropy: Attention entropy quantifies the diversity and concentration of attention scores, capturing the degree to which attention is focused or distributed:\n$A_{entropy} = \\sum_{k=1}^{h}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i,j,k}log (a_{i,j,k} + 6)$\nwhere e is a small constant to prevent numerical instability. High entropy indicates that attention is evenly distributed across tokens, suggesting a lack of focus, while low entropy indicates concentrated attention on specific tokens. Biologically, this mirrors the brain's ability to focus selectively on critical stimuli while maintaining enough diversity to generalize across contexts. Thus, pruning high-entropy layers assumes that distributed attention contributes less to task-specific information flow.\nAttention-based pruning methods provide a biologically plausible and mathematically rigorous approach to identifying redundant transformer layers. Attention weight and entropy offer complementary metrics for assessing layer relevance, with weight reflecting the overall magnitude of token interactions and entropy capturing their diversity. However, these methods may fail when uniformly low or distributed attention scores encode subtle but essential dependencies."}, {"title": "2.2.6. STRATEGIC FUSION", "content": "Strategic fusion pruning methods combine individual strategies to make informed layer-pruning decisions. For a transformer model with l layers, each layer is represented by a set of m layer-specific signals. These signals form a feature matrix X \u2208 Rl\u00d7m, where each row x\u2081 \u2208 Rm corresponds to the metrics of a specific layer l. We obtain 12 signals from 12 strategies for each layer, and therefore, m = 12.\nThe importance of each layer is quantified by the target variable \u2206\u0391 \u2208 R\u00b9, where \u0394\u0391\u012b represents the change in accuracy when a specific layer l is pruned. Formally:\n\u0394\u0391\u03b9 = Aorig \u2013 \u0391\u03b9\nwhere Aorig is the accuracy of the original model and At is the accuracy after pruning layer l. A smaller AA indicates that pruning the layer has a minimal impact on performance, making it a candidate for removal.\nWe introduced two independent fusion methods, linear regression and random forest. Both methods use the feature matrix X \u2208 Rlxm and the corresponding accuracy change vector \u2206\u0391 \u2208 R\u00b9 to predict the pruning impact of each layer. These methods differ in their underlying assumptions and in the way they model relationships between strategies. During the iterative pruning process, both methods identify the layer l* with the lowest predicted impact:\nl* = arg min \u2206\u0391\u03b9\nThis layer is then pruned, and the model is fine-tuned to adapt to the structural change. The process is repeated until a desired number of layers is pruned.\nLinear Regression-Based Pruning: Linear regression assumes a linear relationship between the feature signals and the impacts, i.e. it measures impacts as a linear weighted combination of strategies. In this method, the target variable remains \u2206\u0391 \u2208 R\u00b9, while the feature space X \u2208 Rlxm includes the same layer-specific signals. Linear regression model predicts pruning impact as:\n\u0394A\u2081 \u2248 wx\u2081 + b\nwhere w \u2208 Rm are the learned weights indicating the significance of each metric, and b is the bias.\nRandom Forest-Based Pruning: Random forest pruning provides a nonlinear way to quantify the importance of layers. Unlike linear regression, random forests capture complex relationships through an ensemble of decision trees. Each tree is trained on a random subset of the data, and the overall model aggregates predictions. In this method, the target variable remains \u2206A \u2208 R\u00b9, while the feature space X \u2208 Rlxm includes the same layer-specific signals. The random forest model predicts pruning impact as:\n\u0394\u0391\u03b9 \u2248 RF(x1)\nwhere RF(x1) represents the aggregated prediction from the ensemble. The model provides feature importance scores, which are analyzed to understand the relative contribution of individual strategies in the random forest fusion.\nIn both methods, the layer with the least predicted impact is pruned at each iteration, as guided by Equation (18). The process continues until a target number of layers is pruned.\nLinear regression and random forest are independent approaches for fusion-based pruning. Linear regression offers simplicity and mathematical clarity by assuming linear relationships, while random forest accounts for non-linear interactions, capturing more complex dependencies. Biologically, strategic fusion mirrors how different brain regions process different aspects of input at varying levels of complexity, collectively contributing to the final decision (Mesulam, 1998; Rahman et al., 2020). This emphasizes the importance of integrating multiple strategies for robust and informed layer pruning."}, {"title": "2.2.7. RANDOM PRUNING", "content": "For comparison, we include a simple random pruning baseline in which each layer is selected uniformly at random for removal at each step, independent of any learned signals or metrics. This process is repeated until the desired number of layers has been pruned. To further confirm that an informed pruning sequence is critical for achieving optimal performance, we also repeated random pruning experiments on different dataset subsets, demonstrating that purely random selection consistently underperforms methods informed by layer-specific signals."}, {"title": "2.3. Knowledge Distillation", "content": "Layer pruning often leads to performance drop, e.g. accuracy, in the compressed model. To mitigate the accuracy drop after aggressive pruning, we used a knowledge distillation approach. We used the original (uncompressed) model as the teacher and the pruned model as the student. During training, the teacher produces soft probability distributions over classes for each input sample. The student model is then trained to mimic the teacher's output distribution through a Kullback-Leibler divergence loss. Formally, let z\u0142 and zs be the logits of the teacher and student, respectively. We define the distillation loss LKD for a batch of size N as:\n$L_{KD} = \\frac{1}{N}\\sum_{i=1}^{N}KL(\u03c3(z_{s}^{i}/T) || \u03c3(z_{t}^{i}/T))$\nwhere \u03c3(\u00b7) denotes the softmax function and T denotes the temperature that determines the smoothness of the output distribution. We then combine LKD with the standard cross-entropy loss LCE, computed using ground-truth labels, to form the overall training objective:\nL = a LCE + (1 \u2212 a) LKD\nwhere a controls the trade-off between adhering to the original labels and matching the soft output of the teacher. Empirically, this joint objective helps the student model absorb nuanced decision boundaries from the teacher, thereby recovering or even surpassing the accuracy lost through pruning. All distillation procedures follow the same training protocols used for fine-tuning, including learning rates, batch sizes, and optimizer settings, thus minimizing additional hyperparameter overhead. We used T 2 and a 0.5 in our experiments."}, {"title": "2.4. Layer Pruning and Model Training", "content": "Layer pruning involves removing specific transformer layers of a model to reduce its size and computational complexity while preserving performance. An identity wrapper is used to replace pruned layers in the model by acting as a placeholder. The wrapper is a module that simply passes the input through without performing any computations or transformations. This ensures that the overall architecture of the model remains intact and simplifies the implementation during the pruning process.\nWe used sequential pruning, which is an iterative approach that removes one layer at a time based on its importance level. The importance of a layer is determined by various signals generated by the layer. At each step, the importance of all layers is computed and the least important layer is removed. The pruned model is then fine-tuned with a small subset of training data and evaluated on the test data. The process is repeated until a desired number of layers is pruned. The same train and test subsets are used to fine-tune both the base model and the knowledge-distilled model. The fine-tuning process is identical for all models."}, {"title": "3. Results", "content": "We investigated 14 distinct layer pruning strategies, organized into six categories, activation, mutual information, gradient, weight, attention, and strategic fusion, alongside a random pruning baseline. We evaluated each strategy using nine datasets. This section presents how each strategy reduces model size while retaining (or even surpassing) performance, and highlights the advantages of integrating multiple layer-specific signals in a unified pruning framework."}, {"title": "3.1. Strategic Fusion Optimizes Model Compression", "content": "Figure 1 illustrates the accuracy trends as the model is sequentially compressed by pruning layers, one at a time, across nine datasets. For the newsgroup dataset, the trends derived from strategic fusion methods (Random Forest and Linear Regression) are notably distinct from the others. Other well-performing methods include Weight-Entropy, Task-Relevance-MI, and in some datasets, Gradient-Fisher. Although the performance of different strategies varies across the datasets, the general trends of high-performing strategies remain consistent.\nThe trends in Figure 1 provide a comprehensive view of how each strategy performs as the layers are pruned sequentially across the datasets. However, the ultimate goal is to identify a model, with a specific number of pruned layers, that performs optimally. To this end, we extracted the maximum accuracy achieved by each strategy for each dataset, as summarized in Table 1. The results reveal that the strategic fusion with the random forest performs best for seven out of nine datasets, while the strategic fusion with linear regression leads for five datasets. Other notable methods, such as Task-Relevance-MI, Gradient-Fisher, and Attention-Weight, also perform well in specific datasets. Interestingly, the highest accuracies achieved by these individual strategies are also obtained through the strategic fusion methods, with the exception of Gradient-Fisher for one dataset. This indicates that strategic fusion-based layer pruning consistently outperforms individual strategy-based pruning, highlighting the importance of considering interactions between layer-specific metrics for informed pruning decisions. Moreover, the superior performance of the random forest-based fusion suggests that nonlinear interactions between these metrics are more prevalent and impactful than linear ones.\nSo far, we have focused on maximum accuracy as a measure of effectiveness for the best-performing model on each dataset. However, relying solely on maximum accuracy overlooks three critical factors. First, strategies that yield accuracies close to the maximum, such as the second or third best, can also be effective and merit consideration, as small differences in accuracy may not translate into meaningful performance differences. Second, this approach ignores the accuracy drops from the baseline model, which are crucial for assessing the feasibility of pruning. Ideally, pruning should result in a negligible or no accuracy drop; substantial drops could make pruning unsuitable. Third, some strategies might achieve slightly lower accuracy but with more layers pruned, resulting in smaller models, a trade-off that is essential for many resource-constrained applications. Addressing these factors allows for a more nuanced evaluation of the strategies and their practical effectiveness.\nWe addressed the first factor by computing the rank of each strategy for each dataset. For a given dataset, the strategies are ranked based on maximum accuracy and the rank (index + 1) is assigned to each strategy. These ranks, displayed in Figure 2, reveal that the random forest consistently achieves high ranks in all datasets, followed by linear regression. Both methods are clearly distinguishable from other individual strategies. Although certain datasets rank individual strategies higher, such as Gradient-Fisher, Task-Relevance-MI, Weight-Entropy, and Attention-Weight, most datasets rank these independent strategies lower overall."}, {"title": "3.2. Knowledge Distillation Mitigates Accuracy Drops", "content": "We have established that strategic fusion-based layer pruning, specifically with Random-Forest, is an effective compression technique. However, as with other compression methods, this approach often reduces the accuracy compared to the original model, with some rare exceptions. One way to mitigate this accuracy drop is through knowledge distillation, where the compressed model (student) is trained to mimic the predictions of the original model (teacher). During this process, the teacher model provides \"soft labels\" (probabilistic outputs) as guidance, which helps the student model learn finer-grained information about the data distribution beyond hard labels."}, {"title": "3.3. Why Does Strategic Fusion Outperform Individual Strategies?", "content": "In this section", "1": "and average them across datasets as shown in Figure 6. We observe that Task-Relevance-MI and Gradient-Fisher are dominant in the linear regression fusion", "have": "n$A_{entropy"}, "frac{1}{h}\\sum_{k=1}^{h} \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{n}\\sum_{j=1}^{n} a_{i,j,k} log(a_{i,j,k})$\nwhere ai,j,k is the attention score from token i to token j for head k. For each specific i and k:\n$A_{i,k}^{entropy} = -\\sum_{j=1}^{n}a_{i,j,k} log(a_{i,j,k})$\nHigh entropy arises when attention is evenly distributed (Ai,j,k = 1/n), giving Aentropy = logn. Low entropy arises when attention is entirely focused on a single token (Ai,j*,k = 1 and Oi,j,k = 0 for j \u2260 j\u2217), giving Aentropy = 0. Accordingly, the attention output:\n$Z_{i,k} = \\sum_{j=1}^{n}a_{i,j,k} V_{j,k}$\nwhere Vj,k is the value vector for token j in head k, becomes:\n$Z_{i,k} = \\frac{1}{n}\\sum_{j=1}^{n}V_{j,k}$ (for high entropy)\nand\n$Z_{"]}