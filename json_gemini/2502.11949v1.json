{"title": "Massively Scaling Explicit Policy-conditioned Value Functions", "authors": ["Nico Bohlinger", "Jan Peters"], "abstract": "We introduce a scaling strategy for Explicit Policy-Conditioned Value Functions (EPVFs) that significantly improves performance on challenging continuous-control tasks. EPVFs learn a value function V (0) that is explicitly conditioned on the policy parameters, enabling direct gradient-based updates to the parameters of any policy. However, EPVFs at scale struggle with unrestricted parameter growth and efficient exploration in the policy parameter space. To address these issues, we utilize massive parallelization with GPU-based simulators, big batch sizes, weight clipping and scaled peturbations. Our results show that EPVFs can be scaled to solve complex tasks, such as a custom Ant environment, and can compete with state-of-the-art Deep Reinforcement Learning (DRL) baselines like Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). We further explore action-based policy parameter representations from previous work and specialized neural network architectures to efficiently handle weight-space features, which have not been used in the context of DRL before.", "sections": [{"title": "1 Introduction", "content": "The remarkable success of deep learning in recent years is closely linked to the concept of scaling. In particular, the scaling of neural networks, datasets, and computational resources has led to significant advances in various domains such as computer vision and natural language processing. Hardware improvements for GPUs and TPUs and parallelization techniques like data and model parallelism have enabled the training of large-scale models on massive datasets. The relationship between scaling and improved performance has been formalized into the concept of scaling laws [1, 2].\nWhile the benefits of scaling are well-established in supervised learning, its application to Deep Reinforcement Learning (DRL) is less understood. Naively increasing the size of policy and value function networks and using bigger batch sizes often leads to diminishing returns or even performance degradation [3]. However, recent work has shown that carefully designing the neural network architecture by incorporating normalization layers, like LayerNorm, BatchNorm or WeightNorm, and residual connections can help to scale up DRL algorithms and improve their performance with bigger networks [4, 5, 6, 7]. On-policy algorithms like Proximal Policy Optimization (PPO) have been shown to greatly benefit from bigger batch sizes, which can be collected efficiently using up to thousands of parallel environments [8].\nIn this work, we investigate the scaling capabilities of different variations of Explicit Policy-conditioned Value Functions (EPVFs) with the help of massively parallel environments. While EPVFs have previously struggled on complex tasks, we show the importance of scaling and weight regularization for the training of EPVFs and compare different neural network architectures and training setups on a MuJoCo Ant and Cartpole environment."}, {"title": "2 Explicit Policy-conditioned Value Functions", "content": "In Reinforcement Learning (RL), we consider a Markov Decision Process (MDP) defined by a tuple M = (S, A, P, R, \u03b3, \u03c10), where S is the state space, A is the action space, P is the transition dynamics, R is the reward function, \u03b3 is the discount factor, and \u03c10 is the initial state distribution. The goal of an RL agent is to learn a policy \u03c0\u03b8(a|s) that is parameterized by \u03b8. Rolling out the policy in the environment generates a trajectory \u03c4 = (s0, a0, r0, . . .), where s0 is sam pled from \u03c10 and at \u223c \u03c0\u03b8(\u00b7|st). The return Rt is defined as the sum of discounted rewards $R_t = \\sum_{k=0}^{T \u2212t\u22121} \u03b3^k r_{t+k+1}$, where T is the time horizon. The policy is trained to maximize the expected return J(\u03c0\u03b8) = E\u03c4\u223c\u03c0\u03b8,s0\u223c\u03c10 [R0]. The state-value function for the policy is defined as $V^{\u03c0_\u03b8}(s) = E_{\u03c0_\u03b8}[R_t|s_t = s]$, with which we can re-formulate the policies objective as\n$J(\u03c0_\u03b8) = \\int_{S} \u03c1_0(s)V^{\u03c0_\u03b8}(s) ds$.(1)\nMany RL algorithms use the action-value function Q\u03c0\u03b8 (s, a) = E\u03c0\u03b8 [Rt|st = s, at = a] to decompose the state-value func tion in the policy objective into $V^{\u03c0_\u03b8}(s) = \\int_{A} \u03c0_\u03b8(a|s)Q^{\u03c0_\u03b8}(s, a) da$ to get the gradient w.r.t. \u03b8 to optimize the policy. EPVFs take a different approach by learning a value function that is explicitly conditioned on policy parameters or some other differentiable representation of the policy V (\u03b8), to directly optimize the gradient of the policy objective [9, 10, 11]:\n$\u2207_\u03b8J(\u03c0_\u03b8) = \\int_{S} \u03c1_0(s)\u2207_\u03b8V (s, \u03b8) ds = E_{s\u223c\u03c1_0}[\u2207_\u03b8V (s, \u03b8)] = \u2207_\u03b8V (\u03b8)$.(2)\nThe value function V (\u03b8) is learned using a replay buffer of policy parameters and returns, which can be collected by any policy, and updated using stochastic gradient descent. Because the value function predicts the performance of a policy from start to finish and we consider the episodic setting, the target is the undiscounted return. The policy is then updated by following the gradient of the value function with respect to the policy parameters. The resulting algorithm is presented in Algorithm 1."}, {"title": "3 Experiments", "content": "First, we evaluate the performance of EPVFs on the Gymnasium Cartpole environment [15]. From previous work, we know that EPVFs can fully solve simple tasks like Cartpole without additional scaling, i.e., using a small batch size of 16, a replay buffer of size 1e5, a two layer Multilayer Perceptron (MLP) with 64 neurons for the deterministic policy, and only a single environment [10]. For choosing the policy parameters during rollout, we follow Faccio et al. [10] and simply use the current best policy parameters and perturb them with Gaussian noise N (\u03bc = 0, \u03c3 = 1.0). We compare the single environment setting with using up to 16 parallel environments, where the policy parameters for every environment are perturbed with a different sample of Gaussian noise, i.e. we use as many different policies as environments. Figure 1 shows that scaling the number of environments, and therefore the number of different policies, improves convergence speed, while the perturbed policies in all settings eventually reach the same maximum return of 500. As the Cartpole task is not particularly challenging, we observe diminishing returns with more than 8 environments.\nNext, we evaluate the performance of EPVFs when scaling to massively parallel environments on a custom Ant environ- ment. For the physics simulation, we us MJX, which is a highly parallelizable GPU-based version of MuJoCo [16] based on JAX [17]. Our Ant environment is a continuous control task with a 34-dimensional state space and a 8-dimensional action space. In this task, the Ant's objective is to walk forward at a target velocity of 2 m/s. The time horizon is set to 1000 steps and the reward is calculated as $r = exp(-|V_{xy} - C_{xy}|^2/0.25)$, where $V_{zy}$ is the linear velocity of the Ant and $C_{xy}$ is the target velocity (2,0). This results in a maximum possible return of 1000. We setup the learning environment and algorithm in the DRL framework RL-X [18]. We can jit-compile the full training loop, enabling up to 4096 parallel environments on a single RTX 3090 GPU. This setup allows for extremely fast data collection and throughput of up to 3 million environment steps per second."}, {"title": "4 Conclusion", "content": "We have shown that EPVFs can be scaled to solve complex continuous control tasks and compete with state-of-the-art DRL baselines with the help of massively scaling up the number of parallel environments and the batch size. Key to stability is the use of weight clipping, to restrict the policy parameter space and prevent the parameters from constantly growing, and the use of uniform noise scaled to the magnitude of the parameters for exploring the policy parameter space efficiently. Further investigation on weight-space features from specialized architectures like UNFs might enable even better scaling capabilities for EPVFs in the future."}]}