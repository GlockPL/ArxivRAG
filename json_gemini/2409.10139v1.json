{"title": "TOWARDS EXPLAINABLE AUTOMATED DATA QUALITY ENHANCEMENT WITHOUT DOMAIN KNOWLEDGE", "authors": ["Djibril Sarr"], "abstract": "In the era of big data, ensuring the quality of datasets has become increasingly crucial across various domains. We propose a comprehensive framework designed to automatically assess and rectify data quality issues in any given dataset, regardless of its specific content, focusing on both textual and numerical data. Our primary objective is to address three fundamental types of defects: absence, redundancy, and incoherence. At the heart of our approach lies a rigorous demand for both explainability and interpretability, ensuring that the rationale behind the identification and correction of data anomalies is transparent and understandable. To achieve this, we adopt a hybrid approach that integrates statistical methods with machine learning algorithms. Indeed, by leveraging statistical techniques alongside machine learning, we strike a balance between accuracy and explainability, enabling users to trust and comprehend the assessment process. Acknowledging the challenges associated with automating the data quality assessment process, particularly in terms of time efficiency and accuracy, we adopt a pragmatic strategy, employing resource-intensive algorithms only when necessary, while favoring simpler, more efficient solutions whenever possible. Through a practical analysis conducted on a publicly provided dataset, we illustrate the challenges that arise when trying to enhance data quality while keeping explainability. We demonstrate the effectiveness of our approach in detecting and rectifying missing values, duplicates and typographical errors as well as the challenges remaining to be addressed to achieve similar accuracy on statistical outliers and logic errors under the constraints set in our work.", "sections": [{"title": "1 Introduction", "content": "Data assumes an escalating significance in contemporary business operations. Across diverse sectors such as commerce, entertainment, medicine, and the oil industry, data emerges as a potent catalyst for augmenting business efficacy (Marr, 2016). An abundant reservoir of qualitative data, coupled with Artificial Intelligence (AI), statistical methodologies, and probabilistic approaches, facilitates the discernment of intricate behavioral patterns or associations that would be arduous for humans to discern. However, the efficacy of these methodologies depends on the availability of high-quality data, often necessitating voluminous datasets. Notably, the preparatory phase, which entails rendering data compatible with analytical algorithms, has long been estimated to consume an important part of the process of exploiting the data. Two decades prior, Pyle (1999) already estimated the proportion of time spent to be over 50% of the overall processing time. Presently, notwithstanding the advancements in analytical techniques, the increasing volume and intricacy of data sources have exacerbated the challenges associated with data preparation (Garc\u00eda et al., 2016). This critical aspect, known as data preprocessing, encompasses a suite of techniques executed prior to the extraction of actionable insights from the data, constituting a pivotal facet of data exploitation. Due to the inherent anomalies in data, initiating data mining endeavors directly is impractical without addressing inconsistencies and redundancies. Furthermore, the exponential surge in data generation rates across multiple sectors necessitates the adoption of increasingly sophisticated analytical mechanisms (Han et al., 2022; Zaki and Meira, 2014). In light of constraints pertaining to time and human resources, there exists a pressing imperative to automate and streamline this preparatory phase. Central to this process is the identification and rectification of erroneous data values. They constitute a crucial component of the data cleansing process (refer to, for instance, Section 3.2 of Han et al. (2012)).\nFor the reasons outlined above, it is imperative for practitioners to possess efficient tools for measuring and enhancing data quality. This study focuses on optimal strategies for providing automated tools for this phase. We also assert that without explainability and interpretability, the effectiveness of data quality assessment algorithms is compromised, as users may lack confidence in the reliability of the results. For instance, if an entry is deemed invalid due to the detection of a statistical outlier on a specific line of a data table, but the problematic field(s) cannot be precisely identified by the algorithm, it becomes challenging to fully rely on the algorithm for making informed decisions. We propose a framework comprising five major steps that can automatically identify defects in a dataset without prior knowledge of its contents. This is accomplished while ensuring the ability to explain and justify each conclusion reached by our framework. This feature enables the framework, which consists of a series of algorithms, to provide native corrections for all errors identified in a given dataset. Returning to the earlier example, the ideal framework should not only identify the presence of a statistical outlier but also pinpoint the problematic field and propose a correction. Our framework is applicable to any table containing numerical values, text, or both. It addresses three types of defects that can affect data: absence, redundancy, and inconsistency. The first two steps of the five-steps algorithm we introduce deal with the first two types of errors and the last three steps all handle inconsistencies as we consider statistical outliers, typographical errors and logic errors. While the first two defects may be self-explanatory, indicating respectively missing and duplicated values, the third is less straightforward. In our framework, inconsistencies can impact any type of observation, whether numerical or textual, occurring when an input in a given field appears abnormal relative to others in the same field. While Artificial Intelligence (AI)-based techniques may offer potentially superior performance, they often lack the transparency necessary for thorough understanding and validation of results. Therefore, the algorithms within our comprehensive framework attempt to combine AI-based techniques and statistical results to identify and correct defects in given datasets. This characteristic, specifically its outcome of being able to explain all results, is the main distinction between our work and existing literature.\nThe literature contains several algorithms for analyzing the validity of a data set. As in this work, many of them leverage Machine-Learning (ML), Deep-Learning (DL) and statistical methods. Considering specifically the handling of missing values, they do not constitute per se a tough challenge as they can be identified in pretty straightforward ways. However, in the process of handling them, two challenges arise. The first one is analyzing the rightfulness of the missing data. Indeed, a missing value does not necessarily designate a defect in the dataset as it can, in various situations, be a legitimate and information-providing entry. As developed in Sainani (2015), the most straightforward and commonly employed approach to managing missing data involves excluding incomplete observations entirely from the analysis. That solution, while very easily applicable, is drastic. As already stated, it might delete the valuable information provided by the missing value, but it obviously also discriminates all the other valid entries. The author also presents the very common solution, which is to impute the missing value with a representative value of the field. It can be the average value (for instance, if it is heights of people), it can be a local average, for instance the weight of the males in a specific study, and so on and so forth. Other researchers have instead used ML and DL to handle missing values. For instance, Savarimuthu and Karesiddaiah (2021) imputation method on time series uses an iterative imputation algorithm that clusters univariate time series data, taking into account the data's trend, seasonality, and cyclical patterns. Then within the cluster, the authors employ a similarity-based nearest neighbor imputation method within each cluster to fill in missing values. More recently, Kachuee et al. (2020) used a generator network to generate imputations that a discriminator network is tasked to distinguish; this method allows also to properly estimate the distribution of the targets. While methods using AI are shown by their authors to be very efficient, they hardly provide the level of explainability that we aim for. For these reasons, in section 4.2.2, we will only use a classic imputation method for handling missing numerical values, ensuring explainability and interpretability. However, for character entries, when addressing logic errors in Subsection 4.2.5, we will employ ML algorithms for imputation while maintaining transparency. Our strategy will guarantee that legitimate missing values will not be discriminated, thanks to the subsequent application of the logic error detection algorithm. Our approach also guarantees that every choice will be explainable and interpretable.\nResearch has also explored the combination of neural networks and statistics specifically for outlier detection. For example, Small et al. Dai et al. (2018) trained a neural network to replicate a specific field within their dataset to identify outliers. They compared the neural network's output with the actual values and employed statistical indicators similar to those discussed in Subection 4.2.3. The authors calculated the difference between the output and actual values of the"}, {"title": "2 Definitions", "content": "We begin by presenting key definitions that will guide the selection of algorithms and shape the focus of the framework that we will introduce. Specifically, we will define what we will consider in this work to be valid data and we will provide a contextualised definition of explainability and interpretability. It is worth emphasizing that these definitions might not achieve unanimous agreement in the literature, as they do not always capture clear and universally objective concepts.\nIn what follows, a data set D is defined as a finite collection of n records {r1,r2, ...,rn}, where each record ri is a tuple consisting of m attributes. Formally, we can represent the data set as:\nD = {r1,r2,...,rn},\nwhere ri = (ai1, Ai2, ..., aim) for i = 1, 2, . . ., n. Each attribute aij is an element of a predefined attribute domain Aj. This formalization allows us to consider a data set as a structured collection of multi-dimensional data points, where the dimensionality is determined by the number of attributes m. Note that, any element aij can be a numerical value as well as a text value."}, {"title": "2.1 Valid data", "content": "In the literature, data quality is frequently characterized by the presence or absence of defects. According to this perspective, data is considered to be of high quality if it is devoid of such defects or if their impact is minimal. This conceptualization can be seen for instance in the work of Schelter et al. (2018) and Corrales et al. (2018). We will also adhere to this approach in this paper. It is also important to note that throughout this document, the data is not defined explicitly when unambiguous. It encompasses both individual observations which corresponds to a specific column of a row within a data array and objects of dimensions greater than one denoted as p > 1. They refer to a row or subsets thereof within a data array. Also, obviously the observations can be text as well as numerical values. Let us now define the defaults that will form the basis of the framework.\n1. Absence:\nThis refers to the absence of data or its constituent elements, which can arise during the data collection process due to technical failure, error, or insufficient information (Aydilek and Arslan, 2013). In such instances, the data is deemed invalid.\n2. Redundancy:\nData (where p > 1) that is replicated across multiple entries is regarded as invalid and is often denoted as duplicates in the literature.\n3. Inconsistency:\nWe define inconsistency as the lack of agreement between a data item and other observed data. Consequently, it encompasses several types of defects:\n(a) Statistical outliers: These are data points that are significantly and unjustifiably distant from other observations. An abnormal value may result from inconsistency or aberrant behavior during the measurement process (Barnett and Lewis, 1994; Johnson and Wichern, 2014).\n(b) Typographical errors: These errors encompass all text-related issues, including typing errors (e.g., \"France\" vs. \"Frange\") and formatting discrepancies (e.g., \"Citro\u00ebn\" vs. \"Citro\u00c3?n\").\n(c) Logical errors: These errors, more challenging to define, denote incompatibilities between different variables (columns) within a dataset.\nData will, therefore, be considered valid only when it exhibits none of these defects. The forthcoming framework will be crafted to detect and address these varied defaults. It will strive to rectify such anomalies where feasible, all the while prioritizing the preservation of explainability and interpretability of both the algorithms used and their results."}, {"title": "2.2 Explainability and Interpretability", "content": "The lack of explainability and Interpretability and the black box character it entails is one of the main arguments against a more widespread and systematic use of ML or DL techniques (Escalante et al., 2018). Therefore, one of our focal point is to uphold these principles of explainability and interpretability to the fullest extent we can realize within this framework.\nTo facilitate our comprehension of the concept of interpretability, let us begin with a more literary perspective. According to the Larousse, the verb interpret refers to the act of \"seeking to make a text, an author intelligible, explaining them,"}, {"title": "3 Data set for the application of the framework", "content": "In the following sections, we will introduce each of the algorithms comprising the framework outlined in Section 4. To provide clarity on how these algorithms operate, we propose starting by introducing the dataset. This approach will enable us to reference the dataset when necessary to illustrate an algorithm's function.\nThe data set used in this work is build upon the Blue Book for Bulldozers data set publicly available\u00b9. The data set is made available by French agency AMIES (Agency for Mathematics in Interaction with Industry and Society\u00b2) for their 2021 competition3.\nThe database contains 100,000 observations described by 53 parameters briefly defined in the table 3. This table categorizes various attributes of auction machinery, grouping them primarily under unique identifiers, descriptive information, configuration details, and location-specific data. Key identifiers such as SalesID, MachineID, ModelID, datasource, and auctioneerID are crucial for tracking and analyzing sales transactions. Descriptive attributes like YearMade, MachineHoursCurrentMeter, and UsageBand provide insights into the machine's age, usage level, and operational status. The table also extensively details machine configurations, highlighting features such as Drive_System, Enclosure, Forks, and numerous others that specify the machine's physical and operational setup, which is critical for potential buyers to assess the machinery's capabilities and condition. Furthermore, geographical and market segmentation is addressed through variables like State and ProductGroupDesc, which help in understanding the categorization of the machinery. This structured data arrangement aids in the comprehensive analysis and valuation of the machinery at the point of sale. Table 2 below summarizes all the types of errors that we will be identifying and correction in this paper. Obviously, none of the information provided by Tables 2 or 3 will be used in the algorithms."}, {"title": "4 Automatic data quality enhancement framework", "content": "The framework illustrated in Figure 1 consists of two phases: one preceding the commencement of actual quality enhancement (Pre-Quality Enhancement, PQE phase), and the other constituting the actual data quality enhancement process (Quality Enhancement Phase, QE phase). Initially, the PQEP involves identifying a primary key in the dataset (PQE1) and determining the columns that will receive each of the required treatments (PQE2). These two steps are fundamental in enabling an automated process that does not rely on domain knowledge. Following this preparation, the QEP targets the three key areas that were mentioned earlier:\n\u2022 Redundancies handling encompasses the identification and elimination of duplicate entries (QE1). As depicted in the flowchart, this step necessitates the utilization of the primary key identified in the PQE phase.\n\u2022 Absences Handling concentrates on the imputation of abnormal missing values to preserve data integrity (QE12). This step is divided into two actions: firstly, the identification of abnormal missing values, and secondly, their imputation. Indeed, as explained in Subsection 4.2.2, the process of identifying missing values and their imputation is not simultaneous. Similar to the preceding step, handling absences will also leverage the key identified during step A1 of the PQE phase.\n\u2022 Inconsistencies Handling is divided into three sub-steps: the identification and imputation of statistical outliers (QE31), the detection and correction of typographical errors (QE32), and the identification and correction of logic errors (QE33). None of these sub-steps necessitates splitting the actions, as for these tasks, the processes of identifying the problem and resolving it occur simultaneously. These steps do not require the identification of a primary key. They only benefit from the PQE phase by targeting the right columns to deal with.\nThese steps collectively enhance data quality by systematically removing errors and inconsistencies, thereby preparing the dataset for accurate and reliable analysis.\nIt is noteworthy that in Figure 1, there is an arrow linking logic errors to the imputation of missing values. This is because, as we will see in Sections 4.2.2 and 4.2.5, some specific types of missing values will be handled as logic errors."}, {"title": "4.1 Pre-quality enhancement phase", "content": ""}, {"title": "4.1.1 Identification of the primary key in the data set", "content": "Given a data set D consisting of n records {r1, 2, ...,rn}, where each record ri is a tuple (ai1, Ai2, ..., aim) of m attributes, a primary key is a subset of the set of attributes (columns) {Aj}j\u2208[1,m] that uniquely identifies each record in the data set. Let P = {P1, P2, ..., Pk} \u2286 {A1, A2, . . ., Am } be the set of k fields corresponding to the primary key attributes and Pind \u2286 {1, 2, ..., m} the respective indices of each element of P. The primary key P must satisfy the following two properties:\n1. Uniqueness\nFor any two distinct tuples (rows) ri and rj in the table where i \u2260 j, their projections on the primary key attributes must be different, it thus, must hold that\nr\u2260r\nVri,rj\u2208 D with i \u2260 j,\nHere, rP denotes the projection of tuple r onto the subset of attributes P which is\nr = (aij)j\u2208Pind\nThis ensures that no two rows can have the same value for P, providing a unique identifier for each record.\n2. Minimality\nThe set P must be minimal with respect to the uniqueness property. This means that no proper subset of P satisfies the uniqueness condition. If any attribute is removed from P, the remaining attributes no longer uniquely identify every row in the table. It must therefore hold that if Q \u2282 P and Q \u2260 \u2205, then there exist at least two records ri and rj in D such that:\nr\u2260r,\nwhere rQ denotes the projection of record ri onto the subset Q."}, {"title": "4.1.2 Mapping processes to specific data fields", "content": "As our work aims to enhance and measure data quality without relying on domain knowledge or knowledge of the current table's content, it is crucial to be able to precisely target each analysis and correction to maintain reasonable computer resource demands. A similar approach is taken by Schelter et al. (2018). While the authors do not specifically aim to make their work domain-knowledge-free, their software can propose a set of constraints to verify for each column in the absence of user-provided constraints. This resembles our problem, but according to the authors, their constraint suggestion process is designed to involve human intervention. While the automated process we introduce may be time-consuming, it ultimately saves time and computational resources when considering the entire process. The defined rules for each type of error considered in this work are as follows:\n\u2022 Redundancies\nOnly the subset of attributes comprising the primary key will be analyzed for duplicates. This is one of the justifications for why it is important to begin by identifying it.\n\u2022 Absences\nAll fields will be analyzed for missing values, as they can affect any field. However, as we will see in Subsection 4.2.2, not all fields' missing values will be imputed.\n\u2022 Statistical outliers\nOnly fields with numerical values will be considered. We also assert that without sufficient information on the field, accurately apprehending its statistical properties is challenging. Hence, we will not attempt to detect statistical outliers unless more than half of the values are available.\n\u2022 Typographical errors\nThe algorithm introduced in Subsection 4.2.4 functions for both real words and non-words. However, it is not intended for application to entries containing both numerical and text. For instance, the field fiBaseModel includes values like ZX160. Without domain knowledge, it is at best very difficult to ascertain whether another entry, such as ZX161, is a typographical error or a legitimate model. For this treatment, we also exclude fields with less than half of the values available. Indeed, we argue again that with more than half of the data missing, any analysis or insights derived from this field are already significantly compromised. The presence of typographical errors in a minority of the data is less likely to have a substantial impact on the overall analysis compared to the large proportion of missing values.\n\u2022 Logic errors\nThese fields should contain strings, have less than 75% of their values missing, and be represented by at least five different observations. Allowing up to 75% missing values in these columns considers the unique nature of logic errors errors that might not necessarily be about the absence of data but about data being present when it shouldn't be, or missing when it should be present. Therefore, when analyzing logic errors, it is not conservative enough to disqualify fields solely because they do not have enough data. Moreover, the requirement for at least five different observations ensures a diversity of data entries that can help identify inconsistencies and errors that a smaller number of observations might miss. By setting these parameters, we aim to enhance the reliability and validity of the corrections, focusing on fields where the scope for logic errors is significant and where corrections can substantially improve data quality."}, {"title": "4.2 Quality enhancement phase", "content": ""}, {"title": "4.2.1 Redundancies handling", "content": ""}, {"title": "Methodology", "content": "Given our data set D consisting of n records {r1,2,...,rn}, where each record ri is a tuple (ai1, ai2, ..., aim) of m attributes, we aim to identify duplicates based on the primary key attributes. We recall that in Subsection 4.1.1 we defined P \u2282 {A1, A2, ..., Ak}, the set of attributes defining the primary key. The primary key P must satisfy the properties of uniqueness and minimality, ensuring that each record in D is uniquely identifiable by P. Two records ri and rj are considered duplicates if their projections on the primary key attributes are identical, i.e., r = rf. Therefore, the set of duplicates DUP is defined as:\nDUP = {(ri, rj) \u2208 D \u00d7 D | i \u2260 j and r = r}}.\nTo generalize this concept to sets of more than two records, we define a set S \u2286 D as duplicates if all records in S have identical projections on the primary key attributes which is: \u2200ri, rj \u2208 S, r = r. We hence have, the more generalized expression:\nDUP = {S \u2286 D |\u2200ri,rj \u2208 S, r = r; and |S|> 1}.\nTo identify all duplicate sets in D, we follow these steps:\n1. Projection\nWe start by computing the projection of each record onto the primary key attributes:\n{r,r.....}.\n2. Grouping\nThen, we group records by their primary key projections. Let G be the collection of L groups, where \u2200l\u2208 [1, L] each group Gi, contains records with the same primary key projection:\nG = {G1, G2,..., GL},\nwhere G\u2081 = {ri \u2208 D | r = vi} for some unique primary key projection v\u03b9.\n3. Identifying duplicate sets\nWe now can identify all groups G\u2081 in G that contain more than one record. These groups represent sets of duplicate records:\n4. Dropping duplicates\nFinally, for all subsets G\u2081, we drop all records but one, we typically keep the first instance."}, {"title": "Complexity analysis", "content": "This generalized approach ensures that any set of records with identical primary key attribute values is identified as duplicates. Let's analyze its time and space complexity. It can be analyzed in terms of the number of records n in the data set D and the number of attributes k.\n\u2022 Time complexity\nFirst, we consider the projection step, where we compute the projection of each record onto the primary key attributes. This involves iterating over all n records and projecting each record onto the k primary key attributes. If k is relatively small compared to n, the projection step has a time complexity of O(n), otherwise if k is large, the time required for each projection increases. Specifically, the total time complexity of the projection phase is O(nk).\nNext, in the grouping step, we group records by their primary key projections. This step involves inserting n projected records into a suitable data structure for grouping (typically a dictionary or a hash table). Inserting each projected typically has an average-case time complexity of O(1). Therefore, the grouping step has an average-case time complexity of O(n).\nThen, in the duplicate identification step, we identify all groups that contain more than one record. This step involves iterating over the groups formed in the previous step. In the worst case, there could be up to n groups (if all records have unique primary key projections). Checking the size of each group and collecting groups with more than one record has a time complexity of O(n).\nFinally, for each group G\u2081 that contains more than one record, we keep only the first record and drop the rest. Iterating over all groups takes O(L) time. Again, in the worst case, we have n groups. For each group Gi, dropping the records but the first one takes O(|G1|) where |G1| is the number of records in the group l. The sum of all records in the different groups being n. The time complexity of this step is therefore O(n).\nTherefore, the overall time complexity is O(nk + n + n + n) = O(nk) when k is large. This complexity is reduced to O(n) when n >> k.\n\u2022 Space complexity\nThe space complexity of the algorithm is also important to consider. The projections of the records require O(n)k space for a large k and O(n) otherwise. The hash table used for grouping requires O(n) space. Collecting the duplicate sets requires O(n) space in the worst case (if all records are duplicates). Dropping the duplicates does not require any additional storage. Therefore, the overall space complexity is O(n)k.\nThis analysis indicates that if the number of attributes k defining the primary key PP is reduced, both time and space complexity become O(n). However, for a large value of k, the time and space complexities of the algorithm become O(nk), which could impact performance for large datasets. The typical scenario occurs when no previous work has been done to identify a primary key, resulting in k being exactly equal to the number of fields in the database (53 in our case). This underscores the importance of the pre-quality enhancement phase, as it not only allows for better structuring of the dataset but also reduces the computational cost of the algorithms in terms of both time and space complexities."}, {"title": "4.2.2 Missing values handling", "content": "As we have already stated, identifying missing values is not a tedious task in itself. The real challenge lies in distinguishing justifiably missing values from others and correcting only these values. Without domain knowledge, this task becomes even more difficult. In this section, we decide to implement a rather simple two-step solution.\n\u2022 We identify fields that have missing values which might be unjustified. These fields fall into two categories: first, the attributes of the primary key; and second, the attributes that don't have many missing values. We consistently maintain the same 95% threshold used for primary key candidates (see Subsection 4.1.1). Any field with more than 95% missing values will be considered to have justified missing values, and we will not attempt to impute these missing values. In an industrialized implementation, such fields would be flagged by warnings.\n\u2022 If the field being analyzed is part of the primary key, the missing value is automatically deemed abnormal. Since the projection of any record on the primary key cannot be duplicated, we cannot use the distribution of values in the same field for this case (even if the primary key might be comprised of multiple fields, this approach is not conservative enough). Therefore, we automatically generate a placeholder character that will act as the record for the remainder of the process. Then if the field being analyzed is not part of the primary key, instead of attempting to determine whether a missing value is justified or not, we assume that any observation, including missing values, could be valid and there are two possibilities:\nIf the field is a character-valued field, no imputation is made, and we assume that this missing value is justified. In the remainder of the process, specifically in Subsection 4.2.5, this will be checked as a logical error. For instance, if the Enclosure field in one row is empty, the algorithm introduced later will determine if it is likely for the model description (field fiModelDesc) in the considered row to have an empty Enclosure - without using domain knowledge -.\nIf the field contains numerical values, we apply a simple and classical method by replacing the missing values using linear interpolation with the two closest present values (one with a lower index and one with a higher index). In this work, where we do not have detailed information about the dataset, it is not advisable to replace missing values with a statistic (typically the average or the median) of the observations of the specific field. Indeed, the dataset could, for instance, contain time series with seasonality. In that case, the average might not be suited."}, {"title": "4.2.3 Processing statistical outliers", "content": "Let us now focus on identifying and correcting statistical outliers. We recall that we had defined a dataset D as a finite collection of n records {ri}i\u2208[1,n] with each ri having k attributes, i.e., ri = (ai1, ..., aik). We are interested in analyzing the outliers for a given attribute. That is to say, we want to determine whether, for a given attribute j of record ri, the observation aij is homogeneous with the rest of the observations {alj}l\u2208[1,n]\\{i}. Of course, depending on the meaning given to homogeneous, the identification and handling of statistical outliers may vary."}, {"title": "4.2.4 Processing Typographical Errors", "content": "The correction of typographical errors (TPOs) is a very important issue in the preparation of data processing. Spell checking stands as the crucial process of identifying and suggesting corrections for words that are inaccurately spelled. Essentially, classic spell checkers serve as computational tools leveraging a dictionary of words to execute this task. The efficacy of such a checker relies on the extent of its dictionary. A larger repository of words typically results in an increased ability to detect errors. However, due to their reliance on standard dictionaries, they are inadequate in capturing mistakes such as proper nouns, specialized terms pertinent to particular domains, acronyms, and other specialized terminologies. These types of words are called non-words in the literature (Lee et al., 2020). They represent a notable contrast to conventional words, commonly referred to as real words. This is well explained in Bassil and Alwani (2012). As we have already mentioned in Section 1, on one hand, much of the literature requires the use of external dictionaries, and on the other hand, many algorithms in the literature use ML/DL techniques to handle domain knowledge, particularly to correct non-words. We introduce in what follows a procedure leveraging ML, efficient for both real and non-words without any use of domain knowledge. Let us start by introducing key concepts that are pivotal for our algorithm.\nDamerau-Levenshtein Distance\nConsider two distinct character strings A and B and let us consider the task of finding the minimum number of operations required to transform B into A.\nWhen only substitutions are allowed, the minimum number of operations is given by the Hamming distance (Robinson, 2003), or when only transpositions are allowed, the Jaro distance (Jaro, 1989). The Levenshtein distance (Levenshtein, 1966) is the length of the shortest sequence when substitutions, insertions, and deletions are allowed. This distance is used in the longest common subsequence problem (Maier, 1978). In this paper, we will use the Damerau-Levenshtein distance (Levenshtein, 1966; Damerau, 1964), which is optimal when four operations are possible (substitution, insertion, deletion, and transposition of two consecutive characters). The Damerau-Levenshtein distance (DLD) for the letters number i and j of words A and B is recursively obtained through d(., .) below (Boytsov, 2011):\n0\nif i = j = 0\nda,B(i - 1, j) + 1\nif i > 0\nda,B(i, j) = minda,B(i, j - 1) + 1\nif j > 0\n(5)\nda,B(\u0456 \u2013 1, j \u2212 1) + 1(A\u2081\u2260B\u2081) if i, j > 0\nda,B(\u0456 \u2013 2, j - 2) + 1 if i, j > 1 and A\u2081 = Bj\u22121 and Ai\u22121 = Bj.\nFor example, since it only takes one transposition to go from France to Fracne:\nDLD(France, Fracne) = 1.\nIn practice, we will not use the DLD itself, but rather a score that increases the closer two words are to each other. Indeed, a distance of 2 between two 4-letters words, for example, and a distance of 2 between two 15-letters words should not be interpreted in the same way. The Damerau-Levenshtein Score (DLS) is given as a function of the DLD between A and B, DLD(A, B) and the maximum possible distance between two words of the lengths of A and B, noted DMAX\u0410, \u0412:\nDLS(A, B) = \\frac{DMAX_{A, B} - DLD(A, B)}{DMAX_{A, B}}\n(6)\nConsidering the previous example, note that the maximal Damerau-Levenshtein distance between two words of length 6 is 6. Therefore, the score between France and Fracne is given by:\nDLS(France, Fracne) = \\frac{6-1}{6} \u2248 0.8334."}, {"title": "4.2.5 Processing logic errors", "content": "Logic errors in a dataset are the most delicate to identify and rectify. Unlike typographical errors or certain outliers, these errors do not readily reveal themselves, even through observation of the dataset. Logic errors arise when the data does not adhere to the expected logical relationships between variables, making them challenging to detect. To address this issue, we propose an algorithm that, despite its simplicity, effectively identifies these elusive errors. Our method involves the use of data mining techniques to uncover relationships between various variables in the dataset. By identifying and establishing the strongest relationships as true, we can identify observations that fail to conform to these logical connections. Such non-conforming observations are then flagged as inconsistent with the logical structure of the dataset. This approach is used in the work of (A.M.Rajeswari et al., 2014) and (Karthikeyan and Vembandasamy, 2015b), where the authors used similar techniques not to specifically detect logical errors but to identify outliers7. This method ensures that the integrity of the dataset is maintained by validating that the data adheres to the underlying logical relationships expected within the dataset. As we had already mentioned in Section 4.2.2, unjustified missing text values will also be flagged in this process.\nTo detect relationships between variables, we decided to use the association rule mining algorithm, Apriori (Agrawal et al., 1994). The algorithm is often used in market basket analysis and purchase recommendation systems. Like the Isolation Forest used for outliers, this algorithm seems less known, so we describe its functioning here:"}, {"title": "Apriori algorithm", "content": "Let us start by recalling the main constituents of the Apriori algorithm:\n\u2022 Itemset: An itemset is a collection of one or more items. A k-itemset is an itemset containing k items.\n\u2022 Support: The support is the frequency of a particular itemset. If an itemset I appears in s out of n records", "Rule": "An association rule is an implication of the form I\u2081 \u2192 I2", "Confidence": "Confidence is a measure of the reliability of an association rule. For a rule I\u2081 \u2192 I2, the confidence is given by"}]}