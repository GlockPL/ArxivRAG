{"title": "IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark", "authors": ["Kawshik Manikantan", "Makarand Tapaswi", "Vineet Gandhi", "Shubham Toshniwal"], "abstract": "Recent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models' referential understanding. To address this, we introduce Identify Me, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark also consists of a curated mixture of different mention types and corresponding entities, allowing for a fine-grained analysis of model performance. We evaluate both closed- and open-source LLMs on IdentifyMe and observe a significant performance gap (20-30%) between the state-of-the-art sub-10B open models vs. closed ones. We observe that pronominal mentions, which have limited surface information, are typically much harder for models to resolve than nominal mentions. Additionally, we find that LLMs often confuse entities when their mentions overlap in nested structures. The highest-scoring model, GPT-40, achieves 81.9% accuracy, highlighting the strong referential capabilities of state-of-the-art LLMs while also indicating room for further improvement.", "sections": [{"title": "1 Introduction", "content": "Coreference resolution (CR) is a fundamental task for text comprehension. While LLMs have made tremendous strides on a wide array of NLP tasks (Brown et al., 2020), their performance on CR has been relatively underwhelming, with models struggling at even mention detection (Le and Ritter, 2023; Manikantan et al., 2024). Through extensive analysis, recent work by Gan et al. (2024) has identified that LLMs' excellent referential understanding is underestimated in the typical CR setup due to the span-based output format being ill-suited for LLMs. They suggest adapting CR datasets and task metrics to support LLM evaluations.\nAlong these lines, we create the IdentifyMe benchmark for mention resolution in an MCQ format, commonly used for LLM evaluations (Hendrycks et al., 2021). To construct the benchmark, we use annotations from two long-text coreference benchmarks, namely LitBank (Bamman et al., 2020) and FantasyCoref (Han et al., 2021). To make the benchmark challenging, we restrict it to pronominal and nominal mentions and apply some heuristics for each mention type"}, {"title": "2 IdentifyMe Benchmark", "content": "IdentifyMe is an MCQ-based benchmark where given a document with a marked mention, the task is to identify the entity to which it refers. We derive these mentions from two coreference datasets focused on literary texts: LitBank and FantasyCoref. These datasets provide long contexts (1700 words on average for FantasyCoref and 2000 words for LitBank) and feature multiple entities with rich inter-dependencies (e.g., Mr. and Mrs. Pett) that make resolving mentions more challenging. While LitBank offers diverse writing styles and linguistic structures, FantasyCoref includes entities that often take on different forms (e.g., disguises and transformations), or undergo changes in title (e.g., Prince Rudolph is called The Emperor after his coronation), which further complicate entity mapping.\nCoreference annotations group mentions that refer to the same entity into unlabeled clusters. However, to create an MCQ with entities as options, we need to assign each cluster a representative phrase. We use GPT-40-mini (see Table 8) to generate phrases for each entity based on their mentions and frequencies. These annotations are manually reviewed to ensure a distinct phrase for each entity. To prevent confusion, we discard and avoid labeling clusters that: (i) contain annotation errors (e.g., due to cluster merging or splitting (Kummerfeld and Klein, 2013)); (ii) are too small (< 3 mentions) or difficult/ambiguous to label (e.g., entitites like some money); (iii) are plural, as they often lack explicit surface forms that can be derived from mentions.\nAn MCQ is created from a document using mentions from labeled clusters, with all labeled entities provided as options. To create a high quality benchmark, we exclude short context documents (< 1000 words) or those where the discarded entities represent more than 50% of the mentions."}, {"title": "2.1 Selecting Mentions for IdentifyMe", "content": "Based on previous works which utilize rule-based linguistic patterns to perform (Zhou and Su, 2004; Lee et al., 2013) or analyze (Haghighi and Klein, 2009; Otmazgin et al., 2023) coreference resolution, we propose a two-step heuristic to identify challenging mentions.\nStep 1: Discard easy mentions. We apply two criteria to filter out mentions that can be easily resolved due to syntactic similarity:\nA. Nominal fuzzy score calculates the fuzzy similarity\u00b2 (flexible to order and subset perturbations) between a nominal mention and the corresponding entity's representative phrase. Mentions with a score of 75% or higher are discarded, as we expect that they are easier to correctly identify.\nB. Net distractor score. We categorize pronominal mentions based on attributes like gender, number, and animacy (LingMess (Otmazgin et al., 2023)). Nearby pronominal mentions with the same category as the marked mention (pronominal), and referring to the same entity, may help identify the mention easily. On the other hand, those from a different category but same entity or same category but different entity may make it harder to identify. We define the Net-Distractor-Score of the marked mention as the number of neighboring pronominals that hinder identification minus the number that help it. We discard mentions that score < 0.\nStep 2: Ranking mentions by difficulty. Filtered mentions are ranked from most to least difficult: for nominals, a low Nominal-Fuzzy-Score is preferred; and for pronouns, a high Net-Distractor-Score is preferred. Additionally, the distance between the marked mention and other mentions of the same entity also indicate difficulty. We consider distances to the nearest mention, the nearest nominal mention, and the nearest mention resembling the representative phrase as further criteria for ranking. All the"}, {"title": "3 Experiments", "content": "Models. Among closed-source models, we evaluate GPT-40 (OpenAI, 2024a), GPT-40-mini (OpenAI, 2024b), and Gemini-1.5-Flash\u00b3 (Team et al., 2024). Due to computational constraints, we limit the evaluation of open-source models to sub-10B variants: Llama-3.1-8B (Meta-AI, 2024) and Mistral-7B (Jiang et al., 2023).\nMCQ setup. The selected mention is highlighted in the original text by enclosing it with special tokens (e.g. \"... eject a fourteen-year old boy from . . .\u201d \u2192 \u201c. . . eject {{a fourteen-year old boy}} (#This is the marked span) from ...\". A zero-shot prompt instructs the model to retrieve and resolve the mention and identify who or what it refers to from a given set of entities and NoA (detailed prompt in Appendix A.2).\nInference details. For open-source models, we use regex-based constrained decoding with the outlines library (Willard and Louf, 2023) to limit answers to specific entity representative phrases. We also experiment with a chain-of-thought (CoT) approach (Wei et al., 2023), instructing the model to explain its reasoning before answering the question. As seen in Table 2, using CoT improves the model performance (e.g., +9.5% for Llama-3.1-8B, +3.7% for GPT-40-mini). Based on these results, we use the CoT decoding for evaluation over the test set. For details on prompts and decoding regular expressions, see Appendix A.2."}, {"title": "3.1 Results", "content": "Table 3 presents the overall LLM performance on the IdentifyMe test set, along with a breakdown by nominal and pronominal mention types. The Random baseline, selecting answers uniformly at random, achieves 8% on our benchmark. Although all LLMs outperform Random, open-source models show considerable room for improvement, with Llama-3.1-8B reaching only 53.3% accuracy. GPT-40 is the top-performing model with an accuracy of 81.9%. Meanwhile, GPT-40-mini, an affordable closed-source option, surpasses smaller open-source models but lags behind top performers like GPT-40 and Gemini-1.5-Flash. Across mention types, all closed-source models perform"}, {"title": "3.2 Error Analysis", "content": "Comparing entities vs. NoA. Table 5 provides the accuracy distribution when the correct option is an entity (Ent) vs. NoA. Furthermore, we classify errors into three categories: (a) ground truth is an entity and the model chooses another entity (Ent-Ent), (b) ground truth is an entity, but the model predicts NoA (Ent-NoA), and (c) ground truth is NoA, but the model chooses an entity (NoA-Ent). Open-source models perform extremely poorly on the NoA subset (120 MCQs), leading to high NoA-Ent errors. Among closed-source models, Gemini-1.5-Flash achieves sub-par performance on NoA MCQs (\u2193 48.3%) and prefers to select an entity when the answer is NoA (83/120). Interestingly, GPT-40 and GPT-40-mini are much more resilient on NoA questions, with drops of only \u2193 9.6% and \u2193 0.9%, respectively.\nNested mentions. The dataset contains 352 instances of nested mentions, where the span of one mention overlaps with another. Table 6 shows that the accuracy of nested mentions is comparable to the overall accuracy. However, when models err in resolving these mentions, about 40% of these errors are because the predicted entity corresponds to an overlapping mention."}, {"title": "4 Conclusion", "content": "We present IdentifyMe, a challenging MCQ benchmark designed for the evaluation of the referential capabilities of LLMs. Analysis shows that LLMs' have difficulty with: (i) pronominal resolution without clear surface form clues, (i) questions that require rejecting all wrong options by selecting \"None of the Above\", and (iii) nested mentions that require distinguishing between overlapping spans. GPT-40 scores 81.9% on IdentifyMe, highlighting the strong referential capabilities of frontier LLMs while still leaving ample room for improvement. We believe the IdentifyMe benchmark, with its curated mix of diverse and challenging mentions,"}, {"title": "5 Limitations", "content": "The IdentifyMe is limited in its domain (literary domain), mention type coverage (only nominal and pronominal mentions), and also entity types (no plural entities). The datasets we used are available on the web, and we conducted preliminary investigations to determine whether the LLMs can reproduce the CoNLL annotations for entire stories. Our investigation suggested that's not the case, and we also heavily processed the coreference annotations to construct our benchmark. But there's still a remote possibility of contamination."}]}