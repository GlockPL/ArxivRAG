{"title": "CaDA: Cross-Problem Routing Solver with Constraint-Aware Dual-Attention", "authors": ["Han Li", "Fei Liu", "Zhi Zheng", "Yu Zhang", "Zhenkun Wang"], "abstract": "Vehicle Routing Problems (VRPs) are significant Combinatorial Optimization (CO) problems holding substantial practical importance. Recently, Neural Combinatorial Optimization (NCO), which involves training deep learning models on extensive data to learn vehicle routing heuristics, has emerged as a promising approach due to its efficiency and the reduced need for manual algorithm design. However, applying NCO across diverse real-world scenarios with various constraints necessitates cross-problem capabilities. Current multi-task methods for VRPS typically employ a constraint-unaware model, limiting their cross-problem performance. Furthermore, they rely solely on global connectivity, which fails to focus on key nodes and leads to inefficient representation learning. This paper introduces a Constraint-Aware Dual-Attention Model (CaDA), designed to address these limitations. CaDA incorporates a constraint prompt that efficiently represents different problem variants. Additionally, it features a dual-attention mechanism with a global branch for capturing broader graph-wide information and a sparse branch that selectively focuses on the most relevant nodes. We comprehensively evaluate our model on 16 different VRPS and compare its performance against existing cross-problem VRP solvers. CaDA achieves state-of-the-art results across all the VRPs. Our ablation study further confirms that each component of CaDA contributes positively to its cross-problem learning performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Vehicle Routing Problems (VRPs) involve optimizing transportation costs for a fleet of vehicles to meet all customers' demands while adhering to various constraints. Numerous studies have focused on VRPs due to their extensive real-world applications in transportation, logistics, and manufacturing [1], [2]. Traditional methods for solving VRPs include exact solvers and heuristic methods. Exact solvers, however, struggle with the NP-hard nature of the problem, making them prohibitively expensive to implement. On the other hand, heuristic methods are more cost-effective and provide near-optimal solutions but require significant expert input in their design. Recently, learning-based neural solvers have gained considerable attention and have been successfully applied to VRPs [3]\u2013[5]. These solvers train networks to learn a heuristic, reducing the need for extensive manual algorithm design and minimizing computational overhead.\nDespite the promising performance of neural solvers on VRPs, the majority of existing works require training a distinct model for each type of routing problem [6]\u2013[13]. Given that over 60 VRP variants have been studied, each featuring different constraints [14], developing distinct models for each routing problem is costly and significantly hinders practical application.\nTo tackle this challenge, recent efforts have been made to develop cross-problem learning methods that can solve multiple VRPs with a single model [15]\u2013[18]. These cross-problem methods typically employ an encoder-decoder framework and are trained using reinforcement learning. For example, MTPOMO [15] jointly trains a unified model across five VRPs, each with one or two constraints, enabling zero-shot generalization to problems that feature combinations of these constraints. MVMoE [16] employs a Mixture-of-Experts (MoE) [19] structure in the feed-forward layer of a transformer-based model to enhance its cross-problem learning capacity. Furthermore, RouteFinder [17] directly trains and tests sixteen VRPs using a proposed unified Reinforcement Learning (RL) environment, which enables the simultaneous handling of different VRPs in the same training batch. Additionally, RouteFinder leverages a modern transformer-based model structure [20], along with global embeddings, to enhance performance.\nDespite these advancements, existing cross-problem models remain unaware of constraints [15]\u2013[17]. As different constraints significantly alter the feasible solution space, this oversight notably limits the models' capabilities in cross-problem applications. Furthermore, existing methods employ a transformer encoder which maintains global connectivity throughout the node encoding process, leading to the inclusion of irrelevant nodes and adversely affecting node representation. Conversely, using learnable sparse connections [21], [22] allows the node encoding process to selectively focus on more relevant nodes.\nThis study proposes a novel Constraint-Aware Dual-Attention Model (CaDA) to mitigate these challenges. Firstly, we introduce a constraint prompt to enhance the model's awareness of the activated constraints. Furthermore, we propose a dual-attention mechanism consisting of a global branch and a sparse branch. Since, in the encoder-decoder framework, node pairs with higher attention scores are more likely to be adjacent in the solution, the sparse branch with Top-k sparse attention focuses on the more promising connections between these key node pairs. Meanwhile, the global branch enhances the model's capacity by capturing information from the entire graph, ensuring that the solution is informed globally. The effectiveness and superiority of CaDA have been comprehensively demonstrated across 16 VRPs and real-world benchmarks.\nThe contributions of this paper can be summarized as follows:\n\u2022 We introduce CaDA, an efficient cross-problem learning method for VRPs that enhances model awareness of constraints and representation learning.\n\u2022 We propose a constraint prompt, which facilitates high-quality constraint-aware learning, and a dual-attention mechanism, which ensures that the encoding process is both selectively focused and globally informed.\n\u2022 We conduct a comprehensive evaluation of CaDA across 16 VRP variants. CaDA achieves state-of-the-art performance, surpassing existing cross-problem learning methods. Additionally, our ablation study validates the effectiveness of both the constraint prompt and the dual-attention mechanism."}, {"title": "II. RELATED WORK", "content": "NCO approaches utilize deep reinforcement learning to train a policy that constructs solutions in an autoregressive manner. Nazari et al. [23] are the first to apply Pointer Networks [24] to solve the VRP. Subsequently, the pioneering work Attention Model (AM) [4] employs a powerful Transformer-based architecture. This model is optimized using the REINFORCE algorithm [25] with a greedy rollout baseline. Building on this, Kwon et al. [6] introduce the Policy Optimization with Multiple Optima (POMO) method, which leverages solution symmetries and has demonstrated significantly improved performance. Subsequently, numerous studies have further refined both AM and POMO, enhancing Transformer-based methods [26]\u2013[29]. Given the diverse constraints and attributes in real-world transportation needs, some research focuses on various VRP variants, including heterogeneous Capacitated VRP (HCVRP) [30], VRP with Time Windows (VRPTW) [9]\u2013[12], and Open Route VRP (OVRP) [13]. More information can be found in recent reviews [31], [32].\nNeural methods for solving VRPs typically train and evaluate deep models on the same instance distributions. Some studies have explored generalization across multiple distributions [33]\u2013[38]. Additionally, Zhou et al. [39] consider both problem size and distribution variations. Recent developments have begun to address cross-problem generalization [15], [16], [18], [40]. Wang and Yu [40] use multi-armed bandits to achieve task scheduling. Lin et al. [18] demonstrate how a model pre-trained on the Travelling Salesman Problem (TSP) could be effectively adapted to targeted VRPs through efficient fine-tuning, e.g., inside tuning, side tuning, and Low-Rank Adaptation (LoRA). However, these approaches still focus on relatively few tasks (less than five).\nTo tackle multiple VRP variants in a unified model, MTPOMO [15] conceptualizes VRP variants as combinations of underlying constraints, enabling the model to achieve zero-shot generalizability to more tasks. MVMoE proposes a new model architecture using the MoE [19] approach to improve performance. Furthermore, RouteFinder [17] proposes to use a modern transformer encoder structure incorporating SwiGLU [41], Root Mean Square Normalization (RM-SNorm) [42], and Pre-Norm [43], [44], which considerably improves the model's capability. However, these approaches remain unaware of constraints and maintain only global connectivity throughout the encoding process, which limits their cross-problem capabilities.\nMulti-branch architectures have been widely used and have achieved success in computer vision. Some research employs multiple branches to capture both low and high-resolution image information, ultimately producing a comprehensive and powerful semantic representation that can be used for downstream tasks such as image segmentation [45]\u2013[47] or human pose estimation [48], [49]. Other studies assign different branches to focus on distinct aspects by utilizing attention mechanisms [50]\u2013[53]. For instance, DANet [50] proposes a dual attention network for scene segmentation, with one branch responsible for capturing pixel-to-pixel dependencies and another for capturing channel dependencies across different feature maps, thereby capturing global context [54]. Similarly, Crossformer++ [53] groupes image patches in both local and global ways, incorporating short-distance and long-distance attention to achieve better representation, retaining both small-scale and large-scale features in the embeddings.\nIn this study, we propose a novel dual-attention mechanism featuring a global branch for capturing information from the entire graph and a sparse branch to focus more on important connections."}, {"title": "III. PRELIMINARIES", "content": "In this study, we focus on 16 VRP variants that encompass five different constraints, including Capacity (C), Open Route (O), Backhaul (B), Duration Limit (L), and Time Window (TW). These are summarized in Table I, with illustrations related to these constraints presented in Figure 1. In this section, we begin by outlining a general definition of the VRP instance, then continue with the basic CVRP, and proceed to describe its four additional constraints.\nA VRP instance G is a fully-connected graph defined by a set of nodes V = {vo, V1,...,vN} with the total number of nodes given by |V| = N + 1, and edges E = V \u00d7 V. Furthermore, vo represents the depot, while {21,...,UN} represent the N customer nodes. Each node vi \u2208 V consists of the pair {X, Ai}, where X\u2081 ~ U(0,1)\u00b2 represents the node coordinates, and A\u2081 denotes other attributes of the nodes. Additionally, the travel cost between different nodes is defined by their Euclidean distance, which is denoted by the cost matrix D = {di,j, i = 0, . . ., N, j = 0..., N}.\nIn CVRP, the depot node vo has Ao = \u00d8, and each customer node vi is associated with A\u2081 = {\u03b4\u03af}, where \u03b4\u03b5 is the customer's demand vi that the fleet of vehicles must service. This fleet comprises homogeneous vehicles, each with a specific capacity C. Each vehicle leaves the depot vo, visits a subset of customers, and returns to the depot upon completion of deliveries. The solution to CVRP, denoted by T, consists of the routes taken by all vehicles, that is, T = {T1, T2, . . .,\u2533K}, where K is the total number of sub-routes. Each sub-route Tk = (T,T,..., Th\u2081), k \u2208 1, 2, ..., K, where is the index of the visited node at step i, and Tr\u2081 = Th\u2081 = 0. nk = |tk| represents the number of nodes in it, and \u2211k=1 \u03b7\u03ba = T represents the total length of the solution.\nThe basic CVRP could be easily extended to accommodate various VRPs by incorporating additional constraints. In this paper, we explore four additional constraints as discussed in recent studies [15]\u2013[17].\na) Open Route (O): In the OVRP, vehicles do not return to the depot vo after completing their sub-route.\nb) Time Window (TW): The Time Window constraint requires that each node must be visited within a specific time window, such that each node Ai = {di, ei, li, Si}, where ei is the earliest start time, li is the latest permissible time, and si represents the time taken to service this customer. The depot Vo has so = 0, eo = 0, and lo = T, indicating that each sub-tour must be completed within a time limit of T. Time window constraints are stringent; if a vehicle arrives earlier than ei, it must wait until the start of the window.\nc) Backhaul (B): Customers with \u03b4\u03b5 > 0 are termed linehaul customers, as they require vehicles to load goods at the depot and deliver to their locations. Conversely, customers with \u03b4\u03b5 < 0 are defined as backhaul customers, where vehicles must collect || from their locations and transport it back to the depot. While all customers in the standard CVRP are linehaul, the Vehicle Routing Problem With Backhauls(VRPB)"}, {"title": "B. Learning to Construct Solutions for VRPs", "content": "The process of constructing solutions autoregressively (i.e., during decoding) can be modeled as a Markov Decision Process (MDP), and the policy can be trained using RL methods. As the model sequentially expands each sub-route, for simplicity, at any decoding step t, tt represents the sequence of nodes visited up to that point:\n$T_t = \\bigcup_{k=1}^K (\\tau_1^k, \\tau_2^k, ..., \\tau_t^k) = (\\tau_1, \\tau_2, ..., \\tau_t),$\nwhere denotes the concatenation of sequences from different sub-routes The MDP for the decoding step t can be defined as follows:\na) State: st \u2208 S is the ordered tuple (Tt\u22121,V) given by the current partial solution Tt\u22121 = (T1, T2, ..., Tt\u22121) and the instance V. Initially, \u03c4\u03bf = 0, and at the end, st contains a feasible solution TT.\nb) Action: at \u2208 A is the selected index in the current step, which will be added at the end of the partial solution. If at = 0, i.e., the vehicle returns to the depot node, it signifies the end of the current sub-tour and the start of a new one.\nc) Policy: A neural model \u03c0\u03b8 with learnable parameters @ is used as a policy to generate solutions sequentially, where the probability of generating the final feasible solution is:\n$\\pi_\\theta(\\tau | V) = \\prod_{t=1}^T \\pi_\\theta(a_t|s_t) = \\prod_{t=1}^T \\pi_\\theta(\\tau_t | \\tau_{t-1}, V).$\nd) Reward: r\u2208 R can only be obtained when a whole feasible solution TT is generated and is defined as the negative solution length:\n$r(\\tau_T) = - \\sum_{t=1}^{T-1} d_{\\tau_t \\tau_{t+1}}.$\nSubsequently, \u03c0\u03b8 can be optimized using RL methods to maximize the expected reward J. This study employs the REINFORCE algorithm [25] with a shared baseline proposed by Kwon et al. [6], to update the policy. Specifically, for a VRP instance V, N trajectories are generated, starting with the first action {a1, a, . . ., a' }, which is always 0. Each of the N trajectories then assigns a unique one of the N customer nodes as the second point, i.e., {a1, a2, ..., a^} = {1,2,..., N}. The policy subsequently samples actions for each trajectory"}, {"title": "C. Transformer Layer", "content": "The Transformer [67] comprises a Multi-Head Attention Layer (MHA) and a Feed-Forward Layer (FFD). In some modern large language models [68]\u2013[70], the FFD is replaced by Gated Linear Units (GLUs).\na) Attention Layer: The classical attention function can be expressed as:\nAttention (X, Y) = A (YWv),\nwhere A = Softmax($\\frac{XWQ(YWK)T}{\\sqrt{dk}}$),\nwhere X \u2208 Rn\u00d7d and Y \u2208 Rm\u00d7d represent the input embeddings. The parameters WQ,WK \u2208 Rd\u00d7dk, and Wv \u2208 Rdxdv are trainable matrices for the query, key, and value projections, respectively. After calculating the attention matrix using the query and key matrices, the Softmax function is applied independently across each row to normalize the attention scores. These scores are then rescaled by \u221adk, resulting in the scaled attention score matrix A. The eventual output, denoted as Z, is a matrix in Rn\u00d7dv.\nAdditionally, for efficiency, the MHA projects X into Mh separate sets of queries, keys, and values, upon which the attention function is applied:\nMHA(X, Y) = Concat(Z1,..., ZMn)Wp,\nwhere Zi = Attention\u00bf(X, Y), Vi \u2208 {1, ..., Mh}, where dk = dv in each Attention. The parameter Wp \u2208 Rdxd denotes a trainable matrix combining different attention heads' outputs. For self-attention, we have Y = X.\nb) Gated Linear Unit: The transformer blocks also include an FFD that processes the input X through two learned linear projections, with a ReLU activation function applied between them. In many recent modern transformer-based large language models [68]\u2013[70], this configuration has been replaced by GLUs [41]. GLUs consist of a component-wise product of two linear projections, where one projection is first passed through a nonlinear function. We employ SwishGLU [41], which utilizes the Sigmoid Linear Unit (SiLU) [71] as the nonlinear function, as recommended in the RouteFinder [17]. The SwiGLU is defined as:\nSwiGLU(X) = X \u2299 \u03c3(XW\u2081 + b\u2081) \u2297 SiLU(XW2 + b2)."}, {"title": "IV. METHODOLOGY", "content": "As shown in Figure 2, CaDA follows the general cross-problem learning framework for VRPs which consists of two stages: encoding instance V to node embeddings H(L), and decoding to construct solutions based on H(L) sequentially. CaDA employs a prompt to introduce constraint information during the encoding process and further utilizes a dual-attention mechanism to enhance representation learning.\nTo generate prompts that carry the problem's constraint information, we represent the problem as a multi-hot vector V \u2208 R5, corresponding to five distinct constraints. This multi-hot vector is subsequently processed through a straightforward Multi-Layer Perceptron (MLP) to generate the prompts:\nP(0) = LayerNorm(VWa + ba)Wb + bb,\nwhere Wa\u2208 R5xdh, ba \u2208 Rdh, Wb \u2208 Rdhxdh, and bb \u2208 Rdh are learnable parameters. d\u0127 is the node's embedding dimension. Then this prompt can be concatenated with the node embeddings.\nThe input instance V with |V| = N + 1, is first transformed into high-dimensional initial node embeddings a linear projection. The initial node embedding is denoted as H(0) \u2208 R(N+1)xdh.\nSubsequently, H(0) is concatenated with P(0) and processed through a global branch fg, which consist of L layers. Each consists of a standard MHA layer [67] and a SwiGLU [72]. The standard attention function with Softmax never allocates exactly zero weight to any node, thereby allowing each node access to the entire graph. Concurrently, to capture information from closely related nodes, a sparse branch denoted as fs with Top-k sparse attention layers is introduced. Both branches adaptively fuse information at the end of each layer.\nFinally, the output from the global branch, HL), is used for autoregressive decoding, with the likelihood of node selection being primarily determined by the similarity of the nodes' embeddings.\na) Global Layer: Each layer involves a MHA [67] and a SwiGLU [72], along with RMSNorm [42] and residual connections [73]. The i-th layer is formulated as follows:\n$H_g^{(i)} = RMSNorm^{(i)}_g (H_g^{(i-1)} + MHA^{(i)}(H_g^{(i-1)}, Concat[H_g^{(i-1)}, p^{(i-1)}])),$\n$H_g^{(i)} = RMSNorm^{(i)}_g (H_g^{(i)} + SwiGLU^{(i)}(H_g^{(i)})),$\n$p^{(i)} = RMSNorm^{(i)} (p^{(i-1)} + MHA^{(i)}(p^{(i-1)}, Concat[H_g^{(i-1)}, p^{(i-1)}])),$\n$p^{(i)} = RMSNorm^{(i)} (p^{(i)} + SwiGLU^{(i)}(p^{(i)})),$\nwhere Hi-1) \u2208 R(N+1)\u00d7dn represents the node embeddings output from the (i \u2013 1)-th global layer.\nb) Sparse Layer: In the sparse branch fs, each layer also consists of an attention layer and a SwiGLU activation function. However, to focus more precisely on related nodes, we replace the attention function Attention(\u00b7,\u00b7) in MHA(\u00b7,\u00b7) with SparseAtt(\u00b7,\u00b7), which masks attention scores smaller"}, {"title": "D. Decoder", "content": "After encoding, the output of the global branch, H(L) (L) (L) (L)\n= [h1 ,h2 ,...,hN+1 ], is utilized to construct the solution.\nDuring the autoregressive decoding process, at step t, the context embedding is defined as:\nH = Concat [h, ct, ct, zt, lt, Ot] Wt,\nwhere Tt is the partial solution already generated, and Tt is the last node of the partial solution. The terms 4, a represent the remaining capacity of the vehicle for linehaul and backhaul customers, respectively. The terms zt, lt, and ot represent the current time, the remaining length of the current partial route (if the problem includes a length limitation), and the presence indicator of the open route, respectively. The matrix We \u2208\nR(dh+5)\u00d7dn_is a learnable parameter.\nThen the context embeddings are processed through a MHA to generate the final query:\nqc = MHA(H(L), Concat[h) : i \u2208 It]),\nwhere It represents the set of feasible actions at the current step. The compatibility u\u017e is computed as:\n$u_i = \\begin{cases}\ntanh(\\frac{q_c \\cdot h_i}{\\sqrt{d}})\\\\ - \\infty\\end{cases} \\begin{array}{ll} \\text{if i \\in I_t}, \\\\ \\text{otherwise},\\end{array}$,\nwhere \u00a7 is a predefined clipping hyperparameter. Finally, the action probabilities \u03c0\u03c1(Tg = i | V, T1:9\u22121) are obtained by applying the Softmax function to u = {Ui}i\u2208I.\nAdditionally, to determine the set of feasible actions at the current step It, we utilize the following feasibility testing process."}, {"title": "V. EXPERIMENTS", "content": "To evaluate the effectiveness of the proposed CaDA for VRPs, we conduct experiments on 16 different VRP variants with five constraints. Furthermore, we perform ablation studies to validate the efficiency of the proposed components."}, {"title": "A. Problem Setup", "content": "In this section, we provide a detailed description of the VRP problems setup used in this study.\na) Locations: The nodes' locations are represented by a two-dimensional vector Xi, i \u2208 {0, ..., N}, and are derived from a uniform distribution U(0, 1).\nb) Capacity: In this study, we consider only homogeneous vehicles, with the same vehicle capacity C shared among all vehicles, and the number of vehicles is unlimited. Following the common capacity setup used in previous studies [4], [6], for N = 50 and N = 100, the vehicle capacity C is set to 40 and 50 respectively."}, {"title": "B. Baselines", "content": "We utilize state-of-the-art traditional and neural solvers as baselines. For the traditional approaches, we employ two open-source solvers capable of addressing all 16 VRPs in this study: PyVRP [75], which extends the state-of-the-art heuristic algorithm HGS-CVRP [76], and Google's OR-Tools. Both baseline methods solve each instance using a single CPU core, with a time limit of 10 seconds for VRP50 and 20 seconds for VRP100, respectively. For the neural solvers, we compare our method against representative multi-task learning models: MTP\u039f\u039c\u039f [15], MVMoE [16], and RouteFinder [17], including RF-POMO, RF-MoE, and RF-TE. We utilize the open-source code published by RouteFinder [17]. For each method, we separately train two models from scratch on VRP50 and VRP100 using the same hyperparameter settings in RouteFinder [17]. However, for RF-MoE and MVMoE, due to their higher memory demands, we utilize the pre-trained"}, {"title": "E. Ablation Study", "content": "In this section, we conduct ablation studies to validate the efficacy of the proposed components in CaDA. Specifically, we separately remove the constraint prompt and the Top-k operation, resulting in two CaDA variants: CaDA w/o prompt and CaDA w/o Sparse, where 'w/o' stands for 'without.' In CaDA w/o Sparse, both branches use the standard Softmax with global connectivity. CaDA and its variants are trained and evaluated on VRP50. During testing, \u00d78aug [4] is employed.\nThe results in Table IV show the average gap on 16 VRPs and demonstrate that all components of CaDA make substantial contributions, with the prompt playing a particularly important role. Additionally, to study the effect of different components on various problems, we demonstrate the increased gap of these two ablation models compared with CaDA on different VRP datasets in Figure 4. These results illustrate that both the prompt and the sparse operation improve performance on all VRP variants.\nThen, we further explore the influence of different CaDA settings. Specifically, using CaDA as the baseline, we conduct experiments on VRP50 focusing on the following aspects:"}, {"title": "F. Visualization of Constraint Awareness", "content": "To explore the influence of the prompt, we conducted further statistical experiments on the distribution of attention scores within the encoder. All experiments mentioned below involve 100 VRP50 instances randomly selected from the test dataset, and attention scores were collected from all heads across all global layers.\na) Influence on Open Route Constraint: In the case of the Open Route constraint, where the vehicle does not return to the depot vo, the depot will never be the next node for any customer node vi. Whereas in CVRP, the vehicle must return to the depot, making it a potential next node for any customer. As a result, the model should exhibit different customer-depot attention patterns for CVRP and OVRP. Specifically, in CVRP, the model should exhibit a greater number of high attention scores Aio compared to OVRP.\nFigure 7 shows the distribution of A20 for CaDA and CaDA w/o Prompt on CVRP and OVRP. We apply Kernel Density Estimation (KDE) with Gaussian kernels to estimate the distribution of attention weights between 0.2 and 0.8."}, {"title": "G. Result on Real-World Instances", "content": "To further validate the effectiveness of CaDA in real-world instances, we conducted experiments using sive test suites from CVRPLib\u00b9 benchmark datasets. These datasets comprise"}, {"title": "VI. CONCLUSION", "content": "In this paper, we have proposed the Constraint-Aware Dual-Attention Model (CaDA), a novel cross-problem neural solver for VRPs. CaDA incorporates a constraint prompt and a dual-attention mechanism which consists of a global branch and a sparse branch, to efficiently generate constraint-aware node embeddings. We have thoroughly evaluated CaDA across 16 VRP variants and real-world benchmark instances. CaDA shows superior performance when compared to current leading neural solvers. Additional ablation studies confirm the effectiveness of the proposed constraint prompt and the dual-attention model structure."}]}