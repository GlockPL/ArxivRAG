{"title": "Selective Reviews of Bandit Problems in AI via a Statistical View", "authors": ["Pengjie Zhou", "Haoyu Wei", "Huiming Zhang"], "abstract": "Reinforcement Learning (RL) is a widely researched area in artificial intelligence that\nfocuses on teaching agents decision-making through interactions with their environment. A key\nsubset includes stochastic multi-armed bandit (MAB) and continuum-armed bandit (SCAB) problems,\nwhich model sequential decision-making under uncertainty. This review outlines the foundational\nmodels and assumptions of bandit problems, explores non-asymptotic theoretical tools like concentra-\ntion inequalities and minimax regret bounds, and compares frequentist and Bayesian algorithms for\nmanaging exploration-exploitation trade-offs. We also extend the discussion to K-armed contextual\nbandits and SCAB, examining their methodologies, regret analyses, and discussing the relation\nbetween the SCAB problems and the functional data analysis. Finally, we highlight recent advances\nand ongoing challenges in the field.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) is one of the most prominent and widely discussed\nmethods in artificial intelligence, primarily focusing on how an agent learns to make\ndecisions by interacting with an environment to maximize cumulative rewards [1]. RL\nhas seen extensive applications in various domains, including autonomous driving [2],\nrecommendation systems [3], unmanned aerial vehicles (UAVs) [4], financial trading [5],\ncausal inference [6], and precision medicine [7,8]; see [9,10] for a review.\nThe classic and simplified problem in RL is the stochastic bandit problems. Stochastic\nbandit problems exemplify the exploration-exploitation tradeoff dilemma, where an agent\nmust choose between exploring new options to gather more information and exploiting\nknown options to maximize rewards.\nThe current review literature on stochastic bandit algorithms highlights applications\nin areas such as recommendation systems[11-13], experimental design[14], and precision\nmedicine[8], causal inference[15]. Efficient bandit algorithms are designed from a statistical\nperspective. However, these aspects remain underexplored in existing reviews. This paper\naims to address this gap by focusing on the probabilistic and statistical foundations of\nstochastic algorithms, with particular emphasis on concentration inequalities, minimax\nrate of regret upper bounds, small-sample statistical inferences, linear models, Bayesian\noptimization, statistical learning theory, design of experiments, the Neyman-Rubin causal\nmodel, functional data analysis, robust statistics, information theory, and so on."}, {"title": "1.1. Stochastic Bandit in RL", "content": "A stochastic bandit, from a statistician's perspective, can be represented as a collection\nof possible distributions of populations for the reward random variable (r.v.) Ya ~ Pa of\neach action a\nv = {Pa : a \u2208 A},\nwhere A is the set or space of available actions a \u2208 A.\n\u2022\n\u2022\n\u2022\nThe agent and the environment interact sequentially over T rounds:\nIn each round t \u2208 {1, ..., T} =: [T], the agent selects At \u2208 A, which is then communi-\ncated to the environment. Here T \u2208 N is the horizon (the total number of steps).\nGiven an action At, the environment generates a reward X\u2081 \u2208 R, drawn from the\ndistribution PA\u0141, and discloses the reward Xt to the agent.\nThis interaction between the agent and the environment induces a probability distri-\nbution over the sequence of outcomes (A1, X1, A2, X2, . . ., AT, \u0425\u0442).\nThe time horizon T is finite due to budgetary constraints (non-asymptotic theory)\nin some cases, but we may assume an infinite horizon T = \u221e (asymptotic theory) in\ntheoretical settings. The sequence of outcomes typically satisfy the assumptions [16]:"}, {"title": "1.2. Structured and Unstructured Bandits", "content": "In numerous practical applications of stochastic bandit problems, it is often unrealistic\nto assume that the bandit instance, denoted by v, is fully specified or follows a parametric\ndistribution. Instead, we often possess only partial information regarding its distribution.\nTo capture this uncertainty, we define a set of bandit instances E, which encompasses all\npossible distributions to which v could belong. This set & is referred to as the environment\nclass [16]. The classification of bandits into structured and unstructured environments\nis crucial in statistical inference, ranging from mean estimation to regression prediction.\nStructured bandits incorporate additional information or dependencies between arms,\nwhich can be exploited to improve decision-making. In contrast, unstructured bandits\ncorrespond to the classical formulation of the bandit problems, where each arm operates\nindependently, and no further relationships or information between arms are available.\nThis distinction has a profound impact on the design and efficiency of the policy \u03c0.\nDefinition 1. An environment class E is unstructured if A is finite and there exist sets of distribu-\ntions Ma for each a \u2208 A such that\nE = {v = (Pa : a \u2208 A) : Pa \u2208 Ma for all a \u2208 A}.\nKey Characteristics:\n\u2022\n\u2022\nIndependence: Each arm a yields rewards from an unknown probability distribution\nPa independently of other arms.\nNo Side Information: There are no features or context associated with the arms.\nMoreover, environment classes play a pivotal role in determining the performance of\nlearning algorithms. Parametric environments, such as Bernoulli and Gaussian bandits,\nassume specific density functions. For non-parametric classes, like sub-Gaussian and\nsub-exponential bandits, do not rely on a density function assumption but are instead\ncharacterized by conditions on their moment generating functions (see Section 2). The\ncorrect specification of the environment is critical; failure to do so, or relying on an in-\ncorrect model, can significantly degrade the algorithm's efficacy [16,17]. Depending on\nthe underlying data-generating mechanism, the choice of environment spans a range of\ndistributions-from bounded distributions to those that are light-tailed or heavy-tailed.\nThis forms the foundation of many problems in reinforcement learning and decision theory.\nIn this review, we focus on bounded, sub-Gaussian, and sub-exponential bandits,\nwhich are characterized by finite moment generating functions [18]. A detailed treatment\nof heavy-tailed bandits, such as those with sub-Weibull distributions or distributions with\nfinite moments (or even infinite variance), typically requires additional techniques (see\nSection 2.2 in [19] and [17]) and is left for future work.\nWhen A = [K] in stochastic bandit models with K\u2208 N, the problem reduces to a\nmulti-armed bandit (MAB) problem.\nExample 1 (K-armed bandits). A = [K] is finite and Ma only contains one probability measure\nfor a fixed a \u2208 A.\nIn the field of statistics, the time-uniform confidence sequence problem [20] is often\nframed as a MAB problem, first introduced by [21] in the context of sequential experimental\ndesign. The topic has been extensively studied in the statistical literature, with significant\ncontributions documented in major journals. For a comprehensive review, see Section 1\nin [16]. There are several compelling reasons to begin the study of bandit problems with\nMAB problems. First, their simplicity makes them relatively straightforward to analyze,\nproviding a deep understanding of the fundamental trade-off between exploration and\nexploitation. Second, many algorithms designed for finite-armed bandits, along with\nthe underlying principles, can be generalized to more complex settings. Lastly, finite-\narmed bandits have practical applications, particularly as an alternative to A/B testing,"}, {"title": "1.3. Stochastic Continuum-Armed Bandits", "content": "Stochastic Continuum-Armed Bandits (SCAB, [29]) extend the classical K-armed ban-\ndit problem by allowing the set of possible actions (or arms) to lie in a continuous space\nA rather than in a discrete set A = [K] or N. In this framework, the agent aims to find\nthe optimal reward point from a continuous domain, often modeled as an interval or a\nsubset of Rd. In statistics, the optimal design in SCAB problems belongs to optimal design\nof experiments([30]) and Bayesian optimization [31]. Statistical analysis on the data with\na continuous domain A is closely related to functional data analysis. The reward func-\ntion in SCABs is typically assumed to possess a degree of smoothness, meaning that it\ncan be represented as a stochastic process that exhibits regularity over space, such as a\nLipschitz-continuous function or a Gaussian process [32].\nThe fundamental objective in SCABs, as in the traditional bandit setting, is to effectively\nbalance exploration\u2014sampling from various points in the continuous action space to gather\ninformation about the reward function\u2014and exploitation\u2014leveraging current knowledge\nto select actions believed to yield the highest expected reward. The continuous nature of\nthe action space introduces additional complexity, as the agent must navigate an infinite\nnumber of potential actions, requiring more advanced algorithms for efficient exploration\nand optimization. These methods often rely on the smoothness of the reward function to\nguide the search for optimal actions while minimizing cumulative regret.\nFormally, we focus on the sequential optimization of an unknown reward function\nf : D \u2192 R. In each round t, the process unfolds as follows:\n1.\nAction Selection: The agent chooses a fixed point x\u2081 \u2208 D and receives an observed\nvalue perturbed by noise et:\nYt = Yt(xt) = f(xt) + et, where E[yt] = f(xt)."}, {"title": "2. Concentration inequalities", "content": "In the machine learning, drawing conclusions with minimal data assumptions is\nfundamental. Typically, inference relies on confidence intervals under specific distributional\nassumptions [33]. However, exact distributions are often unavailable or too complex.\nInstead, we may assume the data belongs to sub-classes like sub-Gaussian [34] or sub-\nexponential [35] distributions. These assumptions are widely used in non-asymptotic\ninference and machine learning to derive concentration inequalities with exponential decay."}, {"title": "2.1. Basic Concentration Inequalities", "content": "Concentration inequalities (CI) are a commonly used method to quantify the degree\nof concentration of a measure. Specifically, concentration inequalities quantify the extent to\nwhich a random variable X deviates from its mean EX = \u00b5 by expressing the measure of\nconcentration of X u through one-sided or two-sided tail probabilities (denoted by t > 0\nfor deviation):\nP(X \u2212 \u03bc > t) < \u03b4\u0142 or P(|X \u2212 \u03bc| > t) \u2264 dt, \t > 0\n(3)\nwhere dt is the tail probability estimate. dt can be arbitrary small for suitable large t.\nFurthermore, from the equality of tail probabilities concerning expectation (see Theo-\nrem 12.1(1) in [36]), one can derive that:\nEX \u2212 \u03bc| = \u222b\u221eP(|X \u2212 \u03bc| > t) dt \u2264 \u222b\u221e\u03b4t dt.\n(4)\nThus expectation bounds can be viewed as concentration inequalities after doing a integral\ntransform. Conversely, these expectation bounds also determine tail probabilities directly\nthrough the widely-used Markov's inequality, which we present here.\nLemma 1 (Markov's Inequality). Let q(x) : R \u2192 R+ be a non-decreasing function. For r.v. X\nwith E[\u03c6(X)] <8,\nP(X \u2265 a) \u2264 \\frac{E[\u03c6(X)]}{\u03c6(\u03b1)} Va e R.\n(5)\nProof. By the positivity and the non-decreasing of q, we get\nP(X \u2265 a) = E[I{X > a}] \u2264 E[\\frac{I{X > a}}{\u03c6(\u03b1)}] \u2264 \\frac{E[\u03c6(X)]}{\u03c6(\u03b1)}."}, {"title": "2.2. Sub-Gaussian Concentration Inequalities", "content": "When the r.v.s are unbound like Gaussian r.v., the classical Hoeffding's inequality fails\nto do non-asymptotic analysis. We need the concepts of sub-Gaussian r.v.s to obtain similar\nHoeffding-type concentration inequalities for the sum of independent r.v.s. In statistical\nmachine learning research, it is typically assumed that the data is a r.v. X (particularly\nunbounded) and satisfies the Gaussian moment generating function EesX \u2248 eVar(X)s\u00b2/2 or\nthe tail probability P(|X| \u2265 x) \u2264e-x\u00b2/[2Var(X)]; see [40]. The sub-class distribution is defined\nby the Gaussian upper bound for MGF:\nDefinition 2. A zero-mean r.v. X \u2208 R satisfies:\nE[esX] \u2264 eo2s2/2, Vs \u2208 R\n(11)\nand it is said to follow a sub-Gaussian distribution with variance proxy o\u00b2 (\u03a7 ~ subG(02)).\nAssuming X ~ subG(02), according to Chernoff's inequality, we obtain\nP(X > t) < inf_{s>0} E[e^{sX}] < inf_{s>0} e^{-st+\\frac{o^2 s^2}{2}} =e^{-\\frac{t^2}{2o^2}},s = t/o^2.\n(12)\nSimilarly, we have P(-X > t) < e-t\u00b2/(202), and thus, P(|X| \u2265 t) \u2264 2e-t\u00b2/(202).\nBy independence, the above concentration of a single sub-Gaussian r.v. can be easily\nextended to the concentration inequality for the sum of independent sub-Gaussian r.v.s.\nTheorem 1 (Concentration for the sum of sub-Gaussian r.v.s). Assume {X}_1 are indepen-\ndent zero-mean r.v.s with X\u2081 ~ subG(07). Then, we have\n1.\nfor t \u2265 0, \u2211=1 Xi ~ subG(\u2211=107) and\nP(\u2211{i=1}Xi\u2265t)\u22642exp{\u22121}2021\n(13)\n2.\nFinite mixture sub-Gaussian:\nm\n\u03a3pi subG(\u03c32)~ sub(max \u03c32) for \u03a3pi = 1, pi \u2265 0, m < \u221e,\ni=1\n(14)\nwhere we define Z ~ \u03a3=1 Pi subG(o?) if Z ~ subG(o?) with the probability pi > 0.\n3.\nIf X ~ subG(02), then\nE|X|k < (202)k/2k\u0393(\\frac{k}{2}) and ||X||k := [E(|X|k)]1/k < \u03c3e1/ek1/2, k \u2265 2.\n4.\nIf X ~ subG(02), then o\u00b2 > Var X."}, {"title": "2.3. Do Statistical Inference for Bandit Problems in a Non-Asymptotic Way", "content": "After assuming the sub-class distributions for the population, the classical limit theory\n[48] in probability theory enables us to do a large sample study of the estimators represented\nas the sum of independent random variables. In bandit problems, large T asymptotic\nanalysis of regret Reg\u012b (\u03c0, v) is established in [23]; see Section 16 for more discussion.\nThe law of large number (LNN) and the central limit theory (CLT) ensure the sample\nmean as an estimator can strongly and weakly converge to the population mean and\nnormal variable, respectively. Mathematically, let us consider independent and identically\ndistributed (i.i.d.) r.v.s X1, . . ., Xn drawn from a distribution P on R, where both u = \u0395[Xi]\nand 02 = Var(Xi) are finite. LLN provides fundamental insights into the convergence\nbehavior of the sample mean. The weak law of large numbers(WLLN) states that \u00cen converges\nin probability to \u03bc (\u25ba) as the sample size n approaches infinity, i.e.\nlim_{n\u2192\u221e} P(|Xn - \u03bc| < \u0454) = 1, \u2200 \u0454 > 0.\nThe strong Law of Large Number strengthens WLLN by asserting that \u00cen converges to \u03bc\nalmost surely:\nPlim_{n\u2192\u221e} (Xn = \u03bc)= 1.\nWhen 02 < \u221e, the central limit theorem (CLT) describes the asymptotic distribution of the\nnormalized sample mean:\n\\sqrt{n}(\\frac{X_n - \u03bc}{\u03c3}) = N(0,1),\nwhere \u25ba denotes convergence in distribution, i.e. limn\u2192\u221eP (\u221a(x\u2212u) \u2264 u) = \u03a6(\u0438),\nwhere \u0424(\u0438) = P(Z < u) is the cumulative distribution function of the standard normal\ndistribution. The CLT implies that for sufficiently large n, the tail probabilities of the\nstandardized sample mean can be approximated using the standard normal distribution\nP(| \\frac{X_n - \u03bc}{\u03c3} \\sqrt{n} | > u ) \u2248 P(|Z| > u) = 2\u0424(-\u0438), Z ~ N(0,1).\nThis approximation directly implies feasible conclusions of some statistics like \u017dn under\nthe large sample of data. The early work [33] by Fisher defined the criterion of efficiency:\n\"those statistics which, when derived from large samples, tend to a normal distribution\nwith the least possible standard deviation\". To this point, classical asymptotic analysis has"}, {"title": "3. Bandit Algorithms", "content": "As is well known, probability theory was born in casinos with the purpose of making\nmoney, and the earliest reinforcement learning also originated from casinos. The MAB\nproblem also comes from a gambling game in casinos: In a casino, a gambler is presented\nwith K slot machines that look identical, but each machine has an unknown and random\nreward distribution. The gambler has T opportunities to pull the lever of the machines.\nHow should the gambler choose the next action based on the outcomes of previous actions\nto maximize cumulative rewards?\nAt each time step t \u2208 [T], the agent selects the arm At \u2208 [K] and receives a reward\n{rk(t)}t\u2208 [T] drawing from an unknown distribution Pk (assuming the k-th arm is selected).\nThis rk(t) is characterized by conditional reward r\u2081\u2081 (t) on the random action At\nk:\nrk(t) = ra\u2081(t) | {A\u2081 = k} = X} | {A\u2081 = k},\n(15)\nwith E[ra\u2081(t) | At] = \u03bc\u03b1\u2081(v) and X\u2081 = ra\u2081(t) with the RL notation in Section 1.1.\nAssuming that the optimal arm is denoted by k*, the criterion for the optimal sequence\nof actions {At}t\u2208[T] is to minimize the cumulative regret, defined as:\nReg\u2081(r,v) := \u03a4\u03bc\u03ba* \u2013 E[\u2211Xt]=\nT\u03bc_{k*}-E\\sum_{t=1}^T E[Xt | A_t]=E[\\sum_{t=1}^T(\u03bc_{k*}-\u03bc_{A_t}(v))].\n(16)\nTo achieve minimal regret, the agent must resolve the exploration-exploitation dilemma.\nSpecifically, the agent must decide whether to \"exploit\" the current information by pulling\nthe arm with the highest known average reward to maximize immediate payoff, or to\n\"explore\" arms with greater uncertainty, which may lead to discovering a better strategy\nand securing higher returns in the long run.\nLet v be a stochastic bandit and define \u2206k(v) = \u03bc*(v) \u2013 \u03bc\u3047(v) as the suboptimality gap\nof action k. Let Sk(t) = \u2211=1I{A5 = k} represent the number of times arm k has been\nselected up to round t. The cumulative regret can be decomposed as follows, which is\nuseful to derive the regret upper bounds.\nLemma 6 (Regret decomposition lemma). For any policy \u03c0 and stochastic bandit v, we have\nRT(\u03c0,\u03c5) = \u03a3a\u2208A\u2206a(v)E[Sa(T)] for finite or countable A, T \u2208 N.\nProof. For any fixed t we have \u2211a\u2208A I{At = a} = 1. Hence, the sum of rewards is\nT\nT\nSn = \u03a3 Xt = \u03a3\u03a3X\u2081I{A\u2081 = a},\nt=1\nt=1 a\u2208A"}, {"title": "3.1. Explore-Then-Commit Algorithm", "content": "The basic idea of the Explore-Then-Commit (ETC) algorithm is to divide the search\nprocess for the optimal arm in the MAB problems into two distinct phases: the exploration\nphase and the exploitation phase.\n\u2022\n\u2022\nIn the exploration phase, the algorithm pulls each arm a fixed number of times to\nestimate its expected reward.\nIn the exploitation phase, the algorithm selects the arm with the highest estimated\nreward based on exploration results and continues to select it.\nSpecifically, the ETC algorithm is described as follows: the algorithm conducts m rounds of\nexploration for each arm during the exploration phase. When t \u2264 mK, that is, during the\nfirst mK selections, each of the K arms is pulled once per round according to a certain rule.\nAfter t > mK, the algorithm will always select the arm that performed the best during the\nexploration phase. Let \u00fbk(t) be the average reward for selecting arm k after t rounds:\n\u00fbk(t) = \\frac{1}{S_k(t)}\u2211_{s=1}^t I{A_s = k}r_k(s),\nThe pseudocode of the ETC algorithm is as algorithm 1.\nAlgorithm 1 Explore-then-Commit (ETC)\n1: Input: Total arms K, number of exploration steps m, horizon T > mK.\n2: In round t choose arm:\nAt={\n (t mod K) +1 if t < mK \n arg maxk\u2208[K] \u00fbk(mK) if t > mK\n}\nRegarding the regret of the ETC algorithm, we have the following Theorem 4.\nTheorem 4. When ETC is interacting with any v := subG(1) bandit and 1 < m < T/K, the\nregret of ETC satisfies:\nReg\u03c4(r,\u03c5)\u2264K\u03a3{1}\u0394i+(T\u2212mK)\u2211{1}\u0394ie\u2212m\u03942i4\n(17)\nwhere \u2206k = \u00b5k* \u2014 \u00b5k represents the expected reward gap between the optimal arm and the arm k."}, {"title": "3.2. Upper Confidence Bound Algorithm", "content": "The Upper Confidence Bound (UCB) algorithm is a strategy that remains optimistic\nunder uncertainty (see [61,62]). The core of the algorithm lies in using the data observed\nso far to assign a value to each arm, called the upper confidence bound, which is a high-\nprobability upper estimate of the unknown mean.\nAt time t, the estimate of \u03bc\u03bc(v) is based on information from previous steps s =\n1, 2, . . ., t \u2212 1. Using probability techniques(concentration inequalities or Gaussian approxi-\nmations), a non-asymptotic 100(1 \u2013 a)% confidence interval is derived:\n\u03bc\u03b5(\u03bd) \u2208 [\u00fbk(t \u2212 1) \u2212 ck(t \u2212 1), \u00fbk(t \u2212 1) + ck(t \u2212 1)].\nStatistically, this means estimating the potential reward of each option using confidence\nintervals and quantifying the confidence in these estimates, for example, by using a 95%"}, {"title": "3.3. The Minimax Lower Bound in Instance-Dependent MAB Problems", "content": "For the UCB algorithm of MAB problems, a fundamental question arises: Is the\nobtained regret rate of O(\u221aKT log T) for the exploitation term in the regret upper bound, as\nstated in Theorem 1, truly optimal? The answer to this question has profound implications\nfor both theory and practice in statistical learning and decision-making under uncertainty.\nTo answer it, we turn to the establishment of a minimax lower bound on the regret, a\ncornerstone concept from non-parametric statistical theory.\n\u2022\n\u2022\n\u2022\nMotivation:\nOptimality. Establishing a minimax lower bound allows us to rigorously demonstrate\nthat no algorithm can achieve a better regret rate in the worst-case setting. This is\ncrucial for confirming that the sub-Gaussian UCB algorithm is not just efficient within\nthe class of all possible algorithms.\nInformative Lower bounds. Lower bounds often provide deeper insights than upper\nbounds because they highlight the intrinsic difficulty of the problem itself, independent\nof any specific algorithm. They serve as a benchmark for assessing the performance of\nexisting and future algorithms.\nUnderstanding Problem Complexity. By identifying the fundamental challenges and\nlimitations inherent in the problem through lower bounds, we gain valuable insights\ninto what makes the problem hard. This understanding is essential for designing\nnew algorithms that can effectively address these challenges and for advancing the\ntheoretical foundations of machine learning."}, {"title": "3.3.1. A Lower Bound on the Minimax Regret for Sub-Gaussian Bandits", "content": "The minimax lower bound in Theorem 6 provides a theoretical limit on the perfor-\nmance that any algorithm can achieve in the worst-case scenario. By establishing this bound,\nwe can ascertain whether the regret rate of UCB algorithms matches this fundamental limit,\nthereby confirming its optimality.\nTheorem 6 (Theorem 15.1 in [16]). Let K > 1,T > K \u2212 1. Given \u03bc := (\u03bc1,\u2026\u2026, \u03bc\u03ba) \u2208 RK\nlet vu be the Gaussian bandit for which the i-th arm has reward N(\u00b5\u2081,1). For any \u03c0, there exists a\n\u03bc\u2208 [0,1]\u039a such that"}, {"title": "3.3.2. Minimax Optimal Strategy in the Stochastic Case", "content": "MOSS (Minimax Optimal Strategy in the Stochastic case, [63]) is an algorithm designed\nto minimize regret in the worst-case scenario, specifically tailored for MAB problems in\nstochastic environments. The core idea of MOSS lies in constructing confidence intervals\n, where the confidence level depends on each arm's historical number of pulls, the time\nhorizon T, and the number of arms K. This approach avoids over-exploration or premature\nexploitation, thus maintaining a balance between exploration and exploitation throughout\nthe process to achieve minimax regret under subG(1) rewards. [63] replaced the factor\nT\nlog(1/8) in (20) with d = T-2 by an adaptive factor log+ (KS(1-1)), and proved that the\nMOSS algorithm attains the minimax lower bound in Theorem 6; see [62] for more details.\nAlgorithm 4 Minimax Optimal Strategy in the Stochastic case (MOSS)\n1: Input: K,T.\n2: Choose each arm once.\n3: Subsequently choose:\nAt = arg max ke[K] (Ak(t-1)+ \\frac{4}{S_k(t-1)} log^+(\\frac{T}{KS_k(t-1)}))\nwhere log+(x) = log max{1,x}.\nThis advancement underscores the importance of carefully designing the exploration\ncomponent in bandit algorithms. By tuning the exploration function to more precisely\nreflect the uncertainty and potential of under-explored arms, MOSS effectively balances\nexploration and exploitation. This balance is crucial for minimizing regret and achieving\noptimal performance over the time horizon.\nUsing Doob's submartingale inequality, Theorem 9.1 in [16] obtained the following\nregret bound of the MOSS algorithm.\nTheorem 7. For any v = subG(1) bandit, the regret of MOSS Algorithm 4 satisfies:\nR\u2081(\u03c0,\u03c5) \u2264 39\u221aKT + \u03a3\u0394k.\n(25)\nk=1\nAccording to Theorem 6, Algorithm 4 achieves minimax optimality."}, {"title": "3.4. Thompson Sampling Algorithm", "content": "Thompson Sampling (TS, [64]) is an algorithm based on Bayesian inference originating\nfrom [65]. The core idea is to continuously update the posterior distribution of each arm's\nreward using historical data, and to sample from this distribution to decide the next action.\nDuring the first K time steps, the algorithm plays each arm k \u2208 [K] once and updates\nthe estimated average reward \u00fbk (K + 1) for each k. In the subsequent steps t = K + 1, ..., T,\nthe algorithm samples instances \u03b8\u3047(t) for all k \u2208 [K] from a certain distribution Nk(t \u2212 1)\nwith information at time step t \u2013 1. The algorithm then selects the arm that maximizes 0k (t)."}, {"title": "3.5. Minimax Optimal Thompson Sampling Algorithm", "content": "Minimax Optimal Thompson Sampling (MOTS, [66]) algorithm is an improvement of\nthe classical TS algorithm by introducing a truncation mechanism for the arm reward dis-\ntribution. The core idea is that, at each time step, the algorithm samples from the posterior\ndistribution of each arm, but performs a truncation on the sampling results to avoid overes-\ntimating suboptimal arms and underestimating the optimal arm. Specifically, MOTS uses\ntruncated normal distributions to adjust the estimation of arm rewards. This mechanism\neffectively enhances the robustness of the algorithm when dealing with suboptimal arms\nand reduces the probability of selecting suboptimal arms incorrectly.\nThe theoretical analysis of the MOTS algorithm shows that, within a finite time horizon\nT, the algorithm can achieve a minimax regret upper bound of O(\u221aKT), which is problem-\nindependent. This addresses the limitation of the traditional TS algorithm, which is unable\nto reach this optimal regret bound. This improvement allows MOTS to demonstrate more\nrobust performance in complex decision environments, significantly reducing the growth\nrate of cumulative regret. The pseudocode of the MOTS algorithm is as Algorithm 7."}, {"title": "4. K-Armed Contextual Bandit", "content": "A limitation of standard MAB is that the environment remains constant for every\nround. In practical applications, decision-making often relies on covariate information\nto improve the precision and effectiveness of decisions. For example, in healthcare, an\nindividual's treatment decision may depend on patient-specific characteristics such as\ngenetic background, lifestyle, biomarkers, and environmental factors. Ignoring these\ncovariates could result in suboptimal or even incorrect treatment plans.\nContextual information, such as in advertising recommendation systems and medical\ndiagnosis, is crucial for making accurate decisions. Unlike traditional MAB problems that\nconsider only mean rewards, multi-armed contextual bandits incorporate both contextual\ninformation (or features, covariates, inputs in statistics and machine learning) and the\nindependent reward distributions of each arm. This allows the algorithm to adapt better to"}, {"title": "4.1. Linear Upper Confidence Bound for Specific Arms", "content": "A common approach to solving this is the stochastic K-armed contextual bandit\nproblem [24]. At each time step t, the algorithm receives the covariate vector xt,k \u2208 Rd for\neach arm k and selects an arm kt. The reward rk(t) is then observed, and the algorithm uses\nthe observed context xt,k, selected arm k, and reward r\u3047(t) to update its strategy.\nWe assume that the expected reward for the arm k is a linear function of its d-\ndimensional feature vector xt,k, with an unknown coefficient vector \u03b8k \u2208 Rd; specifically,\nfor all t, along with a noise term \u0ed7t,k ~ subG(02), i.e. disjoint linear models,\nrk(t) = x+kok+Nt,k,\n(28)\nWhen d = 1 and xt,k is fixed as 1, it reduces to the standard MAB setting since\nErk(t) = \u03b8\u03ba.\nFor each arm k, assuming the parameter \u03b8k is bounded, the loss function at time t \u2013 1\nis defined as the ridge penalized least square :\n\u2211^{t-1}_{s=1}(r_k(s)\u2212x_{s}^{T}\u03b8_{k})^2 + \u03bb||\u03b8_{k}||^2.\nwhere x > 0 is a tuning parameter. The estimate of the parameter \u03b8k is obtained through\n\u03b8k(t \u2212 1) = (\u03bbI + \u03a3x_s,kx^T_s,k)^{-1} \u03a3 r_k(s)x_{s,k},\nIn each round t of the experiment, the algorithm selects an arm A\u2081 \u2208 [K], where the\noptimal arm is denoted as k, defined by\nk = arg max_{k \\in [K]} x^T_{t,k}\u03b8_{k}.\nHaving observed a new context x\u2081\u2081k for arm k, it is suggested in [24] that the UCB is\nargmaxke[K]\\frac{x^T_{t,k}\u03b8_{k}(t-1)}{\\sqrt{x^T_{t,k}(\u03bbI+ \\sum{s=1}{t-1}x{s}x^T_{s})x_{t,k}}}.\n(29)\nwhere a > 0 is a parameter that controls the exploration-exploitation trade-off.\nFollowing a principle similar to the UCB algorithm, LinUCB selects the arm with\nthe highest UCB. This approach enables LinUCB to effectively balance the exploitation\nof known rewards with the exploration of new information, progressively improving\ndecision-making accuracy. The pseudocode for the LinUCB algorithm is as Algorithm 8."}, {"title": "4.2. General Linear Upper Confidence Bound", "content": "Below, we introduce the general LinUCB algorithm for linear models with a common\nregression parameter vector. At each time step t, the MAB receives K feature vectors\nXt,1, Xt,2,\u00b7\u00b7\u00b7, Xt,K\u2208 Xt, where X\u2081 \u2286 Rd. It is assumed that the reward obtained by each arm\nis a linear function of its respective feature vector, where the parameter vector 0 is fixed\nand identical for all arms. Assuming that the noise nt,k ~ subG(02) =: \u03b7, we have:\nrk(t) = \\chi^T_{t,k}\u03b8 + N_{t,k}\n(30)"}, {"title": "4.3. Thompson Sampling for Linear Contextual Bandits", "content": "Using the notation defined above, suppose the rewards satisfy the condition in (30).\nFurther, we assume that ||Xt,k|| < 1, ||0|| < 1, and \u2206t < 1.\nFollowing the idea of the TS algorithm, we design the algorithm using a Gaussian\nlikelihood function and a Gaussian prior. More precisely, suppose that at time t, given\nfeature vectors x\u2081\u2081k and parameter 8, the reward rk(t) satisfies\nrk(t) \u223c N(x^T_{t,k}\u03b8,\u03c32),\nwhere v is a constant used to parametrize the algorithm. Then, the posterior distribution of\nthe parameter 0 at time t follows:\nN(0t\u22121,\u03c3^2 \u03a3^{\u22121}_{t}).\nAt each time step t, a sample \u0253t is simply drawn from this distribution, and the arm is\nselected to maximize x\u0113t.\nThe pseudocode of the Thompson Sampling for Linear Contextual Bandits (LinTS)\nalgorithm is as Algorithm 10."}, {"title": "5. Stochastic Continuum-Armed Bandits Algorithms", "content": "A stochastic continuum-armed bandit algorithm (policy) \u03c0 = {A\u2081,...,AT"}]}