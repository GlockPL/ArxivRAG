{"title": "Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition", "authors": ["R\u00f3is\u00edn Luo (Jiaolin Luo)", "James McDermott", "Colm O'Riordan"], "abstract": "Perturbation robustness evaluates the vulnerabilities of models, arising from a variety of perturbations, such as data corruptions and adversarial attacks. Understanding the mechanisms of perturbation robustness is critical for global interpretability. We present a model-agnostic, global mechanistic interpretability method to interpret the perturbation robustness of image models. This research is motivated by two key aspects. First, previous global interpretability works, in tandem with robustness benchmarks, e.g. mean corruption error (mCE), are not designed to directly interpret the mechanisms of perturbation robustness within image models. Second, we notice that the spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially decay over the frequency. This power-law-like decay implies that: Low-frequency signals are generally more robust than high-frequency signals yet high classification accuracy can not be achieved by low-frequency signals alone. By applying Shapley value theory, our method axiomatically quantifies the predictive powers of robust features and non-robust features within an information theory framework. Our method, dubbed as I-ASIDE (Image Axiomatic Spectral Importance Decomposition Explanation), provides a unique insight into model robustness mechanisms. We conduct extensive experiments over a variety of vision models pre-trained on ImageNet, including both convolutional neural networks (e.g. AlexNet, VGG, GoogLeNet/Inception-v1, Inception-v3, ResNet, SqueezeNet, RegNet, MnasNet, MobileNet, EfficientNet, etc.) and vision transformers (e.g. ViT, Swin Transformer, and MaxViT), to show that I-ASIDE can not only measure the perturbation robustness but also provide interpretations of its mechanisms.", "sections": [{"title": "1 Introduction", "content": "Image modeling with deep neural networks has achieved great success (Li et al., 2021; Khan et al., 2022; Han et al., 2022). Yet, deep neural networks are known to be vulnerable to perturbations. For example, the perturbations may arise from corruptions and adversarial attacks (Goodfellow et al., 2014; Hendrycks &\nDietterich, 2019; Szegedy et al., 2013), etc. Perturbation robustness, henceforth referred to as robustness,\ncharacterizes a crucial intrinsic property of models (Hendrycks & Dietterich, 2019; Bai et al., 2021; Goodfellow\net al., 2014; Silva & Najafirad, 2020).\nRobustness mechanisms refer to the mechanisms which lead to robustness in models. The study of robustness\nmechanisms aims to answer the question 'why some models are more robust than others' (Lipton, 2018;\nZhang et al., 2021; Bereska & Gavves, 2024). The causes within this question can arise from multifarious"}, {"title": "2 Notations", "content": "Image classifier. The primary task of an image classifier is to predict the probability distributions over\ndiscrete classes for given images. We use $Q(y|x; \\theta) : (x,y) \\rightarrow [0,1]$ to denote a classifier in the form of\nconditional probability. The $Q$ predicts the probability that an image $x$ is of class $y$. The $\\theta$ are the parameters.\nFor brevity, we ignore the parameter $\\theta$. For example, we denote $Q(y|x;\\theta)$ as $Q(y|x)$.\nDataset and annotation. We use a tuple $(\\mathcal{X}, \\mathcal{Y})$ to denote an image classification dataset, where $\\mathcal{X}$ is the\nimage set and $\\mathcal{Y}$ is the label set. We use $|\\mathcal{Y}||$ to denote the number of classes (i.e. the cardinality of set $\\mathcal{Y}$).\nThe annotation task of image classification datasets is to assign each image with a discrete class probability\ndistribution. We use $P(y|x)$ to denote the ground-truth probability that an image $x$ is assigned as a class $y$.\nWe use $P(x)$ to denote the probability of $x$ in set $\\mathcal{X}$. We use $P(y)$ to denote the probability of $y$ in set $\\mathcal{Y}$. In\nclass-balanced datasets, $P(y) = \\frac{1}{|\\mathcal{Y}|}$."}, {"title": "3 Method", "content": "High-level overview. We apply Shapley value theory for axiomatically assigning credits to spectral bands.\nWithin this framework, the specially devised characteristic function measures the information gains of spectral\nbands. I-ASIDE interprets robustness mechanisms using this axiomatic framework with the information\ntheory.\nProblem formulation. Quantifying the predictive powers of features can be viewed as a value decomposition\nproblem. In this research, the value is the information quantities that the features contribute to decisions."}, {"title": "3.1 Spectral coalitional game", "content": "Spectral player. We use $I_i$ (where $i \\in [M] := \\{0,1,\\ldots, M - 1\\}$) to denote the $i$-th spectral player. The\n$I_0$ contains the most robust features and the $I_{M-1}$ contains the most non-robust features. The $M$ spectral\nplayers constitute a player set $\\mathcal{I} := \\{I_i\\}_{i=0}^{M-1}$. Figure 15 in Appendix C.1 shows two partition schemes to\npartition spectral bands ($l_\\infty$ and $l_2$). We empirically choose $l_\\infty$.\nSpectral coalition. A subset $\\tilde{\\mathcal{I}} \\subseteq \\mathcal{I}$ is referred to as the spectral coalition. The player set $\\mathcal{I}$ is often referred\nto as the grand coalition.\nCharacteristic function. A characteristic function $v(\\tilde{\\mathcal{I}}) : \\mathcal{\\hat I} \\rightarrow \\mathbb{R}$ measures the contribution for a given\ncoalition and satisfies $v(\\emptyset) = 0$. In this research, the contribution of $\\tilde{\\mathcal{I}}$ is measured in the form of the\nlog-likelihood expectation of the predictions by the $Q$, in which the input images only contain the signals\npresent in the $\\tilde{\\mathcal{I}}$. We show that this design of $v$ theoretically measures how much information the $Q$ uses\nfrom the features in the $\\tilde{\\mathcal{I}}$ for decisions.\nShapley value. A spectral coalitional game $(\\mathcal{I}, v)$ is defined on a spectral player set $\\mathcal{I}$ equipped with a\ncharacteristic function $v$. The weighted marginal contribution of a spectral player $I_i$ over all possible coalitions\nis referred to as the Shapley value of the spectral player $I_i$. We use $\\psi_i(\\mathcal{I}, v)$ to represent the Shapley value of"}, {"title": "3.2 Spectral coalition filtering", "content": "We represent the presences and absences of the spectral players through the signal pass-bands and stop-bands\nusing a multi-band-pass digital signal filtering (Oppenheim, 1978; Pei & Tseng, 1998; Steiglitz, 2020), as\nshown in Figure 5. For example, the example spectral coalition $\\{I_0, I_2\\}$ signifies the signals present only in\n$I_0$ and $I_2$. With the spectral coalition filtering, we are able to evaluate the contributions of the combinations\nof various spectral features. Figure 6 shows an example of $2^M$ spectral coalitions.\nTo implement the presences and absences of spectral signals, we define a mask map $T(\\tilde{\\mathcal{I}}) : \\tilde{\\mathcal{I}} \\mapsto \\{0,1\\}^{M \\times N}$\non 2D spectrum, where $\\tilde{\\mathcal{I}}$ is a spectral coalition and $M \\times N$ denotes 2D image dimensions. The mask map is\npoint-wisely defined as:\n$T(\\tilde{\\mathcal{I}})(m, n) = \\begin{cases}1, & \\text{if the frequency point (m,n) is present in coalition } \\tilde{\\mathcal{I}},\\\\0, & \\text{otherwise}\\end{cases}$"}, {"title": "3.3 Characteristic function design", "content": "The characteristic function is needed in order to measure the contributions of the features in $\\tilde{\\mathcal{I}}$. We define\nthe characteristic function as the gains of the negative cross-entropy loss values between feature presences\nand absences."}, {"title": "3.4 Spectral robustness score (SRS)", "content": "Although we are firstly interested in using the spectral importance distributions (SID) for robustness\ninterpretations, they can also be summarized into scalar scores for purposes such as numerical comparisons,\nand later correlation studies.\nAssumption 3.7 (Spectral uniformity assumption of random decisions). The second row in Figure 7 shows\nthe SIDs from various models with randomized weights. We randomize the model weights with Kaiming\ninitialization (He et al., 2015). The measured SIDs exhibit spectral uniformity. This suggests: Un-trained\nmodels do not have spectral preferences. We refer to \u2018the models with randomized parameters' as random\ndecisions. Therefore, we assume that the SIDs of random decisions are uniform: $\\frac{1}{M}$.\nAssumption 3.8 (Robustness prior). We assume: Higher utilization of robust features in decisions\nimplies robust models. This is further substantiated by the experiments in Figure 3. To reflect this\nrobustness prior, we empirically design a series $\\beta := (\\beta^0, \\beta^1,\\ldots,\\beta^{M-1})^T$ where $\\beta \\in (0,1)$ as the summing\nweights of SIDs. Empirically, we choose $\\beta = 0.75$ because this choice achieves the best correlation with model\nrobustness.\nSummarizing with weighted sum. Let $\\Psi(v)$ be the measured spectral importance distribution (SID). Set\n$\\widehat{\\Psi}(v) = \\frac{\\|\\Psi(v)-\\min(\\Psi(v))\\|_1}{\\max(\\Psi(v)) - \\min(\\Psi(v))}$ with min-max normalization. The weighted sum of the $\\widehat{\\Psi}(v)$ with the weights $\\beta$ is\ngiven by:\n$\\beta^T \\widehat{\\Psi}(v) = \\frac{1}{M}$"}, {"title": "4 Experiments", "content": "We design experiments to show the dual functionality of I-ASIDE, which can not only measure robustness\nand but also interpret robustness. We organize the experiments in three categories:"}, {"title": "4.1 Correlation to robustness metrics", "content": "Definition 4.1 (Mean prediction error). In our experiments, we measure model perturbation robustness\nwith mean prediction errors (mPE) besides the mean corruption errors (mCE). Let $x$ be some clean image\nand $x^*$ be the perturbed image. For a classifier $Q$, we define the mean prediction error (mPE) as:\n$\\Delta P := \\mathbb{E}_{x,y\\sim \\langle X, Y \\rangle} |Q(y|x) - Q(y|x^*)|$."}, {"title": "4.2 Studying architectural robustness", "content": "I-ASIDE is able to answer questions such as:\n\u2022 Does model parameter size play a role in robustness?\n\u2022 Are vision transformers more robust than convolutional neural networks (ConvNets)?\nDoes model parameter size play a role in robustness? Figure 11 (a) shows parameter counts do\nnot correlate with model robustness. Thus, the tendency of a model to use robust features is not\ndetermined by parameter counts alone. We would like to carry out further investigation in future work."}, {"title": "4.3 Interpreting how supervision noise levels affect model robustness", "content": "The previous robustness benchmarks with mean corruption errors (mCE) are not able to answer the long-\nstanding question: \"How and why label noise levels affect robustness?\". We demonstrate that\nI-ASIDE is able to answer this question.\nLearning with noisy labels. Supervision signals refer to the prior knowledge provided by labels (Sucholutsky\net al., 2023; Zhang et al., 2020; Shorten & Khoshgoftaar, 2019; Xiao et al., 2020). There is a substantial line\nof previous research on the question of \u201chow supervision noise affects robustness\u201d (Gou et al., 2021; Fr\u00e9nay &\nVerleysen, 2013; Lukasik et al., 2020; Rolnick et al., 2017). This question is not completely answered yet. For\nexample, Flatow & Penner add uniform label noise into CIFAR-10 and study its impact on model robustness\n(Flatow & Penner, 2017). Their results show that classification test accuracy decreases as the training label\nnoise level increases. However, empirical studies like this are not able to answer the underlying 'why' question.\nNoisy-label dataset. We derive noisy-label datasets from a clean Caltech101. We randomly assign a\nproportion of labels with a uniform distribution over label classes to create a noisy-label dataset. We refer to\nthe randomly assigned proportion as supervision noise level. We vary the noise level from 0.2 to 1.0 to derive\nfive training datasets.\nExperiment. We train three models (googlenet, resnet18 and mobilenet_v2) over the clean and the five\nnoisy-label datasets for 120 epochs respectively. We then measure their SIDs. The results are visualized in\nFigure 12 with heat maps. The results show that there is a pattern across the above three models in that:\nThe SIDs are more uniform with higher supervision noise levels. The interpretation regarding\nthe learning dynamics with the presence of label noise is that: Models tend to use more non-robust\nfeatures in the presence of higher label noise within training set."}, {"title": "5 Related work", "content": "We further conduct a literature investigation from three research lines: (1) global interpretability, (2) model\nrobustness, and (3) frequency-domain research. This literature study shows that I-ASIDE provides unique\ninsights in these research lines.\nGlobal interpretability. Global interpretability summarizes the decision behaviours of models from a\nholistic view. In contrast, local interpretability merely provides explanations on the basis of instances\n(Sundararajan et al., 2017; Smilkov et al., 2017; Linardatos et al., 2020; Selvaraju et al., 2017; Arrieta et al.,\n2020; Zhou et al., 2016; Ribeiro et al., 2016; Lundberg & Lee, 2017; Lakkaraju et al., 2019; Guidotti et al.,\n2018; Bach et al., 2015; Montavon et al., 2019; Shrikumar et al., 2017). There are four major research lines in\nimage models: (1) feature visualization, (2) network dissection, (3) concept-based method, and (4) feature\nimportance."}, {"title": "6 Limitations", "content": "I-ASIDE provides a unique insight into the perturbation robustness mechanisms. Yet, our method has two\nmajor limitations: (1) The spectral perspective can merely reflect one aspect of the holistic view of model\nrobustness, and (2) the SID resolutions are low.\nLimitation (1). For example, carefully crafted malicious adversarial perturbations on low-frequency\ncomponents can fool neural networks (Luo et al., 2022; Liu et al., 2023; Maiya et al., 2021). Luo et al.\ndemonstrate that attacking low-frequency signals can fool neural networks, resulting in attacks which are\nimperceptible to humans. This further implies the complexity of this research topic.\nLimitation (2). The computation cost is imposed by $O(2^M)$. Fortunately, we do not need high SID\nresolution to analyze the model robustness problem. For example, a choice with $M = 8$ is sufficient to\ninterpret robustness mechanisms (as we have shown) while the computational cost remains reasonable."}, {"title": "7 Conclusions", "content": "On the solid ground provided by information theory and coalitional game theory, we present an axiomatic\nmethod to interpret model robustness mechanisms, by leveraging the power-law-like decay of SNRs over the\nfrequency. Our method addresses the limitation that scalar metrics fail to interpret robustness mechanisms.\nWe carry out extensive experiments over a variety of architectures. The SIDs, when scalarized, can largely\nreproduce the results found with previous methods, but addresses their failures to answer the underlying\n'why' questions. Our method goes beyond them with the dual functionality in that: I-ASIDE can not only\nmeasure the robustness but also interpret its mechanisms. Our work provides a unique insight into the\nrobustness mechanisms of image classifiers."}, {"title": "A Appendix", "content": ""}, {"title": "B Fairness division axioms", "content": "Symmetry axiom: Let $\\tilde{\\mathcal{I}} \\in 2^{\\mathcal{I}}$ be some spectral player coalition. For $\\forall I_i, I_j \\in \\mathcal{I} \\land I_i, I_j \\notin \\tilde{\\mathcal{I}}$, the statement\n$v(\\tilde{\\mathcal{I}} \\cup \\{I_i\\}) = v(\\tilde{\\mathcal{I}} \\cup \\{I_j\\})$ implies $\\psi_i(\\mathcal{I}, v) = \\psi_j(\\mathcal{I}, v)$. This axiom restates the statement 'equal treatment of\nequals' principle mathematically. This axiom states that the 'names' of players should have no effect on the\n'treatments' by the characteristic function in coalition games (Roth, 1988).\nLinearity axiom: Let $u$ and $v$ be two characteristic functions. Let $(\\mathcal{I}, u)$ and $(\\mathcal{I}, v)$ be two coalition games.\nLet $(u + v)(\\tilde{\\mathcal{I}}) := u(\\tilde{\\mathcal{I}}) + v(\\tilde{\\mathcal{I}})$ where $\\tilde{\\mathcal{I}} \\in 2^{\\mathcal{I}}$. The divisions of the new coalition game $(\\mathcal{I}, u + v)$ should\nsatisfy: $\\psi_i(\\mathcal{I}, u + v) = \\psi_i(\\mathcal{I}, u) + \\psi_i(\\mathcal{I}, v)$. This axiom is also known as 'additivity axiom' and guarantees the\nuniqueness of the solution of dividing payoffs among players (Roth, 1988).\nEfficiency axiom: This axiom states that the sum of the divisions of all players must be summed to the\nworth of the player set (the grand coalition): $\\sum_{i=0}^{M-1} \\psi_i(\\mathcal{I}, v) = v(\\mathcal{I})$.\nDummy player axiom: A dummy player (null player) $I_*$ is the player who has no contribution such that:\n$\\psi_*(\\mathcal{I}, v) = 0$ and $v(\\tilde{\\mathcal{I}} \\cup \\{I_*\\}) = v(\\tilde{\\mathcal{I}})$ for $\\forall I_* \\notin \\tilde{\\mathcal{I}} \\land I_* \\subseteq \\mathcal{I}$.\nRemark B.1. In the literature (Roth, 1988), the efficiency axiom and the dummy player axiom are also\ncombined and relabeled as carrier axiom."}, {"title": "B.1 Spectral signal-to-noise ratio (SNR)", "content": "Discrete Fourier Transform. The notion 'frequency' measures how 'fast' the outputs can change with\nrespect to inputs. High frequency implies that small variations in inputs can cause large changes in outputs.\nIn terms of images, the 'inputs' are the pixel spatial locations while the 'outputs' are the pixel values.\nLet $x : (i, j) \\rightarrow \\mathbb{R}$ be some 2D image with dimension $M \\times N$ which sends every location $(i, j)$ to some real\npixel value where $(i, j) \\in [M] \\times [N]$. Let $\\mathcal{F} : \\mathbb{R}^2 \\rightarrow \\mathbb{C}^2$ be some DFT functional operator. The DFT of $x$ is\ngiven by:\n$\\mathcal{F}(x)(u, v) = \\sum_{j=0}^{N-1} \\sum_{i=0}^{M-1} x(i, j)e^{-i2\\pi(\\frac{u}{M}i+\\frac{v}{N}j)}$\nPoint-wise energy spectral density (ESD). The ESD measures the energy quantity at a frequency. To\nsimplify discussions, we use radial frequency, which is defined as the radius $r$ with respect to zero frequency\npoint (i.e. the frequency center). The energy is defined as the square of the frequency magnitude according\nto Parseval's Power Theorem.\nLet $L_r$ be a circle with radius $r$ on the spectrum of image $x$, as illustrated in Figure 1. The $r$ is referred to as\nradial frequency. The point-wise ESD function is given by:\n$ESD(x) := \\frac{1}{|L_r|} \\sum_{(u,v)\\in L_r} |\\mathcal{F}(x)(u, v)|^2$\nwhere $(u, v)$ is the spatial frequency point and $|L_r|$ is the circumference of $L_r$.\nSpectral signal-to-noise ratio (SNR). The SNR can quantify signal robustness. We define the spectral\nSNR at radius frequency $r$ as:\n$SNR(r) := \\frac{ESD(x)}{ESD(\\Delta x)}$\nwhere $\\Delta x$ is some perturbation. We have characterized the SNRs of some corroptions and adversarial attacks\nin Figure 2."}, {"title": "B.2 Absence assignment scheme", "content": "There exist multiple choices for the assignments of the absences of spectral layers in coalition filtering design:\n(1) Assigning to constant zeros (Zeroing), (2) assigning to complex Gaussian noise (Complex Gaussian) and\n(3) assigning to the corresponding frequency components randomly sampled from other images at the same\ndataset (Replacement).\nZeroing. The $b$ in Equation 5 is set to zeros.\nComplex Gaussian. The $b$ in Equation 5 is sampled from a i.i.d. complex Gaussian distribution:\n$\\mathcal{N}(\\mu, \\frac{\\sigma}{2}) + i\\mathcal{N}(\\mu, \\frac{\\sigma}{2})$.\nReplacement. The $b$ in Equation 5 is set to: $b = \\mathcal{F}(x^*)$ (where $x^* \\in \\mathcal{X}$ is a randomly sampled image from\nsome set $\\mathcal{X}$).\nIn our implementation, we simply choose 'zeroing': $b = 0$. Figure 13 shows the filtered image examples by\nusing the above three strategies and also show the examples of measured spectral importance distributions.\nEmpirically, the three strategies have rather similar performance. In this research, we do not unfold the\ndiscussions regarding the masking strategy choices."}, {"title": "B.3 Proof for Spectral Coalition Information Identity Theorem", "content": "Proof for Spectral Coalition Information Identity. Suppose the probability measures $P(x), P(x,y), P(y|x)$,\nand $Q(y|x)$ are absolutely continuous with respect to $x$ on domain $X \\triangleleft \\tilde{I}$.\n$I(X \\triangleleft \\tilde{I}, Y) = \\sum_{x \\triangleleft \\tilde{I}} \\sum_{y \\in Y} P(x, y) \\cdot \\log \\frac{P(x, y)}{P(x)P(y)} dx$\n$= \\sum_{x \\triangleleft \\tilde{I}} \\sum_{y \\in Y} P(x, y) \\cdot \\log (\\frac{P(y|x) \\cdot P(x)}{P(y) \\cdot P(x)}) dx$\n$= \\sum_{x \\triangleleft \\tilde{I}} \\sum_{y \\in Y} P(x, y) \\cdot \\log (\\frac{P(y|x)}{Q(y|x)} \\cdot \\frac{Q(y|x)}{P(y)}) dx$\n$= \\sum_{x \\triangleleft \\tilde{I}} \\sum_{y \\in Y} P(x, y) \\cdot \\log (\\frac{P(y|x)}{Q(y|x)}) dx + \\sum_{x \\triangleleft \\tilde{I}} \\sum_{y \\in Y} P(x, y) \\cdot \\log (\\frac{Q(y|x)}{P(y)}) dx$\n$= \\sum_{x \\triangleleft \\tilde{I}} (\\sum_{y \\in Y} P(y|x) \\cdot \\log (\\frac{P(y|x)}{Q(y|x)}) \\cdot P(x)) dx + \\sum_{x \\triangleleft \\tilde{I}} P(x) dx \\sum_{y \\in Y} \\log P(y)$\n$= \\sum_{x \\triangleleft \\tilde{I}} \\sum_{y \\in Y} P(x, y) \\cdot \\log Q(y|x) dx$\n$= \\mathbb{E}_{X \\triangleleft \\tilde{I}} KL(P(y|x) || Q(y|x)) + H(Y) + \\sum_{x \\triangleleft \\tilde{I}} (\\sum_{y \\in Y} P(y|x) \\cdot \\log Q(y|x)) dx$\n$= \\mathbb{E}_{X \\triangleleft \\tilde{I}} KL(P(y|x) || Q(y|x)) + H(Y) + v(\\tilde{I}) + C$\nwhere $H(Y)$ is the Shannon entropy of the label set $Y$."}, {"title": "C Information quantity relationship in spectral coalitions", "content": ""}, {"title": "C.1 Partitioning spectrum with $l_{\\infty}$ ball over $l_2$ ball", "content": ""}, {"title": "C.2 Normalizing summarized SIDs", "content": "We normalize the above result and set:\n$\\widehat{\\Psi}(v) := \\frac{\\Psi(v)}{\\frac{||\\beta||_1}{M}}$"}, {"title": "C.3 How much samples are sufficient?", "content": "Error bound analysis. Let $K$ be the number of the samples of some baseline dataset. Let:\n$\\Delta v(\\tilde{I}, L_i) := v(\\tilde{I} \\cup \\{I_i\\}) - v(\\tilde{I})$\n$\\Delta v(I_i) := (\\Delta v(\\tilde{I}, L_i))_{I \\subset I}$"}]}