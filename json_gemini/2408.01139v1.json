{"title": "Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition", "authors": ["R\u00f3is\u00edn Luo (Jiaolin Luo)", "James McDermott", "Colm O'Riordan"], "abstract": "Perturbation robustness evaluates the vulnerabilities of models, arising from a variety of perturbations, such as data corruptions and adversarial attacks. Understanding the mechanisms of perturbation robustness is critical for global interpretability. We present a model-agnostic, global mechanistic interpretability method to interpret the perturbation robustness of image models. This research is motivated by two key aspects. First, previous global interpretability works, in tandem with robustness benchmarks, e.g. mean corruption error (mCE), are not designed to directly interpret the mechanisms of perturbation robustness within image models. Second, we notice that the spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially decay over the frequency. This power-law-like decay implies that: Low-frequency signals are generally more robust than high-frequency signals yet high classification accuracy can not be achieved by low-frequency signals alone. By applying Shapley value theory, our method axiomatically quantifies the predictive powers of robust features and non-robust features within an information theory framework. Our method, dubbed as I-ASIDE (Image Axiomatic Spectral Importance Decomposition Explanation), provides a unique insight into model robustness mechanisms. We conduct extensive experiments over a variety of vision models pre-trained on ImageNet, including both convolutional neural networks (e.g. AlexNet, VGG, GoogLeNet/Inception-v1, Inception-v3, ResNet, SqueezeNet, RegNet, MnasNet, MobileNet, EfficientNet, etc.) and vision transformers (e.g. ViT, Swin Transformer, and MaxViT), to show that I-ASIDE can not only measure the perturbation robustness but also provide interpretations of its mechanisms.", "sections": [{"title": "Introduction", "content": "Image modeling with deep neural networks has achieved great success (Li et al., 2021; Khan et al., 2022; Han et al., 2022). Yet, deep neural networks are known to be vulnerable to perturbations. For example, the perturbations may arise from corruptions and adversarial attacks (Goodfellow et al., 2014; Hendrycks & Dietterich, 2019; Szegedy et al., 2013), etc. Perturbation robustness, henceforth referred to as robustness, characterizes a crucial intrinsic property of models (Hendrycks & Dietterich, 2019; Bai et al., 2021; Goodfellow et al., 2014; Silva & Najafirad, 2020).\nRobustness mechanisms refer to the mechanisms which lead to robustness in models. The study of robustness mechanisms aims to answer the question 'why some models are more robust than others' (Lipton, 2018; Zhang et al., 2021; Bereska & Gavves, 2024). The causes within this question can arise from multifarious"}, {"title": "Notations", "content": "Image classifier. The primary task of an image classifier is to predict the probability distributions over discrete classes for given images. We use $Q(y|x; \\theta) : (x,y) \\rightarrow [0,1]$ to denote a classifier in the form of conditional probability. The Q predicts the probability that an image x is of class y. The $\\theta$ are the parameters. For brevity, we ignore the parameter $\\theta$. For example, we denote $Q(y|x;\\theta)$ as $Q(y|x)$.\nDataset and annotation. We use a tuple $(\\mathcal{X}, \\mathcal{Y})$ to denote an image classification dataset, where $\\mathcal{X}$ is the image set and $\\mathcal{Y}$ is the label set. We use $|\\mathcal{Y}|$ to denote the number of classes (i.e. the cardinality of set $\\mathcal{Y}$). The annotation task of image classification datasets is to assign each image with a discrete class probability distribution. We use $P(y|x)$ to denote the ground-truth probability that an image x is assigned as a class y. We use $P(x)$ to denote the probability of x in set $\\mathcal{X}$. We use $P(y)$ to denote the probability of y in set $\\mathcal{Y}$. In class-balanced datasets, $P(y) = \\frac{1}{|\\mathcal{Y}|}$."}, {"title": "Method", "content": "High-level overview. We apply Shapley value theory for axiomatically assigning credits to spectral bands. Within this framework, the specially devised characteristic function measures the information gains of spectral bands. I-ASIDE interprets robustness mechanisms using this axiomatic framework with the information theory.\nProblem formulation. Quantifying the predictive powers of features can be viewed as a value decomposition problem. In this research, the value is the information quantities that the features contribute to decisions."}, {"title": "Spectral coalitional game", "content": "Spectral player. We use $\\mathcal{I}_i$ (where $i \\in [M] := \\{0,1,\\dots, M - 1\\}$) to denote the i-th spectral player. The $\\mathcal{I}_0$ contains the most robust features and the $\\mathcal{I}_{M-1}$ contains the most non-robust features. The M spectral players constitute a player set $\\mathcal{I} := \\{\\mathcal{I}_i\\}_{i=0}^{M-1}$. Figure 15 in Appendix C.1 shows two partition schemes to partition spectral bands ($l_\\infty$ and $l_2$). We empirically choose $l_\\infty$.\nSpectral coalition. A subset $\\tilde{\\mathcal{I}} \\subseteq \\mathcal{I}$ is referred to as the spectral coalition. The player set $\\mathcal{I}$ is often referred to as the grand coalition.\nCharacteristic function. A characteristic function $v(\\tilde{\\mathcal{I}}) : \\mathcal{\\tilde{\\mathcal{I}}} \\rightarrow \\mathbb{R}$ measures the contribution for a given coalition and satisfies $v(\\emptyset) = 0$. In this research, the contribution of $\\tilde{\\mathcal{I}}$ is measured in the form of the log-likelihood expectation of the predictions by the $Q$, in which the input images only contain the signals present in the $\\tilde{\\mathcal{I}}$. We show that this design of $v$ theoretically measures how much information the $Q$ uses from the features in the $\\tilde{\\mathcal{I}}$ for decisions.\nShapley value. A spectral coalitional game $(\\mathcal{I}, v)$ is defined on a spectral player set $\\mathcal{I}$ equipped with a characteristic function $v$. The weighted marginal contribution of a spectral player $\\mathcal{I}_i$ over all possible coalitions is referred to as the Shapley value of"}, {"title": "Spectral coalition filtering", "content": "We represent the presences and absences of the spectral players through the signal pass-bands and stop-bands using a multi-band-pass digital signal filtering (Oppenheim, 1978; Pei & Tseng, 1998; Steiglitz, 2020), as shown in Figure 5. For example, the example spectral coalition $\\{\\mathcal{I}_0, \\mathcal{I}_2\\}$ signifies the signals present only in $\\mathcal{I}_0$ and $\\mathcal{I}_2$. With the spectral coalition filtering, we are able to evaluate the contributions of the combinations of various spectral features. Figure 6 shows an example of $2^M$ spectral coalitions.\nTo implement the presences and absences of spectral signals, we define a mask map $\\mathcal{T}(\\tilde{\\mathcal{I}}) : \\tilde{\\mathcal{I}} \\leftrightarrow \\{0,1\\}^{M \\times N}$ on 2D spectrum, where $\\tilde{\\mathcal{I}}$ is a spectral coalition and $M \\times N$ denotes 2D image dimensions. The mask map is point-wisely defined as:\n$\\mathcal{T}(\\tilde{\\mathcal{I}})(m, n) = \\begin{cases} 1, \\text{ if the frequency point } (m,n) \\text{ is present in coalition } \\tilde{\\mathcal{I}}, \\\\ 0, \\text{ otherwise} \\end{cases}$"}, {"title": "Characteristic function design", "content": "The characteristic function is needed in order to measure the contributions of the features in $\\tilde{\\mathcal{I}}$. We define the characteristic function as the gains of the negative cross-entropy loss values between feature presences and absences."}, {"title": "Spectral robustness score (SRS)", "content": "Although we are firstly interested in using the spectral importance distributions (SID) for robustness interpretations, they can also be summarized into scalar scores for purposes such as numerical comparisons, and later correlation studies.\nAssumption 3.7 (Spectral uniformity assumption of random decisions). The second row in Figure 7 shows the SIDs from various models with randomized weights. We randomize the model weights with Kaiming initialization (He et al., 2015). The measured SIDs exhibit spectral uniformity. This suggests: Un-trained models do not have spectral preferences. We refer to \u2018the models with randomized parameters' as random decisions. Therefore, we assume that the SIDs of random decisions are uniform: $\\frac{1}{M}$.\nAssumption 3.8 (Robustness prior). We assume: Higher utilization of robust features in decisions implies robust models. This is further substantiated by the experiments in Figure 3. To reflect this robustness prior, we empirically design a series $\\beta := (\\beta_0, \\beta_1,\\dots,\\beta_{M-1})^T$ where $\\beta \\in (0,1)$ as the summing weights of SIDs. Empirically, we choose $\\beta = 0.75$ because this choice achieves the best correlation with model robustness.\nSummarizing with weighted sum. Let $\\Psi(v)$ be the measured spectral importance distribution (SID). Set $\\hat{\\Psi}(v) = \\frac{||\\Psi(v) - min \\Psi(v)||_1}{max||\\Psi(v) - min \\Psi(v)||_1}$ with min-max normalization. The weighted sum of the $\\Psi(v)$ with the weights $\\Beta$ is given by:\n$\\mathcal{\\hat{B}^T\\Psi(v)} = \\sum_{i=0}^{M-1} {\\beta^i \\Psi(v)}$\n(10)\nwhere $\\mathcal{\\hat{B}^T}$ is served as a random decision baseline. Let $S(v) : v \\rightarrow [0,1]$ be the normalized result in Equation 10. The $S(v)$ is given by:\n$S(v) := \\frac{\\mathcal{\\hat{B}^T\\Psi(v)} - \\frac{||\\mathcal{B}||_1}{M}}{1 - \\eta}$  \n(11)\nwhere $\\beta \\in (0,1)$, $\\beta = \\frac{\\sum_{i=0}^{M-1} {\\beta^i}}{M}$ and $\\eta = \\frac{||\\mathcal{B}||_1}{M ||\\mathcal{B}||_2}$. Readers can refer to Appendix C.2 for the simplification deduction."}, {"title": "Experiments", "content": "We design experiments to show the dual functionality of I-ASIDE, which can not only measure robustness and but also interpret robustness. We organize the experiments in three categories:"}, {"title": "Correlation to robustness metrics", "content": "Definition 4.1 (Mean prediction error). In our experiments, we measure model perturbation robustness with mean prediction errors (mPE) besides the mean corruption errors (mCE). Let x be some clean image and x* be the perturbed image. For a classifier Q, we define the mean prediction error (mPE) as:\n$\\Delta P := E_{x,y \\sim (\\mathcal{X}, \\mathcal{Y})} |Q(y|x) - Q(y|x^*)|.$\n(12)\nWe demonstrate that I-ASIDE is able to measure model robustness. The experiments are broken down into three aspects: (1) correlation to mCE scores, (2) correlation to adversarial robustness, and (3) correlation to corruption robustness.\nCorrelation to mCE scores. Figure 8 shows the correlation between spectral robustness scores (SRS) and the mean corruption errors (mCE). The mCE scores are taken from the literature (Hendrycks & Dietterich, 2018). The mCE scores are measured on a corrupted ImageNet which is known as ImageNet-C in the literature (Hendrycks & Dietterich, 2019). The ImageNet-C includes 75 common visual corruptions with five levels of severity in each corruption. This correlation suggests that the results measured with I-ASIDE correlate with the results measured with robustness metric mCE."}, {"title": "Studying architectural robustness", "content": "I-ASIDE is able to answer questions such as:\nDoes model parameter size play a role in robustness?\nAre vision transformers more robust than convolutional neural networks (ConvNets)?\nDoes model parameter size play a role in robustness? Figure 11 (a) shows parameter counts do not correlate with model robustness. Thus, the tendency of a model to use robust features is not determined by parameter counts alone. We would like to carry out further investigation in future work.\nDiscussion. Vision transformers generally outperform ConvNets; nevertheless, state-of-the-art ConvNets, e.g. efficientnet (Tan & Le, 2019), can achieve comparable robustness performance (e.g. by error rates on benchmark datasets) (Li & Xu, 2023). The literature (Devaguptapu et al., 2021) affirms that efficientnet is more robust than most ConvNets. But, why efficientnet is unique? The efficientnet introduces an innovative concept in that the network sizes can be controlled by scaling the width, depth, and resolution with a compound coefficient (Tan & Le, 2019). The base architecture is then searched with neural architecture searching (NAS) (Ren et al., 2021) instead of hand-crafted design. The NAS optimization objective is to maximize the network accuracy subject to arbitrary image resolutions. The searching implicitly encourages that the network structure of efficientnet uses more robust features. This is because: The low-frequency signals in various resolutions are robust signals while high-frequency signals are not. The second column in Figure 7 shows the SID of efficientnet pre-trained on ImageNet. The SID shows that efficientnet_v2_s uses more robust features than alexnet and resnet18."}, {"title": "Interpreting how supervision noise levels affect model robustness", "content": "The previous robustness benchmarks with mean corruption errors (mCE) are not able to answer the long-standing question: \"How and why label noise levels affect robustness?\". We demonstrate that I-ASIDE is able to answer this question.\nLearning with noisy labels. Supervision signals refer to the prior knowledge provided by labels (Sucholutsky et al., 2023; Zhang et al., 2020; Shorten & Khoshgoftaar, 2019; Xiao et al., 2020). There is a substantial line of previous research on the question of \u201chow supervision noise affects robustness\u201d (Gou et al., 2021; Fr\u00e9nay & Verleysen, 2013; Lukasik et al., 2020; Rolnick et al., 2017). This question is not completely answered yet. For example, Flatow & Penner add uniform label noise into CIFAR-10 and study its impact on model robustness (Flatow & Penner, 2017). Their results show that classification test accuracy decreases as the training label noise level increases. However, empirical studies like this are not able to answer the underlying 'why' question.\nNoisy-label dataset. We derive noisy-label datasets from a clean Caltech101. We randomly assign a proportion of labels with a uniform distribution over label classes to create a noisy-label dataset. We refer to the randomly assigned proportion as supervision noise level. We vary the noise level from 0.2 to 1.0 to derive five training datasets.\nExperiment. We train three models (googlenet, resnet18 and mobilenet_v2) over the clean and the five noisy-label datasets for 120 epochs respectively. We then measure their SIDs. The results show that there is a pattern across the above three models in that: The SIDs are more uniform with higher supervision noise levels. The interpretation regarding the learning dynamics with the presence of label noise is that: Models tend to use more non-robust features in the presence of higher label noise within training set."}, {"title": "Related work", "content": "We further conduct a literature investigation from three research lines: (1) global interpretability, (2) model robustness, and (3) frequency-domain research. This literature study shows that I-ASIDE provides unique insights in these research lines.\nGlobal interpretability. Global interpretability summarizes the decision behaviours of models from a holistic view. In contrast, local interpretability merely provides explanations on the basis of instances (Sundararajan et al., 2017; Smilkov et al., 2017; Linardatos et al., 2020; Selvaraju et al., 2017; Arrieta et al., 2020; Zhou et al., 2016; Ribeiro et al., 2016; Lundberg & Lee, 2017; Lakkaraju et al., 2019; Guidotti et al., 2018; Bach et al., 2015; Montavon et al., 2019; Shrikumar et al., 2017). There are four major research lines in image models: (1) feature visualization, (2) network dissection, (3) concept-based method, and (4) feature importance."}, {"title": "Limitations", "content": "I-ASIDE provides a unique insight into the perturbation robustness mechanisms. Yet, our method has two major limitations: (1) The spectral perspective can merely reflect one aspect of the holistic view of model robustness, and (2) the SID resolutions are low.\nLimitation (1). For example, carefully crafted malicious adversarial perturbations on low-frequency components can fool neural networks (Luo et al., 2022; Liu et al., 2023; Maiya et al., 2021). Luo et al. demonstrate that attacking low-frequency signals can fool neural networks, resulting in attacks which are imperceptible to humans. This further implies the complexity of this research topic.\nLimitation (2). The computation cost is imposed by $O(2^M)$. Fortunately, we do not need high SID resolution to analyze the model robustness problem. For example, a choice with $M = 8$ is sufficient to interpret robustness mechanisms (as we have shown) while the computational cost remains reasonable."}, {"title": "Conclusions", "content": "On the solid ground provided by information theory and coalitional game theory, we present an axiomatic method to interpret model robustness mechanisms, by leveraging the power-law-like decay of SNRs over the frequency. Our method addresses the limitation that scalar metrics fail to interpret robustness mechanisms. We carry out extensive experiments over a variety of architectures. The SIDs, when scalarized, can largely reproduce the results found with previous methods, but addresses their failures to answer the underlying 'why' questions. Our method goes beyond them with the dual functionality in that: I-ASIDE can not only measure the robustness but also interpret its mechanisms. Our work provides a unique insight into the robustness mechanisms of image classifiers."}, {"title": "Appendix", "content": null}, {"title": "Fairness division axioms", "content": "Symmetry axiom: Let $\\tilde{I} \\in 2^{\\mathcal{I}}$ be some spectral player coalition. For $\\forall \\mathcal{I}_i, \\mathcal{I}_j \\in \\mathcal{I} \\wedge \\mathcal{I}_i, \\mathcal{I}_j \\notin \\tilde{I}$, the statement v($\\tilde{I} \\cup \\{\\mathcal{I}_i\\}$) = v($\\tilde{I} \\cup \\{\\mathcal{I}_j\\}$) implies $\\psi_i(\\mathcal{I}, v) = \\psi_j(\\mathcal{I}, v)$. This axiom restates the statement 'equal treatment of equals' principle mathematically. This axiom states that the 'names' of players should have no effect on the 'treatments' by the characteristic function in coalition games (Roth, 1988).\nLinearity axiom: Let u and v be two characteristic functions. Let $(\\mathcal{I}, u)$ and $(\\mathcal{I}, v)$ be two coalition games. Let $(u + v)(\\tilde{I}) := u(\\tilde{I}) + v(\\tilde{I})$ where $\\tilde{I}\\in 2^{\\mathcal{I}}$. The divisions of the new coalition game $(\\mathcal{I}, u + v)$ should satisfy: $\\psi_i(\\mathcal{I}, u + v) = \\psi_i(\\mathcal{I}, u) + \\psi_i(\\mathcal{I}, v)$. This axiom is also known as 'additivity axiom' and guarantees the uniqueness of the solution of dividing payoffs among players (Roth, 1988).\nEfficiency axiom: This axiom states that the sum of the divisions of all players must be summed to the worth of the player set (the grand coalition): $\\sum_{i=0}^{M-1} \\psi_i(\\mathcal{I},v) = v(\\mathcal{I})$.\nDummy player axiom: A dummy player (null player) $\\mathcal{I}_*$ is the player who has no contribution such that: $\\psi_*(\\mathcal{I}, v) = 0$ and $v(\\tilde{I} \\cup \\{\\mathcal{I}\\_*\\}) = v(\\tilde{I})$ for $\\forall \\mathcal{I}_* \\notin \\tilde{I} \\wedge \\mathcal{I}\\_* \\subseteq \\mathcal{I}$.\nRemark B.1. In the literature (Roth, 1988), the efficiency axiom and the dummy player axiom are also combined and relabeled as carrier axiom."}, {"title": "Spectral signal-to-noise ratio (SNR)", "content": "Discrete Fourier Transform. The notion 'frequency' measures how 'fast' the outputs can change with respect to inputs. High frequency implies that small variations in inputs can cause large changes in outputs. In terms of images, the 'inputs' are the pixel spatial locations while the 'outputs' are the pixel values.\nLet $x : (i, j) \\rightarrow \\mathbb{R}$ be some 2D image with dimension $M \\times N$ which sends every location $(i, j)$ to some real pixel value where $(i, j) \\in [M] \\times [N]$. Let $F : \\mathbb{R}^2 \\rightarrow \\mathbb{C}^2$ be some DFT functional operator. The DFT of $x$ is given by:\n$F(x)(u, v) = \\sum_{j=0}^{N-1} {\\sum_{i=0}^{M-1} x(i, j)e^{-i2\\pi(\\frac{ui}{M} + \\frac{vi}{N})}}$\n(13)\nPoint-wise energy spectral density (ESD). The ESD measures the energy quantity at a frequency. To simplify discussions, we use radial frequency, which is defined as the radius r with respect to zero frequency point (i.e. the frequency center). The energy is defined as the square of the frequency magnitude according to Parseval's Power Theorem.\nLet $L_r$ be a circle with radius r on the spectrum of image x, as illustrated in Figure 1. The r is referred to as radial frequency. The point-wise ESD function is given by:\n$ESD(x) := \\frac{1}{|L_r|} \\sum_{(u,v)\\in L_r} |F(x)(u, v)|^2$\n(14)\nwhere $(u, v)$ is the spatial frequency point and $|L_r|$ is the circumference of $L_r$.\nSpectral signal-to-noise ratio (SNR). The SNR can quantify signal robustness. We define the spectral SNR at radius frequency r as:\n$SNR(r) := \\frac{ESD(x)}{ESD(\\Delta x)}$\n(15)\nwhere $\\Delta x$ is some perturbation. We have characterized the SNRs of some corroptions and adversarial attacks in Figure 2."}, {"title": "Absence assignment scheme", "content": "There exist multiple choices for the assignments of the absences of spectral layers in coalition filtering design: (1) Assigning to constant zeros (Zeroing), (2) assigning to complex Gaussian noise (Complex Gaussian) and (3) assigning to the corresponding frequency components randomly sampled from other images at the same dataset (Replacement).\nZeroing. The b in Equation 5 is set to zeros.\nComplex Gaussian. The b in Equation 5 is sampled from a i.i.d. complex Gaussian distribution: $\\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{2}}) + i\\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{2}})$.\nReplacement. The b in Equation 5 is set to: $b = \\mathcal{F}(x^*) $ (where $x^* \\in \\mathcal{X}$ is a randomly sampled image from some set $\\mathcal{X}$).\nIn our implementation, we simply choose 'zeroing': $b = 0$. Empirically, the three strategies have rather similar performance. In this research, we do not unfold the discussions regarding the masking strategy choices."}, {"title": "Proof for Spectral Coalition Information Identity Theorem", "content": "Proof for Spectral Coalition Information Identity. Suppose the probability measures $P(x), P(x,y), P(y|x)$, and $Q(y|x)$ are absolutely continuous with respect to $x$ on domain $\\mathcal{X} \\blacktriangleleft \\tilde{I}$.\n$I(\\mathcal{X} < \\tilde{I}, \\mathcal{Y}) = \\sum_{\\mathcal{X} \\in \\tilde{I}} {\\sum_{y \\in \\mathcal{Y}} P(x, y) \\cdot log\\frac{P(x, y)}{P(x) P(y)}} dx$ (16)\n$= - \\sum_{\\mathcal{X} \\in \\tilde{I}} {\\sum_{y \\in \\mathcal{Y}} P(x, y) \\cdot log( \\frac{P(y|x) \\cdot P(x) \\cdot Q(y|x)}{P(y) \\cdot P(x) \\cdot Q(y|x)} )} dx$ (17)\n$= - \\sum_{\\mathcal{X} \\in \\tilde{I}} {\\sum_{y \\in \\mathcal{Y}} P(x, y) \\cdot log( \\frac{P(y|x)}{Q(y|x)} \\cdot \\frac{1}{P(y)} )} dx$ (18)\n$= - \\sum_{\\mathcal{X} \\in \\tilde{I}} { [\\sum_{y \\in \\mathcal{Y}} P(y|x) \\cdot log\\frac{P(y|x)}{Q(y|x)}] } \\cdot P(x) dx$ (19)\n$= - \\sum_{\\mathcal{X} \\in \\tilde{I}} { [\\sum_{y \\in \\mathcal{Y}} P(x, y) dx] } \\cdot log P(y)$ (20)\n$= - \\sum_{\\mathcal{X} \\in \\tilde{I}} P(x) dx \\cdot  \\sum_{y \\in \\mathcal{Y}} P(y) \\cdot log Q(y|x)dx$ (21)\n$= E_{\\mathcal{X} \\in \\tilde{I}}KL(P(y|x) || Q(y||x)) + H(\\mathcal{Y}) + \\int_{\\mathcal{X} \\in \\tilde{I}} \\sum_{\\mathcal{Y} \\in \\mathcal{Y}} P(x) [\\sum_{\\mathcal{X} \\in \\tilde{I}} log Q(y|x)dx$ (22)\n$= E_{\\mathcal{X} < \\tilde{I}}KL(P(y|x) || Q(y||x)) + H(\\mathcal{Y}) + E_{\\mathcal{X} < \\tilde{I}} P(\\mathcal{Y}|x) \\cdot log Q(y|x)$ (23)\n$= E_{\\mathcal{X} < \\tilde{I}}KL(P(y|x) || Q(y||x)) + H(\\mathcal{Y}) + v(\\tilde{I}) + C$ (24)\nwhere $H(\\mathcal{Y})$ is the Shannon entropy of the label set $\\mathcal{Y}$."}, {"title": "Information quantity relationship in spectral coalitions", "content": null}, {"title": "Normalizing summarized SIDs", "content": "We normalize the above result and set:\n$\\mathcal{S}(v) := \\frac{\\hat{\\mathcal{B}^T\\Psi(v)} - \\frac{||\\mathcal{B}||_1}{M}}{sup {(\\hat{\\mathcal{B}^T\\Psi(v)} - \\frac{||\\mathcal{B}||_1}{M})}}$ (25)\n$\\mathcal{S}(v) = \\frac{ \\frac{ sup || \\beta||_2 \\cdot || \\Psi(v) ||_2 - \\frac{|| \\beta||_1}{M}}{sup || ||_2 \\cdot || \\Psi(v) ||_2} -  \\frac{|| \\beta||_1}{M} }{  1 -  \\frac{|| \\beta||_1}{M} }$ (26)\n$= \\frac{ \\frac{ (\\hat{\\mathcal{B}^T\\Psi(v)} - \\frac{|| \\beta||_1}{M}) }{ || \\beta||_2} -  \\frac{|| \\beta||_1}{M} }{1 -  \\frac{|| \\beta||_1}{M || \\beta||_2} }$ (27)\n$\\mathcal{S}(v) = \\frac{\\hat{\\mathcal{B}^T\\Psi(v)} - \\eta}{1 - \\eta} (28)\nwhere  $\\eta = \\frac{ ||B||_1}{M ||B||_2}$  and $sup \\hat{\\mathcal{B}^T\\Psi(v)} - \\frac{ ||B||_1}{M} is derived by:\n$sup \\hat{\\mathcal{B}^T\\Psi(v)} - \\frac{ ||B||_1}{M} = sup  ||\\beta||_2 - \\frac{ ||B||_1}{M}$ (29)\n$ = sup  ||\\beta||_2 \\cdot | | \\Psi(v) ||_2 -  \\frac{ ||B||_1}{M}$,  s.t.  $| | \\Psi(v) ||_1 = 1$ (30)\n$ =  ||\\beta||_2  -   \\frac{ ||B||_1}{M}$ since $ ||\\Psi(v) ||_2 \\lt \\eq  ||\\Psi(v) ||_1^2$. (31)\nSet $\\eta = \\frac{1}{M||B||_2^2}$\n$S(v) = \\frac{\\hat{\\mathcal{B}^T\\Psi(v)} - \\eta}{1 - \\eta}$ (32)"}, {"title": "How much samples are sufficient?", "content": "Error bound analysis. Let K be the number of the samples of some baseline dataset. Let:\n$\\Delta v(\\tilde{I}, I_i) := v(\\tilde{I} \\cup \\{I_i\\}) - v(\\tilde{I})$ (33)\nand\n$\\Delta v(I_i) := (\\Delta v(\\tilde{I}, I_i))_{\\tilde{I} \\subset I}$ (34)\nand\n$W :=  (\\frac{1}{M} {M-1 \\choose |I|})^{-1}$ (35)\nHence:\n$\\psi_i(I, v) = W^T \\Delta v(I_i)$ (36)\nwhere $||W||_1 = 1$ since W is a probability distribution. Let $v_i, \\Delta \\bar{v}(I_i)$ and $\\Delta \\bar{v}(\\tilde{I}, I_i)$ be estimations with K samples using Monte Carlo sampling. The error bound with $l_1$ norm is given by:\n$def \\epsilon  =  sup ||\\psi_i(I, v) - \\bar{\\psi_i}(I, v)||_1 \\frac{\\beta||_1}{M}  sup ||W^T \\Delta \\bar{v}(I_i) - W^T \\Delta v(I_i)||_1$ (37)\n$\\leq sup ||W||_1 . ||\\Delta \\bar{v}(I_i) - \\Delta v(I_i)||_\\infty (Holder's inequality)$ (38)\n$= sup || \\sum_{\\tilde{I} \\subset I} (\\Delta \\bar{v}(\\tilde{I}, I_i) - \\Delta v(\\tilde{I}, I_i) ) ||_\\infty$ (39)\n$\\leq sup 2^{M-1} . sup ||\\Delta \\bar{v}(\\tilde{I}, I_i) - \\Delta v(\\tilde{I}, I_i) ||_\\infty$ (40)\n$\\leq sup 2^{M-1} . sup ||\\Delta \\bar{v}(\\tilde{I}, I_i) - \\Delta v(\\tilde{I}, I_i) ||_1$ (41)\n$\\leq 2^{M-1} (\\frac{Var(\\Delta)}{K}) \\frac{1}{2}$ (42)\nwhere $Var(\\Delta)$ gives the upper bound of the variance of $\\Delta v(\\tilde{I}, I_i)$."}]}