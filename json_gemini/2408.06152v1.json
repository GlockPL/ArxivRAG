{"title": "Palant\u00edr: Towards Efficient Super Resolution for Ultra-high-definition Live Streaming", "authors": ["Xinqi Jin", "Zhui Zhu", "Xikai Sun", "Fan Dang", "Jiangchuan Liu", "Jingao Xu", "Kebin Liu", "Xinlei Chen", "Yunhao Liu"], "abstract": "Neural enhancement through super-resolution (SR) deep neural networks (DNNs) opens up new possibilities for ultra-high-definition live streaming over existing encoding and networking infrastructure. Yet, the heavy SR DNN inference overhead leads to severe deployment challenges. To reduce the overhead, existing systems propose to apply DNN-based SR only on selected anchor frames while upscaling non-anchor frames via the lightweight reusing-based SR approach. However, frame-level scheduling is coarse-grained and fails to deliver optimal efficiency. In this work, we propose Palant\u00edr, the first neural-enhanced UHD live streaming system with fine-grained patch-level scheduling. In the presented solutions, two novel techniques are incorporated to make good scheduling decisions for inference overhead optimization and reduce the scheduling latency. Firstly, under the guidance of our pioneering and theoretical analysis, Palant\u00edr constructs a directed acyclic graph (DAG) for lightweight yet accurate quality estimation under any possible anchor patch set. Secondly, to further optimize the scheduling latency, Palant\u00edr improves parallelizability by refactoring the computation subprocedure of the estimation process into a sparse matrix-matrix multiplication operation. The evaluation results suggest that Palant\u00edr incurs a negligible scheduling latency accounting for less than 5.7% of the end-to-end latency requirement. When compared to the state-of-the-art real-time frame-level scheduling strategy, Palant\u00edr reduces the energy overhead of SR-integrated mobile clients by 38.1% at most (and 22.4% on average) and the monetary costs of cloud-based SR by 80.1% at most (and 38.4% on average). The source code is available at https://palantir-sr.github.io.", "sections": [{"title": "1 INTRODUCTION", "content": "Ultra-high-definition (UHD) videos such as 4K and 8K videos are expected to form a huge market worth more than $1 trillion in the following few years [18]. More and more UHD live-streaming applications are deployed to revolutionize many aspects of our society. For example, the UHD live streaming of major sports events such as the Olympics [24, 25] creates unprecedentedly immersive experiences for audiences. Besides, with the real-time UHD video from inspection and surveillance cameras, human operators can have a precise understanding of the spot and remotely take immediate actions to prevent emergent accidents [13, 17].\nHowever, the bitrates of the encoded UHD videos are significantly larger than videos of lower resolutions and pose great challenges to existing network infrastructure. Specifically, the bitrates of 4K videos can be as large as 45Mbps [21], while the worldwide average uplink bandwidth of mobile broadband networks is only 11.07 Mbps [34]. A common solution to this problem is using dedicated hardware encoders [8, 25, 28], which can encode the UHD video more efficiently and provide much lower bitrates. However, hardware encoders are very expensive and typically cost hundreds to thousands of dollars. Using fixed broadband networks for the uplink is an alternative solution, but it inevitably affects mobility and prohibits applications such as drone-based inspection [17, 37].\nRecently, neural enhancement has been proposed [11, 42, 44] and deployed [14, 27, 32, 45] to improve video streaming. It can potentially boost the broad deployment of UHD live streaming by allowing the streaming source to stream only a low-bitrate low-resolution (LR) video over the bandwidth-limited uplink and using a super-resolution (SR) deep neural"}, {"title": "2 BACKGROUND", "content": "SR DNNs typically incorporate convolution layers and modules specially designed for upscaling (e.g., deconvolution [15], pixel shuffle [36], etc). We refer readers to [10] for further details of the SR DNNs.\nCurrently, there are two common deployment models for SR enhancement. One is to execute the SR DNN inference on the mobile streaming clients [14, 27, 32, 45], and the other is to execute the SR DNN on a cloud server so that a lot of audience for the same video can benefit from the one-time server-side enhancement [44]. Yet, SR DNNs are of high computation complexity and lead to severe deployment challenges in both deployment models. In the first model, the battery of mobile clients can be easily drained. Consequently, Microsoft Edge VSR disables its SR feature when the device is not being charged [32]. As for the second model, the heavy inference overhead appears in the form of the high monetary cost of using cloud servers (estimated to be at least $1.690 per hour per 4K stream [44]). Lowering the overhead is essential to a broader application of SR enhancement."}, {"title": "2.2 Reusing-based SR", "content": "To enable low-cost SR enhancement, researchers have built the NEMO [42] system, where an SR-enhanced decoder is adopted to reduce the cost by using video temporal redundancy. In the SR decoder, video frames are categorically divided: anchor frames are upscaled via DNN-based SR, while non-anchor frames are quickly upscaled via reusing-based SR. To appreciate the intricacies of reusing, a basic understanding of video codecs is essential. Video coding predominantly encodes a block through inter coding; it identifies a similar reference block from a prior frame and only stores the subtle difference or residual between the two blocks.A reference index is retained in the coded video to identify the frame containing the reference block or the reference frame, while a motion vector is stored to represent the potential spatial offset between the current and reference blocks (owing to object movements or camera shifts). To decode an inter-coded block $b \\text{inter}$ (i.e.,  in Fig. 1), the decoder first parses the reference index to determine its reference frame (0) from the decoded frame buffer and then parses the motion vector (2) to determine its reference block $b_{\\text{inter.ref}}$ (3). The reference block is added to the decoded residual $b_{\\text{inter.res}}$ (4). This can be formally expressed as\n$b_{\\text{inter}} = b_{\\text{inter.ref}} + b_{\\text{inter.res}}.$"}, {"title": "2.3 Estimation-based Anchor Selection", "content": "Assuming that the DNN model is well suited for the video, selecting all patches as anchors naturally results in the best quality but also leads to the highest overhead. Similarly, the number of anchor frames is limited in NEMO and NeuroScaler to reduce the cost. These two systems thus greedily select the anchor frame that yields the highest quality until some budget or goal is met. As measuring the video quality for greedy search is too time-consuming, NEMO and NeuroScaler approximate the video qualities with estimation values. In NEMO, a heavy initial measurement phase (reported to take nearly one minute for a video segment of 4 seconds [42]) is required before conducting any estimation. Extending the strategy to patch-level scheduling can even further slow down the measurement phase. Since the strategy in NEMO fails to support live streaming due to its low anchor selection throughput, we do not use it as a baseline in our paper. Alternatively, we choose to use the strategy introduced in NeuroScaler [44] as our baseline."}, {"title": "3 SYSTEM OVERVIEW", "content": "Scope. We aim to support UHD live streaming via SR enhancement. As video frames are highly redundant, we propose to apply DNN-based SR only on carefully chosen anchor patches while upscaling non-anchor patches via the lightweight reusing-based SR mechanism. This optimization is essential to improve the practicality of SR enhancement, considering the effect of SR inference on the battery life of mobile clients and the monetary costs of cloud-based SR inference.\nDesign goals. First, Palant\u00edr aims to pinpoint the most beneficial anchor patches, so that it can use a small anchor patch set to reduce the DNN inference cost while achieving a large quality gain. Second, Palant\u00edr should complete the fine-grained scheduling as quickly as possible, considering the stringent latency requirements in many video applications [9, 16, 20, 37].\nWorkflow. The workflow of Palant\u00edr is shown in Fig. 3. Considering the limited uplink bandwidth [34] and the high bitrate of the original high-resolution (HR) video [21], the streamer only uploads the downsampled LR video to the media server. The server generates an SR error DAG (Sec. 4.3) for every LR video segment whose time duration equals the pre-defined scheduling interval. The quality of the corresponding SR segment under any possible anchor patch set can be quickly estimated using the constructed DAG, and beneficial anchor patches are greedily searched via DAG-based quality estimation. Two novel parallelism strategies are used to further accelerate the searching process without changing its results (Sec. 5). A cache profile is then created, every bit of which indicates whether a patch in the LR segment is an anchor or non-anchor. In the existence of a powerful cloud server [44], both the LR segment and its corresponding cache profile are streamed to the server and then processed by the SR decoder on the server. Or, alternatively, the SR decoder can be executed in the streaming client to perform cache profile-guided SR enhancement.\nDeployment Scenario. Palant\u00edr is designed for UHD live streaming but also supports videos of lower resolutions and video-on-demand services. Our initial prototype is tailored for the VP9 codec, considering the engineering complexities involved in transitioning to other codecs. However, the system is not restricted to VP9-specific functionalities and should be adaptable to a range of other codecs."}, {"title": "4 DAG-BASED QUALITY ESTIMATION", "content": "We first identify the problems with existing quality estimation methods. To design a fine-grained and lightweight estimation method, we give a pioneering analysis of the SR error propagation process. Based on our analysis, we propose"}, {"title": "4.1 Problems with Existing Methods", "content": "A natural way to estimate the video quality under a given anchor patch set is to extend the methods used in NEMO or NeuroScaler. The strategy in NEMO is based on the observation that the quality gain of a frame is mostly determined by the most relevant anchor frame. NEMO first enumerates every anchor frame set consisting of a single frame f and measures the quality (denoted as $FQ(i|f)$) of every frame i under every enumerated single anchor frame set |f|. Then, NEMO estimates the quality under any anchor frame set AP as $FQ(i|AP) \u2248 \\text{max}_{f \\in AP}FQ(i|f)$. Capitalized on the heuristics, NEMO requires a heavy measurement phase in nature and incurs a high latency (reported to be 59.6 seconds for a video segment of 4 seconds [42]) not suitable for live streaming. The scheme can be easily extended to the patch-level granularity by firstly conducting measurements for all single anchor patch sets, but the latency of initial measurements can be further increased.\nIn contrast, the method in NeuroScaler [44] is much more efficient. It models the super-resolution error propagation process as a linked list, where each node corresponds to a frame and each pair of consecutive frames is linked. The SR error of an anchor frame is assumed to be 0, while the error of a non-anchor frame equals that of its preceding frame node plus the residual between the two frames. An anchor set that leads to a lower sum of the errors over all frame nodes is regarded as leading to higher video quality.\nHowever, such a modeling is too simplified and ignores the effect of the degree of reference. In fact, each frame can have at most three reference frames in VP9 [22], and its SR error directly depends on the error of every reference frame rather than that of a single preceding frame. Conversely, some types of frames (i.e., keyframes and alternative reference frames in the VP9 codec [22]) may be referred to by many subsequent frames (rather than a single subsequent frame, as modeled by NeuroScaler). To compensate for over-simplified modeling, NeuroScaler gives priorities to keyframes and alternative reference frames: a normal frame may be selected as the anchor only if all the keyframes and alternative reference frames (AltRefs) have been selected as anchors. Yet, transferring the linked list-based modeling and the heuristic mitigation strategy to our context faces several challenges:\nFirst, a patch can refer to a huge number of preceding patches, so it will be even more unreasonable if we model the SR error propagation process as multiple independent linked lists of patch nodes for fine-grained scheduling. Denoting the number of rows in the patch grid as $n_r$ and"}, {"title": "4.2 Understanding SR Error Propagation", "content": "We next give an analysis of the SR error propagation process, which lays the foundation for our DAG-based quality estimation.\nCase #1: non-anchor patches."}, {"title": "4.3 DAG Construction", "content": "According to the Conclusion #3, the SR error of a non-anchor patch is the weighted sum of those of its depending patches plus its texture complexity. A patch P may depend on another patch Pi for inter-coding only if the frame containing Pi is a reference frame of the frame containing the"}, {"title": "5 PARALLEL SEARCHING", "content": "We employ a greedy searching algorithm to iteratively select anchor patches based on the estimated quality. We refer to the sequential implementation of this method as the vanilla Palant\u00edr. Specifically, the estimation processes for different anchor patch sets are executed sequentially, and the error attributes for different patch nodes under a given anchor patch set are also computed sequentially. As demonstrated"}, {"title": "5.1 Performance Analysis", "content": "Later in Sec. 6, the vanilla Palant\u00edr meets the first goal but fails the second goal.\nTo improve the scheduling latency, an intuitive method is to introduce parallelism into the DAG-based estimation process. However, the complex reference relationship among patches leads to a highly irregular DAG structure, making it challenging to simultaneously guarantee correctness, parallelism, and data locality of computation. We conduct a case study to demonstrate this. For simplicity, we use only the first two frames of an LR segment to construct an SR error DAG (shown in Fig. 7) and identify two challenges.\nFirst, to ensure the correctness of quality estimation, the error attribute of any non-anchor node can be computed only after the error attributes of all its predecessor nodes are computed. The process can be parallelized by processing different nodes in different threads, only if there exists no directed path from one node processed in some thread to another node processed in another thread; otherwise, the correctness requirement may be violated. As shown in Fig. 7, we can divide the DAG into nine disconnected sub-graphs and process different sub-graphs in different threads. However, each sub-graph is rather small due to the sparse connections among patches, making it challenging to effectively utilize the SIMD features of modern CPUs.\nSecond, while configuring data used by an individual thread to ensure memory locality is fairly straightforward, the scenario becomes more complex when multiple threads access data from various sub-graphs. It requires an intricate thread-synchronization mechanism to maintain memory locality throughout the execution of multiple threads."}, {"title": "5.2 Parallelism Solution", "content": "We propose a novel strategy to enable parallel searching in Palant\u00edr. The parallelized Palant\u00edr can generate the same anchor patch set as the vanilla Palant\u00edr as its parallelism mechanism does not hurt the correctness of the computation. As demonstrated later in Sec. 6.3 and 6.4, the parallelized Palant\u00edr improves the DAG-based selection latency by more than 200 times and meets the two design goals simultaneously."}, {"title": "6 EVALUATION", "content": "We evaluate Palant\u00edr by answering three questions:\nDoes Palant\u00edr achieve the first design goal of selecting a beneficial anchor patch set and improving the efficiency of neural-enhanced UHD live streaming?"}, {"title": "6.1 Experimental Setup", "content": "Implementation. We develop our decoder based on the open-source SR decoder in NEMO [42]. We incorporate two novel modes into the decoder. The first is to obtain the data required for graph construction (as introduced in Sec. 4.3). The second is to take both an LR video and a corresponding cache profile as input, and then upscale patches by either SR DNNs or reusing-based SR based on the cache profile.\nHardware. We use a server with a 16-core AMD Ryzen processor as our media server, where graph construction and anchor selection are performed. The scheduling latency is measured on the server. We use a Xiaomi 12S smartphone, which was announced in July 2022 and equipped with the Qualcomm Snapdragon 8+ Gen 1 Mobile Platform, to measure the energy efficiency when running SR DNN inference on mobile receiver devices.\nVideo. We download six popular 4k@30fps videos from YouTube. To demonstrate the universality of Palant\u00edr, the videos contain six distinct categories, including makeup review, computer gaming, skit, shopping, car review, and unboxing. We use FFmpeg (v3.4) [4] to transcode the HR video into the 480p (854 \u00d7 480) LR version in real time. We follow encoding guidelines to set the bitrate to 1800 kbps, the encoding speed to 5 [2], and the group of pictures (GoP) to 60 frames (i.e., 2 seconds) [40]. We use the -auto-alt-ref option in FFmpeg to enable the alternative reference frame feature required by the anchor selection algorithm in NeuroScaler. Unless noted otherwise, We use the first five minutes of each video in our evaluation.\nSR DNN. We adopt the DNN model of NAS [43]. We empirically set the number of residual blocks to 8 and the number of filters to 48. The DNN upscales the resolution of the LR video by 4 times. As the feasibility of online training for live streaming has been demonstrated [29], we train the DNN model for each benchmark video. When comparing the performance of different methods on the same video, the same DNN model is used for fairness.\nAnchor patch size. We use a patch size of 170 \u00d7 160 to compensate for energy efficiency and latency. Consequently, each LR frame consists of 15 patches.\nBaselines. We use two baselines in this part. The first is the NeuroScaler baseline, which uses the algorithm in NeuroScaler to select the anchor frame set. The second is the Key+Uniform baseline, which selects all the patches in"}, {"title": "6.2 Anchor Effectiveness", "content": "Quality Gain. We compute the peak-signal-to-noise-ration (PSNR) between the SR video and the original HR video to quantify the effectiveness of an anchor set. To make a fair comparison, we keep the total size of the anchor regions selected by different methods the same. In other words, we compare the quality gain under the m-ary anchor frame set selected by the NeuroScaler baseline with that under the (15 \u00b7 m)-ary anchor patch set selected by Palant\u00edr or the Key+Uniform baseline. We empirically limit m to not be greater than 3 since: (1) m = 3 can deliver quality gains that are comparable to the setting of applying DNN-based SR on all frames; (2) further increasing the value of m leads to a limited benefit yet a significant overhead.\nAs shown in Fig. 8, our system consistently outperforms the two baselines with its ability to identify beneficial patches."}, {"title": "6.3 Scheduling Latency", "content": "End-to-end (E2E) latency is an important metric in live streaming [9, 16, 20, 37]. To ensure that the live streaming latency can be lower than the GoP, modern streaming standards such as CMAF [26] allow a chunk (which can be part of a GoP) to be immediately packaged (i.e., chunked packaging [12]) and delivered (i.e., chunked delivery [12]) when ready. Here we denote the chunk length as n frames and assume the scheduling interval of Palant\u00edr to be equal to n for simplicity. As shown in Fig. 11, the streamer contributes new video frames at a constant rate. Every new chunk of n frames is contributed per time duration of $L_1 = \\frac{n}{\\text{frame\\_rate}}$. In traditional streaming pipeline without neural enhancement, the new chunk can be immediately packaged and delivered at $t_1$. However, two additional latency sources are presented in Palant\u00edr, i.e., the DAG construction latency $L_2$ and the DAG-based anchor selection $L_3$. We evaluate whether $L_2 + L_3$ is small enough to well support latency-sensitive UHD applications.\nWe first examine the value of $L_2$. Note that we can directly feed a newly contributed frame to the decoder (working in the first mode introduced in Sec. 6.1) for DAG construction, so $L_2$ should be equal to the processing latency of the last frame in the scheduling interval if the decoder runs above 30fps. As shown in Fig. 12, the measured per-frame decoding"}, {"title": "6.4 Ablation Study", "content": "SR error DAG. The key to selecting a beneficial anchor patch set is our DAG-based modeling. We use a theoretical analysis to determine the values of the static weight attributes of the edges and the static TC attributes of patch nodes (see Sec. 4.2 and 4.3). To quantify the importance of setting appropriate values, we evaluate with the makeup review video and compare Palant\u00edr with two variants. In the first variant (Palant\u00edr w/o weight), the only predecessor node of the patch node $P_h^n$ (located at the i-th row and j-th column of the patch grid of the n-th frame) is $P_h^{n-1}$, and the weight of the edge connecting them equals 1. However, the TC attributes in the first variant are kept the same as in Palant\u00edr. Note that the first variant resembles the NeuroSclaer baseline when the patch size equals the frame resolution. In the second variant (Palant\u00edr w/o TC), the weight attributes are kept the same as"}, {"title": "7 LIMITATIONS AND FUTURE WORK", "content": "Exploring a smaller patch size. A natural method to further improve the efficiency of neural enhancement is to use a smaller patch size. However, this method leads to a larger DAG and increases the latency of anchor selection. Potential remedies may be pruning the constructed DAG."}, {"title": "8 RELATED WORK", "content": "Model Compression. Model compression has been utilized in many video super-resolution systems such as OmniLive [35] and Microsoft Edge VSR [32]. Considering the"}, {"title": "9 CONCLUSION", "content": "In this work, we propose Palant\u00edr, the first neural-enhanced UHD live streaming system with fine-grained patch-level scheduling. Palant\u00edr seeks to improve efficiency via reasonable scheduling while minimizing the scheduling latency to better support live streaming. Based on our pioneering and theoretical analysis, Palant\u00edr adopts DAG-based quality estimation to select a beneficial anchor patch set with low computation cost. The per-frame computation sub-procedure of the estimation method is further refactored to facilitate parallelization and acceleration and significantly decrease the scheduling latency. The evaluation findings indicate that Palant\u00edr effectively optimizes the efficiency of neural enhancement and fits the latency requirement of UHD live streaming."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 Energy Measurement", "content": "Here we explain how we measure the energy overhead under a given anchor set. We use the Android Debug Bridge (adb) over Wi-Fi [3] to execute the decoder in the smartphone's shell. We do not use adb over USB as connecting the smartphone to an external computer via USB automatically charges the smartphone battery and affects the measured current value. Our setting adheres mostly to the guidelines in [39] for reproducibility, except that the Wi-Fi module is turned on for adb. To remove the impact of the display screen, native daemons, and Wi-Fi interfaces in our measurements, we first record the average current $C_1$ before the decoder is executed and then record the average current $C_2$ during the execution of the decoder. We also record the duration of decoding, T, and compute the energy overhead of neural-enhanced decoding as $(C_2 - C_1) \\times T$."}]}