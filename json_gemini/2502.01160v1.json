{"title": "Scalable Precise Computation of Shannon Entropy", "authors": ["Yong Lai", "Haolong Tong", "Zhenghang Xu", "Minghao Yin"], "abstract": "Quantitative information flow analyses (QIF) are a class of techniques for measuring the amount of confidential information leaked by a program to its public outputs. Shannon entropy is an important method to quantify the amount of leakage in QIF. This paper focuses on the programs modeled in Boolean constraints and optimizes the two stages of the Shannon entropy computation to implement a scalable precise tool PSE. In the first stage, we design a knowledge compilation language called ADD[\\^] that combines Algebraic Decision Diagrams and conjunctive decomposition. ADD[\\^] avoids enumerating possible outputs of a program and supports tractable entropy computation. In the second stage, we optimize the model counting queries that are used to compute the probabilities of outputs. We compare PSE with the state-of-the-art probably approximately correct tool EntropyEstimation, which was shown to significantly outperform the existing precise tools. The experimental results demonstrate that PSE solved 55 more benchmarks compared to EntropyEstimation in a total of 441. For 98% of the benchmarks that both PSE and EntropyEstimation solved, PSE is at least 10x as efficient as EntropyEstimation.", "sections": [{"title": "1 Introduction", "content": "Quantitative information flow (QIF) is an important approach to measuring the amount of information leaked about a secret by observing the running of a program [Denning, 1982; Gray III, 1992]. In QIF, we often quantify the leakage using entropy-theoretic notions, such as Shannon entropy [Backes et al., 2009; Cerny et al., 2011; Phan et al., 2012; Smith, 2009] or min-entropy [Backes et al., 2009; Meng and Smith, 2011; Phan et al., 2012; Smith, 2009]. Roughly speaking, a program in QIF can be seen as a function from a set of secret inputs X to outputs Y observable to an attacker who may try to infer X based on the output Y. Boolean formulas are a basic representation to model programs [Fremont et al., 2017; Golia et al., 2022]. In this paper, we focus on precisely computing the Shannon entropy of a program expressed in Boolean formulas.\nLet (X, Y) be a (Boolean) formula that models the relationship between the input variable set X and the output variable set Y in a given program, such that for any assignment of X, at most one assignment of Y satisfies the formula (X, Y). Let p represent a probability distribution defined over the set {0,1}. For each assignment \u03c3 to Y, i.e., \u03c3 : Y \u2192 \u03c3, the probability is defined as $p_\u03c3 = \\frac{|Sol(\\varphi(Y \\leftrightarrow \\sigma))|}{|Sol(\\varphi)\\downarrow_X|}$, where Sol(\u03c6(Y \u2192 \u03c3)) denotes the set of solutions of (Y\u2194 \u03c3) and Sol(\u03c6)\u2193x denotes the set of solutions of projected to X. The Shannon entropy of \u03c6 is H(\u03c6) = \u03a3\u03c3\u03b52\u03bd -polog po. Then we can immediately obtain a measure of leaked information with the computed entropy and the assumption that X follows a uniform distribution \u00b9 [Klebanov et al., 2013].\nThe workflow of the current precise methods of computing entropy can often be divided into two stages. In the first stage, we enumerate possible outputs, i.e., the satisfying assignments over Y, while in the second stage, we compute the probability of the current output based on the number of inputs mapped to the output [Golia et al., 2022]. The computation in the second stage often invokes model counting (#SAT), which refers to computing the number of solutions Sol(4) for a given formula 4. Due to the exponential possible outputs, the current precise methods are often difficult to scale to programs with a large size of Y. Therefore, more researchers are focusing on approximate estimation of Shannon entropy. We remark that Golia et al. [Golia et al., 2022] proposed the first Shannon entropy estimation tool, EntropyEstimation, which guarantees that the estimate lies within (1\u00b1\u20ac)-factor of H (6) with confidence at least 1 \u2013 \u03b4. EntropyEstimation employs uniform sampling to avoid generating all outputs, and indeed scales much better than the precise methods.\nAs previously discussed, current methods for precisely computing Shannon entropy struggle to scale with formulas involving a large set of outputs. Theoretically, this requires performing up to 2|Y| model counting queries. The primary"}, {"title": "2 Notations and Background", "content": "In this paper, we focus on the programs modeled by (Boolean) formulas. In the formulas discussed, the symbols x and y denote variables, and literal l refers to either the variable x or its negation \u00acx, where var(l) represents the variable underlying the literal l. A formula is constructed from the constants true, false and variables using negation operator \u00ac, conjunction operator \u2227, disjunction operator \u2228, implication operator \u2192, and equality operator \u2194, where Vars(6) denotes the set of variables appearing in 4. A clause C (resp. term T) is a set of literals representing their disjunction (resp. conjunction). A formula in conjunctive normal form (CNF) is a set of clauses representing their conjunction. Given a formula 4, a variable x, and a constant b, the substitution [x\u2192 b] refers to the transformed formula obtained by substituting the occurrence of x with b throughout \u03c6.\nAn assignment o over variable set V is a mapping from V to {false, true}.The set of all assignments over V is denoted by 2V. Given a subset V' \u2286 V, \u03c3\u03b9\u03bd\u03b9 = {x + b \u2208\u03c3 | x \u2208 V'}. Given a formula 4, an assignment over Vars(4) satisfies \u03c6 (\u03c3 = 4) if the substitution \u03c6[\u03c3] is equivalent to true. Given an assignment \u03c3, if all variables are assigned a value in {false, true}, then o is referred to as a complete assignment. Otherwise it is a partial assignment. A satisfying assignment is also called solution or model. We use Sol(4) to the set of solutions of 4, and model counting is the problem of computing | Sol(4)|. Given two formulas 4 and 4 over V,\n6 = 4 iff Sol( ^ \u00ab\u03c8) = \u00d8."}, {"title": "2.1 Circuit formula and its Shannon entropy", "content": "Given a formula (X, Y) to represent the relationship between input variables X and output variables Y, if \u03c3\u03b9\u03c7 = \u03c3'\u03c7 implies \u03c3 = \u03c3' for each \u03c3,\u03c3' \u2208 Sol(\u03c6), then we say y is a circuit formula. It is standard in the security community to employ circuit formulas to model programs in QIF [Golia et al., 2022].\nExample 1. The following formula is a circuit formula with input variables X = {x1,...,x2n} and output variables Y = {Y1,..., Y2n}: $ \\varphi_{rep} = \\bigwedge_{i=1}^{n}(X_i \\leftrightarrow X_{n+i} \\rightarrow Y_i \\bigwedge Y_{n+i})\\bigwedge (X_i \\bigvee X_{n+i} \\rightarrow \\neg Y_i \\bigwedge \\neg Y_{n+i})$.\nIn the computation of Shannon entropy, we focus on the probability of each output. Let p be a probability distribution defined over the set {false, true}Y. For each assignment \u03c3 to Y, i.e., \u03c3: Y \u2192 {false, true}, the probability is defined as $p_\u03c3 = \\frac{|Sol(\\varphi(Y \\leftrightarrow \\sigma))|}{|Sol(\\varphi)\\downarrow_X|}$, where Sol((Y \u2192 \u03c3)) denotes the set of solutions of \u03c6(Y \u2192 \u03c3) and Sol(6)\u2193x denotes the set of solutions of projected to X. Since 4 is a circuit formula, it is easy to prove that |Sol(6)\u2193x| = |Sol(\u03c6)|. Then, the entropy of o is H(\u03c6) = \u03a3\u03c3\u03b52\u03bd -polog po. The base for log may be chosen freely but we use base 2 by following the convention in QIF [Smith, 2009]."}, {"title": "2.2 Algebraic Decision Diagrams", "content": "The Algebraic Decision Diagram with conjunctive decomposition (ADD[^]) proposed in this paper is an extended form based on Algebraic Decision Diagram (ADD [Bahar et al., 1997]). ADD is an extended version of BDD that incorporates multiple terminal nodes, each assigned a real-valued. ADD is a compact representation of a real-valued function as a directed acyclic graph. The original design motivation for ADD was to solve matrix multiplication, shortest path algorithms, and direct methods for numerical linear algebra [Bahar et al., 1997]. In subsequent research, ADD has also been used for stochastic model checking [Kwiatkowska et al., 2007], stochastic programming [Hoey et al., 2013], and weighted model counting [Dudek et al., 2020].\nWe consider an ADD consisting of a quadruple (X, S, \u03c0, G), where X is the set of variables of a Boolean formula, S is an arbitrary set (called the carrier set), and \u03c0is the order of occurrence of the variables, G is a rooted directed acyclic graph [Dudek et al., 2020]. An ADD is a rooted directed acyclic graph G, where the non-terminal nodes have labels that are elements in the set X and have two outgoing edges labeled false and true (referred to in this paper as the lo edge and the hi edge, represented by dashed and solid lines respectively), and all the terminal nodes are labeled with an element in the set S. The order of the appearance of the labels of non-terminal nodes in all paths from the root to the terminal nodes in G must be consistent with the order of the elements in \u03c0."}, {"title": "3 ADD[^]: A New Tractable Representation", "content": "In order to compute the Shannon entropy of a circuit formula (X, Y), we need to use the probability distribution over the outputs. Algebraic Decision Diagrams (ADDs) are an influential compact probability representation that can be exponentially smaller than the explicit representation. Macii and Poncino [Macii and Poncino, 1996] showed that ADD supports efficient exact computation of entropy. However, we observed in the experiments that the sizes of ADDs often exponentially explode with large circuit formulas. We draw inspiration from a Boolean representation known as the Ordered Binary Decision Diagram with conjunctive decomposition (OBDD[^]) [Lai et al., 2017], which reduces its size through recursive component decomposition and divide-and-conquer strategies. This approach enables the representation to be exponentially smaller than the original OBDD. Accordingly, we propose a probabilistic representation called Algebraic Decision Diagrams with conjunctive decomposition (ADD[^]) and show it supports tractable entropy computation. ADD[A] is a general form of ADD and is defined as follows:\nDefinition 1. An ADD[^] is a rooted DAG, where each node u is labeled with a symbol sym(u). If u is a terminal node, sym(u) is a non-negative real weight, also denoted by w(u); otherwise, sym(u) is a variable (called decision node) or operator (called decomposition node). The children of a decision node u are referred to as the low child lo(u) and the high child hi(u), and connected by dashed lines and solid lines, respectively, corresponding to the cases where var(u) is assigned the value of false and true. For a decomposition node, its sub-graphs do not share any variables. An ADD[^] is imposed with a linear ordering < of variables such that given a node u and its non-terminal child v, var(u) < var(v).\nHereafter, we denote the set of variables that appear in the graph rooted at u as Vars(u) and the set of child nodes of u as Ch(u). We now turn to show that how an ADD[^] defines a probability distribution:\nDefinition 2. Let u be an ADD[^] node over a set of variables Y and let o be an assignment over Y. The weight of o is defined as follows:\n$\\omega(\\sigma, u) =\n\\begin{cases}\n\\omega(u) & \\text{terminal}\n\\\\\\Prod_{v \\in Ch(u)} \\omega(\\sigma, v) & \\text{decomposition}\n\\\\\\omega(\\sigma, lo(u)) & \\text{decision and } \\sigma \\vDash \\neg var(u)\n\\\\\\omega(\\sigma, hi(u)) & \\text{decision and } \\sigma \\vDash var(u)\n\\end{cases}$\nThe weight of an non-terminal ADD[^] rooted at u is denoted by w(u) and defined as \u03a3\u03c3\u22082Vars(u) \u03c9(\u03c3, u). The probability of o over u is defined as $p(\\sigma, u) = \\frac{\\omega(\\sigma,u)}{\\omega(u)}$."}, {"title": "3.1 Tractable Computation of Weight and Entropy", "content": "The computation of Shannon entropy of ADD[^] depends on the computation of its weight. We first show that for an ADD[A] node u, we can compute its weight w(u) in polynomial time.\nProposition 1. Given a non-terminal node u in ADD[^], its weight w(u) can be recursively computed as follows in polynomial time:\n$\\omega(u) =\n\\begin{cases}\n\\Prod_{v \\in Ch(u)} \\omega(v) & \\text{decomposition}\n\\\\2^{n_0} \\cdot w(lo(u)) + 2^{n_1} \\cdot w(hi(u)) & \\text{decision}\n\\end{cases}$\nwhere no = |Vars(u)| - |Vars(lo(u))| \u2013 1 and n\u2081 = | Vars(u)| - | Vars(hi(u))| \u2013 1.\nProof. The time complexity is immediate by using dynamic programming. We prove the equation can compute the weight correctly by induction on the number of variables of the ADD[A] rooted at u. It is obvious that the weight of a terminal node is the real value labeled. For the case of the A node, since the variables of the child nodes are all disjoint, it can be easily seen from Definition 1. Next, we will prove the case of the decision node. Assume that when | Vars(u)| \u2264 n, this proposition holds. For the case where | Vars(u)| = n + 1, we use Yo and Y\u2081 to denote Vars(lo(u)) and Vars(hi(u)), and we have Yo| \u2264 n and |Y\u2081| \u2264 n. Thus, w(lo(u)) and w(hi(u)) can be computed correctly. According to Definition 2, w(u) = \u03a3\u03c3\u03b52Vars(u) \u03c9(\u03c3, u). The assignments over Vars(u) can be divided into two categories:\n\u2022 The assignment o |= \u00acvar(u): It is obvious that \u03c9(\u03c3,\u03ba) = \u03c9(\u03c3\u03b9\u03b3\u03bf, lo(u)). Each assignment over Yo can be extended to exactly 2no different assignments over Vars(u) in this category. Thus, we have the following equation:\n$\\sum_{\\sigma \\in 2^{Vars (u)} \\wedge \\sigma \\vDash \\neg var(u)} \\omega(\\sigma, u) = 2^{n_0} \\cdot \\omega(lo(u)).$"}, {"title": "4 PSE: Scalable Precise Entropy Computation", "content": "In this section, we introduce our tool PSE, designed to compute the Shannon entropy of a specified circuit CNF formula with respect to its output variables. Similar to other Shannon entropy tools, the main process of PSE is divided into two stages: the Y-stage (corresponding to outputs) and the X-stage (corresponding to inputs). In the Y-stage, we execute search with respect to the ADD[^] framework to precisely compute the Shannon entropy. In the X-stage, we perform multiple optimized model counting. PSE, depicted in Algorithm 1, takes in a CNF formula, an input set X, and an output set Y, and returns the entropy H(6) of the formula. Propositions 1-2 and the following observation (its proof is in the appendix) guarantee that the entropy of the original formula is the output of the root call.\nObservation 1. Given a circuit formula 6(X, Y) and a partial assignment o without any input variables, we have the following properties:\n\u2022 \u03c6[\u03c3](X, Y \\ Vars(\u03c3)) is a circuit formula;\n\u2022 Each \u03c8\u2081(X \u2229 Vars(\u03c8i), Y \u2229 Vars(\u03c8\u2081)) is a circuit formula if = Ai-1 Vi and for 1 \u2264 i \u2260 j \u2264 m, Vars(i) \u2229 Vars(\u03c8;) = 0;\n\u2022 If [\u03c3] = true, \u03c3 contains each output variable."}, {"title": "4.1 Implementation", "content": "We now discuss the implementation details that are crucial for the runtime efficiency of PSE. Specifically, leveraging the tight interplay between entropy computation and model counting, our methodology integrates a variety of state-of-the-art techniques in model counting.\nInitially, we have the option to employ various methodologies for the model counting query denoted by CountModels in line 7. The first considered method is to individually employ state-of-the-art model counters, such as SharpSAT-TD [Korhonen and J\u00e4rvisalo, 2021], Ganak [Sharma et al., 2019] and ExactMC [Lai et al., 2021]. The second method, known as ConditionedCounting, requires the preliminary construction of a representation for the original formula to support linear model counting. The knowledge compilation languages that can be used for this method include d-DNNF [Darwiche, 2004], OBDD[^] [Lai et al., 2017], and SDD [Choi et al., 2013]. Upon reaching line 7, the algorithm executes conditioned model counting, utilizing the compiled representation of the formula and incorporating the partial assignment derived from the ancestor calls. The last method, SharedCounting, solves model counting queries by utilizing an exact model counter. Unlike the first method, it shares the component cache within the counter for all queries through a strategy named XCache. To distinguish it from the caching approach used in the X-stage, the caching method employed in the Y-stage is referred to as YCache. Our experimental observations indicate that the Shared Counting method is the most effective within the PSE framework.\nConjunctive Decomposition We employed dynamic component decomposition (well-known in model counting and knowledge compilation) to divide a formula into components, thereby enabling the dynamic programming calculation of their corresponding entropy, as stated in Proposition 2.\nVariable Decision Heuristic We implemented the current state-of-the-art model counting heuristics for picking variable from Y in the computation of Shannon entropy, including VSADS [Sang et al., 2005], minfill [Darwiche, 2009], the SharpSAT-TD heuristic [Korhonen and J\u00e4rvisalo, 2021], and DLCP [Lai et al., 2021]. Our experiments consistently"}, {"title": "5 Experiments", "content": "To evaluate the performance of PSE, we implemented a prototype in C++ that employs ExactMC as the model counter. We extended the benchmarks from the experiments of Golia et al. [Golia et al., 2022] to conduct a more comprehensive evaluation of the performance of PSE. We perform an extensive experimental evaluation over a comprehensive set of 441 benchmarks, which encompass diverse categories. These categories involve QIF benchmarks [Fremont et al., 2017], plan recognition [Soos et al., 2020], bit-blasted versions of SMTLIB benchmarks [Sharma et al., 2019], and QBFEval competitions [Golia et al., 2022]. Among them, the combinatorial circuits category further includes well-known benchmarks like ISCAS'85 and EPFL [Amar\u00fa et al., 2015; Brglez et al., 1989]. All experiments were run on a computer with Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz and 32GB RAM. Each instance was run on a single core with a timeout of 3000 seconds and 8GB memory.\nThrough our evaluations and analysis, we sought to answer the following research questions:\n\u2022 RQ1: How does the runtime performance of PSE compare to the state-of-the-art Shannon entropy tools with (probabilistic) accuracy guarantee?\n\u2022 RQ2: How do the utilized methods impact the runtime performance of PSE?\nGolia et al. [Golia et al., 2022] have already demonstrated that their probably approximately correct tool EntropyEstimation is significantly more efficient than the state-of-the-art precise Shannon entropy tools. Due to space limit, we only compare with EntropyEstimation, and our detailed experimental comparison with the state-of-the-art precise tools is presented in the appendix. We remark that PSE significantly outperforms the precise baseline (the baseline was able to solve only 18 benchmarks, whereas PSE solved 329 benchmarks). This marked improvement is attributed to the linear"}, {"title": "5.1 Comparison with Shannon entropy estimation tool", "content": "We compare PSE with EntropyEstimation, the current state-of-the-art Shannon entropy estimation tool on 441 instances."}, {"title": "5.2 Impact of algorithmic configurations", "content": "To better verify the effectiveness of the PSE methods and answer RQ2, we conducted a comparative study on all the utilized methods, including methods for the Y-stage: Conjunctive Decomposition, YCache, Pre, variable decision heuristics(minfill, DLCP, SharpSAT-TD heuristic, VSADS), and methods for the X-stage: XCache and Conditioned Counting. In accordance with the principle of control variables, we conducted ablation experiments to evaluate the effectiveness of each method, ensuring that each experiment differed from the PSE tool by only one method. The cactus plot for the different methods is shown in Figure 3, where PSE represents our tool. PSE-wo-Decomposition indicates that the ConjunctiveDecomposition method is disabled in PSE, which means that its corresponding trace is ADD. PSE-wo-Pre means that Pre is turned off in PSE."}, {"title": "6 Related work", "content": "Our work is based on the close relationship between QIF, model counting, and knowledge compilation. We introduce relevant work from three perspectives: (1) quantitative information flow analysis, (2) model counting, and (3) knowledge compilation.\nQuantified information flow analysis At present, the QIF method based on model counting encounters two significant challenges. The first challenge involves constructing the logical postcondition Iproc for a program proc [Zhou et al., 2018]. Although symbolic execution can achieve this, existing symbolic execution tools have limitations and are often challenging to extend to more complex programs, such as those involving symbolic pointers. The second challenge concerns model counting, a key focus of our research. For programs modeled by Boolean clause constraints, Shannon entropy can be computed via model counting queries, enabling the quantification of information leakage. Golia et al. [Golia et al., 2022] have made notable contributions to this"}, {"title": "7 Conclusion", "content": "In this paper, we propose a new compilation language, ADD[^], which combines ADD and conjunctive decomposition to optimize the search process in the first stage of precise Shannon entropy computation. In the second stage of precise Shannon entropy computation, we optimize model counting queries by utilizing the shared component cache. We integrated preprocessing, heuristic, and other methods into the precise Shannon computation tool PSE, with its trace corresponding to ADD[^]. Experimental results demonstrate that PSE significantly enhances the scalability of precise Shannon entropy computation, even outperforming the state-of-the-art entropy estimator EntropyEstimation in overall performance. We believe that PSE has opened up new research directions for entropy computing in Boolean formula modeling. We look"}]}