{"title": "How well can LLMs Grade Essays in Arabic?", "authors": ["Rayed Ghazawi\u00b9", "Edwin Simpson2"], "abstract": "This research assesses the effectiveness of state-of-the-art large language models (LLMs), including ChatGPT, Llama, Aya, Jais, and ACEGPT, in the task of Arabic automated essay scoring (AES) using the AR-AES dataset. It explores various evaluation methodologies, including zero-shot, few-shot in-context learning, and fine-tuning, and examines the influence of instruction-following capabilities through the inclusion of marking guidelines within the prompts. A mixed-language prompting strategy, integrating English prompts with Arabic content, was implemented to improve model comprehension and performance. Among the models tested, ACEGPT demonstrated the strongest performance across the dataset, achieving a Quadratic Weighted Kappa (QWK) of 0.67, but was outperformed by a smaller BERT-based model with a QWK of 0.88. The study identifies challenges faced by LLMs in processing Arabic, including tokenization complexities and higher computational demands. Performance variation across different courses underscores the need for adaptive models capable of handling diverse assessment formats and highlights the positive impact of effective prompt engineering on improving LLM outputs. To the best of our knowledge, this study is the first to empirically evaluate the performance of multiple generative Large Language Models (LLMs) on Arabic essays using authentic student data.", "sections": [{"title": "1 Introduction", "content": "Written assessments are a key method for evaluating students' performance, offering a deeper insight into their knowledge compared to other methods [1]. However, scoring essays is time-consuming and challenging for educators due to the significant effort required [1, 2]. AES alleviates this burden, allowing educators to focus more on task development and educational strategies while providing quicker feedback to enhance students' learning [3]."}, {"title": "2 Related Work", "content": "Automated essay scoring (AES) represents a longstanding area of inquiry within natural language processing (NLP) [9]. While considerable attention has been directed towards the English language, evidenced by endeavors to engineer features that capture grammatical and lexical aspects of essays [10-12], subsequent studies have introduced neural networks [13] and hierarchical sentence-document models aimed at more comprehensive essay representation [14, 15]. Pretrained transformers, such as BERT, have achieved SotA results [16, 17], and have continued to outperform more recent LLMs, including ChatGPT and Llama2 [6, 7]."}, {"title": "3 LLMs for Arabic AES", "content": "Llama: We treat AES as an ordinal classification task, where the task is to select the correct grade for an essay [25]. Llama 2 is a multilingual model that has shown strong performance in various text classification tasks, such as binary sentiment classification in English, where it achieved an average accuracy of ~91%, outperforming GPT-3.5 [26]. While the specific language proportions in Llama 2's training data have not been disclosed [27], it is clear that English text dominates, drawing from sources like Common Crawl, C4, GitHub, Wikipedia, and ArXiv [28]. This emphasis on English suggests Llama 2 may perform better in English than other languages [29]. However, recent studies also indicate Llama's capability in handling Arabic [30], with Llama 2 reaching high accuracy in Arabic question-answering (93.70) [31]. In English-language AES, Llama 2 has proven effective in grading short answers and essays [32], supporting its use in our study for Arabic AES.\nWe employed two Llama models for fine-tuning: Llama 2 7B (7-billion parameters) for label-supervised adaptation and the OpenLLaMA model (3-billion parameters) for instruction fine-tuning [28]. OpenLLaMA's training incorporates a diverse range of datasets, including refined-web data from Falcon, the StarCoder dataset, and portions of Wikipedia, ArXiv, and StackExchange from RedPajama. Although explicit evidence of Arabic data inclusion is not provided, it is likely that Wikipedia entries in multiple languages, including Arabic, are part of the training data. Similarly, ArXiv, books, and StackExchange, though primarily in English, may contain some Arabic content.\nChatGPT: ChatGPT has demonstrated effectiveness in a range of NLP tasks, including AES and Automatic Short Answer Grading (ASAG) in English [33-36]. Rasul et al. [37] examine the applications and potential of ChatGPT in educational settings, focusing on its capability to generate and evaluate long-form text, such as essays. The study emphasizes ChatGPT's effectiveness in following instructions and generating coherent, extended responses. In particular, ChatGPT-4 features a context window"}, {"title": "4 The Experiments", "content": "We train and evaluate our models on the AR-AES dataset [8], containing 2,046 essays written by male and female undergraduate students at Umm Al-Qura University, representing a range of disciplines and writing styles. The essays cover three question types: argumentative, narrative, and source-dependent [48]. This inclusion allows for the evaluation of AES models in various categories of essays, which include persuasive writing, analysis of source materials, and storytelling - categories where LLMs could have an advantage over smaller models. The essays originate from both traditional in-person and online exams, reflecting the contemporary assessment landscape, as shown in Table 1.\nThe models were trained and evaluated using the course director's marks as ground truth. We conducted assessments on the entire dataset, as well as on individual courses and questions. This approach enabled us to explore two methods: a general-purpose model applicable to all courses and questions, and specialized models tailored for specific courses or questions, simulating the different ways that AES models could be trained in a real-world application. For fine-tuning, the dataset was split into training, validation, and test sets in a 70/15/15 ratio, following the methodology outlined in [8]."}, {"title": "4.2 Tokenization", "content": "LLM tokenizers often encounter limitations when dealing with languages beyond English, including Arabic [30, 49]. In models like Llama and ChatGPT, tokenization splits each Arabic character into an individual token rather than treating words or phrases as single units [29]. This approach increases sequence length and can negatively impact model performance, as individual characters lack independent meaning. This results in higher GPU memory usage and potential performance degradation. For example, the phrase \"\u0645\u0631\u062d\u0628\u0627\" )English \"Hello\") is tokenized into five separate tokens instead of a single word-level token, as shown in Figure 1, compared to just one token for its English equivalent.\nThis happens because most transformers employ wordpiece or byte pair encoding algorithms, which build a vocabulary based on the training set [28]. When trained on datasets containing little Arabic, Arabic words are not added to the vocabulary and so, when the tokenizer is applied, the unrecognized words are split into single characters [29]. This issue also causes the decoder to generate individual characters one-at-a-time, which may limit its ability to generate complex, multi-character words and sentences.\nThe need for tokenizer optimization, especially for languages with morphological and syntactic structures that differ significantly from English has been highlighted in previous research [50], which points out how tokenization discrepancies in LLMs create fairness challenges. Firstly, there's a cost disparity: users of certain languages may end up paying more than 2.5 times the amount English users pay for the same task, thanks to the increased sequence lengths. Secondly, there's the issue of latency: processing time can increase twofold for languages that require more tokens. Lastly, there's the challenge of long context processing: some language models can handle significantly longer texts than others, impacting service quality.\nTo specifically address these challenges with the Llama model, we developed a custom SentencePiece tokenizer trained on the AR-AES dataset (the same dataset used for evaluation) to capture Arabic words at the word level rather than the character level. To integrate the new Arabic tokens, we concatenated them with the existing Llama vocabulary, expanding the tokenizer's vocabulary to include both original and new tokens. This required an update to the embedding layer to accommodate the"}, {"title": "4.3 Data Preprocessing", "content": "In the fine-tuning approach, we applied data preprocessing, while for few-shot and zero-shot tasks, the data was used in its original form. Preprocessing involved removing punctuation, hashtags, URLs, redundant letter repetitions, emoticons, extra spaces, numerical characters, and diacritics. These steps were taken to reduce noise in the text and focus the models on linguistic content rather than irrelevant features. This approach is consistent with findings by Kwon et al. [51], which demonstrate"}, {"title": "4.4 Training and Scoring Approaches", "content": "Three primary approaches were explored: zero-shot learning, few-shot in-context learning, and fine-tuning. Each approach offers distinct advantages that are particularly relevant to AES.\nZero-shot learning: without any task-specific training examples, relying on model capabilities derived from pretraining allows us to apply only the marking guidelines and scheme without requiring specific examples. This approach is efficient and cost-effective as it removes the need for fine-tuning and does not require the course instructor to provide examples of essay answers. [52].\nFew-shot in-context learning: allows the model to learn from a small number of examples provided within the prompt. This does not involve updating the model's weights, avoiding the costs of fine-tuning and making it particularly useful in situations where gathering large amounts of labeled data is impractical. However, it may not be as effective as fine-tuned pretrained models [53].\nFine-tuning: although requiring more human effort to provide more training examples than in-context learning, is highly effective with a relatively small number of examples, especially when task-specific data is available, and was shown to outperform in-context learning in short answer scoring [54]. In this study, we employed two fine-tuning methods: instruction tuning and label-supervised adaptation. Both approaches leveraged Low-Rank Adaptation (LoRA), a technique that updates only a small subset of the model's parameters. LoRA enables efficient fine-tuning, making it a practical choice for domain-specific applications such as Arabic Automated Essay Scoring (AES) [55].\nInstruction fine-tuning: tailors the model to follow specific prompts for essay scoring tasks, which may contain detailed instructions or examples. The model generates a numerical score as a response to these prompts, which we then extract from the output text using regular expressions, as illustrated in Figure 3. Loss is then computed by comparing the predicted label to the true label, then used to fine-tune the model's response. Each essay was paired with an instruction to assign a grade from"}, {"title": "4.5 Model Implementations", "content": "Llama: For instruction-tuning we use the \u201cLlama-2-7b-hf\" model [27], and for LS-LLAMA we use Open-LLaMA 3B [57], which is based on the same pretrained model as Llama-2 but has fewer parameters. This makes it more suitable for tasks that require efficient fine-tuning and lighter computational resources. Additionally, we implemented a custom SentencePiece tokenizer specifically optimized for Arabic text to reduce tokenized sequence lengths and enhance processing efficiency."}, {"title": "5 Evaluation Metrics", "content": "We utilized Quadratic Weighted Kappa (QWK) as our primary evaluation metric. QWK, an extension of Cohen's kappa, measures the agreement level between the outcomes assessed by two raters. It is particularly favored in Automated Essay Scoring (AES) evaluations because it accounts for agreements that could occur by chance, providing a more reliable measure of scoring consistency compared to mere accuracy and F1 score. Furthermore, QWK is appropriate for essay assessments as it respects the ordinal nature of grading and incorporates quadratic weights to emphasize the significance of class rankings a subtlety not captured by simple accuracy or F1 scores. While the AES models may be trained as nominal classifiers, it is necessary to evaluate their predictions as ordinal values, since larger errors in grade predictions would have a bigger negative impact.\nThe calculation of QWK involves a weighting factor defined as:\nQWK = 1- \u03a3ij Wi,jOi,j/Ei,j Wi,jni,1nj,2\nwhere wi,j = (i-j)2/(N-1)2 is the weight between mark i and mark j, N is the number of marks available, Oi,j is the number of observations where the first assessor gave mark i and the second assessor gave mark j, and ni,k is the number of times that assessor k gave mark i."}, {"title": "6 Prompt Engineering", "content": "ChatGPT: In the initial zero-shot experiments, we tested simple prompts in Arabic, such as \"Correct the following essay answers out of 5 marks.\" However, this simplicity led to inconsistent performance, with the model providing varied outcomes, including unrequested feedback, or assigning scores on a 100% scale, rather than the expected five-point scale. To improve precision, we revised the prompts to be more specific: \"Evaluate the essay by assigning one of the following grades: 0, 1, 2, 3, 4, or 5.\" While this generally produced valid grades, the model occasionally provided explanations or score ranges instead of a single score, which, although potentially useful to an essay marker, deviated from the primary objective of obtaining a final score for comparison with the actual score. To address these issues, we revised the prompts to provide clearer instructions: \"Evaluate the following essay answers by assigning one of the six grades only (0-1-2-3-4-5), without any explanation or comment, only the final grade number.\" Despite improved results, additional text sometimes appeared with the score, (e.g., \u201cThe final grade is: 3 out of 5\"), which was handled using regular expresions to extract the final grade.\nIn addition, we compared different combinations of Arabic text with English prompts on three questions chosen randomly from the narrative, source-dependent and argumentative categories (see Table 2). We observed that using English prompts instead of Arabic led to a noticeable increase in performance, improving the results by approximately 6%. Subsequently, the prompt was expanded to include detailed instructions in English, specifying the task, question, evaluation criteria (rubrics), golden answer, and evaluation scale, with improvement in the results on average. While this approach yielded better results, optimal performance was observed when instructions, such as the criteria for evaluation and the task description, were provided in English, while the essays, details of the question, rubrics, and standard answers remained in Arabic, improving the results by approximately 49.49%. This bilingual strategy avoided translating the Arabic essay answers into English, ensuring that the models evaluated the responses in their original form while leveraging the clarity of English for instructional prompts. This approach is illustrated in the last row of the zero shot section in Table 2, which highlights the improvements in accuracy under these conditions.\nBuilding on this, a few-shot learning configuration was implemented, using the same detailed prompts with the addition of three example answers for each class. The \"golden answer\" served as a reference for the highest score, while the other examples represented varying levels of alignment with the ideal response, corresponding to different scores. This bilingual approach again surpassed a fully English few-shot setup, as shown by the results in Table 2."}, {"title": "7 Results and Discussion", "content": "Our study used the experimental protocol delineated by Ghazawi and Simpson [8]. We adapted this framework for our exploratory analysis involving four LLMs (Llama, ChatGPT, Aya, Jais, and ACEGPT), initially evaluating the models across the entirety of the dataset. This preliminary evaluation aimed to gauge the models' performance when exposed to a wide variety of data and diverse question types. Subsequently, we embarked on a more granular analysis, where each model was trained and evaluated separately on the data of individual courses. Following the course-specific evaluations, the analysis was further refined by assessing each model's performance on distinct questions, allowing us to examine performance with more specialized fine-tuning, albeit with fewer examples. Additionally, we analyzed the models' performance based on essay types (argumentative, narrative, and source-dependent). This multi-tiered approach ensured a comprehensive understanding of each model's strengths and weaknesses in diverse educational and assessment contexts."}, {"title": "8 Conclusion and Future Works", "content": "This study evaluated a range of prominent large language models (LLMs), ChatGPT4, Llama 2, Aya, Jais, and ACEGPT, for automatic scoring of Arabic essays in the AR-AES dataset. We explored zero-shot, few-shot, and fine-tuning approaches. A key preprocessing challenge was tokenization, which we addressed by developing an optimized SentencePiece tokenizer to reduce the length of tokenized Arabic sequences, thereby decreasing memory usage and computational overhead. ACEGPT showed the strongest overall performance among LLMs, with a QWK score of 0.67, reflecting its extensive training on Arabic-specific data, which allowed it to consistently outperform the general-purpose Llama model. Fine-tuned models, particularly those using label-supervised adaptation (LS-LLaMA), including ACEGPT, outperformed zero-shot and few-shot in-context learning approaches, proving more reliable without requiring prompt engineering and despite having as few as 74 training examples. This underscores the importance of fine-tuning models on specialised Arabic datasets.\nAlthough fine-tuned LLMs showed improvements, they were still outperformed by AraBERT, showing the strength of BERT-based models when training sets are small. Prompt engineering was crucial for enhancing model outputs in zero- and few-shot setups, particularly for complex questions. Performance variability across courses and questions indicates the need for more adaptable models. Future research could investigate multitask learning and domain adaptation to provide additional training when few answer examples are available for fine-tuning."}, {"title": "9 Limitations and Ethical Considerations", "content": "A key limitation of this study is that we were unable to use some larger versions of the models, such as Llama-13B and Jais-70B, due to their high computational requirements. This challenge is compounded by the fact that processing Arabic text demands more computational resources compared to English. Furthermore, given the rapid evolution of large language models and the frequent release of new versions, our study was unable to include the most recent models, such as ChatGPT-4o1 and Llama3, which were released after our experiments were conducted. However, we aimed to draw general findings from the most relevant models available at the time of the study, while also acknowledging the ongoing challenges LLMs face in handling the complexities of the Arabic language. We plan to incorporate newer models in future work.\nEthical considerations are an important aspect of Automated Essay Scoring (AES), particularly regarding the potential for scoring errors. Although AES can help human graders improve consistency and reduce errors, it is essential to implement safeguards to address inaccuracies. Errors in scoring may result in unfair outcomes for students, potentially affecting their academic progress. To mitigate this risk, organizations that use AES should provide transparent feedback mechanisms and clear appeal processes for correcting mistakes."}]}