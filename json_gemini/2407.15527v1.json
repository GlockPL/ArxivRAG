{"title": "Interpretable Concept-Based Memory Reasoning", "authors": ["David Debot", "Pietro Barbiero", "Francesco Giannini", "Gabriele Ciravegna", "Michelangelo Diligenti", "Giuseppe Marra"], "abstract": "The lack of transparency in the decision-making processes of deep learning systems presents a significant challenge in modern artificial intelligence (AI), as it impairs users' ability to rely on and verify these systems. To address this challenge, Concept Bottleneck Models (CBMs) have made significant progress by incorporating human-interpretable concepts into deep learning architectures. This approach allows predictions to be traced back to specific concept patterns that users can understand and potentially intervene on. However, existing CBMs' task predictors are not fully interpretable, preventing a thorough analysis and any form of formal verification of their decision-making process prior to deployment, thereby raising significant reliability concerns. To bridge this gap, we introduce Concept-based Memory Reasoner (CMR), a novel CBM designed to provide a human-understandable and provably-verifiable task prediction process. Our approach is to model each task prediction as a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule. The presence of an explicit memory and the symbolic evaluation allow domain experts to inspect and formally verify the validity of certain global properties of interest for the task prediction process. Experimental results demonstrate that CMR achieves comparable accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers logic rules consistent with ground truths, allows for rule interventions, and allows pre-deployment verification.", "sections": [{"title": "Introduction", "content": "The opaque decision process of deep learning (DL) systems represents one of the most fundamental problems in modern artificial intelligence (AI). For this reason, eXplainable AI (XAI) [1-3] is currently one of the most active research areas in AI. Among XAI techniques, Concept Bottleneck Models (CBMs) [4-8] represented a significant innovation that made DL models explainable by design by introducing a layer of human-interpretable concepts within DL architectures. In CBMs, first, a concept encoder maps low-level raw features (e.g., an image's pixels) to high-level interpretable concepts (e.g., \"red\" and \"round\"), then a task predictor uses the learned concepts to solve a downstream task (e.g., \"apple\"). This way, each task prediction can be traced back to a specific pattern of concepts, thus allowing CBMs to provide explanations in terms of high-level interpretable concepts (e.g., concepts \u201cred\u201d and \u201cround\u201d were both active when the model classified an image"}, {"title": "Preliminary", "content": "Concept Bottleneck Models (CBMs) [4, 10] are functions composed of (i) a concept encoder $g : X \\rightarrow C$ mapping each entity $x \\in X \\subseteq R^d$ (e.g., an image) to a set of $n_c$ concepts $c \\in C$ (e.g., \"red\", \"round\"), and (ii) a task predictor $f : C \\rightarrow Y$ mapping concepts to the class $y \\in Y$ (e.g., \u201capple\u201d) representing a downstream task. For simplicity, in this paper, a single task class is discussed, as multiple tasks can be encoded by instantiating multiple task predictors. When sigmoid activations are used for concepts and task predictions, we can consider $g$ and $f$ as parameterizing a Bernoulli distribution of truth assignments to propositional boolean concepts and tasks. For example, $g_{red}(x) = 0.8$ means that there is an 80% probability for the proposition \"x is red\" to be true. During training, concept and class predictions $(c, y)$ are aligned with ground-truth labels $(\\hat{c}, y)$. This architecture and training allows CBMs to provide explanations for class predictions indicating the presence or absence of concepts. Another main advantage of these models is that, at test time, human experts may also intervene on mispredicted concept labels to improve CBMs' task performance and extract counterfactual explanations [4, 11]. However, the task prediction $f$ is still often a black-box model to guarantee high performances, thus not providing any insight into which concepts are used and how they are composed to reach the final prediction."}, {"title": "Method", "content": "In this section, we introduce Concept-based Memory Reasoner (CMR), the first deep learning model that is concept-based, globally interpretable and provably verifiable. Like other CBMS, CMR consists of two main components: a concept encoder and a task predictor. However, CMR's task predictor differs significantly from traditional CBMs. Instead of producing outputs through a black-box process, CMR's task predictor operates transparently by (1) selecting a logic rule from a set of jointly-learned rules, and (2) symbolically evaluating the chosen rule on the concept predictions. This unique approach enables CMR not only to provide explanations by tracing class predictions back to concept activations, but also to explain which concepts are utilized and how they interact to make a task prediction. Moreover, the set of learned rules remains accessible throughout the learning process, allowing users to analyze the model's behavior and automatically verify whether some desired properties are being fulfilled at any time. The logical interpretation of CMR's task predictor, combined with its provably verifiable behavior, distinguishes it sharply from existing CBMs' task predictors."}, {"title": "CMR probabilistic graphical model", "content": "In Figure 1, we show the probabilistic graphical model of CMR. There are four variables, three of which are standard in (discriminative) CBMs: the observed input $x \\in X$, the concepts encoding $c \\in C$, and the task prediction $y \\in \\{0,1\\}$. CMR adds an additional variable: the rule $r \\in \\{P, N, I\\}^{n_c}$. A rule is a conjunction in the concept set, like $c_1 \\wedge c_3$. A conjunction is uniquely identified when, for each concept $c_i$, we know whether, in the rule, the concept is irrelevant (I), positive (P) or negative (N). We call $r_i \\in \\{P, N, I\\}$ the role of the i-th concept in rule r. For example, given the three concepts $c_1, c_2, c_3$, the conjunction $c_1 \\wedge \\neg c_3$ can be represented as $r_1 = P, r_2 = I, r_3 = N$, since the role of $c_1$ is positive (P), the role of $c_2$ is irrelevant (I) and the role of $c_3$ is negative (N).\nThis probabilistic graphical model encodes the joint conditional distribution $p(y, r, c|x)$ and factorizes as:\n$p(y, r, c|x) = p(y|c,r)p(r|x)p(c|x)$\nHere, $p(c|x)$ is the concept encoder. For concept bottleneck encoders, it is simply the product of $n_c$ independent Bernoulli distributions $p(c_i|x)$, whose logits are parameterized by any neural network encoder $g_i: X \\rightarrow R$. Moreover, $p(r|x)$ is the rule selector, described in Section 3.1.1. Given an input x, $p(r|x)$ models the uncertainty over which conjunctive rule must be used. Lastly, $p(y|c, r)$ is the task predictor, which will be described in Section 3.1.2. Given a rule $r \\sim p(r|x)$ and an assignment of truth values $c \\sim p(c|x)$ to the concepts, the task predictor evaluates the rule on the concepts. In all the cases described in this paper, $p(y|c, r)$ is a deterministic degenerate distribution."}, {"title": "CMR rule selector", "content": "We model the rule selector $p(r|x)$ as a mixture of $n_R \\in N$ rule distributions. The selector \"selects\" a rule from a set of rule distributions (i.e. the components of the mixture), which we call a rulebook. The rulebook"}, {"title": "CMR task predictor", "content": "The CMR task predictor $p(y|r, c)$ provides the final y prediction given concept predictions c and the selected ruler r. We model the task predictor as a degenerate deterministic distribution, providing the entire probability mass to the unique value y which corresponds to the logical evaluation of the rule r on the concept predictions c. In particular, let $r_i \\in \\{P, N, I\\}$ be the role of the i-th concept in rule r. Then, the symbolic task prediction y obtained by evaluating ruler on concept predictions $c_i$ is:\n$y \\leftarrow \\bigwedge_{i=1}^{n_c} (r_i = I) \\vee (((r_i = P) \\Rightarrow c_i) \\wedge ((r_i = N) \\Rightarrow \\neg c_i))$\nHere, the y prediction is equivalent to a conjunction of $n_c$ different conjuncts, one for each concept. If a concept i is irrelevant according to the selected ruler (i.e. $r_i = I$), the corresponding conjunct is ignored. If $r_i = P$, then the conjunct is True if the corresponding concept is True. Otherwise, i.e. if $r_i = N$, the conjunct is True if the corresponding concept is False."}, {"title": "Expressivity, interpretability and verification", "content": "In this section, we will discuss the proposed model along three different directions: expressivity, interpretability and verification."}, {"title": "Expressivity", "content": "An interesting property is that CMR is as expressive as a neural network binary classifier.\nTheorem 4.1. CMR is a universal binary classifier [13] if $N_R \\geq 3$.\nProof. Recall that the rule selector is implemented by any neural network $\\phi(s) : X \\rightarrow R^{n_r}$. Consider the following three rules, easily expressible in CMR as showed on the right of each rule:\n$y \\leftarrow True \\quad (i.e. \\forall i: r_i = I)$\n$y \\leftarrow \\bigwedge_{i=1}^{n_c} c_i \\quad (i.e. \\forall i: r_i = P)$\n$y \\leftarrow \\bigwedge_{i=1}^{n_c} \\neg c_i \\quad (i.e. \\forall i: r_i = N)$\nBy selecting one of these three rules, the rule selector can always make a desired y prediction, regardless of the concepts c. In particular, to predict y = 1, the selector can select the first rule. To predict y = 0 when at least one concept is False in the concept predictions c (i.e. $\\exists i: c_i = 0$), it can select the second rule. Lastly, to predict y = 0 when all concepts are True in c (i.e. $\\forall i: c_i = 1$), it can select the last rule."}, {"title": "Interpretability", "content": "CMR task prediction is the composition of a (neural) rule selector and the symbolic evaluation of the selected rule. Therefore, we can always inspect the whole rulebook to know exactly the global behavior of the task prediction. In particular, let s be the selected rule, $e_j$ the embedding of the j-th rule, and $r_i^{(j)} \\in \\{P, N, I\\}$ the role of the i-th concept in the j-th rule at decision time, i.e. $r_i^{(j)} = argmax(\\phi_i^{(r)}(e_j))$. Then, CMR's task predictor can be logically defined as the global rule obtained as the disjunction of all decoded rules, each filtered by whether the rule has been selected or not. That is:\ny = \\bigvee_{j=1}^{N_R} ((s = j) \\wedge (\\bigwedge_{i=1}^{n_c} (r_i^{(j)} = I) \\vee (((r_i^{(j)} = P) \\Rightarrow c_i) \\wedge ((r_i^{(j)} = N) \\Rightarrow \\neg c_i))))(1)"}, {"title": "Verification", "content": "One of the main properties of CMR is that, at decision time, it explicitly represents the task prediction as a set of conjunctive rules. Logically, the mixture semantics of the selector can be interpreted as a disjunction, leading to a standard semantics in stochastic logic programming [20, 21]. As the only neural component is in the selection, task predictions generated using CMR's global formula (cf. Section 4.2) can be automatically verified by using standard tools of formal verification (e.g. model checking), no matter which rule will be selected. Being able to verify properties prior to deployment of the model strongly sets CMR apart from existing models, where verification tasks can only be applied at prediction time. In particular, given any propositional logical formula \u03b1 over the propositional language $\\{C_1, C_2, ..., C_{n_c}\\} \\cup \\{r_i^{(j)}\\}$, \u03b1 can be automatically verified to logically follow from Equation 1. In logical terms, if the formula \u03b1 is entailed by Equation 1, it"}, {"title": "Learning problem", "content": "Learning in CMR follows the standard objective in CBM literature, where the likelihood of the concepts and task observations in the data is maximized. Formally, let \u03a9 be the set of parameters of the probability distributions, such that CMR's probabilistic graphical model is globally parameterized by \u03a9, i.e. p(y, r, c|x; \u03a9). Let $D = \\{(x, \\hat{c}, \\hat{y})\\}$ be a concept-based dataset of i.i.d. triples (input, concepts, task). Then, learning is the optimization problem:\n$\\max_{\\Omega} \\sum_{(x,\\hat{c},\\hat{y}) \\in D} log p(y, \\hat{c}; \\Omega)(2)$\nDue to the factorization of the mixture model in the rule selection, CMR has a tractable likelihood computation. In particular, the following result holds.\nTheorem 5.1 (Log-likelihood). The maximum likelihood simplifies to the following $O(n_c \\cdot n_R)$ objective:\n$\\max_{\\Omega} \\sum_{(x,\\hat{c},\\hat{y}) \\in D} [\\sum_{i=1}^{n_c} log p(c_i = \\hat{c}_{i}|x) + \\sum_{s=1}^{N_R} log p(s = \\hat{s}|x) p(y = \\hat{y}|\\hat{c}, \\hat{s})] (3)$\nwith:\n$p(y|c, s) = \\prod_{i=1}^{n_c} (p(I_{i}|s) + p(P_{i}|s) 1[c_i = 1] + p(N_{i}|s) 1[c_i = 0])$\nwhere 1[.] is an indicator function of the condition within brackets.\nThe maximum likelihood approach only focuses on the prediction accuracy of the model. However, as discussed in Section 4.2, we look for the set of learned rules r to represent good prototypes of concept predictions, as in prototype-based learning [15]. To drive the learning of representative positive prototypes when we observe a positive value for the task, i.e. when y = 1, we add a regularization term to the objective. Intuitively, every time a rule is selected for a given input instance x with task label y = 1, we want the rule to be as close as possible to the observed concept prediction. At the same time, since the number of rules is limited and the possible concept activations are combinatorial, the same rule is expected to be selected for different concept activations. When this happens, we will favor rules that assign an irrelevant role to the inconsistent concepts in the activations. The regularized objective is:\n$\\max_{\\Omega} \\sum_{(x,\\hat{c},\\hat{y}) \\in D} [\\sum_{i=1}^{n_c} log p(c_i = \\hat{c}_{i}|x) + log(\\sum_{s=1}^{N_R} p(s = \\hat{s}|x) p(y = \\hat{y}|\\hat{c}, s) P_{reg}(r = \\hat{c}|\\hat{s}))](4)$\nand:\n$P_{reg}(r = \\hat{c}|s) = \\prod_{i=1}^{n_c} (0.5p(r_i = I|s) + p(r_i = P|s) 1[\\hat{c}_i = 1] + p(r_i = N|s) 1[\\hat{c}_i = 0])$\nThis term favors the selected rule r to reconstruct the observed $\\hat{c}$ as much as possible. When such reconstruction is not feasible due to the limited capacity of the rulebook, the term will favor irrelevant roles for concepts. In"}, {"title": "Experiments", "content": "Our experiments aim to answer the following research questions:\n(1) Generalization: Does CMR attain similar task and concept accuracy as existing CBMs and black boxes? Does CMR generalize well when the concept set is incomplete??\n(2) Explainability and Intervenability: Can CMR recover ground truth rules? Can CMR learn meaningful rules when the concept set is incomplete? Are concept interventions and rule interventions effective in CMR?\n(3) Verifiability: Can CMR allow for post-training verification regarding its behavior?"}, {"title": "Experimental setting", "content": "This section describes essential information about experiments. We provide further details in Appendix C.\nData & task setup. We base our experiments on four different datasets commonly used to evaluate CBMs: MNIST+ [22], where the task is to predict the sum of two digits; C-MNIST, where we adapted MNIST to the task of predicting whether a colored digit is even or odd; MNIST+*, where we removed the concepts for the digits 0 and 1 from the concept set; CelebA [23], a large-scale face attributes dataset with more than 200K celebrity images, each with 40 concept annotations; CUB [24], where the task is to predict bird species based on bird characteristics; and CEBAB [25], a text-based task where reviews are classified as positive or negative based on different criteria (e.g. food, ambience, service, etc). These datasets range across different concept set quality, i.e. complete (MNIST+, C-MNIST, CUB) vs incomplete (CelebA, MNIST+*), and different complexities of the concept prediction task, i.e. easy (MNIST+, MNIST+*, C-MNIST), medium (CEBAB) and hard (CelebA, CUB).\nEvaluation. To measure classification performance on tasks and concepts, we compute subset accuracy and regular accuracy, respectively. For CUB, we instead compute the Area Under the Receiver Operating Characteristic Curve [26] for the tasks due to the large class imbalance. All metrics are reported using the mean and the standard error of the mean over three different runs with different initializations.\nBaselines. In our experiments, we compare CMR with existing CBM architectures. We consider Concept Bottleneck Models with different task predictors: linear, multi-layer (MLP), decision-tree (DT) and XGBoost (XG). Moreover, we add two state-of-the-art CBMs: Concept Embedding Models (CEM) [27] and Deep Concept Reasoner (DCR) [11]. We employ hard concepts in CMR and our competitors, avoiding the problem of input distribution leakage that can affect task accuracy [28, 29] (see Appendix C for additional details). Finally, we include a deep neural network without a concept bottleneck to measure the effect of an interpretable architecture on generalization performance."}, {"title": "Key findings & results", "content": null}, {"title": "Generalization", "content": "CMR's high degree of interpretability does not harm accuracy, which is similar to or better than competitors'. In Table 1, we compare CMR with its competitors regarding task accuracy. On all data sets, CMR achieves an accuracy close to black-box accuracy, either beating its concept-based competitors or obtaining similar results. In Table 5 of Appendix C, we show that CMR's training does not harm concept accuracy, which is similar to its competitors.\nCMR obtains accuracy competitive with black boxes even on incomplete concept sets. We evaluate the performance of CMR on settings with increasingly more incomplete concept sets. Firstly, as shown in Table 1, in MNIST+*, CMR still obtains task accuracy close to the complete setting, beating its competitors which suffer from a concept bottleneck. Secondly, we run an experiment on CelebA where we gradually decrease the number of concepts in the concept set. Figure 3 shows the achieved task accuracies for CMR and the competitors. CMR's accuracy remains high no matter the size of the concept set, while the performance of the competitors with a bottleneck (i.e. all except CEM) strongly degrades."}, {"title": "Explanations and intervention", "content": "CMR discovers ground truth rules. We quantitatively evaluate the correctness of the rules CMR learns on MNIST+ and C-MNIST. In the former, the ground truth rules have no irrelevant concepts; in the latter, they do. In all runs of these experiments, CMR finds all correct ground truth rules. In C-MNIST, CMR correctly learns that the concepts related to color are irrelevant for classifying the digit as even or odd (see Table 2).\nCMR discovers meaningful rules in the absence of ground truth. While the other datasets do not provide ground truth rules, a qualitative inspection shows that they are still meaningful. Table 2 shows two examples for CEBAB, and additional rules can be found in Appendix C."}, {"title": "Verification", "content": "CMR allows verification of desired global properties. In this task, we automatically verify semantic consistency properties for MNIST+ and CelebA whether CMR's task prediction satisfies some properties of interest. For verification, we exploited a naive model checker that verifies whether the property holds for all concept assignments where the theory holds. When this is not feasible, state-of-the-art model formal verification tools can be exploited, as both the task prediction and the property are simply two propositional formulas. For MNIST+, we can verify that, for each task y, CMR never uses more than one positive concept (i.e. digit) per image, i.e. $\\forall y, i, j : c_i \\rightarrow \\neg c_j : i \\neq j)$. This is also easily verifiable by simply inspecting the rules in Appendix C. Moreover, in CelebA, we can easily verify that $Bald \\Rightarrow \\neg Wavy\\_Hair$ with the learned rulebook for nc = 12 (see Table 10 in Appendix C), as $\\neg Bald$ is a conjunct in each rule that does not trivially evaluate to False."}, {"title": "Related works", "content": "In recent years, XAI techniques have been criticized for their vulnerability to data modifications [30, 31], insensitivity to reparameterizations, [32], and lacking meaningful interpretations for non-expert users [33]. To address these issues, Concept-based methods [34, 35, 5, 10] have emerged, offering explanations in terms of human-interpretable features, a.k.a. concepts. Concept Bottleneck Models [4] take a step further by directly integrating these concepts as explicit intermediate network representations. Concept Embeddings Models (CEMs) [7, 8, 11] close the accuracy gap with black-box models through vectorial concept representations. However, they still harm the final interpretability, as it is unclear what information is contained in the embeddings. In contrast, CMR closes the accuracy gap by exploiting a neural rule selector coupled with learned symbolic logic rules. As a result, CMR's task prediction is fully transparent, allowing experts to see how concepts are being used for task prediction, and allowing intervention and automatic verification of desired properties. To the best of our knowledge, there is only one other attempt at analyzing CBMs' task prediction in terms of logical formulae, namely DCR [11]. For a given example, DCR predicts and subsequently evaluates a (fuzzy) logic rule. As rules are predicted on a per-example basis, the global behavior of DCR cannot be inspected, rendering interaction (e.g. manually adding expert rules) and verification impossible. In contrast, CMR learns (probabilistic) logic rules in a memory, allowing for inspection, interaction and verification.\nThe use of logic rules by CMR for interpretability purposes aligns it closely with the field of neurosymbolic AI [36, 37]. Here, logic rules [38, 39, 18] or logic programs [40, 22, 21] are used in combination with neural networks through the use of neural predicates [22]. Concepts in CMR are akin to a propositional version of neural predicates. However, in CMR, the set of rules is learned instead of given by the human and concept direct supervision is used for human alignment.\nFinally, the relationships with prototype-based models in interpretable AI have been already discussed in Section 4.2."}, {"title": "Conclusions", "content": "We propose CMR, a novel Concept Bottleneck Model that offers a human-understandable and provably-verifiable task prediction process. CMR integrates a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rules. Our approach enables global interpretability and verification of task prediction properties. Our results show that (1) CMR achieves near-black-box accuracy, (2) discovers meaningful rules, and (3) facilitates strong interaction with human experts through rule interventions. The development of CMR can have significant societal impact by enhancing the transparency, verifiability, and human-AI interaction, thereby fostering trust and reliability in critical decision-making processes.\nLimitations and future works. CMRs are still foundational models and several limitations need to be explored further in future works. In particular, CMRs are focusing on positive-only explanations, while negative-reasoning explanations have not been explored yet. Moreover, the same selection mechanism can be tested in non-logic, interpretable settings (like linear models). Finally, the verification capabilities of CMR will be tested on more realistic, safety critical domains, where the model can be verified against safety specifications."}, {"title": "Maximum likelihood derivation", "content": "We show that the maximum likelihood problem in Equation 2 simplifies to:\n$log p(\\hat{y}, c|x) = (\\sum_{i=1}^{n_c} log p(c_i = \\hat{c}_{i}|x)) + (log \\sum_{s=1}^{N_R} p(s = \\hat{s}) p(y = \\hat{y}|c, s))$\nwith:\n$p(y|c, s) = \\prod_{i=1}^{n_c} (p(I_{i}|s) + p(P_{i}|s) 1[c_i = 1] + p(N_{i}|s) 1[c_i = 0])$\nProof. Let n := nc. First, we express the likelihood as the marginalization of the distribution over the unobserved variables\n$p(y, c|x) = p(c|x) \\sum_s p(s|x) \\sum_{r_1} ... \\sum_{r_n} p(r_1, ..., r_n|s) p(y|r_1, ..., r_n, c_1, ..., c_n)$\nDue to the independence of the individual components of the rule distribution:\n$p(r_1, ..., r_n|s) = \\prod_{i=1}^n p(r_i|s)$"}, {"title": "Implementation and optimization details", "content": "Selector re-initialization To promote exploration, we re-initialize the parameters of p(s|x) multiple times during training, making it easier to escape local optima. The re-initialization frequency is a hyperparameter that differs between experiments, see Appendix C.\nEffect of the regularization term Figure 4 shows the probabilities to be maximized when the label y = 1, for a selected rule and a single concept, with respect to different roles r for that concept. Figures 4a, 4b and 4c show the probabilities to be maximized without the regularization. Figures 4d, 4e and 4f show the regularization probabilities (remember, we only have these if y = 1). Figures 4g, 4h and 4i show the probabilities when both are present. As mentioned in the main text, when c is True for all examples that select the rule, we want that concept's role to be P. However, when optimizing without regularization, it is clear that e.g. I is also an optimum (Figure 4a). Because the regularization only has an optimum in P (Figure 4d), adding the regularization results in the correct optimum (Figure 4g). A similar reasoning applies"}, {"title": "Datasets", "content": "MNIST+ This dataset [22] consists of pairs of images, where each image is an MNIST image of a digit and the task is the sum of the two digits. For these tasks, all concepts are relevant. There is a total of 30,000 training examples and 5,000 test examples.\nMNIST+* We create this dataset as MNIST+ except that the concepts for digits 0 and 1 are removed from the concept set. This makes the concept set incomplete.\nC-MNIST We derive this dataset from MNIST [41], taking the MNIST training (60,000 examples) and test set (10,000 examples), randomly coloring each digit either red or green, and adding these two colors as concepts. There are two tasks: The first is whether the digit is even, and the second is whether it is odd. For these tasks, the concepts related to color are irrelevant.\nCelebA This is a large-scale face attributes dataset with more than 200K celebrity images [23]. Each image has 40 concept annotations. As tasks, we take the concepts Wavy_Hair, Black_Hair, and Male, removing them from the concept set."}, {"title": "Training", "content": "Reproducibility We seed all experiments using seeds 1, 2 and 3.\nSoft vs hard concepts When departing from pure probabilistic semantics, CBMs allow not only the use of concepts as binary variables, but they allow for concepts to be passed to the task predictor together with their prediction scores, which is called employing soft concepts (vs hard concepts). While some CBMs use soft concepts as this results in higher task accuracy, the downside of this is that the use of soft concepts also comes with the introduction of input distribution leakage: The concept probabilities encode much more information than what is related to the concepts, severely harming the interpretability of the model [28, 29]. For this reason, in our experiments, all models use hard concepts, which is realized by thresholding the soft concept predictions at 50%.\nModel input For MNIST+, MNIST+* and C-MNIST, we train directly on the images. For CelebA and CUB, instead of training on the images, we train the models on pretrained ResNet18 embeddings [42]. Specifically, using the torchvision library, we first resize the images to width and height 224 (using bi-linear interpolation), then normalize them per channel with means (0.485, 0.456, 0.406) and standard-deviations (0.229, 0.224, 0.225) (for CelebA only). Finally, we remove the last (classification) layer from the pretrained ResNet18 model, use the resulting model on each image, and flatten the output, resulting in an embedding. For CEBAB, we use a pretrained BERT model [43] to transform the input into embeddings. Specifically, using the transformers library [44], we create a BERT model for sequence classification from the pretrained model 'bert-base-uncased' with 13 labels (1 per concept and 1 representing both tasks). Then, we finetune this model for 10 epochs with batch size 2, 500 warmup steps, weight decay 0.01 and 8 gradient accumulation steps. After training, we use this model to transform each example into an embedding by outputting the last hidden states for that example.\nGeneral training information In all experiments, we use the AdamW optimizer. All neural competitors are optimized to maximize the log-likelihood of the data with a weight on the likelihood of the task (1 if not explicitly mentioned below). After training, for each neural model in CelebA, \u0421\u0415\u0412\u0410\u0412, MNIST+ and MNIST+*, we restored the weights that resulted in the lowest validation loss. In CUB, C-MNIST and the MNIST+ rule intervention experiment, we do not use a validation set, instead restoring the weights that resulted in the lowest training loss. In CelebA, we use a validation split of 8:2, a learning rate of 0.001, a batch size of 1000, and we train for 100 epochs. In CEBAB, we use a validation split of 8:2, a learning rate of 0.001, a batch size of 128, and we train for 100 epochs. In CUB, we use a learning rate of 0.001, a batch size of 1280, and we train for 300 epochs. In MNIST+ and MNIST+*, we use a validation split of 9:1. We use a learning rate of 0.0001, a batch size of 512, and we train for 300 epochs. In C-MNIST, we also use a learning rate of 0.0001, a batch size of 512, and we train for 300 epochs."}, {"title": "General architecture details", "content": "In CMR, we use two hidden layers with ReLU activation to transform the input into a different embedding. We use 3 hidden layers with ReLU activation and an output layer with Sigmoid activation to transform that embedding into concept predictions. The component $p(s|x)$ takes as input that embedding and is implemented by a single hidden layer with ReLU activation and an output layer outputting Ntasks * NR logits, which are reshaped to $(Ntasks, NR)$ and to which a softmax is applied over the rule dimension. The rulebook is implemented as an embedding module of shape (Ntasks * NR, rule emb size) that is reshaped to $(Ntasks, NR, rule emb size)$. The rule decoder is implemented by a single hidden layer with ReLU activation and an output layer outputting 3 * Nconcepts logits, which are reshaped to (nconcepts, 3), after which a Softmax is applied to the last dimension; the result corresponds to $p(r|s)$. At test time, we make p(ris) deterministic by setting the probability for the most likely role for each concept to 1 and the others to 0 (effectively collapsing each rule distribution to the most likely rule for that distribution). Then, exactly one rule corresponds with each s.\nThe deep neural network is a feed-forward neural network consisting of some hidden layers with ReLU activation and an output layer with Sigmoid activation.\nBoth CBM+linear and CBM+MLP have a concept predictor that is a feed-forward neural network using ReLU activation for the 3 hidden layers and a Sigmoid activation for the output layer. For CBM+linear, the task predictor is a single linear layer per task with Sigmoid activation. For CBM+MLP, this is a feed-forward neural network using ReLU activation for the 3 hidden layers, and a Sigmoid activation for the output layer.\nFor CEM, we use 4 layers with ReLU activation"}]}