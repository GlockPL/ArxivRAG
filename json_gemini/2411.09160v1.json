{"title": "Rationality based Innate-Values-driven Reinforcement Learning", "authors": ["Qin Yang"], "abstract": "Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially developing the awareness of the AI agent through balancing internal and external utilities based on its needs in different tasks is a crucial problem for individuals learning to support AI agents integrating human society with safety and harmony in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model \u2013 innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of AI agents' interaction. We formulated the IVRL model and proposed two IVRL models: DQN and A2C. By comparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the Role-Playing Game (RPG) reinforcement learning test platform VIZDoom, we demonstrated that rationally organizing various individual needs can effectively achieve better performance.", "sections": [{"title": "1 Introduction", "content": "In natural systems, motivation is concerned explicitly with the activities of creatures that reflect the pursuit of a particular goal and form a meaningful unit of behavior in this function [1]. Furthermore, intrinsic motivations describe incentives relating to an activity itself, and these incentives residing in pursuing an activity are intrinsic. Intrinsic motivations deriving from an activity may be driven primarily by interest or activity-specific incentives, depending on whether the object of an activity or its performance provides the main incentive [2]. They also fall in the category of cognitive motivation theories, which include theories of the mind that tend to be abstracted from the biological system of the behaving organism [3].\n\nHowever, when we analyze natural agents, such as humans, they are usually combined motivation entities. They have biological motivations, including physiological, safety, and existence needs; social motivation, such as love and esteem needs; and cognitive motivation, like self-actualization or relatedness and growth needs [4]. The combined motivation theories include Maslow's Hierarchy of Needs [5] and Alderfer's Existence Relatedness Growth (ERG) theory [6].\n\nMany researchers regard motivated behavior as behavior that involves the assessment of the consequences of behavior through learned expectations, which makes motivation theories tend to be intimately linked to theories of learning and decision-making [7]. In particular, intrinsic motivation leads organisms to engage in exploration, play, strategies, and skills driven by expected rewards. The computational theory of reinforcement learning (RL) addresses how predictive values can be learned and used to direct behavior, making RL naturally relevant to studying motivation."}, {"title": "2 Approach Overview", "content": "We assume that all the AI agents (like robots) interact in the same working scenario, and their external environment includes all the other group members and mission setting. In contrast, the internal environment consists of individual perception components including various sensors (such as Lidar and camera), the critic module involving intrinsic motivation analysis and innate values generation, the RL brain making the decision based on the feedback of rewards and description of the current state (including internal and external) from the critic module, and actuators relating to all the manipulators and operators executing the RL brain's decisions as action sequence and strategies.\n\nCompared with the traditional RL model, our model generates the input state and rewards from the critic module instead of directly from the environment. This means that the individual needs to calculate rewards based on the innate value and current utilities and then update its current"}, {"title": "2.1 The Source of Randomness", "content": "In our model, the randomness comes from three sources. The randomness in action is from the policy function: $A \\sim \\pi(\\cdot|s)$; the needs weight function: $W \\sim w(\\cdot|s)$ makes the randomness of innate values; the state-transition function: $S' \\sim p(\\cdot|s, a)$ causes the randomness in state.\n\nSupposing at current state $s_t$ an agent has a needs weight matrix $N_t$ (Eq. (1)) in a mission, which presents its innate value weights for different levels of needs. Correspondingly, it has a utility matrix $U_t$ (Eq. (1)) for specific needs resulting from action $a_t$. Then, we can calculate its reward $R_t$ for $a_t$ through Eq. (2) at the state $s_t$."}, {"title": "2.2 Randomness in Discounted Returns", "content": "According to the above discussion, we define the discounted return $G$ at time $t$ as cumulative discounted rewards in the IVRL model (Eq. (3)) and $\\gamma$ is the discount factor.\n\n$G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots + \\gamma^{n-t} R_n$                                                                                                                              (3)\n\nAt time $t$, the randomness of the return $G_t$ comes from the the rewards $R_t, \\cdots, R_n$. Since the reward $R_t$ depends on the state $S_t$, action $A_t$, and needs weight $W_t$, the return $G_t$ also relies on them. Furthermore, we can describe their randomness as follows:\n\nState transition: $P[A = a|S = s, A = a] = p(s'|s, a)$;                                                                                                                     (4)\n\nNeeds weights function: $P[W = w|S = s] = w(w|s)$;                                                                                                                   (5)\n\nPolicy function: $P[A = a|S = s] = \\pi(a|s)$.                                                                                                                           (6)"}, {"title": "2.3 Action-Innate-Value Function", "content": "Based on the discounted return Eq. (3) and its random factors - Eq. (4), (5), and (6), we can define the Action-Innate-Value function as the expectation of the discounted return $G$ at $t$ time (Eq. (7)).\n\n$Q_{\\pi,\\omega}(S_t, W_t, a_t) = E[G_t|S_t = s_t, W_t = w_t, A_t = a_t]$                                                                                                       (7)\n\n$Q_{\\pi,\\omega}(S_t, W_t, a_t)$ describes the quality of the action $a_t$ taken by the agent in the state $s_t$, using the needs weight $w_t$ generating from the needs weight function $w$ as the innate value judgment to execute the policy $\\pi$."}, {"title": "2.4 State-Innate-Value Function", "content": "Furthermore, we can define the State-Innate-Value function as Eq. (8), which calculates the expectation of $Q_{\\pi,\\omega}(S_t, W_t, a_t)$ for action $A$ and reflects the situation in the state $s_t$ with the innate value judgment $w_t$.\n\n$V_{\\pi}(S_t, W_t) = E_A [Q_{\\pi,\\omega}(S_t, W_t, A)]$                                                                                                                                      (8)"}, {"title": "2.5 Approximate the Action-Innate-Value Function", "content": "The agent's goal is to interact with the environment by selecting actions to maximize future rewards based on its innate value judgment. We make the standard assumption that a factor of $\\gamma$ per time-step discounts future rewards and define the future discounted return at time $t$ as Eq. (3). Moreover, we can define the optimal action-value function $Q^*(s, a, w)$ as the maximum expected return achievable by following any strategy after seeing some sequence $s$, making corresponding innate value judgment $w$, and then taking action $a$, where $w$ is a needs weight function describing sequences about innate value weights and $\\pi$ is a policy mapping sequences to actions.\n\n$Q^*(s, w, a) = \\max_{\\omega,\\pi} E[G_t|S_t = S_t, W_t = w_t, A_t = a_{\\tau, \\omega, \\pi}]$                                                                                                                 (9)"}, {"title": "2.6 IVRL Advantage Actor-Critic (A2C) Model", "content": "Furthermore, we extend our IVRL method to the Advantage Actor-Critic (A2C) version. Specifically, our IVRL A2C maintains a policy network $\\pi(a_t|s_t; \\theta)$, a needs network $w(w_t|s_t; \\delta)$, and a utility value network $u(s_t, a_t; \\varphi)$. Since the reward in each step is equal to the current utilities $u(s_t, a_t)$ multiplying the corresponding weight of needs, the state innated-values function can be approximated by presenting it as Eq. (15). Then, we can get the policy gradient Eq. (16) and needs gradient Eq. (17) of the Eq. (15) deriving $V (s; \\theta, \\delta)$ according to the Multi-variable Chain Rule, respectively. We can update the policy network $\\theta$ and needs network $\\delta$ by implementing policy gradient and needs gradient, and using the temporal difference (TD) to update the value network $\\varphi$.\n\n$V_{\\pi,\\omega}(s) = \\sum_{a_t, W_t} \\pi(a_t | S_t) \\cdot w(w_t|S_t) \\cdot u(S_t, a_t)$                                                                                                                   \n\n$\\sim \\sum_{a,w} \\pi(a_t | S_t; \\theta) \\cdot w(w_t|S_t; \\delta) \\cdot u(S_t, a_t; \\varphi) = V (s; \\theta, \\delta, \\varphi)$                                                                                                               (15)\n\n$\\frac{\\mathrm{grad} V (a_t, \\theta_t)}{\\partial \\theta} = \\frac{\\partial V(s; \\theta, \\delta, \\varphi)}{\\partial \\theta} = \\sum \\frac{\\partial \\pi(a_t|s_t; \\theta)}{\\partial \\theta} \\cdot w(w_t|s_t; \\delta) \\cdot u(s_t, a_t;\\varphi)$                                                                                             (16)\n\n$\\frac{\\mathrm{grad} V (w_t, \\delta_t)}{\\partial \\delta} = \\frac{\\partial V(s; \\theta, \\delta, \\varphi)}{\\partial \\delta} = \\sum \\pi(a_t|s_t; \\theta) \\cdot \\frac{\\partial w(w_t|s_t; \\delta)}{\\partial \\delta} \\cdot u(s_t, a_t; \\varphi)$                                                                                         (17)\n\nUsing an estimate of the utility $u$ function as the baseline function, we subtract the $V$ value term as the advantage value. Intuitively, this means how much better it is to take a specific action and a needs"}, {"title": "3 Experiments", "content": "Considering cross-platform support and the ease of creating custom scenarios, we selected the VIZDoom Role-Playing Game (RPG) reinforcement learning test platform [20, 21] to evaluate the performance of the proposed innate-value-driven reinforcement learning model. We choose four scenarios: Defend the Center, Defend the Line, Deadly Corridor, and Arens (Fig. 7), and compare our models with several benchmark algorithms, such as DQN [16], DDQN [17], A2C [18], and PPO [19]. These models were trained on an NVIDIA GeForce RTX 3080Ti GPU with 16 GiB of RAM."}, {"title": "3.1 Environment Setting", "content": "In our experiments, we define four categories of utilities (health points, amount of ammo, environment rewards, and number of killed enemies), presenting three different levels of needs: low-level safety and basic needs, medium-level recognition needs, and high-level achievement needs. When the agent executes an action, it receives all the corresponding innate utilities, such as health points and ammo costs, and external utilities, such as environment rewards (living time) and the number of killed enemies. At each step, the agent can calculate the rewards for the action by multiplying the current utilities and the needed weight for them. In our experiments, the initial needs weight for each utility category is 0.25, which has been fixed in the benchmark DRL algorithms' training, such as DQN, DDQN, and PPO. For more details about the experiment code, please check the supplementary materials."}, {"title": "3.2 Evaluation", "content": "The performance of the proposed IVRL DQN and A2C models is shown in the Fig 8. Fig. 7(a), 7(b), 7(c), and 7(d) demonstrate that IVRL models can achieve higher average scores than traditional RL benchmark methods (Fig. 8(a), 8(d), 8(g), and 8(j)). Especially for the IVRL A2C algorithm, it presents more robust, stable, and efficient properties than the IVRL DQN model. Although the IVRL DQN shows better performance in the Arena scenario (Fig. 8(j)), the small perturbation introduced by the innate-values utilities may have made the network weights in some topology difficult to reach convergence.\n\nFurthermore, we also analyze their corresponding tendencies in different scenarios to compare the needs weight differences between the IVRL DQN and A2C models. In the defend-the-center and defend-the-line experiments, each category of the need weight in the IVRL DQN model does not split and converges to a specific range compared with its initial setting in our training (Fig. 8(b) and 8(e)). In contrast, the weights of health depletion, ammo cost, and sub-goal (environment rewards) shrink to approximately zero, and the weight of the number of killed enemies converges to one in the IVRL A2C model. This means that the top priority of the IVRL A2C agent is to eliminate all the threats or adversaries in those scenarios so that it can survive, which is similar to the Arena task. According to the performance in those three scenarios (Fig. 8(c), 8(f), and 8(1)), the IVRL A2C agent represents the characteristics of bravery and fearlessness, much like the human hero in a real battle. However, in the deadly corridor mission, the needs weight of the task goal (getting the vest) becomes the main priority, and the killing enemy weight switches to the second for the IVRL A2C agent (Fig. 8(i)). They converge to around 0.6 and 0.4, respectively. In training, by adjusting its different needs weights to maximize rewards, the IVRL A2C agent develops various strategies and skills to kill the encounter adversaries and get the vast efficiently, much like a military spy.\n\nIn our experiments, we found that selecting the suitable utilities to consist of the agent innate-values system is critically important for building its reward mechanism, which decides the training speed and sample efficiency. Moreover, the difference in the selected utility might cause some irrelevant experiences to disrupt the learning process, and this perturbation leads to high oscillations of both innate-value rewards and needs weight.\n\nGenerally, the innate value system serves as a unique reward mechanism driving agents to develop diverse actions or strategies satisfying their various needs in the systems. It also builds different personalities and characteristics of agents in their interaction. From the environmental perspective, due to the various properties of the tasks, agents need to adjust their innate value system (needs weights) to adapt to different tasks' requirements. These experiences also shape their intrinsic values in the long term, similar to humans building value systems in their lives. Moreover, organizing agents with similar interests and innate values in the mission can optimize the group utilities and reduce costs effectively, just like \u201cBirds of a feather flock together.\" in human society."}, {"title": "4 Conclusion", "content": "This paper introduces the innate-values-driven reinforcement learning (IVRL) model mimicking the complex behaviors of agent interactions. By adjusting needs weights in its innate-values system, it can adapt to different tasks representing corresponding characteristics to maximize the rewards efficiently. For theoretical derivation, we formulated the IVRL model and proposed two types of IVRL models: DQN and A2C. Furthermore, we compared them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the RPG reinforcement learning test platform VIZDoom. The results prove that rationally organizing various individual needs can effectively achieve better performance."}]}