{"title": "Large Language Models Struggle in Token-Level Clinical Named Entity Recognition", "authors": ["Qiuhao Lu, Ph.D.", "Rui Li, Ph.D.", "Andrew Wen, M.S.", "Jinlian Wang, Ph.D.", "Liwei Wang, M.D., Ph.D.", "Hongfang Liu, Ph.D."], "abstract": "Large Language Models (LLMs) have revolutionized various sectors, including healthcare where they are employed in diverse applications. Their utility is particularly significant in the context of rare diseases, where data scarcity, complexity, and specificity pose considerable challenges. In the clinical domain, Named Entity Recognition (NER) stands out as an essential task and it plays a crucial role in extracting relevant information from clinical texts. Despite the promise of LLMs, current research mostly concentrates on document-level NER, identifying entities in a more general context across entire documents, without extracting their precise location. Additionally, efforts have been directed towards adapting ChatGPT for token-level NER. However, there is a significant research gap when it comes to employing token-level NER for clinical texts, especially with the use of local open-source LLMs. This study aims to bridge this gap by investigating the effectiveness of both proprietary and local LLMs in token-level clinical NER. Essentially, we delve into the capabilities of these models through a series of experiments involving zero-shot prompting, few-shot prompting, retrieval-augmented generation (RAG), and instruction-fine-tuning. Our exploration reveals the inherent challenges LLMs face in token-level NER, particularly in the context of rare diseases, and suggests possible improvements for their application in healthcare. This research contributes to narrowing a significant gap in healthcare informatics and offers insights that could lead to a more refined application of LLMs in the healthcare sector.", "sections": [{"title": "Introduction", "content": "Electronic Health Records (EHRs) are a key component in modern healthcare, encapsulating vast amounts of patient data, most notably in clinical notes. Their widespread adoption by healthcare providers in recent years has revolutionized the manner in which patients' visits and health information are recorded and managed\u00b9, thereby elevating the quality of patient care. However, extracting pertinent information from these records presents a significant challenge due to the abundant, heterogeneous, and private nature of clinical texts. At the heart of addressing this challenge is Named Entity Recognition (NER), a fundamental task in natural language processing aimed at identifying and categorizing key information units in the text. In the clinical domain, the goal of the task is to identify all occurrences of specific clinically relevant named entities in the given unstructured clinical narrative2.\nThere has been a surge of interest in developing clinical NER systems in the last few years. Early methods mostly rely on manually-crated rules and traditional machine learning techniques, such as MetaMap\u00b3, KnowledgeMap\u2074, cTAKES5, etc. With the trending of deep learning methods and the Transformer architecture, more researchers shift to building clinical NER systems and other NLP applications upon pre-trained language models such as BERT7. A typical solution is to insert a multilayer perception (MLP) on top of the language model and train the entire model via fine-tuning. For example, Alsentzer et al. fine-tune BioClinicalBERT on four i2b2 NER tasks8. Similarly, Sung et al. present BERN2, which uses Bio-LM\u00ba as the foundation model and achieves better performance via multitask learning10.\nA common theme among the aforementioned deep-learning-based methods is their data-hungry nature, which unfortunately poses significant challenges in the medical field, where issues of data scarcity, heterogeneity, and confidentiality are consistently prevalent. The situation is even worse for rare diseases due to their diversity, complexity, and specificity. Rare diseases refer to diseases or conditions that affect fewer than 200,000 Americans by definition in the United States\u00b9\u00b9. These conditions are often underrepresented in datasets, leading to a scarcity of information that deep learning models can learn from.\nRecent advancements in computational linguistics have led to the emergence of Large Language Models (LLMs). These models have shown remarkable capabilities in various domains, including the clinical field12,13,14. Their strengths in"}, {"title": "Methods", "content": "Task Overview Token-level named entity recognition (NER), also known as span-based NER, involves identifying and classifying named entities within a text into predefined categories such as diseases, symptoms, genes, etc. Formally, in a sentence with N tokens $X = [x_1, x_2,..., x_N]$, an entity is defined as a contiguous span of tokens $e = [x_i, ..., x_j]$, where $0 \\leq i \\leq j \\leq N$, and each is associated with a specific entity type. The core task is treated as a sequence labeling problem, where the goal is to assign a corresponding sequence of labels $Y = [Y_1, Y_2, \u2026\u2026\u2026, Y_N]$ to the sentence X. In this study, the BIO (Beginning, Inside, Outside) schema is employed for labeling. According to this scheme, the first token of an entity of a certain type is tagged as B-type, any subsequent tokens within the same entity are tagged as I-type, and tokens not part of an entity are tagged as 0. In this regard, the following eleven categories or labels are used in the experiments: 0, B-Disease, I-Disease, B-RareDisease, I-RareDisease, B-SkinRareDisease, I-SkinRareDisease, B-Symptom, I-Symptom, B-Sign, and I-Sign.\nIn this study, we aim to understand the capabilities of various general and medical LLMs in token-level NER, which is underexplored in clinical settings. Similar to Shry et al.'s work\u00b9\u2077, we choose rare diseases as our case study. Our focus is particularly on extracting information about rare diseases and their associated phenotypes, driven by two"}, {"title": "Results", "content": "LLM Initial Evaluation In our initial evaluation, we focus on assessing the performance of the aforementioned LLMs in token-level NER using the RareDis-v1 dataset, particularly targeting the two key target entities, i.e., Disease and Rare Disease. The results in Table 2 show that, generally, all the models struggle with this task, as evidenced by their modest performance metrics. Notably, Meditron, with its medical training, demonstrates marginally better results than LLaMA-2. However, even UniversalNER, which is optimized for document-level NER, does not show significantly improved performance in this token-level task. In contrast, Llama2-MedTuned-7b demonstrates promising results, closely trailing behind ChatGPT-4, which highlights the potential of local open-source LLMs in clinical NLP applications, specifically in token-level NER.\nFocused Evaluation of Token-Level NER Capable LLMs Table 3 provides an in-depth performance analysis of ChatGPT-3.5, ChatGPT-4, and Llama2-MedTuned, selected based on their promising results in our initial evaluation. We encompass a range of experimental settings in this subsection, including zero-shot, few-shot, and retrieval-augmented generation (RAG). The discussion on the fine-tuning approach is reserved for the following subsection, as it requires the use of the entire training dataset.\nA notable observation is the relatively high performance in identifying Rare Disease, likely due to its uniqueness and specificity. However, the models have varying degrees of difficulty in identifying subjective experiences reported by the patient, i.e., Symptom, as reflected by their close-to-zero precision and F1-scores. Moreover, we show that few-shot learning can effectively improve the performance across all models, with an increase ranging from 3% to 8% in average F1 scores. We also observe that ChatGPT-3.5 shows a substantial improvement of 20% in the F1-score for Rare Disease. These observations suggest that few-shot learning serves as an effective method to enhance LLMs'"}, {"title": "Discussion and Conclusion", "content": "In this study, we investigate the capabilities of LLMs in token-level NER for rare diseases and their associated phenotypes on the RareDis-v1 dataset. Essentially, we find that in general, most LLMs struggle with this challenging task. We also find that a medically-adapted LLaMA-2-7b model, i.e., Llama2-MedTuned, surpasses the performance of ChatGPT-3.5 and matches the performance of ChatGPT-4 on this task, highlighting the potential of local open-source LLMs in clinical NLP applications. Our further analysis suggests that via in-context learning, i.e., few-shot learning and RAG, the performance can be improved while RAG has a relatively limited influence. We also show that after fine-tuning Llama2-MedTuned on this dataset, its performance gets boosted and significantly outperforms that of ChatGPT-4, approaching the levels of BioClinicalBERT. It is worth noting that we use LoRA adapters to perform the fine-tuning, instead of the entire model. Based on these results, we anticipate that full model fine-tuning could potentially enable the Llama2-MedTuned to match BioClinicalBERT's performance.\nLarge language models (LLMs) have demonstrated superior performance in question-answering-related tasks across almost all domains, including the clinical field. However, their performance in conventional NLP tasks, such as information extraction, has been questioned. Unlike existing studies that either focus on document-level NER or solely rely on the prompt engineering of ChatGPT-series models, our experiment covers a broad scope of LLMs, from proprietary to local open-source and from general-purpose to medically adapted models. We thoroughly evaluate the LLMs on the token-level NER task, showing their struggle and potential directions for refinement. This study could shed light on adapting LLMs to specific NLP applications in clinical settings.\nThere are several factors that contribute to the struggle of LLMs on this task. Firstly, token-level NER is more challenging than document-level NER. Token-level NER is a token classification task and places significant emphasis on the representation of each individual token. This is in contrast to document-level NER which is a sequence classification task where a pooled representation is often sufficient for predictions, making the role of individual token representation less critical. Secondly, the foundational pre-training objective of most LLMs (decoder-only transformers) is causal language modeling, i.e., next token prediction. Unlike encoder-only transformers like BERT, LLMs don't have an encoder structure that is pre-trained to maximize the model's text representation power, making them less effective in classification tasks. Thirdly, token-level NER often involves a variety of special, previously unseen symbols, such as the diverse BIO tags. These unique elements can present additional complexities for LLMs to understand and operate.\nThere are also a few options to adapt these LLMs to the token-level NER task. A typical solution is continuous pre-training, or instruction-fine-tuning, just as Llama2-MedTuned and what we do using the rare disease data. Basically, the idea is to cast the NER problem as text generation, so the LLM can have a better understanding of the task and thus process it correctly in the same manner as it was pre-trained. For instance, Yang et al. integrate Human Phenotype Ontology (HPO) labels into clinical notes and fine-tune GPT-based models (e.g., GPT-J) for phenotype recognition\u00b3\u00b3.\nThey also conduct a comprehensive comparative analysis of encoder-based and decoder-based models for this task. Another solution is to leverage LLMs to generate synthetic data and use the data to train smaller specified models. In addition to the data, one can also alter the model architecture of LLMs. For example, Li et al. propose to fine-tune LLaMA-2 models in a label-supervised way, by removing the causal mask from the decoders to enable bidirectional self-attention which is essential for token classification tasks like NER34. However, in our preliminary experiment, the method does not work as expected and the performance is worse than instruction-fine-tuning on the rare disease dataset. We leave it for future study.\nThis study has certain limitations. Firstly, it does not explore the potential benefits of specific prompt engineering techniques, such as utilizing feedback from error analysis to refine prompts and thereby enhance model performance. Additionally, our approach mainly focuses on prompting LLMs to generate BIO tags. However, there are alternative approaches that are unexplored, such as the generation of text marked by special symbols to represent span information, which could offer a different perspective on model efficacy. Furthermore, in terms of instruction-fine-tuning of LLMs, our methodology is limited to employing LoRA adapters. A more extensive approach such as full model fine-tuning remains untested and could be valuable for future research.\nTo conclude, in this study, we explore the capabilities and limitations of existing LLMs, especially local open-source LLMs, in the task of token-level NER of rare diseases and their associated phenotypes. Through this exploration, we aim to contribute to the advancement of LLM-based token-level NER methodologies in the clinical domain, ultimately aiding in the improvement of patient care by enhancing the processing and understanding of clinical texts."}]}