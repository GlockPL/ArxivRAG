{"title": "Large Language Models Struggle in Token-Level Clinical Named Entity Recognition", "authors": ["Qiuhao Lu, Ph.D.", "Rui Li, Ph.D.", "Andrew Wen, M.S.", "Jinlian Wang, Ph.D.", "Liwei Wang, M.D., Ph.D.", "Hongfang Liu, Ph.D."], "abstract": "Large Language Models (LLMs) have revolutionized various sectors, including healthcare where they are employed in diverse applications. Their utility is particularly significant in the context of rare diseases, where data scarcity, complexity, and specificity pose considerable challenges. In the clinical domain, Named Entity Recognition (NER) stands out as an essential task and it plays a crucial role in extracting relevant information from clinical texts. Despite the promise of LLMs, current research mostly concentrates on document-level NER, identifying entities in a more general context across entire documents, without extracting their precise location. Additionally, efforts have been directed towards adapting ChatGPT for token-level NER. However, there is a significant research gap when it comes to employing token-level NER for clinical texts, especially with the use of local open-source LLMs. This study aims to bridge this gap by investigating the effectiveness of both proprietary and local LLMs in token-level clinical NER. Essentially, we delve into the capabilities of these models through a series of experiments involving zero-shot prompting, few-shot prompting, retrieval-augmented generation (RAG), and instruction-fine-tuning. Our exploration reveals the inherent challenges LLMs face in token-level NER, particularly in the context of rare diseases, and suggests possible improvements for their application in healthcare. This research contributes to narrowing a significant gap in healthcare informatics and offers insights that could lead to a more refined application of LLMs in the healthcare sector.", "sections": [{"title": "Introduction", "content": "Electronic Health Records (EHRs) are a key component in modern healthcare, encapsulating vast amounts of patient data, most notably in clinical notes. Their widespread adoption by healthcare providers in recent years has revolutionized the manner in which patients' visits and health information are recorded and managed\u00b9, thereby elevating the quality of patient care. However, extracting pertinent information from these records presents a significant challenge due to the abundant, heterogeneous, and private nature of clinical texts. At the heart of addressing this challenge is Named Entity Recognition (NER), a fundamental task in natural language processing aimed at identifying and categorizing key information units in the text. In the clinical domain, the goal of the task is to identify all occurrences of specific clinically relevant named entities in the given unstructured clinical narrative2.\nThere has been a surge of interest in developing clinical NER systems in the last few years. Early methods mostly rely on manually-crated rules and traditional machine learning techniques, such as MetaMap\u00b3, KnowledgeMap\u2074, cTAKES5, etc. With the trending of deep learning methods and the Transformer architecture, more researchers shift to building clinical NER systems and other NLP applications upon pre-trained language models such as BERT7. A typical solution is to insert a multilayer perception (MLP) on top of the language model and train the entire model via fine-tuning. For example, Alsentzer et al. fine-tune BioClinicalBERT on four i2b2 NER tasks. Similarly, Sung et al. present BERN2, which uses Bio-LM\u00ba as the foundation model and achieves better performance via multitask learning10.\nA common theme among the aforementioned deep-learning-based methods is their data-hungry nature, which unfortu- nately poses significant challenges in the medical field, where issues of data scarcity, heterogeneity, and confidentiality are consistently prevalent. The situation is even worse for rare diseases due to their diversity, complexity, and specificity. Rare diseases refer to diseases or conditions that affect fewer than 200,000 Americans by definition in the United States\u00b9\u00b9. These conditions are often underrepresented in datasets, leading to a scarcity of information that deep learning models can learn from.\nRecent advancements in computational linguistics have led to the emergence of Large Language Models (LLMs). These models have shown remarkable capabilities in various domains, including the clinical field12,13,14. Their strengths in in-context learning, such as retrieval-augmented generation (RAG), present new opportunities for tackling the challenges of NER in clinical texts. Despite the promise of LLMs, most existing research in this area has either focused on document-level NER (i.e., aggregation NER), which involves identifying entities at a broader level in entire documents or paragraphs without predicting the exact span or location of these entities, or on adapting ChatGPT to this task. For example, Zhou et al. introduce UniversalNER, a distillation approach using mission-focused instruction-tuning to create efficient models that excel in NER. They distill ChatGPT into a more compact LLaMA-based15 model UniversalNER and demonstrate superior document-level NER accuracy across diverse domains, including biomedicine16. Similarly, in their exploration of ChatGPT-3.5-turbo, Shry et al. focus on the extraction of rare diseases and their associated phenotypes via prompt engineering, also limiting their analysis to document-level output, which overlooks the precise span and location of these entities\u00b9\u2077. In contrast, the potential of LLMs, especially local open-source LLMs, in token-level NER (i.e., span-based NER) in clinical settings, particularly for rare diseases, remains relatively unexplored.\nThe difference between document-level and token-level NER is illustrated in Figure 1. This gap is significant as token-level NER offers the potential for more detailed and precise entity recognition, which is crucial in clinical applications. For instance, to interpret clinical texts like chemotherapy treatment records, in a scenario where a patient's record contains multiple chemotherapy sessions over several months, document-level NER may not efficiently distinguish between the different treatment phases and their respective impacts. In contrast, token-level NER can capture specific details. For example, consider a clinical text, \"Patient experienced nausea after chemo session 1 in January and significant fatigue following chemo session 3 in March.\" While document-level NER might identify terms like \"chemo\u201d, \u201cnausea\u201d, and \"fatigue\", it fails to connect these conditions to specific treatment events. Token-level NER, however, can extract fine-grained details, such as linking \u201cnausea\u201d directly to \u201cchemo session 1 in January\u201d, providing valuable context for understanding and tracking the patient's treatment response over time. This precision is vital for personalized patient care.\nThis study aims to explore the effectiveness of LLMs in token-level NER for rare diseases within clinical texts. By focusing on this specific application, we seek to understand the limitations and capabilities of LLMs in handling the challenges of token-level entity recognition in a domain where data is scarce and highly specialized. Essentially, we explore the performance of various state-of-the-art local open-source LLMs, including LLaMA-218, Meditron19, Llama2-MedTuned20, and UniversalNER16, on the task of NER of rare diseases and their phenotypes. Additionally, we assess the performance of prominent models such as ChatGPT-3.5 and ChatGPT-4. Our investigation extends to examining their capabilities in in-context learning, evaluating performance enhancements through few-shot learning, retrieval-augmented generation (RAG), and fine-tuning. The experimental results reveal that without proper fine-tuning, local LLMs generally struggle with token-level NER, despite prior training on clinical texts. We observe that while few-shot learning can effectively improve the performance of most LLMs, the influence of RAG on the same task is relatively minimal. Notably, our findings indicate that a medically-adapted LLaMA-2-7b model, specifically Llama2-MedTuned20, can outperform ChatGPT-4 on this task. This is particularly noteworthy as it achieves these results without having been trained specifically on the rare disease data used in the experiments, and this highlights the potential of fine-tuning local LLMs for specialized applications in clinical NER and other clinical NLP tasks."}, {"title": "Related Work", "content": "Recently, researchers have released various LLMs dedicated to the medical field. LLaMA15 and LLaMA-218 are one of the first and most popular open-source LLMs, laying the foundation for extensive research and applications. Meditron19 is a suite of open-source medical LLMs pre-trained on the GAP-Replay corpus, including clinical guidelines, research papers, and general domain data, and surpass the performance of LLaMA-218, ChatGPT-3.5, and Flan-PaLM21 on multiple medical reasoning benchmarks. BioMistral22 is another significant contribution, built upon the Mistral model23 and further enhanced with training on PubMed Central, showing superior performance on various medical question-answering tasks.\nIn parallel, considerable efforts have been devoted to adapting these LLMs to the task of NER through prompting or fine-tuning. For example, Zhou et al. instruction-fine-tune the LLaMA model15 using ChatGPT-generated synthetic data for NER from broad-coverage unlabeled web text16. Their UniversalNER model shows promising NER performance across multiple domains. However, they only generate document-level outputs, i.e., a list of extracted entities in the given text as shown in Figure 2a, without considering their exact span information, which limits their impact and usage in practical scenarios. Hu et al. explore the token-level NER capabilities of ChatGPT-3.5 and ChatGPT-4 on two clinical NER tasks by manually crafting specific prompts24. However, they limit their exploration to ChatGPT models and do not investigate local and open-source LLMs. Another study that aligns closely with our research is by Shry et al., who focus on extracting rare diseases and their phenotypes at a document-level by prompting ChatGPT-3.517.\nUnlike these studies, we aim to bridge the gap by exploring the capabilities of both local open-source LLMs and ChatGPT models in the specific context of token-level NER in clinical settings. This approach seeks to understand how these models perform in an area that is not only highly specialized but also characterized by its complexity and data scarcity, particularly in the study of rare diseases."}, {"title": "Methods", "content": "Task Overview Token-level named entity recognition (NER), also known as span-based NER, involves identifying and classifying named entities within a text into predefined categories such as diseases, symptoms, genes, etc. Formally, in a sentence with N tokens \\(X = [x_1, x_2,..., x_N]\\), an entity is defined as a contiguous span of tokens \\(e = [x_i, ..., x_j]\\), where \\(0 \\le i \\le j \\le N\\), and each is associated with a specific entity type. The core task is treated as a sequence labeling problem, where the goal is to assign a corresponding sequence of labels \\(Y = [Y_1, Y_2, \u2026\u2026\u2026, Y_N]\\) to the sentence X. In this study, the BIO (Beginning, Inside, Outside) schema is employed for labeling. According to this scheme, the first token of an entity of a certain type is tagged as B-type, any subsequent tokens within the same entity are tagged as I-type, and tokens not part of an entity are tagged as 0. In this regard, the following eleven categories or labels are used in the experiments: 0, B-Disease, I-Disease, B-RareDisease, I-RareDisease, B-SkinRareDisease, I-SkinRareDisease, B-Symptom, I-Symptom, B-Sign, and I-Sign.\nIn this study, we aim to understand the capabilities of various general and medical LLMs in token-level NER, which is underexplored in clinical settings. Similar to Shry et al.'s work\u00b9\u2077, we choose rare diseases as our case study. Our focus is particularly on extracting information about rare diseases and their associated phenotypes, driven by two primary reasons: 1) Data related to rare diseases is scarce and highly specialized, presenting an ideal scenario to leverage the strengths of LLMs; and 2) Patients with rare diseases are a relatively understudied group, necessitating increased research and advocacy efforts, as highlighted by Nguyen et al.25 To this end, our investigation spans across the overall performance of both local open-source LLMs and ChatGPT models on this task. More specifically, we initially explore the zero-shot performance of these LLMs using manually-designed prompts. Secondly, to enhance the models' performance for this specific task, we employ in-context learning strategies, i.e., few-shot learning and retrieval-augmented generation (RAG). These approaches provide rich information to the prompts to facilitate a deeper understanding of the task by the LLMs. Finally, we instruction-fine-tune Llama2-MedTuned20 on the rare disease dataset to assess the performance of LLMs in a fully supervised way. Our intention is to explore how well LLMs, when fine-tuned with direct supervision, can adapt to and accurately identify entities in complex rare disease data.\nDataset In our experiments, we use the RareDis-v1 dataset26,27, a curated collection of texts from the National Organization for Rare Disorders (NORD) database\u00b9. This dataset specifically comprises selected sections from NORD articles, which have been manually annotated to identify five key entity types: Disease, Rare Disease, Skin Rare Disease, Symptom, and Sign. The statistics of the dataset are detailed in Table 1, where the first two rows show the number of documents and sentences, respectively. The last four rows are a breakdown of the specific clinical entities present in the dataset. It is important to distinguish between Sign, which are objectively observable indicators or test results suggesting a disease, and Symptom, which are subjective experiences reported by the patient26.\nModels We consider the following LLMs for our experiments:\n\u2022 LLaMA-2-7b18 is one of the first and most popular local open-source pre-trained LLMs released. Though it is not dedicated to the clinical domain, the model is often considered a critical benchmark in recent related studies due to its impact and generalizability. For consistency, we utilize the 7-billion-parameter version of all local LLMs in this study.\n\u2022 Meditron-7b19 is an adapted version of LLaMA-2 to the medical domain through continued pre-training on a comprehensively curated medical corpus, including selected PubMed papers and abstracts, a new dataset of internationally-recognized medical guidelines, and general domain data from RedPajama-v128. It outperforms LLaMA-2-7B and PMC-LLaMA29 on multiple medical reasoning tasks."}, {"title": "Learning Strategies", "content": "In this study, we consider both in-context learning and instruction-fine-tuning to further adapt the LLMs to the token-level NER task. For in-context learning, we employ few-shot learning and retrieval-augmented generation (RAG), the prompt templates of which are shown in Figure 2. Essentially, we randomly select 5 samples from the training set as the demonstration to the LLMs. To ensure consistency, we fix the 5 samples for all few-shot experiments in the study. For RAG implementation, we utilize the LlamaIndex framework 3, incorporating NORD rare disease articles as the knowledge base. Specifically, these articles are segmented into chunks (chunk_size = 512), embedded using the bge-large-en-v1.5 model30, and stored in a vector database. In the retrieval stage, we convert a query using the embedding model and retrieve the top-k (similarity_top_k = 2) most relevant text chunks based on similarity with the query embedding. The retrieved chunks are then integrated into the LLM prompts for enriched context.\nFor instruction-fine-tuning, we follow previous work20 and convert the RareDis dataset to the Stanford Alpaca\u00b3\u00b9 format. This format is structured for instruction-following, comprising three parts: an instruction, an input, and an output, as shown in Figure 2. We then fine-tune the Llama2-MedTuned-7b model on the transformed RareDis dataset. We use LoRA adapters32 for the fine-tuning. It is worth noting that while instruction-fine-tuning (or instruction-tuning) somewhat contradicts our initial intent due to its reliance on sufficient data, it remains crucial for our investigation into LLMs' performance capabilities and limitations in comparison to smaller BERT-like models in data-rich scenarios.\nEvaluation We utilize Precision (P), Recall (R), and F1-score (F1) as our evaluation metrics for the token-level NER task. For models designed for document-level NER, such as UniversalNER-7b16, we adapt their outputs to token-level by exact string matching. Initially, we assess a range of LLMs to investigate their overall performance. This is followed by a more focused evaluation of those LLMs that demonstrate proficiency in token-level NER. Subsequently, we conduct a detailed analysis of the aforementioned learning strategies, examining their influence on the performance of the LLMs. Additionally, we perform an error analysis to gain insights into the common errors across different models and learning strategies."}, {"title": "Results", "content": "LLM Initial Evaluation In our initial evaluation, we focus on assessing the performance of the aforementioned LLMs in token-level NER using the RareDis-v1 dataset, particularly targeting the two key target entities, i.e., Disease and Rare Disease. The results in Table 2 show that, generally, all the models struggle with this task, as evidenced by their modest performance metrics. Notably, Meditron, with its medical training, demonstrates marginally better results than LLaMA-2. However, even UniversalNER, which is optimized for document-level NER, does not show significantly improved performance in this token-level task. In contrast, Llama2-MedTuned-7b demonstrates promising results, closely trailing behind ChatGPT-4, which highlights the potential of local open-source LLMs in clinical NLP applications, specifically in token-level NER.\nFocused Evaluation of Token-Level NER Capable LLMs Table 3 provides an in-depth performance analysis of ChatGPT-3.5, ChatGPT-4, and Llama2-MedTuned, selected based on their promising results in our initial evaluation. We encompass a range of experimental settings in this subsection, including zero-shot, few-shot, and retrieval-augmented generation (RAG). The discussion on the fine-tuning approach is reserved for the following subsection, as it requires the use of the entire training dataset.\nA notable observation is the relatively high performance in identifying Rare Disease, likely due to its uniqueness and specificity. However, the models have varying degrees of difficulty in identifying subjective experiences reported by the patient, i.e., Symptom, as reflected by their close-to-zero precision and F1-scores. Moreover, we show that few-shot learning can effectively improve the performance across all models, with an increase ranging from 3% to 8% in average F1 scores. We also observe that ChatGPT-3.5 shows a substantial improvement of 20% in the F1-score for Rare Disease. These observations suggest that few-shot learning serves as an effective method to enhance LLMs' performance in token-level NER.\nOn the other hand, while RAG shows some promise, its overall impact on enhancing model performance in this task is limited. The data shows that RAG particularly benefits the identification of Rare Disease and Skin Rare Disease across all three models. This improvement could be attributed to RAG's ability to leverage external knowledge bases, i.e., NORD articles about rare diseases, which might be rich in rare disease terminology. However, for more general and common entities like Disease, Symptom and Sign, RAG's advantages are less impressive. This highlights the need for further refinement of RAG methods in clinical NLP applications.\nBesides zero-shot prompting and in-context learning, we also investigate the impact of instruction-fine-tuning on these LLMs in this task. As shown in Table 2 and Table 3, despite the performance gain via few-shot learning, the overall performance of prompting LLMs is far from satisfactory. In this experiment, we aim to investigate LLMs' performance capabilities and limitations compared to fine-tuned BERT-like models under data-rich conditions, which is the common solution to NER in current practice. The results are shown in Figure 3. Essentially, Figure 3 visualizes the F1 scores of various models, including BioClinicalBERT fine-tune, Llama2-MedTuned under different learning strategies (zero-shot, few-shot, fine-tune), and ChatGPT-4 in zero-shot and few-shot scenarios. Specifically, the performance of Llama2-MedTuned models demonstrates significant effectiveness, not only outperforming ChatGPT-4 in most scenarios but also closely rivaling BioClinicalBERT when fine-tuned. This highlights the promising potential of local open-source LLMs in this specialized NER task.\nError Analysis We present an error analysis to further understand the mistakes made by the LLMs in this task. Essentially, we randomly select 50 sentences from the test dataset and manually categorize the errors for each incorrect prediction. We investigate the two best-performing models, i.e., ChatGPT-4 under few-shot learning and Llama2-MedTuned under fine-tuning. We identify 4 types of errors in the experiment, i.e., Inaccurate Boundary, where the model incorrectly identifies the start or end of an entity; Wrong Type, where the entity is recognized, but its type is misclassified; False Negative, where the model overlooks an entity present in the text; and False Positive, where the model mistakenly identifies a non-entity as an entity.\nThe results in Figure 4 show that Inaccurate Boundary errors are more prevalent for ChatGPT-4, suggesting challenges in precisely detecting entity boundaries. In contrast, Llama2-MedTuned demonstrates a much higher chance of False Negatives, indicating its inability to identify entities that are present. Both models show fewer Wrong Type and False Positive errors, reflecting a relatively stronger performance in correctly classifying entities and avoiding over-detection. Insights from these error patterns highlight the strengths and weaknesses of each model in token-level NER and indicate directions for future improvements, especially in fine-tuning local open-source LLMs."}, {"title": "Discussion and Conclusion", "content": "In this study, we investigate the capabilities of LLMs in token-level NER for rare diseases and their associated phenotypes on the RareDis-v1 dataset. Essentially, we find that in general, most LLMs struggle with this challenging task. We also find that a medically-adapted LLaMA-2-7b model, i.e., Llama2-MedTuned, surpasses the performance of ChatGPT-3.5 and matches the performance of ChatGPT-4 on this task, highlighting the potential of local open-source LLMs in clinical NLP applications. Our further analysis suggests that via in-context learning, i.e., few-shot learning and RAG, the performance can be improved while RAG has a relatively limited influence. We also show that after fine-tuning Llama2-MedTuned on this dataset, its performance gets boosted and significantly outperforms that of ChatGPT-4, approaching the levels of BioClinicalBERT. It is worth noting that we use LoRA adapters to perform the fine-tuning, instead of the entire model. Based on these results, we anticipate that full model fine-tuning could potentially enable the Llama2-MedTuned to match BioClinicalBERT's performance.\nLarge language models (LLMs) have demonstrated superior performance in question-answering-related tasks across almost all domains, including the clinical field. However, their performance in conventional NLP tasks, such as information extraction, has been questioned. Unlike existing studies that either focus on document-level NER or solely rely on the prompt engineering of ChatGPT-series models, our experiment covers a broad scope of LLMs, from proprietary to local open-source and from general-purpose to medically adapted models. We thoroughly evaluate the LLMs on the token-level NER task, showing their struggle and potential directions for refinement. This study could shed light on adapting LLMs to specific NLP applications in clinical settings.\nThere are several factors that contribute to the struggle of LLMs on this task. Firstly, token-level NER is more challenging than document-level NER. Token-level NER is a token classification task and places significant emphasis on the representation of each individual token. This is in contrast to document-level NER which is a sequence classification task where a pooled representation is often sufficient for predictions, making the role of individual token representation less critical. Secondly, the foundational pre-training objective of most LLMs (decoder-only transformers) is causal language modeling, i.e., next token prediction. Unlike encoder-only transformers like BERT, LLMs don't have an encoder structure that is pre-trained to maximize the model's text representation power, making them less effective in classification tasks. Thirdly, token-level NER often involves a variety of special, previously unseen symbols, such as the diverse BIO tags. These unique elements can present additional complexities for LLMs to understand and operate.\nThere are also a few options to adapt these LLMs to the token-level NER task. A typical solution is continuous pre-training, or instruction-fine-tuning, just as Llama2-MedTuned and what we do using the rare disease data. Basically, the idea is to cast the NER problem as text generation, so the LLM can have a better understanding of the task and thus process it correctly in the same manner as it was pre-trained. For instance, Yang et al. integrate Human Phenotype Ontology (HPO) labels into clinical notes and fine-tune GPT-based models (e.g., GPT-J) for phenotype recognition33.\nThey also conduct a comprehensive comparative analysis of encoder-based and decoder-based models for this task. Another solution is to leverage LLMs to generate synthetic data and use the data to train smaller specified models. In addition to the data, one can also alter the model architecture of LLMs. For example, Li et al. propose to fine-tune LLaMA-2 models in a label-supervised way, by removing the causal mask from the decoders to enable bidirectional self-attention which is essential for token classification tasks like NER34. However, in our preliminary experiment, the method does not work as expected and the performance is worse than instruction-fine-tuning on the rare disease dataset. We leave it for future study.\nThis study has certain limitations. Firstly, it does not explore the potential benefits of specific prompt engineering techniques, such as utilizing feedback from error analysis to refine prompts and thereby enhance model performance. Additionally, our approach mainly focuses on prompting LLMs to generate BIO tags. However, there are alternative approaches that are unexplored, such as the generation of text marked by special symbols to represent span information, which could offer a different perspective on model efficacy. Furthermore, in terms of instruction-fine-tuning of LLMs, our methodology is limited to employing LoRA adapters. A more extensive approach such as full model fine-tuning remains untested and could be valuable for future research.\nTo conclude, in this study, we explore the capabilities and limitations of existing LLMs, especially local open-source LLMs, in the task of token-level NER of rare diseases and their associated phenotypes. Through this exploration, we aim to contribute to the advancement of LLM-based token-level NER methodologies in the clinical domain, ultimately aiding in the improvement of patient care by enhancing the processing and understanding of clinical texts."}]}