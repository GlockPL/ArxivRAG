{"title": "Fact-Level Confidence Calibration and Self-Correction", "authors": ["Yige Yuan", "Bingbing Xu", "Hexiang Tan", "Fei Sun", "Teng Xiao", "Wei Li", "Huawei Shen", "Xueqi Cheng"], "abstract": "Confidence calibration in LLMs, i.e., aligning their self-assessed confidence with the actual accuracy of their responses, enabling them to self-evaluate the correctness of their outputs. However, current calibration methods for LLMs typically estimate two scalars to represent overall response confidence and correctness, which is inadequate for long-form generation where the response includes multiple atomic facts and may be partially confident and correct. These methods also overlook the relevance of each fact to the query. To address these challenges, we propose a fact-level calibration framework that operates at a finer granularity, calibrating confidence to relevance-weighted correctness at the fact level. Furthermore, comprehensive analysis under the framework inspired the development of confidence-guided fact-level self-correction (ConFix), which uses high-confidence facts within a response as additional knowledge to improve low-confidence ones. Extensive experiments across four datasets and six models demonstrate that ConFix effectively mitigates hallucinations without requiring external knowledge sources such as retrieval systems.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently shown remarkable capabilities in understanding and generating language that closely resembles human communication. Nonetheless, a major obstacle to their reliability is the prevalence of hallucinations, a phenomenon where the models generate incorrect and unreliable outputs. This issue not only undermines user trust but also restricts the application of LLMs in domains where reliability is crucial, such as in the legal, financial, and educational fields.\nEchoing the ancient adage that \u201cTo know what you know and what you do not know, that is true wisdom\", confidence calibration in LLMs emerges as an effective method to mitigate hallucinations. By confidence calibrating, models can better align their self-assessed confidence with the actual accuracy of responses, empowering them to self-evaluate the correctness of outputs. This mechanism offers an effective way to identify hallucinations by using the model's confidence as a basis to either trust or question the model's response.\nHowever, current confidence calibration methods for LLMs typically estimate two scalars to represent the overall confidence and correctness for the entire response. This approach is unreasonable for long-form generation, where responses may contain multiple atomic facts (illustrated in Fig.1). In such cases, considering the varying of facts in one response, the confidence and correctness should also be diverse, capable of reflecting higher certainty in some facts and greater uncertainty in others. Furthermore, within long-"}, {"title": "2 Related Work and Formulation", "content": "Related Work Several prior works, such as Self-CheckGPT and so on, focus on uncertainty-based hallucination detection. While these methods share a common goal with ours, there are several key distinctions: (1) Their focus is primarily on detection, whereas our work goes beyond that by investigating the mechanisms of fine-grained confidence estimation and identifying three novel phenomena. (2) We introduce a self-correction method that functions without relying on external knowledge, in contrast to their approaches, which are limited to detection. (3) While their methods operate at the sentence level, we take a more granular approach by breaking down text into atomic facts, enabling a deeper analysis of confidence at the fact level. (4) Furthermore, we propose a general framework for confidence estimation, with their methods representing specific instances within this broader approach. There are also works like FactScore that focus on fact-level factuality evaluation by leveraging external knowledge. In contrast, our method centers on the model's internal confidence and calibration, providing a self-correction solution that operates without the external knowledge.\nProblem Formulation Let the dataset be \\( D = {X_1, X_2, ..., X_N} \\), where \\( x_i \\) represents the i-th query, and the model's responses are \\( A = {y_1, y_2,..., y_N} \\). Each query-answer pair \\( (x_i, y_i) \\) is associated with a confidence score \\( confi \\), indicating the model's certainty to the answer, and a correctness score \\( corri \\), measuring the objective accuracy of the answer. In long-form generation, we extend this to the fact level, where each response \\( y_i \\) contains \\( M_i \\) individual facts \\( {f_j}^{M_i}_{j=1} \\), with each fact \\( f_i \\) assigned a relevance score \\( rel_j \\), a correctness score \\( corr_j \\), and a confidence score \\( conf_j \\). The aim is to align confidence with relevance-weighted correctness across all facts in each response."}, {"title": "3 Fact-Level Confidence Calibration", "content": "In this section, we first outline the motivation and provide a overview of the Fact-Level Confidence"}, {"title": "3.1 Motivation", "content": "Compared to the confidence calibration for short-form generation or traditional classification problems, a significant challenge in calibrating long-form text generation is that a response may contain multiple facts, making it unreasonable to assign a single correctness measure and a single confidence score to the entire response. The reason is that the answer might be partially correct, and the model might also be partially confident in only a subset of the facts of a response. Meanwhile, some facts in response are irrelevant to the query, so the calibration based solely on correctness is insufficient.\nBased on the above motivation, our proposed calibration framework aims to calibrate the confidence to relevance-weighted correctness on the fact level, which leads to the following two advantages: (1) Finer Granularity: we assign a confidence vector rather than a scalar to a response, where each item represents confidence for a single fact. This fine-grained framework allows for more nuanced and precise calibration. (2) Relevance Awareness: we assess both the correctness and the relevance of each fact, which ensures that the confidence score attributed to each fact can reflect its significance and appropriateness within the given context."}, {"title": "3.2 Architecture", "content": "To calibrate confidence with relevance-weighted correctness at fact level, our framework comprises four components, as shown in fig. 2: (1) fact extraction, (2) correctness and relevance evaluation, (3) confidence estimation, (4) calibration evaluation.\nFact Extraction Given query-answer pair \\( (x_i, y_i) \\), the first step is to extract the individual facts from response. This extraction can be efficiently carried out using a powerful external language model, such as GPT-4, producing a set of facts \\( {f_j}^{M_i}_{j=1} \\) for \\( y_i \\).\nCorrectness and Relevance Evaluation Once facts are extracted, we evaluates both the correctness and relevance of each fact in relation to the query. Correctness is assessed by verifying the factual accuracy of each fact using GPT-4 in combination with retrieval methods, such as search engines and ground truth answers from relevant datasets, yielding correctness scores \\( {corr_j}^{M_i}_{j=1} \\). Similarly, relevance scores \\( {rel_j}^{M_i}_{j=1} \\) can also be measured using GPT-4 to capture each fact's pertinence to the query within the context of the overall response.\nConfidence Estimation Confidence estimation quantifies the certainty of the LLM for each fact in its response. To derive a confidence vector \\( {conf_j}^{M_i}_{j=1} \\) for each response, we employ three widely used confidence estimation methods: (1) Verbalization-based method: model is prompted to verbally express confidence score for each fact in the response. (2) Logit-based method: confidence is determined by the probability assigned to the \"True\" token when evaluating the response as True or False. (3) Consistency-based method: confidence is determined by the consistency across multiple sampled outputs. Formally,"}, {"title": "4 Key Observation", "content": "This section discusses three important phenomena observed under our fact-level calibration. These findings not only demonstrate the superiority of our framework over traditional response-level calibration, but also inspire the development of a confidence-guided fact-level self-correction method based on these insights."}, {"title": "4.1 Experiment Setup", "content": "This section outlines the experimental setups, including the datasets, models, and evaluations.\nDatasets We utilize two long-form response datasets: (1) LongFact : a dataset contains prompts that evaluate model's factuality in open-end responses. (2) ASQA : a dataset centers on ambiguous factoid questions with multiple reference answers.\nModels We use five popular models from different families and scales, including: (1) LLaMA : include LLaMA-7b-chat and LLaMA-13b-chat. (2) Vicuna : include Vicuna-7b and Vicuna-13b. (3) GPT : GPT-3.5-turbo."}, {"title": "4.2 Results on Fact-Level Calibration", "content": "This section highlights three key phenomena observed during our fact-level calibration. These findings not only underscore the advantages of our framework compared to traditional response-level's, but also inform the design of a confidence-guided fact-level self-correction approach.\nObservation 1: Fact-level calibration imposes a stricter standard. We compare fact-level and response-level calibration following the protocol outlined in , where reliability histograms and statistical metrics for is summarized. For both fact and response level, we estimate confidence through all three confidence estimation method, where the correctness/relevance evaluation"}, {"title": "5 Application: Confidence-Guided Fact-Level Self-Correction", "content": "In this section, we introduce the motivation and architecture of Confidence-Guided Fact-Level Self-Correction, dubbed ConFix. ConFix utilizes facts with high confidence as references to revise facts with low confidence, thereby enhancing the generation quality and mitigating hallucinations. Importantly, ConFix operates in real-time during the generation process, eliminating the need for additional fine-tuning or retraining, which reduces computational costs and enhances its adaptability. Furthermore, it functions without relying on external knowledge sources, significantly broadening its applicability across diverse scenarios."}, {"title": "5.1 Motivation", "content": "The development of Confidence-Guided LLM Self-Correction is driven by the three observations discussed earlier. The rationale behind these observations supporting self-correction is as follows: (1) Observations 1 and 2 demonstrate that, even under strict conditions, the fact-level framework reduces overconfidence and enhances the model's calibration, aligning confidence more closely with factual accuracy. This improved calibration is crucial for enabling effective confidence-guided self-correction. (2) Observation 3 reveals that high-confidence and low-confidence facts often coexist within the same response. Even when the overall confidence is relatively consistent, outliers tend to be low-confidence facts. This enables high-confidence facts to serve as some kind of references, providing the necessary information to refine and correct the low-confidence facts."}, {"title": "5.2 Architecture", "content": "The overall architecture of ConFix is illustrated in fig. 6, including three steps: (1) fact extraction & confidence estimation, (2 )factor extraction & fact correction, and (3) fact confidence re-estimation.\nS1: Fact Extraction & Confidence Estimation Given a response \\( y_i \\), ConFix first conducts fact extraction and confidence estimation for each extracted fact, following the same process as described in section 3.2. After obtaining the facts \\( {f_j}^{M_i}_{j=1} \\) for \\( y_i \\) and their corresponding confidence scores \\( {conf_j}^{M_i}_{j=1} \\), we then split the facts into two groups: high-confidence and low-confidence, based on a confidence threshold \\( \\tau \\). The high-confidence group shown in eq. (4) is used as a form of internal knowledge base, whose knowledge is leveraged to reinforce and augment facts within the low-confidence group, which is shown in eq. (5),\n\\begin{equation}\nf_h = \\{ f_i \\mid conf \\geq \\tau \\} \\tag{4}\n\\end{equation}\n\\begin{equation}\nf_l = \\{ f_i \\mid conf < \\tau \\}, \\tag{5}\n\\end{equation}\nwhere the threshold is defined as the mean confidence score across facts \\( \\tau = \\frac{1}{M_i} \\sum_{l=1}^{M_i} conf_l \\).\nS2: Factor Extraction & Fact Correction To ensure that only the erroneous parts of the low-confidence facts are modified without changing the overall meaning, we restrict the modifiable parts. Specifically, we first parse the key factors through factor extraction. Let \\( {fa_k}^K_{k=1} \\) represent the K factors extracted from the target fact \\( f_i \\in f_l \\),\n\\begin{equation}\n\\{fa_k\\}^K_{k=1} = F(LLM(\u00b7), p_f(f)), \\tag{6}\n\\end{equation}\nwhere \\( p_f \\) is the prompt, which includes: (1) A clear task description. (2) Several instances. (3) The task containing the input sentence. The model is expected to output its extracted factors.\nAfter extracting factors, we then perform fact correction, targeting only the extracted factors for modification. This process can be represented as,\n\\begin{equation}\nf' = R(LLM(\u00b7), p_r(f_i, \\{fa_k\\}^K_{k=1}, f_h )\\tag{7}\n\\end{equation}"}, {"title": "5.3 Experiment on Self-Correction", "content": "where \\( p_r \\) is the prompt, which includes: (1) A clear task description. (2) Several instances. (3) The task containing the input target fact, the extracted factors and the high-confidence reference facts. The model is expected to output the modified target fact, noting that the model allows for returning \"NoError\" to make no modifications to the input.\nS3: Fact Confidence Re-Estimation Finally, the modified facts undergo the confidence estimation process again to obtain new confidence scores:\n\\begin{equation}\nconf' = C (LLM(\u00b7), p_c(f', x_i, y_i)), \\tag{8}\n\\end{equation}\nwhere \\( conf' \\) represents the confidence score of the modified fact \\( f' \\). Finally, if \\( conf' > conf \\), the modification is deemed successful and is accepted. Otherwise, ConFix will repeat the process of factor extraction, fact correction, and confidence re-estimation. This iterative process continues until either a satisfactory confidence score is achieved or a predetermined maximum number of iterations \\( N \\) is reached, where the model return \"NoError\" and make no modifications to the input.\nExperiment Setup We adopt a two-fold evaluation. First, we assess error detection performance using standard metrics such as Accuracy, Precision, and Recall . Subsequently, for evaluating self-correction, we leverage GPT-4 in a zero-shot pairwise evaluation framework (see prompts in Appendix C), measuring improvement ratio, consistency ratio, and regression ratio to capture the performance of correction."}, {"title": "6 Conclusion", "content": "This paper presents a novel fact-level calibration and self-correction framework designed to address hallucination issues in long-form responses generated by large language models. Traditional response-level methods fall short when handling complex outputs containing multiple facts. Our approach evaluates each fact's correctness and relevance individually, both externally and internally, enabling fine-grained confidence assessments. This framework imposes a stricter standard than response-level methods, reduces overconfidence, and highlights significant confidence variance across facts within a response. By leveraging high-confidence facts for in-context learning, our self-correction method ConFix can effectively mitigate hallucinations without the need for external knowledge, as demonstrated across various models."}, {"title": "7 Limitations and Broader Impacts", "content": "In this work, we propose a fact-level calibration framework and, based on this framework, introduce a confidence-guided fact-level self-correction method. However, for this self-correction method to be effective, the model itself must possess a certain level of calibration ability. In our paper, we discuss how our calibration framework can alleviate over-confidence. In future work, we will further explore ways to enhance calibration ability within the calibration framework, paving the way for more effective confidence-guided self-correction."}]}