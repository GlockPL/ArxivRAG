{"title": "LEARNING-GUIDED ROLLING HORIZON OPTIMIZATION FOR LONG-HORIZON FLEXIBLE JOB-SHOP SCHEDULING", "authors": ["Sirui Li", "Wenbin Ouyang", "Yining Ma", "Cathy Wu"], "abstract": "Long-horizon combinatorial optimization problems (COPs), such as the Flexible Job-Shop Scheduling Problem (FJSP), often involve complex, interdependent decisions over extended time frames, posing significant challenges for existing solvers. While Rolling Horizon Optimization (RHO) addresses this by decomposing problems into overlapping shorter-horizon subproblems, such overlap often involves redundant computations. In this paper, we present L-RHO, the first learning-guided RHO framework for COPs. L-RHO employs a neural network to intelligently fix variables that in hindsight did not need to be re-optimized, resulting in smaller and thus easier-to-solve subproblems. For FJSP, this means identifying operations with unchanged machine assignments between consecutive subproblems. Applied to FJSP, L-RHO accelerates RHO by up to 54% while significantly improving solution quality, outperforming other heuristic and learning-based baselines. We also provide in-depth discussions and verify the desirable adaptability and generalization of L-RHO across numerous FJSP variates, distributions, online scenarios and benchmark instances. Moreover, we provide a theoretical analysis to elucidate the conditions under which learning is beneficial.", "sections": [{"title": "1 INTRODUCTION", "content": "Enhancing the efficiency and scalability of solving Combinatorial Optimization Problems (COPs) has been a central focus of both the Operations Research (OR) and the emerging Neural Combinatorial Optimization (NCO) communities. Numerous methods have been proposed to decompose large-scale problems into smaller, more tractable subproblems, with the majority focusing on spatial or structural decomposition of decision variables. Despite their success, these methods often face limitations in addressing the unique temporal challenges of long-horizon COPs - frequently encountered by industrial practitioners - which involve optimizing complex decisions over extended time horizons. Such unique challenges, coupled with the inherent NP-hardness and large-scale nature of the problems, call for advanced temporal decomposition strategies.\nBuilding on successes in control for complex dynamical systems, Rolling Horizon Optimization (RHO) has emerged as a natural temporal decomposition technique for long-horizon COPs. RHO breaks the problem into overlapping subproblems with shorter planning horizons rolling forward over time, allowing for much better scalability. The key to RHO is its temporal overlap, which enhances decision-making at the boundaries of consecutive subproblems, mitigating myopic decisions and facilitating modelling interdependencies across the temporal dimension. However, such overlaps often lead to redundant computations that reduce the efficiency especially when only a small subset of variables needs re-optimization. This presents an opportunity to accelerate RHO by identifying such redundancies, an approach that, to our knowledge, has not yet been explored in the combinatorial optimization context.\nTo this end, this work introduces a novel learning-based RHO framework, termed L-RHO, designed to accelerate RHO for long-horizon COPs by identifying overlapping decision variables that do"}, {"title": "2 RELATED WORKS", "content": "Decomposition for COPs. Despite advances in traditional solvers, learning-guided solvers and neural solvers for COPs, scalability and adaptability to real-world complexities remain challenging. Various decomposition strategies have been explored by OR community, such as variable partitioning, adaptive randomized decomposition (ARD), and sub-problem constraint relaxation. More recently, machine learning has been exploited to guide the decomposition by selecting subproblems or to auto-regressively solve decomposed subproblems, leading to notable improvements. However, they primarily emphasize spatial or problem-structural decomposition (e.g., for routing problems), which is not suitable for long-horizon time-structured COPs involving complex, interdependent decision variables and constraints spanning extended time horizons. This highlights the need for effective temporal decomposition. We note such temporal decomposition can be orthogonal to other existing ones, and future work could combine them to improve scalability and flexibility.\nRHO for Long-horizon COPs. RHO is a temporal decomposition method originating from Model Predictive Control (MPC). It improves the scalability by dividing the time-structured problem into overlapping subproblems. While such overlap improves boundary decision-making, it can introduce redundant computations. Thus, many control and robotics studies leverage previous decisions to reduce the computations of the current subproblem. This is done, e.g., through hand-crafted methods, such as recording repeat computations or tightening primal and dual bounds, and learning-based models that predict active constraints or solutions for discrete variables."}, {"title": "3 PRELIMINARIES", "content": "FJSP Definition. A FJSP instance consists of a set T of jobs, a set M of machines, and a set O of operations. Each job $j\\in T$ consists of a set of $n_j$ operations $\\{O_{j,k}\\}_{k=1}^{n_j}$ required to be processed in a precedence order $O_{j,1} \\rightarrow O_{j,2} \\rightarrow ... \\rightarrow O_{j,n_j}$. Each operation $O_{j,k}$ can be processed by any of the compatible machines $M_{j,k} \\subseteq M$; the process duration of operation $O_{j,k}$ by machine $m \\in M_{j,k}$ is denoted by $p_{j,k}^m$. A solution to the FJSP, denoted as $II = (m, \\pi)$, consists of (i) an assignment $m: O \\rightarrow M$ that represents the machine assignment of each operation, with $m(O_{j,k}) \\in M_{j,k}$ for all $j, k$, and (ii) a schedule $\\pi: \\{O_{j,k} | \\forall j, k\\} \\rightarrow \\mathbb{N}$ that represents the process start time of each operation $O_{j,k}$ as $\\pi(O_{j,k})$; with the process duration as $p_{j,k}^{m(O_{j,k})}$, the process end time of the corresponding operation is $\\pi_t(O_{j,k}) := \\pi(O_{j,k}) + p_{j,k}^{m(O_{j,k})}$. Many FJSP objectives exist in the literature, and we consider testing our method on a variety of objectives including makespan, total start delays, and end delays. Formally, the makespan objective can be expressed as $\\max_{O_{j,k}} \\pi_t(O_{j,k})$. For the delay-based objectives, let each operation $O_{j,k}$ be further associated with (i) a release time $s_{j,k}$ with a constraint $\\pi(O_{j,k}) \\geq s_{j,k}$ such that the operation can only be processed after the release time, and (ii) a target end time $t_{j,k}$; both respect the operations' precedence orders within the same job, that is, $\\forall$ job $j$ and operations $k_1 < k_2$, we assume $s_{j,k_1} \\leq s_{j,k_2}$ and $t_{j,k_1} \\leq t_{j,k_2}$. The total start delay objective is expressed as $\\Sigma_{O_{j,k}} \\pi(O_{j,k}) - s_{j,k}$, and the total end delay objective as $\\Sigma_{O_{j,k}} \\max(\\pi_t(O_{j,k}) - t_{j,k}, 0)$. Appendix A.1 provides a detailed list of notations.\nRHO for Long-Horizon FJSP. The temporal structure of FJSP enables the use of RHO to decompose a long-horizon FJSP into a sequence of shorter-horizon FJSP subproblems.\nOur RHO utilizes a planning window size H but executes only a step size S with $S \\leq H$. Each sub-problem is limited to solving for T seconds. Given the long-horizon FJSP instance, we first sort the operations O into $\\{O^{(1)}, ..., O^{(|O|)}\\}$ based on the precedence order within each job or the associated release time. These operations are then divided into overlapping subproblems. Specifically, the $r$th RHO iteration considers a FJSP subproblem $P_r$ given by a subset of operations $O_{plan,r}$, consisting of the next H non-executed operations according to the sorted RHO sequence order; we introduce additional constraints in $P_r$ to handle boundary conditions \u2013 for example, each operation in $O_{plan,r}$ should start after all previously executed operations from the same job. After solving $P_r$ under the time limit T, we obtain the subproblem solution $II_r = (m_r, \\pi_r)$. We then execute the first S operations with the earliest process start time $\\pi_r(O)$ following the solution $II_r$, while deferring the remaining H \u2013 S operations for replanning in future iterations. The procedure is then repeated until we solve the full FJSP P, resulting in |O|/S subproblems. A detailed algorithm can be found in Appendix A.2. RHO is also well-suited for online settings with limited visibility, solving early operations while deferring the rest until future batches in the next iteration (see Sec. 5.2)."}, {"title": "4 LEARNING-GUIDED ROLLING HORIZON OPTIMIZATION (L-RHO)", "content": "We now introduce our learning framework, L-RHO, that accelerates RHO for long-horizon FJSP. We refer to the original RHO, RHO for training data collection, and RHO for inference as $RHO_O$, $RHO_{data}$, and $RHO_{test}$. In the standard $RHO_0$, each consecutive iterations $r-1$ and $r$ share overlapping operations $O_{overlap,r} = O_{plan,r} \\cap O_{plan,r-1}$. While new operations $O_{new,r} = O_{plan,r} \\setminus O_{plan,r-1}$ in subproblem $P_r$ may change the solution of the overlap parts $O_{overlap,r}$, we find two key observations that, for various FJSP distributions evaluated in Sec. 5:\n(1) A significant subset of the overlapping operations retain the same machine assignment between the consecutive optimizations. Formally, the shared operations are given by $O_{fix,r}^* = \\{O \\in O_{overlap,r} | m_r(O) = m_{r-1}(O)\\}$, where $II_r = (m_r, \\pi_r)$ and $II_{r-1} = (m_{r-1}, \\pi_{r-1})$ are the solutions of the unrestricted subproblem $P_r$ and $P_{r-1}$ in $RHO_O$.\n(2) Let $P_{fix,r}^*$ be the restricted subproblem where we leverage the solution of the $r \\text{ \u2013 } 1$th RHO iteration and fix the machine assignments of the shared operations $O_{fix,r}^*$. The solve time of $P_{fix,r}^*$ is significantly reduced from that of the unrestricted subproblem $P_r$.\nAssignment-Based Subproblem Restriction. Based on these observations, we propose restricting the subproblem by fixing machine assignments for a set of operations $O_{fix,r} \\subseteq O_{overlap,r}$, selected through oracle, learning, or heuristic methods. This results in the restricted subproblem $P_r$, with a formal formulation provided in Appendix A.5.2. Notably, this approach can be easily extended to other subproblem restriction methods, which we leave for future work.\nOur L-RHO Pipeline. We design the L-RHO pipeline (which is shown in Fig. 1) as follows: First, we collect training data via a Look-Ahead Oracle by solving the unrestricted subproblem $P_r$ and identifying $O_{fix,r}^*$ as the set of overlapping operations with the same assignments in $m_r$ and $m_{r-1}$. A neural network $f_\u03b8$ is then trained with such collected labels. During inference, $f_\u03b8$ predicts a subset $O_{fix,r}$, from which we form the assignment-based restricted subproblem $P_r$. This largly speeds up the inference time optimization by replacing the expensive $P_r$ with the easier-to-solve $P_r$."}, {"title": "5 EXPERIMENT", "content": "In Sec. 5.1, we evaluate L-RHO in a standard offline setting, comparing it to various baselines for long-horizon FJSP. Next, in Sec. 5.2, we dive into detailed analysis of L-RHO under different FJSP variants including online settings. We provide additional experimental results in Appendix A.6, including a deep dive on the architecture design and evaluation on a real-world dataset. We aim to answer: (1) To what extent does L-RHO improve upon competitive offline baseline methods and RHO variants? (2) Can L-RHO adapt to different FJSP distributions and objectives? Moreover, a unique benefit of RHO is its adaptability to online settings with limited visibility. As a step toward realistic online settings, we then study (3) Is L-RHO robust under noisy observation of process durations? (4) is L-RHO reliable under machine breakdowns?\nFJSP Data Distribution and Solver. We generate synthetic large-scale FJSP instances following common procedure in, characterizes by (|M|, |T|,n), where each job jhas $|\\{O_{j,k}\\}_{k=1}^{n_j}| = n$ operations. The total number of operations |O| ranges from 600 to 2000. Notably, the problem horizon in this work (e.g., 600-2000) is considerably larger than those in previous studies, which typically consider fewer than 500 operations. Following the conventions , we employ the competitive constraint programming (CP) solver OR-Tools CP-SAT to solve FJSP, with its detailed CP formulation provided in Appendix A.5.2. We consider makespan objective in Sec. 5.1, and delay-based objectives in Sec. 5.2 (start and end delay) for comprehensive evaluation. More details are in Appendix A.5.1.\nProposed Method (L-RHO) Setup. We provide descriptions of the training set up in Appendix A.4, and the details on the choices of RHO parameters in Appendix A.5.4 and A.5.6.\nBaselines. We include a range of traditional and learning-based baselines, with and without decomposition for comprehensive evaluation:\n(1) Traditional solver w/o decomposition - We include the widely used benchmark solver CP-SAT and Genetic Algorithm (GA).\n(2) Learning-based solver w/o decomposition - We compare with the state-of-the-art DRL constructive method for FJSP , which we denote as DRL-20K (using 20,000 training instances as in the original paper). We further compare with two recent learinng methods: DRL-Echeverria and DRL-Ho .\n(3) Traditional solver w/ decomposition - We include the time decomposition method ARD-LNS (Time-based) and machine decomposition method ARD-LNS (Machine-based) in, which decompose large neighborhood search (LNS) by randomly fixing temporal intervals or machine subsets as subproblems, respectively.\n(4) Learning-guided solver w/ decomposition - While learning-guided decomposition exist, e.g, for routing problems , they are not adaptable to long-horizon COPs like FJSP. We thus opt to include an Oralcle-LNS (Time-based) baseline to estimate the upper bound of enhancing ARD-LNS. It samples K subproblems and selects the best one by looking ahead.\n(5) RHO decomposition - We include Default RHO, where each iteration solves an unrestricted FJSP subproblem Pr without fixing any variables, and Warm-Start RHO, where previous machine assignments of overlapping operations $\\{m_{r-1}(O) \\forall O \\in O_{overlap,r}\\}$ are provided as hints (not fixed) to CP-SAT as a warm start, akin to in control systems.\nEvaluation Metric. We compare the objective and solve time in Table 1. Objective improvement (OI%) and time improvement (TI%) over Default RHO are reported in Table 2. For a solver with an objective obj and a solve time t, its OI and TI are calculated as $(obj_0 \u2013 obj)/obj_0 \\times 100\\%$ and $(t_0 \u2013 t)/t_0 \\times 100\\%$, respectively, where negative values indicate degradation."}, {"title": "5.1 CANONICAL OFFLINE FJSP UNDER MAKESPAN OBJECTIVE", "content": "We consider the canonical offline FJSP with makespan as the objective, with horizons up to 2000 operations (significantly longer than literature), following a similar synthetic data distribution as in. The number of jobs and machines are set to 20 and 10, respectively. For both L-RHO and DRL, we train separate models for 30, 40, and 60 operations per job, testing each in its respective setting and transferring the 60 operations-per-job model to a large-scale test with 100 operations per job. Results are gathered in Table 1. We highlight the following findings:\nComparison with baseline solvers w/o decomposition. L-RHO outperforms both traditional solvers (CP-SAT, GA) and the learning-based DRL solver in the standard (offline) FJSP setting. Specifically, (1) L-RHO achieves the best objective, outperforming DRL and all heuristic baselines in both in-domain and zero-shot generalization to larger scales; (2) We find the performance of DRL-Echeverria and DRL-Ho poor; (3) Comparing with DRL-20K, we see that DRL Greedy is the fastest method but has a worse performance than Default RHO. (4) DRL"}, {"title": "5.2 DETAILED COMPARISON WITH RHO BASELINES UNDER DIFFERENT FJSP VARIANTS", "content": "We now benchmark L-RHO against diverse RHO baselines across FJSP variants. We evaluate ob-jectives such as makespan, start delay, and end delay, while also testing its adaptability under higher system congestion, observation noise, and machine breakdowns to simulate real-world dynamics.\nWe include two more RHO baselines, following the $RHO_{test}$ procedure but obtaining the fixed op-erations $O_{fix,r}$ with the following heuristics: (a) Random $\\sigma_R \\in [0, 1]$: $O_{fix,r}$ is obtained by select-ing each operation in $O_{overlap,r}$ with a probability $\u03c3_R$ uniformly at random, (b) First $\u03c3_F \\in [0,1]$: $O_{fix,r}$ is obtained as the first $\u03c3_F$ fraction of operations in $O_{overlap,r}$ with the earliest RHO sequence order based on the precedence order (for makespan) or release time (for delay-based objectives).\nNew Objective: Start Delay. In Table 2, under the new start delay objective, our L-RHO consis-tently outperforms all RHO variants in both solve time and objective across time horizons. Notably,"}, {"title": "6 THEORETICAL PROBABILISTIC ANALYSIS", "content": "In this section, we present a theoretical probabilistic analysis to identify when RHO can benefit from machine learning. Intuitively speaking, the more irregular the operations to be fixed ($O_{fix}^*$), the more advantageous L-RHO can be, because of the greater potential gain from prediction. Also, L-RHO must balance both False Positive (FP) and False Negative (FN) errors: fixing something that should not have been (FP) harms the objective but helps the solve time, while failing to fix something that should have been (FN) harms the solve time and also indirectly harms the objective (under a fixed time limit). Thus, an ideal L-RHO method will balance the two errors (see Fig. 4).\nNotation. We analyze a generic subproblem $P_r := P$ at each iteration $r$, and drop the r subscript for ease of notation. We consider a fixed set of RHO parameters and denote $W = H \u2013 S = |O_{overlap}|$. Given $O_{fix}^*$ and $O_{fix,r}$ from the Oracle and a method in $\\{Random\\ OR,\\ First\\ \u03c3_F,\\ L\\text{-}RHO\\}$, we denote $E[n_{fix}] = E[|O_{fix}|], O_{fn} = O_{fix}^* \\setminus O_{fix,r}$, and $O_{fp} = O_{fix,r} \\setminus O_{fix}^*$. We define the FN and FP errors $E[n_{fn}] = E[|O_{fn}|]$ and $E[n_{fp}] = E[|O_{fp}|]$. The (expected) FP and FN rates of each method are $E[n_{fp}]/E[n_{fix}], E[n_{fn}]/(W - E[n_{fix}])$, with L-RHO's FPR and FNR denoted as $(\\alpha, \\beta)$.\nWe leverage an intuitive linear decay assumption, validated in Appendix A.7.2: in a given FJSP subproblem, the later an operation appears in the RHO sequence, the less likely its assignment should be fixed. This assumption allows us to formally relate the FP and FN errors of different methods (L-RHO versus Random versus First, see Prop. 1 and Fig. 4 (middle)).\nAssumption 1 (Linear Decay). Let the probability that each operation $O^{(i)} \\in O_{overlap}$ is included in $O_{fix}^*$ be denoted as $p_{fix}(i) \\in [0,1]$, where the index $i \\in \\{1, ..., W\\}$ follows the RHO sequence order. We consider linearly decreasing $p_{fix}(i) = b - m \\cdot \\frac{i}{W}$, where constants $0 \\leq b, m \\leq 1$ reflect average behavior across RHO iterations. We have $E[n_{fix}^*] = \\sum_i p_{fix}(i) = (b - \\frac{m}{2})W - \\frac{m}{2}$."}, {"title": "7 CONCLUSION", "content": "We contribute the first learning-guided rolling-horizon method for COPs, which we call L-RHO. By learning which overlapping solutions in consecutive RHO iterations do not need to be re-optimized, our method substantially reduces RHO subproblem sizes and thus accelerates their solve time. L-RHO scales and accelerates RHO by up to 54%, improves solution quality, and outperforms a wide range of baselines, both with and without decomposition. We also analyze L-RHO's performance across various FJSP settings, distributions, and online scenarios, highlighting its flexibility and adaptability. We further provide a probability analysis to identify conditions where learning-guided RHO is most beneficial. One limitation is that L-RHO alone may not achieve state-of-the-art performance when extended to other long-horizon COPs. Future work could extend L-RHO to - 1) other long-horizon COPs, e.g., multi-item inventory management and vehicle routing with time windows, 2) training on diverse distributions for better generalization, 3) integration with other decomposition or subproblem restriction methods, and 4) addressing more complex real-world online scenarios. Our theoretical analysis may also guide the development of more effective RHO warm-start techniques with applications to control and robotics, e.g., accelerating Hybrid MPCs. We believe L-RHO offers a valuable tool for long-horizon COPs and will inspire future work in various domains. Our code is publicly available at https://github.com/mit-wu-lab/l-rho."}]}