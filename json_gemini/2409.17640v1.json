{"title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training on an Assistant Task for a Target Task", "authors": ["Xindi Tong", "Yujin Zhu", "Shijian Fan", "Liang Xu"], "abstract": "Long text summarization, gradually being essential for efficiently processing large volumes of information, stays challenging for Large Language Models (LLMs) such as GPT and LLaMA families because of the insufficient open-sourced training datasets and the high requirement of contextual details dealing. To address the issue, we design a novel zero-shot transfer learning framework, abbreviated as T3, to iteratively training a baseline LLM on an assistant task for the target task, where the former should own richer data resources and share structural or semantic similarity with the latter. In practice, T3 is approached to deal with the long text summarization task by utilizing question answering as the assistant task, and further validated its effectiveness on the BBC summary, NarraSum, FairytaleQA, and NLQUAD datasets, with up to nearly 14% improvement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore compared to three baseline LLMs, demonstrating its potential for more assistant-target task combinations.", "sections": [{"title": "1 Introduction", "content": "In recent years, long text summarization has gradually played an important role in various real-world scenarios such as current affairs commentary, book summaries, news aggregation, and literature research, benefiting from the fast developing of Large Language Models (LLMs) (Nallapati et al., 2016; Vaswani et al., 2023; Dubey et al., 2024; Islam and Moushi, 2024). Different from classical summarization tasks, which typically summarize texts of around 500 words or less into 100 or fewer words, long text summarization involves dealing with much longer texts. Conceivably, as the original text length increases, the amount of contextual details and relevance required in the summary also grows, bringing new challenges to LLMs.\nConsidering solutions, we find that long text summarization could be categorized to a more general task, which shares two crucial issues: (1) This task lacks sufficient diverse open-source datasets to post-train or fine-tune LLMs, which might be due to the task itself being relatively new and special, or because the labeling details of the task make the construction of its datasets so complex and costly, whatever through manual annotation or algorithmic generation. (2) The output of this task demands a high level of contextual details and consistence. This makes it challenging for existing prompt engineering techniques (Schick and Sch\u00fctze, 2021; Chakraborty and Pakray, 2023), most of which rely on task-oriented predefined rules to tune LLMs, thus probably losing insight into richer contextual information, as well as failing to capture similar semantic laws behind different tasks.\nIn this paper, we propose a novel zero-shot Transfer learning framework iteratively training on an assistant Task for a target Task (abbreviated as T3). T3 fine-tunes the chosen baseline LLM through the assistant task that must meet two basic criteria: (1) It should have relatively abundant open-source datasets to address the first \"insufficient dataset\" issue. (2) It should exhibit similarity at the structural or semantic level to the target task to address the second \"more details\" issue mentioned above. Moreover, inspired by the ability of emergence from the new generation of LLMs to understand more complex semantics, an iterative optimization strategy is adopted to enable the used LLM to learn deeper from the assistant task and summarize useful rules (called as \"experiences\" in this paper) automatically by itself. Hereafter, the fine-tuned LLM is applied to the target task in a zero-shot manner. Further in practice, with long text summarization as the target task, we find that the Question Answering task (QA) is suitable as the assistant task because it has various open-source datasets, its used text is relatively long, and its question-answer pairs naturally contain richer entities and relationships. In addition, the Question Generation task (QG) is used during the training process of T3 to help LLMs better understand common and different contextual features between the assistant and the target tasks. Our contributions are concluded as follows:\n\u2022 We propose a novel zero-shot transfer learning framework named T3. Different from most previous works, the baseline model in T3 only learns from the assistant task and then directly runs for the target task in zero-shot way. Further, T3 facilitates the transfer of knowledge from QA and QG tasks to the long text summarization task.\n\u2022 We adopt an iteration strategy in training process to make the used baseline model conclude useful rules automatically, with specially designed prompts and selected metrics, in order to ensure the high quality of the summaries generated during the test process in T3.\n\u2022 We demonstrate significant effectiveness of T3 on four datasets across different tasks by comparing with seven representative baseline LLMs."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Summarization", "content": "Early automatic text summarization has begun with Luhn's keyword frequency method and Edmundson's positional weighting in the 1950s. Machine learning approaches have emerged in the 1990s, such as Kupiec's Bayesian classifier. Later, deep learning methods like TextRank (Mihalcea and Tarau, 2004) advance the field. Recent research focuses on improving faithfulness in abstractive summarization, with PEGASUS (Zhang et al., 2020) and methods like ECC (Zhang et al., 2022b). Other works include contrastive learning (Chen et al., 2021), graph attention (Zhu et al., 2021), and few-shot approaches (Schick and Sch\u00fctze, 2021). LLMs, like GPT, also contribute to faithful summarization (Zhang et al., 2023).\nTexts with more than 1,000 words could be considered as long text (Tuteja and Gonz\u00e1lez Jucl\u00e0, 2023). Transformer-based models are efficient to handling of lengthy texts. Longformer (Beltagy et al., 2020) uses a local attention mechanism to reduce computational complexity, enabling efficient processing of longer texts without losing global context. HyperGraph Transformers introduced by (Zhang et al., 2022a) tackle the complexity and context retention issues in Transformer-based models. Additionally, datasets like SQUALITY are crucial for advancing long-document summarization (Wang et al., 2022). GPT could also applied for long text summarization which shows better performance than LLaMA (Fan et al., 2024)."}, {"title": "2.2 Question Answering", "content": "Datasets like NQ (Kwiatkowski et al., 2019) and SQUAD (Rajpurkar et al., 2016, 2018) have been foundational for the task. NQ focuses on real-world queries with Wikipedia annotations, while SQUAD emphasizes extractive QA. MS MARCO (Bajaj et al., 2016) and DuReader (He et al., 2018) have broaden the QA field with free-form answers and non-English tasks. Recent works like (Eo et al., 2023) enhance QA diversity via multimodal inputs and iterative generation. Transformer-based models like BERT (Devlin et al., 2019), fine-tuned on NQ and SQUAD, improve QA precision and recall. Further, (Eo et al., 2023) distills complex information for summarization, aiding coherent QA generation. Here, QA is used as an assistant task for the proposed T3 on the summarization task."}, {"title": "2.3 Question Generation", "content": "Earlier work by (Du et al., 2017) has highlighten the effectiveness of sequence-to-sequence models for QG in reading comprehension, while (Song et al., 2018) improve QG accuracy using a multi-perspective context-matching model. (Chali and Baghaee, 2018) explore summarization for opinion-based QG to reduce repetition. (Lyu et al., 2021) introduce an unsupervised approach, utilizing summaries to generate diverse questions with less lexical overlap. Frameworks like MQAG by (Manakul et al., 2023) ensure coherence in QG through summarization. Recently, (Choudhary and Du, 2024) has proposed the QAEVENT paradigm, using summarization to represent document-level events as question-answer pairs. In this paper, QG is considered as a generation strategy in the training process of T3."}, {"title": "2.4 Zero-shot Transfer Learning", "content": "Recent advancements in zero-shot transfer learning have significantly improved the capabilities of LLMs across diverse tasks to solve problems such as data misalignment, label mismatches, and the limitations of pre-trained models without further fine-tuning (Kojima et al., 2023; Zhao et al., 2023). Furthermore, multilingual LLMs have demonstrated impressive zero-shot learning abilities in multimodal settings and cross-lingual instruction tuning, showing that LLMs trained on one language can generalize to others (Hu et al., 2023). ARL2 introduces an adaptive retriever learning technique that aligns with LLMs for better zero-shot generalization, particularly in tasks like QA (Zhang et al., 2024). ZeroG applying GPT models focuses on zero-shot transfer for graph-based tasks, pioneering cross-dataset node classification by combining graph representations with LLM (Li et al., 2024). In this paper, a novel zero-shot transfer learning strategy using both assistant and target task is proposed."}, {"title": "3 Methodology", "content": "In this section, we first demonstrate the general workflow of proposed T3, and then formulate the summarization task under T3, thereafter dive into the detailed training and test processes."}, {"title": "3.1 T3 General Workflow", "content": "The general workflow of T3 is shown in Figure 1. The keypoint of T3 is that: in the whole training process, both the assistant task and the target task share the samples of the assistant task. At first, the used LLM \u00b9 acquires experience to deal with the assistant task through the labeled data, which means a deeper comprehension of the samples. Subsequently, LLM iteratively tries to accomplish the target task on these same samples and gain the experience for the target task by leveraging the previous learned contextual experiences, as well as being modulated by the predefined conditions. Finally, the model applies the experiences required for both the assistant and target tasks during the test process."}, {"title": "3.2 T3 for Summarization", "content": "Now the general workflow of T3 is implemented to our target task: summarization, with the question-answering issue as the assistant task."}, {"title": "3.2.1 Task Formulation", "content": "Given the training dataset for Question-Answering task (QA), each sample from the dataset contains both the text $D_i$ and several labeled question-answer pairs $QA_i$ (where i implies the ith sample), which form the text set $D = {D_1, D_2, ..., D_n}$ and the label set $QA = {QA_1, QA_2, ..., QA_n}$. Relatively, the question-answer pairs generated by LLM from $D_i$ are written as $QA'_i$. Given the test dataset for Summarization task (Sum), each sample from the dataset contains a text $D_j$, while LLM needs to generate the summary of the text $Sum'_j$."}, {"title": "3.2.2 Training Process", "content": "The overview of T3 for summarization is illustrated in Figure 2. The training process can be found at the top of the figure, while the test process is demonstrated at the bottom. Both inputs and outputs in these processes are written in bold in the figure. Furthermore, the detailed information of the training process is given in Algorithm 1 in Appendix A.1, including the following key steps:\nUpdating $Exp_{QA}$: At first, both summary and QA generation experiences $Exp_{Sum}$ and $Exp_{QA}$ are initialized as null. Then, LLM generates new QA pairs $QA'$ for each text $D_i$ and updates $Exp_{QA}$ according to several rules concluded by itself through comparing $QA'$ with the ground truth $QA_i$. The prompt $P_{QA}$ is predefined to control all steps mentioned in this part.\nUpdating $Exp_{Sum}$: In this part, LLM follows the predefined prompts $P_{Sum}$ and try to summarize the text $D_i$. As an prior-knowledge that might bring extra contextual information, both $QA_i$ and $QA'$ are also taken into account as the inputs. After the current summary $Sum'$ is generated, three thresholds are used to investigate whether the generated content is well enough. LLM would repeat the summary generation step and update its experience $Exp_{Sum}$ every iteration, till the stop conditions are fulfilled.\nThresholds Used in Iteration: The iterative process mentioned above continues until the generated summary $Sum'$ simultaneously meets the required thresholds for similarity, readability (Flesch, 1948), and compression rate (or the number of iterations reach the preset maximum iteration times K). In details, readability R is calculated as:\n$R = 206.835 - 1.015 \\times (\\frac{TW}{TSE}) - 84.6 \\times (\\frac{TSY}{TW}) (1)$\nwhere, $TW$ is the total number of words in the given text, $TSE$ is the total number of sentences, and $TSY$ is the total number of syllables. $TW/TSE$ calculates the average sentence length: a higher average sentence length makes the text harder to read, so it subtracts from the score. $TSY/TW$ calculates the average syllables per word: more syllables per word reduce the score because longer words tend to make the text more difficult to read. Moreover, the compression rate C is calculated as:\n$C = length(Summary)/length(Text) (2)$\nwhich is used to control the length of the generated summary."}, {"title": "3.2.3 Test Process on Summarization Dataset", "content": "According to the bottom of Figure 2, during the test process, LLM would first use $Exp_{QA}$ learned from the training process to generated auxiliary question-answer pairs, and bring them with the updated $Exp_{Sum}$ to generate the final summary. Prompts in test process are given in Appendix A.4."}, {"title": "3.2.4 Test Process on QA Dataset", "content": "In particular, as known from Figure 2. We design QG, a more challenging text generation strategy for LLM, to replace the text understanding strategy of predicting answers for the questions from the given QA dataset during the training process, in order to better fit LLM to the summarization task. A by-product of this approach is that we can even run the QA dataset in test process to generate summaries for its own texts based on its QA pairs, and adopt appropriate metrics for effectiveness evaluation, as shown in Figure 3. Prompts in test process for QA dataset are given in Appendix A.5."}, {"title": "4 Experiments", "content": "This section presents experimental details on four datasets, including two narrative dataset and two news datasets. We compare the quality of the summaries generated by the baseline models and our model using the generation algorithm from different aspects and perform an ablation study to analyze the influence of different components of T3."}, {"title": "4.1 Settings", "content": ""}, {"title": "4.1.1 Datasets", "content": "Four datasets are used, including two summarization ones (BBC summary and NarraSum), and two QA ones (NLQuAD and FairytaleQA):\nBBC Summary includes 2,225 BBC news from 2004-2005, used for categorization and summary research (Greene and Cunningham, 2006).\nNarraSum has 122K narrative synopses from movies and TV episodes, aimed at improving summarization research (Zhao et al., 2022).\nNLQUAD contains 31,000 questions and answers derived from 13,000 BBC news, for non-factoid long QA, focusing on multi-sentence descriptive answers to enhance document-level language models and challenges models with long context and complex language understanding (Soleimani et al., 2021).\nFairytaleQA includes 278 children's stories from Project Gutenberg, with questions and answers created by educational experts, aimed at assessing and training narrative comprehension for students (K-8) and models (Xu et al., 2022).\nIn experiments, 386 long articles from BBC summary, 260 long stories from FairytaleQA, 100 long stories from NarraSum, 501 long articles from NLQUAD are randomly picked up for evaluation. Each selected texts has a median word count exceeding 1,000."}, {"title": "4.1.2 Evaluation Metrics", "content": "To make more comprehensive evaluation, especially focusing on generated text quality and hallucinations alleviation, a range of classical and innovative metrics are used here:\nROUGE compares the overlap of n-grams, word sequences, and word pairs between the generated summary and reference summaries (Lin, 2004).\nBLEU compares the overlap of n-grams between the generated text and reference translations, with higher scores indicating closer alignment with human translations and thus better quality (Papineni et al., 2002).\nFactSumm FactSum uses named-entity recognition and relation extraction to extract facts from the source text and the generated summary to evaluate fact consistency by comparing extracted facts represented as triples (subject, relation, object) (Zhang et al., 2021).\nChatGPT evaluates factual inconsistency in summaries, outperforming previous methods with fewer hallucinations (Luo et al., 2023)."}, {"title": "4.1.3 Baseline Models", "content": "Seven advanced LLMs from three most representative series are selected as the baselines. All of them are accessed through Application Programming Interface (API), which not only standardizes the experiment for reproducibility and consistency, but also saves computational resources.\nGPT Series are developed by OpenAI and show strong language comprehension and generation capabilities for various natural language processing tasks (Achiam et al., 2023). Here, GPT-3.5-turbo, GPT-4 Turbo, and the newest lauched GPT-4o are adopted here.\nClaude Series introduce advanced multimodal capabilities developed by Anthropic (Anthropic, 2023b). Claude 3.5 Sonnet is the newest one of them, showing strong performance in graduate-level and text-based reasoning (Anthropic, 2023a). Claude 3.5 Sonnet and Claude 3.0 Sonnet are chosen here.\nGemini Series surpasses Google's capabilities in understanding images, audio, video, and text (Team et al., 2023). Currently, Gemini-1.5-pro is the newest and best-performing one of them (Reid et al., 2024). Both Gemini-1.5-pro and Gemini-1.0-pro are used here."}, {"title": "4.2 Implementation Details", "content": "The comparative experiment are designed and executed to validate both the feasibility and effectiveness of the proposed T3, by investigating the generated summarization results for all four datasets from all used LLMs with/without using T3.\nThe baseline summary of each text for each LLM is generated by using original text and the following prompt: \"You are one helpful text assistant. Based on the input text, provide a summary in proper length\". Correspondently, each LLM with T3 is guided by the experience and prompts derived from training process.\nFor BBC summary and NarraSum in test process, both the original texts from them, the QA generation experience $Exp_{QA}$ and the summary experience $Exp_{Sum}$ are contained into the input prompt, whose detailed prompts are given in Appendix A.6, A.7, A.8 and A.9 for News-style Datasets and Narratives-style Datasets respectively.\nFor FairytaleQA and NLQUAD in test process, 10 texts and their corresponding QA pairs from each dataset are selected for training, while the remaining texts are used for testing. Both the original texts from the datasets, the QA generation experience $Exp_{QA}$ and the summary experience $Exp_{Sum}$ are contained into the input prompt, whose detailed prompts are given in Appendix A.6, A.7, A.8 and A.9 for News-style Datasets and Narratives-style Datasets respectively.\nThe higher scores from the used metrics are, indicating the better generation of summaries. It should be noted that the metric Factscore was calculated by GPT-4o for NarraSum and FairytaleQA, while calculated by FactSumm for BBC summary and NLQUAD, because it cannot extract facts from the story source text in practice so that does not perform well on narratives.\nFor BBC summary and NarraSum, the provided summary was used as the reference, while for FairytaleQA and NLQuAD datasets, the original text was used as the reference. As mentioned in Part 3.2.4, T3 can also deal with QA datasets for summarization task. When the dataset does not provide reference summaries, the generated summary can be directly compared to the original text through certain automated metrics tools (e.g., ROUGE) (Louis and Nenkova, 2009). According to (Saggion and Poibeau, 2013), the overlap of the generated summaries with the original text is important in assessing information coverage and the validity.\nFor models from GPT or Claude series, default values for temperature and other related hyperparameters are applied. For Gemini models, we set 'HARASSMENT', 'HATE_SPEECH', 'SEXUALLY_EXPLICIT', 'DANGEROUS' in safety setting into 'block_none'."}, {"title": "4.3 Results", "content": "Table 1 illustrates the performance of all used models with (right \"T3\" column) and without (left \"w/o\" column) T3 on all adopted datasets. The better one of the two values from each row of the two columns is marked in bold. Especially, for each row where all results from columns with T3 are better than those columns without T3, the row is colored blue. From this table, we observe that:\n(1) LLMs using T3 outperforms themselves without T3 under most metrics of most datasets, which demonstates the feasibility and effectiveness of T3. Specially, the average improvement rates of GPT-4o, Gemini-1.5-pro, and Claude-3.5-sonnet using T3 compared to the baselines without T3 across different tasks are calculated and shown that these LLMs with T3 nearly improves the ROUGE score by 14%, the BLEU score by 35%, and the Factscore score by 16%.\n(2) Of all the models in the four series, it is almost always the newest ones with T3 that achieve the most significant results comparing with itself without T3. For example, GPT-4o is 4% and 5% higher on Rouge-1 metrics compared to GPT-4 and GPT-3.5 on BBC Summary, respectively. These results could be due to the improvement of capabilities for latest LLMs on handling longer texts and more comprehensive tasks through understanding more complicated prompts and locating better details and events within lengthy texts (Anthropic, 2023a; Islam and Moushi, 2024; Reid et al., 2024).\n(3) Four used datasets are divided into news- and narrative-styles. For most models with/without T3, the generated summaries on narrative-style datasets NarraSum seems not as good as that of news-style ones BBC Summary. According to (Genette, 1980; Van Dijk, 2013; Barthes, 2016), the structure of news-style texts is compared with that of narrative-style texts, highlighting how news is easier to be summarized because of its concise layout and fixed pattern, while the narrative texts is more complexity, including chronological order, shifts in point of view, and narrative hierarchy, so that increasing the difficulty of summarization.\n(4) The improvement of LLMs with T3 seems more significant on NLQuAD and FairytaleQA, i.e., QA task-oriented datasets, which might be benefited from using the prior information of questions and answers pairs to enhance the contextual analyzing capability of T3. Moreover, incorporating QA information implies to improve summarization quality by enhancing text comprehension, focusing on key information, increasing coverage, ensuring coherence, and aligning summaries with user needs (Fabbri et al., 2022; Siledar et al., 2024)."}, {"title": "4.4 Ablation Study", "content": "We further analyze the influence of different components of T3. Table 2 shows the experimental results for Claude-3.5-sonnet after removing summary experience and QA generation experience on the BBC summary dataset, and the results for GPT-40 after removing summary experience and QA generation experience on the NLQuAD dataset. For models with less reasoning ability, longer inputs and complex instructions of T3 might diminish their performance, while for more advanced models, each component of T3 could be utilized to enhance their overall effectiveness."}, {"title": "5 Conclusion", "content": "A novel T3 framework is proposed to address the challenge of long text summarization, using QA datasets to iteratively train baseline model at first, then to deal with summarization in a zero-shot way. Experiments on four datasets among seven representative baselines under five evaluation metrics validate the significant effectiveness of T3. Since T3 is model-agnostic and theoritically could be used for any suitable assistant task and target task pair, the future work will aim to explore the potential of it in a broader range of task combinations."}, {"title": "Limitations", "content": "When using the proposed T3 framework in real scenarios, the following limitations that need to be further optimized:\nLimited Baseline Models Due to the costs and time constraints of API, we only test T3 on models from GPT, Claude, and Gemini series. More open-source models (such as LLaMA) and their private deployment situation will be considered in the future.\nLimited Tasks Few empirical guidelines for the selection of assistant tasks are given in this paper. In the future, the matching rules of assistant tasks and target tasks will be comprehensively explored in various task combinations."}, {"title": "Ethics Statement", "content": "Data Availability and Safety The training and test data used for summarization and QA in this paper are publicly available. While the original datasets were filtered, some content may still include sensitive material, such as reports on violent crimes or incidents. Furthermore, when utilizing the Gemini model service, the safety mode is set to 'block_none,' which may result in outputs containing violent or explicit content.\nUsage of Large Language Models Models like GPT, Gemini, and Claude are utilized to generate QA results, refine prompt descriptions, and produce text summaries for input texts in summarization tasks. The generated text is used exclusively for experimental and analytical purposes, and the findings are presented in the relevant sections."}]}