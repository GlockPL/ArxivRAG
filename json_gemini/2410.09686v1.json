{"title": "Generalization of Compositional Tasks with Logical Specification via Implicit Planning", "authors": ["Duo Xu", "Faramarz Fekri"], "abstract": "In this work, we study the problem of learning generalizable policies for compositional tasks given by a logic specification. These tasks are composed by temporally extended subgoals. Due to dependencies of subgoals and long task horizon, previous reinforcement learning (RL) algorithms, e.g., task-conditioned and goal-conditioned policies, still suffer from slow convergence and sub-optimality when solving the generalization problem of compositional tasks. In order to tackle these issues, this paper proposes a new hierarchical RL framework for the efficient and optimal generalization of compositional tasks. In the high level, we propose a new implicit planner designed specifically for generalizing compositional tasks. Specifically, the planner produces the selection of next sub-task and estimates the multi-step return of completing the rest of task from current state. It learns a latent transition model and conducts planning in the latent space based on a graph neural network (GNN). Then, the next sub-task selected by the high level guides the low-level agent efficiently to solve long-horizon tasks and the multi-step return makes the low-level policy consider dependencies of future sub-tasks. We conduct comprehensive experiments to show the advantage of proposed framework over previous methods in terms of optimality and efficiency.", "sections": [{"title": "1 Introduction", "content": "In real-world applications, completing the task involves multiple subgoals which are distant in time and have to be achieved under temporal order constraints specified by the user. For example, a service robot on the factory floor might have to fetch the a set of components but in different orders depending on the product being assembled, and it may need to avoid some unsafe situations. These tasks are specified by logic compositional languages which have long been used for objective specification in sequential decision-making (De Giacomo & Vardi, 2013). Using a vocabulary of domain-specific properties, expressed as propositional variables, formal languages such as Linear Temporal Logic (LTL) (Pnueli, 1977) and SPECTRL (Jothimurugan et al., 2019), capture complex temporal patterns by composing variables via temporal operators and logical connectives. These languages provide well-defined semantics while enabling semantics-preserving transformations to deterministic finite-state automata (DFA), which can expose the discrete structure underlying an objective to a decision-making agent. Generalization to multiple tasks is a key requirement for deploying autonomous agents in many real-world domains (Taylor & Stone, 2009). In this work, we consider the problem of generalizing compositional tasks where, at the test time, the trained agent is given a DFA description of an unseen task and expected to perform the task with no additional training.\nAlthough reinforcement learning (RL) algorithms have achieved many successes in many fields (Mnih et al., 2015; Wang et al., 2022), they still have difficulties on generalization of compositional tasks which are different from regular problems solved by previous RL methods. Previous related works on generalization of compositional tasks suffer from various issues (Kuo et al., 2020; Araki et al., 2021; Vaezipoor et al., 2021; den Hengst et al., 2022; Liu et al., 2022a), such as lack of optimality or slow convergence rate. Some works (Araki et al., 2021; den Hengst et al., 2022; Liu et al., 2022a) solve new compositional tasks by leveraging the trained reusable skills or options, which may produce sub-optimal solutions. These methods train every option for reaching a specific subgoal independently and myopically without considering the whole task,"}, {"title": "2 Related Work", "content": "Applying the RL paradigm to solve logic compositional tasks has been studied by many previous works. These approaches first equivalently convert the compositional task formula into its automaton representation, and then augment the state space by multiplying the environmental MDP and task automaton together, yielding a product MDP. Representative previous approaches based on product MDP include Q-learning for reward machines (Q-RM) (Camacho et al., 2019; Icarte et al., 2018; 2022), LPOPL (Toro Icarte et al., 2018) and geometric LTL (G-LTL) (Littman et al., 2017). In addition, authors in (Jothimurugan et al., 2021) proposed a DiRL framework to complete LTL task successfully by using hierarchical RL which interleaves graph-based planning on the automaton and guide the agent's exploration for task satisfaction. However, all of these approaches incorporate the exact task formula into their algorithms, and the found policies have to be learned again for any unseen task, which cannot be directly used to solve any new tasks. Hence, these methods do not achieve zero-shot generalization in any sense.\nSome previous works train reusable skills or options to achieve generalization in a compositional task setting (Andreas et al., 2017; Araki et al., 2021; Le\u00f3n et al., 2020; 2021). The unseen task is satisfied by sequentially composing learned option policies based on value iterations over possible subgoal selections. However, these methods do not consider dependencies of subgoals, which cannot produce optimal solutions in the motivating examples shown in Figures 1 and 2. Therefore, the optimality and even feasibility of the solution cannot be achieved. Although authors in (Kvarnstr\u00f6m & Doherty, 2000) consider causal dependencies of sub-tasks, their method assumes the dependencies are known and given explicitly. Our framework does not need to know sub-task dependencies and is applicable to general sub-task dependencies.\nAuthors in (Kuo et al., 2020; Vaezipoor et al., 2021) propose task-conditioned policies to achieve zero-shot generalization in compositional tasks, where the policy is conditioned on task embedding extracted by recurrent graph neural network (Kuo et al., 2020) or graph convolutional networks (Vaezipoor et al., 2021). Although the optimality of the learned policies can be achieved with sufficient training, these methods suffer from the issue of poor learning efficiency, since they do not decompose the task, without leveraging compositional nature of the task. This work proposes a hierarchical RL framework which achieves global optimality and efficient learning at the same time.\nIn addition, the goal-conditioned RL (GCRL) has been a hot topic for long, which trains a unified policy to reach arbitrary goals in the specified goal space (Liu et al., 2022b). However, these GCRL methods solved the problem where the agent only needs to reach a single goal in each episode, while the compositional tasks to be solved here has multiple subgoals to achieve in fixed time orders. In addition, some GCRL papers also proposed to generate subgoals in a hierarchical framework (Li et al., 2021; Chane-Sane et al., 2021), where multiple subgoals need to be achieved in one episode. However, these subgoals are only used to facilitate the agent's exploration, which can be achived in any time orders. Therefore, GCRL papers are not comparable with our work."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Episodic Labeled MDP", "content": "The proposed framework is to solve tasks in an episodic labeled Markov decision process (MDP) M defined as a tuple (S, A, P, R, So, \u03b3, \u0397, Po, L) (Sutton & Barto, 2018; Icarte et al., 2018; Jothimurugan et al., 2019) where S is the state space, A is the set of actions or action space, P(s'|s, a) is the transition probability of reaching next state s' from s \u2208 S given action a \u2208 A, R : S \u00d7 A \u00d7 S \u2192 R denotes the reward function, So is the initial state distribution, \u03b3 \u2208 [0, 1] is the discount factor, H is the maximum length of each episode, Po is the set of atomic propositions, and L : S \u2192 2P\u00ba is the labeling function mapping any state s \u2208 S to a finite set of propositions in Po holding true in state s. When interacting with the environment, the agent selects action given the current state s and task & according to its policy, i.e., \u03c0(\u00b7|s, $), making progress towards completing by observing symbols of next state L(s'). In each episode, the agent's target is to complete the task & within H steps and maximize the accumulated rewards (G = \u2211t=0H-1 \u03b3tR(st, at, St+1)) at the same time."}, {"title": "3.2 Task Specification Language", "content": "A compositional task considered in this work is described by a logic specification formula 4, a Boolean function that determines whether the objective formula is satisfied by the given trajectory or not (Pnueli, 1977). In this work, we adopt the specification language SPECTRL (Jothimurugan et al., 2019) to express the logic and temporal relationships of subgoals in tasks. A specification & in SPECTRL is a logic formula applied to trajectories, determining whether a given trajectory ( = (80, 81, ...) successfully accomplishes the task specified by . For rigor of math, & can be described as a function \u00a2 : Z \u2192 {0,1} producing binary outputs, where Z is the set of all the trajectories.\nSpecifically, a specification is defined based on a set of atomic propositions Po. For each proposition p \u2208 Po, the MDP state s of the agent satisfies p (denoted as s = p) when p \u2208 L(s) and L is labeling function introduced above. The set of symbols P is composed by conjunctions and disjunctions of atomic propositions in Po. The grammar for defining a symbol b \u2208 P is written as b ::= p|(b\u2081 ^b2)|(b\u2081 V b\u2082), where p \u2208 Po.\nBased on definitions above, the grammar for formulating SPECTRL specifications can be written as:\n ::= achieve b | 1 ensuring b | 1; 2 | 1 or 2\nwhere b \u2208 P. Here \"achieve\" and \"ensuring\" correspond to \"eventually\" and \"always\" operators in LTL (Pnueli, 1977; Araki et al., 2021). Given any finite trajectory ( with length t, the semantics of SPECTRL are defined as:\n1. |= achieve b if \u2203i < t, si |= b (or b \u2208 L(si))\n2. |= ensuring b if \u2203i < t, si |= b\n3. |= 1; 2 if \u2203i < t, 60:i |= $1 and (i+1;t |= $2\n4. |= $1 or 2 if |= $1 or |= $2\nSpecifically, the statement 1) signifies that the trajectory should eventually reach a state where the symbol b holds true. The statement 2) means that the trajectory should satisfy specification & while always remaining in states where b is true. The statement 3) signifies that the trajectory should sequentially satisfy $1 and then 42. The statement 4) says that the trajectory should satisfy either $1 or $2. We say a trajectory satisfies specification & if there is a time step t such that the prefix (o:t satisfies $.\nIn addition, every SPECTRL specification & is guaranteed to have an equivalent directed acyclic graph (DAG), termed as abstract graph (Jothimurugan et al., 2019). An abstract graph G is defined as G ::= (Q, \u0395, 90, F, \u03ba), where is the set of nodes, E C Q \u00d7 Q is the set of directed edges, qo \u2208 Q denotes the initial node, FCQ denotes the accepting nodes, subgoal region mapping \u03b2 : Q \u2192 25 which denotes the subgoal region for every node in Q, and safe trajectories Zsafe = \u22c3eEE Zeafe where Zeafe denotes the safe trajectories for any edge e \u2208 E. Note that the environmental MDP M is connected with task specification & and G4 by \u1e9e and Zsafe which may change for different tasks. Furthermore, the function k labels each edge e := q \u2192 q' with the symbol be (labeled edge denoted as e := q +q'). Given \u03ba, the agent transits from node q to q' when the states si and si+l of trajectory ( satisfy si \u2208 \u03b2(q) and be \u2286 L(si+1) for l \u2265 0.\nGiven a task specification 4, the corresponding abstract graph G can be constructed based on its definition, such that, for any trajectory ( \u2208 Z, we have ( |= $ if and only if ( |= G. Hence, the RL problem for task can be equivalent to the reachability problem for G. It is obvious that every task DAG has a single initial node in SPECTRL language, which can be converted into a tree."}, {"title": "3.2.1 Sub-task Definition", "content": "Given the DAG G corresponding to task specification $, we can define sub-tasks based on edges of the DAG. Formally, an edge from node q to p \u2208 Q can define a reach-avoid sub-task specified by the following SPECTRL formula:\nTask(q, p) := achieve(b(q,p)) ensuring (\u2227r\u2208N(q),r\u2260pb(q,r))\nwhere b(q,r) is the propositional formula labeled over the edge (q,p) in the DAG, and N(q) is the set of neighboring nodes which the out-going edges of q point to in the DAG. For instance, in Figure 2, the propositional formula over the edge (q0,92) is b(90,q2) = \u00abd ^a. When e = (q,p), the notation Task(e) is same as Task(q,p).\nFor each sub-task Task(q, p) and any MDP state so \u2208 S, there is a policy \u03c0(q,p) which can guide the agent to produce a trajectory 8081 . . . Sn in MDP. It induces the path qqq . . qp in the DAG, meaning that the agent's DAG state remains at q until it transits to p, i.e., sn \u2208 \u03b2(p) and si \u2209 \u03b2(p) for i < n. In this work, since we consider the dependencies of sub-tasks, the policy \u03c0(q,p) is also dependent on the future sub-tasks to complete.\nGiven the environmental MDP M, for any SPECTRL task specification 4, the agent first transforms & to its corresponding DAG (abstract graph) G\u00a2 = (Q, E, 90, F, \u03b2, Zsafe, \u03ba). Then, the sub-tasks can be formulated based on G."}, {"title": "3.3 Problem Formulation", "content": "Given the environmental MDP M with unknown state transition dynamics, a SPECTRL specification represents the logic compositional task consisting of temporally extended sub-tasks, and Go is the DAG (abstract graph) corresponding to the task . Let Paths(q, F) be the set of all paths in the DAG starting in q and terminating at an accepting node FC Q.\nThe target of this work is to train an reinforcement learning agent in a data-efficient manner which can be zero-shot generalized to complete any unseen SPECTRL task & without further training. In addition to task completion, we also consider the optimality of the found solution for the unseen task \u00f8, maximizing the accumulated environmental rewards during task completion. Specifically, the reward function R of MDP M is unknown to the agent, and the reward of any state s is available to the agent only when s is visited."}, {"title": "4 Methodology", "content": "In this work, we propose a novel hierarchical RL framework for the generalization of tasks described in the SPECTRL specification language. The proposed framework aims at efficiently training a hierarchical RL agent which is zero-shot generalizable to new tasks. In the following sections, we will first introduce components and working mechanism of the proposed framework composed by high-level and low-level agents. Then, we present the training algorithm of the proposed framework, including training of low-level and high-level agents, training curriculum and experience replay, which are designed to make the training algorithm more robust and data-efficient."}, {"title": "4.1 Framework", "content": "The proposed framework consists of high-level and low-level agents. The high-level agent is essentially an implicit planner. It selects the next sub-task for the low-level agent to complete. According to Section 3.2, a sub-task is composed by positive propositions to achieve (p+) and negative propositions to avoid (p). In previous works on the generalization of temporal logic tasks, the global optimality of the found solution will be lost when the sub-tasks are dependent on each other, and the training process can converge slowly when the task has long horizon. In order to tackle these issues, we incorporate new techniques into both high-level and low-level agents.\nIn the high level, since the Markovian property does not hold when sub-tasks are dependent in planning, the implicit planner predicts the best selection of next sub-task based on the feature of future sub-tasks extracted by the GNN. Furthermore, in addition to achieving the assigned sub-task, the low-level agent also trains its policy to look into the future sub-tasks by leveraging the accumulated discounted rewards (return) for completing the rest of task which is estimated by the high-level agent. This estimated return can also fasten the learning of the low-level agent, resolving the slow convergence issue in long-horizon tasks."}, {"title": "4.1.1 High-level Agent", "content": "When the dependencies of sub-tasks are taken into consideration, the planning problem of selecting next high-level sub-task will not satisfy Markovian property anymore. In this case, we cannot use the widely-adopted value iteration (VI) method to plan, since it derives a value function based on Bellman equations (Sutton & Barto, 2018) which only work in Markovian environments. Therefore, we propose an implicit planner which directly predicts the optimal selection of next sub-task and estimates the return for completing future sub-tasks based on a embedding representing the future sub-tasks produced by a graph neural network (GNN) (Scarselli et al., 2008; Zhou et al., 2020) and latent transition model (Kipf et al., 2020; van der Pol et al., 2020).\nAs shown in Figure 3, the proposed implicit planner consists of a latent state transition model (\u04e9, \u0422\u04e9), \u0430 GNN (Mo, Uo), a policy network \u03c0\u03cc and a value network Vp. Since all the components of implicit planner are trained end-to-end together, their trainable parameters are all denoted as 0. The implicit planner works in forward and backward pass. With the task DAG G and current state st as input, in the forward pass, the planner infers the latent representations of current and future states based on encoder Ee and latent dynamic model To, building a latent tree. In the backward pass, based on the latent tree, the GNN is applied to extract an embedding ho representing the situations of completing future sub-tasks. Then, with ho as input, the policy and value networks produce the sub-task selection and estimated return, which are passed to the low-level agent.\nEncoder. The encoder function, Eo : S \u2192 Rd, takes environmental state as input and produces its latent representation, where d is the dimension of the latent space. The realization of encoder is specific to the environment. CNN is used in pixel-based environments, while MLP is adopted in environments with continuous observations.\nLatent Dynamic Function. The dynamic function, To : Rd \u00d7 P \u2192 Rd, predicts next latent state by consuming current latent state and positive propositions of sub-task. Given current state s and sub-task t, the next latent state is predicted as Ee(s) + To(Eo, p\u00b2), where p is the positive propositions of t. It models the changes of the latent state caused by the completion of a sub-task. In implementation, To is usually realized by MLP.\nGraph Neural Network. The GNN is used to extract the embedding which represents the situations of completing future sub-tasks. As shown in Figure 3, each edge in GNN is in the reversed direction of that in latent tree built in the forward pass. For each node k in the GNN, a set of messages is first obtained, i.e., one message from every neighboring node j, by applying the message passing function Me to the node features (hk and hj) and edge feature e(j,k). The initial node feature is the latent state predicted by Te in the forward pass, and updated with incoming messages with function U\u0189. The edge feature e(j,k) is a binary encoding of the sub-task b(k,j) labeled over the edge (j, k). Specifically, the edge feature is the concatenation of two binary"}, {"title": "4.1.2 Low-level Agent", "content": "The target of low-level agent is to complete the sub-task \u03b7 specified by the high-level agent. Since every sub-task in SPECTRL language is a reach-avoid task, sub-task ng can be decomposed into positive proposition to achieve and negative propositions to avoid, denoted as p+ and p, respectively. For example, for the sub-task b^\u00aba>\u00abd, p+ = {b} and p_ = {a,d}. Hence, the low-level policy and value functions, denoted as \u2030 and \ub9f5, are conditioned on p+ and p_ encoded into binary vectors. The diagram of processing inputs to the low-level agent is in Figure 4.\nAdditionally, in order to consider the dependencies of future sub-tasks, \u03c0\u03ce and V\ubc11 also consume the DAG of the future sub-tasks, denoted as Gr, which is the DAG of the remaining part of current task & to achieve after \u03b7 is completed. Essentially, $' is the progression (Kvarnstr\u00f6m & Doherty, 2000; Vaezipoor et al., 2021) of task & with sub-task \u03b7. For example, the progression of task in Figure 2 with sub-task b ^ \u00aba > \u00acd produces the task c > \u00acd which is the rest part to achieve after b^a^\u00acd is done."}, {"title": "4.2 Algorithm", "content": "The high-level and low-level agents are trained separately. In the high level, all the modules are trained end-to-end. In the low level, the agent is trained to complete the assigned sub-task by considering the dependencies of future sub-tasks. In order to reduce the sample complexity, a training curriculum is designed for both high-level and low-level agents. Besides, we also propose an experience relay method for relabeling trajectories with synthesized task DAGs.\nBefore proceeding to algorithmic details, we will first present some notations and definitions. For a trajectory ( = {(\u03b4\u03b9, \u03b1\u03b9, \u03b9)}01, assume that completes K\u00e7 sub-tasks, denoted as no, 71,\u06f0\u06f0\u06f0, \u03b7\u03ba\u03b5 -1, and the time steps of completing these sub-tasks are denoted as to, t1, ..., tk \u22121. Therefore, we can define a high-level transition tuple as (s, \u03c6, \u03b7, R, s', $'), meaning that starting from state s, the agent completes the sub-task \u03b7 at state s', and task o' is the progression of \u03c6 with sub-task n, and R is the accumulated discounted rewards during the process of completing \u03b7. With these definitions, based on trajectory \u0120 and the task & completed by \u015a, the set of high-level transitions can be specified as Fh := {(St\u2081, \u03a6i, Ni, Ri, Sti+1, \u00d8i+1)}i, where Ri := \u2211\u03c4=ti+1ti+1\u22121\u03b3\u03c4\u2212tir\u03c4 and r is the environmental reward received at time step \u03c4."}, {"title": "4.2.1 High-level Training", "content": "The success of the high-level agent relies on latent transition model and the GNN to extract the embedding representing the completion of future sub-tasks. Following previous papers on learning latent dynamic space (Bordes et al., 2013; Kipf et al., 2020), we adopt TransE (Kipf et al., 2020) loss to train the encoder Ee and latent dynamic function To. For any high-level transition data (s, \u03c6, \u03b7, R, s', \u03c6') and a negatively sampled state \u00a7, it can be expressed as below:\nLTranse((s, n, s'), \u0161; 0) = d(Eo(s) + To(Eo(s), \u03b7), Eo(s')) + max(0, \u00a7 \u2013 d(Eo(s), Eo(5)))\nwhere @ are the trainable parameters, d is the distance function which is the Euclidean distance in this work, and & is a positive hyper-parameter. The task \u03c6, \u03c6' and reward R are not used in the training loss of Ee and Te.\nFor the GNN part, models Me and U\u0189 are trained together with policy and value networks in an end-to-end manner. Since a learning curriculum is designed to start from simple tasks, there is no need to pre-train the GNN part.\nThe policy \u03c0\u03cc and value Vr networks are trained with PPO algorithms (Schulman et al., 2017) with set of possible sub-tasks as the action space. Given the set of high-level transitions as \u0393h, for any transition tuple (Sti, \u03a6\u03af, \u03b7, Ri, Sti+1, \u03a6i+1), the PPO loss is evaluated by expanding the latent tree \u03a8 based on state st\u2081 and task di and using GNN to produce the embedding, following the forward and backward passes in Section 4.1.1. One iteration of training the high-level agent can be summarized as the following steps:\n1. Sample trajectories ( from the replay buffer B and form the high-level transition dataset \u0393h;\n2. Based on transition tuples in these trajectories, compute the PPO (Schulman et al., 2017) and TransE (4) losses, where the negative samples \u0161 in (4) are randomly sampled from states in Fh;\n3. Update the trainable parameters @ of all the components together, based on the following loss:\nL(Th; 0) = LPPO (Th; 0) + \u03bb\u2211iLTranse ((Sti, Ni, Sti+1), \u00a7i; 0)"}, {"title": "4.2.2 Low-level Training", "content": "The low-level agent is trained by PPO algorithm (Schulman et al., 2017) with transitions from sampled trajectory, the target task & to complete, sub-tasks and estimated return given by the high-level agent. Based on notations introduced above, if the i-th sub-task (ni) is completed at time step ti, then at the same time the current task will be progressed to $i+1 with ni and the high-level agent will specify next sub-task Ni+1. Based on $i+1 and Ni+1, the inputs to low-level models, i.e., St, P p\u00b2+1, p2+1 and $i+1, can be easily obtained, where p+1 and p\u00b2+\u00b9 are decomposed from Ni+1 and $i+1 is the progression of $i+1 with Ni+1.\nSince the low-level agent also considers the dependencies of sub-tasks, the low-level policy is conditioned on future sub-tasks, and its value function needs to be trained with reward information of future sub-tasks as well. Otherwise, the optimality can be lost. Additionally, in order to fasten the reward propagation along the time horizon, in contrast to conventional PPO (Schulman et al., 2017), we introduce the max operator for computing the targets to update the low-level value function V at the time step. Specifically, whenever the sub-task assigned by the high-level agent is completed, V is trained to predict the maximum of one-step return from low-level agent itself and multiple-step return estimated by the high-level agent. At time step ti, the target to update V can be written as\nVarget (Sti, p\u00b2+1, p2+1, 'i+1) := max{rt\u2082 + V (St+1, p\u00b2+1, p\u00b2+1, i+1), V (Sti, i+1)}\nWith the help of multi-step return V learned in the high level, the reward information of future sub-tasks can be faster back-propagated along the trajectory, making the learning of low-level policy converge faster. The future reward information make the low-level agent consider the dependencies of future sub-tasks when completing the current sub-task, hence improving the optimality."}, {"title": "4.2.3 Training Curriculum", "content": "Due to the compositional nature of SPECTRL specification, the number of possible task DAGs can be extremely large, growing exponentially with the number of sub-tasks to complete before reaching an accepting node. Randomly sampling tasks from the space of SPECTRL specification can make the computational complexity of training process intractable. In order to improve the training efficiency, we propose a curriculum where the complexity of agent's training tasks increases gradually and adaptively.\nSpecifically, the design of training curriculum is based on the number of sub-tasks for completing the whole task. Although every task in SPECTRL specification can be converted to a unique DAG, it is processed in the form of a tree by both high-level and low-level agents as introduced in Section 4.1.1 and 4.1.2. For any trajectory where the task is successfully completed, the agent executes a path (from root to a leaf) of the tree converted from the task, which is essentially a sequence of sub-tasks. Therefore, starting from level 1 to K, in level every k, the agent is trained to complete a sequence of k randomly generated sub-tasks, where both the positive and negative propositions are randomly selected in each sub-task. Here K is the largest depth of the tree converted from task formula and specified by the user as the highest complexity of the task. When the agent's performance satisfies some criteria, e.g., the success rate of complete a task is above 95%, the training will go to a higher level of the curriculum."}, {"title": "4.2.4 Experience Replay", "content": "In addition, we also propose an experience replay method for generating more data to train the GNN part of high-level and low-level agents. The modules in the high-level agent consume data in the sub-task level, which is scarce and difficult to collect when the task horizon is long. Additionally, for any task, both high-level and low-level agents process the tree converted from the DAG of the task specification. Training the agent to complete sequences of sub-tasks is not enough and tasks represented as trees of sub-tasks should be generated in the training, so that the GNN module in the agent can learn to select the optimal path to complete the task. In order to improve the data efficiency of the training, we relabel every on-policy trajectory and generate multiple off-policies trajectories by modifying the task the agent accomplishes, which notably does not require any"}, {"title": "4.2.5 Avoiding Propositions", "content": "Every sub-task assigned by the high-level agent is a reach-avoid sub-task executed by the low-level policy, i.e., \u03c0\u03ce(\u00b7|s, P+, P\u2212, $'), meaning that from a state s the agent is required to avoid any propositions in p_ before reaching propositions p+ (\u22c0gj\u2208p+ gj holds true), conditioned on completing the future task $'. In order to improve learning efficiency, the low-level policy in our method only performs avoidance in situations where there is a high likelihood of colliding with any propositions in p_. The likelihood of colliding with a proposition g in p\u2013 from current state s can be assessed by the low-level value function (s, g, \u00d8, \u00d8) (denoted as (s, g) in the following), where \u00d8 refers to empty input. If (s, g) exceeds a threshold v, it indicates a high chance of colliding with proposition g. According to the design of training curriculum, for any g \u2208 Po, V(s, g) and the corresponding Q function (s, g, a) are well trained in the first level.\nSpecifically, define k := arg maxgk\u2208p_ Vu (s, gk). If its value V(s, gk) is below the threshold v (e.g. 0.9), it means that the agent is in the safe zone, and can take a goal-reaching action, i.e., \u03c0\u03b9 \u03c0\u03c9 (\u00b7|s, p+, p\u2212, \u0444\u2032). Otherwise, the agent needs to select a safe action to avoid proposition gk, i.e., arg mina Qu(s, gk, a), which moves the agent away from proposition gk\u00b7\nWith the guidance of safe actions, the frequency of violation of avoidance constraints (visiting propositions in p_) can be reduced in every collected trajectory. Hence, since the low-level policy (\u00b7|s, p+, p\u2212, $') is trained by collected trajectories for solving reach-avoid task (p+, p\u2212) via PPO, the data-efficiency of low-level training can be improved."}, {"title": "5 Experiments", "content": "Our experiments are designed to evaluate the performance of multi-task RL agent trained by the proposed framework, in terms of learning efficiency, optimality and generalization. Specifically, our experiments will answer the following questions: 1) Overall performance: can the proposed framework outperform baseline methods in terms of optimality and learning efficiency when the dependencies of sub-tasks are introduced? 2) Ablation study 1: can the trained implicit planner select optimal sub-tasks in novel tasks unseen in the training? 3) Ablation study 2: can the return estimated by the high-level agent accelerate the learning of low-level agent? 4) Visualization: can the trajectories of trained agent follow the safety constraints and specify hidden rules in the reward function which cause the dependencies of sub-tasks?\nBefore presenting the experiment results, we will first introduce the environments. Then the training setup and baseline methods will be presented. Finally, the experiments about overall performance comparison will be demonstrated. Other experiment results and details are deferred to Appendix."}, {"title": "5.1 Environments", "content": "We conducted experiments across different environments with both discrete and continuous action and state spaces. All the environments are procedurally generated, where the layout and positions of objects are randomly generated upon reset. The positions and properties of objects are unknown to the agent. As such, none of the environments adopted here can be solved by simple tabular-based methods. Each task is described by a SPECTRL specification in terms of these propositions specific to the environment. The target of the agent is to complete the given task with maximal accumulated rewards.\nLetter. This environment is an \u00d7 n grid game with multiple letters allocated on the map. Out of the n\u00b2 grid cells, m grids are associated with k (where m > k) unique propositions (letters). Note that some letters may appear in multiple cells, making the agent easily visit other letters on its way to a target letter. An example layout is shown in Figure 6 with n = 7, m = 10 and k = 5. At each step the agent can move along the cardinal directions (up, down, left and right). The agent is given the task specification and is assumed to observe the full grid (and letters) from an egocentric point of view with the image-based observation.\nRoom. This environment is also a grid-world game, but its observation is divided into four rooms as shown in Figure 7. There are 5 letters located in 8 positions, corresponding to 5 propositions randomly allocated in these rooms. An example of layout is shown in Figure 7. The agent is randomly placed into one of these rooms. Each room is connected to its neighbors by corridors. Two randomly selected corridors are blocked by locks. The agent can open a lock by using a key corresponding to that specific lock (having the same color). These (green and yellow) keys are placed in positions which the agent can reach. But keys and doors are not propositions written in the task specification. So, the agent has to learn to pick up the right key and open the door by itself. Every movement of the agent receive -0.1 reward. The observation is also image-based here.\nZone. This is a robotic environment with continuous action and state spaces. It is modified from OpenAI's Safety Gym (Ray et al."}]}