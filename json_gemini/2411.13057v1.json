{"title": "Branches, Assemble! Multi-Branch Cooperation Network for Large-Scale Click-Through Rate Prediction at Taobao", "authors": ["Xu Chen", "Zida Cheng", "Yuangang Pan", "Shuai Xiao", "Xiaoming Liu", "Jinsong Lan", "Qingwen Liu", "Ivor W. Tsang"], "abstract": "Existing click-through rate (CTR) prediction works have studied the role of feature interaction through a variety of techniques. Each interaction technique exhibits its own strength, and solely using one type could constrain the model's capability to capture the complex feature relationships, especially for industrial large-scale data with enormous users and items. Recent research shows that effective CTR models often combine an MLP network with a dedicated feature interaction network in a two-parallel structure. However, the interplay and cooperative dynamics between different streams or branches remain under-researched. In this work, we introduce a novel Multi-Branch Cooperation Network (MBCnet) which enables multiple branch networks to collaborate with each other for better complex feature interaction modeling. Specifically, MBCnet consists of three branches: the Expert-based Feature Grouping and Crossing (EFGC) branch that promotes the model's memorization ability of specific feature fields, the low rank Cross Net branch and Deep branch to enhance both explicit and implicit feature crossing for improved generalization. Among branches, a novel cooperation scheme is proposed based on two principles: branch co-teaching and moderate differentiation. Branch co-teaching encourages well-learned branches to support poorly-learned ones on specific training samples. Moderate differentiation advocates branches to maintain a reasonable level of difference in their feature representations. The cooperation strategy improves learning through mutual knowledge sharing via co-teaching and boosts the discovery of diverse feature interactions across branches. Extensive experiments on large-scale industrial datasets and online A/B test at Taobao app demonstrate MBCnet's superior performance, delivering a 0.09 point increase in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes will be released soon.", "sections": [{"title": "1 Introduction", "content": "Click-Through Rate (CTR) prediction which estimates the probability of a user clicking on a candidate item, is a fundamental task in online services like recommendation, retrieval, and advertising [6, 18, 34]. The precision in predicting CTR not only substantially impacts user engagement but also has a significant effect on the revenue of industrial businesses. One key aspect of developing precise CTR prediction is to capture complex feature interactions, enabling the model to generalize across various scenarios.\nFor many years, researchers have examined the role of feature interaction in CTR by employing a range of methodologies. Earliest algorithm is Logistic Regression (LR) [11], which is a linear model and heavily relies on hand-crafted input features by domain experts. Factorization Machine (FM) [20] combines polynomial regression models with factorization techniques to automatically capture effective feature interactions. These methods primarily focus on low-order feature interactions and have shown limitations in capturing complex patterns among features [30]. Due to the superior non-linear feature learning ability of Deep Neural Networks (DNN), many scholars have investigated deep learning techniques on CTR prediction [28]. For example, Wide & Deep [6] consists of a jointly trained wide linear model and deep neural networks, combining the benefits of memorization and generalization for recommender systems. DeepFM [8] and xDeepFM [13] combine the power of factorization machines and deep learning, to emphasize both low- and high-order feature interactions. DeepIM [31] effectively captures high-order interactions via a deep interaction machine module, serving as an efficient, exact high-order FM implementation. Instead of employing FM to model feature relationships, Wang et al. [25, 27] introduced a novel cross network that explicitly performs feature crossing with each layer, which eliminates the need for manual feature engineering. Further, Mao et al. [16] observed that even two MLP networks in parallel can achieve surprisingly good performance compared to many well-designed models. Thereby, they developed a simple yet strong dual MLP model to capture diverse feature interactions and this model has achieved state-of-the-art performance on public benchmarks.\nEach feature interaction technique brings its own advantages, and solely depending on one type may hinder the model's potential to capture complex feature relationships. Particularly in industrial contexts with vast numbers of users and items, the data patterns can be exceedingly intricate. Relying on just one feature interaction technique is proved to be insufficiently effective [14]. In Figure 1 (a), we show the number of users and items across by different data collection sizes in our image2product search\u00b9 at Taobao\u00b2 app. The hundreds of millions of users and items indicate a broader spectrum of personalization while with higher data sparsity, bringing much more complex patterns and great challenges for modeling. As shown in Figure 1 (b) and (c), different feature interaction approaches have different modeling abilities and relying on one single approach cannot well cover the complex high-order feature space for CTR prediction. Recent researchers have highlighted that existing successful CTR models [6, 8, 13, 21, 25, 27] usually adopt a two-branch architecture and work in an ensemble style. The ensemble style permits the model to learn feature interactions from different perspectives. Despite the success of above models, the interplay and cooperative dynamics between different streams or branches remain under-researched. This motivates us to study the working principles of multi-branch CTR networks and develop a more effective model to capture the complex patterns.\nIn this work, we propose a novel Multi-Branch Cooperation Network (MBCnet) which enables multiple branch networks to collaborate with each other for better complex feature interaction modeling. MBCnet consists of three different prediction branches and a cooperation scheme. One important prediction branch is a designed Expert-based Feature Grouping and Crossing (EFGC) module, which groups hundreds of feature fields and only conducts feature interaction between specific group pairs by domain-expert knowledge. This branch enhances the model's memorization ability to capture expert-driven feature interactions for CTR prediction. The other two branches are the existing popular and powerful low-rank Cross Net [27] for explicit feature crossing and Deep Net [8, 25] for implicit feature crossing, which both improve the model's generalization ability [6]. These three branches exhibit diverse characteristics and modeling capabilities, which can be integrated into a more powerful model. Further, to better utilize the advantages of different branches, we propose a multi-branch cooperation scheme based on two principles. The first principle (branch co-teaching) dictates that well-learned branch should assist poorly-learned branch on particular samples, while the second principle (moderate differentiation) maintains that latent features of distinct branches should preserve a moderate level of differentiation, avoiding extremes of either excessive divergence or excessive similarity. Specifically, the first principle is embodied in a co-teaching objective function, where the well-learned branch uses its predictions as pseudo labels to guide the poorly-learned one on disagreed predicted samples. The second principle is formulated as an equivalent transformation regularization loss between latent features of branches. The cooperative strategy promotes learning abilities of different branches through knowledge transfer via co-teaching and also enhances their capabilities to uncover various patterns of feature interactions.\nThrough experiments, we have shown that MBCnet can largely improve CTR prediction performance. We have conducted an online A/B test in image2product search at Taobao app, which revealed that MBCnet could achieve an absolute 0.09 point CTR improvement, along with a relative 1.49% deal growth and a relative 1.62% GMV rise. To sum up, the contributions are summarized as follows:\n\u2022 We propose a novel multi-branch cooperation network (MBCnet) which ensembles different feature interaction branches to enlarge the capacity of capturing complex feature patterns within large-scale data. Specifically, MBCnet consists of a designed EFGC branch and two popular and powerful branches. Each of them possess unique capabilities and strengths, enabling them to form a more powerful CTR prediction model.\n\u2022 We introduce a novel cooperation scheme that facilitates cooperation and complements of branches to promote the overall learning ability. The cooperation scheme is constructed upon two principles: branch co-teaching and moderate differentiation. These principles promote each branch's learning ability, while also enabling them to uncover distinct patterns even with the same supervision.\n\u2022 Through extensive experiments on our large-scale datasets, we demonstrate the effectiveness of the proposed method. MBCnet has been deployed in image2product retrieval at Taobao app, and achieved obvious improvements."}, {"title": "2 Related Work", "content": "Click-Through Rate (CTR) Prediction: Capturing the complex feature interactions is a key to a successful CTR model. Early works combine Logistic Regression (LR) [11] and Factorization Machine (FM) [20] to capture feature interactions. While these methods either rely much on handcrafted features or focus on lower-order feature interactions, showing limitations in learning complex feature patterns [30]. Leveraging the superior capacity of deep neural networks (DNN) for feature extraction, deep learning techniques have been widely examined in the context of CTR prediction [6, 8, 26]. For example, Cheng et al. [6] combined shallow linear models and deep non-linear networks to promote the model's memorization and generalization ability for recommendation systems. Guo et al. [8] and Lian et al. [13] combines the power of factorization machines and deep learning for feature learning. Wang et al. [27] designed Deep and Cross Network (DCN) which can more efficiently and explicitly learn certain bounded-degree feature interactions. They later introduced a mixture of low-rank expert network into DCN to make the model more practical in large-scale industrial settings [25]. In FinalMLP [16], researchers observed that even two parallel MLP networks can achieve satisfied performance. They further proposed feature gating and interaction aggregation layers that can be easily plugged to make an enhanced two-stream MLP model. Some other CTR works investigate sequential user behavior modeling [4, 18, 33, 34], cross-domain knowledge transfer [2, 3, 5, 12, 19, 22] and multi-task learning [15, 23] for improved performance. These topics are beyond the scope of this paper, so we do not elaborate them here.\nExisting works have studied different ways to model feature interactions and they usually contain two parallel branches for enhanced CTR performance. Our MBCnet also works in a multi-branch style, but it additionally contains a novel cooperation scheme that enables different branches to learn from each other and uncover various feature interaction patterns. Moreover, we design the EFGC branch, which can better incorporate domain-expert knowledge and promote the model's memorization ability.\nEnsemble Learning: Ensemble learning is a machine learning technique that aggregates two or more learners (e.g., regression models, neural networks) to produce better prediction performance [17, 29]. The fundamental idea is that single machine learning model suffers from various limitations (e.g., high variance, high bias, low accuracy). However, combining different models can address these limitations and achieve higher accuracy [1]. For instance, bagging (e.g., random forest) reduces variance without increasing the bias, while boosting (e.g., XGBoost) reduces bias [7]. Generally speaking, an ensemble learner is more robust and able to perform better than the individual learners. In CTR prediction, different feature interaction techniques exhibit different modeling advantages. It is necessary to combine them together to better capture the complex patterns. Researchers have investigated the ensemble idea in CTR prediction from several aspects [14]. He et al. [9] utilized GBDT for non-linear feature transformation and feed them to the LR for final prediction. While Zhang et al. [32] used the FM embedding learned from sparse features to DNN. Researchers have found that simply replicating them cannot not yield satisfied performance. Ling et al. [14] investigated eight ensemble variants of GBDT, LR and DNN. They found that initializing the GBDT sample target with DNN's prediction score yielded the best performance in their business context. These methods do not work in an end-to-end style and require hard efforts on hand-craft tuning. Later, deep learning based ensemble CTR works [6, 8, 26] are becoming more popular. EnKD [35] specifically studies knowledge distillation technique for model ensemble in order to learn light and accurate student prediction model.\nCompared to EnKD [35], our MBCnet has one novel EFGC branch for feature interaction modeling and a completely different ensemble scheme. Detailed comparisons would be discussed later."}, {"title": "3 Method", "content": "3.1 Overview\nIn CTR prediction, given the raw input feature \\(x = \\{X_1, X_2, ..., X_M\\}\\) with M feature fields and user feedback label \\(y \\in \\{0, 1\\}\\), we aim to learn a mapping function \\(F : x \\rightarrow y\\) to predict user behaviors for online services. Usually, F denotes different deep neural networks (e.g., DCNv2 [27]) and \\(x_i\\) denotes the i-th feature field that can be categorical, multi-valued, or numerical feature of users or items. Especially in industrial scenarios, the number of feature fields M can be in hundreds or even thousands.\nMBCnet comprises three feature interaction branches: the proposed EFGC branch, the low-rank Cross Net branch, and the Deep branch. It also includes an innovative multi-branch cooperation scheme. The general architecture is given in Figure 2. Details are provided in subsequent sections.\n3.2 Expert-based Feature Grouping and\nCrossing (EFGC)\nThe EFGC branch is designed to improve the model's memorization capability for specific feature pairs, guided by domain-expert knowledge. It consists of two main components: the embedding layer, and the feature grouping and crossing module.\nEmbedding Layer: The embedding layer aims to encode raw input feature \\(x = \\{X_1, X_2, ..., X_M\\}\\) as embedding vectors. Each field has specific meaning such as userID, age and gender. Let \\(E_i\\) be the i-th embedding function of \\(x_i\\), the embedding layer is written as:\n\\(\\{e_1, e_2, ..., e_M\\} = \\{E_1(x_1), E_2(x_2), ..., E_M(x_M)\\}\\)  (1)\nwhere \\(e_i\\) is the embedding vector of \\(x_i\\).\nFeature Grouping: In CTR prediction, different feature fields have different physical meanings, and their combinations are crucial for personalized ranking. For instance, in e-commence search, combining user profile feature fields (e.g., userID, age) and item profile feature fields (e.g., itemID, price, sales volume) can reveal a user's general preferences regarding item price and quality. Conversely, combinations like item title and item category could show less contributions to personalized ranking. In essence, different feature field combinations have varying degrees of influence on CTR prediction. The common practice of concatenating \\(e_i\\) in Eq. 1 may fail to highlight certain specific combinations and might introduce irrelevant feature patterns into subsequent deep modules.\nIt is crucial to organize feature fields into distinct groups and perform feature crossing within these specific groups, driven by domain-expert knowledge. To achieve this, we can follow three"}, {"title": "Feature Crossing", "content": "After feature grouping, we can perform feature crossing with subsequent MLP layers as follows:\n\\(h^{EFGC}_i = f^{EFGC}(e^{Group}_i)\\)  (3)\nwhere \\(h^{EFGC}_i\\) denotes the output of i-th group feature crossing. \\(f^{EFGC}\\) is a non-linear MLP network. Finally, as shown in Figure 2, the output of our EFGC branch, denoted as \\(h^{EFGC}\\), is formulated as:\n\\(h^{EFGC} = F_C(h^{EFGC}_1 \\oplus ..., \\oplus h^{EFGC}_{N_g})\\)  (4)\nwhere \\(N_g\\) is the number of feature groups and \\(F_C\\) means one fully-connected layer to reduce the output dimension.\nRemark: Through splitting feature fields into groups and performing feature crossing within these groups, EFGC branch enhances the model's ability to memorize specific patterns and avoid undesired feature crossing. Additionally, it is flexible and can be extended to include more types of feature groups. We note feature engineering techniques also use specific feature fields to create new features, such as the count of clicks between userID and brand. Both feature engineering and EFGC employ domain-expert knowledge. However, the difference is feature engineering creates raw input features, whereas EFGC works in a learning-based manner, enabling the automatic discovery of patterns among feature fields within groups. They are not mutually exclusive and can be used together in practice.\n3.3 Deep Net and Low Rank Cross Net\nDeep Net: Deep neural networks, composed of stacked non-linear MLP layers, have the capability to implicitly learn feature interactions [6, 8, 13, 16, 25, 27]. The input of Deep Net is typically formed by concatenating all embedding fields as:\n\\(e^{Deep} = e_1 \\oplus e_2, ..., \\oplus e_M\\)  (5)"}, {"title": "Low Rank Cross Net", "content": "Despite the effectiveness of Deep Net in modern applications, it often struggles with capturing feature interactions, especially higher-order ones [27]. The popular low rank cross net(i.e., CrossNetV2) maps feature interactions in low-rank space and employs a mixture of experts architecture to improve its expressiveness. It is cost-efficient and expressive for feature interaction learning. The input of low-rank cross net branch is also the concatenation of all field embeddings, which means \\(e^{Cross} = e^{Deep}\\). Then the resulting low-rank cross net layer is formulated as:\n\\(h^{Cross} = F_C(f^{Cross}(e^{Cross}))\\) (7)\nwhere \\(F_C\\) denotes one MLP layer for dimension reduction. \\(f^{Cross}\\) denotes a mixture of low-rank cross network blocks. Due to the page limit, we do not elaborate it in detail here. For a detailed formulation, please refer to [27]. After the feature interaction learning of each branch, we further include a shared top layer \\(f_o()\\) to reduce parameters and regularize training. The output latent features of different branches are formulated as:\n\\(z^{EFGC} = f_o(h^{EFGC}), z^{Deep} = f_e(h^{Deep}), z^{Cross} = f_e(h^{Cross})\\) (8)\nwhere \\(z^{EFGC}, z^{Deep}\\) and \\(z^{Cross}\\) are also the input of logit layer.\n3.4 Multi-branch Cooperation Scheme\n3.4.1 Branch Co-teaching. Different branches of MBCnet possess distinct advantages and inherently focus on modeling various patterns. These patterns, in turn, are represented by the training samples. As a result, the learning capabilities of different branches can vary significantly across different samples. On particular samples, if a robust learning branch has been effectively trained, it can assist a comparatively weak learning branch in enhancing its performance. We formulate this collaboration idea as \"branch co-teaching\" objective function, whereby given specific samples, the strong branch provides guidance to the weak branch in training.\nSample Selection by Disagreement: To perform the above co-teaching, we first need to identify on which samples that one branch exhibits strong prediction performance while the other one shows weak performance. In other words, two branches have disagreement predictions on these samples. In CTR prediction, it is naturally to use binary cross entropy (BCE) loss to measure whether a model has accurate prediction on one sample. To be specific, we denote the predicted click probability of one branch as:\n\\(p^i = \\sigma(f^i(z^i))\\)  (9)\nwhere \\(i \\in \\{EFGC, Deep, Cross\\}\\), \\(\\sigma(\\cdot)\\) denotes the sigmoid function. \\(f^i(\\cdot)\\) and \\(z^i\\) denote the logit layer and latent feature of branch i, respectively. Therefore on one particular sample, we can identify strong and weak branches as:\n\\((p^i \\text{ is strong, } p^j \\text{ is weak}) \\text{ if } \\begin{cases} BCE(p^i, y) < -log(0.5) \\\\ BCE(p^j, y) > - log(0.5) \\end{cases}\\)  (10)"}, {"title": "Branch Co-teaching", "content": "\\((p^j \\text{ is strong, } p^i \\text{ is weak}) \\text{ if } \\begin{cases} BCE(p^j, y) < -log(0.5) \\\\ BCE(p^i, y) > - log(0.5) \\end{cases}\\) (11)\nwhere BCE means binary cross entropy loss. Since CTR prediction is usually regarded as a binary classification task, we employ \\(- log(0.5)\\) as a threshold to assess the learning progress of each branch\u00b3. When the loss value is less than \u2013 log(0.5), we consider the branch on these samples is learned effectively. Otherwise, we consider the branch is not learned well.\nBranch Co-teaching Objective Formulation: Given the above defined strong and weak branches, we can write the co-teaching loss as follows:\n\\(L_{BCT} = - \\frac{1}{C} \\sum_{i} \\sum_{j \\neq i} [I_{SG}^{ij}(p^i) log(p^j) + I_{SG}^{ji}(p^j) log(p^i)]\\)  (12)\nwhere \\(i, j \\in \\{EFGC, Deep, Cross\\}\\). SG means stop gradient. \\(I^{ij} \\in \\{0, 1\\}\\) is the indicator that only equals 1 when \\(p^i\\) is strong and \\(p^j\\) is weak shown in Eq. 11. Similarly, \\(I^{ji} \\in \\{0, 1\\}\\) is the indicator that equals 1 only when \\(p^j\\) is strong and \\(p^i\\) is weak shown in Eq. 10. C is the number of non-zero values calculated from \\(I^{ij}\\) and \\(I^{ji}\\), indicating how many disagreement pairs are used in \\(L_{BCT}\\). In this equation, we take the prediction of strong branches as soft label to guide the learning of weak branches on selected samples.\nRemark: The main idea of this branch co-teaching objective is that the teaching guidance is only conducted on selected samples where two branches show disagreed predictions. It is one important difference between this approach and the knowledge distillation in [35] which performs full sample supervision between teachers and students. Given specific samples, learning abilities of different branches vary and are gradually improved, which suggests that the branches cannot offer appropriate supervision for all samples, especially in the early training stage. Employing supervision on all samples could easily introduce noisy supervision, and this is why EnKD [35] uses pre-trained teachers and temperature scaling tricks to mitigate the effects of such noise. In contrast, our branch co-teaching operates in an end-to-end manner, focusing solely on samples with disagreeing predictions to minimize supervision noise.\n3.4.2 Moderate Differentiation. In MBCnet, multiple feature interaction branches make predictions for the same input sample. When these branches are supervised on the same samples, they have risk to learn identical feature interaction patterns. This redundancy can significantly hinder the model's capacity to explore a variety of feature interactions. To enhance the model's robustness, it is better to enable distinct branches to learn differentiated latent features, encouraging them to capture diverse interaction patterns even on the same sample. However, excessive differentiation between these latent features may disrupt the consistency of shared patterns across branches, potentially harming model performance. In this context, we propose \"Moderate Differentiation\" for the latent features \\(z^i\\) across branches. This approach promotes the exploration of diverse interaction patterns while ensuring a balance between differentiation and consistency.\nEquivalent Transformation of Latent Features: To achieve moderate differentiation between any two branches, we assume that their latent features satisfy equivalent transformation [10]. This"}, {"title": "Moderate Differentiation Regularization", "content": "indicates these two latent features have relations but also learning flexibility to ensure differentiation [5]. Specifically, if \\(z^i \\in R^{1 \\times d}\\) and \\(z^j \\in R^{1 \\times d}\\) are the latent features from two distinct branches on the same sample, they satisfy the equivalent transformation as\u2074:\n\\(z^i = z^jW^{ji}, z^j = z^iW^{ij}, s.t. W^{ji}W^{ij} = I_d\\) (13)\nwhere \\(W^{ij}, W^{ji} \\in R^{d \\times d}\\) are two learnable matrices and they are inverses of each other. To avoid feature collapse where multiple \\(z^i\\) collapse to one point after transformation, we employ orthogonal transformation, a specification of equivalent transformation. This means \\(W^{ij}\\) and \\(W^{ji}\\) are the transpose to each other, i.e., \\(W^{ji} = (W^{ij})^T\\). The orthogonal transformation has an advantage of preserving inner product of vectors, namely it can keep similarities of vectors after transformation.\nModerate Differentiation Regularization: Taking the above transformation constraint analysis into consideration, we can write the regularization loss for moderate differentiation of branches as:\n\\(L_{MDR} = \\frac{1}{Z} \\sum_{i} \\sum_{j \\neq i} \\{ ||z^iW^{ij} - z^j||_2 + ||z^jW^{ji} (W^{ij})^T - z^i||_2 \\}\\) (14)\nwhere the first term indicates the transformation relationship between \\(z^i\\) and \\(z^j\\), while the second term shows the constraint of orthogonal transformation on \\(W^{ij}\\). This loss not only enhances the model's capability but also facilitates the learning process by maintaining relevant feature representations across branches.\n3.4.3 Branch Fusion and Training Objective Function. We perform branch fusion to incorporate their knowledge and make better behavior predictions. Specifically, given latent feature of three branches, we have:\n\\(z^{fusion} = Avg\\_Pool(z^{EFGC}, z^{Deep}, z^{Cross})\\) (15)\nwhere \\(z^{fusion}\\) is the fused latent feature of different branches. Avg_Pool means average pooling of latent features in feature dimension. Then, the training objective for CTR prediction is formulated as:\n\\(L_{CTR} = \\sum_{i} BCE(\\sigma(f(z^i)), y)\\)  (16)\nwhere \\(i \\in \\{fusion, EFGC, Deep, Cross\\}\\) and BCE is the binary cross entropy loss. \\(f (\\cdot)\\) indicates the logit layer. \\(\\sigma(\\cdot)\\) denotes the sigmoid function. It is also worthwhile to mention that BCE loss is one choice for CTR prediction, it can be replaced with other CTR loss functions. To sum up, the overall objective function of MBCnet is defined as:\n\\(L = L_{CTR} + \\alpha * L_{BCT} + \\beta * L_{MDR}\\) (17)\nwhere \\(\\alpha\\) and \\(\\beta\\) are two hyper-parameters to weight the importance of loss terms. Notice that we use an arbitrary sample to succinctly demonstrate above objective function with minimal symbols. When using the objective function in practice, we compute the expectation of loss values of mini-batch samples during training.\nDiscussion: Popular CTR works [6, 8, 16, 27] investigate various feature interaction techniques for improved performance. However, each technique offers its own advantages, and relying on just one may not adequately capture the patterns in complex datasets, especially in industrial scenarios with enormous users and items. Our"}, {"title": "4 Experiment and Analysis", "content": "4.1 Experiment Setup\n4.1.1 Datasets. We conduct experiments on two industrial datasets: Pailitao-12month and Pailitao-24month. Both are collected from ourimage2product retrieval, also known as \"Pailitao\u201d at Taobao. Pailitao-12month and Pailitao-24month contain user search behaviors ranging from 2023-06-01 to 2024-05-31 and 2022-06-01 to 2024-05-31, respectively. For both datasets, there are various feature fields, including query image feature, user profiles, user statistical features, item image feature, item attributes, item statistical features and some other designed clustering and crossing features with expert knowledge. All experimental models utilize the same input features to ensure a fair comparison of their modeling differences. The statistics of datasets are provided in Table 2.\n4.1.2 Baselines. We leverage different strong algorithms for comparison including single branch methods and branch ensemble methods. The single branch methods are as follows. 1) DNN is a deep multi-layer perception model, 2) CrossNetV2 [27] is a low-rank version of cross net [25] to explicitly learn high-order feature interactions. The branch ensemble methods are listed as follows. 3) Wide&Deep [6] combines a wide set of cross-product feature transformations and deep neural networks. 4) DCNv2 [27] contains mixture-of-expert based low-rank feature crossing and DNN to learn bounded-degree feature interactions. 5) MMOE+ [15] originally is a multi-task learning model. We employ its mixture-of-expert encoder here to work as a baseline with multiple-expert ensemble. 6) EnKD [35] studies knowledge distillation technique for model ensemble. 7) FinalMLP [16] designs feature gating and interaction aggregation layers to make an enhanced two-stream MLP model.\n4.1.3 Parameter Settings. In our experiments, we split the datasets by chronological order, with the last seven-day data as validation and test set, and the rest is taken as the train set. For all methods, we set the latent dimension of last feature learning layer as 128. The validation performance is set as early-stop condition in training. In MBCnet, \\(\\alpha\\) and \\(\\beta\\) both equal 0.1 by hyper-parameter searching. In EFGC branch, hidden units of each group crossing are [1024,128]. In CrossNet branch, the number of experts is 2 and each expert has 2 layers with low rank dimension as 16. The hidden units of dimension reduction layer in EFGC and CrossNet are both 512. In Deep branch, the hidden units are [2048,1024,512,512,512]. Hidden"}, {"title": "4.2 Overall Comparison", "content": "4.2.1 Offline Comparison. We make model comparisons with recent competitive baselines on our two large-scale industrial datasets. The results are given in Table 3. From this table, we can observe that: 1) our MBCnet achieves obvious improvement over recent competitive baselines. It has a 0.61% and 0.72% AUC increase over the most competitive model on two datasets, which are impressive improvements in large-scale industrial data scenarios. 2) On Pailitao-24month dataset, single DNN outperforms other baselines. This trend is commonly observed in large-scale industrial settings, where the enormous data can overwhelm many modeling techniques. Even in this context, MBCnet still achieves superior improvements, demonstrating the effectiveness of our approach that leverages cooperative multi-branches to predict user behaviors.\n4.2.2 Online Production Deployment. We further evaluate the method through online A/B testing in image2product search of Alibaba Taobao system. In this experiment, the training data is our Pailitao-24month dataset. The baseline is our previously online serving model. Online evaluation metrics are real CTR, deal number by query, deal number by user and GMV. Results in Table 4 illustrate that our MBCnet has an absolute 0.09 point CTR increase, a relative 1.49% deal growth and a relative 1.62% GMV increase. In Figure 3, we also present the CTR improvements compared to online base model across query categories and user groups. The figure clearly shows that MBCnet achieves significant improvements from multiple evaluation perspectives, highlighting its effectiveness in our production system. Since August 2024, MBCnet has been fully deployed online, serving hundreds of millions of customers.\n4.3 Why Using Multi-Branch Cooperation\nIn this part, we conduct an experiment to demonstrate why using multi-branch cooperation in large-scale CTR prediction. In particular, we show some learning dynamics of different branches during training. Moreover, we compare the feature distribution before logit layer across branches with t-SNE [24]. The results are summarized in Figure 4, where \"EFGC\", \"Deep\u201d, \"CrossNet\" are three branches and \"MBC\" is the fused one.\nFrom Figure 4 (a)-(c), we see that: 1) The fused \"MBC\" model consistently achieves lower training log loss and validation log loss, along with a higher validation AUC throughout the training process. This indicates that a single branch has limited learning capacity, whereas MBCnet enhances its learning ability by cooperating multiple branches. As shown in Figure 4 (d), the output features from \"EFGC,\" \"Deep,\" and \"CrossNet\" branches are distinct from one another. In contrast, the fused \"MBC\" features are more evenly distributed across the feature space. This demonstrates that each branch has its own modeling patterns, and MBCnet can integrate their diverse capabilities. These findings emphasize the value of utilizing collaboration among branches to model the complex distributions of user behaviors. More experiments about why using multi-branch cooperation are given in Appendix A.1 and A.2.\n4.4 How to do Multi-Branch Cooperation\nWe also explore various approaches in multi-branch cooperation to validate the effectiveness of our proposed two principles: branch co-teaching and moderate differentiation. To examine the first principle, we modify our \"strong to weak\" learning scheme described in Eq. 10-12 to create two variants. The first variant, labeled \"no discrimination\", does not perform sample selection during co-teaching; instead, each branch can teach the other using the entire dataset. The second variant, labeled \"weak to strong\", reverses the teaching direction by having the weak branch instruct the strong"}, {"title": "4.5 Study of Different Model Parts", "content": "To evaluate the impact of each model component, we performed an ablation study by systematically removing specific modules, as detailed in Table 6. The results indicate a clear decline in performance on both datasets when any component is omitted. For instance, excluding the EFGC branch leads to a 0.94% decrease in AUC on Pailitao-24month. This result highlights the importance of our EFGC branch. The mechanism in EFGC highlights feature crossing within feature groups, which encourages the model to capture specific patterns and meanwhile reduces redundant crossing features. Additionally, removing the cooperation scheme, i.e., \"w/o LBCT, w/o LMDR\", results in a 0.79% reduction in AUC on Pailitao-12month. These outcomes demonstrate that each component positively contributes to the overall performance of our model.\n5 Conclusion and Future Work\nIn this paper, we introduced Multi-Branch Cooperation Network (MBCnet) for enhancing CTR prediction in large-scale online services. MBCnet effectively integrates three distinct branches (i.e., EFGC, low rank CrossNet and Deep Net) in capturing complex feature interactions. The proposed cooperation scheme, based on the principles of branch co-teaching and moderate differentiation, facilitates meaningful collaboration between branches. It also enables the model to leverage diverse patterns and improve overall prediction accuracy. Our extensive experiments, including an online A/B test, demonstrated significant improvements in CTR, deal growth, and GMV, validating its effectiveness and scalability in real-world applications.\nIn future work, we plan to incorporate more powerful branches to further improve the model's performance. Meanwhile, we use loss values to classify branches as strong or weak on particular samples. This method may produce unreliable loss measurements during the early training stage, potentially limiting the model's learning ability. We will develop more effective techniques for identifying strong and weak branches to achieve better prediction performance."}, {"title": "A Additional Experiments", "content": "A.1 Branch Difference at Neuron Level\nWe design an experiment to illustrate the branch difference at neuron level. In particular, MBCnet integrates latent features from multiple branches to execute the final CTR prediction task. These latent features are derived from the outputs of neurons, as depicted in Figure 6 (a). Each dimension of a feature value reflects the strength of a specific high-order feature."}]}