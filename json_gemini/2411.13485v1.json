{"title": "Utilizing Large Language Models to Synthesize Product Desirability Datasets", "authors": ["John D. Hastings", "Sherri Weitl-Harms", "Joseph Doty", "Zachary L. Myers", "Warren Thompson"], "abstract": "This research explores the application of large language models (LLMs) to generate synthetic datasets for Product Desirability Toolkit (PDT) testing, a key component in evaluating user sentiment and product experience. Utilizing gpt-40-mini, a cost-effective alternative to larger commercial LLMs, three methods, Word+Review, Review+Word, and Supply-Word, were each used to synthesize 1000 product reviews. The generated datasets were assessed for sentiment alignment, textual diversity, and data generation cost. Results demonstrated high sentiment alignment across all methods, with Pearson correlations ranging from 0.93 to 0.97. Supply-Word exhibited the highest diversity and coverage of PDT terms, although with increased generation costs. Despite minor biases toward positive sentiments, in situations with limited test data, LLM-generated synthetic data offers significant advantages, including scalability, cost savings, and flexibility in dataset production.", "sections": [{"title": "I. INTRODUCTION", "content": "Market demand is high for valuable and efficient systems and algorithms that can give concise advice or conclusions from big datasets [1]. A major challenge of information extraction is a frequent lack of sufficiently labeled data for training machine learning algorithms due to labor intensiveness and cost of acquiring labels [2]. Generating synthetic datasets is often quite useful, particularly for testing purposes in numerous areas of computing, including artificial intelligence, data mining, data visualization and software engineering [3]. The sizes of samples, rules, and attributes of the synthetic datasets can be readily adjusted to meet the needs of evaluating various learning algorithms [1]. With synthetic data, attributes such as missing values, dimensions, outliers, data format and type, trends, and patterns can be controlled [4]. Synthetic data is controlled by the researcher, whereas real data is messy and depend on the data collection approaches, such as survey data. Obtaining data can be a relevant problem because it may have privacy concerns, have associated costs, and require a prolonged time to acquire [5], [6]. Bol\u00f3n-Canedo et al. [7] found that synthetic datasets were useful because they offered a controlled environment for learning algorithms to evaluate classifiers."}, {"title": "II. BACKGROUND", "content": "According to ISO 9241-11 [15], usability is \"the extent to which a product can be used by a user to achieve a goal with effectiveness, efficiency, and satisfaction.\u201d Measuring usability is complex because it is intrinsic to the system or object under evaluation [16]. In product development, understanding implicit user sentiment is crucial for creating products that truly appeal to their intended audience, especially in user-centered design (UCD) [17].\nMapping user needs onto design specifications through sentiment analysis with deep learning tools [18] has promise. However, current methods are limited to document-level sentiment classification which is unable to capture attribute-level information [19]. Additionally, these methods require manually labeled data for training, and often classify sentiment into predefined categories which have limited implications for UCD designers [19]. In general, the supervised nature of most sentiment classification approaches limits their practical as they require extensive manual data labeling and annotation for training [19].\nUnderstanding desirability is important for improving and marketing products, but is difficult to measure [20]. Concepts like \"enjoyment\", \"fun\" or a product's desirability for purchase or use are not effectively captured by traditional usability studies [21]. To address situations in which sentiment data or user review data is lacking, tools such as surveys and the Microsoft Product Desirability Toolkit (PDT) [21], [22] can be used to evaluate user experiences. Measuring desirability along with usability offers a layer of qualitative impact of the system, hence providing a much better picture of the user experience [16].\nThe PDT is recognized as a valuable qualitative tool for evaluating user experience, and satisfaction for products [23]\u2013[30]. It aims to \"understand the illusive, intangible aspect of desirability resulting from a user's experience with a product\" [24]. However, while the PDT excels at gathering rich qualitative data, it lacks inherent quantitative abilities [29], [31].\nThe PDT asks users to select five adjectives from a given set that best describe their feelings about the experience along with providing an optional explanation of their word choices. By gathering this group of word/explanation pairs, the approach is designed to capture rich, qualitative data about user experiences and perceptions.\nThe advantages of using the PDT include \"1) it aims to avoid a bias toward the positive found in typical questionnaires (e.g., it has been found that if a respondent thinks that a survey intends to assess the quality of a product, they are likely to provide more positive answers about quality) and 2) it is able to more effectively uncover constructive negative criticisms in the guided interview\" [26]. The PDT is described as the closest tool that uses \"psychometric theory to create a user experience (UX)-relevant measure of product or service desirability\" [31]. The design of the PDT prompts users to tell insightful stories of their experience as they explain their word choices [23] and provides a rich set of qualitative data related to the user's implicit desirability of the product in question [32], and only takes about 5 minutes to administer [24].\nGretzel et al. [33] suggests that a UCD methodology is needed to effectively elicit embedded and implicit knowledge while remaining applicable to a larger sample of respondents. Similarly, Volo [34] highlighted the need for a UCD method capable of unobtrusively accessing individuals' experience while minimizing investigator and selection biases in its translations, measurements and analyses. These requirements demand an approach that is structured and controlled yet also flexible and open-ended [35]. A major barrier to UCD is the relative lack of formal mechanisms to translate individual user \"voices\" into the design of distinct product attributes that reflect divergent preferences [36]. Devising intelligent systems that can identify users' unique needs at scale and translate them into attribute-level design feedback and recommendations is essential for effective UCD processes [19].\nWeitl-Harms et al. [37] addressed the gap between qualitative sentiment data and quantitative analysis of PDT data for UCD by applying recent LLMs to PDT data. The findings showed that the LLMs were able to quantitatively measure product desirability at the same level that matched the expert labeled and annotated PDT qualitative user responses, providing promise for a new method for understanding desirability. The results from [37] show a promising novel method to quantitatively measuring implicit user desirability. The combination of using the PDT and LLM sentiment analysis limits investigator and selection biases as suggested by [34], while the method is controlled and structured but also open-ended and flexible as suggested by [35]. The question of whether or not their method of using LLMs for quantifying user desirability from PDT data addresses the scaling up issues noted by [19], [33] remains unanswered. Unfortunately, available PDT datasets are often small, such as n=29 in [38], n=10 in [39], n=50 in [40], and n=56 in [26]. This is likely due to challenges inherit in data collection from users as discussed above.\nBefore the method from [37] can be efficiently analyzed in its scaling to a larger sample of respondents, large PDT datasets are needed. Generating synthetic PDT data will be useful in answering that question."}, {"title": "III. METHODOLOGY", "content": "In this research, experiments utilized the gpt-40-mini model [41], accessed via the OpenAI API [42], to generate synthetic PDT datasets. Although [37] effectively employed gpt-40 for sentiment tasks, this research utilized gpt-40-mini as a significantly more cost-effective alternative for generating large datasets. Specifically, gpt-4o-mini's API cost is 6% of gpt-4o's, which makes it a better fit for large-scale data generation, assuming reasonable performance. A dataset size of 1000 hypothetical software product reviews was chosen to ensure adequate coverage of the original PDT word set [22], which contains 118 words in total.\nThree methods were tested to generate these hypothetical PDT datasets:\n1) Word+Review: A random target sentiment score between 0.0 and 1.0 (where 0 is the most negative sentiment and 1 is the most positive sentiment) was provided to gpt-40-mini, along with a list of 10 randomly selected PDT words. The gpt-40-mini LLM was tasked with selecting an appropriate word based on the target sentiment and generating a corresponding product review. This process was repeated 1000 times to achieve a uniform coverage of sentiments. The method could be tailored in future applications if specific sentiments (e.g., more negatively or positively skewed) are required.\n2) Review+Word: Because Word+Review tended to produce product reviews that reused the selected word, this approach aimed to reduce that bias by first generating the hypothetical product review in line with the random target sentiment, then selecting a word that matches the meaning of the review. This process was repeated 1000 times.\n3) Supply-Word: Because the first two methods sometimes picked words not included in the list of valid words, and did not use all 118 PDT words, this method randomly selected one of the 118 words. The word was supplied to gpt-40-mini, which was asked to score the sentiment expressed by the word, and produce a hypothetical software product review in line with that target score. This process was repeated 1000 times. Note: although this method didn't supply a target score, by randomly supplying words across the PDT word set, it was expected to get a reasonable coverage of all possible sentiments.\nIn order to measure the quality of the datasets, they were assessed according to how well the sentiment of the generated text aligned with the target scores, the diversity of text found within the sets, and the coverage of the PDT word list.\n1) Assessing Alignment (RQ2): After generating the datasets, the word/review pairs were scored using approaches described in [37], which had determined that LLMs more accurately score implicit sentiment of PDT data versus traditional sentiment analysis approaches. For Word + Review, initially gpt-40-mini was instructed to score the word, and then adjust that base score according to the sentiment found in the text of the review. The gpt-40-mini LLM sometimes struggled with the 'Base+Adjust' approach and would greatly adjust negative reviews upward when it didn't make sense to do so. Some effort was made to help gpt-40-mini better understand the 'Base+Adjust' approach (which it handled well in most cases), but in an effort to keep prompt sizes small for cost reasons and given that gpt-40 and Claude were known to handle this approach well [37], and that this research does not aim to establish that gpt-40-mini can properly score text using this specific approach - ultimately, all three methods were scored by asking gpt-40-mini to analyze the review collectively based on the word and review. The gpt-40-mini LLM exhibited better behavior with this approach, and it was then used across all three PDT datasets.\nBecause differences between the target and evaluated scores could be influenced either at the PDT generation phases (with larger differences being caused by text that is misaligned with the target score) or in the scoring phase (with sentiment score being misaligned with the actual sentiment of the text), Supply-Word was scored with both gpt-40-mini and gpt-4o in order to examine the differences in the scoring phase of the alignment calculations \u2013 these methods of scoring are referred to as Supply-Word-Mini and Supply-Word-4o, respectively.\n2) Scoring for Text Diversity (RQ3): Truly human representative text should be relatively diverse, so the datasets were scored for text diversity to see if any synthesis method might be better than another in terms of diversity. Each of the three datasets were scored based on the techniques described in [43] using their python diversity scoring package which includes:\n1) Compression ratio (CR). A simple measure calculated by dividing the size of the original text by the size of the compressed text. A higher score indicates greater redundancy, and thus less diversity, of the text.\n2) Part-of-speech compression ratio (CR-POS). Measures the redundancy of part-of-speech tag sequences by compressing them. A higher score indicates more repeated syntactic patterns, and thus less diversity in the text's structure.\n3) Homogenization Score: ROUGE-L (HS). Measures the similarity between pairs of texts based on the longest common subsequences. A higher score indicates more overlap and thus less diversity between the texts.\n4) N-gram diversity score (NDS). Calculates the ratio of unique n-grams to total n-grams in the text. A higher score indicates greater diversity, with fewer repeated sequences of words.\nShaib et al. [43] reported that compression ratio is highly correlated with other standard text diversity metrics, and in particular found CR-POS to be quite informative. Based on their recommendations informed by results and the available scoring options in their scoring package, each PDT dataset was scored using: CR, CR-POS, HS, and NDS with 4 as the maximum n-gram length."}, {"title": "IV. RESULTS & DISCUSSION", "content": "The three methods described in Section III were used to produce the corresponding synthetic PDT datasets, each with 1000 hypothetical software reviews. These datasets are publicly available on Zenodo [44]. Each of the three methods were fully capable of generating synthetic data with some slight differences in terms of alignment, cost, and diversity which are discussed in the following subsections.\nAlthough this research prompted gpt-40-mini only for software product reviews, the approach could be easily used for PDT dataset production for any product with minor adjustments to the prompt by simply telling it to give reviews for a different product. The prompt could be further adjusted to have the reviews look at particular aspects of the product.\n1) Word Choices: To be truly representative, each PDT dataset should include a reasonable coverage of each word in the PDT word list. Supply-Word used all 118 PDT as they were selected programmatically at random. However, neither Word+Review nor Review+Word included all words. Given that LLMs are trained on human text, it's not surprising that they could be biased toward selecting certain words. Word+Review used 111 of the 118 words. Review+Word used 108 of the 118 PDT words.\nIn terms of generating valid PDT data, if utilizing Word+Review and Review+Word for large scale production, code on the client side should verify that each word selected by gpt-40-mini is in the PDT word list. If not, the request to gpt-40-min (with the same score and word list) should be resent.\n2) Distribution of Synthetic Sentiment: Although Word+Review and Review+Word targeted random scores between 0.0 and 1.0, there is clearly bias in the evaluated scores toward the negative and especially positive ends of the range, with relatively few scores in the mid-range.\nThe tendency toward the ends of the range is even more pronounced for Supply-Word, whether scored by gpt-40-mini or gpt-40. Part of this was caused by the Supply-Word approach, which supplied a word to gpt-40-mini for scoring.\nGiven the minor differences between Supply-Word-Mini and Supply-Word-40, one could surmise that the text generation itself is the cause of bias, and not the scoring. Perhaps mixed sentiment is the hardest for gpt-40-mini to produce, or the model itself is naturally more positive. In addition, the PDT word list itself is inherently skewed with a 60% positive to 20% negative and 20% neutral sentiment word ratio (as acknowledged in the original PDT paper [21]), so the positive bias in the evaluated scores is not entirely surprising, although the influence of word list bias on evaluated scores is undetermined at this point. Producing a dataset with a more even distribution of evaluated scores (if that is a goal) might simply require weighting the target scores more heavily in certain areas, but that will require further experimentation.\nB. Alignment between Target and Evaluated Scores (RQ2)\nThe differences were generally low, meaning good alignment and suggesting that gpt-40-mini can do a reasonably good job of producing synthetic data via any of the described methods. The mean absolute differences for each method were Word+Review (0.128), Review+Word (0.091), and Supply-Word (0.103).\nAs shown, all methods had low absolute differences, with the supply-word-mini and supply-word-40 having nearly identical distributions.\nthe review. An example is Word: 'Flexible' and Review: \"I found the software to be quite flexible in adapting to my needs. While there were some larger mismatches between target and evaluated scores using gpt-40-mini (e.g., there were two instances with absolute differences of 0.45 for Supply-Word-Mini), in general the differences were low. In rare cases where the differences were greater, it appeared that gpt-40-mini got confused in sentiment scoring of long text reviews. Supply-Word-40 did not have this issue as gpt-4o is naturally better at understanding sentiment. However, scoring PDT data at scale would not be cost effective with gpt-40. Further, Supply-Word-4o is somewhat of an apples to oranges comparison because it compares the target scores of synthetic text generated by gpt-40-mini (with its own understanding of sentiment) with evaluated scores produced by gpt-40 (with a slightly different understanding of sentiment). While gpt-4o handled the cases where gpt-40-mini got confused, there were other medium differences across the dataset, presumably because the two models have a slightly different understanding of sentiment.\nMeasures include mean, variance, mean absolute difference (MAD), mean squared difference (MSD), Pearson and tStat. As shown, the Pearson correlation results between the target and evaluated sentiment scores highlight the effectiveness of the synthetic data generation methods used in this study. Word+Review, Review+Word, and Supply-Word demonstrated strong correlations, with values of 0.93, 0.96, and 0.97, respectively. These high correlations suggest that gpt-40-mini can accurately produce product reviews that align closely with the intended sentiment scores. Review+Word and Supply-Word, in particular, exhibited the best alignment, making them more reliable for applications requiring precise sentiment reflection. However, while the high Pearson scores indicate overall success, the slight variations in mean differences, the out of range tStat values, and occasional mismatches in sentiment scoring warrant further exploration to reduce these discrepancies in future work.\nC. Text diversity scoring (RQ3)\nWithin the Supply-Word dataset, 79.3% of the reviews started with \"I recently.\u201d Despite that redundancy, the low CR, CR-POS and HS scores, and high NDS score suggest greater text diversity for that dataset than the others. Likewise, the scores for the Word+Review dataset suggest greater text diversity than the Review+Word dataset. The caveat to these findings is the difference in dataset sizes, which may limit confidence in the meaning found in direct comparisons. More comparable results might be obtained if the datasets were of similar size (according to [43]).\nIn terms of practical use, CR, CR-POS and NDS ran quickly. However, HS is quite slow and not recommended for larger datasets. Even for the smaller datasets in this study, HS was quite slow on a basic desktop, taking 41 minutes for the Word+Review dataset, 1 hour 40 minutes for the Review+Word dataset, and 4 hours 43 minutes for the Supply-Word dataset."}, {"title": "D. PDT Dataset Costs (RQ4)", "content": "Table VI shows the costs of data generation in terms of time, tokens and gpt-4o-mini price per method. At the time of running the experiments in Oct 2024, gpt-4o-mini dollar costs were relatively low at $0.15/1M input tokens and $0.60/1M output tokens [45], making it more ideal for big data synthesis tasks compared to some of the other massive commercial LLMs. For example, gpt-40-mini represents a 94% savings over gpt-40 which costs $2.50/1M input tokens and $10.00/1M output tokens [45]. Scaling up generation, the estimated costs to produce a PDT dataset with 1M responses using Word+Review (the cheapest approach) would be ~$60 with gpt-40-mini, and ~$1000 with gpt-40.\nThe input sizes (i.e., the prompts sent to gpt-40-mini) were relatively similar in size. The output from Supply-Word was much more verbose, thus the overall cost for that method was higher. The prompt for Supply-Word (and the others) could likely be adjusted to make the output results less verbose, although longer text might be desired in some situations.\nThe times required to produce the data were relatively high, averaging 1.5-3.1 seconds per review. Although the cost to produce 1M items using Word+Review might be acceptably low, the estimated time to produce such a set for this fastest approach is ~17.72 days which may or may not be acceptable depending on the criticality of such an effort.\nThe environmental impacts of training and running AI models are an additional important cost consideration [46]-[48]. Crawford [49] argues that immediate action is needed to limit the environmental impacts of AI. The gpt-40-mini LLM is designed to be a smaller, more cost efficient model in comparison to larger models [41], which should normally translate to a reduced carbon footprint. However, OpenAI has not commented specifically on this aspect or the nature of the resources (and thus the associated environmental impacts) consumed in producing gpt-40-mini relative to other models.\nE. Ethical considerations\nThe production of synthetic data raises potential ethical considerations. First, because LLMs such as gpt-40-mini are trained on vast amounts of human text, which inherently contains biases, there will naturally be bias in the generated PDT data. This is easily seen in the words selected in methods 1 and 2, but will be present in the synthetic text as well. This bias might be present in the model's understanding of sentiment. Future efforts could look at options for reducing unintended biases.\nAlthough this is synthetic data, care must be taken to ensure the exclusion of personal identifiable or confidential information, particularly if the data will be produced at scaled and used publicly. The datasets produced in this work were small enough that simple inspection revealed no issues, but an automated approach would be needed at scale.\nThese synthetic datasets are intended solely for internal sentiment analysis research. If distributing such datasets, they should be clearly marked as synthetic data, and that no real-world decisions (e.g., related to marketing, product development, or public relations) should be made based on the content."}, {"title": "V. FUTURE WORK", "content": "This study provided an initial examination of the production of synthetic PDT datasets. Future work will investigate potential improvements in a variety of areas including text diversity as well as methods for improving alignment between target and evaluated scores. In addition, we will explore the generation of elements that may appear in actual human datasets (e.g., satire and emojis), along with appropriate measures to assess the human-like quality of such content. Also, we will generate synthetic data for the two software products described in [40] and [50], each of which has corresponding human PDT datasets [26], [30] for which direct comparisons can be made. We will also use the synthetic data to evaluate the method from [37]. We also plan to complete a detailed comparison with other existing synthetic data generation tools or methods to test the validity of the methods described in this work."}, {"title": "VI. CONCLUSION", "content": "This research demonstrates the potential of LLMs, specifically gpt-40-mini, as a powerful tool for generating synthetic datasets tailored for PDT sentiment analysis. By comparing three methods, Word+Review, Review+Word, and Supply-Word, the study reveals that LLMs can reliably produce synthetic product reviews with strong alignment to target sentiment scores and notable text diversity. While some challenges such as bias toward positive sentiments and occasional misalignment were found, the efficiency and scalability of gpt-40-mini make it a cost-effective option for large-scale synthetic data generation.\nIn addition, running simply on a basic desktop computer, the findings suggest that LLMs can be effectively employed in low-resource scenarios, providing valuable alternatives to real-world data acquisition, which can be costly, time-consuming, or even impossible. Future research will focus on refining sentiment alignment techniques, improving textual diversity, and addressing biases inherent in LLMs to further enhance the quality and applicability of synthetic PDT datasets. The utilization of LLMs in synthetic PDT data generation not only advances the field of sentiment analysis, but also offers significant opportunities for scaling data-driven product development efforts."}]}