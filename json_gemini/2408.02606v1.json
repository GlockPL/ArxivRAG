{"title": "Backward explanations via redefinition of predicates", "authors": ["L\u00e9o Sauli\u00e8res", "Martin C. Cooper", "Florence Dupin de Saint-Cyro"], "abstract": "History eXplanation based on Predicates (HXP), studies the behavior of a Reinforcement Learning (RL) agent in a sequence of agent's interactions with the environment (a history), through the prism of an arbitrary predicate [21]. To this end, an action importance score is computed for each action in the history. The explanation consists in displaying the most important actions to the user. As the calculation of an action's importance is #W[1]-hard, it is necessary for long histories to approximate the scores, at the expense of their quality. We therefore propose a new HXP method, called Backward-HXP, to provide explanations for these histories without having to approximate scores. Experiments show the ability of B-HXP to summarise long histories.", "sections": [{"title": "1 Introduction", "content": "Nowadays, Artificial Intelligence (AI) models are used in a wide range of tasks in different fields, such as medicine, agriculture and education [12, 8, 3]. Most of these models cannot be explained or interpreted without specific tools, mainly due to the use of neural networks which are effectively black-box functions. Numerous institutions [25, 9] and researchers [6, 15] have emphasized the importance of providing comprehensible models to end users. This is why the explainable AI (XAI) research field, which consists in providing methods to explain AI behavior, is flourishing. In this context, we propose a method for explaining AI models that have learned using Reinforcement Learning (RL).\nIn RL, the agent learns by trial and error to perform a task in an environment. At each time step, the agent chooses an action from a state, arrives in a new state and receives a reward. The dynamics of the environment are defined by the non-deterministic transition function and the reward function. The agent learns a policy \u03c0 to maximize its reward; this policy assigns an action to each state (defining a deterministic policy). Our eXplainable Reinforcement Learning (XRL) method is restricted to the explanation of deterministic policies.\nVarious works focus on explaining RL agents using a notion of importance. To provide a visual summary of the agent's policy, Amir and Amir [2] select a set of interactions of the agent with the environment (sequences) using \"state importance\" [4]. From a set of sequences, Sequeira et Gervasio propose to learn a set of information, to deduce interesting elements to show the user in the form of a visual summary [23]. Using a self-explainable model, Guo et al. determine the critical time-steps of a sequence for obtaining the agent's final reward [11].\nTo explain an RL agent, explanation must capture concepts of RL [16]. To this end, the HXP method [21], consists of studying a"}, {"title": "2 History eXplanation via Predicates (HXP)", "content": "An RL problem is modeled using a Markov Decision Process [24], which is a tuple $(S, A, R, p)$. S represents the state space and A the action space. A(s) denotes the set of available actions that can be performed from s. $R : S \\times A \\rightarrow R$ and $p : S \\times A \\rightarrow Pr(S)$ are respectively the reward function and the transition function of the environment. $p(s'|s, a)$ represents the probability of reaching state s', having performed action a from state s. $\\pi : S \\rightarrow A$ denotes a deterministic policy that maps an action a to each state s; thus, $\\pi(s)$ is the action performed by the agent in state s. Due to the Markovian nature of the process, transitions at different instants are independent and hence the probabilities given by the transition function p can be multiplied when calculating the probability of a scenario (sequence of states). In the following, we use the function next, based on the policy and the transition function described by p, to compute the next possible states (associated with their probabilities) given a set S of (state, probability) pairs: $next_{\\pi,p}(S) = \\{(s', pr \\times p(s'|s, a)) : (s,pr) \\in S, \\alpha = \\pi(s) and p(s'|s, a) \\neq 0\\}$. In order to compute the set of final states reachable at horizon k from a set of states S, using the agent's policy and the transition function p, the function $succ^k_{\\pi,p}$ is defined recursively by $succ^0_{\\pi,p}(S) = S$ and $succ^{n+1}_{\\pi,p}(S) = next_{\\pi,p}(succ^n_{\\pi,p}(S))$.\nHXPs [21] provide to the user important actions for the respect of a predicate d, given an agent's policy \u03c0, by computing an importance score for each action in the history. The language used for the predicate is based on the features that characterize a state. Each feature $f_i$ has a range of values defined by a domain $D_i$, the set of all features is denoted $F = \\{f_1, ..., f_n\\}$. The feature space is therefore $F = D_1 \\times ... \\times D_n$. The state space S is a subset of F. A predicate is"}, {"title": "3 Backward HXP (B-HXP)", "content": "The idea of B-HXP is to iteratively look for the most important action in the near past of the state that respects the predicate under study.\nWhen an important action is found, we look at its associated state s to define the new predicate to be studied. The process then iterates treating s as the final state with this new predicate. Indeed, by observing only a subset of the actions in the history (near past), the horizon for calculating importance scores is relatively small. In this sense, importance scores can be calculated exhaustively. The predicate is then modified so that actions can be evaluated with respect to a predicate that they can achieve within a shorter horizon. The following example will be used throughout this section to illustrate the method.\nExample 1. Consider the end of Bob's day. The history of Bob's actions is: [work, shop, watch TV, nap, eat, water the plants, read]. Bob's state is represented by 5 binary features: hungry, happy, tired, fridge, fuel. Fridge and fuel means respectively that the fridge is full and that the car's fuel level is full. Bob's final state is: (-hungry, happy, tired, fridge, fuel) (for the sake of conciseness, Bob's states are represented by a boolean 5-tuple. Thus, Bob's last state is: (0, 1, 1, 0, 0)). The environment is deterministic and the predicate under study is \"Bob is not hungry\". We are looking for the most im-portant actions for Bob not to be hungry. Starting from the final state, the most important action in the near past is 'eat'. We are interested in its associated state, i.e. the state in the history before doing the action 'eat', which is assumed to be (1, 0, 0, 1, 0). The new predicate deduced from this state is \"Bob is hungry and has a full fridge\". In the near past of (1,0,0,1,0), the 'shop' action is the most impor-tant one (among work, shop, watch TV and nap) for respecting this new predicate. To sum up, we can say that the reason that Bob is not hungry in the final state is that he went shopping (to fill his fridge) and then ate.\nBefore describing the B-HXP method in detail, we introduce some notations. \u0397 = (so, ao, 81, ..., ak\u22121, Sk) denotes a length-k history, with Hi = (si, ai) denoting the state and action performed at time i, and for i < j, H(i,j) denotes the sub-sequence H(i,j) = (Si, ai, ..., 8j). To define the near past of a state in H, it is necessary to introduce the maximum length of sub-sequences: 1. This length must be sufficiently short to allow importance scores to be calcu-lated in a reasonable time. The value of l depends on the RL problem being addressed, and specifically on the maximal number of possible transitions from any observable state-action pair, namely b. It follows that the lower b is, the higher l can be chosen to be.\nTo provide explanations for long histories, we need a way of defin-ing new intermediate predicates (such as Bob is hungry and the fridge is full in Example 1). For this we use Probabilistic Abductive eXpla-nations, shortened to PAXp [13]. The aim of this formal explana-tion method is to explain the prediction of a class c by a classifier"}, {"title": "Definition 1 (utility).", "content": "Given a predicate d, the utility $u_d$ of a set of\n(state, probability) pairs S is:\n$u_d(S) = \\sum_{(s,pr) \\in S,s \\models d} pr$"}, {"title": "Definition 2 (importance).", "content": "Given a predicate d, a policy \u03c0, a tran-sition function p, the importance score of a from s at horizon k is:\n$imp^k_{d,\\pi,p}(s, a) = u_d(succ^k_{\\pi,p}(S_{(s,a)})) - avg_{a'\\in A(s)\\{a\\}} u_d(succ^k_{\\pi,p}(S_{(s,a')}))$\nwhere avg is the average and $S_{(s,a)}$ is the support of p(.|s, a).\u00b9\nThe importance score lies in the range [-1, 1], where a positive (negative) score denotes an important (resp. not important) action in comparison with other possible actions. Its computation is #W[1]-hard [21], so it is necessary to approximate it, in particular by generating only part of the length-k scenarios with the succ function.\nTo handle long histories on problems where the number of possible transitions is large, i.e. large horizon k and large branching factor (denoted b hereafter), it is necessary to use approximate methods to provide explanations in reasonable time, at the expense of only approximating the importance scores. In the next section, we propose a new way of explaining histories in a step-by-step backward approach, which allows us to provide explanations in reasonable time for long histories, without having to approximate the calculation of scores. As we will see, this leads to other computational difficulties. The result is thus a novel method for the explanation of histories with different pros and cons compared to forward-based history explana-tion."}, {"title": "Definition 3 (PAXp [13]).", "content": "Given a threshold $\u03b4 \\in [0,1]$, a specific point $v \\in F$ and the class $c \\in K$ such that $\\kappa(v) = c$, $X \\subseteq F$ is a weak PAXp if:\n$Prop(\\kappa(x) = c | x_X = v_X) \\geq \u03b4$\nwhere $x_X$ and $v_X$ are the projection of x and v onto features X respectively and $Prop(\\kappa(x) = c | x_X = v_X)$ is the proportion of the states x \u2208 F satisfying $x_X = v_X$, that the classifier maps to c, in other words $|\\{x \\in F | x_X=v_X and \\kappa(x)=c\\}|/|\\{x \\in F | x_X=v_X\\}|$.\nThe set of all weak PAXp's for k(v) = c wrt the threshold \u03b4 is denoted WeakPAXp(\u043a, \u0475, \u0441, \u0431, F).\n$X \\subseteq F$isa PAXp if it is a subset-minimal weak PAXp. The set of all PAXp's for k(v) = c wrt the threshold \u03b4 is denoted \u0420\u0410\u0425\u0440(\u043a, \u0475, \u0441, \u0431, F)."}, {"title": "Definition 4 (B-HXP classifier).", "content": "Given a state s, a policy \u03c0, a transi-tion function p, a predicate d and a horizon k. The B-HXP classifier, denoted $K_{s,\\pi,p,d,k}$, is a function such that: for all x \u2208 S,\n$K_{s,\\pi,p,d,k} (x) = \n\\begin{cases}\nTrue & \\text{if } u_{d,\\pi,p}(x) \\geq u_{d,\\pi,\\rho}(s) \\\\\nFalse & \\text{otherwise}\n\\end{cases}$\nThis classifier is specific to B-HXP. The utility threshold value depends on the state s which is the state associated with the most important action in the sub-sequence studied. It is used to generate a predicate d' which reflects a set of states at least as useful as s (with a probability of at least 8) for the respect of d. The predicate d' can then be seen as a sub-goal for the agent in order to satisfy d."}, {"title": "Definition 5.", "content": "Given a state $s = (s_1,...,s_n) \\in S$, a B-HXP clas-sifier k on S, the predicate PAXpred associated with s for a given threshold & is:\n$PAXpred(s, \u03b4) = \\bigvee_{X \\in PAXp(\\kappa,s,True,\\delta,S)} \\left( \\bigwedge_{f_i \\in X} f_i = s_i \\right)$"}, {"title": "Proposition 1.", "content": "Given a policy \u03c0, a transition function p and predi-cate d, the importance score computation of an action a at horizon k from any state s for the respect of d, $imp^k_{d,\\pi,p}(s, a)$, is #P-hard."}, {"title": "Lemma 1.", "content": "Given a policy \u3160 and a transition function p, we assume that b, the maximum number of successor states is a polynomial function of the instance size (i.e. the number of bits necessary to specify the history together with \u3160, p and the predicate d). For a constant search horizon l, the computation of the importance score of any ac-tion a from any state s for the respect of d ($imp^l_{a,\\pi,p}(s, a)$) and the utility of the state s are in time which is polynomial in the size of the instance."}, {"title": "Lemma 2.", "content": "Given a state s, in the worst case, PAXpred returns a predicate of size which may be exponential in the size of s."}, {"title": "Lemma 3.", "content": "Given a state s, the computation of a locally-minimal PAXp is in FP#P (i.e. in polynomial time using a #P-oracle) and determining whether a given subset of features is a locally-minimal PAXp is #P-hard."}, {"title": "Proposition 2.", "content": "The B-HXP computation is in FP#P and is #P-hard."}, {"title": "Proposition 3.", "content": "Given a sequence of k states, with each state of size n, a policy \u03c0, a predicate d, a transition function p, a threshold \u03b4, a constant length l and b, the maximum number of successor states from any given state using which is assumed to be polynomial in the instance's size, the complexity of finding a B-HXP is FPT in n."}, {"title": "4.1 Description of the problems", "content": "Frozen Lake In this problem, the agent moves on the surface of a frozen lake (2D grid) to reach a certain goal position, avoiding falling into the holes. The agent can move in any of the 4 cardinal directions and receives a reward only by reaching the goal position. However, due to the slippery surface of the frozen lake, if the agent chooses a direction (e.g. up as in the second state of Figure 1), it has 0.6 probability to go in this direction and 0.2 to go towards each re-maining direction except the opposite one (e.g., for up, 0.2 to go left and 0.2 to go right, as occurred in the scenario of Figure 1 where the agent moved right in the third state after performing up in the second one). The agent's state is composed of 5 features: its position (P) and previous position (PP) on the map, the position of one of the two holes closest to the agent (HP), the Manhattan distance between the agent's initial position and his current position (PD), and the total number of holes on the map (HN). This last feature was added as a check: since it is a constant it should never appear in the redefined predicate, which was indeed the case.\nPredicates win, holes and region were studied. They respectively determine whether the agent reaches the goal, falls into a hole or reaches a pre-defined set of map positions.\nConnect4 The Connect4 game is played on a 6 by 7 vertical board, where the goal is to align 4 tokens in a row, column or diagonal. Two players play in turn. An agent's state is the whole board. An action corresponds to dropping a token in a column. The agent receives a reward only by reaching terminal states: 1, -1, 0.5 if the state repre-sents an agent's win, loss or draw respectively. As the agent does not\nDrone Coverage In this problem, four drones must cover (observe) the largest area of a windy 2D map, while avoiding crashing into a tree or another drone. A drone can move in any of the 4 cardinal di-rections or remain stationary. A drone's cover is the 3 \u00d7 3 square centered on it. A drone's cover is optimal when it does not contain any trees and there is no overlap with the covers of the other drones. Hence, its reward is based on its cover and neighbourhood. More-over, it receives a negative reward in the case of a crash. An agent's state is made up of its view, a 5 x 5 image centered on it, and its po-sition, represented by (x, y) coordinates. After an agent's action, the wind pushes the agent left, down, right, up according to the follow-ing distribution: [0.1, 0.2, 0.4, 0.3] unless the action is stop or the agent and wind directions are opposite (in these cases the wind has no effect)."}, {"title": "4.2 B-HXP examples", "content": "To provide B-HXPs in reasonable time, the sample parameter, which corresponds to the maximum number of states observed for a feature evaluation in the findLmPAXp algorithm [13], i.e. the pred-icate generation, was set to 10 in the following examples. In other words, to avoid an exhaustive search over F, the proportion in Defi-nition 3 was computed based on 10 samples.\nFrozen Lake In Figure 1, the agent is symbolized by a red dot, the dark blue cells are holes and the destination cell is marked by a star. Each action performed is represented by a red arrow which cor-responds to the direction chosen by the agent in the state described below. In the states associated with the most important actions, the genericity of the computed predicates is represented by colored cells. A predicate is said to be generic if it is respected by a large num-ber of different states. The first predicate is represented in purple, the second in green. A colored cell (purple or green) means that the predicate is valid for all the states whose position (P) is this cell on the grid (i.e. the states whose feature value P is this cell).\nA B-HXP (computed in 2 seconds) for a FL history is shown in Figure 1, with l=4, 8=0.7. Importance scores are presented in Ta-ble 1. The right action linked to the penultimate state 811 = {P = (7,8), PP = (6,8), HP = (6,7), PD = 13, HN = 10} is the most important in the first sub-sequence studied in order to win. The predicate PAXpredk($11,0.7) = (PD = 13), computed from 811 with 8 = 0.7, is named purple hereafter. The states described by the predicate are shown in purple in Figure 1. In the following sub-sequence, the down action linked to state s9 = {P = (5,8), PP = (5,7), HP = (6,7), PD = 11, HN = 10} is the most impor-tant to respect purple. The predicate PAXpredk (89,0.7) = (P = (5,8)) ^ (PP = (5,7)) ^ (HP = (6,7)), computed from 89, is named green (the states described are shown in green in Figure 1).\nConnect4 In Figure 2, we are interested in the actions of the agent playing the yellows tokens. The other player is considered as the en-vironment's response. Each action performed by the agent is rep-resented by a number which corresponds to the column number in which the agent has chosen to drop a token. The predicates displayed on the third line have been calculated from the states associated with the most important actions.\nDrone Coverage In Figure 4, we look at the actions of the blue drone. In the history, each drone is represented by a colored dot, and trees by green triangles. The coverage of a drone is represented by cells of the same color as the drone. Dark grey cells mean that there is an overlap of coverage between two drones."}, {"title": "5 Related work", "content": "XRL methods can be clustered according to the scope of the expla-nation (e.g. explaining a decision in a given situation or the policy in general), the key RL elements used to produce the explanation (e.g. states [10, 18], rewards [14, 1]), or the form of the explanation (e.g. saliency maps [10] sequence-based visual summaries [2, 23, 20]).\nOne approach consists in generating counterfactual trajectories (state-action sequences) and comparing them with the agent's trajec-tory. In [1], reward influence predictors are learnt to compare trajec-tories. The counterfactual one is generated based on the user's sug-gestion. In [28], a contrastive policy based on the user's question is built to produce the counterfactual trajectory. In the MDP context, Tsirtsis et al. generate optimal counterfactual trajectories that differ at most by k actions [26]. The importance score used in this paper is in line with the counterfactual view by evaluating scenarios where a different action took place at a given time.\nEDGE [11] is a self-explainable model. Like HXP, it identifies the important elements of a sequence. However, EDGE is limited to im-portance based on the final reward achieved, whereas HXPs allow the study of various predicates. In addition, HXP relies on the transi-tion function (which is assumed to be known) and the agent's policy to explain, whereas EDGE [11] requires the learning of a predictive model of the final-episode reward."}, {"title": "6 Conclusion", "content": "HXP (History eXplanation via Predicates) [21] is a paradigm that, for a given history, answers the question: \"Which actions were im-portant to ensure that the predicate d was achieved, given the agent's policy \u03c0?\". To do this, an importance score is computed for each ac-tion in the history. Unfortunately this calculation is #W[1]-hard with respect to the length of the history to explain. To provide explanations for long histories, without resorting to importance score approxima-tion, we propose the Backward-HXP approach: starting from the end of the history, it iteratively studies a subsequence, highlighting the most important action in it and defining a new intermediate predi-cate to study for the next sub-sequence (working backwards). The intermediate predicate is a locally-minimal PAXp corresponding to the state where the most important action took place.\nIn the experiments, we observed that the genericity of a predicate d and the search horizon l influence the importance scores. The more generic the predicate d, the greater the probability of finding states at an horizon l that respect d. Conversely, a less generic predicate makes it more difficult to evaluate an action. A too specific predicate d can lead to insignificant importance scores (close to 0) for the respect of d. In several histories, notably C4 ones, the predicates generated are not generic enough, leading to less interesting explanations.\nAlthough in the examples the actions identified as important are often related to the respect of the initial predicate, this is not always"}]}