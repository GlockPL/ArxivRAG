{"title": "Backward explanations via redefinition of predicates", "authors": ["L\u00e9o Sauli\u00e8res", "Martin C. Cooper", "Florence Dupin de Saint-Cyr"], "abstract": "History eXplanation based on Predicates (HXP), studies the behavior of a Reinforcement Learning (RL) agent in a sequence of agent's interactions with the environment (a history), through the prism of an arbitrary predicate [21]. To this end, an action importance score is computed for each action in the history. The explanation consists in displaying the most important actions to the user. As the calculation of an action's importance is #W[1]-hard, it is necessary for long histories to approximate the scores, at the expense of their quality. We therefore propose a new HXP method, called Backward-HXP, to provide explanations for these histories without having to approximate scores. Experiments show the ability of B-HXP to summarise long histories.", "sections": [{"title": "Introduction", "content": "Nowadays, Artificial Intelligence (AI) models are used in a wide range of tasks in different fields, such as medicine, agriculture and education [12, 8, 3]. Most of these models cannot be explained or interpreted without specific tools, mainly due to the use of neural networks which are effectively black-box functions. Numerous institutions [25, 9] and researchers [6, 15] have emphasized the importance of providing comprehensible models to end users. This is why the explainable AI (XAI) research field, which consists in providing methods to explain AI behavior, is flourishing. In this context, we propose a method for explaining AI models that have learned using Reinforcement Learning (RL).\nIn RL, the agent learns by trial and error to perform a task in an environment. At each time step, the agent chooses an action from a state, arrives in a new state and receives a reward. The dynamics of the environment are defined by the non-deterministic transition function and the reward function. The agent learns a policy \u03c0 to maximize its reward; this policy assigns an action to each state (defining a deterministic policy). Our eXplainable Reinforcement Learning (XRL) method is restricted to the explanation of deterministic policies.\nVarious works focus on explaining RL agents using a notion of importance. To provide a visual summary of the agent's policy, Amir and Amir [2] select a set of interactions of the agent with the environment (sequences) using \"state importance\" [4]. From a set of sequences, Sequeira et Gervasio propose to learn a set of information, to deduce interesting elements to show the user in the form of a visual summary [23]. Using a self-explainable model, Guo et al. determine the critical time-steps of a sequence for obtaining the agent's final reward [11].\nTo explain an RL agent, explanation must capture concepts of RL [16]. To this end, the HXP method [21], consists of studying a"}, {"title": "History eXplanation via Predicates (HXP)", "content": "An RL problem is modeled using a Markov Decision Process [24], which is a tuple (S, A, R, p). S represents the state space and A the action space. A(s) denotes the set of available actions that can be performed from s. R : S \u00d7 A \u2192 R and p : S \u00d7 A \u2192 Pr(S) are respectively the reward function and the transition function of the environment. p(s'|s, a) represents the probability of reaching state s', having performed action a from state s. \u03c0 : S \u2192 A denotes a deterministic policy that maps an action a to each state s; thus, \u03c0(s) is the action performed by the agent in state s. Due to the Markovian nature of the process, transitions at different instants are independent and hence the probabilities given by the transition function p can be multiplied when calculating the probability of a scenario (sequence of states). In the following, we use the function next, based on the policy and the transition function described by p, to compute the next possible states (associated with their probabilities) given a set S of (state, probability) pairs: next\u03c0,p(S) = {(s', pr \u00d7 p(s'|s, a)) : (s,pr) \u2208 S, \u03b1 = \u03c0(s) and p(s'|s, a) \u2260 0}. In order to compute the set of final states reachable at horizon k from a set of states S, using the agent's policy and the transition function p, the function succ\u03c0,p is defined recursively by $succ_{\\pi,p}^0(S) = S$ and $succ_{\\pi,p}^{n+1}(S) = next_{\\pi,p}(succ_{\\pi,p}^n(S))$.\nHXPs [21] provide to the user important actions for the respect of a predicate d, given an agent's policy \u03c0, by computing an importance score for each action in the history. The language used for the predicate is based on the features that characterize a state. Each feature fi has a range of values defined by a domain Di, the set of all features is denoted F = {f1, ..., fn}. The feature space is therefore F = D\u2081 \u00d7 ... \u00d7 Dn. The state space S is a subset of F. A predicate is"}, {"title": "Backward HXP (B-HXP)", "content": "The idea of B-HXP is to iteratively look for the most important action in the near past of the state that respects the predicate under study. When an important action is found, we look at its associated state s to define the new predicate to be studied. The process then iterates treating s as the final state with this new predicate. Indeed, by observing only a subset of the actions in the history (near past), the horizon for calculating importance scores is relatively small. In this sense, importance scores can be calculated exhaustively. The predicate is then modified so that actions can be evaluated with respect to a predicate that they can achieve within a shorter horizon. The following example will be used throughout this section to illustrate the method.\nExample 1. Consider the end of Bob's day. The history of Bob's actions is: [work, shop, watch TV, nap, eat, water the plants, read]. Bob's state is represented by 5 binary features: hungry, happy, tired, fridge, fuel. Fridge and fuel means respectively that the fridge is full and that the car's fuel level is full. Bob's final state is: (-hungry, happy, tired, fridge, fuel) (for the sake of conciseness, Bob's states are represented by a boolean 5-tuple. Thus, Bob's last state is: (0, 1, 1, 0, 0)). The environment is deterministic and the predicate under study is \"Bob is not hungry\". We are looking for the most important actions for Bob not to be hungry. Starting from the final state, the most important action in the near past is 'eat'. We are interested in its associated state, i.e. the state in the history before doing the action 'eat', which is assumed to be (1, 0, 0, 1, 0). The new predicate deduced from this state is \"Bob is hungry and has a full fridge\". In the near past of (1,0,0,1,0), the 'shop' action is the most important one (among work, shop, watch TV and nap) for respecting this new predicate. To sum up, we can say that the reason that Bob is not hungry in the final state is that he went shopping (to fill his fridge) and then ate.\nBefore describing the B-HXP method in detail, we introduce some notations. \u0397 = (so, ao, 81, ..., ak\u22121, Sk) denotes a length-k history, with Hi = (si, ai) denoting the state and action performed at time i, and for i < j, H(i,j) denotes the sub-sequence H(i,j) = (Si, ai, ..., 8j). To define the near past of a state in H, it is necessary to introduce the maximum length of sub-sequences: l. This length must be sufficiently short to allow importance scores to be calculated in a reasonable time. The value of l depends on the RL problem being addressed, and specifically on the maximal number of possible transitions from any observable state-action pair, namely b. It follows that the lower b is, the higher l can be chosen to be.\nTo provide explanations for long histories, we need a way of defining new intermediate predicates (such as Bob is hungry and the fridge is full in Example 1). For this we use Probabilistic Abductive eXplanations, shortened to PAXp [13]. The aim of this formal explanation method is to explain the prediction of a class c by a classifier"}, {"title": null, "content": "K by providing an important set of features among F. Setting these features guarantees (with a probability at least \u03b4) that the classifier outputs class c, whatever the value of the other features. A classifier maps the feature space into the set of classes: \u043a : F \u2192 K. We represent by x = (x1,...,xn) an arbitrary point of the feature space and v = (V1, ..., Vn) a specific point, where each vi has a fixed value of domain Di. [13] defines a weak PAXp as a subset of features for which the probability of predicting the class c = k(v) is above a given threshold \u03b4 when these features are fixed to the values in v. A PAXp is simply a subset-minimal weak PAXp.\nDefinition 3 (PAXp [13]). Given a threshold \u03b4 \u2208 [0,1], a specific point v \u2208 F and the class c \u2208 K such that \u043a(v) = c, X \u2286 F is a weak PAXp if:\n$Prop(\\kappa(x) = c | x_X = v_X) \\geq \\delta$\nwhere xx and vx are the projection of x and v onto features X respectively and Prop(\u043a(x) = c | xx = vx) is the proportion of the states x \u2208 F satisfying xx = vx, that the classifier maps to c, in other words |{x \u2208 F | xx=vx and \u043a(x)=c}|/|{x \u2208 F |\nxx=vx}.\nThe set of all weak PAXp's for k(v) = c wrt the threshold \u03b4 is denoted WeakPAXp(\u043a, \u0475, \u0441, \u0431, F).\nX\u2286Fisa PAXp if it is a subset-minimal weak PAXp. The set of all PAXp's for k(v) = c wrt the threshold \u03b4 is denoted PAXp(\u043a, \u0475, \u0441, \u0431, F).\nThe idea is to use PAXp's to redefine the predicate to be studied for the next sub-sequence as we progress backwards. In order to fit into the PAXp framework we define the classifier Ks,,p,d,k as a binary classifier based on the utility of the state s. We note ud,,p(s) = ua(succ,p({(s, 1)})), the utility of s wrt d given an horizon k, a policy \u03c0 and a transition function p. The class \u03ba\u03b5,\u03c0,p,d,k(x) of any state x is the result of a comparison between the utility of s and the utility of x for the respect of d.\nDefinition 4 (B-HXP classifier). Given a state s, a policy \u3160, a transition function p, a predicate d and a horizon k. The B-HXP classifier, denoted Ks,\u03c0,p,d,k, is a function such that: for all x \u2208 S,\n$\\mathcal{K}_{s,\\pi,\\rho,d,k}(x) = \\begin{cases}\nTrue & \\text{if } u_{d,\\pi,\\rho}(x) \\geq u_{d,\\pi,\\rho}(s) \\\\\nFalse & \\text{otherwise}\n\\end{cases}$\nThis classifier is specific to B-HXP. The utility threshold value depends on the state s which is the state associated with the most important action in the sub-sequence studied. It is used to generate a predicate d' which reflects a set of states at least as useful as s (with a probability of at least \u03b4) for the respect of d. The predicate d' can then be seen as a sub-goal for the agent in order to satisfy d.\nTo assess whether a subset X \u2286 F is a PAXp, it is necessary to calculate the utility of each state having this subset of features, which involves using the agent's policy and the environment transition function p. A weak PAXp is then a sufficient subset of state features which ensures that a state utility is greater than or equal to the utility of s with probability at least \u03b4. The new predicate is defined as the disjunction of every possible PAXp from s.\nDefinition 5. Given a state s = (81,...,Sn) \u2208 S, a B-HXP classifier k on S, the predicate PAXpred associated with s for a given threshold \u03b4 is:\n$PAXpred(s,\\delta) = \\bigvee_{X \\in PAXp(\\kappa,s,True,d,S)}\\left( \\bigwedge_{f_i \\in X} (f_i = s_i) \\right)$\nExample 1 (Cont). For this history, \u03b4 is set to 1 and l is 4. The first sub-sequence studied is: [nap, eat, water the plants, read]. The most important action relative to the achievement of \"Bob is not hungry\" is 'eat', with a score of, say, 0.5 and its associated state, say, (1,0,0,1,0). We extract a PAXp, which includes the features fridge and hungry set to 1. This means that, whatever the values of the other features, a state with fridge and hungry set to 1 has (with 100% probability) a utility greater than or equal to 0.5. Indeed, this is the only PAXp, so PAXPred is (fridge=1 \u2227 hungry=1). Now, we study the subsequence [work, shop, watch TV, nap], in which the most important action to achieve the new intermediate predicate is 'shop'.\nIn the backward analysis of H, the change of predicate allows us to look at a short-term objective to be reached, thus keeping the calculation of HXP reasonable. Our method is explained in pseudo-code in Algorithm 1. This algorithm allows us to go backwards through the history H, successively determining in each sub-sequence studied, the important action and its associated state predicate. The argmax function is used to find, in a given sub-sequence H(i,j), the most important action a, its associated state s, and its index in H. The latter is used to determine the next sub-sequence to consider. The PAXpred function is used to generate the new predicate to study in the next sub-sequence, based on Definition 5. The process stops when all actions have been studied at least once, or when the utility of the most important action in the current sub-sequence is 0. Finally, the algorithm returns a list of important actions and the different predicates found.\nAlgorithm 1 B-HXP algorithm\nInput: history H, maximal sub-sequence length l,\nagent's policy \u03c0, predicate d, transition function p,\nprobability threshold \u03b4, state space S\nOutput: important actions A, predicates D\n```\n\n```\nA \u2190 []; D \u2190 []; u \u2190 1\nimax \u2190 len(H); imin \u2190 max(0, imax - l)\nwhile imin \u22600 and u \u2260 0 do\ni, s, a \u2190 argmax i \u2208 [imin, imax] impd,\u03c0,\u03c1(si, ai)\nu \u2190 ud, \u03c0,\u03c1 (8)\nd\u2190 PAXpreds,\u03c0,\u03c1,d,1 (8, \u03b4)\nA.append(a); D.append(d)\nimax \u2190 i; imin\u2190 max(0, imax - l)\nend while\nreturn A, D\nWith B-HXPs, it is interesting to note that the number of actions to be presented to the user is not fixed. In the worst-case scenario, a user could end up with an explanation that refers to all actions as important. This would happen if it was always the last action in each subsequence which is the most important for achieving the current predicate. However, this problem was not observed in our experiments.\nB-HXP provides the user with an explanation even for long histories. Our motivation behind B-HXP is to reduce the complexity of calculating important actions in comparison with forward HXP. In the remainder of this section, we justify the choices made in defining B-HXP by analysing in detail the theoretical complexity of certain subproblems encountered by HXP and B-HXP.\nProposition 1. Given a policy \u3160, a transition function p and predicate d, the importance score computation of an action a at horizon k from any state s for the respect of d, impa,,p(s, a), is #P-hard.\n```"}, {"title": null, "content": "Proof. This complexity is a direct consequence of the result in [21]: given the length of the search horizon k as a parameter, calculating the importance of an action is #W[1]-hard.\nTo avoid this computational complexity, the search horizon l is chosen to be a small constant in the B-HXP calculation of importance scores. Using b to denote the maximum number of successor states from any given state (i.e. b = maxses |{s' \u2208 S : p(s|s, \u03c0(s)) > 0}), there are a maximum of bl scenarios generated, and hence we have the following lemma.\nLemma 1. Given a policy \u3160 and a transition function p, we assume that b, the maximum number of successor states is a polynomial function of the instance size (i.e. the number of bits necessary to specify the history together with \u3160, p and the predicate d). For a constant search horizon l, the computation of the importance score of any action a from any state s for the respect of d (impa,n,p(s, a)) and the utility of the state s are in time which is polynomial in the size of the instance.\nHowever, in comparison with forward HXP, it is necessary to compute intermediate predicates, in particular with PAXpred (Definition 5). It is interesting to note that the B-HXP classifier (Definition 4) \u043a used in PAXpred can be evaluated in polynomial time because it is based on the calculation of the utility of a state (the main component in the importance score computation). Unfortunately, from a state s, the number of PAXp's according to \u043a, can be exponential in the size of s.\nLemma 2. Given a state s, in the worst case, PAXpred returns a predicate of size which may be exponential in the size of s.\nTo support this assertion, consider the following example.\nExample 2. A state consists of n features f1,..., fn, initially all set to 0. An agent's ith action consists in assigning a value to fi from its domain Di, where Di = {0,1} (i = 1,...,n-1) and Dn = {0,1,n}. The transition function is deterministic and the agent obtains a reward only by reaching any state sgoal such that \u03a3i=1 fi \u2265 n + 21. Let the predicate d be goal, i.e. a predicate checking whether the agent reaches a state sgoal. Consider the state s = (1,...,1,0) after the agent has made n \u2212 1 assignments. Clearly, assigning n to fn establishes the predicate in one step."}, {"title": null, "content": "Moreover, any state with at least (n-1)/2 assignments of 1 among the first n \u2212 1 features, allows us to attain the goal in one step by assigning n to the last feature fn. Hence, the PAXp's of this predicate d from s and for l = 1, \u03b4 = 1 are precisely the subsets of (n-1)/2 literals of the form fi = 1 (1 < i < n\u22121). There are $\\binom{n-1}{(n-1)/2}$ such PAXp's, so the corresponding predicate PAXpred (for 8 = 1) is of exponential size.\nTo exhaustively determine an intermediate predicate PAXpred, therefore, exponential space in the size of the state s is required. One approximation to PAXpred is to calculate only one AXp (i.e. a PAXp with \u03b4 = 1). This reduces the problem to a more amenable co-NP problem [5] where only a counterexample state s' is needed to show that the set of features X C F is not an AXp. Unfortunately, this approximation yields predicates that are often too specific because of \u03b4 = 1. Instead, in order to provide sparser intermediate predicates, the considered approximation consists in computing one PAXp, or more precisely one locally-minimal PAXp. Locally-minimal PAXp's is a particular class of weak PAXp which are not necessarily subset-minimal. Formally, a set of features X \u2286 F is a locally-minimal PAXp if X \u2208 WeakPAXp(\u043a, v, c, d, F) and for all j \u2208 X, X\\{j} \u2209 WeakPAXp(\u043a, v, c, d, F).\nThe findLmPAXp algorithm [13] is used to calculate a locally-minimal PAXp. Although the approximation consists of calculating just one LmPAXp, it remains a hard counting problem. We first prove hardness before explaining why this is nevertheless an improvement over the hardness of forward HXP.\nLemma 3. Given a state s, the computation of a locally-minimal PAXp is in FP#P (i.e. in polynomial time using a #P-oracle) and determining whether a given subset of features is a locally-minimal PAXp is #P-hard.\nProof. We first show the inclusion in FP#P. From a specific state s, the search for a locally-minimal PAXp X \u2286 F begins by initialising X to F (the trivially-correct explanation consisting of all the features). Then, the following test is carried out for a feature fi \u2208 X: if fi is removed from X, does X remain a WeakPAXp? If so, fi is removed, otherwise retained. This test is performed for each feature of X. Determining whether a subset X C F is a WeakPAXp amounts to counting the number of states x which match s on X (i.e. xx = sx) that are classified as True (i.e. such that Kx,,p,d,k =True). Lemma 1"}, {"title": null, "content": "tells us that can be evaluated in polynomial time, so WeakPAXp belongs to #P. The search for a locally-minimal PAXp can thus be performed in polynomial time in the size of s, i.e. its number of features, using a #P-oracle WeakPAXp.\nTo prove #P-hardness, it suffices to give a polynomial reduction from PERFECT MATCHINGS which is a #P-complete problem [27]. Consider a graph G = (V, E) where V is the set of vertices and E is the set of edges. V is decomposed into two parts V\u2081 and V2 so that each edge has one end in V\u2081 and another in V2. In other words, G is a bipartite graph. A perfect matching is a set of edges such that no pairs of vertices has a common vertex.\nLet us define a set of n features fi \u2208 F such that each feature fi corresponds to a vertex, say i, in V1. We define the domain Di of each feature fi as the set of vertices in V2 that are related to i by E. In other words, an edge e \u2208 E is a possible assignment of a feature fi to a value vj. The feature space IF is modeled by G. Indeed, each point x = (f1,..., fn) \u2208 F can be obtained by assigning each feature in V\u2081 to a value in V2. Thus, a state x is represented by a set of edges ExCE. To compute WeakPAXp, the following classifier is used:\n$\\kappa(x) =\\begin{cases}\nTrue & \\text{if } \\forall i \\neq j, f_i \\neq f_j\\\\\nFalse & \\text{otherwise}\n\\end{cases}$\nThis classifier outputs True for any state x comprising a distinct valuation for each feature. In G, such a state x is represented by a set of edges ExCE in such a way that each pair of edges in Ex has no common vertices. Thus, Ex is a perfect matching. From a specific point v such that (v) = True, we are interested in knowing whether X = \u00d8 is a weakPAXp, for different values of \u03b4. Based on Definition 3, it is necessary to calculate the proportion of states classified as True to determine whether X is a weakPAXp. With X = 0, the total number of states that match X corresponds to the number of states in F. This is easily obtained by multiplying the domain-sizes |Di|, |F| = \u041fi=1|Di|. The number of states which match X and are classified by k to True, is obtained by counting the states which have a distinct valuation for each feature, i.e. by counting the perfect matchings in G. X = \u00d8 is a weakPAXp iff it is a (locally-minimal) PAXp. Thus, we have reduced the PERFECT MATCHINGS problem to a polynomial number of calls to the locally-minimal PAXp problem. Hence there is a polynomial-time Turing reduction from PERFECT MATCHINGS to the locally-minimal PAXp problem, which is therefore #P-hard.\nConsequently, the B-HXP complexity is deduced from the complexity of the computation of importance scores and the generation of the intermediate predicates.\nProposition 2. The B-HXP computation is in FP#P_and is #P-hard.\nProof. From Lemma 1 and Lemma 3, we deduce that the computational complexity of B-HXP is in FP#P and is #P-hard, in particular due to the complexity of the generation of a new predicate.\nThe computation of a B-HXP and the importance score computation in forward HXP are both #P-hard. But, it is important to note that counting states (in B-HXP) is less computationally expensive than counting scenarios (i.e. a sequence of k states).\nWe can provide a finer analysis by studying fixed-parameter tractability in n, the size of a state. A problem is fixed parameter tractable (FPT) with respect to parameter n if it can be solved by an algorithm running in time O(f(n) \u00d7 Nh)), where f is a function of n independent of the size N of the instance, and h is a constant [7]."}, {"title": null, "content": "Proposition 3. Given a sequence of k states, with each state of size n, a policy \u03c0, a predicate d, a transition function p, a threshold \u03b4, a constant length l and b, the maximum number of successor states from any given state using which is assumed to be polynomial in the instance's size, the complexity of finding a B-HXP is FPT in n.\nProof. Recall that the input includes a length-k history made up of k states of size n. An exhaustive search over all possible alternative length-k scenarios would require (nbl)k time (and hence is not FPT in n because of the exponential dependence on k, even if b is a constant). On the other hand, B-HXP need only exhaust over scenarios of constant length 1. The complexity of redefining the predicate by finding one LmPAXp is f(n)bl for some function f of n (the state size). This redefinition of the predicate must be performed at most k times, giving a complexity in O(f(n)b'k). It follows that B-HXP is FPT in n since b'k is a polynomial function of the size of an instance.\nIn short, B-HXP keeps the calculation of importance scores exhaustive, by cutting the length-k history into sub-sequences of length l, starting from the end. The most important action of a sub-sequence is retained, and its associated state is used to define d', the new predicate to study, using locally-minimal PAXp. d' is then studied in a new sub-sequence. This process is iterated throughout the history. The next section presents examples of B-HX\u03a1.\nThe calculation of new predicates is computationally challenging but is feasible (using certain approximations: calculating just one rather than all PAXp's and, as we will see later, the use of sampling rather than exhaustive search over feature space). The resulting method B-HXP provides a novel approach to explaining histories."}, {"title": "Experiments", "content": "The experiments were carried out on 3 RL problems: Frozen Lake (FL), Connect4 (C4) and Drone Coverage (DC) [20]. Q-learning [29] was used to solve the FL problem, and Deep-Q-Network [17] for C4 and DC. The agents' training was performed using a Nvidia GeForce GTX 1080 TI GPU, with 11 GB of RAM. The B-HXP examples were run on an HP Elitebook 855 G8 with 16GB of RAM (source code available [22]).\nThe first part of this section describes the problems and the studied predicates. The second part presents B-HXP examples. In the associated figures, the history is displayed over two lines. Each history is composed of 13 states and 12 actions. The action taken by the agent from a state is shown above it. Important actions and their states are highlighted by a green frame. The third line in figures 2,4 corresponds to the predicates generated during the B-HXP, where a dark grey cell means that this feature is not part of the predicate. In Tables 1, 2 and 3, the importance scores given are w.r.t. either the initial predicate or the intermediate ones."}, {"title": "4.1 Description of the problems", "content": "Frozen Lake In this problem, the agent moves on the surface of a frozen lake (2D grid) to reach a certain goal position, avoiding falling into the holes. The agent can move in any of the 4 cardinal directions and receives a reward only by reaching the goal position. However, due to the slippery surface of the frozen lake, if the agent chooses a direction (e.g. up as in the second state of Figure 1), it has 0.6 probability to go in this direction and 0.2 to go towards each remaining direction except the opposite one (e.g., for up, 0.2 to go left and 0.2 to go right, as occurred in the scenario of Figure 1 where the agent moved right in the third state after performing up in the second one). The agent's state is composed of 5 features: its position (P) and previous position (PP) on the map, the position of one of the two holes closest to the agent (HP), the Manhattan distance between the agent's initial position and his current position (PD), and the total number of holes on the map (HN). This last feature was added as a check: since it is a constant it should never appear in the redefined predicate, which was indeed the case.\nPredicates win, holes and region were studied. They respectively determine whether the agent reaches the goal, falls into a hole or reaches a pre-defined set of map positions.\nConnect4 The Connect4 game is played on a 6 by 7 vertical board, where the goal is to align 4 tokens in a row, column or diagonal. Two players play in turn. An agent's state is the whole board. An action corresponds to dropping a token in a column. The agent receives a reward only by reaching terminal states: 1, -1, 0.5 if the state represents an agent's win, loss or draw respectively. As the agent does not"}, {"title": null, "content": "know the next move of the opponent, transitions are stochastic.\nFive predicates were studied, including the obvious win and lose. For the other three, the initial state is compared with the final states of the generated scenarios. The predicate control mid-column is satisfied when the agent has more tokens in the middle column. The predicates 3 in a row and counter 3 in a row are satisfied respectively when the agent obtains more alignments of 3 tokens on the board, and prevents the opponent from obtaining more alignments of 3 tokens on the board.\nDrone Coverage In this problem, four drones must cover (observe) the largest area of a windy 2D map, while avoiding crashing into a tree or another drone. A drone can move in any of the 4 cardinal directions or remain stationary. A drone's cover is the 3 \u00d7 3 square centered on it. A drone's cover is optimal when it does not contain any trees and there is no overlap with the covers of the other drones. Hence, its reward is based on its cover and neighbourhood. Moreover, it receives a negative reward in the case of a crash. An agent's state is made up of its view, a 5 x 5 image centered on it, and its position, represented by (x, y) coordinates. After an agent's action, the wind pushes the agent left, down, right, up according to the following distribution: [0.1, 0.2, 0.4, 0.3] unless the action is stop or the agent and wind directions are opposite (in these cases the wind has no effect).\nTen predicates for the DC problem were studied (local and global versions of): perfect cover, maximum reward, no drones, crash and region. Local versions concern a single agent, whereas the global versions concern all agents. Local versions of predicates allow to"}, {"title": "4.2 B-HXP examples", "content": "To provide B-HXPs in reasonable time, the sample parameter, which corresponds to the maximum number of states observed for a feature evaluation in the findLmPAXp algorithm [13], i.e. the predicate generation, was set to 10 in the following examples. In other words, to avoid an exhaustive search over F, the proportion in Definition 3 was computed based on 10 samples.\nFrozen Lake In Figure 1, the agent is symbolized by a red dot, the dark blue cells are holes and the destination cell is marked by a star. Each action performed is represented by a red arrow which corresponds to the direction chosen by the agent in the state described below. In the states associated with the most important actions, the genericity of the computed predicates is represented by colored cells. A predicate is said to be generic if it is respected by a large number of different states. The first predicate is represented in purple, the second in green. A colored cell (purple or green) means that the predicate is valid for all the states whose position (P) is this cell on the grid (i.e. the states whose feature value P is this cell).\nA B-HXP (computed in 2 seconds) for a FL history is shown in Figure 1, with l=4, 8=0.7. Importance scores are presented in Table 1. The right action linked to the penultimate state 811 = {P = (7,8), PP = (6,8), HP = (6,7), PD = 13, HN = 10} is the most important in the first sub-sequence studied in order to win. The predicate PAXpredk(811,0.7) = (PD = 13), computed from 811 with 8 = 0.7, is named purple hereafter. The states described by the predicate are shown in purple in Figure 1. In the following subsequence, the down action linked to state s9 = {P = (5,8), PP = (5,7), HP = (6,7), PD = 11, HN = 10} is the most important to respect purple. The predicate PAXpredk (89,0.7) = (P = (5,8)) \u2227 (PP = (5,7)) \u2227 (HP = (6,7)), computed from 89, is named green (the states described are shown in green in Figure 1).\nWe note that purple describes more states than green. The latter is not generic enough, which is reflected in the importance scores, which are close to 0: whatever the action, it is unlikely to respect this predicate after 4 time steps. The entire history is not explored when calculating the B-HXP, as the utility of the last state selected s6 is 0. The selected actions form a meaningful explanation when we look at the predicates studied. However, the redefined predicates fairly quickly become very specific and probably of little help in explaining why the agent won."}, {"title": null}]}