{"title": "Decoding Report Generators: A Cyclic Vision-Language Adapter for Counterfactual Explanations", "authors": ["Yingying Fang", "Zihao Jin", "Shaojie Guo", "Jinda Liu", "Yijian Gao", "Junzhi Ning", "Zhiling Yue", "Zhi Li", "SIMON LF WALSH", "Guang Yang"], "abstract": "Despite significant advancements in report generation methods, a critical limitation remains: the lack of interpretability in the generated text. This paper introduces an innovative approach to enhance the explainability of text generated by report generation models. Our method employs cyclic text manipulation and visual comparison to identify and elucidate the features in the original content that influence the generated text. By manipulating the generated reports and producing corresponding images, we create a comparative framework that highlights key attributes and their impact on the text generation process. This approach not only identifies the image features aligned to the generated text but also improves transparency but also provides deeper insights into the decision-making mechanisms of the report generation models. Our findings demonstrate the potential of this method to significantly enhance the interpretability and transparency of AI-generated reports.", "sections": [{"title": "Introduction", "content": "The automated and precise interpretation of chest X-rays represents a transformative potential for improving healthcare outcomes. Over the past three years, substantial efforts have been invested in refining the language generation capabilities, aligning visual and linguistic features, and increasing the accuracy of clinical report findings. The advent of large language models (LLMs) has introduced further advancements in report generation, prioritizing linguistic precision and sophistication (Lee et al. 2023; He et al. 2024; Liu et al. 2024). Despite these enhancements, the reports generated by these models often emerge as cryptic outputs from a \"black box\", leaving users with little understanding of the underlying processes. Furthermore, the proliferation of diverse models leads to inconsistent reports even when analyzing identical X-rays, raising concerns about the reliability of these automated systems. This variability and lack of transparency have impeded their broader adoption in clinical settings (Hertz et al. 2022; M\u00fcller, Kaissis, and Rueckert 2024).\nIn response, numerous studies have turned to existing Explainable AI (XAI) techniques to uncover the visual features influencing generated content, thereby aiming to bolster the interpretability and reliability of these black-box systems. However, the most widely used XAI methods in this field, which typically produce heatmaps through the attention maps (Liu et al. 2019; Cao et al. 2023b; Chen et al. 2020) or GradCAM method (Alfarghaly et al. 2021; Spinks and Moens 2019; Wang et al. 2024), struggle to precisely locate relevant visual features, often highlighting areas irrelevant to the actual findings.\nTo address these shortcomings, pioneering research (Tanida et al. 2023) has introduced an interactive report generation method that enhances interpretability through anatomically precise annotations. This method provides bounding boxes that delineate anatomical regions associated with report findings, thereby offering a clearer localization and understanding of the report content. Yet, this approach is constrained by its reliance on the pretrained anatomy detection model and extensive fine-grained labeled datasets (paired frame and report) for training, which are costly to prepare and limit scalability, making it less generalised to other report generators and dataset."}, {"title": "Related work", "content": "The most widely applied explanation methods are post-hoc and model-agnostic, meaning they can be generalized to explain different models. Popular methods include activation-based methods, backpropagation-based methods, and perturbation-based methods. Among these, counterfactual explanation, a perturbation-based method, aims to provide counterfactual images that elicit the opposite decision from a pretrained black-box model with minimal, human-identifiable alterations to the original image. Comparing the original image with its counterfactual counterpart facilitates the identification of critical features influencing the model's predictions.\nWith the recent evolution of generative AI models, counterfactual explanations have excelled in producing highly realistic counterfactual examples with subtle alterations, enabling model users to detect differences between similar classes - a common challenge in medical image classification tasks such as X-ray (Atad et al. 2022; Mertes et al. 2022; Singla et al. 2023; Schutte et al. 2021; Sankaranarayanan et al. 2022), Magnetic Resonance Imaging (MRI) (Tanyel, Ayvaz, and Keserci 2023; Fontanella et al. 2023), ultrasound (Reynaud et al. 2022), and histopathology images (Karras et al. 2020; Schutte et al. 2021). Over time, counterfactual generation methods have evolved from variational autoencoders (Rodriguez et al. 2021) and generative adversarial networks (Lang et al. 2021; Atad et al. 2022) to diffusion models (Rombach et al. 2022)."}, {"title": "Explainability in report generation models", "content": "The architectures of report generator models often incorporate cross-attention mechanisms, which are commonly used to enhance the explainability of these models. Most works in report generation demonstrate the explainability of their models by identifying the most relevant image features corresponding to specific word embeddings within the cross-attention architecture, thereby providing an explanation for the generated keywords (Wang et al. 2023; Cao et al. 2023b; Chen et al. 2023). However, the heatmaps generated by these methods often provide only coarse localization of relevant areas for the text and fail to offer fine-grained localization of detected abnormalities. Some methods (Alfarghaly et al. 2021; Spinks and Moens 2019; Wang et al. 2024) have applied other heatmap explanation techniques, such as GradCAM, to provide visual explanations. Nevertheless, these methods suffer from similar issues of lower localization accuracy.\nIn contrast to these approaches, Tanida et al. (2023) introduced a region-guided radiology report generation (RGRG) method, which significantly enhances the interpretability and transparency of generated reports by basing the report on detected anatomical areas. However, this approach requires the preparation of a large paired dataset of anatomical areas and corresponding reports for both the anatomical detection model and the report generation model. This necessity for extensive manual labeling increases costs and limits the ability to incorporate larger training datasets. While the method achieves higher explainability in the generated reports, it is not easily transferable to other advanced report generation models. In this paper, we aim to develop a model-agnostic explanation method that achieves localization capabilities similar to RGRG, but without the need for extensive manual labeling and applicable to various existing report generation models to enhance the explainability and transparency of their generated reports."}, {"title": "Text-controlled image editing", "content": "In recent years, text-guided image editing has gained increasing interest due to the convenience of editing images through natural language input (Lyu et al. 2023; Kim, Kwon, and Ye 2022; Patashnik et al. 2021; Abdal et al. 2022; Cao et al. 2023a; Brack et al. 2023).\nA significant body of work utilizes the alignment between text and image embeddings within a pretrained large vision-language model like CLIP (Radford et al. 2021). These methods leverage changes in the text embeddings before and after editing and map these changes to the image embeddings to generate the edited image. For instance, Kim, Kwon, and Ye (2022) fine-tunes generative models using the CLIP loss to guide image distance, while some approaches (Patashnik et al. 2021; Abdal et al. 2022; Lyu et al. 2023) operate in a latent space to learn these changes without altering the network parameters.\nAnother class of methods focuses on more efficient text-guided image editing using pretrained text-to-image generation models like Stable Diffusion (Rombach et al. 2022). These approaches directly edit images during the forward pass without fine-tuning the network (Brooks, Holynski, and Efros 2023; Hertz et al. 2022; Liang et al. 2023). However, a challenge with these methods is that minor changes in prompts do not necessarily guarantee minor changes in the generated images. To address this, Hertz et al. (2022) introduced a prompt-to-prompt alignment method to achieve localized edits in the generated image, while Brooks, Holynski, and Efros (2023) further improved this by training an instructive editing network using paired images generated from it.\nThe key distinction of our proposed editing method is its objective. Rather than simply aligning the image with its semantic meaning, our goal is to manipulate the image to produce a specific altered report from the target report generator, providing an explanation for the generated text. While traditional methods like CLIP-based editing rely solely on text-image alignment, they do not ensure the desired report change when processed by the report generator, as illustrated in Fig. 1. Therefore, our approach adopts the second strategy to achieve this targeted manipulation."}, {"title": "Method", "content": "The overall framework for utilizing the proposed Cyclic Vision-Language Adapter (CVLA) to generate counterfactuals and explain a report produced by a given report generator is illustrated in Fig. 3 (A). Next, we will detail the establishment of the CVLA and then describe its ability to provide counterfactual examples for explaining the generated report from a target report generator."}, {"title": "Cyclic Vision Language Adapter", "content": "The proposed Cyclic Vision-Language Adapter (CVLA) module comprises an off-the-shelf report generator that produces reports from a query X-ray, and an image generator, which is specifically tailored to generate the images from the findings generated by the given report generator. The term \"cyclic\" refers to the bidirectional generation capability between these two modalities, particularly their adaptive ability to changes on either side. Specifically, when manipulations are applied to the text, corresponding changes will be reflected in the generated X-rays (referred to as the manipulated images). Furthermore, these changes in the manipulated images can be verified by the consistent changes observed in the regenerated text derived from the manipulated X-rays, which has been highlighted by the dashed arrows in Fig. 1. To achieve the \u201ccyclic\u201d capability, the image generators in the CVLA are designed to meet three specific targets, as detailed in Fig. 2: (a) Reconstruction ability, which ensures the query images can be accurately reconstructed from the generated report; (b) Minimal manipulation resulting from the textual alterations; and (c) Ensuring that the generated manipulated image produces the expected manipulated report. To effectively achieve these targets, we implemented the following adaptations to our model, based on the advanced capabilities of a text-to-image stable diffusion model (Rombach et al. 2022)."}, {"title": "Dataset preparation", "content": "To ensure that the image generator serves to explain the pretrained report generator's results rather than merely manipulating the image based on prior knowledge of the manipulated words, the CVLA is designed to reconstruct the original query X-rays under the guidance of the generated report from the target report generator. It then generates the manipulated image by altering the generated reports.\nIt is noteworthy that while training the image generators with the ground truth reports of the X-ray images can also result in editing abilities, even with changes more aligned with the word meanings, the reconstructed image from the generated report may significantly differ from the initial image, especially when the generated report deviates from the image's ground truth report. As seen in the example in Fig. 4, the 'GT' model's reconstructed image from its generated report enhances the feature of 'cardiomegaly', which is present only in the generated report and not in the real report. When 'cardiomegaly' is removed from the text to observe its influence on the image, the model successfully removes cardiomegaly compared to the reconstructed image. However, it shows minimal differences when compared to the initial query image, failing to explain the specific features in the initial image that led to the report generator identifying 'cardiomegaly'.\nFor this reason, we inferred the target report generators on the dataset they were trained on, pairing the initial X-ray image with the inferred results on this dataset. We then trained the model to reconstruct the initial image under the conditions of this report. Fig. 4 (b) shows that the model trained with this tailored dataset for the target report generator ensures accurate image reconstruction from the generated report. It further edits the image by removing keywords from the generated reports, enabling the detection of differences between the edited image and the initial image based on changes to the input prompt.\nFurthermore, to enable the image generators to detect the features of major abnormalities identified in the generated report, we classify the generated reports into 13 abnormalities (Enlarged Cardiomediastinum, Cardiomegaly, Lung Opacity, Lung Lesion, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Effusion, Pleural Other, Fracture, Support Devices) using the pretrained CheXbert classifier (Smit et al. 2020). We then reorganize and align the prompt paired with the image as \"The lung with the abnormalities of X\", where X represents abnormalities identified in the generated reports by the target report generators. The data preparation process is illustrated in Fig. 3."}, {"title": "Training objective", "content": "Our training objective follows the Stable Diffusion training procedure, which is given as below:\n$L_{LDM} := E_{[\u03f5\u2212\u03f5_\u03b8 (z_T,\u03c4,\u03b8_T) ]^2}$ (1)\nwhere $z_T$ is the encoded feature of the initial query X-ray image from the encoder of a variational autoencoder, $x_0$, added with a Gaussian noise \u03f5, \u03c4 is the text encoder than transforms the prompt to the text embedding. During our training, we leverage the pretrained model weight for the text embedding and image autoencoder modules by a stable diffusion model pretrained on MIMIC (Liang et al. 2023). During training, we initialize the weight of UNET architecture by the stable diffusion pretrained weight 'CompVis/stable-diffusion-v1-4' and freeze the parameter in the image autoencoder."}, {"title": "Real image manipulation", "content": "To enable the CVLA to explain the generated report of a real X-ray query, we employ Denoising Diffusion Implicit Models (DDIM), a non-stochastic variant of Denoising Diffusion Probabilistic Models (DDPMs), as the sampling process for image generation. DDPM learns to generate data samples through a sequence of denoising steps, which is given by:\n$x_{t\u22121} = \\sqrt{\u03b1_{t\u22121}} (\\frac{x_t\u2212\\sqrt{1\u2212\u03b1_t}\u03f5_\u03b8(x_t,t)}{\\sqrt{\u03b1_t}}) + \\sqrt{1 \u2212 \u03b1_{t\u22121} \u2212 \u03c3_t^2}\u00b7\u03f5_\u03b8(x_t, t) + \u03c3_te_t$ (2)\nwhere $e_t \\sim N(0, I)$ represents a standard normal distribution, and $\u03c3_t$ controls the stochasticity of the forward process.\nSharing the same optimization objective as DDPM, DDIM sets $\u03c3_t$ in Eqn. (2) to zero, allowing for a deterministic reconstruction without randomness. Therefore, to reconstruct the initial image, we approximate the noise using DDIM Inversion, which reintroduces noise to the image through the diffusion model."}, {"title": "Counterfactual explanation", "content": "While the edited image reflects the manipulation in the report generator, as shown in Fig. 2, we refer to these manipulated images as \u201ccyclic\u201d counterfactual images. These images are then used to decode the report generator by identifying the visual features associated with the reports generated for each query X-ray."}, {"title": "Removal of Visual Abnormality", "content": "To detect the underlying visual features associated with the context generated by the report generator, we modify the reorganised prompt by removing the findings mentioned in the generated report and send it to the image generation model for counterfactual generation, as shown in Fig. 3 (B). A successful cyclic counterfactual image is defined as one that successfully removes the targeted findings in the regenerated report. We then leverage these counterfactual images to detect the visual changes that lead to the reversal of the report findings."}, {"title": "Unsupervised frame generation", "content": "To facilitate the detection of crucial features that alter the findings in the regenerated report, we propose an unsupervised anatomical-aware difference frame. This frame is calculated based on the absolute difference map between the initial X-ray and its counterfactual, enabling the observation of visual alterations that lead to changes in the report. Specifically, we first calculate the absolute difference between the two images, followed by applying a Gaussian blur with a size of H \u00d7 W and a threshold L to reduce noise in the difference map. To detect abnormalities that are semantically represented in the image, we extract the contours of isolated pixels, group them into connected components, and retain the most significant ones by selecting the contours with the largest area. The final difference frame is then formulated based on the selected top K major components. An example of the detailed processing is provided in Fig. 5."}, {"title": "Experiments", "content": "In this section, we first outline the experimental settings, followed by the presentation of results, which include the effectiveness of CVLA, comparisons of explanation performance, and ablation studies to assess CVLA's effectiveness and explanation capability."}, {"title": "Experimental Setting", "content": "We developed and evaluated CVLA for two different report generators named R2Gen (Chen et al. 2020) and R2GenCMN (Chen et al. 2022) respectively, to detect the visual features within X-ray each exploit for report generation. For each CVLA, we prepare the training dataset with MIMIC-CXR (Johnson et al. 2019), as it was used for training the corresponding report generators. The dataset comprises 473,057 chest X-ray images and 206,563 paired reports from 63,478 patients. Following the two works, we utilize the training dataset which includes 270,790 X-rays to train the CVLA. A validation set of 2,130 X-rays is used for model selection and the test set including 3858 images and reports is used to generate their counterfactual images."}, {"title": "Implementation details", "content": "For developing the CVLA, we initialized the model using the weights of publicly available Stable Diffusion checkpoints (CompVis/stable-diffusion-v1-4) and trained with a batch size of 8 and a learning rate of 5e-5 on one A6000 GPU with 40 GB of memory. We trained the model with 100k steps over about one week. The final model for cyclic counterfactual generation was selected based on the highest PSNR achieved on this validation set. For counterfactual generation, the DDIM step is set to 25. For the frame mask generation, the Gaussian blur is set at 5x5, the threshold is between 95\u00b110, and we select the best value for each manipulated finding, keeping K at 5."}, {"title": "Evaluation methods", "content": "We first assess the effectiveness of CVLA by testing its ability to achieve cyclic counterfactual explanations in Table 1. This involves manipulating an image and sending it to the report generation model to see if the generated report reflects the intended changes (e.g., removing a finding). The success rate of cyclic counterfactual generation is calculated by counting the number of counterfactual images that successfully remove the manipulated findings in the regenerated reports.\nAfter validating CVLA's ability to generate successful cyclic counterfactual X-rays, we use these successful images to identify the visual features that report generators rely on for report creation. We illustrated the generated frame on the counterfactual images to localize the major differences between the counterfactual and initial query images, that contributes to the removal of the findings in the regenerated report under different report generators in Fig. 6.\nFinally, we compare the explanation results of different methods with the anatomical-aware difference frame generated by our counterfactual images in Fig. 7.\nWe compare our difference frame returned by the CVLA with the heatmap generated by the most widely applied cross attention in terms of their explanation and localisation accuracy. Furthermore, we compare our frame with the generated frame and the generated report from the explainable report generator model (Tanida et al. 2023)."}, {"title": "Results", "content": "Table. 1 shows the quantitative results of CVLA in obtaining the cyclic counterfactual examples for R2Gen and R2GenCMN respectively, where both models achieve a success rate around 0.7, with CVLA for R2GEN achieving a higher manipulation success rate.\nWe present the visual explanation results from the cyclic counterfactual X-rays in Fig. 6 for R2Gen and R2GenCMN respectively. Specifically, we remove the abnormalities from their generated report and generate the counterfactuals respectively, and resend the counterfactual images to their respective report generators to see if the abnormalities have been removed in the generated report.\nFor the query X-ray in Fig. 6, R2GenCMN detected three abnormalities Cardiomegaly, Support Device, and Atelectasis, while R2Gen two abnormalities Cardiomegaly and Support Device. Both models successfully remove the findings in their report generator models and we can clearly observe the visual features contributing to the generated findings in their reports."}, {"title": "Baseline comparison", "content": "For the proposed cyclic counterfactual explanation method, we compare it to other explanation method: RGRG and cross attention methods. Fig. 7 illustrates the different explanations generated for different abnormalities. Compared with the cross-attention method, our approach produces more accurate localization for the major findings it generates. The heatmaps generated by the cross-attention method appear to be unstable. For instance, the findings in Fig. 7 such as enlarged cardiomediastinum, lung opacity, edema, and consolidation are not correctly localised to the correct anatomical areas.\nRGRG method provides reasonable interpretable results by providing the findings for each anatomy it detected. However, this method relies heavily on a pretrained detection model and requires a substantial volume of annotated frames within the training datasets. Although the model achieves interpretability internally, the framework used by the RGRG cannot be adapted to other report generators with different models and training datasets. In contrast, our proposed method provides precise localization explanations across various report generation models, as depicted in Fig. 6."}, {"title": "Ablation study", "content": "To demonstrate that the images generated by our CVLA align with the reports produced by the target report generator, we compare the trained CVLA for R2Gen with a model trained on the most accurate ground truth reports (GT model). We provide both qualitative and quantitative ablations to justify our training dataset choice for achieving cyclic success in explanation. Fig. 4 highlights the importance of pretraining CVLA to align with generated reports for accurate explanation. We compare this with the GT model, trained on the most accurate ground truth reports.\nWhen examining counterfactual and reconstructed images, with and without the keyword 'cardiomegaly', both models highlight differences in heart size, with the GT model showing a more pronounced effect. However, comparing the initial query image with the reconstructed one, the GT model artificially enhances the heart size due to the inclusion of 'cardiomegaly' in the generated report, even though the original ground truth report did not mention this finding. This suggests that while counterfactual images may highlight features like 'cardiomegaly', they do not necessarily explain why this finding was generated in the initial X-ray, as these features were not present in the initial X-ray. This observation is further supported by the higher success rate of the tailored model in altering report findings compared to the GT model, as shown in Table 1.\nWe also investigate the impact of training time on the stable diffusion model for achieving CVLA. Specifically, we compare models achieving the best reconstruction (best PSNR) and models trained with more iterations. Table 1 demonstrates that the model with the best reconstruction ability, when paired with the generated text, achieves the highest cyclic manipulation effectiveness for report explanation.\nWe compare the manipulation method within CVLA to a direct report manipulation approach, where Stable Diffusion is trained directly with reports without pre-cleaning. The result in Fig. 8 shows that the organised prompt which focuses on the findings brings more significant change compared to the performance by removing the full sentence in the unorganised report."}, {"title": "Limitation and future work", "content": "The proposed manipulation method is currently limited to abnormalities classified by CheXbert, restricting its ability to manipulate other existing abnormalities outside this classification. In the future, we plan to extend the method by enabling the manipulation of a broader range of words. Additionally, we will involve radiologists in evaluating the explanation results and broaden the application of XAI methods to a wider array of report generation models."}, {"title": "Conclusion", "content": "In this paper, we propose a cyclic vision-language adapter (CVLA) module to generate counterfactual images for the query X-ray images sent to the report generator. These counterfactual images modify the findings within the generated reports, providing users with insights into the underlying reasoning behind the report generation. Our method enhances feature localization within the images for the findings generated in the reports, enabling users to understand the underlying reasons for the generated report, rather than merely accepting the report as a final output. This approach offers an effective way to compare and evaluate different report generator models, which is especially valuable in the rapidly evolving era of report generation models."}, {"title": "Unsupervised difference map generation", "content": "The generation of counters to highlight the differences between the counterfactual and initial images, which subsequently alter the report generator's output, follows the pipeline outlined below:\n1. Difference Map: The absolute difference between the given initial image (1st column) and the counterfactual images, generated by removing the target findings from the reports (2nd column), is first calculated. A blur kernel of size 5x5 is then applied. To filter out noisy pixels, a threshold L is used, followed by the application of Otsu's method to calculate an adaptive threshold for each difference map, resulting in a binarised image (3nd column).\n2. Extraction of Main Components: Morphological operations are employed to extract the major components from the separated pixels. Specifically, a morphological opening process is used to remove small objects with a fixed 3x3 kernel and an iteration count t1. This is followed by a morphological dilation process, using the same kernel and an iteration count t2, to connect nearby components. The iteration counts t1 and t2 are empirically determined and fixed for each object. The results of the opening and closing processes are displayed in the 4th and 5th columns, respectively.\n3. Component Visualisation: The extracted components are visualised by assigning each a distinct colour, as shown in the 6th column.\n4. Component Filtering: Components with areas smaller than 5% of the total component area are removed. The top K components (with K = 5) are selected as the final result. If the reserved components are fewer than the set threshold, all the components will be shown accordingly. For the identification related to the \u2018Cardiomegaly' manipulation, we will apply one more step to remove the frames outside the heart areas by applying a heart mask.\nThe parameters L, t1, and t2 are selected and fixed for each object based on empirical evidence. The parameter L ranges between 0 and 25 for images represented by integers between 0 and 255, t1 ranges from 2 to 4, and t2 ranges from 3 to 4. The specific parameters used for manipulating different findings are illustrated in the samples shown in Figure 1."}, {"title": "Interpretation of the different report generators", "content": "In this section, we present supplementary examples using two query X-rays and the explanation results generated by two different report generators, R2GenCMN and R2GEN. The examples are illustrated in Figures 2 and 3. The counterfactuals generated from these images remove the findings by feeding the regenerated reports back into the abnormality classification models.\nFrom these samples, we observe that different models can produce varying counterfactual images when given the same manipulation object. This variability assists in identifying the specific features that contribute to particular findings within each model. Moreover, by comparing the differing findings generated by the two models, we gain insights into the underlying reasons for these variations."}, {"title": "Comparison to other methods", "content": "In this section, we further analyse the explanation results by comparing the counterfactual images with their corresponding difference frames and cross-attention maps generated by the same model, R2GenCMN. Additionally, we compare these results with the reports and generated frames from the RGRG method. Our evaluation of the proposed method is based on the following observations:\n1. Localisation Correspondence: We assess whether the localisation in the proposed frames derived from the counterfactual images aligns with the manipulated text generated by R2GenCMN.\n2. Cross-Attention Map Comparison: We compare the cross-attention maps with the localisation provided by the proposed frames.\n3. Localisation and Report Comparison: We compare the localisation and corresponding reports from the frames generated by RGRG with those from the proposed method.\nThe supplementary results are presented in Figures 4 through 7, where bold, red, and blue fonts indicate the relevant statements in the different reports. The underlined are the words"}]}