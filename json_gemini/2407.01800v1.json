{"title": "Normalization and effective learning rates in reinforcement learning", "authors": ["Clare Lyle", "Zeyu Zheng", "Khimya Khetarpal", "James Martens", "Hado van Hasselt", "Razvan Pascanu", "Will Dabney"], "abstract": "Normalization layers have recently experienced a renaissance in the deep reinforcement learning and continual learning literature, with several works highlighting diverse benefits such as improving loss landscape conditioning and combatting overestimation bias. However, normalization brings with it a subtle but important side effect: an equivalence between growth in the norm of the network parameters and decay in the effective learning rate. This becomes problematic in continual learning settings, where the resulting effective learning rate schedule may decay to near zero too quickly relative to the timescale of the learning problem. We propose to make the learning rate schedule explicit with a simple re-parameterization which we call Normalize-and-Project (NaP), which couples the insertion of normalization layers with weight projection, ensuring that the effective learning rate remains constant throughout training. This technique reveals itself as a powerful analytical tool to better understand learning rate schedules in deep reinforcement learning, and as a means of improving robustness to nonstationarity in synthetic plasticity loss benchmarks along with both the single-task and sequential variants of the Arcade Learning Environment. We also show that our approach can be easily applied to popular architectures such as ResNets and transformers while recovering and in some cases even slightly improving the performance of the base model in common stationary benchmarks.", "sections": [{"title": "Introduction", "content": "Many of the most promising application areas of deep learning, in particular reinforcement learning (RL), require training on a problem which is in some way nonstationary. In order for this type of training to be effective, the neural network must maintain its ability to adapt to new information as it becomes available, i.e. it must remain plastic. Several recent works have shown that loss of plasticity can present a major barrier to performance improvement in RL and in continual learning [Dohare et al., 2021, Lyle et al., 2021, Nikishin et al., 2022]. These works have proposed a variety of explanations for plasticity loss such as the accumulation of saturated ReLU unit and increased sharpness of the loss landscape [Lyle et al., 2023], along with mitigation strategies, such as resetting dead unitss [Sokar et al., 2023] and regularizing the parameters towards their initial values [Kumar et al., 2023]. Many of these explanations and their corresponding mitigation strategies center around reducing drift in the distribution of pre-activations [Lyle et al., 2024], a problem which has historically been resolved in the supervised learning setting by incorporating normalization layers into the network architecture. Indeed, normalization layers have been shown to be highly effective at stabilizing optimization in both continual learning and RL [Hussing et al., 2024, Ball et al., 2023]."}, {"title": "Background and related work", "content": "We begin by providing background on trainability and its loss in nonstationary learning problems. We additionally give an overview of neural network training dynamics and effective learning rates."}, {"title": "Training dynamics and plasticity in neural networks", "content": "Early work on neural network initialization centered around the idea of controlling the norm of the activation vectors [LeCun et al., 2002, Glorot and Bengio, 2010, He et al., 2015] using informal arguments. More recently, this perspective has been formalized and expanded [Poole et al., 2016, Daniely et al., 2016, Martens et al., 2021] to include the inner-products between pairs of activation vectors (for different inputs to the network). The function that describes the evolution of these inner-products determines the network's gradients at initialization up to rotation, and this in turn determines trainability (which was shown formally in the Neural Tangent Kernel regime by Xiao et al. [2020] and Martens et al. [2021]). A variety of initialization methods have been developed to ensure the network avoids \u201cshattering\u201d [Balduzzi et al., 2017] or collapsing gradients [Poole et al., 2016, Martens et al., 2021, Zhang et al., 2021b].\nOnce training begins, learning dynamics can be well-characterized in the infinite-width limit by the neural tangent kernel and related quantities [Jacot et al., 2018, Yang, 2019], although in practice optimization dynamics diverge significantly from the infinite-width limit [Fort et al., 2020]. A com- plementary line of work has empirically and theoretically characterized self-stabilization properties"}, {"title": "Effective learning rates", "content": "As noted by several prior works [Van Laarhoven, 2017, Li and Arora, 2020, Li et al., 2020b], normalization introduces scale-invariance into the layers to which it is applied, where by a scale- invariant function f we mean $f (c\\theta,x) = f(\\theta,x)$ for any positive scalar $c > 0$. This leads to the gradient scaling inversely with the parameter norm, whereby $\\nabla f(c\\theta) = \\frac{1}{c} \\nabla f(\\theta)$. The intuition behind this property is simple: changing the direction of a large vector requires a greater perturbation than changing the direction of a small vector. This motivates the concept of an 'effective learning rate', which provides a scale-invariant notion of optimizer step size. In the following definition, we take the approach of Kodryan et al. [2022] and assume an implicit 'reference norm' of size 1 for the parameters."}, {"title": "Analysis of normalization layers and plasticity", "content": "Although widely used and studied, the precise reasons behind the effectiveness of layer normalization remain mysterious. In this section, we provide some new insights into how normalization can help neural networks to maintain plasticity by facilitating the recovery of saturated nonlinearities, and highlight the importance of controlling the parameter norm in networks which incorporate normalization layers. We leverage these insights to propose Normalize-and-Project, a simple training protocol to maintain important statistics of the layers and gradients throughout training."}, {"title": "Layer normalization", "content": "It is widely accepted that achieving approximately mean-zero, unit-variance pre-activations (assuming suitable choices of activation functions) is useful to ensure a network is trainable at initialization [e.g Martens et al., 2021], and many neural network initialization schemes aim to maintain this property"}, {"title": "Parameter norm and effective learning rate decay", "content": "While the output of a scale-invariant function is insensitive to scalar multiplication of the parameters, its gradient magnitude scales inversely with the parameter norm. This results in the opposite behaviour of what we would expect in an unnormalized network: in networks with layer normalization, growth in the norm of the parameters corresponds to a decline in the network's sensitivity to changes in these parameters. In a sense this is preferable, as the glacially slow but stable regime of vanishing gradients is easier to recover from than the unstable exploding gradient regime. However, if the parameter"}, {"title": "Normalize-and-Project", "content": "We conclude from the above investigation that normalizing a network's pre-activations and fixing the parameter norm presents a simple but effective defense against loss of plasticity. In this section, we propose a principled approach to combine these two steps which we call NaP. Our goal for NaP is to provide a flexible recipe which can be applied to essentially any architecture, and which improves the stability of training, motivated by but not limited to non-stationary problems. Our approach can be decomposed into two steps: the insertion of normalization layers prior to nonlinearities in the network architecture, and the periodic projection of the network's weights onto a fixed-norm radius throughout training, along with a corresponding update to the per-layer learning rates into the optimization process. Algorithm -1 provides an overview of NaP. While some design choices, such as the use of scale and offset terms, can be tailored to a particular problem setting, we will aim to provide principled guidance on how to reason about the effects of these choices.\nLayer normalization: Introducing layer normalization allows us to benefit from the properties discussed in Section 3.1, though fully benefiting from these properties depend on normalization being applied each time a linear transform precedes a nonlinearity. While it might seem extreme, this proposal is in line with most popular language and vision architectures. For example, Vaswani et al. [2017] apply normalization after every two fully-connected layers, and recent results suggesting that adding normalization to the key and query matrices in attention heads [Henry et al., 2020] can provide further benefits.\nWeight projection: As discussed in Section 3.2, we must then take care in controlling the network's effective learning rate. We propose disentangling the parameter norm from the effective learning rate by enforcing a constant norm on the weight parameters of the network, allowing scaling of the layer outputs to depend only on the learnable scale and offset parameters. This approach is similar to that proposed by Kodryan et al. [2022], but importantly takes care to treat the scale and offset parameters"}, {"title": "Lessons on the effective learning rate", "content": "NaP constrains the network's effective learning rate to follow an explicit rather than implicit schedule. In this section, we explore how this property affects network training dynamics, demonstrating how implicit learning rate schedules due to parameter norm growth can be made explicit and be leveraged to improve the performance of NaP in deep RL domains."}, {"title": "Replicating the dynamics of parameter norm growth in NaP", "content": "We begin our study of effective learning rates by illustrating how the implicit learning rate schedule induced by the evolution of the parameter norm can be translated to an explicit schedule in NaP. We study a small CNN described in Appendix B.4 with layer normalization prior to each nonlinearity trained on CIFAR-10 with the usual label set. We train two 'twin' scale-invariant networks with the Adam optimizer in tandem: both networks see the exact same data stream and start from the same initialization, but the per-layer weights of one are projected after every gradient step to have constant norm, while the other is allowed to vary the norms of the weights. We then consider three experimental settings: in the first, we re-scale the per-layer learning rates of the projected network so that the explicit learning rate is equal to the effective learning rate of its twin. In the second, we re-scale the global learning rate based on the ratio of parameter norms between the projected and unprojected network, but do not tune per-layer. In the third, we do no learning rate re-scaling. We see in Figure 2 that the shapes of the learning curves for all networks except for the constant-ELR"}, {"title": "Implicit learning rate schedules in deep RL", "content": "When taken to extremes, learning rate decay will eventually prevent the network from making nontrivial learning progress. However, learning rate decay plays an integral role in the training of many modern architectures, and is required to achieve convergence for stochastic training objectives (unless the interpolation applies or Polyak averaging is employed). In this section we will show that, perhaps unsurprisingly, naive application of NaP with a constant effective learning rate can sometimes harm performance in settings where the implicit learning rate schedule induced by parameter norm growth was in fact critical to the optimization process. More surprising is that the domain where this phenomenon is most apparent is one where common wisdom would suggest learning rate decay would be undesirable: deep RL.\nRL involves a high degree of nonstationarity. As a result, deep RL algorithms such as DQN and Rainbow often use a constant learning rate schedule. Given that layer normalization has been widely observed to improve performance in value-based RL agents on the arcade learning environment, and that parameter norm tends to increase significantly in these agents, one might at first believe that the performance improvement offered by layer normalization is happening in spite of the resulting implicit learning rate decay. A closer look at the literature, however, reveals that several well-known algorithms such as AlphaZero [Schrittwieser et al., 2020], along with many implementations of popular methods such as Proximal Policy Optimization [Schulman et al., 2017], incorporate some form of learning rate decay, suggesting that a constant learning rate is not always desirable. Indeed, Figure 3 shows that constraining the parameter norm to induce a fixed ELR in the Rainbow agent frequently results in worse performance compared to unconstrained parameters. This is particularly striking given that many of the benefits supposedly provided by layer normalization, such as better conditioning of the loss landscape [Lyle et al., 2023] and mitigation of overestimation bias [Ball et al., 2023], should be independent of the effective learning rate. Instead, these properties appear to either be irrelevant for optimization or dependent on reductions in the effective learning rate.\nWe can close this gap by introducing a learning rate schedule (linear decay from the default $6.25 \\cdot 10^{-5}$ to $10^{-6}$, roughly proportional to the average parameter norm growth across games). We further observe in Appendix C.1 that when we vary the endpoint of the learning rate schedule, we often obtain a corresponding x-axis shift in the learning curves, suggesting that reaching a particular learning rate was necessary to master some aspect of the game. We conclude that, while beneficial, the implicit schedule induced by the parameter norm is not necessarily optimal for deep RL agents, and it is possible that a more principled adaptive approach could provide still further improvements."}, {"title": "Experiments", "content": "We now validate the utility of NaP empirically. Our goal in this section is to validate two key properties: first, that NaP does not hurt performance on stationary tasks; second, that NaP can mitigate plasticity loss under a variety of both synthetic and natural nonstationarities."}, {"title": "Robustness to nonstationarity", "content": "We begin with the continual classification problem described in Appendix B.4. We evaluate our approach on a variety of sources of nonstationarity, using two architectures: a small CNN, and a fully-connected MLP (see Appendix B.4. for details). We first evaluate a number of methods designed to maintain plasticity including Regenerative regularization [Kumar et al., 2023], Shrink and Perturb [Ash and Adams, 2020], ReDo [Sokar et al., 2023], along with leaky ReLU units (inspired by the CReLU trick of Abbas et al. [2023]), L2 regularization, and random Gaussian perturbations to the optimizer update, a heuristic form of Langevin Dynamics. We track the average online accuracy over the course of training for 20M steps, equivalent to 200 data relabelings, using a constant learning rate. We find varying degrees of efficacy in these approaches in the base network architectures, with regenerative regularization and ReDO tending to perform the best. When we apply the same suite of methods to networks with NaP, in Figure 4, we observe near-monotonic improvements (with the exception of ReDO, which we conjecture is because the reset method designed for unnormalized networks) in performance and a significant reduction in the gaps between methods, with the performance curves of the different methods nearly indistinguishable in the MLP. Further, we observe constant or increasing slopes in the online accuracy, suggesting that the difference between methods has more to do with their effect on within-task performance than on plasticity loss once the parameter and layer norms have been constrained."}, {"title": "Stationary supervised benchmarks", "content": "Having observed remarkable improvements in synthetic tasks, we now confirm that NaP does not interfere with learning on more widely-studied, natural datasets.\nLarge-scale image classification. We begin by studying the effect of NaP on two well-established benchmarks: a VGG16-like network [Simonyan and Zisserman, 2014] on CIFAR-10, and a ResNet- 50 [He et al., 2016] on the ImageNet-1k dataset. We provide full details in Appendix B.4. In Table 1 we obtain comparable performance in both cases using the same learning rate schedule as the baseline."}, {"title": "Deep reinforcement learning", "content": "Finally, we evaluate our approach on a setting where maintaining plasticity is critical to performance: RL on the Arcade Learning Environment. We conduct a full sweep over 57 Atari 2600 games comparing the effects of normalization, weight projection, and learning rate schedules on a Rainbow agent [Hessel et al., 2018]. In the RHS of Figure 5 we plot the spread of scores, along with estimates of the Mean and IQM of four agents: standard Rainbow, Rainbow + LayerNorm, Rainbow + NaP without an explicit LR schedule, and Rainbow + NaP with the LR schedule described in Section 4.2. We find that NaP with a linear schedule outperforms the other methods.\nWe also consider the sequential setting of Abbas et al. [2023]. In this case, we consider an idealized setup where we reset the optimizer state and schedule every time the environment changes, using a cosine schedule with warmup described in Appendix B.2. To evaluate NaP on this regime, we train on each of 10 games for 20M frames, going through this cycle twice. We do not reset parameters of the continual agents between games, but do reset the optimizer. We plot learning curves for the second round of games in the LHS of Figure 5, finding that NaP significantly outperforms a baseline Rainbow agent with and without layer normalization. Indeed, even after 200M steps the networks trained with NaP make similar learning progress to a random initialization."}, {"title": "Discussion", "content": "This paper has shown that loss of plasticity can be substantially mitigated by constraining the norms of the network's layers and parameters. This finding was made possible by two key insights: first, that in addition to the obvious benefits relating to maintaining constant (pre-)activation norms, layer normalization can also protect against saturated units, and second, that it introduces a correspondence between the parameter norm and the effective learning rate which has significant consequences on performance. We proposed NaP as a means of making the effective learning rate explicit in the optimization process, and leveraged this to gain new insights into the importance of learning rate decay schedules in deep reinforcement learning. In particular, we showed that Rainbow agents trained on the Arcade Learning Environment in fact under-perform their unconstrained counterparts when a constant effective learning rate is enforced. Provided a suitable decay schedule is used, however, a network's performance and robustness to nonstationarity can both be improved by using NaP. Our analysis suggests a number of exciting directions for future work; in particular, the use of adaptive learning rate schedules in reinforcement learning, and the possibility of tuning not only the global learning rate but also per-layer learning rates in networks which use normalization layers."}, {"title": "Broader Impact", "content": "This work concerns basic properties of optimization of neural networks. We do not anticipate any broader societal impacts as a direct consequence of our findings."}, {"title": "Derivations", "content": "Analysis of a network's training dynamics depends on characterizing the evolution of (pre-)activations and gradients in the forward and backward passes respectively. We lay out basic notation for fully- connected layers first, and then note additional details which must be considered for convolutional, skip connection, and attention layers. We will write the parameters of a layer as $\\theta_{l}$, and use f to denote a neural network.\nFully-connected layers: We write the forward pass through a network $f : \\mathbb{R}^{d_{0}} \\rightarrow \\mathbb{R}^{d_{L}}$ as a composition of layer-wise computations $f^{l} : \\mathbb{R}^{d_{l-1}} \\rightarrow \\mathbb{R}^{d_{l}}$ of the form:\n$a^{l} = f^{l}(a^{l-1}) = \\phi(\\sigma^{l}f_{LN}(h^{l}) + \\mu^{l}), h^{l} = W^{l}a^{l-1} W^{l} = \\theta_{l}$"}, {"title": "Gradients and signal propagation", "content": "One perspective which can provide some additional insight into our approach is to consider the following decomposition of the gradient being backpropagated through a layer.\n$\\nabla_{W_{l}} L(\\theta) = d_{a_{l}} L(\\theta) d_{W_{l}}(f_{LN}(W_{l}a_{l-1}))$\n$d_{a_{l}} L(\\theta) D_{h_{l}}(d_{h_{l}}f_{LN}(h_{l}))a_{l-1}$"}, {"title": "Details of NaP", "content": "Guiding principles: in general, the goal of NaP is to avoid dramatic distribution shifts in the pre- activation and parameter norms, and to ensure that the network can perform updates to parameters even if a nonlinearity is saturated. With these in mind, there are two key properties that a network designer should aim to maintain:\n1. All parametric functions entering a nonlinearity should have a normalization layer that at ensures the gradients of all units' parameters are correlated. If there are no parameters between nonlinearities (as is sometimes the case in e.g. resnets) normalization is not essential.\n2. Based on our investigations in Appendix C.4, L2 normalization of the pre-activations is crucial to obtain the positive benefits of layer normalization, while centering does not have noticeable effects on the network's robustness to unit saturation. As a result, applying at least RMSNorm is crucial prior to nonlinearities, but the choice of whether or not to incorporate centering is up to the designer's discretion.\nBatch normalization layers: by default, we put layer normalization prior to batch normalization if an architecture already incorporates batch normalization prior to a nonlinearity. This preserves the property of batch norm that individual units have mean zero across the batch, which may not be the case if layer normalization is applied after. We also always omit offset parameters if layernorm is succeeded by batchnorm, as these offset parameters will be zeroed out by batchnorm.\nSkip-connect layers: provided that layer normalization is applied to the outputs of a linear transfor- mation prior to a nonlinearity, NaP is agnostic to whether normalization is applied prior to or after a residual connection's outputs are added to the output of a layer. In particular, if we have a layer of the form $\\phi(a_{1} + a_{2})$ and $a_{1}$ and $a_{2}$ are the outputs of some subnetwork of the form $\\phi_{1}(f_{LN}(h_{1}))$ and $\\phi_{2}(f_{LN}(h_{2}))$ where $\\phi_{1}$ and $\\phi_{2}$ are (possibly trivial) activation functions, then the relevant parameters will already benefit from Proposition 1 and it is not necessary to add an additional normalization layer prior to the activation $\\phi$."}, {"title": "Dynamics of NaP", "content": "Weight projection non-interference: NaP incorporates a projection onto the ball of constant norm after each update step. A natural question is whether this projection step might simply be the inverse of the update step, leaving the parameters of the network constant. Fortunately, we note that the normalization layers have the effect of projecting gradients onto a subspace which is orthogonal to the current parameter values, i.e.\n$(\\nabla_{x}f_{RMS}(x))^{\\top}(x) = 0$ .\nWe also note that except for extreme situations such as Neural Collapse [Papyan et al., 2020], real- world gradient updates are almost never colinear with the parameters, meaning that even without normalization layers this problem would be unlikely.\nAnother concern that arises from the constraints we place on the weights and features is the possibility that these constraints will limit the network's expressivity. Normalization does remove the ability to distinguish colinear inputs of differing norms, meaning that the inputs x and ax will map to the same output for all a; however, since many data preprocessing pipelines already normalize inputs, we argue this is not a significant limitation. Indeed, under a more widely-used notion of expressivity, the number of activation patterns [Raghu et al., 2017], NaP does not limit expressivity at all. While straightforward, we provide a formal statement and proof of this claim in Appendix A.7.\nLayer normalization and parameter growth: In fact, if we incorporate normalization layers into the network we might expect an even more aggressive decay schedule. Recall that in a scale invariant network, we have $\\langle\\nabla_{\\theta}f(\\theta),\\theta\\rangle = 0$. Thus we know that the gradient at each time step will be orthogonal to the current parameters. In an idealized setting where we use the update rule $\\theta_{t+1} \\leftarrow \\theta_{t} + \\alpha_{t} \\frac{\\nabla_{\\theta} L(\\theta)}{|| \\nabla_{\\theta} L(\\theta) ||}$, this would result in the parameter norm growing at a rate $R(t)$, corresponding to an effectively linear learning rate decay."}, {"title": "Scale-invariance and layer-wise gradient norms", "content": "One benefit of NaP is that, because we normalize layer outputs, we limit the extent to which divergence in the norm of one layer's parameters can propagate to the gradients of other layers. For e.g. linear homogeneous activations such as ReLUs, the gradient of some objective function for some input with respect to the parameters of a particular layer contains a sum of matrix products whose norm will depend multilinearly on the norm of each matrix. In particular, in the simplified setting of a deep linear network where $f(\\theta, x) = [\\prod_{k=1}^{L} W^{l}]x$, we recall Saxe et al. [2013]\n$\\nabla_{W^{l}} f(\\theta; x) = [ \\prod_{k>l} W^{k}]^{\\top} x [\\prod_{k<l} W^{k}]^{\\top}$\nIn particular, with $\\theta' = W^{1}, ...,cW^{k}, ..., W^{L}$, for $k \\neq l$ we would have\n$\\nabla_{W^{l}} f(\\theta'; x) = c\\nabla_{W^{l}} f(\\theta; x)$"}, {"title": "Expressivity of NaP", "content": "Finally, we discuss the effect of normalization and weight projection on a notion of expressivity known as the number of activation patterns [Raghu et al., 2017] exhibited by a neural network. This quantity relates to the complexity of the function class a network can compute, giving the following result the corollary that NaP doesn't interfere with this notion of expressivity.\nProposition 2. Let f be a fully-connected network with ReLU nonlinearities. Let $\\tilde{f}$ be the function computed by f after applying NaP. Then the activation pattern of a particular architecture f and parameter $\\theta$ be $A_{\\theta}$, we have\n$A_{\\tilde{f}(\\theta,x)} = A_{f(N(\\theta), x)}$ .\nFurther, the decision boundary $\\arg \\max_{i \\in dout} f_{i}(x)$ is preserved under NaP.\nProof. We apply an inductive argument on each layer. In particular, when $\\phi$ is a ReLU nonlinearity we have\n$A(\\phi(h)) = A(\\phi(f_{RMS}(h)))$\n$\\phi(f_{RMS}(h)) = \\frac{\\phi(h)}{||h||}$\n$\\tilde{f}(x) = \\frac{1}{\\prod_{l=1}^{L}||h_{l}(x)||}f(x)$\nwhich trivially results in identical activation patterns in the normalized and unnormalized networks. It is worth noting that one distinguishing factor from a standard ReLU network is that the resulting scaling factor will be different for each x. Thus while the activation patterns will be the same, the two different inputs x y might have different scaling factors, which will be a nonlinear function of the input. NaP networks, even with ReLU activations, thus do not have the property of being piecewise linear."}, {"title": "Rescaling scale/offset parameters (linear homogeneous networks)", "content": "We observe that for any c > 0, letting $\\phi(x) = max(x, 0)$ we have:\n$f_{LN}(\\phi(W \\sigma x + \\mu)) = f_{LN}(c\\phi(W \\sigma x + \\mu))$\n$= f_{LN}(\\phi(cW(\\sigma x + \\mu)))$ by homogeneity of ReLU\n$= f_{LN}(\\phi(W(c\\sigma x + c\\mu)))$"}, {"title": "Toy experiment details", "content": "We conduct a variety of illustrative experiments on toy problem settings and small networks.\nNetwork: in Figures 1 and 2, we use a DQN-style network which consists of two sets of two convolutional layers with 32 and 64 channels respectively. We then apply max pooling and flatten the output, feeding through a 512-unit hidden linear layer before applying a final linear transform to obtain the output logits. The network uses ReLU nonlinearities. When NaP is applied, we add layer normalization prior to each nonlinearity."}, {"title": "RL details", "content": "Single-task atari: We base our RL experiments off of the publicly available implementation of the Rainbow agent [Hessel et al., 2018] in DQN Zoo [Quan and Ostrovski, 2020]. We follow the default hyperparameters detailed in this codebase. In our implementation, we add normalization layers prior to each nonlinearity except for the final softmax. We train for 200M frames on the Atari 57 suite [Bellemare et al., 2013]. We also allow for a learning rate schedule, which we explicitly detail in cases where non-constant learning rates are used.\nSequential ALE: we use the same rainbow implementation as for the single-task results, using a cosine decay learning rate for all variants. We restart the cosine decay schedule at every task change for all agents. Our cosine decay schedule uses an init value of $10^{-8}$, a peak value of the default LR for Rainbow (0.000625), 1000 warmup steps after the optimizer is reset, and end-value equal to $10^{-6}$ as in the single-task settings. We choose cosine decay due to its popularity in supervised learning, and to highlight the versatility of NaP to different LR schedules. We follow the game sequence used by Abbas et al. [2023], training for 20M frames per game."}, {"title": "Language details", "content": "Sequence memorization: we set a dataset size of 1024 and a sequence length of 512. We use a vocabulary size of 256, equivalent to ASCII tokenization. We use the adam optimizer, and train all networks for a minimum of 10 000 steps. We reset the dataset every 1000 optimizer steps, generating a new set of 1024 random strings of length 512. We use as a baseline a transformer architecture [Vaswani et al., 2017, Raffel et al., 2020] consisting of 4 attention blocks, with 8 heads and $d_{model}$ equal to 256. We use a batch size of 128.\nIn-context learning: our in-context learning experiments use the same overall setup as the sequence memorization experiments, with identical architectures and baseline optimization algorithm. In this"}, {"title": "Vision details", "content": "CIFAR-10 Memorization: We consider three classes of network architecture: a fully-connected multilayer perceptron (MLP), for which we default to a width of 512 and depth of 4 in our evaluations; a convolutional network with k convolutional layers followed by two fully connected layers, for which we default to depth four, 32 channels, and fully-connected hidden layer width of 256. In all networks, we apply layer normalization before batch normalization if both are used at the same time. By default, we typically do not use batch normalization. We use ReLU activations\nOur continual supervised learning domain is constructed from the CIFAR-10 dataset, from which we construct a family of continual classification problems. Each continual classification problem is characterized by a transformation function on the labels. For label transformations, we permute classes (for example, all images with the label 5 will be re-assigned the label 2), and random label assignment, where each input is uniformly at random assigned a new label independent of its class in the underlying classification dataset. Figures in the main paper concern random label assignments, as this is a more challenging task which produces more pronounced effects.\nIn our figures in the main paper, we run a total of 20M steps and a total of 200 random target resets. All networks are trained using the Adam optimizer. We conducted a sweep over learning rates for the different architectures, settling on $10^{-4}$ as this provided a reasonable balance between convergence speed and stability in all architectures, to ensure that all networks could at least solve the single-task version of each label and target transformation.\nVGGNet and ResNet-50 baselines: we use the standard data augmentation policies for the CIFAR- 10 and ImageNet-1k experiments. In our ImageNet-1k experiments, we use the ResNetv2 architecture variant, with a label smoothing parameter of 0.1, weight decay $10^{-4}$, and as an optimizer we use SGD with a cosine annealing learning rate schedule, and Nesterov momentum with decay rate 0.9. We use a batch size of 256. Our CIFAR-10 experiments use a VGG-Net architecture. We use the sgd optimizer with a batch size of 32, and set a learning rate schedule which starts at 0.025 and decays by a factor of 0.1 iteratively through training. We use Nesterov momentum with decay rate 0.9. We train for a total of 400 000 steps."}, {"title": "Additional experimental results", "content": "Our choice of learning rate schedule in the paper is motivated by an attempt to roughly approximate the shape of the implicit schedule obtained by parameter growth on average across games in the suite, with a slightly smaller terminal point than would typically be achieved by the parameter norm alone. We consider learning rate schedules which linearly interpolate between the initial learning rate of 0.000625 and a learning rate of $10^{-6}$, which is roughly equivalent to increasing the parameter norm by a factor of 60. We explore the importance of the duration of this decay in Figure 6, where we conduct a sweep over a subset of the full Atari 57 suite (selected by sorting alphabetically and selecting every third environment). We observe that faster decay schedules result in better performance initially, but often plateau at a lower value. Decaying linearly over the entire course of training, in contrast, exhibits slow initial progress but often picks up significantly towards the end of training. We conclude that in many games, reducing the learning rate is necessary for performance improvements in this agent, and the linear decay over the entire training period doesn't give the agent sufficien time to take advantage of the finer-grained updates to its predictions that a lower learning rate affords."}, {"title": "Non-stationary MNIST", "content": "We include additional network statistics from the experiments shown in Figure 4 in Figure 7.\nLinearized units: given a large batch, what fraction of the ReLU units in the penultimate layer are either 0 for all units or nonzero for all units. This gives a slightly more nuanced take on the amount of computation performed in the penultimate layer than the feature rank.\nFeature rank: we compute the numerical rank of the penultimate layer features by sampling a batch of data and computing the singular values of the b \u00d7 d matrix of d-dimensional feature vectors, $\\sigma_{1},...,\\sigma_{d}$. We then compute the numerical rank as $\\sum_{i} 1(\\sigma_{i}/\\sigma_{1} > 0.01)$.\nParameter and gradient norm: these are both computed in the standard way by flattening out the set of parameters / per-parameter gradients and computing the 2-norm of this vector."}, {"title": "Non-stationary sequence modeling", "content": "We find additionally that NaP is capable of improving the robustness of sequence models to nonstation- arities, while also not interfering with the formation of in-context learning circuits. We demonstrate"}, {"title": "ReLU revival experiments", "content": "Many previous works have noted that adaptive optimizers are particularly damaging to network plasticity [Dohare et al., 2021, Lyle et al., 2023, Dohare et al., 2023]. The primary mechanism underlying this is due to the sudden distribution shift in gradient moments due to changes in the learning objective \u2013 when the gradient norm increases, adaptive optimizers are slow to catch up and can take enormous update steps when this occurs. Layer normalization mitigates this effect due to two facts: first, the gradients of negative preactivations are nonzero, and second, all nonzero gradients are treated essentially the same by adaptive optimizers (up to $\\epsilon$). As a result, networks with layer norm can still perform significant updates to parameters feeding into 'dead' units, meaning that these networks have a good chance of turning on again later.\nWe illustrate this with a simple experimental setting, where we model optimizer updates with isotropic Gaussian-distributed gradient signals and perform a (truncated at zero) random walk. Formally we look at the time evolution of the system:\n$v_{t} = v_{t-1} + max(v, 0) z_{t}, z_{t} \\sim \\mathcal{N}(0, I)$"}, {"content": "to model the evolution of features under a gradient descent trajectory. To simulate the steps taken by an adaptive optimizer like RMSProp or Adam, where updates to each parameter have fixed norm, we modify this process slightly as follows:\n$v_{t} = v_{t-1} + sign(max(v, 0) z_{t}), z_{t} \\sim \\mathcal{N}(0, I)$ .\nTo simulate layer normalization, we compute the dot product between $z_{t}$ and the Jacobian $\\nabla_{v} \\frac{v}{||v||}$ to simulate gradient descent:\n$v_{t} = v_{t-1} + [\\nabla_{v} \\frac{v}{||v||}]^{\\top} z_{t} = z_{t} - z_{t} \\frac{\\nabla_{v} max(v,0)}{||v||}, z_{t} \\sim \\mathcal{N}(0, I)$\nand analogously compute the sign of this update to model RMSProp-style optimizers:\n$v_{t} = v_{t-1} + sign[\\nabla_{v} \\frac{v}{||v||}]^{\\top} max((z_{t}^{\\top} [\\nabla_{v} \\frac{v}{||v||}]), 0), z_{t} \\sim \\mathcal{N}(0, I)$ ."}, {"title": "Stationary supervised benchmarks", "content": "We provide the results with standard deviations in Table 2 and Table 3."}, {"title": "A note on scale and offset parameters", "content": "One loose end from our presentation of NaP is what to do with the learnable scale and offset terms, which are not by default projected and so may drift from their initial values. Most supervised tasks are too short for this drift to present problems, and adding layer-specific regularization or normalization adds additional engineering overhead to an experiment. However, in deep RL or in the synthetic continual tasks we present in Figure 4, this is a real concern. In the case of homogeneous activations such as ReLU, the scale and offset parameters can be viewed identically to the weight and bias terms and normalized accordingly, noting that now all that matters is the relative ratio of the scale and the offset. To account for this, we propose to treat the joint set $\\sigma$, $\\mu$ as a single parameter to be normalized. This resolves the issues involved with normalizing a parameter to an initial value of zero, and can be shown not to change the network output (see Appendix A.8 for a derivation of this fact). With non-homogeneous nonlinearities, however, this property will not hold, and we suggest in the general case to use mild weight decay towards a the initial values of 1 and 0 for the scale and offset terms respectively. These two approaches can be summarized in the following two update rules:\n$U_{norm}(\\sigma, \\mu) = \\frac{(\\sigma, \\mu)}{\\sqrt{||\\sigma||^{2} + ||\\mu||^{2}}}$ and $U_{decay}(\\sigma, \\mu) = (\\alpha \\sigma + (1 - \\alpha)1, \\alpha \\mu)$ ."}]}