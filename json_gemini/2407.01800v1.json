{"title": "Normalization and effective learning rates in reinforcement learning", "authors": ["Clare Lyle", "Zeyu Zheng", "Khimya Khetarpal", "James Martens", "Hado van Hasselt", "Razvan Pascanu", "Will Dabney"], "abstract": "Normalization layers have recently experienced a renaissance in the deep reinforce- ment learning and continual learning literature, with several works highlighting diverse benefits such as improving loss landscape conditioning and combatting overestimation bias. However, normalization brings with it a subtle but important side effect: an equivalence between growth in the norm of the network parameters and decay in the effective learning rate. This becomes problematic in continual learning settings, where the resulting effective learning rate schedule may decay to near zero too quickly relative to the timescale of the learning problem. We propose to make the learning rate schedule explicit with a simple re-parameterization which we call Normalize-and-Project (NaP), which couples the insertion of normalization layers with weight projection, ensuring that the effective learning rate remains constant throughout training. This technique reveals itself as a powerful analytical tool to better understand learning rate schedules in deep reinforcement learning, and as a means of improving robustness to nonstationarity in synthetic plasticity loss benchmarks along with both the single-task and sequential variants of the Arcade Learning Environment. We also show that our approach can be easily applied to popular architectures such as ResNets and transformers while recovering and in some cases even slightly improving the performance of the base model in common stationary benchmarks.", "sections": [{"title": "1 Introduction", "content": "Many of the most promising application areas of deep learning, in particular reinforcement learning (RL), require training on a problem which is in some way nonstationary. In order for this type of training to be effective, the neural network must maintain its ability to adapt to new information as it becomes available, i.e. it must remain plastic. Several recent works have shown that loss of plasticity can present a major barrier to performance improvement in RL and in continual learning [Dohare et al., 2021, Lyle et al., 2021, Nikishin et al., 2022]. These works have proposed a variety of explanations for plasticity loss such as the accumulation of saturated ReLU unit and increased sharpness of the loss landscape [Lyle et al., 2023], along with mitigation strategies, such as resetting dead unitss [Sokar et al., 2023] and regularizing the parameters towards their initial values [Kumar et al., 2023]. Many of these explanations and their corresponding mitigation strategies center around reducing drift in the distribution of pre-activations [Lyle et al., 2024], a problem which has historically been resolved in the supervised learning setting by incorporating normalization layers into the network architecture. Indeed, normalization layers have been shown to be highly effective at stabilizing optimization in both continual learning and RL [Hussing et al., 2024, Ball et al., 2023]."}, {"title": "2 Background and related work", "content": "We begin by providing background on trainability and its loss in nonstationary learning problems. We additionally give an overview of neural network training dynamics and effective learning rates."}, {"title": "2.1 Training dynamics and plasticity in neural networks", "content": "Early work on neural network initialization centered around the idea of controlling the norm of the activation vectors [LeCun et al., 2002, Glorot and Bengio, 2010, He et al., 2015] using informal arguments. More recently, this perspective has been formalized and expanded [Poole et al., 2016, Daniely et al., 2016, Martens et al., 2021] to include the inner-products between pairs of activation vectors (for different inputs to the network). The function that describes the evolution of these inner-products determines the network's gradients at initialization up to rotation, and this in turn determines trainability (which was shown formally in the Neural Tangent Kernel regime by Xiao et al. [2020] and Martens et al. [2021]). A variety of initialization methods have been developed to ensure the network avoids \u201cshattering\u201d [Balduzzi et al., 2017] or collapsing gradients [Poole et al., 2016, Martens et al., 2021, Zhang et al., 2021b].\nOnce training begins, learning dynamics can be well-characterized in the infinite-width limit by the neural tangent kernel and related quantities [Jacot et al., 2018, Yang, 2019], although in practice optimization dynamics diverge significantly from the infinite-width limit [Fort et al., 2020]. A com- plementary line of work has empirically and theoretically characterized self-stabilization properties"}, {"title": "2.2 Effective learning rates", "content": "As noted by several prior works [Van Laarhoven, 2017, Li and Arora, 2020, Li et al., 2020b], normalization introduces scale-invariance into the layers to which it is applied, where by a scale- invariant function $f$ we mean $f(c\\theta,x) = f(\\theta,x)$ for any positive scalar $c > 0$. This leads to the gradient scaling inversely with the parameter norm, whereby $\\nabla f(c\\theta) = \\nabla f(\\theta)$. The intuition behind this property is simple: changing the direction of a large vector requires a greater perturbation than changing the direction of a small vector. This motivates the concept of an 'effective learning rate', which provides a scale-invariant notion of optimizer step size. In the following definition, we take the approach of Kodryan et al. [2022] and assume an implicit 'reference norm' of size 1 for the parameters.\nDefinition 1 (Effective learning rate). Consider a scale-invariant function $f$, parameters $\\theta$ and update function $\\theta_{t+1} \\leftarrow \\theta_t + \\eta g(\\theta_t)$. Letting $\\rho = \\frac{1}{\\|\\theta\\|} $, we then define the effective learning rate $\\tilde{\\eta}$ as follows:\n$\\tilde{\\eta} = \\begin{cases}\\eta \\rho^2, & \\text{if } g(\\theta_t) = \\nabla_{\\theta} f(\\theta_t) \\\\\\\\ \\eta \\rho, & \\text{if } g(\\theta_t) = \\frac{\\nabla_{\\theta}f(\\theta_t)}{\\|\\nabla_{\\theta}f(\\theta_t) \\|} \\end{cases}$\nwhere, letting $\\tilde{\\theta} = \\theta $, we then have $f(\\theta + \\tilde{\\eta}g(\\theta)) = f(\\theta + \\eta g(\\theta))$\nThis suggests that regularization of the network norm can have the dual effect of increasing the effec- tive learning rate, as noted by prior work [Van Laarhoven, 2017, Hoffer et al., 2018]. Several works have since offered more fine-grained theoretical analyses of such implicit learning rate schedules [Arora et al., 2018], and suggested means of translating these implicit schedules into explicit ones [Li and Arora, 2020, Li et al., 2020a]. More recently, the work of Lobacheva et al. [2021] and Kodryan et al. [2022] has studied the training properties of scale-invariant networks trained with parameters constrained to the unit sphere, a training regime we expand upon in this work."}, {"title": "3 Analysis of normalization layers and plasticity", "content": "Although widely used and studied, the precise reasons behind the effectiveness of layer normalization remain mysterious. In this section, we provide some new insights into how normalization can help neural networks to maintain plasticity by facilitating the recovery of saturated nonlinearities, and highlight the importance of controlling the parameter norm in networks which incorporate normalization layers. We leverage these insights to propose Normalize-and-Project, a simple training protocol to maintain important statistics of the layers and gradients throughout training."}, {"title": "3.1 Layer normalization", "content": "It is widely accepted that achieving approximately mean-zero, unit-variance pre-activations (assuming suitable choices of activation functions) is useful to ensure a network is trainable at initialization [e.g Martens et al., 2021], and many neural network initialization schemes aim to maintain this property"}, {"title": "3.2 Parameter norm and effective learning rate decay", "content": "While the output of a scale-invariant function is insensitive to scalar multiplication of the parameters, its gradient magnitude scales inversely with the parameter norm. This results in the opposite behaviour of what we would expect in an unnormalized network: in networks with layer normalization, growth in the norm of the parameters corresponds to a decline in the network's sensitivity to changes in these parameters. In a sense this is preferable, as the glacially slow but stable regime of vanishing gradients is easier to recover from than the unstable exploding gradient regime. However, if the parameter"}, {"title": "3.3 Normalize-and-Project", "content": "We conclude from the above investigation that normalizing a network's pre-activations and fixing the parameter norm presents a simple but effective defense against loss of plasticity. In this section, we propose a principled approach to combine these two steps which we call NaP. Our goal for NaP is to provide a flexible recipe which can be applied to essentially any architecture, and which improves the stability of training, motivated by but not limited to non-stationary problems. Our approach can be decomposed into two steps: the insertion of normalization layers prior to nonlinearities in the network architecture, and the periodic projection of the network's weights onto a fixed-norm radius throughout training, along with a corresponding update to the per-layer learning rates into the optimization process. Algorithm -1 provides an overview of NaP. While some design choices, such as the use of scale and offset terms, can be tailored to a particular problem setting, we will aim to provide principled guidance on how to reason about the effects of these choices.\nLayer normalization: Introducing layer normalization allows us to benefit from the properties discussed in Section 3.1, though fully benefiting from these properties depend on normalization being applied each time a linear transform precedes a nonlinearity. While it might seem extreme, this proposal is in line with most popular language and vision architectures. For example, Vaswani et al. [2017] apply normalization after every two fully-connected layers, and recent results suggesting that adding normalization to the key and query matrices in attention heads [Henry et al., 2020] can provide further benefits.\nWeight projection: As discussed in Section 3.2, we must then take care in controlling the network's effective learning rate. We propose disentangling the parameter norm from the effective learning rate by enforcing a constant norm on the weight parameters of the network, allowing scaling of the layer outputs to depend only on the learnable scale and offset parameters. This approach is similar to that proposed by Kodryan et al. [2022], but importantly takes care to treat the scale and offset parameters"}, {"title": "4 Lessons on the effective learning rate", "content": "NaP constrains the network's effective learning rate to follow an explicit rather than implicit schedule. In this section, we explore how this property affects network training dynamics, demonstrating how implicit learning rate schedules due to parameter norm growth can be made explicit and be leveraged to improve the performance of NaP in deep RL domains."}, {"title": "4.1 Replicating the dynamics of parameter norm growth in NaP", "content": "We begin our study of effective learning rates by illustrating how the implicit learning rate schedule induced by the evolution of the parameter norm can be translated to an explicit schedule in NaP. We study a small CNN described in Appendix B.4 with layer normalization prior to each nonlinearity trained on CIFAR-10 with the usual label set. We train two 'twin' scale-invariant networks with the Adam optimizer in tandem: both networks see the exact same data stream and start from the same initialization, but the per-layer weights of one are projected after every gradient step to have constant norm, while the other is allowed to vary the norms of the weights. We then consider three experimental settings: in the first, we re-scale the per-layer learning rates of the projected network so that the explicit learning rate is equal to the effective learning rate of its twin. In the second, we re-scale the global learning rate based on the ratio of parameter norms between the projected and unprojected network, but do not tune per-layer. In the third, we do no learning rate re-scaling. We see in Figure 2 that the shapes of the learning curves for all networks except for the constant-ELR"}, {"title": "4.2 Implicit learning rate schedules in deep RL", "content": "When taken to extremes, learning rate decay will eventually prevent the network from making nontrivial learning progress. However, learning rate decay plays an integral role in the training of many modern architectures, and is required to achieve convergence for stochastic training objectives (unless the interpolation applies or Polyak averaging is employed). In this section we will show that, perhaps unsurprisingly, naive application of NaP with a constant effective learning rate can sometimes harm performance in settings where the implicit learning rate schedule induced by parameter norm growth was in fact critical to the optimization process. More surprising is that the domain where this phenomenon is most apparent is one where common wisdom would suggest learning rate decay would be undesirable: deep RL.\nRL involves a high degree of nonstationarity. As a result, deep RL algorithms such as DQN and Rainbow often use a constant learning rate schedule. Given that layer normalization has been widely observed to improve performance in value-based RL agents on the arcade learning environment, and that parameter norm tends to increase significantly in these agents, one might at first believe that the performance improvement offered by layer normalization is happening in spite of the resulting implicit learning rate decay. A closer look at the literature, however, reveals that several well-known algorithms such as AlphaZero [Schrittwieser et al., 2020], along with many implementations of popular methods such as Proximal Policy Optimization [Schulman et al., 2017], incorporate some form of learning rate decay, suggesting that a constant learning rate is not always desirable. Indeed, Figure 3 shows that constraining the parameter norm to induce a fixed ELR in the Rainbow agent frequently results in worse performance compared to unconstrained parameters. This is particularly striking given that many of the benefits supposedly provided by layer normalization, such as better conditioning of the loss landscape [Lyle et al., 2023] and mitigation of overestimation bias [Ball et al., 2023], should be independent of the effective learning rate. Instead, these properties appear to either be irrelevant for optimization or dependent on reductions in the effective learning rate.\nWe can close this gap by introducing a learning rate schedule (linear decay from the default $6.25 \\cdot 10^{-5}$ to $10^{-6}$, roughly proportional to the average parameter norm growth across games). We further observe in Appendix C.1 that when we vary the endpoint of the learning rate schedule, we often obtain a corresponding x-axis shift in the learning curves, suggesting that reaching a particular learning rate was necessary to master some aspect of the game. We conclude that, while beneficial, the implicit schedule induced by the parameter norm is not necessarily optimal for deep RL agents, and it is possible that a more principled adaptive approach could provide still further improvements."}, {"title": "5 Experiments", "content": "We now validate the utility of NaP empirically. Our goal in this section is to validate two key properties: first, that NaP does not hurt performance on stationary tasks; second, that NaP can mitigate plasticity loss under a variety of both synthetic and natural nonstationarities."}, {"title": "5.1 Robustness to nonstationarity", "content": "We begin with the continual classification problem described in Appendix B.4. We evaluate our approach on a variety of sources of nonstationarity, using two architectures: a small CNN, and a fully-connected MLP (see Appendix B.4. for details). We first evaluate a number of methods designed to maintain plasticity including Regenerative regularization [Kumar et al., 2023], Shrink and Perturb [Ash and Adams, 2020], ReDo [Sokar et al., 2023], along with leaky ReLU units (inspired by the CReLU trick of Abbas et al. [2023]), L2 regularization, and random Gaussian perturbations to the optimizer update, a heuristic form of Langevin Dynamics. We track the average online accuracy over the course of training for 20M steps, equivalent to 200 data relabelings, using a constant learning rate. We find varying degrees of efficacy in these approaches in the base network architectures, with regenerative regularization and ReDO tending to perform the best. When we apply the same suite of methods to networks with NaP, in Figure 4, we observe near-monotonic improvements (with the exception of ReDO, which we conjecture is because the reset method designed for unnormalized networks) in performance and a significant reduction in the gaps between methods, with the performance curves of the different methods nearly indistinguishable in the MLP. Further, we observe constant or increasing slopes in the online accuracy, suggesting that the difference between methods has more to do with their effect on within-task performance than on plasticity loss once the parameter and layer norms have been constrained."}, {"title": "5.2 Stationary supervised benchmarks", "content": "Having observed remarkable improvements in synthetic tasks, we now confirm that NaP does not interfere with learning on more widely-studied, natural datasets.\nLarge-scale image classification. We begin by studying the effect of NaP on two well-established benchmarks: a VGG16-like network [Simonyan and Zisserman, 2014] on CIFAR-10, and a ResNet-50 [He et al., 2016] on the ImageNet-1k dataset. We provide full details in Appendix B.4. In Table 1 we obtain comparable performance in both cases using the same learning rate schedule as the baseline."}, {"title": "5.3 Deep reinforcement learning", "content": "Finally, we evaluate our approach on a setting where maintaining plasticity is critical to performance: RL on the Arcade Learning Environment. We conduct a full sweep over 57 Atari 2600 games comparing the effects of normalization, weight projection, and learning rate schedules on a Rainbow agent [Hessel et al., 2018]. In the RHS of Figure 5 we plot the spread of scores, along with estimates of the Mean and IQM of four agents: standard Rainbow, Rainbow + LayerNorm, Rainbow + NaP without an explicit LR schedule, and Rainbow + NaP with the LR schedule described in Section 4.2. We find that NaP with a linear schedule outperforms the other methods.\nWe also consider the sequential setting of Abbas et al. [2023]. In this case, we consider an idealized setup where we reset the optimizer state and schedule every time the environment changes, using a cosine schedule with warmup described in Appendix B.2. To evaluate NaP on this regime, we train on each of 10 games for 20M frames, going through this cycle twice. We do not reset parameters of the continual agents between games, but do reset the optimizer. We plot learning curves for the second round of games in the LHS of Figure 5, finding that NaP significantly outperforms a baseline Rainbow agent with and without layer normalization. Indeed, even after 200M steps the networks trained with NaP make similar learning progress to a random initialization."}, {"title": "6 Discussion", "content": "This paper has shown that loss of plasticity can be substantially mitigated by constraining the norms of the network's layers and parameters. This finding was made possible by two key insights: first, that in addition to the obvious benefits relating to maintaining constant (pre-)activation norms, layer normalization can also protect against saturated units, and second, that it introduces a correspondence between the parameter norm and the effective learning rate which has significant consequences on performance. We proposed NaP as a means of making the effective learning rate explicit in the optimization process, and leveraged this to gain new insights into the importance of learning rate decay schedules in deep reinforcement learning. In particular, we showed that Rainbow agents trained"}, {"title": "Broader Impact", "content": "This work concerns basic properties of optimization of neural networks. We do not anticipate any broader societal impacts as a direct consequence of our findings."}, {"title": "A Derivations", "content": "A.1 Notation\nAnalysis of a network's training dynamics depends on characterizing the evolution of (pre-)activations and gradients in the forward and backward passes respectively. We lay out basic notation for fully- connected layers first, and then note additional details which must be considered for convolutional, skip connection, and attention layers. We will write the parameters of a layer as $\\theta_l$, and use $f$ to denote a neural network.\nFully-connected layers: We write the forward pass through a network $f : \\mathbb{R}^{d_0} \\rightarrow \\mathbb{R}^{d_1}$ as a composition of layer-wise computations $f^l : \\mathbb{R}^{d_{l-1}} \\rightarrow \\mathbb{R}^{d_l}$ of the form:\n$a^l = f^l(a^{l-1}) = \\phi(\\sigma^l f_{LN}(h^l) + \\mu^l), h^l = W^l a^{l-1} W^l = \\theta_l$\nwhere $\\phi$ denotes a nonlinearity and $f_{LN}$ is a (possibly absent) normalization operator. We let $a^0$ denote the network inputs. We will refer to a forward pass through a subset of a network with the notation $f^{l_1:l_2} = f^{l_2} \\circ ... \\circ f^{l_1}$, and use interchangeably $f = f^{1:L}$. We will refer to the full set of network parameters by $\\theta = vec(\\{\\theta_l\\}_{l=1}^L)$.\nConvolutional layers: since a convolution can be viewed as a parameterization of a matrix with a particular symmetry, we express these layers identically to the fully-connected layers, with a change in semantics such that $W^l$ is the matrix representation of the convolutional parameters $\\theta_l$, where we write the embedding of $\\theta_l$ into a matrix as $W_{conv} (\\theta_l)$. For simplicity, we ignore the choice of padding.\n$\\phi(\\sigma^l f_{LN}(h^l) + \\mu^l), h^l = W^l a^{l-1} \\text{ and } W^l = W_{conv}(\\theta_l)$\nSkip-connect layers: in some network architectures, a nonlinearity is placed on the outputs of two subnetworks to produce a function of the form\n$f^l = \\phi \\left( \\sum_{i \\in L} a_{l_i} \\right) \\text{ or } f^l = \\sum_{i \\in L} \\phi( a_{l_i})$\nfor some index set $L$. The choice of whether to apply normalization to the sum of the subnetwork outputs or to each output individually depends on the desired signal propagation properties of the network [De and Smith, 2020]."}, {"title": "A.2 Derivation of Proposition 1", "content": "The result described in proposition 1 follows straightforwardly from the chain rule. For pre-activation RMSNorm we have\n$\\frac{d}{dh_i} \\phi(f_{rms}(h))_j = \\phi'(f_{rms}(h))_j \\frac{d}{dh_i} f_{rms}(h)_j$\n$\\frac{d}{dh_i} f_{rms}(h)_j = \\frac{d}{dh_i} \\frac{h_j}{(\\sum_i h_i^2)^{1/2}}$\n$= \\frac{1}{2} \\frac{h_j}{(\\sum_i h_i^2)^{3/2}} \\frac{d}{dh_i} \\sum_i h_i^2$\n$= - \\frac{h_j h_i}{(\\sum_i h_i^2)^{3/2}}$\n$\\frac{d}{dh_i} \\phi(f_{rms}(h))_j = - \\phi'(f_{rms}(h))_j \\frac{h_j h_i}{\\| h \\|^3}$"}, {"title": "A.3 Gradients and signal propagation", "content": "One perspective which can provide some additional insight into our approach is to consider the following decomposition of the gradient being backpropagated through a layer.\n$\\nabla_\\theta \\mathcal{L}(0) = d_{a_l} \\mathcal{L}(0) d_{W_l} (f_{LN}(W_l a_{l-1}))$\n$= d_{a_l} \\mathcal{L}(0) D_{W_l} (h_l f_{LN}(h_l)) a_{l-1}$\nWith this decomposition, we obtain the following interpretation of some common pathologies.\nSaturated nonlinearities: a saturated nonlinearity implies that $D_{h_l \\phi}$ and the gradient is exactly (in the case of ReLU) or very close to (e.g. tanh) zero. As a result, $\\nabla_{W_l} \\mathcal{L}$ will be zero for the affected coordinates and the corresponding parameters will remain frozen. NaP addresses this problem by using Layer or RMSNorm prior to any nonlinearity in the network.\nSaturated normalization layers: the normalization transformation $x \\rightarrow \\frac{x}{\\|x\\|}$ is vulnerable to saturation as $\\|x\\|$ grows, in the sense that for a fixed-norm update $u$ we will have\n$\\lim_{\\|x\\| \\rightarrow \\infty} \\frac{x+u}{\\|x+u\\|} = \\frac{x}{\\|x\\|}$\nand so the term $\\nabla_{h_l} f_{LN}(h_l)$ will vanish. In this case, while the optimizer will be able to update the parameters, these updates will have a diminishing effect on the network's output.\nVanishing and exploding gradients: divergence and disappearance of the activations $a_{l-1}$ and backpropagated gradients $d_{a_l} \\mathcal{L}(0)$ are well-known pathologies which can make networks untrainable. However, even networks which start training from a well-tuned initialization may still encounter exploding gradients due to parameter norm growth over time [Dohare et al., 2021, Wortsman et al., 2023], or vanishing gradients and activations, such as in the case of saturated (i.e. dead/dormant) ReLU units [Sokar et al., 2023]."}, {"title": "A.4 Details of NaP", "content": "Guiding principles: in general, the goal of NaP is to avoid dramatic distribution shifts in the pre- activation and parameter norms, and to ensure that the network can perform updates to parameters even if a nonlinearity is saturated. With these in mind, there are two key properties that a network designer should aim to maintain:\nAll parametric functions entering a nonlinearity should have a normalization layer that at ensures the gradients of all units' parameters are correlated. If there are no parameters between nonlinearities (as is sometimes the case in e.g. resnets) normalization is not essential.\nBased on our investigations in Appendix C.4, L2 normalization of the pre-activations is crucial to obtain the positive benefits of layer normalization, while centering does not have noticeable effects on the network's robustness to unit saturation. As a result, applying at least RMSNorm is crucial prior to nonlinearities, but the choice of whether or not to incorporate centering is up to the designer's discretion.\nBatch normalization layers: by default, we put layer normalization prior to batch normalization if an architecture already incorporates batch normalization prior to a nonlinearity. This preserves the property of batch norm that individual units have mean zero across the batch, which may not be the case if layer normalization is applied after. We also always omit offset parameters if layernorm is succeeded by batchnorm, as these offset parameters will be zeroed out by batchnorm.\nSkip-connect layers: provided that layer normalization is applied to the outputs of a linear transfor- mation prior to a nonlinearity, NaP is agnostic to whether normalization is applied prior to or after a residual connection's outputs are added to the output of a layer. In particular, if we have a layer of the form $\\phi(a_1 + a_2)$ and $a_1$ and $a_2$ are the outputs of some subnetwork of the form $\\phi_1(f_{LN}(h_1))$ and $\\phi_2(f_{LN}(h_2))$ where $\\phi_1$ and $\\phi_2$ are (possibly trivial) activation functions, then the relevant parameters will already benefit from Proposition 1 and it is not necessary to add an additional normalization layer prior to the activation $\\phi$."}, {"title": "A.5 Dynamics of NaP", "content": "Weight projection non-interference: NaP incorporates a projection onto the ball of constant norm after each update step. A natural question is whether this projection step might simply be the inverse of the update step, leaving the parameters of the network constant. Fortunately, we note that the normalization layers have the effect of projecting gradients onto a subspace which is orthogonal to the current parameter values, i.e.\n$(\\nabla_x f_{RMS}(x))^T (x) = 0$.\nWe also note that except for extreme situations such as Neural Collapse [Papyan et al., 2020], real- world gradient updates are almost never colinear with the parameters, meaning that even without normalization layers this problem would be unlikely.\nAnother concern that arises from the constraints we place on the weights and features is the possibility that these constraints will limit the network's expressivity. Normalization does remove the ability to distinguish colinear inputs of differing norms, meaning that the inputs $x$ and $a x$ will map to the same output for all $a$; however, since many data preprocessing pipelines already normalize inputs, we argue this is not a significant limitation. Indeed, under a more widely-used notion of expressivity, the number of activation patterns [Raghu et al., 2017], NaP does not limit expressivity at all. While straightforward, we provide a formal statement and proof of this claim in Appendix A.7.\nLayer normalization and parameter growth: In fact, if we incorporate normalization layers into the network we might expect an even more aggressive decay schedule. Recall that in a scale invariant network, we have $\\langle \\nabla_\\theta f(\\theta), \\theta \\rangle = 0$. Thus we know that the gradient at each time step will be orthogonal to the current parameters. In an idealized setting where we use the update rule $\\theta_{t+1} \\leftarrow \\theta_t + \\alpha_t \\frac{\\nabla_\\theta f(\\theta)}{|| \\nabla_\\theta f(\\theta) ||}$, this would result in the parameter norm growing at a rate $\\alpha(t)$, corresponding to an effectively linear learning rate decay."}, {"title": "A.6 Scale-invariance and layer-wise gradient norms", "content": "One benefit of NaP is that, because we normalize layer outputs, we limit the extent to which divergence in the norm of one layer's parameters can propagate to the gradients of other layers. For e.g. linear homogeneous activations such as ReLUs, the gradient of some objective function for some input with respect to the parameters of a particular layer contains a sum of matrix products whose norm will depend multilinearly on the norm of each matrix. In particular, in the simplified setting of a deep linear network where $f(\\theta, x) = \\prod_l W^l x$, we recall Saxe et al. [2013]\n$\\nabla_{W^l} f(\\theta; x) = \\prod_{k>l} W^k \\cdot x^T \\prod_{k<l} W^k$\nIn particular, with $\\theta' = W^1,...,c W^k,...,W^L$, for $k \\neq l$ we would have\n$\\nabla_{W^l} f(\\theta'; x) = c \\nabla_{W^l} f(\\theta; x)$"}, {"title": "A.7 Expressivity of NaP", "content": "Finally, we discuss the effect of normalization and weight projection on a notion of expressivity known as the number of activation patterns [Raghu et al., 2017] exhibited by a neural network. This quantity relates to the complexity of the function class a network can compute, giving the following result the corollary that NaP doesn't interfere with this notion of expressivity.\nProposition 2. Let f be a fully-connected network with ReLU nonlinearities. Let $\\tilde{f}$ be the function computed by f after applying NaP. Then the activation pattern of a particular architecture f and parameter $\\theta$ be $A_\\theta$, we have\n$A_{\\tilde{f}(\\theta,x)} = A_{f(N(\\theta), x)}$.\nFurther, the decision boundary $\\underset{i \\in Dout}{\\operatorname{argmax}} f_i(x)$ is preserved under NaP.\nProof. We apply an inductive argument on each layer. In particular, when $\\phi$ is a ReLU nonlinearity we have\n$A(\\phi(h)) = A(\\phi(f_{RMS}(h)))$\n$\\phi(f_{RMS}(h)) = \\frac{\\phi(h)}{\\| h \\|}$\n$\\tilde{f}(x) = \\frac{f(x)}{\\prod_{l=1}^L \\| h_l(x) \\|}$\nwhich trivially results in identical activation patterns in the normalized and unnormalized networks. It is worth noting that one distinguishing factor from a standard ReLU network is that the resulting scaling factor will be different for each $x$. Thus while the activation patterns will be the same, the two different inputs $x$ y might have different scaling factors, which will be a nonlinear function of the input. NaP networks, even with ReLU activations, thus do not have the property of being piecewise linear."}, {"title": "A.8 Rescaling scale/offset parameters (linear homogeneous networks)", "content": "We observe that for any $c > 0$, letting $\\phi(x) = \\max(x, 0)$ we have:\n$f_{LN}(\\phi(W \\sigma x + \\mu)) = f_{LN}(c \\phi(W \\sigma x + \\mu))$\n$= f_{LN}(\\phi(cW(\\sigma x + \\mu)))$ by homogeneity of ReLU\n$= f_{LN}(\\phi(W(c \\sigma x + c \\mu)))$"}]}