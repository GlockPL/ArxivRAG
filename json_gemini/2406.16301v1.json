{"title": "UBISS: A Unified Framework for Bimodal Semantic Summarization of Videos", "authors": ["Yuting Mei", "Linli Yao", "Qin Jin"], "abstract": "With the surge in the amount of video data, video summarization techniques, including visual-modal(VM) and textual-modal(TM) summarization, are attracting more and more attention. However, unimodal summarization inevitably loses the rich semantics of the video. In this paper, we focus on a more comprehensive video summarization task named Bimodal Semantic Summarization of Videos(BiSSV). Specifically, we first construct a large-scale dataset, BIDS, in(video, VM-Summary, TM-Summary) triplet format. Unlike traditional processing methods, our construction procedure contains a VM-Summary extraction algorithm aiming to preserve the most salient content within long videos. Based on BIDS, we propose a Unified framework UBiSS for the BiSSV task, which models the saliency information in the video and generates a TM-summary and VM-summary simultaneously. We further optimize our model with a list-wise ranking-based objective to improve its capacity to capture highlights. Lastly, we propose a metric, NDCGMS, to provide a joint evaluation of the bimodal summary. Experiments show that our unified framework achieves better performance than multi-stage summarization pipelines. Code and data are available at https://github.com/MeiYutingg/UBiSS.", "sections": [{"title": "1 INTRODUCTION", "content": "Video is becoming a critical information source on the Internet with a significant surge in volume\u00b9. Video summarization has emerged as a crucial technology for efficiently browsing, retrieving, and recommending videos. Based on modality, previous approaches could be divided into two categories: visual-modal summarization and textual-modal summarization of videos.\nAs the video content is inherently multimodal, unimodal summarization approaches, whether focusing on selecting informative clips [2, 8, 14, 15, 19, 28, 30, 42, 48] or generating text descriptions [24, 41, 43, 45, 46], inevitably sacrifice semantic richness in their pursuit of brevity. As shown in Figure 1, most textual-modal summarization approaches tend to generate a global summary description of the video content (e.g., \"A girl doing a hotel room tour\"), while many visual details are lost. On the other hand, visual-modal summarization approaches provide highlighted and detailed visual information (e.g., a viewer can tell what the hotel room is like browsing through the summary clips). However, since summary clips are unfolded sequentially, they cannot immediately convey the global core semantic content of the video.\nTo strike a balance between offering a quick global overview and preserving the rich visual semantics of the original video, we focus on the task of Bimodal Semantic Summarization of Videos (BiSSV) in this work. Our goal is to provide both textual-modal summary (TM-Summary) and visual-modal summary (VM-Summary) for videos. Creating such a bimodal summary is not trivial since it requires a thorough comprehension of both modalities and their relationship. An early attempt [4] fails to guarantee the correlation between bimodal summaries due to a lack of paired annotations. More recently, Lin et al. [23] construct the first video-to-video+text"}, {"title": "2 RELATED WORK", "content": "We can broadly divide related work on video summarization into two categories according to the output format: unimodal or multimodal video summarization. BiSSV belongs to the second category.\nUnimodal Video Summarization. Traditional unimodal video summarization includes Textual-Modal Summarization of Videos (TMSV) and Visual-Modal Summarization of Videos (VMSV). Video captioning [41] is a typical task in TMSV, which uses generic language to summarize videos, resulting in the loss of rich semantics. Dense video captioning [18], on the other hand, aims to generate multiple captions with their corresponding time intervals for a given video. However, as dense captions focus on specific events within videos, they cannot provide an overview of the entire video. Traditional video summarization [10], as a representative task in VMSV, aims to extract the most informative segments from videos. Due to the complex annotation process, mainstream video summarization datasets [5, 6, 10, 37] are limited in size. Most video summarization datasets and approaches rely on the Knapsack algorithm to convert saliency scores into summarized segments [2, 8, 10, 14, 15, 19, 37, 48]. However, Otani M et al. [29] point out a bias in the Knapsack algorithm towards favoring shorter segments. Although VM-Summary offers more details than TM-Summary, it requires extended comprehension time. In conclusion, unimodal video summarization inevitably results in semantic loss, while multimodal summarization leverages complementary modalities to provide concise yet informative summaries.\nMultimodal Video Summarization. Chen et al. [4] first tries to improve the performance of both TMSV and VMSV. Nevertheless, the lack of paired summary annotations leads to low consistency between the two sub-tasks. Recently, Lin et al. [23] introduce the VideoXum dataset and optimize TMSV and VMSV in a unified way. However, the TM-Summary in VideoXum is generated by concatenating dense captions from ActivityNet Captions [18] with an average length of 49.9 words, which falls short of providing a succinct overview. The evaluation of multimodal video summarization also remains a challenge. Lin et al. [23] adopt average CLIPScore between VM- and TM-Summary [12] to assess consistency. However, it's essential for TM-Summary to align with visual elements according to their saliency. To address these challenges, we construct a dataset, design a unified framework for bimodal summarization and propose NDCGMS to assess bimodal summaries jointly."}, {"title": "3 BIDS DATASET", "content": "As it is very costly to build a bimodal summarization dataset from scratch, we, therefore, leverage the QVHighlights dataset [20] to construct a Bimodal VIDeo Summarization dataset (BIDS) to support the investigation of the BiSSV task. The constructed BIDS dataset finally contains 8130 videos with corresponding ground-truth Visual-Modal (VM) and Textual-Modal (TM) Summaries and saliency scores annotated for each 2-second clip, indicating its significance. Following the restrictions of traditional video summarization [10], we ensure that the length of the VM-Summary does not exceed 15% of the original video's duration. We describe the data processing and analysis in detail in the following subsections."}, {"title": "3.1 Data Processing", "content": "We aim to build a bimodal video summarization dataset with triplet data samples (video, TM-Summary, VM-summary), where the TM-Summary is a concise text description, and the VM-summary contains highlighted segments within the video. Firstly, we merge"}, {"title": "3.2 Data Analysis", "content": "Traditional video summarization datasets use the Knapsack algorithm to generate VM-Summary [10, 37]. However, Otami M et al. [29] point out that their segmentation-selection pipeline favors short segments since selecting long segments costs more. However, long and visually consistent segments can also contain informative moments. For example, when watching a video of someone playing basketball, most of the visual content is similar, but we can still identify key moments, such as shooting. Inspired by humans' ability to distinguish important moments in long videos, we choose to scale the candidate segments instead of rejecting them entirely. As a result, our VM-Summary shows a stronger correlation between the saliency scores and the selected segments.\nWe use Spearman's correlation coefficient \\( \\rho \\) [51] to validate the effectiveness of our VM-Summary extraction algorithm. A higher coefficient between the saliency scores S and the frame-level selection sequence F (1 for the frame being selected into the VM-Summary and 0 for otherwise) indicates more salient content is preserved, which is the goal of summarization. As presented in Table 1, BIDS has the highest Spearman's \\( \\rho \\) compared to traditional datasets. Moreover, Spearman's \\( \\rho \\) between S and F (generated by annotators) surpasses the \\( \\rho \\) between S and GT-F (obtained by applying Knapsack algorithm over the annotated saliency scores) in SumMe [10], which further demonstrates that Knapsack algorithm can not effectively preserve salient parts within long segments."}, {"title": "4 METHOD", "content": "For a given video, the BiSSV task aims to generate a textual-modal summary (TM-Summary) and extract the most related clips to form a visual-modal summary (VM-Summary). We first introduce the necessary symbols and then present our proposed Unified framework for Bimodal Semantic Summarization of videos, UBiSS, along with our training strategy and the proposed evaluation metric in the subsequent sections.\nLet us denote the input video as V = \\( \\{v_i\\}_{i \\in [0,T]}, v_i \\varepsilon R^{C \\times h \\times w} \\), where T represents the number of frames, and C, h, w refer to the channel, height, and width of video frames, respectively. The goal of BiSSV is to generate the TM-Summary W = \\( \\{w_i\\}_{i \\in [0, l]} \\) that globally summarizes the video, and a VM-Summary represented by a frame-level selection sequence F = \\( \\{f_i\\}_{i \\in [0,T]}, f_i \\varepsilon \\{0, 1\\} \\), indicating whether each frame is selected for the summary."}, {"title": "4.1 Model Architecture", "content": "Figure 4 depicts the overall model architecture of UBiSS, which consists of three main modules: saliency-sensitive encoder, TM-Summary decoder, and VM-Summary regressor.\nSaliency-sensitive encoder. The input video is first embedded as a feature sequence V' = \\( \\{v'_i\\}_{i \\in [0,t]}, v'_i \\varepsilon R^d \\) via a pretrained model [26, 38], where t is the length of the sequence, and d represents the feature dimension. V' is then fed into the saliency-sensitive encoder. Following [36], each encoder layer contains a saliency-sensing layer for learning temporal saliency information, except for the traditional multi-head attention layer followed by a position-wise feed-forward layer. The saliency-sensing layer first calculates the sigmoid function \\( s^i \\) based on the output of the feed-forward layer. Then, it uses \\( s^i \\) as weights to scale the input. In this way, the importance of visual content is considered during temporal modelling. Its calculation process is as follows, and all layers included do not change the dimension of the input vector:\n\\( V^2 = V^{i-1} + MultiHeadAttention(V^{i-1}) \\) (1)\n\\( s^i = \\sigma(PositionwiseFeedForward(V^i)) \\) (2)\n\\( V^i = s^i . v^i \\) (3)\nwhere \\( \\sigma \\) denotes the sigmoid function, and \\( s^i \\) represents the saliency of the visual features output by the i-th layer of the encoder.\nVM-Summary regressor. Two linear layers transform the score \\( s^i \\) from the last saliency-sensing layer into the predicted saliency score S. Scores are then transformed into F using the same extraction algorithm described in Section 3.1.\nTM-Summary decoder. The TM-Summary decoder [7] utilizes the input visual features \\( V^N \\) and textual tokens to generate TM-Summary. Its attention layer is constrained so that textual tokens can only attend to previously generated tokens, and visual features cannot attend to textual features. During training, masked ground-truth TM-Summary is used as input. During inference, the decoder generates TM-Summary in an auto-regressive manner."}, {"title": "4.2 Training Strategy", "content": "Following previous approaches [24], we employ masked language modelling (LMLM) as the optimization objective for textual-modal summarization. A specific percentage of the ground-truth TM-Summary is replaced with a [MASK] token, and the model is required to predict these masked tokens. This objective effectively drives the model to learn cross-modality representations."}, {"title": "4.3 Evaluation of BiSSV", "content": "As the BiSSV task involves multimodal output, a global evaluation metric is needed to evaluate the overall performance of different approaches.\nWe first explore visual-modal summarization metrics for inspiration. One way to evaluate VM-summary is to assess global ranking similarity, as proposed by [29], which measures the alignment of the ground-truth and the predicted saliency score sequences. However, these metrics overlook the inherent inequality among segments during the summarization process. Given the direct correlation between score prediction and VM-Summary extraction, more salient segments should have a more significant influence on the evaluation result. However, previous metrics [17, 51] that measure global ranking similarity treat segments equally. A quantitative example is illustrated in Figure 5, where Prediction A incorrectly predicts the highest-scored segments. In contrast, Prediction B makes an incorrect prediction on the lowest-scored segments. Previous metrics, including Kendall's \\( \\tau \\) [17] and Spearman's \\( \\rho \\) [51], favor Prediction A more, though its mistaken prediction of most salient segments directly leads to inaccurate VM-Summary. Only our proposed NDCG@15%, taking ground-truth saliency as weights for different segments, can distinguish Prediction B's superior performance in prioritizing higher-scored segments. We introduce NDCG in detail as follows.\nBy assigning weights to ground-truth scores based on predicted saliency ranking (Eq. (4)), NDCG naturally gives greater significance to segments with higher ground-truth saliency. Therefore, we employ NDCG [13] for bimodal summarization evaluation. We use NDCG@15% and NDCG@all to represent the ranking similarity of\n\\( DCG@k = \\sum_{j=1}^{k} \\frac{2^{s_j} - 1}{log_2(j + 1)} \\) (4)\n\\( NDCG@k = \\frac{DCG@k}{maxDCG@k} \\) (5)\nwhere j represents the ranking of a visual feature in the model's prediction; \\( s_j \\) represents the ground-truth saliency score of the j-th ranked feature; maxDCG@k denotes the DCG@k when the model's output ranking is identical to the ground-truth; k indicates the number of elements taken into account (the top k visual features, in our case).\nOur overall optimization objective is the combination of \\( L_{MLM} \\) and \\( L_{RBL} \\). We observe that learning textual-modal summarization takes longer than learning temporal saliency. Therefore, we first only optimize \\( L_{MLM} \\) for N epochs as a warm-up for textual-modal summarization, then simultaneously train UBiSS with \\( L_{MLM} + L_{RBL} \\). More details about the texual-modal summarization warm-up are described in Appendix A.3."}, {"title": "5 EXPERIMENTS", "content": "Videos are downsampled to 8 fps. We use the ImageNet-1k pretrained GoogleNet [38] pool (5b) layer and Video Swin Transformer [26] for feature extraction. For GoogleNet, the mean pooling of features every 2 seconds is used as the pre-extracted video features. The feature dimension is 1024. The saliency-sensitive encoder has an embedding dimension of 1024, an intermediate layer dimension of 2048, and 4 encoder layers with 4 attention heads. The TM-Summary decoder has a maximum text sequence length of 45 and is implemented based on the Hugging Face Transformer\u00b2. The VM-Summary regressor employs a dropout probability of p=0.5. We use the AdamW optimizer [27] and the MultiStepLR learning rate scheduler. Textual-modal summarization warm-up epoch N is set to 30. We use CLIP-ViT-B/32 [34] as the vision-language pre-training model to calculate NDCGMS. Unless otherwise specified, the test results are reported after training for 200 epochs with batch size 32, and the model is selected based on CIDEr [40] of the validation set. We report the average results of five runs with random seeds."}, {"title": "5.2 Comparison to Multi-stage Baselines", "content": "Table 3 presents the comparison of our UBiSS with multi-stage baselines (Swin-PGL and PGL-Swin) that simply integrate two state-of-the-art unimodal summarization methods: PGL-SUM [2] is a traditional visual-modal summarization model that utilizes global and local attention with positional encoding to identify highlighted moments in videos. SwinBERT [24] is the first end-to-end video captioning model that updates both the visual extractor and text generator during training. For Swin-PGL, we train a linear layer that projects the output token embeddings of SwinBERT [24] into the same dimension as the visual features and concatenates them to train PGL-SUM [2]. For PGL-Swin, we use VM-Summaries extracted by PGL-SUM [2] to train and test SwinBERT [24]. The pretrained SwinBERT and PGL-SUM are selected based on CIDEr [40] and Kendall's \\( \\tau \\) [17], respectively.\nAs shown in Table 3, using only VM-Summary as video inputs results in diminished performance in the textual-modal summarization for PGL-Swin. Swin-PGL, utilizing a robust textual-modal summarization baseline [24] that incorporates an online visual extractor, attains the highest NDCGTM score. However, the performance of Swin-PGL lags behind UBiSS in visual-modal summarization, suggesting that a straightforward fusion method fails to leverage the connections between modalities thoroughly."}, {"title": "5.3 Comparison to Unimodal Summarization", "content": "We also compare UBiSS with unimodal summarization models. The comparison to visual-modal summarization model PGL-SUM [2] and other basic models are presented in Table 4, all chosen by Kendall's \\( \\tau \\) [17]. All models reported are trained with 2s-feature. Since F-score is significantly influenced by the post-processing algorithm [29], we obtain the final VM-Summary by the VM-Summary extraction algorithm introduced in Section 3.1 to ensure fair comparison. From Table 4, UBiSS outperforms other baselines in terms of both interval overlap (F-score) and global ranking similarity (Kendall's \\( \\tau \\) [17] and Spearman's \\( \\rho \\) [51]), as our model could learn saliency information from the integration of different modalities.\nThe comparison results between UBiSS and the end-to-end video captioning model SwinBERT [24] are presented in Table 5. The comparison is not entirely fair since UBiSS utilizes pre-extracted visual features and has significantly fewer learnable parameters than SwinBERT, which relies on an online feature extractor and requires substantial computational resources. For instance, the 32-frame version of SwinBERT consumes approximately 16 times more GPU memory during training compared to UBiSS with the same batch size. We observed significant improvements with better visual features for UBiSS, indicating a potential direction for advancement."}, {"title": "5.4 Loss Comparison", "content": "The comparison between UBiSS trained with different losses is shown in Table 6. In contrast to the traditional Mean Square Error (MSE) regression objective, we empirically observe that employing a list-wise ranking-based objective [32] enables the model to grasp the relative relationships among video frames. Consequently, UBiSS (NeuralNDCG) demonstrates improvements across the majority of metrics, indicating that our optimization objective can effectively enhance performance in both traditional and our proposed metrics. Specifically, models trained with MSE may take shortcuts by pushing the average score of the model's prediction toward the average score of the ground-truth data. Figure 6 also shows that the model trained with MSE tends to produce overly smoothed prediction, resulting in a lower NDCGVM score. In contrast, the model trained with a ranking-based loss is much more sensitive to temporal saliency changes."}, {"title": "5.5 Human Evaluation and Qualitative Studies", "content": "We recruit 11 participants to score 30 groups of summaries based on their Satisfaction (Satis) and Informativeness (Inform). Each example is scored from 1 to 5 (the higher, the better). The results from Table 7 show combining bimodal summary could significantly improve the browsing experience, as TM-Summary and VM-Summary can complement each other. To evaluate the performance of UBiSS and pipeline-type baselines, the participants also score 20 randomly selected sets of summarization results in terms of accuracy and consistency, as presented in Table 8. UBiSS trained with NeuralNDCG(N) [32] outperforms other approaches across all metrics, suggesting a stronger ability to capture highlighted visual-modal information could enhance the quality of summaries. The inter-annotator agreement for comparing different forms of summaries is 0.646, and for comparing bimodal summaries generated by UBiSS and baselines is 0.604. Spearman's correlation coefficient is used to compute the agreement. More details are in Appendix A.4.\nFigure 7 visualizes bimodal summaries generated by pipeline-type baselines and UBiSS. PGL-Swin suffers from inaccuracies in identifying salient segments, leading to TM-Summary either unrelated to the video (case (a)) or failing to pinpoint the highlight moments (case (b)). While text embeddings assist Swin-PGL in filtering out less relevant visual details, it still encounters challenges with longer videos, as demonstrated in case (b) where the model incorrectly identifies the hotel room as the primary focus. In contrast, UBiSS has a superior capability in capturing salient moments, resulting in more accurate summaries."}, {"title": "6 CONCLUSION", "content": "We introduce BIDS, a new large-scale biomodal summarization dataset designed to support the BiSSV task. Based on BIDS, we then propose a unified framework named UBiSS, which harnesses saliency information to enhance the quality of bimodal summaries. A list-wise ranking-based optimization objective [32] is employed to help the model learn saliency trends. Furthermore, we design a novel evaluation metric NDCGMS for joint assessment of bimodal outputs. Experiments show that UBiSS excels in capturing salient content, resulting in superior bimodal summarization performance. The enriched semantics of bimodal summaries also leads to a more satisfying and informative browsing experience. In the future, we will extend our UBiSS to tackle more intricate tasks, such as user-guided multimodal summarization."}, {"title": "A APPENDIX", "content": "A.1 Related Task Comparison\nWe could categorize related tasks into four categories based on input/output modality: Textual-Modal Summarization of Videos (TMSV), Visual-Modal Summarization of Videos (VMSV), Bimodal Semantic Summarization of Videos (BiSSV), and Bimodal Summarization of Multimodal Data (BSMD). The former two tasks are primarily concerned with unimodal summarization, while BiSSV and BSMD focus on multimodal summarization. Figure 8 illustrates a detailed task comparison.\nCan unimodal summarization datasets be modified for BiSSV? One closely related task in unimodal summarization is Dense Video Captioning (DVC) [18], which provides text description and grounded segments for the events within the video. DVC could be divided into two sub-tasks: event localization and event captioning [44, 49]. The primary divergence between BiSSV and DVC lies in their objectives. The goal of BiSSV is to generate a TM-Summary as a global overview of the entire video instead of chronological captions, and the VM-Summary is a collection of highlighted moments instead of localized events. This difference in focus implies that simply concatenating dense captions from DVC may not produce a suitable TM-Summary, as in the case of the first video-to-video&text dataset VideoXum [23], which has TM-Summaries that are, on average, 49.9 words in length.\nCan BSMD datasets be modified for BiSSV? Recently, the task of multimodal summarization with multimodal output [50], a typical task of BSMD, is proposed to generate a bimodal summary for multimodal inputs. Some BSMD approaches use video and corresponding textual metadata (document [9, 21, 39] or transcript [9, 11, 33]) as input, yielding a bimodal summary comprising text descriptions and keyframes [9, 21, 33, 39] or key segments [11]. We summarize BSMD datasets with video input in Table ?? for comparison.\nIn summary, we discover that BSMD can be more accurately characterized as a combination of unimodal summarization tasks, with a primary focus on information extraction, e.g. direct selection of the most informative sentences from the source text [11] or detailed description of transcripts [33]. Given the time-consuming nature of acquiring auxiliary information in real-world scenarios, BSMD and BiSSV serve distinct application purposes. BSMD is particularly well-suited for video-contained documents, such as news articles, while BiSSV is better suited for various web applications like video browsing, retrieval and recommendation. Furthermore, it's worth mentioning that the majority of visual outputs of BSMD datasets consist of keyframes [9, 21, 33, 39], which lacks smoothness for online browsing compared to short videos."}, {"title": "A.2 Pseudo-Code for VM-Summary Extraction", "content": "We provide a pseudo-code for VM-Summary extraction in Algorighm 1."}, {"title": "A.3 Textual-Modal Summarization Warm-up", "content": "Results of different epochs for textual-modal summarization warm-up are presented in Table 10, all chosen by CIDEr [40]. The model's capability in visual-modal summarization demonstrates an initial improvement followed by a decline as the number of warm-up epochs increases. It is important to note that though the precise number of epochs required for textual-modal summarization warm-up may vary across different machines, a closer alignment between visual-modal and textual-modal summarization consistently yields superior results. Inadequate training or overfitting for each sub-task can lead to a decline in overall performance.\nThe results presented in Table 10 also reveal an issue with the ranking-based optimization objective [32], as discussed in Section 5.2. This issue arises when the model takes shortcuts by assigning extremely low scores to insignificant features. For instance, when considering models trained with N=10 and 20 or 30 warm-up epochs, we observe a consistent increase in NDCGVM. However, there is a decline in textual-modal summarization performance, indicating that partially absent features may not be sufficient to generate a global TM-Summary. We address exploring the integration of saliency learning and visual modeling as a promising direction for future research."}, {"title": "A.4 Human Evaluation Details", "content": "We conduct human evaluation under two settings. To evaluate how bimodal summaries could contribute to Satisfaction (Satis) and Informativeness (Inform) in comparison with unimodal summaries, we randomly select 30 sets of videos and ask participants to score VM-Summary only, TM-Summary only, and bimodal summaries on a scale ranging from 1 to 5. For both metrics, a rating of 1 indicates very dissatisfactory while 5 indicates very satisfactory. For informativeness, a rating of 1 indicates the video content was not summarized adequately while 5 indicates that the video content was perfectly summarized. We present different forms of summaries to participants after shuffling. Figure 9 (a) shows an example set of summaries.\nBesides automatic evaluation metrics, we also conduct a comparative evaluation between UBiSS and concatenated unimodal summarization baselines. We randomly sample 20 sets of videos. Different summaries generated by UBiSS trained with NeuralNDCG/MSE, PGL-Swin, and Swin-PGL are presented to participants in random order. The participants are asked to rate the accuracy of VM- and TM-Summary, based on how they could capture the highlights (VM-Summary) or present a global overview (TM-Summary), along with the consistency of bimodal summaries. Figure 9 (b) offers an illustrative example.\nThe participants are college students with an educational background in computer science. The average age of the participants is 22, and the gender ratio is 7:4 (males: females). According to DataReportal (due April 2023)3, most YouTube users are between 25 and 34, and the gender ratio is approximately 1.195. Therefore, participants' distribution is similar to real-world users' distribution."}]}