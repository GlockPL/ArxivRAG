{"title": "MultiPDENet: PDE-embedded Learning with Multi-time-stepping for Accelerated Flow Simulation", "authors": ["Qi Wang", "Yuan Mi", "Haoyun Wang", "Yi Zhang", "Ruizhi Chengze", "Hongsheng Liu", "Ji-Rong Wen", "Hao Sun"], "abstract": "Solving partial differential equations (PDEs) by numerical methods meet computational cost chal- lenge for getting the accurate solution since fine grids and small time steps are required. Machine learning can accelerate this process, but strug- gle with weak generalizability, interpretability, and data dependency, as well as suffer in long- term prediction. To this end, we propose a PDE-embedded network with multiscale time stepping (MultiPDENet), which fuses the scheme of nu- merical methods and machine learning, for ac- celerated simulation of flows. In particular, we design a convolutional filter based on the structure of finite difference stencils with a small number of parameters to optimize, which estimates the equiv- alent form of spatial derivative on a coarse grid to minimize the equation's residual. A Physics Block with a 4th-order Runge-Kutta integrator at the fine time scale is established that embeds the structure of PDEs to guide the prediction. To alleviate the curse of temporal error accumula- tion in long-term prediction, we introduce a mul- tiscale time integration approach, where a neural network is used to correct the prediction error at a coarse time scale. Experiments across vari- ous PDE systems, including the Navier-Stokes equations, demonstrate that MultiPDENet can accurately predict long-term spatiotemporal dy- namics, even given small and incomplete train- ing data, e.g., spatiotemporally down-sampled datasets. MultiPDENet achieves the state-of-the-art performance compared with other neural base- line models, also with clear speedup compared to classical numerical methods.", "sections": [{"title": "1. Introduction", "content": "Complex spatiotemporal dynamical systems, e.g., climate system (Schneider et al., 2017) and fluid dynamics (Ferziger et al., 2019), are fundamentally governed by partial differ- ential equations (PDEs). To capture the intricate behaviors of these systems, various numerical methods have been de- veloped. Direct Numerical Simulation (DNS) is a widely used method for solving PDEs. It requires specifying initial conditions (ICs), boundary conditions (BCs), and PDE pa- rameters, followed by discretizing the equations on a grid using techniques like finite difference (FD), finite element (FE), finite volume (FV), or spectral methods. Despite their accuracy, traditional numerical methods face key challenges of high computational costs (Goc et al., 2021), when ad- dressing with high-dimensional problems or necessitating fine spatial and temporal resolutions.\nRecent advances in deep learning have introduced neural- based approaches (Lu et al., 2021; Li et al., 2021; Gupta & Brandstetter, 2023) for solving PDEs. These data-driven methods eliminate the need for explicit theoretical formula- tions, enabling networks to learn underlying patterns directly from data through end-to-end training. While promising, these approaches face notable challenges, including a heavy dependence on large training datasets and limited generaliza- tion. For instance, achieving accurate predictions becomes particularly challenging when models encounter unseen ICs or scenarios beyond the training distribution.\nA representative work in the field of scientific computing introduces a novel paradigm with Physics-informed neural networks (PINNs) (Raissi et al., 2019), which incorporates physical prior knowledge (such as PDE residuals and I/BCs) as constraints within the loss function. This approach allows the network to fit the data while simultaneously maintain- ing a certain degree of physical consistency. Variants of PINNs (Raissi et al., 2020; Wang et al., 2020; Eshkofti & Hosseini, 2023) have shown notable success across various domains, reducing the dependency on extensive datasets to some degree. However, such methods still face scalability and generalizability challenges when applied to complex nonlinear dynamical systems. Additionally, optimizing com- plex loss functions (Rathore et al., 2024) and ensuring model interpretability remain challenges."}, {"title": "2. Related Work", "content": "Related works on numerical, machine learning, physics- inspired learning, and hybrid learning methods for simula- tion of PDE systems are given in Appendix A."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Problem Description", "content": "Let's consider a general spatiotemporal dynamical system governed by the following PDE:\n$u_t = F(u, u^2, . . ., \\nabla u, \\Delta u, . . . ; A) + f$,\n(1)\nwhere $u(x, t) \\in R^n$ denotes the physical state in the spa- tiotemporal domain $\\Omega\\times [0, T]$; $u_t$ the first-order time deriva- tive term; $F(\\cdot)$ a linear/nonlinear functional parameterized by PDE parameters A (e.g., the Reynolds number Re); $\\nabla$ the Nabla operator is defined as $[d_x, d_y, ...]^T$; and f the source term. Additionally, we define $I(u, u_t; x \\in \\Omega, t = 0) = 0$ and $B(u, \\nabla u,...... ; x \\in \\partial\\Omega) = 0$ specified ICs and BCs, where $\\partial\\Omega$ represents the domain boundary.\nWe aim to accelerate the simulation of fluid flows by using a PDE-embedded network with multiscale time stepping based on a limited training data (coarse in both spatial and temporal scales). The model is capable of rapid simula- tion, achieving high solution accuracy while demonstrating strong generalizability across varying ICs, source terms, complex domains, and PDE parameters."}, {"title": "3.2. Model Architecture", "content": "In this section, we introduce MultiPDENet and show how our model efficiently captures the underlying spatiotem- poral dynamics. As illustrated in Figure 1(a), predicting $u^{k+1}$ from the input $u^k$ involves two main components: the Physics Block and the M\u2082NN Block."}, {"title": "3.2.1. MULTI-SCALE FORWARD TIME STEPPING SCHEME", "content": "While the learnable neural solver can be used independently, its accuracy for long-term prediction is limited due to error accumulation. To address this issue, we introduce a multi- scale time stepping scheme, incorporating micro-scale and macro-scale steps, to improve predictive accuracy and en- ables fast prediction of PDE solutions on coarse spatiotem- poral grids. Specifically, we define two types of time step- ping: micro-scale step and macro-scale step, to enhance the performance of spatiotemporal dynamics prediction. At the macro scale, given the coarse solution $u^{k}$ at time $t^k$, MultiPDENet is expected to predict the next-step solution $u^{k+1}$ at $t^{k+1}$, which can be expressed as:\n$u^{k+1} = u^{k} + \\sum_{m=1}^M d\\bar{u}^{k}_m + M_{a}NN(u^{k}, \\Delta t, d_x),$ (2)\nwhere \u0394t is the macro-scale time interval, and $d_x$ the spatial resolution of mesh grid. Here, $d\\bar{u}^{k}_m$ is the incremental up- date by the Physics Block (see Section 3.2.2) at each micro step, as shown in Eq. (3), where M denotes the number of micro-scale time steps in one macro-scale step (e.g., M = 4 herein). The M\u2082NN Block (see Section 3.2.4) refines these incremental updates generated by the Physics Block on coarse grids, yielding the final update for the macro step."}, {"title": "3.2.2. PHYSICS BLOCK: LEARNABLE NEURAL SOLVER", "content": "To accurately predict at the micro-scale step, we developed a neural solver, referred to as the Physics Block, as illustrated"}, {"title": "PDE Block.", "content": "The PDE block computes the residual of the governing PDEs. It incorporates a learnable filter bank with symmetry constraints, which calculates derivative terms based on the corrected solution produced by a Correction"}, {"title": "Poisson Block.", "content": "In solving incompressible NSE, the pres- sure term, p, is obtained by solving an associated Pois- son equation. To compute the pressure field, we imple- mented a pressure-solving module shown in Figure S1(a), which solves the Poisson equation, $\\Delta p = \\psi(u)$, where $\\psi(u) = 2 (u_xv_y \u2013 u_yv_x)$ for 2D problems (the subscripts indicate the spatial derivatives along x or y directions). To"}, {"title": "3.2.3. ADAPTIVE FILTER WITH CONSTRAINT", "content": "Traditional FD methods often yield inaccurate derivatives on coarse grids. To address this, we propose a learnable filter with constraints that approximates equivalent derivatives on coarse grids, minimizing the PDE residuals during training and thereby improving the model's predictive accuracy. By leveraging the symmetry of central difference stencils, our filter maintains structural integrity while enhancing network flexibility. As shown in Figure 2, we construct two 5 \u00d7 5 symmetric matrices, each requiring only six learnable parameters due to symmetry constraints. These matrices are designed to compute the first-order (g') and second- order (g", "g": "s = 4\u00d7(a3+a4+a5+a6)+2\u00d7(a1+a2). This design leverages the structural properties of central difference methods. By satisfying the Order of Sum Rules (Long et al., 2018), this filter can achieve up to 4th-order accuracy in approximating the derivatives via optimization of trainable parameters."}, {"title": "3.2.4. NN BLOCK", "content": "To alleviate the error accumulation during long-term predic- tions on coarse grids, we introduce the M\u00a1NN and MaNN Blocks, operating at micro- and macro-scales, respectively. The M\u00a1NN Block employs a lightweight model (e.g., FNO, DenseCNN (Liu et al., 2024)) for efficient micro-step pre- dictions, whereas the MaNN Block delivers more accurate predictions at larger steps (Gupta & Brandstetter, 2023). In this study, we utilized FNO as the M\u00a1NN Block and UNet"}, {"title": "4. Experiment", "content": "We validate the performance of our method against baseline models on various PDE datasets. We then perform general- ization tests across different external forces (f), Reynolds numbers (Re), and domain sizes on the Kolmogorov flow (KF) dataset. Finally, we present ablation studies to demon- strate the contributions of each component in our model."}, {"title": "4.1. Setup", "content": "Dataset. We generate the data using high-order FD/FV methods with high resolution under periodic boundary con- ditions and then downsample it spatially and temporally to a coarse grid. The low-resolution dataset is used for both training and testing. We consider four distinct dynamical systems: Korteweg-de Vries (KdV), Burgers, Gray-Scott"}, {"title": "4.2. Sovling PDE Systems", "content": "KdV. The primary challenge of this dataset lies in accu- rately capturing the complex interplay between nonlinearity and dispersion, leading to phenomena like soliton formation (Gardner et al., 1967). As shown in Figure 3(a), each base- line model struggles to produce accurate predictions, with DeepONet exhibiting significant divergence. In contrast, MultiPDENet demonstrates superior accurate predictions for ICs outside the training range. The correlation curve in Figure 3(b) highlights the significantly higher correla- tion of MultiPDENet compared to the baselines. The error distribution in Figure 3(c) confirms its lower error levels. Table 2 shows our model's generalizability, with performance improvements ranging from 61.1% to 186.3%.\nBurgers. As shown in Figure 3(d), the solution snapshots predicted by MultiPDENet are significantly more accurate than those of the baseline models. The baseline models, limited by the coarse training data, produce incorrect pre- dictions. The correlation curve in Figure 3(e) shows that MultiPDENet maintains a high correlation with the ground truth throughout the prediction, while other baseline models diverge. This is further evidenced by the error distribution in Figure 3(f), demonstrating that MultiPDENet's error is over an order of magnitude lower than that of the baselines. Table 2 confirms these findings with MultiPDENet's im- provements exceeding 94.1% across all evaluation metrics.\nGS. This reaction-diffusion system is nonlinear, making it challenging to capture its complex patterns (see Figure 3(g)). Only MultiPDENet accurately predicts the trajectory evolu- tion. The baseline models struggle to learn the spatiotem- poral dynamics, and even PeRCNN, despite its embedded physics, produces inaccurate predictions due to the limited"}, {"title": "4.3. Model Generalization", "content": "We conducted generalization tests on the KF flow dataset to assess our model's ability to capture the underlying dynam- ics. The model was initially trained on 5 sets of trajectories,"}, {"title": "Test on flow with $Re = 4000$.", "content": "Turbulence at high Re's presents significant challenges for prediction due to its non- linearity and complex vortex structures. To further demon- strate the superior capability of our model, we conducted an"}, {"title": "Test on flow within larger domains.", "content": "We extended the spa- tial domain from $(0, 2\u03c0)^2$ to $(0, 4\u03c0)^2$ to further evaluate our model's generalizability over larger mesh grids in a more complex scenario. Larger domains introduce diverse physi- cal phenomena, challenging the model to capture global and local dynamics on coarse grids. Using the same 64 \u00d7 64 grid, we tested our trained model on 10 unseen trajecto- ries (details in Appendix C). As shown in Appendix Figure S4(a), the snapshots over 300 time steps remain accurate. The correlation curve in Appendix Figure S4(b) depicts our"}, {"title": "4.4. Ablation Study", "content": "To quantify the contribution of each module, we conducted ablation experiments on the KF dataset. Specifically, we compared the following model variations: (1) Model A (no Poisson Block); (2) Model B (no filter structure constraint); (3) Model C (only Physics Block for prediction); (4) Model D (FD convolution instead of symmetric filter); (5) Model E (no Correction Block); (6) Model F (no M\u00a1NN Block); (7) Model G (no MaNN Block); (8) Model H (no Physics Block); (9) Model I (forward Euler); and (10) the full model."}, {"title": "5. Conclusion", "content": "We introduce an end-to-end physics-encoded network (aka, MultiPDENet) with multi-scale time stepping for acceler- ated simulation of spatiotemporal dynamics such as turbu- lent flows. MultiPDENet consists of a multi-scale temporal learning architecture, a learnable Physics Block for solution prediction at the fine time scale, where trainable symmetric filters are designed for improved derivative approximation on coarse spatial grids. Such a method is capable of long- term prediction on coarse grids given very limited training data (see the data size scaling test in Appendix D.4). Mul- tiPDENet outperforms other baselines through extensive tests on fluid dynamics and reaction-diffusion equations. In particular, such a model excels in generalizability over ICs, Reynolds numbers, and external forces in the turbulent flow experiments. MultiPDENet also exhibits strong stabil- ity in long-term prediction of turbulent flows, effectively capturing both global and local patterns in larger domains.\nWe also tested the computational efficiency of trained Multi- PDENet for accelerated flow prediction (more details shown in Appendix G.2). For a certain given accuracy (e.g., corre- lation \u2265 0.8), MultiPDENet achieves \u2265 5\u00d7 speedup com- pared with GPU-accelerated DNS (Appendix Table S8), e.g., JAX-CFD, where all the tests were performed on a single Nvidia A100 80G GPU. However, MultiPDENet still faces some unresolved challenges. Firstly, the model currently only handles regular grids, due to the limitation of convolu- tion operation used in the model. In the future, we aim to address this issue by incorporating graph neural networks to manage irregular grids. Secondly, the model has only been currently tested on 1D and 2D problems. We will extend it to 3D systems in our future work."}, {"title": "APPENDIX", "content": null}, {"title": "A. Related work", "content": null}, {"title": "Numerical Methods.", "content": "Numerical methods have been extensively applied to solve PDEs. Approaches such as FD (Thomas, 2013), FE (Zienkiewicz et al., 2005), and FV methods (Moukalled et al., 2016) discretize the continuous domain into mesh grids, transforming PDEs into algebraic equations that can be solved with high accuracy. However, these methods often require fine spatiotemporal grids and substantial computational resources to achieve accurate solutions, particularly in high-dimensional spaces. This leads to two main challenges: (1) the need for repeated computations when conditions change (e.g., ICs); (2) the demand for fast simulations in many industrial applications."}, {"title": "Machine Learning Methods.", "content": "Building on the success of machine learning in fields like natural language processing (Vaswani, 2017) and computer vision (He et al., 2016), these techniques have also been applied to solving PDEs. With abundant labeled data, it is possible to train end-to-end models to predict solutions. Representative works include ResNet (Lu et al., 2018), CNN-based models (Bhatnagar et al., 2019; Stachenfeld et al., 2022; Gupta & Brandstetter, 2023), Transformers-based models (Cao, 2021; Geneva & Zabaras, 2022; Li et al., 2024) and Graph-based models (Brandstetter et al., 2022b). Many notable neural operators (Lu et al., 2021; Li et al., 2021; Rahman et al., 2023; Bonev et al., 2023), which learn a mapping between functional spaces, enable the approximation of complex relationships in PDEs. While these methods show promise in learning complex dynamics and approximating solutions, they often require substantial amounts of labeled data."}, {"title": "Physics-inspired Learning Methods.", "content": "Recently, physics-inspired learning methods have demonstrated impressive ca- pabilities in solving PDEs, which can be classified into two categories according to the way of adding prior knowledge: physics-informed and physics-encoded. The physics-informed methods take PDEs and I/BCs as a part of the loss functions (e.g., the family of PINN (Raissi et al., 2019; 2020; Wang et al., 2020; Tang et al., 2024), PhySR (Ren et al., 2023)). On the other hand, the physics-encoded methods employ a different approach that preserves the structure of PDEs, ensuring that the model adheres to the given equations to capture the underlying dynamics, e.g., EquNN (Wang et al., 2021), TiGNN (Hern\u00e1ndez et al., 2023), and PeRCNN (Rao et al., 2023). In addition, other related studies (Long et al., 2018; Kossaczk\u00e1 et al., 2021) have explored the use of CNN as alternative spatial derivative operators for approximating derivatives and capturing the dynamics of interest."}, {"title": "Hybrid Learning Methods.", "content": "Hybrid learning methods combine the strengths of numerical approaches and NNs to improve prediction accuracy. For efficient modeling of spatiotemporal dynamics, these methods can be trained on coarse grids. Representative methods include FV-based neural methods (Kochkov et al., 2021; Sun et al., 2023), FD-based neural methods (Zhuang et al., 2021; Liu et al., 2024; Wang et al., 2024), and spectral-based neural methods (Dresdner et al., 2023; Arcomano et al., 2022). While these approaches show efficacy in modeling spatiotemporal dynamics, their representation capacities are often limited by the fixed structure of their numerical components. As a result, most of these models still require sufficiently large amounts of training data."}, {"title": "B. The Details of MultiPDENet", "content": null}, {"title": "B.1. Correction Block", "content": "The Correction Block leverages a neural network to refine the coarse solution, with the Fourier Neural Operator (FNO) (Li et al., 2021) as the correction mechanism within this block. FNO functions by decomposing the input field into frequency components, processing each frequency individually, and reconstructing the modified spectral information back into the physical domain via the Fourier transform. This layer-wise update process is expressed as:\n$v^{l+1}(x) = \\sigma (W^lv^l(x) + (\\kappa(\\phi)v^l) (x))$, (S1)\nwhere $v^l(x)$ denotes the latent feature map at the l-th layer, defined on the coarse grid x. The initial feature map is $v^0(x) = P(\\bar{u}^k_m)$, where P is a local mapping function that projects $\\bar{u}^k_m$ into a higher-dimensional space. The kernel integral transformation is defined as $\\kappa(\\phi)(z) = iFFT(R\\cdot FFT(z))$, which applies the Fourier transform, spectral filtering via R, convolution in the frequency domain, and the inverse Fourier transform to the latent feature map z. Here, $\\phi$ represents the trainable parameters, $\\sigma(\\cdot)$ is the GELU activation function, and $W^l$ denotes the weights of the linear layer."}, {"title": "B.2. Physics Block", "content": "To accurately predict at the micro-scale step, we developed a neural solver called the Physics Block, ensuring stability, accuracy, and efficiency through adherence to the Courant-Friedrichs-Lewy (CFL) conditions (LeVeque, 2007). The Physics Block comprises three key components: the Poisson Block (Figure S1(a)-b)), the PDE Block (Figure 1(c)), and the M\u00a1NN Block (Figure S1(c))."}, {"title": "Poisson Solver.", "content": "The pressure field is computed using the spectral method, which involves solving the Poisson equation:\n$\\Delta p = \\psi(u)$. (S2)\nHere, $\\psi(u) = 2 (u_xv_y - u_yv_x)$ represents the source term for the pressure.\nApplying the Fast Fourier Transform (FFT) to Eq. (S2), we obtain:\n$-(k^2_x + k^2_y)p^* = \\psi^*(u)$, (S3)\nwhere $k_x$ and $k_y$ are the wavenumbers in the x and y directions, respectively. Assuming $k^2_x + k^2_y \\neq 0$, we can solve for the pressure in the frequency domain:\np^* = \\frac{\\psi^*(u)}{-(k^2_x + k^2_y)}$ (S4)\nFinally, the pressure field is recovered in the spatial domain using the inverse FFT (iFFT):\np = iFFT [p^*]. (S5)\nThis spectral method offers an efficient approach to calculating the pressure field without the need for labeled data or training."}, {"title": "BC encoding.", "content": "To ensure that the solution obeys the given periodic boundary conditions and that the feature map shape remains unchanged after differentiation, we employ periodic BC padding (see Figure S2) in our architecture. This method of hard encoding padding not only guarantees that the boundary conditions are periodic, but also improves accuracy."}, {"title": "B.3. RK4 Integration Scheme", "content": "RK4 is a widely used numerical integration method for solving ordinary differential equations (ODEs) and PDEs, commonly employed as a time integration solver. It provides a balance between computational efficiency and accuracy by calculating intermediate slopes at various points within each time step. The general numerical integration method for time marching from $u_{t_j}$ to $u_{t_{j+1}}$ can be written as:\n$u_{j+1} = u_j + \\int_{t_j}^{t_{j+1}} B(u_j (x, t))dt$. (S6)\nAmong them, $u_{j+1}$ and $u_j$ are solutions at time j + 1 and j. RK4 is a high-order integration scheme, which divides the time interval into multiple equally spaced small time steps to approximate the integral. The final update of the above state change can be written as:\n$r_1 = B (u_j, t_j)$,\n$r_2 = B\\Big(u_j + \\frac{\\delta t}{2} r_1, t_j + \\frac{\\delta t}{2}\\Big)$,\n$r_3 = B\\Big(u_j + \\frac{\\delta t}{2} r_2, t_j + \\frac{\\delta t}{2}\\Big)$,\n$r_4 = B (u_j + \\delta t \u00d7 r_3, t_j + \\delta t)$,\n$u_{j+1} = u_j + \\frac{1}{6} \\delta t(r_1 + 2r_2 + 2r_3 +r_4)$, (S7)\nwhere \u03b4t denotes the step size and $r_1, r_2, r_3, r_4$ represent four intermediate variables (slopes). The global error is proportional to the step size to the fourth power, i.e., $O(\\delta t^4)$."}, {"title": "C. Data Details", "content": "KdV. The Korteweg-de Vries (KdV) equation is a well-known nonlinear PDE used to describe the movement of shallow water waves with small amplitude in a channel. It models the dynamics of these waves and is particularly noted for its ability to represent solitary waves, or solutons, given by:\n$\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} + \\frac{\\partial^3 u}{\\partial x^3} = 0$, (S8)"}, {"title": "Burgers.", "content": "This equation models the behavior of a viscous fluid (Kumar, 2023), incorporating both nonlinear dynamics and diffusion effects. It finds extensive applications across various scientific disciplines, including fluid mechanics, materials science, applied mathematics and engineering. The equation is expressed as follows:\n$\\frac{\\partial u}{\\partial t} = v\\nabla^2u \u2013 u \\cdot Vu,  t\\in [0,T]$, (S9)\nwhere u = {u, v} \u2208 R2 represents the fluid velocities, v is the viscosity coefficient set to 0.002, and \u2206 is the Laplacian operator."}, {"title": "GS.", "content": "The Gray-Scott (GS) reaction-diffusion model is a system of PDEs that describes the interaction and diffusion of two reacting chemicals. It is known for its ability to produce intricate and evolving patterns, making it a popular model for studying pattern formation. It is widely used in fields such as chemistry, biology, and physics to simulate processes like chemical reactions and biological morphogenesis. The equation is expressed by:\n$\\frac{\\partial u}{\\partial t} = D_u\\Delta u \u2013 uv^2 + a(1 \u2013 u)$,\n$\\frac{\\partial v}{\\partial t} = D_v\\Delta v + uv^2 \u2013 (\u03b1 + \u03ba)v$, (S10)\nwhere u and v denote the concentrations of two distinct chemical species, with $D_u$ and $D_v$ indicating their respective diffusion coefficients. The first equation models the change in the concentration of u over time. The term $D_u\\Delta u$ represents the diffusion of u, $-uv^2$ describes the reaction between u and v, and $a(1 \u2013 u)$ represents the replenishment of u based on the feed rate a. The second equation models the evolution of v, where $D_v\\Delta v$ accounts for diffusion, $uv^2$ represents the creation of v from the reaction with u, and \u2013 (\u03b1 + \u03ba)v describes the decay of v, with K as the decay rates."}, {"title": "NSE.", "content": "The Navier-Stokes equations (NSE) are fundamental to the study of fluid dynamics, governing the behavior of fluid motion. In this paper, we focus on a two-dimensional, incompressible Kolmogorov flow with periodic boundary conditions, expressed in velocity-pressure form as:\n$\\frac{\\partial u}{\\partial t} + (u \\cdot \\nabla)u =  -\\frac{1}{Re} \\nabla^2u - \\nabla p+f,  t\\in [0,T]$,\n$\\nabla \\cdot u = 0$, (S11)\nwhere u = {u, v} \u2208 R\u00b2 denotes the fluid velocity vector, p \u2208 R represents the pressure, and Re is the Reynolds number that characterizes the flow regime. The Reynolds number serves as a scaling factor in the NSE, balancing the inertial forces, represented by the advection term $(u \\nabla)u$, with the viscous forces, captured by the Laplacian term \u0394u. When Re is low, the flow remains predominantly laminar and smooth due to the dominance of the viscous forces. Conversely, at high Reynolds numbers, the inertial forces take precedence, leading to a more chaotic and turbulent flow behavior."}, {"title": "D. Additional Experimental Results", "content": null}, {"title": "Learned Interpolation (LI).", "content": "The LI (Kochkov et al., 2021) employs a finite volume approach enhanced with neural networks as a replacement for conventional polynomial-based interpolation schemes in computing velocity tensor product. The network adapts to the local flow conditions by learning a dynamic interpolation mechanism that can adjust to the characteristics of the flow. This enables LI to provide accurate fluid dynamics predictions even on coarse grids, improving computational efficiency while maintaining prediction fidelity."}, {"title": "Temporal Stencil Modeling (TSM).", "content": "TSM (Sun et al., 2023) addresses time-dependent partial differential equations (PDEs) in conservation form by integrating time-series modeling with learnable stencil techniques. It effectively recovers information lost during downsampling, enabling enhanced predictive accuracy. TSM is particularly advantageous for machine learning models dealing with coarse-resolution datasets."}, {"title": "G. Computational Details", "content": null}, {"title": "G.1. Training Details", "content": "All experiments (both training and inference) in this study were conducted on a single Nvidia A100 GPU (with 80GB memory) running on a server with an Intel(R) Xeon(R) Platinum 8380 CPU (2.30GHz, 64 cores). All model training efforts were performed on coarse grids (see Table 1)."}]}