{"title": "Semi-Supervised One-Shot Imitation Learning", "authors": ["Philipp Wu", "Kourosh Hakhamaneshi", "Yuqing Du", "Igor Mordatch", "Aravind Rajeswaran", "Pieter Abbeel"], "abstract": "One-shot Imitation Learning (OSIL) aims to imbue AI agents with the ability to\nlearn a new task from a single demonstration. To supervise the learning, OSIL\ntypically requires a prohibitively large number of paired expert demonstrations\ni.e. trajectories corresponding to different variations of the same semantic task. To\novercome this limitation, we introduce the semi-supervised OSIL problem setting,\nwhere the learning agent is presented with a large dataset of trajectories with no\ntask labels (i.e. an unpaired dataset), along with a small dataset of multiple demon-\nstrations per semantic task (i.e. a paired dataset). This presents a more realistic\nand practical embodiment of few-shot learning and requires the agent to effectively\nleverage weak supervision from a large dataset of trajectories. Subsequently, we\ndevelop an algorithm specifically applicable to this semi-supervised OSIL setting.\nOur approach first learns an embedding space where different tasks cluster uniquely.\nWe utilize this embedding space and the clustering it supports to self-generate pair-\nings between trajectories in the large unpaired dataset. Through empirical results\non simulated control tasks, we demonstrate that OSIL models trained on such self-\ngenerated pairings are competitive with OSIL models trained with ground-truth\nlabels, presenting a major advancement in the label-efficiency of OSIL.", "sections": [{"title": "1 Introduction", "content": "Humans are capable of learning new tasks and behaviors by imitating others we observe. Fur-thermore, we are remarkably data efficient, often requiring just a single demonstration. One-shotimitation learning (OSIL) (Duan et al., 2017) aims to imbue AI agents with similar capabilities. Ittakes a meta-learning approachand considers several paired demonstrations \u2013 i.e. expert trajectories corresponding to different vari-ations of the semantic task. OSIL learns to reconstruct one trajectory by conditioning on its pairedtrajectory, implicitly capturing the task semantics. At test time, the resulting agent can directlycomplete a new task by conditioning on a demonstration of the said task. However, this method oftenrequires prohibitively large amounts of paired trajectories such that the agent experiences enoughtask variations in diverse environment instantiations to learn a generalizable policy. Collecting sucha dataset of demonstrations can be prohibitively expensive, requiring significant engineering effortand/or human data annotation time. In order to improve the data efficiency of OSIL, and expandits applicability, we introduce and study a semi-supervised paradigm for OSIL.\nIn recent years, we have seen an increase in our ability to collect unsupervised trajectory data in sev-eral applications including robotics. This includes access to historical offline datasets, teleoperation and play data in virtual reality."}, {"title": "3 Problem Formulation", "content": "Following Duan et al. (2017), in supervised OSIL we denote a set of of tasks as T, each individualtask t \u2208 T, and a distribution of demonstrations of task t as D(t). The supervised OSIL objectiveis to train a policy which, conditioned on a demonstration d ~ D(t), can accomplish a task t. Thisamounts to learning a goal conditioned policy \\(\\pi_{\\theta}(a_t| s_t, d)\\), parameterized by \u03b8, that takes an expertdemonstration and the current state of the environment as input and emits the proper actions at eachtime-step t (we differentiate time t and task t, which is in bold). During training, we have access toa large dataset of demonstrations dtrain ~ D(ttrain), for a set of training tasks ttrain \u2208 Ttrain \u2282T,\nwhere ti is the ith task. We formulate the dataset D as follows\n\\begin{equation}\nD = \\{(t_i, \\{d_1, d_2, ...\\}) \\forall t_i \\in T^{train}\\},\n\\end{equation}\nWe further assume the existence of a binary valued function \\(R_t(d)\\) which indicates whether a givendemonstration or policy rollout d successfully accomplishes the task t, which we use for evaluatingour method. At test time, the policy is provided with one new test demonstration dtest ~ D(t) thatcan be either be a new demonstration of a seen task (i.e. t \u2208 Ttrain) or a new demonstration of anunseen task (i.e. t \u2208 T \\ Ttrain).\nSemi-supervised OSIL builds on the supervised OSIL setting, which we formulate as follows. Wesimilarly assume access to a small labeled dataset of demonstrations Dlabeled where each demonstra-tion has its associated task label. We additionally assume access to a large dataset of demonstrationsDunlabeled which does not have the associated task label ti. These datasets are defined below:\n\\begin{equation}\nD^{labeled} = \\{(t_i, \\{d_1, d_2, ...\\}) \\forall t_i\\}\n\\end{equation}\n\\begin{equation}\nD^{unlabeled} = \\{d_1, d_2, ...\\}\n\\end{equation}\nAn effective semi-supervised method should be able to leverage both annotated and un-annotateddatasets effectively to maximize the performance of the OSIL agent at test time."}, {"title": "4 Method", "content": "At its core, OSIL can be simply construed as two modules that are jointly optimized together: (1)an encoder network \\(f_{\\theta}(d)\\) which embeds demonstrated trajectories into a latent space z, and (2) apolicy decoder \\(\\pi_{\\theta}(a_t| s_t, z)\\) that is conditioned on the demonstration embedding and current stateof the environment to output actions. The prior state of the art work on OSILlearn both the demonstration encoder module and thepolicy decoder jointly by minimizing the predicted action errors on the imitated trajectory, possiblywith other auxiliary losses. This method works well when paired trajectories are abundant. In themore realistic semi-supervised OSIL setting, the question becomes \"How can we group sufficentlyabundant demonstration pairs from the unlabeled data to train an OSIL agent?\" To address this,we propose an iterative student teacher method."}, {"title": "4.1 Student-Teacher Training", "content": "The core of our hypothesis is that discriminating or clustering trajectories that share the samesemantic task is easier (and thus more data efficient) compared to generative modeling of actions toaccomplish a task. To instantiate this in practice, we use a teacher-student self-training paradigmto effectively remove the need for large human-annotation on task labels. In our"}, {"title": "5 Experiments", "content": "Through our experiments, we aim to study the effectiveness of the semi-supervised OSIL setting, aswell as the performance of our algorithm. Concretely, we study the following questions.\nHow to train the demonstration encoder to effectively cluster trajectories?\nHow to use the learned clusters to effectively improve agent performance?"}, {"title": "5.1 Environment setup", "content": "Semantic Goal Navigation. We construct a custom pointmass-based reaching task using theMuJoCo simulator with the DMControl suite.This task is inspired from the simulated reaching task first introduced in. Thetask is to navigate the pointmass to a goal of a given color and shape when also presented with adistractor goal of a different color and shape. Concretely, there are 2 shapes and 5 possible colorsthe shapes can take on, totalling 10 variations for each object, and 100 possible semantic scenes.See Figure 3 for a visual illustration. Note that within each scene configuration, the locations ofthe objects can be randomized. We collect 800 trajectories for each target goal object, resulting ina total training dataset size of 8000 trajectories.\nSequential Goal Navigation. We use a modified version of the discrete pinpad world environ-ment from. This task requires the agent to navigate and press two buttons out ofsix in a specified order. The agent is only considered successful if it is able to correctly reach all the"}, {"title": "5.2 Metrics", "content": "Task Success. Our goal is to maximize task success rate using limited task labeled demonstrations.For both environments, we report the success rate of the agent as the performance after 100 trialsin the environment, averaged over 3 seeds. We evaluate the agent on both new instantiations of thetraining tasks and an unseen test task, which we report as \"Train\" and \"Test\" respectively. We usedifferent numbers of the total labeled trajectories to show how the number of labeled trajectorieseffects final task performance.\nTrajectory Retrieval (TR) Score For each trajectory in dtest \u2208 D, we retrieve the K nearestneighbors by measuring the L2 distance in the embedding space of the teacher. Let dret be the ithretrieved trajectory and t be the task label of dtest. For each trajectory, the retrieval accuracy isdefined as the percentage of time that \\(R_t(d^{ret}_i) = 1\\). We take an average of this measure across allsamples in the training set.\n\\begin{equation}\nTR_{score}(D) = \\frac{1}{ \\vert D \\vert} \\sum_{d \\in D} \\frac{1}{k} \\sum_{j=1}^{k} R_t(d^{ret}_i)\n\\end{equation}"}, {"title": "5.3 Results", "content": "For each experiment, we train the OSIL policy using the learned goal embedding and behaviorcloning loss on the labeled subset of data. We report the task success rate and trajectory retrievalscores for all experiments.\nSemantic Goal Navigation First we consider the Semantic Goal Navigation pointmass task.We consider 5 main settings:\nAn agent trained with only the imitation loss on the demonstrated actions.\nAn agent trained with an additional contrastive loss on the goal embeddings in addition toto the imitation loss.\nThe same as (2.) but with an added self-supervised loss on the entire dataset (includingunlabeled data).\nA student model trained by using the demonstration encoder (2.) as a teacher model.\nA student model trained by using the demonstration encoder of (3.) as a teacher model.\nThe model trained with the method specified in (3.) acts as an alternative semi-supervised baselinein the special case of using the final frame as the demonstration representation. In this setting,"}, {"title": "6 Conclusions", "content": "In this paper, we introduce the problem setting of semi-supervised OSIL, which we believe to be amore realistic setting for developing OSIL methods that can scale to real world settings. In semi-supervised OSIL we aim to maximize agent performance in settings where we have access to a largenumber of task-agnostic expert demonstrations, but only a small task-labeled dataset. We introducea student teacher training method and show that training a teacher network based on the limitedlabeled data and bootstrapping on the resulting task encoder can allow us to assign effective pseudo-labels to the large unlabeled dataset. Using the pseudo-labeled dataset to train a student networkcan result in out-performing its teacher, reaching task performance parity with a model trained on"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Hyperparameters", "content": "In both experiments the image observations are first encoded with a 5 layer CNN with ReLU ac-tiviations. The CNN encoder is shared across embedding the current state and the demonstrationtrajectory. The policy network is also the same, which consists of 3 FiLM blocks using GELU nonlin-earities and using 128 hidden units per layer. For all experiments that use the contrastive objectiveacross paired trajectories, a weighting of 10 is used on the contrastive loss. For all experiments us-ing pseudo-labeled trajectories, a weighting of 0.5 is used on the imitation loss with pseudo-labeledtrajectories.\nSemantic Goal Navigation For this task we adopt the oracle trajectory encoder, which takesthe final frame of the trajectory (which fully specifies the task) and encodes it with CNN. Theimages are 64x64. The policy is trained with a learning rate of 1e-3 with 4000 warm up steps. Framestacking of 2 is employed on the observations. For each experiment we train for 200k iterations.\nFor the self supervised augmentations, we employ random resizing, cropping, horizontal flip, and ver-tical flip. An additional one layer projection is applied before applying the self supervised contrastiveloss, which we employ with a weight of 0.05.\nSequential Goal Navigation For this task, we make no assumptions on what frames are im-portant and use a bidirectional transformer that attends over all states in the trajectory. Thetransformer has 2 hidden layers and 2 attention heads, and the goal encoding is extracted with anadditional class token. Images are 16x16. We use a learning rate of 3e-4 and train for 60k iterations."}, {"title": "A.2 Effect of more pseudo label pair value k", "content": "Here we study how choosing different values of k and different iterations of relabeling effect finalperformance. In the pseudo-labeling stage, we fix k controls the number of possible pairs eachunlabeled trajectory can use for training. In addition, we experiment with using the student modelas a teacher model for one additional iteration of training. We use the Semantic Goal reaching task,with 15% of the full dataset size. The results are summarized in Table 3. Results are mixed overall,and it seems the exact choice of k does not have a significant impact on results. In addition it seemsthat repeating the pseudo labeling process for additional iterations does not have any significantgains on performance. This could be due to the simplicity of the task, as well as the already hightrajectory retrieval scores across all k. We suspect that for more difficult tasks, these parameterswill have a more significant impact on final performance."}]}