{"title": "Semi-Supervised One-Shot Imitation Learning", "authors": ["Philipp Wu", "Kourosh Hakhamaneshi", "Yuqing Du", "Igor Mordatch", "Aravind Rajeswaran", "Pieter Abbeel"], "abstract": "One-shot Imitation Learning (OSIL) aims to imbue AI agents with the ability to\nlearn a new task from a single demonstration. To supervise the learning, OSIL\ntypically requires a prohibitively large number of paired expert demonstrations\ni.e. trajectories corresponding to different variations of the same semantic task. \u03a4\u03bf\novercome this limitation, we introduce the semi-supervised OSIL problem setting,\nwhere the learning agent is presented with a large dataset of trajectories with no\ntask labels (i.e. an unpaired dataset), along with a small dataset of multiple demon-\nstrations per semantic task (i.e. a paired dataset). This presents a more realistic\nand practical embodiment of few-shot learning and requires the agent to effectively\nleverage weak supervision from a large dataset of trajectories. Subsequently, we\ndevelop an algorithm specifically applicable to this semi-supervised OSIL setting.\nOur approach first learns an embedding space where different tasks cluster uniquely.\nWe utilize this embedding space and the clustering it supports to self-generate pair-\nings between trajectories in the large unpaired dataset. Through empirical results\non simulated control tasks, we demonstrate that OSIL models trained on such self-\ngenerated pairings are competitive with OSIL models trained with ground-truth\nlabels, presenting a major advancement in the label-efficiency of OSIL.", "sections": [{"title": "Introduction", "content": "Humans are capable of learning new tasks and behaviors by imitating others we observe. Fur-\nthermore, we are remarkably data efficient, often requiring just a single demonstration. One-shot\nimitation learning (OSIL) (Duan et al., 2017) aims to imbue AI agents with similar capabilities. It\ntakes a meta-learning approach\nand considers several paired demonstrations \u2013 i.e. expert trajectories corresponding to different vari-\nations of the semantic task. OSIL learns to reconstruct one trajectory by conditioning on its paired\ntrajectory, implicitly capturing the task semantics. At test time, the resulting agent can directly\ncomplete a new task by conditioning on a demonstration of the said task. However, this method often\nrequires prohibitively large amounts of paired trajectories such that the agent experiences enough\ntask variations in diverse environment instantiations to learn a generalizable policy. Collecting such\na dataset of demonstrations can be prohibitively expensive, requiring significant engineering effort\nand/or human data annotation time. In order to improve the data efficiency of OSIL, and expand\nits applicability, we introduce and study a semi-supervised paradigm for OSIL.\nIn recent years, we have seen an increase in our ability to collect unsupervised trajectory data in sev-\neral applications including robotics."}, {"title": "Related work", "content": "One-Shot Imitation Learning (OSIL) The OSIL framework was originally introduced by Duan\net al. (2017) to endow AI agents with the capability to learn from a single demonstration. OSIL relies\non access to \"paired\" demonstrations - i.e. expert trajectories that correspond to different variations\nof the same semantic task. OSIL then learns by conditioning on one trajectory to reconstruct the\npaired demonstration, enabling it to implicitly learn the notion of task semantics. Through this\nview, OSIL has parallels to meta-learning or learning-to-learn as studied broadly in (supervised)\nmachine learning and inverse RL.\nSince the original work of Duan et al. (2017), OSIL has seen several extensions including extensions\nto visual observation spaces , improving task-level generalization , and architectural innovations like transformers . Nevertheless, the\nneed for a large number of paired demonstrations has limited the broad applicability of OSIL.\nOur work aims to improve this label efficiency of OSIL by also effectively utilizing a large number of\nunlabelled (i.e. unpaired) demonstrations, which are often substantially easier to obtain, for example\nthrough play data collection.\nSemi-Supervised Learning The field of semi-supervised learning studies methods\nto simultaneously learn from large unlabelled datasets and small labelled datasets. Computer vision,\nNLP, and speech recognition have been exploring ways to utilize large unlabelled datasets scraped\nfrom the internet without expensive and time-intensive human annotations. This has resulted in\na wide array of approaches to semi-supervised learning. One dominant paradigm involves pre-training visual representations using unlabelled datasets fol-\nlowed by downstream supervised learning. The representations can be pre-trained with contrastive\nlearning , generative modeling , autoencoders and more. However, such rep-\nresentations lack knowledge of downstream task, and thus might be harder to train, require human\npriors like appropriate choice of augmentations, or demand very large quantities of unlabelled data.\nAn alternative and popular approach to semi-supervised learning is self-training , where a supervised \"teacher\" model is first trained on a\nsmall labelled dataset and used to generate pseudo-labels for the unsupervised dataset. Subsequently,\na student model is trained on both the supervised dataset and the pseudo-labelled dataset. We\nrefer readers to survey works  on semi-supervised learning for more\ndiscussion. Our algorithmic approach to semi-supervised OSIL is closer to self-training, and thus\nhas the advantage of being more task-directed in nature. We also perform contrastive representation\nlearning as an auxiliary task and find that it plays an important role, but is insufficient by itself.\nSemi-Supervised Learning in RL and IL Improving label efficiency for policy learning,\nthrough approaches similar to semi-supervised learning, has been studied in other contexts like\nreward and goal labels. Prior works tackle the challenge of learning from data without reward/goal\nlabels by either explicitly learning a reward function through inverse RL , adversarial imitation learning , learning a reward/goal classifier ."}, {"title": "Problem Formulation", "content": "Following Duan et al. (2017), in supervised OSIL we denote a set of of tasks as T, each individual\ntask $t \\in T$, and a distribution of demonstrations of task t as $D(t)$. The supervised OSIL objective\nis to train a policy which, conditioned on a demonstration $d \\sim D(t)$, can accomplish a task t. This\namounts to learning a goal conditioned policy $\\pi_\\theta(a_t | s_t, d)$, parameterized by $\\theta$, that takes an expert\ndemonstration and the current state of the environment as input and emits the proper actions at each\ntime-step t (we differentiate time t and task t, which is in bold). During training, we have access to\na large dataset of demonstrations $d_{\\text{train}} \\sim D(t_{\\text{train}})$, for a set of training tasks $t_{\\text{train}} \\in T_{\\text{train}} \\subset T$,\nwhere $t_i$ is the $i^{th}$ task. We formulate the dataset D as follows\n$$D = \\{(t_i, \\{d_1, d_2, ...\\}) \\forall t_i \\in T_{\\text{train}}\\},$$\nWe further assume the existence of a binary valued function $R_t(d)$ which indicates whether a given\ndemonstration or policy rollout d successfully accomplishes the task t, which we use for evaluating\nour method. At test time, the policy is provided with one new test demonstration $d_{\\text{test}} \\sim D(t)$ that\ncan be either be a new demonstration of a seen task (i.e. $t \\in T_{\\text{train}}$) or a new demonstration of an\nunseen task (i.e. $t \\in T \\backslash T_{\\text{train}}$).\nSemi-supervised OSIL builds on the supervised OSIL setting, which we formulate as follows. We\nsimilarly assume access to a small labeled dataset of demonstrations $D_{\\text{labeled}}$ where each demonstra-\ntion has its associated task label. We additionally assume access to a large dataset of demonstrations\n$D_{\\text{unlabeled}}$ which does not have the associated task label $t_i$. These datasets are defined below:\n$$D_{\\text{labeled}} = \\{(t_i, \\{d_1, d_2, ...\\}) \\forall t_i\\}$$\n$$D_{\\text{unlabeled}} = \\{d_1, d_2, ...\\}$$\nAn effective semi-supervised method should be able to leverage both annotated and un-annotated\ndatasets effectively to maximize the performance of the OSIL agent at test time."}, {"title": "Method", "content": "At its core, OSIL can be simply construed as two modules that are jointly optimized together: (1)\nan encoder network $f_\\theta(d)$ which embeds demonstrated trajectories into a latent space z, and (2) a\npolicy decoder $\\pi_\\theta(a_t|s_t, z)$ that is conditioned on the demonstration embedding and current state\nof the environment to output actions. The prior state of the art work on OSIL learn both the demonstration encoder module and the\npolicy decoder jointly by minimizing the predicted action errors on the imitated trajectory, possibly\nwith other auxiliary losses. This method works well when paired trajectories are abundant. In the\nmore realistic semi-supervised OSIL setting, the question becomes \"How can we group sufficently\nabundant demonstration pairs from the unlabeled data to train an OSIL agent?\" To address this,\nwe propose an iterative student teacher method."}, {"title": "Student-Teacher Training", "content": "The core of our hypothesis is that discriminating or clustering trajectories that share the same\nsemantic task is easier (and thus more data efficient) compared to generative modeling of actions to\naccomplish a task. To instantiate this in practice, we use a teacher-student self-training paradigm\nto effectively remove the need for large human-annotation on task labels. In our\nsetting, a \"teacher\" is the encoder $f_\\theta$ that embeds trajectories into the latent space. Using a quality\nteacher encoder, we can retrieve the k-nearest neighbors of each trajectory in the dataset using\na distance measure (e.g L2 distance) on the embedding space and use that as a labeled pair for\ndownstream training of a student OSIL policy.\nTo train the teacher encoder, we proceed with the standard OSIL training procedure on the smaller\nlabeled dataset, $D_{\\text{labeled}}$. The encoder and policy are trained end to end with an imitation loss on\nthe predicted action from the policy, $\\pi_\\theta(a_t|s_t, z)$, where $z = f_\\theta(d_t)$. To encourage learning a more\nstructured latent space, we also employ a contrastive InfoNCE loss , where a positive pair is taken from the labeled subset of data, and the rest of the goals in the batch\nare treated as negative examples. This structured latent space is necessary for teacher relabeling.\nIn general, we also find that the contrastive loss helps with learning a better OSIL policy with higher\ntask success rate, which is consistent with the works of James et al. (2018); Mandi et al. (2021).\nAfter training the teacher encoder to convergence, we then generate a set of pseudo labels for\nthe trajectories in the unlabeled dataset. This is done by embedding all of the demonstrations of\nthe dataset $D_{\\text{unlabeled}}$ with the teacher encoder $f_\\theta$. We then find the k nearest neighbors of each\ndemonstration in the embedding space, where k is a hyperparameter. Let $kNN_\\theta(d, D)$ denote the k\nnearest neighbors of d in the dataset D using the feature embeddings from a demonstration encoder\n$f_\\theta$. If the nearest neighbors are demonstrations associated with the same semantic task, we can\nsupervise an effective student OSIL policy with this dataset of pseudo-pairs of trajectories, which\nwe formulate as:\n$$D_{\\text{pseudo\\_labeled}} = \\{(d_i, \\{kNN(d_i, D_{\\text{unlabeled}})\\}) \\forall d_i \\in D_{\\text{unlabeled}}\\}$$\nFinally, the student policy is trained using both $D_{\\text{pseudo\\_labeled}}$ and $D_{\\text{labeled}}$. During training we\ncontinue to use the labeled dataset for the imitation and contrastive losses, but additionally sample\nbatches from the pseudo-labeled dataset, which is trained only with the imitation loss. We can\ncontinue iterating this process by treating the encoder $f_\\theta$ of the trained student as the teacher for\nthe subsequent round and improving the accuracy on the KNN retrievals from the unlabeled dataset\nuntil we get diminishing returns from the process."}, {"title": "Experiments", "content": "Through our experiments, we aim to study the effectiveness of the semi-supervised OSIL setting, as\nwell as the performance of our algorithm. Concretely, we study the following questions.\n\u2022 How to train the demonstration encoder to effectively cluster trajectories?\n\u2022 How to use the learned clusters to effectively improve agent performance?"}, {"title": "Environment setup", "content": "Semantic Goal Navigation. We construct a custom pointmass-based reaching task using the\nMuJoCo simulator  with the DMControl suite . This task is inspired from the simulated reaching task first introduced in (Finn et al., 2017b). The\ntask is to navigate the pointmass to a goal of a given color and shape when also presented with a\ndistractor goal of a different color and shape. Concretely, there are 2 shapes and 5 possible colors\nthe shapes can take on, totalling 10 variations for each object, and 100 possible semantic scenes.\nNote that within each scene configuration, the locations of\nthe objects can be randomized. We collect 800 trajectories for each target goal object, resulting in\na total training dataset size of 8000 trajectories.\nSequential Goal Navigation. We use a modified version of the discrete pinpad world environ-\nment from Hafner et al. (2022). This task requires the agent to navigate and press two buttons out of\nsix in a specified order. The agent is only considered successful if it is able to correctly reach all the\ngoals in the correct sequence. There are 6 possible goal pads for the agent to reach, totaling 30 tasks.\nThe agent's action is one of five possible actions: up, down, left, right, or no-op. The observation\nspace is the raw pixels in the environment. We randomize both\nthe color assignments of the pads and the agent starting location for each task variation. The agent\nmust pay attention to the entire trajectory to correctly determine the desired task. As such, we\nparametrize the demonstration encoder for this environment as a small bi-directional transformer\nthat takes in a sequence of states and a class token to predict a latent z encoding of the trajectory."}, {"title": "Metrics", "content": "Task Success. Our goal is to maximize task success rate using limited task labeled demonstrations.\nFor both environments, we report the success rate of the agent as the performance after 100 trials\nin the environment, averaged over 3 seeds. We evaluate the agent on both new instantiations of the\ntraining tasks and an unseen test task, which we report as \"Train\" and \"Test\" respectively. We use\ndifferent numbers of the total labeled trajectories to show how the number of labeled trajectories\neffects final task performance.\nTrajectory Retrieval (TR) Score For each trajectory in $d_{\\text{test}} \\in D$, we retrieve the K nearest\nneighbors by measuring the L2 distance in the embedding space of the teacher. Let $d_{ret}^i$ be the ith\nretrieved trajectory and t be the task label of $d_{\\text{test}}$. For each trajectory, the retrieval accuracy is\ndefined as the percentage of time that $R_t(d_{ret}^i) = 1$. We take an average of this measure across all\nsamples in the training set.\n$$TR_{score}(D) = \\frac{1}{|D|}\\sum_{d \\in D}\\frac{1}{k}\\sum_{j=1}^k R_t(d_{ret}^i)$$"}, {"title": "Results", "content": "For each experiment, we train the OSIL policy using the learned goal embedding and behavior\ncloning loss on the labeled subset of data. We report the task success rate and trajectory retrieval\nscores for all experiments.\nSemantic Goal Navigation First we consider the Semantic Goal Navigation pointmass task.\nWe consider 5 main settings:\n1. An agent trained with only the imitation loss on the demonstrated actions.\n2. An agent trained with an additional contrastive loss on the goal embeddings in addition to\nto the imitation loss.\n3. The same as (2.) but with an added self-supervised loss on the entire dataset (including\nunlabeled data).\n4. A student model trained by using the demonstration encoder (2.) as a teacher model.\n5. A student model trained by using the demonstration encoder of (3.) as a teacher model.\nThe model trained with the method specified in (3.) acts as an alternative semi-supervised baseline\nin the special case of using the final frame as the demonstration representation. In this setting,\nwe use the supervised labels as in the supervised OSIL case, but further leverage the unlabeled\ntrajectories through adding an additional self supervised loss contrastive loss on augmentations of\nthe goal image . The augmentations we use are restricted to random flip\nand random crop.\nFigure 4 show the task performance across each experiment. As expected, train and validation\nperformance drops when the amount of labeled data decreases. Without relabelling, we see some\ntask performance gains from applying both the contrastive loss and self unsupervised losses. After\ntraining a student model using the pseudo-labels from the representations learned by the teacher\nencoder, we see a leap in performance, matching an agent that has access to 100% ground truth\nlabels, even when only using 15% of the labels.\nTable 1 shows the trajectory retrieval scores across different values of k. Despite having much less\nlabeled data and decreased task performance, the retrieval scores consistently remain high. This\nsuggests that we are able to learn a meaningful representation for the task, allowing us to cluster\nthe trajectories. We find that the contrastive loss is necessary for learning representations that have\nhigh retrieval. Interestingly, we find that relabeling gives much greater gains for task success while"}, {"title": "Conclusions", "content": "In this paper, we introduce the problem setting of semi-supervised OSIL, which we believe to be a\nmore realistic setting for developing OSIL methods that can scale to real world settings. In semi-\nsupervised OSIL we aim to maximize agent performance in settings where we have access to a large\nset of task-agnostic expert demonstrations, but only a small task-labeled dataset. We introduce\na student teacher training method and show that training a teacher network based on the limited\nlabeled data and bootstrapping on the resulting task encoder can allow us to assign effective pseudo-\nlabels to the large unlabeled dataset. Using the pseudo-labeled dataset to train a student network\ncan result in out-performing its teacher, reaching task performance parity with a model trained on\nmuch more labeled data. We evaluate our methodology on simulated environments with varying\ncomplexity and showed that this can be a promising direction towards semi-supervised OSIL.\nOur work aims to provide agents the ability to quickly imitate a demonstration. The work does not\nassume any particular type of demonstration. A malicious actor might be able to provide nefarious\ndemonstrations to AI agents and safeguards must be considered when deploying such imitation\nlearning systems in the real world. At a more immediate level, we do not anticipate any societal\nrisks due to this work."}, {"title": "Appendix", "content": "A.1 Hyperparameters\nIn both experiments the image observations are first encoded with a 5 layer CNN with ReLU ac-\ntiviations. The CNN encoder is shared across embedding the current state and the demonstration\ntrajectory. The policy network is also the same, which consists of 3 FiLM blocks using GELU nonlin-\nearities and using 128 hidden units per layer. For all experiments that use the contrastive objective\nacross paired trajectories, a weighting of 10 is used on the contrastive loss. For all experiments us-\ning pseudo-labeled trajectories, a weighting of 0.5 is used on the imitation loss with pseudo-labeled\ntrajectories.\nSemantic Goal Navigation For this task we adopt the oracle trajectory encoder, which takes\nthe final frame of the trajectory (which fully specifies the task) and encodes it with CNN. The\nimages are 64x64. The policy is trained with a learning rate of 1e-3 with 4000 warm up steps. Frame\nstacking of 2 is employed on the observations. For each experiment we train for 200k iterations.\nFor the self supervised augmentations, we employ random resizing, cropping, horizontal flip, and ver-\ntical flip. An additional one layer projection is applied before applying the self supervised contrastive\nloss, which we employ with a weight of 0.05.\nSequential Goal Navigation For this task, we make no assumptions on what frames are im-\nportant and use a bidirectional transformer that attends over all states in the trajectory. The\ntransformer has 2 hidden layers and 2 attention heads, and the goal encoding is extracted with an\nadditional class token. Images are 16x16. We use a learning rate of 3e-4 and train for 60k iterations.\nA.2 Effect of more pseudo label pair value k\nHere we study how choosing different values of k and different iterations of relabeling effect final\nperformance. In the pseudo-labeling stage, we fix k controls the number of possible pairs each\nunlabeled trajectory can use for training. In addition, we experiment with using the student model\nas a teacher model for one additional iteration of training. We use the Semantic Goal reaching task,\nwith 15% of the full dataset size. The results are summarized in Table 3. Results are mixed overall,\nand it seems the exact choice of k does not have a significant impact on results. In addition it seems\nthat repeating the pseudo labeling process for additional iterations does not have any significant\ngains on performance. This could be due to the simplicity of the task, as well as the already high\ntrajectory retrieval scores across all k. We suspect that for more difficult tasks, these parameters\nwill have a more significant impact on final performance."}]}