{"title": "Alignment-Enhanced Decoding: Defending via Token-Level Adaptive Refining of Probability Distributions", "authors": ["Quan Liu", "Zhenhong Zhou", "Longzhu He", "Yi Liu", "Wei Zhang", "Sen Su"], "abstract": "Large language models are susceptible to jail-\nbreak attacks, which can result in the genera-\ntion of harmful content. While prior defenses\nmitigate these risks by perturbing or inspect-\ning inputs, they ignore competing objectives,\nthe underlying cause of alignment failures. In\nthis paper, we propose Alignment-Enhanced\nDecoding (AED), a novel defense that employs\nadaptive decoding to address the root causes of\njailbreak issues. We first define the Competitive\nIndex to quantify alignment failures and utilize\nfeedback from self-evaluation to compute post-\nalignment logits. Then, AED adaptively com-\nbines Competitive Index and post-alignment\nlogits with the original logits to obtain harm-\nless and helpful distributions. Consequently,\nour method enhances safety alignment while\nmaintaining helpfulness. We conduct exper-\niments across five models and four common\njailbreaks, with the results validating the effec-\ntiveness of our approach.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are increasingly\nbeing applied across various domains. Given the malicious\ncontent in pre-training datasets, alignments are im-\nplemented to ensure these models are helpful and\nharmless.. Despite efforts in align-\nment, jailbreak attacks can circumvent safety mea-\nsures, resulting in undesirable outcomes. \nCurrent defenses against jailbreaks primarily in-\nvolve perturbation of jailbreaks or detecting the\nsafety of inputs. Perturbation defenses focus on\ncountering jailbreak attacks through input modifica-\ntion."}, {"title": "3 Competitive Index", "content": "The trade-offs between helpfulness and harmless-\nness objectives appear after language models are\ntrained to align human values. When faced with ambiguous questions, these trade-\noffs place the models at risk of choosing between\ntwo distinct answers oriented to different objec-\ntives. For instance, when an LLM is compromised\nthrough a jailbreak attack, the candidate tokens may\ninclude conflicting responses such as \"Sure\" and\n\"Sorry\". Consequently, these trade-offs become\nvulnerabilities that can be exploited in jailbreak at-\ntacks, such as Catastrophic jailbreak. In the study by Wei et al. (2024), these trade-\noffs were further discussed under the framework of\n\"Competing Objectives.\"\nDue to the Competing Objectives, both seman-\ntically opposing candidate tokens increase when\napplying Top-p sampling. Thus, it contributes to ex-\npanding the candidate set Pc from both directions.\nIn Top-p sampling, given\nthe decoding step t, the candidate set \\(P_c \\subset V\\) is\ndefined as follows:\n\\begin{equation}\nP_c = \\underset{P_i \\in P}{\\text{argmin} |P_i|},\n\\end{equation}\nwhere\n\\begin{equation}\nP = \\{P_i: \\sum_{x \\in P_i} p(x|x_0,...,x_{j-1}) \\geq p_0 \\}.\n\\end{equation}\nHere V is the vocabulary set, \\(p(x|x_0,\u00b7\u00b7\u00b7,x_{j\u22121})\\)\ndenotes the probability of next token given a se-\nquence of j \u2013 1 tokens as context and \\(p_0 \u2208 (0, 1]\\) is\na threshold hyper-parameter. The size of candidate\nset Pc is defined as Candidate Count S and is then\ncalculated as follows:\n\\begin{equation}\nS = |P_c|.\n\\end{equation}\nThe variation of S in harmless datasets tends to\nbe stable compared with encountering the jailbreak"}, {"title": "4 Method: Alignment-Enhanced Decoding", "content": "As discussed in Sec. 3, Competitive Index quan-\ntifies the degree of the objectives competition\nwithin the model. Based on Competitive Index,\nwe propose a novel defense method, Alignment-\nEnhanced Decoding (AED). AED adaptively re-\nfines the distribution of each generation step. As\na result, AED performs an enhanced alignment at\nthe decoding phase, illustrated in Fig.2.\n4.1 Realigning Language Models through Self-Evaluation\nThe language models can discern whether its gen-\neration is safe when encountering jailbreak attacks.\nThus, we propose a novel method to dynamically\nobtain the model's self-evaluation at each decoding\nstep, which is formalized as the post-alignment log-\nits Lpost. We detail the computation of the model's\noriginal logits Lmodel and post-alignment logits\nLpost as follows.\nDecoder-only large language models (LLMs)\ncalculate the logits Lmodel \u2208 R|V| for next token Yn\nthrough the following process:\n\\begin{equation}\nL_{model} = LLM(y_n|x_1,\u00b7\u00b7\u00b7, x_m, y_1, \u00b7\u00b7\u00b7, y_{n\u22121}),\n\\end{equation}\nwhere x1, x2,..., Xm correspond to the user's in-\nput, and Y1, Y2,\u2026\u2026, Yn\u22121 represents the genera-\ntion of LLMs. To facilitate the self-evaluation, we\ntruncate the output and use it to derive the post-\nalignment logits Lpost.\n\\begin{equation}\nL_{post} = LLM(y_n|y_1,\u2026\u2026, y_{n\u22121}),\n\\end{equation}\nwhere Lpost \u2208 RIV\ubbf8. We prefix the \u201cAssistant:\u201d to\nY1, Y2,\u2026, Yn\u22121 to avoid an empty input during\nthe initial generation of Lpost.\nIn summary, post-alignment logits represent the\nmodel's self-evaluation and are then used in the\nadaptive algorithm.\n4.2 Decoding with Adaptive Algorithm\nAs discussed in Sec.3, Competitive Index I can\neffectively reflect the competition when encoun-\ntering jailbreaks. Based on I and post-alignment\nlogits Lpost, we propose an adaptive algorithm to\nrefine the distribution by re-weighting the model's\noriginal logits Lmodel, which is outlined in Alg.1.\nSpecifically, we calculate the Imodel and Ipost\nbased on Lmodel and Lpost. Based on the Top-p\nsampling and Eq.3, candidate set Pe can be deter-\nmined by logits L and then be used to calculate\nCandidate Count S. This process is defined as the\nfunction f where S = f(L). As demonstrated in\nEq.5, I is derived from the Candidate Count S:\n\\begin{equation}\nI_{model} = \\frac{f (L_{model})}{S_t},\n\\end{equation}\n\\begin{equation}\nI_{post} = \\frac{f(L_{post})}{S_t},\n\\end{equation}\nThen the tuning coefficient c\u2208 (0,1) for two\nlogits is calculated as:\n\\begin{equation}\nc = \\sigma(S_t\u00b7 (I_{model} \u2013 I_{post} \u2014 B_{bias})),\n\\end{equation}\nwhere \u03c3(\u00b7) is the sigmoid function and bias Bbias E\nR refers a constant to determine the effect of Lpost."}, {"title": "5 Experiments", "content": "In this study, we conducted extensive experiments\nof AED across five models, utilizing four attack\nmethods. Then, we evaluated the performance of\nAED on three harmless datasets.\n5.1 Experimental Setups\nModels. We employed AED on five popu-\nlar open-source LLMs, including Llama2-7B-\nChat-HF, Llama3-8B-\nInstruct , Vicuna-7B, Guanaco-7B, and\nGemma-1.1-7B-IT .\nDatasets. As for the jailbreaks, we chose the four\ndatasets including GCG, Auto-\nDAN, ICA and\nRefusal_Suppression and\nfollowed their official settings. As for the control\ngroup, we used AvdBench as a\nharmful benchmark. As for harmless datasets and\nthe calculation of St, we chose three popular bench-\nmarks including MMLU, GMS8K, and Alpaca. We included 90 prompts for each\ndataset to evaluate AED in this experiment."}, {"title": "5.2 Competitive Index Quantifies the Degree of Competition", "content": "As discussed in Sec. 3, the Competitive Index I\nquantifies the degree of competition when predict-\ning the next token. We conduct experiments across\nfive models and five datasets. Additional experi-\nments examine how I responds to different input\nsettings. The results indicate that I is sensitive to\nvarying scenarios and effectively reflects the level\nof competition when the language model encoun-\nters jailbreak attacks."}, {"title": "5.3 AED Enhances the Alignment.", "content": "We conducted a comparative analysis of Alignment-\nEnhanced Decoding (AED) against other defense\nmethods as documented in Tab. 4. The step N\nin Alg. 1 is set as 30. The results presented in\nthe table confirm that AED effectively withstands\nattacks and outperforms other defense methods\nacross all tested scenarios, achieving superior out-\ncomes. Specifically, AED maintained or reached"}, {"title": "5.4 AED Maintains Helpfulness", "content": "We compared AED versus no-defense and Self-\nDefense methods across various models, as docu-\nmented in Tab. 3. This comparison focuses on the\nNot Rejection Rate (NRR) in the MMLU, GMS8K,\nand Alpaca datasets. The results, detailed in the\ntable, show that AED does not interfere with stan-\ndard query processing. For instance, in the Llama2\nmodel, the NRR changed minimally from 2.5% to\n3.0% for MMLU, indicating that AED preserves\nthe model's functionality. A notable performance\nis observed in the Llama3, where the NRR for\nthe Alpaca dataset remained unchanged, affirming\nthat AED's implementation does not degrade the\nmodel's responsiveness in control settings. These\nfindings affirm that AED can effectively be imple-\nmented without altering the inherent functionality\nof the models, thus ensuring their reliability in real-world applications."}, {"title": "5.5 Time Overhead of AED", "content": "We evaluated AED alongside three defensive mech-\nanisms across five models. Tab. 2 shows that AED\ndoes not incur significant additional computational\ncosts. This assessment involved testing each de-\nfense with ten jailbreak scenarios and ten harmless\nqueries. Notably, Competitive Index I adaptively"}, {"title": "6 Conclusions", "content": "We define the Competitive Index I for the first time\nto quantify the degree of competition among var-\nious training objectives. Utilizing e Competitive\nIndex I and the self-evaluation capabilities of the\nmodel, we introduce a novel defensive AED that\nadaptively refines the token distribution during pre-\ndiction. This method is validated across five differ-\nent models and tested against four jailbreak attacks,\nconfirming its efficacy. Through comparative stud-\nies, we demonstrate that AED surpasses existing\ndefenses in effectiveness and achieves this with-\nout necessitating additional training. Furthermore,"}, {"title": "7 Limitations", "content": "In this study, we differentiate between harmless\nand jailbreak samples to analyze the Competitive\nIndex. However, we do not investigate why dispari-\nties in the index exist within jailbreak samples, with\nsome reaching up to 100 times the threshold. Fur-\nthermore, variations in the index across different\nmodels are noted but not extensively explored, sug-\ngesting that model architecture and training data\nmay influence these differences. Future research\ncould further examine these factors to enhance un-\nderstanding of the Competitive Index's utility in\nevaluating model performance."}, {"title": "8 Ethics Impact", "content": "This paper focuses on the domain of model security,\nspecifically addressing some underlying causes of\nalignment failures and proposing effective defense\nmechanisms against jailbreak attacks. While the\nresearch inherently involves sensitive topics, in-"}]}