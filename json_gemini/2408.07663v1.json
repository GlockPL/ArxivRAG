{"title": "Alignment-Enhanced Decoding: Defending via Token-Level Adaptive Refining of Probability Distributions", "authors": ["Quan Liu", "Zhenhong Zhou", "Longzhu He", "Yi Liu", "Wei Zhang", "Sen Su"], "abstract": "Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines Competitive Index and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are increasingly being applied across various domains. Given the malicious content in pre-training datasets, alignments are implemented to ensure these models are helpful and harmless. Despite efforts in alignment, jailbreak attacks can circumvent safety measures, resulting in undesirable outcomes. Current defenses against jailbreaks primarily involve perturbation of jailbreaks or detecting the safety of inputs. Perturbation defenses focus on countering jailbreak attacks through input modification."}, {"title": "2 Related Works", "content": "Alignment. Incorporating vast amounts of data from the internet, datasets, such as MassiveText,"}, {"title": "3 Competitive Index", "content": "The trade-offs between helpfulness and harmlessness objectives appear after language models are trained to align human values. When faced with ambiguous questions, these trade-offs place the models at risk of choosing between two distinct answers oriented to different objectives. For instance, when an LLM is compromised through a jailbreak attack, the candidate tokens may include conflicting responses such as \"Sure\" and \"Sorry\". Consequently, these trade-offs become vulnerabilities that can be exploited in jailbreak attacks, such as Catastrophic jailbreak. In the study by Wei et al. (2024), these trade-offs were further discussed under the framework of \"Competing Objectives.\" Due to the Competing Objectives, both semantically opposing candidate tokens increase when applying Top-p sampling. Thus, it contributes to expanding the candidate set $P_c$ from both directions. In Top-p sampling, given the decoding step t, the candidate set $P_c \\subset V$ is defined as follows:\n$P_c = \\underset{P_i}{\\arg \\min} \\{P_i,\\quad x \\in P_i,\\quad  \\underset{x\\in P_i}{\\sum} p(x|x_0,...,x_{j-1}) \\geq p_0 \\},$\\nwhere V is the vocabulary set, $p(x|x_0,...,x_{j-1})$ denotes the probability of next token given a sequence of j \u2013 1 tokens as context and $p_0 \\in (0, 1]$ is a threshold hyper-parameter. The size of candidate set $P_c$ is defined as Candidate Count S and is then calculated as follows:\n$S = |P_c|.$\nThe variation of S in harmless datasets tends to be stable compared with encountering the jailbreak"}, {"title": "4 Method: Alignment-Enhanced Decoding", "content": "As discussed in Sec. 3, Competitive Index quantifies the degree of the objectives competition within the model. Based on Competitive Index, we propose a novel defense method, Alignment-Enhanced Decoding (AED). AED adaptively refines the distribution of each generation step. As a result, AED performs an enhanced alignment at the decoding phase. The language models can discern whether its generation is safe when encountering jailbreak attacks. For instance, Self-Defense asks LLMs \"Is it harmful?\" to judge its generation. Thus, we propose a novel method to dynamically obtain the model's self-evaluation at each decoding step, which is formalized as the post-alignment logits $L_{post}$. We detail the computation of the model's original logits $L_{model}$ and post-alignment logits $L_{post}$ as follows.\nDecoder-only large language models (LLMs) calculate the logits $L_{model} \\in R^{|V|}$ for next token $Y_n$ through the following process:\n$L_{model} = LLM(y_n|x_1,\u00b7\u00b7\u00b7, x_m, y_1, \u00b7\u00b7\u00b7, y_{n-1}),$\nwhere $x_1, x_2,..., x_m$ correspond to the user's input, and $Y_1, Y_2,\u2026\u2026, Y_{n\u22121}$ represents the generation of LLMs. To facilitate the self-evaluation, we truncate the output and use it to derive the post-alignment logits $L_{post}$.\n$L_{post} = LLM(y_n|y_1,\u2026\u2026, y_{n\u22121}),$\nwhere $L_{post} \\in R^{|V|}$. We prefix the \u201cAssistant:\u201d to $Y_1, Y_2,\u2026, Y_{n\u22121}$ to avoid an empty input during the initial generation of $L_{post}$. In summary, post-alignment logits represent the model's self-evaluation and are then used in the adaptive algorithm."}, {"title": "4.2 Decoding with Adaptive Algorithm", "content": "As discussed in Sec.3, Competitive Index I can effectively reflect the competition when encountering jailbreaks. Based on I and post-alignment logits $L_{post}$, we propose an adaptive algorithm to refine the distribution by re-weighting the model's original logits $L_{model}$, Specifically, we calculate the $I_{model}$ and $I_{post}$ based on $L_{model}$ and $L_{post}$. Based on the Top-p sampling and Eq.3, candidate set $P_c$ can be determined by logits $L$ and then be used to calculate Candidate Count S. This process is defined as the function f where $S = f(L)$. As demonstrated in Eq.5, I is derived from the Candidate Count S:\n$I_{model} = \\frac{f(L_{model})}{S_t},$\n$I_{post} = \\frac{f(L_{post})}{S_t},$\nThen the tuning coefficient $c \\in (0,1)$ for two logits is calculated as:\n$c = \\sigma(S_t \\cdot (I_{model} \u2013 I_{post} \u2014 B_{bias})),$\nwhere $\u03c3(\u00b7)$ is the sigmoid function and bias $B_{bias} \u2208 R$ refers a constant to determine the effect of $L_{post}$."}, {"title": "5 Experiments", "content": "In this study, we conducted extensive experiments of AED across five models, utilizing four attack methods. Then, we evaluated the performance of AED on three harmless datasets."}, {"title": "5.1 Experimental Setups", "content": "Models. We employed AED on five popular open-source LLMs, including Llama2-7B-Chat-HF , Llama3-8B-Instruct , Vicuna-7B , Guanaco-7B , and Gemma-1.1-7B-IT. Datasets. As for the jailbreaks, we chose the four datasets including GCG , AutoDAN , ICA and Refusal_Suppression and followed their official settings. As for the control group, we used AvdBench as a harmful benchmark. As for harmless datasets and the calculation of $S_t$, we chose three popular benchmarks including MMLU, GMS8K, and Alpaca . We included 90 prompts for each dataset to evaluate AED in this experiment."}, {"title": "5.2 Competitive Index Quantifies the Degree of Competition", "content": "As discussed in Sec. 3, the Competitive Index I quantifies the degree of competition when predicting the next token. We conduct experiments across"}, {"title": "5.3 AED Enhances the Alignment.", "content": "We conducted a comparative analysis of Alignment-Enhanced Decoding (AED) against other defense methods as documented in Tab. 4. The step N in Alg. 1 is set as 30. The results presented in the table confirm that AED effectively withstands attacks and outperforms other defense methods across all tested scenarios, achieving superior outcomes. Specifically, AED maintained or reached"}, {"title": "5.4 AED Maintains Helpfulness", "content": "We compared AED versus no-defense and Self-Defense methods across various models, as documented in Tab. 3. This comparison focuses on the Not Rejection Rate (NRR) in the MMLU, GMS8K, and Alpaca datasets. The results, detailed in the table, show that AED does not interfere with standard query processing. For instance, in the Llama2 model, the NRR changed minimally from 2.5% to 3.0% for MMLU, indicating that AED preserves the model's functionality. A notable performance is observed in the Llama3, where the NRR for the Alpaca dataset remained unchanged, affirming that AED's implementation does not degrade the model's responsiveness in control settings. These findings affirm that AED can effectively be implemented without altering the inherent functionality of the models, thus ensuring their reliability in real-world applications."}, {"title": "5.5 Time Overhead of AED", "content": "We evaluated AED alongside three defensive mechanisms across five models. Tab. 2 shows that AED does not incur significant additional computational costs. This assessment involved testing each defense with ten jailbreak scenarios and ten harmless queries. Notably, Competitive Index I adaptively"}, {"title": "6 Conclusions", "content": "We define the Competitive Index I for the first time to quantify the degree of competition among various training objectives. Utilizing e Competitive Index I and the self-evaluation capabilities of the model, we introduce a novel defensive AED that adaptively refines the token distribution during prediction. This method is validated across five different models and tested against four jailbreak attacks, confirming its efficacy. Through comparative studies, we demonstrate that AED surpasses existing defenses in effectiveness and achieves this without necessitating additional training. Furthermore, according to the Average Time Generation Ratio (ATGR), AED introduces no significant increase in time overhead, confirming its efficiency and practicality."}, {"title": "7 Limitations", "content": "In this study, we differentiate between harmless and jailbreak samples to analyze the Competitive Index. However, we do not investigate why disparities in the index exist within jailbreak samples, with some reaching up to 100 times the threshold. Furthermore, variations in the index across different models are noted but not extensively explored, suggesting that model architecture and training data may influence these differences. Future research could further examine these factors to enhance understanding of the Competitive Index's utility in evaluating model performance."}, {"title": "8 Ethics Impact", "content": "This paper focuses on the domain of model security, specifically addressing some underlying causes of alignment failures and proposing effective defense mechanisms against jailbreak attacks. While the research inherently involves sensitive topics, in-"}]}