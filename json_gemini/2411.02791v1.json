{"title": "Language Models and Cycle Consistency for Self-Reflective Machine Translation", "authors": ["Jianqiao Wangni"], "abstract": "This paper introduces a novel framework that leverages large language models (LLMs) for machine translation (MT). We start with one conjecture: an ideal translation should contain complete and accurate information for a strong enough LLM to recover the original sentence. We generate multiple translation candidates from a source language A to a target language B, and subsequently translate these candidates back to the original language A. By evaluating the cycle consistency between the original and back-translated sentences using metrics such as token-level precision and accuracy, we implicitly estimate the translation quality in language B, without knowing its ground-truth. This also helps to evaluate the LLM translation capability, only with monolingual corpora. For each source sentence, we identify the translation candidate with optimal cycle consistency with the original sentence as the final answer. Our experiments demonstrate that larger LLMs, or the same LLM with more forward passes during inference, exhibit increased cycle consistency, aligning with the LLM model size scaling law [Kaplan et al. (2020)] and test-time computation scaling law [Snell et al. (2024)]. This work provide methods for, 1) to implicitly evaluate translation quality of a sentence in the target language, 2), to evaluate capability of LLM for any-to-any-language translation, and 3), how to generate a better translation for a specific LLM.", "sections": [{"title": "Introduction", "content": "Machine Translation (MT) has been a cornerstone of natural language processing, facilitating globalization by cross-linguistic communication and democratizing newest information access to all population. In recent years, transformer-based large language models (LLMs) have fundamentally changed the field of natural language processing. Introduced by [Vaswani et al. (2017)], the transformer architecture facilitates parallel processing of input word tokens, significantly improving computational efficiency and successfully scales to unseen model size. Strong language capabilities beyond human imagination emerges from LLM scaling, and create an image of Silicon intelligence for the first time. Transformer-based LLMs can be categorized into several paradigms: encoder-only architectures, like BERT [Devlin (2018)], which focus on meaningful embeddings of input sequences, and don't fit translation task; the rest two architectures, the encoder-decoder architectures, like T5 [Raffel et al. (2020)], which separately process input and output sequences, were born for translation; and decoder-only architectures, like GPT [Radford et al. (2019)], which generate text in an autoregressive manner. Although GPT is not initially trained specifically for translation tasks like T5, it is exceptionally well-suited for a wide range of natural language processing (NLP) tasks via supervised fine-tuning on downstream tasks, including translation, and provides applications to users via prompting [Brown et al. (2020)].\nOn a related subject, MT evaluation poses significant challenges as there is no unit test for human languages, like Python or Java. Machine-based evaluation metrics, such as BLEU [Papineni et al."}, {"title": "Methodology", "content": "Pretrained LLMs like GPT are trained to perform machine translation via prompting. Let $S_A$ be the original sentence in language A that we want to translate. The input can be represented as:\n$S_A = \\{w_1, w_2,..., w_l\\}$\nwhere w represents the tokens, and l is the total number of tokens. To perform machine translation, we construct a prompt P with a prefix that guides the GPT model to produce a translation in language B. The prompt can be defined as:\nP = \"Translate the following sentence from language A to language B: \" + $S_A$\nThe GPT model takes the prompt P as input and generates a sequence of tokens in language B, denoted as $S_B$:\n$S_B = \\{w'_1, w'_2, ..., w'_m\\}$ = Translate($S_A$; A \u2192 B; $\\theta$)\nwhere w' represents the tokens of the translated sentence in language B, m is the total number of tokens generated, and $\\theta$ is all of the decoding hyper-parameters of LLM, including temperature, number of beams (beam-search), top-K candidates, and even the random seed."}, {"title": "Self-Reflective Translation", "content": "The self-reflective framework involves thinking ahead, which includes a two-step translation process. The Forward Translation is to translate the original sentence $S_A$ from language A to language B, generating multiple translation candidates $\\{S_B^{(i)}\\}_{i=1}^N$:\n$\\{S_B^{(i)}\\}_{i=1}^N = \\{\\text{Translate}(S_A; A \\rightarrow B; \\theta_i)\\}_{i=1}^N$.\nThe key point in this step is to create enough diversity in answers while not sacrificing accuracy. With many hyper-parameters such as sampling temperatures are unknown, there is also a good chance of"}, {"title": "Metrics for Consistency", "content": "The consistency between two sentences were measured using different standards. We can focus on the precision, the proportion of correctly translated words [Pradeep and Bhattacharyya (2018)]; or focus on the accuracy, the exact match rate between the original and back-translated sentences [Brown et al. (2020)]; we can use BLEU (Bilingual Evaluation Understudy) Score [Papineni et al. (2002)], which is the N-gram overlap:\n$\\text{BLEU} = \\text{ABP} \\times \\exp \\left(\\sum_{n=1}^N w_n \\log p_n\\right)$\nwhere ABP is the brevity penalty for shorter candidate translations, $p_n$ is the modified N-gram precision for N-grams of order n in the candidate translation, and $w_n$ are the weights for each precision score, typically set to 1/N for equal weighting of N-grams from 1 to N.\nThe BLEU score ranges from 0 to 1, with a higher score indicating better quality translations that more closely resemble the reference texts. However, while BLEU is effective for capturing N-gram overlap, it has limitations in evaluating semantic meaning and context, which can sometimes lead to misleading results in translation quality assessments [Papineni et al. (2002)].\nAlthough there is no golden standard of the consistency, in our experiments, we mainly use ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [Lin (2004)], which complements BLEU by focusing on recall-based metrics, evaluating the overlap of N-grams and longest common subsequences between the candidate and reference."}, {"title": "Foundation Model Architecture", "content": "In the landscape of machine translation, GPT [Radford et al. (2019) and T5 [Raffel et al. (2020)] represent two distinct transformer-based architectures and training paradigms for machine translation. GPT learns a much wider range of knowledge before focusing on translation tasks, often leading to a higher level of human-like intelligence in processing language, albeit at the cost of a larger parameter size. In contrast, T5 utilizes a full encoder-decoder transformer architecture, inherently suitable for machine translation under small model size, and is capable of the task after training on smaller datasets compared to GPT. GPT, on the bright side, has higher potential for difficult translations, given enough pretraining data and model size. Their distinct strengths encourage us to combine both architectures with chain-of-thought reasoning in forward translation."}, {"title": "Experiments", "content": "To evaluate the effectiveness of our cycle consistency framework for machine translation, we conducted experiments using range of available open-sourced LLMs with varying parameter sizes, and a carefully curated dataset of short paragraphs. The primary objectives were to assess how model scaling influences cycle consistency and to determine the suitability of validity of this metric.\nThe models used in our experiments include Gemma-2 (9B and 27B parameters) [Team et al. (2024)], and Qwen-2.5 (0.5B, 1.5B, 3B, 7B and 14B parameters) [Yang et al. (2024)]. The transformer"}, {"title": "Related Work", "content": "Before the LLM era, similar ideas of cycle consistency were used for NLP tasks such as unsupervised machine translation, text style transfer, and dialogue systems. Early research used forward and backward translation to train models without parallel corpora [Artetxe et al. (2018)]. In text style transfer, it maintains the semantic content while altering stylistic attributes, ensuring that the transformed text remains faithful to the original meaning [Shen et al. (2017)]. Additionally, in dialogue systems, cycle consistency helps maintain conversational coherence by ensuring that responses can be accurately reverted to the original dialogue context [Kim et al. (2018)].\nIn speech processing, cycle consistency plays a crucial role in tasks like voice conversion and speech synthesis. Models leveraging cycle consistency can transform a source speaker's voice to a target speaker's voice without parallel data, ensuring that the converted voice can be reverted back accurately [Lee et al. (2018)]. This bidirectional consistency maintains the linguistic content while altering vocal characteristics, resulting in natural-sounding conversions. Moreover, in speech enhancement, cycle consistency helps in denoising by ensuring that enhanced speech can revert to its noisy form, thereby preserving speech quality and intelligibility [Liu et al. (2022)]. These applications highlight the effectiveness of cycle consistency in maintaining speech integrity during transformations.\nIn computer vision, cycle consistency has been fundamental in various tasks by ensuring transformations retain essential information. In image-to-image translation, CycleGAN [Zhu et al. (2017)] enables converting images between domains without paired examples, such as turning horses into zebras while maintaining structural integrity. For optical flow estimation, cycle consistency ensures that motion vectors from frame A to frame B can be accurately reversed, enhancing flow accuracy [Baker and Scharstein (2009)]. In depth estimation, enforcing cycle consistency helps maintain"}, {"title": "Conclusion", "content": "This paper presents a self-reflective capabilities framework for machine translation that leverages large language models. Our findings demonstrate that larger models and more repetition both exhibit significant improvements in cycle consistency, highlighting the benefits of model size-scaling and inference computation-scaling in MT tasks. This should have broader applications to reinforcement learning from human feedback (RLHF) and chain-of-thought reasoning (CoT). This approach offers a scalable and efficient alternative to traditional MT evaluation methods, paving the way for more intelligent and autonomous translation systems."}]}