{"title": "Are LLMs Any Good for High-Level Synthesis?", "authors": ["Yuchao Liao", "Tosiron Adegbija", "Roman Lysecky"], "abstract": "The increasing complexity and demand for faster, energy-efficient hardware designs necessitate innovative High-Level Synthesis (HLS) methodologies. This paper explores the potential of Large Language Models (LLMs) to streamline or replace the HLS process, leveraging their ability to understand natural language specifications and refactor code. We survey the current research and conduct experiments comparing Verilog designs generated by a standard HLS tool (Vitis HLS) with those produced by LLMs translating C code or natural language specifications. Our evaluation focuses on quantifying the impact on performance, power, and resource utilization, providing an assessment of the efficiency of LLM-based approaches. This study aims to illuminate the role of LLMs in HLS, identifying promising directions for optimized hardware design in applications such as Al acceleration, embedded systems, and high-performance computing.", "sections": [{"title": "1 Introduction", "content": "The increasing demand for custom hardware accelerators, driven by applications ranging from artificial intelligence to high-performance computing, necessitates innovative design methodologies to meet the challenges of rapidly evolving technology. High-Level Synthesis (HLS) has emerged as a valuable approach for designing, synthesizing, and optimizing hardware systems. HLS [8] enables designers to define systems at a high abstraction level, independent of low-level circuit specifics, and utilize HLS tools to produce an optimized low-level hardware description of the target system. With current HLS tools (e.g., Vitis HLS, SmartHLS), designers can create application-specific embedded systems using high-level languages like C/C++ and translate them into register-transfer level (RTL) implementations using hardware description languages (e.g., Verilog, VHDL), thereby enhancing design productivity and reducing both design time and cost. Despite the advantages of HLS, the tools can still be time-consuming to use and demand considerable expertise, thus creating the potential for substantial improvement, especially with the integration of technologies like large language models (LLMs).Recent advancements in LLMs [30] have showcased their ability to automate various computational tasks, including code generation and software engineering. This presents a unique opportunity"}, {"title": "2 Taxonomy of LLM for HLS", "content": "The application of LLMs to different stages of the HLS process has emerged as a promising research direction. To provide a structured overview of this evolving landscape, we present a taxonomy (illustrated in Figure 1) that categorizes LLMs based on their primary role in HLS: specification generators, design space exploration assistants, code generators, and hardware verification tools. This classification provides a framework for understanding how LLMs can augment HLS methodologies, as detailed in the following subsections."}, {"title": "2.1 LLM as Specification Generator", "content": "LLMs hold promise as specification generators in HLS, translating natural language or higher-level code into HLS-compatible formats (e.g., HLS-C) [7, 23, 29]. This allows for intuitive and accessible expression of hardware functionality. Challenges persist in mitigating ambiguities inherent in natural language, which can lead to misinterpretations. Techniques like prompting, clarification dialogues, and formal verification are crucial for ensuring the correctness of LLM-generated specifications [15]."}, {"title": "2.2 LLM as Code Generator", "content": "LLMs can help with code generation, directly generating synthesizable HDL from high-level specifications [4, 6, 26]. This automation can boost productivity and reduce errors. The challenge lies in ensuring generated code quality and providing designers control over code structure and style [15]. Recent research demonstrates LLM capabilities in generating functional HDL for various hardware components, including arithmetic units [14], controllers, and simple processors [4], suggesting a promising future for this approach."}, {"title": "2.3 LLM as Hardware Verification Assistant", "content": "LLMs can assist with hardware verification in HLS, by automating the generation of test cases and identifying potential design flaws [1, 12]. This can lead to significant time savings and improved design reliability. However, challenges persist in ensuring the accuracy of LLM-generated test cases and their integration into existing HLS workflows. Ongoing research [19] explores the potential of LLMs in areas like formal verification, further highlighting their potential in ensuring the correctness of complex designs."}, {"title": "2.4 LLM as Design Space Exploration Assistant", "content": "Although receiving less attention than other applications, LLMs are promising in aiding HLS design space exploration (DSE) by suggesting optimizations and exploring design alternatives [13]. Their ability to analyze design constraints and objectives can lead to faster design cycles and innovative solutions. However, effective LLM DSE assistance requires incorporating domain-specific knowledge and addressing potential biases in suggestions. Recent research shows LLMs can optimize hardware accelerators, explore neural network architectures, and propose circuit-level optimizations, emphasizing their transformative potential for DSE [25]."}, {"title": "3 Survey of the State-of-the-Art in LLMs for HLS", "content": "This section surveys the diverse applications of LLMs in HLS, spanning hardware design automation, software-hardware co-design, and design of embedded systems. We examine key research areas such as natural language processing (NLP) to HDL translation, code generation, optimization and verification, and multimodal approaches. We also discuss input modalities used in the state-of-the-art, like textual descriptions and pseudocode, and the output modalities such as HDLs (VHDL, Verilog, SystemVerilog) and HLS-compatible programs (e.g., HLS-C). Finally, we highlight current approaches to benchmarking and evaluating LLM-driven HLS, emphasizing the need for standardized metrics and datasets to facilitate fair comparisons and drive further advancements in this rapidly evolving field."}, {"title": "3.1 LLMs Used for HLS", "content": "Recent advancements in LLMs such as ChatGPT, Gemini, Claude, and LLAMA have great potential for use in HLS. While many current works leverage the popular ChatGPT for their HLS experimentation, both general-purpose and custom-tuned LLMs have been utilized to automate and optimize synthesis processes [10]. As expected, fine-tuning models on domain-specific data often yields superior performance in generating desired outputs within the HLS workflow. For instance, Nadim et al. [17] introduced a multi-expert LLM architecture to address the challenges of design complexity. By using specialized models and a complexity classifier, they achieved an improvement of up to 23.9% in the pass@k metric. However, a consistent theme emerging from both existing literature and our experiments is the necessity of human-in-the-loop (HITL) approaches for successful LLM integration in HLS. For example, Collini et al. [7] highlighted the significant human expert guidance required for converting a C-based QuickSort kernel to HLS-C. Similarly, Swaroopa et al. [23] demonstrated a semi-automated approach for generating HLS-C from natural language using LLMs, acknowledging the need for human intervention in the design process, though their work did not evaluate the quality of the resulting designs. Such a HITL approach leverages the computational strengths of LLMs while retaining the nuanced understanding and decision-making capabilities of human experts, to achieve superior HLS outcomes."}, {"title": "3.2 Applications", "content": "The increasing interest in applying LLMs to HLS has led to promising developments across various domains. For example, LLMs have shown success in automating the generation of analog/mixed-signal (AMS) circuit netlists from transistor-level schematics [24]. In the domain of RTL generation, LLMs have demonstrated their capability to generate RTL code from natural language descriptions [15] and, as explored in [4], have the potential to aid in writing and debugging HDL code through conversational interactions with existing LLM tools like ChatGPT. Additionally, LLMs are being integrated into tools like MATLAB and Simulink to translate high-level design specifications into synthesizable Verilog and VHDL code, streamlining the HDL generation process. In the domain of code security, Nair et al. [18] investigated the vulnerabilities in hardware code generated by ChatGPT, specifically analyzing common"}, {"title": "3.3 Input and Output Modalities", "content": "The versatility of LLMs in HLS stems, in part, from their ability to process and generate information across diverse modalities. Textual descriptions, including high-level design specifications, natural language explanations of functionality, and code snippets in languages like C/C++ often serve as primary input modalities. LLMs can transform these textual inputs into HDL such as Verilog or VHDL, as seen in applications that convert natural language descriptions directly to HDL [4, 15, 16]. Beyond text, advanced LLMs are increasingly capable of handling multimodal inputs, which incorporate images, schematics, or other data types [5]. This can allow for a more nuanced understanding of design requirements by integrating visual and textual information.\nThe output modalities of LLMs for HLS are equally diverse. Primarily, LLMs can generate synthesizable HDL code from textual or multimodal inputs [15]. Additionally, LLMs can optimize existing code by automatically inserting and tuning pragmas to enhance the synthesis process. Moreover, LLMs can generate testbenches and verification scripts, which are vital to validate the functionality and performance of the synthesized hardware."}, {"title": "3.4 Benchmarking and Evaluation", "content": "The evaluation and advancement of LLMs in HLS rely on robust benchmarks and datasets. Several key initiatives have emerged to address this need, including the RTLLM benchmark [15], which provides a framework for evaluating LLM performance in generating RTL from natural language instructions, encompassing syntax, functionality, and code quality. The RTL-Repo benchmark [2] expands this evaluation by assessing LLM capabilities in generating Verilog code autocompletions within large-scale and complex RTL projects, reflecting real-world design scenarios. VerilogEval [14] is a framework for evaluating the effectiveness of LLMs in generating Verilog code, including tasks like module implementation, code debugging, and testbench construction, to assess their potential in hardware design automation. Similarly, VHDL-Eval [27] is a specialized framework designed to evaluate LLM performance specifically in VHDL code generation. Wan et al. [28] explored using LLMs to insert bugs into HLS code, and created a dataset including both correct and injected buggy codes. These benchmarks and datasets, along with other emerging efforts, are crucial in LLM-driven HLS research, facilitating the evaluation of LLM capabilities and guiding the development of more robust HLS solutions."}, {"title": "4 Experimental Methodology", "content": "This section details our experimental methodology for evaluating the effectiveness of integrating LLMs into the HLS process. We aim to assess both the design process and the quality of the hardware generated using LLMs in comparison to solely using traditional HLS tools. We investigate four approaches:\n(1) Baseline: Generating Verilog using a standard HLS tool (Vitis HLS) from C code.\n(2) Direct LLM translation: Employing LLMs to translate C code into Verilog.\n(3) Natural language to Verilog: Directly generating Verilog code from natural language specifications using LLMs.\n(4) Natural language to code: Using LLMs to interpret natural language specifications into HLS-C benchmarks, which are then translated into Verilog using Vitis HLS."}, {"title": "4.1 HLS Approach", "content": "The general HLS design flow, as illustrated in Figure 2a, transforms a high-level language input to a synthesizable hardware description (e.g., in Verilog or VHDL). This process starts with describing the desired hardware functionality in a high-level language like C/C++/SystemC), followed by synthesis for a specific hardware target, e.g., FPGAs like the Artix-7 or Zynq UltraScale+. We refer to this process as C\u2192HLS Verilog.\nHLS tools offer a range of directives to guide the synthesis process, allowing designers to control various aspects of the design, such as loop unrolling, pipelining, array partitioning, and performance optimization. While these directives provide flexibility, the resulting HDL code generated by HLS tools can often be complex and challenging to interpret for designers who are primarily accustomed to higher-level programming languages. This limited visibility into the generated HDL code is a key consideration that motivates the exploration of LLMs in HLS, aiming to improve the design process by providing higher-level abstractions or enhancing code understandability."}, {"title": "4.2 LLM-Assisted HLS Approaches", "content": "Here, we describe the three LLM-assisted approaches explored herein, showcasing the diverse ways in which LLMs can contribute to hardware design. The direct LLM translation approach, denoted as C\u2192LLM Verilog, and the natural language to Verilog approach, denoted as NL\u2192LLM\u2192Verilog, demonstrate the capability of LLMs to generate Verilog directly from either code or natural language descriptions. The natural language to benchmark approach, denoted as NL\u2192LLM\u2192HLS-C, on the other hand, highlights the potential for LLMs to augment existing HLS tools by raising the level of abstraction to natural language input. Figure 2b illustrates the design flow for each of these LLM-assisted HLS methodologies."}, {"title": "4.2.1 C LLM\u2192 Verilog", "content": "The use of LLMs to directly generate synthesizable hardware accelerators in Verilog requires a well-defined procedure. This procedure involves the steps to generate Verilog code from high-level specifications and subsequent steps to produce a fully functional accelerator, from simulation to place-and-route. For example, a testbench is necessary to validate the accelerator's functionality during simulation. A place-and-route-ready hardware accelerator consists of Verilog code, TCL commands to automate the assembly of the accelerator's design (instantiating IP cores, connecting them, and setting up the overall project structure), and XDC files to specify the constraints of the accelerator such as clock period and I/O delay.\nFigures 3, 4, 5, 6, and 7 illustrate our C\u2192LLM\u2192Verilog process for different components of the hardware design flow. The first step defines the context of the generation process, including, but not limited to, the designer's role, the hardware background, and the constraints that the LLM (ChatGPT-40, in our case) should follow to better identify the corresponding context and purpose of this process. Figure 3 shows the context we used in our experiments. We identify ourselves as hardware engineers and aim to translate a C program to HDL in Verilog. We specify that this Verilog module should target the Xilinx FPGA part xc7a200tfbg-484-1. Although ChatGPT-40 records the part in its memory, the design is not guaranteed to meet the I/O or resource constraints unless we explicitly instruct the LLM to meet the I/O constraints. If the specification of the part does not exist or is incorrect in the LLM, we must manually provide this information to the LLM.\nAfter providing the role, background, and constraints of the designer and hardware to the LLM, we provide the source code to the LLM. It is important to be mindful of ChatGPT-40's limitations: a 128k token limit for combined input and output, with a maximum of 4k tokens for the output alone. If a larger program is needed, it should be divided accordingly. In our experiments, all C benchmarks were within the 128k token limit, allowing us to input the entire program at once. However, due to the 4k output constraint, generating the complete Verilog accelerator required multiple iterations. Once generated, the Verilog output undergoes syntax and design error checking.\nFor designers proficient in hardware design, syntax and design error checking can be performed directly within the LLM. Otherwise, a validation tool like Vivado is necessary. Once an error is identified, we describe the error in natural language to the LLM and regenerate the Verilog code. This process is repeated until successful simulation and implementation in Vivado. We encountered"}, {"title": "4.2.2 NL LLM Verilog", "content": "The second approach is similar to C\u2192LLM\u2192Verilog but uses natural language descriptions (or pseudocode) of the program's functionality as input to the LLM, instead of a programming language like C/C++. We described details such as input/output, variable types, loops, and operations. The number of prompts required in this approach depends on the complexity of the program and the designer's preferences, with LLMs like ChatGPT-40 potentially accommodating the entire program in a single prompt, as in our experiments."}, {"title": "4.2.3 NL LLM\u2192HLS-C", "content": "The third approach differs from the previous two by leveraging the strengths of both LLMs and traditional HLS tools. Instead of generating Verilog directly, it utilizes an LLM to translate natural language descriptions into HLS-compatible input (HLS-C), which is then processed by the HLS tool to produce the synthesizable Verilog output. This approach combines the expressiveness of natural language with the power and completeness of existing HLS tools, ultimately lowering the barrier to entry for hardware design by minimizing the need for proficiency in high-level programming languages."}, {"title": "5 Experimental Setup", "content": "To evaluate the three LLM-based approaches and compare them with the baseline HLS approach, we used nine benchmarks (syrk, syr2k, mvt, k3mm, k2mm, gesummv, gemm, bicg, and atax) from the Polybench suite [20], specifically designed for evaluating the performance of HLS tools and compiler technologies. These benchmarks encompass computational kernels common in scientific and engineering applications, such as matrix multiplication, 2D convolution, and Cholesky decomposition. We employed ChatGPT-40 as our LLM model, Vitis HLS 2023.2 as our HLS tool, and Vivado 2023.2 for implementation targeting a Xilinx xc7a200tfbg484-1 FPGA. For each benchmark, we generated designs using all four approaches and collected data on resource utilization, power consumption, execution cycles, and critical path delay from Vitis HLS and Vivado.\nNote that the NL\u2192LLM\u2192Verilog approach yielded an initial Verilog design with an equivalent structure to the initial Verilog design generated using the C\u2192LLM Verilog approach. As such, these approaches share the same steps after the initial input stage, and thus have the same evaluation data. We tracked the number of"}, {"title": "6 Results and Analysis", "content": "Table 1 presents the number of prompts required for each file type (HLS-C, Verilog, TCL, testbench, and XDC) to construct a complete hardware accelerator from C benchmarks. As demonstrated in Sec. 5, the C\u2192LLM\u2192Verilog and NL\u2192LLM\u2192Verilog approaches share the same prompts after the initial input, leading to identical place-and-route results. For the NL\u2192LLM\u2192HLS-C approach, we also include the number of prompts needed to generate the HLS-C code. Since we targeted the same functionality as the C benchmark, the NL\u2192LLM\u2192HLS-C and C\u2192HLS\u2192 Verilog approaches share the same place-and-route outcomes.\nNotably, generating the Verilog code generally required the most prompts compared to other file types. But the number of prompts required varied significantly depending on the benchmark, as well as our growing familiarity with the LLM's behavior with Verilog generation. The syrk benchmark, for example, required considerably more interaction with the LLM compared to atax (the last benchmark we worked on). The syrk kernel exhibits a higher level of complexity, containing four nested loops with multiple multiplications in a single operation and three 2D arrays for inputs and outputs. Conversely, atax only comprises two nested loops and one 2D array for input. This suggests that the inherent complexity of the benchmark code, as well as our initial learning curve to effectively prompt the LLM to minimize errors, heavily influenced the number of prompts needed for accurate Verilog generation. As we gained experience and refined our prompting strategies, we were able to consolidate prompts, leading to faster generation for subsequent benchmarks. In contrast, the number of prompts for TCL generation remained relatively consistent across all benchmarks, implying that this task is less sensitive to the specific characteristics of the input code. The complexity of the benchmark and the designer's growing familiarity with LLM interaction are key factors in determining the number of prompts needed for successful Verilog generation, although prior design experience can also play a role.\nTable 2 presents the simulation and implementation results for both LLM-based and HLS-based approaches. For each benchmark,"}, {"title": "7 The Energy Elephant in the LLM-HLS Room", "content": "While the initial excitement surrounding the integration of LLMs into the HLS workflow has spurred significant research, a critical aspect has been conspicuously absent from most discussions: the energy implications. The majority of studies have focused on the potential of LLMs to streamline the design process, enhance automation, and improve the quality of generated hardware. However, they have largely overlooked the energy consumption associated with both the training and inference of these models.\nLLMs, particularly large-scale models like GPT-3 and GPT-4, are notorious for their computational demands. Training LLMs can consume hundreds of megawatt-hours to several gigawatt-hours of electricity [22]. Even inference, the process of generating responses to prompts, can be computationally intensive, requiring substantial energy resources. The Electrical Power Research Institute (EPRI) estimates that a single ChatGPT query can consume approximately 2.9 W-hours of energy-nearly 10 times the power of a single Google search [9]-a considerable amount when numerous queries are needed for HLS tasks. This raises concerns about the overall energy efficiency of incorporating LLMs into the HLS flow. Given that a primary goal of HLS is to design hardware accelerators that are more energy efficient than general-purpose computers, the energy overhead of utilizing LLMs could outweigh the intended benefits.\nFurthermore, the process of fine-tuning LLMs for specific HLS tasks can exacerbate the issue of energy consumption. Fine-tuning involves retraining the model on domain-specific data, which is computationally expensive. If the energy cost of fine-tuning and utilizing an LLM is greater than the energy saved across all resulting hardware designs, then employing LLMs in this way would be counterproductive for energy efficiency.\nThe lack of attention to power/energy implications in current research raises concerns about the sustainability and practicality of LLM-driven HLS. As the field progresses, it is imperative to thoroughly investigate and quantify the energy costs associated with LLM utilization. This will enable a more comprehensive evaluation of the trade-offs between design efficiency and power consumption, ultimately leading to more informed decisions regarding the appropriate use of LLMs in HLS."}, {"title": "8 Conclusion", "content": "This paper has explored the application of Large Language Models (LLMs) in High-Level Synthesis (HLS), evaluating their potential to transform hardware design workflows. Through a survey and experimental evaluations, we assessed the ability of LLMs to generate Verilog code from high-level specifications, including both C benchmarks and natural language descriptions. Our findings reveal that LLM-based approaches can significantly enhance the efficiency of the HLS process, demonstrating notable improvements in resource utilization, execution cycles, and power consumption for most benchmarks compared to traditional HLS tools. However, challenges remain in ensuring the quality and optimization of LLM-generated code, particularly regarding critical path delays and the complexity of initial prompt interactions. Additionally, the substantial energy consumption associated with training and utilizing LLMs raises concerns about the overall energy efficiency of their integration into HLS workflows. Despite these challenges, the promising results suggest that with further refinement and research, LLMs could play a pivotal role in the future of hardware design automation, offering a powerful tool to streamline and optimize the HLS process."}]}