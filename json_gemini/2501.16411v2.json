{"title": "PHYSBENCH: BENCHMARKING AND ENHANCING VISION-LANGUAGE MODELS FOR PHYSICAL WORLD UNDERSTANDING", "authors": ["Wei Chow", "Jiageng Mao", "Boyi Li", "Daniel Seita", "Vitor Guizilini", "Yue Wang"], "abstract": "Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited. To close this gap, we introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs' physical world understanding capability across a diverse set of tasks. PhysBench contains 10,002 entries of interleaved video-image-text data, categorized into four major domains: physical object properties, physical object relationships, physical scene understanding, and physics-based dynamics, further divided into 19 subclasses and 8 distinct capability dimensions. Our extensive experiments, conducted on 75 representative VLMs, reveal that while these models excel in common-sense reasoning, they struggle with understanding the physical world\u2014likely due to the absence of physical knowledge in their training data and the lack of embedded physical priors. To tackle the shortfall, we introduce PhysAgent, a novel framework that combines the generalization strengths of VLMs with the specialized expertise of vision models, significantly enhancing VLMs' physical understanding across a variety of tasks, including an 18.4% improvement on GPT-4o. Furthermore, our results demonstrate that enhancing VLMs' physical world understanding capabilities can help embodied agents such as MOKA. We believe that PhysBench and PhysAgent offer valuable insights and contribute to bridging the gap between VLMs and physical world understanding. Project Page is here", "sections": [{"title": "1 INTRODUCTION", "content": "Understanding the physical world is a fundamental challenge in embodied AI (Gupta et al., 2021; Srivastava et al., 2021). Embodied agents are required to understand the physical properties of objects (e.g., mass, stiffness) to accurately interact with these objects. They also need to understand the relationships of physical objects to operate efficiently in cluttered environments, understand the structure of physical scenes for safe navigation and manipulation, and anticipate the outcomes of interactions and physics-based dynamics for better planning and preventing accidents. These capabilities of intuitive physics (McCloskey et al., 1983; Carey, 2000) are innate to humans and can also greatly benefit embodied agents, allowing them to perform complex tasks and operate safely in real-world scenarios (Kill & Kim, 2020).\nVision-language models (VLMs) (Liu et al., 2024c; Achiam et al., 2023; Team et al., 2023) have emerged as promising solutions for building embodied agents (Liu et al., 2024a; Nasiriany et al., 2024; Huang et al., 2023a). Trained on large amounts of human knowledge, these models have developed strong capabilities in reasoning and task planning (Yue et al., 2024; Lu et al., 2024b; Kim et al., 2024; Niu et al., 2024; Zhen et al., 2024). However, relying solely on these capabilities is insufficient for developing generalist embodied agents. A series of studies have highlighted a gap in understanding the physical world, leading to operational errors (Liu et al., 2024a), such as mishandling fragile objects (Wang et al., 2023c) or failing to recognize appropriate grasping affordances (Guo et al., 2024). Since these agents operate in and interact with the real world, VLMs must possess a comprehensive understanding of the physical world\u2014a critical yet underexplored domain. This deficiency in physical world understanding limits the effective deployment of VLMs in embodied applications (Liu et al., 2024a; Guo et al., 2024; Gao et al., 2024a)."}, {"title": "2 RELATED WORK", "content": "Physical Comprehension Datasets. Early benchmarks (Riochet et al., 2018; Rajani et al., 2020) were developed primarily for vision-only models, while more recent efforts (Yi et al., 2019; Chen et al., 2022; Wang et al., 2024g) have predominantly focused on simple visual primitives, such as spheres, cubes, and rigid object collision events, often restricted to a limited set of simulated scenarios (Zheng et al., 2024b; Tung et al., 2023). However, existing VQA datasets assessing physical knowledge (Lu et al., 2022; He et al., 2024) mainly focus on commonsense reasoning rather than physical world perception. Spatial VQA benchmarks (Chen et al., 2024a; Lyu et al., 2024; Bonnen et al., 2024; Wang et al., 2024d) emphasize geometric relationships in 3D sence, which represent only a part of the physical. In contrast, PhysBench is the first comprehensive dataset designed to evaluate models' understanding of the physical world, encompassing a wide variety of scenarios and tasks not covered by previous benchmarks.\nPhysical Reasoning Models. Models for understanding the physical world generally fall into two categories. The first comprises physics-specialized models (Guen & Thome, 2020; Duan et al., 2022), which are typically limited to predicting the next state and are not applicable to other tasks. The second includes physical oracle models (Zheng et al., 2024b; Tung et al., 2023), which are suitable for only a narrow range of tasks due to their reliance on predefined rules. These models often require training additional modules like R-CNN, and their probabilistic outputs restrict them to classification tasks, limiting their ability to handle open-ended questions. In contrast, PhysAgent offers greater flexibility and adaptability across a broader spectrum of problems without these limitations.\nVision-Language Models. Vision-Language Models (VLMs) are large-scale models that integrate visual modalities with language understanding (Wu et al., 2023b; Zhan et al., 2024; Dai et al., 2024). In recent years, there has been a surge of work leveraging VLMs as agents for embodied AI (Liu et al., 2024a; Nasiriany et al., 2024). Although these approaches are generalizable, they face challenges due to weak physical world understanding capabilities (Liu et al., 2024a; Guo et al., 2024). By employing PhysBench and PhysAgent, these shortcomings can be mitigated, enhancing the physical world understanding capabilities of VLMs and enabling more reliable robotic control. Additionally, spatial VLMs (Bonnen et al., 2024) have identified that most VLMs lack 3D spatial reasoning capabilities due to insufficient data. However, since spatial reasoning represents only a subset of physical world understanding, our work aims to provide a more comprehensive evaluation and improvement of VLMs' physical world understanding abilities. For additional related work, see Appendix G."}, {"title": "3 PHYSBENCH", "content": "To assess VLMs' physical world understanding ability, we first define the concept of physical world understanding and introduce PhysBench in Section 3.1. Next, we provide a detailed description of the data collection process in Section 3.2. Utilizing PhysBench, we conduct experiments to determine whether VLMs can effectively comprehend the physical world in Section 3.3. Finally, in Section 3.4, we discuss the potential reasons for poor performance."}, {"title": "3.1 OVERVIEW OF PHYSBENCH", "content": "Understanding the physical world is essential yet fundamentally challenging for embodied AI, as systems must perceive, interpret, and predict the properties and dynamics of objects and environments. This involves comprehending object properties and relationships, interpreting environmental scenes, and anticipating interaction outcomes based on visual cues and core physical principles to ensure safe and effective operation."}, {"title": "3.2 DATASET COLLECTION PROCESS", "content": "To ensure data quality, all questions were manually annotated by graduate students in STEM fields and further refined through a rigorous review process after collecting and clipping the raw images or videos. To maintain consistency in annotations, we implemented multiple rounds of cleaning and validation throughout the following steps. We have preserved intermediate outputs from the annotation process, such as depth and reflectance maps for simulator-generated data and human-annotated physical principles for many web-sourced videos. The process involves the following sequential steps: (a) Video Collection. Videos and images are gathered from web searches, simulations, and real-world captures. The collection process uses predefined simulation rules, LLM-guided queries, and other strategies to find related images or videos (see Appendix A). Human annotators further refine the data by clipping and annotating physical principles in the images or videos. (b) Video Captioning. Human-annotated raw videos are processed through automatic filtering, followed by GPT-4o annotations that generate captions with human check. (c) Questions Design. For videos annotated with physical principles, we generate physics-related questions using both manual design and GPT-4o, following predefined rules. An automated filter and manual review processes eliminate irrelevant questions. (d) File Organization. The remaining valid questions are categorized by task, sub-task, and ability type by human experts. (e) Quality Check. The organized dataset undergoes a human review to ensure that the questions are physical world relevant, rely on all input information, are not grounded in common sense, and are accurately categorized with clear questions and corresponding answers. Due to space limitations, the collection guidelines are provided in Appendix B."}, {"title": "3.3 CAN VLMS UNDERSTAND THE PHYSICAL WORLD", "content": "To assess whether VLMs can understand the physical world, we evaluated 75 representative VLMs on PhysBench and found that a significant performance gap remains between VLMs and human-level understanding. The primary results are presented in Table 3, while detailed analyses of sub-task performance and ability types across the four task categories are provided in Appendix F.3.\nSetup. Our evaluation was conducted under three configurations: (a) Image VLMs, which support only single-image input (e.g., LLaVA-1.5 and BLIP-2); (b) Video VLMs, designed for video comprehension (e.g., Chat-UniVi and PLLaVA); and (c) General VLMs, which support multiple images and interleaved inputs (e.g., VILA-1.5 and GPT-4o). It is important to note that the data used for evaluating setups (a) and (b) is a subset of PhysBench test subset with interleaved QA pairs removed, whereas setup (c) was evaluated on the full dataset. For most models, we followed the standard protocol outlined in VLMEvalKit Contributors (2023), setting the temperature to 0. For models that do not support multiple images as input, we employed two methods: the merge method, where video frames are concatenated into a single image (Fu et al., 2024; Zhang et al., 2024a; Jiang et al., 2024), and the seq method, where video frames are input sequentially as individual images. Notably, only models using the seq setup can handle interleaved text-image sequences. For details on VLM prompts and hyperparameters, see Appendix E.\nVLMs exhibit a limited understanding of the physical world. Our evaluation indicates that most models achieve an average accuracy of approximately 40%, which is significantly below human-level performance. Even the best-performing model, GPT-4o, attains only 49.49% accuracy, underscoring the substantial gap between current VLMs and true comprehension of the physical world."}, {"title": "3.4 WHY DO VLMS STRUGGLE WITH PHYSICAL WORLD UNDERSTANDING", "content": "To further investigate why VLMs struggle with physical world understanding, we analyzed PhysBench and discovered that it differs significantly from common VQA tasks. Additionally, we found that the performance of larger model size or more training data does not result in clear improvements on PhysBench, which may be due to a lack of physical world knowledge in the training data. Furthermore, we found that many errors stem from this deficiency; when we augmented the models with physical world knowledge, their performance improved. This further suggests that the gap between VLMs and physical world understanding may be attributed to limitations in the training data.\nPhysical world understanding differs significantly from common VQA tasks. To assess the relationships between our tasks and other VLM benchmarks, we adopted the methodology proposed by (Tong et al., 2024; Fang et al., 2024) to construct a correlation map, as shown in Figure 4(a). Details on the construction of the correlation map are provided in Appendix F.6. Our analysis reveals that PhysBench differs significantly from traditional VLM benchmarks, exhibiting closer alignment with POPE (Li et al., 2023h) in tasks such as hallucination detection, while also showing that performance does not consistently improve with increased data or model scale.\nVLMs's physical world understanding ability does not scale with model size, data, or frames. (1) Model Size Scalability. Figure 6(a) shows that increasing model size using the same dataset significantly enhances performance on common QA tasks. However, this improvement does not extend to PhysBench, where gains are limited or even negative. For instance, while VILA-1.5's performance improves by 7.1% on common QA tasks when scaling from 3B to 7B parameters, it"}, {"title": "4 PHYSAGENT", "content": "Recognizing perceptual inaccuracies and knowledge gaps as key sources of error shown in Section 3.4, we introduce PhysAgent in Section 4.1 to improve VLMs' understanding of the physical world by integrating vision foundation models for enhanced perception and incorporating physical knowledge memory. To verify whether enhancing VLMs' physical understanding facilitates the deployment of embodied agents, we conducted five embodied agent tasks as detailed in Section 4.2."}, {"title": "4.1 HOW TO ENHANCE VLMS FOR PHYSICAL WORLD UNDERSTANDING", "content": "We propose PhysAgent, a novel framework that integrates knowledge memory and vision foundation models to enhance physical world understanding in VLMs. This framework is inspired by our findings in Section 3.4, where we identified perceptual errors and insufficient knowledge as the"}, {"title": "4.2 CAN PHYSICAL WORLD UNDERSTANDING HELP IN EMBODIED APPLICATIONS", "content": "Despite gaining significant attention in recent years for their strong generalization capabilities, VLMs as embodied agents (Liu et al., 2024a) still exhibit fundamental operational errors during physical world interactions. In this section, we investigate whether enhancing the physical world perception abilities of VLMs can improve their performance in downstream embodied agent tasks."}, {"title": "5 CONCLUSION", "content": "In conclusion, we introduce PhysBench, a benchmark designed to assess Vision-Language Models' understanding of the physical world. Through experiments on 75 models, we identified significant gaps in physical world understanding, particularly in open-source models, due to inadequate training data. To address this, we developed PhysAgent, a novel framework that improves physical reasoning by 18.4% on GPT-4o. Additionally, we demonstrated the utility of our dataset and approach in robotic tasks, helping to advance the understanding of the physical world in machine intelligence."}]}