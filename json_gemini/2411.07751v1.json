{"title": "SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State Space Model", "authors": ["Xinyuan Qian", "Jiaran Gao", "Yaodan Zhang", "Qiquan Zhang", "Hexin Liu", "Leibny Paola Garcia", "Haizhou Li"], "abstract": "Speech enhancement plays an essential role in various applications, and the integration of visual information has been demonstrated to bring substantial advantages. However, the majority of current research concentrates on the examination of facial and lip movements, which can be compromised or entirely inaccessible in scenarios where occlusions occur or when the camera view is distant. Whereas contextual visual cues from the surrounding environment have been overlooked: for example, when we see a dog bark, our brain has the innate ability to discern and filter out the barking noise. To this end, in this paper, we introduce a novel task, i.e. Scene-aware Audio-Visual Speech Enhancement (SAV-SE). To our best knowledge, this is the first proposal to use rich contextual information from synchronized video as auxiliary cues to indicate the type of noise, which eventually improves the speech enhancement performance. Specifically, we propose the VC-S2E method, which incorporates the Conformer and Mamba modules for their complementary strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and AudioSet datasets, where the results demonstrate the superiority of VC-S2E over other competitive methods. We will make the source code publicly available. Project demo page: https://AVSEPage.github.io/", "sections": [{"title": "I. INTRODUCTION", "content": "In our daily living environments, speech signals are often distorted by various environmental background noises during their propagation. Speech enhancement (SE) is a task aiming at isolating the clean speech in the presence of noise interference, resulting in improved speech intelligibility and perceptual quality [1]\u2013[4]. It enables natural and effective Human-Robot Interaction (HRI) and plays a crucial role in various applications, such as hearing aids, mobile communication, automatic speech recognition [5]\u2013[7], speaker verification [8], and speaker tracking [9]\u2013[11]. These applications underscore the importance of SE in realistic scenarios. Traditional signal processing-based SE approaches, which are derived from the assumed properties on speech and noise, are incapable of suppressing highly non-stationary noise sources [12]\u2013[14]. In the past decade, with the advent of deep learning technology and increased computational resources, supervised speech enhancement solutions has achieved great success [2].\nDespite the significant strides made in the field, the challenge of noise reduction without inflicting artifacts on the speech signal persists, particularly in dynamic environments characterized by non-stationary and multi-source noise [15]. This difficulty is further compounded by the need to maintain the integrity of the speech signal, ensuring that the naturalness of the human voice is preserved.\nTo address this challenge, researchers have been exploring cutting-edge signal processing methodologies and sophisticated machine learning paradigms. One promising solution involves the use of neural networks, which has demonstrated great capabilities in extracting features and separating signals from complex acoustic environments. A variety of network architectures are trained to learn the underlying patterns in noisy audio data, thus accomplishing the objective of speech enhancement [16]. Each of these models contributes unique strengths to the task of learning and generalizing from noisy audio data. For example, Multi-Layer Perceptrons (MLPs) are proficient in detecting intricate, non-linear data patterns, whereas Recurrent Neural Network (RNN) effectively manage the sequential dependencies in audio signals. Temporal Convolutional Network (TCN) excel in capturing long-range dependencies without suffering from the vanishing gradient problem that plagues standard RNN. The Transformer architecture, featuring self-attention, has transformed the field by allowing models to process any part of the input sequence, which is crucial for tasks involving widespread noise-speech relationships. The Mamba architecture [17], as the latest advancement, further extends the capabilities of noise reduction and speech enhancement.\nResearchers have increasingly acknowledged the importance of maintaining semantic, temporal, and spatial coherence between audio and video sources [18], [19]. This motivates attempts to use video information as a complement of audio input to recover details that are lost in audio-only scenarios. Existing Audio-Visual Speech Enhancement (AVSE) schemes often exploit temporal synchronized facial and lip movements to improve the clarity and perception of enhanced speech [20]\u2013[22]. Despite outperforming audio-only SE systems, they are infeasible in many practical scenarios (e.g., outdoors or pandemic period) where human visual cues are not available. Moreover, inaccurate face or lip detection (e.g., in low-quality videos) may also result in degraded performance. In contrast, visual cues of environmental information, such as noise scenes or background objects emitting the noise, are easier to capture. It is more practical to use visual environmental cues to provide a valuable complement to speech enhancement. Thus, to fully leverage audio-visual information to enhance uni-modal learning, it is essential to consider these modality-specific attributes.\nIn this paper, we introduce a novel AVSE framework, as illustrated in Figure 1, which uses visual information of the surrounding scenes as an auxiliary prompt to improve SE performance. Specifically, it addresses the limitations of current technologies, particularly in scenarios where an accurate capture of facial or lip information is not available.\nThe contributions of this paper are summarized as follows:\n1. We introduce a novel and more practical scene-aware AVSE task, namely SAV-SE. Unlike existing AVSE studies that rely primarily on visual facial and lip movements, this paper explores auxiliary visual contextual cues from the surrounding scenes to mitigate environmental background noise.\n2. We are the first to explore selective State Space Model (SSM) for audio-visual speech enhancement. Specifically, we propose a Visual-prompting ConMamba for Scene-aware Speech Enhancement (VC-S2E), a novel approach that leverages audio-visual modalities to improve speech quality and intelligibility. Built upon innovative hybrid convolution-SSM architecture, ConMamba can capture both long-range global interactions and localized fine-grained feature patterns.\n3. We comprehensively evaluate our proposed method across three widely used AV datasets. The results consistently confirm the superiority of our VC-S2E over other competing methods in speech quality and intelligibility. Meanwhile, the visualization analysis illustrates that visual focal areas locate at the sounding object, demonstrating the contribution of visual scene information."}, {"title": "II. RELATED WORK", "content": "This section describes the evolution of speech enhancement, from traditional to state-of-the-art audio-visual and state space model techniques, assessing their impact on speech processing and their capacity to overcome current limitations."}, {"title": "A. Audio only Speech Enhancement", "content": "Traditional SE mainly include spectral subtraction [23], Wiener filtering [24], statistical model methods [25], and Non-negative Matrix Factorization (NMF) algorithms [26]. Due to assumptions about the statistical characteristics of speech and noise, these methds fail to handle highly non-stationary noise signals. In the past decades, Deep Learning (DL) has revolutionized the field of SE, demonstrating impressive performance over traditional schemes [2], [27].\nDL-based SE methods can be broadly grouped into temporal domain and time-frequency (T-F) domain methods. Temporal domain methods, which perform denoising directly on the raw waveform, optimize a neural network to extract the clean waveform from noisy input [28]\u2013[30]. In contrast, T-F domain methods separate clean speech from T-F representation, such as the magnitude spectrum [31], log-power spectrum [32], and complex spectrum [33]. Typically, a neural network is trained to map the noisy T-F representation to the clean T-F representation or a multiplicative T-F mask.\nDifferent network architectures have been investigated for SE. With the ability to capture long-range dynamic dependencies, the Long-Short Term Memory (LSTM) network has shown substantial performance gains over the MLP, especially in generalizing to unseen speakers [34]. Later, the TCN, which uses stacked 1-D dilated convolution with residual connection, has demonstrated comparable or better performance than LSTM, with fewer parameters and faster training speed [35], [36]. The Transformer learns global interactions effectively, enabling recent advances in SE [28], [37]. By combining convolutions with self-attention, Conformer [5] learns global interactions while capturing fine-grained local characteristics to achieve state-of-the-art (SoTA) results [38].\nMost recently, the Mamba [39] neural architecture, a novel structured SSM has demonstrated great potential as an alternative to Transformer architecture in a variety of speech processing domains, including speech enhancement [17], [40] and speech separation [41]. There are further researches which investigate the capabilities of Mamba and other SSMs in SE, especially in scenarios where multi-modal data exist or of varying quality."}, {"title": "B. Audio-Visual Speech Enhancement", "content": "Humans rely on multi-modal cues such as visual and auditory signals to explore, capture, and perceive the real world. These sensory modalities work together to provide a rich and integrated understanding of the environment. Over the past few decades, research has focused on investigating the semantic, temporal, and spatial consistencies between auditory and visual signals. Studies have shown that the complementary and mutually enhancing nature of these modalities can significantly improve perception and understanding in various contexts [42], [43]. For example, synchronizing audio and visual information has been shown to enhance object recognition, improve speech comprehension in noisy environments, and help more robust scene understanding in dynamic settings. In particular, video can be used as a supplement to discover details that are lost in acoustic scenarios. The resulting AVSE methods offer a more robust and precise enhancement capability, exceeding what could be achieved with sound alone [44].\nThe AVSS method [20] employs a dual-tower CNN architecture, where video frames (focusing on the speaker's mouth region) and audio spectrograms are encoded separately. A multi-modal mask is then generated to separate the target voice from the mixed audio, significantly improving speech enhancement performance by exploiting the spatial alignment between the visual and auditory signals. VisualVoice [45] uses the appearance of the speaker's face as an additional visual prior to isolating the corresponding vocal qualities. By incorporating cross-modal matching and speaker consistency losses from unlabeled video data, this approach enhances speech enhancement (SE) performance, especially in scenarios where the audio signal is heavily degraded. The model benefits from visual information to maintain speaker consistency and refine the target speech signal. FlowAVSE [46] takes a novel approach by adopting a conditional flow matching algorithm and optimizing a diffusion-based U-net architecture. This method enables high-quality speech generation based on cropped face images, demonstrating improved performance in real-world SE tasks. The integration of cooperative dual-attention mechanisms and dynamic audio-visual fusion strategies further enhances the robustness and effectiveness of AVSE, making it more adaptable to a variety of noisy environments. In [47], a score-based generative model is employed which leverages audio-visual embeddings from the self-supervised AV-HuBERT model to enhance speech quality and reduce artifacts like phonetic confusions. Specifically, it integrates layer-wise features from AV-HuBERT into a noise conditional score network, demonstrating improved performance and noise robustness. LA-VOCE [48] employs a transformer-based architecture to predict mel-spectrograms from noisy audio-visual inputs, while the second stage uses a neural vocoder (HiFi-GAN) to convert these spectrograms into clear waveform audio under low SNR conditions. All the aforementioned methods rely on the effective usage of human facial information. However, the effective integration of facial information requires not only accurate facial feature extraction but also robust alignment between the visual and audio modalities under varying conditions."}, {"title": "C. State Space Models", "content": "State space models (SSMs) have emerged as a powerful alternative to Transformer-based architectures for modeling long-range contextual dependencies in sequential data. Unlike Transformers, which rely heavily on self-attention mechanisms to capture interactions between distant tokens, SSMs provide a more efficient framework for processing sequences by leveraging dynamic state transitions that model temporal or spatial dependencies. This approach enables them to handle longer-range dependencies with fewer computational resources and greater scalability. A recent advancement in this area is the introduction of Mamba [39], a selective state space model (SSM) designed to overcome some of the limitations of traditional SSMs. Mamba incorporates a data-dependent SSM layer, which allows it to learn more flexible and task-specific representations. This enables Mamba to serve as a powerful backbone for a variety of language models, achieving superior performance over Transformers on several large-scale, real-world datasets. In particular, Mamba demonstrates linear scalability with respect to sequence length, making it an attractive alternative for tasks that require processing long sequences efficiently.\nThe success of Mamba has extended beyond natural language processing (NLP) to other domains such as computer vision and multimodal processing. For example, the Vim model [49] leverages bidirectional Mamba to capture dynamic global context, resulting in significant improvements in various vision tasks. Similarly, VMamba [50] introduces a cross-scan module, which enhances global context modeling by applying a four-way selective scan mechanism. In speech domain, the most recent study [17] explores Mamba as a alternative to Transformer architectures in both causal and non-causal configurations, demonstrating its great potential to be the next-generation backbone for ASR and SE. Specifically, Mamba's ability to model long-range dependencies and capture fine-grained temporal relationships makes it a compelling choice for these speech-related tasks, where context over long time horizons is crucial for accurate processing."}, {"title": "D. Summary", "content": "Although previous AVSE methods have achieved remarkable success, they often encounter challenges such as visual input quality dependency, narrow focus on facial features, and limited use of environmental signals. To address these problems, we propose a novel VC-S2E method which is built on the selective SSM architecture.\nSpecifically, it incorporates visual environmental information as an auxiliary cue to conventional SE, which eventually improves speech perceptual quality and intelligibility in various scenarios."}, {"title": "III. PROPOSED VC-S2E", "content": "The observed noisy speech signals $y \\in \\mathbb{R}^{1\\times L}$ can be modeled as the addition of the clean speech $s \\in \\mathbb{R}^{1\\times L}$ and the noise $d \\in \\mathbb{R}^{1\\times L}$: $y(l) = s(l) + d(l)$, $l = 1, ..., L$. Let us denote $Y_{t,k}$, $S_{t,k}$, and $D_{t,k}$ as the short-time Fourier transform (STFT) coefficients of $y$, $s$ and $d$, where $t\\in [1,T]$ and $k \\in [1, K]$ index the time frames and frequency bins. A typical neural SE solution is to train a DNN to estimate a T-F mask $M_{t,k}$. Here, we employ the phase-sensitive mask (PSM) [51], defined as\n$M_{t,k} = \\frac{S_{t,k}}{Y_{t,k}} \\cos \\big(arg(S_{t,k}) - arg(Y_{t,k})\\big)$  (1)"}, {"title": "B. Method", "content": "In this section, we first describe the preliminaries of state space model (SSM). We then detail the workflow of our proposed VC-S2E and the network architecture.\n1) Preliminaries: The SSM originates from the Kalman filter, which takes a time-dependent set of inputs $u(t) \\in \\mathbb{R}$ and maps it into a set of outputs $y(t) \\in \\mathbb{R}$ through a hidden state $h(t) \\in \\mathbb{R}^N$. Formally, the mapping process of SSMs can be represented as follows:\n$h'(t) = Ah(t) + Bu(t)$,\n$y(t) = Ch(t)$,  (3)\nwhere $A\\in\\mathbb{R}^{N\\times N},B\\in\\mathbb{R}^{N\\times 1}$, and $C\\in\\mathbb{R}^{1\\times N}$ are state matrix, input matrix, and output matrix. A discretization process is applied in advance to integrate SSM into deep learning architectures, where a timescale $\\Delta$ is used to transform matrices $A, B$ to their discrete counterparts $\\overline{A}, \\overline{B}$. A commonly employed technique for transformation is zero-order hold (ZOH), defined as:\n$\\overline{A} = \\exp(\\Delta A)$,\n$\\overline{B} = (\\Delta A)^{-1}(\\exp \\Delta A - I) \\cdot \\Delta B$.\nThus, Eq. (3) can be rewritten as:\n$h_t = \\overline{A}h_{t-1} + \\overline{B}x_t$,\n$Y_t = Ch_t$. (5)\nFurthermore, the output can be calculated via a global convolution:\n$K = (CB, CAB, ..., CA^{\\overline{L-1}}B)$,\n$y = u * K$, (6)\nwhere $L$ is the length of the input sequence, K is a structured convolution kernel, and * denotes the convolution operation. Mamba introduces a selective scan mechanism, allowing the model to dynamically adjust A, B, and C as functions of the input. This enables the model to learn dynamic representations while filtering out irrelevant information.\n2) Overview: Figure 2 provides the workflow of our proposed VC-S2E which enhances human speech using the auxiliary visual cues from the noise scenario. Given the synchronized audio-visual streams, it first extracts the visual scene embedding $E_v \\in \\mathbb{R}^{T \\times d_{model}}$, the scenario-aware audio embedding $E_a \\in \\mathbb{R}^{T_a \\times d_{model}}$, and the acoustic spectrum embedding $E_y\\in \\mathbb{R}^{T_y \\times d_{model}}$. Then, it estimates the PSM mask to facilitate the reconstruction of the enhanced speech waveform $s$ using the ConMamba backbone:\n$M = OLayer(ConMamba(X; \\Omega))$ (7)\nwhere $X = [E_y\\, E_a\\, E_v] \\in \\mathbb{R}^{(T_y+T_a+T_v)\\times d_{model}}$ is the fused feature representation and $\\Omega$ is the trainable parameters. $OLayer()$ indicates the output layer.\n3) Visual Encoder: We employ the pre-trained contrastive audio-visual masked autoencoder (CAV-MAE) [52] to extract the visual scene embedding. Specifically, CAV-MAE employs both contrastive learning and masked data modeling to jointly learn the coordinated audio-visual representations. This enables the generation of robust audio and visual features, offering significant benefits for multi-modal tasks that require comprehension of cross-modal interaction. To ensure temporal synchronization between video and audio, we use an upsampling layer to match the video feature length to the audio one,"}, {"title": "4) Audio Encoder", "content": "Thanks to the employed contrastive learning strategy in CAV-MAE, which captures the temporal and semantic consistencies between audio and video signals, we adopt the pretrained audio encoder to extract the visual scenario-aware audio embedding:\n$E_a = \\text{CAV-MAE}^{\\prime}(F)$  (9)\nwhere $\\text{CAV-MAE}^{\\prime}(\\cdot)$ and $F$ denote the audio encoder and the Fbank acoustic feature of $y$, respectively.\nThe STFT magnitude spectrum $|Y|$ is encoded using a 1D convolution layer to have same dimension with $E_v$ and $E_a$:\n$E_y = \\text{Conv1D}(|Y|)$ (10)"}, {"title": "5) ConMamba", "content": "In Figure 2, the rectangle with gradient blue illustrates the internal architecture of ConMamba used in our proposed VC-S2E. Specifically, it is composed of four layers stacked together, which takes the combined multi-modal feature representation $X \\in \\mathbb{R}^{(T_y+T_a+T_v)\\times d_{model}}$ as the input to produce the PSM estimate $M \\in \\mathbb{R}^{T\\times K}$.\nLet us denote $Z_i$ as the input to the $i$-th ConMamba layer. It is initially processed by a feed-forward layer, followed by the BiMamba layer to capture bidirectional dependencies within the input sequence. Subsequently, a convolution layer is employed to learn fine-grained local feature pattern, leveraging pointwise convolutions and depthwise convolutions controlled by Gated Linear Unit (GLU). Finally, another feed-forward layer is incorporated to solidify the learned transformations. This process can be formulated as:\n$Z_i^1 = Z_i + \\text{FFN}(Z_i)$\n$Z_i^2 = Z_i^1 + \\text{BiMamba}(Z_i^1)$\n$Z_i^\\prime = Z_i^2 + \\text{Conv}(Z_i^2)$\n$Z_{out} = \\text{LN}(Z_i^\\prime + \\text{FFN}(Z_i^\\prime))$  (11)\nwhere $Z_{out}$ is the output, FFN refers to the feed forward layer, BiMamba refers to the BiMamba layer, Conv refers to the convolution layer, and LN refers to layer normalization as described in the preceding sections.\nIn particular, the BiMamba layer consists of two Mamba modules, each processing the input sequence along opposing temporal axes \u2013 forward and backward, respectively. Then, the bidirectional outputs are added, enabling the model to capture complex patterns and dependencies effectively. Let U denote the input sequence to a BiMamba layer. Then, the forward and backward pass can be formulated as:\n$h_f = \\text{Mamba}(U)$ $h_b = \\text{Mamba}(flip(U))$  (12)\nThe bidirectional outputs are added to a layer normalization to provide output of the BiMamba layer:\n$h_{fused} = h_f + flip(h_b)$.\n$Z^{out} = \\text{LN} (h_{fused} + U)$ (13)\nMamba Layer. In Figure 3, we illustrate the detailed workflow of the Mamba layer. Given the input feature representation denoted as U, it is transformed through two parallel paths. The right path consists of a linear fully-connected layer followed by the SiLU activation function. The left path consists of a linear layer, a depthwise 1-D convolution unit, a SiLU activation, an SSM layer, and layer normalization in that sequence. The outputs of the two paths are then merged via element-wise multiplication, followed by a linear projection layer. Formally, the workflow of the Mamba layer can be represented as:\n$U_1 = \\text{LN} (\\text{SSM} (\\text{SiLU} (\\text{DWConv} (\\text{Linear} (U)))))$,\n$U_2 = \\text{SiLU} (\\text{Linear} (U))$,\n$U^\\prime = \\text{Linear} (U_1 \\odot U_2)$  (14)\nwhere DWConv refers to the depthwise 1-D convolution."}, {"title": "IV. EXPERIMENTS", "content": "Datasets For clean speech data, we employ the LibriSpeech train-clean-100 subset [53], containing 28 539 utterances from 251 speakers. For noise data including noisy recording and the corresponding visual scene, we employ three commonly used datasets: MUSIC [54], AVSpeech [55], and AudioSet [56]. Specifically, the MUSIC dataset contains specialized high-quality music recordings. AVSpeech involves a wide variety of interference speakers, languages, and face poses, simulating complex multi-speaker scenarios. AudioSet includes diverse environmental sound samples. These three datasets cover various aspects of audio environments and provide a comprehensive evaluation."}, {"title": "B. Experimental Results", "content": "Table I showcases the PESQ and STOI scores of the models across five SNR levels, with the best results are highlighted in bold. The evaluation results demonstrate that our VC-S2E model consistently achieves superior performance in all three datasets. In the MUSIC dataset, the unprocessed data have an average PESQ and STOI of 1.47 and 83.65%, respectively. By applying conformer-based MP-SENet, the resulting enhanced speech has improved PESQ and STOI to 2.54 and 91.65%, respectively. The AVITAR method, which employs a Conformer-based cross-attention AV framework, achieving a PESQ of 2.73 and a STOI of 92.81%. In contrast, our VC-S2E proposal shows superior performance by incorporating the BiMamba architecture with an average PESQ of 3.02 and STOI of 94.11%. It can also be seen that by integrating the spectrum, visual, and scenario-aware audio embedding, VC-S2E exceeds ExtBiMamba with an average PESQ and STOI of 2.77 and 93.09%, respectively. It should be highlighted that the advances of our VC-S2E is evident across all SNR levels. Furthermore, the same observations can be seen on the AVSpeech and AudioSet datasets as well, where our proposed VC-S2E achieves superior performance with the PESQ and STOI of 2.76 and 92.70% for AVSpeech, 2.59 and 92.25% for AudioSet, respectively.\nFigure 4 illustrates the PESQ and STOI of our proposed VC-S2E method and its counterpart without visual and scenario-aware embeddings where AudioSet is used as the noise dataset. In both figures, VC-S2E consistently outperforms the other across all categories. For example, the PESQ of 'Fire' improves from 2.50 to 2.62 while for category 'Child speech', the score improves from 2.58 to 2.73. Similarly, the STOI scores (Figure 4b) also show significant improvements. For 'Fire', the STOI score increases from 91.61% to 92.57% while for 'Child speech', it improves from 92.53% to 93.27%. These results clearly reveal that incorporating visual and audio scenario-aware embeddings leads to substantial improvements in both perceptual quality and intelligibility across each individual audio category."}, {"title": "C. Ablation Study", "content": "1) Model Input and Architecture: In Table II, we ablate the model inputs and architectures using the MUSIC noise dataset. When removing the scenario-aware audio embedding $E_a$ from our proposed VC-S2E model, the PESQ and STOI values were reduced by 0.21 and 0.86%. Moreover, without the video prior $E_v$, PESQ and STOI further degrade to 2.77 and 93.09%, respectively. This is because $E_a$ and $E_v$ are pre-trained through contrastive learning, adding $E_a$ can better integrate visual embedding $E_v$ in the subsequent fusion part. The same trends are observed when using Conformer as the backbone. For example, removing scenario-aware audio embedding $E_a$ resulted in the degraded PESQ and STOI from 2.73 and 92.81% to 2.66 and 92.63%. Further removal of E provides the worst results with the PESQ and STOI of 2.54 and 91.65%, respectively.\n2) Feature Combination: In Table III, we evaluate various feature combination strategies. Initially, we present the model's performance using only the spectral embedding $E_y$, resulting in a PESQ of 2.77 and a STOI of 93.09%. We then analyze different feature combination strategies. Using $E_y$ concatenated with $(E_a \\oplus E_v)$ yields a slight improvement, achieving a PESQ of 2.84 and a STOI of 93.28%. Similarly, the strategy of $E_y$ concatenated with $(E_a \\odot E_v)$ shows some improvement over the baseline, but not substantially. The best performance is achieved with our employed fusion strategy: $E_y \\, E_a \\, E_v$, resulting in a PESQ of 3.02 and a STOI of 94.11%. This strategy not only offers superior performance but also maintains the simplest complexity.\n3) Visualization: We employ Gradient-weighted Class Activation Mapping (Grad-CAM), a widely used scheme, to visually interpret which image regions have significant contributions to SE. As illustrated in Figure 5, the original video frames are shown in the upper row while the corresponding Grad-CAM heatmaps are depicted the bottom row, which emphasize regions crucial for detecting noise sources. The colors varying from blue to red correspond to the higher values assigned. These heatmaps reveal the model's ability to focus on relevant contextual elements, such as moving ducks, pedestrians, and the guitar, etc. which depicts the origins of the noise and indicates that the detected visual sounding object assists SE.\nIn Figure 6, we present a comparison of different log-magnitude spectrograms, showcasing the results for noisy audio, clean speech, and the speech generated by our proposed VC-S2E model under two different configurations: without (w/o) and with (w) the scenario-aware audio embedding $E_a$ and visual embedding $E_v$. From left to right, the images represent the spectrograms of noisy audio, clean speech, and the outputs of our VC-S2E model under the two conditions. Notably, when comparing the clean speech spectrograms with those generated by our model, we observe that the speech generated with scenario-aware embeddings exhibits a clearer and more accurate representation of the target speech signal. This improvement is highlighted by the white-circled areas in the spectrograms, where the noise reduction is most evident. These observations are consistent with the quantitative results presented in Table II, where we report the performance of our model in terms of PESQ and STOI. Thus, the results from the spectrogram analysis and the objective evaluation confirm that our proposal achieves significant SE improvements, leveraging the complementary nature of visual scene information."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce a novel SAV-SE task that addresses speech enhancement by leveraging visual information from noisy environments. In contrast to existing works that rely on visual lip or face motions, our work is the first to enhance speech using rich contextual information from synchronized video as an auxiliary cue. Specifically, our proposed VC-S2E method integrates the Conformer and Mamba modules, using their distinct advantages to create a robust framework that distinguishes and mitigates environmental noise. Extensive experiments conducted on three different datasets demonstrate the superiority of VC-S2E over the other competitive methods, achieving the improved PESQ of 3.64, 3.48, 3.38, and STOI of 93.52%, 91.25%, 92.33% on MUSIC, AVSpeech and AudioSet, respectively."}]}