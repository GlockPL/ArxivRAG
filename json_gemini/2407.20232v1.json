{"title": "Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing", "authors": ["Ekaterina Iakovleva", "Fabio Pizzati", "Philip Torr", "St\u00e9phane Lathuili\u00e8re"], "abstract": "Text-based editing diffusion models exhibit limited performance when the user's input instruction is ambiguous. To solve this problem, we propose Specify ANd Edit (SANE), a zero-shot inference pipeline for diffusion-based editing systems. We use a large language model (LLM) to decompose the input instruction into specific instructions, i.e. well-defined interventions to apply to the input image to satisfy the user's request. We benefit from the LLM-derived instructions along the original one, thanks to a novel denoising guidance strategy specifically designed for the task. Our experiments with three baselines and on two datasets demonstrate the benefits of SANE in all setups. Moreover, our pipeline improves the interpretability of editing models, and boosts the output diversity. We also demonstrate that our approach can be applied to any edit, whether ambiguous or not.", "sections": [{"title": "1. Introduction", "content": "Recent advances in text-to-image generation have attracted a lot of attention in the research community and beyond it. This success is primarily due to development of text-conditioned diffusion models [10, 40, 45, 46, 48] which allow to translate textual concepts, described in natural language in the form of text prompts, into semantically coherent visualizations. Besides image synthesis, text-conditioned diffusion models have demonstrated strong performance on the image editing task [6, 21, 27, 31, 49, 56], where users describe in plain text the editing instructions to apply to input images.\nDespite the remarkable success of text-conditioned editing methods, in this work, we start from the observation that these approaches usually fail to succesfully edit images when the edit prompt provided by the user is ambiguous. To illustrate this, consider the example in Figure 1, which presents an editing task for a picture of a dog with the user instruction \"Make the dog cool.\" What does it mean for a dog to look cool? Is there only one way for a dog to appear cool? To answer these questions, the editing model requires a nuanced contextual understanding. The same cool adjective would suggest entirely different modifications if the subject were an inanimate object, like furniture, or a landscape. Moreover, there are multiple ways to make the dog look cool (e.g., adding glasses, making it squint, changing the surroundings), all of which are equally valid. To address the multimodal nature of this task, editing models need reasoning and abstraction capabilities that current editing diffusion models lack [16].\nTo address ambiguous input instructions, we propose our method Specify ANd Edit (SANE), which leverages the reasoning capacities and the general knowledge of Large Language Models (LLMs) [3, 5, 7, 36, 37]. More precisely,"}, {"title": "2. Related work", "content": "Diffusion-based Image Editing Diffusion models [22, 46, 50, 51] have achieved remarkable results in generative image modeling by representing the generative process as a series of denoising steps. Conditioning these models on text has enabled controllable text-to-image synthesis [40,45,46,48], as well as development of different diffusion-based editing systems [8, 11,31,35, 40, 42]. Some systems rely on image inversion technique [12, 51] to provide edited versions of an input scene [8, 29, 38, 39, 52, 54]. Although effective, these techniques are normally compute-intensive due to the inversion process. The seminal InstructPix2Pix [6] solved this probelm by finetuning a diffusion model on instruction prompts, benefiting from synthetic pairs of images for training. Although many built on this result [18, 27, 49, 55, 56], the impact of ambiguous instructions on instruction-based editing models is still not explored, motivating the proposed SANE.\nLarge Language Models. LLMs [3, 5, 7, 36, 37] are not only capable of human-like text completion, but are also successfully solving various reasoning tasks [1,3,4,7,9,47]. This is achieved by applying various reasoning and prompting techniques, e.g. In-Context Learning (ICL) [7], where the model is given a few task examples in the form of demonstration [13]. Another important direction of research is Visual-Language Models (VLMs) [2, 25, 32, 33, 43,44] which aim to connect visual and language spaces. In this work, we use multimodal GPT-40 [41] to caption original images, and to decompose ambiguous instructions into sets of specific instructions using ICL and captions. There are several editing systems relevant to SANE, which jointly finetune diffusion models and VLMs to enhance input instructions [16, 26]. While addressing a similar problem of commonsense reasoning for image editing, these models rely on implicit concept interpretations learned by VLMs. In contrast, SANE is zero-shot and relies on explicit concept decomposition performed by GPT-40.\nMulti-instruction Editing There are several works that investigate multi-instruction scenarios for text-to-image generation and editing [14, 16, 19, 26, 28, 34, 55]. In image editing, the diffusion model is given a pre-defined set of instructions to follow. Fol [19] extracts a region of interest for each of them, and restricts InstructPix2Pix [6] to remain within the union of these regions. Instead of editing an image in one step, EMILIE [28] applies InstructPix2Pix iteratively, one instruction at a time. To avoid artifacts, the authors propose to remain in the latent space of the diffusion model, and to decode only the last edited latent variable. CCA [20] builds a multi-agent system that takes a composite instruction as input, then splits it into elementary instructions and iteratively applies them using an LLM, a VLM, and several editing diffusion models. In this work, we also consider sets of instructions, however, SANE is the first to address instruction decomposition for ambiguous instructions."}, {"title": "3. Method", "content": "This section introduces the SANE pipeline. First, we present notations and preliminaries for diffusion-based image editing in Section 3.1. After that, we introduce our LLM-based instruction decomposition in Section 3.2, and our novel instruction combination strategy in Section 3.3."}, {"title": "3.1. Background on Diffusion-based Image Editing", "content": "We consider an instruction-based diffusion model such as InstructPix2Pix [6]. The purpose of such models is to edit an input image x while following a user-defined editing instruction c, in order to generate an edited image x. The edited image respects the input instruction while preserving the appearance of the original image. Following existing work [6, 27,56] we use a latent diffusion model [46], where the denoising operation is performed in the latent space of an autoencoder with encoder E and decoder D. These models typically include a denoising U-Net f\u03b8. At inference time, x\u0303 is produced by iterativly denoising a sampled Gaussian noise zT with f\u03b8 over T iterations. In other words, at step t, f\u03b8 is used to estimate the noise \u03f5t in a corresponding noisy latent zt. The latent zt is then updated to zt\u22121, by removing the estimated noise \u03f5t and reintroducing Gaussian noise with lower intensity, following strategies from literature [22, 30, 51]. This is repeated for each t\u2208[1,T], resulting in the final image x\u0303 = D(z0).\nIn instruction-based models, noise estimation is conditioned on the instruction c, which describes the desired changes, and on the input x, to enforce consistency with the original image. This noise is denoted as \u03f5t = f\u03b8(zt,E(x),c). In practice, instruction-based diffusion models benefit from classifier-free guidance [23] to boost consistency towards the instruction and the input image [6]. This means that for each t, zt is denoised using \u03f5\u0304t, rather than \u03f5t, with \u03f5\u0304t being a combination of three terms: unconditioned, conditioned on the image, and conditioned on the instruction c:\n$\\bar{\\epsilon}_{t}=\\alpha \\epsilon_{t}^{u} + \\alpha \\epsilon_{t}^{x} + \\alpha \\epsilon_{t}^{c},$ (1)\nwhere\n$\\epsilon_{t}^{u} = f_{\\theta}(z_{t}, \\emptyset , \\emptyset),$ (2)\n$\\epsilon_{t}^{x} = w^{I} \\cdot (f_{\\theta}(z_{t}, E(x), \\emptyset) - f_{\\theta}(z_{t}, \\emptyset , \\emptyset)),$\n$\\epsilon_{t}^{c} = w^{c} \\cdot (\\epsilon_{t} - f_{\\theta}(z_{t}, E(x), \\emptyset)).$\nIn Equation (2), \u00d8 indicates that the conditioning element is replaced with zeros, and wI and wc are guidance strength parameters, controlling the conditioning strength on x and c, respectively."}, {"title": "3.2. Instruction Specification", "content": "We noticed that directly applying ambiguous input instructions as c may lead to limited editing performance. This is due to the ambiguity of c which can be represented by multiple editing interventions. Let us assume that x represents a cat, while c= \u201cmake the cat look funny\u201d is a user instruction, as in Figure 2 (left). As mentioned in Section 1, we aim to map c with an LLM to a set of specific and interpretable instructions that would satisfy the user's request, e.g. map c to \u201cadd a hat to the cat\u201d.\nFormally, we want to extract from c a set of N editing instructions S={si}Ni=1, where each si is a specific instruction describing one modification consistent with c, to apply to the input scene. As shown in Figure 2 (left), we prompt an LLM to map c to N specific instructions, providing a rich caption of x as context. Our prompt contains general descriptions of ambiguous and specific modifications, and two in-context learning examples [7] of ambiguous input instructions associated with desired specific instructions. Furthermore, we set additional restrictions on the content and formatting style of the output instructions, to preserve image consistency and to simplify parsing. Due to limited space, we report the full prompt in the appendix. Notably, obtained specific instructions S are available to the user during model inference, providing insights on how the input instruction c is respected. This also allows SANE to explicitly show how c impacts the input image x, which increases interpretability of the image editing process."}, {"title": "3.3. Instructions Combination", "content": "After extracting decomposition S={si}Ni=1, we can use it to guide the image editing process. Our idea is to include si in the denoising procedure of f\u03b8 along the original ambiguous instruction c. Intuitively, this constrains the state of solutions satisfying the required editing c, allowing the model to focus on the selected specific interventions S. On the other hand, including c in the denoising process allows the diffusion model to synthesize complementary elements necessary for satisfying the user request, but not included in S. Hence, we propose a combination strategy that aggregate c and each si \u2208 S, balancing the influence of the specific instructions without losing consistency with c.\nWe start by conditioning the denoising process on each specific instruction, to isolate their effects. Hence, for each si \u2208 S we extract the estimated noise \u03f5i,t at timestep t:\n$\\epsilon_{i,t} = f_{\\theta}(z_{t}, E(x), s_{i}), \\forall i \\in [1,N].$ (3)\nThis results in the set of estimated noises {\u03f5i,t}Ni=1, one for each si \u2208 S. Next, we aim to combine this set of noise estimations into a single noise estimate that aggregates all specific instructions. Simple averaging would diminish the impact of specific instructions that affect particular regions by blending them with other noise estimates. Therefore, we propose an alternative aggregation scheme, assuming that each image region is predominantly affected by a single specific instruction. To identify the spatial locations where the instruction si most significantly impacts the diffusion process, we calculate the absolute difference between the estimated noises in the set and the noise obtained with conditioning only on the input image. Formally, it can be written as:\n$\\Delta \\epsilon_{i}^{t} = |\\epsilon_{i,t} - f_{\\theta}(z_{t}, E(x), \\emptyset)|.$ (4)\nWe then aggregate these in a mask Mt, capturing the index of the most significant element across i:\n$M_{t} = arg\\underset{i}{max} \\Delta \\epsilon_{i}^{t}.$ (5)\nFinally, to aggregate the noises \u03f5i,t based on the mask Mt, we use Mt to select the most significant estimated noise for each spatial location. The aggregated noise \u03f5ts can be computed as follows:\n$\\hat{\\epsilon}_{t} = \\sum_{i} I(M_{t}=i) \\epsilon_{i,t},$ (6)\nwhere I(Mt = i) is an indicator function that is 1 if Mt equals i, and 0 otherwise. Similarly to existing literature [34], we use classifier-free guidance [23] for instruction combination, redefining Equation (1) as:\n$\\hat{\\epsilon}_{t} = \\alpha \\epsilon_{t}^{u} + \\alpha \\epsilon_{t}^{x} + \\alpha \\epsilon_{t}^{c} + \\alpha \\epsilon_{t}^{s},$ (7)\nwhere the classifier-free guidance term for the specific instructions is given by:\n$\\epsilon_{t}^{s} = w^{s}(\\hat{\\epsilon}_{t} - f_{\\theta}(z_{t}, E(x), \\emptyset)).$ (8)\nThe process is shown in Figure 2, center. We apply this for every iteration t\u2208[1, T]."}, {"title": "4. Experiments", "content": "We test SANE by adapting the editing diffusion models trained in InstructPix2Pix [6] (IP2P), MB [56] (MB) and HQEdit [27]. We use the default inference hyperparameters for all methods: wC = 7,wI = 1.5. We select with a grid search ws = 7 for IP2P, ws = 5 for MB, and ws = 9 for HQEdit. We test with N = {1,2,3}, limiting the number of instructions due to increased computational time associated with higher N values. We incrementally build S for increasing N values, in such a way that S with lower N are subsets of those with higher N. We use 30 diffusion steps for image generation. Images are 512 \u00d7 512.\nDatasets We test SANE on two datasets. We first consider the global split of the EMU-Edit dataset, including 220 real images with ambiguous instructions satisfying our definition [49]. We also introduce a subset of 370 images and editing instructions from the IP2P dataset [6], following related works [19]. We call this set IP2P data. To focus our evaluation on ambiguous instructions, we manually classify the 370 samples. We process instructions in both datasets with GPT-40 to extract S.\nMetrics We evaluate the quality of edited images with three CLIP-based [44] metrics: input image preservation, editing strength and adherence to the edit. First, we use CLIPI to measure input image preservation as in [19], which is the CLIP space cosine similarity between x and x\u0303. This captures how similar is x\u0303 to the original x. Then, we measure the editing strehgth CLIPd as the cosine distance between the CLIP image embedding of x\u0303 and the text embedding of the final caption, following [44]. This assesses the fidelity of the edited image to the final caption. Finally, we use the directional similarity of [17], referred to as CLIP\u25b3, to measure adherence to the edit. For that, we first process each pair (x, c) with GPT-40 to obtain a short initial caption, describing the input scene, and a final caption, describing the desired scene after the editing. For instance, if the initial caption is \"a woman by the pool\" and c is \u201cmake her a robot\", a final caption may be \"a robot by the pool\". CLIP\u25b3 is evaluated as the cosine similarity between the difference of the CLIP image embeddings of x and x\u0303, and the difference of the text embeddings of the initial and final captions. This compares the change in the image to the change in the caption. In addition to these three metrics, we also use TIFA [24] to evaluate the effectiveness of si \u2208 S, as well as LPIPS [57] and DreamSim [15] to evaluate image diversity. Finally, we evaluate pairwise image preferences with GPT-40."}, {"title": "4.2. Editing Performance", "content": "Here, we compare against the state of the art. For each of the three editing diffusion models, we evaluate their performance with and without SANE applied on top of them. We evaluate models on EMU-Edit and IP2P data both quantitatively (Table 1) and qualitatively (Figure 3).\nCLIP Metrics We present results of CLIP-based metrics in Table 1a. Overall, we observe performance improvement across all metrics, models and datasets, advocating the advantages of SANE. Notably, performance increases with N, with models using N = 3 specific instructions performing best. In particular, we report the biggest improvements in IP2P, where we report for SANE/Baseline 0.1858/0.1203 on EMU-Edit for N = 3. This suggests that our instruction decomposition helps to follow the ambiguous instruction c. Moreover, we observe an increase in image consistency in MB and HQEdit, where we report improvements in CLIPi metric. We attribute this behavior to our decomposition strategy. Indeed, while general instructions c alone may lead to ambiguous edits impacting the entire scene, injecting si \u2208 S for inference guides the editing process on spatially-constrained edits. These still convey the desired modifications, as proved by the improvements in CLIPA and CLIPd. In IP2P, the Baseline reports the highest CLIPi. While it might seem as if Baseline outperforms all other methods, in reality we observe that IP2P may fail to perform any change to x when the input instruction is too ambiguous, thus artificially inflating the CLIPi metric. This is also confirmed by the lower performance in CLIPd and CLIP\u25b3.\nGPT Evaluation We use GPT-40 to measure pairwise preference for our SANE against the chosen baselines. We choose the configuration with N = 3, since it yields the best performance in Table 1a. For each image-instruction pair (x, c) on each dataset, we prompt GPT to choose the best edited image x\u0303 between one of the baselines and SANE. We ask to take into account the fidelity to the editing instruction, the quality and realism of the generated image, and the content preservation from the original picture while making the decision. The original image x is also provided for reference. The prompt is shown in the appendix. We report the average of GPT choices for all model pairs in Table 1b. As before, we significantly outperform all baselines. In particular, we beat the MB baseline on IP2P data (SANE/Baseline is 66.0%/34.0%). This is especially interesting, considering that it was the only setup where SANE was not outperforming the baseline CLIP\u25b3 in Table 1a, reporting 0.1998/0.2028. This result proves that we can benefit from the improved quality of the generated images, even when the fidelity to c is slightly penalized. We attribute this observation to the improved precision of our editing process.\nQualitative Results We show qualitative results in Figure 3. We use IP2P as a baseline model, and we add SANE on top of it with N = {1,2,3} instructions. We use the same hyperparameters and random seed for the baseline and SANE. SANE gradually adds details with the specific instructions to respect the ambiguous instruction. Interestingly, adding more specific instructions (higher N) brings advantages of removing editing artifacts. This is especially evident in the third row, where the last two specific instructions help to remove the background artifacts generated by IP2P. We believe such artifacts are generated by"}, {"title": "4.3. SANE Properties", "content": "In this section, we discuss additional properties of SANE, namely the impact of instructions in S (Table 2) and its effects on the variability of outputs (Figure 4). We use real images from EMU-Edit for all evaluations.\nEffects of Specific Instructions To prove that SANE is working correctly, we need to evaluate whether specific instructions are applied. As mentioned in Section 3.2, this would also enable interpretability of the editing process: at inference time, we can provide specific instructions S={si}Ni=1 to the user, to explain how the editing instruction c has been applied. For this reason we quantify how much the edited image x\u0303 respects each instruction in a reference set. We average, for each i and for N = {1,2,3}, CLIPd"}, {"title": "4.4. Ablation Studies", "content": "We focus our ablations on instruction combination strategies (Table 3), also providing insights on our design choices (Table 4) and on the effectiveness of SANE on different types of input instructions (Table 5).\nInstruction Combination We combine specific instructions as presented in Section 3.3. Here, we study the effectiveness of alternative solutions for instruction combination. We first propose a naive Prompt Concatenation baseline, where we concatenate the text of the instruction c with all si \u2208 S, using commas as separator. We then perform the editing using the obtained concatenated instruction. We also combine the effects of c and all si \u2208 S with Composable Diffusion [34]. We refer to the original paper [34] for details. For a fair comparison, we set wc and ws as weights for c and each si, respectively. We test with N = 3. From results in Table 3, we infer that our strategy for instruction combination outperforms other strategies. We explain this with the hierarchical nature of instructions: we preserve the"}, {"title": "5. Conclusions", "content": "We have introduced Specify ANd Edit (SANE), a zero-shot inference pipeline that improves the performance of diffusion-based text-to-image editing methods with ambiguous instructions. By using an LLM to decompose instructions into specific interventions, SANE enhances both interpretability and editing quality. Our experiments show consistent performance improvements and increased output diversity. Moreover, SANE is versatile, and it can benefit both ambiguous and clear editing tasks. In the future, we plan to address the limitations of SANE, such as the difficulty in handling a high number of specific instructions and the lack of guarantee that each specific instruction is actually applied (see appendix)."}]}