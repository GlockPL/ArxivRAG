{"title": "Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing", "authors": ["Ekaterina Iakovleva", "Fabio Pizzati", "Philip Torr", "St\u00e9phane Lathuili\u00e8re"], "abstract": "Text-based editing diffusion models exhibit limited performance when the user's input instruction is ambiguous. To solve this problem, we propose Specify ANd Edit (SANE), a zero-shot inference pipeline for diffusion-based editing systems. We use a large language model (LLM) to decompose the input instruction into specific instructions, i.e. well-defined interventions to apply to the input image to satisfy the user's request. We benefit from the LLM-derived instructions along the original one, thanks to a novel denoising guidance strategy specifically designed for the task. Our experiments with three baselines and on two datasets demonstrate the benefits of SANE in all setups. Moreover, our pipeline improves the interpretability of editing models, and boosts the output diversity. We also demonstrate that our approach can be applied to any edit, whether ambiguous or not. Our code is public at https://github.com/fabvio/SANE.", "sections": [{"title": "1. Introduction", "content": "Recent advances in text-to-image generation have attracted a lot of attention in the research community and beyond it. This success is primarily due to development of text-conditioned diffusion models [10, 40, 45, 46, 48] which allow to translate textual concepts, described in natural language in the form of text prompts, into semantically coherent visualizations. Besides image synthesis, text-conditioned diffusion models have demonstrated strong performance on the image editing task [6, 21, 27, 31, 49, 56], where users describe in plain text the editing instructions to apply to input images.\nDespite the remarkable success of text-conditioned editing methods, in this work, we start from the observation that these approaches usually fail to succesfully edit images when the edit prompt provided by the user is ambiguous. To illustrate this, consider the example in Figure 1, which presents an editing task for a picture of a dog with the user instruction \"Make the dog cool.\" What does it mean for a dog to look cool? Is there only one way for a dog to appear cool? To answer these questions, the editing model requires a nuanced contextual understanding. The same cool adjective would suggest entirely different modifications if the subject were an inanimate object, like furniture, or a landscape. Moreover, there are multiple ways to make the dog look cool (e.g., adding glasses, making it squint, changing the surroundings), all of which are equally valid. To address the multimodal nature of this task, editing models need reasoning and abstraction capabilities that current editing diffusion models lack [16].\nTo address ambiguous input instructions, we propose our method Specify ANd Edit (SANE), which leverages the reasoning capacities and the general knowledge of Large Language Models (LLMs) [3, 5, 7, 36, 37]. More precisely,\nSANE breaks down the input edit into a series of specific instructions that, when applied together, transform the input into well-defined editing tasks. Through this process, known as specification, we reverse the abstractions associated with input instructions. By incorporating various specific details, we effectively reduce the overall ambiguity of the input instructions. After that, we condition a pre-trained editing diffusion model using both the specific and original ambiguous instructions. More precisely, we start with inferring individual noise estimations for each specific instruction using the chosen diffusion model. These noise estimations are then combined using a novel strategy described in Section 3.3. Finally, the combined noise, along with the noise predicted from conditioning on the initial ambiguous instruction, is used in a modified classifier-free guidance [23]. This allows to preserve the fidelity to the original user indication, while guiding the process with the specific interventions. Among benefits on performance, this allows us to provide the specific instructions to the user at inference time, potentially raising the interpretability of the editing instruction. Notably, SANE can be applied to an arbitrary pre-trained instruction-based diffusion model in a zero-shot manner.\nIn short, our contributions are: (i) We propose the first editing method designed specifically to address ambiguous instructions. (ii) We introduce an LLM-based instruction decomposition pipeline, and a conditioning mechanism for editing diffusion models combining ambiguous and specific instructions, specifically designed for the task. Our approach requires no training. (iii) We perform extensive experiments on two datasets, with three state-of-the-art methods, outperforming all. We provide additional results on the properties of SANE, and ablations."}, {"title": "2. Related work", "content": "Diffusion-based Image Editing Diffusion models [22, 46, 50, 51] have achieved remarkable results in generative image modeling by representing the generative process as a series of denoising steps. Conditioning these models on text has enabled controllable text-to-image synthesis [40,45,46,48], as well as development of different diffusion-based editing systems [8, 11,31,35, 40, 42]. Some systems rely on image inversion technique [12, 51] to provide edited versions of an input scene [8, 29, 38, 39, 52, 54]. Although effective, these techniques are normally compute-intensive due to the inversion process. The seminal InstructPix2Pix [6] solved this probelm by finetuning a diffusion model on instruction prompts, benefiting from synthetic pairs of images for training. Although many built on this result [18, 27, 49, 55, 56], the impact of ambiguous instructions on instruction-based editing models is still not explored, motivating the proposed SANE.\nLarge Language Models. LLMs [3, 5, 7, 36, 37] are not only capable of human-like text completion, but are also successfully solving various reasoning tasks [1,3,4,7,9,47]. This is achieved by applying various reasoning and prompting techniques, e.g. In-Context Learning (ICL) [7], where the model is given a few task examples in the form of demonstration [13]. Another important direction of research is Visual-Language Models (VLMs) [2, 25, 32, 33, 43,44] which aim to connect visual and language spaces. In this work, we use multimodal GPT-40 [41] to caption original images, and to decompose ambiguous instructions into sets of specific instructions using ICL and captions. There are several editing systems relevant to SANE, which jointly finetune diffusion models and VLMs to enhance input instructions [16, 26]. While addressing a similar problem of commonsense reasoning for image editing, these models rely on implicit concept interpretations learned by VLMs. In contrast, SANE is zero-shot and relies on explicit concept decomposition performed by GPT-40.\nMulti-instruction Editing There are several works that investigate multi-instruction scenarios for text-to-image generation and editing [14, 16, 19, 26, 28, 34, 55]. In image editing, the diffusion model is given a pre-defined set of instructions to follow. Fol [19] extracts a region of interest for each of them, and restricts InstructPix2Pix [6] to remain within the union of these regions. Instead of editing an image in one step, EMILIE [28] applies InstructPix2Pix iteratively, one instruction at a time. To avoid artifacts, the authors propose to remain in the latent space of the diffusion model, and to decode only the last edited latent variable. CCA [20] builds a multi-agent system that takes a composite instruction as input, then splits it into elementary instructions and iteratively applies them using an LLM, a VLM, and several editing diffusion models. In this work, we also consider sets of instructions, however, SANE is the first to address instruction decomposition for ambiguous instructions."}, {"title": "3. Method", "content": "This section introduces the SANE pipeline. First, we present notations and preliminaries for diffusion-based image editing in Section 3.1. After that, we introduce our LLM-based instruction decomposition in Section 3.2, and our novel instruction combination strategy in Section 3.3."}, {"title": "3.1. Background on Diffusion-based Image Editing", "content": "We consider an instruction-based diffusion model such as InstructPix2Pix [6]. The purpose of such models is to edit an input image x while following a user-defined editing instruction c, in order to generate an edited image \\$\\tilde{x}\\$. The edited image respects the input instruction while preserving the appearance of the original image. Following existing work [6, 27,56] we use a latent diffusion model [46], where\nthe denoising operation is performed in the latent space of an autoencoder with encoder \\$\\mathcal{E}\\$ and decoder \\$\\mathcal{D}\\$. These models typically include a denoising U-Net \\$\\mathbf{f}\\_{\\theta}\\$. At inference time, \\$\\tilde{x}\\$ is produced by iterativly denoising a sampled Gaussian noise \\$z\\_T\\$ with \\$\\mathbf{f}\\_{\\theta}\\$ over T iterations. In other words, at step t, \\$\\mathbf{f}\\_{\\theta}\\$ is used to estimate the noise \\$\\epsilon\\_t\\$ in a corresponding noisy latent \\$z\\_t\\$. The latent \\$z\\_t\\$ is then updated to \\$z\\_{t-1}\\$, by removing the estimated noise \\$\\epsilon\\_t\\$ and reintroducing Gaussian noise with lower intensity, following strategies from literature [22, 30, 51]. This is repeated for each \\$t\\in [1,T]\\$, resulting in the final image \\$\\tilde{x} = \\mathcal{D}(z\\_0)\\$.\nIn instruction-based models, noise estimation is conditioned on the instruction c, which describes the desired changes, and on the input x, to enforce consistency with the original image. This noise is denoted as \\$\\epsilon\\_t = \\mathbf{f}\\_{\\theta}(z\\_t, \\mathcal{E}(x), c)\\$. In practice, instruction-based diffusion models benefit from classifier-free guidance [23] to boost consistency towards the instruction and the input image [6]. This means that for each t, \\$z\\_t\\$ is denoised using \\$\\tilde{\\epsilon}\\_t\\$, rather than \\$\\epsilon\\_t\\$, with \\$\\tilde{\\epsilon}\\_t\\$ being a combination of three terms: unconditioned, conditioned on the image, and conditioned on the instruction c:\n\\$\\tilde{\\epsilon}\\_t = \\epsilon\\_t^U + \\epsilon\\_t^I + \\epsilon\\_t^c\\$,\nwhere\n\\$\\epsilon\\_t^U = \\mathbf{f}\\_{\\theta}(z\\_t, \\varnothing, \\varnothing),\\$\n\\$\\epsilon\\_t^I = w^I \\cdot (\\mathbf{f}\\_{\\theta}(z\\_t, \\mathcal{E}(x), \\varnothing) - \\mathbf{f}\\_{\\theta}(z\\_t, \\varnothing, \\varnothing)),\\$\n\\$\\epsilon\\_t^c = w^c \\cdot (\\epsilon\\_t - \\mathbf{f}\\_{\\theta}(z\\_t, \\mathcal{E}(x), \\varnothing)).\\$\nIn Equation (2), \\$\\varnothing\\$ indicates that the conditioning element is replaced with zeros, and \\$w^I\\$ and \\$w^c\\$ are guidance strength parameters, controlling the conditioning strength on x and c, respectively."}, {"title": "3.2. Instruction Specification", "content": "We noticed that directly applying ambiguous input instructions as c may lead to limited editing performance. This is due to the ambiguity of c which can be represented by multiple editing interventions. Let us assume that x represents a cat, while c = \u201cmake the cat look funny\" is a user instruction, as in Figure 2 (left). As mentioned in Section 1, we aim to map c with an LLM to a set of specific and interpretable instructions that would satisfy the user's request, e.g. map c to \"add a hat to the cat\".\nFormally, we want to extract from c a set of N editing instructions \\$\\mathcal{S} = \\{s\\_i\\}\\_{i=1}^N\\$, where each \\$s\\_i\\$ is a specific instruction describing one modification consistent with c, to apply to the input scene. As shown in Figure 2 (left), we prompt an LLM to map c to N specific instructions, providing a rich caption of x as context. Our prompt contains general descriptions of ambiguous and specific modifications, and two in-context learning examples [7] of ambiguous input instructions associated with desired specific instructions. Furthermore, we set additional restrictions on the content and formatting style of the output instructions, to preserve image consistency and to simplify parsing. Due to limited space, we report the full prompt in the appendix. Notably, obtained specific instructions \\$\\mathcal{S}\\$ are available to the user during model inference, providing insights on how the input instruction c is respected. This also allows SANE to explicitly show how c impacts the input image \\$\\tilde{x}\\$, which increases interpretability of the image editing process."}, {"title": "3.3. Instructions Combination", "content": "After extracting decomposition \\$\\mathcal{S} = \\{s\\_i\\}\\_{i=1}^N\\$, we can use it to guide the image editing process. Our idea is to include \\$s\\_i\\$ in the denoising procedure of \\$\\mathbf{f}\\_{\\theta}\\$ along the original ambiguous instruction c. Intuitively, this constrains the state of solutions satisfying the required editing c, allowing the model to focus on the selected specific interventions \\$\\mathcal{S}\\$. On the other hand, including c in the denoising process allows the diffusion model to synthesize complementary elements necessary for satisfying the user request, but not included in \\$\\mathcal{S}\\$. Hence, we propose a combination strategy that aggregate c and each \\$s\\_i \\in \\mathcal{S}\\$, balancing the influence of the specific instructions without losing consistency with c.\nWe start by conditioning the denoising process on each specific instruction, to isolate their effects. Hence, for each \\$s\\_i \\in \\mathcal{S}\\$ we extract the estimated noise \\$\\epsilon\\_t^i\\$ at timestep t:\n\\$\\epsilon\\_t^i = \\mathbf{f}\\_{\\theta}(z\\_t, \\mathcal{E}(x), s\\_i), \\quad \\forall i \\in [1,N].\\$\nThis results in the set of estimated noises \\$\\lbrace \\epsilon\\_t^i \\rbrace\\_{i=1}^N\\$, one for each \\$s\\_i \\in \\mathcal{S}\\$. Next, we aim to combine this set of noise estimations into a single noise estimate that aggregates all specific instructions. Simple averaging would diminish the impact of specific instructions that affect particular regions by blending them with other noise estimates. Therefore, we propose an alternative aggregation scheme, assuming that each image region is predominantly affected by a single specific instruction. To identify the spatial locations where the instruction \\$s\\_i\\$ most significantly impacts the diffusion process, we calculate the absolute difference between the estimated noises in the set and the noise obtained with conditioning only on the input image. Formally, it can be written as:\n\\$\\Delta\\epsilon\\_t^i = |\\epsilon\\_t^i - \\mathbf{f}\\_{\\theta}(z\\_t, \\mathcal{E}(x), \\varnothing)|.\\$\nWe then aggregate these in a mask \\$M\\_t\\$, capturing the index of the most significant element across i:\n\\$M\\_t = \\arg\\max\\limits\\_i \\Delta\\epsilon\\_t^i\\$\nFinally, to aggregate the noises \\$\\epsilon\\_t^i\\$ based on the mask \\$M\\_t\\$, we use \\$M\\_t\\$ to select the most significant estimated noise for each spatial location. The aggregated noise \\$\\epsilon\\_t^S\\$ can be computed as follows:\n\\$\\epsilon\\_t^S = \\sum\\limits\\_{i=1}^{N} \\mathcal{I}(M\\_t = i) \\epsilon\\_t^i\\$,\nwhere \\$\\mathcal{I}(M\\_t = i)\\$ is an indicator function that is 1 if \\$M\\_t\\$ equals i, and 0 otherwise. Similarly to existing literature [34], we use classifier-free guidance [23] for instruction combination, redefining Equation (1) as:\n\\$\\tilde{\\epsilon}\\_t = \\epsilon\\_t^U + \\epsilon\\_t^I + \\epsilon\\_t^c + \\epsilon\\_t^S\\$,\nwhere the classifier-free guidance term for the specific instructions is given by:\n\\$\\epsilon\\_t^S = w^S (\\epsilon\\_t^S - \\mathbf{f}\\_{\\theta}(z\\_t, \\mathcal{E}(x), \\varnothing)).\\$\nThe process is shown in Figure 2, center. We apply this for every iteration \\$t\\in [1, T]\\$."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nBaselines We test SANE by adapting the editing diffusion models trained in InstructPix2Pix [6] (IP2P), MB [56] (MB) and HQEdit [27]. We use the default inference hyperparameters for all methods: \\$w^C = 7,w^I = 1.5\\$. We select with a grid search \\$w^S = 7\\$ for IP2P, \\$w^S = 5\\$ for MB, and \\$w^S = 9\\$ for HQEdit. We test with \\$N = \\{1,2,3\\}\\$, limiting the number of instructions due to increased computational time associated with higher N values. We incrementally build \\$\\mathcal{S}\\$ for increasing N values, in such a way that \\$\\mathcal{S}\\$ with lower N are subsets of those with higher N. We use 30 diffusion steps for image generation. Images are 512 \u00d7 512.\nDatasets We test SANE on two datasets. We first consider the global split of the EMU-Edit dataset, including 220 real images with ambiguous instructions satisfying our definition [49]. We also introduce a subset of 370 images and editing instructions from the IP2P dataset [6], following related works [19]. We call this set IP2P data. To focus our evaluation on ambiguous instructions, we manually classify the 370 samples. We process instructions in both datasets with GPT-40 to extract \\$\\mathcal{S}\\$.\nMetrics We evaluate the quality of edited images with three CLIP-based [44] metrics: input image preservation, editing strength and adherence to the edit. First, we use CLIPi to measure input image preservation as in [19], which is the CLIP space cosine similarity between x and \\$\\tilde{x}\\$. This captures how similar is \\$\\tilde{x}\\$ to the original x. Then, we measure the editing strehgth CLIPd as the cosine distance between the CLIP image embedding of \\$\\tilde{x}\\$ and the text embedding of the final caption, following [44]. This assesses the fidelity of the edited image to the final caption. Finally, we use the directional similarity of [17], referred to as CLIP\\$\\Delta\\$, to measure adherence to the edit. For that, we first process each pair (x, c) with GPT-40 to obtain a short initial caption, describing the input scene, and a final caption, describing the desired scene after the editing. For instance, if the initial caption is \"a woman by the pool\" and c is \u201cmake her a robot\u201d, a final caption may be \u201ca robot by the pool\u201d. CLIP\\$\\Delta\\$ is evaluated as the cosine similarity between the difference of the CLIP image embeddings of x and \\$\\tilde{x}\\$, and the difference of the text embeddings of the initial and final captions. This compares the change in the image to the change in the caption. In addition to these three metrics, we also use TIFA [24] to evaluate the effectiveness of \\$s\\_i \\in \\mathcal{S}\\$, as well as LPIPS [57] and DreamSim [15] to evaluate image diversity. Finally, we evaluate pairwise image preferences with GPT-40."}, {"title": "4.2. Editing Performance", "content": "Here, we compare against the state of the art. For each of the three editing diffusion models, we evaluate their performance with and without SANE applied on top of them. We evaluate models on EMU-Edit and IP2P data both quantitatively (Table 1) and qualitatively (Figure 3).\nCLIP Metrics We present results of CLIP-based metrics in Table 1a. Overall, we observe performance improvement across all metrics, models and datasets, advocating the advantages of SANE. Notably, performance increases with N, with models using N = 3 specific instructions performing best. In particular, we report the biggest improvements in IP2P, where we report for SANE/Baseline 0.1858/0.1203 on EMU-Edit for N = 3. This suggests that our instruction decomposition helps to follow the ambiguous instruction c. Moreover, we observe an increase in image consistency in MB and HQEdit, where we report improvements in CLIPi metric. We attribute this behavior to our decomposition strategy. Indeed, while general instructions c alone may lead to ambiguous edits impacting the entire scene, injecting \\$s\\_i \\in \\mathcal{S}\\$ for inference guides the editing process on spatially-constrained edits. These still convey the desired modifications, as proved by the improvements in CLIPd and CLIP\\$\\Delta\\$. In IP2P, the Baseline reports the highest CLIPi. While it might seem as if Baseline outperforms all other methods, in reality we observe that IP2P may fail to perform any change to x when the input instruction is too ambiguous, thus artificially inflating the CLIPi metric. This is also confirmed by the lower performance in CLIPd and CLIP\\$\\Delta\\$"}]}