{"title": "Recommending Pre-Trained Models for IoT Devices", "authors": ["Parth V. Patil", "Wenxin Jiang", "Huiyun Peng", "Daniel Lugo", "Kelechi G. Kalu", "Purdue University", "Josh LeBlanc", "Lawrence Smith", "Hyeonwoo Heo", "Nathanael Aou", "Purdue University", "James C. Davis", "Purdue University"], "abstract": "The availability of pre-trained models (PTMs) has enabled faster deployment of machine learning across applica- tions by reducing the need for extensive training. Techniques like quantization and distillation have further expanded PTM applicability to resource-constrained IoT hardware. Given the many PTM options for any given task, engineers often find it too costly to evaluate each model's suitability. Approaches such as LogME, LEEP, and ModelSpider help streamline model selection by estimating task relevance without exhaustive tuning. However, these methods largely leave hardware constraints as future work-a significant limitation in IoT settings. In this paper, we identify the limitations of current model recommendation ap- proaches regarding hardware constraints and introduce a novel, hardware-aware method for PTM selection. We also propose a research agenda to guide the development of effective, hardware- conscious model recommendation systems for IoT applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Many IoT applications today use deep neural networks (DNNs) for advanced functionality. Examples include object detection in autonomous vehicles [1], crop health monitoring in agriculture [2], and natural language processing for smart home assistants [3]. Given the cost of developing and training a DNN from scratch [4], engineers often leverage pre-trained models (PTMs) to expedite development and deployment pro- cesses. However, as highlighted by recent studies [5], selecting an appropriate PTM frequently requires manual evaluation of model performance on downstream tasks, a process that is time-intensive and prone to variability. Furthermore, a separate study [6] found that practitioners often encounter hardware constraints or lack the expertise needed to adapt DNN as the main hurdle. This underscores the value of a systematic approach to PTM recommendation, particularly one that addresses the resource limitations of IoT hardware.\nPTM selection extends beyond IoT, with prior work mak- ing significant strides in recommending PTMs for specific tasks without model fine-tuning. These methods fall into two categories: heuristic-based [7]\u2013[14] and learning-based [15]\u2013 [17], as detailed in II. However, their primary focus is maximizing model performance (e.g., accuracy), with limited attention to hardware constraints. Deploying PTMs on IoT devices introduces many such constraints, including limited CPU, memory, energy, and low-bandwidth communication. Adapting current recommendation approaches to address these IoT-specific limitations is nontrivial, as many of these methods rely on measures such as class similarity between source and target tasks or the model's capacity for distinguishing classes. Therefore, an effective IoT solution must incorporate both task suitability and hardware constraints, with potential further insights from modeling device energy consumption. We identify two critical gaps in the state-of-the-art PTM recommendation systems: (1) Lack of IoT-specific inputs for model recommendation (2) Lack of Ground-Truth rankings for IoT devices. Developing a hardware-aware recommender requires first establishing a ground truth model ranking on devices, which is essential for addressing the first gap.\nIn this work, we introduce approaches and research agendas to address these issues. First, we propose two modifications to the Model Spider framework [15] to enable hardware-aware recommendations, which we term Model Spider Fusion and Model Spider Shadow. Both approaches aim to address the first gap, each with distinct methods. Additionally, we outline methods for collecting raw data by defining essential metrics and generating custom rankings based on these hardware- specific performance indicators. We close by discussing future opportunities to support the ongoing development of hardware- conscious model recommendation systems.\nOur contributions are:\n1) We identify key gaps in PTM recommendation for IoT.\n2) We introduce methods to address these gaps, focusing on hardware-specific metrics and tweaks to existing methods.\n3) We outline a research agenda aimed at advancing PTM rec- ommendations with a focus on hardware and sustainability."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "This section covers background and related works on pre- trained model recommendation (\u00a7II-A) and model benchmark- ing in IoT systems (\u00a7II-B).\nA. Works for Model Recommendation\nResearchers have developed various methods to help soft- ware engineers identify the most suitable PTMs, aiming to minimize fine-tuning and forward passes to reduce compu- tational costs, as summarized in Table I. This section re- views state-of-the-art approaches for PTM recommendations in downstream tasks, categorizing them into two main types: heuristic-based and learning-based.\nHeuristic-Based Methods: These methods typically devise a novel heuristic or scoring system to rank a PTM's effective- ness for a target task. This category includes methods, such as NCE [7], and H-Score [8], which estimate the similarity between source and target labels directly. Additional methods, such as LEEP [9], N-LEEP [10], LogME [11], PACTran [12], GBC [13], and LFC [14], rely on a forward pass on the target dataset to capture representations. These representations are then analyzed for their usefulness based on target labels. However, heuristic approaches often struggle to capture the nuances of hardware specifications and their correlation with model performance, overlooking the practical challenges soft- ware engineers face when deploying models on hardware- constrained environments. For example, on IoT devices, larger models can leverage hardware-accelerated activation functions to boost performance, while smaller models with complex functions may still encounter bottlenecks.\nLearning-Based Methods: Recent methods, including Model Spider [15], EMMS [16], and Fennec [17], employ machine learning to eliminate the need for forward passes. These methods encode both the PTM and target dataset into a latent space, using learning techniques to discern patterns and predict performance. Given their promising results, we focus on learning-based techniques for our hardware-aware recommendation solutions, as this approach is well-suited for software engineers to encode complex hardware specifications, such as CPU type, architecture, memory size, and I/O speed. Model Spider tokenizes tasks and pre-trained models to generate recommendations that balance efficiency and accu- racy [15]. The Model Spider approach encodes each pre- trained model into a token @Om using an extractor \u03a8, which encapsulates essential characteristics such as architectures and parameters. Simultaneously, tasks are embedded as other to- kens \u03bc(T), reflecting dataset statistics and task characteristics. The key contribution of Model Spider is its use of a multi-head attention mechanism to assess the similarity sim(0m, \u03bc(T)) between model and task tokens. This similarity score serves as a reliable indicator of model performance, enabling a ranked selection of pre-trained models aligned with the task requirements. Optionally, it also allows for the application of a forward pass, leading to a refined token 0 that further captures data-specific attributes. This is done for top K-ranked models from the first iteration. Unlike EMMS [16] and Fennec [17], Model Spider's tokenized, attention-based design is more suitable to capture the nuances of hardware specifications."}, {"title": "III. MOTIVATION AND GAP ANALYSIS", "content": "While previous work has focused on model recommen- dations for downstream reuse, no studies or data currently support hardware-aware model selection. This section outlines key gaps in hardware awareness (\u00a7III-A) and the lack of ground truth (\u00a7III-B) in model recommender works.\nA. Lack of IoT specific inputs for model recommendation\nA key limitation of current model recommendation approaches is that they fail to consider the hardware per- formance implications while selecting models. For example, a recommendation system may rank models M1, M2, and M3, prioritizing M1 based on task performance. However, if M1 is a large model requiring extensive time-such as 10 minutes for a forward pass on certain IoT hardware-it may be impractical for real-time applications, making M2 the more effective choice by balancing accuracy with practical performance constraints. Furthermore, deep learning compilers like ONNX could also influence model rankings by optimizing specific models for certain hardware types [21], [22]. Never- theless, performance remains a major challenge for engineers when reusing PTMs, as highlighted by [23], and this issue is particularly critical in IoT contexts where resource limitations amplify its impact. As one participant in that study noted, \"Performance bugs are silent bugs. You will never know what happened until it goes to deployment.\" This emphasizes the need for a structured approach to assess a model's overall performance on IoT devices.\nTo further explore the identified research gaps, we propose the following research questions:\nRQ1: How can different parameters be weighted to tailor recommendations? Specifically, can the solution be tuned to prioritize speed, cost-efficiency, or energy efficiency?\nRQ2: How do model optimization techniques, such as quantization [24] and distillation [25], impact model rank- ings in the context of hardware-aware recommendations?\nB. Lack of Ground-Truth rankings for IoT devices\nA fundamental barrier to progress in the earlier gap is the lack of empirical data comparing the performance of different models across various IoT devices. This gap prevents researchers from finding patterns or correlations in model parameters that could provide a quick, reliable way to assess hardware compatibility. Previous benchmarking works [19], [20], only consider one model and do not compare or rank them. The lack of this data restricts our ability to develop predictive models to recognize patterns in rankings. Model Spider [15] includes a method to approximate ground truth by combining multiple approaches using Copeland's rank aggregation [26], [27]. However, since there are few heuristic ranking approaches in the hardware space, this strategy is currently infeasible. This lack of comprehensive performance data across the hardware landscape highlights the need for dedicated datasets and benchmarks to enable informed and hardware-aware model recommendations.\nThe following research questions will guide our analysis after data collection, aiming to uncover trends and correlations:\nRQ3: Can specific trends be identified between hardware specifications and model performance?\nRQ4: Is there a significant correlation between hardware resources and model characteristics, such as parameters, architecture, or problem type?"}, {"title": "IV. RESEARCH AGENDA", "content": "This section outlines a research agenda to enhance model recommendations for IoT applications. We introduce two approaches to make Model Spider hardware-aware (\u00a7IV-A) and propose methods for dataset creation (\u00a7IV-B), improv- ing adaptability across IoT environments. We then discuss extending our approach to broader deep learning systems and complex model reuse scenarios (\u00a7IV-C).\nA. Modifications to Model Spider\nTo address the first gap identified, we propose two ap- proaches to make the existing Model Spider framework hardware-aware. These build upon the existing framework, leveraging its robustness while enhancing its capability to account for hardware constraints with minimal modifications.\n1) Model Spider Fusion \u2013 Augmenting Task Tokens: As shown in Figure 2, in this approach we propose to introduce a separate Extractor \u03a8h which would encode the hardware specification for any given hardware. Its inputs comprise hardware model, CPU specifications, RAM size, Memory Size, etc. We would append these to the existing Task Tokens \u03bc(Td, Th). This modification enables the similarity block to learn correlations between model performance and the specific hardware in use, thereby enhancing the hardware-awareness.\n2) Model Spider Shadow New Hardware task: This approach builds on Model Spider's capability to recommend the best model for a downstream task, expanding it to include hardware requirements. By redefining a \"downstream task\" to encompass specific hardware, we replicate the framework with a hardware extractor \u03a8\u0127 that encodes hardware-specific fea- tures. This results in two ranking systems: a task-based selector for model suitability to tasks and a hardware-based selector for compatibility with hardware. We combine these rankings using Copeland's method [26], [27], yielding a balanced rec- ommendation that accounts for both task and hardware. This framework can be further extended to include energy-based or cost-based selectors, enabling tailored recommendations.\nB. Dataset Creation\nDefining Metrics: We propose to use metrics laid out in Sayeedi et al. [20], categorized into two groups. These groups can either be combined to establish the ground truth in Model Spider Fusion (\u00a7IV-A1) or used independently to rank models in Model Spider Shadow (\u00a7IV-A2).\na) Methodology to collect data: Additionally, to con- struct the ground truth data, we consider multiple PTMs and datasets. Each PTM is fine-tuned on all datasets. A robust methodology is then defined for collecting data from these (Fine-Tuned Model, Dataset) pairs, ensuring that experiments do not interfere with previous runs and that sensitive metrics, such as CPU temperature, are accurately reported.\nb) Creating rankings from collected data: We propose a tunable ranking system that adjusts to specific requirements, ranking models based on selected metrics such as best exe- cution time, best accuracy, or best energy consumption. To aggregate these metrics, we employ the weighted Copeland's rank-choice voting method. Each metric rank is assigned a weight wi in [0, 1], with \u2211wi = 1. The objective function (1) maximizes each model's combined score, incorporating both model performance and hardware efficiency [28], [29]. Here, f(a) denotes model performance for configuration a (e.g., based on accuracy), and HW\u00bf(a) represents the i-th hardware metric (e.g., execution time or power consumption). Each hardware metric is normalized by a threshold Ti and scaled by adjustable wi to reflect the relevant importance.\n\nC. Future Directions\nWe propose to extend the model recommender to a broader deep learning system (\u00a7IV-C1) and complex reuse (\u00a7IV-C2).\n1) Extending to Broader Deep Learning Systems: Our proposed approach would extend Model Spider, which handles only basic tasks (e.g., classification) on edge devices. However, hardware constraints are a crucial engineering consideration not only on edge devices but across a range of deep learning systems, including distributed learning frameworks (e.g., fed- erated learning [30]). Additionally, in real-world applications, the interaction between data collection and model prediction components must be addressed [31], [32]. We propose broad- ening our research to include DL systems, enhancing model reuse and adaptability across diverse environments.\n2) Extending to More Complex Model Reuse Scenarios: Our work, along with most prior ML research, has primarily addressed basic tasks, such as classification and regression. Although Model Spider has explored model reuse in gen- erative models (e.g., Large Language Model) [15], broader applications of DL model reuse remain largely unexplored. In computer vision, for instance, classification serves as a foundation for more complex tasks, like object detection and segmentation, which build upon classification models as backbones [33]. These downstream tasks require fine-tuning the model and adding task-specific heads or decoders for complex objectives. Expanding our approach to support such reuse scenarios is essential."}, {"title": "V. CONCLUSION", "content": "In conclusion, this paper identifies and addresses critical gaps in PTM recommendation for IoT by proposing hardware- aware enhancements to the Model Spider framework. This work establishes a foundation for further research in optimiz- ing PTM recommendations across diverse IoT environments."}]}