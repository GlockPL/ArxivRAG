{"title": "TORCHGT: A Holistic System for Large-scale Graph Transformer Training", "authors": ["Meng Zhang", "Jie Sun", "Qinghao Hu", "Peng Sun", "Zeke Wang", "Yonggang Wen", "Tianwei Zhang"], "abstract": "Graph Transformer is a new architecture that surpasses GNNs in graph learning. While there emerge inspiring algorithm advancements, their practical adoption is still limited, particularly on real-world graphs involving up to millions of nodes. We observe existing graph transformers fail on large-scale graphs mainly due to heavy computation, limited scalability and inferior model quality.\nMotivated by these observations, we propose TORCHGT, the first efficient, scalable, and accurate graph transformer training system. TORCHGT optimizes training at three different levels. At algorithm level, by harnessing the graph sparsity, TORCHGT introduces a Dual-interleaved Attention which is computation-efficient and accuracy-maintained. At runtime level, TORCHGT scales training across workers with a communication-light Cluster-aware Graph Parallelism. At kernel level, an Elastic Computation Reformation further optimizes the computation by reducing memory access latency in a dynamic way. Extensive experiments demonstrate that TORCHGT boosts training by up to 62.7x and supports graph sequence lengths of up to 1M.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph-structured data has long been prevalent and indispensable in many real-life applications such as social network construction and molecule analysis. Thus, there emerges a specific family of graph learning methods, namely graph neural networks (GNNs) [1]\u2013[3]. GNNs have gained giant breakthroughs and exhibit impressive performance in many tasks such as node classification [4]-[7] and link prediction [3], mainly due to their message passing mechanism [8], which models the inherent properties of graph structures. However, this module in classic GNNs also leads to commonly acknowledged over-smoothing [9], over-squashing [10], [11] and limited expressivity [12] issues.\nTo address these deficiencies, a latest approach called graph transformer shows more promising power in capturing the inter-dependencies among nodes. Graph transformer is built upon the classical Transformer [13] which allows nodes to attend to all other nodes, and integrates multiple graph structure encodings to include important graph properties. Due to the great modeling capability, graph transformer has garnered surging interest in recent years and a large number of models have been proposed [14]\u2013[17]. Existing graph transformers mainly operate by treating graph nodes as input tokens and constructing an input sequence consisting of all the graph nodes. Besides, graph structure encoders are designed as a graph adaptation of the original Transformer architecture. By integrating structural information, graph transformers exhibit competitive performance and outperform traditional message-passing GNNs (e.g., GCN [1] and GAT [3]) on both node classification [16]\u2013[21] and graph classification [14], [15], [22] tasks, as shown by Table I. We can obviously see graph transformers obtain the highest scores than GNNs on all tasks.\nReal-world graphs can easily involve millions of nodes [23], [24], making the sequence length enormously large. For example, in the current graph transformers' operation way, processing the citation graph dataset ogbn-papers100M from Open Graph Benchmark [23] (including more than 100 million nodes) requires high dimensional inputs with prohibited sequences. Moreover, as illustrated by the profiled results in \u00a7II-B, training graph transformers with long sequence is crucial for model quality and the development of versatile graph transformer application scenarios. However, we find most existing graph transformer research works [14], [15], [25], [26] are only limited to small graphs due to a lack of compatible systems tailored for the graph transformer model training with long sequences. More specifically, there are three deficiencies in current works:\nFirst, graph transformers with standard attention have poor scalability to long sequences, due to the computation and memory complexity of $O(N^2)$, quadratic on the number of nodes (N) in a graph [14], [15], [27]\u2013[30]. Taking fully-connected attention as graph foundation encoders captures the implicit all-pair influence beyond neighboring nodes, but also limits existing graph transformers only on small-graph applications [14], [15], [27]\u2013[30]. Figure 2 demonstrates that even with a state-of-the-art attention library, i.e., FlashAttention [31], the computation of the dense attention mechanism is still a bottleneck during the graph transformer training.\nSecond, current algorithms either compromise model quality or are only applicable to a single graph learning task. To reduce computation pressure, some graph transformers shorten the input sequences by harnessing neighbor sampling [16], [32] similar adopted in classic GNNs [2], [33], [34]. Others like [25] attempt to overcome the quadratic complexity by replacing full attention with approximate attention methods. However, using sampling methods or simply adopting sparse patterns like [35], [36] loses critical connectivity information and thus sacrifices model precision. On the other hand, some works [17], [20], [21] use self-defined adapted attention modules to reduce memory consumption, but are limited to a specific task, e.g., node classification. They are neither general to versatile graph learning tasks nor portable to be scaled in large-scale training.\nThird, no existing works exploit systematic optimizations to realize efficient and scalable training. Several graph transformers [25], [26] apply graph structure to relieve the computation burden. However, this sparse pattern is highly irregular in memory access due to the skewed property of graphs, which is challenging for optimizing the system throughput. Moreover, with large datasets, the memory consumption of model activations grows rapidly, necessitating a scalable system design and memory optimization. But existing works [14]\u2013[16], [25], [26] only focus on the implementation of graph transformers in a single GPU, thus limited to very small graphs. Although there has been a breakthrough for large language models (LLMs) by partitioning along the input sequence dimension and training long sequences across devices [37]\u2013[40], those sequence parallelism ways cannot be directly transplanted on graph transformers due to the extra graph encodings and neglection of structure properties.\nTo bridge these gaps, we design TORCHGT, the first distributed training system that scales graph transformer model to large graphs with billions of edges. Our system abides four design goals: scalable, efficient, convergence-maintained, and task-agnostic. Existing graph transformer works neither facilitates efficiency by well-designed parallelism from the system perspective nor propose scalable algorithms for universal graph learning tasks, thus making it challenging to meet those goals. This hinders the practical development of advanced graph transformer models on real-world graphs. The core design of TORCHGT derives from the following three key insights.\nFirst, the learning of graph transformers highly benefits from graph structures. Specifically, the structure of many real-world graphs is highly sparse [41]\u2013[43], which reflects the inherent vertex-vertex interactions. This sparsity could be a guide for how graph transformers attend to nodes to reduce computation costs while maintaining correct connections. In addition, considering the structural property in the system design also contributes to optimal hardware throughput. Second, the order of input graph tokens is alterable. Unlike inputs in famous LLMs like GPT [44] whose token order is crucial for model understanding and generation process [44], [45], graph transformers focus more on connections between nodes. Thus, we can modify the input arrangement to exploit graph properties (e.g., local clusters) for more specialized optimizations.\nThird, the block-sparse format is a good match for irregular graph clusters. Block-sparse formats store data contiguously in memory, reducing storage overheads and memory access. But directly exploiting it on dense attention matrices will drop connectivity and result in substantial accuracy loss [46], [47]. However, by integrating it into our specialized clustered pattern, we find the computation can be further accelerated while maintaining model accuracy.\nAs such, our key idea is to design an accuracy-maintained and compute-efficient system from both algorithm and system perspectives to support large-scale graph transformer training. Specifically, TORCHGT consists of three key innovations. Dual-interleaved Attention is a local-global interleaved attention that integrates graph structural topology into the attention module and selectively combines the global information into the attention with the graph structure search, which efficiently speeds up the attention computation while maintaining the models' qualities. Cluster-aware Graph Parallelism splits the input graph tokens according to the cluster nature of graphs, thus boosting the attention computation throughput and facilitating system scalability. It also allows us to take advantage of the cluster feature in more fine-grained kernel optimizations. Inspired by the block-sparse format, Elastic Computation Reformation dynamically transfers the clustered attention pattern into a specialized cluster-sparse format to reduce the irregular memory access latency. It includes an Auto Tuner to automatically control the transfer to maintain the model convergence. Through extensive experiments, we show TORCHGT successfully achieves scalable and efficient graph transformer training on large graphs. It also boosts training by up to 62.7\u00d7 across various graph learning tasks while maintaining accuracy.\nIn summary, we make the following contributions:\n* TORCHGT is the first graph transformer system that facilitates efficient, scalable, and accurate training on large-scale graphs as well as universal graph learning tasks.\n* TORCHGT is the first to identify the major challenges that hinder existing graph transformers from scaling to large graphs and explore the graph-specific optimization opportunities which are neglected previously.\n* We propose three key techniques to meet all design goals from algorithm and system co-design perspectives.\n* Experiments show TORCHGT achieves up to 62.7\u00d7 speedup and near-linear scalability, supporting graph sequence lengths of up to millions."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "A. Graph Transformer\nGraph transformer architecture has attracted surging attention in graph representation learning in recent years [48]. Current representative graph transformers integrate graph structural encodings into the input and attention map in the Transformer architecture. The input sequence is built by tokens generated with graph attributes. Specifically, some works [14]\u2013[16], [18], [49]\u2013[51] calculate node positional encodings beforehand and add them to the inputs before the attention module. Other works [14], [15], [17], [32], [52] add graph structural information into the self-attention matrix as bias. Several works [19], [25], [26] combine message-passing GNNs and the attention mechanism together. Here we only focus on the former two types of graph transformers since they are currently most representative.\nA basic Transformer consists of multi-head attention (MHA) and feed-forward network (FFN) which contains two linear layers. Given an input sequence $H = [h_1,\u2026,h_s]^T \u2208 R^{S\u00d7d}$ where S is the sequence length and d is the hidden dimension, MHA first projects its input H to three subspaces: Q, K and V with projection weight matrices $W_Q \u2208 R^{d\u00d7d_k},W_k \u2208 R^{d\u00d7d_K}, W_v \u2208 R^{d\u00d7d_v}$. The MHA output is calculated as:\n$H' = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V$ (1)\nwhere $d_K$ is the second dimension of K. MHA captures the pair-wise similarity of input tokens in the sequence.\nFor a graph $G = (V, E)$ with nodes $V = {v_1,\u2026,v_N}$ and edges $E = {e_1,\u2026,e_{|E|}}$, here we list the formulation of Graphormer [14] as an example:\n$A_{ij} = (h_i^{(0)} W_Q)^T (h_j^{(0)} W_K) / \\sqrt{d_k} + bias(v_i,v_j)$ (2)\n$h_i^{(0)} = x_i + z_{deg^-(v_i)} + z_{deg^+(v_i)}$ (3)\nwhere $h_i^{(0)}$ is the beginning attribute of node i, $x_i$ is the node feature, and $z^-,z^+ \u2208 R^d$ are learnable embeddings specified by the in-degree $deg^-(v_i)$ and out-degree $deg^+(v_i)$. The encodings in Equation 2 allow the attention to capture the node importance. $A_{ij}$ is the (i, j)-element of Query-Key product matrix, namely the attention coefficient. bias is a learnable scalar shared across all layers, and 4 (vi, vj) is the distance of the shortest path (SPD) between node vi and vj, which is the shortest hops that vi needs to pass to reach vj.\nB. Long Sequence for Graph Transformers\nFor better illustration, we categorize current graph learning tasks into two types to discuss the need of training in long sequences: (1) graph-level task, and (2) node-level task.\nLong Sequence for Graph-level Tasks. For such tasks, the input sequences represent a set of graphs while the output is a set of labels representing the types of corresponding graphs. When processed by graph transformers, all nodes in each input graph need to be encoded as input tokens and are concatenated into an input sequence. As such, the length of each sequence equals the number of nodes in each graph. In this task, if the graph size, i.e., the number of nodes, grows very large, the input sequence can be too long to be trained by current methods. For instance, the MalNet [53] dataset contains graphs with up to 552K nodes.\nLong Sequence for Node-level Tasks. These tasks classify each node in an input graph with a specific label. In the node classification task, the input sequences can either encode all nodes in the graph or a mini-batch of nodes. For the former case, the input sequence can be enormously long for large-scale graphs, which is not supported by most models. For the latter, with a larger batch size, both the training throughput and the trained model quality can be improved. Both cases validate the necessity and advantages of long sequence training.\nHowever, existing graph transformers have some inherent constraints in performing the above tasks. While graph-level scenario has been explored in [14], [15], existing endeavors do not generalize to large-scale graphs endemic to node-level prediction. Our TORCHGT strives to include both tasks by joint algorithm-system design. The scale of graphs applicable to current models is still limited, thus leaving long sequence training still an urgent necessity. Besides, training large-scale graphs in short sequence suffers from lower training throughput, downgraded model quality and limited graph transformer applications. Figure 1 illustrates the impact of sequence length on the test accuracy of two representative models Graphormer [14] and NodeFormer [17] on two datasets. Both models show superior performance on longer sequences. On the AMiner-CS dataset, Graphormer with a 4K sequence length improves the test accuracy by up to 0.9% compared to the short sequences. On the Pokec dataset, sampling-based NodeFormer with 100K sequence length outperforms the case with 10K sequence length by a staggering 12% accuracy. These results necessitate the need for long sequence training of graph transformers.\nC. Issues and Opportunities\nMost existing graph transformer works [14], [15], [19], [25], [26], [32] are only limited on small graphs due to a lack of compatible systems tailored for the graph transformer model training with long sequences. They have three main issues when applied to long sequence training.\n11: Attention Computation Bottleneck. Graph transformers with standard (dense) attention treat the graph as fully-connected with the MHA mechanism calculating attention for all node pairs. Thus, it requires the computation complexity of the attention module to be quadratic on the number of nodes ($N^2$) in a graph, which limits the models' scalability to extremely long sequences. Currently, there is a breakthrough in standard attention optimization, i.e., FlashAttention [31]. FlashAttention accelerates the attention module by fusing the IO-bound GPU kernels like Softmax and Dropout within the attention computation. However, even with FlashAttention to train graph transformers with long sequences, e.g., sequence length of 512K, we still identify that the attention module dominates the overall training time.\nTo show this, we conduct an experiment to record the iteration time breakdown when using FlashAttention, as illustrated in Figure 2. Current FlashAttention does not support the modified attention module like those augmented with bias encodings [14], [15], [17], [32], so we disable the bias in this experiment to only examine the computation efficiency. We separate the computation time of FlashAttention from the comprehensive training iteration. We can obviously observe that no matter on longer or shorter sequences, attention computation still dominates over 80% of training time, indicating a severe attention bottleneck. However, both the standard attention and FlashAttention fail to leverage one important characteristic of the graph, namely its topology structure, which we find profoundly impacts the effectiveness of system optimizations. 12: Degraded Model Convergence and Limited Tasks. Many efforts [17], [25], [35], [54] have been made to overcome the computation bottleneck of the attention module. Among them, [54] prunes the attention module and leaves a major backbone to reduce the computation cost for LLMs. Some works [35], [36], [55], [56] propose sparse patterns on attention to scale linearly, but most of them are designed for natural language processing (NLP). They cannot be simply grafted to graph transformers since they fail to consider the inherent graph structure information when approximating attention, thus resulting in subpar model performance. Several graph transformers [16], [32], [57] harness neighbor sampling or graph pooling that only selects a subset of nodes to be trained at each iteration, without reducing the computation complexity. Nonetheless, all the above methods sacrifice model precision by dropping the connectivity information.\nIn the graph domain, efficient attention is not well studied. Few graph transformers like [26] apply the graph structure to attend nodes and maintain graph-specific information. However, they limit the implementation to the GNN-encoding-based model architecture, e.g., GraphGPS [25], and highly rely on the message-passing scheme for excellent model performance. Other methods [17], [20], [21] use self-defined adapted attention modules to achieve linear complexity. However, all those works are constrained to a single application task, failing to generalize to versatile graph tasks. Additionally, with GNN structure encodings or self-defined attention, the model can hardly be scaled to multiple workers.\n13: Lack of Specific System Optimizations. As far as we know, currently there is no existing framework to optimize graph transformer training from the system level. FFN operations in MHA are dense in computation and regular in memory access. However, utilizing graphs on the attention module is sparse in computation and requires irregular and fine-grained memory access due to the skewed nature of graph structures, which inevitably becomes the performance barrier.\nExisting solutions [15], [25], [26] directly apply graph topology in the attention computation while ignoring the pattern differences between graph transformer and standard Transformer-based models. To better illustrate, we experimentally examine the impact of irregular memory access by the topology-pattern attention in Table II. The topology-induced memory access latency is tremendous, reaching up to 33.2\u00d7 slowdown than dense computation. To increase models' scalability, recent works [37]\u2013[40] split the input sequences and distribute the computation across devices. However, this parallelism neglects various graph encoding modules and fails to distinguish input tokens in graph domain from tokens in NLP. Those differences invoke specialized and dedicated system designs for graph transformers towards more efficient memory optimizations and more aggressive parallelism."}, {"title": "III. TORCHGT DESIGN", "content": "We propose TORCHGT, an algorithm-system co-optimized system tailored for graph transformer training on large-scale graphs. It follows four design principles:\n* Scalable. TORCHGT can scale graph transformer training to extremely large graphs (solving 13).\n* Efficient. TORCHGT reduces over 90% computation required by standard attention, overcoming the attention computation bottleneck (solving 11).\n* Convergence-maintained. TORCHGT maintains comparable model convergence to the graph transformer with standard attention, and successfully balances the trade-off between training efficiency and model quality (solving 12).\n* Task-agnostic. TORCHGT generalizes to various graph transformer models and graph learning tasks (graph-level and node-level) (solving 12).\nA. System Overview\nMotivated by all the observations in \u00a7II-C, our key idea is to co-design an accuracy-maintained and compute-efficient attention module with a graph-parallelism-enabled system framework to support long sequence training. As shown in Figure 3, TORCHGT intelligently optimizes training across three levels from the top to bottom hierarchy: algorithm, runtime and kernel. We propose a topology-induced and accurate attention algorithm in the algorithm level. We present a novel cluster-aware graph parallelism to scale the training in the runtime level. In the kernel level, we design a memory-optimized computation pattern specialized for clustered attention. Specifically, TORCHGT consists of three key modules:\n* Dual-interleaved Attention: In the algorithm level, it integrates locally graph-induced topology into the attention computation pattern and periodically overlays it with the fully-connected information, which efficiently reduces the computation burden while maintaining the model's quality as much as possible. It is tailored for versatile graph transformer models with local-global interleaved attention.\n* Cluster-aware Graph Parallelism: From the distributed runtime perspective, we design a cluster-aware graph parallelism tailored for graph transformers. It splits the graph tokens in sequences according to the clustering nature of graphs, thus computing attention with locality and facilitating the system scalability.\n* Elastic Computation Reformation: It reformats the graph-induced pattern obtained at the runtime level into our customized and fine-grained cluster-sparse pattern at the underlying execution kernel level. It further improves the attention computation throughput by greatly alleviating irregular memory access. To balance the efficiency-quality trade-off, we build an Auto Tuner to make an elastic transfer of cluster sparsity.\nB. Dual-interleaved Attention\nMotivated by I1 in \u00a7II-C, TORCHGT explores the opportunity of integrating graph structure to reduce the substantial computation cost. For optimizations aiming at graph transformers, we design an interleaved attention to realize a local-global interleaved attention and ensure model convergence.\nLocal Topology-induced Pattern. In NLP tasks, the tokens in a sequence represent words, while in graph transformers the tokens are nodes of the input graph. Besides, most graph transformers like [14], [17] adopt the standard attention, which can be viewed as a fully-connected graph since all tokens attend to every other token to perform inner products, leading to quadratic complexity. Motivated by the sparse attention methods [35], [36], we find the local topology-induced pattern that makes use of the underlying structure of the input graph is desirable to guide the pair-wise node interactions. Graphs innately own two desiderata for attention mechanism: (1) small pair-wise node interactions (large sparsity), and (2) data locality. In addition, most sparse patterns in NLP are only approximations [36] to their dense counterparts under specific contexts, while in our scenario the graph structure is real and valid, without the need of approximations. Thus, we compute attention by attending each node to its immediate neighbors in the graph, reducing the interacted node pairs.\nWe formulate the local topology-induced pattern as below. To train on a graph G = (V, E), we generate an input sequence $S\u2208 R^{S\u00d7d}$ comprised of graph tokens corresponding to a node set $V\u2208 V$. V can be equal to either the whole nodes V, e.g., in graph-level tasks, or a subset of V, e.g., in node-level tasks if the node number is too large. For each node set \u00d1, we construct a local attention graph G = (V, \u1ebc), where the edge set E is also a subset of the original edge set E. If there exists a global token in the model that attends to all nodes in the graph and is attended to by all nodes, we augment \u1ebc with the global token's edges. The general attention coefficient \u00c4ij of graph transformers without graph encodings is computed in: $A_{ij} = softmax(\\frac{(h_iW_Q)\u00b7(h_jW_K)}{\\sqrt{d_K}})$\nThe updated node attribute hi for each node i is computed as the weighted sum of the features of its neighboring nodes from V: $h' = \u2211_{j\u2208N(i)} A_{ij} (h_jW_v)$, where N(i) denotes the set of neighboring nodes of node i. Each neighbor's feature vector hj is weighted by the attention coefficient $A_{ij}$, and these weighted features are summed to update the attribute of node i. By using our local topology-induced pattern G, the attention only computes coefficients of connected node pairs.\nInterleave Fully-connected Pattern. Implementing the attention computation via the graph structure can greatly reduce the computation cost. However, it sometimes slows the model accuracy and convergence, which can be shown by experiment results in Figure 10. The local graph-induced attention slightly degrades the model convergence, which is mainly because the topology-induced pattern restricts the attention mechanism from extracting the high-order neighboring information. Intuitively, larger sparsity induces more absence in the attention computation, and increases the model error. Building on this, we empirically interleave a fully-connected attention on the local graph-induced attention.\nTo fill the performance gap between sparse attention and its dense counterpart, we need to figure out when to interleave the dense pattern. Motivated by the sparse attention theories in [58], we conclude three critical conditions under which we use the topology-induced pattern on attention:\n* C1: Every node in the sequence S always attends to itself.\n* C2: There exists a Hamiltonian path that directly connects all nodes V in the sequence.\n* C3: All nodes in the sequence should be able to attend to other nodes, either directly or indirectly after L graph transformer attention layers.\nThe Hamiltonian path [59] or traceable path is a path in a graph that visits each node exactly once. For each graph \u011e corresponding to the input sequence, the Dual-interleaved Attention module searches it across the above three conditions. We use a heuristic approach Dirac's theorem [60] to do quick checks so the overhead is negligible in epoch time. If it satisfies these conditions, we perform attention computation with the topology-induced sparse pattern. Otherwise, TORCHGT heuristically determines the current sparse pattern may introduce more errors and we utilize the fully-connected attention mechanism in this case to ensure model quality.\nComputation & Memory Complexity. The topology of many real-world graphs can be immensely sparse. For instance, the ogbn-arxiv graph has 169K nodes and 1.2M edges, resulting in a sparsity of 4.1\u00d710-5 (the proportion of nonzero elements in the whole adjacency matrix). As a result, the local topology-induced attention significantly reduces the computation and memory-access complexity from $O(N^2)$ to $O(E)$. Though we interleave several fully-connected attention occasionally, the overall computation efficiency is still improved significantly.\nModel Convergence. Graph-centric attention [26] and classical GNNs [2], [3] prove that sparse attention can maintain the model convergence comparable to its dense counterpart. [18], [58] propose sparse attention can obtain similar universality as dense attention under some assumptions. Borrowing it to TORCHGT, our Dual-interleaved Attention can provide universal approximation properties that every continuous function f can be approximated to any desired accuracy using a suitable sparse pattern under the three conditions, thus obtaining convergence similar to dense counterparts.\nC. Cluster-aware Graph Parallelism\nTo better fit the topology-induced attention pattern and increase the system scalability, we introduce a graph transformer-specialized parallel training style: Cluster-aware Graph Parallelism, which exploits the graph cluster characteristics to guide the distributed training.\nUtilization of Graph Cluster. Graph cluster (community) [61], [62] is one essential characteristic of real-world graphs, referring to a subset of nodes within a graph that exhibit a higher degree of connectivity with each other compared to nodes in other parts of the graph. Although the graph structure-based attention in \u00a7III-B greatly reduces the computation, this sparse and highly-skewed nature of graphs triggers substantial irregular memory access since edge connections are distributed in an uneven pattern, bringing extra overhead to training. Consequently, employing the graph cluster structure on GPUs is promising for graph transformer training improvement. There exist some approaches in traditional graph learning [62], [63] to utilize graph cluster, but they are aimed for CPU processing with limited parallelization. [64] also exploits graph cluster but focuses on redundant data loading in GNN computing.\nTherefore, to explore the performance benefits of graph cluster on graph attention computing, we incorporate a lightweight node reordering to cluster nodes and improve the spatial locality during attention computation, without changing the connectivity correctness. The key insight is that the proximity of node IDs is more likely to be scheduled to the adjacency of computing units on GPUs where they get processed. In detail, we leverage METIS [65], a community-based graph reordering technique for great cluster locality and ease of integration with parallelism. Specifically, it uses multilevel recursive bipartitioning to divide and coarsen the graph while preserving the essential structure. We optimize the implementation of METIS for a lower cost: we capture the cluster information of graphs and map such locality from the upper level to the underlying GPU kernels, which also enables us to leverage the L1 & L2 cache for refined cluster capturing (later discussed in \u00a7III-D).\nSpecialized Graph Parallelism. To increase the scalability of graph transformers, intuitively TORCHGT employs parallelism technologies to dispense the computation across devices. There have been extensive studies in sequence parallelism technologies [37]\u2013[39] for LLMs to support efficient long sequence training. However, current parallelism methods for language models trigger two challenges when applied to graph transformers: (1) failing to leverage graph properties; (2) not supporting various graph encodings. In traditional language models, the input sequence encodes the context of a specific sentence. As such, training the language model requires tokens in the input sequence concatenated in a pre-defined order. In contrast, we observe that for graph learning tasks, there is no need for graph transformers to predict sequences (graph-level task) or tokens (node-level task) within a position-fixed context, since they only rely on the graph topology to construct the structural encodings. A motivating example of parallelizing graphs with graph cluster is the graph-level task, where only the global token is critical for inferring the graph type and other node tokens can be arranged in any order.\nBased on this insight, we are the first to design a Cluster-aware Graph Parallelism specialized for graph transformers, as shown in Figure 4. Specifically, the raw input sequences S and graph encodings B are randomly partitioned across P devices. Each local sub-sequence $S_{sub}$ and sub-encodings are projected to local matrices: $Q_{sub}, K_{sub}, V_{sub}, B_{sub} \u2208 R^{S\u00d7d}$, assuming they have the same dimensionality. Then in each graph transformer layer, all subspaces are combined together into complete matrices Q, K, V, and B via the highly efficient all-to-all collective communication operation. All-to-all operation owns an advantage over other communication operations (e.g., all-gather and reduce-scatter) in terms of much smaller communication volume and overall better scalability, which is also proved in [38]. All-to-all gathers matrices in sequence dimension and splits in the head, resulting in $Q,K,V, and B \u2208 R^{S\u00d7\u20a4}$. Now that since matrices are complete in the sequence dimension, TORCHGT reorganizes the layout according to the clustering nature of graphs discussed before. Then the Dual-interleaved Attention conducts attention computation in the clustered layout, exemplified as blue, orange and green rectangles in Figure 4. After attention computation, another all-to-all transforms the output tensor back to subspace $S_{sub}$ for subsequent operators such as FFN and layer normalization in the graph transformer layer.\nCommunication Complexity. Thanks to all-to-all, Cluster-aware Graph Parallelism has low communication volume and scales exactly well with more servers. Given hidden size d, sequence length S, and parallelism degree P, TORCHGT performs all-to-all with a total message size 3Sd before the attention computation, and another all-to-all for attention output with size Sd. Therefore, TORCHGT performs two all-to-alls with communication volume per GPU of 4Sd/P and communication complexity of O(S/P), while other operations like all-gather have communication complexity of O(S). Thus, TORCHGT has better scalability with increasing parallelism degree on extremely long sequences.\nIn summary, compared with sequence parallelism methods for LLMs, our Cluster-aware Graph Parallelism favors the graph transformer architecture in several aspects. First, all-to-all gathers in sequence dimension, leading to exactly integrated graph topology, which the topology-induced sparse pattern in \u00a7III-B can be perfectly applied to. Second, the graph encodings B share the same sparse layout as attention mapping so the parallelism of graph transformers only brings a trivial memory footprint and communication overhead, thus facilitating model scalability and ensuring memory efficiency."}, {"title": "D. Elastic Computation Reformation", "content": "Cluster Sparsity. The topology-induced attention pattern can significantly reduce the computation cost", "36": [46], "47": "we reformat the clustered layout in Figure 5(b) to a fine-grained cluster-level fashion in Figure 5(c). Specifically", "strategies": "n* Indolent Transferring. TORCHGT only transfers clusters that are extremely sparse and irregular. Although such an inactive way may miss some optimization opportunities", "as": "n$k = \\sqrt{Q_{L2}/d} \u2208 N$, where $Q_{L2}$ is the L2 cache size and d is model hidden dimension. To determine the sub-block dimension do, we profile the computation throughput and some hardware statistics of the indexing kernel w.r.t. different do values. Figure 6(a) shows the workload balance in"}]}