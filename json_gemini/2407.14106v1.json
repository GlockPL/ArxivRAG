{"title": "TORCHGT: A Holistic System for Large-scale Graph Transformer Training", "authors": ["Meng Zhang", "Jie Sun", "Qinghao Hu", "Peng Sun", "Zeke Wang", "Yonggang Wen", "Tianwei Zhang"], "abstract": "Graph Transformer is a new architecture that surpasses GNNs in graph learning. While there emerge inspiring algorithm advancements, their practical adoption is still limited, particularly on real-world graphs involving up to millions of nodes. We observe existing graph transformers fail on large-scale graphs mainly due to heavy computation, limited scalability and inferior model quality.\nMotivated by these observations, we propose TORCHGT, the first efficient, scalable, and accurate graph transformer training system. TORCHGT optimizes training at three different levels. At algorithm level, by harnessing the graph sparsity, TORCHGT introduces a Dual-interleaved Attention which is computation-efficient and accuracy-maintained. At runtime level, TORCHGT scales training across workers with a communication-light Cluster-aware Graph Parallelism. At kernel level, an Elastic Computation Reformation further optimizes the computation by reducing memory access latency in a dynamic way. Extensive experiments demonstrate that TORCHGT boosts training by up to 62.7x and supports graph sequence lengths of up to 1M.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph-structured data has long been prevalent and indispensable in many real-life applications such as social network construction and molecule analysis. Thus, there emerges a specific family of graph learning methods, namely graph neural networks (GNNs) [1]\u2013[3]. GNNs have gained giant breakthroughs and exhibit impressive performance in many tasks such as node classification [4]-[7] and link prediction [3], mainly due to their message passing mechanism [8], which models the inherent properties of graph structures. However, this module in classic GNNs also leads to commonly acknowledged over-smoothing [9], over-squashing [10], [11] and limited expressivity [12] issues.\nTo address these deficiencies, a latest approach called graph transformer shows more promising power in capturing the inter-dependencies among nodes. Graph transformer is built upon the classical Transformer [13] which allows nodes to attend to all other nodes, and integrates multiple graph structure encodings to include important graph properties. Due to the great modeling capability, graph transformer has garnered surging interest in recent years and a large number of models have been proposed [14]\u2013[17]. Existing graph transformers mainly operate by treating graph nodes as input tokens and constructing an input sequence consisting of all the graph nodes. Besides, graph structure encoders are designed as a graph adaptation of the original Transformer architecture. By integrating structural information, graph transformers exhibit competitive performance and outperform traditional message-passing GNNs (e.g., GCN [1] and GAT [3]) on both node classification [16]\u2013[21] and graph classification [14], [15], [22] tasks, as shown by Table I. We can obviously see graph transformers obtain the highest scores than GNNs on all tasks.\nReal-world graphs can easily involve millions of nodes [23], [24], making the sequence length enormously large. For example, in the current graph transformers' operation way, processing the citation graph dataset ogbn-papers100M from Open Graph Benchmark [23] (including more than 100 million nodes) requires high dimensional inputs with prohibited sequences. Moreover, as illustrated by the profiled results in \u00a7II-B, training graph transformers with long sequence is crucial for model quality and the development of versatile graph transformer application scenarios. However, we find most existing graph transformer research works [14], [15], [25], [26] are only limited to small graphs due to a lack of compatible systems tailored for the graph transformer model training with long sequences. More specifically, there are three deficiencies in current works:\nFirst, graph transformers with standard attention have poor scalability to long sequences, due to the computation and memory complexity of $O(N^2)$, quadratic on the number of nodes (N) in a graph [14], [15], [27]\u2013[30]. Taking fully-connected attention as graph foundation encoders captures the implicit all-pair influence beyond neighboring nodes, but also limits existing graph transformers only on small-graph applications [14], [15], [27]\u2013[30]. Figure 2 demonstrates that even with a state-of-the-art attention library, i.e., FlashAttention [31], the computation of the dense attention mechanism is still a bottleneck during the graph transformer training.\nSecond, current algorithms either compromise model quality or are only applicable to a single graph learning task. To reduce computation pressure, some graph transformers shorten the input sequences by harnessing neighbor sampling [16], [32] similar adopted in classic GNNs [2], [33], [34]. Others like [25] attempt to overcome the quadratic complexity by replacing full attention with approximate attention methods. However, using sampling methods or simply adopting sparse patterns like [35], [36] loses critical connectivity information and thus sacrifices model precision. On the other hand, some works [17], [20], [21] use self-defined adapted attention modules to reduce memory consumption, but are limited to a specific task, e.g., node classification. They are neither general to versatile graph learning tasks nor portable to be scaled in large-scale training.\nThird, no existing works exploit systematic optimizations to realize efficient and scalable training. Several graph transformers [25], [26] apply graph structure to relieve the computation burden. However, this sparse pattern is highly irregular in memory access due to the skewed property of graphs, which is challenging for optimizing the system throughput. Moreover, with large datasets, the memory consumption of model activations grows rapidly, necessitating a scalable system design and memory optimization. But existing works [14]\u2013[16], [25], [26] only focus on the implementation of graph transformers in a single GPU, thus limited to very small graphs. Although there has been a breakthrough for large language models (LLMs) by partitioning along the input sequence dimension and training long sequences across devices [37]\u2013[40], those sequence parallelism ways cannot be directly transplanted on graph transformers due to the extra graph encodings and neglection of structure properties.\nTo bridge these gaps, we design TORCHGT, the first distributed training system that scales graph transformer model to large graphs with billions of edges. Our system abides four design goals: scalable, efficient, convergence-maintained, and task-agnostic. Existing graph transformer works neither facilitate efficiency by well-designed parallelism from the system perspective nor propose scalable algorithms for universal graph learning tasks, thus making it challenging to meet those goals. This hinders the practical development of advanced graph transformer models on real-world graphs. The core design of TORCHGT derives from the following three key insights.\nFirst, the learning of graph transformers highly benefits from graph structures. Specifically, the structure of many real-world graphs is highly sparse [41]\u2013[43], which reflects the inherent vertex-vertex interactions. This sparsity could be a guide for how graph transformers attend to nodes to reduce computation costs while maintaining correct connections. In addition, considering the structural property in the system design also contributes to optimal hardware throughput. Second, the order of input graph tokens is alterable. Unlike inputs in famous LLMs like GPT [44] whose token order is crucial for model understanding and generation process [44], [45], graph transformers focus more on connections between nodes. Thus, we can modify the input arrangement to exploit graph properties (e.g., local clusters) for more specialized optimizations. Third, the block-sparse format is a good match for irregular graph clusters. Block-sparse formats store data contiguously in memory, reducing storage overheads and memory access. But directly exploiting it on dense attention matrices will drop connectivity and result in substantial accuracy loss [46], [47]. However, by integrating it into our specialized clustered pattern, we find the computation can be further accelerated while maintaining model accuracy.\nAs such, our key idea is to design an accuracy-maintained and compute-efficient system from both algorithm and system perspectives to support large-scale graph transformer training. Specifically, TORCHGT consists of three key innovations. Dual-interleaved Attention is a local-global interleaved attention that integrates graph structural topology into the attention module and selectively combines the global information into the attention with the graph structure search, which efficiently speeds up the attention computation while maintaining the models' qualities. Cluster-aware Graph Parallelism splits the input graph tokens according to the cluster nature of graphs, thus boosting the attention computation throughput and facilitating system scalability. It also allows us to take advantage of the cluster feature in more fine-grained kernel optimizations. Inspired by the block-sparse format, Elastic Computation Reformation dynamically transfers the clustered attention pattern into a specialized cluster-sparse format to reduce the irregular memory access latency. It includes an Auto Tuner to automatically control the transfer to maintain the model convergence. Through extensive experiments, we show TORCHGT successfully achieves scalable and efficient graph transformer training on large graphs. It also boosts training by up to 62.7\u00d7 across various graph learning tasks while maintaining accuracy.\nIn summary, we make the following contributions:\n\u2605 TORCHGT is the first graph transformer system that facilitates efficient, scalable, and accurate training on large-scale graphs as well as universal graph learning tasks.\n\u2605 TORCHGT is the first to identify the major challenges that hinder existing graph transformers from scaling to large graphs and explore the graph-specific optimization opportunities which are neglected previously.\n\u2605 We propose three key techniques to meet all design goals from algorithm and system co-design perspectives.\n\u2605 Experiments show TORCHGT achieves up to 62.7\u00d7 speedup and near-linear scalability, supporting graph sequence lengths of up to millions."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "Graph transformer architecture has attracted surging attention in graph representation learning in recent years [48]. Current representative graph transformers integrate graph structural encodings into the input and attention map in the Transformer architecture. The input sequence is built by tokens generated with graph attributes. Specifically, some works [14]\u2013[16], [18], [49]\u2013[51] calculate node positional encodings beforehand and add them to the inputs before the attention module. Other works [14], [15], [17], [32], [52] add graph structural information into the self-attention matrix as bias. Several works [19], [25], [26] combine message-passing GNNs and the attention mechanism together. Here we only focus on the former two types of graph transformers since they are currently most representative.\nA basic Transformer consists of multi-head attention (MHA) and feed-forward network (FFN) which contains two linear layers. Given an input sequence $H = [h_1,\u2026,h_S]^T \u2208 R^{S\u00d7d}$ where S is the sequence length and d is the hidden dimension, MHA first projects its input H to three subspaces: Q, K and V with projection weight matrices $W_Q \u2208 R^{d\u00d7d_K}, W_k \u2208 R^{d\u00d7d_K}, W_v \u2208 R^{d\u00d7d_v}$. The MHA output is calculated as:\n$H' = softmax(\\frac{QK^T}{\\sqrt{d_K}})V$ (1)\nwhere $d_K$ is the second dimension of K. MHA captures the pair-wise similarity of input tokens in the sequence.\nFor a graph $G = (V, E)$ with nodes $V = {v_1,\u2026,v_N}$ and edges $E = {e_1,\u2026,e_{|E|}}$, here we list the formulation of Graphormer [14] as an example:\n$A_{ij} = \\frac{(h_i^{(0)}W_Q)^T (h_j^{(0)}W_K)}{\\sqrt{d_K}} + bias(v_i,v_j)$ (2)\n$h_i^{(0)} = x_i + z_{deg^-(v_i)} + z_{deg^+(v_i)}$ (3)\nwhere $h_i^{(0)}$ is the beginning attribute of node i, $x_i$ is the node feature, and $z_{deg^-},z_{deg^+} \u2208 R^d$ are learnable embeddings specified by the in-degree $deg^-(v_i)$ and out-degree $deg^+(v_i)$. The encodings in Equation 2 allow the attention to capture the node importance. $A_{ij}$ is the (i, j)-element of Query-Key product matrix, namely the attention coefficient. bias is a learnable scalar shared across all layers, and $\u03c8(v_i, v_j)$ is the distance of the shortest path (SPD) between node $v_i$ and $v_j$, which is the shortest hops that $v_i$ needs to pass to reach $v_j$."}, {"title": "B. Long Sequence for Graph Transformers", "content": "For better illustration, we categorize current graph learning tasks into two types to discuss the need of training in long sequences: (1) graph-level task, and (2) node-level task.\nLong Sequence for Graph-level Tasks. For such tasks, the input sequences represent a set of graphs while the output is a set of labels representing the types of corresponding graphs. When processed by graph transformers, all nodes in each input graph need to be encoded as input tokens and are concatenated into an input sequence. As such, the length of each sequence equals the number of nodes in each graph. In this task, if the graph size, i.e., the number of nodes, grows very large, the input sequence can be too long to be trained by current methods. For instance, the MalNet [53] dataset contains graphs with up to 552K nodes.\nLong Sequence for Node-level Tasks. These tasks classify each node in an input graph with a specific label. In the node classification task, the input sequences can either encode all nodes in the graph or a mini-batch of nodes. For the former case, the input sequence can be enormously long for large-scale graphs, which is not supported by most models. For the latter, with a larger batch size, both the training throughput and the trained model quality can be improved. Both cases validate the necessity and advantages of long sequence training.\nHowever, existing graph transformers have some inherent constraints in performing the above tasks. While graph-level scenario has been explored in [14], [15], existing endeavors do not generalize to large-scale graphs endemic to node-level prediction. Our TORCHGT strives to include both tasks by joint algorithm-system design. The scale of graphs applicable to current models is still limited, thus leaving long sequence training still an urgent necessity. Besides, training large-scale graphs in short sequence suffers from lower training throughput, downgraded model quality and limited graph transformer applications. Figure 1 illustrates the impact of sequence length on the test accuracy of two representative models Graphormer [14] and NodeFormer [17] on two datasets. Both models show superior performance on longer sequences. On the AMiner-CS dataset, Graphormer with a 4K sequence length improves the test accuracy by up to 0.9% compared to the short sequences. On the Pokec dataset, sampling-based NodeFormer with 100K sequence length outperforms the case with 10K sequence length by a staggering 12% accuracy. These results necessitate the need for long sequence training of graph transformers."}, {"title": "C. Issues and Opportunities", "content": "Most existing graph transformer works [14], [15], [19], [25], [26], [32] are only limited on small graphs due to a lack of compatible systems tailored for the graph transformer model training with long sequences. They have three main issues when applied to long sequence training.\nI1: Attention Computation Bottleneck. Graph transformers with standard (dense) attention treat the graph as fully-connected with the MHA mechanism calculating attention for all node pairs. Thus, it requires the computation complexity of the attention module to be quadratic on the number of nodes ($N^2$) in a graph, which limits the models' scalability to extremely long sequences. Currently, there is a breakthrough in standard attention optimization, i.e., FlashAttention [31]. FlashAttention accelerates the attention module by fusing the IO-bound GPU kernels like Softmax and Dropout within the attention computation. However, even with FlashAttention to train graph transformers with long sequences, e.g., sequence length of 512K, we still identify that the attention module dominates the overall training time.\nTo show this, we conduct an experiment to record the iteration time breakdown when using FlashAttention, as illustrated in Figure 2. Current FlashAttention does not support the modified attention module like those augmented with bias encodings [14], [15], [17], [32], so we disable the bias in this experiment to only examine the computation efficiency. We separate the computation time of FlashAttention from the comprehensive training iteration. We can obviously observe that no matter on longer or shorter sequences, attention computation still dominates over 80% of training time, indicating a severe attention bottleneck. However, both the standard attention and FlashAttention fail to leverage one important characteristic of the graph, namely its topology structure, which we find profoundly impacts the effectiveness of system optimizations.\nI2: Degraded Model Convergence and Limited Tasks. Many efforts [17], [25], [35], [54] have been made to overcome the computation bottleneck of the attention module. Among them, [54] prunes the attention module and leaves a major backbone to reduce the computation cost for LLMs. Some works [35], [36], [55], [56] propose sparse patterns on attention to scale linearly, but most of them are designed for natural language processing (NLP). They cannot be simply grafted to graph transformers since they fail to consider the inherent graph structure information when approximating attention, thus resulting in subpar model performance. Several graph transformers [16], [32], [57] harness neighbor sampling or graph pooling that only selects a subset of nodes to be trained at each iteration, without reducing the computation complexity. Nonetheless, all the above methods sacrifice model precision by dropping the connectivity information.\nIn the graph domain, efficient attention is not well studied. Few graph transformers like [26] apply the graph structure to attend nodes and maintain graph-specific information. However, they limit the implementation to the GNN-encoding-based model architecture, e.g., GraphGPS [25], and highly rely on the message-passing scheme for excellent model performance. Other methods [17], [20], [21] use self-defined adapted attention modules to achieve linear complexity. However, all those works are constrained to a single application task, failing to generalize to versatile graph tasks. Additionally, with GNN structure encodings or self-defined attention, the model can hardly be scaled to multiple workers.\nI3: Lack of Specific System Optimizations. As far as we know, currently there is no existing framework to optimize graph transformer training from the system level. FFN operations in MHA are dense in computation and regular in memory access. However, utilizing graphs on the attention module is sparse in computation and requires irregular and fine-grained memory access due to the skewed nature of graph structures, which inevitably becomes the performance barrier.\nExisting solutions [15], [25], [26] directly apply graph topology in the attention computation while ignoring the pattern differences between graph transformer and standard Transformer-based models. To better illustrate, we experimentally examine the impact of irregular memory access by the topology-pattern attention in Table II. The topology-induced memory access latency is tremendous, reaching up to 33.2\u00d7 slowdown than dense computation. To increase models' scalability, recent works [37]\u2013[40] split the input sequences and distribute the computation across devices. However, this parallelism neglects various graph encoding modules and fails to distinguish input tokens in graph domain from tokens in NLP. Those differences invoke specialized and dedicated system designs for graph transformers towards more efficient memory optimizations and more aggressive parallelism."}, {"title": "III. TORCHGT DESIGN", "content": "We propose TORCHGT, an algorithm-system co-optimized system tailored for graph transformer training on large-scale graphs. It follows four design principles:\n\u2022 Scalable. TORCHGT can scale graph transformer training to extremely large graphs (solving I3).\n\u2022 Efficient. TORCHGT reduces over 90% computation required by standard attention, overcoming the attention computation bottleneck (solving I1).\n\u2022 Convergence-maintained. TORCHGT maintains comparable model convergence to the graph transformer with standard attention, and successfully balances the trade-off between training efficiency and model quality (solving I2).\n\u2022 Task-agnostic. TORCHGT generalizes to various graph transformer models and graph learning tasks (graph-level and node-level) (solving I2).\nMotivated by all the observations in \u00a7II-C, our key idea is to co-design an accuracy-maintained and compute-efficient attention module with a graph-parallelism-enabled system framework to support long sequence training. As shown in Figure 3, TORCHGT intelligently optimizes training across three levels from the top to bottom hierarchy: algorithm, runtime and kernel. We propose a topology-induced and accurate attention algorithm in the algorithm level. We present a novel cluster-aware graph parallelism to scale the training in the runtime level. In the kernel level, we design a memory-optimized computation pattern specialized for clustered attention. Specifically, TORCHGT consists of three key modules:\n\u2022 Dual-interleaved Attention: In the algorithm level, it integrates locally graph-induced topology into the attention computation pattern and periodically overlays it with the fully-connected information, which efficiently reduces the computation burden while maintaining the model's quality as much as possible. It is tailored for versatile graph transformer models with local-global interleaved attention.\n\u2022 Cluster-aware Graph Parallelism: From the distributed runtime perspective, we design a cluster-aware graph parallelism tailored for graph transformers. It splits the graph tokens in sequences according to the clustering nature of graphs, thus computing attention with locality and facilitating the system scalability.\n\u2022 Elastic Computation Reformation: It reformats the graph-induced pattern obtained at the runtime level into our customized and fine-grained cluster-sparse pattern at the underlying execution kernel level. It further improves the attention computation throughput by greatly alleviating irregular memory access. To balance the efficiency-quality trade-off, we build an Auto Tuner to make an elastic transfer of cluster sparsity."}, {"title": "B. Dual-interleaved Attention", "content": "Motivated by I1 in \u00a7II-C, TORCHGT explores the opportunity of integrating graph structure to reduce the substantial computation cost. For optimizations aiming at graph transformers, we design an interleaved attention to realize a local-global interleaved attention and ensure model convergence.\nLocal Topology-induced Pattern. In NLP tasks, the tokens in a sequence represent words, while in graph transformers the tokens are nodes of the input graph. Besides, most graph transformers like [14], [17] adopt the standard attention, which can be viewed as a fully-connected graph since all tokens attend to every other token to perform inner products, leading to quadratic complexity. Motivated by the sparse attention methods [35], [36], we find the local topology-induced pattern that makes use of the underlying structure of the input graph is desirable to guide the pair-wise node interactions. Graphs innately own two desiderata for attention mechanism: (1) small pair-wise node interactions (large sparsity), and (2) data locality. In addition, most sparse patterns in NLP are only approximations [36] to their dense counterparts under specific contexts, while in our scenario the graph structure is real and valid, without the need of approximations. Thus, we compute attention by attending each node to its immediate neighbors in the graph, reducing the interacted node pairs.\nWe formulate the local topology-induced pattern as below. To train on a graph $G = (V, E)$, we generate an input sequence $S\u2208 R^{S\u00d7d}$ comprised of graph tokens corresponding to a node set $V\u2208 V$. $V$ can be equal to either the whole nodes V, e.g., in graph-level tasks, or a subset of V, e.g., in node-level tasks if the node number is too large. For each node set \u00d1, we construct a local attention graph $\\~G = (V, \\~E)$, where the edge set $\\~E$ is also a subset of the original edge set E. If there exists a global token in the model that attends to all nodes in the graph and is attended to by all nodes, we augment $\\~E$ with the global token's edges. The general attention co-efficient $\\~A_{ij}$ of graph transformers without graph encodings is computed in: $A_{ij} = softmax(\\frac{(h_iW_Q)\u00b7(h_jW_K)}{\\sqrt{d_K}})$. The updated node attribute h' for each node i is computed as the weighted sum of the features of its neighboring nodes from V: $h' = \\sum_{j\u2208N(i)} A_{ij} (h_jW_v)$, where N(i) denotes the set of neighboring nodes of node i. Each neighbor's feature vector hj is weighted by the attention coefficient Aij, and these weighted features are summed to update the attribute of node i. By using our local topology-induced pattern G, the attention only computes coefficients of connected node pairs.\nInterleave Fully-connected Pattern. Implementing the attention computation via the graph structure can greatly reduce the computation cost. However, it sometimes slows the model accuracy and convergence, which can be shown by experiment results in Figure 10. The local graph-induced attention slightly degrades the model convergence, which is mainly because the topology-induced pattern restricts the attention mechanism from extracting the high-order neighboring information. Intuitively, larger sparsity induces more absence in the attention computation, and increases the model error. Building on this, we empirically interleave a fully-connected attention on the local graph-induced attention.\nTo fill the performance gap between sparse attention and its dense counterpart, we need to figure out when to interleave the dense pattern. Motivated by the sparse attention theories in [58], we conclude three critical conditions under which we use the topology-induced pattern on attention:\n\u2022 C1: Every node in the sequence S always attends to itself.\n\u2022 C2: There exists a Hamiltonian path that directly connects all nodes V in the sequence.\n\u2022 C3: All nodes in the sequence should be able to attend to other nodes, either directly or indirectly after L graph transformer attention layers."}, {"title": "C. Cluster-aware Graph Parallelism", "content": "To better fit the topology-induced attention pattern and increase the system scalability, we introduce a graph transformer-specialized parallel training style: Cluster-aware Graph Parallelism, which exploits the graph cluster characteristics to guide the distributed training.\nUtilization of Graph Cluster. Graph cluster (community) [61], [62] is one essential characteristic of real-world graphs, referring to a subset of nodes within a graph that exhibit a higher degree of connectivity with each other compared to nodes in other parts of the graph. Although the graph structure-based attention in \u00a7III-B greatly reduces the computation, this sparse and highly-skewed nature of graphs triggers substantial irregular memory access since edge connections are distributed in an uneven pattern, bringing extra overhead to training. Consequently, employing the graph cluster structure on GPUs is promising for graph transformer training improvement. There exist some approaches in traditional graph learning [62], [63] to utilize graph cluster, but they are aimed for CPU processing with limited parallelization. [64] also exploits graph cluster but focuses on redundant data loading in GNN computing.\nTherefore, to explore the performance benefits of graph cluster on graph attention computing, we incorporate a lightweight node reordering to cluster nodes and improve the spatial locality during attention computation, without changing the connectivity correctness. The key insight is that the proximity of node IDs is more likely to be scheduled to the adjacency of computing units on GPUs where they get processed. In detail, we leverage METIS [65], a community-based graph reordering technique for great cluster locality and ease of integration with parallelism. Specifically, it uses multilevel recursive bipartitioning to divide and coarsen the graph while preserving the essential structure. We optimize the implementation of METIS for a lower cost: we capture the cluster information of graphs and map such locality from the upper level to the underlying GPU kernels, which also enables us to leverage the L1 & L2 cache for refined cluster capturing (later discussed in \u00a7III-D).\nSpecialized Graph Parallelism. To increase the scalability of graph transformers, intuitively TORCHGT employs parallelism technologies to dispense the computation across devices. There have been extensive studies in sequence parallelism technologies [37]\u2013[39] for LLMs to support efficient long sequence training. However, current parallelism methods for language models trigger two challenges when applied to graph transformers: (1) failing to leverage graph properties; (2) not supporting various graph encodings. In traditional language models, the input sequence encodes the context of a specific sentence. As such, training the language model requires tokens in the input sequence concatenated in a pre-defined order. In contrast, we observe that for graph learning tasks, there is no need for graph transformers to predict sequences (graph-level task) or tokens (node-level task) within a position-fixed context, since they only rely on the graph topology to construct the structural encodings. A motivating example of parallelizing graphs with graph cluster is the graph-level task, where only the global token is critical for inferring the graph type and other node tokens can be arranged in any order.\nBased on this insight, we are the first to design a Cluster-aware Graph Parallelism specialized for graph transformers, as shown in Figure 4. Specifically, the raw input sequences S and graph encodings B are randomly partitioned across P devices. Each local sub-sequence $S_{sub}$ and sub-encodings are projected to local matrices: $Q_{sub}, K_{sub}, V_{sub}, B_{sub} \u2208 R^{S\u00d7d}$, assuming they have the same dimensionality. Then in each graph transformer layer, all subspaces are combined together into complete matrices Q, K, V, and B via the highly efficient all-to-all collective communication operation. All-to-all operation owns an advantage over other communication operations (e.g., all-gather and reduce-scatter) in terms of much smaller communication volume and overall better scalability, which is also proved in [38]. All-to-all gathers matrices in sequence dimension and splits in the head, resulting in $Q,K,V, and B \u2208 R^{S\u00d7d}$. Now that since matrices are complete in the sequence dimension, TORCHGT reorganizes the layout according to the clustering nature of graphs discussed before. Then the Dual-interleaved Attention conducts attention computation in the clustered layout, exemplified as blue, orange and green rectangles in Figure 4. After attention computation, another all-to-all transforms the output tensor back to subspace $S_{sub}$ for subsequent operators such as FFN and layer normalization in the graph transformer layer.\nCommunication Complexity. Thanks to all-to-all, Cluster-aware Graph Parallelism has low communication volume and scales exactly well with more servers. Given hidden size d, sequence length S, and parallelism degree P, TORCHGT performs all-to-all with a total message size 3Sd before the attention computation, and another all-to-all for attention output with size Sd. Therefore, TORCHGT performs two all-to-alls with communication volume per GPU of 4Sd/P and communication complexity of O(S/P), while other operations like all-gather have communication complexity of O(S). Thus, TORCHGT has better scalability with increasing parallelism degree on extremely long sequences.\nIn summary, compared with sequence parallelism methods for LLMs, our Cluster-aware Graph Parallelism favors the graph transformer architecture in several aspects. First, all-to-all gathers in sequence dimension, leading to exactly integrated graph topology, which the topology-induced sparse pattern in \u00a7III-B can be perfectly applied to. Second, the graph encodings B share the same sparse layout as attention mapping so the parallelism of graph transformers only brings a trivial memory footprint and communication overhead, thus facilitating model scalability and ensuring memory efficiency."}, {"title": "D. Elastic Computation Reformation", "content": "Cluster Sparsity. The topology-induced attention pattern can significantly reduce the computation cost, but also leading to substantial irregular memory access due to the highly skewed nature of graphs. Figure 5(b) gives an example with the cluster dimensionality of 8 and sequence length of 64K. We observe that only the diagonal clusters in the clustered adjacency matrix appear in dense patterns most and show lower sparsity, which can benefit from locality since nodes in each cluster are close to each other. On the other hand, other clusters appear highly sparse patterns and more irregular shapes (denoted as sparse cluster). Accessing the embeddings of computation like aggregation in this pattern requires a large number of atomic operations. Consequently, those remaining irregular clusters still engender heavy overhead. To exemplify, training Graphormer on ogbn-arxiv (S=64K) in Figure 5(b) takes 375 ms per epoch, while its dense counterpart only takes 81ms.\nTo further reduce the memory access latency, we propose a memory-efficient Elastic Computation Reformation which introduces the cluster sparsity. Motivated by the block-sparse pattern in [36], [46], [47], we reformat the clustered layout in Figure 5(b) to a fine-grained cluster-level fashion in Figure 5(c). Specifically, as shown in Figure 4, for each sparse cluster, TORCHGT transfers the scattered edges inside it to multiple substructures of compact and adjacent edge connections, which is denoted as sub-blocks. The transferred dense cluster can have multiple randomly scattered sub-blocks, the number of which is decided by the number of real edges in the cluster and the dimension of sub-block dr. Note that there will be totally $k^2$ clusters for the whole layout with the cluster dimensionality of k. Figure 5(c) depicts the cluster-sparse layout with k=8, in which most sparse clusters are transferred to dense ones with tight sub-blocks. This cluster sparsity offsets the downside of irregular memory access incurred by the topology-induced pattern.\nTransfer Strategy. Applying static cluster-sparse transferring will result in model quality degradation since the cluster sparsity changes the original graph structure by modifying edges. Some performance-related information (e.g., convergence) for model training is only available at runtime. Without considering the runtime information, the system will suffer from an inferior model accuracy or inefficient memory access.\nThus, TORCHGT designs the following two strategies:\n\u2022 Indolent Transferring. TORCHGT only transfers clusters that are extremely sparse and irregular. Although such an inactive way may miss some optimization opportunities, it can refrain from model quality decline and be more portable. Concretely, TORCHGT only transfers sparse clusters whose sparsity value $\u03b2_c$ is smaller than that of the current whole graph $B_G$. Note that the sparsity value refers to the proportion of nonzero elements in the whole adjacency matrix.\n\u2022 Elastic Transferring. We dynamically adjust the amount of transferred dense clusters along the training. First, we set a threshold value $\u03b2_{thre}$ for controlling cluster-sparse transfer. If the sparsity of a cluster $B_c$ is smaller than the threshold $\u03b2_{thre}$, TORCHGT transfers it to a dense cluster. To decide the value of $\u03b2_{thre}$ in each training epoch, we design an Auto Tuner in the next part for modeling $\u03b2_{thre}$.\nHyperparameter Modeling. The hyperparameters can be tuned to accommodate various graph patterns. We design an Auto Tuner to dynamically select the hyperparameters k, do and $\u03b2_{thre}$, and formulate the analytical model.\n(1) Cluster dimensionality k, sub-block dimension $d_b$. We can leverage GPU L1 and L2 caches to improve the memory access locality of sub-block computation. In this way, smaller sub-blocks can enjoy the data locality benefit from the L1 cache while larger clusters can enjoy the locality from the larger L2 cache. Specifically, we determine k as:\n$k = \\sqrt{Q_{L2}/d}$ \u2200 $Q_{L2} \u2208 N$, where $Q_{L2}$ is the L2 cache size and d is model hidden dimension. To determine the sub-block dimension $d_b$, we profile the computation throughput and some hardware statistics of the indexing kernel w.r.t. different $d_b$ values. Figure 6(a) shows the workload balance in GPU computing unit downgrades (the lower warp occupancy, the worse balance) as $d_b$ increases, while both L1 & L2 cache hit rates increase. Thus, there exists a trade-off between these two metrics in deciding $d_b$. Moreover, Figure 6(b) also demonstrates the values of obtaining the optimal computation throughput lie in the middle range. Both cases suggest the middle value is the ideal choice. For example, for RTX 3090 GPU and model hidden dimension of 64, we fit k=8 and db=16. These two values can also be selected by users.\n(2) Transfer threshold $\u03b2_{thre}$. Motivated by [7], to estimate the convergence, Auto Tuner tracks a running average loss $F_t = 0.9F_{t-1} + 0.1L_t$, where $L_t$ is the loss at epoch t. Considering the training throughput, a Loss Descent Rate (LDR) $\u03b5_{tt}$ is defined as $LDR_t = \\frac{F_t - F_{t-1}}{\u03b5_{tt}}$, where $\u03b5_{tt}$ is the t-th epoch training time. At the beginning $\u03b2_{thre,0}$ is initialized as $B_G$ from the set {$0, B_G, 1.5B_G, 5B_\u03b1, 7B_\u03b1, 10B_G, 1$}, which is developed by profiling different datasets. When $LDR_t > LDR_{t-\u03b4}$ for some \u03b4 \u2208 N (here we use d = 10) which specifies the range of epochs for LDR comparisons, TORCHGT heuristically determines the current $\u03b2_{thre}$ suffices to reduce the loss. Then Auto Tuner increases $\u03b2_{thre}$ to the next value in the set to gain higher speed. On the other hand, $LDR_t < LDR_{t-\u03b4}$ in \u03b4 epochs denotes the training is about to converge or too many errors are introduced by quantization. In this case, Auto Tuner reduces Bthre to the previous value from the set to enable more stable and accurate training."}, {"title": "IV. EVALUATION", "content": "We implement TORCHGT atop PyTorch 2.1 [67]. We study the performance of TORCHGT on versatile datasets and graph learning tasks in the following aspects: (1) Efficiency (\u00a7IV-A), (2) Convergence (\u00a7IV-B), (3) Scalability (\u00a7IV-C), and (4) micro-benchmarks and ablation studies to examine the impact of each technique and hyperparameter (\u00a7 IV-D).\nDatasets and Models. We evaluate TORCHGT on versatile real-world graph datasets with multiple scales. The detailed information is shown in Table III, including both node-level and graph-level tasks. The MalNet dataset is constructed from all categories of the full datasets. We use three classical graph transformer models commonly adopted for evaluation, including Graphormerslim (GPHslim) [14], GraphormerLarge (GPHLarge) [14], and GT [15]. Note that TORCHGT can also be applied to other graph transformer models. As shown in Table IV, we follow the hyperparameter configurations reported in their original papers as closely as possible.\nBaselines. All models cannot be directly trained on selected large graphs. Due to the lack of existing graph transformer systems, we meticulously replicate each model with simple graph parallelism following its original implementation as the vanilla version, denoted as GP-RAW. On the basis of this, we have also developed other variants incorporating FlashAttention [31] denoted as GP-FLASH, and topology-induced sparse attention denoted as GP-SPARSE.\nTestbed. Our experiments are performed on two GPU servers. \ufffd13 GPU servers each with 8 NVIDIA RTX 3090 GPUs (24GB). Intra-server connections (CPU-GPU, GPU-GPU) are based on PCIe 4.0x16 lanes and inter-server connections are via 1Gbps Ethernet. \ufffd2 servers each with 8 A100 GPUs (80GB) with NVLink and 200Gbps InfiniBand.\nA. End-to-end Training Throughput\nWe compare the end-to-end training time per epoch and test accuracy of TORCHGT with all baselines on one server, as shown in Table V. The sequence length is 256K for GPHslim and GT, and 32K for GPHLarge. When training on ogbn-arxiv, we set the sequence length to 64K for GPHslim and GT. The speedup in the bracket is the relative throughput of each method on the basis of GP-FLASH. In each training task, we treat the first 10 epochs as the warmup stage and only record statistics afterward. TORCHGT substantially outperforms GP-FLASH by 3.3~62.7\u00d7. This is mainly because TORCHGT significantly reduces the computation complexity of the attention module. Additionally, GP-RAW runs out of memory (OOM) on all datasets under the current sequence lengths due to its $O(N^2)$ memory complexity of the attention module. For instance, GP-RAW requires over 200GB memory to store the attention score, i.e., $QK^T$, of only one attention head for the ogbn-products dataset. We also conduct evaluations of GPHLarge on one A100 server as shown in Table VI. TORCHGT still shows impressive acceleration and outperforms GP-FLASH up to 4.2\u00d7 on such frontier equipment. In summary, TORCHGT realizes efficient training of graph transformers with a marvelous improvement.\nThe speedup difference is mainly related to the input graph topology and model structure under long sequences. If the input graph is very sparse (up to 99% sparsity), Dual-interleaved Attention first boosts attention by a large margin. If an obvious clustering pattern exists in the graph, then attention can be further accelerated by 2~3\u00d7 with the other two modules. Since we mainly improve attention computation, the more proportion it accounts in total model computation, the higher speedup on epoch time. For instance, in Table V GPHslim achieves notable speedup on papers100M owing to the above. Ogbn-arxiv shows a smaller speedup on all models since it has poorer sparsity and cluster property."}, {"title": "B. Model Convergence", "content": "We examine the model accuracy and convergence curves of TORCHGT on various models in Table V and Figure 8. Table V summarizes the test accuracy achieved by all systems on three models. On large-scale datasets, TORCHGT gives higher model accuracy while GP-RAW runs out of memory. GP-FLASH harms the model accuracy in some datasets, e.g., Malnet and Amazon since FlashAttention only supports FP16/BF16 precision [31] in computing which may downgrade model convergence compared to FP32 precision. In contrast, TORCHGT supports FP32 precision without compromising model accuracy. To better validate this, we compare the training throughput and test accuracy of GP-FLASH and TORCHGT-BF16 in Table VII. On BF16, TORCHGT obtains similar accuracy with FlashAttention, indicating the accuracy drop of FlashAttention is mainly caused by reduced precision. TORCHGT even achieves higher speedup with BF16, but we choose to use FP32 since it gives higher accuracy with notable speedup. From Figure 8, we also see that GP-FLASH converges to low accuracy and lead to far slower convergence than our system, verifying it preserves model quality well."}, {"title": "C. System Scalability", "content": "Training Throughput on Multiple Servers. First, we evaluate the training throughput of TORCHGT on multiple A100 servers to validate it also scales out well with more servers. As shown in Figure 7, we conduct two sets of scalability evaluations on up to 8 servers(each with 8 A100 GPUs) with extremely long sequences and record the sequence training time on the ogbn-products dataset. In Figure 7(a), we fix the sequence length to 1024K and increase the server number. We can see TORCHGT still obtains notable speedup when scaling to more servers. Especially, when the GPU count is doubled, the training throughput correspondingly increases by almost 1.7x, indicating a certain degree of scalability. In Figure 7(b), we fix the computational load per GPU when increasing the sequence length from 256K to 512K. Note that when doubling the sequence length, we need 4\u00d7 GPUs than before to keep the same computational load per GPU(attention calculation is proportional to $S^2/P$). In this case, TORCHGT achieves approximately the same throughput on each GPU as before, also verifying good scalability.\nSequence Length w.r.t. Number of GPUs. We examine the maximum sequence length of GPHslim that can be trained on 1~8 GPUs with TORCHGT in Figure 9(a). Note that GP-RAW employs standard full attention. We can see the maximum sequence length of TORCHGT can reach up to 1.3M on 8 GPUs. It also enables the sequence length of 400K with only 1 GPU, substantially 50\u00d7 larger than that of GP-RAW. Moreover, the sequence length of TORCHGT almost scales linearly w.r.t. the number of GPUs, while the maximum sequence length GP-Raw can support nearly remains unchanged with the growth of GPU numbers. With 8 GPUs, TORCHGT supports 1.3M in length while GP-RAW only supports 22K in length.\nThroughput w.r.t. Sequence Length. We further compare the training throughput of TORCHGT and GP-FLASH under sequence lengths varying from 128K to 1300K in Figure 9(b). We fix the number of GPUs to 8 and report the throughput as samples per second. Figure 9(b) shows that the training throughput of GP-FLASH sharply decreases from 1.9 \u00d7 105 samples/s to 2.2 \u00d7 104 samples/s when the sequence length increases. The speed degradation of GP-FLASH mainly comes from the computation bottleneck of FlashAttention with O(N2) complexity. In contrast, TORCHGT maintains the training throughput at around 2.5 \u00d7 106 samples/s by significantly reducing the attention computation costs (in \u00a7III-B)."}, {"title": "D. Micro-benchmarks", "content": "We explore the effects of each component in TORCHGT via ablation studies and perform sensitivity analysis of the introduced hyperparameters on one 3090 GPU server.\nImpact of Dual-interleaved Attention. After employing the topology-induced attention and interleaving mode introduced in \u00a7III-B, we investigate their effect on model quality.\n(1) On large-scale graphs. We measure the convergence curves of GPHslim and GT on ogbn-arxiv, and compare the convergence of interleaved attention with that of FlashAttention and the sparse variant in Figure 10. The model with interleaved attention shows faster convergence than the other two and finally converges to higher accuracy, verifying that the interleaved attention improves computation efficiency while displaying great convergence.\n(2) On small graphs. Since the raw graph transformer models fail to be trained on large graphs, we further evaluate the convergence of interleaved attention on small graphs in Figure 11. Sparse attention shows the worst convergence rate while full attention has the best. The model with interleaved attention converges to nearly the same as the model with full attention and obviously outperforms the sparse variant in both convergence speed and final test score."}, {"title": "E. Pre-processing Cost", "content": "We record the pre-processing cost versus model convergence time on both tasks to understand how much extra time is brought by TORCHGT. The proportion is 5.2s (5.4%) versus 91.2s (94.6%) for ogbn-arxiv, and 239.7s (2.0%) versus 11732.4s (98.0%) for MalNet. The overhead only occupies less than 5.4% of the total training time on all epochs, which is acceptable compared with the huge model convergence time."}, {"title": "V. RELATED WORK", "content": "As a new kind of graph learning algorithms outperforming traditional GNNs, many graph transformer architectures have arisen in recent years. Among them, models [14], [16], [27], [28], [30] utilize standard attention as foundation encoders to capture the all-pair interactions between nodes, leading to quadratic computation complexity. Some other works [16], [32], [57] adopt sampling or pooling methods which only select a subset of nodes to be trained at each iteration, without reducing the computation complexity. [17], [20], [21] use self-defined adapted attention with poor generality and scalability.\nAs for system optimizations for transformers, sparse attention [35], [36], [55], [56] has been widely studied in NLP area for linear complexity. Borrowing this, [15], [25], [26] directly apply graph topology in the attention computation. Besides, multiple works for LLM [37]\u2013[40] split the input sentence sequences and train distributedly for larger scalability."}, {"title": "VI. CONCLUSION", "content": "To conclude, TORCHGT reveals challenges and opportunities in training graph transformer on large graphs, with our efforts in designing a scalable and efficient training system."}]}