{"title": "Question-to-Question Retrieval for Hallucination-Free Knowledge Access: An Approach for Wikipedia and Wikidata Question Answering", "authors": ["Santhosh Thottingal"], "abstract": "This paper introduces an approach to question answering over knowledge bases like Wikipedia and Wikidata by performing \"question-to-question\" matching and retrieval from a dense vector embedding store. Instead of embedding document content, we generate a comprehensive set of questions for each logical content unit using an instruction-tuned LLM. These questions are vector-embedded and stored, mapping to the corresponding content. Vector embedding of user queries are then matched against this question vector store. The highest similarity score leads to direct retrieval of the associated article content, eliminating the need for answer generation. Our method achieves high cosine similarity ( > 0.9 ) for relevant question pairs, enabling highly precise retrieval. This approach offers several advantages including computational efficiency, rapid response times, and increased scalability. We demonstrate its effectiveness on Wikipedia and Wikidata, including multimedia content through structured fact retrieval from Wikidata, opening up new pathways for multimodal question answering.", "sections": [{"title": "1 Introduction", "content": "Question answering (QA) is a task that answers factoid questions using a large collection of documents. In the context of Wikipedia, it is to answer questions beyond the keyword based traditional search. The rise of large language models (LLMs) has opened up new possibilities for building more capable QA systems, yet the challenge remains in ensuring their reliability and avoiding the generation of fabricated or hallucinated responses(Gao et al., 2024). As trustable encylopedic information is the unique value that Wikipedia provides, providing information as accurate as possible is in its mission.\nRetrieval-Augmented Generation (RAG) has become a common approach to address this challenge by combining the knowledge retrieval capabilities with the generative power of LLMs (Lewis et al., 2020). A significant limitation in conventional RAG models stems from the typically low semantic similarity between vector embeddings of natural language questions and typical document passages(Karpukhin et al., 2020). This disparity arises primarily from their distinct structural differences: questions are interrogative, while passages are predominantly declarative. For example, when querying with \"Where is Eiffel Tower located?\", the question's embedding is compared against passages like \"Eiffel Tower is located in Paris.\" Although \"Paris\" is semantically crucial, the contrasting sentence structures can lead to low cosine similarity scores, often falling within the range of 0.4-0.7, even for relevant content. This \"question-to-passage\" comparison, exacerbated by lexical differences and the differing focus of questions and passages, hinders high-precision retrieval, particularly for queries seeking specific information. Addressing this challenge requires strategies like question reformulation, passage summarization, or hybrid approaches that combine semantic search with keyword matching.\nFurthermore, while LLMs excel at generating human-like text, the generation step itself introduces the risk of hallucination, where LLMs may create plausible but factually incorrect answers. We argue that a retrieval mechanism that can precisely identify the relevant text and thereby removing the generation step to mitigate hallucination is a practical and useful approach in the context of Wikipedia.\nThis paper introduces a novel approach to knowledge-based question answering that addresses these limitations by performing \"question-to-question\" retrieval and avoids the generation step. Instead of embedding document content, we employ an instruction-tuned LLM to generate a comprehensive set of questions for each logical content unit within the knowledge base. These"}, {"title": "2 Background", "content": "The problem of Wikipedia question answering is as follows: Given a factoid question like \"Who is the architect of Eiffel Tower?\", \"How many people died in Chernobil accident?\", the retrieval system takes the user to a Wikipedia article's paragraph that answer's the question. Compared to common RAG techniques, there is no LLM based articulation of answer. We assume the question is extractive, in which the answer is available within such a paragraph and does not need to analyze content across multiple paragraphs or articles.\nAssume that our collection contains D documents, $d_1, d_2, \\dots, d_D$. We first split each of the Wikipedia article into text passages as the basic retrieval units and get M total passages in our corpus $C = {p_1, p_2, \\dots, p_M}$, where each passage $p_i$ can be viewed as a sequence of tokens $(w_1^{(i)}, w_2^{(i)}, \\dots)$. Given a question q, the task is to find a p that can answer the question. For the sake of quick and precice mapping between passage and article, assume there is a unique cryptographic hash based on the content of each p that is mapped to a d and saved in a relational database.\nA QA system needs to include an efficient retriever component that can select a small set of relevant texts(Chen et al., 2017). Formally speaking, a retriever $R : (q, C) \\rightarrow C_F$ is a function that takes as input a question q and a corpus C and returns a much smaller filter set of texts $C_F \\subset C$, where $|C_F| = k < |C|$. For a fixed k, a retriever can be evaluated in isolation on top-k retrieval accuracy, which is the fraction of questions for which $C_F$ contains a span that answers the question(Karpukhin et al., 2020).\nIn usual RAG approach, the retrieval process begins with a user's query being converted into a vector representation using a text embedding model. The knowledge base (e.g., a collection of documents or articles) is preprocessed by chunking large bodies of text into smaller, semantically meaningful units such as paragraphs or sentences. These chunks are also embedded into a vector space, often using the same embedding model that was used to embed the user query. The embeddings of these"}, {"title": "3 Methodology", "content": "We focus our research in this work on improving the retrieval component in open-domain QA. Given a collection of M text passages, the goal of our retriever is to index all the passages in a low-dimensional and continuous space, such that it can retrieve efficiently the top k passages relevant to the input question for the reader at run-time. Note that M can be very large(e.g., 6 million English Wikipedia articles multiplied by number of paragraphs in each) and k is usually small, such as 1-5. We used two distinct knowledge bases in our experiments: English Wikipedia and Wikidata. The English Wikipedia articles is parsed into a structured format to extract the content of each article. Articles are further divided into logical units, primarily paragraphs. Each paragraph is treated as an independent context unit for which questions are generated. A unique hash is computed for every paragraph which acts as the key to locate it in the original article. This ensures that when a question is retrieved, the associated original context can be found.\nThe Wikipedia content is primarily in Wikitext markup. It can also be rendered to HTML. But for the purpose of our system, we prepared plain text version of earch passage so that it is comprehensible for an LLM. We also removed reference numbers that appear in usual plain text format. Let $D = {d_1,d_2, \\dots, d_M }$ represent the knowledge base consisting of M content units.\nFor each content unit $d_i \\in D$, a set of questions is generated by an LLM, denoted as $Q_i = {q_{i1}, q_{i2}, \\dots, q_{in_i} }$, where $n_i$ is the number of questions generated for $d_i$. The prompt used for the LLM is given in Appendix ??. The input to the prompt not only contains the passage text, but contextual information like article title, section titles to resolve coreferences. The prompt also uses few shot prompting to get machine readable output.\nLet $\\E$ be the embedding function that maps a text input to a vector space, so $\\E : \\text{Text} \\rightarrow \\mathbb{R}^d$, where d is the dimensionality of the embedding space. The embedding of a question $q_{ij}$ is denoted as $e_{ij} = \\E(q_{ij})$, where $e_{ij} \\in \\mathbb{R}^d$. The set of all question embeddings is denoted as $V = {e_{11}, e_{12}, \\dots, e_{Mn_M } }$.\nWe create an index I that maps the embeddings $e_{ij}$ to the corresponding content unit $d_i$ via the hash function h, such that $I(e_{ij}) = h(d_i)$. The hashing function used is SHA-256 with 32 bytes length.\nLet $q_u$ be the user's query. The embedding of the user's query is $e_u = \\E(q_u)$.\nWe use cosine similarity $\\text{sim}(e_u, e_{ij})$ to compare the user query embedding with each of the generated question embeddings. The best match is chosen using argmax function:"}, {"title": "3.1 QA on Wikidata", "content": "To incorporate Wikidata, we needed to extract factual information from the knowledge base in a format that is suitable for the question generation task. We wrote a SPARQL query(See Listing 1) that retrieves all subjects, predicates, and objects (triples) associated with each Wikidata entity (QID). The query is designed to be comprehensive in extracting fact triples in a human-readable textual format (e.g., \"India: inception: 15 August 1947\" or \"India: Prime Minister: Narendra Modi (2014-current)\"). A unique hash of QID and PID pair is calculated for linking the extracted content with generated questions.\nLet Q be a Wikidata item (e.g., Q668 for India), and let $S_Q$ be the set of statements associated with item Q. Each statement $s \\in S_Q$ can be represented as a tuple s = (p, v, \\Pi), where:\n\u2022 p is the property (PID).\n\u2022 v is the value.\n\u2022 \\Pi is the set of qualifiers (optional).\nLet T be the function that maps a statement s to a textual representation t.\n$\\qquad T: s \\rightarrow t$\nThe function T involves:\n\u2022 Label retrieval for QIDs and PIDs.\n\u2022 Formatting of dates and times into human-readable text."}, {"title": "4 Discussion", "content": "Our approach offers several distinct advantages over traditional RAG systems:\n\u2022 Hallucination-Free Responses: The most important advantage is that the response is hallucination-free, since we directly retrieve the original content instead of relying on generated answers.\n\u2022 High Precision Retrieval: Comparing questions with questions leads to highly accurate results with very high cosine similarity scores.\n\u2022 Fast Retrieval Time: Due to the absence of LLM calls during inference time, query time is significantly lower than that of RAG based systems. This also implies significant cost savings.\n\u2022 Pseudo-multimodality: Retrieval of images, audio and other forms of data are handled seamlessly because we retrieve facts as structured text and treat them uniformly with other textual context."}, {"title": "4.1 Advantages", "content": ""}, {"title": "4.2 Limitations", "content": "The system focuses on direct fact retrieval (extractive). Answering more complex questions that require multi-hop reasoning, aggregation, or synthesis might require further advancements. For questions that need more than just one paragraph or fact, the system might fall short. While the diversity of"}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel question-to-question retrieval approach for open-domain question answering over Wikipedia and Wikidata, which achieves high precision and eliminates the risk of hallucination by avoiding the traditional generation step. Our approach leverages an instruction-tuned LLM to generate comprehensive sets of questions for each content unit in the knowledge base, which are then embedded and indexed for efficient retrieval. This method leads to cosine similarity scores consistently above 0.9 for relevant question pairs, demonstrating a high degree of alignment between user queries and the indexed content. By directly retrieving the original text and structured data based on the best matching generated questions, our system offers a fast, scalable, and reliable alternative to conventional RAG pipelines, with the added benefit of supporting pseudo-multimodal question answering."}, {"title": "A LLM Prompts", "content": ""}, {"title": "A.1 LLM prompt to generate all possible questions for a given passage", "content": "You are an expert question generator tasked with creating simple, natural-language questions from a given Wikipedia article that people might typically search on the internet based on a provided text passage.\nThe input will include:\n\u2022 Article Title\n\u2022 Section Title\n\u2022 Paragraph Text\nGuidelines for Question Generation\nUse the article and section titles to resolve any ambiguous references in the text Create questions that can be directly answered by the text Prioritize who, what, where, when, and how questions Ensure questions are simple, clear and concise mimicking common search engine query patterns Avoid yes/no questions unless the answer is explicitly stated in the text Avoid generating questions that require external knowledge not present in the text Avoid generating speculative or opinion-based questions Avoid very long questions that may be difficult to understand Include questions about:\n\u2022 Key concepts\n\u2022 Specific details\n\u2022 Important processes\n\u2022 Significant events or characteristics\n\u2022 Dates, places, and people\n\u2022 Questions that can be answered by current subject\nOutput Format\n\u2022 Provide a bullet list of questions\n\u2022 Each question should be a single, complete interrogative sentence"}, {"title": "A.2 LLM prompt to generate all possible questions for a Wikidata statement", "content": "You are a specialized question generation system. Your task is to convert knowledge triplets into natural questions. Each triplet follows the format: [Subject]: [Predicate] : [Object]\nGuidelines for question generation:\nTransform the predicate into an appropriate question word:\n\u2022 \"founded by\" -> \"Who founded\"\n\u2022 \"located in\" -> \"Where is\"\n\u2022 \"born in\" -> \"When was\"\n\u2022 \"invented\" -> \"What did\"\n\u2022 \"composed\" -> \"Who composed\"\nQuestion formation rules:\n\u2022 Start with the appropriate question word (Who, What, Where, When, How)\n\u2022 Place the subject appropriately in the question\n\u2022 End all questions with a question mark\n\u2022 Maintain proper grammatical structure\n\u2022 Preserve proper nouns and capitalization\n\u2022 Remove the predicate's passive voice if present (\"founded by\" -> \"Who founded\")\nHandle special cases:\n\u2022 Multiple objects: Generate separate questions for each object\n\u2022 Complex predicates: Break down into simpler components\n\u2022 Dates: Use \"When\" for temporal relations\n\u2022 Locations: Use \"Where\" for spatial relations"}, {"title": "B SPARQL Query for getting all statements for a given Qitem", "content": ""}, {"title": "C Example question-question matching for Wikidata", "content": ""}]}