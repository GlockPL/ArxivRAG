{"title": "Neural Network Interpretability With Layer-Wise Relevance Propagation: Novel Techniques for Neuron Selection and Visualization", "authors": ["Deepshikha Bhati", "Fnu Neha", "Md Amiruzzaman", "Angela Guercio", "Deepak Kumar Shukla", "Ben Ward"], "abstract": "Interpreting complex neural networks is crucial for understanding their decision-making processes, particularly in applications where transparency and accountability are essential. This proposed method addresses this need by focusing on Layer-wise Relevance Propagation (LRP), a technique used in explainable artificial intelligence (XAI) to attribute neural network outputs to input features through backpropagated relevance scores. Existing LRP methods often struggle with precision in evaluating individual neuron contributions. To overcome this limitation, we present a novel approach that improves the parsing of selected neurons during LRP backward propagation, using the Visual Geometry Group 16 (VGG16) architecture as a case study. Our method creates neural network graphs to highlight critical paths and visualizes these paths with heatmaps, optimizing neuron selection through accuracy metrics like Mean Squared Error (MSE) and Symmetric Mean Absolute Percentage Error (SMAPE). Additionally, we utilize a deconvolutional visualization technique to reconstruct feature maps, offering a comprehensive view of the network's inner workings. Extensive experiments demonstrate that our approach enhances interpretability and supports the development of more transparent artificial intelligence (AI) systems for computer vision applications. This advancement has the potential to improve the trustworthiness of AI models in real-world machine vision applications, thereby increasing their reliability and effectiveness.", "sections": [{"title": "I. INTRODUCTION", "content": "Understanding the decision-making processes of complex neural networks (NN) has become increasingly critical as these models are deployed in real-world applications such as autonomous driving, medical diagnostics, and financial forecasting. Interpretability is particularly crucial in these domains to ensure transparency, build trust, and facilitate regulatory compliance. Layer-wise Relevance Propagation (LRP) is a prominent technique aimed at interpreting the decisions of NN by propagating the output backward through the layers to the input [1]. This technique highlights which pixels or intermediate neurons significantly contribute to the final decision by generating relevance values (i.e., R), for each pixel or neuron, thereby enabling a detailed examination of the network's inner working process.\nDespite its effectiveness, LRP's practical application and visualization present significant challenges. Traditional methods do not provide clear and scalable interpretability, especially when dealing with large-scale NNs and high-dimensional data. To address these limitations, we propose a novel approach that leverages LRP's backward propagation capabilities to evaluate and visualize interpretability of convolutional neural networks (CNNs). Our approach enhances interpretability by identifying and highlighting the most relevant neurons and their paths through network layers.\nOur main contributions to this paper include:\n1) Generating NN graphs to identify significant paths (connecting activated neurons layer by layer);\n2) Developing algorithms for optimized path selection (the path follows the activation from the start to end the end);\n3) Creating visualization heatmaps for the selected k-paths (out of total n-number of paths to selected k-paths);\nIn this paper, we detail the fundamentals of LRP, the specific steps involved in its computation, and the application of our proposed methods to enhance interpretability in NN. We also introduce accuracy metrics such as Mean Squared Error (MSE) and Symmetric Mean Absolute Percentage Error (SMAPE) to evaluate our approach, along with a comprehensive algorithm for optimized path selection in neuron visualization.\nThe paper is organized as follows: Section 2 reviews related work on NN interpretability and LRP. Section 3 covers LRP, and the Visual Geometry Group 16 (VGG16) architecture. Section 4 outlines our proposed approach. Section 5 presents a case study based on VGG16. Finally, section 6 concludes with a summary of findings and future directions."}, {"title": "II. RELATED WORK", "content": "Interpreting NNs, especially CNNs, is a critical research area due to their complexity and extensive use. CNNs excel in image classification and segmentation tasks [2], and explainable AI (XAI) techniques in biomedical imaging [3]\u2013[5],"}, {"title": "III. METHOD AND MODEL OVERVIEW", "content": "In this section, we present an overview of the methodologies and models central to our research. We start with a discussion on Layer-wise Relevance Propagation (LRP), which is important for understanding NN decision-making processes. Next, we describe the VGG16 architecture, outlining its design principles and its application in image classification."}, {"title": "A. Layer-wise Relevance Propagation (LRP) Fundamentals", "content": "As mentioned earlier, LRP provides insights into NN decision-making by tracing the network's weights and activations from the forward pass and propagating the output relevance backward. This process ensures that the total relevance at the output layer equals the sum of the relevance (i.e., R) values at the input layer, maintaining the integrity of relevance scores throughout the network. Figure 1 illustrates this conservation property.\nFor a network output vector y of size M, corresponding to M classes, we select one class c to explain. The relevance of the output neuron for class c is set equal to its activation, while the relevance of all other output neurons is zero. The relevance scores $R_j$ for neurons $j$ in layer $k$ are computed using the following basic LRP rule:\n$R_j = \\sum_k \\frac{a_j w_{jk}}{\\sum_j a_j w_{jk}} R_k$\nwhere a denotes neuron activations and w represents the weights between neurons in consecutive layers. The numerator $a_j w_{jk}$ measures neuron j's influence on neuron k, and the denominator normalizes these contributions to ensure relevance conservation.\nThe LRP computation involves four main steps:\n\u2022 Forward Pass:\n$z_k = \\epsilon + \\sum_j a_j \\rho(W_{jk})$\nThis step aggregates influences for each higher-layer neuron. For ReLU layers, this step mirrors the regular forward pass.\n\u2022 Element-wise Division:\n$S_k = \\frac{R_k}{Z_k}$\nRelevance in the higher layer is divided by its total influence $z_k$ to maintain conservation.\n\u2022 Backward Pass:\n$C_j = \\sum_k \\rho(W_{jk}) S_k$\nComputes a quantity $C_j$ for every neuron in the preceding layer, reflecting the relevance passed down from the higher layer.\n\u2022 Element-wise Product:\n$R_j = a_j * C_j$\nRelevance is multiplied by neuron activation to determine the neuron's relevance score.\nDifferent propagation rules are applied based on the layer being analyzed:\n\u2022 Input Layer: The Deep Taylor Decomposition rule provides accurate relevance mapping using both the lowest and highest admissible pixel values:\n$R_i = \\frac{(x_i W_{ij} - l_i w_{ij} - h_i w_{ij})}{\\sum_i (x_i W_{ij} - l_i w_{ij} - h_i W_{ij})} R_j$\n\u2022 Lower Layers: The LRP-y rule generates smooth relevance maps with minimal noise, emphasizing positive contributions:\n$R_j = \\sum_k \\frac{a_j w_{jk}}{\\sum_j a_j w_{jk}} R_k$"}, {"title": "B. Visual Geometry Group 16 (VGG16) Architecture", "content": "The VGG16 is a CNN for image classification with a straightforward yet effective design, consisting of 16 layers: 13 convolutional and 3 fully connected (FC) layers (see Figure 2). Convolutional layers use 3x3 kernels with a stride of 1, followed by max-pooling layers to downsample feature maps. The flattened output is processed through three FC layers with 4096, 4096, and 1000 channels, respectively. VGG16 has approximately 138 million parameters and achieves 92.70% top-5 accuracy on the ILSVRC2014 dataset [16]- [17]."}, {"title": "IV. PROPOSED APPROACH", "content": "NNs process input data through multiple layers, each layer extracting specific features. To improve interpretability, we propose an approach that focuses on analyzing the relevance of detected features within these hidden layers by leveraging LRP and visualizing the most significant paths."}, {"title": "A. Neural Network Graph Generation", "content": "We first generate a NN graph to identify and highlight important paths within the network, as shown in Figure 3, by selecting the optimal path using Algorithm 1.\n\u2022 Path Identification: The longest paths in the graph are identified, which correspond to the most critical connections based on LRP relevance scores.\n\u2022 Path Visualization: These longest paths are highlighted in red, emphasizing their importance in backward propagation."}, {"title": "B. Visualization Heatmap", "content": "To further interpret the network's decisions, heatmaps are generated based on the selected paths.\n\u2022 Heatmap Generation: Heatmaps are created to correspond to the selected paths, providing a visual representation of the relevance scores as shown in Figure 4."}, {"title": "\u2022 Performance Metrics:", "content": "To assess the effectiveness of our approach, we utilize performance metrics such as MSE and SMAPE. We calculate these metrics to evaluate the accuracy of predictions and optimize the number of paths needed for prediction.\n\u2022 MSE: It measures the average squared difference between predicted and actual values.\n$MSE = \\frac{1}{n} \\sum_{i=1}^n (D_{pre} - D_{act})^2$\n\u2022 SMAPE: It provides a percentage-based accuracy measurement, offering a unit-free comparison of prediction errors.\n$SMAPE = \\frac{100\\%}{n} \\sum_{i=1}^n \\frac{|D_{pre} - D_{act}|}{(|D_{pre}|+|D_{act}|)/2}$"}, {"title": "C. Optimized Path Selection Algorithm", "content": "Our approach includes an optimized path selection algorithm designed to identify and visualize the most contributing neurons, as detailed in Algorithm 1:"}, {"title": "D. Optimizing Threshold and Visualization Techniques", "content": "In this subsection, we present our approach to determining the optimal threshold for path selection, which involves averaging MSE values to identify the most contributing neurons for predictions. Additionally, we utilize deconvolutional methods to reconstruct feature maps from input images, allowing us to visualize significant neurons through backward operations. This process is detailed in Algorithm 2 and Algorithm 3.\n1) Deconvolution Process\n\u2022 Set all activations to zero except the selected neuron."}, {"title": "2) Feature Map Reconstruction", "content": "\u2022 Create sub-networks for convolution and deconvolution layers.\n\u2022 Initialize deconvolution layers with the weights from corresponding convolution layers.\n\u2022 Store max locations for unpooling."}, {"title": "V. EXPERIMENTS", "content": "For our experiments, we used the ImageNet dataset [18], a widely recognized benchmark in image classification. ImageNet contains over 14 million labeled images spanning over 20,000 categories, offering a diverse and extensive resource for training and evaluating deep learning models. We experiment on the VGG16 model, pre-trained on ImageNet, to leverage its learned features for our interpretability analysis."}, {"title": "B. Case Study", "content": "To illustrate our approach, we consider an example where John, a researcher, analyzes a castle image from the ImageNet dataset using our method.\n\u2022 Initial Exploration: First, John examines the VGG16 model's predictions on the castle image, focusing on understanding which image regions most influence the model's decision.\n\u2022 Relevance-Based Analysis: He applies LRP to the VGG16 model to derive relevance scores for each neuron. Figure 6 presents the K-5 Path heatmap, highlighting areas of the image that significantly impact the model's prediction. These relevance scores show which parts of the castle-such as the towers, walls, and windows-are crucial for the model's decision-making.\n\u2022 Activation-Based Analysis: Next, John analyzes neuron activations in VGG16. Figure 7 displays the K-5 Path heatmap based on activations, showing how specific neurons respond to the castle image. This helps him understand the broader regions the network is focused on, including features like the overall shape of the castle and its structural elements.\n\u2022 Comparative Analysis: In Figure 5, John compares the original image, model predictions, and back-prediction graphs. He observes how the relevance and activation heatmaps align with the model's output. The comparative analysis allows him to see the overlap between the regions of high relevance and strong activation, providing insights into how VGG16 interprets and processes the castle image."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This paper explored LRP applied to the VGG16 architecture, focusing on feature contributions by analyzing neuron selections during LRP's backward propagation. We developed methods to generate NN graphs, visualize heatmaps, and reconstruct feature maps via deconvolution. Our findings show that precise neuron selection enhances NN interpretability, offering clearer insights into model processing.\nFuture work will extend this approach to architectures such as residual networks and transformer-based models to test its generalizability. Integrating LRP with other techniques such as SHAP or Grad-CAM could provide complementary insights. Automating neuron selection through optimization strategies and conducting user studies to evaluate visualization impact are also planned. Investigating the relationship between interpretability, model robustness, and fairness is a key area for advancing transparent and trustworthy AI systems."}]}