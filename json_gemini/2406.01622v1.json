{"title": "Sifting through the Noise: A Survey of Diffusion Probabilistic Models and Their Applications to Biomolecules", "authors": ["Trevor Norton", "Debswapna Bhattacharya"], "abstract": "Diffusion probabilistic models have made their way into a number of high-profile applications since their inception. In particular, there has been a wave of research into using diffusion models in the prediction and design of biomolecular structures and sequences. Their growing ubiquity makes it imperative for researchers in these fields to understand them. This paper serves as a general overview for the theory behind these models and the current state of research. We first introduce diffusion models and discuss common motifs used when applying them to biomolecules. We then present the significant outcomes achieved through the application of these models in generative and predictive tasks. This survey aims to provide readers with a comprehensive understanding of the increasingly critical role of diffusion models.", "sections": [{"title": "Introduction", "content": "Diffusion probabilistic models (or more simply diffusion models) have attracted attentions from researchers and the public alike for their success in image generation. After usurping generative adversarial networks (GANs) in sampling high-quality images [1, 2], we have seen many diffusion-based image synthesis models introduced. Diffusion models have also been successfully applied to problems in computer vision [3], audio generation [4, 5, 6], robotics [7, 8, 9, 10, 11], and many others.\nDiffusion models are a category of deep generative models, which seek to easily sample some underlying distribution p(x). Sampling a distribution can be difficult in cases where the distribution has many modes or is in a high-dimensional space. This problem can be especially prevalent when working with thermodynamic systems, where energy barriers can prevent adequate exploration of the distribution. Non-machine learning techniques have relied on expensive search methods, such as Markov chain Monte Carlo (MCMC) simulations, to accurately sample a distribution [12, 13, 14, 15]. Diffusion models instead sample a tractable prior (a normal distribution, for example), which is then transformed into the correct distribution. They do this by adding Gaussian noise to the data distribution and then learning how to iteratively remove the noise. This iterative denoising breaks down the generation process into simpler, learnable steps and allows sampling from very high-dimensional, rough distributions.\nThis ability makes diffusion models an attractive choice for the modeling of biomolecules. Problems in the field typically result in very high-dimensional data that have rough distributions. Levinthal's paradox [16] famously noted this difficulty with the protein folding problem: put simply, if one were to search every possible conformation of a protein for its native strucutre, it would take longer than the age of the universe (even if checking the state only took on the order of picoseconds). The outlook worsens when one considers that the energy landscape may have several meta-stable states that are not the native structure. Because of this, straight-forward calculations are usually intractable, and deep learning methods have provided tremendous leaps forward in predictive tasks. Most saliently, AlphaFold2 has been able to provide experimental accuracy of the native folded structures for many proteins [17]. Diffusion models benefit from scalable deep learning architectures and can circumvent the issues of dimensionality through the iterative process. Furthermore,"}, {"title": "Preliminaries on diffusion models", "content": "Early diffusion models were heavily inspired by techniques from thermodynamics and statistics. The common observation among them was that it is possible to progressively transform a normal distribution into a given data distribution. However, the theories behind this transformation differed. Some viewed this process as learning to reverse a noising process, where the model progressively removes noise from a sample. Others thought about this process as accurately sampling a continuous family of perturbed distributions. Both theories can be unified into a flexible framework where the diffusion is described by continuous processes. In this section, we briefly recall the early work on diffusion models and the main results."}, {"title": "Denoising diffusion probabilistic modeling", "content": "Inspired by techniques in nonequlibrium thermodynamics and statistics [19, 20], Sohl-Dickstein et al. first proposed diffusion models as a way to develop probabilistic models that are flexible enough to capture complex distributions while providing exact sampling [21]. The main idea behind the algorithm was to start with samples from the desired distribution and iterative inject Gaussian noise until approximately reaching a stationary standard normal distribution. Training the model then comes down to learning how to reverse the noising process and recover the initial distribution.\nMore concretely, we want to approximate the distribution of data given as X0 ~ Pdata (x0). The forward process is a Markov chain whose transitions are given by Gaussian distributions according to a variance schedule 0 < \u03b2\u2081,..., \u03b2\u03c4 < 1:\nq(xt | xt-1) := N(xt; \u221a1 \u2013 \u03b2txt\u22121, \u03b2\u0399).\nThe transition kernel can be given exactly as\nq(xt | xo) = N(xt; \u221aatxo, (1 \u2013 at)I),\nwhere at \u03a0t =1(1-\u03b2t), and so the exact distribution at time t is\nq(xt) =\n\u222b\nq(xt | X0) Pdata (x0) dxo."}, {"title": "Noise conditional score network", "content": "In [23], the authors approach the problem of sampling from an unknown distribution pdata by estimating its (Stein) score \u2207x log Pdata(x) at varying noise levels. The score is related to ideas from statistical mechanics. Namely, if we have an energy function E(x), then the associated Boltzmann distribution of the data is given according to\np(x) = exp(-E(x))/Z\nwhere Z is a normalizing factor that is typically intractable to compute. Thus determining the score of the distribution is essentially equivalent to finding -VE(x), from which Langevin dynamics of the system may be computed.\nTheoretically, approximating \u2207x log Pdata would allow sampling of the distribution directly through sim-ulations. However, as Song et al. note, there are two major challenges with this approach:\n(1) Typically data lies on a low-dimensional manifold in the ambient space, in which case the score may be undefined or difficult to accurately approximate.\n(2) There may be low-density regions of the distribution. This results in poor initial sampling of the distribution in those spaces making it harder to estimate the score. Furthermore, when sampling with Langevin dynamics, slow mixing can occur leading inaccurate estimates of the distribution in the short term.\nBoth challenges can be addressed by instead estimating the scores of perturbations of Pdata. The perturbation kernel is defined as\nqo (x | x') := N(x; x', \u03c3\u00b2\u0399),\nso that the data distribution is perturbed to\nqo (x) =\n\u222b\nq(x | x')Pdata(x') dx'.\nThe aim of the model is to approximate the score \u2207x log qo (x) for increasing levels of noise 5min = 01 < 02 <\u2026\u2026 <\u03c3\u03b7 = omax. The levels of noise are chosen so that qomin (x) \u2248 Pdata(x) and qomax (x) \u2248 N(x; 0, I). Thus there will be a family of distributions ending with pure noise and progressively getting closer to the desired distribution."}, {"title": "Score based modeling with stochastic differential equations", "content": "Previous diffusion models used discrete Gaussian noising process to transform their distributions into stan-dard normal distributions. However, modeling the noising process as a stochastic differential equation (SDE), as suggested in [24], has many advantages. In particular, the framework is flexible and generalizes DDPMs and NCSNs as discretizations of different equations.\nThe forward diffusion process is modeled with an It\u00f4 SDE:\ndx = f(x, t)dt + g(t)dw,\nwhere w is a standard Wiener process, f : Rd \u00d7 [0, \u221e) \u2192 Rd is the drift coefficient and g : [0, \u221e) \u2192 R is the diffusion coefficient. The initial condition xo has the distribution of the data, po(x), and it is assumed that for a sufficiently long time T that xy is approximately distributed as the stationary solution of eq. (1).\nTaking the step sizes for DDPMs and NCSNs to zero gives a continuous version of the models that we can write as SDEs. The DDPM converges to the SDE\n dx = -B(t)xdt + \u221a\u1e9e(t)dw,\nwhich we refer to as the variance preserving (VP) SDE. Also, NCSNs converge to the SDE\n dx = \u221ad[r(t)\u00b2] dw,\nwhich we refer to as the variance exploding (VE) SDE.\nIf the score \u2207x log pt (x) is known, then the diffusion process can be reversed by the following SDE:\n dx = [f(x, t) - g(t)\u00b2\u2207x log pt(x)] dt + g(t)dw,\nwhere w is a standard Wiener process when time flows backward from T to 0. Thus the goal is to train a score network se(x,t) to approximate \u2207x log pt (x). A typical objective for training is by again using denoising score matching:\n Et{x(t)Epo(x')Epo,t(x|x') [||sa (x, t) \u2013 Vx log po,t(x | x')||2] }.\nOther score-matching objectives can also be used here, such as sliced score matching [25]."}, {"title": "Motifs for diffusion applied to biomolecules", "content": "Due to the geometry of biomolecules and the common goals of researchers, when diffusion is applied to problems in the field several techniques and ideas come up often. Some of these are standard notions for diffusion generally, but others are more niche. This section covers ideas that appear frequently in research with the aim of highlighting the most important concepts when engaging in the literature."}, {"title": "Controllable generation", "content": "Unconditionally sampling from the learned distribution of a diffusion model is usually insufficient for appli-cations. For example, in protein design, one may desire not just a protein sequence and conformation but also that the final structure satisfies some prescribed function. The SDE formulation of diffusion models allows for a straight-forward description of controllable generation. Given an observation y, the goal is to sample from the distribution of x(0) conditioned on y, i.e., the distribution po(x(0) | y). The score of this distribution when perturbed can be rewritten using Bayes' rule to get\nVx log pt (x(t) | y) = \u221ax log pt (x(t)) + \u221ax log p(y | x(t)).\nTypically, \u2207x log pt (x(t)) will be approximated by a score network and so \u2207x log p(y | x(t)) will need to be estimated.\nOne option is to train another network to approximate this term. Training of the networks can often be done simultaneously so that the conditioned and unconditioned scores can be learned together. In some cases, domain knowledge or heuristics can be used to get an approximation. In the particular case of in-painting, the replacement method has been used to approximate the conditional gradient. However, as pointed out in [32], this leads to an irreducible error, but a similar method using particle filtering can guarantee an approximation with the correct limiting distribution."}, {"title": "Equivariant/Invariant score networks", "content": "For many problems involving biomolecules, the space is three-dimensional Euclidean space (that is, R\u00b3) and the results should not depend on the choice of coordinate axes. For example, when predicting the native structure of a protein, two conformations are equivalent if one can be transformed into the other by translations and rotations. To be precise, the solution should be equivariant (or invariant) to rigid motions. A function f: X \u2192 Y is equivariant with respect to a group G if for each g\u2208 G and x \u2208 X\nf(g.x) = g. f(x),\nwhere gx and g. f(x) represents a group action on X and Y, respectively. A function is invariant if f(g.x) = f(x) for each g\u2208 G and x \u2208 X. Restricting machine learning architectures to the smaller space of equivariant functions speeds up training and makes models robust to such transformations. Thus, networks that are equivariant with respect to the special Euclidean group SE(3) are a natural choice for problems involving biomolecules\u00b9.\nA common choice for incorporating equivariance into diffusion models is to use an equivariant network for se(x,t). Given that the diffusion process and the score are both equivariant with respect ot SE(3), the reverse diffusion and sampling processes are also equivariant (c.f. [26, Prop. 1] for instance). Common choices of architecture for the score network include Invariant Point Attention [17], Tensor Field Networks [37], and Equivariant Graph Neural Networks [38], among other."}, {"title": "Diffusion on manifolds", "content": "While equivariance provides numerous benefits when designing diffusion models, it may also lead unrealistic generation of molecules. This is due to the chirality of some biomolecules, which is not preserved under reflections of R3. Furthermore, for molecules with n particles, the effective number of degrees of freedom is typically much smaller than 3n. For instance, bond lengths and angles are usually fairly rigid in molecules and thus contribute little to the diversity of conformations. Thus, one can imagine the configurations of a biomolecule as lying on a smaller submanifold of the ambient space, and by defining coordinates on that manifold the configuration of the molecules can be specified. It is possible to define the diffusion process on these new coordinates, which reduces the dimension of the problem while enforcing certain geometric priors.\nScore-based diffusion models may be carried over to Riemannian manifolds while retaining many of the characteristics of the models from Cartesian space [28]. A nautral choice for forward diffusion on compact"}, {"title": "Diffusion on T: torsional space", "content": "The manifold T can be used to represent angular data, with the points being associated with the interval [0,2\u03c0) [29]. The conformation for many molecules can almost entirely be described by their torsional angles, and so modeling the diffusion process on Tm can effectively reduce the dimensionality without a significant loss of accuracy.\nFor the VE formulation the transition kernel on Tm is given by the wrapped Gaussian:\n\u03a3 exp(-\u03c0\u03c0/2\u03c3(t) 2 )\n2\u03c0\u03ba2/k\u2208Zm/Pt(TT') x."}, {"title": "Diffusion on SO(3): orientations in three dimensions", "content": "The manifold SO(3) is the set of 3 \u00d7 3 orthogonal matrices with determinant 1, and so they are used to represent rigid rotations of R\u00b3 or the orientation of three-dimensional objects. Diffusing on SO(3) can be useful during generation when the structure being diffused is not spherical and properties are dependent on its orientation - for example a ligand or the residue of a protein. The transition kernel for the VE formulation is given by the IGSO(3) distribution [39], which easily computable via a series expansion."}, {"title": "Direct product of manifolds", "content": "More complicated manifolds can be built from the direct product of simpler manifolds. The VE forward process diffuses each coordinate independently, and so for direct product the diffusion can be carried out separately on each component manifold. This allows for diffusion across multiple coordinates: for instance, diffusing the position and orientation of n particles amounts to diffusing on the manifold R\u00b3n \u00d7 SO(3)n, for which we know each of the transition kernels."}, {"title": "Low-temperature sampling", "content": "A common problem with training generative models is overdispersion. When this occurs, the coverage of the modes of an underlying distribution is emphasized more than sample quality, leading to poor quality in the samples. This can be particularly deleterious when using generative models for predictions. It is common to use modified sampling to sacrifice diversity for higher-quality. See for instance shruken encodings in normalizing flows [40].\nLow-temperature sampling of a distribution is an attractive way to combat overdispersion. That is, if p(x) is the original distribution, sampling instead from p(x)^, where Z is a normalizing factor and \u5165 > 0 is an inverse temperature parameter, will emphasize high likelihood areas of the distribution. Taking X \u2192 \u221e will lead the distribution to concentrate around the global maximum of p(x). This process is analogous to lowering the temperature for a thermodynamic system so that enthalpy is emphasized over entropy. However, low-temperature sampling in many cases is intractable or requires expensive Markov chain Monte Carlo (MCMC) simulations to compute.\nFor diffusion models, it is tempting to simply increase the score and decrease the noise, but doing this does not give the correct distribution and is generally ineffective [2]. Ingraham et al. demonstrate how to derive an approximate scheme in the case of a normal distribution [41]. One can then approximate the perturbed score function Sperturb by multiplying the original score function s by a time-dependent constant A: that is,\nSperturb (x, t) \u2248 dts(x, t).\nReplacing s with Sperturb leads to new reverse diffusion SDE that one can use to sample the low-temperature distribution. This approximating only holds in the case of a Gaussian distribution for Pdata, and so applying the rescaling to arbitrary distribution will not lead to a proper reweighting. However, mixing the low-temperature reverse SDE with a Langevin SDE allows for proper sampling of a generic distribution. For more details, we refer readers to [41, Appendix C]."}, {"title": "Applications", "content": "Applications for diffusion models can be roughly divided into two categories: generation/design and pre-diction. Methods between these two categories are similar, but the goals are different in terms of sampling the underlying distribution. For generative applications, the goal is to faithfully sample the distribution. This means not only having accurate samples but also having good coverage of all modes. Design tasks are usually downstream from generation task, where one conditionally samples the distribution of interest. For predictive applications, the goal is to sample the most likely point in the probability distribution. This tends to correspond with an optimization goal, e.g., minimizing the total energy of a system."}, {"title": "Generative modeling", "content": "Diffusion models are particularly useful for generating samples from a high-dimensional distribution, and so it is natural to apply them to the problem of creating plausible molecular structures. Furthermore, conditional diffusion allows for additional constraints to be added to the generation process, allowing for the design of proteins. Despite the recentness of the techniques, there have already been significant results. Many results focus on generating the backbone of the protein sequence (for example the locations of the Ca atoms), and so this direction will be the main focus of this section."}, {"title": "Benchmarking", "content": "The ability of generative models to effectively sample the desired distribution is typically measured in three aspects: sample quality, sample diversity, and novelty. Sample quality looks at whether the generated samples are likely to come from the underlying distribution. Sample diversity looks at whether the distribution of samples reasonably spans the entirety of the underlying distribution. Novelty is a measure of how well the model generalizes from the training set; a model that simply memorizes the training data would do well with quality and diversity, but would not be able to produce new samples from the distribution.\nTo measure the quality of generated protein structures, one needs to determine whether a polypeptide chain has a native conformation matching the generated structure. By design, the generated structures are novel and thus cannot be directly compared with previous structures. Validating these structures experi-mentally is time-intensive, and so in silico methods for determining the structure \u2013 such as AlphaFold2 [17] and ESMFold [69] \u2013 are employed to evaluate the samples. Self-consistency of a method is the measure of whether a sequence can be found and independently folded into the generated structure. For many models, no sequence is generated and so a reverse folding method, such as ProteinMPNN [70], must first be used to find a number of plausible sequences. The self-consistency score is computed by comparing the folded struc-ture with the generated structure and taking the closest comparison. The self-consistency TM (scTM) score computes the TM score between the generated backbone and each folded backbone and returns the highest"}, {"title": "Backbone generation models", "content": "Trippe et al. created a generative model for the Ca backbone of proteins with a specific focus on controllable generation for the motif-scaffold problem [32]. Diffusion was carried out on the Cartesian coordinates and used an E(3) equivariant score network. Despite producing plausible protein structures, generally the back-bones were not designable. Furthermore, the equivariance of the network to reflections produced incorrect left-handed a-helices, and so the model did not respect the chirality of the proteins. Wu et al. diffused on the internal angles of the protein backbone in order to create a generative model that respects chirality [44]. Again, the generated proteins were plausible and had distributions of secondary structures similar to native proteins, but most generated backbones were not designable.\nProteinSGM was one of the first models to be able to generate highly designable protein backbones [45]. The diffusion is carried out on the six-dimensional space of inter-residue coordinates as defined in trRossetta [71], using the VE SDE formulation. Protein backbones are obtained from coordinates using an adaptation of the trRossetta minimization method, which enables generation of realistic structures. ProteinSGM is able to generate proteins up to length 128 with scTM of 90.2%, versus 11.8% and 22.7% as reported by [32] and [44], respectively.\nIngraham et al. developed Chroma, which generates backbones by diffusing on the Ca atoms [41]. The backbones generated are more likely to be designable than the previously mentioned methods, with around 50% being designable by their criterion. Notable features of their model include (1) choosing a covariance structure of the initial distribution to replicate the statistics of realistic protein backbones (such as radius of gyration), (2) a random graph neural score network to maintain subquadratic computational time, and (3) an annealed sampling scheme that allows the model to sample from p(x)^ without retraining. Furthermore, the programmability of the model and code is emphasized in the paper. That is, several conditioners are created to allow for a wide range of conditions for protein generation, such as on the three-dimensional shape and point symmetries of the final structure. The generated protein structures are diverse and cover all of natural protein space.\nOther recent attempts at diffusion models for protein generation carry out the diffusion process on the coordinates and orientations of the backbone atoms. That is, for n atoms, diffusion will be carried out on R\u00b3n \u00d7 SO(3)\" ~ SE(3). This allows the models to reason about the three-dimensional structure of the protein during the generation step while retaining angular information between residues so that chirality is respected. This paired with SE(3) equivariant networks like IPA have been particularly successful.\nThe most salient example of this method is RFdiffusion, which is considered the gold standard of backbone generation [35]. As argued by the authors, previous models lacked deep understanding of protein structure, and so taking advantage of structure predictions methods could allow for better sample quality. In particular, pretraining using the weights from RoseTTAFold [72] provided an enormous benefit to the diffusion model. RFdiffusion has high designabiltiy of unconditionally generated proteins, with independent benchmarking showing over 95% of moderately sized backbones being designable [73, 43] and some of the protein designs being experimentally verified [35]. Furthermore, the model is capable of conditional generation of proteins, such as producing oligomers with point symmetries and functional-motif scaffolding. Recent work has been done on expanding RFdiffusion's capabilities to more general biomolecular tasks. RoseTTAFold All-atom (RFAA) [42] expands RoseTTAFold's by including atomic-level inputs for other biomolecular components. This allows RFAA to more accurately predict protein structure in the context of assemblies of molecules. RFdiffusionAA (similar to the original RFdiffusion) uses the pretrained weights of RFAA to develop a\""}, {"title": "Other generative models", "content": "Although generating proteins based on backbones coordinates is a popular choice, other approaches have been attempted. One deficit to focusing on coordinates is that conditional generation based on sequence information is not possible: the backbone methods above only consider structural information and obtain the sequences via a reverse folding method. Thus diffusing in sequence space could be useful for conditioning on sequence information. Up to this point, we have only discussed diffusion on continuous spaces (either Euclidean space or a manifold), while diffusing on sequences requires being in a discrete space. One option for bringing diffusion to discrete space is to simply embed categorical information into continuous (e.g. using a one-hot encoding scheme) and then performing a traditional diffusion. A more principled options is to create a diffusion scheme for discrete data. For instance, Discrete Denoising Diffusion Probabilistic Models (D3PMs) describe a corruption process for discrete data that is analogous to Gaussian noise [74].\nThis more principled approach was taken by the authors of EvoDiff, which is a protein sequence generation model [55]. EvoDiff carries out discrete diffusion in a method similar to D3PM, but the authors found it"}, {"title": "Molecular dynamics and ensemble generation", "content": "Molecular dynamics simulations is an important tool for computational biology. It sees uses in the refinement of predictions and the study of dynamics of biomolecules [80, 81, 82, 83, 84, 85, 86]. Typically, simulations are carried out using Langevin dynamics and a prescribed potential energy function. However, computation of a conventional energy function is expensive (usually quadratic in the number of particles), and so simulating the dynamics on a suitably long time-scale is infeasible. Generative models have the ability to speed up these computations by directly sampling the distribution of future states conditioned on the current state. This has been applied to the simulation of molecules with diffusion models and other generative models [87, 88]. However, this direction has not been thoroughly explored for the simulation of protein dynamics.\nLu et al. made progress in this direction with the introduction of Str2Str, a model for sampling protein conformations [49]. The method shares similarities to simulated annealing, where the temperature for MD simulations is raised to overcome energetic barriers. Here a structure is perturbed by diffusion, and then the process is reversed to sample a new conformation of the protein. The noising process is carried out in SE(3) as described by Yim et al. [46] and the score network is given by a variant of IPA [17]. Str2Str is able to recapture many of the statistics of the equilibrium distributions of fast-folding proteins in negligible amount of time compared to direct MD simulation (510 GPU seconds versus over 160 GPU days, respectively.)\nA similar approach was carried out by Zheng et al. with the development of the Distributional Graphormer (DiG) [50]. This model samples the equilibrium distribution of molecular systems using a diffusion model, where the score architecture is given by a Graphormer. In this work, the equilibrium distribution of proteins is used during training and validation. Again, diffusion is carried out over frame data in SE(3) using the VE formulation of the SDE. One notable addition to the typical diffusion model is the use of physics-informed diffusion pre-training (PIDP). Since MD simulation are expensive to compute, the authors sought a method to utilize prior energy functions to train the score model. The main idea is to enforce that the score network satisfies the corresponding Fokker-Plank equation - a partial differential equation which governs the value of the score through the diffusion process and the initial condition given by the gradient of the prior energy. The PIDP does not require samples from the equilibrium distribution since information from the energy function will enforce the correct distributions. A tractable number of samples can be selected during this process since the equilibrium distribution lies on a much lower dimensional manifold than the latent space. DiG is shown to recapture protein equilibirum distributions, as well as generate plausible transition paths between conformations.\nA related direction is the training of coarse-grained models for proteins. The free-energy landscape for an all-atom representation of a protein is rough and difficult to simulate, so coarse-grained models look to simplify the simulations by combining particles. The training of coarse-grained models is typically done"}, {"title": "Predictive modeling", "content": "Diffusion models being generative models are traditionally designed to sample the whole probability dis-tribution they are trained on. However, there has been success with using diffusion models to predict the solution to regression and/or optimization problems with a single solution. Non-generative machine learning methods sometimes struggle to account for uncertainty and thus try to minimize the expected error at the cost of realistic predictions (see [33, \u00a73] for an example). A generative model instead handles the uncertainty by sampling over all possibilities, giving a more accurate representation of the likely solution.\nFor generating a single prediction, one can repeatedly sample from the diffusion model and then choose the \"best\" sample. A typical strategy for making the final choice is training a separate confidence model which would rank each of the samples based on the confidence of being the optimal solution. A training-free alternative would be to use a likelihood estimate or evidence-based lower bound (ELBO) to determine the most likely sample from the distribution.\nFollowing this strategy, several diffusion models have been applied to prediction problems and have achieved state-of-the-art results."}, {"title": "Protein side-chains", "content": "The protein side-chain packing problem focuses on the case where the backbone of a protein is fixed and asks for the positions of its side-chain atoms. Traditional methods for solving the problem involve searching the space of positions for the side-chains using rotamer libraries and comparing energies [90, 91, 92, 93, 94, 95, 96, 97]. Due to the complexity of the energy landscape and the number of possible conformations of the side-chains, this process can be time-intensive. Machine-learning methods aim to speed up this process while maintaining precision. Methods such as AttnPacker [98] predict the positions of the side-chain atoms but do not account for bond lengths and angles. Other machine learning methods treat side-chain packing as a regression problem and do not model the diversity of possible conformations in the energy landscape [99, 12].\nDiffPack is a diffusion-based model for side-chain packing that predicts the torsional angles, X1,X2,..., of each side-chain [65]. Ideal bond lengths and angles are assumed for the side-chain atoms, so that the degrees of freedom are greatly reduced and the samples produced have natural bond lengths and angles. The diffusion process is carried out on Tn, where a sample corresponds to a choice of torsional angles for the side-chains. One notable change in the diffusion process is that the angles are sampled autoregressively: X1 is first sampled, followed by X2 and so on. Since the value of X1 affects the coordinates of atoms further down the chain, sampling all angles simultaneously can lead to excess steric clashes due to the sensitivity of earlier angles on coordinates. Using an autoregressive generation process mitigates this effect because the model can self-correct for earlier choices of angles. A confidence model is trained along side the diffusion model in order to score the sampled conformations. During inference, multi-round sampling and annealed temperature sampling are used to choose conformations with lower energies. DiffPack is able to achieve state-of-the-art performance in the mean absolute error of the predicted torsional angles when compared to other machine learning approaches to the side-chain packing problem. Diffpack's reported results are given in tables 2 and 3. For details on the benchmarking, see appendix A.2.\nA related problem to side-chain packing for apo protein structures is determining the conformations of"}, {"title": "Protein structure prediction", "content": "Diffusion models have also been applied to the problem of predicting the native structure of proteins. In particular, EigenFold predicts the coordinates of the Ca atoms of a protein [67]. Diffusion is carried out on the Cartesian space R\u00b3n, but the diffusion process is defined so that the prior distribution uses a Harmonic potential for the relative distances of points. This guarantees that diffused points do not separate too far and that relative distances more closely reflect observed bond lengths. Thus the prior distribution models more realistic molecular conformations, but the stiffness of the diffusion process requires the SDE be carried out with a basis of eigenvectors. An estimated ELBO is used in lieu of training a score model. The results reported in table 4 are modest compared to established machine learning approaches such as AlphaFold2 and ESMFold [17, 69] (see appendix A.3 for details). One benefit to using a generative model over a single-structure prediction methods is that generative models can naturally capture conformational diversity of a protein. If a protein is flexible, then sampling the distribution of conformations should result in different possible poses. Eigenfold shows some moderate results in this direction, with a slight correlation between the diversity of samples and the flexibility of the protein. Overall, the model demonstrates the possibilities of diffusion models to the problem of predicting native structures.\nA recent development in protein-structure prediction was the announcement of AlphaFold 3 (AF3) [36]."}, {"title": "Inverse protein folding", "content": "As opposed to structural prediction methods, the inverse folding problem looks at determining an amino acid sequence given a protein's Ca backbone. GraDe-IF applies a discrete diffusion model to the inverse folding problem [68]. The diffusion is carried out in the categorical space of amino acid types using the D3PM framework [74]. Notably, the authors use the Blocks Substitution Matrix (BLOSUM) [103] to create the transition matrix for the noising step. This allows the model to explore closely related amino acids during the latter stage of the denoising process instead of random, unrelated residue types. The score network is given by an adaptation of an equivariant graph neural network (EGNN) [38] to account for the roto-translational invariance of the problem. The model outperforms other inverse folding methods in the authors' benchmarking as reported in table 5. Details of the benchmarking can be found in appendix A.4."}, {"title": "Protein complexes", "content": "Ligands bind to proteins and modulate their biological function, so an important step in drug design is determining where a ligand will bind to a protein. Traditionally, computational methods for this problem search for the ligand position that directly optimizes a score function [100, 14, 104", "106": ".", "33": "instead treats the finding the ligand pose as a generative problem. Put simply, Diff-Dock samples possible ligand poses from an underlying distribution and then makes an optimal choice based on a learned confidence score. This circumvents the ruggedness of the energy landscape by enumerating multiple options instead of averaging possibilities together as done in regression-type approaches. DiffDock uses a diffusion model to sample different poses and a separate score model to determine the confidence that the RMSD is below 2 \u00c5. The dimensionality of the search space is reduced by fixing the conformation of the protein and assuming ideal bond lengths and bond angles in the ligand. Thus the degrees of freedom are the position, orientation, and dihedral angles of the ligand. Then sampling a ligand pose is the same as sampling from the manifold R\u00b3 \u00d7 SO(3) \u00d7 Tm, where m is the number of torsional angles. DiffDock was benchmarked on the PDBBind dataset [107", "60": ".", "108": ".", "110": ".", "111": "."}]}