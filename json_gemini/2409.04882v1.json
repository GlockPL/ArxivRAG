{"title": "Learning to Open and Traverse Doors\nwith a Legged Manipulator", "authors": ["Mike Zhang", "Yuntao Ma", "Takahiro Miki", "Marco Hutter"], "abstract": "Using doors is a longstanding challenge in robotics and is of significant\npractical interest in giving robots greater access to human-centric spaces. The task\nis challenging due to the need for online adaptation to varying door properties and\nprecise control in manipulating the door panel and navigating through the confined\ndoorway. To address this, we propose a learning-based controller for a legged\nmanipulator to open and traverse through doors. The controller is trained using\na teacher-student approach in simulation to learn robust task behaviors as well as\nestimate crucial door properties during the interaction. Unlike previous works,\nour approach is a single control policy that can handle both push and pull doors\nthrough learned behaviour which infers the opening direction during deployment\nwithout prior knowledge. The policy was deployed on the ANYmal legged robot\nwith an arm and achieved a success rate of 95.0% in repeated trials conducted in\nan experimental setting. Additional experiments validate the policy's effectiveness\nand robustness to various doors and disturbances. A video overview of the method\nand experiments can be found at youtu.be/tQDZXN_k5NU.", "sections": [{"title": "1 Introduction", "content": "Legged manipulator robots offer significant potential in combining the ability of legged robots to\nnavigate varied environments with the potential for rich interactions afforded by the manipulator.\nAn essential skill for these robots is the autonomous opening and traversal of doors, significantly\nexpanding their reach in human-centered environments. While door opening seems a quotidian task,\nit poses a challenge for control, especially for a high degree-of-freedom system such as a legged\nmanipulator. Moreover, doors can vary in ways that are not immediately observable, such as in their\nspring stiffness and whether they are push or pull. The task becomes more difficult when the robot\nmust also pass through the door as it must make additional decisions such as when to disengage\nfrom the handle or if it needs to hold the door open against the door's self-closing mechanism."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Model-Based Door Opening", "content": "Several works studied door opening for a manipulator on a wheeled base by tracking predefined\nreference velocities and forces [7, 8, 9, 10]. For legged manipulators, Model Predictive Control\n(MPC) has been applied for door opening [11, 2, 12]. MPC controllers rely on tracking a reference\ntrajectory, which necessitates using a planner and requires knowing the properties of the door such as\nits opening direction and precise dimensions. As such, the practical applicability of such controllers\nto unseen doors is limited. The commercially available Spot robot with the arm manipulator includes\na door opening controller. While its details are not public, it is likely a model-based controller.\nAccording to available documentation [4], Spot's controller must be provided information a priori\nsuch as the door's opening and swing directions."}, {"title": "2.2 Learning-Based Door Opening Control", "content": "RL is a natural approach for door opening as specifying the desired behavior through rewards for\nopening doors is intuitive. Several works used door opening as a benchmark task for RL algo-\nrithms [13, 14, 15]. Urakami et al. [1] developed the DoorGym simulation environment for RL\ndoor opening with a specific focus on robustness and generalization via domain randomization and\ndemonstrated their trained policies in hardware experiments on a fixed-based manipulator. Schwarke\net al. [3] trained and deployed an RL door opening policy on a wheeled-legged robot. However, these\nworks produce control policies that can only handle a single opening direction as they treat push and\npull doors as separate RL tasks. The works above have focused on opening the door and ignor-\ning the subsequent task of passing through. Ito et al. [16] demonstrated a learning-based approach\nfor opening and passing through doors with a wheeled base mobile manipulator. Their approach\ncombines separate control modules for handling opening and passing through. However, the robot\nlearns its motions from human demonstrations via teleoperation which is difficult to scale to differ-\nent doors. Moreover, the method requires two separate modules for handling push and pull doors."}, {"title": "2.3 Teacher-Student Distillation", "content": "Recent successes in applying teacher-student training to legged robot locomotion [17, 18] have\ndemonstrated remarkable robustness and the student policy's ability to estimate properties of the\nenvironment. For example, in the locomotion task, the student policy can infer the height of the\nlocal terrain, even in the presence of corrupted exteroceptive sensor measurements by leveraging the\nknowledge of the feet's position. For door opening, the information necessary to complete the task\ncan only be acquired through direct interaction with the door (e.g. push or pull). Given this, the\nteacher-student approach could be particularly beneficial for this task."}, {"title": "3 Method", "content": "An overview of our method is shown in Fig. 2. Both\nteacher and student policies are trained in a simulation\nenvironment in Isaac Gym [19] as shown in Fig. 3. The\nrobot modeled in simulation and used in real-world ex-\nperiments is ANYmal with an arm manipulator [20]."}, {"title": "3.0.1 Door Model", "content": "We focus on hinged doors that consist of a single door\npanel with a handle for unlocking the door. The door\nhas two degrees of freedom corresponding to the hinge\nand handle angles 0 and \u03c6 respectively. To manipu-\nlate the handle, the robot is equipped with a hook end-"}, {"title": "3.0.2 Policy Actions", "content": "The policy directly controls the robot's arms and commands a lower-level locomotion policy that\ngoverns the legs. We use the learned locomotion policy from Ma et al. [21] which takes as input the\nplanar linear velocities vx and vy, angular velocity about z in the robot base frame, and the applied\nwrench on the robot base from the arm and its motion. Instead of estimating the applied wrench, we\nfound it sufficient and simpler to set it to a fixed mean value. To prevent the policy from moving\nthe base too fast, we clip the linear and angular velocity commands to the locomotion controller to\nmaximum magnitudes of 0.5 m/s and 1 rads/s respectively. Conversely, commands below 0.1 m/s\nand 0.1 rads/s are set to zero. The policy controls the arm by setting joint angle targets for the arm's\njoint-level PD controllers. The commanded target angle for joint i is computed as\n\nclip($\\hat{q}_i + a_i, q_i - \\sigma T_i / K_p, q_i + \\sigma T_i / K_p$),\n\nwhere ai is the action, \u011di is default position, and qi is the joint position for joint i. Finally, s is an\naction scaling factor. For safety, the commanded target for joint i is clipped based on a proxy for\nthe actuator torque threshold computed from the joint's proportional gain Kp, torque limit Ti, and\na saturation parameter \u03c3. We set \u03c3 to 0.7 for all arm joints. This action clipping is applied during\ntraining and on the real robot."}, {"title": "3.0.3 Sim-to-Real Considerations", "content": "The following were implemented in the simulation training environment to facilitate policy transfer\nonto the real robot. Detailed domain randomization parameters are reported in the Appendix.\nRandomize Initial Robot Configuration: The robot base starts an episode in a random location and\nyaw angle in front of the door with a random initial base velocity. The initial arm configuration is\nnot randomized to avoid self-colliding configurations.\nRandomize Arm Joint PD Gains: The PD gains for each arm joint are resampled for each episode.\nRandomize Door Dimensions: We generate door models while randomizing the handle locations on\nthe door panel, the doorway width, and the door panel thickness.\nRandomize Door Dynamics Properties: Resistance torques and the damping coefficients at the han-\ndle and hinge, the door mass, and the maximum handle turning angle are resampled for each episode.\nZero Handle Contact Friction: Many door handles have slippery surfaces which are especially hard\nto turn with a hook end-effector. We address this by zeroing the friction coefficient of the handle and\nhook end-effector contact in simulation. With non-zero handle contact friction, the policy tended to\nlearn more aggressive behaviours that relied on the end-effector momentum to turn the handle."}, {"title": "3.1 Teacher Training", "content": "The teacher is trained as an Actor-Critic using Proximal Policy Optimization [22]. We separate the\ntask into two stages. The first is to approach and open the door and the second is to pass through the\ndoor. This is done as the rewards are simpler to define when considering each stage in isolation."}, {"title": "3.1.1 Observations", "content": "The teacher observation includes measurements that are available to the robot during deployment\nalong with privileged information only accessible in simulation. No noise is introduced to the teacher\nobservation. The measurements available during deployment are the robot's proprioception and ex-\nteroceptive measurements of the location of the doorway and handle relative to the robot. Propri-\noception includes the base orientation, base linear and angular velocities, arm joint positions and\nvelocities, and the previous step actions. The privileged observations include the door joint (hinge\nand handle) positions and velocities, the mass of the door panel, the applied torques at the door hinge\nand handle due to spring stiffness and damping, the door type, and a task stage observation denoting\nif the policy is currently in the stage of opening or passing through the door."}, {"title": "3.1.2 Rewards", "content": "The opening and passing stage rewards are denoted by\nro and rp respectively. r\u3002 is further decomposed into\nrewards for handle manipulation, rhm, and a reward\nfor opening the door to a target angle, rod. rhm com-\nprises reward terms for moving the end-effector to the\nhandle, grasping the handle, and turning the handle.\nThe policy does not need to interact with the handle\nonce the door has been unlocked and opened enough.\nAs such, we set rhm to its maximum value such that\nthe policy ignores it once the door is opened enough.\nWe considered the door opened enough at 30\u00b0.\nThe opening stage transitions to the passing stage\nwhen the door has been opened more than 70\u00b0. After\ntransitioning to the passing stage, the policy receives\nthe maximum possible value of ro in addition to rp.\nThis is done so the policy can pursue rp without trad-\ning off ro as otherwise the policy can refuse to transition.\nThe passing stage reward rp is given by the dot product of the base velocity v\u00df with the unit progress\nvector p. Before the robot has passed through the doorway, p is the vector from the base to the\ndoorway center. After the robot has passed through, p is the vector along the direction of the\ndoorway. This dot product is normalized by the maximum commandable velocity of the locomotion\ncontroller ||V1B|| max. rp is clipped to a maximum of 1, otherwise, the policy can learn undesired\nbehaviors such as throwing its arm to move the base faster than ||VB||max.\nFor pull doors, rp alone is insufficient for the policy to learn to move around the open door panel.\nTherefore, once a pull door is opened enough in the opening stage, we reward it for moving its base\nand end-effector around the door panel as shown in Fig. 4. We also require the base and end-effector\nto be behind the door panel to transition to the passing stage for pull doors.\nShaping rewards rs are used to regularize the behavior such that the policy respects the hardware\nlimitations of the robot and is safe to deploy. rs is applied during both the open and passing stages\nand comprises terms such as avoiding collisions, minimizing arm motions, penalizing unwanted\nbase motions, penalizing out-of-limit commands, and penalizing singular arm configurations.\nDetailed definitions and scales of all reward terms are given in the Appendix."}, {"title": "3.2 Student Training", "content": "The student policy is trained to imitate the teacher policy's actions given only the proprioceptive and\nexteroceptive door observations that are available during deployment. Gaussian noise is added to all\nof the student's observations. The student policy also receives supervision through estimating the\nprivileged information of the door and handle locations relative to the robot, the door joint states,\nthe door mass, the door hinge and handle torques, and the door type. The Smooth L1 Loss is used\nfor losses except for the door type where the Cross Entropy Loss is used.\nThe student policy is based on a recurrent neural network (RNN) with an architecture that follows\nthe student policy for legged locomotion from Miki et al. [18] with some differences. Specifically,\nthe attention-gate decoder in the original architecture is replaced with a linear layer as the decoder.\nThis was done as unlike locomotion where it is possible to do blind, we assume that the exteroceptive\nmeasurements during door opening can be noisy but not degraded to the point of being useless.\nWe do not include all available measurements in the student policy observation similar to Tan et al.\n[23]. Specifically, the previous actions and arm joint velocities are omitted. Training the student\npolicy with much higher noise on the arm joint velocity resulted in better transfer of the policy to\nthe real robot. Given this, we completely removed the arm joint velocity from the observation."}, {"title": "4 Results", "content": "The student policy is deployed directly onto the robot without further fine-tuning. The policy is\nboth trained and deployed at 50Hz. We tasked the robot with opening and traversing through doors\nvarying in their opening and swing directions, doorway and handle dimensions, door panel inertia,\nhandle dynamics, and the presence of a door self-closing mechanism.\nProprioceptive observations are provided by sensors onboard the robot, such as encoders measuring\nthe joint states. The robot also runs onboard lidar odometry [24] to estimate its base pose. For\nexteroception, the policy only needs to know the handle and doorway locations relative to the robot\nbase. We provide these door measurements using either motion capture or AprilTags [25] as an\nexternal tracking system. The motion capture system provides accurate measurements but requires\nseveral fixed tracking cameras where as the AprilTags can be easily deployed on any door by tracking\na tag on the door panel with an external camera. As the door measurements are relative to the robot,\nthey could also be obtained using onboard sensing, but we leave this as future work."}, {"title": "4.1 A Single Control Policy for All Door Types", "content": "We evaluate whether the policy can recognize the door type (opening and swing directions) during\nthe task and open them without being given this information beforehand. Indeed our policy could\nsuccessfully deduce the correct type and pass through for all door types as shown in Fig. 5. When the\npolicy was uncertain of the opening direction, it shifted its estimate between push and pull. This shift\nof the policy's belief was correlated to its commanded actions as shown in Fig. 6, where the policy\nlearned to push and pull until it could move the door panel. We also study how the policy's belief\nof the door type is represented in its RNN hidden state by visualizing the hidden states trajectories\nusing a UMAP [26] projection in Fig. 7. The figure shows that the policy learned to represent the\nopening direction (push/pull) in separate regions of the hidden state space. Moreover, the regions\nfor the opening directions appear linearly separable in the projection."}, {"title": "4.2 Repeatability of the Control Policy", "content": "Repeated trials of the policy were conducted to evaluate its repeatability. A spring-loaded door was\nplaced within the motion capture space and 20 continuous trials were performed for both the pull and\npush sides. The policy successfully traversed the door in 20/20 and 18/20 trials for the pull and push\nsides respectively, with an overall success rate of 95.0%. In all trials, the policy was successfully\nable to open the door. The two failed trials on the push side were caused by the robot getting stuck\non the side of the doorway which was protruding due to a lack of walls around the door which differs\nfrom the simulation model. A video of the trials is provided in the supplementary material"}, {"title": "4.3 Ablations in Simulation", "content": "We ablated the student policy training to study if supervision from the estimation task helps in\nlearning the control task and if the recurrence of the student policy is necessary.\nDoor Type Estimation Loss: The student policy is trained without supervision from estimating the\ndoor type and we find that the imitation loss alone is sufficient for learning to distinguish between\nboth push and pull doors. Moreover, removing the estimation loss has little effect on the training\ndynamics of the imitation loss as seen in Fig. 8, but its presence neither helps nor hurts the imitation\nlearning. Given this, the estimation loss and decoder module are still included in our final policy\narchitecture as they are useful for helping to understand the internal belief state of the policy.\nStudent Policy Recurrence: We replaced the RNN module of the student policy with an MLP and\ntrained the policy with the imitation loss to study the effect of removing recurrence. Fig. 9 shows\nthat the student policy fails to imitate the teacher policy without recurrence. Specifically, the MLP\nstudent policy cannot imitate the teacher given the noisy student observations. As the MLP oper-\nates on a single-step observation, any noise on the observation makes it difficult for the policy to\ndisambiguate different states of the system and to take the appropriate action."}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Emergent Robustness to Unmodeled Disturbances", "content": "The student policy showed robust recovery from disturbances not explicitly modeled during training\nin simulation. For example, the policy could handle pushing or pulling on the door or robot as\nseen in Fig. 10, even though random pushes/pulls on the robot were not used during training. It\ncan be expected that the teacher policy would be robust to such disturbances as it acts on the current\nobservation without regard to the history. As the student does not perfectly imitate the teacher during\ntraining, mistakes in the student's actions during training provide opportunities to learn recovery\nbehaviours from the teacher. For example, if the student's action missed the handle grasp, the\nsubsequent teacher's actions to imitate would be to retry grasping the handle."}, {"title": "5.2 Failure Cases", "content": "A common failure case of the policy was not turning the handle enough to unlock the door. When\nthis failure occurs, the policy becomes stuck moving the robot back and forth trying to push and\npull on the door as it shifts its belief of the door type. The policy does not have a direct observation\nwhich indicates that the handle has been turned. As such, it can learn to incorrectly associate the\nrobot's handle turning motion with the handle turning. Using visual or force sensing, which can\ndirectly observe the turning of the handle, could alleviate this issue.\nThe performance of the policy was notably degraded when using AprilTags to track the door due to\ncharacteristics of the AprilTag measurement noise being unmodeled in simulation when training the\npolicy. We note a failure case caused by the AprilTag latency where the policy lets go of the handle\nand attempts to grasp it again as it receives delayed measurements of the handle location.\nLastly, unmodeled door geometry in simulation causes failure cases such as the hook end-effector\ncatching on the door panel or the robot base getting stuck on the protruding sides of a doorway as\nshown in the videos provided in the supplementary material."}, {"title": "6 Conclusion", "content": "This paper presented a learned control policy for a legged manipulator to traverse through doors. The\npolicy was trained in simulation using a teacher-student approach such that it learned both robust\nbehaviors and the ability to estimate properties of the door through interaction during the task. The\nlatter allowed the policy to handle doors of varying properties, such as the opening direction, without\nbeing given this information a priori. The policy was deployed on the ANYmal robot with an arm\nand achieved a 95.0% success rate in trials on the push and pull sides of a spring-loaded door.\nAdditional experiments demonstrated the policy traversing through doors of all opening and swing\ndirections, various dimensions, and various hinge and handle dynamics, as well as robustness to and\nrecovery from external disturbances. Future works include using onboard sensing for obtaining the\ndoor measurements, adding force sensing, and handling a larger set of door handles such as knobs."}, {"title": "A.1 Opening Rewards", "content": "The opening reward ro is composed of the reward for opening the door to the target angle rod along\nwith rewards for manipulating the door handle denoted by rhm. For pull doors, we additionally\ninclude a reward for encouraging the robot to move its base and end-effector around the door panel\ndenoted by radp. These rewards together compose the opening reward\n\nro = $\\begin{cases}\n3r_{od} + r_{hm} & 0 < 30^\\circ \\\\\nr_{hm} + 0.5r_{adp} & otherwise\n\\end{cases}$\n\nWhen the door has been opened enough, set as 0 > 30\u00b0, rhm is set to its maximum value Thm as it\nis no longer necessary for the policy to interact with the handle to open the door further. radp is only\napplied once the door has been opened enough.\nThe handle manipulation reward is composed of the following components\nrhm = rehd+rth + reho +0.5rhg + rplg\nAll individual reward terms in ro are defined as follows.\n\u2022 rehd (end-effector to handle): Minimizes the distance between the end-effector point e and the\nhandle point h:\n\nrehd = exp(-||e - h||_2)\n\n\u2022 rth (turn handle): Rewards increasing the handle turning angle \u03c6:\n\nr_{th} = \\phi / \\phi_{max}\n\nwhere \u03c6max is the maximum the handle can be turned.\n\u2022 reho (end-effector grasp orientation): Rewards the end-effector for tracking a desired orientation\nfor grasping the handle.\n\nr_{eho} = 1 - \\frac{\\epsilon_{eo}}{\\pi}\n\nwhere e. angular error between the end-effector orientation and the desired end-effector orienta-\ntion.\n\u2022 Thg (handle in end-effector grasp): Give a binary reward when the handle point h is within the\ngrasp zone G of the end-effector.\n\nr_{hg} = $\\begin{cases}\n1_G(h) & ||e - h||_2 \\leq 1 \\\\\n0 & otherwise\n\\end{cases}$\n\n14(x) is the indicator function of value 1 if x \u2208 A and 0 otherwise. For the hook-end effector\nused in this work, we defined the grasp zone G as the region along the opening of the hook. This\nreward is only active when the end-effector point e is close enough to the handle (within 1 m).\n\u2022 rplg (penalize lost grasp): Give a binary penalty when if the handle point h is in the grasp zone at\nstept - 1 and leaves the grasp zone at t.\n\nr_{plg} = $\\begin{cases}\n-1_G(h_{t-1})(1 - 1_G(h_t)) & ||e - h||_2 \\leq 1 \\\\\n0 & otherwise\n\\end{cases}$\n\n\u2022 Similar to rhg, rplg is only active when the end-effector is close enough to the handle.\nrod (open door to target angle): Rewards opening the door to the target opening angle\n\nr_{od} = 1 - \\frac{|\\theta - \\hat{\\theta}|}{\\hat{\\theta}}\n\nwhere \u03b8 is the door hinge joint angle. This reward can also be used to train an opening only policy\nthat opens the door to \u03b8. For training the opening and passing through policy is set to 75\u00b0."}, {"title": "A.2 Passing Rewards", "content": "\u2022 radp (move around the door panel): This reward is only applied for pull doors.\nGiven zones Z1 and Z2 defined relative to the door panel as shown in Fig 4, radp is computed\nbased on the locations of the base b and the end-effector e as\n\nr_{adp} = $\\begin{cases}\n1 & b \\in Z_1 \\\\\n2 & b \\in Z_2 \\\\\n0 & otherwise\n\\end{cases}$ + $\\begin{cases}\n1 & e \\in Z_1 \\\\\n2 & e \\in Z_2 \\\\\n0 & otherwise\n\\end{cases}$\n\n\u2022 rp (passing progress): Rewards the base velocity VB for moving along the unit progress vector p\n\nr_p = max(1, \\frac{p \\cdot v_B}{||v_B||_{max}})\n\nwhere || VB || max is the the max allowable commanded velocity of the locomotion controller."}, {"title": "A.3 Shaping Rewards", "content": "The shaping reward rs is defined as\nrs = 0.3rma +0.5rpbt + rpsa +0.1rpcl + 2rpc\nIndividual terms of rs are defined as:\n\u2022 rma (minimize arm motion): Rewards minimizing the arm joint velocities and accelerations\n\nr_{ma} = \\sum_{i=1}^{6} exp(0.01\\dot{q}_i^2) + exp(0.000001\\ddot{q}_i^2)\n\nwhere \u0121i and \u0121i are the joint velocity and acceleration for the ith arm joint respectively.\n\u2022 rpbt (penalize base tilt): Penalizes large tilt of the robot base. The base tilt angle \u03c8 can be com-\nputed from the projected gravity vector expressed in the robot base frame g\u00df and expressed in the\nworld frame gw as follows\n\n\\psi = arccos(\\frac{g_w \\cdot g_B}{||g_w|| ||g_B||})\n\n\u2022 Then rpbt -1 if \u03c8 > 4, where 4 is a tilt threshold, and 0 otherwise. We set & as 8\u00b0.\n\u2022 rpsa (penalize stretched arm): Penalize the arm from reaching out too far to prevent singular arm\nconfigurations.\n\nr_{psa} = -clip(\\frac{||e - s||}{\\ell_{e - s}} - (0.7 - 0.1), 0, 1)\n\nwhere e and s are the locations of the end-effector and shoulder joint.\n\u2022 rpcl (penalize command out of limits): As the arm PD target and locomotion commands are\nclipped within certain bounds we penalize the policy for commands that exceed these bounds.\n\nr_{pcl} = \\sum_{i=1}^{9} -clip(\\frac{a_i - \\bar{a}_i}{\\sigma_i}, 0, 1)\n\nwhere ai, \u0101i, and \u03c3\u03b5 corresponding to the action, action limit, and penalty ramp up speed for the\nith component of the policy's output action. The action limits are discussed in Sec. 3.0.2.\n\u2022 rpc (penalize collisions): Penalizes robot collisions.\n\nr_{pc} = -\\sum_{c \\in C} $\\begin{cases}\n1 & ||\\lambda_c|| > 0 \\\\\n0 & otherwise\n\\end{cases}$\n\nwhere (.) is the contact force on robot link (\u00b7) and C is the set of robot links where collisions are\npenalized including the base, thighs, and arm."}, {"title": "C.1 Is Teacher-Student Training Necessary?", "content": "To evaluate the necessity of the teacher-student framework for this task we trained two modified\nteacher policies to see if a policy trained with RL directly on the observations available on the robot\ncan learn the task. The first is given only the student observations without access to the privileged\nobservations. The second is the same as the first except that noise is added to the observations at the\nsame amount used for training the student. We evaluated the teacher policies in simulation on 4000\nenvironments (a different randomized door in each) for 10 episodes, each lasting 10 seconds, with\nall domain randomizations active. The success rates are reported in Fig.13.\nWe find that without the privileged observations, the teacher policy could not learn to pass-through\nfor both opening directions, but is surprisingly able to learn an opening behavior that can sometimes\nopen both push and pull doors, albeit at a significantly worse success rate. Examining this behavior\nqualitatively suggests that without the door type privileged information, the policy learns to associate\nspecific robot configurations with either push or pull, learning to move between these configurations\nuntil the door opens. Adding noise to the observations further reduces the success rate for this type\nof behavior as disambiguating these specific robot configurations becomes more difficult."}, {"title": "C.2 Evaluating the Student Policy's Capabilities", "content": "The capability of the student policy is evaluated in simulation by tasking it with traversing through\ndoors of increasing hinge resistance torques. Increasing the hinge resistance makes the doors more\ndifficult to open as the robot must apply more force before the door moves, and more difficult to pass\nthrough as the door closes faster after opening. The student policy is tested on whether it could open\nthe door and if it could successfully pass through in episodes lasting 10 seconds with all domain\nrandomizations active. We report these results in Fig.14.\nThe teacher and student policies were trained with a hinge resistance randomization range of 0 Nm\nto 30 Nm. As expected, the success rates for both opening and passing through the door decreases\nfor increasing hinge resistance. For the doors with greater hinge resistances, the policy could still\nsometimes open them but passing through is more difficult with the success rate dropping to near\nzero for hinge resistances greater than 50 Nm. For push doors, the higher resistance makes the door\npanel more difficult for the robot to hold open and the robot can also get trapped in the door way by\nthe door panel. For pull doors, the higher resistance causes the door to close faster, making it more\ndifficult for the policy to move the robot around the door panel in time to pass through."}]}