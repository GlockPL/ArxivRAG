{"title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning", "authors": ["Xiaoye Qu", "Jiashuo Sun", "Wei Wei", "Yu Cheng"], "abstract": "Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multi-modal context comprehension. However, they still suffer from hallucination problems referring to generating inconsistent outputs with the image content. To mitigate hallucinations, previous studies mainly focus on retraining LVLMs with custom datasets. Although effective, they inherently come with additional computational costs. In this paper, we propose a training-free framework, MVP, that aims to reduce hallucinations by making the most of the innate capabilities of the LVLMs via Multi-View Multi-Path Reasoning. Specifically, we first devise a multi-view information-seeking strategy to thoroughly perceive the comprehensive information in the image, which enriches the general global information captured by the original vision encoder in LVLMs. Furthermore, during the answer decoding, we observe that the occurrence of hallucinations has a strong correlation with the certainty of the answer tokens. Thus, we propose multi-path reasoning for each information view to quantify and aggregate the certainty scores for each potential answer among multiple decoding paths and finally decide the output answer. By fully grasping the information in the image and carefully considering the certainty of the potential answers when decoding, our MVP can effectively reduce hallucinations in LVLMs. The extensive experiments verify that our proposed MVP significantly mitigates the hallucination problem across four well-known LVLMs. The source code is available at: https://github.com/GasolSun36/MVP.", "sections": [{"title": "1 Introduction", "content": "Large Vision-Language Models (LVLMs) have become indispensable and marked a significant milestone in the field of Artificial Intelligence. These LVLM models, owing to their ability to generate contextually relevant textual renditions of visual inputs, are being extensively employed across a diverse spectrum of applications, such as healthcare, autonomous systems, and robotics. Despite substantial advancements, LVLMs suffer from a significant challenge termed \u201challucination\u201d, whereby the models produce semantically plausible but factually inaccurate text, misaligned with the ground-truth content of the associated image. As shown in Figure 1, LVLMs fail to recognize \"backpack\" and incorrectly identify the number of people in the image. In applications where precision and reliability of generated content are paramount, such hallucinations can trigger a cascade of erroneous decisions. Consequently, addressing the hallucination issue is indispensable for strengthening the trustworthiness of LVLMs across practical applications.\nTo tackle hallucination, most recent studies focusing on retraining the LVLMs with constructed hallucination-related datasets by supervised fine-tuning (SFT)"}, {"title": "2 Method", "content": "As shown in Figure 2, given that hallucinations commonly arise due to incomplete comprehension of image content, we propose to seek complementary information from the input image with three different views. Subsequently, the acquired information is leveraged to augment the global vision information from the vision encoder for LLM reasoning. For each view, considering different decoding paths have different certainty for potential answers, we introduce certainty-driven multi-path reasoning, which quantifies and aggregates the certainty score for each potential answer among multiple decoding paths. In this stage, we maximize the inherent reasoning ability of the model. Finally, with the multi-view information and multi-path reasoning, we achieve superior performance for alleviating hallucinations."}, {"title": "2.2 LVLMs Input and Decoding", "content": "The input of LVLMs contains both image and text. The image is first processed by a vision encoder to obtain visual tokens. Then, the image tokens are mapped to the input space of LLMs for decoding. We denote the visual tokens as\n\\(x^o = \\{x_1, x_2,...,x_N\\}\\).\nHere N is the length of the visual tokens. Correspondingly, the input query is tokenized with the tokenizer. We denote it as \\(x_q = \\{x_1,x_2,...,x_M\\}\\) with length M. The image and text tokens are concatenated as the final input sequence X with length N + M.\n\\(X = [x^o : x_q] = [x_1, x_2, ..., x_N, x_1, x_2, ..., x_M],\\)\n(1)\nAfter feeding the input tokens X to the LVLMs, the model outputs answers in an auto-regressive manner which predicts the next token based on previous tokens, formally:\n\\(p(O_t|O_{<t}) = SoftMax[LVLM([{O_i\\}_{i=1}^{t-1}])],\\)\n(2)\nwhere we omit the input query X and \\(\\{O_i\\}_{i=1}^{t-1}\\) are decoding tokens from the previous t-1 rounds and the first decoding token O1 is decoded with the input X in Eq. 1. At time step t, the token with the highest probability is chosen from the vocabulary. During the decoding period, hallucinations arise when probabilities are improperly attributed to tokens that fail to correlate with the presented visual image."}, {"title": "2.3 Multi-view Image Information Seeking", "content": "Previous LVLM research, utilizing a CLIP for global image representation, may neglect intricate, object-specific details and background components, consequently leading to hallucinations precipitated by a partial grasp of the input imagery. For instance, when querying the detailed information that is not captured by the video encoder, the LVLMs tend to hallucinate. Thus, it is imperative to master comprehensive information about the image before responding to the input query. Naturally, a wealth of visual information exists in images and can be located by various methods, such as invoking external visual detection tools"}, {"title": "2.4 Multi-path Certainty-driven Reasoning", "content": "Decoding strategies are important in guiding how LVLMs produce textual answer. Previous decoding strategies commonly consider each output token with the same level of importance, thus ignoring the unique importance of the answer token. However, we observe that the answer tokens present different certainty during diverse decoding paths. In Figure 2, for the question (How many cars are in this image), the first decoding path of \"Bottom-up\" and \"Regular\" perspectives produce different answers \"four\" and \"three\u201d but their certainty is significantly different (0.65 and 0.03, respectively). This phenomenon indicates that hallucinations occur more frequently when the certainty of answer tokens is low and inspires us with certainty-driven reasoning to alleviate hallucinations. Formally, we quantify the Centainty Score S, which is the difference between the probabilities of the two tokens with the highest probabilities at time step t:\n\\(S = p(x_t^1 | v, x_{<t}) - p(x_t^2 | v, x_{<t})\\)\n(4)\nwhere \\(x_t^1\\) and \\(x_t^2\\) represent the post-softmax probabilities of top-two tokens at each decoding step t. It is worth noting that we only consider the probability disparity of the answer tokens."}, {"title": "2.4.1 Multi-Path Certainty-driven Reasoning", "content": "To illustrate certainty-driven reasoning, we first consider a basic situation where only one greedy decoding paths exist. As shown in Figure 5, given the input query, we observe that LVLMs tend to hallucinate when a low certainty score occurs, where greedy decoding mistakenly takes the bottle for a cup and outputs the wrong answer: \"Yes, there is a cup in the image\", while the certainty score of the answer token \"Yes\" is only 0.02. With further investigation, when decoding the first token, besides \"Yes\", there are many other candidates (i.e. \"Based\", \"The\"), which are displayed by underline in Figure 5 and sorted by probability from high to low. Instead of introducing complex methods to building multiple decoding paths, we simply inspect more top-K paths starting from relatively lower probability tokens, namely decoding from the second word \"Based\", the third word \"The\", and so on. Notably, the second path leads to the correct answer \"no\" with a significantly higher certainty score of 0.92.\nThus, we introduce a multi-path reasoning which explicitly considers the certainty of the answer tokens. Specifically, to build multiple paths, we consider the top-K candidates in the decoding process of the first token, and then continue decoding based on each candidate to generate the K paths with different answers. Formally, each path corresponds to an answer Ak. Here the answers can be identified by the question type or specified prompt format. For instance, we can search for numbers in the output to answer the question in Figure 2, or identify \"yes\" or \"no\" in Figure 5. Then, we aggregate the certainty score for same Ak from K paths:\n\\(S_k = \\sum_{j=1}^{K} M_j (p(A_t^1 | v, A_{<t}) \u2212 p(A_t^2 | v, A_{<t}))\\)\n(5)\nwhere A<t denotes the sequence before generating answer Aj. Here Mj is an optional parameter denoting the probability of the first token in the K-th path and we will explore it in the experiment. Thus, we can obtain the certainty score for each potential answer."}, {"title": "2.4.2 Multi-View Multi-Path Reasoning", "content": "In Section 3.3, we seek image information from three perspectives c \u2208 {Top-down, Bottom-up, Regular}. Considering that each view captures different information from the input image, the corresponding reasoning paths also present specific preference, thus we can further aggregate certainty scores for multi-view multi-path:\n\\(S_{Ack} = \\sum_{i=1}^{C} \u03b1_i \\sum_{j=1}^{K} M_{ij} (p(A_{tij}^1 | v, A_{<t}) \u2212 p(A_{tij}^2 | v, A_{<t}))\\)\n(6)\nwhere \u03b1i is a hyperparameter denoting the importance of a specific perspective. Finally, the answer with the highest certainty score is selected as our final answer:\n\\(A_{final} = argmax(S_{Ack})\\)\n(7)"}, {"title": "3 Experiment", "content": ""}, {"title": "3.1 Evaluation Benchmarks", "content": "Following previous works, we use the following two benchmarks POPE and MME.\nPOPE the Polling-based Object Probing Evaluation. In this benchmark, LVLMs are queried to determine whether a specific object is present in the provided image. It encompasses three distinct settings: random, popular, and adversarial, each differing in the construction of negative samples. The POPE benchmark aggregates data from three distinct sources: MSCOCO, A-OKVQA, and GQA. It involves 500 images from each dataset under each sampling setting. The performance is gauged using four key metrics: Accuracy, Precision, Recall, and F1.\nMME acts as a comprehensive benchmark designed to evaluate LVLMs across a range of dimensions. It is composed of ten perception-related subtasks and four cognition-focused ones. In the experiments, we evaluate the full dataset. In addition, we take into account the existence and count subsets for the inspection of object-level hallucination, along with the position and color subsets for attribute-level hallucination evaluation. The combined metric of accuracy and accuracy+ is used to quantify the performance as per the official implementation."}, {"title": "3.2 Evaluation LVLM and Baselines", "content": "LVLMs. To comprehensively evaluate our model and have a fair comparison with previous works, we experiment with our proposed MVP on four state-of-the-art LVLMs, including LLaVA1.5, Qwen-VL, InstructBLIP, and mPLUG-Owl2. All four LVLMs are based on 7B LLM backbone models.\nBaselines. To verify the effectiveness of our framework, we compare MVP with the vanilla LVLMs and two recent training-free methods, including VCD and OPERA. In our main experiments, for fair comparison, vanilla, VCD, and our MVP all adopt the decoding strategy of direct sampling. In addition, OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue."}, {"title": "3.3 Experiment Results", "content": "Results on POPE. Table 1 summarizes the experimental results on the MSCOCO part of POPE benchmark, including experiments under random, popular, and adversarial settings. The results of A-OKVQA and GQA are presented in the Appendix. Specifically, under different settings, our method significantly surpasses the vanilla model's performance across all LVLMs. For example, with LLaVA1.5, MVP achieves an average improvement of 15.9 in Accuracy and 21.84 in F1 score across random, popular, and adversarial settings. For LLaVA1.5, Qwen-VL, and InstructBLIP, the improvement in F1 scores is mainly due to an increase in recall, while in mPLUG-Owl2, the increase comes from the simultaneous improvement of precision and recall. Furthermore, compared to VCD and OPERA, our method still achieves better results in most cases. These results demonstrate the effectiveness of our MVP in alleviating hallucinations."}, {"title": "3.4 Ablation Study", "content": ""}, {"title": "3.4.1 Effectiveness of Multi-view Caption", "content": "Table 3 presents the performance from different perspectives. The first row presents the performance without using any additional caption information, while rows 2-4 respectively use a single perspective. The improvement is more pronounced with more perspectives involved. These results have confirmed that multi-view information can contribute to more comprehensive image understanding, thus mitigating the hallucinations in LVLMs."}, {"title": "3.4.2 The transferability of captions.", "content": "Intuitively, the quality of the captions has a direct impact on the model performance. Therefore, in this study, we explore the transferability of captions. Specifically, we employ a more powerful open-source model LLaVA1.6 to generate three-perspective captions for images in the random part of POPE MSCOCO, and use these captions for our model. As depicted in Table 4, with better captions, our method provides further and stable improvements across four LVLMs. This result also confirms the significance of the multi-view information for alleviating hallucinations in LVLMs, as well as the plug-and-play flexibility of our method."}, {"title": "3.4.3 Multi-path Reasoning", "content": "Considering that we have multiple views of captions, we only adopt the regular perspective in this ablation study. We experiment on the random and adversarial part of the POPE MSCOCO benchmark, as shown in Figure 6.\nWe first conduct experiments on Top-K in Equation 5. As K increases from 1 to 5, peak performance is observed at K equals 3. When K becomes larger, the performance does not improve. This is due to the decoding path extending from the first tokens with minuscule probabilities do not provide any beneficial information.\nSecondly, we explore a new aggregation strategy MVP-Max. Instead of accumulating the certainty scores in Equation 5, the potential answer with maximum certainty score among all paths is chosen as the final answer. It can be seen that after using MVP-Max, the final performance of the model decreases significantly. This demonstrates the effectiveness of our aggregation strategy.\nFinally, we explore removing Mj in Equation 5, we found that relying solely on the certainty of tokens can damage the stability and effectiveness of our model."}, {"title": "3.5 Decoding Strategy", "content": "In this section, we analyze the impact of different decoding strategies on our method. Specifically, we investigate five decoding methods. Notably, nucleus sampling is used in our main experiments for a fair comparison with recent methods. Our MVP can further enhance the performance with training-free decoding methods such as VCD and OPERA, as shown in Table 5. We can observe that using beam search as the decoding strategy performs the best accuracy on the random setting, while OPERA achieves the most significant accuracy on the adversarial part. These results also imply that our method is a novel plug-and-play approach, which can be flexibly integrated with other techniques."}, {"title": "4 Conclusion", "content": "In this paper, we propose a novel training-free framework MVP to reduce hallucinations by making the most of the innate capabilities of the LVLMs through Multi-View Multi-Path Reasoning. Specifically, we devise a multi-view information-seeking strategy to perceive the intricate details of the image information, which contributes to comprehensive image understanding. Furthermore, we propose multi-path reasoning to quantify and aggregate the certainty scores for each potential answer and finally decide the output answer. With the multi-view multi-path reasoning, our method effectively alleviates hallucinations in LVLMs."}, {"title": "A Implementation Details", "content": "In our paper, we adopt three different views to capture the information from the image. For \"Bottom-up\" perspective, we use following prompt: \u201cThrough a systematic examination of the image at the pixel level and by analyzing various visual features, such as shape, color, and texture, along with employing object detection techniques, describe this image in details.\". In addition, we use \u201cDescribe this image in details\u201d for the regular caption. The prompt for top-down perspective has been described in Section 3.3. In addition, to generate these multi-view captions, a temperature of 0.9 and a top-p parameter of 0.95 are set to guarantee diversity. K in Equation 5 is set to 3. \u03b1 in Equation 6 is set to 0.4, 0.2, and 0.4 for \u201cbottom-up\u201d, \u201cregular\u201d, and \u201ctop-down\u201d perspectives, considering that the \u201cbottom-up\u201d and \u201ctop-down\u201d perspectives bring more beneficial information. We do not tune \u03b1 much as the aggregation from multiple decoding paths inherently presents certain robustness."}, {"title": "B Related Work", "content": ""}, {"title": "B.1 Hallucination in LVLMs", "content": "Recently, the potential hallucination, conflict, and safety issues in large models have garnered considerable attention, mainly due to the direct impact on the downstream applications. In LVLMs, the term \u201challucination\" refers to models that generate seemingly plausible outputs inclusive of objects, attributes, or relations that do not correspond with the images.\nRegarding hallucination mitigation, the primary focus of most current methods has been to enhance the quality of the supervised fine-tuning or reinforcement learning data. VIGC presents a component to correct visual instructions with the aim to minimize hallucinations generated from lengthy sequences. LRV attempt to alleviate hallucinations by developing expansive and diverse SFT data. For methods based on reinforcement learning, LLaVA-RLHF is the pioneer in applying Reinforcement Learning with Human Feedback (RLHF) to mitigate hallucination in LVLMs. RLHF-V further develops a fine-grained correctional human feedback. Considering the instability and training difficulty of RLHF, employ Direct Preference Optimization (DPO) and build a hallucination-aware dataset for alleviating hallucination. Although these methods have achieved significant improvements, they inevitably introduce a large training cost and are prone to overfitting to the training data. Instead, there are also training-free works aiming to solve the hallucination without introducing training cost. VCD contrasts the output distributions derived from original and distorted visual inputs, aiming to recalibrate the model's excessive dependence on unimodal priors and statistical biases. OPERA introduces a penalty term to the model logits during the beam-search decoding to alleviate the overconfidence problem, complemented by a rollback strategy. In this paper, we focus on the training-free paradigm for mitigating hallucination in LVLMs."}, {"title": "B.2 Training-free Decoding Strategy", "content": "As the recent training-free methods for mitigating hallucination focus on the decoding process, we describe several decoding strategies here. Decoding strategies in language models are instrumental in guiding how these models produce text. They are a significant factor in influencing the quality, relevance, and coherence of the output. Greedy decoding takes the simplest approach, choosing the most probable next word at every step. Despite its speed and computational efficiency, this method often results in repetitive and monotonous text. Conversely, beam search offers a more advanced technique that maintains a predetermined number of hypotheses at each step, elaborating on them to identify a more optimum sequence. In nucleus sampling, a flexible range of words is considered, which accumulate to achieve the given probability p. More recently, there are two methods specifically proposed for mitigating hallucinations. VCD adopts contrastive decoding to calibrate the model's output distribution. OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue. In this paper, we focus on the certainty of the answer token during the decoding process, which does not conflict with designing different decoding paths. Thus, our MVP framework is plug-and-play and can further combine with the above decoding strategies."}, {"title": "C Qualitative Results", "content": "To qualitatively verify the effectiveness of our method on downstream tasks, we presented two examples from the MSCOCO POPE and MME datasets. As illustrated in Figures 7, in both examples, MVP is able to accurately address questions. In the top figure where the chair is in the top right corner and not fully visible, our MVP comprehensively captures multi-view information, thus contributing to identifying this chair. In the bottom figure where distant people appear blurry, our MVP effectively decides the accurate number of people with multi-path reasoning. Through these two direct comparisons, our method answers questions more precisely than the currently existing strong baselines, significantly reducing hallucinations in LVLM models."}, {"title": "D More Model Performance", "content": "To further demonstrate the effectiveness of our proposed MVP, we conduct experiments on POPE based on AOKVQA and GQA with random, popular, and adversarial settings, respectively. The experiment settings are the same as the main experiment in the MSCOCO. The results are shown in Tables 6 and 7. It is obvious that our proposed MVP has greatly improved from these two tables compared with the baseline models. Specifically, under different settings, our method significantly surpasses the vanilla model's performance across all LVLMs. For example, with LLaVA1.5, MVP achieves an average improvement of 4.38 in Accuracy and 6.29 in F1 score across random, popular, and adversarial settings on AOKVQA. For LLaVA1.5, InstructBLIP and mPLUG-Owl2, the improvement in F1 and Accuracy scores is mainly due to an increase in precision, while in Qwen-VL, the increase comes from the simultaneous improvement of precision and recall. These results demonstrate the effectiveness of our MVP in alleviating hallucinations."}]}