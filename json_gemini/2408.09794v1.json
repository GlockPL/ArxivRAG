{"title": "AutoML-guided Fusion of Entity and\nLLM-based representations", "authors": ["Boshko Koloski", "Senja Pollak", "Roberto Navigli", "Bla\u017e \u0160krlj"], "abstract": "Large semantic knowledge bases are grounded in factual knowl-\nedge. However, recent approaches to dense text representations (em-\nbeddings) do not efficiently exploit these resources. Dense and robust\nrepresentations of documents are essential for effectively solving down-\nstream classification and retrieval tasks. This work demonstrates that\ninjecting embedded information from knowledge bases can augment the\nperformance of contemporary Large Language Model (LLM)-based rep-\nresentations for the task of text classification. Further, by considering\nautomated machine learning (AutoML) with the fused representation\nspace, we demonstrate it is possible to improve classification accuracy\neven if we use low-dimensional projections of the original representation\nspace obtained via efficient matrix factorization. This result shows that\nsignificantly faster classifiers can be achieved with minimal or no loss in\npredictive performance, as demonstrated using five strong LLM baselines\non six diverse real-life datasets.", "sections": [{"title": "Introduction and Background", "content": "Robust document representations are crucial for many NLP tasks [20]. Early\nmethods like bag-of-words were limited, relying on counting schemes and re-\nsulting in high-dimensional representations without capturing richer semantics.\nTechniques such as Latent Semantic Analysis (LSA) [5] addressed this by project-\ning high-dimensional spaces into lower dimensions, providing more meaningful\nrepresentations even in multilingual contexts [11]. The representation learning\nparadigm [4] popularized learning representations across modalities as an auxil-\niary task for training deep learning models. Le et al. [14] introduced Doc2Vec,\nwhich learns word or paragraph-level representations by corrupting text and pre-\ndicting the missing parts using shallow neural networks. This technique remains\nkey for obtaining document representations. Depending on how the corruption\nand learning are conducted, there are two main paradigms: masked language\nmodeling and causal language modeling. Devlin et al. [6] demonstrated that\nrandomly masking parts of the input (masked language modeling) and sequen-\ntially predicting them with the Transformer architecture [32] not only performs\nwell but also learns contextual word embeddings. Conversely, Radford et al.\n[25] approached document representation learning as a generative task, where a\nTransformer model [32] is fed part of the input and tasked with predicting the\nremainder. This training paradigm produces generative models and is currently\nthe most popular approach towards LLMs [35]. However, both paradigms focus\non contextual word embeddings, which are insufficient for document-level rep-\nresentations. To leverage the expressiveness of deep models, Reimers et al. [27]\nproposed using BERT-based embeddings as a foundation for learning document-\nlevel representations via Siamese networks. Similarly, LLM2Vec [3] suggested\nrepresenting documents by extracting the internal weights of large generative\nmodels, such as LLaMa3 [1]. These embeddings can be efficiently obtained from\na pre-trained model and serve as a strong competitor in a recently proposed\nmassive text-embeddings benchmark (MTEB) [20]. Contrastive representation\nlearning [16] involves learning document representations by placing similar doc-\numents together and repelling dissimilar ones. Angle [17] was recently proposed,\nwhere models optimize representations based on the angle between their vec-\ntors in the latent space. However, high-dimensional representations can impair\nclassifier performance due to the curse of dimensionality [9], increase memory\nfootprint for storage and retrieval, and adapting these representations to spe-\ncific corpora is laborious and expensive. Alternatively, large semantic knowledge\nbases grounded in factual knowledge, such as Wikidata and BabelNet [33,21],\nare available. Koloski et al. [10] proposed a document representation approach\nthat fused multiple transformer-based representations with a knowledge graph-\ngrounded embedding. The method building on the knowledge enabled represen-\ntations [22], treated each n-gram tuple as a candidate entity, matched it to the\nknowledge graph, retrieved the embedding if present, and aggregated these em-\nbeddings into a single representation vector per document. These representations\nproved highly expressive for downstream tasks like multilingual semantic textual\nsimilarity assessment [37]. However, the work did not explore document represen-\ntations from generative and large language models [3] or apply sophisticated en-\ntity linking and word sense disambiguation [19]. Additionally, combining multiple\nrepresentations is impractical for real applications due to the high-dimensional\ninputs negatively impacting classifier learning [9]. One solution is to project\nhigh-dimensional inputs to a lower-dimensional space using dimensionality re-\nduction methods like singular-value decomposition. Studies [12,38] show that\nthis procedure not only preserves the representations but also creates more rep-\nresentative spaces, further improving final-task performance. On the other side\nrecent works show that contextual embeddings live on low-dimensional geometry\n[8]. In addition, road-map for unifying LLMs and knowledge bases was recently\nproposed [23] highlighting the potential of the symbiosis. Leveraging computa-\ntional resources, evolutionary-based AutoML for learning document represen-"}, {"title": "BabelFusion: Methodology", "content": "We proceed by discussing BabelFusion, the key contribution of this work. Let\nD = {T,Y} denote a dataset, where T is a collection of textual documents\nand Y is a collection of corresponding labels. Let g be a representation learning\nfunction that maps the texts to a real-valued space of dimension d, such that\ng(T) \u2192 Xtxt \u2208 Rd. Let KG represent a knowledge graph, and let k be a function\nthat, for a given text (t), detects relevant entries in the knowledge graph and\nretrieves a list of vector representations of these detected entries, correlated with\nthe text from the knowledge graph, such that k(t) \u2192 {e1, e2,..., en}, where each\nei \u2208 R. Having obtained a list of embeddings for a single document, we next\naverage them to transform the the collection of entity embeddings to a single\nvector in Re. This results in a representation of the texts as Xkg."}, {"title": "Fusing text and knowledge graphs", "content": "Given the text-based representations Xtxt and the knowledge graph representa-\ntions Xkg, we aim to concatenate these representations obtain richer text rep-\nresentations. This combined representation, denoted as Xconcat, is obtained by\nconcatenating the vectors from both sources:\nXconcat = [Xtxt | Xkg] \u2208 Rd+c\nHowever, concatenating them directly and using them as concatenated results\ninto higher (d + c) dimensions, which can degrade classifier performances as\nmore dimensions can actually harm classifier performance due to the curse of\ndimensionality [9,2].\nOnce we have Xconcat, we apply Singular Value Decomposition (SVD) [24]\nto reduce its dimensionality and capture the most significant features. SVD\ndecomposes Xconcat into three matrices: U, \u03a3, and V, such that:\nXconcat = U\u03a3VT\nHere, U contains the left singular vectors, \u03a3 is a diagonal matrix with singular\nvalues, and V contains the right singular vectors. To focus on the most relevant\ninformation, we perform a truncated SVD by selecting only the top k singular\nvalues and their corresponding singular vectors. Mathematically, we truncate \u03a3\nto \u03a3k by keeping only the k highest singular values, and similarly truncate U\nand V to Uk and Vk, respectively. By multiplying these truncated matrices, we\nobtain the final representation Xfinal = Uk\u03a3kV \u2208 Rk.\nThe truncation reduces the dimensionality of Xconcat while preserving the\nmost important features [38]."}, {"title": "AutoML: Learning to classify", "content": "To classify the documents into the y\nlabels, we fit a function f over the representation Xfinal, such that f(Xfinal) \u2192\n{0,...,y}. We usually learn by selecting over a family of functions with respect\nto some minimization of error. Specifically, we focus on applying the TPOT [15]\nlibrary as an AutoML approach that leverages genetic algorithms to search the\nspace of functions f that minimize some error between the real labels (y) and\nthe predicted labels \u0177, in our case the negative log loss defined as:\nAUTOML(L(Xfinal)), L = - \u03a3 \u03a3 yj log(\u0177j)"}, {"title": "Contemporary document representations", "content": "Documents can be represented by both encoder-based and decoder-based large\nlanguage models (LLMs). With this in mind, we explore several methods that\nmap the documents into dense high-dimensional real space, g(T) \u2192 X \u2208 Rd.\nFor decoder-based models, we use the recently proposed LLM2Vec paradigm [3]"}, {"title": "Knowledge representations", "content": "In our experiments, we use the WikiData subgraph of BabelNet, which con-\ntains embedded nodes of the WikiData knowledge graph, using the RotatE [31]\nmethod in a 512-dimensional real-valued space. We use GraphVite [36] to obtain\nthe embeddings. First, we define the mapping function k, which produces a set\nof entities present in a knowledge graph. We employ Babelfy [19], a method that\noperates on the following principle:Given a lexicalized semantic network (Babel-\nNet) and an input text, Babelfy identifies all linkable fragments. It then performs\na graph-based semantic interpretation, constructing a graph where nodes repre-\nsent candidate meanings and edges denote semantic relationships. The algorithm\nextracts the densest subgraph as the best candidate meaning for each fragment.\nThe resulting output is a list of these candidate meanings, providing a coher-\nent semantic representation of the input text Example of one such mapping is\npresented in Figure 2."}, {"title": "Experimental setup", "content": "In this section we present the experimental setup. We present the dataset s for\nevaluation in Section 3.1, followed by the evaluation setup in Section 3.2."}, {"title": "Datasets", "content": "We aim to evaluate the proposed method in two distinct classification domains:\nsentiment analysis and news genre classification. For sentiment analysis, we uti-\nlize the standard Amazon Reviews for sentiment Analysis dataset, which includes\nreviews from three different subforums (Books, DVD, and Music), as well as a\nhate speech classification dataset consisting of short social media posts catego-\nrized into hate speech and non-hate speech [26]. For news genre classification,"}, {"title": "Evaluation setup", "content": "We aim to evaluate the performance potential of knowledge-induced, low-dimensional\nrepresentations for document classification.\nBaselines Our objective is to enhance document representation quality. The\nbaseline method involves training a linear classifier with ridge regression penal-"}, {"title": "End-to-end classification", "content": "We present the results of the best performing BabelFusion approach compared\nto the baseline Ridge classifier over the text in high dimensions in Table 3.\nOur proposed method outperformed the baseline on average by 0.52%, with the\ndifference being statistically significant as per the Wilcoxon Signed-Rank Test\n(statistic\n98.0, p-value = 0.01)."}, {"title": "Impact of dimensionality reduction", "content": "Next, we analyse the impact of the projected dimension c on performance. We\nshow the results in Figure 3. Learning in lower dimensions for the genre, hate\nspeech, and MLDoc datasets shows lower results for all embeddings, as expected.\nInterestingly, for the amazon datasets, where some embeddings (mxbai and oa-\nsmall) outperform the full-text-based representation baselines, we can learn even\nif we project in two dimensions. We also find that on all datasets we can out-\nperform baselines for all methods on different dimensions, even on the more\ndifficult datasets hatespeech and XGENRE. We examined the correlation be-\ntween the dimension and the score across all embeddings and found no statisti-\ncal correlation, implying that the dimensionality of the projection is crucial and\nshould be evaluated for each dataset and problem. We then analysed the corre-\nlation between each dataset's dimension and score. We find that it is significant\nfor the Hatespeech dataset (correlation=0.62, p-value<0.01, CI-95=[0.40, 0.78]),\nthe XGENRE dataset (correlation=0.52, p-value<0.01, CI-95=[-0.26,0.33]) and\nMLDoc (correlation=0.47, p-value=0.01, CI-95=[0.20, 0.67]).\nNext we aggregate the results across dimensions for Embeddings (Figure4)\nand for Datasets (Figure 5) and compare them what happens when we learn\non this joint space without any projection (left-most column in the heatmaps,\ndenoted as 'baseline'). We see that across embeddings, we can learn more ro-\nbust spaces by injection embedded entities and projection to low dimensions,\nmeaning that we cannot only learn in low-dimensional space but obtain better\nresults. This follows the related work by \u0160krlj et al. [38], where it was shown\nthat compressing the space lowers the memory footprint and can improve the\nend performance. Across datasets, we notice that learning from low dimensions is\nalso, on average, better than learning from high, as learning from low dimensions\nimproves the results."}, {"title": "Few-shot learning", "content": "In Figure 6, we show the method's performance on fractions of data. The re-\nsults indicate that the method performs on par compared to text-only baselines\non the same fraction of data. For some datasets (DVD, music and books), the\nmethod achieved better results with a smaller sample (compared to the full-shot\napproach). Recent works [7,18] have shown that this can be the case for LLMs\nWe also note that the Hatespeech dataset, being one of the shortest, had approx-\nimately 22% of its documents without any matched entity against the knowledge\ngraph. The high number of detected entries in BabelNet for the XGENRE and\nMLDoc datasets reflects the nature of the data, as news articles tend to be\nlonger on average. The results indicate that the nature and length of the text\nsignificantly influence the number of matched entities."}, {"title": "Qualitative results", "content": "Statistics of the retrieved KG entries Next, we explore the statistics of the\nretrieved entries in WikiData as matched by Babelfy [21], as shown in Table 4.\nFor the datasets derived from news corpora, we observe a higher extraction of\nconcepts. We attribute this to the standardized language typically used in news\nwriting, in contrast to the non-standard language, slurs, and typos prevalent in\nsocial media posts, as noted in the seminal paper on the Hatespeech dataset [26]."}, {"title": "Conclusions and Further Work", "content": "In this work, we propose new knowledge-enriched, LLM-based low-dimensional\ndocument embeddings. The results suggest that fusing modalities in low dimen-\nsions not only preserves space but also enables efficient representations that\nsurpass even proprietary embeddings. This is in line to an extent with the work\nof [8] where it was shown that contextual embeddings can be approximated by\nlow-dimensional geometry. We advance the related line of work by introducing\nsophisticated named entity linking and AutoML while leveraging representations\nextracted from LLMs. Our findings demonstrate that these embeddings perform\nwell across different datasets and domains, showing promising potential for fu-\nture applications. The results indicate that even in unsupervised applications,\nsuch as clustering, these lightweight embeddings might provide robust document\nrepresentations. Furthermore, by applying AutoML, we show that learning in low\ndimensions is feasible and competitive with high-dimensional embeddings.\nThe implications of these results are that: A1. KG-enriched representations\ncan outperform text representations, including both encoder and decoder-based\nmodels (A3), and that learning on these representations in low dimensions is\nfeasible (A2) even for proprietary document representations (A4).\nIn this study, we utilized only a portion of the BabelNet graph and the Wiki-\nData5m subgraph. In the future, we aim to include the entire graph, improve\nthe fusion process with advanced disambiguation methods, and explore injection\nof external knowledge at the token level to create a synergy between LLMs and\nKGs, as suggested by Pan [23]. Finally, we plan to explore if recursive compres-\nsion of our proposed representations provides better down-stream results."}]}