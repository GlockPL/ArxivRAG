{"title": "AutoML-guided Fusion of Entity and LLM-based representations", "authors": ["Boshko Koloski", "Senja Pollak", "Roberto Navigli", "Bla\u017e \u0160krlj"], "abstract": "Large semantic knowledge bases are grounded in factual knowledge. However, recent approaches to dense text representations (embeddings) do not efficiently exploit these resources. Dense and robust representations of documents are essential for effectively solving downstream classification and retrieval tasks. This work demonstrates that injecting embedded information from knowledge bases can augment the performance of contemporary Large Language Model (LLM)-based representations for the task of text classification. Further, by considering automated machine learning (AutoML) with the fused representation space, we demonstrate it is possible to improve classification accuracy even if we use low-dimensional projections of the original representation space obtained via efficient matrix factorization. This result shows that significantly faster classifiers can be achieved with minimal or no loss in predictive performance, as demonstrated using five strong LLM baselines on six diverse real-life datasets.", "sections": [{"title": "Introduction and Background", "content": "Robust document representations are crucial for many NLP tasks [20]. Early methods like bag-of-words were limited, relying on counting schemes and resulting in high-dimensional representations without capturing richer semantics. Techniques such as Latent Semantic Analysis (LSA) [5] addressed this by projecting high-dimensional spaces into lower dimensions, providing more meaningful representations even in multilingual contexts [11]. The representation learning paradigm [4] popularized learning representations across modalities as an auxiliary task for training deep learning models. Le et al. [14] introduced Doc2Vec, which learns word or paragraph-level representations by corrupting text and predicting the missing parts using shallow neural networks. This technique remains key for obtaining document representations. Depending on how the corruption and learning are conducted, there are two main paradigms: masked language modeling and causal language modeling. Devlin et al. [6] demonstrated that randomly masking parts of the input (masked language modeling) and sequentially predicting them with the Transformer architecture [32] not only performs well but also learns contextual word embeddings. Conversely, Radford et al. [25] approached document representation learning as a generative task, where a Transformer model [32] is fed part of the input and tasked with predicting the remainder. This training paradigm produces generative models and is currently the most popular approach towards LLMs [35]. However, both paradigms focus on contextual word embeddings, which are insufficient for document-level representations. To leverage the expressiveness of deep models, Reimers et al. [27] proposed using BERT-based embeddings as a foundation for learning document-level representations via Siamese networks. Similarly, LLM2Vec [3] suggested representing documents by extracting the internal weights of large generative models, such as LLaMa3 [1]. These embeddings can be efficiently obtained from a pre-trained model and serve as a strong competitor in a recently proposed massive text-embeddings benchmark (MTEB) [20]. Contrastive representation learning [16] involves learning document representations by placing similar documents together and repelling dissimilar ones. Angle [17] was recently proposed, where models optimize representations based on the angle between their vectors in the latent space. However, high-dimensional representations can impair classifier performance due to the curse of dimensionality [9], increase memory footprint for storage and retrieval, and adapting these representations to specific corpora is laborious and expensive. Alternatively, large semantic knowledge bases grounded in factual knowledge, such as Wikidata and BabelNet [33,21], are available. Koloski et al. [10] proposed a document representation approach that fused multiple transformer-based representations with a knowledge graph-grounded embedding. The method building on the knowledge enabled representations [22], treated each n-gram tuple as a candidate entity, matched it to the knowledge graph, retrieved the embedding if present, and aggregated these embeddings into a single representation vector per document. These representations proved highly expressive for downstream tasks like multilingual semantic textual similarity assessment [37]. However, the work did not explore document representations from generative and large language models [3] or apply sophisticated entity linking and word sense disambiguation [19]. Additionally, combining multiple representations is impractical for real applications due to the high-dimensional inputs negatively impacting classifier learning [9]. One solution is to project high-dimensional inputs to a lower-dimensional space using dimensionality reduction methods like singular-value decomposition. Studies [12,38] show that this procedure not only preserves the representations but also creates more representative spaces, further improving final-task performance. On the other side recent works show that contextual embeddings live on low-dimensional geometry [8]. In addition, road-map for unifying LLMs and knowledge bases was recently proposed [23] highlighting the potential of the symbiosis. Leveraging computational resources, evolutionary-based AutoML for learning document representations and models have achieved significant results [30]. Motivated by these parallel approaches, we propose BabelFusion (see Figurel), a novel approach towards document representation for classification where we leverage AutoML and low-dimensional projection of knowledge-informed representations, utilizing sophisticated entity linking [19]. The novelty of this work can be summarized as follows: Firstly, to our knowledge, this is the first work that exploits the effect of injecting knowledge-based representations into LLM-based representations. Secondly, we demonstrate that by projecting in low dimensions, one can learn robust and expressive representations, which, when combined with simple models, achieve competitive results in both full-shot and few-shot classification. We present the methodology in Section 2, followed by the experimental setting in Section 3. Section 4 presents the results which are followed by discussion in Section 5."}, {"title": "BabelFusion: Methodology", "content": "We proceed by discussing BabelFusion, the key contribution of this work. Let D = {T,Y} denote a dataset, where T is a collection of textual documents and Y is a collection of corresponding labels. Let g be a representation learning function that maps the texts to a real-valued space of dimension d, such that g(T) \u2192 Xtxt \u2208 Rd. Let KG represent a knowledge graph, and let k be a function that, for a given text (t), detects relevant entries in the knowledge graph and retrieves a list of vector representations of these detected entries, correlated with the text from the knowledge graph, such that k(t) \u2192 {e1, e2,..., en}, where each ei \u2208 Re. Having obtained a list of embeddings for a single document, we next average them to transform the the collection of entity embeddings to a single vector in Re. This results in a representation of the texts as Xkg."}, {"title": "Fusing text and knowledge graphs", "content": "Given the text-based representations Xtxt and the knowledge graph representations Xkg, we aim to concatenate these representations obtain richer text representations. This combined representation, denoted as Xconcat, is obtained by concatenating the vectors from both sources:\nXconcat = [Xtxt | Xkg] \u2208 Rd+c\nHowever, concatenating them directly and using them as concatenated results into higher (d + c) dimensions, which can degrade classifier performances as more dimensions can actually harm classifier performance due to the curse of dimensionality [9,2].\nOnce we have Xconcat, we apply Singular Value Decomposition (SVD) [24] to reduce its dimensionality and capture the most significant features. SVD decomposes Xconcat into three matrices: U, \u03a3, and V, such that:\nXconcat = U\u03a3VT\nHere, U contains the left singular vectors, \u03a3 is a diagonal matrix with singular values, and V contains the right singular vectors. To focus on the most relevant information, we perform a truncated SVD by selecting only the top k singular values and their corresponding singular vectors. Mathematically, we truncate \u03a3 to \u03a3k by keeping only the k highest singular values, and similarly truncate U and V to Uk and Vk, respectively. By multiplying these truncated matrices, we obtain the final representation Xfinal = Uk\u03a3kVT \u2208 Rk.\nThe truncation reduces the dimensionality of Xconcat while preserving the most important features [38]."}, {"title": "AutoML: Learning to classify", "content": "To classify the documents into the y labels, we fit a function f over the representation Xfinal, such that f(Xfinal) \u2192 {0,...,y}. We usually learn by selecting over a family of functions with respect to some minimization of error. Specifically, we focus on applying the TPOT [15] library as an AutoML approach that leverages genetic algorithms to search the space of functions f that minimize some error between the real labels (y) and the predicted labels \u0177, in our case the negative log loss defined as:\nAUTOML(L(Xfinal)), L = - \\sum_{i=0}^N \\sum_{j=0}^y y_j log(\\hat{y}_j)"}, {"title": "Contemporary document representations", "content": "Documents can be represented by both encoder-based and decoder-based large language models (LLMs). With this in mind, we explore several methods that map the documents into dense high-dimensional real space, g(T) \u2192 X \u2208 Rd. For decoder-based models, we use the recently proposed LLM2Vec paradigm [3]"}, {"title": "Knowledge representations", "content": "In our experiments, we use the WikiData subgraph of BabelNet, which contains embedded nodes of the WikiData knowledge graph, using the RotatE [31] method in a 512-dimensional real-valued space. We use GraphVite [36] to obtain the embeddings. First, we define the mapping function k, which produces a set of entities present in a knowledge graph. We employ Babelfy [19], a method that operates on the following principle:Given a lexicalized semantic network (BabelNet) and an input text, Babelfy identifies all linkable fragments. It then performs a graph-based semantic interpretation, constructing a graph where nodes represent candidate meanings and edges denote semantic relationships. The algorithm extracts the densest subgraph as the best candidate meaning for each fragment. The resulting output is a list of these candidate meanings, providing a coherent semantic representation of the input text Example of one such mapping is presented in Figure 2."}, {"title": "Experimental setup", "content": "In this section we present the experimental setup. We present the dataset s for evaluation in Section 3.1, followed by the evaluation setup in Section 3.2."}, {"title": "Datasets", "content": "We aim to evaluate the proposed method in two distinct classification domains: sentiment analysis and news genre classification. For sentiment analysis, we utilize the standard Amazon Reviews for sentiment Analysis dataset, which includes reviews from three different subforums (Books, DVD, and Music), as well as a hate speech classification dataset consisting of short social media posts categorized into hate speech and non-hate speech [26]. For news genre classification,"}, {"title": "Evaluation setup", "content": "We aim to evaluate the performance potential of knowledge-induced, low-dimensional representations for document classification.\nBaselines Our objective is to enhance document representation quality. The baseline method involves training a linear classifier with ridge regression penal-"}, {"title": "Results", "content": "We proceed by discussing results of experimental evaluation outlined in Section 3."}, {"title": "End-to-end classification", "content": "We present the results of the best performing BabelFusion approach compared to the baseline Ridge classifier over the text in high dimensions in Table 3. Our proposed method outperformed the baseline on average by 0.52%, with the difference being statistically significant as per the Wilcoxon Signed-Rank Test (statistic = 98.0, p-value = 0.01)."}, {"title": "Impact of dimensionality reduction", "content": "Next, we analyse the impact of the projected dimension c on performance. We show the results in Figure 3. Learning in lower dimensions for the genre, hate speech, and MLDoc datasets shows lower results for all embeddings, as expected. Interestingly, for the amazon datasets, where some embeddings (mxbai and oa-small) outperform the full-text-based representation baselines, we can learn even if we project in two dimensions. We also find that on all datasets we can outperform baselines for all methods on different dimensions, even on the more difficult datasets hatespeech and XGENRE. We examined the correlation between the dimension and the score across all embeddings and found no statistical correlation, implying that the dimensionality of the projection is crucial and should be evaluated for each dataset and problem. We then analysed the correlation between each dataset's dimension and score. We find that it is significant for the Hatespeech dataset (correlation=0.62, p-value<0.01, CI-95=[0.40, 0.78]), the XGENRE dataset (correlation=0.52, p-value<0.01, CI-95=[-0.26,0.33]) and MLDoc (correlation=0.47, p-value=0.01, CI-95=[0.20, 0.67]).\nNext we aggregate the results across dimensions for Embeddings (Figure4) and for Datasets (Figure 5) and compare them what happens when we learn on this joint space without any projection (left-most column in the heatmaps, denoted as 'baseline'). We see that across embeddings, we can learn more robust spaces by injection embedded entities and projection to low dimensions, meaning that we cannot only learn in low-dimensional space but obtain better results. This follows the related work by \u0160krlj et al. [38], where it was shown that compressing the space lowers the memory footprint and can improve the end performance. Across datasets, we notice that learning from low dimensions is also, on average, better than learning from high, as learning from low dimensions improves the results."}, {"title": "Few-shot learning", "content": "In Figure 6, we show the method's performance on fractions of data. The results indicate that the method performs on par compared to text-only baselines on the same fraction of data. For some datasets (DVD, music and books), the method achieved better results with a smaller sample (compared to the full-shot approach). Recent works [7,18] have shown that this can be the case for LLMs"}, {"title": "Qualitative results", "content": "Statistics of the retrieved KG entries Next, we explore the statistics of the retrieved entries in WikiData as matched by Babelfy [21], as shown in Table 4. For the datasets derived from news corpora, we observe a higher extraction of concepts. We attribute this to the standardized language typically used in news writing, in contrast to the non-standard language, slurs, and typos prevalent in social media posts, as noted in the seminal paper on the Hatespeech dataset [26]."}, {"title": "Conclusions and Further Work", "content": "In this work, we propose new knowledge-enriched, LLM-based low-dimensional document embeddings. The results suggest that fusing modalities in low dimensions not only preserves space but also enables efficient representations that surpass even proprietary embeddings. This is in line to an extent with the work of [8] where it was shown that contextual embeddings can be approximated by low-dimensional geometry. We advance the related line of work by introducing sophisticated named entity linking and AutoML while leveraging representations extracted from LLMs. Our findings demonstrate that these embeddings perform well across different datasets and domains, showing promising potential for future applications. The results indicate that even in unsupervised applications, such as clustering, these lightweight embeddings might provide robust document representations. Furthermore, by applying AutoML, we show that learning in low dimensions is feasible and competitive with high-dimensional embeddings.\nThe implications of these results are that: A1. KG-enriched representations can outperform text representations, including both encoder and decoder-based models (A3), and that learning on these representations in low dimensions is feasible (A2) even for proprietary document representations (A4).\nIn this study, we utilized only a portion of the BabelNet graph and the Wiki-Data5m subgraph. In the future, we aim to include the entire graph, improve the fusion process with advanced disambiguation methods, and explore injection of external knowledge at the token level to create a synergy between LLMs and KGs, as suggested by Pan [23]. Finally, we plan to explore if recursive compression of our proposed representations provides better down-stream results."}]}