{"title": "The Optimist: Towards Fully Automated Graph Theory Research", "authors": ["Randy Davila"], "abstract": "This paper introduces the Optimist, an autonomous system developed to advance automated conjecture generation in graph theory. Leveraging mixed-integer programming (MIP) and heuristic methods, the Optimist generates conjectures that both rediscover established theorems and propose novel inequalities. Through a combination of memory-based computation and agent-like adaptability, the Optimist iteratively refines its conjectures by integrating new data, enabling a feedback process with minimal human (or machine) intervention. Initial experiments reveal the Optimist's potential to uncover foundational results in graph theory, as well as to produce conjectures of interest for future exploration. This work also outlines the Optimist's evolving integration with a counterpart agent, the Pessimist (a human or machine agent), to establish a dueling system that will drive fully automated graph theory research.", "sections": [{"title": "1 Introduction", "content": "This paper introduces the Optimist, an autonomous agent for generating conjectures in graph theory that iteratively adapts its output based on feedback. The Optimist builds upon the principles of TxGraffiti [1] with enhancements in mixed-integer programming, heuristic search, and a memory-based structure that allow it to efficiently produce and refine conjectures. Through its agent-based framework, the Optimist can systematically generate inequalities involving graph invariants and filter results"}, {"title": "2 Related Work", "content": "The concept of intelligent machines contributing to mathematical research dates back to Turing's 1948 proposal, which envisioned machines capable of high-level reasoning with minimal external data [2]. This vision set the stage for early computer-assisted mathematics. Among the first systems to embody this idea was Newell and Simon's Logic Theorist, developed in the 1950s, which demonstrated the potential for machines to prove theorems in first-order logic. Newell and Simon anticipated that computers would one day play a central role in mathematical discovery [3]. While early systems like the Logic Theorist were focused on theorem proving, concurrent efforts in computer-assisted conjecturing emerged with Wang's Program II [4], designed to generate statements interpretable as conjectures in logic. A significant challenge in Wang's work, however, was filtering meaningful statements from the overwhelming volume of output. As Wang noted:\n\"The number of theorems printed out after running the machine for a few hours is so formidable that the writer has not even attempted to analyze the mass of data obtained\" [4].\nRefining large lists of generated statements into meaningful conjectures became a focal challenge in automated conjecture systems, prompting later programs to develop heuristics for identifying statements of mathematical significance."}, {"title": "2.1 Fajtlowicz's Graffiti and the Dalmatian Heuristic", "content": "A major advancement in filtering meaningful conjectures emerged with Fajtlowicz's Graffiti program in the 1980s. Graffiti generated inequalities between real-valued functions (invariants) on mathematical objects, primarily within graph theory. The program operated on a database of graphs, aiming to identify inequalities of the form:\n$\\alpha \\leq f(invariants)$,\nwhere $\\alpha$ is a target invariant and $f(invariants)$ is an expression involving other invariants, often in the form of sums, products, or more complex combinations constructed through an arithmetic expression tree. For example, Graffiti might conjecture an inequality such as $\\alpha \\leq n/2$, where $\\alpha$ represents the independence number of a graph, and $n$ denotes the graph's order (number of vertices).\nThe challenge of distinguishing nontrivial conjectures from trivial ones persisted. To address this, Graffiti introduced the innovative Dalmatian heuristic, a technique for refining conjectures based on their significance and validity across known examples. The Dalmatian heuristic operates with two primary criteria:\nThe Dalmatian Heuristic\n\u2022 Truth Test: The conjectured inequality must hold for all graphs in the database.\n\u2022 Significance Test: The conjecture must provide a stronger bound for at least one graph compared to previously generated conjectures.\nFajtlowicz described the heuristic's process as follows:\n\"The program keeps track of conjectures made in the past, and when it encounters a new candidate for a conjecture, it first verifies whether an example in the database shows that the conjecture does not follow from earlier conjectures. If no such example exists, the conjecture is rejected as non-informative. If one exists, the program proceeds with testing the conjecture's correctness, and then checks whether the conjecture should be rejected by other heuristics. If accepted, the list of conjectures is revised, and less informative conjectures are removed from the list and stored separately in case the new conjecture is refuted later.\" [5]\nThe Dalmatian heuristic not only reduced the number of conjectures but also enhanced their relevance by prioritizing conjectures that advanced current knowledge. When applied to identifying bounds on invariants, particularly those of active research interest, Graffiti produced conjectures where existing theory offered insufficient predictions for invariant values. Such conjectures, if extending current understanding, were considered significant. Indeed, Graffiti generated numerous conjectures that contributed substantially to both graph theory [6\u201331] and mathematical chemistry [32-39]. For further details on Graffiti and the Dalmatian heuristic, see [1, 40]."}, {"title": "2.2 Extensions of the Dalmatian Heuristic: Graffiti.pc and Conjecturing", "content": "Building upon the Dalmatian heuristic, DeLaVi\u00f1a's Graffiti.pc and Larson's Con-jecturing programs extended automated conjecturing in graph theory and beyond."}, {"title": "2.3 TxGraffiti", "content": "TxGraffiti [1] is an automated conjecturing program maintained by the author that focuses on generating meaningful conjectures in graph theory by leveraging principles similar to Fajtlowicz's Graffiti, particularly the emphasis on the mathematical strength of statements. However, TxGraffiti applies linear optimization models to a precomputed tabular dataset of graph invariants, generating both upper and lower bounds for target invariants by solving optimization problems that minimize or maximize the discrepancy between a target invariant and an expression involving other invariants. Specifically, to establish an upper bound, TxGraffiti minimizes a linear objective subject to constraints that enforce the inequality across all graphs meeting certain boolean conditions. This approach results in inequalities of the form $\\alpha \\leq m \\cdot f(invariants) + b$, where $\\alpha$ is the target invariant, $f(invariants)$ represents other graph properties, and $m$ and $b$ are optimized scalars. Similarly, lower bounds are generated by maximizing this objective under reversed constraints, offering a dual approach to bounding invariants within the dataset.\nTo prioritize conjectures, TxGraffiti employs the touch number heuristic, which measures how often an inequality holds as an equality across different graphs, thus reflecting the conjecture's strength and relevance. Conjectures with high touch numbers are prioritized, as they suggest a tighter relationship between invariants. Additionally, TxGraffiti uses the static-Dalmatian heuristic, a refinement of the original Dalmatian heuristic, to eliminate redundant conjectures by filtering out those that can be derived from others via transitivity. This heuristic operates on a fixed set of conjectures, iteratively discarding any conjecture that does not introduce new information relative to previously accepted ones. Together, the optimization methods and"}, {"title": "3 The Optimist: An Automated Conjecturing Agent", "content": "The Optimist is an automated conjecturing agent dedicated to conjecture generation in graph theory, designed to construct, evaluate, and refine conjectures systematically. Drawing inspiration from the principles underlying TxGraffiti, Optimist implements a range of computational techniques aimed at producing conjectures with mathematical rigor and relevance. The Optimist incorporates foundational components, including graph data, invariants, and a repository of known theorems, which serve as the basis for its conjecture generation.\nOptimist distinguishes itself from static conjecturing tools through its adaptability and agent-like autonomy. By employing iterative refinement and adaptive updates, Optimist dynamically integrates new information, such as additional theorems or counterexamples, which allows it to iteratively enhance the quality and accuracy of its conjectures. This capability to evolve its knowledge base and refine its conjectures in response to new data characterizes Optimist as a continuously learning system within automated reasoning, with the potential to contribute robustly to the development of theoretical insights in graph theory."}, {"title": "3.1 Initial Knowledge and Setup", "content": "At the core of the Optimist agent's framework is an initial set of structured inputs that constitute its foundational knowledge, forming the basis for conjecture formulation. This setup includes three primary components: a collection of graphs, a dictionary of computable graph invariant functions, and a repository of known theorems. Each graph in the collection provides a distinct context in which relationships between invariants can be examined, thereby supplying diverse examples to inform potential conjectures.\nThe dictionary of invariants encompasses functions that compute essential properties of each graph, such as independence number, vertex count, and bipartiteness, which serve as key components in constructing conjectures. These invariants provide quantifiable data points that the Optimist utilizes in identifying relationships and generating bounds.\nFurthermore, Optimist references a repository of known theorems to prevent the generation of redundant conjectures that merely replicate established results. This enables the system to focus on conjectures that extend beyond current knowledge, prioritizing relationships that are novel or have yet to be formally proven.\nTo facilitate conjecture generation and analysis, Optimist employs several internal data structures. The framework organizes conjectures into separate lists of upper and"}, {"title": "3.2 Constructing a Knowledge Base from Graph Invariants", "content": "To systematically investigate relationships between graph invariants, the Optimist framework constructs a tabular dataset, termed its knowledge base, or knowledge table (implemented as a Pandas DataFrame). In this structure, each row corresponds to a unique graph instance, while each column represents a specific invariant or boolean property. This organization enables efficient data access and facilitates conjecture generation by allowing the framework to process invariant data in a manner akin to feature-based analysis in machine learning models. Analogous to how learning models identify patterns across features, Optimist identifies potential conjectures by exploring relationships among these stored invariants.\nA primary advantage of this knowledge base is that it stores all invariant values upon initial computation, obviating the need for recomputation during conjecture generation and validation. Given that calculating graph invariants can be computationally intensive, this storage approach significantly reduces the overhead of conjecture generation, streamlining the exploration process. By storing these invariant values centrally, Optimist can efficiently retrieve relationships among invariants in real time, thereby facilitating more dynamic conjecture formulation.\nThis knowledge base thus functions as a structured \"memory\" for the Optimist agent, enabling it to effectively leverage previously computed data in the same way a reasoning system might recall facts to inform subsequent decision-making. Through this design, Optimist not only enhances computational efficiency but also supports a more comprehensive exploration of graph-theoretic relationships."}, {"title": "3.3 Mixed-Integer Programming for Conjecture Generation", "content": "The Optimist framework employs a mixed-integer programming (MIP) approach to generate conjectures establishing upper and lower bounds on a target invariant. Unlike TxGraffiti, which typically relates a single invariant to the target invariant, Optimist formulates conjectures by considering linear combinations of multiple invariants under various boolean conditions. The Optimist agent simultaneously seeks both upper and lower bounds, and in cases where bounds converge, it identifies exact equalities.\nTo produce meaningful bounds on a target invariant, Optimist solves two MIP formulations that optimize for tight upper and lower bounds. The framework emphasizes maximizing instances where each bound holds as an equality, ensuring conjectures that are not only accurate but also sharp. This focus on equality instances aligns with the goal of producing conjectures that provide informative bounds on extreme values of the target invariant.\nLet $\\alpha$ denote the target invariant, and let $f(invariants)$ represent a linear combination of other invariants. The bounds we seek can be expressed as:\n$\\alpha\\leq f(invariants) + b$ (upper bound) and $\\alpha \\geq f(invariants) + b$ (lower bound),"}, {"title": "3.4 Systematic Generation of Conjectures", "content": "The Optimist framework explores diverse combinations of invariants systematically, aiming to generate a comprehensive set of conjectures for each target invariant. For each target invariant, Optimist constructs bounds by iterating through pairs of explanatory invariants, each conditioned by a single boolean property. This process is managed through the function make_all_linear_conjectures, which organizes these combinations to generate upper and lower bounds; see Listing 1.\nThe function make_all_linear_conjectures performs the following operations:\n\u2022 For each pair of invariants, it applies all specified boolean properties, ensuring that the conjecture generation covers a broad space of possible relationships.\n\u2022 Within each pair, the target invariant is constrained to avoid redundancies, ensuring that neither explanatory invariant directly corresponds to the target.\n\u2022 The function calls the MIP-based make_linear_conjectures function to compute bounds on the target invariant, yielding a candidate set of upper and lower conjectures.\nThe output of this procedure is a set of candidate conjectures for each target invariant, reflecting a systematic approach to exploring multi-invariant relationships under varied boolean conditions."}, {"title": "3.5 The Hazel Heuristic", "content": "After invoking the make_all_linear_conjectures function on a target invariant, the Optimist agent typically produces extensive lists of potential conjectures often in the hundreds depending on the numerical and boolean properties considered. To identify and prioritize conjectures with strong empirical backing, Optimist applies the Hazel Heuristic, a filtering process centered on each conjecture's touch number.\nThe touch number of a conjectured inequality is defined as the number of graphs in the Optimist knowledge base for which the inequality holds as an equality. A high touch number suggests that the conjectured bound is not only generally valid but also sharp for a significant subset of graphs, indicating a potentially fundamental relationship between the target invariant and the explanatory invariants. High-touch conjectures are, therefore, more likely to capture structural properties of graph invariants and yield insights beyond loose or incidental bounds.\nThe Hazel Heuristic applies three successive steps:\n1. Deduplication: Removes duplicate conjectures to ensure the uniqueness of bounds.\n2. Filtering: Discards conjectures with touch numbers below a specified threshold, removing bounds that lack consistent sharpness across the dataset.\n3. Sorting: Orders conjectures in descending order of touch number, bringing conjectures with the highest empirical support to the forefront.\nBy prioritizing conjectures with the highest touch numbers, the Hazel Heuristic ensures that the most empirically significant conjectures are advanced for further analysis. This ranking approach confers several advantages:\n\u2022 Relevance: Conjectures with high touch numbers are more likely to represent tight bounds, making them strong candidates for formal validation or proof.\n\u2022 Robustness: A high touch number indicates that a conjecture's bound is consistently representative across diverse graph structures, enhancing its robustness and potential generalizability."}, {"title": "3.6 The Morgan Heuristic", "content": "The Morgan Heuristic identifies and removes redundant conjectures, prioritizing the most general conjectures with broad applicability. Specifically, a conjecture is flagged as redundant if it shares the same conclusion as another but is based on a more specific (less general) hypothesis. By focusing on generality, this heuristic ensures that the final set of conjectures includes only the most powerful and widely applicable statements.\nFor example, consider the two conjectures in Listing 3. Every tree is a bipartite graph, but not every bipartite graph is a tree. Therefore, Conjecture 4, which applies to all connected bipartite graphs, is more general than Conjecture 2, which applies only to trees. As such, Conjecture 2 is discarded in favor of Conjecture 4.\nIn practice, the generality of a conjecture's hypothesis is determined by counting the number of graphs in the Optimist knowledge base that satisfy the hypothesis. If one hypothesis applies to a larger subset of graphs than another, it is considered more general. This count-based approach, though heuristic, provides a practical method for determining generality even when relationships between graph classes are not fully defined; see the implementation in Listing 4."}, {"title": "3.7 The Strong-Smokey and Weak-Smokey Heuristics", "content": "To further refine the conjecture set, the Optimist agent employs two selective heuristics: the weak-smokey and strong-smokey heuristics. Both heuristics reduce redundancy by examining the set of sharp graphs instances where each conjectured inequality holds as an equality-though each applies distinct criteria to retain only conjectures that offer unique insights.\nThe weak-smokey heuristic iterates through conjectures in descending order of touch number, retaining conjectures that introduce new sharp graphs not already covered by previously selected conjectures. This approach prioritizes conjectures with complementary equality instances, achieving a balance between inclusiveness and informativeness. By focusing on unique sharp instances, weak-smokey substantially reduces the initial conjecture set while maintaining broad empirical relevance."}, {"title": "3.8 Conjecture Generation, Filtering, and Adaptive Knowledge Update", "content": "The Optimist agent is designed as an iterative system for conjecture generation, filtering, and continuous refinement based on new information. At the core of this process is the use of mixed-integer programming (MIP) to generate conjectures. Using combinations of graph invariants, Optimist generates potential upper and lower bounds for a target invariant. Each conjecture is initially stored in memory, organized by target invariant.\nGiven the potentially large number of conjectures generated, Optimist applies a multi-stage filtering process to ensure that only the most informative and robust conjectures are retained. The first phase employs the Hazel heuristic, which prioritizes conjectures with the highest empirical relevance by sorting them according to touch number a measure of how frequently each conjecture holds as an equality across the dataset. This step ensures that Optimist focuses on conjectures that provide tight bounds for the target invariant.\nNext, the Morgan heuristic is applied to remove redundant conjectures with more restrictive hypotheses that do not contribute new insights over more general conjectures with the same conclusion. This heuristic ensures the final conjecture set retains only the most widely applicable statements.\nA final layer of filtering applies either the strong-smokey or weak-smokey heuristic, depending on the desired strictness. The weak-smokey heuristic favors inclusiveness, retaining conjectures that add new sharp instances, while the strong-smokey heuristic strictly selects conjectures that cover strictly broader sets of sharp graphs. This stage produces a minimal, highly informative set of conjectures, prioritizing those with the broadest empirical applicability - essentially equivalent to an unpublished version of TxGraffiti called TxGraffiti II, which is available as an interactive website\u00b3.\nKnowledge Update and Counterexample Handling\nThe Optimist agent is designed to adapt dynamically to new data, allowing it to refine its conjectures and avoid rediscovering known results. Before retaining any conjecture, Optimist references a repository of established theorems, removing any conjecture implied by these known results. This approach focuses the agent's attention on novel insights.\nThe agent further adapts based on counterexamples provided by users. If a counterexample graph contradicts an existing conjecture, Optimist incorporates the new graph into its knowledge base and updates all conjectures to ensure consistency with observed data. This self-improvement process enables the agent to maintain accurate and reliable conjectures, continuously refining its knowledge and conjecture quality with each iteration. By iterating through these stages of generation, filtering, and adaptive updates, Optimist exemplifies a robust approach to autonomous reasoning and knowledge refinement in graph theory."}, {"title": "4 Case Study: Conjecture Generation for the Independence Number", "content": "This section presents a case study to evaluate the Optimist agent's ability to generate conjectures that are both empirically grounded and theoretically insightful. Our objective is to demonstrate the Optimist's effectiveness in autonomously rediscovering classical bounds on the independence number, $\\alpha(G)$, and its adaptability in refining conjectures in response to iterative user feedback.\nOptimist was assessed on its capacity to conjecture bounds on $\\alpha(G)$, a fundamental but computationally challenging graph invariant. Beginning with a minimal dataset, we examined whether the agent could autonomously derive meaningful bounds on $\\alpha(G)$ and subsequently refine its conjectures through feedback and new information."}, {"title": "4.1 Experimental Setup and Initial Conjecture Generation", "content": "The initial setup for Optimist included three of the smallest nontrivial connected graph structures: the complete graphs $K_2$ and $K_3$ and the path graph $P_3$, as shown in Figure 2. The agent was configured with a selection of invariants relevant to conjecturing $\\alpha(G)$, including the order $n$, minimum degree $\\delta$, maximum degree $\\Delta$, matching number $\\mu$, and minimum maximal matching number $\\mu^*$, among others. With this setup, Optimist could explore a variety of relationships between $\\alpha(G)$ and these properties.\nFrom this minimal configuration, Optimist generated an initial series of conjectures aimed at establishing upper and lower bounds for $\\alpha(G)$. The resulting conjectures were broad, reflecting the limited dataset and covering a variety of possible inequalities. This initial series included nearly 60 conjectures, illustrating the heuristic's dependency on dataset size and the need for further refinement to isolate the most significant bounds."}, {"title": "4.2 Iterative Knowledge Refinement through User Feedback", "content": "Following the initial conjecture set, the user identified conjectures that could be invalidated by counterexamples. For instance, the conjecture presented in Listing 7 was shown to be false for the path graph $P_6$.\nNoting this counterexample, the user informed the Optimist agent of $P_6$, prompting the agent to update its knowledge base and regenerate its conjecture set.\nWith each new graph added, Optimist refined its conjecture set, retaining inequalities that consistently held across the expanded dataset and discarding or modifying those that failed to generalize. This interactive process continued over multiple iterations, with Optimist generating conjectures and the user acting as a Pessimist by providing counterexamples. Each update expanded the agent's knowledge base, enabling it to generate more reliable conjectures. Figure 3 shows the set of counterexample graphs introduced during this process.\nThis iterative refinement process allowed Optimist to progressively narrow down the set of plausible bounds on $\\alpha(G)$, adapting its conjectures in response to new data. The user's counterexamples acted as a form of hypothesis testing, whereby each graph introduced was a probe challenging Optimist's emerging hypotheses. Through this feedback loop, the agent evolved toward a more reliable and theoretically grounded set of conjectures.\nOccasionally, the Optimist agent generated conjectures that aligned with known bounds or equations for the independence number, often appearing at the top of its conjecture lists. When these familiar relationships were recognized by the user, they were communicated to the agent, enabling Optimist to store this knowledge and exclude known results from its conjecture set going forward. This process allowed"}, {"title": "4.3 The Evolution of the Optimist Agent and the Concept of GraphMind", "content": "The Optimist agent begins as a knowledge-based system, equipped with a foundational set of graphs and a suite of functions for computing graph invariants. From this initial setup, the agent constructs a tabular dataset, representing each graph as a unique instance and each function as a feature. This tabular structure serves as a dynamic memory, enabling Optimist to generate conjectures based on observed relationships across invariants, initially producing bounds and identities for specific graph properties.\nThrough interaction with a counterexample generator-referred to as the Pessimist, which can be either a human user or an autonomous agent-Optimist adapts its conjectures and knowledge base. When a counterexample is found that disproves a conjecture, the Pessimist introduces a new graph, prompting Optimist to re-evaluate its existing conjectures. This iterative feedback loop allows Optimist to refine its dataset, accumulate new graph structures, and distill empirically valid conjectures. Over time, Optimist builds a repository of both new conjectures and established theorems, increasing its capability to generate stronger, more general conjectures that remain consistent with an ever-growing dataset; effectively increasing its intelligence; see Figure 4 for an illustration.\nIn a future implementation where the Pessimist is also an autonomous agent, this feedback loop could operate without human intervention. The Optimist would continuously propose conjectures, while the Pessimist systematically seeks counterexamples. This interaction forms an automated cycle of discovery, where conjecture generation and counterexample search evolve concurrently, refining both conjectures and the underlying graph structures in the dataset. This setup, while unable to formally prove conjectures, would create a self-sustaining environment for empirical discovery in graph theory\u2014a system we refer to as GraphMind."}, {"title": "5 Conclusion and Future Directions", "content": "In this paper, we presented the Optimist agent, a framework designed for automated conjecture generation in graph theory. Through the use of mixed-integer programming, heuristic filtering, and iterative refinement, Optimist has demonstrated the ability to autonomously derive empirically strong and mathematically relevant conjectures. In particular, our case study on the independence number $\\alpha(G)$ highlighted Optimist's capability to rediscover established results, such as K\u0151nig's theorem and classical bounds, while dynamically refining its conjectures based on iterative user feedback. Notably, the Optimist agent often conjectures published conjectures of TxGraffiti (see [44-49]) as a subset of other conjectures which remain open and under investigation.\nThe Optimist framework represents an important step toward an automated approach to hypothesis generation in discrete mathematics. By integrating a counterexample-searching counterpart the Pessimist we envision a collaborative"}, {"title": "Appendix A Python Code for Inequality Generation", "content": "###"}, {"title": "Appendix B Python Code Implementing Heuristics", "content": "###"}, {"title": "Appendix C Python Code for the Optimist Python class", "content": "###"}]}