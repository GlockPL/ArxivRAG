{"title": "An Empirical Study of Methods for Small Object Detection from Satellite Imagery", "authors": ["Xiaohui Yuan", "Aniv Chakravarty", "Lichuan Gu", "Zhenchun Wei", "Elinor Lichtenberg", "Tian Chen"], "abstract": "This paper reviews object detection methods for finding small objects from remote sensing imagery and provides an empirical evaluation of four state-of-the-art methods to gain insights into method performance and technical challenges. In particular, we use car detection from urban satellite images and bee box detection from satellite images of agricultural lands as application scenarios. Drawing from the existing surveys and literature, we identify several top-performing methods for the empirical study. Public, high-resolution satellite image datasets are used in our experiments.", "sections": [{"title": "Introduction", "content": "Object detection provides cues for many computer vision applications such as object recognition and tracking movements of targets in videos [1, 2, 3]. There are many learning-based methods developed in recent years such as You Only Look Once (YOLO) [4], Single Shot MultiBox Detection (SSD) [5], and derivations of Region-Based Convolutional Neural Networks (R-CNN) [6] (e.g., Faster R-CNN [7] and Cascade R-CNN [8]). However, the application of object detection methods for satellite imagery analysis faces unique challenges. One of them being small object detection. More specifically, relatively small objects in a large field of view and multiple instances with ambiguous spectral features make the object detection task challenging.\nDetecting objects from images and videos is a long-standing computer vision problem and many methods have been developed in the past decades, from sliding window strategies to deep learning methods. To understand the technical landscape and the open challenges, surveys were conducted and several papers were published on the review of the methods [9, 10]. Most recently, Sun et al. [11] reviewed deep learning-based object detection methods including Convolution Neural Network-based and transformer-based methods. Garcia et al. [12] reviewed deep network methods focused on the iterative labeling step in model training. Huang et al. [13] focused on the few-shot and self-supervised learning methods that deal with learning from unlabeled data. Gui et. al. [14] reviewed deep learning methods for object detection, emphasizing approaches addressing data and label limitations. Liu et al. [15] surveyed deep learning methods for small object detection from aerial imagery. These surveys provide a broad view of the recent technological aspects of object detection methods, as well as their chronological and geographical evolution. Yet, benchmarking these methods is missing.\nIn particular, evaluating how the existing methods perform for the emerging problem of detecting small but critical objects in complex and large views of satellite imagery requires investigation. In addition, such small object detection tasks are often complicated by the lack of examples.\nIn this paper, we review object detection methods for finding small objects from remote sensing imagery and provide an empirical evaluation to gain insights into method performance and technical challenges. In particular, we use car detection from urban satellite images and bee box detection from satellite images of agricultural lands as application scenarios. Drawing from the existing surveys and literature, we identify several top-performing methods for the empirical study. Two public, high-resolution satellite image datasets (xView and SkySat imagery) are used in our experiments.\nThe rest of this paper is organized as follows: Section 2 reviews the related studies that document recent object detection methods and provide a rationale of top performing methods for the empirical"}, {"title": "Related Work", "content": "Since many object detection methods have been developed in recent years due to the explosive advances of deep networks, survey studies were conducted that cover many aspects of this field. Liu et al. [15] reviewed deep learning methods for small object detection in aerial imagery, where single convolution layers tend to struggle with small objects due to restrictions in relevant information obtained from deeper feature layers. Such features also lack contextual information at low resolutions. In addition, small object detection data often suffer from imbalanced foreground and background instances and insufficient positive examples. Kang et al. [16] opted for more generalized aerial and satellite imagery. The study mentioned the relation between spatial resolution and average precision score performance. This paper focuses more on the details of the various datasets rather than any empirical evaluation of the performance of the different small object detection methods. Li et al. [17] provided an overview of deep learning methods for remote sensing images. Apart from the typical challenges mentioned in previous surveys, objects, such as cross-sea bridges and slender roads, may have extreme aspect ratios that hinder the detection. Over-cluttered background scenes and human annotation errors also contribute to performance loss. Zou et al. [18] traced the chronological evolution of object detection methods. Gui et. al. [14] reviewed deep learning methods for object detection emphasizing approaches addressing data and label limitations.\nBesides documentation and categorization of methods, evaluation is included in several recent surveys. Huang et al. [13] focused on the few-shot and self-supervised learning methods that deal with learning from unlabeled data. Methods were evaluated on a backbone pre-trained on ImageNet for generic object detection. Cheng et al. [19] provide benchmarks for methods on large-scale datasets. Apart from the one and two-stage methods covered in previous review studies, they also consider anchor-free and query-based methods such as evaluating small objects on oriented boxes from the satellite imagery inclusive DOTA dataset. Sun et al. [11] reviewed the technical issues of deep learning-based object detection with CNN-based and transformer-based methods. The survey goes into detail with categorizing the different techniques from region proposal and bounding box regression-based methods in the CNN-based model to vision and end-to-end methods for transformers. While the study includes comparative results for small-scale objects, the list of datasets included in their evaluation does not cover nadir satellite imagery.\nDespite the extensive surveys on object detection methods, the review and discussion of detecting small objects from remote sensing imagery are missing. There are many real world applications such as detecting fixtures along urban streets and managed pollination near crop fields. The performance and challenges faced by the existing methods require an investigation. Small objects pose unique challenges in the design and training models. In fact, the definition of small objects is vague. Based on the limited empirical evaluation benchmarks for small object detection and the lack of study on the implication of model transferability in real-world applications, we conduct an empirical evaluation of the state-of-the-art object detection methods using satellite imagery and assess the generalization ability between datasets and applications."}, {"title": "Preliminaries", "content": "Convolution-based methods have proven to be capable of learning from features from complex datasets. The addition of multi-scale feature extraction through methods such as feature pyramid networks (FPNs) further enables the retention of information on lower-level features. These methods have been well-established for general object detection from images [11]. These methods also offer a decent balance between speed and accuracy. Additionally, anchor-based methods provide additional improvements for detecting smaller objects.\nAmong the one-stage anchor-free approaches, YOLO [4] models have been improving in terms of accuracy and computation time for small object detection with each successive version and are effective due to feature fusion at multi-scale. The recent version implements an attention method akin to transformer models to further improve their detection capabilities. The second single-stage approach is the well-established SSD [5] model that offers a grid-based approach with default anchored boxes at different scales that offer a balance between accuracy and speed. The SSD model still maintains relevance for small object detection due to its simplicity and speed in terms of fine-tuning a model for small object detection at a lower computational overhead from additional clustering operations. For two-stage approaches, we look at two variants of region proposal-based R-CNN. Faster R-CNN [7] offers high accuracy potential balanced with speed and Cascade R-CNN [8] employs an adaptive threshold with its multi-stage approach to handle misclassifications at lower thresholds in images with dense objects and noise."}, {"title": "YOLO", "content": "The YOLO models frame object detection as a regression problem where a single convolutional network predicts objects' bounding boxes and probabilities. While the main architecture initially consisted of convolutional and max pooling layers, it has since expanded to a more modular block structure to improve performance. The models from version 4 onward, adopted a backbone-neck-head structure. Fig. 1 depicts the general structure used since YOLOv5.\nWhile there have been further alterations to the internal blocks of the architecture, the general structure remains relatively the same. The backbone is customized from the DarkNet53 structure that has convolutional layers having kernels of varying sizes and strides. The convolutional layers represent blocks with batch normalization and SiLU activation. The k3 blocks have a kernel size of 3, stride 2, and padding of 1. The k6 blocks have a kernel size 6, stride 2, and padding 2.\nThe main blocks in the YOLO model are the cross-stage partial connection (CSP) blocks that split convolution feature maps to be processed in parallel before aggregation. A c3 block consists of a series of bottleneck layers between the convolution layers. These bottleneck layers are added to reduce computation costs. Two different bottleneck configurations were used for the backbone and neck layers. The c3 block was later replaced with the more efficient c2f block in subsequent YOLO versions. The extracted features from the backbone go to the neck for refinement. The neck hosts refinement modules such as fast spatial pyramid pooling (SPPF) and the path aggregation network (PAN) added in version 10. Intermediate features obtained in the neck from the k1 layers are upsampled before concatenation. The head obtains the refined features and predicts feature maps at three scales based on objectness and confidence in its bounding boxes and classes. The latest update to the model is version 11 with improvements in computational efficiency and accuracy performance for object detection based on adaptability [20]. The model architecture retains its backbone-neck-head structure replacing the c2f block with the computationally efficient transformer-based c3k2 block having a smaller kernel size. The c3k2 block consists of initial convolution passing intermediate features through a series of c3k layers. The output feature from the final c3k layer is concatenated with the initial convolution layer before applying a 1 \u00d7 1 convolution. The SPPF block extracts features through multiple scales for detecting objects of varying sizes. Additionally, this version introduces the cross-stage partial with spatial attention (CSPSA) block after the SPPF and before the upsampling layer to improve spatial attention with enhanced retention of specific regions of interest for accuracy across objects of varying sizes and positions. The current version of YOLOv11 uses the same IoU based cross-entropy loss functions from YOLOv8 consisting of varifocal loss (VFL) [21], distribution focal loss and CIoU for box regression."}, {"title": "SSD", "content": "The SSD model employs multiple defined bounding boxes with anchor points in a grid format to predict their positions with convolutions across different scales. The overall architecture of SSD is shown in Fig. 2.\nThe backbone used for the original version of SSD was the VGG-16 configuration. The reliance of such a model on predefined anchor boxes has limitations with new objects or small objects that do not conform to predefined boundaries. SSD uses the sum of its localization loss through smooth L1 and confidence loss from softmax to determine the overall loss."}, {"title": "Variants of R-CNN", "content": "Another approach to object detection involves region proposals based on the \"recognition from regions\" paradigm. These methods are derived from the R-CNN method which involves obtaining several possible proposed regions. The proposals come from the features extracted from the input image by passing them through the CNN backbone and then integrating class-specific linear support vector machines (SVMs) for classifying the regions. Fig. 3 shows the general architecture of R-CNN.\nThe base R-CNN method uses the selective search algorithm for obtaining region proposals. The proposals are then passed to an O-Net model having pre-trained VGG-16 weights. The object regions initially proposed are a rough estimate and typically require further refinement. Additionally, utilization of the O-Net comes with a higher computational overhead for improved accuracy compared to the alternative T-Net."}, {"title": "Faster R-CNN", "content": "Since the first proposal of R-CNN, many derived models have been developed to reduce computational costs. Fast R-CNN [22] introduced a more streamlined approach with region of interest (RoI) based max pooling layers that provide fixed dimension feature maps to be passed to the network. Truncated single-value decomposition is added for faster computations of these Rols. The training stage uses a multi-scale approach for approximate scale invariance while the testing and inference stages use the image pyramid approach. Extending Fast R-CNN, Faster R-CNN uses a GPU-based region proposal network (RPN) with a training scheme that alternates between region proposal and fine-tuning tasks. The RPN works by using a sliding window with a base assumption that both Fast R-CNN and RPN share a common convolution layer structure. Reference boxes called anchors are generated at the center of each sliding window. Fig. 4 shows the overall workflow of the Faster R-CNN method. The loss function is a multitask loss function that involves classification and regression losses of predicted proposal bounding boxes with the ground truth."}, {"title": "Cascade R-CNN", "content": "One major drawback of these methods is the handling of thresholds based on overlap with intersection over union (IoU) metric. This leads to a trade-off between noise and degradation in detection performance. This method extends Faster R-CNN's two-stage to a multi-stage approach with specialized regressors and sequential refinement where each consecutive pooling operation takes the refined proposals from box regression of the previous stage with the feature maps obtained from the backbone in a cascaded fashion shown in Fig. 5. RPN generally favors low-quality proposals which nullify any learning on high-quality class proposals. Resampling based on the cascade regression helps avoid such issues.\nThe loss function on Cascade-RCNN is based on optimizing the threshold used with cross entropy as classification loss and smoothing L1 as localization loss."}, {"title": "Small Object Detection and Method", "content": "A small object is considered to have a coverage area consisting of a few pixels. The threshold considered in terms of the area is relative to the dataset and task. For example, the definition of small objects according to the MS COCO [23] dataset that consists of an average image size of 640 \u00d7 480 is any object smaller than their defined smallest bounding box of 32 \u00d7 32 pixels. However, a small object can also be defined relative to the image size [24]. Depending on the spatial resolution of the satellite imagery, these objects may be smaller than one pixel at minimum or a cluster of four to five pixels if each pixel is sub-1m resolution. Detecting such small objects becomes a challenge due to the inability of convolutional kernels to pick up on any distinct features from such objects that may be ignored as noise or part of the background leading to misclassifications. Defining a small object based on area coverage is also not ideal as we obtain more refined, high-resolution images that would increase the pixel count and make objects easier to identify. For example, we might have a hard time finding vehicles in 0.5m resolution images but they can be easily identified in 0.3m resolution images Fig. 6 due to their distinct features being more prominent.\nTherefore, we define a small object as an object in the image within a few pixels within 1% of overall pixel count relative to the spatial resolution to the point where no discernible features can be extracted and the object cannot be distinguished from other similar-sized objects without any additional external information."}, {"title": "Evaluation Strategy", "content": "To determine the impact of these proposed multi-scale algorithms on detecting, we would need to consider our evaluation from various aspects based on the challenges faced in the task."}, {"title": "Feature Extraction", "content": "Implementing an effective feature extraction approach across multiple resolutions provides contextual and semantic information to be fused back into the model enriching the deeper-level features. The different multi-scale algorithms determine the extent of contextual information that can be retained in the final feature and would thereby affect the accuracy model. Efficient feature extraction will lead to better average precision.\nThe feature extraction methods are typically represented by the backbone architecture of the model. For this study, we evaluate the effectiveness of feature extraction of the default backbones and the strategies of these methods. A good backbone selection determines the amount of information that can be obtained from the final feature layer as successive convolutions lead to the down-sampling of the image. Having a balance between its down-sampling strategy and feature information retention.\nWe correlate the feature maps extracted from the outputs of the backbone layer. Base models will be trained with baseline parameters to consider for 3 scenes having different landscapes. The results from the different models"}, {"title": "Anchor Size", "content": "The use of predefined anchors of certain sizes and anchor-less approaches have significant effects on the learning loss of models. YOLOv11 uses an anchor-less approach and performs better than the other three approaches. The size of the anchor based on the dataset further improves precision at the cost of additional computation overhead. 300 iterations\nFaster R-CNN was trained on the same parameters of iterations 10000, learning rate 1 \u00d7 10-4, head score threshold of 0.5, batch size per image as 1, and anchor generation range is kept between 2 and 4 for small and 32 to 64 for large"}, {"title": "Region Proposals", "content": "For region proposal-based techniques the number and quality of the region proposals affect the coverage of the detection of small objects. Faster R-CNN and Cascade R-CNN both use region proposals from the RPN. Having more proposals may increase accuracy at the cost of computation."}, {"title": "Generalization Ability", "content": "Provided the objects meet the definition, we observe how well these models can adapt to a different resolution dataset such as SkySat for finding bee boxes."}, {"title": "Training", "content": "Evaluation of these methods can be determined through experiments that showcase the impact of their respective methods. The key property common across all four methods is the ability to perform multi-scale feature extraction. First, by performing small object detection on the targeted class of small vehicles named cars on the xView dataset, we evaluate the overall performance to determine anchor-free approaches. Next we observe the effects of anchor box size on small object detection. We then evaluate the quality of the region proposal between Faster R-CNN and Cascade R-CNN.\nYOLOv11 was trained using 532 training images and 86 validation images trained for 600 epochs with an image size of 1120, batch size of 2, and default learning rate of 0.01.\nSSD model was trained on 532 images with 82 validation samples, image size of 300, learning rate of 10-4, momentum 0.9, weight decay 5 \u00d7 10\u22124, gamma 0.1 and overlap threshold of 0.5. The anchor box size was kept between 2 to 351. Number of iterations 500\nFaster R-CNN was trained on 532 images with 82 validation samples, head score threshold of 0.2, bounding box sizes of 2 and 4 augmented with an aspect ratio of 0.1,1 and 2. learning rate of 10-4, 105 iterations with a batch size of 1.\nCascade R-CNN used the same training and validation split as SSD and Faster R-CNN with a base learning rate of 5 \u00d7 10\u22124, RoI head batch size of 128 trained for 500 iterations."}, {"title": "Datasets", "content": "All models use their respective default backbones of c3k2, VGG-16, and ResNet-50.\nOur evaluation conducted using two datasets: xView and SkySat. The xView dataset [25] is a collection of satellite imagery from WorldView-3 at 0.3m pixel resolution and offers a standard and publicly accessible high resolution dataset with over a million objects for tasks related to small object detection. While xView consists of 60 classes of objects, we use xView as our baseline dataset for initial object detection to identify cars in the satellite image. The 16 types of different cars from small vehicles to trucks with and without backs get pushed to the same class making it a binary class object detection with the background. Data quality assessment for the dataset was conducted by the authors through three stages to maintain the quality of the dataset.\nThe SkySat program imagery data are obtained from high-resolution Earth imaging satellites maintained by Planet Labs. These images have a pixel resolution of 0.5m obtained from 21 satellites that orbit around the planet at different altitudes. Our target objects from these images are small bee boxes that appear in dimensions from around 5 to 18 inches clustered close to each other in lines next to fields or within barren fields. From the satellite imagery perspective, our criteria would be around 5 to 8 pixels next to farmlands. The region selected for this study is Van Buren County, Michigan in 2018. This was due to an abundance of blueberry crops grown in their fields, which provided an improved chance of locating apiary bee boxes next to fields for pollination from April to May.\nWith the limited number of images we split the dataset into 25 images for the training set and 5 images for the validation set. We then perform 6-fold cross-validation to evaluate the performance of the trained models on the new dataset with results listed in Table 6."}, {"title": "Results and Discussion", "content": "The baseline results from the trained models show that the YOLOv11, SSD, Faster R-CNN, and Cascade R-CNN fine tuned on xView and SkySat datasets are shown in Table 6. We observe that the anchor-free approaches such as Cascade R-CNN and Yolov11 achieve better at 50% overlap. The YOLOv11 model can detect more objects above the 75% overlap. The grid-based SSD suffers from the overall range of the bounding boxes and threshold selection from multiple predictions. Faster R-CNN provides better accuracy than SSD due to the region proposals which can be observed from the qualitative results in Fig. 7\nHardware used is a 24GB VRAM RTX 3090 GPu, 16 Intel core i7-10700K CPU, 31GB RAM, Labelstudio [26] was used to annotate SkySat images and Python with pytorch was the main platform for building the networks. Common metrics of intersection over union (IoU) based on mean average precision at different thresholds of 50 and 75 are used to determine how well the boxes align in terms of detection."}, {"title": "Conclusion", "content": null}]}