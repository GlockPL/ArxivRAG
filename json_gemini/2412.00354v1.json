{"title": "On the Role of Noise in Factorizers for Disentangling Distributed Representations", "authors": ["Geethan Karunaratne", "Michael Hersche", "Abu Sebastian", "Abbas Rahimi"], "abstract": "To efficiently factorize high-dimensional distributed representations to the constituent atomic vectors, one can exploit the compute-in-superposition capabilities of vector-symbolic architectures (VSA). Such factorizers however suffer from the phenomenon of limit cycles. Applying noise during the iterative decoding is one mechanism to address this issue. In this paper, we explore ways to further relax the noise requirement by applying noise only at the time of VSA's reconstruction codebook initialization. While the need for noise during iterations proves analog in-memory computing systems to be a natural choice as an implementation media, the adequacy of initialization noise allows digital hardware to remain equally indispensable. This broadens the implementation possibilities of factorizers. Our study finds that while the best performance shifts from initialization noise to iterative noise as the number of factors increases from 2 to 4, both extend the operational capacity by at least 50\u00d7 compared to the baseline factorizer resonator networks. Our code is available at: https://github.com/IBM/in-memory-factorizer", "sections": [{"title": "Introduction", "content": "Some basic Visual perception tasks, such as disentangling static elements from moving objects in a dynamic scene and enabling the understanding of object persistence, depend upon the factorization problem [1-4]. The principle of factorization extends to auditory perception, e.g. separating individual voices or instruments from a complex soundscape [5]. Factorization also plays a key role in higher-level cognitive tasks. Understanding analogies for example requires decomposing the underlying relationships between different concepts or situations [6-10]. While biological neural circuits solve the above challenges deftly, factorization remains a problem unsolvable within polynomial time complexity [11]. Outside the biological domain, factorization is at the core of many rapidly developing fields. In robotics, factorization can enable robots to 1) understand their environment by parsing visual scenes and identifying objects, locations, and relationships [12], and 2) plan and execute tasks by decomposing complex actions into a simple sequence of steps. Factorizing semi-primes has implications for cryptography and coding theory [13]. Factorization helps develop more transparent and explainable AI systems [14], by decomposing complex decision processes into understandable components.\nVector-symbolic architectures (VSA) [15\u201318] is an emerging computing paradigm that can represent and process a combination of attributes using high-dimensional holographic vectors. Unlike the traditional methods where information might be stored in specific locations (e.g., a single bit representing a value), VSA distributes the information across the entire vector signifying a holographic nature. This means if some components of the vector are corrupted or lost, the overall information can still be recovered due to the distributed nature of the encoding. This makes VSA an ideal candidate for realization on low signal-to-noise ratio in-memory computing fabrics based on e.g., resistive"}, {"title": "Background: Resonator Networks", "content": "The resonator network is an iterative algorithm designed to factorize high-dimensional vectors [24]. Essential to the operation of resonator networks are codebooks, which serve as repositories of quasi-orthogonal high-dimensional vectors called codevectors, with each codevector representing a distinct attribute value. For instance, in a system designed for visual object recognition, 3 separate codebooks might store representations for shapes (like circles, squares, triangles), colors (red, green, blue), and positions (top left, bottom right, center) each having 3 distinct possibilities for the attribute values. The number of potential combinations of these codevectors to form product vectors grows $M^F$ with respect to the number of attributes (i.e., factors F) and attribute values (i.e., codebook size M) exponentially. However, the dimensionality (D) of these vectors is usually fixed"}, {"title": "Breaking Free from Limit Cycles with Noise", "content": "As we discussed, resonator networks operate iteratively, progressively refining their estimates for each factor by comparing them to codebook entries. However, during this iterative process, the network can get trapped in a repeating sequence of states. Then the network's estimate for a factor oscillates between a small set of codevectors without ever settling on the true factor. This phenomenon, referred to as a limit cycle, can prevent the network from reaching the optimal solution.\nThe emergence of limit cycles can be attributed to the symmetric and deterministic nature of the codebooks used in the AS and RC phases of the baseline resonator network's (BRN) [24, 25] search procedure. This deterministic behavior is particularly problematic when the search space is large and contains many local minima, which can trap the network's updates in a repeating pattern. When a resonator network gets stuck in a limit cycle, it fails to converge to the correct factorization, even if given an unlimited number of iterations. This lack of convergence can significantly impact the network's accuracy and efficiency, rendering it ineffective for tasks that require precise factorization."}, {"title": "Results and Discussion", "content": "We conduct software experiments to measure the operational capacity and other behaviors of different variants of factorizers, namely the BRN, IMF, and ACF. Operational capacity is defined as the maximum size of the search space that can be handled with more than 99% accuracy while requiring fewer iterations than what a brute force approach would have taken. A brute force approach would in the worst case find the correct factors in $M^F$ steps.\nThe results are presented in Fig. 3. We consider 3 cases for the number of factors F = {2,3,4}. The dimensions of the codevectors for these cases are set based on the minimum dimensions reported in the BRN, namely D = 1000, 1500, 2000 respectively for F = 2, 3, 4 respectively. For each case, we span the search space size starting from $10^4$ up to a maximum of $10^{11}$ until the operational capacity is reached. At each search space size, we conduct 5000 trials factorizing randomly sampled product vectors. We calculate the average accuracy and the average number of iterations.\nThe BRN reaches its capacity at approximately $10^5$, $10^6$, and $10^7$ for F = 2, 3, 4 cases respectively. Although the accuracy momentarily dropped slightly below 99% in a few search space sizes between $10^5$ and $10^6$ at F = 2, IMF does not reach the operational capacity for all search space sizes tested. For ACF, we observe the reaching of the operational capacity at $> 5 \u00d7 10^9$ for F = 4. In the other two cases, ACF did not reach the operational capacity point for the search space sizes tested. The momentary drop in accuracy and rise in iterations in certain search spaces can be attributed to inadequate hyperparameter exploration. The optimum hyperparameter setting we achieved during our experiments is further detailed in Appendix 5.4\nBoth IMF and ACF exhibit better performance in terms of operational capacity and the number of iterations compared to the BRN. In theory, the IMF has better control over noise as it is applied over the iterations. This becomes clear in the F = 4 case where it outperforms ACF by achieving greater operational capacity. ACF however edges over IMF in the F = 2 case where there are fewer interactions among factors.\nAnother aspect to consider is the hardware design costs. As shown in Fig. 2, IMF achieves area efficiency with a single copy of codebooks in a crossbar array and achieves energy efficiency by performing arithmetic operations implicitly using device physics. ACF on the other hand has explicit multipliers and adders but saves the bulk of the energy spent on converting data from digital to analog and vice versa several times per decoding iteration. As a consequence of the converter-less design, ACF can operate faster, with several nanoseconds per iteration as opposed to several microseconds. Thus ACF can achieve more iterations per unit period of time.\nThe principles used in the IMF and ACF are not mutually exclusive. While in this work we study and compare their standalone performance, there is no reason that prevents them from being employed in unison. One possible realization of this involves perturbing the target conductance values corresponding to the codebook values before they get programmed into the analog memory devices in the IMF. Incorporating both sources of notice may result in a synergistic effect enabling higher operational capacity factorizers."}, {"title": "Conclusion", "content": "In conventional wisdom, stochasticity and noise are considered a bane in computing. We demonstrate that factorizers grounded on VSAs empower a new computing paradigm that embraces noise to push the limits of operational capacity. We discuss two variants, the first harnessing intrinsic noise in analog in-memory computing during MVM operation, the second initializing the codebooks with noisy perturbations yielding a model that widely appeals to deterministic digital design systems. While there are tradeoffs, both these variants empirically outperform the baseline resonator networks"}, {"title": "Appendix", "content": "Here, we provide a brief overview of vector-symbolic architectures (VSAs) [15\u201318] of which the resonator networks [24, 25] are based on. VSA is a powerful computing framework that is built on an algebra in which all representations are high-dimensional holographic vectors of the same, fixed dimensionality denoted by D. This is attributed to modeling the representation of information in the brain as distributed over many neurons. In this work, we consider a VSA model based on bipolar vector space [15], i.e., {\u22121, +1}D. The similarity between two vectors is defined as the cosine similarity:\n$sim(x_1,x_2) = \\frac{(X_1,X_2)}{||X_1||||X_2||} = \\frac{(X_1,X2)}{D}$                                      (5)\nAs one of the main property of the high-dimensional vector space, any two randomly drawn vectors lie close to quasi-orthogonality to each other, i.e., their expected similarity is close to zero with a high probability [18]. The vectors can represent symbols, and can be manipulated by a rich set of dimensionality-preserving algebraic operations:\n\u2022 Binding: Denoted by \u25ca, the Hadamard (i.e., element-wise) product of two input vectors implements the binding operation. It is useful to represent a hierarchical structure whereby the resulting vector lies quasi-orthogonal to all the input vectors. The binding operation follows the commutative law $X_1 \u25ca X_2 = X_2 \u25ca X_1 = p$.\n\u2022 Unbinding: The unbinding operation reverses the binding operation. As the element-wise multiplication in the bipolar space is self-inverse, the same operation as for the binding can be used. Using the unbinding operator the operation is defined as $p \u25ca x\u2081 = X_2$.\n\u2022 Bundling: The superposition of two vectors is calculated by the bundling operation \u2295. The operation is defined by an element-wise sum with consecutive bipolarization. In case of an element-wise sum equal to zero, we randomly bipolarize.\n\u2022 Clean-up: The clean-up operation maps a noisy vector to its noise-free representation by an associative memory lookup.\n\u2022 Permutation: Permutation is a unary operation on a vector that yields a quasi-orthogonal vector of its input. This operation rotates the coordinates of the vector. A simple way to implement this is as a cyclic shift by one position.\nInterested readers can refer to a detailed survey [22, 23] about VSAs."}, {"title": "Detection of convergence", "content": "The iterative factorization problem is said to be converged if, for two consecutive time steps, all the estimates are constant, i.e., $x_f(t + 1) = x_f(t)$ for f \u2208 [1, F]. We define an early convergence detection algorithm since it avoids unnecessary iterations and in the case of Asymmetric Codebook Factorizer (ACF), the legacy definition of convergence no longer holds.\nIn the new definition, the factorizer is said to be converged if a single similarity value across all the factors surpasses a convergence detection threshold:\n$converged = \\begin{cases}  true, & \\text{if } max(a_f(t)[i]) > T_{convergence} \\\\  false, & \\text{otherwise,} \\end{cases}$       (6)\nwhere i \u2208 [1, M] for f \u2208 [1, F]. Upon convergence, the predicted factorization is given by the codevector associated with the largest similarity value per each codebook. This algorithm also eliminates the need to store the history of prior estimates. In the legacy definition of convergence, the previous estimate for each factor had to be stored to be able to compare it to the current estimate and detect convergence, resulting in a total of F \u00b7 D stored bits. Our experiments show that the optimal convergence detection threshold stays at a fixed ratio of D for any given set of hyperparameters and problem sizes."}, {"title": "Threshold-based Activation", "content": "Replacing the identity function, which acts as a linear activation in the BRN, with a nonlinear winner-take-all approach can enhance both the convergence rate and the maximum solvable search space[27]. This strategy sparsifies the similarity vector, essentially zeroing out weaker similarity values and focusing the network's attention on the most promising candidates. By suppressing less likely solutions, sparse activation functions can help prevent the network from getting bogged down in local minima and facilitate convergence to the global optimum. For a threshold T, the threshold-based attention activation is given as:\n$\\forall i \\in (1, M) \\hspace{0.2cm} a'[i] = \\begin{cases} a[i], & \\text{if } a_i > T \\\\  0, & \\text{otherwise.} \\end{cases}$                                       (7)"}, {"title": "Hyperparameter Search", "content": "For the three cases of experiments we conducted, we first set the following common hyperparameters for both ACF and IMF. These include dimension (D) and search space size ($M^F$). Then the following hyperparameters have to be tuned to achieve the best results. For ACF, they include the sparsity parameter r and the activation threshold (T). For IMF, they include iterative noise standard deviation \u03c3 and the activation threshold (T). Tables 1, 2, and 3 provide the optimum hyperparameter combinations that give rise to the best accuracy and number of iterations results reported in Fig. 3."}]}