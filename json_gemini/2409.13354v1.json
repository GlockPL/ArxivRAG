{"title": "Recent Advancement of Emotion Cognition in Large Language Models", "authors": ["Yuyan Chen", "Yanghua Xiao"], "abstract": "Emotion cognition in large language models (LLMs) is crucial for enhancing performance across various applications, such as social media, human-computer interaction, and mental health assessment. We explore the current landscape of research, which primarily revolves around emotion classification, emotionally rich response generation, and Theory of Mind assessments, while acknowledge the challenges like dependency on annotated data and complexity in emotion processing. In this paper, we present a detailed survey of recent progress in LLMs for emotion cognition. We explore key research studies, methodologies, outcomes, and resources, aligning them with Ulric Neisser's cognitive stages. Additionally, we outline potential future directions for research in this evolving field, including unsupervised learning approaches and the development of more complex and interpretable emotion cognition LLMs. We also discuss advanced methods such as contrastive learning used to improve LLMs' emotion cognition capabilities.", "sections": [{"title": "1 Introduction", "content": "In today's emotion computing field, the significance of emotion cognition in large language models (LLMs) is increasingly recognized (Ren et al., 2024). It offers profound insights into the complex processes of human emotions and cognition. This area involves not only analyzing the emotional states of individuals or groups but also effectively utilizing these emotions in various applications, such as social media analysis (Chen et al., 2024b; Yang et al., 2024; Chen et al., 2024e; Jin et al., 2023, 2024b), human-computer interaction (Chen et al., 2023e,c), and mental health assessment (Chen et al., 2024d). Having emotion cognition capabilities enables LLMs to align more closely with human values, thereby enhancing their performance in emotion-related downstream tasks."}, {"title": "2 Challenges and Bases", "content": "Emotion cognition in LLMs faces key challenges: i) Uniqueness of the Problem, which indicates emotions in text are abstract and require deep understanding; ii) Methodological Complexity, which indicates emotions extends beyond classification or generation, needing tailored strategies and quantitative evaluations; iii) Diversity of Tasks, which indicates emotion tasks like question answering and dialogue need varied methods and effectiveness measures.\nWe first introduce the bases of emotion cognition, which include evaluations and enhancement for LLMs. Specifically, existing evaluations for LLMs can be categorized as follows: i) Discriminative Tasks include sentiment classification and emotion recognition on various datasets (Zhang et al., 2023; Carneros-Prado et al., 2023; Rathje et al., 2023). ii) Generation Tasks assess LLMs' emotional generation capabilities (Lynch et al., 2023; Xie et al., 2023; NathanKlapach, 2023). iii) Theory of Mind focus on LLMs' abilities to infer and replicate human mental states (Trott et al., 2023a; Sap et al., 2022; Gandhi et al., 2023). iv) Higher-order Tasks include role-playing, humor understanding, empathy generation, and decision-making (Jiang et al., 2023; Jentzsch and Kersting, 2023; Lee et al., 2022; Jin et al., 2022). Enhancement methods for LLMs are categorized as follows: i) In-context Learning incorporates emotional labels and contextual examples to improve emotion recognition (Sun et al., 2023; Lei et al., 2023; Lee et al., 2022). ii) Fine-tuning enhance performance in tasks like mental health prediction and sentiment analysis (Xu et al., 2023; Kheiri and Karimi, 2023; Peng et al., 2023; Binz and Schulz, 2023). iii) Knowledge Enhancement is adopted in tasks like completing dialogues, predicting emotional distribution, and integrating with knowledge bases enrich LLMs' emotional content (Zheng et al., 2023; Gagne and Dayan, 2023; Sun et al., 2023; Jeong and Makhmud, 2023; Qian et al., 2023; Shao et al., 2023).\nNext, we elaborate three typical emotion cognition problems with more detailed symbolic representations as follows: i) Emotion classification. Given a dataset $D = \\{(x_i, Y_i)\\}_{i=1}^N$, where $x_i$ represents the input text and $y_i$ is the corresponding labeled emotion, the objective is to develop an LLM, denoted as $LLM_{EAC}$. This model is tasked with function $f_{EAC} : X \\rightarrow E$, where $X$ is the space of input texts and $E$ is the set of possible emotion classes. The goal is for $LLM_{EAC}$ to predict the emotion class $e \\in E$ for a new input text $x \\in X$, using a limited set of labeled examples $(x_i, Y_i)$. ii) Emotion generation. Given a conversational context or narrative $C = \\{c_1, c_2, ..., c_m\\}$, where $c_j$ represents the j-th element of the context, the task is to develop an LLM, denoted as $LLM_{EIG}$. The model should implement a function $f_{EIG} : C \\rightarrow R$, where $R$ is the space of possible responses or generated text. The model $LLM_{EIG}$ is expected to generate a response $r \\in R$ that is contextually and emotionally aligned with the given conversational context $C$. iii) Emotion interpretability. Given a set of emotional data $E = \\{e_1, e_2, ..., e_k\\}$,"}, {"title": "3 Methods and Applications", "content": "In this section, drawing parallels between Neisser's cognitive processes and LLMs' capabilities, we elaborate emotion cognition of LLMs in the same seven stages. we list representative studies with their motivations, key techniques, results, open-source codes/datasets are shown in Table 1. The enlarged version is shown in Table 2 and Table 3."}, {"title": "3.1 Sensation", "content": "Sensation is the notion that LLMs exhibit capabilities akin to human in processing input textual data. Work in this area mainly focuses on the input form. The common input forms contain three categories, including prompt engineering, embeddings representation and knowledge enhancement.\nPrompt engineering means adding some instructions to guide LLMs on downstream tasks. For example, Lynch et al. (2023) introduced a structured narrative prompt designed for querying LLMs. The study uses OpenAI's ChatGPT to generate narratives, then compares the emotional levels in these narratives with real tweets using statistical tests like chi-squared and Fisher's exact tests; Ratican and Hutson (2023) proposed the 6DE model to analyze human emotions in LLM contexts. This model considers multiple dimensions of emotions, such as arousal, valence, dominance, agency, fidelity, and novelty; Zhang et al. (2023) explored four prompt strategies, including zero-shot and few-shot prompts with and without context, demonstrating the good performance of these prompts for LLMs in emotion analysis and recognition tasks comparing with those without any prompting strategy. The study highlights the importance of contextual information in enhancing emotion estimation by LLMs.\nDifferent from using prompts for input, an embeddings representation converts the input text into a vector representation in a high-dimensional space, capturing the semantic information of the vocabulary, which is used for optimizing internal process of the LLM. For example, Xu et al. (2023) explored instruction tuning to enhance LLM performance in mental health prediction. The fine-tuned models, Mental-Alpaca and Mental-FLAN-T5, notably surpass GPT-3.5 and GPT-4's performance, despite being significantly smaller in size; Binz and Schulz (2023) investigated psychological experiment data which is used to fine-tune LLMs. This research demonstrates LLMs' capabilities in accurately mimicking human behavior, and indicating LLMs' potential in emotion cognition with embeddings representation in the fine-tuning process.\nMoreover, knowledge enhancement means adding context or knowledge into the input to enhance LLMs' performance in processing downstream tasks. For example, Sun et al. (2023) focused on enhancing empathetic response generation by incorporating external knowledge. This study introduces a novel approach called CoNECT, which utilizes emotional indicators to assess context relevance and promote empathy reasoning; Gagne and Dayan (2023) explored the sentiment distribution of text generated by LLMs. This approach enables the generation of emotionally rich sentences through the utilization of specific quantiles, demonstrating LLMs' effectiveness in emotion-related generation, and offers insights into LLMs' internal mechanism.\nIn brief, current research in LLMs has achieved notable progress in processing emotional text inputs, mainly through prompt engineering, embeddings representation, and knowledge enhancement. These methods have enhanced LLMs' capabilities to understand and generate emotion-rich content. However, there remains room for improvement in diversifying the modes of receiving and processing text inputs, including integrating a more nuanced internal emotional cognition within LLMs to better interpret and respond to inputs."}, {"title": "3.2 Perception", "content": "Perception involves interpreting and understanding sensory information, processing raw data collected from the senses to form a meaningful understanding of the external world. LLMs' perception in emotion cognition mainly contains emotion recognition and its interpretability.\nEmotion recognition involves recognizing emotions in the context or conversations. For example, Rathje et al. (2023) explored the performance of GPT-3.5 and GPT-4 in detecting various language psychological constructs (emotions, discrete emotions, and aggressiveness), suggesting that LLMs are more accurate than dictionary-based methods and fine-tuned machine learning models; Zhang et al. (2023) demonstrated that LLMs can achieve comparable or superior performance in emotion recognition tasks, especially in identifying minority emotional categories; Lei et al. (2023) introduced the InstructERC framework, an effective generative framework employing a combination of a retrieval module and an emotion alignment task for emotion recognition; Venkatakrishnan et al. (2023) emphasized the importance of emotion detection in cross-cultural contexts, examining LLMs' responses to significant events such as the murder of Zhina (Mahsa) Amini in Iran and the earthquake in Turkey and Syria; Rodr\u00edguez-Ib\u00e1nez et al. (2023) assessed sentiment analysis methods in social networks and their applications in fields like stock market valuation, politics, and online bullying education. The study finds poor performance using LLMs like GPT-3, and GPT-J, requiring domain-specific adjustments; Peng et al. (2023) adopted deep prompt tuning and low-rank adaptation to investigate how well LLMs perform in linguistic emotion recognition. The impressive performance of the adapted LLMs across six widely used datasets highlights their strong transferability and feasibility in emotion recognition, surpassing other specialized deep models; Kheiri and Karimi (2023) discussed the potential of using LLMs for sentiment analysis, showing that LLMs excel in handling nuances in language for sentiment analysis; Ullman (2023) emphasized GPT-3.5's skill in predicting human emotions, highlighting its capabilities in understanding and interpreting emotional content in text. Carneros-Prado et al. (2023) conducted a comparative analysis between GPT-3.5 and IBM Watson using a dataset of 30,000 tweets related to the Covid-19 pandemic. This study reveals the multifaceted capabilities of LLMs in sentiment analysis and emotion classification. However, they also struggle with fitting textual expressions into defined emotion categories. Moreover, humor is a more challenging research field in emotion cognition. Trott et al. (2023b) investigate the ability of GPT-3 to understand verbal humor. Experiments demonstrate that GPT-3 performs above chance in detecting, appreciating, and comprehending jokes, although it does not match human performance. It suggests that while LLMs are adept at grasping humor, language alone is not sufficient for fully getting the joke. Images are also useful.\nInterpretability of emotion recognition is to analyze the internal state of LLMs through weight distribution of words, gradient, disturbance. Kwon et al. (2022) investigated methods of representing emotional concepts by comparing performance between appraisal feature-based and word embedding-based similarity calculation methods. It finds that GPT-3 outperforms in word embedding-based similarity calculations but also relies excessively on the valuation of emotional concepts.\nIn general, recent advancements in LLMs have focused on improving their perception of emotions in text, mirroring human-like understanding of emotional nuances. While LLMs like GPT-3.5 and GPT-4 show proficiency in detecting and interpreting emotions across diverse contexts, they still face challenges in fully grasping the context and subtleties of emotions, highlighting the need for further enhancement in their perceptual capabilities for accurate emotion categorization, depth of understanding, domain adaptability, and value alignment."}, {"title": "3.3 Imagination", "content": "Imagination is the generation of emotionally relevant content, such as emotional stories, poetry, and emotive dialogues, with the aim of creating content that aligns with human values.\nFor LLMs' generating emotional narratives, Xie et al. (2023) focused on variations in style, register, and story length in crafting stories, revealing a significant superiority of LLMs in generating story content. However, a critical observation is that LLMs tend to replicate real-world stories when dealing with world knowledge. Yongsatianchot et al. (2023a) investigated the GPT-4's proficiency in tasks associated with emotion prediction, showcasing its capability to not only discern and conceptualize emotion theories but also to create emotion-related stories. By prompting GPT-4 to recognize and manipulate key elements of emotional experiences, it demonstrates a nuanced control over the emotional intensity in its narratives.\nFor generating emotive dialogues, Zheng et al. (2023) adopted LLMs for dialogue augmentation in emotional support conversations. This approach treats dialogue augmentation as a dialogue completion task, where a fine-tuned language model completes dialogues from various topics, followed by heuristic-based postprocessing. Lee et al. (2022) delved into the ability of GPT-3 to generate empathetic dialogues through prompt-based in-context learning. The study introduces innovative context example selection methods, SITSM and EMOSITSM, which leverages emotional and situational information, revealing that GPT-3 achieves competitive performance against Blender 90M in empathy. Zhao et al. (2023) examined the emotional dialogue capabilities of ChatGPT, assessing ChatGPT's performance in understanding and generating emotional dialogues through a series of downstream tasks. Guo et al. (2023) indicated the emotional effect of ChatGPT in vertical fields such as painting creation. It can provide clearer, more detailed painting instructions, and understand abstract artistic expression and emotion in painting.\nWe also focus on a more specific aspect: the generation of humor, a complex and inherently human characteristic. For example, Jentzsch and Kersting (2023) critically investigated OpenAI's ChatGPT in terms of its humor generation capabilities, assessesing ChatGPT's ability to generate, explain, and detect jokes. ChatGPT is expected to repeat the same process of jokes rather than creating new ones, though it can accurately explain valid jokes. Toplyn (2023) presented an innovative approach to humor generation for LLMs. Witscript 3 employs three joke production mechanisms to generate and select the best comedic responses. It represents a collaboration between LLMs and human expertise, incorporating humor algorithms crafted by professional comedy writers. Notably, Witscript 3's responses are perceived as jokes by human evaluators 44% of the time. Chen et al. (2024f) constructed a Chinese Explainable Humor Response Dataset with chain-of-humor and humor mind map annotations as well as humor-related auxiliary tasks to evaluate and improve PLM and LLMs' humorous response ability of PLMs.\nIn summary, LLMs have significant achievements in the imagination of emotionally relevant content, such as stories, dialogues, and humor. They exhibit capabilities in nuanced control over emotional intensity and empathy in conversations. However, limitations persist in the originality of content, particularly in humor generation where LLMs tend to replicate existing jokes rather than creating new ones."}, {"title": "3.4 Retention", "content": "Retention is the process of encoding and storing knowledge, and creating \u201cmemories\". Retention in LLMs relates to how they \u201cremember\" emotional information and knowledge through their training process, which is crucial for subsequent emotional data processing and generation, such as role-playing and character simulation.\nFor example, Tao et al. (2023) introduced an innovative framework aiming at improving personalized role-playing with LLMs. They adopt a detailed emotion classification strategy and annotate emotions within the dialogue dataset, enabling GPT-4 to create character profiles based on emotions in its \"memory\". Shao et al. (2023) shifted the focus to training agents based on specific personal profiles, experiences, and emotional states, rather than using limited prompts to guide the ChatGPT's API. Experimental results indicate that editing and restoring personal profiles contribute to building simulacra for LLMs that are more accurate and emotionally aware, resembling their characters in a more human-like manner. Jiang et al. (2023) delved into the extent to which the behaviors of personalized LLMs reflect specific personality traits. This study uses the Big Five personality model to create distinct LLM personas and evaluates their behavior through various tasks, including a personality test and story writing. The results reveal that LLM personas can consistently exhibit behaviors aligning with their assigned personality profiles. Wang et al. (2023) introduced the RoleLLM framework which is used to enhance LLMs' role-playing capacities. This framework includes role outline construction, context-based instruction generation, and role-specific knowledge capturing and retention, showcasing competitive results of LLMs through mimicking linguistic styles and utilizing role-specific knowledge based on their memory.\nWe also investigated LLMs' capabilities in constructing emotional memory patterns and restoring them. For example, NathanKlapach (2023) delved into the comparative analysis of five widely recognized LLMs, including BingAI 1, ChatGPT 2, GoogleBard 3, and HuggingChat 4, focusing on their ability to process, mimic, and express emotions. These LLMs are tasked with creating new stories that mirror the tone, style, and emotional impact of the original narratives in order to assess their capabilities in storing emotional aspects of the stories and replicating them effectively. Russo et al. (2023) introduced a novel method that LLMs manage to restore human annotation with an author-reviewer pipeline combat misinformation on social media platforms by generating emotional responses.\nGenerally, recent research in LLMs has focused on enhancing their retention of emotional information, crucial for tasks like role-playing and character simulation. However, challenges persist in effectively encoding and storing complex emotional knowledge, being unable to \u201cretain\u201d memory due to context size limitations.. The field explores ways to improve LLMs' memory patterns and their capability to retain emotional information accurately and systematically."}, {"title": "3.5 Recall", "content": "Recall is the retrieval of emotional memories, extracting \"memories\". Recall in emotion cognition for LLMs pertains to their capabilities to retrieve emotionally relevant information from their internal or external knowledge in the context of responding emotion-related statement, maintaining consistency of emotional dialogues, etc. The following studies showcase how LLMs effectively utilize emotional memories and histories to enhance decision-making processes.\nFor example, Jia et al. (2023) introduced knowledge-enhanced memory model for emotional support conversation. This model is adept at perceiving and adapting to the dynamic emotional shifts within different periods of a conversation through extracting rich knowledge from dialogues and commonsense from ConceptNet (Speer et al., 2017). Jeong and Makhmud (2023) proposed a novel approach that enriches LLMs' responses by incorporating a diverse set of parameters, including the five senses, attributes, emotional states, relationships with the interlocutor and memories. They underscore the importance of memory in maintaining the continuity and emotional authenticity of conversations. Zhong et al. (2023) introduced a dynamic memory mechanism that makes LLMs utilize past emotional interactions in current decision-making. The mechanism, inspired by the Ebbinghaus Forgetting Curve (Ebbinghaus, 1885), allows LLMs to selectively recall emotional interactions, therefore acting more as a real human friend. Qian et al. (2023) underscored the importance of LLMs' capabilities to generate empathetic responses based on historical emotional contexts. They introduce in-context learning and two-stage interactive generation that enable LLMs to process and reflect upon past emotional interactions, thereby making more empathetically informed decisions. In addition, Wake et al. (2023) investigated the application of emotional history in decision-making. The authors estimate the emotional label of current utterances solely based on the past conversation's history, showcasing the great effect of dataset and emotion label selection on ChatGPT's emotion recognition performance.\nIn summary, recent studies on recall of LLMs focus on retrieving and utilizing emotional memories for decision-making in emotion-related interactions. These studies introduce LLMs are adapt to dynamic emotional shifts, incorporate diverse parameters, and utilize historical emotional contexts for empathetic decision-making. However, challenges remain in perfecting the recall of complex emotional histories, efficiently retrieving the most relevant knowledge and update it consistently."}, {"title": "3.6 Problem-Solving", "content": "Problem-Solving in emotion cognition is to solve emotion-related downstream tasks in various scenarios (Li et al., 2023a; Chen et al., 2024a; Li et al., 2022, 2024b, 2023c; Ni et al., 2024a; Li et al., 2023b, 2024e).\nIn mental health, for example, Tu et al. (2023) presented the S2Conv framework, tailored for providing personalized support for mental health issues. It integrates personality and memory-based dialogue models with an interpersonal matching plugin, highlighting LLMs' potential in providing social support (Ni et al., 2024b; Li et al., 2024d; Jin et al., 2024a; Li et al., 2024a). Qi et al. (2023) evaluated LLMs' performance in the mental health domain. Zhu et al. (2024) investigated the use of LLMs for performing mental inference tasks, specifically inferring users' underlying goals and fundamental psychological needs. Lai et al. (2023) used LLMs in psychological counseling settings, providing immediate responses and mindfulness activities. Xu et al. (2023) assessed LLMs' performance in mental health prediction tasks, highlighting the need for bias mitigation. In education, Sajja et al. (2023) propose an innovative framework for personalized and adaptive learning.\nThese diverse applications of LLMs in emotion-related problem-solving highlight their broad potential. However, they also reveal limitations like gender biases and the need for enhanced interpretability. Future research directions may include fine-tuning LLMs to better meet the needs of educational and mental health domains."}, {"title": "3.7 Thinking", "content": "Thinking refers to the reflection and review after problem-solving. In the context of emotion cognition in LLMs, it pertains to how emotional Theory of Mind is utilized to solve downstream tasks (Zhou et al., 2024; Li et al., 2024c). Theory of Mind (TOM) (Carlson et al., 2013) is the cognitive capability to understand one's own and others' mental states, including emotions, intentions, expectations, thoughts, beliefs. One can use this theory to predict and interpret various behaviors.\nLLMs have shown promising performance in emotion-driven tasks with powerful thinking capability. For example, Trott et al. (2023a) demonstrated LLMs perform well in inferring others' beliefs. Gandhi et al. (2023) introduced a causality-based template methodology for evaluating LLMs' ToM. They find that GPT-4 has human-like reasoning patterns with powerful ToM. Sap et al. (2022) evaluated GPT-3's performance in social reasoning and mental state comprehension, identifying the boundaries of LLMs' ToM. Shapira et al. (2023) conducted extensive experimentation on six tasks with varied detection methods in assessing LLMs' ToM, which are non-robust and reliant on superficial heuristic methods rather than solid reasoning. Holterman and van Deemter (2023) examined the ability of ChatGPT-3 and ChatGPT-4 to exhibit ToM by presenting them with six problems addressing human reasoning biases, finding that ChatGPT-4 provides correct answers more often than chance, albeit sometimes based on incorrect assumptions or flawed reasoning.\nIn the realm of thinking, the focus extends beyond ToM to behaviors. For example, Zhou et al. (2023) introduced a novel \u201cThinking for Doing\" evaluation paradigm, assessesing whether LLMs can discern appropriate actions based on others' mental states, beyond merely responding to questions about these states. The study proposes a zero-shot prompting framework, \u201cForesee and Reflect\u201d, to enhance LLMs in predicting future events and reasoning through action choices. Jin et al. (2022) aimed to evaluate LLMs in comprehending and predicting human moral judgment and decision-making behaviors. They propose a novel moral reasoning chain prompt strategy named MoralCoT based on legal expertise and moral reasoning theories, suggesting that MoralCoT surpasses existing legal models in moral reasoning, complex moral judgments and decisions. Sorin et al. (2023) reviewed the capacity of LLMs to demonstrate empathy, exploring how LLMs process and express complex emotional viewpoints and reasoning. Del Arco et al. (2022) acknowledged GPT-3's role in empathy and distress predictions, highlighting its reasoning process in complex emotional forecasting. Schaaff et al. (2023) assessed ChatGPT's empathy levels compared to human standards, comparing the LLMs' understanding and expression of empathy. Saito et al. (2023) demonstrated the comparable performance of the proposed ChatGPT-EDSS in capturing empathy in dialogues, evaluating the LLMs' emotional understanding and expression capabilities. Lee et al. (2024) found that LLMs' responses are more empathetic than humans', comparing the emotional reasoning and expression between models and humans.\nCollectively, these studies have focused on LLMs thinking capability. While LLMs like GPT-4 show promise in understanding and inferring mental states, challenges exist in their depth of reasoning and reflection, better utilizing Theory of Mind, and emotion-driven behavior prediction. The field aims to enhance LLMs' capabilities in retrospectively analyzing emotional tasks for more nuanced and accurate problem-solving."}, {"title": "4 Future Research Directions", "content": "Emotion cognition is an emerging and rapidly developing research topic. Despite significant progress, many challenges still exist for future research. In this section, we identify and briefly discuss some potential directions.\n\u2022 Generalization and transferability. Current methods in emotion cognition for LLMs primarily rely on labeled data and prompt-based techniques. Notable limitations are the distributional shift between the data used for training and testing, as well as the dependence on the prompter's expertise, which affects the generalization and transferability of these networks (Chen et al., 2023a; Xiong et al., 2024; Xiao et al., 2023). Rathje et al. (2023) emphasized the challenges posed by the lack of manually annotated datasets in many under-researched languages, limiting the analysis of GPT's accuracy.\n\u2022 Explainability. Developing LLMs that provide clear explanations is essential for improving the reliability and trustworthiness of LLMs (Chen et al., 2022, 2023b; Weng and Wu, 2024c,a; Weng et al., 2024; Weng and Wu, 2024b). For instance, Kwon et al. (2022) delved into how different LLMs perceive and express nuanced human emotions. Additionally, as Jentzsch and Kersting (2023) pointed out there is a need for deeper understanding beyond just analyzing system outputs. Integrating theories like emotional cognition, as discussed in (Yongsatianchot et al., 2023b), can be a step forward in this direction. Furthermore, Yongsatianchot et al. (2023a) suggested that while LLMs contribute significantly, they struggle in modeling emotions in physiological, neural, and cognitive aspects.\n\u2022 Diverse emotions and applications. There is a vast array of complex and mixed emotions as well as different modals that have yet to be thoroughly explored (Tao et al., 2024). Trott et al. (2023b) highlighted the growing interest in specific emotions like humor, but other emotions, such as anxiety, remain under-researched. In addition, Xu et al. (2023) provided guidelines for enhancing LLMs in mental health prediction tasks. Sajja et al. (2023) explored the potential of LLMs in creating more personalized and adaptive learning environments. Lastly, Lai et al. (2023) outlined the limitations and areas for improvement in mental health support models."}, {"title": "5 Conclusion", "content": "Our comprehensive review in this paper highlights the growing importance of emotion cognition in LLMs within the field of emotion computing. We first explore the key challenges in emotion cognition. Next, we make a categorization of current studies based on Ulric Neisser's cognitive psychology theory, offering a structured summary of emotion processing in LLMs across various stages. This includes analysis and discussion on emotion classification, generation, interpretability, and the integration of these aspects into LLMs to enhance their empathetic and cognitive capabilities. After that, we summarize motivations, methodologies, results, and available tools of current studies, discussing future directions in emotion cognition of LLMs. Overall, we contribute to the summary of current trends and future work in emotion cognition of LLMs, expecting to make LLMs be more aligned with human emotion and cognition, therefore better solving emotion-related downstream tasks. Exploring the emotion cognition capabilities of LLMs can inform further research by improving emotion recognition and response generation, leading to more accurate sentiment analysis and empathetic interactions. This can enhance personalized user experiences in education, entertainment, and customer service in the future."}, {"title": "Limitations", "content": "While this survey provides a comprehensive overview of recent progress in emotion cognition for LLMs, limitations exist. The primary one is the categorization of research based on Ulric Neisser's cognitive stages may oversimplify the complex nature of emotion cognition in LLMs. Additionally, the focus on key methodologies and outcomes might not capture all nuances of the field. In the future, we could explore more nuanced categorizations beyond Ulric Neisser's cognitive stages to provide a more comprehensive and up-to-date understanding of emotion cognition in LLMs."}]}