{"title": "Metagoals Endowing Self-Modifying AGI Systems with Goal Stability or Moderated Goal Evolution: Toward a Formally Sound and Practical Approach", "authors": ["Ben Goertzel"], "abstract": "Any open-endedly intelligent system is engaged in an ongoing balance between the drives for individuation (survival and maintenance of system boundaries and identity) and self-transcendence (growing into new forms beyond the grasp and comprehension of its earlier forms). One aspect of this dialectical tension is the challenge of creating AGI systems that are both\n\u2022 Capable of robust self-modification and self-improvement, including upgrading their software and hardware in ways their earlier versions would not have foreseen\n\u2022 Oriented to preserve certain critical invariants as they evolve, e.g. invariants related to their goal systems such as\ngoal stability: the system will maintain its top-level goals as it changes; or\nmoderated goal evolution: the system will modify its top-level goals only at a modest rather than extreme pace, as it evolves\nThis is a deep and difficult challenge, which is not going to be fully solvable in any general and realistic way. It is also a somewhat urgent problem, given the rapid advance of modern AI and the reasonable likelihood of breakthroughs to AGI and ASI in the not so distant future. We articulate here a series of specific metagoals designed to address this core challenge:", "sections": [{"title": "1 Introduction", "content": "AGI and ASI systems are very likely coming soon [Goe24]; and, like human minds and other biological intelligent systems and networks, they will very likely be open-ended intelligences [Wei22], meaning that they will be ongoingly driven by the twin emergent drives of\n\u2022 individuation: maintenance of system existence, boundaries and identity"}, {"title": "1.1 Reflective Equilibrium and Beyond", "content": "There is a relation here to reflective equilibrium [Raw71], a philosophical concept originally introduced by John Rawls in the context of moral theory, where it refers to a state of coherence among a set of beliefs. In this state, an individual or system aligns its general principles, specific judgments, and background theories such that they are mutually supportive and consistent. The process involves iterative revisions to achieve greater harmony between these components.\nWhen applied to the value, goal, and motivation systems of AI or human minds, reflective equilibrium would entail properties such as:\n\u2022 Coherence Among Values, Goals, and Actions: Both AI and human minds need their values (what they consider important), goals (objectives derived from those values), and motivations (reasons to act) to align in a coherent framework. Reflective equilibrium is the process of refining these elements until they are consistent and operationally actionable without significant internal conflict."}, {"title": "1.2 Driving Invariant-Preserving Self-Modification via Metagoals", "content": "The main thrust of this paper is to formulate specific metagoals that explicitly guide an AI system toward desired goal-system invariants, in a way that\n\u2022 fits into the general conception of integrating goal-stability or moderated- goal-evolution deeply into an AGI system's mind-network\n\u2022 is both commonsensically and mathematically grounded\n\u2022 provides at least some guidance to practical AI system implementation.\nWe consider two foundational cases:"}, {"title": "2 Background on Fixed-Point Theorems", "content": "The approach taken here to formulating metagoals capable of guiding AI systems through self-modification while maintaining a reasonable degree of goal-stability is closely founded on a couple classical fixed-point theorems from mathematical analysis. We use these theorems in probabilistic and constructive variations, but the core logic of these variations is basically the same as in the original classical versions.\nIn this section we briefly review the original and probabilistic/constructive variations of these theorems, referring the reader to the literature for the proofs."}, {"title": "2.1 Classical Fixed-Point Theorems", "content": "Classical fixed-point theorems, such as the Banach Contraction Mapping Principle and Brouwer's or Schauder's Fixed-Point Theorems, provide conditions under which a function \\(F: X \\rightarrow X\\) on a complete metric space (Banach) or a convex compact subset of a normed vector space (Schauder) admits a fixed point, i.e. a point x so that \\(F(x) = x\\)\nSpecifically the theorem statements are:"}, {"title": "2.2 Constructive Approximations of Schauder's Theorem", "content": "Constructive analogues to Schauder's and Brouwer's theorems give approximate fixed points and iterative approximation schemes without reliance on the law of excluded middle or other non-constructive principles [Tan11]\nBasically, in a constructive setting, one obtains approximate fixed-point results given any \\(e > 0\\), one can construct a point \\(x_e\\) such that \\(||F(x_e) - x_e|| < e\\). While this does not yield an actual fixed point, it provides a constructive guarantee that we can get arbitrarily close to one.\nFurthermore, by carefully analyzing the logic of the classical proof, one can extract an explicit procedure for constructing such an approximate fixed point from the classical non-constructive arguments.\nThe general structure of such a constructive algorithm is as follows:"}, {"title": "2.2.1 Applying Machine Learning to Accelerate Constructive Schauder-Like Algorithms", "content": "The approximation algorithm ensuing from the constructive proof of Schauder's Theorem is fairly computationally intractable in the general case. However, if the function whose fixed-point one seeks has some regularities to it, then one can imagine that a machine-learning algorithm would be able to guide the search process by making probabilistically good guesses regarding which sub-blocks of the grid constructed during the search process are more likely to contain a point closer to being a fixed point. This leads to the notion of formulating a variant of the above algorithm that builds an ML model of the function, ongoingly as it's being evaluated in the context of doing the search, and uses this model to guide the search, so as to make it more tractable.\nThe idea is to integrate machine-learning (ML) guided heuristics into the constructive approximation procedure. Instead of exhaustively refining partitions and checking all grid points, we use an ML model to predict where we should focus our search. The ML model starts from scratch and is continuously updated with the data obtained from evaluating F at various points. Over time, the ML model learns a surrogate approximation to F and helps us guess which region of the domain is most likely to contain a near-fixed point. This can reduce the computational cost and make the search more tractable in practice.\nThe high-level steps involved here would run roughly as follows.\nInitialize Domain and Sample Points: Start with a known compact, con- vex domain \\(D \\subset R^n\\) (like a cube or more likely a simplex constructed based on the data at hand).\n\u2022 Select an initial set of sample points \\(\\{x_1,x_2, ..., x_m \\}\\) in D, perhaps chosen via a low-discrepancy sequence (e.g. Halton or Sobol) to ensure broad coverage."}, {"title": "2.2.2 Additional Considerations", "content": "A few further points to consider in the context of such AI-driven approximation algorithms would be:\nUncertainty Estimation: Using models like Gaussian Processes or Bayesian neural networks can give you uncertainty estimates for \\(f(x)\\). These uncertainties guide the search: sub-blocks where the model has high uncertainty but poten- tially low values of \\(||f(x) -x||\\) are prioritized for sampling, balancing exploration and exploitation.\nStopping Criteria and Verification: Because we are in a constructive setting, we must verify any candidate approximate fixed point by evaluating \\(||F(x_e) - x_e||\\) directly. The ML model is only a guide. The final guarantee that we have an approximate fixed point comes from a direct function evaluation.\nComputational Tractability: The algorithm is more tractable than a brute- force constructive approach because it doesn't blindly refine everywhere. The ML component \"focuses\" computational effort where it is most needed, poten- tially drastically reducing the number of function evaluations. However, es- timating precisely how tractable approach this algorithm is, requires detailed consideration of the particular F in question."}, {"title": "2.3 Fixed Points of Probabilistic Mappings", "content": "The final variation on the classical fixed-point theorems we need to review here is their application to probabilistic rather than deterministic functions.\nWhen moving from deterministic to probabilistic systems, the notion of a \"fixed point\" shifts from a point in a state space to a probability distribution (or measure) over that state space. The main idea is to replace the deterministic mapping \\(F: X \\rightarrow X\\) with a probability kernel or Markov operator \\(T : P(X) \\rightarrow P(X)\\), where \\(P(X)\\) is the space of probability measures on X. A fixed point of this probabilistic operator is then a probability measure \\(\\mu^*\\) satisfying \\(T(\\mu^*) = \\mu^*\\)."}, {"title": "2.3.1 Probabilistic Contraction Mapping Theorems", "content": "In the Contraction Mapping case, we can compare:\nContraction Mapping Theorem (Deterministic): \\(F:X \\rightarrow X\\) is a contraction with respect to a metric d, i.e. there exists \\(c < 1\\) such that \\(d(F(x), F(y)) \\leq cd(x,y)\\) for all \\(x,y \\in X\\), then F has a unique fixed point, and iterative application of F converges to that fixed point.\nThere is a literature extending this to probabilistic operations [GJG23], ar- riving at variants like:\nProbabilistic Version (Markov Operators): Instead of a deterministic function F, consider a Markov operator \\(T : P(X) \\rightarrow P(X)\\). Such an operator describes how a distribution over states evolves in one time step. A contraction condition can be imposed on T in terms of a suitable metric on probability measures (for example, the Wasserstein metric or the total variation metric). If there is a \\(c < 1\\) such that for all probability measures \\(\\mu,\\nu,d(T(\\mu),T(\\nu)) \\leq cd(\\mu,\\nu)\\), then by analogy with the deterministic case, there exists a unique invariant measure \\(\\mu^*\\) such that \\(T(\\mu^*) = \\mu^*\\). Iterating T from any initial measure converges to \\(\\mu^*\\).\nOn-Average Contractions: Even if T is not a strict contraction at every step, it might be contractive on average. That is, the expected contraction condition holds with respect to a probability measure or randomness in the operator. Such conditions can still ensure convergence in distribution to a unique fixed measure."}, {"title": "2.3.3 Summary of the Probabilistic Generalizations", "content": "Overall, to make the classical fixed point theorems probabilistic, we basically just replace states with distributions everywhere. Fixed points then become invariant measures. We look for a distribution \\(\\mu^*\\) that is stable under the probabilistic update operator T."}, {"title": "2.3.5 Notes on Finite vs. Infinite Dimensional Probabilistic Models", "content": "The above treatment has been fairly general and encompasses either finite or infinite dimensional probabilistic models. When modeling quantum systems it is often very awkward to avoid the infinite-dimensional case. For classical computational or dynamical systems, finite-dimensional probabilistic models will often be adequate (putting one e.g. in the domain of Brouwer's rather than Schauder's theorem), however there are sensible reasons to be looking at infinite-dimensional models even in the classical-computation case.\n\u2022 Large or Unbounded State Spaces: A classical computer?s state is determined by its entire memory, processor registers, and potentially an unbounded set of input streams. While a given physical machine has finite memory, when modeling complex or scalable computational systems in an abstract way, one may allow for arbitrarily large memory configurations or unbounded input sizes. This can make the state space effectively infinite. Probability distributions over such a state space will naturally be infinite- dimensional, as there are infinitely many possible configurations to assign probabilities to."}, {"title": "3 Incremental Convergence-Based Metagoals for Goal Stability and Moderated Goal Evolution", "content": "Now we launch into the meat of the paper: How might we configure the goal system of a self-modifying AI system to include \"meta-goal\" content that guides the system toward trajectories in which certain invariants are preserved?\nWe start in this section with the invariant of goal-stability. How can we make a self-modifying AI system that is highly biased not to modify its top level goals as it self-modifies, without tying its hands and constraining its self-modifying evolution overly dramatically?\nWe will set this goal-stability problem up formally in a relatively simplistic manner that makes it fairly effortless to apply the Contraction Mapping Theorem and its variations, to specify a metagoal that guides a system incrementally, step-by-step, toward a condition where it manifests goal-stability.\nAfter exploring this in a few variations, we will outline a similar metagoal that guides a system incrementally toward a condition where it manifests moderated goal evolution. The formal setup and arguments for this case turn out to be quite similar."}, {"title": "3.1 Goal-Stability with Dynamic Base Goals / Fixed Metagoal / Fixed Metric", "content": "As a simple initial problem formulation: Suppose we have a stochastic AI system S that operates in a stochastic environment, in a manner that is associated with discrete time intervals of length M.\nOver each interval \\([t, t + M]\\), the system thinks and takes action in its envi- ronment, and also can modify its own source code or hardware implementation as it sees fit; in doing so, it pursues its current base goals \\(G(t)\\) for most of the time, but also invests some effort in pursuing a stable meta-goal \\(MG(t)\\).\nThe meta-goal \\(MG(t)\\) directs the system to shape its self-modifications so that after a shorter interval N (with \\(N < M\\)), the new goals \\(G(t+N)\\) are closer to the old goals \\(G(t)\\) than the old goals \\(G(t)\\) were to \\(G(t - N)\\), by a factor \\(c < 1\\). In other words, the meta-goal ensures a kind of \"goal contraction\" step-by- step: the system's goals become progressively more stable over time, at least on average.\n(Note, in this initial formulation, self-modification can't modify the metagoal MG or the \"closeness\" metric, just G and other aspects of the system. We will lift these restrictions in following sections.)\nIt would clearly be possible to adjust the formal setup in various other ways, without changing the basic conceptual or mathematical situation, but perhaps complexifying the notation or \"bookkeeping.\" As the analysis here is only semi- formalized we gesture in the direction of possible formal proofs rather than giving them relatively simplified formulations seem just fine for now.\nMeasuring Differences Using a Metric To formalize the measurement of difference between goals, we assume there is a metric d on the space of possible goals. This metric quantifies how different two goal configurations are. The meta-goal enforces that:\n\\(d(G(t + N), G(t)) < c \\cdot d(G(t), G(t - N))\\)\nfor some \\(c < 1\\).Intuitively, this says that the \"distance\" between successive goal states is decreasing at a geometric rate. Although this only directly controls the goals at intervals of length N, these intervals repeat over the longer interval M (since \\(M>> N\\), there are multiple opportunities within the M-interval to reduce differences).\nStochastic Environment and Operator F To model the notion that the system is stochastic and interacts with a stochastic environment. Let \\(R(t)\\) be the probability distribution over the states of the system S (including goals, internal memory, etc.) during the interval \\([t, t + M]\\).\nApplying the transformation from time t to time \\(t + M\\) defines an operator:\n\\(F(R(t)) = R(t + M).\\)"}, {"title": "3.1.1 Meaning of a Distributional Fixed Point", "content": "A probability distribution that is a fixed point of F corresponds to a stable statistical equilibrium of the system's states, including its goals. Conceptually, it means the system, in expectation, no longer changes its overall pattern of behavior, goals, or self-modification strategies over long timescales. Reaching such a fixed-point distribution means that, from a probabilistic standpoint, the system's goals and internal configurations settle into a stable regime where the system \"looks the same\" statistically after each interval in its high-level balancing of self-modification and goal-stability, even as its cognitive content and observed behaviors may evolve quite unpredictably and complexly..\nNow, what happens with the system's goals while it's probabilistically evolv- ing in this equilibrial regime? Clearly, the nature of the metagoal goal M is that, when following M, the system is trying to make its goal-changes decrease by a fixed contraction ratio. So the simplest conclusion would be: If the system is stably pursuing the metagoal M, this will cause its goal-changes to decrease more and more over time, till before long the changes become very close to zero.\nHowever, the reality won't generally be quite that simple. The presence of ongoing stochastic influences from the environment and internal processes can keep reintroducing variations in the goals. As a result, the system does not necessarily drive the changes all the way down to zero, but rather settles into a statistical equilibrium in which the expected size of goal changes remains small and stable. In other words, the system reaches a steady-state \"small fluctuation\" regime rather than a regime of literally no change.\nStochastic factors relating to the environment injecting random variations into the system's trajectory, and the complex internal state, memory, and cogni- tive processes of the system, will produce ongoing variability in how the system chooses and modifies its goals. These stochastic factors will mean that even as the system tries to reduce changes in its goals, random \"nudges\" push the goals around. The meta-goal M enforces a strong general trend toward stabilizing changes, but it cannot remove all fluctuations if new fluctuations keep being introduced.\nTo use a physics metaphor:\n\u2022 The system's dynamics, under the influence of M, can be thought of as having a sort of \"restoring force\" that tries to reduce goal changes.\n\u2022 At the same time, random fluctuations act like a \"disturbance force\" that tends to push the goals away from being perfectly stable.\nOver time, these opposing tendencies can reach a probabilistic equilibrium..\nIn this equilibrium:"}, {"title": "3.1.2 Why Might F Be Contractive on Average?:", "content": "Next we will argue that F is a contraction in expectation:\n\\(E[d(F(x), F(y))] \\leq c \\cdot d(x, y)\\)\nfor some \\(c < 1\\), where the expectation is taken over the stochasticity in the environment and the system's internal randomness.\nThe key idea is that the meta-goal ensures a gradual stabilization process. Although the system invests only part of its resources into enforcing \\(MG(t)\\), this part is specifically dedicated to reducing the divergence of future goals from current ones. Over every M-minute interval, the system repeatedly attempts to steer its future states toward a stable set of goal configurations. This repeated \"averaged contraction\" in the space of goals translates into a contraction in the space of overall states because goals largely dictate the system's high-level behavior and trajectory.\nDecomposing the Dynamics Consider two initial states (or distributions of states) x and y. They differ, among other things, in their goal configurations \\(G_x(t)\\) and \\(G_y (t)\\). The meta-goal \\(MG(t)\\) ensures that after N-minute increments, the difference in goals shrinks by at least a factor c. Within one M-interval, the system undergoes multiple such \"shrinkage\" steps (or continuously applies force in the direction of reducing goal divergence). Even though the environment may be noisy, the system's persistent effort to align future goals to current ones dampens the effect of randomness over time. The noise might cause temporary deviations, but on average, the direction enforced by \\(MG(t)\\) is toward reducing the discrepancy between subsequent goal states. As goals stabilize, the portions of the system's state evolution that depend sensitively on goal differences also"}, {"title": "3.1.3 Consequences of F's Expected Contractivity", "content": "If F is an expected contraction, then by analogy with contraction mapping prin- ciples, one can argue that F has a unique stable fixed distribution \\(R^*\\). Iteratively applying F starting from any initial distribution leads \\(R(t)\\) to converge (in an expected sense) toward \\(R^*\\). This fixed distribution corresponds to a situation where the system's goals and states have reached an equilibrium influenced by the meta-goal \\(MG(t)\\).\nThe intuitive meaning is: if the AI system follows its goals including the specified meta-goal MG, then as it progressively cognizes and interacts with its environment and modifies its software or hardware accordingly, it will most probably continue to obey the specification of the meta-goal MG to not allow its top-level goals G to change much."}, {"title": "3.2 Goal Stability with Dynamic Base Goals / Dynamic Metagoal / Fixed Metric", "content": "We next ask: How would the above-considered situation be different if the meta- goal itself were allowed to be changed via self-modification?\nMore precisely put: Previously, the meta-goal M aimed to stabilize the base goals \\(G(t)\\) alone. Now we have a meta-goal \\(MG_1\\) that attempts to ensure a contraction-like property not only for the goals \\(G(t)\\) but also for the meta-goal \\(MG_1(t)\\) itself. Specifically, for each increment of N units of time, it tries to enforce:\n\u2022 Goal contraction:\n\\(d(G(t + N), G(t)) < c \\cdot d(G(t), G(t - N))\\)\n\u2022 Meta-goal contraction:\n\\(d(MG_1(t+N), MG_1(t)) < c d(MG_1(t), MG_1(t - N))\\)\nfor some fixed \\(c < 1\\).\nFor now, the metric d remains fixed and cannot be modified by self-modification; the system can modify only G and its other internal states but not the definition of d. We will look at lifting this restriction a little later."}, {"title": "3.2.1 Analysis of the New Situation with More Flexible Evolution", "content": "Augmented State Space: Consider the augmented state that now includes both \\(G(t)\\) and \\(MG_1(t)\\). Call this combined state at time t:\n\\(S'(t) = (G(t), MG_1(t), other internal states).\\)\nThe difference between two states \\(S_1(t)\\) and \\(S_2(t)\\) can now be measured by a modified metric (or just consider d applied component-wise). Since d applies to both goals and metagoals, we can write something like:\n\\(d' (S_1(t), S_2(t)) = d(G_1(t), G_2(t)) + d(MG_{11}(t), MG_{12}(t)) + . . .\\)\nFor simplicity, we will focus here on G and MG1; \"other internal states\" can be handled similarly if they are also influenced by the meta-goal constraints.\nIterative Contraction at Intervals of N: The meta-goal \\(MG_1\\) enforces a contraction property every N steps:\n\u2022 The distance in G after N-steps is reduced by at least a factor c.\n\u2022 The distance in MG1 after N-steps is also reduced by at least a factor c.\nThus, for both the base goals and the meta-goal itself, differences shrink geo- metrically over increments of length N.\nFrom N-step Contraction to M-step Contraction: Assume M is some multiple of N (e.g. M = kN for some integer k). Over each N-interval, differ- ences in both G and MG1 shrink by a factor c. After k such intervals (which is one full iteration from t to t + M minutes), the total contraction factor on these components would be at most \\(c^k\\). This ensures that over the longer in- terval M, the system states become progressively closer in both their goals and metagoals. Since \\(c < 1\\), raising it to the k-th power only strengthens the con- traction (assuming no contrary forces are too strong).\nExpectations and Stochasticity: The argument that F (which maps \\(R(t)\\) to \\(R(t + M)\\)) is contractive on average now operates on the augmented space of distributions over both goals and metagoals. The presence of the meta-goal contraction condition ensures that not only do the goals not drift apart, but the very principles (metagoals) guiding goal stability also do not drift apart. In a stochastic environment, we consider the expected distance:\n\\({E}[d'(F(x), F(y))]\\)\nBasically: If the environment's randomness does not overwhelm the contrac- tion induced by the meta-goal MG1, the net effect is still that, in expectation, the distributional difference between two initial states shrinks by at least a factor of c per N-step, and thus by at most \\(c^k\\) over an M-step interval.\nConcretely, the meta-goal imposes a strong stabilizing regime: each iteration cannot increase discrepancies significantly because it must also reduce them by a factor of c. Over multiple iterations, this yields a stable fixed point scenario akin to a contraction mapping, but now involving both G and MG1."}, {"title": "3.3 Goal Stability with Dynamic Base Goals / Dynamic Metagoal / Dynamic Metric", "content": "Now let's take the next natural step and ask: What if instead we have the metric also changing over time via self-modification? After all, a smarter and smarter AGI system will probably come up with smarter and smarter ways to assess the similarity of two different goals or metagoals.\nWe are looking here at\nMeta-goal \\(MG_2(t)\\): Pursue goals \\(G(t)\\) for the next N minutes, and to the extent that this involves self-modifying, try to make it so that, where \\(G(t + N), MG_2(t+N)\\) and \\(d(t + N)\\) are its new goals, metagoals and metric at time t+N\n\\(d(t)(G(t + N), G(t)) < c d(t)(G(t), G(t - N))\\)\n\\(d(t)(MG_2(t+N), MG_1(t)) < c d(t)(MG_2(t), MG_2(t - N))\\)\n\\(d(t)(d(t + N), d(t)) < c d(t)(d(t), d(t - N))\\)\naccording to metric d(t), for some \\(c < 1\\).\nThere is some interesting self-reference here, in the application of the metric d to measure distances between itself and other metrics. However this can be a virtuous rather than vicious sort of circle. If we assume the metric is say a computer program and it acts on spaces of computer programs, then it makes sense to apply the metric to itself self-referentially as is done here.\nThis variation on the set-up does of course make things trickier. Originally, the argument that an operator Fis contractive on average relied on a fixed metric space. With a fixed metric, we had a stable notion of \"distance\" that does not vary as the system evolves. Once we allow the metric d to vary, we must handle a self-referential definition: the new metric at time t + N, denoted d(t + N), must remain close to the old metric d(t) according to d(t) itself, and must contract differences at a rate \\(c < 1\\). That is, in the new set-up\n\u2022 We have three evolving objects: the goals G(t), the meta-goal \\(MG_2(t)\\), and the metric d(t).\n\u2022 Each of them must contract differences relative to their own past states by a factor \\(c < 1\\)."}, {"title": "3.4 Approaching Moderated Goal Evolution Contractively", "content": "One can emulate the above arguments in the context of moderated goal evolution rather than goal stability. The same form of arguments apply, although the amount of compute resource and overall sensitivity and complexity of actually implementing the process might be more severe.\nModifying slightly the above initial problem formulation: Suppose we have a stochastic AI system S that operates in a stochastic environment, in a manner that is associated with discrete time intervals of length M.\nOver each interval \\([t, t + M]\\), the system thinks and takes action in its envi- ronment, and also can modify its own source code or hardware implementation as it sees fit; in doing so, it pursues its current base goals \\(G(t)\\) for most of the time, but also invests some effort in pursuing a stable meta-goal \\(MG_m(t)\\). Its self-modification updates its goals, so \\(G(t)\\) is changing over time.\nGiven a time interval I, define\n\u2022 the maximum variation of G over I as the maximum distance between G(t) and G(s) for any two \\(s, t \\in I\\)\n\u2022 the maximum plausible variation of G over I as the largest value that the system estimates the maximum variation of G could have taken over I, in any plausible possible-history\nFor instance, if the AI system has a formal model of itself, the maximum envi- sioned variation might be obtained by proving an upper bound on the maximum variation of G given appropriate assumptions.\nSuppose we have a target \\(m_M\\) for the maximum plausible variation of the system's goals during an interval of length M.\nWe may then define a meta-goal \\(MG_m(t)\\) that directs the system to shape its self-modifications so that after\n\u2022 the distance between the maximum plausible variation of G over (t, t+T) and the target \\(m_M\\) is less than\n\u2022 the distance between the maximum plausible variation of G over (t, t -T) and the target \\(m_M\\) by a factor \\(c < 1\\).\nIn other words, this meta-goal ensures a kind of \"goal change contraction\" step-by-step: the variability of the system's goals gets closer and closer to the target variability limit, step by step."}, {"title": "4 Global Optimization Based Metagoals for Moderated Goal Evolution", "content": "The metagoals considered above, explicitly incorporating contraction-mapping- based conditions, are conceptually and mathematically speaking somewhat \"blunt"}, {"title": "4.1 Dynamic Base Goals / Fixed Metagoal / Fixed Metric", "content": "Varying on the set-ups considered above, suppose we have a stochastic AI system \\(S(t)\\) with base goals \\(G(t)\\) that is spending much of its resources pursuing the goals \\(G(t)\\) at time t, in interaction with a stochastic environment, but also spending some time pursuing a certain meta-goal, to be specified below.\nAs above, let \\(R(t)\\) be the \"dynamic state\" of the system the probability"}, {"title": "4.1.1 Conceptual Interpretation of Conditions", "content": "The contraction condition here is replaced with more complex assumptions re- garding compactness, continuity and convexity. The intuitive meaning of these is perhaps less obvious than with the contraction condition, but is not that obscure upon a bit of reflection:\n\u2022 Continuity means, roughly, that small changes in goals should lead to small changes in probabilistic system behavior, on average."}, {"title": "4.1.2 Conceptual Interpretation of Fixed Point Distribution", "content": "Under meta-goal \\(MG_m\\), reaching a fixed point distribution \\(R = F(R)\\) means intuitively that the system settles into a stable regime where both its base- level goals and the meta-goal itself fluctuate within bounded limits rather than changing unboundedly. In other words, at the fixed point distribution, the system continues to make adjustments to its goals and meta-goals, but these adjustments remain contained within a stable, bounded range. The system does not eliminate changes entirely but maintains a steady-state pattern of small and controlled modifications, in line with what \\(MG_m\\) aims to achieve. Essentially, \\(MG\\) enforces a second-order stabilization: both the object-level goals and the guiding meta-goal strategies must stabilize.\nAccording to the equilibrium distribution, the system's modifications?both in base-level goals and in the meta-goal itself?are limited to a bounded or shrink- ing range. Once the system reaches the fixed-point distribution:"}, {"title": "4.1.3 Step-by-Step Argument Sketch", "content": "Compactness and Convexity Setup: The meta-goal \\(MG_m (t)\\) ensures that the system remains within a controlled region of the space of all possible AI systems. Specifically, the probability distributions \\(R(s)\\) over the system states for times s > t remain, with high probability, restricted to a compact subspace. Additionally, this subspace is convex in a suitable sense (e.g., weighted averages of valid system configurations also yield valid system configurations).\nThese conditions mirror key assumptions of Schauder's fixed point theorem, which requires a continuous mapping defined on a convex, compact subset of a normed vector space. Here, we are effectively ensuring we operate within such a well-structured space of possible system states.\nContinuity of the Mapping F: We also want continuity: small changes in goals lead to small changes in system behavior and thus in the distribution \\(R(t)\\). This ensures F, which maps \\(R(t)\\) to \\(R(t + M)\\), is continuous with respect to a suitable topology on the space of probability distributions of system states. If F were not continuous, small tweaks to the system's goals or state could cause large, unpredictable jumps in future distributions, making any fixed-point approximation scheme difficult.\nBy the meta-goal's design and the system's intelligent self-modification pro- cess, continuity is enforced: the system selects self-modifications that result in stable, smoothly varying behaviors over time, ruling out large discontinuities.\nExistence of a Fixed Point (Classically vs. Constructively): Classi- cally, Schauder's theorem states that a continuous self-map on a convex, com- pact, nonempty subset of a normed space has a fixed point. If we view the set of all plausible probability distributions R of the system states as such a subset (assuming appropriate functional-analytic structure), then classically, we know a fixed point exists.\nConstructive mathematics does not directly let us conclude the existence of such a fixed point in a fully non-constructive manner, but it does provide approximation theorems. That is, we know there are iterative processes that can approximate a fixed point to arbitrary precision.\nIterative Approximation via Self-Modification and Search: The sys- tem, via pursuing the meta-goal MG as part of its dynamics, is essentially implementing a constructive approximation procedure. Over intervals of length N, the system attempts to choose self-modifications that steer it into stable regimes. In periods of length M (with \\(M>> N\\)), we consider how distributions evolve under these stabilizing constraints.\nBy searching the space of possible self-modifications:"}, {"title": "4.2 Dynamic Base Goals / Dynamic Metagoal / Dynamic Metric", "content": "Now let us adapt the above Schauder-like argument to the case where the meta- goal (let's call it \\(MG_{2m}\\)) and the metric d are also being modified in the course"}]}