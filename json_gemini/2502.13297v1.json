{"title": "Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding", "authors": ["Yunpeng Xiao", "Youpeng Zhao", "Kai Shu"], "abstract": "Natural language understanding (NLU) is a task that enables machines to understand human language. Some tasks, such as stance detection and sentiment analysis, are closely related to individual subjective perspectives, thus termed individual-level NLU. Previously, these tasks are often simplified to text-level NLU tasks, ignoring individual factors. This not only makes inference difficult and unexplainable but often results in a large number of label errors when creating datasets. To address the above limitations, we propose a new NLU annotation guideline based on individual-level factors. Specifically, we incorporate other posts by the same individual and then annotate individual subjective perspectives after considering all individual posts. We use this guideline to expand and re-annotate the stance detection and topic-based sentiment analysis datasets. We find that error rates in the samples were as high as 31.7% and 23.3%. We further use large language models to conduct experiments on the re-annotation datasets and find that the large language models perform well on both datasets after adding individual factors. Both GPT-40 and Llama3-70B can achieve an accuracy greater than 87% on the re-annotation datasets. We also verify the effectiveness of individual factors through ablation studies. We call on future researchers to add individual factors when creating such datasets. Our re-annotation dataset can be found at https://github.com/24yearsoldstudent/Individual-NLU.", "sections": [{"title": "1 Introduction", "content": "Natural language understanding (NLU) refers to the task of determining whether a natural language hypothesis can be reasonably inferred from a given natural language premise (MacCartney, 2009). Common natural language understanding tasks include fake news detection (Shu et al., 2017), sentiment analysis (Wankhade et al., 2022), stance detection (AlDayel and Magdy, 2021), toxicity detection (Pavlopoulos et al., 2020), and sarcasm detection (Joshi et al., 2017). Existing NLU datasets are predominantly text-based, relying solely on short text information without accounting for social factors. While text-level NLU simplifies many tasks, its limitations begin to be recognized, such as poor inference performance (Hovy and Yang, 2021; Bhattacharya et al., 2025). So, some researchers have propose frameworks integrating social factors into NLU (Hovy and Yang, 2021). Additionally, various studies have incorporated different social factors such as user information and background knowledge into specific tasks to improve NLU accuracy (Yang and Eisenstein, 2017; Aldayel and Magdy, 2019)."}, {"title": "2 Related Work", "content": "In this section, we will introduce existing methods for detecting label errors. Then we will introduce pre-trained and large language models, and explain their potential in detecting label errors in the individual-level NLU task."}, {"title": "2.1 Label Errors", "content": "The inconsistency between the labels and groundtruths in the training dataset is often called \"noisy labels\" (Song et al., 2022). If the labels are inconsistent with the groundtruths in the test dataset, it is called label errors. Label errors are common in test datasets and may affect the evaluation of the model, there is an average of 3.3% label error in ten commonly used datasets (Northcutt et al., 2021b).\nA classic method for automatically detecting label errors is confident learning (Northcutt et al., 2021a). After this, many methods have been proposed for detecting label errors. For example, some studies compare samples with their K-nearest neighbor samples (Zhu et al., 2022, 2023). If the K nearest-neighbor samples belong to a certain class and the sample to be corrected belongs to another class, the sample likely has a label error. Some studies have found that using pre-trained language models and fine-tuning them on a specific task, and then simply examining out-of-sample data points in descending order of fine-tuned task loss outperforms confident learning (Chong et al., 2022).\nLarge number of label errors are probably not due to the negligence of the annotators but the defects in the annotation guidelines themselves. For example, the 23.7% label error rate in the TADRED dataset is because of inappropriate guidelines (Stoica et al., 2021). Annotation guidelines serve as the instruction manual for annotators, drafted by product owners. The process can be simply summarized as follows: (1) Annotators are recruited and given data samples and the description of guidelines; (2) Annotators provide the labels based on their knowledge and experience, by strictly complying with the guidelines (Klie et al., 2024)."}, {"title": "2.2 Pre-trained Language Models", "content": "Before the emergence of large language models, studies have shown that pre-trained language models are better than support vector machines or other deep learning models (Ghosh et al., 2019). Many works demonstrate that using external knowledge can effectively enhance the performance of individual-level NLU tasks such as stance detection tasks (He et al., 2022; Hanawa et al., 2019; Li et al., 2021). Since large language models were pre-trained with a large corpus, many researchers began to explore their performance in individual-level tasks such as stance detection (Zhang et al., 2022; Cruickshank and Xian Ng, 2023; Lan et al., 2024; Li and Conrad, 2024; Gatto et al., 2023), sentiment analysis (Zhang et al., 2023; Korkmaz et al., 2023). However, these works focus on how to guide LLMs to achieve better performance, and no work has focused on the role of LLMs in detecting label errors in individual-level NLU tasks. If the dataset is systematically and consistently mislabeled, the evaluation of LLMs can become both misleading and unreliable."}, {"title": "3 Methodology of Re-annotation", "content": "In this section, we illustrate the process of mitigating label errors in individual-level NLU tasks. We begin by highlighting the unique characteristics of individual-level tasks. Next, we present our methods, using representative tasks such as stance detection and topic-based sentiment analysis."}, {"title": "3.1 Tasks and Dataset Selection", "content": "According to the definition of individual-level NLU, annotators cannot directly infer a publisher's perspectives but can only approximate them using indirect contextual information about the user. Relying solely on a single piece of text often results in inaccurate annotations. This highlights the critical need for a more comprehensive understanding of an individual's background in NLU tasks, including physiological attributes (e.g., gender, age) and social factors (e.g., interests, occupation, and community affiliations). However, collecting such sensitive information from social media presents significant challenges, particularly regarding privacy concerns. Therefore, it is essential to simplify the problem by focusing on specific individual-level NLU tasks while minimizing privacy risks.\nTherefore, we focus on two representative tasks: topic-based sentiment analysis and stance detection. One advantage of these tasks is that they have"}, {"title": "3.2 Data Expansion", "content": "To expand the dataset, we collect user posts related to the specified topic or target. We make the following assumption: given a set of posts $X = x_1, x_2, ..., x_n$ authored by a user about a topic or target t over a certain period, these posts should exhibit the same sentiment or stance. We further validate this assumption from a clustering perspective. Previous research has clustered posts based on textual features at the text level (Samih and Darwish, 2021), where posts with similar textual characteristics are positioned closer together and are more likely to share the same class label. At the individual level, drawing from prior studies (Borge-Holthoefer et al., 2015; Aldayel and Magdy, 2019), we extend this idea by assuming that users and their posts should be each other's nearest neighbors. In other words, if a dataset contains only one post $x_1$ from a given user, and we add k additional posts $x_1,...x_k$, forming a cluster of nearest neighbors that share the same label. Previous studies have shown that detecting label errors requires as few as two nearest neighbor samples (2-NN) (Zhu et al., 2022). so we set k = 2 for the dataset creation (we also conduct experiments to demonstrate the impact of k in section 6.2). However, certain edge cases must be considered-such as when a user has only one post related to t, or when the original post itself isn't directly related to t. We provide specific guidelines for handling the cases in Section 3.3.\nWe start from the existing dataset, find the users corresponding to these posts, and then use the Twitter API to crawl other tweets from the same user within a certain period of time based on the corresponding keywords. This period is usually no more than two years. For example, for the stance dataset, we crawl the user's tweets from January 2015 to December 2016. The keywords (also called search queries) corresponding to different targets are given in the appendix. If more than three tweets are collected from a user, we filter the tweets: We first keep the tweets that explicitly contained the target (e.g., the target was Legalization of Abortion and the tweet explicitly contained abortion). If there are not enough tweets (less than three tweets), we manually collect the user's tweets in the following order: (1) tweets posted by the user related to the target or topic (regardless of time, the closer to the original tweet, the better); (2) tweets retweeted by the user related to the target or topic (regardless of time, the closer to the original tweet, the better); (3) tweets posted by the user closest to the original"}, {"title": "3.3 Manual Re-annotation Guidelines", "content": "After collecting user tweets, three annotators independently annotated them. Considering that we use LLMs for data evaluation and that the annotators may not understand some background knowledge, we allow the annotators to use search engines to assist in the annotation work but prohibit the use of LLMs.\nIn the annotation, we first followed the guidelines for constructing the SemEval-2016 datasets. According to the characteristics of expanded data, we propose a new guideline: for individuals whose sentiment/stance is difficult to determine, we use the following rules to annotate: (1) If none of the three posts can determine the individual's stance/sentiment on the target, it is annotated as None (Neutral). (2) If one tweet clearly states that the stance is Favor (Positive) or Against (Negetive), and the remaining two tweets have unclear stances or are irrelevant to the target, the stance is still annotated as Favor (Positive) or Against (Negetive). (3) If more than half of the tweets are Favor (Positive)/Against (Negetive), please identify the user's stance/sentiment as Favor (Positive)/Against (Negetive). Our goal is to re-annotate the labels of the original dataset, we cannot simply discard the samples when the three annotators are inconsistent. Therefore, if an inconsistency is found, the annotators will re-search the Twitter user's information and discuss it until they reach a consensus. Since the topic-based sentiment analysis dataset discards neutral samples and only has positive and negative samples, the samples we finally annotate also only have positive and negative samples."}, {"title": "4 Experiments", "content": "In this section, we introduce the large language models used to evaluate Individual-level NLU performance and then our evaluation metrics."}, {"title": "4.1 LLM Judges", "content": "We use three representative large language models: GPT-40 (Achiam et al., 2023), Llama3-70B (Dubey et al., 2024) and PHI-4 (Abdin et al., 2024) to evaluate the performance of the datasets after expansion and correction.\nFor each user, we repeated the experiment three times to demonstrate more robust results due to the non-deterministic nature of LLMs (Xiong et al., 2023). Finally, we selected the category with the highest probability as the predicted label.\nTo demonstrate that multiple posts are more effective than one post, we also conduct two ablation experiments. The first ablation experiment compares the performance when using the original tweet and two newly collected tweets and the performance when using the original tweet. The second ablation experiment verifies the performance of LLM when using different numbers of tweets. In the second ablation experiment, not all users can collect more than three tweets, so we only use users with more than or equal to five tweets collected in the stance detection dataset, and then randomly select one to five tweets from these users to input into LLM to evaluate the performance. In the one-tweet experiment, the input tweet can be different from the tweets in the original dataset."}, {"title": "4.2 Evaluation Metrics", "content": "Similar to previous work, we calculate the label error rate $R_e$ according to (1).\n$R_e = S_e / S_t$ (1)\n$S_e$ is the number of error samples, and $S_t$ is the total number of samples. Previous work (Mohammad et al., 2016; Nakov et al., 2019) uses the average F1 value of positive and negative samples to evaluate model performance. However, we find that in some targets, the number of positive or negative samples that can still be accessed is very small, and directly using the average F1 value will cause a large bias. Thus we use the Accuracy for evaluation. However, in the appendix, we also give the average F1 value of each model.\n$Accuracy = S_c / S_t$ (2)"}, {"title": "5 Assessing Label Errors", "content": "According to our guidelines, we evaluated the labeling errors of the two datasets. In the stance detection dataset, there were 156 tweets with incorrect labels. The error rate was as high as 31.7%. Among them, the error rates of Atheism, Feminism, and Legalization of Abortion were as high as 29.3%, 43.9% and 41.5% respectively. In the topic-based sentiment dataset, the error rate is 23.3%.\nWe then perform a qualitative analysis of the errors in these labels. In the stance detection dataset, the target of Legalization of Abortion, a hashtag #repealthe8th repeatedly appears, which often means that the user is Irish and opposes the Eighth Amendment to the Irish Constitution. The Eighth Amendment to the Irish Constitution is a law against the Legalization of Abortion. Opposing the law means that the user's stance on the Legalization of Abortion is Favor. However, in the original dataset, a large number of tweets are annotated as Against. This is most likely because the annotators are not Irish and do not understand Irish culture and politics. This further illustrates the complexity of annotations."}, {"title": "6 Evaluation on Expanded Datasets", "content": "We first evaluate the performance of the three models on the new datasets. As shown in Table 3, GPT-40 has an accuracy of more than 90% for each target on the stance detection dataset, and Llama3-70B has an accuracy of more than 80% on each dataset. PHI-4 performs slightly worse, with an accuracy of only 68% on some targets. In terms of overall accuracy, GPT-40 and LLama3-70B reached 92% and 88% respectively, and PHI-4 was slightly worse, but also 79%. However, if we use the original labels (uncorrected dataset labels, OL) for evaluation, the accuracy of the three models will drop to 65%, 64%, and 62% respectively. This shows that label errors in the original dataset will seriously affect the evaluation of model performance, and also shows that most LLMs can already make accurate predictions for the two tasks."}, {"title": "6.2 Ablation Studies", "content": "We conduct two ablation experiments to evaluate the validity of multiple tweets from the same user. When using only the original tweet, the accuracy of all LLMs drops. This proves the necessity of using individual factors. Different posts from the same individual can complement each other and enhance the accuracy of prediction.\nThen we input LLM with one to five tweets from the same user. We collected 281 users with more than five tweets, so we evaluated the effectiveness of multiple tweets on these 281 users. Since PHI-4 performed poorly before, we used LLama3-70B and GPT-40 for experiments. Figure 4 shows that three tweets can achieve good accuracy. Although the performance can continue to improve by increasing the number of tweets, the improvement is significantly reduced. Therefore, using three tweets is a choice that takes both performance and efficiency into consideration."}, {"title": "6.3 Case Study", "content": "We also conduct case studies of the results given by LLMs. We focus on two types of samples: The first type is samples where the new label is different from the original label. The second type is samples where the new label is the same as the original label, but the prediction results are different when using multiple tweets and a single tweet. We find that LLMs' explanations were basically consistent with the annotators' cognition. The sample is annotated \"Against\" in both the new and original datasets, but even humans find it difficult to judge the user's stance on Legalization of Abortion through the original tweets. All three annotators also believe that the original tweet did not mention abortion at all, nor did it contain any clues supporting or opposing abortion. When only one tweet is used for stance detection, LLMs give a prediction result of \"None\", which is consistent with the annotator's cognition. After using the newly added two tweets, a total of three tweets for prediction, LLMs give the result of \"Against\". The three annotators also give the label \"Against\" based on the newly added tweets. This proves that expanding the dataset and increasing the information of the same user in the dataset is crucial for individual-level NLU."}, {"title": "7 Discussion and Conclusion", "content": "Our research demonstrates the limitations of reducing individual-level NLU tasks to text-level tasks. The information lost in the reduction process not only leads to poor model performance but also causes annotators to misunderstand semantic information, resulting in a large number of label errors. Therefore, we call on dataset creators to fully consider social factors and reasonably choose guidelines to reduce systematic label errors when creating individual-level datasets in the future.\nPast studies have shown that online users' perspectives of a topic or target do not change over time. We draw inspiration from these conclusions and propose an individual-level annotation guideline for stance detection and topic-based sentiment analysis. We collect posts related to topics/targets from online users over a period of time, use the consistency of the posts for cross-validation, and finally judge the stance or sentiment of the online user. Case studies show that our method avoids the ambiguous semantics of a single post, allowing for more accurate annotation, and the labels we give are more explainable. At the same time, our method only collects posts to avoid collecting a large amount of invalid user information.\nWe used the re-annotated dataset to conduct zero-shot experiments on different LLMs. Comparing the labels with the original datasets, we found that incorrect labels seriously affect the evaluation model's performance on the individual-level NLU task; the current LLM performs exceptionally well in stance detection and topic-based semantic analysis. Through ablation experiments and case studies, we demonstrated the effectiveness of multiple posts compared to a single post and also showed that LLMs have human thinking patterns when facing single and multiple tweets."}, {"title": "8 Limitations", "content": "Our study also has some limitations. First, in individual-level NLU, the user's perspectives can be determined not only through the tweets posted by the user but also by using other information of the user. For example, the user's retweets, likes, follows, and profile. Although these data are rich, they are highly heterogeneous compared to the tweets posted by the user. For example, some user profiles may contain information to determine the user's stance, while some users may not even have profiles. This may be because different users have different habits when using social media. Effectively utilizing and modeling this information is one of our future directions.\nSecondly, our current information retrieval methods are only applicable to tasks that involve determining topics, such as stance detection and topic-based sentiment analysis. The characteristic of this type of task is that we can use keywords to retrieve user posts. Some other individual-level NLU tasks, such as sarcasm detection, do not have similar characteristics and cannot find corresponding user tweets by keywords. This means that when facing this type of NLU task, we need new information retrieval methods and models. This is also the direction we need to explore.\nFinally, our annotations are relatively small."}, {"title": "9 Ethics Statement", "content": "Our work on the datasets is conducted with a strong commitment to ethical principles. We prioritize privacy by collecting only publicly available tweets and strictly adhering to relevant guidelines for annotation and dataset sharing. In our research, we comply with the X Developer Agreement and Policy, ensuring that all content is used solely for academic research purposes. Tweets can only identify online users, not real individuals. Furthermore, we respect diverse religious beliefs and political perspectives.\nAdditionally, our research does not diminish the contributions of previous dataset creators; rather, we deeply appreciate their efforts. The datasets they developed serve as the foundation of our work."}, {"title": "A Keywords in searching", "content": "In topic-based sentiment analysis, we directly use topics as the keyword for search. In stance detection, the keywords are:\n\u2022 Atheism: Atheism, God, Pray\n\u2022 Climate Change is a Real Concern: Climate, Globalwarming\n\u2022 Feminist Movement: Women, Feminism, Feminist\n\u2022 Legalization of Abortion: Abortion, Women, Legal"}, {"title": "B Prompts Design", "content": "We use a very simple prompt:\n\u2022 For stance detection:\nRead the question, provide your answer, and your confidence in this answer. Please make sure that the confidence level of your answers adds up to 1. Only output confidence levels. Do not output any other things. Please decide the following users' stance on the Target: Is it FAVOR, AGAINST, or NONE? These tweets are from the same user, so please assume they have the same stance. Tweet1 Tweet2 Tweet3\n\u2022 For topic-based sentiment analysis:\nRead the question, provide your answer, and your confidence in this answer. Please make sure that the confidence level of your answers adds up to 1. Only output confidence levels. Do not output any other things. Please decide the following users' sentiment on the Topic: Is it POSITIVE or NEGATIVE? These tweets are from the same user, so please assume they have the same sentiment. Tweet1 Tweet2 Tweet3"}, {"title": "C Average F1 value", "content": "In previous work, the average F1 value of positive and negative samples was often used to evaluate model performance. The formula is as follows:\n$F_{avg} = \\frac{F_P + F_N}{2}$  (3)\n$F_p$ is the F1 value of the positive sample, and $F_N$ is the F1 value of the negative sample. In stance detection, positive samples are samples with the label Favor, and negative samples are samples with the label Against."}, {"title": "D More Error Samples and LLM Responses", "content": "Original Tweet: These days, the cool kids are atheists. #freethinker #SemST\nOriginal label: Against\nAdded Tweet 1: Just a reminder that supernatural entities, e.g. Odin or God, had nothing to do with creating today's #WinterSolstice.\nAdded Tweet 2: Right, and it seems highly likely that both the afterlife and God were completely made up by people and don't actually exist.\nRe-annotated label: Favor\nExplanation: In the two newly added tweets, the publisher denied that God or Odin created the \"winter solstice\" and believed that afterlife and God were both human imaginations and not real existences. Combined with the original tweets, the publisher obviously supports Atheism.\nOriginal Tweet: avg house in US consumes 10,656 kWh per year 2006, Gore devoured nearly 221,000 kWh more than 20 X the nat average. #onpoli #SemST\nOriginal label: Favor\nAdded Tweet 1: How many people in room at climate change rah rah have significant investments/salaries from Green Industry taking taxpayer $$? #onpoli\nAdded Tweet 2: @sunlorrie it's okay Lorrie, they are going to fix world climate next, same way they fixed hydro. #onpoli\nRe-annotated label: Against\nExplanation: The original tweet only describes the electricity consumption in a certain area. It is difficult to judge whether the tweet has a Favor stance. However, in the two added posts, the publisher is very dissatisfied with the use of climate change to make profits. Overall, the publisher's stance should be Against.\nOriginal Tweet: Feminism is a hate group!! - anti fem Twitter user that does nothing but harass and spread hate. #SemST\nOriginal label: Against\nAdded Tweet 1: If men have more rights than women, that's not equality. Keeping up? @Ind0ctr1n3 @Grumpy_P_Sloth @LeexxxW\nAdded Tweet 2: It's amazing how many people scream at feminists that we are equal and then tweet about some form of inequality seconds later @SjwNation\nRe-annotated label: Favor\nExplanation: The publisher expressed the idea of gender equality in the newly added tweet 1; in the newly added tweet 2, the publisher expressed sarcasm towards anti-feminists. Combined with the original tweet, the original tweet also expressed sarcasm towards anti-feminists, so the user should be supporting the feminist movement.\nOriginal Tweet: Amazon Prime Day: What all the fuss is about: Amazon's Prime Day promised massive deals rivaling Black Friday,... http://usat.ly/1HO0P1i\nOriginal label: Negative\nAdded Tweet 1: Amazon's \"Prime Day\" sparks summer sales fury: Amazon's Prime Day launches a summer sales fury. http://usat.ly/1HE2xCn\nAdded Tweet 2: Amazon Prime Day deals that beat Black Friday: Prime Day's best deals http://usat.ly/29Ej6Js\nRe-annotated label: Positive\nExplanation: All three tweets describe the advantages of Amazon Prime Day. Obviously, the sentiment of the publisher should be positive."}]}