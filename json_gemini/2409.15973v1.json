{"title": "Edge-device Collaborative Computing for Multi-view Classification", "authors": ["Marco Palena", "Tania Cerquitelli", "Carla Fabiana Chiasserini"], "abstract": "Motivated by the proliferation of Internet-of-Thing (IoT) devices and the rapid advances in the field of deep learning, there is a growing interest in pushing deep learning computations, conventionally handled by the cloud, to the edge of the network to deliver faster responses to end users, reduce bandwidth consumption to the cloud, and address privacy concerns. However, to fully realize deep learning at the edge, two main challenges still need to be addressed: (i) how to meet the high resource requirements of deep learning on resource-constrained devices, and (ii) how to leverage the availability of multiple streams of spatially correlated data, to increase the effectiveness of deep learning and improve application-level performance. To address the above challenges, we explore collaborative inference at the edge, in which edge nodes and end devices share correlated data and the inference computational burden by leveraging different ways to split computation and fuse data. Besides traditional centralized and distributed schemes for edge-end device collaborative inference, we introduce selective schemes that decrease bandwidth resource consumption by effectively reducing data redundancy. As a reference scenario, we focus on multi-view classification in a networked system in which sensing nodes can capture overlapping fields of view. The proposed schemes are compared in terms of accuracy, computational expenditure at the nodes, communication overhead, inference latency, robustness, and noise sensitivity. Experimental results highlight that selective collaborative schemes can achieve different trade-offs between the above performance metrics, with some of them bringing substantial communication savings (from 18% to 74% of the transmitted data with respect to centralized inference) while still keeping the inference accuracy well above 90%.", "sections": [{"title": "1. Introduction", "content": "Accelerated by the remarkable success of Internet of Things (IoT) technologies, more and more practical environ- ments are being equipped with sensors connected to mobile and IoT devices. Prominent examples of such environments include smart cities, smart transportation systems, and smart factories. The large amounts of data collected by the above devices, in conjunction with recent breakthroughs in deep learning, are driving the proliferation of intelligent applica- tions and services. In addition, the edge computing paradigm is increasingly shifting computing loads from the core to the edge of the network to deliver faster responses to end users, reduce bandwidth consumption towards the cloud, and address privacy concerns.\nHowever, several significant challenges still remain to be addressed in order to fully realize deep learning at the edge. On the one hand, increasingly complex inference tasks require highly parametrized deep neural networks (DNNs) to be executed, which crave for computational and memory resources. On the other hand, most edge devices that perform inference tasks in real-time have limited computation, mem- ory, energy, and communication capabilities. Furthermore, IoT devices are often deployed with some degree of redun- dancy or overlap, resulting in data streams exhibiting sig- nificant spatial correlation. Being able to fuse data coming from multiple sources is key to achieving better prediction accuracy in many fine-grained inference tasks, although it comes at a higher communication and orchestration cost. Considering these trends, it is critical to envision efficient mechanisms that allow for the collaborative execution of increasingly complex inference tasks at the edge of the network, with the different edge nodes sharing data as well as the computational burden.\nIn this paper, we take the above challenge, focusing on the relevant case of computer vision tasks applied to images collected by different end devices. Computer vision tasks, enabling machines to derive meaningful information from digital images, have been at the forefront of deep learning applications due to their important role in solving problems arising from various fields. Differently from existing works tackling the orchestration of DNNs at the edge [1, 2] or the offloading of such tasks from end devices to the edge [2, 3], our first goal is to leverage the redundancy or overlap of the images collected by such cameras to increase the accu- racy of the inference while reducing the computational and communication load that such inference implies. Our second goal is to investigate the benefits and the hurdles of different collaborative approaches between edge servers and end de- vices capturing the images, each reflecting a different level of cooperation and split of the computational/communication burden at the edge and at the devices. Collaboration can involve sharing features or inferences and/or splitting com- putation (i.e., distributing the machine learning operations) between the different nodes. Thus, it is crucial to assess their performance from the point of view of both inference quality and consumption of radio/computational resources.\nSpecifically, we envision a networked system with per- node sensing and/or processing capabilities, in which sens- ing nodes are equipped with cameras with partially over- lapping fields of view. At distinct time instants, the sensing"}, {"title": "2. Related Work", "content": "A large body of works have focused on supporting AI tasks at the edge, a research field known as edge intelli- gence [7, 8]. A common solution to this problem is compu- tation offloading, in which end devices offload the inference computational burden (or part of it) to another entity, being it a cloud server, an edge server, or another end device. These approaches are often based on model partitioning, splitting a multi-layered DNN across the different entities participating in the execution of the inference task. Existing approaches can be broadly categorized into (i) strategies that offload computation to a single centralized entity (ei- ther on the cloud [9, 10] or at the edge [11, 2]), and (ii) strategies that partition the DNN model across multiple end devices, enabling joint computation either in a purely decentralized [12, 13] or centrally-orchestrated way [14]. In both cases, each participating device retains only a portion of the DNN layers and transmits its output features, potentially compressed to minimize overhead [15], to another node. The primary drawback of this method is that no single device can perform inference independently; the entire group of devices sharing the DNN layers must be available. To overcome these limitations, some studies have focused on enabling inference at edge devices via DNN compression techniques such as parameter pruning, parameter quantiza- tion, and knowledge distillation [16]. Importantly, such tech- niques, although orthogonal to the collaborative inference schemes we investigate, can be integrated with those we consider to further reduce computation and communication overhead.\nIt is worth it to remark, however, that most of the above existing studies focus on inference tasks arising from a single end device, thus considering other nodes merely for their additional computational resources. Conversely, in our work, we consider a scenario in which the inference task can make use of the information generated by multiple end devices, each located at a different position. In such a scenario, collaboration among different nodes entails not only sharing the computational burden but also fusing correlated data. The need, and opportunity, to leverage spatially correlated data from different sources raises additional challenges such as deciding whether or not all captured data is actually needed to perform the inference, as the consumption of computational and network resources should be minimized. Although some recent work [17, 18] has considered a multi- agent collaborative inference scenario, where a single edge server coordinates the inference of multiple UEs, inference based on spatially correlated data captured by different nodes has not been addressed. Rather, the goal of the studies in [17, 18] is to minimize inference latency when multiple end devices offload their inference computation to the same edge server."}, {"title": "3. System Model and Inference Task", "content": "In this section we describe the network and computing system under study (Sec. 3.1). Then, we provide some back- ground on single-view and multi-view classification, as well as on the architecture of the deep learning models used to perform them (Sec. 3.2). Finally, we briefly introduce color histograms, a lightweight image descriptor that will be used in some of the proposed schemes (Sec. 3.3). The notation we use throughout the paper is summarized in Table 1.\nWe consider $V=\\{n_1,...,n_V\\}$, a set of source nodes composed of V edge devices equipped with a camera, a radio interface, as well as computational and memory resources. Source nodes may capture images of the same object, which differ in the perspective with which the object is captured, or in quality, size, or resolution. We refer to such an image, displaying an object captured by a source node, as a view."}, {"title": "3.2. Single- and multi-view inference tasks", "content": "Below, we define our reference inference tasks, namely, single-view and multi-view image classification. For each of them, we also present the high-level architecture of state-of- the-art deep learning pipelines that can be used to execute the task.\nSingle-view image classification. Given an input image $x\\in X$ and a set of K labels $Y=\\{y_1,...,y_K \\}$ representing classes of interest, the single-view image classification task consists in inferring which label $y\\in Y$ to assign to x, i.e., a single-view image classifier is a function mapping input im- ages to class labels. State-of-the-art image classifiers are of- ten implemented through complex, resource-craving convo- lutional neural networks (CNNs) [24, 25]. A CNN for single- view image classification $CNN_{SU}$ is a model composed of two parts: a convolutional feature extraction network (back- bone) and a fully-connected classification network (head), as shown in Fig. 4. The feature extraction network, denoted by $B_{SU}$, consists of a sequence of 2D convolutional blocks"}, {"title": "3.3. Color histograms", "content": "In some of the inference schemes we describe in Sec. 4, we will make use of color histograms as lightweight descrip- tors of views. Given a view $x_i(t)$ and a number of bins B, we denote with hist the function responsible for computing the color histogram $h_i(t)$ of $x_i(t)$. Color histograms are represented as 2-dimensional tensors in the space $H=\\mathbb{R}^{B\\times B}$: $h_i(t) = hist(x_i(t); B)$. The color histogram of a view is computed as follows:\nFirst, the view image is converted from the RGB to the L*a*b* color space [26], designed to be more perceptually uniform than the RGB.\nThen the ranges of the L*a*b* color channels (a* and b*) are divided into B bins and each pixel is counted into one of the resulting B\u00d7B buckets.\nFinally, the pixel count of each bucket is normalized by the number of pixels."}, {"title": "4. Collaborative Inference Schemes", "content": "A straightforward approach to handle classification tasks in which multiple nodes sense spatially correlated data would be to have each source node perform its inference in isolation, relying only on locally captured sensory data. This approach, however, has several drawbacks:\nSource nodes are often IoT devices with limited hard- ware resources and may be unable to execute entire deep learning pipelines;\nFor fine-grained image classification tasks character- ized by high inter-class similarity and a high intra-class variability (e.g., vehicle identification [27, 22]), the visual information conveyed by a single image may be insufficient to make accurate predictions.\nWe aim at mitigating such drawbacks by leveraging the spatial correlation among views taken by different source"}, {"title": "4.1. Centralized inference schemes", "content": "In centralized inference schemes, the central controller is responsible for aggregating single-view data received from the source nodes and then performing inference on the aggregated multi-view data. Source nodes, on the other hand, collect input images and may perform feature extrac- tion. Below, we specify how the central controller and the source nodes split the computational burden and collabo- rate towards the multi-view image classification, when all views get processed as well as when source nodes identify and discard redundant information before views are further processed."}, {"title": "4.1.1. Non-selective centralized inference", "content": "In the non-selective centralized inference scheme (CI), source nodes act merely as data sources, transmitting their views to the central controller. Specifically, the CI scheme, depicted in Fig. 7, operates as follows:\nSource nodes do not perform any processing, therefore they do not include the View Processing module and the in- put view $x_i(t)$ is forwarded directly to the central controller, i.e., $z_i(t)=x_i(t)$;\nThe central controller is responsible for performing multi-view classification; accordingly, its View Aggregation module aggregates the received views into a collection and then executes a multi-view CNN to obtain a prediction \u0177(t). The class prediction will then be delivered back to the source nodes.\nAs the CI scheme is non-selective, there is no sharing of contextual information between the central controller and source nodes. Consequently, neither the Quality Estimation module at the source nodes nor the Context Manager module at the central controller are implemented."}, {"title": "4.1.2. Selective centralized inference", "content": "In selective centralized inference schemes (SCI), each source node decides whether or not to transmit its view to the central controller based on a measure of similarity between the view and the current context. The context is received from the central controller and is used as a low-dimensional representation of the set of features collectively captured by the network at the previous or current time instant. Com- paring the current view to the context enables each source node to estimate how informative its view is with respect to the data sensed by the rest of the network. Only those views yielding sufficiently distinctive information with respect to the context will be forwarded to the central controller and will then contribute to the inference task, while the others will be discarded. The central controller is still responsible for performing multi-view classification but its input will be restricted to a subset of the most informative views. Further, we consider two variants of the SCI scheme, which suit two different types of use cases: one based on view embeddings and the other based on color histograms, as detailed below.\nSCI based on embeddings. In this case, referred to as SCI-E, each source node decides whether or not to send its view to the central controller by comparing it to a represen- tation of the features collectively captured by the network at the previous time period. Specifically, at every inference time t, the central controller sends to the source nodes the pooled view embedding computed at time t \u2212 1 as contextual information to discriminate views. As this scheme allows detecting the correlation among views taken at different time (temporal change of context), it processes the current views only if sufficiently different from what previously captured by the source nodes. The SCI-E scheme, depicted in Fig. 8, includes the following steps:\nSource nodes perform feature extraction to obtain an embedding $e_i(t)$ of their view $x_i(t)$. Thus, the View"}, {"title": "4.2. Ensemble inference schemes", "content": "In ensemble inference schemes, each source node per- forms a local inference task based on its captured view. The class predictions resulting from these inferences are then aggregated by the central controller using a consensus protocol."}, {"title": "4.2.1. Non-selective ensemble inference", "content": "According to this scheme, named EI, single-view clas- sification is performed locally by each source node while the central controller acts as a prediction aggregator. The EI scheme, depicted in Fig. 10, consists of the following steps:\nSource nodes perform local single-view classifica- tion on their input views $x_i(t)$; therefore the View Processing module implements a single-view CNN model to obtain a local prediction $\\hat{y}_i(t)$. The local prediction is then forwarded to the central controller, i.e., $z_i(t)=\\hat{y}_i(t)$;\nThe central controller acts as a prediction aggregator, hence its View Aggregation module implements a consensus protocol that selects as final prediction the label appearing most frequently among the predictions made by source nodes. Thus, \n$\\hat{y}(t) = arg \\max_y \\sum_{i=1}^{V} 1_{\\{\\hat{y}_i(t)=y\\}}$\nwhere $1_y$ for a given class label y is the indicator function taking on 1 if x=y, and 0 otherwise. The final class prediction \u0177(t) is then sent back to the source nodes;\nBeing the scheme non-selective, there is no sharing of contextual information between the central controller and source nodes. Consequently, it does require nei- ther the Quality Estimation at the source nodes nor the Context Manager at the central controller."}, {"title": "5. Experimental Evaluation", "content": "This section presents our experimental setup (Sec. 5.1) and the obtained results. In particular, we first analyze the impact of different values of the similarity threshold used in the selective schemes (Sec. 5.2). Then we compare the differ- ent schemes in terms of accuracy, communication overhead, and inference latency, under different operational conditions (Sec. 5.3)."}, {"title": "5.1. Experimental setup", "content": "Experiments are performed by running pre-trained single- and multi-view models under the experimental conditions detailed below.\nClassification dataset. We use the ModelNet40 [30] dataset to train the underlying CNN models and test the dif- ferent collaborative inference schemes. The dataset consists of a collection of multi-view 3D shape recognition instances with 12 views each and labels spanning across 40 classes. Instances are split into 9,483 training instances and 2,468 test instances. Consistently with previous works [6, 31], we use the standard train-test split provided by the dataset in our experiments. At a given time period t, the selective inference schemes based on view embeddings (SCI-E and SEI-E) require the embeddings of the views captured at t - 1 to generate the contextual information used to discriminate views. As the instances in our dataset are not temporally correlated with each other, for each instance we randomly select 6 out of the 12 available views and use them as the set of views captured by the network at the previous time period to derive the context. The remaining 6 views are used as the multi-view collection captured at the current time period. For a fair comparison, we limit the maximum number of"}, {"title": "5.2. Similarity thresholds evaluation", "content": "Here, we provide an experimental evaluation assessing the impact on accuracy and communication overhead of dif- ferent values of similarity threshold y for different variants of"}, {"title": "Inference latency.", "content": "To estimate the inference latency of the different schemes, we consider the previously mentioned reference scenario with 6 source nodes, randomly located within the coverage of a 5G base station (gNB), resulting in levels of signal-to-noise ratio (SNR) between 0 and 20 dB. We recall that we consider a slice of 50 Resource Blocks (RBs) allocated for the service. In the following experi- ments, the available RBs are evenly assigned to the source nodes.\nFor each scheme, we estimate inference latency by con- sidering two main components: the transmission and the processing latency. Notably, the processing latency is com- puted by profiling the execution time of the processing modules foreseen by each scheme and using a virtual ma- chine equipped with a single NVIDIA Tesla V100 GPU for the central controller, and a virtual machine equipped with an NVIDIA GeForce GTX 1070 Ti GPU for the generic"}, {"title": "Robustness to unreliable communication links.", "content": "We are also interested in assessing the robustness of the different inference schemes in the presence of unreliable communica- tion links between source nodes and edge servers. The results of the previous experiment can be interpreted considering a reduced number of source nodes N to mimic faults in the communication links between the central controller and a subset of source nodes. Looking at how the accuracy is affected by the partial availability of views, SCI-CH and SEI-CH emerge as the most robust schemes, both boasting values of standard deviation for the accuracy below 2.5% (Table 4).\nLessons learnt on robustness to link failures. The ac- curacy of all schemes seems to be affected to some degree by the reduced availability of sources nodes. However, selective schemes based on color histograms are to be preferred to embedding-based ones when the reliability of the commu- nication links is a concern. In these schemes in fact, before transmitting their views, all nodes will communicate in order to establish the current context. This context is then used to collectively decide which views should participate in the inference. Assuming that failures do not occur between this context-sharing phase and the actual transmission of views, any offline node would not contribute to the current context, thus enabling the rest of the nodes to collectively select the more relevant set of views based solely on their present availability."}, {"title": "Sensitivity to noise.", "content": "To assess the sensitivity of different inference schemes to noise, we performed another exper- iment in which we applied Gaussian RGB noise to each sample in the dataset and then tested how the accuracy of the different schemes was affected. Specifically, we applied a random additive noise with zero-mean and different values of standard deviation o to each channel independently. By varying \u03c3, we generated multiple perturbed datasets with"}, {"title": "Overall quantitative comparison.", "content": "At last, we present the qualitative comparison of the considered schemes in"}, {"title": "6. Discussion and Conclusions", "content": "The experimental results presented above clearly indi- cate that the inference schemes we analysed can achieve different trade-offs between accuracy, communication over- head and latency by leveraging different splits of computa- tion between edge and end devices. In this section we provide an additional qualitative comparison between the aforemen- tioned schemes in terms of computational requirements and privacy preservation. Furthermore, we discuss some relevant applications that can benefit from the proposed schemes (Sec. 6.1) and provide some future research directions (Sec. 6.2)."}, {"title": "Computational requirements.", "content": "Considering the MVCNN based on VGG-16 used in the experiments, the most compu- tationally intensive task is feature extraction, with the con- volutional network accounting for 30.7 GFLOPs per view. In contrast, the view pooling layer and the classification portion of the network account for 0.3 and 239.4 MFLOPs per inference, respectively. The CI scheme clearly minimizes the computational burden allocated to the source nodes, while SCI-CH aims at leveraging the advantages of view selection while keeping the computational requirements at the source nodes low. Conversely, selective, embedding-based schemes and ensemble schemes require the source nodes to perform feature extraction and are thus suited only for scenarios in which sensing devices are equipped with sufficient process- ing capabilities (e.g., connected vehicles). Importantly, the"}, {"title": "Privacy preservation.", "content": "As the data captured by source nodes may contain private information, some scenarios may be subject to privacy preservation requirements. Clearly, the CI and SCI-CH schemes, in which the raw input data is shared between source nodes and the central controller, are intrinsically unable to preserve privacy. On the contrary, the non-selective ensemble inference scheme, in which nodes exchange only predictions, is the most robust to privacy leakage. Schemes in which intermediate representation of views are shared between the nodes, being those embeddings or color-histograms, enhance the level of privacy but are only partially privacy-preserving, as they are not immune to leakage of private information [34]."}, {"title": "6.1. Application scenarios", "content": "Some relevant applications that can greatly benefit from the proposed collaborative inference schemes are discussed below. The main strength of centralized inference is the superior prediction accuracy it can provide. The completely centralized CI scheme is thus best suited for application scenarios characterized by devices with scarce computing capabilities, ample communication bandwidth, and no pri- vacy concerns, such as Quality Control in smart manufactur- ing. In this context, indeed, resource-constrained IoT devices are used to analyze images of food products or drugs pack- aging to detect defects, inconsistencies, and quality issues. Conversely, mission and safety critical applications, like process automation in smart factories or pedestrian detection in connected autonomous vehicles, are often characterized"}, {"title": "6.2. Challenges and future directions", "content": "We advocate that collaborative inference schemes, in which nodes share semantically correlated data and com- putation to make a prediction, can allow an increasingly larger set of fine-grained inference tasks to be pushed from the cloud domain to the network edge. However, a number of challenges should be addressed in order to make these schemes easier to apply in real-world scenarios.\nNode selection. We considered a scenario in which a set of end devices capture data that are spatially correlated. This easily holds in contexts with static end nodes whose cameras are deployed at fixed locations and orientations, in which case the spatial correlation between sensed data can be determined a priori. In reality, we often deal with hybrid systems comprising both static and mobile nodes, in which the correlation between the sensed data often arises in real time and most likely involves only a subset of the system nodes at a time. In these cases, how to select the nodes that should contribute to a collaborative multi-view classification task is still an open issue.\nAdapt to dynamic network conditions. As detailed pre- viously, different collaborative inference schemes, with dif- ferent parameters, may lead to more or less favorable trade- offs between accuracy, communication overhead and la- tency, depending on the condition of the network at a given"}, {"title": "7. Acknowledgements", "content": "This work was supported by the European Commis- sion under Grant Agreement No. 101095363 (ADROIT6G project) and Grant Agreement No. 101139266 (6G-INTENSE project)."}]}