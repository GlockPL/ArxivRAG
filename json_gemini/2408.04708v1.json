{"title": "MulliVC: Multi-lingual Voice Conversion With Cycle Consistency", "authors": ["Jiawei Huang", "Chen Zhang", "Yi Ren", "Ziyue Jiang", "Zhenhui Ye", "Jinglin Liu", "Jinzheng He", "Xiang Yin", "Zhou Zhao"], "abstract": "Voice conversion aims to modify the source speaker's voice to re-semble the target speaker while preserving the original speech content. Despite notable advancements in voice conversion these days, multi-lingual voice conversion (including both monolingual and cross-lingual scenarios) has yet to be extensively studied. It faces two main challenges: 1) the considerable variability in prosody and articulation habits across languages; and 2) the rarity of paired multi-lingual datasets from the same speaker. In this paper, we propose MulliVC, a novel voice conversion system that only converts timbre and keeps original content and source language prosody without multi-lingual paired data. Specifically, each training step of MulliVC contains three substeps: In step one the model is trained with monolingual speech data; then, steps two and three take in-spiration from back translation, construct a cyclical process to disentangle the timbre and other information (content, prosody, and other language-related information) in the absence of multi-lingual data from the same speaker. Both objective and subjective results indicate that MulliVC significantly surpasses other methods in both monolingual and cross-lingual contexts, demonstrating the system's efficacy and the viability of the three-step approach with cycle consistency. Audio samples can be found on our demo page (mullivc.github.io).", "sections": [{"title": "1 INTRODUCTION", "content": "These days, voice conversion (VC) has seen significant advance-ments owing to the emergence of various pretrained speech rep-resentations and major progress in speech synthesis models. VC can be broadly classified into parallel and non-parallel systems [11] according to the type of training data. Considering the difficulty of getting parallel speech data [1] (source speaker and target speaker record the same speech content), non-parallel techniques are main-stream for voice conversion. Non-parallel techniques mainly focus on disentangling the content and speaker information from the source speech data and then reconstructing the speech [8, 13, 23, 39, 49] using target speaker information. Some earlier work [35, 48, 49] typically used pre-trained automatic speech recognition (ASR) models to extract Phoneme posterior-gram (PPG) as con-tent features. Due to the emergence of large-scale pre-trained self-supervised learning (SSL) models such as Hubert [16], WavLM [5] and Wav2Vec [2], recent voice conversion models [8, 23, 39] tend to use SSL to extract content features and use speaker ID or speaker encoder to encode speaker information. However, the content fea-tures extracted by SSL models still contain the prosody and timbre information, leading to inadequate voice conversion[7, 23, 30].\nTo bridge the gap between different languages, multi/cross-lingual voice conversion [48, 50] has been developed, which is a special case in nonparallel systems and is more challenging. Given the high costs associated with gathering bilingual speaker datasets, current methods collect monolingual speaker datasets of different languages to train the models. The content and speaker information are sourced from the same language throughout the training phase; however, during inference, the disparity emerges as content and speaker information come from different languages. The inherent prosodic and pronunciation differences among various languages place traditional multi/cross-lingual VC methods into an out-of-domain context, resulting in generated speech with compromised intelligibility and speaker similarity.\nIn order to bolster the lingual generalization and the disentangle performance of the VC models, we propose MulliVC, a novel multi-lingual VC system that leverages cycle training strategy. Specifically, we divide each training iteration into three substeps. step 1 is the same as a traditional VC training step, we synthesize the speech by using the content and timbre information from the same speaker. In step 2, we use the speech of two speakers who speak different languages as content and timbre inputs, constraining the outputs using timbre loss and asr loss. During step 3, we reconstruct the speech, preserving the timbre identified in step 2, by combining it with content from a different sentence by the same speaker, thus constituting a cross-lingual voice conversion cycle. Though we have no multi-lingual paired data, by strategically designing the information flow within the cycle comprising step 2 and step 3, we compel the model to exclusively learn timbre information from the timbre input while disregarding any extraneous information present in that input, which encourages the disentanglement of timbre and other information. Furthermore, to improve the model's effectiveness in extracting timbre information, we introduce the fine-grained timbre conformer, designed to aid the model in cap-turing subtle aspects of timbre. The experimental results denote that MulliVC outperforms baseline models in terms of both objec-tive and subjective metrics and achieves substantial gains across monolingual and cross-lingual voice conversion scenarios."}, {"title": "2 RELATED WORKS", "content": "2.1 Cross-lingual Voice Conversion\nCross-lingual voice conversion aims to modify a source monolin-gual speaker's identity towards a target speaker who speaks another language while preserving the source linguistic content. It is more challenging than conventional monolingual voice conversion. [50] proposes to use a bilingual Phonetic PosteriorGram for the content representation of speech, together with an averaging model [37] de-signed to synthesize the average speech that embodies the speakers present in the dataset. The averaging model serves as a genera-tive model which is paired with an i-vector [37] seating adaptation step computed using a speaker verification formula [9], to synthe-size the target speaker's speech to achieve Cross-Language Voice Conversion (XVC). [48] proposes to use a jointly trained speaker en-coder instead of i-vector for better XVC results. FastSpeech-VC [46] pointed out that there are significant mismatches between phonetic sets and speech prosody of different languages, and PPG alone does not preserve rhythms well, for which they introduced normalized logarithm-scale fundamental frequency (Log-F0) to compensate for the prosodic mismatches. CyclePPG-XVC [49] points out that the loss of spectral reconstruction optimized to match the identity of the target speaker causes the transformation model to capture the articulation of the target speech from a different language and the native pronunciation or articulation of the source speech cannot be preserved, making the intelligibility of the converted speech worse. For this reason, they introduced a cyclic loss on the PPG features to force the converted speech to carry the same linguistic content as the natural input speech. ConsistencyVC [13] argues that some previous VC work used pre-trained speaker encoders in speaker classification tasks to obtain speaker embeddings, which are then used to guide speech synthesis. The main goal of the pre-trained speaker encoders is not speech synthesis, but speaker recognition. Therefore, this approach may miss emotional information in the reference speech. They use a jointly trained speaker encoder and after certain steps, this jointly trained speaker encoder is used to compute speaker consistency loss for improving speaker similarity and emotion similarity.\n2.2 Multi-lingual TTS\nMulti-lingual Text-To-Speech(TTS) is to synthesize speech in mul-tiple languages, where the speaker's language can be the same or different from the target language. [25] uses a Tacotron [41] syn-thesizer with shared phonemes for inputs and a speaker encoder module to achieve multilingual TTS. They introduce tone/stress embeddings to represent tone and stress information for speech generation with native accents. To improve the speaker similarity between the synthesized speech and the recordings of the native speaker, [15] introduces multi-task learning and speaker classifier joint training, they additionally add the speaker classification Cross Entropy loss and cross-lingual loss to the original loss. [22] ar-gues that the L2 (second-language) accents problem often occurs in cross-linguistic TTS and uses vowel space analysis, to study the L2 accents problem. They point out that the L2 accents of the parallel architecture (Glow-TTS) [20] are less than the L2 accents of the autoregressive architecture (Tacotron). [4] explores cross-lingual TTS in data-sufficient and low-resource scenarios. They propose that models that work well in data-sufficient scenarios do not per-form well in low-resource scenarios for cross-language TTS. For this reason, they synthesize a pipeline that consists of a bilingual TTS system, a bottleneck feature extractor, a speaker embedding extractor, a multi-speaker voice conversion system, and a vocoder to achieve cross-language TTS. VALL-E X [45] uses a rule-based converter to convert the transcriptions to phoneme sequences and uses a neural codec encoder [10] to convert the speech into acoustic tokens. Then they concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual con-ditional language model with a language ID module to alleviate accent problems. The generated acoustic tokens will be sent to the codec decoder to generate speech.\n2.3 Cycle/Back-Translation\nBack-translation [14, 34] technique was first introduced in ma-chine translation. It brings about the bridge between source and target languages by using a backward model that translates data from target to source. The (source and target) monolingual data is translated back and forth iteratively to progress the machine translation model in both directions. It is particularly effective in the case of missing data for parallel bilingual data. After that, some researchers introduced back-translation into the field of speech. In a low-resource scenario lacking text-to-speech alignment data, they use ASR models to generate pseudo-labels for speech. Then they use TTS models to regenerate speech with the transformed pseudo-labels, and the two models were jointly trained to achieve the training of an unsupervised TTS model [24, 29, 33]. In the field"}, {"title": "3 METHODS", "content": "In this section, we will first describe the training pipeline overview of MulliVC. Next, we provide the cycle training strategy of Mul-liVC, which aims to improve the cross-lingual performance and timbre disentanglement of the model. Finally, we present the model architecture of MulliVC.\n3.1 Pipeline Overview\nObtaining data for the same speaker speaking multiple languages is a expensive and difficult task. Consequently, existing XVC meth-ods often rely on combining monolingual speaker data to create a multilingual dataset. These methods aim to disentangle content and timbre information from speech and reconstruct speech using these two components. However, since the timbre and content infor-mation used during training belong to the same speaker's speech, existing models struggle to generate speech where the content in-formation is from language A and the timbre information is from language B. As a result, suboptimal results are obtained. To address this limitation and fully leverage the potential of multilingual data, we propose a cycle training strategy.\nSuppose we possess a large multilingual corpus consisting of two languages, namely language A and language B, with numerous speakers fluent in both languages. We randomly select two speakers for this illustration: Speaker 1, who speaks language A, and Speaker 2, who speaks language B. We represent the speech spoken by Speaker 1 in language A as SPK_1|LAN_A|#1, where speech#1 and speech#2 refer to two distinct utterances.\nAs depicted in Figure 1, each training step of our proposed Mul-liVC model consists of three substeps, and the losses from all three steps are summed up to perform a single model update. The training process of step 1 is similar to the previous works [8, 13, 23] in voice conversion, where the model takes the speech of the same person (take speaker 1 as an example in the figure) as both content input and timbre input to reconstruct the voice used as the content input, preserving the ability to transfer the timbre information when both inputs are in the same language. The loss of step 1 can be expressed as:\n$L = \\lambda_1 L_{Adv} + \\lambda_2 L_{Rec} + \\lambda_3 L_{Timbre} + \\lambda_4 L_{Pitch}$  (1)\n$L_{Timbre} = 1 - \\frac{f_r(m_t) \\cdot f_r(\\hat{m_t})}{||f_r(m_t)|| \\cdot ||f_r(\\hat{m_t})||}$  (2)\nWhere $\\lambda$ are weighting parameters, we set $\\lambda_1 = 0.05, \\lambda_2 = 1, \\lambda_3 = 0.1, \\lambda_4 = 1$ in our experiment. $L_{Rec} = ||m_t - \\hat{m_t}||_2$ is the reconstruction loss, $|| \\cdot ||_2$ is the L2 norm distance. $L_{Adv}$ is the LSGAN-styled adversarial loss [26] whose objective is to minimize the distribu-tion distance between the generated mel-spectrograms $\\hat{m_t}$ and the"}, {"title": "3.2 Cycle Consistency", "content": "Due to the unavailability of speech data from the same speaker speaking two different languages, we addressed this limitation by simulating this scenario in step 3 through a cyclical approach encompassing steps 2 and 3.\nIn step 2, we employ speech#3 of speaker 2 who speaks language B as content input and take speech#2 of speaker 1 from language A as timbre input, and assume the model can generate a speech SPK_1|LAN_B|#3 which means the content is the same as speech#3 but with the timbre of speaker 1. The loss of step 2 is\n$L = \\lambda_1 L_{Adv} + \\lambda_3 L_{Timbre} + \\lambda_5 L_{ASR}$ (3)\nWhere $L_{ASR} = ||f_A(m_t) - f_A(\\hat{m_t})||_2$ is the ASR perceptual loss. $\\lambda_5 = 0.5$, $f_A$ is a pre-trained automatic speech recognition model, we use the last layer embedding of $f_A$ to calculate the perceptual loss. Considering there is no ground truth data of SPK_1|LAN_B|#3, ASR perceptual loss is necessary to align the content information. Furthermore, the timbre loss ensures that the generated speech matches the timbre of speaker 1.\nIn step 3, we utilize the output SPK_1|LAN_B|#3 obtained in step 2 as the timbre input. As for the content input, we use speech#4 of speaker 1 speaking language A. Since both speeches possess the timbre of speaker 1, we simulated the same speaker speaking two different languages. Consequently, another cross-lingual voice conversion can be performed. Moreover, we have the ground-truth data SPK_1|LAN_B|#4 available during this step, which enables us to calculate the pitch perceptual loss and reconstruction loss. These losses ensure the model's output aligns with the ground-truth data distribution. The loss calculation in step 3 follows the same methodology as step 1.\nBy incorporating step 1 and the cross-lingual voice conversion cycle of step 2 and step 3, we guarantee the model's ability to convert voices within the same language while also enhancing the performance of cross-lingual voice conversion.\nAdditionally, the previous voice conversion scheme solely con-sisted of step 1, where the content features generated by SSL still contained certain timbre information. During audio reconstruction, the model unavoidably reads timbre information from the content features, resulting in insufficient disentanglement of timbre and content and sub-optimal generalization capabilities. Conversely, in our training strategy, step 2 ensures that the content and timbre inputs originate from different speakers, compelling the model to exclusively extract timbre information from the timbre inputs. This approach enhances the model's ability to disentangle timbre and"}, {"title": "3.3 Model Architecture", "content": "As illustrated in Figure 2. Denote $Y = \\{y_1, ..., y_n\\}$ as the speech corpus for a certain speaker. In training, $Y$ is partitioned as target audio $y_t$ and reference audio $\\tilde{y}$. $y_t$ is fed into the pre-trained content encoder $E_c$ to get the content feature $z_c \\in R^{T \\times C}$, where $T$ is the length of time-axis and $C$ is channels. We adopt ContentVec[30] which aims to disentangle speaker information from audio and only encode content information as our content encoder.\\tilde{y} is fed into the global timbre encoder $E_s$ to encode the global timbre feature $S \\in R^{1 \\times C}$. Inspired by MegaTTS2[18] and CDFSE[47] that fine-grained timbre information can represent the speakers' speaking habits and better help the model imitate the timbre of the refer-ence audio, we design a Fine-grained Timbre Conformer [12] to capture fine-grained timbre information. As illustrated in Figure 3, the global timbre feature $S$ is first repeated along the time axis to $z_s \\in R^{T \\times C}$ and concatenated with $z_c$ in the channel axis to get fea-ture $z_u \\in R^{T \\times 2C}$. The reference mel-spectrogram $m \\in R^{T' \\times D}$ where D denotes the number of mel bins is firstly compressed into acoustic hidden states by a factor of $d$ in length then projected by a linear layer to $m_c \\in R^{\\frac{T'}{d} \\times 2C}$ and concatenate with $z_u$ along time axis sending to Conformer. In conformer, fine-grained timbre informa-tion will be merged with $z_u$ by Convolution and Self-Attention[40] mechanism."}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Setup\n4.1.1 Dataset.\nWe use several datasets to train our model. We use Libritts [44] for the English speech corpus. And we use aidatatang_200zh\u00b9, MAGIC-DATA2 and ST-CMDS3 Chinese Mandarin speech corpus. We use VCTK[43], Aishell-1[3] and EMIME[42] datasets to evaluate our"}, {"title": "4.3 Method Analyses", "content": "4.3.1 Validating The Effectiveness Of SV Model for Cross-linguistic Scenario.\nThe SV model WavLM-TCDNN is trained using a multilingual dataset composed of monolingual speakers. Consider that we need to test SIM scores with the it to evaluate the VC model's abil-ity to convert voice across languages. It is essential to determine whether the SV model can correctly identify that two speeches in different languages come from one speaker. We experiment on the EMIME[42] dataset which contains bilingual audio recordings by the same speakers. We sample 5 female speakers and 5 male"}, {"title": "4.3.2 Speaker Clustering Comparison.", "content": "To further investigate the speaker embedding space of the WavLM-TDCNN models and explore the cross-lingual voice conversion performance of each model, we conducted an experiment on the EMIME dataset. We reconstructed language A/B by utilizing speech from language A/B as the content input and the same speaker's speech from language B/A as the timbre input. Utilizing WavLM-TDCNN, we obtained the reconstructed voice speaker embedding, which was then subjected to clustering using t-SNE [38]. The clus-tering results, visualized in Figure 5, indicate that there is a minor difference between the distributions of speaker embeddings derived from speech in different languages spoken by the same individual. However, this difference is significantly smaller compared to the disparity observed between speaker embeddings from different speakers. This finding further validates Section 4.2's assertion re-garding the applicability of WavLM-TDCNN in evaluating voice conversion within cross-language scenarios. Moreover, when com-paring the distributions of speaker embeddings among different speakers, MulliVC exhibits more distinct and tightly grouped clus-ters, suggesting superior performance in timbre disentangling and voice conversion than the other baselines."}, {"title": "4.4 Ablation Study", "content": "4.4.1 Cross-lingual Steps.\nTo verify the effectiveness of the cross-lingual steps (step 2 and step 3), we evaluate the performance of the voice conversion models without them and list the results in Setting #2 and Setting #3 of Table 4. Compare Setting #2 with Setting #3 of Table 4, speaker similarity decreases significantly after removing step 2. It is worth noting that after adding the cross-lingual voice conversion step 2, the speaker similarity of the VCTK dataset with timbre migration within the same language is also highly improved. This is because the embedding encoded by the content encoder holds part of the timbre information[7, 30] when there is only intra-language timbre migration, the model tends to partly use the timbre information from the content encoder, resulting in insufficient timbre disen-tanglement. The cross-language timbre migration scenario of step 2 forces the model to encode timbre only from the timbre input, which improves the timbre disentanglement ability of the model. On the other hand, adding step 2 leads to a rise in WER and CER, further addition of step 3 makes WER and CER similar to that of only step 1, which shows that ASR loss is not enough to align the content, the reconstruction loss in step 3 is important.\n4.4.2 ASR Perceptual Loss.\nWe evaluated the performance of the model when removing asr perceptual loss from step 2, and removing asr perceptual loss leads to an increase in WER and CER for all three scenarios. However, adding asr perceptual loss at the same time leads to a decrease in the speaker similarity, because although the asr model's training target is CTC loss, even though the last layer of the output embedding still inevitably encodes some of the timbre information. When we optimize the pairing of SPK_1|LAN_B|#3 and SPK_2|LAN_B|#3 in step 2 will negatively affect the timbre disentanglement.\n4.4.3 Fine-grained Timbre Conformer.\nRemoving the Fine-grained timbre conformer leads to a double decrease in the intelligibility of the generated speech and speaker similarity. fine-grained timbre conformer facilitates the interaction between fine-grained timbre and content information, with positive effects on both WER and SIM."}, {"title": "5 CONCLUSION", "content": "This research paper presents MulliVC, a multi-lingual VC system designed for high-fidelity timbre migration and mel-spectrogram generation. The proposed three-step training architecture enhances the model's performance in speaker adaptation, both within and across languages. And the Fine-grained timbre conformer com-ponent improves the speaker similarity and intelligibility of the generated speech. The experimental results demonstrate that our model surpasses the state-of-the-art in both intra-language and cross-lingual zero-shot voice conversion scenarios.\nHowever, despite the considerable improvement in speaker adap-tation achieved by our method, several aspects still require further enhancement, particularly in zero-shot scenarios. Firstly, the cur-rent training dataset utilized in our experiments remains relatively small. Consequently, it may not be adequate for tasks such as movie dubbing, where expressive voices and highly diverse timbre charac-teristics are prevalent. Additionally, the employed content encoder retains some prosody and timbre information, which hinders the effective separation of timbre from content. Moreover, the compu-tation of timbre loss relies on a pre-trained speaker verification (SV) model, which could benefit from larger datasets encompass-ing more languages to enhance its accuracy. This, in turn, would contribute to better speaker adaptation."}, {"title": "A.1 Speaker Verification Model", "content": "In this section, we introduce our pre-trained speaker verification model which takes mel-spectrogram as input to calculate timbre loss. The model's architecture is the same as the speaker encoder described in section 4.1, with a linear layer in the last to project the embedding from 512 to 256. The model is trained by distillation, with WavLM-TDCNN as the teacher model and the MSE loss between WavLM-TDCNN's output and our model's output. The model is trained by 240K timesteps on our train set, with batchsize=48."}, {"title": "A.2 ASR model", "content": "Here we introduce our pretrained ASR model to calculate ASR loss. Our asr model takes the mel-spectrogram as input and predicts the phoneme corresponding to each of the 4 melbins. The phoneme is obtained and aligned with speech by external alignment tools MFA [27]. The model was trained using CTC loss with 160K time steps on the training set and batch size=48. The architecture of ASR model is displayed in Figure 6"}, {"title": "A.3 Pitch Predictor", "content": "We use REAPER [36] to extract F0(pitch) from raw audio, and in-terpolate the F0's length with mel-spectrogram. We train the pitch predictor to predict F0 from mel-spectrogram and calculate MSE loss with the extracted F0. We adopt the pitch predictor architecture from FastSpeech2 [32] as the architecture of our pitch predictor."}]}