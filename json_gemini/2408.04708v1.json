{"title": "MulliVC: Multi-lingual Voice Conversion With Cycle Consistency", "authors": ["Jiawei Huang", "Chen Zhang", "Yi Ren", "Ziyue Jiang", "Zhenhui Ye", "Jinglin Liu", "Jinzheng He", "Xiang Yin", "Zhou Zhao"], "abstract": "Voice conversion aims to modify the source speaker's voice to resemble the target speaker while preserving the original speech content. Despite notable advancements in voice conversion these days, multi-lingual voice conversion (including both monolingual and cross-lingual scenarios) has yet to be extensively studied. It faces two main challenges: 1) the considerable variability in prosody and articulation habits across languages; and 2) the rarity of paired multi-lingual datasets from the same speaker. In this paper, we propose MulliVC, a novel voice conversion system that only converts timbre and keeps original content and source language prosody without multi-lingual paired data. Specifically, each training step of MulliVC contains three substeps: In step one the model is trained with monolingual speech data; then, steps two and three take in-spiration from back translation, construct a cyclical process to disentangle the timbre and other information (content, prosody, and other language-related information) in the absence of multi-lingual data from the same speaker. Both objective and subjective results indicate that MulliVC significantly surpasses other methods in both monolingual and cross-lingual contexts, demonstrating the system's efficacy and the viability of the three-step approach with cycle consistency. Audio samples can be found on our demo page (mullivc.github.io).", "sections": [{"title": "1 INTRODUCTION", "content": "These days, voice conversion (VC) has seen significant advancements owing to the emergence of various pretrained speech representations and major progress in speech synthesis models. VC can be broadly classified into parallel and non-parallel systems [11] according to the type of training data. Considering the difficulty of getting parallel speech data [1] (source speaker and target speaker record the same speech content), non-parallel techniques are mainstream for voice conversion. Non-parallel techniques mainly focus on disentangling the content and speaker information from the source speech data and then reconstructing the speech [8, 13, 23, 39, 49] using target speaker information. Some earlier work [35, 48, 49] typically used pre-trained automatic speech recognition (ASR) models to extract Phoneme posterior-gram (PPG) as content features. Due to the emergence of large-scale pre-trained self-supervised learning (SSL) models such as Hubert [16], WavLM [5] and Wav2Vec [2], recent voice conversion models [8, 23, 39] tend to use SSL to extract content features and use speaker ID or speaker encoder to encode speaker information. However, the content features extracted by SSL models still contain the prosody and timbre information, leading to inadequate voice conversion[7, 23, 30].\nTo bridge the gap between different languages, multi/cross-lingual voice conversion [48, 50] has been developed, which is a special case in nonparallel systems and is more challenging. Given the high costs associated with gathering bilingual speaker datasets, current methods collect monolingual speaker datasets of different languages to train the models. The content and speaker information are sourced from the same language throughout the training phase; however, during inference, the disparity emerges as content and speaker information come from different languages. The inherent prosodic and pronunciation differences among various languages place traditional multi/cross-lingual VC methods into an out-of-domain context, resulting in generated speech with compromised intelligibility and speaker similarity.\nIn order to bolster the lingual generalization and the disentangle performance of the VC models, we propose MulliVC, a novel multi-lingual VC system that leverages cycle training strategy. Specifically, we divide each training iteration into three substeps. step 1 is the same as a traditional VC training step, we synthesize the speech by using the content and timbre information from the same speaker. In step 2, we use the speech of two speakers who speak different languages as content and timbre inputs, constraining the outputs using timbre loss and asr loss. During step 3, we reconstruct the speech, preserving the timbre identified in step 2, by combining it with content from a different sentence by the same speaker, thus constituting a cross-lingual voice conversion cycle. Though we have no multi-lingual paired data, by strategically designing the information flow within the cycle comprising step 2 and step 3, we compel the model to exclusively learn timbre information from the timbre input while disregarding any extraneous information present in that input, which encourages the disentanglement of timbre and other information. Furthermore, to improve the model's effectiveness in extracting timbre information, we introduce the fine-grained timbre conformer, designed to aid the model in capturing subtle aspects of timbre. The experimental results denote that MulliVC outperforms baseline models in terms of both objective and subjective metrics and achieves substantial gains across monolingual and cross-lingual voice conversion scenarios."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 Cross-lingual Voice Conversion", "content": "Cross-lingual voice conversion aims to modify a source monolingual speaker's identity towards a target speaker who speaks another language while preserving the source linguistic content. It is more challenging than conventional monolingual voice conversion. [50] proposes to use a bilingual Phonetic PosteriorGram for the content representation of speech, together with an averaging model [37] designed to synthesize the average speech that embodies the speakers present in the dataset. The averaging model serves as a generative model which is paired with an i-vector [37] seating adaptation step computed using a speaker verification formula [9], to synthesize the target speaker's speech to achieve Cross-Language Voice Conversion (XVC). [48] proposes to use a jointly trained speaker encoder instead of i-vector for better XVC results. FastSpeech-VC [46] pointed out that there are significant mismatches between phonetic sets and speech prosody of different languages, and PPG alone does not preserve rhythms well, for which they introduced normalized logarithm-scale fundamental frequency (Log-F0) to compensate for the prosodic mismatches. CyclePPG-XVC [49] points out that the loss of spectral reconstruction optimized to match the identity of the target speaker causes the transformation model to capture the articulation of the target speech from a different language and the native pronunciation or articulation of the source speech cannot be preserved, making the intelligibility of the converted speech worse. For this reason, they introduced a cyclic loss on the PPG features to force the converted speech to carry the same linguistic content as the natural input speech. ConsistencyVC [13] argues that some previous VC work used pre-trained speaker encoders in speaker classification tasks to obtain speaker embeddings, which are then used to guide speech synthesis. The main goal of the pre-trained speaker encoders is not speech synthesis, but speaker recognition. Therefore, this approach may miss emotional information in the reference speech. They use a jointly trained speaker encoder and after certain steps, this jointly trained speaker encoder is used to compute speaker consistency loss for improving speaker similarity and emotion similarity."}, {"title": "2.2 Multi-lingual TTS", "content": "Multi-lingual Text-To-Speech(TTS) is to synthesize speech in multiple languages, where the speaker's language can be the same or different from the target language. [25] uses a Tacotron [41] synthesizer with shared phonemes for inputs and a speaker encoder module to achieve multilingual TTS. They introduce tone/stress embeddings to represent tone and stress information for speech generation with native accents. To improve the speaker similarity between the synthesized speech and the recordings of the native speaker, [15] introduces multi-task learning and speaker classifier joint training, they additionally add the speaker classification Cross Entropy loss and cross-lingual loss to the original loss. [22] argues that the L2 (second-language) accents problem often occurs in cross-linguistic TTS and uses vowel space analysis, to study the L2 accents problem. They point out that the L2 accents of the parallel architecture (Glow-TTS) [20] are less than the L2 accents of the autoregressive architecture (Tacotron). [4] explores cross-lingual TTS in data-sufficient and low-resource scenarios. They propose that models that work well in data-sufficient scenarios do not perform well in low-resource scenarios for cross-language TTS. For this reason, they synthesize a pipeline that consists of a bilingual TTS system, a bottleneck feature extractor, a speaker embedding extractor, a multi-speaker voice conversion system, and a vocoder to achieve cross-language TTS. VALL-E X [45] uses a rule-based converter to convert the transcriptions to phoneme sequences and uses a neural codec encoder [10] to convert the speech into acoustic tokens. Then they concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model with a language ID module to alleviate accent problems. The generated acoustic tokens will be sent to the codec decoder to generate speech."}, {"title": "2.3 Cycle/Back-Translation", "content": "Back-translation [14, 34] technique was first introduced in machine translation. It brings about the bridge between source and target languages by using a backward model that translates data from target to source. The (source and target) monolingual data is translated back and forth iteratively to progress the machine translation model in both directions. It is particularly effective in the case of missing data for parallel bilingual data. After that, some researchers introduced back-translation into the field of speech. In a low-resource scenario lacking text-to-speech alignment data, they use ASR models to generate pseudo-labels for speech. Then they use TTS models to regenerate speech with the transformed pseudo-labels, and the two models were jointly trained to achieve the training of an unsupervised TTS model [24, 29, 33]. In the field of voice conversion, there is also some research with ideas similar to back-translation. CycleGAN-VC [19] achieves one-to-one voice conversion without parallel data by jointly training two GAN models, one responsible for converting the speech of A to the speech of B timbre and the other vice versa. However, our approach is different to CycleGAN-VC, we use one single model to perform voice conversion between the two languages. In addition, our \"back-translation\" process maintains the timbre input as the same speaker, rather than keeping the speech content unchanged. We will discuss the detials in section 3.1."}, {"title": "3 METHODS", "content": "In this section, we will first describe the training pipeline overview of MulliVC. Next, we provide the cycle training strategy of MulliVC, which aims to improve the cross-lingual performance and timbre disentanglement of the model. Finally, we present the model architecture of MulliVC."}, {"title": "3.1 Pipeline Overview", "content": "Obtaining data for the same speaker speaking multiple languages is a expensive and difficult task. Consequently, existing XVC methods often rely on combining monolingual speaker data to create a multilingual dataset. These methods aim to disentangle content and timbre information from speech and reconstruct speech using these two components. However, since the timbre and content information used during training belong to the same speaker's speech, existing models struggle to generate speech where the content information is from language A and the timbre information is from language B. As a result, suboptimal results are obtained. To address this limitation and fully leverage the potential of multilingual data, we propose a cycle training strategy.\nSuppose we possess a large multilingual corpus consisting of two languages, namely language A and language B, with numerous speakers fluent in both languages. We randomly select two speakers for this illustration: Speaker 1, who speaks language A, and Speaker 2, who speaks language B. We represent the speech spoken by Speaker 1 in language A as SPK_1|LAN_A|#1, where speech#1 and speech#2 refer to two distinct utterances.\nAs depicted in Figure 1, each training step of our proposed MulliVC model consists of three substeps, and the losses from all three steps are summed up to perform a single model update. The training process of step 1 is similar to the previous works [8, 13, 23] in voice conversion, where the model takes the speech of the same person (take speaker 1 as an example in the figure) as both content input and timbre input to reconstruct the voice used as the content input, preserving the ability to transfer the timbre information when both inputs are in the same language. The loss of step 1 can be expressed as:\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{Adv} + \\lambda_2 \\mathcal{L}_{Rec} + \\lambda_3 \\mathcal{L}_{Timbre} + \\lambda_4 \\mathcal{L}_{Picth}$ (1)\n$\\mathcal{L}_{Timbre} = 1 - \\frac{\\langle f_T(m_t), f_T(\\hat{m}_t) \\rangle}{||f_T(m_t)|| \\cdot || f_T(\\hat{m}_t) ||}$ (2)\nWhere $\\lambda$... are weighting parameters, we set $\\lambda_1 = 0.05$, $\\lambda_2 = 1$, $\\lambda_3 = 0.1$, $\\lambda_4 = 1$ in our experiment. $\\mathcal{L}_{Rec} = ||m_t - \\hat{m}_t||_2$ is the reconstruction loss, $||\\cdot||_2$ is the L2 norm distance. $\\mathcal{L}_{Adv}$ is the LSGAN-styled adversarial loss [26] whose objective is to minimize the distribution distance between the generated mel-spectrograms $\\hat{m}_t$ and the"}, {"title": "3.2 Cycle Consistency", "content": "Due to the unavailability of speech data from the same speaker speaking two different languages, we addressed this limitation by simulating this scenario in step 3 through a cyclical approach encompassing steps 2 and 3.\nIn step 2, we employ speech#3 of speaker 2 who speaks language B as content input and take speech#2 of speaker 1 from language A as timbre input, and assume the model can generate a speech SPK_1|LAN_B|#3 which means the content is the same as speech#3 but with the timbre of speaker 1. The loss of step 2 is\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{Adv} + \\lambda_3 \\mathcal{L}_{Timbre} + \\lambda_5 \\mathcal{L}_{ASR}$ (3)\nWhere $\\mathcal{L}_{ASR} = ||f_A(m_t) - f_A(\\hat{m}_t)||_2$ is the ASR perceptual loss, $\\lambda_5 = 0.5$, $f_A$ is a pre-trained automatic speech recognition model, we use the last layer embedding of $f_A$ to calculate the perceptual loss. Considering there is no ground truth data of SPK_1|LAN_B|#3, ASR perceptual loss is necessary to align the content information. Furthermore, the timbre loss ensures that the generated speech matches the timbre of speaker 1.\nIn step 3, we utilize the output SPK_1|LAN_B|#3 obtained in step 2 as the timbre input. As for the content input, we use speech#4 of speaker 1 speaking language A. Since both speeches possess the timbre of speaker 1, we simulated the same speaker speaking two different languages. Consequently, another cross-lingual voice conversion can be performed. Moreover, we have the ground-truth data SPK_1|LAN_B|#4 available during this step, which enables us to calculate the pitch perceptual loss and reconstruction loss. These losses ensure the model's output aligns with the ground-truth data distribution. The loss calculation in step 3 follows the same methodology as step 1.\nBy incorporating step 1 and the cross-lingual voice conversion cycle of step 2 and step 3, we guarantee the model's ability to convert voices within the same language while also enhancing the performance of cross-lingual voice conversion.\nAdditionally, the previous voice conversion scheme solely consisted of step 1, where the content features generated by SSL still contained certain timbre information. During audio reconstruction, the model unavoidably reads timbre information from the content features, resulting in insufficient disentanglement of timbre and content and sub-optimal generalization capabilities. Conversely, in our training strategy, step 2 ensures that the content and timbre inputs originate from different speakers, compelling the model to exclusively extract timbre information from the timbre inputs. This approach enhances the model's ability to disentangle timbre and"}, {"title": "3.3 Model Architecture", "content": "As illustrated in Figure 2. Denote $\\textbf{Y} = {y_1, ..., y_n}$ as the speech corpus for a certain speaker. In training, $\\textbf{Y}$ is partitioned as target audio $y_t$ and reference audio $\\tilde{y}$. $y_t$ is fed into the pre-trained content encoder $E_c$ to get the content feature $z_c \\in R^{T \\times C}$, where T is the length of time-axis and C is channels. We adopt ContentVec[30] which aims to disentangle speaker information from audio and only encode content information as our content encoder.$\\\tilde{y}$ is fed into the global timbre encoder $E_s$ to encode the global timbre feature $S\\in R^{1\\times C}$. Inspired by MegaTTS2[18] and CDFSE[47] that fine-grained timbre information can represent the speakers' speaking habits and better help the model imitate the timbre of the reference audio, we design a Fine-grained Timbre Conformer [12] to capture fine-grained timbre information. As illustrated in Figure 3, the global timbre feature S is first repeated along the time axis to $z_s \\in R^{T \\times C}$ and concatenated with $z_c$ in the channel axis to get feature $z_u \\in R^{T \\times 2C}$. The reference mel-spectrogramm $\\in R^{T'\\times D}$ where D denotes the number of mel bins is firstly compressed into acoustic hidden states by a factor of $d$ in length then projected by a linear layer to $m_c \\in R^{\\frac{T'}{d} \\times 2C}$ and concatenate with $z_u$ along time axis sending to Conformer. In conformer, fine-grained timbre information will be merged with $z_u$ by Convolution and Self-Attention[40] mechanism."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Experimental Setup", "content": ""}, {"title": "4.1.1 Dataset.", "content": "We use several datasets to train our model. We use Libritts [44] for the English speech corpus. And we use aidatatang_200zh\u00b9, MAGIC-DATA2 and ST-CMDS3 Chinese Mandarin speech corpus. We use VCTK[43], Aishell-1[3] and EMIME[42] datasets to evaluate our"}, {"title": "4.1.2 Model Configuration.", "content": "MulliVC consists of a content encoder, a timbre encoder, a fine-grained timbre conformer, a mel decoder, and a Patch-GAN discriminator. The timbre encoder consists of 5 convolution layers with 512 hidden size and 5 kernel size. The Fine-grained timbre conformer consists of 6 conformer layers. The mel decoder consists of 5 convolutional blocks with 512 hidden size and 5 kernel size. In the training stage, we involve three pre-trained models to calculate auxiliary loss: a speaker verification model, an automatic speech recognition model, and a pitch predictor. These models are trained on the same dataset of MulliVC, please refer to Appendix A for the details of these models."}, {"title": "4.1.3 Training Details.", "content": "MulliVC is trained on 1 A100 GPU with a batch size of 8 speeches. Considering 1 training step is split into 3 substeps, the model takes 24 speeches as input for 1 step in total. We use the Adam optimizer with learning rate of $10^{-4}$, $\u03b2_1 = 0.9$, $\u03b2_2 = 0.999$, $\u03f5 = 10^{-9}$. We train MulliVC for 240K training steps. In training, language A and language B will be randomly switched. The predicted mel-spectrograms are transformed into audio samples using pre-trained HiFi-GAN V1 [21]."}, {"title": "4.1.4 Evaluation Metrics.", "content": "Following [8], we conduct the naturalness and similarity mean opinion score (nMOS and sMOS, respectively) for subjective evaluation, 16 subjects are employed to provide the subjective measures. we evaluate the word error rate (WER) of the English corpus, the character error rate (CER) of the Chinese corpus, and speaker similarity (SIM) for objective evaluation. We use whisper-large-v3[31] to transcribe the generated speech into text. Then, the WER/CER between the transcribed text and the original target text is measured. In terms of the cosine speaker similarity, we use the WavLM-TDCNN"}, {"title": "4.2 Main Results", "content": ""}, {"title": "4.2.1 Baseline Models.", "content": "We use pre-trained FreeVC5, ConsistencyVC6 and DiffHier-VC7 as our baseline models. The pre-trained FreeVC was trained on the VCTK dataset, which is not the zero-shot scenario. To train with the same settings as our model, we retrain FreeVC with our training dataset and denote the retrained model as FreeVC*. DiffHier-VC is trained on the LibriTTS dataset. ConsistencyVC is trained on a combination of English, Chinese, and Japanese datasets."}, {"title": "4.2.2 Zero-shot Voice Conversion Comparison.", "content": "We conducted subjective and objective evaluations for three zero-shot voice conversion (VC) scenarios. The first scenario involved voice conversion in English, using source and target speeches from the VCTK dataset. Additionally, we conducted two cross-lingual voice conversion experiments: VCTK-AIS1 and AIS1-VCTK. This implies using source speeches from VCTK and target speeches from Aishell-1 for the former experiment, and source speeches from Aishell-1 and target speeches from VCTK for the latter. For objective evaluation of each experiment, we randomly created 400 speaker pairs, and each pair was randomized to use 5 speeches, for a total of 2000 speech pairs to calculate WER and SIM. For human evaluation of each experiment, we randomly select 30 synthesized speech records to conduct nMOS and sMOS evaluation. The results are listed in Table 1.\nFor speech intelligibility, our method achieves lower WER compared with baseline models in VCTK and VCTK-AIS1. The CER metric of our method on the AIS1-VCTK dataset is comparable to ConsistencyVC. Also, we achieved the highest nMOS score on all datasets. For speaker similarity, the SIM score and sMOS score of our method are significantly improved compared to the baselines. It is worth noting that FreeVC is trained on the VCTK dataset, while our method surpasses FreeVC on the VCTK dataset, indicating that the zero-shot performance of our method outperforms the performance of FreeVC's seen speaker. In addition we observed that ConsistencyVC had lower SIM scores than FreeVC* and DiffHierVC under VCTK and AIS1-VCTK tests, but obtained higher sMOS scores. It suggests that WavLM-TDCNN pays attention to some details that are weaker in human perception compared to human rators. In addition, the clarity and naturalness of the audio also affect the sMOS scores compared to WavLM-TDCNN. Speeches with low intelligibility may also receive high SIM scores, as further confirmed by our research in the ablation study section.\nWe further compare our model's zero-shot voice conversion performance with baselines on the EMIME bilingual dataset, the results are displayed in Table 2. On the EMIME dataset, the results of our model have a significant advantage over the baselines. In"}, {"title": "4.3 Method Analyses", "content": ""}, {"title": "4.3.1 Validating The Effectiveness Of SV Model for Cross-linguistic Scenario.", "content": "The SV model WavLM-TCDNN is trained using a multilingual dataset composed of monolingual speakers. Consider that we need to test SIM scores with the it to evaluate the VC model's ability to convert voice across languages. It is essential to determine whether the SV model can correctly identify that two speeches in different languages come from one speaker. We experiment on the EMIME[42] dataset which contains bilingual audio recordings by the same speakers. We sample 5 female speakers and 5 male"}, {"title": "4.3.2 Speaker Clustering Comparison.", "content": "To further investigate the speaker embedding space of the WavLM-TDCNN models and explore the cross-lingual voice conversion performance of each model, we conducted an experiment on the EMIME dataset. We reconstructed language A/B by utilizing speech from language A/B as the content input and the same speaker's speech from language B/A as the timbre input. Utilizing WavLM-TDCNN, we obtained the reconstructed voice speaker embedding, which was then subjected to clustering using t-SNE [38]. The clustering results, visualized in Figure 5, indicate that there is a minor difference between the distributions of speaker embeddings derived from speech in different languages spoken by the same individual. However, this difference is significantly smaller compared to the disparity observed between speaker embeddings from different speakers. This finding further validates Section 4.2's assertion regarding the applicability of WavLM-TDCNN in evaluating voice conversion within cross-language scenarios. Moreover, when comparing the distributions of speaker embeddings among different speakers, MulliVC exhibits more distinct and tightly grouped clusters, suggesting superior performance in timbre disentangling and voice conversion than the other baselines."}, {"title": "4.4 Ablation Study", "content": ""}, {"title": "4.4.1 Cross-lingual Steps.", "content": "To verify the effectiveness of the cross-lingual steps (step 2 and step 3), we evaluate the performance of the voice conversion models without them and list the results in Setting #2 and Setting #3 of Table 4. Compare Setting #2 with Setting #3 of Table 4, speaker similarity decreases significantly after removing step 2. It is worth noting that after adding the cross-lingual voice conversion step 2, the speaker similarity of the VCTK dataset with timbre migration within the same language is also highly improved. This is because the embedding encoded by the content encoder holds part of the timbre information[7, 30] when there is only intra-language timbre migration, the model tends to partly use the timbre information from the content encoder, resulting in insufficient timbre disentanglement. The cross-language timbre migration scenario of step 2 forces the model to encode timbre only from the timbre input, which improves the timbre disentanglement ability of the model. On the other hand, adding step 2 leads to a rise in WER and CER, further addition of step 3 makes WER and CER similar to that of only step 1, which shows that ASR loss is not enough to align the content, the reconstruction loss in step 3 is important."}, {"title": "4.4.2 ASR Perceptual Loss.", "content": "We evaluated the performance of the model when removing asr perceptual loss from step 2, and removing asr perceptual loss leads to an increase in WER and CER for all three scenarios. However, adding asr perceptual loss at the same time leads to a decrease in the speaker similarity, because although the asr model's training target is CTC loss, even though the last layer of the output embedding still inevitably encodes some of the timbre information. When we optimize the pairing of SPK_1|LAN_B|#3 and SPK_2|LAN_B|#3 in step 2 will negatively affect the timbre disentanglement."}, {"title": "4.4.3 Fine-grained Timbre Conformer.", "content": "Removing the Fine-grained timbre conformer leads to a double decrease in the intelligibility of the generated speech and speaker similarity. fine-grained timbre conformer facilitates the interaction between fine-grained timbre and content information, with positive effects on both WER and SIM."}, {"title": "5 CONCLUSION", "content": "This research paper presents MulliVC, a multi-lingual VC system designed for high-fidelity timbre migration and mel-spectrogram generation. The proposed three-step training architecture enhances the model's performance in speaker adaptation, both within and across languages. And the Fine-grained timbre conformer component improves the speaker similarity and intelligibility of the generated speech. The experimental results demonstrate that our model surpasses the state-of-the-art in both intra-language and cross-lingual zero-shot voice conversion scenarios.\nHowever, despite the considerable improvement in speaker adaptation achieved by our method, several aspects still require further enhancement, particularly in zero-shot scenarios. Firstly, the current training dataset utilized in our experiments remains relatively small. Consequently, it may not be adequate for tasks such as movie dubbing, where expressive voices and highly diverse timbre characteristics are prevalent. Additionally, the employed content encoder retains some prosody and timbre information, which hinders the effective separation of timbre from content. Moreover, the computation of timbre loss relies on a pre-trained speaker verification (SV) model, which could benefit from larger datasets encompassing more languages to enhance its accuracy. This, in turn, would contribute to better speaker adaptation."}, {"title": "A DETAILED EXPERIMENTAL SETTINGS", "content": ""}, {"title": "A.1 Speaker Verification Model", "content": "In this section, we introduce our pre-trained speaker verification model which takes mel-spectrogram as input to calculate timbre loss. The model's architecture is the same as the speaker encoder described in section 4.1, with a linear layer in the last to project the embedding from 512 to 256. The model is trained by distillation, with WavLM-TDCNN as the teacher model and the MSE loss between WavLM-TDCNN's output and our model's output. The model is trained by 240K timesteps on our train set, with batchsize=48."}, {"title": "A.2 ASR model", "content": "Here we introduce our pretrained ASR model to calculate ASR loss. Our asr model takes the mel-spectrogram as input and predicts the phoneme corresponding to each of the 4 melbins. The phoneme is obtained and aligned with speech by external alignment tools MFA [27]. The model was trained using CTC loss with 160K time steps on the training set and batch size=48. The architecture of ASR model is displayed in Figure 6"}, {"title": "A.3 Pitch Predictor", "content": "We use REAPER [36] to extract F0(pitch) from raw audio, and interpolate the F0's length with mel-spectrogram. We train the pitch predictor to predict F0 from mel-spectrogram and calculate MSE loss with the extracted F0. We adopt the pitch predictor architecture from FastSpeech2 [32] as the architecture of our pitch predictor."}]}