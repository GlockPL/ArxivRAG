{"title": "FACTTEST: FACTUALITY TESTING IN LARGE LANGUAGE MODELS WITH FINITE-SAMPLE AND DISTRIBUTION-FREE GUARANTEES", "authors": ["Fan Nie", "Xiaotian Hou", "Shuhang Lin", "James Zou", "Huaxiu Yao", "Linjun Zhang"], "abstract": "The propensity of Large Language Models (LLMs) to generate hallucinations and non-factual content undermines their reliability in high-stakes domains, where rigorous control over Type I errors (the conditional probability of incorrectly classifying hallucinations as truthful content) is essential. Despite its importance, formal verification of LLM factuality with such guarantees remains largely unexplored. In this paper, we introduce FACTTEST, a novel framework that statistically assesses whether an LLM can confidently provide correct answers to given questions with finite-sample and distribution-free correctness guarantees. We formulate factuality testing as hypothesis testing problem to enforce an upper bound of Type I errors at user-specified significance levels. Notably, we prove that our framework also ensures strong Type II error control under mild conditions and can be extended to maintain its effectiveness when covariate shifts exist. Our approach is distribution-free and works for any number of human-annotated samples. It is model-agnostic and applies to any black-box or white-box LM. Extensive experiments on question-answering (QA) and multiple-choice benchmarks demonstrate that FACTTEST effectively detects hallucinations and improves the model's ability to abstain from answering unknown questions, leading to an over 40% accuracy improvement.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) like ChatGPT and GPT-4 (Ouyang et al., 2022; OpenAI et al., 2024) have demonstrated substantial advancements in various domains such as summarization systems, search engines and virtual assistants. However, their outputs cannot be fully trusted due to their propensity to generate nonfactual and incorrect information with seemingly high confidence, a challenge often referred to as hallucination (Maynez et al., 2020b; Huang et al., 2023; Ji et al., 2023). This tendency undermines the reliability and trustworthiness of the generated content, highlighting a critical need for robust mechanisms to verify the factuality and correctness of LLM outputs.\nExisting approaches to hallucination detection like retrieval-based methods (Thorne et al., 2018b; Gou et al., 2024; Chen et al., 2024) and training-based approaches Zhang et al. (2023) either rely on external databases or resource-intensive fine-tuning process, which are often impractical or costly. Therefore, there has been growing interest in uncertainty estimation as a zero-resource alternative for hallucination detection (Varshney et al., 2023; Xiong et al., 2024), operating under the premise that hallucinations are intrinsically tied to the model's uncertainty Huang et al. (2023). However, none of these methods can provide theoretical guarantees for the detection or testing results, which are essential for deploying LLMs in high-stakes domains Kumar et al. (2023) where precise control of Type I errors (incorrectly flagging a hallucination as truthful content) is needed for decision-making. For instance, incorrect medical diagnoses in healthcare or the provision of uncertain legal advice in the legal field could result in detrimental consequences.\nTo address these limitations, we introduce FACTTEST, a framework that statistically evaluates whether an LLM can reliably generate correct answers to given questions with provable correctness guarantees and teach LLMs to abstain from answering uncertain questions. We formulate the factuality testing"}, {"title": "STATISTICAL FACTUALITY TESTING", "content": "In this section, we formulate the evaluation of factuality in LLMs as a statistical hypothesis testing problem and introduce our FACTTEST framework to overcome hallucination issues."}, {"title": "PROBLEM FORMULATION", "content": "We consider a text generation task in which a language model M will generate its answers M(q) based on a question q. Our goal is to statistically evaluate whether M can correctly answer q, termed as M being certain of q. We formulate this objective as a hypothesis testing problem with the following hypotheses:\nHo: The model M cannot answer the question q certainly.\nH1: The model M can answer the question q certainly.\nFor any question-answer pair (q, a) with a to be the correct answer for question q, we apply M to generate an answer M(q). Then we classify the question-generated answer pair (q, M(q)) as certain if the null hypothesis Ho is rejected, i.e., M(q) aligns with a; otherwise, it is deemed uncertain. Let Po and P1 represent the distributions of uncertain and certain question-generated answer pair (q, M(q)), respectively."}, {"title": "FINITE-SAMPLE AND DISTRIBUTION-FREE TYPE I ERROR CONTROL", "content": "Calibration Dataset Construction. Following the methodology of Zhang et al. (2023), we adopt a supervised identification strategy to partition the dataset D into a certain subset D1 and an uncertain subset Do.\nSpecifically, for each question-generated answer pair (qi, M(qi)) in D, we define an indicator variable yi \u2208 {0, 1} to indicate whether M is certain about qi such that\n$y_i = \\begin{cases}\n1, & \\text{if } M(q_i) \\text{ aligns with the true answer } a_i, \\\\\n0, & \\text{otherwise}.\n\\end{cases}$\nBased on these indicators, the dataset is divided into:\nD1 = {(qi, M(qi)) \u2208 Q \u00d7 A : yi = 1, i \u2208 [n]}, Do = {(qi, M(qi)) \u2208 Q \u00d7 A : yi = 0, i \u2208 [n]}.\nAssuming that {(qi, M(qi), yi)}_{i=1}^n are independent and identically distributed (i.i.d.) samples from distribution P, and that the distributions of Do and D1 are Po and P1, respectively.\nCertainty Predictor based on Score Function. Suppose there is a score function \\hat{\\eta} : Q \u00d7 A \u2192 \\mathbb{R} that measures the certainty. The value is expected to be large if M has the ability to provide a factual answer. The predictor fa(q, M(q)) can then be defined as:\n$f_a(q, M(q)) = I(\\hat{\\eta}(q, M(q)) > t_a)$ (2)\nwhere I is the indicator function and ta is a threshold to be determined. The task thus reduces to selecting a threshold ta that satisfies the requirement in Eq. 1:\n$\\mathbb{P}_{D}(\\mathbb{P}_{(q,M(q))\\sim P_0}(\\hat{\\eta}(q, M(q)) > t_a) > \\alpha) \\leq \\delta.$ (3)\nCalibration and Threshold Selection To determine the appropriate threshold \\hat{t}_a, we utilize the calibration subset Do. Denote the no samples in Do as $D_0 = \\{(q_i^{(0)}, M(q_i^{(0)})) : i \\in [n_0]\\}$. For each calibration sample $(q_i^{(0)}, M(q_i^{(0)})) \\in D_0$, we compute the certainty score $T_i = \\hat{\\eta}(q_i^{(0)}, M(q_i^{(0)}))$. We then order these scores in ascending order to obtain the order statistics $T_{(1)} \\leq ... \\leq T_{(n_0)}$, and set $T_{(n_0+1)} = +\\infty$. Motivated by Tong et al. (2018), the threshold \\hat{t}_a is selected based on the following probabilistic guarantee:\n$\\mathbb{P}_{D}(\\mathbb{P}_{(q,M(q))\\sim P_0}(\\hat{\\eta}(q, M(q)) > T_{(k)}) > \\alpha) \\leq \\sum_{j=k}^{n_0} \\binom{n_0}{j} (1-\\alpha)^{n_0-j} \\alpha^j \\eqqcolon \\upsilon(k), k\\in [n_0 + 1]$ (4)\nwhen k = no + 1, v(k) is defined to be 0. We then determine k as\n$k = \\min\\{k \\in [n_0 + 1] : \\upsilon(k) \\leq \\delta\\}$, (5)\nSubsequently, the threshold is set to: \\hat{t}_a = T_{(k)}. Note that ta is well defined for any no, ensuring Type I error control irrespective of the calibration sample size n. Specifically, when no is small such that v(no) > \u03b4, the threshold becomes ta = T(no+1) = +\u221e, causing fa to conservatively classify all pairs (q, M(q)) as uncertain, thereby abstaining from answering any question. The derivation is deferred to Appendix A."}, {"title": "TYPE II ERROR CONTROL", "content": "The effectiveness of FACTTEST not only hinges on Type I error control but also on ensuring sufficient statistical power to detect true positives. We then analyze the Type II error of the constructed classifier.\nDenote $\\eta(q', M(q')) = \\mathbb{P}_{(q,M(q),y)\\sim P}(y = 1|q = q',M(q) = M(q'))$ to be the conditional prob- ability that M answers the question q' certainly given any question q' and the generated answer M(q'). Note that a question q may have multiple correct answers and a is just one realization. Therefore, a, and thus y, may still be random given (q, M(q)), implying \u03b7(q, M(q)) may take value in (0, 1). Suppose there exists an increasing function H such that $||H \\circ \\hat{\\eta} - \\eta||_\\infty \\leq \\epsilon_\\eta$, where $H \\circ \\hat{\\eta}(q, M(q)) = H(\\hat{\\eta}(q, M(q)))$ is the composition of H and \\hat{\\eta}. Note that k, and thus ta, are invariant if we replace \\hat{\\eta} in Section 2 by $H \\circ \\hat{\\eta}$. Therefore, without the loss of generality, we as- sume H is the identity function. Denote $F(t) = \\mathbb{P}_{(q,M(q))\\sim P_0}(\\hat{\\eta}(q, M(q)) \\leq t)$ to be the CDF of \\hat{\\eta}(q, M(q)) under Po. For any classifier f, we set $R_0(f) = \\mathbb{P}_{(q,M(q))\\sim P_0}(f(q, M(q)) = 1)$ (resp. $R_1(f) = \\mathbb{P}_{(q,M(q))\\sim P_1}(f(q, M(q)) = 0)$) to be the Type I error (resp. Type II error). It follows from Theorem 1 in Tong (2013) that the Bayes optimal classifier $f_a$\n$f_{a_{\\epsilon}} = \\arg \\min_{f:Q\\times A \\to \\{0,1\\}} R_1(f) \\text{ s.t. } R_0(f) \\leq \\alpha$\nhas the form $f_a(q, M(q)) = I(\\eta(q, M(q)) > \\tau_a)$ for some \\tau_a \u2208 [0, 1].\nLet $P_y = \\mathbb{P}_{(q,M(q),y)\\sim P}(y = 1)$ denote the marginal probability that model M is certain. We define\n$\\xi_\\alpha = \\frac{\\tau_a(1 - p_y)}{(1-\\tau_a)p_y}$\n$\\epsilon_\\tau = \\tau_a' - \\tau_a + \\epsilon_\\eta$,\n$\\delta' = a - c \\sqrt{\\frac{\\alpha}{n_0}} \\log \\frac{1}{\\delta}$,\nfor some constant c > 0. If we denote $G_a(\\epsilon) = \\mathbb{P}_{(q,M(q))\\sim P_0}(|\\eta(q, M(q)) - \\tau_a| \\leq \\epsilon)$ to be the probability measure around the classification boundary of $f_a$, then the Type II error of the proposed algorithm can be controlled as follows."}, {"title": "EXTENSION OF FACTTEST TO COVARIATE SHIFTS", "content": "The threshold selection procedure developed in Section 2 relies on the assumption that the calibration dataset $D_0 = \\{(q_i, M(q_i)) \\in Q | y_i = 0\\}$ follows the target distribution Po of uncertain question- generated answer pairs. However, labeled samples from the target distribution may not always be available in practice. Instead, people may use the labeled data that they believe to be similar to the target distribution, which necessitates methods to handle distribution shifts. In this section, we study the case of covariate shift, where the distribution of the question-generated answer pairs in the calibration data differs from that in the target distribution, while the conditional distribution of y given (q, M(q)) remains the same."}, {"title": "SETUP", "content": "Suppose we observe n samples $D = \\{(q_i, M(q_i), y_i) : i \\in [n]\\}$ from the source distribution P. We assume $P_{Y|q,M(q)} = P_{y|q,M(q)}$ but $P_{q,M(q)} \\neq P_{q,M(q)}$. Following Section 2, we split D into a certain subset $D_1 = \\{(q_i, M(q_i)) : y_i = 1,i \\in [n]\\} = \\{(q_i^{(1)}, M(q_i^{(1)})) : i \\in [n_1]\\}$ and an uncertain subset $D_0 = \\{(q_i, M(q_i)) : y_i = 0, i \\in [n]\\} = \\{(q_i^{(0)}, M(q_i^{(0)})) : i \\in [n_0]\\}$. We denote the distribution of Do, D\u2081 to be P0, P1, respectively. We further denote the density ratio between the target distribution Po of uncertain question-generated answer pair and the source distribution Po to be $w(q, M(q)) = \\frac{d\\mathbb{P}_0}{d\\mathbb{P}_0}(q, M(q))$. In this section, we assume w is known and satisfies $w(q, M(q)) \\leq B$ for all (q, M(q)) \u2208 Q \u00d7 A."}, {"title": "TYPE I ERROR CONTROL UNDER COVARIATE SHIFT", "content": "To extend the procedure in Section 2 to the covariate shift setting, we take an additional step to transform the samples in Do from Po to Po distributed random variables by rejection sampling.\nIn the first step, we generate no uniform random variables $U_1, ..., U_{n_0} \\stackrel{i.i.d.}{\\sim} Unif[0, B]$ and select the indexes $I = \\{i \\in [n_0] : U_i \\leq w(q_i^{(0)}, M(q_i^{(0)}))\\}$. If we collect all the samples in Do with indexes in I to form $\\tilde{D_0} = \\{(q_i^{(0)}, M(q_i^{(0)})) : i \\in I\\} = \\{(\\tilde{q}_i, M(\\tilde{q}_i)) : i \\in [\\tilde{n}_0]\\}$. Then it can be shown that $\\tilde{D_0}|I \\stackrel{i.i.d.}{\\sim} \\mathbb{P}_0$.\nIn the second step, we apply the procedure introduced in Section 2 to the uncertain subset Do. Specifically, given the uncertain subset Do, we calculate the certainty scores $T_i = \\hat{\\eta}(\\tilde{q}_i, M(\\tilde{q}_i))$ and order them in increasing order to get $\\hat{T}_{(1)} \\leq ... \\leq \\hat{T}_{(\\tilde{n}_0)}$, and set $\\hat{T}_{(\\tilde{n}_0+1)} = +\\infty$. Then we set the threshold $t_a$ to be $\\hat{T}_{(\\tilde{k})}$, with k satisfies\n$\\tilde{k} = \\min\\{k \\in [\\tilde{n}_0 + 1] : \\tilde{\\upsilon}(k) \\leq \\delta\\}$, $\\tilde{\\upsilon}(k) = \\sum_{j=k}^{\\tilde{n}_0} \\binom{\\tilde{n}_0}{j} (1-\\alpha)^{\\tilde{n}_0-j} \\alpha^j$, $\\tilde{\\upsilon}(\\tilde{n}_0 + 1) = 0$.\nThen we are able to control the Type I error as\n$\\mathbb{P}_{D}(\\mathbb{P}_{(q,M(q))\\sim P_0}(\\hat{\\eta}(q, M(q)) > \\hat{T}_{(\\tilde{k})}) > \\alpha) \\leq \\mathbb{E}\\tilde{\\upsilon}(k) \\leq \\delta.$"}, {"title": "EXPERIMENTS", "content": "In this section, we empirically investigate FACTTEST in addressing the hallucination problem of LLMs, focusing on the following questions: Q1: Can FACTTEST improve the accuracy and lead to more factual LLMs? Q2: Can FACTTEST effectively control the Type I error? Q3: Can FACTTEST generalize well when covariate shifts exist?"}, {"title": "EXPERIMENTAL SETUPS", "content": "Datasets. Following R-Tuning (Zhang et al., 2023), we conduct our experiments on the knowledge- extensive QA tasks, which can then be categorized into two generation tasks. More details about the datasets can be referred to Appendix B.1.\nQuestion-Answering: Given a question, the model directly predicts its answer, which is one sentence with tokens no longer than 15. We include ParaRel (Elazar et al., 2021) and HotpotQA (Yang et al., 2018). For experiments considering distirbution shifts, we utilize ParaRel-OOD as the testing dataset.\nMultiple-Choice: Given a question with several choices, the model chooses one option among A, B and C. We include WiCE (Kamoi et al., 2023) and FEVER (Thorne et al., 2018a).\nCertainty Score Functions. We can either fit a prediction model to predict the certainty of a given question or use any off-the-shelf certainty estimation function to serve as the score function \\hat{\\eta}. Particularly, in this paper, we introduce three entropy-based certainty functions. Details about the score functions and equations are deferred to Appendix B.2."}, {"title": "MAIN EXPERIMENTAL RESULTS", "content": "We first conduct in-distribution experiments on question-answering and multiple choice datasets ParaRel, HotpotQA, WiCE and FEVER.\nMain Performance. The accuracy results are presented in Table 1, where the significance level a for FACTTEST is set to 0.05. Additional experimental results for other significance levels (e.g., a = 0.10) are provided in Appendix C.1. Analysis of the results reveals that FACTTEST significantly outperforms pretrained models by a substantial margin in terms of accuracy on the questions it is willing to answer, compared to baselines that respond to all questions indiscriminately. Notably, FACTTEST can yield an over 40% accuracy improvement on ParaRel, WiCE and FEVER. These"}, {"title": "COMPARING WITH FINETUNED MODELS", "content": "Figure 3 illustrates the accuracy performance of FACTTEST-t compared to the baseline methods R-Tuning, Finetune-All, and Finetune-Half. We randomly divide D into two equal parts: D1 for instruction-tuning and De for constructing the calibration dataset. The pretrained model is finetuned on D\u2081 to obtain Finetune-Half, while Finetune-All is obtained by training on the entire dataset D. For R-Tuning, we also utilize the entire dataset to finetune the model. It is evident that FACTTEST-t consistently outperforms R-Tuning by a large margin, while utilizing only half of the available training data, thereby reducing training costs by 50%. Notably, FACTTEST-t yields 34% and 28% accuracy improvement over R-Tuning on HotpotQA and FEVER, respectively. Despite the reduced size of the calibration dataset, FACTTEST-t maintains effective control over Type I error, with further details provided in Appendix C.3. Answer rate analysis of FACTTEST-t and baselines is also provided in Appendix C.2."}, {"title": "EXTENSION TO COVARIATE SHIFTS", "content": "In this subsection, we evaluate the extension of our framework, denoted as FACTTESTO (FACTTEST for Out-of-distribution domains), on the dataset containing distribution shifts.\nSetup. We utilize ParaRel as the training dataset, consistent with the aforementioned experiments. We randomly split ParaRel-OOD into a validation dataset comprising 1,000 samples and a testing dataset containing 11k samples. As outlined in Section 3, it is necessary to calculate the density ratio between the target distribution and the source distribution. To achieve this, we employ the training data from the source domain and the validation data from the target domain to train a classifier for approximating density ratios. Subsequently, we select B as the y upper quantile of density ratios to filter out anomalous values. We set the default value of y as 90%."}, {"title": "EXTENSION TO BLACK-BOX APIS", "content": "We further evaluate our framework on black-box models, such as GPT-40 Mini (OpenAI, 2024), to broaden the applicability of our framework. While certainty functions like SE and KLE require token probabilities, which are unavailable for black-box APIs, we utilize open-source models to calculate the certainty scores on calibration datasets constructed by black-box models. Table 3 illustrates the performance of FACTTEST on GPT-40 Mini. The results demonstrate that the certainty scores derived from open-source models are effective for black-box APIs, achieving a 33% accuracy improvement on ParaRel and an 11% improvement on WiCE, while maintaining control over Type I error. These findings illustrate that our framework provides a practical and effective solution for detecting hallucinations in closed-box models."}, {"title": "RELATED WORK", "content": "Factuality of LLMs. The factuality of LLMs is a major problem and of significant research interest since these models are likely to hallucinate unfaithful and nonfactual facts, which severely impacts the reliability and trustworthiness of LLMs (Ji et al., 2023; Maynez et al., 2020a; Li et al., 2023).\nRecently, a variety of works have been done towards hallucination detection, mitigation and eval- uation (Huang et al., 2023; Wang et al., 2023). Our work relates more to hallucination detection, which is imperative for assuring the reliability of the generated content. Retrieving-based approaches (Kadavath et al., 2022) proposes self-evaluation by asking models to first propose answers, and then evaluate their previous prediction. Azaria & Mitchell (2023) trains a classifier based on hidden layer activations, which is a white-box approach that requires internal states of the LLMs. Lee et al. (2023) uses factual and nonfactual prompts and creates a benchmark for measuring the factuality of generations. Manakul et al. (2023) introduces SelfCheckGPT to fact-check the responses of LLMs"}, {"title": "Uncertainty Quantification of LLMs", "content": "Our work relates to a line of work on uncertainty quantifi- cation (UQ) for LLMs, as we employ certainty functions to assess models' ability to reliably give an answer. Predictive entropy that measures the entropy of the model's predicted token distribution has been used as a simple baseline for UQ in LLMs (Braverman et al., 2019). Kuhn et al. (2023) introduced Semantic Entropy, which incorporates linguistic invariances to measure uncertainty. Most recently, Nikitin et al. (2024) introduced Kernel Language Entropy (KLE), which defines positive semidefinite unit trace kernels and quantifies uncertainty using the von Neumann entropy. Lin et al. (2024) proposed a simple supervised approach for uncertainty estimation in black-box LLMs using labeled datasets. Duan et al. (2024) identifies that existing methods treat all tokens equally when estimating uncertainty and proposes jointly Shifting Attention to more Relevant (SAR) components.\nThese works are complementary to ours, as our contribution is a meta-algorithm that works with any uncertainty quantification method to serve as the certainty score functions and assess the factuality. Future developments in this line of work can greatly improve the performance of our framework."}, {"title": "Neyman-Pearson Classification", "content": "Instead of minimizing a weighted combination of Type I and II errors, as studied in standard classification and cost-sensitive learning, the Neyman-Pearson classification paradigm focuses on the setting where we prioritize the Type I error control and aim to achieve minimum Type II error while keeping the Type I error below a user-specified level a. To achieve this goal, Rigollet & Tong (2011); Scott & Nowak (2005) propose to use the empirical risk minimization strategy and Tong (2013) utilizes the plug-in approaches to construct the NP classifier. In a more related work Tong et al. (2018), the authors propose an umbrella algorithm that achieves Type I error control for any pretrained classifiers. However, this work (Tong et al., 2018) does not provide any Type II error guarantee for the proposed algorithm.\nOur work takes an initial step to use the NP classification idea to do the factuality testing for LLMs. And the Type II error analysis for our method can be directly applied to the standard NP umbrella algorithm, which is of independent interest. Moreover, we extend the NP classification framework to the case with covariate shifts, enabling us to address more practical and real-world problems."}, {"title": "CONCLUSION: SUMMARY AND LIMITATIONS", "content": "Summary. In this paper, we introduced FACTTEST, a novel framework for factuality testing in Large Language Models (LLMs) that leverages the principles of Neyman-Pearson (NP) classification to provide finite-sample and distribution-free guarantees. By formulating factuality testing as a hypothesis testing problem, FACTTEST effectively enforces an upper bound on Type I errors. We prove that our framework ensures strong power control under mild conditions and can be extended to maintain its effectiveness in the presence of covariate shifts. These theoretical analyses can be seamlessly integrated with the standard NP umbrella algorithm, not limited to our framework. Our approach is distribution-free and works for any number of human-annotated samples. It applies to any LLM including closed-box models. Empirical evaluations have demonstrated that FACTTEST consistently outperforms both pretrained and fine-tuned baselines by a large margin. Besides, FACTTEST can be extended to maintain superior performance under distribution shifts, ensuring its robustness and reliability in real-world scenarios. Additionally, our framework effectively enhances the reliability of black-box APIs, highlighting its practical applicability.\nLimitation. One limitation of our work is the current implementation of only three entropy-based certainty functions, which may not be optimal choices in every task. Exploring additional certainty estimation methods could further enhance the framework's performance. Furthermore, our framework constructs the certainty predictor in an offline manner. Future work could extend FACTTEST to support online testing, thereby enabling real-time factuality assessments."}, {"title": "DERIVATION AND PROOF", "content": "Assume that when (q, M(q)) ~ Po, \\hat{\\eta}(q, M(q)) has CDF F. We denote the 1 a quantile of F as\n$F^{-1}(1 - \\alpha) = \\inf\\{x \\in \\mathbb{R} | F(x) \\geq 1 - \\alpha\\}$. Then we can show that:\n$\\mathbb{P}_{D}(\\mathbb{P}_{(q,M(q))\\sim P_0}(\\hat{\\eta}(q, M(q)) > T_{(k)}) > \\alpha) = \\mathbb{P}_{D}(T_{(k)} < F^{-1}(1 - \\alpha))$ (8)\nConsidering the property of the order statistics, we have that\n$\\mathbb{P}_{D}(T_{(k)} < F^{-1}(1 - \\alpha)) = \\mathbb{P}_{D}(\\sum_{i=1}^{n_0} I(\\hat{\\eta}(q_i^{(0)}, M(q_i^{(0)})) < F^{-1}(1 - \\alpha)) \\geq k)$ (9)\n$\\leq \\sum_{j=k}^{n_0} \\binom{n_0}{j} (1-\\alpha)^{n_0-j} \\alpha^j \\eqqcolon \\upsilon(k)$ (10)\nwhere I() is the indicator function, defined as:\n$I(\\hat{\\eta}(q_i^{(0)}, M(q_i^{(0)})) < Q_a) = \\begin{cases}\n1 & \\text{if } \\hat{\\eta}(q_i^{(0)}, M(q_i^{(0)})) < Q_a\\\\\n0 & \\text{otherwise}\n\\end{cases}$\nProof 1 (Proof of Theorem 1) Theorem 1 follows from the definition of k.\nLemma 1 (Theorem 1 in Skorski (2023)) Suppose $X_1, ..., X_n$ are i.i.d. continuous random vari- ables with CDF F, denote\n$\\xi_k = \\sqrt{\\frac{4(n - 2k + 1)}{3(n+1)(n+3)} \\log \\frac{2}{\\delta}} + \\frac{2k(n - k + 1)}{(n+1)^2(n+2)} \\log \\frac{2}{\\delta}$\nthen\n$\\mathbb{P}\\{|F(X_{(\\frac{k}{n+1})}) - \\frac{k}{n+1}| < \\xi_k\\} \\geq 1-\\delta.$\nProof 2 (Proof of Theorem 2) 1) Firstly, we show that $R_0(f_a)$ is not much smaller than a. To see this, since we assume \\hat{\\eta}(q, M(q)) with (q, M(q)) ~ Po is a continuous random variable, it follows from the definition of k that\n$\\mathbb{P}(\\mathbb{P}_{(q,M(q))\\sim P_0}(\\hat{\\eta}(q, M(q)) > T_{(k)}) > \\alpha) = \\mathbb{P}_{D}(T_{(k)} < F^{-1}(1 - \\alpha))$ (8)\nConsidering the property of the order statistics, we have that\n$\\mathbb{P}_{D}(T_{(k)} < F^{-1}(1 - \\alpha)) = \\mathbb{P}_{D}(\\sum_{i=1}^{n_0} I(\\hat{\\eta}(q_i^{(0)}, M(q_i^{(0)})) < F^{-1}(1 - \\alpha)) \\geq k)$ (9)\n$\\leq \\sum_{j=k}^{n_0} \\binom{n_0}{j} (1-\\alpha)^{n_0-j} \\alpha^j \\eqqcolon \\upsilon(k)$ (10)\nwhere I() is the indicator function, defined as:\n$I(\\hat{\\eta}(q_i^{(0)}, M(q_i^{(0)})) < Q_a) = \\begin{cases}\n1 & \\text{if } \\hat{\\eta}(q_i^{(0)}, M(q_i^{(0)})) < Q_a\\\\\n0 & \\text{otherwise}\n\\end{cases}$\nProof 1 (Proof of Theorem 1) Theorem 1 follows from the definition of k.\nLemma 1 (Theorem 1 in Skorski (2023)) Suppose $X_1, ..., X_n$ are i.i.d. continuous random vari- ables with CDF F, denote\n$\\xi_k = \\sqrt{\\frac{4(n - 2k + 1)}{3(n+1)(n+3)} \\log \\frac{2}{\\delta}} + \\frac{2k(n - k + 1)}{(n+1)^2(n+2)} \\log \\frac{2}{\\delta}$\nthen\n$\\mathbb{P}\\{|F(X_{(\\frac{k}{n+1})}) - \\frac{k}{n+1}| < \\xi_k\\} \\geq 1-\\delta.$\nProof 2 (Proof of Theorem 2) 1) Firstly, we show that $R_0(f_a)$ is not much smaller than a. To see this, since we assume \\hat{\\eta}(q, M(q)) with (q, M(q)) ~ Po is a continuous random variable, it follows from the definition of k that\n$\\mathbb{P}(F(T_{(k-1)}) < 1 - \\alpha) = \\mathbb{P}(\\mathbb{P}_{(q,M(q))\\sim P_0}(\\hat{\\eta}(q, M(q)) > T_{(k-1)}) > \\alpha) > \\delta.$\nHere we only consider the case where k > 1, as will be shown in Equation equation 11, it holds as long as no is not too small. Since k is deterministic given no, it follows from Lemma 1 that\n$\\mathbb{P}(F(T_{(k-1)}) \\leq \\frac{k-1}{n_0 + 1} + \\xi_{k-1}) \\geq 1 - \\delta.$\nTherefore we have\n$\\frac{k-1}{n_0 + 1} + \\xi_{k-1} < 1 - \\alpha.$\nDenote the event E\u2081 as\n$E_1 = \\{F(T_{(k)}) \\leq \\frac{k}{n_0 + 1} + \\xi_k\\}$,\nit follows from Lemma 1 that $\\mathbb{P}(E_1) \\geq 1 - \\delta$. Under E1, we know\n$F(T_{(k)}) \\leq \\frac{k}{n_0 + 1} + \\xi_k < 1 - \\alpha + \\frac{1}{n_0 + 1} + \\xi_{k-1} + \\xi_k$,"}]}