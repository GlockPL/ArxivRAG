{"title": "Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression", "authors": ["Xiaoyi Qu", "David Aponte", "Colby Banbury", "Daniel P. Robinson", "Tianyu Ding", "Kazuhito Koishida", "Ilya Zharkov", "Tianyi Chen"], "abstract": "Structured pruning and quantization are fundamental techniques used to reduce the size of deep neural networks (DNNs), and typically are applied independently. Applying these techniques jointly via co-optimization has the potential to produce smaller, high quality models. However, existing joint schemes are not widely used because of (1) engineering difficulties (complicated multi-stage processes), (2) black-box optimization (extensive hyperparameter tuning to control the overall compression), and (3) insufficient architecture generalization. To address these limitations, we present the framework GETA, which automatically and efficiently performs joint structured pruning and quantization-aware training on any DNNs. GETA introduces three key innovations: (i) a quantization-aware dependency graph (QADG) that constructs a pruning search space for generic quantization-aware DNN, (ii) a partially projected stochastic gradient method that guarantees layerwise bit constraints are satisfied, and (iii) a new joint learning strategy that incorporates interpretable relationships between pruning and quantization. We present numerical experiments on both convolutional neural networks and transformer architectures that show that our approach achieves competitive (often superior) performance compared to existing joint pruning and quantization methods.", "sections": [{"title": "1. Introduction", "content": "Deep neural networks (DNNs) have been widely used in varying applications [30, 39, 41, 64]. However, their increasing size has raised several concerns. One major challenge is the substantial storage space required to hold these models, which can be impractical for everyday devices such as standard PCs and even more so for resource-constrained edge devices [58]. Furthermore, as model sizes increase, inference cost often lengthens, leading to delays that can be frustrating for users who expect quick responses. Therefore, it is of practical interest and importance to compress the model while maintaining performance similar to the full model. To address the above concerns, various model compression techniques have been studied in recent years [16].\nPruning and quantization are two fundamental techniques that are widely deployed, each following different methodologies. Structured pruning is perhaps the most popular pruning scheme, which aims to remove redundant structures in DNNs while preserving performance [11, 20]. Quantization reduces the bit width of the data flowing through a DNN [16]. In practice, structured pruning is typically applied first to identify a high-performing subnetwork, which is then quantized to further reduce its size and enhance its processing speed on specified hardware [27, 50]. However, treating pruning and quantization separately has limitations. For example, more heavily structurally pruned models are typically more sensitive to quantization and thus require higher bit widths. Thus, joint structured pruning and quantization becomes an important topic."}, {"title": "1.1. Challenges", "content": "Many studies [3, 27, 34, 46, 50, 55, 60, 63, 67, 70, 73] have combined pruning and quantization to obtain high compression ratios. However, these joint methods are not commonly used in practice due to one or more of the following reasons: engineering difficulty, black-box optimization, and insufficient architecture generalization, which we now discuss.\nEngineering Difficulties. First, many joint pruning and quantization methods follow a two-stage process. For example, [60, 63, 67, 70] first determine the configurations (pruning ratio and bit width) for each layer of the network, and then train the pruned and quantized model. They require separate compression and retraining stages since the two stages may be incompatible with each other. Thus, two-stage pipelines increase the execution time, especially for large datasets (e.g., ImageNet). For these reasons, a one-shot (all-in-once) framework is preferred. Second, while recent automated structured pruning frameworks propose dependency graphs to support generic architectures [12, 20],"}, {"title": "1.2. Our Contributions", "content": "To tackle the above challenges, we propose GETA, a General and Efficient Training framework that Automates joint structured pruning and quantization aware training. By streamlining the workflow, GETA significantly reduces the engineering burdens and minimizes the user intervention (See Framework Usage).\nAs shown in Fig. 1, GETA begins by incorporating the parameterized quantization layer [61] into the full model, which allows for layerwise bit widths to be learned during training (see Sec. 3). Next, the framework proposes a quantization-aware dependency graph (QADG) (see Sec. 4) to address previously unconsidered graph transformations introduced by parameterized quantization layers, ensuring support for any architecture. To train the neural network using the quantization-aware dependency graph, we employ a quantization-aware structured sparse optimizer (see Sec. 5) to determine the optimal tradeoff between the pruning ratio and bit width for each layer. Our main contributions are summarized as follows.\n\u2022 Quantization-Aware Dependency Graph (QADG). We propose the quantization-aware dependency graph (QADG) to support joint structured pruning and quantization applied to any quantization-aware deep neural network (QADNN). By eliminating the need to handle each architecture individually, QADG significantly reduces the model-specific engineering workloads.\n\u2022 Quantization-Aware Structured Sparse Optimizer (QASSO). We propose a quantization-aware structured sparse optimizer, to provide reliable joint structured pruning and mixed precision quantization-aware training. To the best of our knowledge, QASSO is the first white-box joint optimizer that explicitly controls the sparsity ratio and bit width. Particularly, QASSO employs a partial"}, {"title": "2. Related Work", "content": "Structured Pruning. Structured pruning aims to remove redundant structures to reduce the size of DNNs. The identification of redundancies can be performed based on different criteria such as sparsity [6\u20138, 11, 24, 26, 36, 47, 52, 68, 71, 79], Bayesian pruning [63, 78], ranking importance [12, 43, 45, 75], grouped kernel search [77], spectral graph analysis [42], reinforcement learning [5, 31], and the lottery ticket hypothesis [21, 22]. Previous methods typically use a complicated, time-consuming process that requires extensive domain knowledge to effectively train the DNN. Another challenge is to define a pruning search space procedure that can be generalized to various DNNs. Recent frameworks, such as OTO [10, 12, 13] and DepGraph [20], have automated the construction of this search space using dependency graphs. However, these methods are not suitable for QADNNs due to prevalent issues such as weight-sharing and shape ambiguous operators. This limitation highlights the ongoing challenge of automating structured pruning for any QADNN.\nQuantization-Aware Training (QAT). The standard approach to QAT is applying a uniform bit width across all layers. However, [18, 33] empirically show that different layers in DNNs exhibit different sensitivities to quantization, suggesting that mixed-precision quantization may be a better approach for reducing performance loss. Several strategies including parameterized quantizers [61], heuristic approaches [55], reinforcement learning [3, 19], multi-objective Bayesian optimization [53], and Hessian information guided methods [17, 18, 72] have been proposed to determine the optimal bit width for each layer.\nJoint Pruning and Quantization. The challenge of using a joint approach lies in determining an optimal tradeoff between the pruning ratio and quantization levels for the model. Two primary strategies have been explored to address this challenge. The first strategy is to efficiently search the joint parameter space with prior work considering heuristics [55], reinforcement learning [3], and"}, {"title": "3. Quantization with Learnable Parameters", "content": "Instead of freezing the bit width in standard QAT approach, we introduce quantization parameters $q_m, t$, and $d$ to learn the bit width of each layer [62]. In particular, $q_m$ represents the maximum value to be mapped and $t$ is the exponent controlling the shape of the mapping and $d$, known as quantization step size, characterizes the interval between adjacent quantization levels. For each quantization operation, we first quantize the input tensor $x$ as $\\tilde{x}$ by applying a nonlinear function [67]\n$\\tilde{x} = \\begin{cases} sgn(x) \\cdot \\frac{|x|^t}{(q_m)^{t-1}}, & |x| \\leq q_m, \\\\ (q_m), & |x|>q_m. \\end{cases}$     (1)\nAfter applying the nonlinear mapping, we perform symmetric uniform quantization on $\\tilde{x}$, resulting in the mapping\n$\\hat{x} = d[\\tilde{x}/d],$     (2)\nwhere $[\\,]$ represents rounding to the nearest integer. The associated bit width $b$ is computed as\n$b = \\log_2(\\lceil \\frac{q_m}{d} \\rceil + 1) + 1.$     (3)\nTo optimize the learnable quantization variables $d$, $t$, and $q_m$, we compute their gradients using the straight-through estimator [61]. In particular, the gradient of the quantization"}, {"title": "4. Quantization-Aware Dependency Graph", "content": "To automate joint structured pruning and quantization-aware training, we first establish a pruning search space. This space is defined as the set of minimally removable structures within the target DNN, ensuring that the remaining sub-network remains functional post-removal. However, establishing this search space automatically is challenging due to the complex topology of DNNs and the diverse roles of operators. Recent advancements in dependency graph [12, 20] address some of these challenges, but existing approaches remain insufficient for QADNN.\nTo automate the construction of the pruning search space for QADNN, we construct a Quantization-Aware Dependency Graph (QADG). QADG efficiently captures pruning dependencies across both weight and activation quantization. Challenges arise due to the numerous parameterized layers introduced during layer conversion, which include weight-sharing and shape-ambiguous layers that previous algorithms do not account for. Weight and activation quantization-aware layers exhibit distinct structural patterns. As shown in Fig. 2(a), weight quantization introduces a prominent attached branch connected to the target layer. In contrast, activation quantization inserts a set of layers between the activation layer and its subsequent layer, referred to as the inserted branch."}, {"title": "5. QASSO", "content": "After obtaining a QADG using Algorithm 1, we obtain the pruning search space of the QADNN, i.e., the parameter groups G. Each $g \\in G$ represents the collection of trainable variables in one minimally removal structure. We then apply our proposed QASSO optimizer (see Algorithm 2) to solve the problem\n$\\min_{(d,q_m,t) \\in \\mathbb{R}^{L} \\times \\mathbb{R}^{L} \\times \\mathbb{R}^{L}} f(x, d, q_m, t)$     (7a)\ns.t.  $\\text{Card}\\{g \\in G|[x]_g = 0\\} = K,$     (7b)\n$b_i \\in [b_l, b_u], i \\in L,$     (7c)\nwhere $K$ represents the target sparsity ratio, $[b_l, b_u]$ specifies the target bit width range, and $L$ denotes the index set of layers that have parameterized quantization layers added, and $|L|$ represents the cardinality of set $L$, and bit width $b_i$ is computed using formula Eq. (3) given in Sec. 3.\nOverview of QASSO. Our framework QASSO (see Algorithm 2) aims to compress the size of the DNN while preserving full model performance by removing redundant structures, determining the optimal bit width for each layer that has a parameterized quantization layer added, and recovering knowledge lost during pruning and quantization phases. This is accomplished through a sequential four-stage optimization process: warm-up stage, projection stage, joint stage, and a cool-down stage. The warm-up stage consists of optimizing over all trainable variables using the stochastic gradient (SGD) method or any of its variants at Line 2, which allows us to achieve a better initialization for improved performance. Next, we step into the projection stage (see Line 3-9), where we progressively reduce the bit width range until the bit width constraint Eq. (7c) is satisfied. This progressive technique enables us to transfer information lost in the low bit precision representation back to the current model. We then proceed to the joint stage (see Line 10-21), where we progressively forget the quantized information (see Eq. (9)) within the redundant groups until the constraint Eq. (7b) is satisfied. In addition, the bit width selected depends on the amount of information removed within each layer at each step. Specifically, when a significant amount of information is removed, we will consider employing a high bit width for quantization. Once we"}, {"title": "5.1. Projection Stage", "content": "During the projection stage, we aim to compute a feasible bit width. To do so, we consider the problem\n$\\min_{x \\in \\mathbb{R}^{n}} f(x, d, q_m, t)$    (10a)\n$(d,q_m,t) \\in \\mathbb{R}^{L} \\times \\mathbb{R}^{L} \\times \\mathbb{R}^{L}$\ns.t.  $b_i \\in [b_l, b_u], i \\in L.$    (10b)"}, {"title": "5.2. Joint Stage", "content": "During the joint stage, we aim to identify redundant groups of G, to forget the information within the redundant groups and transfer to the important groups being aware of the quantization parameters, and to determine the layerwise bit widths in terms of the information removed at each layer. We first partition our parameter group G into a set of important groups G\u2081 and a set of redundant groups $G_R$ based on saliency scores detailed in [13] at Line 12. For variables in G\u2081, we proceed with vanilla stochastic gradient or its variants at Eq. (8). For variables in $G_R$, we progressively project them to zero by forgetting redundant information at Eq. (9). Due to the addition of parameterized quantization layers to the original model, weight parameters $x$ are converted to its quantized counterpart, denoted as $\\hat{x}$. This observation underscores the necessity to forget the quantized information $\\[\\hat{x}\\]_{G_R}$ instead of the original information $\\[x\\]_{G_R}$. Additionally, it is essential to develop a new update rule for the forget rate \u03b3 that is aware of quantization parameters to better maintain and transfer the knowledge.\nFor ease of notation, we denote the stochastic gradient of function $f (x, d, q_m, t)$ with respect to x as $\\nabla_x f$. Consequently, the search direction $s(x)$ for updating $x$ is\n$s(x) = \\begin{cases} -\\alpha [\\nabla_x f]_g, & g \\in G_I, \\\\ -\\alpha [\\nabla_x f]_g - \\gamma[\\hat{x}]_g, & g \\in G_R. \\end{cases}$     (11)\nThe quantized value $\\hat{x}$ in Eq. (11) can be rewritten as\n$\\hat{x} = sgn(x) \\cdot clip_{\\frac{q_m}{d}}(|x|) + d \\cdot sgn(x) \\cdot R(x)$,     (12)\nwhere the clipped value can be written as\n$clip_{\\frac{q_m}{d}} (x) = \\begin{cases} x, & |x| \\leq q_m, \\\\ (\\frac{q_m}{d}), & |x| > q_m, \\end{cases}$     (13)\nand the residual value is given by\n$R(x) = \\begin{cases} \\frac{|x|^t}{(q_m)^{t-1}} - \\frac{|x|}{d}, & |x| \\leq q_m, \\\\ (\\frac{q_m}{d}) - (\\frac{q_m}{d}), & |x| > q_m. \\end{cases}$     (14)\nWe denote the angle between $- \\[\\nabla_x f\\]_g$ and $-\\[sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|)\\]_g$ as $\\theta_y$ and the angle between $-\\[\\nabla_x f\\]_g$ and $-\\[sgn(x) d \\cdot R(x)\\]_g$ as $\\theta_d$. The $\\text{clip}$ represents the mean of the clipped value within the redundant group $G_R$, i.e.,\n$\\text{clip} = mean( \\[clip_{\\frac{q_m}{d}} (|x|)\\]_{G_R})$.     (15)\nWith the above notations, the forget rate \u03b3 selection rule is expressed, for pre-specified small $\\epsilon$ and $\\eta \\in (0, 1)$, as\n$\\gamma = \\begin{cases} 0, & \\text{clip} < \\epsilon, \\\\ \\frac{1}{K_p-k} (1 - \\eta) \\frac{\\alpha ||\\[\\nabla_x f\\]_g||}{\\text{cos}(\\theta_y) ||\\[sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|)\\]_g||}, & \\text{cos}(\\theta_y) \\geq 0, \\text{clip} > \\epsilon, \\\\ \\frac{1}{K_p-k-1} (1 - \\eta) \\frac{\\alpha ||\\[\\nabla_x f\\]_g||}{\\text{cos}(\\theta_y) ||\\[sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|)\\]_g||}, & \\text{cos}(\\theta_y) < 0, \\text{clip} > \\epsilon. \\end{cases}$     (16)\nThe quantization step size $d$ selection rule is, for $\\xi \\in (0, 1)$,\n$d = \\begin{cases} \\frac{q_m}{2^{\\lceil\\log_2(\\frac{q_m}{d})\\rceil+1}-1}, & \\text{cos}(\\theta_d) \\geq 0, \\\\ \\frac{\\xi \\eta \\alpha ||\\[\\nabla_x f\\]_g||}{\\text{cos}(\\theta_d) ||\\[sgn(x) \\cdot R(x)\\]_g||}, & \\text{cos}(\\theta_d) < 0. \\end{cases}$     (17)\nInterpretation of Update Rules for \u03b3 and d. At a high level, the update rule for the forget rate and quantization step size ensures that the search direction in Eq. (11) is a descent direction for the objective function $f$, as stated in Proposition 5.1. Consequently, forgetting knowledge stored in the redundant groups for pruning and quantizing the variables jointly in this manner can make progress towards convergence. Therefore, the conflict between pruning and quantization is largely resolved via our design."}, {"title": "6. Numerical Experiments", "content": "In this section, we present numerical experiments to demonstrate the effectiveness of our approach, accompanied by ablation studies to assess the contribution of each component to the success of GETA.\nDNN Architectures and Datasets. The experiments are performed across a wide range of popular CNN architectures, such as VGG7 [38], ResNet20, ResNet50 and RseNet56 [30], and transformer architectures, such as Bert [39], varying vision transformers [1] and Large Language Models (LLMs) such as Phi2-2.7B [37]. The selected datasets include the benchmark CIFAR10 [40], ImageNet2012 [15], Squad [39], and commen-sense benchmarks in LM-Evaluation-Harness [25].\nComparing Methods. To validate the effectiveness and superiority of our framework, we consider the following methods for comparisons: ANNC [70], QST-B [55], DJPQ [67] along with its variants, BB [63], Clip-Q [60], OBC [23], and a standard first-prune-then-quantize method. All the compared methods consider both pruning and quantization. Furthermore, they use the same strategy that first conducts a search based on the same pretrained model and then fine-tunes the resulting model with the configurations obtained from the search.\nEvaluation Metrics. We evaluate the performance of each method on two folds, model performance and computa-"}, {"title": "6.1. CNN Architectures", "content": "ResNet20 on CIFAR10. We first test our framework GETA using ResNet20 on CIFAR10 dataset. For fair comparison, only weight quantization is applied, excluding activation quantization. As shown in Tab. 2, GITA achieves a 4.5% relative BOPs compression ratio with only a loss of 0.28% in test accuracy, which demonstrates significantly better performance than ANNC [70]. Compared to QST-B [55], GETA reduces BOP by 13% with only a minimal accuracy drop of 0.08%. We argue that GETA is better suited for practical applications, as QST-B focuses on joint unstructured pruning and quantization. While unstructured pruning tends to deliver higher accuracy at similar compression ratios, its theoretical speedups are challenging to achieve without specialized hardware and software supports [28, 35, 76]. In contrast, the structurally pruned and quantized model produced by GETA is more easily deployed in practical applications.\nVGG7 on CIFAR10. We then test GETA using VGG7 on CIFAR10 to compare with the joint structured pruning and quantization benchmarks. In this case, we enable both weight and activation quantization and report the results in Tab. 4. Based on the results, GETA could significantly outperform other competitors in terms of the test accuracy by 0.61%- 1.14%, and achieves the second best relative BOP ratio which is only worse than BB [63]. BB separates the model architecture compression and training stages, requiring substantial effort for each. In contrast, GETA offers practical advantages, including efficiency and broad architecture compatibility, enabling an end-to-end, automated joint structured pruning and quantization approach.\nResNet50 on ImageNet. We next test GETA using ResNet50 on ImageNet. We select ResNet50 on ImageNet because it serves as one of most common benchmarks in"}, {"title": "6.2. Transformer", "content": "Bert on SQUAD. We now apply GETA to the transformer architecture. The first is the representative encoder-based BERT model [64] on the SQUAD benchmark [57]. While previous works on joint quantization and structured pruning have not been applied to the transformer architecture, we make a more relevant comparison by contrasting our joint optimization approach with the sequential baseline, which first applies pruning-aware training (HESSO) [13] and then performs standard post-training quantization (PTQ) [56]. An alternative sequential baseline, the quantize-then-prune approach, is excluded from our comparison for the following two reasons: (i) Applying PTQ to the full model introduces challenges when attempting to prune the model afterward, as calculating gradients with respect to quantized values requires careful handling. (ii) A recent work [29] mathematically shows that prune-then-quantize approach is the optimal sequential strategy. Therefore, we focus on comparing GETA with the prune-then-quantize baselines.\nThe comparison in Tab. 3 clearly highlights the advantages of joint structured pruning and quantization during training, versus only pruning at training time and quantization during post-training. At all sparsity ratios, GETA consistently outperforms the multi-stage approach by a large margin. In particular, we observe improvements in exact-match rates (EM) and F1-scores while achieving better compression rates. These results empirically validate that joint pruning and quantization during training is superior to the conventional approach of pruning-aware training followed by post-training quantization, both in terms of model quality and computational efficiency.\nPhi2 on Common-Sense. We next evaluate GETA on popular large language models. Since GETA leverages full gradient information, we select Phi2-2.7B [37], a model with fewer than 3 billion parameters, to ensure computational feasibility on a single A100 GPU. Similar to the experiments on BERT, we compare GETA with a prune-then-quantize baseline. This baseline first applies pruning-aware training techniques, including SliceGPT [2], LoraShear [9], LoraPrune [74], and LLMPruner [51], followed by PTQ. For a fair comparison, the average bit width across all layers after applying GETA is set to approximately 8 bits, while the baseline uses uniform 8-bit PTQ. As shown in Fig. 3, GETA consistently outperforms all prune-then-quantize baselines in terms of average performance in common-sense tasks including BoolQ, PIQA, HellaSwag, WinoGrande, ARC-e, ARC-c and OBQA."}, {"title": "6.3. Ablation Study", "content": "Our proposed QASSO consists of four distinct stages: warm-up stage, projection stage, joint stage, and cool-down stage. To evaluate the contribution of each stage, we conduct an ablation study on two benchmarks, ResNet56 trained from scratch on CIFAR10 and Phi2 fine-tuned from a pre-trained model on the Common-Sense task. The results demonstrate that each stage positively contributes to the model's performance, as measured by test accuracy. As shown in Fig. 4a, removing any of the four stages, especially the joint stage and cool-down stage, results in a noticeable decline in test accuracy. The significance of the joint stage and cool-down stage stems from the fact that a significant knowledge transfer is conducted to retain the information lost when applying pruning and quantization.\nMoreover, each stage's contribution varies over downstream applications. For instance, the joint stage plays a more critical role when fine-tuning a pre-trained model compared to training from scratch. This can be attributed to the fact that pre-trained models inherently possess a wealth of useful knowledge, and the joint stage helps preserve performance by effectively transferring this knowledge under quantization constraints.\nIn addition, we perform an ablation study (See Fig. 4b) using ResNet56 on CIFAR10 benchmark to study the limit of each compression technique within GETA framework. As highlighted in [32], structured pruning methods typically achieve sparsity greater than 80%. However, under joint setup, accuracy begins to degrade significantly beyond 60% sparsity. This suggests quantization error constrains aggressive pruning, lowering the achievable sparsity threshold from 80% to 60% for ResNet56-CIFAR10. For quantization, satisfactory accuracy is typically retained with bit width > 2bits when sparsity \u2264 60%. When sparsity exceeds 60%, model becomes less tolerant to lower bit width, requiring at least 4-bit to retain performance."}, {"title": "7. Conclusion", "content": "We proposed GETA, an automatic framework designed to jointly apply structured pruning and quantization-aware training to deep neural networks, addressing key limitations of existing methods. By leveraging quantization-aware dependency graph analysis, GETA enables structured pruning and quantization-aware training across a wide range of architectures, including both CNNs and transformers. The proposed QASSO optimizer provides explicit control over bit width and sparsity, resolving black-box issues prevalent in existing approaches. With merits such as improved generalization, white-box optimization, and a one-shot framework, GETA offers an easy-to-use and user-friendly solution for practical deployment. In the future, it will be interesting to explore adapting GETA for specialized hardware to improve real-world deployment on different platforms."}, {"title": "A. Proof for Proposition 5.1", "content": "In this section, we present the proof for Proposition 5.1. For convenience, we restate the proposition as follows.\nProposition A.1. Let $\\nabla_x f$ be the full gradient of function $f(x, d, q_m, t)$ with respect to $x$. With forget rate \u03b3 selection rule Eq. (16) and quantization step size $d$ selection rule Eq. (17), the search direction $s(x)$ is a descent direction for the function $f$ with respect to $x$ at $x$.\nProof. Denote the full gradient of function $f (x, d, q_m, t)$ with respect to $x$ as $\\nabla_x f$. The search direction $s(x)$ is rewritten as\n$s(x) = \\begin{cases} -\\alpha [\\nabla_x f", "f": "g - \\gamma[\\hat{x}", "f\\": "g^T \\[\\nabla_x f\\", "right": "g^T \\left[-\\alpha [\\nabla_x f", "gamma[\\hat{x}": "g\\right"}, {"f": "g - \\gamma[\\hat{x}"}, {"f": "g - \\gamma[sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|)", "R(x)": "g.$\n$\\[s_{\\text{clip}} (x)\\", "f\\": "g$ and $-\\[sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|)\\", "x)\\": "g$ can be decomposed into two orthogonal vectors, i.e.,\n$\\[s_{\\text{clip}} (x)\\", "x)\\right": "g + \\left[\\tilde{s}_{\\text{clip}} (x)\\right"}, {"x)\\right": "g$ is orthogonal to vector $\\[\\nabla_x f\\"}, {"x)\\right": "g$ is parallel to vector $\\[\\nabla_x f\\"}, {"x)\\right": "g^T \\[\\nabla_x f\\"}, {"x)\\right": "g \\right| \\right|| = \\gamma \\sin {\\theta} \\left| \\left| \\left[ sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|) \\right"}, {"x)\\right": "g$ and vector $\\left[\\tilde{s}_{\\text{clip}} (x)\\right"}, {"x)\\right": "g \\right| \\right||^2 = \\left| \\left| \\left[s_{\\text{clip}} (x)\\right"}, {"x)\\right": "g \\right| \\right||^2$\n$= \\left| \\left| - \\alpha \\[\\nabla_x f\\", "x|)\\": "g \\right| \\right||^2 - \\gamma^2 \\sin^2 {\\theta} \\left| \\left| \\left[ sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|) \\right", "f\\": "g \\right| \\right||^2 + 2 \\alpha \\gamma \\left[ \\nabla_f(x) \\right", "right": "g + \\gamma^2 \\cos^2 {\\theta} \\left| \\left| \\left[ sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|) \\right"}, {"f\\": "g \\right| \\right|| + \\gamma \\cos {\\theta} \\left| \\left| \\left[ sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|) \\right", "right": 2, "x)\\right": "g$, we have $\\[s_{\\text{clip}} (x)\\", "x)\\": "g = \\frac{\\alpha \\left| \\left| \\[\\nabla_x f\\"}, {"right": "g \\right| \\right||}{\\left| \\left| \\[\\nabla_x f\\", "f\\": "g.$    (19)\nCombining the forget rate selection rule (16) and the expression (19) allows us to have that for $g \\in G_R$,\n$\\left[ \\nabla_x f \\right", "x)\\right": "g = \\left[ \\nabla_x f \\right"}, {"x)\\right": "g + \\left[\\tilde{s}_{\\text{clip}} (x)\\right", "right": "n$= \\left[ \\nabla_x f \\right"}, {"x)\\right": "g$\n$= -\\alpha \\left| \\left| \\[\\nabla_x f\\", "f\\": "g \\right| \\right|| \\left| \\left| \\left[ sgn(x) \\cdot clip_{\\frac{q_m}{d}} (|x|) \\right"}, {"f\\": "g \\right| \\right||^2.$\nFurther, our quantization step size $d$ selection rule (17) guarantees that"}]}