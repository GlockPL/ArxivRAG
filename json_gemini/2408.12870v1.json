{"title": "Can AI assistance aid in the grading of handwritten answer sheets?", "authors": ["Pritam Sil", "Parag Chaudhuri", "Bhaskaran Raman"], "abstract": "With recent advancements in artificial intelligence (AI), there has been growing interest in using state of the art (SOTA) AI solutions to provide assistance in grading handwritten answer sheets. While a few commercial products exist, the question of whether AI-assistance can actually reduce grading effort and time has not yet been carefully considered in published literature. This work introduces an AI-assisted grading pipeline. The pipeline first uses text detection to automatically detect question regions present in a question paper PDF. Next, it uses SOTA text detection methods to highlight important keywords present in the handwritten answer regions of scanned answer sheets to assist in the grading process. We then evaluate a prototype implementation of the AI-assisted grading pipeline deployed on an existing e-learning management platform. The evaluation involves a total of 5 different real-life examinations across 4 different courses at a reputed institute; it consists of a total of 42 questions, 17 graders, and 468 submissions. We log and analyze the grading time for each handwritten answer while using AI assistance and without it. Our evaluations have shown that, on average, the graders take 31% less time while grading a single response and 33% less grading time while grading a single answer sheet using AI assistance.", "sections": [{"title": "1 Introduction", "content": "Grading of handwritten answer sheets is a time consuming process. Traditionally, during the grading of handwritten answer sheets the grader has to go through each answer and grade it without any external assistance. On the other hand, recent advances in AI have shown a growing interest in using SOTA methods to provide assistance in this area. Presently, most commercial solutions today provide an easy-to-use user interface (UI) that the graders can use to upload their set of answer sheets, add rubrics and grade them. While some strictly restrict the question paper format and answer format in order to provide assistance, others relax these restrictions by only providing an easy-to-use UI (more has been discussed in Section 2). Moreover, none of these solutions provide AI-based assistance in grading short and long-answer type questions by highlighting certain keywords that will help them in grading such answers."}, {"title": "2 Related Work", "content": "The logistics involved in grading handwritten answer sheets can be quite challenging for courses with large enrollments. Gradescope [7] (GS) by Singh et al. allows the grader to manually demarcate the answer and roll number regions on a blank question paper, upload a set of scanned answer sheets and subsequently use this to highlight the whole answer region on the answer sheet. Gradescope supports AI based grouping of similar answers. However, the system does not crop out the answer regions; instead, it just places a bounding box to highlight the answer region for a question as demarcated by the instructor. Additionally, it does not provide any assistance while grading paragraph-type answers.\nSmart Authenticated Fast Exams (SAFE) by Chebrolu et al. [5] provides a secure solution to conduct examinations in a proctored environment while following a Bring Your Own Device (BYOD) model. SAFE provides a one stop"}, {"title": "3 AI Assisted Grading Pipeline", "content": "The AI-assisted grading pipeline, as shown in Figure 1, assumes that each question will have an answer region provided just below it, and the students will write their answers within this region. Also, the blank question paper must be available as a single PDF file at the start of the process. The first part of this pipeline automatically detects the question regions ([3][2]) present in the PDF file of a blank question paper uploaded by the instructor. Once the instructor is satisfied with the detected question bounding boxes and the corresponding text, they can be saved to the interface.\nOnce the examination is over, the instructor will have a set of scanned handwritten answer sheets. Note each such answer sheet is scanned to a separate PDF file. It is difficult to manually create a mapping between each such scanned PDF file to the corresponding student's name and roll number. Our pipeline partially automates this problem using AI. The instructor uploads this set of scanned answer sheets to our interface, and also marks the corresponding bounding box for name and roll number on one PDF. Once done, the system will automatically extract (perform word recognition) the text present in these bounding boxes. For this word recognition step, we use the docTR library as described earlier for question text extraction. Once each answer sheet has been mapped to a student's name and roll number, the mappings are shown to the instructor via a separate user interface. Incorrect mappings, if any, can be manually corrected by the instructor. Once done, the mappings are saved.\nNow, the system automatically deduces the handwritten answer regions from each answer sheet. The deduced answer region for a particular question is the region between the bottom part of the bounding box of that question and the top part of the bounding box of the next question. The system automatically crops out the handwritten answer region for a question for a particular student's answer sheet and displays it on the grading interface. This allows the grader to focus on the relevant region where the answer is present. The grader has the option to look at the entire page image, if needed."}, {"title": "4 Field evaluation of grading time reduction", "content": "This section describes the field evaluation of the AI-assisted grading pipeline described above."}, {"title": "4.1 Evaluation Method", "content": "A prototype implementation of the pipeline is used to grade real-life examinations at a reputed institute\u00b9. The handwritten answer sheets of the students are scanned and uploaded to the platform, and the answer regions were highlighted using the given keywords. The graders were then asked to grade their respective questions using the manual grading interface, and the time needed to grade each response was automatically recorded by the system. No timer was shown to the grader, but the time was logged by the system in a manner transparent to the grader. This was done to ensure a fair evaluation and not put any pressure on the grader."}, {"title": "4.2 Analysis at per response level (Course B)", "content": "During our evaluation process, the time taken to grade each response was recorded. Both the $S_{HNA}$ and $S_{HighlightAttempted}$ sets contained few grading time instances, which are quite high compared to the mean time recorded. We conjecture that these maybe cases where the grader took a break between grading one question and the next. These are considered to be outliers and are eliminated by removing the top 5% data points in a set."}, {"title": "4.3 Analysis at per answer sheet level", "content": "The time to grade an answer sheet is considered to be the total time taken to grade all the questions in that answer sheet. The average grading per answer sheet for each examination is analysed here."}, {"title": "5 Conclusion", "content": "We introduce an AI-assisted pipeline for grading of handwritten answer scripts. The system aids the grader by automatically detecting the question regions and text in the blank question paper. Subsequently, it extracts the name and roll number of students from scanned answer sheets and maps the scanned answer sheets to corresponding students. At grading time, it automatically crops out the handwritten answer regions and displays it on our manual grading interface and highlights important keywords in the answer. Extensive field evaluation of this pipeline reveals a 31% reduction in average grading time for each handwritten response and a 33% reduction in average grading time for entire answer sheets, when using the keyword highlight feature. Thus, AI-based assistance can definitely aid in the grading of handwritten answer sheets.\nThe current work only highlights keywords if there is an exact match. Future work involves but is not limited to using techniques from Natural Language Processing (NLP) to detect different forms or even synonyms of the keywords and eventually automatically grade such handwritten answers."}]}