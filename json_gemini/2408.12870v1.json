{"title": "Can AI assistance aid in the grading of handwritten answer sheets?", "authors": ["Pritam Sil", "Parag Chaudhuri", "Bhaskaran Raman"], "abstract": "With recent advancements in artificial intelligence (AI), there\nhas been growing interest in using state of the art (SOTA) AI solu-\ntions to provide assistance in grading handwritten answer sheets. While\na few commercial products exist, the question of whether AI-assistance\ncan actually reduce grading effort and time has not yet been carefully\nconsidered in published literature. This work introduces an AI-assisted\ngrading pipeline. The pipeline first uses text detection to automatically\ndetect question regions present in a question paper PDF. Next, it uses\nSOTA text detection methods to highlight important keywords present\nin the handwritten answer regions of scanned answer sheets to assist in\nthe grading process. We then evaluate a prototype implementation of\nthe AI-assisted grading pipeline deployed on an existing e-learning man-\nagement platform. The evaluation involves a total of 5 different real-life\nexaminations across 4 different courses at a reputed institute; it consists\nof a total of 42 questions, 17 graders, and 468 submissions. We log and\nanalyze the grading time for each handwritten answer while using AI\nassistance and without it. Our evaluations have shown that, on average,\nthe graders take 31% less time while grading a single response and 33%\nless grading time while grading a single answer sheet using AI assistance.", "sections": [{"title": "1 Introduction", "content": "Grading of handwritten answer sheets is a time consuming process. Traditionally,\nduring the grading of handwritten answer sheets the grader has to go through\neach answer and grade it without any external assistance. On the other hand,\nrecent advances in AI have shown a growing interest in using SOTA methods to\nprovide assistance in this area. Presently, most commercial solutions today pro-\nvide an easy-to-use user interface (UI) that the graders can use to upload their\nset of answer sheets, add rubrics and grade them. While some strictly restrict\nthe question paper format and answer format in order to provide assistance, oth-\ners relax these restrictions by only providing an easy-to-use UI (more has been\ndiscussed in Section 2). Moreover, none of these solutions provide AI-based as-\nsistance in grading short and long-answer type questions by highlighting certain\nkeywords that will help them in grading such answers."}, {"title": "2 Related Work", "content": "The logistics involved in grading handwritten answer sheets can be quite\nchallenging for courses with large enrollments. Gradescope [7] (GS) by Singh\net al. allows the grader to manually demarcate the answer and roll number\nregions on a blank question paper, upload a set of scanned answer sheets and\nsubsequently use this to highlight the whole answer region on the answer sheet.\nGradescope supports AI based grouping of similar answers. However, the system\ndoes not crop out the answer regions; instead, it just places a bounding box\nto highlight the answer region for a question as demarcated by the instructor.\nAdditionally, it does not provide any assistance while grading paragraph-type\nanswers.\nSmart Authenticated Fast Exams (SAFE) by Chebrolu et al. [5] provides a\nsecure solution to conduct examinations in a proctored environment while fol-\nlowing a Bring Your Own Device (BYOD) model. SAFE provides a one stop"}, {"title": "3 AI Assisted Grading Pipeline", "content": "The AI-assisted grading pipeline, as shown in Figure 1, assumes that each ques-\ntion will have an answer region provided just below it, and the students will\nwrite their answers within this region. Also, the blank question paper must be\navailable as a single PDF file at the start of the process. The first part of this\npipeline automatically detects the question regions ([3][2]) present in the PDF\nfile of a blank question paper uploaded by the instructor. Once the instructor is\nsatisfied with the detected question bounding boxes and the corresponding text,\nthey can be saved to the interface.\nOnce the examination is over, the instructor will have a set of scanned hand-\nwritten answer sheets. Note each such answer sheet is scanned to a separate PDF\nfile. It is difficult to manually create a mapping between each such scanned PDF\nfile to the corresponding student's name and roll number. Our pipeline partially\nautomates this problem using AI. The instructor uploads this set of scanned an-\nswer sheets to our interface, and also marks the corresponding bounding box for\nname and roll number on one PDF. Once done, the system will automatically\nextract (perform word recognition) the text present in these bounding boxes.\nFor this word recognition step, we use the docTR library as described earlier\nfor question text extraction. Once each answer sheet has been mapped to a stu-\ndent's name and roll number, the mappings are shown to the instructor via a\nseparate user interface. Incorrect mappings, if any, can be manually corrected\nby the instructor. Once done, the mappings are saved.\nNow, the system automatically deduces the handwritten answer regions from\neach answer sheet. The deduced answer region for a particular question is the\nregion between the bottom part of the bounding box of that question and the\ntop part of the bounding box of the next question. The system automatically\ncrops out the handwritten answer region for a question for a particular student's\nanswer sheet and displays it on the grading interface. This allows the grader to\nfocus on the relevant region where the answer is present. The grader has the\noption to look at the entire page image, if needed."}, {"title": "4 Field evaluation of grading time reduction", "content": "This section describes the field evaluation of the AI-assisted grading pipeline\ndescribed above."}, {"title": "4.1 Evaluation Method", "content": "A prototype implementation of the pipeline is used to grade real-life examina-\ntions at a reputed institute\u00b9. The handwritten answer sheets of the students are\nscanned and uploaded to the platform, and the answer regions were highlighted\nusing the given keywords. The graders were then asked to grade their respective\nquestions using the manual grading interface, and the time needed to grade each\nresponse was automatically recorded by the system. No timer was shown to the\ngrader, but the time was logged by the system in a manner transparent to the\ngrader. This was done to ensure a fair evaluation and not put any pressure on\nthe grader."}, {"title": "4.2 Analysis at per response level (Course B)", "content": "During our evaluation process, the time taken to grade each response was recorded.\nBoth the SHNA and SHighlightAttempted sets contained few grading time in-\nstances, which are quite high compared to the mean time recorded. We con-\njecture that these maybe cases where the grader took a break between grading\none question and the next. These are considered to be outliers and are elimi-\nnated by removing the top 5% data points in a set. Figure 3 shows the average"}, {"title": "4.3 Analysis at per answer sheet level", "content": "The time to grade an answer sheet is considered to be the total time taken to\ngrade all the questions in that answer sheet. The average grading per answer\nsheet for each examination is analysed here. Figure 4 shows the average grading"}, {"title": "5 Conclusion", "content": "We introduce an AI-assisted pipeline for grading of handwritten answer scripts.\nThe system aids the grader by automatically detecting the question regions and\ntext in the blank question paper. Subsequently, it extracts the name and roll\nnumber of students from scanned answer sheets and maps the scanned answer\nsheets to corresponding students. At grading time, it automatically crops out\nthe handwritten answer regions and displays it on our manual grading interface\nand highlights important keywords in the answer. Extensive field evaluation of\nthis pipeline reveals a 31% reduction in average grading time for each hand-\nwritten response and a 33% reduction in average grading time for entire answer\nsheets, when using the keyword highlight feature. Thus, AI-based assistance can\ndefinitely aid in the grading of handwritten answer sheets.\nThe current work only highlights keywords if there is an exact match. Future\nwork involves but is not limited to using techniques from Natural Language\nProcessing (NLP) to detect different forms or even synonyms of the keywords\nand eventually automatically grade such handwritten answers."}]}