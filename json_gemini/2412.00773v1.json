{"title": "DIVD: Deblurring with Improved Video Diffusion Model", "authors": ["Haoyang Long", "Yan Wang", "Wendong Wang"], "abstract": "Video deblurring presents a considerable challenge owing to the complexity of blur, which frequently results from a combination of camera shakes, and object motions. In the field of video deblurring, many previous works have primarily concentrated on distortion-based metrics, such as PSNR. However, this approach often results in a weak correlation with human perception and yields reconstructions that lack realism. Diffusion models and video diffusion models have respectively excelled in the fields of image and video generation, particularly achieving remarkable results in terms of image authenticity and realistic perception. However, due to the computational complexity and challenges inherent in adapting diffusion models, there is still uncertainty regarding the potential of video diffusion models in video deblurring tasks. To explore the viability of video diffusion models in the task of video deblurring, we introduce a diffusion model specifically for this purpose. In this field, leveraging highly correlated information between adjacent frames and addressing the challenge of temporal misalignment are crucial research directions. To tackle these challenges, many improvements based on the video diffusion model are introduced in this work. As a result, our model outperforms existing models and achieves state-of-the-art results on a range of perceptual metrics. Our model preserves a significant amount of detail in the images while maintaining competitive distortion metrics. Furthermore, to the best of our knowledge, this is the first time the diffusion model has been applied in video deblurring to overcome the limitations mentioned above.", "sections": [{"title": "Introduction", "content": "Video deblurring poses a longstanding and intricate challenge, which entails reviving successive frames amidst spatially and temporally fluctuating blurring effects. This endeavor is exacerbated by the inherent complexities introduced by camera shakes, moving objects, and depth variations within the exposure duration. To overcome this challenge, exploring how to utilize the highly correlated information among adjacent frames and addressing the misalignment between adjacent frames have become key directions.\nPrevious deblurring efforts have predominantly aimed for exceedingly high PSNR metrics by em-ploying L1 or L2 loss to minimize the discrepancy between the deblurred image and the ground truth. This approach often results in generated images with smoothly transitioning edges, as pixel values at the edges fluctuate significantly. Even minor errors can incur considerable penalties in these loss functions. We investigated the effects of varying levels of image smoothing on traditional distortion-based metrics and perceptual metrics. Specifically, as illustrated in Fig. 1, we analyze the influence of smoothing on PSNR, FID, and LPIPS. \"Base\" refers to the result from a single sampling. \"Sample average (SA)\" involves averaging multiple images generated by our model, denoted as SA-x, where x represents the number of images averaged. We observed that as x increases, resulting in smoother images, PSNR progressively improves, while FID and LPIPS performance deteriorates.\nIn conclusion, distortion-based metrics can be misleading regarding image smoothing. Highly smoothed images may achieve superior distortion metrics, such as PSNR and SSIM, while performing poorly on perceptual metrics. High distortion scores do not necessarily indicate that the smoothed image closely resembles the reference image, as human observers can easily detect discrepancies. Therefore, perceptual metrics must be factored into the overall evaluation of image restoration quality.\nTo leverage the high correlation between adjacent frames and address the challenge of temporal misalignment, we approach video deblurring from a novel angle, framing it as a conditional generative modeling problem and leveraging the video diffusion model [2] as the foundation. We propose an implicit method named Window-based Temporal Self-Attention (WTSA) for processing video frames in parallel using attention mechanisms. In WTSA, the use of attention to process long video sequences in parallel allows for comprehensive modeling of the input without loss of temporal information. This enables the implicit alignment and fusion of information from long-distance and misaligned frames. Furthermore, we introduce a joint positional encoding method called Multi-frame Relative Positional Encoding (MRPE), which provides complete positional information for WTSA, significantly boosting the model's performance. In summary, our main contributions are two-fold as follows:\n1) We highlight the limitations of conventional evaluation methods in image restoration and discuss how models can easily manipulate these metrics. We underscore the importance of incorporating perceptual metrics for more reliable assessments.\n2) We introduce the WTSA module alongside a joint positional encoding method termed MRPE. The WTSA module enables parallel processing of long video sequences while implicitly performing alignment and information fusion. MRPE supplies comprehensive spatial information to the WTSA module. Together, they markedly boost model performance. Our model achieves state-of-the-art results across various perceptual metrics in the task of video deblurring."}, {"title": "Related Work", "content": "Diffusion Probabilistic Models (DPMs), a powerful class of generative models originally proposed in [3], have garnered significant attention in the field of large-scale image and video synthesis [4, 5]. These models have consistently demonstrated remarkable effectiveness, as evidenced by numerous studies. In fact, DPMs have emerged as viable alternatives to other dominant generative models, such as Generative Adversarial Networks (GANs) [6] and Variational Autoencoders (VAEs) [7]. What sets DPMs apart is their ability to achieve both high diversity and fidelity in the generated images.\nTo leverage the powerful performance of diffusion models, Image-conditioned DPMs (icDPMs) have been proposed and widely utilized in various image restoration tasks such as super-resolution [8, 9] and deblurring [10, 11]. icDPMs take in degraded images y as input to produce high-quality samples corresponding to the degraded samples. In other words, they generate samples from the conditional distribution $p(x | y)$ (posterior). Typically, a conditional DPM $G_{\\theta} ([x_t, y], t)$ is used, where y and $x_t$ are concatenated along the channel dimension [8, 10]."}, {"title": "Video deblurring", "content": "Video deblurring poses a longstanding and intricate challenge, which entails reviving successive frames amidst spatially and temporally fluctuating blurring effects. This endeavor is exacerbated by the inherent complexities introduced by camera shakes, moving objects, and depth variations within the exposure duration. To overcome this challenge, exploring how to utilize the highly correlated information among adjacent frames and addressing the misalignment between adjacent frames have become key directions.\nTo handle information within adjacent frames, existing video restoration methods typically fall into three categories: sliding window-based methods [12, 13, 14, 15, 16, 17, 18, 19, 20], recurrent methods [21, 22, 23, 24, 25, 26, 27, 28, 29], and parallel methods [30, 31]. Sliding window-based methods restore one intermediate frame using multiple adjacent degraded video frames, processing the entire video by continuously moving the window. However, the overlap during window movement leads to significant computational costs. Recurrent methods utilize previously restored video frames\nas information for recovering subsequent frames. Due to its recurrent nature, the initial quality of restored frames is often poor, and training and inference with recurrent networks are linear, resulting in slower speeds. Moreover, recurrent methods suffer from rapid forgetting and struggle to propagate long-range information in processing long videos. Parallel methods simultaneously input multiple adjacent video frames, allowing information to flow among these frames to help synchronously restore multiple clear frames. Parallel methods can efficiently handle video frames in synchronization, without forgetting information between input long video frames. Such methods have achieved state-of-the-art performance in video deblurring tasks.\nDue to the high correlation but misalignment between consecutive video frames, it is often necessary to perform temporal alignment to leverage the correlated information across multiple frames. Traditionally, many methods [32, 33, 34, 12, 35, 36] use optical flow predicted from adjacent frames for image registration. However, using optical flow for alignment introduces model complexity and relies on manual priors. Zhu et al. [37] have demonstrated that optical flow or deformable convolution cannot estimate alignment information well when images face significant motion blur. A series of methods [37, 38, 39] using convolution to process video frames implicitly have been proposed. However, convolution faces small receptive fields and the inability to effectively capture long-range spatial dependencies."}, {"title": "Perceptual Metrics", "content": "Humans can quickly assess image similarity through high-level image structures [40] and context-dependent, a type known as perceptual similarity involving complex underlying processes. However, the widely used metric in image restoration, PSNR (Peak Signal-to-Noise Ratio), is a per-pixel measure that assumes pixel-wise independence. SSIM (Structural Similarity Index) [40] evaluates image similarity using simple shallow functions based on contrast, luminance, and structural similarity, failing to capture and reflect many nuances of human perception.\nTherefore, various perceptual metrics such as LPIPS [41], FID (Fr\u00e9chet Inception Distance) [42], and KID (Kernel Inception Distance) [43] have been proposed to assess image quality comprehensively. These perceptual metrics utilize deep models to extract deep features from images and compare the similarity at the feature level between different images. This feature-level similarity corresponds more closely to human perceptual judgments and performs better than metrics like SSIM.\nUnlike traditional pixel-level image similarity measurement methods, LPIPS focuses more on perceptual differences in images, making it more aligned with human subjective perception. Therefore, LPIPS is widely used in tasks such as evaluating image restoration quality [10, 11] and image style transfer [44]. FID was initially introduced for assessing the quality of images generated by GAN [45] models but has since been widely adopted for evaluating various image generation tasks [2, 46]. Unlike FID, KID measures the difference between two sets of samples without relying on biased empirical estimates, leading to more consistent alignment with human perception."}, {"title": "Method", "content": "The overall architecture of our model is depicted in Fig. 3. The model is based on a convolutional 2D UNet [47]. The input of dimension $F \\times H \\times W \\times C$ (6 channels) consists of concatenated noise (3 channels) and conditional frames (3 channels) along the channel dimension following [10, 11], where C represents the number of channels, F denotes the number of frames, and H and W represent the height and width of video frame respectively. The model processes the input parallelly and the output is the restored clear frames with the same shape as the input blur frames. All blocks in the UNet comprise a ResBlock [48] and four Window-based Temporal Attention modules (WTSA). ResBlock extracts the feature from each frame, and the WTSA module aligns and fuses misaligned but related features across all frames through self-attention operations within a window. Meanwhile, Multi-frame Relative Positional Encoding (MRPE) is incorporated into the WTSA module to provide complete positional information. Then, self-attention operations are conducted within a window of size $M \\times M$, allowing misaligned but related features to be aligned and fused."}, {"title": "Window-based Temporal Self-Attention (WTSA)", "content": "When the distance between two frames increases, video frames may suffer from misalignment issues, such as those caused by camera movement. In Fig. 3 (c), we show an example of misalignment: related features are in different positions in the first and second frames. To overcome the inherent misalignment problem in videos, we propose window-based temporal self-attention which computes self-attention within a window to assist the model in capturing corresponding information across different frames and merging them. This module is added after the ResBlock to process the features extracted by ResBlock as shown in Fig. 3 (b).\nWe define the feature after ResBlock as $Vector_{res} \\in R^{F\\times H\\times W\\times C}$, where the F, H, W, and C are the video frames, feature height, feature width and channel, respectively. As shown in Eq. 1, a window with size of $M \\times M$ is arranged to partition the feature in a non-overlapping manner evenly in the WTSA module to obtain the window vector $Vector_{win}$\n$Vector_{win} = rearrange (Vector_{res}, F, H,W,C, M)$\nWhere rearrange operation is defined as:\n$rearrange: F \\timesH\\timesW\\timesC \\rightarrow (\\frac{H}{M} \\frac{W}{M}) \\times (F\\times M \\times M) \\times C$\n$Vector_{win} \\in R^{N\\times(F\\times M^2)\\times C}$, where N i.e. $(\\frac{H\\timesW}{M})$ represents the total number of $Vector_{win}$.\nFeatures with dimensions of $M^2 x C$ (see more details in Fig. 4) are obtained from each different\nvideo frame and there are total F frames. These features, correlated information from multiple frames, undergo an improved self-attention operation, enabling the model to align and fuse features implicitly."}, {"title": "Multi-frame Relative Positional Encoding (MRPE)", "content": "To fully leverage the ability of the window-based temporal attention mechanism to capture long-range information dependencies, we introduce a technique termed Multi-frame Relative Positional Encoding. This approach integrates Multi-frame Positional Encoding with Relative Position Bias as shown in Fig. 4, providing complete positional information for the window-based temporal attention mechanism. The experiments show that adding multi-frame positional encoding or relative positional encoding can significantly improve the model's capability. Combining both into multi-frame relative positional encoding can further enhance the model's performance."}, {"title": "Multi-frame Positional Encoding", "content": "Because of the parallel processing nature of the attention mechanism, it struggles to comprehend the sequential or positional relationships within incoming information. For instance, when dealing with language processing tasks [49, 50, 51], incorporating absolute positional information for each word in the sentence greatly enhances the performance of the model. Similarly, there are temporal relationships between video frames, where frames farther apart often exhibit greater misalignment. Therefore, as shown in Fig. 3 (b), multiple-frame positional encoding is incorporated between the ResBlock and Attention module. This adds positional identifiers to the features extracted by ResBlock for each frame, enabling the subsequent self-attention operations in the WTSA module to effectively capture the temporal relationships."}, {"title": "Relative Position Bias", "content": "To account for the relative positional relationships among all features within the window of the WTSA module, following the work of [52, 53, 54, 55, 56], we first extend an image-based relative position bias $B_{img} \\in R^{M^2\\times M^2}$ to $B_{video} \\in R^{(F\\times M^2)\\times(F\\times M^2)}$ with Eq. 3 for the adaption to the structure\nof video data.\n$B_{video} = reshape(repeat(B_{img}, F^2))$\nThe repeat operation denote that it repeat $B_{img}$ for $F^2$ times resulting in a matrix $B \\in R^{F^2\\times M^2\\times M^2}$. Subsequently the reshape operation is employed to reshape it from $B \\in R^{F^2\\times M^2\\times M^2}$ to $B_{video} \\in R^{(F\\times M^2)\\times(F\\times M^2)}$.\nThen we add $B_{video}$ to the attention score matrix, which contains frame temporal information, to\nobtain a matrix that simultaneously contains frame temporal and relative positional information as\nshown in Fig. 4. Finally, the self-attention is deployed using $B_{video}$:\n$Attention(Q, K, V) = SoftMax (\\frac{Q K^T}{\\sqrt{D}} + B_{video}) V$\nwhere $Q, K, V \\in R^{M^2\\times D}$ are the query, key, and value matrices in the attention module; D is the query and key dimension, and $M^2$ is the number of feature vectors in a window."}, {"title": "Experiments", "content": ""}, {"title": "Data and Evaluation", "content": "We trained and evaluated our model on the GOPRO [1] and DVD [17] datasets following previous video deblurring work [31, 30, 29, 57, 58, 27]. The GOPRO dataset consists of 2,103 frames for training and 1,111 frames for testing. The DVD dataset includes 5,708 frames for training and 1,000 frames for testing. We evaluate our method on four different perceptual metrics: LPIPS [41], NIQE [59], FID (Fr\u00e9chet Inception Distance) [42], and KID (Kernel Inception Distance) [43]. We also employ distortion-based metrics PSNR and SSIM [40].\nThe reason we focus on perceptual metrics rather than traditional distortion-based metrics is that according to [41], even if two images are very close at the pixel level, human observers may still perceive them as different. To overcome the limitations of traditional methods, perceptual evaluation techniques such as FID, KID, and LPIPS extract high-dimensional features (texture, semantics, etc.) from images using pre-trained models, and then compute the distribution distance between generated images and reference images. Besides, it is worth noting that our test set lacks a sufficient number of images to compute FID and KID. Therefore, similar to [60], each sampled image is segmented into 15 non-overlapping patches of size 240x240, and FID and KID are computed at the patch level. For readability, following [10], the KID metric is scaled up by a factor of 1000."}, {"title": "Implementation Details", "content": "The UNet [47] channel numbers are set to [64, 128, 256] for three stages, with two blocks (Fig. 3 b) in each stage. Each block consists of one ResBlock and four different WTSA modules, with window sizes of [6, 4, 3, 2]. A ResBlock contains one Group Normalization Layer [61] with group size 8, two Convolutional Layers, and one nonlinear activation function (Swish) [62], the Frame absolute positional encoding is incorporated into it.\nOur base model is implemented using PyTorch and trained on 8 V100 GPUs for 12 days. We employ a linear warm-up of the learning rate from 0.000001 to 0.0001 over 5,000 steps, followed by cosine annealing [63] back to the initial learning rate, for 1, 000, 000 steps following [11]. The Adam [64] optimizer with $\\beta_1$ = 0.9, $\\beta_2$ = 0.999 is deployed for this model. As for the input of our model, consecutive 4 frames are selected per iteration with batch size 24, each frame is randomly cropped to a 144 \u00d7 144 region for input. We utilize the DDPM [65] with T set to 1,000 steps, the initial $\\beta$ is set to 0.000001, and the last $\\beta$ is set to 0.01 with a linear $\\beta$ schedule."}, {"title": "Deblurring Results", "content": "Tab. 1 and Tab. 2 demonstrate the strong competitiveness of our model compared to other models on the GoPro and DVD datasets, respectively. Thanks to the advantages of the diffusion model, our model not only achieves state-of-the-art (SOTA) results far surpassing other models in human perception metrics but also exhibits great performance in distortion-based metrics.\nFig. 5 6 7 visually demonstrate the advantages of our model on the GoPro and DVD datasets. Many previous works [31, 30, 29, 57, 58, 27] have overly focused on PSNR and SSIM, two distortion-based metrics, resulting in images generated by models that are excessively smooth and lack significant details, leading to poor human perceptual quality. Our models can preserve a large amount of detail in images to achieve the best perceptual metrics while maintaining high distortion-based metrics, thereby greatly enhancing the realism of the images."}, {"title": "Ablation Study", "content": "In this section, we investigate the impact of the proposed modules on model performance. All ablation experiments are conducted on the GoPro dataset, trained for 1 million steps, with input randomly cropped to 96 \u00d7 96 and a batch size of 18. We set the initial channel of UNet to 54. And we adopt the DDIM sampler [69] using 50 steps to accelerate sampling speed. The results are reported in Tab. 3"}, {"title": "Effects of Window-based Temporal Self-Attention", "content": "We conducted tests on a smaller model to further investigate the impact of window size on our model in Tab. 4. We set the initial channels of the model to 32 and experimented with different window size combinations. The tests were performed on the GoPro [1] dataset. The training batch size was set to 4, with random cropping of inputs to 48 \u00d7 48, and a frame length of 4. The models are trained for 300, 000 steps.\nExpanding the window gradually can enhance the performance of the model. This is because performing attention within the window not only captures temporal information but also establishes\nspatial connections. Simultaneously aggregating and integrating related information from different video frames significantly strengthens the model's representation capability, effectively leveraging the temporal correlations present in the video data."}, {"title": "Effects of single position encoding", "content": "There are two ablation experiments to explore the contributions of relative positional encoding and multi-frame positional encoding to the WTSA module, with results shown in the third and fourth rows of Tab. 3. We found that with individual positional encoding, the model's performance is significantly improved compared to using the WTSA module alone. This is because the parallel processing capability of the attention mechanism prevents it from obtaining positional or sequential information. By using positional encoding, the model can further understand the relationships between features, thereby improving its performance. This indicates that providing positional information to the WTSA module aids in aligning and fusing information within the window."}, {"title": "Effects of Multi-frame Relative Positional Encoding", "content": "We explore the effect of Multi-frame Relative Positional Encoding. The joint positional encoding combines two individual positional encodings, providing complete positional information for the WTSA module, thereby identifying features within the window across different frames and positions. By incorporating joint positional encoding, we provide the model with complete positional information. The sequence and relative proximity of all features to be processed are identified by positional encoding, greatly simplifying the learning task for the model. As a result, the performance of the model is further improved. This suggests that comprehensive and multi-dimensional positional information can further enhance information communication within the model."}, {"title": "Conclusion", "content": "In this paper, we first introduce the video diffusion model to debluring task and improve video diffusion model by incorporating a Window-based Temporal Self-Attention (WTSA) module and Multi-frame Relative Position Encoding (MRPE). By using attention mechanisms to process input video frames in parallel, we overcame the high computational demands of frame-sliding window methods and the forgetting issues associated with RNN-based approaches. By employing windows within the attention mechanism, we surpassed the limitations of traditional methods, achieving implicit frame alignment, and leveraging the MRPE module to notably enhance the performance of the WTSA module. Additionally, we discussed the relationship between perceptual metrics and distortion metrics, emphasizing the importance of perceptual metrics in evaluating image restoration. Our model achieves state-of-the-art (SOTA) performance in the field of video deblurring on perceptual metrics, while maintaining competitive performance on distortion-based metrics.\nOur approach still has limitations. In Fig. 1 we made the images generated by our model as smooth as possible, but the distortion-based metric (PSNR) we achieved is still about 1.8 dB behind the current SOTA methods. Therefore, we believe there is still room for improvement in this aspect of our method. Moreover, due to the special nature of diffusion models, the inference speed of our model is slower than that of traditional models."}, {"title": "Appendix", "content": ""}, {"title": "The impact of smoothing on evaluation metrics", "content": "We explored the impact of different levels of image smoothing on traditional distortion-based evaluation metrics as well as perceptual metrics. Specifically, as shown in Fig. 1, we demonstrate the influence of image smoothing on PSNR, FID, and LPIPS. \"Base\" refers to the result of a single sampling. \"Sample average (SA)\" refers to taking the average of images generated by our model through multiple samplings, denoted as SA-x, where x indicates the number of images averaged. We observed that as x increases, i.e., as the images become smoother, PSNR gradually improves while the performance of FID and LPIPS gradually deteriorates.\nIn summary, distortion-based metrics can be deceived by image smoothing. A very smooth image may obtain high distortion metrics, such as PSNR and SSIM, and low perceptual metrics. High distortion metrics do not necessarily mean that the smoothed image is very close to the reference image; human observers can easily notice differences. Therefore, perceptual metrics should be included in the overall consideration of image restoration."}, {"title": "Compared with generative models", "content": "We compared the deblurring performance of our model with other generative models on the GOPRO dataset in Tab. 5. These models only conduct deblurring task on a single image. Our model surpasses these models on all matrices."}, {"title": "How to evaluate other methods on perceptual metrics", "content": "Since we need to compare perceptual metrics in Tab. 1 and Tab. 2, and the models being compared only provide PSNR and SSIM, to ensure fairness in our comparisons, we use the official open-source code and pre-trained weights provided by the authors of these models to perform image restoration tasks and use open-source library functions to calculate these perceptual metrics."}, {"title": "Additional Visual Results", "content": "In Figures 8 - 10 we present additional results on the GoPro dataset [1] and Figures 11 we present additional results on the DVD dataset [17] where we compare our diffusion deblurring method with GShift-Net [31], PVDNet [27] and RVRT [29]."}]}