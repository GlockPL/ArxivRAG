{"title": "Weighted Diversified Sampling for Efficient Data-Driven\nSingle-Cell Gene-Gene Interaction Discovery", "authors": ["Yifan Wu", "Yuntao Yang", "Zirui Liu", "Zhao Li", "Khushbu Pahwa", "Rongbin Li", "Wenjin Zheng", "Xia Hu", "Zhaozhuo Xu"], "abstract": "Gene-gene interactions play a crucial role in the manifestation of complex human diseases.\nUncovering significant gene-gene interactions is a challenging task. Here, we present an innovative\napproach utilizing data-driven computational tools, leveraging an advanced Transformer model,\nto unearth noteworthy gene-gene interactions. Despite the efficacy of Transformer models, their\nparameter intensity presents a bottleneck in data ingestion, hindering data efficiency. To mitigate\nthis, we introduce a novel weighted diversified sampling algorithm. This algorithm computes\nthe diversity score of each data sample in just two passes of the dataset, facilitating efficient\nsubset generation for interaction discovery. Our extensive experimentation demonstrates that by\nsampling a mere 1% of the single-cell dataset, we achieve performance comparable to that of\nutilizing the entire dataset.", "sections": [{"title": "Introduction", "content": "Gene-gene interactions play a crucial role in the manifestation of complex human diseases, including\nmultiple sclerosis [Brassat et al., 2006, Motsinger et al., 2007, Slim et al., 2022], pre-eclampsia [Li et al.,\n2022, Diab et al., 2021, Williams and Pipkin, 2011, Oudejans and Van Dijk, 2008], and Alzheimer's\ndisease [Ghebranious et al., 2011, Hohman et al., 2016]. Computational tools equipped with machine\nlearning (ML) prove effective in uncovering these significant gene interactions [McKinney et al.,\n2006, Cui et al., 2022, Yuan and Bar-Joseph, 2021b, Wei et al., 2024, Upstill-Goddard et al., 2013].\nBy learning an ML model on massive single-cell transcriptomic data, we can identify gene-gene\ninteractions associated with complex but common human diseases. Existing models rely on prior\nknowledge such as transcription factors (TF) [Wang et al., 2019, Yuan and Bar-Joseph, 2021a, Chen\net al., 2021a, Shu et al., 2021] or existing gene-gene interaction (GGI) networks [Ata et al., 2020,\nYuan and Bar-Joseph, 2019a], to infer new relationships. Although GGI networks and TFs are\ncrucial for mapping biological processes, they frequently suffer from high false-positive rates and\nbiases, particularly in large-scale in vitro experiments [Mahdavi and Lin, 2007, Rasmussen and et al.,\n2021]. In response to these challenges, we propose that gene-gene interactions can be uncovered\nusing purely data-driven methods."}, {"title": "The Rise of Transformers on Single-Cell Transcriptomic Data", "content": "Recent advances in\nnatural language processing, particularly the development of Transformer models [Vaswani et al.,\n2017], have demonstrated significant potential in biological data analysis [Hao et al., 2023, Theodoris\net al., 2023, Bian et al., 2024, Cui et al., 2024]. Transformer models are known for their ability to\ncapture the dependencies between gene expressions. The information fused through the self-attention\nmechanism [Vaswani et al., 2017] is particularly suited for analyzing the intricate relationships in\nsingle-cell transcriptomic data. On the other hand, Transformer models also demonstrated superior\nperformance when we scaled up their parameter size [Hao et al., 2023]. This scaling capacity raises\nthe researcher's interest in training and deploying parameter-intensive Transformer models, denoted\nas single-cell foundation models [Cui et al., 2024]. We would like to take this advantage for better\ngene-gene interaction discovery by identifying feature interactions within Transformer models."}, {"title": "Data-Driven Gene-Gene Interaction via Attention", "content": "In this work, we would like to advance\nthe gene-gene interaction discovery with the Transformer models that have demonstrated superior\nperformance on single-cell transcriptomic data. We see the self-attention mechanism [Vaswani et al.,\n2017] as a pathway to facilitate the modeling of gene-gene interactions. In single-cell foundation\nmodels, the input to the model is a bag of m gene expressions for a single cell. Next, in each layer\nand each head of the Transformer, there will be an attention map with shape $m \\times m$ generated for\nthis cell. Each entry of this attention map represents the interaction between two genes in this layer\nand this head. Assuming that we have a perfect Transformer that takes a cell gene expressions and\ncorrectly predicts if it is infected by a disease, we view the attention map of this cell as a strong\nindicator of disease-oriented gene-gene interactions."}, {"title": "Efficiency Challenge in Data Ingestion", "content": "Despite the transformative capabilities of Trans-\nformer models, one significant challenge remains: the efficient ingestion and processing of massive\nvolumes of single-cell transcriptomic data. We are utilizing Transformer models with parameter sizes\nthat exceed the hardware capacity, particularly that of the graphics processing unit (GPU). As a\nresult, given a pre-trained Transformer, we have to perform batch-size computation on a massive\nsingle-cell transcriptomic dataset for computing gene-gene interactions through attention maps.\nThis batch-size computation significantly enlarges the total execution time for scientific discovery.\nMoreover, the hardware in the real-world deployment environment for gene-gene interaction detection\nmay have even more limited resources. Therefore, the current computational framework cannot\nsupport gene-gene interaction discovery on real-world single-cell transcriptomic datasets."}, {"title": "Our Proposal: Two-Pass Weighted Diversified Sampling", "content": "In this paper, we introduce a\nnovel weighted diversified sampling algorithm. This randomized algorithm computes the diversity\nscore of each data sample in just two passes of the dataset. The proposed algorithm is highly\nmemory-efficient and requires constant memory that is independent of the cell dataset size. Our\ntheoretical analysis suggests that this diversity score estimates the density of the Min-Max kernel\ndefined on the cell-level gene expressions, which provides the foundation and justification of the\nproposed strategy. Through extensive experiments, we demonstrate how the proposed sampling\nalgorithm facilitates efficient subset generation for interaction discovery. The results show that by\nsampling a mere 1% of the single-cell dataset, we can achieve performance comparable to that of\nutilizing the entire dataset."}, {"title": "Our Contributions", "content": "We summarize our contributions as fellows.\n\u2022 We introduce a computational framework that advances the discovery of significant gene-gene\ninteractions with CelluFormer, our proposed Transformer model that is trained on single-cell\ntranscriptomic data.\n\u2022 We pinpoint the challenge in data ingestion for the data-driven gene-gene interaction. Moreover, we\nargue that we should perform diversified sampling that selects a representative subset of single-cell\ntranscriptomics data to fulfill the objective."}, {"title": "Data-Driven Single-Cell Gene-Gene Interaction Discovery", "content": "In this section, we propose a computing framework to perform gene-gene interaction discovery on\nsingle-cell transcriptomic data. We start by introducing the format of single-cell transcriptomic\ndata. Next, we propose the formulation of our CelluFormer model tailored to single-cell data. Next,\nwe present our multi-cell-type training to build an effective transformer model on single-cell data.\nFinally, given a pre-trained transformer, we showcase how to perform gene-gene interaction discovery\nby analyzing the attention maps."}, {"title": "Single-Cell Transcriptomic Data", "content": "Single-cell transcriptomic is a technology that profiles gene expression at the individual cell level.\nThe profiled results, namely single-cell transcriptomic data, provide a unique landscape of gene\nexpressions. In contrast to traditional bulk RNA-seq analysis, single-cell transcriptomic data allows\nfor cell-level sequencing, which captures the variability between individual cells [Ata et al., 2020].\nLeveraging this high-resolution data allows scientists to gain insights into developmental processes,\ndisease mechanisms, and cellular responses to environmental changes.\nThe single-cell transcriptomic data can be formulated as a set of high-dimensional and sparse\nfeature vectors. We denote a single-cell transcriptomic dataset at X, where each cell $x \\in X$ is a sparse\nvector with dimensionality $V \\in N_+$. Here V represents the total number of genes we can observe in\nX. Since cell $x \\in R^V$ is a sparse vector, we can represent $x$ as a set $\\{(i_1, v_1), (i_2, v_2),\\cdots, (i_k, v_k)\\}$.\nIn this set, every tuple (i, v) represents the expression of gene $i \\in [V]$ with expression level $v \\in R$.\nBesides we can also denote cell x as $[x_1,x_2,\\ldots,x_V]$, where most of the $x_i$s are zeros.\nIn this data formulation, single-cell transcriptomic data for each cell is represented as a set of\ngene expressions, with different cells expressing varying genes. Additionally, even when two cells\nexpress the same gene, their expression levels may differ. Our research objective is to identify\ngene-gene interactions within the vocabulary V that drive complex biological processes and disease\nmechanisms."}, {"title": "CelluFormer: A Single-Cell Transformer", "content": "Here, we propose our Transformer architecture, CelluFormer, to learn gene-gene interactions within\nsingle-cell transcriptomic data. Based on the set formulation of single-cell transcriptomic data, we\nbelieve that the order of genes is arbitrary and biologically meaningless. Similar to scGPT [Cui\net al., 2024], and scFoundation [Hao et al., 2024], our method adopts a permutation-invariant design.\nWe define our permutation-invariant condition as follows.\nCondition 2.1. Let X denote a single-cell transcriptomic dataset. Given a single-cell data of cell\n$x \\in X$, denoted as a set $\\{(i_1, v_1), (i_2, v_2), \\cdots, (i_k, v_k)\\}$, a function $f : X \\rightarrow R$ should satisfy that, for\nany permutation $\\pi$, $f(x) = f(\\pi(x))$."}, {"title": "Multi-Cell-Type Training of CelluFormer", "content": "We observe that there is a significant\nperformance difference between Trans-\nformer models if we feed them with\ndifferent styles of single-cell transcrip-\ntomic data. It is known that cells\ncan be categorized into different types\nbased on their functionality. For in-\nstance, neuronal cells represent the cell\ntypes that fire electric signals called\naction potentials across a neural net-\nwork [Levitan and Kaczmarek, 2015].\nOur study suggests that Transformers\nshould be trained on single-cell transcriptomic data from various cell types to achieve better perfor-\nmance. We showcase an example in Table 1. We train a Transformer model to classify whether a\ncell is an Alzheimer's disease-infected cell or not. According to our study, CelluFormer proposed\nin Section 2.2 trained on neuronal cells outperforms traditional multilayer perceptron (MLP) with\ndownstream training on a single cell type. However, we do not see this gap when we perform training\nof CelluFormer on a single cell type. As a result, we see that the Transformers generally prefer\nmassive exposure to the single-cell transcriptomic data."}, {"title": "Gene-Gene Interaction Discovery via Attention Maps", "content": "In this paper, we would like to accomplish the following objective.\nObjective 2.2 (Gene-gene interaction discovery). Let X denote a single-cell transcriptomic dataset.\nLet V denote the genes expressed in at least one $x \\in X$. Let $f : X \\rightarrow R$ denote a permutation\ninvariant (see Condition 2.1) CelluFormer. f can successfully predict whether any $x \\in X$ is infected\nby disease D. We would like to find a gene-gene pair $(v_1, v_2)$ that contributes the most to f's\nperformance in X. Here $V_1, V_2 \\in V$."}, {"title": "Weighted Diversified Sampling", "content": "In this section, we start by showcasing the data-efficiency problem when we use the trained Cellu-\nFormer for gene-gene interaction discovery. Following this, we define a diversity score for each cell\nin the dataset and propose a two-pass randomized algorithm to efficiently compute it. Lastly, we\npropose a weighted diversified sampling strategy on massive single-cell data."}, {"title": "Data-Intensive Computation for Gene-Gene Interaction Discovery", "content": "As illustrated in Section 2.4, once we have a pre-trained CelluFormer that can successfully predict\nwhether a cell is infected by a disease or not with its gene expressions, we can perform gene-gene\ninteraction discovery by passing massive cells into this model and get the accumulated attention map\nas Figure 3. However, this process requires data-intensive computation. For every cell in the dataset,\nwe first need to compute the average attention map as illustrated in Figure 2. Next, we perform\naggregations as shown in Figure 3. It is known that CelluFormer uses plenty of trainable parameters\nto achieve good disease infection classification performance. As a result, the computation complexity\nfor generating a cell's averaged attention map is expensive. Moreover, since the attention map for\ncell x is $m \\times m$, where m is the number of genes expressed in x. According to Figure 1, we see that\nm can be 12,000 or more. These giant attention maps consume the limited high bandwidth memory\n(HBM) in the graphics processing unit. Therefore, we have to perform batch-wise computation on a\nmassive cell dataset for computing gene-gene interactions. Moreover, given the scale of the dataset,\nany sampling algorithm with a runtime that grows exponentially with the dataset size is impractical."}, {"title": "Two-Pass Randomized Algorithm for Computing Min-Max Density", "content": "In this work, we would like to address this data-efficiency challenge by raising and asking the\nfollowing research question: Can we find a representative and small subset from the large cell dataset"}, {"title": "Two-Pass Algorithm for Estimating Min-Max Density", "content": "Input: Cell dataset X, 0-bit CWS function family H (see Definition 3.3), Hash range B, Rows R\nOutput: Min-Max density set w for every $x \\in X$.\nInitialize: $A \\leftarrow 0^{R \\times B}$\nGenerated R independent 0-bit CWS functions $h_1,\\ldots, h_R$ from H with range B at Random.\n{We set $R = O(\\log |X|)$ following the theoretical analysis of Definition 3.3}\n$W \\leftarrow \\{\\}$\nfor $x \\in X$ do\nfor $r = 1 \\rightarrow R$ do\n$A_{r,h_r(x)}+ = 1$\nend for\nend for\nfor $x \\in X$ do\nfor $r = 1 \\rightarrow R$ do\n$w_x \\leftarrow w_x + A_{r,h_r(x)}$\nend for\n$w_x \\leftarrow w_x/R$ {$w_x$ is the estimated Min-Max density for x.}\n$W \\leftarrow \\{w_x\\}$\nend for\nreturn W\nand still perform successful gene-gene interaction discovery? Moreover, we would like the procedure\nfor finding this small subset as efficient as possible.\nWe would like to answer this question by proposing a diversity score of a cell in the dataset. To\nbegin with, we introduce the Min-Max similarity between two cell's gene expressions."}, {"title": "Min-Max Similarity", "content": "Given two cell's gene expressions, denoted as $x, y \\in R^V$ (see\nSection 2.1), we define their Min-Max similarity as: $Min-Max(x, y) = \\frac{\\sum_{i=1}^V min(x_i,y_i)}{\\sum_{i=1}^V max(x_i,y_i)}$\nAccording to the definition, Min-Max(x, y) $\\in [0,1]$. Higher Min-Max means that two cell's gene\nexpressions are closer to each other. Min-Max is widely viewed as a kernel [Li, 2015b, Li et al., 2021,\nLi and Li, 2021] in statistical machine learning. In this paper, we would like to define a kernel\ndensity on top of the Min-Max similarity."}, {"title": "Min-Max Density", "content": "Given a cell dataset X, for every $q \\in X$, we define its Min-Max\ndensity as: $K(q) = \\sum_{x \\in X} \\varphi(q, x)$, where $\\varphi(q, x) : R \\rightarrow R$ is a monotonic increasing function along\nwith Min-Max(q, x) similarity defined in Definition 3.1.\nWe view Min-Max(q) density as an indicator of how diverse q is in X. Smaller Min-Max(q) means\nthat all other $x \\in X$ may be less similar to q, making q a unique cell. On the other hand, higher\nMin-Max(q) means that X has some cells that have similar gene expressions with q, making q less\nunique. However, to compute Min-Max(q) for every $q \\in X$ following Definition 3.2, we have to\ncompute all pairwise Min-Max(x, y) for any $x, y \\in X$, which results in an unaffordable $O(n^2NNZ(X))$\ntime complexity, where n is the size of X and NNZ(X) is the maximum possible number of genes\nexpressed in a cell $x \\in X$. To reduce this $n^2$ computation, we propose a randomized algorithm that\ntakes advantage of 0-bit consistent weighted sampling (CWS) [Li, 2015a] hash functions."}, {"title": "0-bit Consistent Weighted Sampling Hash Functions", "content": "Let H denote a randomized hash function family. If we pick a $h\\in H$ at random, for any two cell\nexpressions $x, y \\in R^V$, we have Pr[h(x) = h(y)] = Min-Max(x, y) + o(1). Here every $h\\in H$ is a hash\nfunction that maps any $x \\in X$ to an integer in [0, B). We denote B as the hash range.\nHere the o(1) is a minor additive term with complex form. For simplicity, we refer the readers to\n[Li et al., 2021], Theorem 4.4 for more details.\nThis work presents an efficient randomized algorithm that estimates Min-Max density K(q) (see\nDefinition 3.2) for every $q \\in X$. As showcased in Algorithm 1, we initialize an array A with all\nvalues as zeros. Next, we conduct a pass over X. In this pass, for every $x \\in X$, we compute its hash\nvalues after R independent hash functions. Next, we increment $A_{r,h_r(x)}$ with 1. After this pass, we\ntake another pass at the dataset, for every $x \\in X$, we take an average over the $A_{r,h_r(x)}$ and build\na density score $w_x$. We would like to highlight that Algorithm 1 requires only two linear scans of\nthe dataset. The time complexity for this algorithm is O(nNNZ(X)), which is linear to the dataset.\nMoreover, we show that Algorithm 1 produces an estimator to Min-Max density."}, {"title": "Min-Max Density Estimator, informal version of Theorem B.1", "content": "Given a cell dataset X,\nfor every $q \\in X$, we compute $w_q$ following Algorithm 1. Next, we have $E[w_q] = \\sum_{x \\in X}(Min-Max(x, q)+\no(1))$, where Min-Max is the Min-Max similarity defined in Definition 3.1. As a result, $w_q$ is an\nestimator for Min-Max density K(q) defined in Definition 3.2 with $\\varphi(q, x) = Min-Max(x, q) + o(1)$."}, {"title": "Weighted Diversified Sampling with Inverse Min-Max Density", "content": "We propose to use the inverse form of Min-Max density in Definition 3.2 as a score for diversity. We\ndefine it as normalized inverse Min-Max density as below."}, {"title": "Inverse Min-Max Density", "content": "Given a cell dataset X, for every $q \\in X$, we\ndefine its normalized inverse Min-Max density as $I(q) = Softmax(1/K(q))$, where K(q) is the Min-Max\ndiversity for q in Definition 3.2, Softmax is the softmax function that takes over all cells in X.\nWe view the IMD $I(q) \\in [0,1]$ as a monotonic increasing function for the diversity of q. Higher\nI(q) means that all other $x \\in X$ may be less similar to q, making q a unique cell. Moreover, IMD can\nbe directly used as a sample probability to generate a representative subset of X for Objective 2.2.\nGiven X, we perform sampling without replacement to generate a subset $X_{sub} \\subset X$, where $x \\in X\nhas the sampling probability I(x). The advantages of sampling with IMD (see Definition 3.5) can\nbe summarized as follows.\n\u2022 The IMD I(q) can be an effective indicator for how diverse q is in dataset X.\n\u2022 Computing IMD is an efficient one-shot preprocessing process with just two linear scans of X with\ntime complexity O(nNNZ(X)), where n and NNZ(X) is defined in Section 3.2.\n\u2022 The memory complexity of computing IMD is O(RB), which can be viewed as constant since it is\nindependent of n and NNZ(X)."}, {"title": "Estimated Interaction Score with WDS", "content": "Let $Z_x(v_i, v_j)$ denote the interaction value\nof gene $v_i$ and $v_j$ in the average attention map of cell x obtained in the CelluFormer. For dataset X,\nwe perform a sampling where each cell $x \\in X$ is sampled with probability I(x) (see Definition 3.5)\nand get a subset $X_s$. Next, we define the estimated interaction score between gene $v_i$ and $v_j$ learned\nfrom X as:\n$\\hat{Z}(v_i, v_j) = \\frac{\\sum_{x \\in X_s} Z_x(v_i, v_j) \\cdot I(x)}{\\sum_{x \\in X_s} I(x)}$\nwhere $\\hat{Z}(v_i, v_j)$ is an unbiased estimator for the expectation of $Z(v_i, v_j)$ in distribution with density\nI(x). Formally,\n$E[\\hat{Z}(v_i, v_j)] = E_{x \\sim I(x)}[Z_x(v_i, v_j)]$,\n$Var[\\hat{Z}(v_i, v_j)] = \\frac{\\sum_{x \\in X} I(x)^2}{(\\sum_{x \\in X} I(x))^2} Var_{z \\sim I(x)} [Z_z (v_i, v_j)].$"}, {"title": "Experiment", "content": "In this section, we want to validate the effectiveness of our gene-gene interaction pipeline as well as\nthe two-pass diversified sampling algorithm 1. There are a few research questions we want to answer:\n\u2022 RQ1: How does the proposed Transformer-based computing framework introduced in Section 2\nperform in gene-gene interaction discovery?\n\u2022 RQ2: How does the Min-Max density estimated by two-pass diversified sampling Algorithm 1\ncharacterize the diversity of a cell in the whole dataset? Is this estimated Min-Max density useful?\n\u2022 RQ3: How does the estimated Min-Max density perform in improving data-efficiency of gene-gene\ninteraction discovery? How is the quality of the subset sampled according to the estimated\nMin-Max density?"}, {"title": "Settings", "content": "Dataset: For the training dataset, we employ the Seattle Alzheimer's Disease Brain Cell Atlas\n(SEA-AD) [Gabitto et al., 2023], which includes single nucleus RNA sequencing data of 36,601 genes\n(as 36,601 features) from 84 senior brain donors exhibiting varying degrees of Alzheimer's Disease\n(AD) neuropathological changes. By providing extensive cellular and genetic data, SEA-AD enables\nin-depth exploration of the cellular heterogeneity and gene expression profiles associated with AD.\nTo facilitate a comparative analysis between AD-affected and non-AD brains, we select cells from 42\ndonors classified within the high-AD category and 9 donors from the non-AD category, based on\ntheir neuropathological profiles. This selection criterion ensures a robust comparison, aiding in the\nidentification of gene-gene interactions linked to AD progression [Gabitto et al., 2023]. The dataset\nis comprehensively annotated, covering 1,240,908 cells across 24 distinct cell types. We selected\n18 neuronal cell types as our final training dataset since we believe neuronal cells are more likely\nto reveal explainable gene-gene interactions that are related to Alzheimer's Disease compared to\nnon-neuronal cells. To better detect expression relationships among genes, we apply the Seurat\nTransformation Function [Stuart et al., 2019] to eliminate the problem of sequence depth difference.\nModel: For the SEA-AD dataset, we designed a CelluFormer model as explained in 2.2 to predict\nlabels indicative of Alzheimer's disease conditions. Further details can be found in the Appendix\nC.1.\nBaselines: Our proposed algorithm leverages the attention maps of the Transformer models.\nAccordingly, we compare our method with three statistical methods, Pearson Correlation, CS-CORE,\nand Spearman's Correlation [Freedman et al., 2007, Su et al., 2023, De Smet and Marchal, 2010]."}, {"title": "The Effectiveness of Transformers in Gene-Gene Interaction Discovery (RQ1)", "content": "To evaluate the effectiveness of our Transformer-based framework for gene-gene interaction discovery,\nwe performed feature selection across seven different cell types used as inference datasets. Additionally,\nwe used a dataset encompassing all neuronal cell types to assess the overall performance of various\nmodels. As shown in Table 2, Transformer-based methods, including CelluFormer, scGPT and\nscFoundation, significantly outperformed other baselines.\nThis result indicates that our proposed Transformer-based framework is more effective and stable\nat extracting general and global gene-gene interaction information. In addition, the foundation\nmodels, scGPT and scFoundation, achieved comparable performances with other baselines across\nsome of the datasets. We attribute this outcome to two main factors. Overfitting to Pretrained\nKnowledge: A foundation model, particularly a large one, might have learned very general or specific\nknowledge during its pretraining phase. When fine-tuning for a specific task, the model might overfit\nthe preexisting knowledge, leading to poor generalization of the new task data. Mismatch Between\nPretraining and Fine-Tuning Data: If the data distribution for fine-tuning is significantly\ndifferent from the data on which the foundation model was trained, the model might struggle to\nadapt, resulting in worse performance. A model trained from scratch on the specific task data may\nperform better as it directly optimizes for that data distribution."}, {"title": "Ablation Studies (RQ2 & RQ3)", "content": "We addressed these questions by comparing our weighted diversified sampling (WDS) method with\nuniform sampling across various sample sizes, ranging from 1% to 10% of the original dataset. We\ngenerated data subsets for each cell type using WDS and uniform sampling. We then applied\nour Transformer-based framework for feature selection at each sample size. Since CelluFormer\nconsistently outperformed other baselines, we selected it as our base model. We repeated Each\nexperiment five times and recorded the NES scores as the results. To evaluate the sampling methods,\nwe calculated the average NES score across the five experiments. We also computed the Mean Square\nError (MSE) between the NES scores from the sampling experiments and the ground truth derived\nfrom the entire dataset, as shown in Table 2. The evaluation results are presented in Table 3. We\nnote that WDS consistently produced higher NES scores compared to uniform sampling. As the\nsample size increased, the NES scores from uniform sampling began to converge with the ground\ntruth. In contrast, the NES scores from WDS consistently remained close to the ground truth,\neven at smaller sample sizes. The result indicates that while WDS offers a significant advantage in\nsmall samples by enabling the Transformer to capture a broader range of genetic interactions, its\nbenefits diminish as more data becomes available. Moreover, we find that for some cell types, smaller\nsamples of data outperformed larger samples of data on NES. This suggests that: (1) single-cell\ntranscriptomic data may contain noises that affect gene-gene interaction discovery, and, (2) some\ncomplex gene-gene interaction patterns in single-cell transcriptomic data cannot be interpreted\ndirectly through attention maps. We also provide a detailed study on the choice of parameter R in\nAlgorithm 1 in Appendix D.2."}, {"title": "Related Work", "content": "Single-Cell Transformer Models. Single-cell RNA sequencing (scRNA-seq), or single-cell\ntranscriptomics, enables high-throughput insights into cellular systems, amassing extensive databases\nof transcriptional profiles across various cell types for the construction of foundational cellular models\n[Hao et al., 2023]. Recently, there has emerged a large number of transformer models pre-trained for\nsingle-cell RNA sequencing tasks, including scFoundation [Hao et al., 2023], Geneformer [Theodoris\net al., 2023], scMulan [Bian et al., 2024], scGPT [Cui et al., 2024]. These foundation models have\ngained a progressive understanding of gene expressions and can build meaningful gene encodings\nover limited transcriptomic data. Yet, the previous work did not pay attention to pairwise gene-gene"}, {"title": "Conclusion", "content": "Gene-gene interactions are pivotal in the development of complex human diseases, yet identifying these\ninteractions remains a formidable challenge. In response, we have developed a pioneering approach\nthat utilizes an advanced Transformer model to effectively reveal significant gene-gene interactions.\nAlthough Transformer models are highly effective, their extensive parameter requirements often\nimpede efficient data processing. To overcome this limitation, we have introduced a weighted\ndiversified sampling algorithm. This innovative algorithm efficiently calculates the diversity score\nof each data sample across just two passes of the dataset. With this method, we enable the rapid\ngeneration of optimized data subsets for interaction analysis. Our comprehensive experiments\nillustrate that by leveraging this method to sample a mere 1% of the single-cell dataset, we can\nachieve results that rival those obtained using the full dataset, significantly enhancing both efficiency\nand scalability."}, {"title": "More Related Work on Gene-Gene Interaction Discovery", "content": "In this section, we provide a more detailed review of the existing work on gene-gene interaction\ndiscovery. For gene-gene interaction network construction, genome-wide association studies (GWAS)\nare widely adopted by biologists to study gene associations using single-nucleotide polymorphisms\n(SNPs) [Uffelmann et al., 2021]. However, GWAS have high computational costs and are simply\nbased on direct genotype-phenotype associations instead of wired graph structure. To address this\nproblem, many graphic models have emerged in recent years [Ata et al., 2020]. Network-based\nmethods regard gene-gene interaction discovery as a task to construct a homogeneous graph among\ngenes. For example, PRINCE [Vanunu et al., 2010] and VAVIEN [Erten et al., 2011] apply random\nwork to predict new edges on existing protein-protein interaction (PPI) or gene-gene interaction\n(GGI) knowledge graphs. VGAE [Singh and Lio', 2019] and GCAS [Rao et al., 2018] explore the\npotential to incorporate GNN and auto-encoder structure in the GGI network. In addition, existing\nwork like DeepDRIM [Chen et al., 2021b] and CNNC [Yuan and Bar-Joseph, 2019b] successfully\nimprove the construction of GGI through inferencing gene associations on scRNA sequencing data\nand known transcription factors (TF). While GGI networks and TFs are instrumental for mapping\nbiological processes, they are often plagued by high false-positive rates and context-dependent\ninaccuracies, especially when derived from large-scale in vitro experiments [Mahdavi and Lin, 2007,\nRasmussen and et al., 2021]. Existing methods that rely on pre-established protein-protein interaction\n(PPI) or transcription factor (TF) networks are prone to bias because they tend to reinforce known\ninteractions, making it difficult to objectively uncover novel gene-gene interactions. In contrast,\nour method circumvents this issue by directly discovering GGIs from scRNA-seq data without\ndependence on prior network knowledge."}, {"title": "Proofs of Theorem 3.4", "content": "Theorem B.1 (Min-Max Density Estimator, formal version of Theorem 3.4). Given a cell dataset\nX, for every $q \\in X$, we compute $w_q$ following Algorithm 1. Next"}]}