{"title": "Efficient Pose 6D: Scalable and Efficient 6D Object Pose Estimation", "authors": ["Zixuan Fang", "Thomas P\u00f6llabauer", "Tristan Wirth", "Sarah Berkei", "Volker Knauthe", "Arjan Kuijper"], "abstract": "In industrial applications requiring real-time feedback, such as quality control and robotic manipulation, the demand for high-speed and accurate pose estimation remains critical. Despite advances improving speed and accuracy in pose estimation, finding a balance between computational efficiency and accuracy poses significant challenges in dynamic environments. Most current algorithms lack scalability in estimation time, especially for diverse datasets, and the state-of-the-art (SOTA) methods are often too slow. This study focuses on developing a fast and scalable set of pose estimators based on GDRNPP to meet or exceed current benchmarks in accuracy and robustness, particularly addressing the efficiency-accuracy trade-off essential in real-time scenarios. We propose the AMIS algorithm to tailor the utilized model according to an application-specific trade-off between inference time and accuracy. We further show the effectiveness of the AMIS-based model choice on four prominent benchmark datasets (LM-O, YCB-V, T-LESS, and ITODD).", "sections": [{"title": "Introduction", "content": "In the field of computer vision, object pose estimation can be considered one of the most important tasks. It aims to determine the rotation and translation of an object in three dimensions respectively. Due to its wide range of applications such as robotics, autonomous driving, and augmented reality, researchers have attempted to tackle this problem. In robotics, for example, high-quality 6D pose estimation plays a crucial role in autonomous robotic object manipulation in real-world scenarios such as picking up industrial bins [3].\nIn applications requiring real-time feedback, low inference times play an important role in pose estimation. During robot navigation or autonomous driving, pose estimation systems must be capable of analyzing and responding rapidly and accurately to changes in the environment in order to prevent collisions and ensure the safety of the robot or passenger. Fast pose estimation can be used for sports analysis and video surveillance to capture critical motions and behaviors, which can be utilized for real-time decision support, as well. As a result, enhancing the processing speed of a pose estimation algorithm is essential to broaden its practical applications.\nOften the inference time of a model is closely linked to its accuracy. Based on this observation, there are a lot of applications that have strong constraints regarding their inference time budget. For these applications, finding the most accurate model, that fulfils its inference time constraints is key. Tan et al. [21] introduce a family of algorithms for Object Detection, that perform well under different time budgets, giving the user the possibility to chose the variation, that achieves the highest accuracy given their time budget. P\u00f6llabauer et al. [18] propose such a family of architectures for 6D pose estimation.\nIn this work, we propose multiple candidate architectures based on GDRNPP [15], which constitutes an enhanced version of GDR-Net [26], that optimize inference time while maintaining or surpassing its accuracy on multiple BOP challenge [1] datasets. Furthermore, we introduce the Adaptive Margin-Dependent Iterative Selection (AMIS) algorithm that selects a subset out of candidate architectures, that constitute a beneficial trade-off between inference time and accuracy over multiple datasets. The proposed AMIS algorithm can be applied to a diverse range of task-specific datasets, allowing the choice of a model that reflects the domain-specific requirements with regard to a trade-off between inference time and accuracy.\nIn summary,\nwe propose 40 candidate architectures based on modifications of GDRNPP [15] by adapting backbone and Geo Head architecture with the primary goal to enhance the resulting inference time while maintaining high accuracy,\nwe present the AMIS algorithm, which identifies a suitable set of candidate models that constitute an optimal trade-off between inference time and their 6D pose estimation quality over multiple datasets, and\nwe present quantitative results of the candidate models identified by the proposed AMIS algorithm for the LM-O, YCB-V, T-LESS, and ITODD datasets [28, 4, 9]."}, {"title": "Related Work", "content": "The estimation of 6D positions from monocular RGB-D pictures has a multitude of applications in the industry, e.g., robot grasping, and therefore constitutes an important computer vision task overall. Especially in the industrial area the need for real-time 6D object pose estimation is prevalent [6], therefore this work focuses on high accuracy applications under constraint inference time.\nHodan et al. [8] give a comprehensive overview over the recent developments in 6D object pose estimation as benchmarked by the BOP challenge, that uses a wide variety of datasets and relevant metrics. Li et al. [12] increase accuracy and robustness by introducing Coordinates-based Disentangled Pose Network (CDPN) that uniquely separates the prediction of rotation from the prediction of translation, demonstrating a high level of flexibility and efficiency even with"}, {"title": "Speed and Efficiency in Deep Learning", "content": "Modern deep learning architectures have been specifically tailored to address the computational bottlenecks in 6D pose estimation. For instance, lightweight neural networks that incorporate depthwise separable convolutions, such as MobileNet and EfficientNet, have demonstrated significant reductions in computational complexity and latency without compromising accuracy by reducing the number of parameters and operations, thereby speeding up the inference time and allowing for scaling and using different configurations [13, 25].\nBeyond architectural changes, algorithmic adjustments also play a crucial role. For example, employing more sophisticated loss functions that focus on critical parts of the pose estimation task or the application of better regularization techniques can lead to more efficient learning dynamics and prevent overfitting. [14, 5].\nP\u00f6llabauer et al. [18] propose a set of scalable 6d pose estimation architectures over a wide scale of inference time budgets based on a fixed set of datasets from the BOP challenge [1]. In contrast to that, we propose a strategy that is able to"}, {"title": "Preliminaries", "content": "GDRNPP [15] is a 6D object pose estimation architecture, that constitutes an enhanced version of GDR-Net [26]. It estimates the pose of an object given an RGB image, by firstly detecting relevant image regions, containing the object region, then predicting relevant features in these image regions using Convolutional Neural Networks (CNN) in the form of a backbone and a subsequent Geo Net, based on which a PnP-Module directly regresses the rotation and translation from the learned features. Subsequently a depth-based pose refinement can be performed as an optional step.\nWe choose to adopt the GDRNPP architecture as a base for the proposed scalable 6D object pose estmation models, due to its high performance in the BOP Challenge [1] and for its highly adoptable architecture, as demonstrated in a range of diverse extensions [16, 17]. To identify relevant parts of the architecture for inference time optimization, we subdivide the process of GDRNPP into six conceptual stages, i.e., Data Load, Backbone, Geo Head, Data Process, Patch PnP and Data Process Afterwards, which are illustrated with the architecture of GDR-Net (respectively GDRNPP) in Fig. 1. Preliminary experiments show that the major part of inference time are caused by the Data Load, Backbone and Geo Head stages (Fig. 2). Therefore, in the following we propose multiple changes to the architecture in these conceptual stages to reduce inference time, while aiming to preserve accuracy."}, {"title": "Methodology", "content": "In this section, we show potential optimizations to GDRNPP [15] especially by choosing suitable backbone architectures (see Sec. 4.1) and proposing alterations to the applied Geo head architecture (see Sec. 4.2) akin to the proposed optimizations of P\u00f6llabauer et al. [18]. Therefore, we select the five backbones from a pool of 22 candidate backbones which exhibit outstanding performance under a specified GMAC budget (see Sec. 4.1). Furthermore, we propose 11 potential alterations of the Geo Head architecture, from which we chose 4 according to their performance on the LM-O dataset (see Sec. 4.2). The combination of selected backbones and Geo Head architecture results in a pool of 40 candidate architectures. We propose the AMIS algorithm, that selects a subset of these architectures, which allows efficient inference time and estimate quality trade-off by choosing the appropriate architecture from that pool (see Sec. 4.3). In the end, we propose some implementation optimizations regarding the GDRNPP net architecture, that we applied to further increase inference time (see Sec. 4.4)."}, {"title": "Backbone", "content": "The impact of choosing different backbones for feature extraction on inference time and accuracy is crucial, prompting experiments with various models available in the Timm [22] package, which offers a broad array of pre-trained CNN and ViT [29] models, enabling access to advanced architectures and pre-trained weights which facilitate transfer learning and accelerate model development.\nTo select the most effective models, we set our criteria based on balancing performance speed and accuracy. Faster models with a Giga Multiply-Accumulate Operations per Second ratio up to the standard set by GDRNPP were prioritized.\nAmong models with similar sizes, those offering superior accuracy were chosen."}, {"title": "Geo Head", "content": "The Geo Head part of GDRNPP is one of the most time consuming parts during inference. The standard structure of the Geo Head comprises three convolution blocks processing the 1024 \u00d7 8 \u00d7 8 output from the Backbone including up-sampling and convolution layers. To reduce the inference time requirements, we propose two variants of the vanilla Geo Head architecture: Geo Head Variation 1, reducing the convolution blocks to two and modifying the layer setup to handle changes in the feature map size without upsampling, and Geo Head Variation 2, which further streamlines this process by eliminating an additional upsampling layer and adjusting the convolution sequence, resulting in smaller feature map sizes and faster data processing. These modifications are aimed at decreasing the computational load and accelerating inference time by reducing the number of operations the GPU processes and by lowering the input size to the Data Process part, enhancing overall performance. The proposed architecture variations are illustrated in Fig. 4.\nInspired by U-Net [19], we implemented skip connections in our neural network architectures to address the gradient vanishing problem and enhance learning capabilities by directly connecting layers across the network. These connections facilitate detail recovery and image segmentation by leveraging rich contextual information during up-sampling. However, introducing skip connections adds complexity to the model, increases computational demands, and may impact the network's ability to generalize to unseen data. To balance these factors, we simplified the skip connection structure to involve minimal additional computations, ensuring that connected feature maps match in size to avoid unnecessary computations.\nFor each of the three Geo Head Variants (Vanilla, Variation 1 and Variation 2), there are three different candidate locations, where skip connections could be added regarding the aforementioned criteria. These locations are illustrated in Fig. 5."}, {"title": "Adaptive Margin-Dependent Iterative Selection (AMIS) algorithm", "content": "We propose the Adaptive Margin-Dependent Iterative Selection (AMIS) algorithm to identify a subset of models that excel regarding their estimate quality in comparison to their time budget, i.e., inference time, over arbitrary datasets. Existing strategies often average results over datasets, which essentially over-weights the results of datasets that require high inference time, due to their absolute inference time differences being higher. In contrast to that, we opt for a strategy that reduces the influence of this effect. The proposed strategy pinpoints models that are part of an optimal wrap-line (Fig. 6a) in the space of inference time and estimation quality, which show substantial accuracy improvement over its quicker counterparts with only minimal gains compared to slower successors. Recognizing these models aids in selecting models that effectively balance speed and accuracy, enhancing model selection strategies.\nIn an initial phase, we measure inference time and accuracy for each candidate model and each dataset. For each dataset we fit a straight line in the 2D space spanned by inference time and accuracy metrics using linear regression, which we call the default slope. For each model and dataset, we calculate the distance of its result from this default slope, normalizing them for each dataset on a scale of 0 to 100. These scores are weighted depending on the desired dataset weight, resulting in a final score for each model. We rank these scores assigning every model a fixed amount of scoring points depending on their rank. We then iteratively repeat that ranking process for 100 adjustment factors, between 0.001 and 3, which are multiplied with the default slope (Fig. 6b). We accumulate the ranking points per model for different adjustment factors except from the case, where the 10 best performing models do not change in comparison to the previous adjustment factor. This procedure ensures, that the resulting model selection is robust against diverse trade-off preferences. After completing this step, we select the best ranking model repeating the process with the repeating models, until the number of selected models meets the task-specific requirements."}, {"title": "Other optimization", "content": "To reduce extended inference times linked with dynamic tensor creation, which is resource-intensive due to repeated memory allocation and initialization, we have adopted alternative measures. Our strategy minimizes new tensor creation by pre-allocating tensor memory before inference, using a reusable tensor pool throughout the inference cycle to avoid frequent new allocations. This approach enhances efficiency, which is particularly critical in real-time applications."}, {"title": "Geo Head", "content": "In this section, we summarize the findings from the proposed alterations to the Geo Head part of GDR-Net / GDRNPP as evaluated on the LM-O dataset (see Tab. 1).\nOur results indicate that the proposed optimizations (see Sec. 4.4) improve the inference time without a relevant impact on the measured accuracy.\nThe proposed Geo Head variation 1 leads to a speed-up in inference time and an improvement in accuracy, constituting it a successful optimization to the Geo Head architecture. In contrast to that, the proposed Geo Head variation 2, further increased inference time, while reducing accuracy significantly. The availability of such an adaption, however, is highly beneficial when looking for highly accurate 6D pose estimation models under varying inference time budgets.\nAdding connections within the Geo Head was generally found to negatively impact inference speed, particularly in the vanilla Geo Head setup, where it was deemed not worthwhile due to significant slowdowns without notable accuracy benefits. However, in the second variation of Geo Head, adding a connection at the first location improved both speed and accuracy, making it a promising adjustment for balancing performance metrics."}, {"title": "AMIS", "content": "We employ the aforementioned AMIS algorithm (see Sec. 4.3) to identify a suitable subset of models within the previously identified candidate architectures on the IMO, LM-O, YCB-V, T-LESS, and ITODD datasets [28, 4, 9]. The results of the five identified candidates are illustrated in Fig. 7 on scatterplot showing inference time and 6D object pose estimation accuracy. Furthermore, the average results of the models are illustrated in Tab. 3.\nThe experimental results show, that even with minimal time budget, i.e., when we expect our model to perform the fastest inference, the required inference time is reduced by 35% in comparison to GDRNPP, while the achieved performance only drops by 3% measured by the average of MSPD, MSSD, and VSD.\nAs the time budget increases, the performance of our candidates also gradually improves. This variation can adapt to the complex scenarios of different time and accuracy requirements in industrial environments. It is noteworthy that compared to GDRNPP, using about 31% additional time can lead to approximately a 25% improvement in performance. Fig. 7 furthermore shows that the selected models show an increase in accuracy with increased inference time for all 4 datasets."}, {"title": "Conclusion", "content": "We presented a fast and scalable pose estimator, dynamically adjusting to custom needs of estimation quality versus inference times trade-offs. To that end, we proposed 40 candidate architectures which aim to otimize the trade-off between inference time and 6D object pose estimation accuracy based on GDRNPP, which were curated by choosing promising backbones and identifying beneficial architectural changes to the Geo Head part of the GDRNPP architecture. Additionally, we proposed the AMIS algorithm, a tool designed to quantitatively identify model architectures that represent sweet spots which effectively address the challenges posed by the diverse scales of precision and time across various datasets. In our comparison with GDRNPP, we demonstrate the candidates selected by the AMIS algorithm exhibit an outstanding inference time and progressively enhancing model accuracy for a wide variety of inference time budgets.\nFuture directions might include temporal trajectory composition of different frequencies for long-horizon tasks, policy composition methods on large-scale datasets, and policy distillation from composed policies. For further research, there are some potential areas of improvement: adopting end-to-end methods that integrate detection or segmentation and exploring other lightweight methods like teacher-student models."}]}