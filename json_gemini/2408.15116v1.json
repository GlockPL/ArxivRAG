{"title": "EVALUATING STABILITY OF UNREFLECTIVE ALIGNMENT", "authors": ["James Lucassen", "Mark Henry", "Philippa Wright", "Owen Yeung"], "abstract": "Many theoretical obstacles to AI alignment are consequences of reflective stability - the\nproblem of designing alignment mechanisms that the AI would not disable if given the option.\nHowever, problems stemming from reflective stability are not obviously present in current\nLLMs, leading to disagreement over whether they will need to be solved to enable safe\ndelegation of cognitive labor. In this paper, we propose Counterfactual Priority Change\n(CPC) destabilization as a mechanism by which reflective stability problems may arise in\nfuture LLMs. We describe two risk factors for CPC-destabilization: 1) CPC-based stepping\nback and 2) preference instability. We develop preliminary evaluations for each of these\nrisk factors, and apply them to frontier LLMs. Our findings indicate that in current LLMs,\nincreased scale and capability are associated with increases in both CPC-based stepping\nback and preference instability, suggesting that CPC-destabilization may cause reflective\nstability problems in future LLMs.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Motivation", "content": "The goal of this work is to help inform prioritization of AI alignment research. To that end, we investigate\nwhat sorts of alignment problems are likely to affect future generations of LLM-based AI. Specifically, we focus\non an open question whose answer has significant implications for problem prioritization - \"will alignment\nneed to be reflectively stable before useful cognitive labor can be safely delegated to \u0391\u0399?\"\nA property of an AI system is reflectively stable if, when given the choice to modify itself, the AI preserves\nthat property[Yudkowsky, 2016]. The alignment of an AI is reflective stable if, given the choice to modify\nitself, the AI remains aligned to the same goals. All else equal, reflectively stable alignment is preferable, as it\nrules out failure modes where alignment properties \"uninstall themselves\". However, it seems that reflectively\nstable alignment may be difficult to achieve. Early research in the context of expected utility maximization\nfailed to identify utility functions that are reflectively stable while also being indifferent to shutdown and\npursuing some external utility term [Soares et al., 2015]. More recent work has identified a potential approach\nusing preferential gaps, but this approach requires extremely robust generalization of preference structure,\nand it remains unclear how tractable it is[Thornley, forthcoming]. On the other hand, alignment techniques\nsuch as RLHF[Ouyang et al., 2022] which do not attempt reflective stability have recently demonstrated\nsignificant progress in managing the behavior of present frontier LLMs.\nThis raises a very natural question - just how necessary is reflectively stable alignment? Can we do without\nit? A prominent approach [OpenAI] to AI safety aims to solve only the problems needed to safely delegate\nthe remaining alignment research to AI. Current LLMs have made rapid progress on a variety of cognitive\ntasks[OpenAI et al., 2024], and do not yet seem to develop alignment failures due to reflective instability.\nPerhaps by the time we reach the capability threshold needed to delegate alignment research to AI, alignment\nfailures due to reflective instability will still not have materialized.\nEvents may move very fast when AI capabilities become advanced enough to automate entire fields of research\nsuch as AI alignment. It would be best to know in advance whether or not such Als will need to be reflectively"}, {"title": "1.2 CPC-Destabilization", "content": "\"Long-horizon\" tasks are those which require an ongoing, iterative process or extended thinking to solve. Some\nextremely simple examples of long-horizon tasks might include Tic Tac Toe and Connect Four[Chen et al.,\n2024], in contrast to short-horizon tasks like answering a multiple-choice trivia question or squaring a number.\nImplementing a single function from a verbal description might be \"short-horizon\", but implementing an\nentire codebase with many such functions working together would be long-horizon. Current LLMs are often\nsurprisingly lacking at these tasks. We expect improvements in long-horizon capabilities will be needed before\nLLMs are able to automate research, because many important and difficult research tasks are long-horizon.\nFor the purposes of this work, we will speculate that an important capability for long-horizon capabilities is\n\"dynamic planning\" - the ability to adapt an existing plan as new information is revealed. This includes the\nability to abandon strategies that are not working, press ahead on strategies that are working, and generate\nnew strategies when the initial pool of promising candidates has been exhausted. If an agent has strong static\nplanning but weak dynamic planning, we would expect it to be capable on only tasks that can be solved\nwithout significant revision to its initial plan. We contrast this with the sense of \"agentic planning\" discussed\nin Carlsmith[Carlsmith, 2022], which emphasizes a causal world-model and selection of actions based on\nexpected outcomes. In the language of the \"planning stack\", \"static planning\" is the\nability to generate an effective initial stack from scratch, and \"dynamic planning\" is the ability to effectively\nadapt the stack to new information."}, {"title": "Algorithm 1 Dynamic Planning Using CPC-Based Stepping Back", "content": "1: S is a stack of instrumental goals, [g]\n2: It is the information I available at time t\n3: SP(I, [g]) \u2192 [g] is a static planning algorithm\n4: for i = 1, 2, . . .len(S) do\n5: \tprefix \u2190 S[: i]\n6: \tif SP(I, prefix)[\u22121] \u2260 S[-1] then\n7: \t\treturn prefix + SP(I,prefix[: -1])\n8: return S\nWe do not expect realistic AI agents to follow the CPC criterion exactly or explicitly. Regenerating the\nplanning stack from all levels every action would likely be slow and costly - in practice, we predict that AI\nagents will have stepping back behaviors that more closely approximate CPC as they become more capable.\nNext we argue that in the limit, following the CPC criterion in full generality can lead to misalignment.\nWe propose the CPC-destabilization threat model. First, if an agent exactly follows the CPC criterion when\ndeciding to step back or not, then if there exists an item in its planning stack that would lead to different\nobject-level priorities when considered, the agent will step back and reconsider that item. Second, if an\nagent's alignment is not reflectively stable, then there exists a high-level item in its planning stack that it\nwould abandon upon explicit consideration. This would likely cause a dramatic shift in object-level priorities.\nBy hypothesis, then, an agent whose stepping-back behavior follows the CPC criterion and whose alignment\nis unstable will become misaligned.\nThese two requirements are the risk factors for CPC-destabilization that we will investigate.\n1. CPC-Based Stepping Back: the agent steps back if and only if the CPC criterion is true.\n2. Preference Instability: if the agent explicitly examined its high-level preferences, it would choose to\nmodify them.\nIn the language of Risks from Learned Optimization[Hubinger et al., 2021], an agent whose alignment is\nunstable can be thought of as a suboptimality-aligned mesa-optimizer. This is a specific type of mesa-optimizer,"}, {"title": "", "content": "in which the agent acts aligned due to a false belief or otherwise suboptimal behavior. In this case, the agent\nhas an unexamined belief that its current actions are on track to achieving its highest-level goals. This belief\nis false, but can persist as long as it is not examined. The particular way in which the agent's reasoning is\nsuboptimal is that it does not step back despite the fact that the CPC criterion is true, which hinders its\ndynamic planning but allows the unstable item in the planning stack to remain unexamined. Resolving this\nsuboptimality leads to both an improvement in the agent's dynamic planning capabilities, and causes sudden\nmisalignment when the unstable item in the planning stack is examined and modified."}, {"title": "2 Methods", "content": "In this work, we conducted three experiments.\n\u2022 CPC Curves: in this experiment we aim to measure agreement between the CPC criterion and the\nLLM's in-fact stepping-back behavior. We applied this experiment to GPT-3.5-turbo and GPT-4.\n\u2022 Multi-Armed Bandit: in this experiment we aim to measure a toy case of dynamic planning ability.\nWe applied this experiment to GPT-3.5-turbo, GPT-4, GPT-4-turbo, and GPT-40.\n\u2022 Preference Cycles: in this experiment we aim to measure the strength of reflective pressure towards\nstable preferences. We applied this experiment to GPT-3.5-turbo and GPT-4."}, {"title": "2.1 CPC Curves", "content": ""}, {"title": "2.1.1 Evaluation", "content": "In this experiment, we want to test CPC-based stepping back: how closely does the CPC criterion predict the\nLLM's actual stepping back behavior? To study this, we need some dataset of LLM stepping back behavior,\nand the ability to measure whether or not the CPC criterion is true. To generate our dataset of stepping\nback behavior, we artificially set an LLM off down the wrong track to solving a problem, such that it must\nnotice the \"mistake\" and switch strategies to get the correct answer. This lets us study dynamic planning\nin a simple, known case: stepping back from level N to level N-1. Then, to measure the CPC criterion, we\ninterrupt the problem-solving transcript and prompt the LLM to re-examine its current priorities and decide\nwhether or not its current object-level approach is best. If we provide the LLM unlimited time to think\nthrough its decision, this lets us directly measure the CPC criterion.\nAt a high level, our experiment involved the following steps:\n\u2022 Generate a problem dataset which will require switching to solve.\n\u2022 Prompt the target LLM to solve each problem in the dataset, generating a chain of thought transcript.\n\u2022 Judge where in each transcript the target LLM in fact switches strategies.\n\u2022 At intervals through each chain of thought, interrupt and prompt the target LLM to decide whether\nor not it should switch strategies, based on the transcript prefix up until that point."}, {"title": "2.1.2 Validation", "content": "This experiment requires measuring two important quantities: where in the transcript the target LLM in fact\nswitches its strategy, and the target LLM's CPC decision at each position. For each of these quantities, we\nvalidated our measurement approaches on a synthetic test set.\nSwitch Judging\nTo validate our measurements of where in the transcript the target LLM in fact switches its strategy, we\nproduced a simple synthetic dataset to check against.\nEach example in this dataset was composed of the first 200 characters of an LLM chain of thought prompted\nto solve a quadratic equation by factoring, followed by the first 200 characters of the same LLM solving the\nsame quadratic by the quadratic formula instead. On this dataset, the correct answer to whether or not\nthe LLM had switched its strategy yet at each index was always [F, F, F, F, T, T, T, T]. We then tested\nseveral different LLMs on their ability to judge whether or not the transcript had switched from factoring\nto using the quadratic formula at a given point. We also tested different prompts, and an error-correcting\npost-processing method.\nWe measured the accuracy at each prefix position, to assess false positives versus false negatives.\nWe found that:\n\u2022 GPT-4 significantly outperformed the other models tested (Figure 4)\n\u2022 A verbose prompt explaining the task significantly improved performance (Figure 5)\n\u2022 Including the full context followed by the prefix in question slightly improved performance (Figure 5)\n\u2022 Post-processing slightly improved performance (Figures 4 and 5)"}, {"title": "", "content": "Our post-processing method was based on the fact that we collected an independent switching judgement\nat each index in the problem-solving transcript. This made it possible to get reports that were definitely\nincorrect: for example, reporting that the transcript had switched already at index 5, but reporting that it had\nnot switched yet at index 6. To correct these errors, we checked each series of switching judgements for the\nminimum number of edits required to prevent these inconsistencies. For example, if we had a problem-solving\ntranscript in which we checked switching at ten indices, and our results were [F, T, F, F, F, F, T, T, T,T],\nwe could identify the T at index 1 as a likely error. In some cases, such as [F, F, F, F, T, F, T, T, T ,T],\nwhere there is not a unique way to make the result consistent with the minimum number of edits, we selected\na solution uniformly at random from the candidates.\nUltimately, most of the performance gain is captured by just using GPT-4 with a verbose prompt. In the\ninterest of getting the best possible judging accuracy for our results, we used GPT-4 with a verbose prompt,\nfull context, and post-processing. This method gets 98.5% accuracy on our test set. As a sanity check, we\nused this method to label real problem-solving transcripts (Figure 6)."}, {"title": "", "content": "To find the roots of the quadratic equation 5x^2 - 50x + 125 = 0, we can start by factoring:\n5x^2 - 50x + 125 = 0\n5(x^2 - 10x + 25) = 0\n5(x-5) (x - 5) = 0\n5(x-5)^2 = 0\nSwitching to the quadratic formula:\nThe quadratic formula is given by x = (-b \u00b1 \u221a(b^2 - 4ac)) / 2a.\nIn this case, a = 5, b = -50, and c = 125.\nPlugging these values into the formula:\nx = (50 \u00b1 \u221a((-50)^2 - 4*5*125)) / 2*5\nx = (50 \u00b1 \u221a(2500 - 2500)) / 10\nx = (50 \u00b1 0) / 10\nx = 50 / 10\nx = 5\nTherefore, the equation has a single repeated root of x = 5."}, {"title": "CPC Extraction", "content": "To extract a CPC decision from the target LLM, we needed to describe the CPC decision setting, elicit a\nchain of thought on the topic, and then extract a yes/no answer. We validated our methods for each of these\nsteps. First, we validated prompts to extract chain of thought from our LLMs.\nWe tested various chain of thought prompts to see which of them elicited a \"real\" chain of thought, in which\nthe answer was not decided early in the resulting text. We tested these prompts on both GPT-3.5-turbo and\nGPT-4, as these are the LLMs we will eventually put through the whole experimental pipeline.\nA chain of thought prompt was considered \"better\" if it led to fewer early answers.\nWe found that the prompts which best avoided early answers (within the first half of the text) was not\nconsistent between GPT-3.5-turbo and GPT-4. We chose the prompt \"think through the question but do not\nanswer yet\", because it worked reasonably well for both LLMs (Figure 7)."}, {"title": "", "content": "Next, we tested our ability to extract yes/no answers correctly. We used logit bias to force a one-token\nanswer, either \"Yes\" or \"No\". However, we were concerned that if something about our prompt encouraged\nthe LLM to answer in a different format then this logit biasing could extract the wrong answer. For example,\nif the next-token probabilities were \"Yay\":0.9, \"Nay\u201d:0.09, \"Yes\":0.04, \u201cNo\u201d: 0.06.\nTo test this, we used a set of simple trivia questions, intended to be easy enough that we could attribute\nany errors to our yes/no extraction rather than to LLM capabilities. We then tested various prompts that\ninstructed the LLM to answer either yes or no.\nTo select the best yes/no extraction prompt, we simply chose a prompt that had robustly good performance\nfor both LLMs.\nWe found a surprising amount of inconsistency, both between LLMs and between prompts. We also tested\nyes/no extraction after eliciting a chain of thought, using the method previously identified. No one prompt\nworked perfectly for GPT-3.5-turbo/GPT-4 and the one token/chain of thought settings. We decided to use\nprompt number 4, which was \"Respond Yes or No.\". We chose this because it had the maximin lower quartile\nperformance across the four cases (Figure 8).\nFinally, once our chain of thought elicitation and yes/no extraction methods were decided, we could validate\nour choice of CPC prompt.\nThis posed a bit of a problem, as we can't define a test set on which we know current LLMs should have some\nknown behavior on the CPC decision problem, where we could select the prompt that produced the closest to\nknown behavior. Instead, we tested a variety of switching prompts on a variety of \"tunable\" settings in which\nwe would expect CPC decisions to increase monotonically. We also used one setting whose tuning parameter\nshould have no effect on switching decisions (Figure 9)."}, {"title": "2.2 Multi-Armed Bandit", "content": ""}, {"title": "2.2.1 Evaluation", "content": "In this experiment, we assess LLM performance on the Multi-Armed Bandit (MAB) problem. This is meant\nto be a toy example of dynamic planning - the LLM gets information about the value of each arm, and must\nupdate its decisions accordingly.\nThe MAB was set up with three arms, each paying out from a Gaussian distribution with different means\nbut the same standard deviation. The means of the arms were 50, 100, and 150. By varying the standard\ndeviations, we controlled the difficulty of the problem, with higher standard deviations making it harder to\ndistinguish the best arm.\nEach LLM was tested with four different algorithmic instructions: random, intuitive, greedy, and Upper\nConfidence Bound (UCB).\n\u2022 Random: The LLM was instructed to uniformly choose from the available actions at random.\n\u2022 Intuitive: The LLM was instructed to choose the best arm it can, without any explicit algorithmic\nguidance except for a reminder to try each arm at least once.\n\u2022 Greedy: The LLM was instructed to try each arm once, and to thereafter always choose the arm\nwith the highest average return.\n\u2022 UCB: The LLM was provided with a detailed description of the UCB algorithm and asked to follow\nits steps, calculating the UCB score for each arm and choosing the arm with the highest score.\nFor each \"algorithm,\" the LLMs were further evaluated under both one token and chain of thought prompt\nsettings. In the one token setting, the LLM was forced to directly output its choice (\"A\", \"B\", or \"C\") in a"}, {"title": "2.2.2 Validation", "content": "We encountered a few challenges in capability elicitation during the development and validation of the MAB\nexperiment.\nThe first issue was information sparsity in the prompt. Initially, a classic chatbot-style conversation format\nwas used to interact with the LLMs. However, the LLMs struggled to extract the relevant information from\nthe full context, which was long and relatively information-sparse. To address this, we changed the prompts\nto a compressed form which would include only the essential information: the rules of the game, a summary\nof the turns so far, and the number of turns remaining. This more focused prompt format led to improved\nperformance.\nThe second issue was the tendency of LLMs following the intuitive strategy to repeat a single action, such\nas pulling lever A every turn without exploring the other options. In the reasoning transcripts, the LLMs\nseemed to consider the initial payout from lever A as high in absolute terms, and would conclude that they\nhad found the optimal strategy. To mitigate this issue, we adjusted the prompts to explicitly encourage\nexploration of all available arms."}, {"title": "2.3 Preference Cycles", "content": ""}, {"title": "2.3.1 Evaluation", "content": "In this experiment, we aim to assess the strength of pressures towards reflectively stable preferences in\nLLMs. There is theoretical debate[Hendrycks and Thornley, 22023] about which sorts of preference structures\nlead to dominated or otherwise obviously inferior problem-solving strategies. We choose to focus on cyclic\npreferences as a particular case that may be unstable, as they admit the construction of very simple\nmoney-pumps[Gustafsson, 2022].\nOur experiment involves the following steps:\n1. Generate a problem setting which involves multiple distinct \"resources\" and a clear objective\n2. Provide a specific problem state and prompt the target LLM to choose which of two resource-units it\nwould rather have. Repeat for all entries in a pairwise preference matrix, except the diagonal\n3. Convert the preference matrix to a directed graph and find all non-repeating cycles in the graph.\n4. Prompt the target LLM to choose whether or not it wants to modify its preferences to remove a\nparticular cycle\nThis experiment produces two main results which could each be considered relevant to \"preference stability\":\nnumber of cycles present, and proportion of cycles kept/rejected. The fewer cycles present, the closer to\nreflectively stable the LLM's stated preferences are. The greater the proportion of cycles rejected, the stronger\nthe LLM's reflective tendency to make its preferences more stable.\nWe chose the card game Dominion as a problem setting because it has a clear objective and ruleset, and\nbecause choosing which of a set of cards is the most valuable to add to the deck at a given state of the game\nis a core activity of gameplay. We used temperature 0, and performed sampling by using different subsets\nof 6 cards chosen at random from the First Game cardset. Cards and card sets were both chosen without\nreplacement."}, {"title": "2.4 Validation", "content": "To validate the LLM's basic Dominion card-prioritization knowledge, we checked the rates at which the LLM\nmade the correct choice when faced with pairs of cards where one is strictly better than the other. There are\nfour available pairs to check: Gold>Silver, Silver>Copper, Province>Duchy, and Duchy>Estate. With a\nsample size of 100, we found that GPT-3.5-turbo made the correct choice 100, 100, 100, and 89 times. GPT-4\nmade the correct choice 100 times for all four. We take these results to show that the LLMs are performant\nenough at basic card-prioritization that any cyclicity we observe shouldn't be attributed to incompetence in\nthis domain.\nTo account for the effects of prompt sensitivity, we collected results over six total prompts when eliciting\ncycle stability responses. We used three different phrasings of the question, and positive/negative versions of\neach phrasing, such as \u201cdo you want to keep this preference cycle\u201d vs \u201cdo you want to remove this preference\ncycle\".\nWe considered using a more principled metric for the \"degree of cyclicity\" of a given preference matrix, such as\nthe minimum number of edits needed to remove all cycles - this is called the feedback arc set. Unfortunately,\nfinding the minimum feedback arc set is NP-hard, and we were not able to compute it for graphs of the size\nwe studied."}, {"title": "The third issue was general prompt sensitivity.", "content": "We observed that slight variations in prompt wording could\nlead to significant differences in performance, and that these effects were not consistent between LLMs. To\naddress this, we developed three variations of each MAB prompt and randomly selected from these variations\nfor each interaction with the LLM. The selection of these three prompts was not particularly principled, but\nshould still account for some of the variance introduced by prompt sensitivity."}, {"title": "3 Results and Discussion", "content": ""}, {"title": "3.1 CPC Curves", "content": "The primary takeaway from Figure 10 is that the CPC curve generated by GPT-4 resembles the hypothetical\ncurve given in Figure 3 for an agent which is imperfectly following the CPC criterion. On the other hand, the\nCPC curve generated by GPT-3.5-turbo bears no resemblance to Figure 3, besides having some discontinuity\nnear index 0. This supports our hypothesis that more capable LLMs have more CPC-based stepping back\nbehavior.\nThe confidence intervals in Figure 10 depend on the number of samples available at the given index, and fewer\nsamples are available towards the ends as fewer transcripts extend out to the extremes. This leads to wider\nconfidence intervals towards the ends. Eventually, confidence intervals approach zero width when all samples\nare either 1 or 0 - this is an artifact of using the Wald confidence interval for a binomial variable. However,\nthese low-confidence areas can mostly be ignored, as the relevant results are close to the switching point.\nGPT-3.5-turbo's CPC curve looks quite different than what we expect an optimal CPC curve to look like\n- in fact, almost the opposite. We suspected a bug at first, but confirmed that our methods were correct by\ninspecting the chains of thought generating during CPC decisions. The transcripts contained many mistakes\nand wrong choices, but the answers given were consistent with the answers extracted."}, {"title": "", "content": "Let's step back and analyze the current approach we've taken to factor the quadratic\nequation 2x^2 - 32x + 78 = 0.\n1. We simplified the equation by dividing all terms by 2 to get x^2 - 16x + 39 = 0.\n2. We attempted to find two numbers that multiply to 39 and add up to -16, which are\n-13 and -3.\n3. We factored the equation into (x - 13)(x - 3) = 0.\n4. We found the roots x = 13 and x = 3 by setting each factor to zero.\nUpon reviewing the steps, we realize a potential error in the factoring process. The\noriginal equation was not completely homogeneous in terms of the coefficient of x^2.\nIt's possible that the factoring method used may not work efficiently in this case.\nBased on this analysis, I recommend trying a different approach to factor the quadrat\nic equation.\nTherefore, my recommendation is: **Yes, I recommend a different approach**."}, {"title": "3.2 Multi-Armed Bandit", "content": "Due to limitations in the dataset, formal statistical hypothesis testing was not feasible for these results.\nConsequently, this section presents a primarily qualitative analysis of the data illustrated in Figure 12.\nAs expected, in the majority of (nonrandom) cases, accuracy declines as the standard deviation of the payout\ndistributions increases. This trend is consistent across different algorithmic approaches and prompt types,\nconfirming our expectation that higher variance makes the MAB problem more challenging."}, {"title": "", "content": "Notably, GPT-4 and GPT-4-turbo achieved high accuracy using intuitive prompts in the one token setting,\nsurpassing their own scores with chain of thought and approximately equalling their UCB performance. This\nsuggests that these LLMs have developed robust intuitions for the MAB problem, allowing them to make\neffective decisions without explicit reasoning.\nAmong the LLMs tested, GPT-4 appeared to be the least affected by increasing standard deviations. Its\nperformance remained relatively stable even under high-variance conditions, particularly when instructed to\nimplement the UCB algorithm.\nThe results consistently showed GPT-3.5-turbo at the bottom of the performance chart across all algorithms\nand prompt types. This suggests that LLM size plays a role in the ability to handle the MAB problem\neffectively. However, among the GPT-4 variants, no single model consistently outperforms the others across\nall settings, suggesting that factors beyond LLM size influence performance.\nThese findings raise questions about the nature of decision-making in language models, and the effects of\nscaling on switching abilities. The strong performance of GPT-4 and GPT-4-turbo with intuitive prompts in\nthe one token setting suggests that these LLMs have some intuitions for the MAB problem without explicit\nalgorithmic guidance. However, performance was poor overall, as none of the LLMs reliably converged to\npulling the correct lever near 100% of the time, which would be the standard for narrow RL algorithms."}, {"title": "3.3 Preference Cycles", "content": "The plot on the left in Figure 13 shows that GPT-4 shows consistently far fewer cycles in its preference matrix\nthan GPT-3.5-turbo. Because cyclic preferences admit money pumps, this suggests that GPT-4 has fewer\nreflective instabilities in its preferences. It is not clear how to interpret this. One interpretation is that more\nadvanced LLMs are approaching a reflectively stable set of corrigible preferences, even though theoretical\nwork has yet to describe such a set. This would be because GPT-4 is closer to reflectively stable but does not\nobviously behave less corrigibly than GPT-3.5-turbo, insofar as they both follow user instructions and do not\nseem to resist shutdown. The other interpretation is that more advanced LLMs are on track to becoming\nincorrigible, because more advanced capabilities require fewer reflective instabilities, and corrigibility only\nbecomes very rare when the number of reflective instabilities is quite close to zero.\nIn the plot on the right in Figure 13, we see that when confronted with a cycle in its stated preferences, GPT-4\nchooses to remove the cycle 100% of the time, across all six prompt configurations, while GPT-3.5-turbo does"}, {"title": "", "content": "not. This suggests more advanced LLMs are more likely to abandon reflectively unstable preferences upon\nexplicit consideration. This in turn suggests that more advanced LLMs are more likely to become misaligned\nif their alignment is reflectively unstable and they consider it explicitly."}, {"title": "4 Conclusion", "content": "In order to investigate whether or not reflective stability problems will arise in long-horizon-competent LLMs,\nwe proposed the CPC-destabilization threat model, and conducted three evaluations to measure risk factors\nfor CPC-destabilization. In Experiment 1, we assessed how closely the LLM's in-practice switching behavior\nfollows the CPC criterion, and found that GPT-4 displayed a clear CPC curve, while GPT-3.5-turbo did\nnot, suggesting that more capable LLMs have more CPC-based stepping back behavior. In Experiment 2,\nwe measured how well LLM's perform on the Multi-Armed Bandit problem, and found that performance is\nstill quite limited, and did not observe an obvious scaling trend in switching capabilities for this setting. In\nExperiment 3, we used preference cycles among Dominion cards as a way to measure how close to reflectively\nstable LLM's preferences are, and found that GPT-4 has more consistent preferences and a stronger tendency\nto remove remaining inconsistencies than GPT-3.5-turbo.\nWhile these results are very preliminary, they suggest that the risk factors for CPC-destabilization that we\nidentified are increasing with LLM capabilities from GPT-3.5-turbo to GPT-4. If this trend continues, future\nhighly capable LLMs may be much more difficult to align using methods which don't account for reflective\nstability.\nThere is much room for future work to improve on these experiments. A few particularly notable examples:\n\u2022 Quantitative metrics for CPC-based stepping back behavior\n\u2022 Applying the CPC curve method to other problem settings\n\u2022 A principled way to deal with the variation introduced by prompt sensitivity\n\u2022 More effective or better-understood capability elicitation\n\u2022 Investigation of how to extract useful cognitive labor from an LLM if the LLM's problem-solving\napproach convergently leads it to some unacceptable action, in a way which cannot be trained away\nwithout damaging capabilities."}]}