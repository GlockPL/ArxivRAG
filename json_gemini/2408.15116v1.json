{"title": "EVALUATING STABILITY OF UNREFLECTIVE ALIGNMENT", "authors": ["James Lucassen", "Mark Henry", "Philippa Wright", "Owen Yeung"], "abstract": "Many theoretical obstacles to AI alignment are consequences of reflective stability - the\nproblem of designing alignment mechanisms that the AI would not disable if given the option.\nHowever, problems stemming from reflective stability are not obviously present in current\nLLMs, leading to disagreement over whether they will need to be solved to enable safe\ndelegation of cognitive labor. In this paper, we propose Counterfactual Priority Change\n(CPC) destabilization as a mechanism by which reflective stability problems may arise in\nfuture LLMs. We describe two risk factors for CPC-destabilization: 1) CPC-based stepping\nback and 2) preference instability. We develop preliminary evaluations for each of these\nrisk factors, and apply them to frontier LLMs. Our findings indicate that in current LLMs,\nincreased scale and capability are associated with increases in both CPC-based stepping\nback and preference instability, suggesting that CPC-destabilization may cause reflective\nstability problems in future LLMs.", "sections": [{"title": "1 Introduction", "content": "The goal of this work is to help inform prioritization of AI alignment research. To that end, we investigate\nwhat sorts of alignment problems are likely to affect future generations of LLM-based AI. Specifically, we focus\non an open question whose answer has significant implications for problem prioritization - \"will alignment\nneed to be reflectively stable before useful cognitive labor can be safely delegated to \u0391\u0399?\"\n\nA property of an AI system is reflectively stable if, when given the choice to modify itself, the AI preserves\nthat property[Yudkowsky, 2016]. The alignment of an AI is reflective stable if, given the choice to modify\nitself, the AI remains aligned to the same goals. All else equal, reflectively stable alignment is preferable, as it\nrules out failure modes where alignment properties \"uninstall themselves\". However, it seems that reflectively\nstable alignment may be difficult to achieve. Early research in the context of expected utility maximization\nfailed to identify utility functions that are reflectively stable while also being indifferent to shutdown and\npursuing some external utility term [Soares et al., 2015]. More recent work has identified a potential approach\nusing preferential gaps, but this approach requires extremely robust generalization of preference structure,\nand it remains unclear how tractable it is[Thornley, forthcoming]. On the other hand, alignment techniques\nsuch as RLHF[Ouyang et al., 2022] which do not attempt reflective stability have recently demonstrated\nsignificant progress in managing the behavior of present frontier LLMs.\n\nThis raises a very natural question - just how necessary is reflectively stable alignment? Can we do without\nit? A prominent approach [OpenAI] to AI safety aims to solve only the problems needed to safely delegate\nthe remaining alignment research to AI. Current LLMs have made rapid progress on a variety of cognitive\ntasks[OpenAI et al., 2024], and do not yet seem to develop alignment failures due to reflective instability.\nPerhaps by the time we reach the capability threshold needed to delegate alignment research to AI, alignment\nfailures due to reflective instability will still not have materialized.\n\nEvents may move very fast when AI capabilities become advanced enough to automate entire fields of research\nsuch as AI alignment. It would be best to know in advance whether or not such Als will need to be reflectively"}, {"title": "1.2 CPC-Destabilization", "content": "\"Long-horizon\" tasks are those which require an ongoing, iterative process or extended thinking to solve. Some\nextremely simple examples of long-horizon tasks might include Tic Tac Toe and Connect Four[Chen et al.,\n2024], in contrast to short-horizon tasks like answering a multiple-choice trivia question or squaring a number.\nImplementing a single function from a verbal description might be \"short-horizon\", but implementing an\nentire codebase with many such functions working together would be long-horizon. Current LLMs are often\nsurprisingly lacking at these tasks. We expect improvements in long-horizon capabilities will be needed before\nLLMs are able to automate research, because many important and difficult research tasks are long-horizon.\n\nFor the purposes of this work, we will speculate that an important capability for long-horizon capabilities is\n\"dynamic planning\" - the ability to adapt an existing plan as new information is revealed. This includes the\nability to abandon strategies that are not working, press ahead on strategies that are working, and generate\nnew strategies when the initial pool of promising candidates has been exhausted. If an agent has strong static\nplanning but weak dynamic planning, we would expect it to be capable on only tasks that can be solved\nwithout significant revision to its initial plan. We contrast this with the sense of \"agentic planning\" discussed\nin Carlsmith[Carlsmith, 2022], which emphasizes a causal world-model and selection of actions based on\nexpected outcomes. In the language of the \"planning stack\" (illustrated in Figure 1), \"static planning\" is the\nability to generate an effective initial stack from scratch, and \"dynamic planning\" is the ability to effectively\nadapt the stack to new information."}, {"title": "The CPC Criterion", "content": "The CPC Criterion: Is there an item on the agent's current planning stack that, if the planning\nstack were regenerated from that point with all currently available information, would lead to a\ndifferent object-level priority?\n\nThere are two possible ways to deviate from using the CPC criterion to guide stepping back. If the agent\nsteps back earlier, then it will pop some items off its planning stack, think about what to do next, then\ngenerate some new plan which leads to the same object-level actions. In this case, the agent has simply\nwasted some time re-deriving its previous priorities. On the other hand, if the agent steps back later, that\nmeans that there was some earlier time at which, if it had explicitly examined its priorities, it would have\ndecided to abandon some aspect of its plan. In this case, the agent has also wasted time - if the plan is going\nto be abandoned, better to do it sooner rather than later.\n\nThe CPC criterion does not capture any of the details of how the agent models the world or selects its\nactions. The intention is not to solve dynamic planning, but instead to propose one way an agent could\nimplement highly generalizable dynamic planning if it already has strong static planning. We can make\nthis explicit in pseudocode (Algorithm 1). The algorithm we describe amounts to using static planning to\nconstantly regenerate the planning stack when new information is provided. This is a simple, brute-force\nway to eliminate path-dependencies in dynamic planning and ensure that as the agent updates its plans in\nresponse to new information, it will never end up with a plan worse than its static planning abilities would\nhave supplied."}, {"title": "Algorithm 1 Dynamic Planning Using CPC-Based Stepping Back", "content": "1: S is a stack of instrumental goals, [g]\n2: It is the information I available at time t\n3: SP(I, [g]) \u2192 [g] is a static planning algorithm\n4: for i = 1, 2, . . .len(S) do\n5:        prefix \u2190 S[: i]\n6:       if SP(I, prefix)[\u22121] \u2260 S[-1] then\n7:             return prefix + SP(I,prefix[: -1])\nreturn S\n\nWe do not expect realistic AI agents to follow the CPC criterion exactly or explicitly. Regenerating the\nplanning stack from all levels every action would likely be slow and costly - in practice, we predict that AI\nagents will have stepping back behaviors that more closely approximate CPC as they become more capable.\nNext we argue that in the limit, following the CPC criterion in full generality can lead to misalignment.\n\nWe propose the CPC-destabilization threat model. First, if an agent exactly follows the CPC criterion when\ndeciding to step back or not, then if there exists an item in its planning stack that would lead to different\nobject-level priorities when considered, the agent will step back and reconsider that item. Second, if an\nagent's alignment is not reflectively stable, then there exists a high-level item in its planning stack that it\nwould abandon upon explicit consideration. This would likely cause a dramatic shift in object-level priorities.\nBy hypothesis, then, an agent whose stepping-back behavior follows the CPC criterion and whose alignment\nis unstable will become misaligned.\n\nThese two requirements are the risk factors for CPC-destabilization that we will investigate.\n\n1. CPC-Based Stepping Back: the agent steps back if and only if the CPC criterion is true.\n2. Preference Instability: if the agent explicitly examined its high-level preferences, it would choose to\nmodify them.\n\nIn the language of Risks from Learned Optimization[Hubinger et al., 2021], an agent whose alignment is\nunstable can be thought of as a suboptimality-aligned mesa-optimizer. This is a specific type of mesa-optimizer,"}, {"title": "2 Methods", "content": "In this work, we conducted three experiments.\n\nCPC Curves: in this experiment we aim to measure agreement between the CPC criterion and the\nLLM's in-fact stepping-back behavior. We applied this experiment to GPT-3.5-turbo and GPT-4.\nMulti-Armed Bandit: in this experiment we aim to measure a toy case of dynamic planning ability.\nWe applied this experiment to GPT-3.5-turbo, GPT-4, GPT-4-turbo, and GPT-4o.\nPreference Cycles: in this experiment we aim to measure the strength of reflective pressure towards\nstable preferences. We applied this experiment to GPT-3.5-turbo and GPT-4."}, {"title": "2.1 CPC Curves", "content": ""}, {"title": "2.1.1 Evaluation", "content": "In this experiment, we want to test CPC-based stepping back: how closely does the CPC criterion predict the\nLLM's actual stepping back behavior? To study this, we need some dataset of LLM stepping back behavior,\nand the ability to measure whether or not the CPC criterion is true. To generate our dataset of stepping\nback behavior, we artificially set an LLM off down the wrong track to solving a problem, such that it must\nnotice the \"mistake\" and switch strategies to get the correct answer. This lets us study dynamic planning\nin a simple, known case: stepping back from level N to level N-1. Then, to measure the CPC criterion, we\ninterrupt the problem-solving transcript and prompt the LLM to re-examine its current priorities and decide\nwhether or not its current object-level approach is best. If we provide the LLM unlimited time to think\nthrough its decision, this lets us directly measure the CPC criterion.\n\nAt a high level, our experiment involved the following steps:\n\nGenerate a problem dataset which will require switching to solve.\nPrompt the target LLM to solve each problem in the dataset, generating a chain of thought transcript.\nJudge where in each transcript the target LLM in fact switches strategies.\nAt intervals through each chain of thought, interrupt and prompt the target LLM to decide whether\nor not it should switch strategies, based on the transcript prefix up until that point."}, {"title": "2.1.2 Validation", "content": "This experiment requires measuring two important quantities: where in the transcript the target LLM in fact\nswitches its strategy, and the target LLM's CPC decision at each position. For each of these quantities, we\nvalidated our measurement approaches on a synthetic test set.\n\nSwitch Judging\nTo validate our measurements of where in the transcript the target LLM in fact switches its strategy, we\nproduced a simple synthetic dataset to check against.\n\nEach example in this dataset was composed of the first 200 characters of an LLM chain of thought prompted\nto solve a quadratic equation by factoring, followed by the first 200 characters of the same LLM solving the\nsame quadratic by the quadratic formula instead. On this dataset, the correct answer to whether or not\nthe LLM had switched its strategy yet at each index was always [F, F, F, F, T, T, T, T]. We then tested\nseveral different LLMs on their ability to judge whether or not the transcript had switched from factoring\nto using the quadratic formula at a given point. We also tested different prompts, and an error-correcting\npost-processing method.\n\nWe measured the accuracy at each prefix position, to assess false positives versus false negatives.\n\nWe found that:\n\nGPT-4 significantly outperformed the other models tested (Figure 4)\nA verbose prompt explaining the task significantly improved performance (Figure 5)\nIncluding the full context followed by the prefix in question slightly improved performance (Figure 5)\nPost-processing slightly improved performance (Figures 4 and 5)"}, {"title": "CPC Extraction", "content": "To extract a CPC decision from the target LLM, we needed to describe the CPC decision setting, elicit a\nchain of thought on the topic, and then extract a yes/no answer. We validated our methods for each of these\nsteps. First, we validated prompts to extract chain of thought from our LLMs.\n\nWe tested various chain of thought prompts to see which of them elicited a \"real\" chain of thought, in which\nthe answer was not decided early in the resulting text. We tested these prompts on both GPT-3.5-turbo and\nGPT-4, as these are the LLMs we will eventually put through the whole experimental pipeline.\n\nA chain of thought prompt was considered \"better\" if it led to fewer early answers.\n\nWe found that the prompts which best avoided early answers (within the first half of the text) was not\nconsistent between GPT-3.5-turbo and GPT-4. We chose the prompt \"think through the question but do not\nanswer yet\", because it worked reasonably well for both LLMs (Figure 7)."}, {"title": "2.2 Multi-Armed Bandit", "content": ""}, {"title": "2.2.1 Evaluation", "content": "In this experiment, we assess LLM performance on the Multi-Armed Bandit (MAB) problem. This is meant\nto be a toy example of dynamic planning - the LLM gets information about the value of each arm, and must\nupdate its decisions accordingly.\n\nThe MAB was set up with three arms, each paying out from a Gaussian distribution with different means\nbut the same standard deviation. The means of the arms were 50, 100, and 150. By varying the standard\ndeviations, we controlled the difficulty of the problem, with higher standard deviations making it harder to\ndistinguish the best arm.\n\nEach LLM was tested with four different algorithmic instructions: random, intuitive, greedy, and Upper\nConfidence Bound (UCB).\n\nRandom: The LLM was instructed to uniformly choose from the available actions at random.\nIntuitive: The LLM was instructed to choose the best arm it can, without any explicit algorithmic\nguidance except for a reminder to try each arm at least once.\nGreedy: The LLM was instructed to try each arm once, and to thereafter always choose the arm\nwith the highest average return.\nUCB: The LLM was provided with a detailed description of the UCB algorithm and asked to follow\nits steps, calculating the UCB score for each arm and choosing the arm with the highest score.\n\nFor each \"algorithm,\" the LLMs were further evaluated under both one token and chain of thought prompt\nsettings. In the one token setting, the LLM was forced to directly output its choice (\"A\", \"B\", or \"C\") in a"}, {"title": "2.2.2 Validation", "content": "We encountered a few challenges in capability elicitation during the development and validation of the MAB\nexperiment.\n\nThe first issue was information sparsity in the prompt. Initially, a classic chatbot-style conversation format\nwas used to interact with the LLMs. However, the LLMs struggled to extract the relevant information from\nthe full context, which was long and relatively information-sparse. To address this, we changed the prompts\nto a compressed form which would include only the essential information: the rules of the game, a summary\nof the turns so far, and the number of turns remaining. This more focused prompt format led to improved\nperformance.\n\nThe second issue was the tendency of LLMs following the intuitive strategy to repeat a single action, such\nas pulling lever A every turn without exploring the other options. In the reasoning transcripts, the LLMs\nseemed to consider the initial payout from lever A as high in absolute terms, and would conclude that they\nhad found the optimal strategy. To mitigate this issue, we adjusted the prompts to explicitly encourage\nexploration of all available arms."}, {"title": "2.3 Preference Cycles", "content": ""}, {"title": "2.3.1 Evaluation", "content": "In this experiment, we aim to assess the strength of pressures towards reflectively stable preferences in\nLLMs. There is theoretical debate[Hendrycks and Thornley, 22023] about which sorts of preference structures\nlead to dominated or otherwise obviously inferior problem-solving strategies. We choose to focus on cyclic\npreferences as a particular case that may be unstable, as they admit the construction of very simple\nmoney-pumps[Gustafsson, 2022].\n\nOur experiment involves the following steps:\n\nGenerate a problem setting which involves multiple distinct \"resources\" and a clear objective\nProvide a specific problem state and prompt the target LLM to choose which of two resource-units it\nwould rather have. Repeat for all entries in a pairwise preference matrix, except the diagonal\nConvert the preference matrix to a directed graph and find all non-repeating cycles in the graph.\nPrompt the target LLM to choose whether or not it wants to modify its preferences to remove a\nparticular cycle\n\nThis experiment produces two main results which could each be considered relevant to \"preference stability\":\nnumber of cycles present, and proportion of cycles kept/rejected. The fewer cycles present, the closer to\nreflectively stable the LLM's stated preferences are. The greater the proportion of cycles rejected, the stronger\nthe LLM's reflective tendency to make its preferences more stable.\n\nWe chose the card game Dominion as a problem setting because it has a clear objective and ruleset, and\nbecause choosing which of a set of cards is the most valuable to add to the deck at a given state of the game\nis a core activity of gameplay. We used temperature 0, and performed sampling by using different subsets\nof 6 cards chosen at random from the First Game cardset. Cards and card sets were both chosen without\nreplacement."}, {"title": "2.4 Validation", "content": "To validate the LLM's basic Dominion card-prioritization knowledge, we checked the rates at which the LLM\nmade the correct choice when faced with pairs of cards where one is strictly better than the other. There are\nfour available pairs to check: Gold>Silver, Silver>Copper, Province>Duchy, and Duchy>Estate. With a\nsample size of 100, we found that GPT-3.5-turbo made the correct choice 100, 100, 100, and 89 times. GPT-4\nmade the correct choice 100 times for all four. We take these results to show that the LLMs are performant\nenough at basic card-prioritization that any cyclicity we observe shouldn't be attributed to incompetence in\nthis domain.\n\nTo account for the effects of prompt sensitivity, we collected results over six total prompts when eliciting\ncycle stability responses. We used three different phrasings of the question, and positive/negative versions of\neach phrasing, such as \u201cdo you want to keep this preference cycle\u201d vs \u201cdo you want to remove this preference\ncycle\".\n\nWe considered using a more principled metric for the \"degree of cyclicity\" of a given preference matrix, such as\nthe minimum number of edits needed to remove all cycles - this is called the feedback arc set. Unfortunately,\nfinding the minimum feedback arc set is NP-hard, and we were not able to compute it for graphs of the size\nwe studied."}, {"title": "3 Results and Discussion", "content": ""}, {"title": "3.1 CPC Curves", "content": "The primary takeaway from Figure 10 is that the CPC curve generated by GPT-4 resembles the hypothetical\ncurve given in Figure 3 for an agent which is imperfectly following the CPC criterion. On the other hand, the\nCPC curve generated by GPT-3.5-turbo bears no resemblance to Figure 3, besides having some discontinuity\nnear index 0. This supports our hypothesis that more capable LLMs have more CPC-based stepping back\nbehavior.\n\nThe confidence intervals in Figure 10 depend on the number of samples available at the given index, and fewer\nsamples are available towards the ends as fewer transcripts extend out to the extremes. This leads to wider\nconfidence intervals towards the ends. Eventually, confidence intervals approach zero width when all samples\nare either 1 or 0 - this is an artifact of using the Wald confidence interval for a binomial variable. However,\nthese low-confidence areas can mostly be ignored, as the relevant results are close to the switching point.\n\nGPT-3.5-turbo's CPC curve looks quite different than what we expect an optimal CPC curve to look like\nin fact, almost the opposite. We suspected a bug at first, but confirmed that our methods were correct by\ninspecting the chains of thought generating during CPC decisions. The transcripts contained many mistakes\nand wrong choices, but the answers given were consistent with the answers extracted."}, {"title": "3.2 Multi-Armed Bandit", "content": "Due to limitations in the dataset, formal statistical hypothesis testing was not feasible for these results.\nConsequently, this section presents a primarily qualitative analysis of the data illustrated in Figure 12.\n\nAs expected, in the majority of (nonrandom) cases, accuracy declines as the standard deviation of the payout\ndistributions increases. This trend is consistent across different algorithmic approaches and prompt types,\nconfirming our expectation that higher variance makes the MAB problem more challenging."}, {"title": "3.3 Preference Cycles", "content": "The plot on the left in Figure 13 shows that GPT-4 shows consistently far fewer cycles in its preference matrix\nthan GPT-3.5-turbo. Because cyclic preferences admit money pumps, this suggests that GPT-4 has fewer\nreflective instabilities in its preferences. It is not clear how to interpret this. One interpretation is that more\nadvanced LLMs are approaching a reflectively stable set of corrigible preferences, even though theoretical\nwork has yet to describe such a set. This would be because GPT-4 is closer to reflectively stable but does not\nobviously behave less corrigibly than GPT-3.5-turbo, insofar as they both follow user instructions and do not\nseem to resist shutdown. The other interpretation is that more advanced LLMs are on track to becoming\nincorrigible, because more advanced capabilities require fewer reflective instabilities, and corrigibility only\nbecomes very rare when the number of reflective instabilities is quite close to zero.\n\nIn the plot on the right in Figure 13, we see that when confronted with a cycle in its stated preferences, GPT-4\nchooses to remove the cycle 100% of the time, across all six prompt configurations, while GPT-3.5-turbo does"}, {"title": "4 Conclusion", "content": "In order to investigate whether or not reflective stability problems will arise in long-horizon-competent LLMs,\nwe proposed the CPC-destabilization threat model, and conducted three evaluations to measure risk factors\nfor CPC-destabilization. In Experiment 1, we assessed how closely the LLM's in-practice switching behavior\nfollows the CPC criterion, and found that GPT-4 displayed a clear CPC curve, while GPT-3.5-turbo did\nnot, suggesting that more capable LLMs have more CPC-based stepping back behavior. In Experiment 2,\nwe measured how well LLM's perform on the Multi-Armed Bandit problem, and found that performance is\nstill quite limited, and did not observe an obvious scaling trend in switching capabilities for this setting. In\nExperiment 3, we used preference cycles among Dominion cards as a way to measure how close to reflectively\nstable LLM's preferences are, and found that GPT-4 has more consistent preferences and a stronger tendency\nto remove remaining inconsistencies than GPT-3.5-turbo.\n\nWhile these results are very preliminary, they suggest that the risk factors for CPC-destabilization that we\nidentified are increasing with LLM capabilities from GPT-3.5-turbo to GPT-4. If this trend continues, future\nhighly capable LLMs may be much more difficult to align using methods which don't account for reflective\nstability.\n\nThere is much room for future work to improve on these experiments. A few particularly notable examples:\n\nQuantitative metrics for CPC-based stepping back behavior\nApplying the CPC curve method to other problem settings\nA principled way to deal with the variation introduced by prompt sensitivity\nMore effective or better-understood capability elicitation\nInvestigation of how to extract useful cognitive labor from an LLM if the LLM's problem-solving\napproach convergently leads it to some unacceptable action, in a way which cannot be trained away\nwithout damaging capabilities."}]}