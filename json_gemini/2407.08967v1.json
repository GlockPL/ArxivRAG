{"title": "Empowering Few-Shot Relation Extraction with The Integration of Traditional RE Methods and Large Language Models", "authors": ["Ye Liu", "Kai Zhang", "Aoran Gan", "Linan Yue", "Feng Hu", "Qi Liu", "Enhong Chen"], "abstract": "Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE) that utilizes limited training instances, appeals to more researchers in Natural Language Processing (NLP) due to its capability to extract textual information in extremely low-resource scenarios. The primary methodologies employed for FSRE have been fine-tuning or prompt tuning techniques based on Pre-trained Language Models (PLMs). Recently, the emergence of Large Language Models (LLMs) has prompted numerous researchers to explore FSRE through In-Context Learning (ICL). However, there are substantial limitations associated with methods based on either traditional RE models or LLMs. Traditional RE models are hampered by a lack of necessary prior knowledge, while LLMs fall short in their task-specific capabilities for RE. To address these shortcomings, we propose a Dual-System Augmented Relation Extractor (DSARE), which synergistically combines traditional RE models with LLMs. Specifically, DSARE innovatively injects the prior knowledge of LLMs into traditional RE models, and conversely enhances LLMs' task-specific aptitude for RE through relation extraction augmentation. Moreover, an Integrated Prediction module is employed to jointly consider these two respective predictions and derive the final results. Extensive experiments demonstrate the efficacy of our proposed method.", "sections": [{"title": "1 Introduction", "content": "Relation Extraction (RE) aims to determine the relation expressed between two entities within an unstructured textual context [23]. Few-Shot Relation Extraction (FSRE), as a subtask of RE, seeks to solve the RE problem by utilizing only K instances per relation (K-shot) in the training and validation phases [3,20]. The primary methodologies employed to address the FSRE task have been fine-tuning or prompt tuning techniques grounded on Pre-trained Language"}, {"title": "2 Related Work", "content": "Few-shot Relation Extraction. Due to the large computation ability of pre-trained language models, existing few-shot relation extraction methods mainly adopt the fine-tuning method to solve the few-shot relation extraction prob-lem [13,23]. In recent years, in order to bridge the gap between pre-training objectives and RE task, prompt tuning has been proposed and demonstrated remarkable capability in low-resource scenarios [3,6,7].\nCurrently, with the arise of large language models, many researchers at-tempt to tackle few-shot relation extraction via In-Context Learning technol-ogy [5,19,20]. However, these approaches simply apply LLMs to few-shot relation extraction tasks through straightforward queries, which fails to fully harness the potential of LLMs. More importantly, they overlook the possibility that LLMs and traditional RE models could mutually enhance each other's performance.\nLarge Language Models. The emergence of Large Language Models (LLMs) such as GPT-4, LLama-2 and others [14,15,17,18], represents a significant ad-vancement in the field of natural language processing. By leveraging In-Context Learning, a novel few-shot learning paradigm was first introduced by [2]. Up to now, LLMs have demonstrated remarkable performance across a range of NLP tasks, such as text classification, named entity recognition, question answering and relation extraction [5,8,19,20].\nPrevious research efforts [5,19,20] have sought to solve few-shot relation ex-traction by directly asking LLMs or retrieving more suitable demonstrations. For instance, Wan et al. [19] attempted to introduce the label-induced reasoning logic to enrich the demonstrations. Meanwhile, Xu et al. [20] designed task-related instructions and a schema-constrained data generation strategy, which could boost previous RE methods to obtain state-of-the-art few-shot results."}, {"title": "3 Problem Statement", "content": "Let C denote the input text and \\(e_{sub} \\in C\\), \\(e_{obj} \\in C\\) denote the pair of subject and object entities. Given the entity type of \\(e_{sub}, e_{obj}\\), and a set of pre-defined relation classes R, relation extraction aims to predict the relation \\(y \\in R\\) between the pair of entities \\((e_{sub}, e_{obj})\\) within the context C [19,23].\nAs for the few-shot settings, following the strategy adopted by [4,20], we ran-domly sample K instances per relation (K-shot) for the training and validation phases. The whole test set is preserved to ensure the effectiveness of evaluation."}, {"title": "4 DSARE Model", "content": "4.1 LLM-augmented RE\nLLM Data Augmentation. In this part, we aim to implement the data aug-mentation via LLMs, anticipated to enrich the training data for relation extrac-tion. Specifically, drawing inspiration from [20], we construct prompts to tell the"}, {"title": "4.2 RE-augmented LLM", "content": "KNN Demonstration. In Section 4.1, we train a traditional relation extraction model, which allows us to implement a k-nearest neighbors (KNN) search method to retrieve more valuable samples from the training set. Specifically, we utilize the obtained entity representation \\(H = [h_{sub}, h_{obj}]\\) to represent each sample, and further obtain the representation and label pair \\((H_i, r_i)\\) on the training set, which we denote as a datastore D.\nWhen inferring a new sample j, we utilize its entity representation \\(H_j\\) to query D according to the euclidean distance to obtain the k nearest neighbors: \\(N = \\{(H_i, r_i)\\}_{i=1}^k\\), which we adopt as demonstrations for LLM inference."}, {"title": "4.3 Integrated Prediction", "content": "In Section 4.1 and 4.2, we apply traditional RE models and LLMs to conduct few-shot relation extraction from dual perspectives. In this part, we aim to obtain the final outputs by considering both the LLM-augmented RE inference result \\(P_{re}\\) and the RE-augmented LLM inference result \\(P_{llm}\\).\nMore specifically, as illustrated in Figure 2, if the two results are equal (i.e., \\(P_{re} = P_{llm}\\)), our model directly yields the predicted relation. In circumstances where the two results diverge, we design a selector to further ask the LLM to make a choice between these two relations. In order to improve the effectiveness of the selector, we directly retrieve m samples labeled with these two relations from the training dataset, respectively. Subsequently, we ask the LLM via a similar way we introduced in LLM Inference to obtain the final results\u00b9."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDatasets and Evaluation Metrics. For extensive experiments, we conduct our experiments on three widely-used relation extraction datasets: TACRED [22], TACREV [1] and Re-TACRED [16]. More statistics about the datasets can be found in Table 1. Regarding the evaluation metrics, we adopt the micro-F1 scores of RE as the primary metric to evaluate models, considering that F1 scores can assess the overall performance of precision and recall [3,9,10,21]."}, {"title": "5.2 Experimental Result", "content": "The main results are illustrated in Table 2. Our proposed DSARE model outper-forms all baselines across all metrics. Particularly on the TACRED and TACREV datasets, our method manifests a significant advantage. This demonstrates the effectiveness of our designs and the benefits of integrating traditional RE models and LLMs. Furthermore, there are also some interesting phenomena:\nFirst, the vast majority of methods exhibit superior performance on the Re-TACRED dataset compared to the TACRED and TACREV datasets. This is reasonable as Re-TACRED is an improved version among these three datasets, which addresses some shortcomings of the original TACRED dataset, refactors its training set, development set and test set. The more precise labels contribute to the learning process of these models, thereby yielding superior performance.\nSecond, among these LLM-based methods, Zephyr (7B) demonstrates compet-itive performance and significantly outperforms GPT-3.5 and LLama-2 on the TACRED and TACREV datasets. This proves its strong information extraction ability, as claimed in [18]. Third, Unleash introduces a schema-constrained data augmentation method through LLMs to enhance the Knowprompt baselines. It achieves a certain degree of improvement compared to Knowprompt, verifying the the feasibility of this line of thinking. And our DSARE model significantly surpasses Unleash, which further demonstrates the effectiveness of our designs from another perspective."}, {"title": "5.3 Ablation Study", "content": "In this subsection, we carry out ablation experiments to validate the effectiveness of various components of DSARE model. Specifically, we first remove the Inte-grated Prediction module, consequently leading to two ablated variants: LLM-augmented RE and RE-augmented LLM. As shown in Table 3, there are obvious"}, {"title": "5.4 Case Study", "content": "In this section, we conduct case study to more intuitively illustrate the effective-ness of integrating traditional RE models and LLMs. Specifically, as illustrated in Figure 3, we present the input information (i.e., document, subject/object entity, subject/object entity type), ground truth relation and the prediction of DSARE and its ablated variants, respectively.\nIn Figure 3 (a), both the LLM-augmented RE and the RE-augmented LLM make the correct prediction. In Figure 3 (b) and (c), the LLM-augmented RE and RE-augmented LLM correctly infer the relations (per : identity and per : siblings), respectively. And with the aid of the Integrated Prediction module, DSARE finally derives the correct predictions. These cases intuitively demon-strate the significant role of integrating traditional RE methods and LLMs, and further verify the validity of our DSARE model."}, {"title": "6 Conclusions", "content": "In this paper, we explored a motivated direction for empowering few-shot relation extraction with the integration of traditional RE models and LLMs. We first an-alyzed the necessity to joint utilize traditional RE models and LLMs, and further proposed a Dual-System Augmented Relation Extractor (DSARE). Specifically, we designed a LLM-augmented RE module, which could inject the prior knowl-edge of LLMs into the traditional RE models. Subsequently, a RE-augmented LLM module was proposed to identify and retrieve the most valuable samples from the training data, which provided more useful demonstrations for the In-Context Learning of LLMs. More importantly, we designed an Integrated Predic-tion module to joint consider the predictions of both LLM-augmented RE and RE-augmented LLM modules, thus taking advantages of each other's strengths and deriving the final results. Finally, extensive experiments on three publicly available datasets demonstrated the effectiveness of our proposed method. We hope our work could lead to more future studies."}]}