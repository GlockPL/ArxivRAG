{"title": "Automatic Detection of LLM-generated Code: A Case Study of Claude 3 Haiku", "authors": ["MUSFIQUR RAHMAN", "SAYEDHASSAN KHATOONABADI", "AHMAD ABDELLATIF", "EMAD SHIHAB"], "abstract": "Using Large Language Models (LLMs) has gained popularity among software developers for generating source code. However, the use of LLM-generated code can introduce risks of adding suboptimal, defective, and vulnerable code. This makes it necessary to devise methods for the accurate detection of LLM-generated code. Toward this goal, we perform a case study of Claude 3 Haiku (or Claude 3 for brevity) on CodeSearchNet dataset. We divide our analyses into two parts: function-level and class-level. We extract 22 software metric features, such as Code Lines and Cyclomatic Complexity, for each level of granularity. We then analyze code snippets generated by Claude 3 and their human-authored counterparts using the extracted features to understand how unique the code generated by Claude 3 is. In the following step, we use the unique characteristics of Claude 3-generated code to build Machine Learning (ML) models and identify which features of the code snippets make them more detectable by ML models. Our results indicate that Claude 3 tends to generate longer functions, but shorter classes than humans, and this characteristic can be used to detect Claude 3-generated code with ML models with 82% and 66% accuracies for function-level and class-level snippets, respectively.", "sections": [{"title": "INTRODUCTION", "content": "With the growing popularity of Large Language Models (LLMs) and the use of LLM-generated code in software engineering, it is now more critical than ever to build robust systems that can accurately detect code snippets written by LLMs. The importance of the detection task can be motivated by existing literature that reports that LLMs can provide vulnerable code [56]. Another issue of controversy regarding the use of LLM-generated code is the ownership of the code [42]. Who wons the LLM-generated code? \u2013 still remains an open question."}, {"title": "DATASET", "content": "To compare human-written and Claude 3-generated code we need a data source containing code already authored by human programmers so that we can generate code using Claude 3 for the same task. In this section, we explain how we choose our data source, and generate corresponding code from Claude 3 for our analysis."}, {"title": "Data Source", "content": "As mentioned before, we break our analyses into two levels: function-level and class-level. In the following, we explain how we prepare the dataset for these two levels of code."}, {"title": "Function-level", "content": "In this work, we choose CodeSearchNet [39] as our data source for function-level code. We chose this dataset because our goal in this paper is to study real-life software projects and this dataset was curated using real-life OSS projects from GitHub. Furthermore, it has been used by many existing works [14, 29, 31, 53, 55, 61, 66, 72]. CodeSearchNet is a collection of functions (both standalone functions and methods) extracted from real-life projects on GitHub along with their function signatures compiled as (comment, code) pairs. A comment refers to a top-level function docstring [2], and a code refers to the corresponding human-written function. An example of such (comment, code) pair is shown in Listing 1."}, {"title": "Class-level", "content": "To perform a comparative analysis between function-level and class-level Claude 3-generated code, we curate our class-level dataset by extracting standalone classes from the same OSS projects that were used in curating the CodeSearchNet dataset. A class is a standalone class when no other classes inherit from this class and this class inherits from no other class. There are two main reasons behind choosing only standalone classes. Firstly, when there is a hierarchical relationship between classes due to inheritance, an LLM (Claude 3 in this case) needs to be prompted with not only class definitions but also many other contexts associated with the class hierarchy. This makes the input prompts arbitrarily long which becomes too expensive in terms of both cost and processing time related to code generation. Secondly, the code snippets used in the function-level analysis are all standalone functions. Therefore, by choosing only standalone classes we make sure that the source as well as the basic characteristics of the data for both function-level code and class-level code remain identical. Following is an example of a standalone class with its docstring from our dataset."}, {"title": "Code Generation with Claude 3:", "content": "We chose Claude 3 for our case study because, at the time of writing this paper 1, 1, it is one of the top 3 best-performing models for Python code generation and the cheapest one among the top 3 [7]. We use function and class docstrings as part of the prompt sent to the model, and the response received from the model is the corresponding Claude 3-generated code. We format our prompt as follows:\n\nTo reduce the cost of generating code with Claude 3, we added the \u2018Do not explain the code' instruction as part of the prompt so that the generated response does not get unnecessarily long. With the output from this step, we obtain pairs of human-written code and corresponding Claude 3-generated code for all functions and classes in our dataset."}, {"title": "Feature Extraction:", "content": "Exisiting works on program comprehension reveal that software metrics can be a valuable source of information for understanding the properties of a piece of software [25, 63, 73]. Building on top of this existing finding, we aim to leverage software metrics from the point of view of distinguishing"}, {"title": "RQ1: THE EXPLORATORY ANALYSIS: HOW UNIQUE IS CLAUDE 3-GENERATED CODE?", "content": "Identifying the unique characteristics of Claude 3-generated code is the first step toward building predictive models for the detection task. To achieve that goal, we set out to understand the uniqueness of the code generated by Claude 3. In this RQ, we aim to identify and quantify to what extent the generated code differs from human-written code with respect to the extracted features in the previous section. In the following, we first explain our approach and then discuss our findings for answering this RQ."}, {"title": "Approach", "content": "To determine the differences in the features described in  between Claude 3-generated code and human-written code, we begin our analysis by comparing, for each feature, the distributions of the generated code and human-written code. Next, we test the statistical significance and practical significance of the differences between the distributions.\nWe perform Mann-Whitney U test [52] to check the differences in the extracted features. We chose this method because the features are not guaranteed to follow a normal or near-normal distribution and as a nonparametric test, this method does not require the distribution of data to be normal. We set the level of significance $\u03b1 = 0.01$, which determines the probability of observing the obtained results due to chance.\nHypothesis tests, such as Mann-Whitney U test, tell us whether or not there is a statistically significant difference between two distributions. However, such tests do not convey any information about how big or small the difference is. For this purpose, we use Cliff's delta [21] which estimates the magnitude of the difference, also known as effect size. Cliff's delta, $d$, is bounded between \u20131 and 1. Based on the value of d, the effect size can be categorized as one of the following qualitative magnitudes [36]:\nNegligible, if $|d| \u2264 0.147$\nSmall, if $0.147 < |d| \u2264 0.33$\nEffect size =\nMedium, if $0.33 < |d| \u2264 0.474$\nLarge, if $0.474 < |d| \u2264 1$\nIn this study, any effect size other than \u2018negligible' is considered to be of practical significance. If a feature is both statistically and practically different between Claude 3-generated code and human-written code then we consider that feature to be significantly different."}, {"title": "Findings", "content": "Function-level: In  the middle column presents the differences in the features of Claude 3-generated code compared to human-written code on a function level. We find that total 9 features are significantly different on a function level. 8 out of these 9 features are code stylometric features and only one is code complexity feature (Average Cyclomatic Complexity). The biggest effect size (medium) observed is for Average Comment Lines, Blank Lines, Comment Lines, and Comment to Code Ratio. For all of these four features, Claude 3-generated code has greater value than Human-written code. However, in terms of Average Code Lines, humans tend to write longer code compared to Claude 3 meaning that Claude 3 generates code in a more concise manner than their human programmer counterparts. However, due to the presenence of more comments and blank lines Clude 3-generated code is overall lenghtier than human-written code as evident from Lines feature. To identify the underlying reason behind this finding, we perform a qualitative analysis. We observe that Claude 3-generated code snippets, excluding comments, are relatively smaller than human-written code. An example is presented in . We can see from the example that the core functionality of the function is implemented in a broken-down way (lines 5-15) by the human programmer whereas Claude 3 implemented the similar functionality in only a few lines of code (lines 16, 19, and 22). We conjecture from this finding that on a function level, humans usually write code in small steps one at a time to facilitate program comprehension and readability. Breaking down complex tasks into multiple smaller sub-tasks can make a piece of code more readable and understandable, which makes it easier to debug. On the contrary, Claude 3 does not consider the potential cognitive burdens that may be posed by implementing a complex task in fewer lines of code. The potential effect of this style of implementation can be seen in all other\nfeatures under consideration. For example, Average Cyclomatic Complexity shows that generated code is less complex than human-written code, which can be directly attributed to the shorter length of the generated code snippets. In other words, the lesser the number of Average Code Lines, the lesser the number of decision points, and hence the lesser the complexity of the code."}, {"title": "Class-level", "content": "The right-most column in  presents the differences in the features of Claude 3-generated code compared to human-written code on a class level. Only 6 features including 4 code stylometric features (Average Lines, Average Blank Lines, Average Code Lines, and Executable Code Lines) and 2 code complexity features (Average Cyclomatic Complexity, and Max Cyclomatic Complexity) show significant differences between the generated class-level code snippets and the corresponding human-written snippets. The degree of difference is less compared to the function-level code with a samll effect size in all 6 cases. The class-level result shows that human-written code is slighter longer than the Claude 3-generated code, however, in this case, the difference is relatively smaller compared to the generated functions. Qualitaitve analysis of the classes reveals that, similar to standalone functions, Claude 3 tend to write more conscise methods within a class than human programmers which eventually reduces the overall length of the class as shown in"}, {"title": "RQ2: THE DETECTION: HOW WELL CAN WE DETECT CLAUDE 3-GENERATED CODE?", "content": "Findings from RQ1 show that Claude 3-generated code has unique characteristics that are both statistically and practically significant on both function and class levels. Our aim in RQ2 is to leverage the uniqueness of the generated code to build ML models that can accurately distinguish between human-written and Claude 3-generated code."}, {"title": "Approach", "content": "To determine how well predictive models can differentiate between Claude 3-generated code and human-written code, we experiment with different families of classifiers. The classifiers we train are Logistic Regression (LR) [24] (a linear classifier), K-Nearest Neighbour (KNN) [23] (a distance-based classifier), Support Vector Machine (SVM) [35] (a kernel-based classifier), Random Forest (RF) [20] (a tree-based bagging classifier), and CatBoost (CB) [57] (a tree-based boosting classifier). We choose these classifiers because they have been used in existing software engineering literature and have shown high performance in software engineering tasks [16, 30, 38, 40, 44, 45, 70].\nFor all models, the target variable is whether the author is Claude 3 or human and the features are the software metrics extracted in RQ1. However, we realize that many features extracted are highly correlated with each other. We remove the highly correlated features because keeping correlated features can have a negative effect on the interpretation of the models [27]. We use Spearman's correlation [64] to identify the correlated features. If two features have a Spearman correlation coefficient $\u03c1 \u2265 0.8$, we keep one of the features. We also remove the features that\nhave a 'negligible' difference (obtained from Cliff's delta) between human-authored and Claude 3-generated code because they are unlikely to add any additional information for the model to learn from. For the function-level data the features used to train the models are Average Blank Lines, Average Code Lines, Average Comment Lines, Average Cyclomatic Complexity, Executable Units, Lines, and Comment To Code Ratio. For the class-level data the features used are Average Blnak Lines, Executable Code Lines, Average Cyclomatic Complexity, and Average Code Lines. For each model, we perform a K-fold cross-validation [26, 33] with K = 10 which gives an estimate of a classifier's generalization ability [17] to reduce bias in evaluation. The performance of the classifiers is determined based on the classification metrics listed below. Our datasets do not suffer from class unbalance which makes sure that none of these metrics show biased results towards one or the other class [32]. In the following classification metrics, TP stands for True Positives - the number of correctly classified Claude 3-generated code, FP stands for False Positives - the number of human-written code incorrectly classified as Claude 3-generated code, TN stands for True Negatives - the number of correctly classified human-written code, and FN stands for False Negatives - the number of Claude 3-generated code incorrectly classified as human-written code.\n\u2022 Precision: Also known as Positive Predictive Value, this metric determines what proportion of data points classified as positive class is correctly classified.\n$Precision = \\frac{TP}{TP + FP}$\n\u2022 Recall: Also known as Sensitivity or True Positive Rate, this metric determines how well a model can classify the positive class.\n$Recall = \\frac{TP}{TP + FN}$\n\u2022 Accuracy: This metric determines the proportion of data points correctly classified by the model.\n$Accuracy = \\frac{TP + TN}{TP + FP+ TN + FN}$\n\u2022 F1-score: This is the harmonic mean of Precision and Recall.\n$F1-score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n\u2022 AUC-ROC: This metric measures the area under the Receiver Operating Characteristic (ROC) curve [19].\nAll the aforementioned classification metrics are bounded between 0 and 1. The closer the value is to 1, the better the performance of the model is."}, {"title": "Findings", "content": "Function-level: The column representing the function-level detection performance in shows that the tree-based models, specifically the CB model, outperform other families of models across all of the metrics except Recall. The detection performance improvement achieved by the CB model range from 1% to 4% for Precision, from 2% to 7% for Accuracy, from 2% to 9% for F1-Score, and from 2% to 7% for AUC-ROC. In the case of Recall, the SVM model shows between 1% and 16% improved performance compared to other models.\nClass-level: The right-most column in  shows the performance of different models for class-level detection. The detection performance achieved is identical to the function-level detection in that the CB model outperforms all other models with respect to all metrics except Recall. As evident from both class-level and function-level detection performance, the SVM model achieves higher Recall compared to all other models. However, in class-level detection Recall, the SVM model outperforms other models with a much bigger margin with an increased Recall ranging from 14% to 49%. In the case of all other metrics, the increased performances achieved by the CB model range from 2% to 8% for Precision, from 2% to 4% for Accuracy, from 4% to 19% for F1-Score, and from 2% to 6% for AUC-ROC.\nThe CB model can detect Claude 3-generated code with an average improved performance of 2.5% to 5% in Precision, 3% to 4.5% in Accuracy, 5.5% to 11.5% in F1-Score and 4% to 4.5% in AUC-ROC. The SVM model, on the other hand, can detect Claude 3-generated code with an average improved performance of 8.5% to 31.5%."}, {"title": "RQ3: THE EXPLANATORY ANALYSIS: WHAT ARE THE MAJOR PREDICTORS IN DETECTING CLAUDE 3-GENERATED CODE?", "content": "As mentioned before the goal of this research is not only to study how well Claude 3-generated code can be automatically detected using ML techniques but also to explain the performance of the detection models by identifying which features have the maximum impact on the detection performance. The explainability of these models can pave the way for new research on LLM-generated code. With the goal of explainability, we aim to determine which features contribute the most towards the correct detection. In order to find the most impactful features on the model performance, we take the Shapley Additive Explanations [49] or SHAP analysis approach using the SHAP framework [51] to compute Shapely values [67]. Shapely values are a method for showing the relative impact of each feature from a model on the output of the model by comparing the relative effect of the inputs against the average. SHAP is a popular tool which has been used in existing works [40, 44]."}, {"title": "Approach", "content": "In this RQ, we focus on the overall best-performing model which is the CB model. We generate SHAP layered violin plots for the previously trained function-level and class-level Claude 3-generated code detection models. The layered violin plot combines feature importance with feature effects. Each violin represents the distribution of Shapley values for a feature."}, {"title": "Findings", "content": "Function-level: Figure 1 shows that the most important factors in detecting the function-level generated code are code stylometric features representing Lines in the code snippet including Average Code Lines, Average Comment Lines, and Average Blank Lines. The plot shows that as the number of Lines, Blank Lines, and Comment Lines in the snippet increases it is more likely to be Claude 3-generated. However, if the value of Average Code Line in a snippet increases it is more likely to be human-authored. The only non-stylometric feature is Average Cyclomatic Complexity which shows that Claude 3-generated code tends to be less complex than human-written code.\nClass-level: Figure 2 shows that similar to function-level code, class-level Claude 3-generated code can also be detected mostly based on stylometric features. Claude 3 tends to have smaller Average Code Lines, Executable Code Lines, and Average Blank Lines than humans. Similar to function-level code, the only non-stylometric feature contributing to differentiating between human-written and Claude 3-generated classes is Average Cyclomatic Complexity. The Average Cyclomatic Complexity of Claude 3-generated classes is lower than that of human-written classes. For all these features, as the values tend to increase the detected code snippet is more likely to be human-written.\nOur findings show that length-related features are the most dominant ones in differentiating between Claude 3-generated and human-written code. The concise nature of the generated code along with the presence of more comments and blank lines make it detectable using predictive models on a function level, whereas, relatively shorter as well as less complex classes generated by Claude 3 contribute most to the class-level detection."}, {"title": "DISCUSSION", "content": "In this paper, we study the uniqueness of Claude 3-generated code with respect to different features obtained from various software metrics. We do our analysis on two levels of granularity of source code: function-level and class-level. In this section, we discuss the implications of our findings."}, {"title": "Implications for Practitioners", "content": "We find that Claude 3-generated functions and classes have distinct characteristics compared to corresponding human-authored code that can be leveraged to detect the generated code snippets. The detection models obtain better performance on function-level data than class-level data although the major predictors in both cases are mostly related to the length of the code. However, we realize that the detection performance can potentially vary between LLMs. To determine whether the\ndetection performance is indeed affected by what LLM was used to generate code we run an additional experiment on function-level data where we generate code for the same functions using GPT-3.5 2.  shows that although we used the same functions and the corresponding docstrings along with the same prompt to generate the code, the resulting functions were different from different between Claude 3 and GPT-3.5. Accordingly, the detection performance also varies between the two LLMs under investigation. The better the generated code is, the harder it is to be\ndetected."}, {"title": "Implications for Researchers", "content": "Comparing our results with the existing work presented in [40] we find that, on a function level, it is relatively harder to detect Claude 3-generated code for real-life software tasks compared to competitive programming tasks. This finding may be attributed to two things. Firstly, real-life tasks are more complex and diverse compared to competitive programming tasks. This issue is observable in [40] as well where we see the drop in detection performance for more difficult programming tasks. Secondly, unlike competitive programming tasks where the problem is well-defined with a set of (input, output) pairs, real-life tasks may be more abstractly represented in docstrings. Our qualitative analysis reveals that the majority of the real-life function docstrings do not show any example (input, output). Therefore, the prompts passed to an LLM from real-life projects are more abstract than the prompts passed from the programming contest problems. This may cause higher diversity in LLM-generated functions for real-life projects making it harder for the detection models\nto detect the generated code snippets."}, {"title": "RELATED WORK", "content": "With the advancement of LLMs and the increase in the use of LLMs in a variety of software engineering tasks, many researchers have already worked on various applications of LLMs in the domain of software engineering. Although, the work on the detection of LLM-generated code is a relatively new topic of interest in the community, the detection of LLM-generated text has been being worked on for a while. Other related topics on the intersection of LLMs and software engineering exist. In this section, we summarize some of the existing works."}, {"title": "Work on LLM-generated code detection", "content": "Detection of LLM-generated code is a very recent topic of interest among software engineering researchers. In the most recent work by Idialu et al. [40] the authors trained a graident boosting classifier to detect OpenAI's GPT-4-generated code on a function level. They used programming competition problems to generate code from GPT-4. They reported achieving high detection accuracy and, similar to our findings, they also found that the most important features in detecting GPT-4-generated code are code stylometric features. Shi et al. [62] proposed a perturbation-based detection technique inspired by the naturalness of code"}, {"title": "LLMs in software engineering", "content": "Many other works used LLMs in different software engineering tasks. For example, Abedu et al. [12] studied the challenges and opportunities in using LLM-based chatbots in software repository mining. Kang et al. [43] reported the use of LLMs for bug reproduction and program repairs. Wnag et al. [66] proposed \u201cCodeT5+\u201d, which can support programming-related tasks such as natural language to code generation. Other LLM-supported software engineering tasks have been reported including but not limited to automated code review [48, 50], generation\nof comments [47], and code summarization [13]."}, {"title": "Work on LLM-generated text detection", "content": "Much effort has been put into detecting LLM-generated content, especially text lately. Beresneva et al. [18] reported in their survey study that the initial computer-authored text detection works mostly focused on machine translation problems and used simple statistical approaches. Later, Jawahar et al. [41] published another survey which was the first work on detecting text generated by more sophisticated and powerful LLMs like GPT-2 from OpenAI. Tang et al. [65] in their latest study categorized detection methods into black-box detection and white-box detection and highlighted that technologies like watermarking can be\nused for the detection tasks. Yang et al. [69] and Wu et al. [68] reported in their survey studies that the two most common detection methods are zero-shot detection and training-based detection.\nOur work is different from the existing works in several ways. Firstly, none of the aforementioned works studied Claude 3, the LLM used in this study. Secondly, they did not perform a comparison between the detection of generated functions and generated classes. Thirdly, we incorporated a set of unique complexity-related features like Cyclomatic Complexity and its variants which were not used before. Lastly, we compared multiple ML models in detecting Claude 3-generated code which\nis another unique contribution."}, {"title": "THREATS TO VALIDITY", "content": "In this section, we discuss potential threats to the validity of our study."}, {"title": "Internal Validity:", "content": "Threats to the internal validity of our study is two-fold. Firstly, although we trained different types of ML models, this is not an exhaustive list of models. There can be other models (or even the\nsame models with different values of hyperparameters) that can outperform the best-performing\nmodel reported in this paper. Secondly, in our analysis, we only include standalone functions and\nclasses due to the fact that there may be a hierarchical dependency between classes and methods\nof different classes due to inheritance and it may not be possible to provide Claude 3 with enough\ncontext, and hence there is a higher likelihood of receiving incomplete code, or no code at all\nfrom Claude 3. Furthermore, providing enough context to CLaude 3 for tasks with a hierarchical\nnature will require much longer prompts, and by extension will cause higher costs. However, we\nacknowledge that in real life software consists of both standalone and non-standalone artifacts and\nincluding non-standalone artifacts may change the detection performance."}, {"title": "External Validity:", "content": "First, in our analysis, we only included OSS projects. Although existing studies suggest that the quality of OSS projects is not very different from that of commercial software [34, 59] because\nmany OSS projects do take standard quality control measures, we cannot guarantee that all projects\nin our dataset did the same. We cannot claim that the addition of commercial software data will\nnot change the performance of the detectors. Another threat to the external validity concerns\nthe generalizability of our findings. As mentioned in the previous section, the performance of\nthe detection model can vary due to several reasons including but not limited to the difficulty of\nthe tasks for which the code is being generated, and the LLM used in the generation of the code.\nTherefore, our findings may not be generalizable for all cases. For real-life applications, we suggest\nthat classifiers should be trained or tuned based on the data at hand because, as evident in Section 6,\ngiven the current state-of-the-art of LLMs, it is unrealistic to expect that any detection model will\nbe LLM-agnostic and will perform equally well across the board. We leave this discussion for future\nwork."}, {"title": "CONCLUSION", "content": "In this work, we analyzed Claude 3-generated functions and classes to identify their distinct patterns and used those patterns to automatically detect generated code snippets. We find that Claude 3-generated functions are longer compared to human-written functions, whereas the opposite is true for class-level code. Our results further show that ML models are more accurate in detecting Claude\n3-generated functions than Claude 3-generated classes. Complementing existing works [40, 62] we\nalso find that code stylometric features are the major contributors to the success of the detection\ntasks. The existing works focused on function-level code whereas we performed our analysis on\nboth function and class levels. To the best of our knowledge, we curated the first class-level dataset\nfrom real-life projects that can be leveraged by other researchers. Our findings do not negate any\nexisting works, rather it complement them by investigating the performance of detection models\non real-life problems. We make our data and scripts available for the other researchers to make our\nwork reproducible."}]}