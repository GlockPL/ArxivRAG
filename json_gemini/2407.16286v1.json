{"title": "A deeper look at depth pruning of LLMs", "authors": ["Shoaib Ahmed Siddiqui", "Xin Dong", "Greg Heinrich", "Thomas Breuel", "Jan Kautz", "David Krueger", "Pavlo Molchanov"], "abstract": "Large Language Models (LLMs) are not only resource-intensive to train but even more costly to deploy in production. Therefore, recent work has attempted to prune blocks of LLMs based on cheap proxies for estimating block importance, effectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b models without any significant degradation in downstream metrics. This work explores different block importance metrics by considering adaptive metrics such as Shapley value in addition to static ones explored in prior work. We show that adaptive metrics exhibit a trade-off in performance between tasks i.e., improvement on one task may degrade performance on the other due to differences in the computed block influences. Furthermore, we extend this analysis from a complete block to individual self-attention and feed-forward layers, highlighting the propensity of the self-attention layers to be more amenable to pruning, even allowing removal of up to 33% of the self-attention layers without incurring any performance degradation on MMLU for Mistral 7b (significant reduction in costly maintenance of KV-cache). Finally, we look at simple performance recovery techniques to emulate the pruned layers by training lightweight additive bias or low-rank linear adapters. Performance recovery using emulated updates avoids performance degradation for the initial blocks (up to 5% absolute improvement on MMLU), which is either competitive or superior to the learning-based technique\u00b9.", "sections": [{"title": "1. Introduction", "content": "The utility of training language models with an increasing number of parameters is currently undisputed [19; 6; 31; 18].\nThis has led to tremendous gains in performance, including the emergence of in-context learning [6]. However, aside from training, which is a one-time cost, deploying these models in production presents unique challenges, particularly high inference costs. Therefore, efficiency research for language models has gained significant popularity in the recent past [10; 2; 32; 22].\nAn extreme form of (structured) pruning is full-block pruning from a pretrained model. Men et al. [22]; Gromov et al. [12] recently showed that it is possible to completely drop blocks from a range of pretrained language models based on cheap proxies for block importance/influence such as computing the cosine distance between the input and the output representations for each block in a transformer.\nThis paper explores this direction further by analyzing the impact of different metrics on block identification, specifically focusing on adaptive metrics such as the one based on Shapley-value [29] in addition to static ones such as cosine block influence mainly used in prior work [22; 12]. We further look at the impact of individual self-attention and the feed-forward layers, which together form a block as analyzed by prior work. Finally, we evaluate the effectiveness of simple strategies for performance recovery, including a simple baseline of an additive bias (called 'emulated update') which is based on the empirical mean of the update applied by the block, as well as learning-based techniques"}, {"title": "2. Related work", "content": "He et al. [14] popularized residual layers for training deep models. Veit et al. [35] hypothesized residual networks to be ensembles of smaller sub-networks, exhibiting layer dropping at inference time with minimal performance degradation. Veit & Belongie [34] extended this to dynamic input-conditioned layer-skipping. Vaswani et al. [33] introduced the famous transformer architecture by combining ideas of attention and residual networks to develop highly performant architectures. While there is a large body of work on pruning for efficient inference, we focus on depth pruning and refer the readers to Wan et al. [36] for a more comprehensive treatment of the literature.\nSamragh et al. [28] initialized a subnetwork using blocks from a pretrained GPT-2 [25]. Sheared LLaMa [37] proposed an optimization-based view for simultaneous depth and width pruning of LLaMa-2 7b model [31]. Shortened LLaMa [20] showed that depth pruning is competitive against width-only pruning, or a sophisticated combination of both, while exploring several different metrics for estimating block influence. Men et al. [22] focused on LLaMa-2 [31] by using cosine distance (between activations before and after a block) as a proxy of block importance. Gromov et al. [12] also showed similar results on both LLaMa-2 [31] and Mistral [18], while also proposing a healing method by optimizing low-rank adapters [16] for the remaining blocks. This healing process is aimed at minimizing performance degradation with block pruning.\nJaiswal et al. [17] extended this by computing the cosine similarity of the representations at inference time and skipping only the feed-forward layers in the less important regions of the network (particularly in the middle of the network). In a similar spirit, Raposo et al. [26] trained a router to decide which blocks to skip i.e., reduce network depth (similar to the expert router in MoE [30]).\nThis paper attempts to take a deeper look at depth pruning of LLMs by looking at multiple metrics, datasets, block granularities (going to individual feed-forward and self-attention layers), and recovery techniques to establish the utility of each of these decisions on the resulting model."}, {"title": "3. Methods", "content": null}, {"title": "3.1. Block influence metrics", "content": "We consider different block influence metrics including cosine that computes the angular distance between the input and output representations to a block [22; 12], relative $L_1/L_2$ that computes the norm of the update with respect to the norm of the input representation [28; 22], and Shapley-value-based [29] estimate which computes the marginal contribution of a block by computing the difference in performance between all subsets where a particular block is present and the block is absent. We focus on computing Shapley-value for the distance to the full model logits or the language modeling loss, except Fig. 1 where we compute it directly for the 0-1 loss on the MMLU test set. In this case, the Shapley computation is:\n$\\text{SHAP}_i = E_{x, s \\subseteq S\\{i\\}}[L(O_{s\\cup \\{i\\}}(x)) - L(O_s^*(x))]$"}, {"title": "3.2. Performance recovery techniques", "content": "As dropping blocks can induce a distribution shift in the representation, we consider two simple performance recovery techniques."}, {"title": "3.2.1. EMULATED UPDATE", "content": "Emulated update is a particularly simple strategy that computes the average update applied by each block on a small calibration set, and applies that average additive update at inference time when a particular block is dropped. This can be viewed as having a 'bias' term instead of the full block."}, {"title": "3.2.2. Low-RANK LINEAR ADAPTERS", "content": "Following ideas from layer-stitching literature [3], we evaluate the efficacy of training low-rank linear adapters as a performance recovery measure. More precisely, we introduce two low-rank matrices per block to be learned (aside from the per-channel weightings learned as part of the normalization layer): This is similar in spirit to the healing process based on LoRA adapters by Gromov et al. [12], but we apply it in place of the missing blocks instead of applying it on the remaining blocks.\nWe consider three different ways of training these linear adapters: (i) minimize MSE between the output of the block and the low-rank adapter, (ii) use supervised fine-tuning (SFT) on the final output, and (iii) use logit-distillation on the final model output. We use the calibration set to train this low-rank adapter independently one block at a time."}, {"title": "4. Experiments", "content": "We use a subset of 150k sequences (context window of 2048 tokens) with cross-document concatenation from OpenWebText [1] as our full calibration set, and another 15k sequences as our validation set. We report 5-shot MMLU [15] accuracy as commonly reported in the literature [22; 12]. We use LLaMa-2 7b [31] and Mistral 7b [18] for our evaluations. We focus on 7b models primarily due to them being a sweet spot between compute cost and model quality. Larger models are more amenable to block pruning, as already established in prior work [22; 12]. Therefore, we expect our findings to be equally applicable to larger models."}, {"title": "4.1. Block influence metrics", "content": "We compare different block influence metrics in Fig. 2 (see Fig. 8 for more detailed results), where we apply min-max normalization on the block influences for visualization. As cosine distance-based block influence matches the proposed approach in [22; 12], our results also closely match theirs in terms of robustness against block pruning. Furthermore, the blocks that are assigned the least importance are located towards the end of the network i.e., the second half. Relative $L_p$ norm metrics also closely follow this trend, and hence, achieve robustness comparable to that of cosine distance.\nA significant deviation however is shown by the Shapley-value-based estimation, which is an adaptive metric, providing significant gains in terms of reduction of the average loss (visualized in Fig. 8) as it is specifically computed to reduce the model's language modeling loss. However, this results in a drastic reduction in accuracy when evaluating MMLU. Block influence plots show that in contrast to all other metrics that primarily focus on the second half of the network for removal, Shapley instead focuses on removing blocks from the first half of the network. When computing loss-based Shapley-value directly on the MMLU test set, we see that performance is significantly better than the cosine baseline (see Fig. 1), highlighting that task-specific (Shapley-based) block dropping might retain higher performance at the same number of dropped blocks as compared to task-agnostic techniques but at the expense of sacrificing more performance on tasks not directly considered.\nWe also visualize the results on other tasks from OpenLLM leaderboard [4] using lm-eval-harness [11] in Fig. 6 and Fig. 7. These results highlight that despite just a relatively minor impact on performance for MMLU, some metrics are significantly more impeded (such as GSM-8k). The proxy used in this case i.e., 'cosine', matches closely the right influence values to optimize MMLU, which is not aligned with the importance ideal for other datasets. On the other hand, adaptive metrics can be optimized individually for each task at hand."}, {"title": "4.2. Disentangling the impact of individual layers", "content": "As each block in a transformer is composed of self-attention as well as a feed-forward network, one can apply the same influence techniques to understand the impact of performance when pruning individual layers rather than complete blocks. The results are highlighted in Fig. 3 (see Fig. 9 and Fig. 10 for complete results where we also visualize the results with joint layer ranking).\nWe see that models exhibit higher resilience against the dropping of self-attention layers in contrast to feed-forward layers. In contrast to all other experiments, loss shapley in the case of self-attention layers achieves competitive influence as compared to other approaches, highlighting that it might be easier to estimate in contrast to feed-forward layers (Fig. 9 and Fig. 10). Furthermore, Mistral 7b exhibits a higher propensity for self-attention layer removal compared to LLaMa-2 7b.\nAlthough most transformer parameters are in the feed-forward layers, pruning the self-attention layer is equally useful due to its quadratic dependency on sequence length and the complex management of key-value caches, which introduces significant latency [38; 23]. Developing techniques that combine block and layer pruning, potentially pruning only a single layer in some blocks, is an exciting direction for future research."}, {"title": "4.3. Performance recovery techniques", "content": "We trained all our adapters for 800 steps with an effective batch size of 8, where the training loss plateaued. The results for emulated update and linear adapter with a rank of 8 trained using logit-distillation are shown in Fig. 4 (see Appendix B for full results with different ranks and training strategies). We observe a clear improvement for both LLaMa-2 and Mistral when using emulated update where performance improves significantly at some stages ($\\geq 5$% on MMLU). However, performance with a linear adapter is either comparable (LLaMa-2 7b) or worse (Mistral 7b) than the performance improvement observed by the simple emulated block update. This can be partially attributed to the overfitting of the adapter on the training corpus, which is not reflective of the true pertaining distribution. Gromov et al. [12] observed a similar performance degradation when trying to do parameter-efficient fine-tuning after dropping blocks from Mistral as compared to LLaMa-2.\nThese results indicate minor differences introduced by later blocks can be recovered using simple techniques. However, this mitigation technique is unable to change the tipping point in performance."}, {"title": "5. Conclusion", "content": "This paper explores depth pruning of LLMs, highlighting performance differences with various influence techniques, including adaptive ones. Block-sensitivity metrics like Shapley improve perplexity but degrade performance on tasks like MMLU, showing block relevance tension. Our results indicate self-attention layers are more amenable to pruning than feed-forward layers, with performance unaffected by pruning many self-attention layers. We also address misalignment from block pruning using basic performance recovery techniques. Our simplest baseline, using an average update, matches or outperforms low-rank adapters with different training formulations."}]}