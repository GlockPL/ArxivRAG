{"title": "AI methods for approximate compiling of unitaries", "authors": ["David Kremer", "Victor Villar", "Sanjay Vishwakarma", "Ismael Faro", "Juan Cruz-Benito"], "abstract": "This paper explores artificial intelligence (AI) methods for the approximate compiling of unitaries, focusing on the use of fixed two-qubit gates and arbitrary single-qubit rotations typical in superconducting hardware. Our approach involves three main stages: identifying an initial template that approximates the target unitary, predicting initial parameters for this template, and refining these parameters to maximize the fidelity of the circuit. We propose AI-driven approaches for the first two stages, with a deep learning model that suggests initial templates and an autoencoder-like model that suggests parameter values, which are refined through gradient descent to achieve the desired fidelity. We demonstrate the method on 2 and 3-qubit unitaries, showcasing promising improvements over exhaustive search and random parameter initialization. The results highlight the potential of AI to enhance the transpiling process, supporting more efficient quantum computations on current and future quantum hardware.", "sections": [{"title": "I. INTRODUCTION", "content": "Transpiling is a critical stage in the current quantum com-puting workflows, and consists of adapting a given circuit to a set of target instructions supported on specific quantum devices. Modern quantum circuit transpilers include several passes dedicated to different tasks [1]\u2013[3], and aim to adapt the given quantum circuit to the hardware with as little gate and depth overhead as possible to reduce the effects of noise on the quantum hardware.\nUnitary Synthesis is a fundamental task within most tran-spilers. It consists of, given a unitary matrix, finding a se-quence of gates within the hardware's instruction set that implement the given matrix.\nIn this paper we focus on exploring how AI methods can improve Unitary Sythesis, a central part within typical tran-spiling workflows. We focus on the case where the instruction set is composed of fixed two-qubit gates (such as CZ or CX) and single qubit rotations (such as RZ) that can implement arbitrary rotations, as is common in current superconducting hardware [4], [5].\nWithin this setting, exising algorithms broadly fall into two categories:\nExact methods, such as the two qubit KAK-based decom-position described in [6] or the more general Quantum Shannon Decomposition [7], [8] that exactly implement the given unitary. These methods provide an upper bound on the number of two-qubit gates for general unitaries, but except for the two qubit case, the circuits they produce are far from optimal for unitaries that can be found in typical applications.\nApproximate methods that, based on a given circuit template (a circuit with fixed gates and adjustable single qubit rotations), adjust the circuit parameters to maximize the fidelity of the circuit with respect to the given unitary [9], [10].\nAlthough the approximate methods scale beyond 2 qubits, they have some drawbacks that make their widespread use impractical for transpiling:\nTemplate selection is not straightforward, and most meth-ods resort to search methods that require iterating over several templates to find one that can successfully im-plement the target unitary (up to some high fidelity threshold).\nOptimization of the parameters is time consuming, and it often reaches local minima, resulting in sub-optimal fidelities even when the given template is capable of reaching perfect fidelity for the target unitary.\nThere have been recent attempts to improve this process. In [9], they use tensor networks to reduce the amount of computation during the optimization, allowing them demon-strate approximate compiling on shallow circuits on 27 qubits, but only for state preparation and not for the full unitary transformation.\nIn [11], they propose a tree-based search method where they train a deep learning model to predict which template to use from a set of 3-qubit templates, and start a tree-based template search from there, but they don't address the problem of parameter optimization.\nHere we explore a variety of Machine Learning based meth-ods to potentially speed up the process and allow synthesis of larger unitaries on typical transpiling pipelines, and propose simple AI-based algorithms for template selection and circuit parameter optimization.\nThe paper is organized as follows: Section II describes the general approach and the Machine Learning methods used and the data generation procedures used for the template selection and the parameter optimization. Section III shows the results obtained by the method for 2 and 3 qubit unitary synthesis. Finally, in Section IV we discuss the results and future research directions of interest."}, {"title": "II. MATERIALS AND METHODS", "content": "For addressing the unitary synthesis problem, we develop a pipeline for unitary synthesis composed of three stages:\n1) Template selection. A deep learning model that suggests a template given a target unitary.\n2) Parameter prediction. A deep learning model that suggests an initial value for the template's parameters as a starting point for the optimization.\n3) Parameter optimization. Further Optimization of the parameters via gradient descent, up to the desired fidelity.\nFor synthesizing a given unitary, the unitary is first assessed by the template selection model, which suggests a first tem-plate, and then the unitary is passed to the second model that provides suggested values for the template. These values are then used as starting point for an optimization via gradient descent. If the desired fidelity is not reached, the procedure starts again by selecting the second template suggested by the model, and continues until the objective fidelity is reached, or defaults to other methods if the suggestions are exhausted.\nWhen used in the context of circuit optimization, where circuit blocks are collected from a larger circuit and re-synthesized, the two qubit depth and count of the block presents an upper bound on the template size. This can be used to filter the suggestions to the ones that incur lower cost. If the model is confident that the cost of a block cannot be improved, the block can be left as is without attempting optimization.\nFor the cases when the template selection model guesses correctly, and where the second model suggests a useful starting point, this may result in significant speedups with respect to exhaustive template search, and might result in better fidelities by avoiding local minima.\nIn the next subsections we focus on the first and second stages and describe the network architectures used and the data generation processes.\nFor the first stage we use supervised deep learning tech-niques to select the right template to be used in the second step.\n1) Network architecture and training: We train a multi layer neural network to predict a template, which poses a supervised classification problem. The input to the neural network is a vector with the raw values of the entries of the unitary matrix, split in their real and imaginary parts (this gives a total number of elements of $2\\cdot 2^N$, being N the number of qubits). For our experiments, we have seen that simply using standard fully connected layers for the network architecture is enough for achieving high accuracy.\nThe output of the network is a vector with the probabilities assigned to each of the possible templates. Since we only allow one \"correct\" template for a given unitary, we train the network to minimize the cross entropy loss between the probabilities provided by the network and the template for which the unitary was generated. As it is often done in multiclass classification, the last layer of the neural network normalizes the output with a softmax so that the probabilities sum up to one.\n2) Data generation: During the training, we generate batches of unitary-template pairs on the fly. This ensures an \"unlimited\" supply of data and acts as a form of regularization preventing overfitting to a particular fixed set of unitaries.\nWe first define a set of templates for which we want to predict. Note that this doesn't need to include the complete set of possible templates for a given number of qubits; in particular we are typically interested in shallower unitaries that appear frequently on larger circuits, and we can always include an extra term in the model's output to indicate if a given unitary is not implementable in any of the templates of the set. In our experiments, we build the templates by combining single-qubit unitary gates and two-qubit CZ gates.\nFor the second stage we define a network architecture and training procedure to generate good suggested values for the template parameters for a given template.\nA naive approach for predicting good parameters for a given unitary would be to simply train a neural network model on unitary-parameter pairs. For generating a unitary-parameter pair one would first uniformly sample a group of parameters from the allowed parameter range (typically [-\u03c0, \u03c0] for rotation angles), and then obtain the corresponding unitary by setting these parameters in the template. Repeating this for multiple parameter values would yield a dataset for training. However, we have found that with this method the models fail to produce useful results, or to learn anything at all. Because a given unitary can be implemented by multiple (very different) sets of parameters for the given template, by using this procedure we may generate multiple different target"}, {"title": "B. Parameter suggestion", "content": "parameters for unitaries that are potentially very close. This results in a dataset of unitary-parameters pairs that cannot be described by a mapping that assigns a single value for the parameters given a unitary matrix, and prevents the network from learning any useful relationship.\nAlthough one could overcome this by generating the dataset in a consistent manner, for example by first sampling a unitary matrix and then using gradient descent from a fixed starting point, this approach still has some fundamental limitations. One of the main drawbacks is that by training on unitary-parameter pairs we are teaching the network to, in some way, interpolate between the parameters in the dataset; however, the actual objective is not to provide parameters that mimick the dataset, but to provide parameters that maximize the fidelity of the circuit.\n1) Network architecture and training: For overcoming this limitation, we define a deep learning model based on the autoencoder architecture [12]\u2013[14], consisting on an encoder network and a decoder. Here, the encoder is a deep learning network (as in standard decoders) that takes the unitary matrix as input and outputs a value for each of the parameters on the template. The decoder, unlike typical autoencoders, is fixed, and corresponds to the simulation of the actual template of the quantum circuit from the parameters output by the encoder.\nFor training, we first take an input unitary and pass it through the encoder to generate the predicted parameters. We then pass this parameters through the decoder to reconstruct the unitary. With both unitaries, we calculate the reconstruction fidelity of the output unitary with respect to the input unitary. Since the simulation of the circuit is also differentiable, we backpropagate the gradients of the fidelity with respect to the encoder network's weights and use them to train the network. This allows us to train the network directly to maximize the reconstruction fidelity.\nFor the encoder network architecture, we use again fully connected layers, but this time we choose complex-valued weights for the trainable parameters. This allows us to directly input the complex values of the unitary matrix and output com-plex values for the entries of the single-qubit unitary matrices of the template that represent the template's parameters.\nAn illustration of the encoder and decoder architecture can be found in Figure 2."}, {"title": "III. RESULTS", "content": "In this section we show the results obtained in training models for 2 and 3 qubits for both stages. For the models we trained, we selected the instruction set of the IBM Quantum Heron devices [15]: we use CZ for the two-qubit gates, and implement the single-qubit unitaries as a sequence of 3 CZ rotations separated by SX gates.\nFor the 2 qubits model, we use 4 different templates, containing from 0 to 3 CZ gates respectively. As pointed out in [6], this set of templates is complete in the sense that it allows to implement any two qubit unitary with optimal number of two qubit gates.\nFor 3 qubits, we train two models: one for 15 different templates, covering all possible combinations using up to 3 CZ gates, and another for 63 possible templates, covering all possible combinations up to 5 CZ gates. This set of templates is not enough to cover all possible unitary circuits, but it is useful for optimizing small blocks that typically appear on circuits.\nIn order to assess if model capacity was a limiting factor, we train multiple models of different sizes. The results are summarized on Table I.\nFor 2 qubits we reach a validation accuracy of ~ 96%, with a difference of only 2% between the worst and the best models. Even if we see a trend on decreasing accuracy with decreasing model sizes, the difference is small and may be improved with different training hyperparameters or longer training time. Overall, an interesting thing to note is that we can reach high accuracy with very small models, with the smallest one we tested being only 0.8 MB.\nTo test the robustness of the template prediction for 2 qubits, we run a simple experiment where we predict the template for\na circuit consisting of a two consecutive CNOT gates separated by an RZ gate on the first qubit. We vary the angle of the RZ gate from -\u3160 to \u03c0, allowing us to test how the model transitions between templates: the two-CZ template for most of the range, the zero-CZ template when the angle is a multiple of \u03c0 (allowing the two CNOTs to cancel out) and a less evident one-CZ template when the angle is \u00b1\u03c0/2.\nIn Figure 3a we can see the probability the model assigns to each template (with 0, 1, 2 or 3 CZ gates). The model predicts with high probability the right template in most of the range, except around \u03b7/\u03c0 ~ 0.45, where it incorrectly predicts the template with 3 CZ gates instead of the one with 2 CZ gates by a close margin. In figure 3b we validate the predictions obtained against the solution provided by the Qiskit transpiler (using the KAK-based method described in [6]).\nFor 3 qubits with up to 3 CZ, the best model reaches an accuracy of ~ 77% when selecting the highest probability template. If we also consider the second highest probability, the correct template is present in this set approximately ~ 98% of the time. The model 3 qubits and up to 5 CZ reaches ~ 37% after 24h of training on a single GPU (NVIDIA V100 16GB), and ~ 75% if considering the templates with top 5 probability, which can likely improve with longer training. If we use this model as a template recommender, and \"visit\" the templates in the order suggested by the model, on average we would take 3.5 visits until we reach the correct template. This is in contrast with the 63/2 visits on average that we would have to make if we did an exhaustive template search.\nTo understand the kind of errors the model makes, we show the confusion matrix for the 3-CZ model in Figure 4.\nIn the table we can see that the model guesses correctly for most of the templates, but fails for specific templates. In particular, the model usually has problems telling apart templates with the same CZ gates but where they appear in different order, such as templates 9 and 12 shown in Figure 1. This is likely due to these templates generating unitaries that have a significant overlap, and might improve with more careful generation of the dataset."}, {"title": "B. Parameter suggestion", "content": "For parameter suggestion, we train models for the 4 different two-qubit templates, and for 6 and 10 layer 3-qubit templates. The 10 layer 3-qubit template is shown in Figure 5.\nFor the two qubit templates, the models achieve a very high fidelity of 0.95 on average. For the three qubit templates, the models achieve around 0.5 fidelity after around 24h of training. The results are summarized in Table II.\nIn Figure 6 we show the fidelities corresponding to the parameters suggested by the model for the 10 layer 3-qubit template, for 100 random unitaries, compared against the fidelities obtained by selecting random parameters. Although the results may still improve with further training, we see that the suggestions are already useful when paired with the optimization procedure to refine the results.\nFigure 7 shows the result of compiling the Toffoli gate for the same template by using different starting points. For the parameters selected by the model, we see that the fidelity starts high and quickly converges to 1 at around 1000 iterations. For comparison, out of the 5 random starting points shown, two got stuck in a local minimum and the other three converged to fidelity 1, but took 20-30% more iterations."}, {"title": "IV. DISCUSSION", "content": "In this paper we have described an AI-based procedure for approximate synthesis of unitaries. We address the problem in three stages: template selection, parameter suggestion, and parameter refinement, and propose AI-based methods for the first two stages.\nWe demonstrate the idea by training two and three qubit models for the two stages. The results look in general promising: for the two qubit models, we reach almost perfect accuracy for template selection, and very high fidelity for the parameter suggestion. The accuracy and fidelity results for 3 qubits are not as high, but the models we show have only been partially trained for 24h (on a single GPU), and as described in Section III they already provide advantage over exhaustive template search and random parameter initialization.\nInterestingly, we have not found the model capacity to be a limited factor in the learning, at least in the range we have tried, and we have been able to train relatively small models (less than 1MB) that still produce high accuracy. This hints at the possibility that further improvements can be made by a more careful generation of the datasets and by tuning the training hyperparameters."}]}