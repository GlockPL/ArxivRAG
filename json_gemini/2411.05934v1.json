{"title": "Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving", "authors": ["Saad Tahmid", "Sourav Sarker"], "abstract": "In this research paper, we present an innovative approach tailored to solving mathematical problems in Bengali, developed for the DL Sprint 3.0 - BUET CSE Fest 2024 Competition. Our methodology harnesses the power of advanced deep learning models, notably the Qwen 2.5 series, with iterative improvements made through prompt engineering, model quantization, and Tool Integrated Reasoning (TIR) to handle complex calculations. Initially, we explored various model architectures, such as fine-tuned Mistral and quantized Qwen models, progressively refining them through translation techniques, RAG (Retrieval-Augmented Generation), and custom dataset curation. Through manual hyperparameter tuning, we optimized parameters like temperature and top-p to improve model adaptability and response accuracy. Additionally, the removal of RAG and careful parameter adjustments further contributed to our final model's robustness. Our approach demonstrates the potential of advanced NLP techniques in effectively interpreting and solving Bengali mathematical problems.", "sections": [{"title": "I. INTRODUCTION", "content": "The ability to understand and solve mathematical problems is a foundational skill for AI, essential for advancements across fields like science, engineering, and finance. However, while AI models have made strides in various languages, they still face significant challenges when tackling mathematical reasoning in low-resource languages, such as Bengali. This gap becomes particularly evident in tasks involving complex problem-solving and precise calculations. To address this, the DL Sprint 3.0 - BUET CSE Fest 2024 Competition [1] introduced the unique challenge of building an AI model capable of solving mathematical problems in Bengali, targeting issues akin to those in the Bengali Math Olympiad. This competition not only tests participants' technical skills but also aims to push the boundaries of Al's adaptability and performance in Bengali. Our work contributes to this pioneering effort, focusing on enhancing AI's mathematical reasoning in Bengali through advanced NLP and deep learning techniques. We explore and iteratively refine state-of-the-art models, such as the Qwen series, alongside strategies like prompt engineering, Tool Integrated Reasoning (TIR), and manual hyperparameter tuning to achieve robust problem-solving capabilities. By contributing to this research, we aim to advance AI's reach into Bengali language processing, ultimately creating models that can assist students, educators, and researchers in tackling complex problems with precision and reliability."}, {"title": "II. METHODOLOGY", "content": "For this task, we aimed to select models capable of efficiently solving mathematical problems in Bengali, considering both performance and computational efficiency. Our initial approach involved using a fine-tuned Mistral 7B model [2], which, despite being a strong general-purpose model, did not deliver the desired accuracy for mathematical reasoning tasks. This led us to explore other models better suited for handling mathematical challenges, particularly in a low-resource language like Bengali. The Qwen series emerged as the most promising option due to its strong performance on mathematical reasoning benchmarks. The Qwen-32B-Instruct model [3], with impressive scores on the MATH benchmark (83.1) and GSM8K benchmark (95.9) [4], was especially appealing for its capability in solving high-level mathematical problems. Given its robust performance, we chose to focus on the Qwen-2.5 series, including the 7B, 14B, and 32B models, each offering different trade-offs between accuracy and computational demands. We initially fine-tuned the Qwen-14B-Instruct model for a single epoch, which, though promising, did not provide sufficient improvements in performance. To enhance model efficiency, we implemented VLLM (Variable-Length Language Model) for faster inference, which allowed us to speed up the testing process while maintaining model accuracy. Additionally, we employed model quantization techniques to reduce memory requirements, making the models more practical for large-scale inference tasks. To further refine performance, we incorporated Tool Integrated Reasoning (TIR), which enabled the model to perform complex calculations using Python. This method improved the model's ability to handle mathematical operations effectively. Moreover, manual hyperparameter tuning of parameters like temperature and top-p helped optimize the model's response accuracy and adaptability. In summary, after exploring various models, we selected the Qwen series, particularly the Qwen-32B-Instruct model, for its exceptional performance in mathematical reasoning. Combined with techniques like VLLM, TIR, and hyperparameter optimization, we were able to enhance the model's ability to solve mathematical problems in Bengali effectively."}, {"title": "B. Preprocessing", "content": "In this project, preprocessing was crucial to handling Bengali mathematical problems and enhancing the model's ability to solve them accurately. Given the complexity of understanding Bengali text directly in the initial stage, we leveraged the Qwen-32B-Instruct model to translate Bengali mathematical questions into English for improved processing. This approach enhanced the performance of our model significantly."}, {"title": "C. Prompt Tuning", "content": "Prompt tuning played a key role in optimizing the performance of our model as we experimented with various reasoning techniques, including Chain of Thought (COT), Tool Integrated Reasoning (TIR), RAG, self-consistent TIR, and self-consistent COT. We initially designed prompts to guide the model through step-by-step reasoning for COT. For TIR, we adapted the prompts to instruct the model to perform calculations using Python tools for more complex problems. With Self-COT and Self-TIR, the prompts were modified to encourage the model to generate multiple reasoning paths and select the most consistent solution. we experimented with Retrieval-Augmented Generation (RAG). The RAG approach was implemented to provide context and improve the quality of answers in both Bengali and translated English questions. However after experimenting with Retrieval-Augmented Generation (RAG), we decided to discontinue its use due to its poor performance, as it did not significantly improve the model's accuracy compared to other approaches.Additionally, we fine-tuned hyperparameters such as temperature and topp to control the diversity and confidence of the model's responses. Lower temperatures were used for more deterministic answers, while higher values promoted creativity.Adjusting topp helped the model select the most plausible solutions.These prompt tuning and hyperparameter adjustments, particularly with TIR and Self-TIR, significantly improved the model's ability to solve complex Bengali math problems."}, {"title": "III. RESULTS", "content": "In this section, we present the performance of our deep learning model using various approaches. The model was evaluated on the public leaderboard of the DL Sprint 3.0, with the baseline score being 28 out of 100. We tested different configurations, including using various versions of the Qwen model, translation, retrieval-augmented generation (RAG), and Tool Integrated Reasoning (TIR)."}, {"title": "IV. DISCUSSION", "content": "Throughout the development of our AI model for solving Bengali mathematical problems, we encountered several key challenges and insights:"}, {"title": "A. Translation Impact", "content": "Bengali questions presented without translation consistently scored lower than their translated counterparts. Translating questions into English improved model understanding and problem-solving, leveraging the model's richer pre-trained knowledge in English."}, {"title": "B. Model Size for Translation", "content": "Using the Qwen 2.5-32B-Instruct model for translation yielded better results compared to the Qwen 2.5-14B-Instruct model. The larger model demonstrated superior language comprehension, which improved translation quality and contributed to overall score improvements."}, {"title": "C. RAG Limitations", "content": "Initial experiments with Retrieval-Augmented Generation (RAG) aimed to enhance the model's responses by adding contextual information. However, RAG often introduced noise, leading to a decrease in performance. Removing RAG subsequently improved scores, indicating that the inherent capabilities of the larger Qwen models were sufficient without additional retrieved context for this task."}, {"title": "D. Scarcity of Bengali Datasets", "content": "The limited availability of high-quality Bengali mathematical datasets restricted the model's exposure to diverse problem types."}, {"title": "E. Inference Optimization with VLLM", "content": "Integrating Variable-Length Language Modeling (VLLM) enabled faster inference, which was crucial for handling the computational demands of larger models. This optimization helped streamline the testing process within our resource constraints"}, {"title": "F. Prompt Sensitivity", "content": "The model exhibited a high sensitivity to prompt wording, with minor adjustments in phrasing significantly affecting output quality and accuracy. Effective prompt engineering became essential to guide the model toward optimal solutions, underscoring the importance of precision in prompt construction."}, {"title": "G. Quantization for Resource Constraints", "content": "Due to Kaggle's limited GPU memory, we had to quantize the larger Qwen models to make them feasible for testing. This process reduced memory requirements, allowing us to utilize the larger 32B model in a resource-limited environment without compromising performance excessively."}, {"title": "H. Fine-Tuning Limitations for Larger Models", "content": "While fine-tuning smaller Qwen models was manageable, memory constraints in Kaggle prevented us from fine-tuning the Qwen 2.5-32B model. This limitation hindered further customization of the 32B model, restricting our ability to fine-tune it for specific Bengali problem-solving tasks."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This paper presented a novel approach to solving Bengali mathematical problems by leveraging the Qwen 2.5 series models and optimizing through prompt engineering, translation, and Tool Integrated Reasoning (TIR). Our methods demonstrated the efficacy of advanced NLP techniques in mathematical problem-solving, particularly for low-resource languages like Bengali. The Qwen 2.5-32B model, combined with techniques such as self-consistency and TIR, achieved significant improvements over baseline models, highlighting the model's potential in handling complex reasoning tasks with minimal Bengali-specific data. Despite these achievements, challenges remain, especially regarding translation dependencies, limited Bengali datasets, and memory constraints for large models. These factors impacted both model performance and adaptability to a wider range of problem types. Our results also revealed the sensitivity of model outputs to prompt phrasing, underscoring the need for refined prompt engineering techniques.\nTo build on this work, we propose the following areas for future exploration:\n\u2022\tEnhanced Bengali Data Collection: Increasing the availability and diversity of Bengali mathematical datasets will enable better model training and adaptability to complex problem types.\n\u2022\tDomain-Specific Fine-Tuning: With sufficient resources, fine-tuning the Qwen 2.5-32B model specifically on Bengali math problems could further improve accuracy and reasoning ability.\n\u2022\tOptimized Prompt Engineering: Researching prompt optimization strategies for mathematical reasoning tasks, especially in low-resource languages, could make the model responses more consistent and accurate.\n\u2022\tExploration of Lightweight Models: Investigating smaller, efficient models tailored for Bengali could balance memory constraints and inference speed without sacrificing accuracy.\nBy addressing these areas, we aim to enhance the utility of deep learning models in Bengali problem-solving, making advanced educational tools more accessible to Bengali-speaking learners and educators. This research lays a foundation for further progress in low-resource language applications of AI in mathematical education and reasoning."}]}