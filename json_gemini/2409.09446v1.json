{"title": "MulCPred: Learning Multi-modal Concepts for Explainable Pedestrian Action Prediction", "authors": ["Yan Feng", "Alexander Carballo", "Keisuke Fujii", "Robin Karlsson", "Ming Ding", "Kazuya Takeda"], "abstract": "Pedestrian action prediction is of great significance for many applications such as autonomous driving. However, state-of-the-art methods lack explainability to make trustworthy predictions. In this paper, a novel framework called MulCPred is proposed that explains its predictions based on multi-modal concepts represented by training samples. Previous concept-based methods have limitations including: 1) they cannot directly apply to multi-modal cases; 2) they lack locality to attend to details in the inputs; 3) they suffer from mode collapse. These limitations are tackled accordingly through the following approaches: 1) a linear aggregator to integrate the activation results of the concepts into predictions, which associates concepts of different modalities and provides ante-hoc explanations of the relevance between the concepts and the predictions; 2) a channel-wise recalibration module that attends to local spatiotemporal regions, which enables the concepts with locality; 3) a feature regularization loss that encourages the concepts to learn diverse patterns. MulCPred is evaluated on multiple datasets and tasks. Both qualitative and quantitative results demonstrate that MulCPred is promising in improving the explainability of pedestrian action prediction without obvious performance degradation. Furthermore, by removing unrecognizable concepts from MulCPred, the cross-dataset prediction performance is improved, indicating the feasibility of further generalizability of MulCPred.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous vehicles should be able to understand the intentions of other road users to navigate safely and efficiently in urban traffic environments, especially those of pedestrians [1]\u2013[3]. Pedestrians' behaviors often exhibit diversity and randomness, which makes predicting their future actions par- ticularly challenging. Recent years have witnessed significant progress in pedestrian behavior prediction, either in form of trajectories [4]\u2013[10] or action categories [11]\u2013[18]. Pioneering works have exploited uni-modal features such as historical trajectories [6], [7] and poses [14]. Recent works used more sophisticated models and multiple modalities of inputs [5], [11], [18]. Most of the \u201cmultiple modalities\u201d are different aspects of information manually selected from the sensor data, such as skeletons, appearance and trajectories. Such manual disentanglement of raw inputs has so far been effective in exploiting limited training data and showed improvements in prediction accuracy. Despite the improvement in prediction performance, recent learning-based models still face the lack of explainability in decision-making, due to the intrinsic black- box characteristic of deep neural networks, which presents difficulties for users to understand the working principle and for the developers to make further improvements.\nExisting works in explainable artificial intelligence (XAI) [19], [20] have made significant progress in fundamental tasks such as image recognition and speech recognition, among which concept-based models [21]\u2013[25] as a category of self- explaining methods received wide attention. The basic idea 0000-0000/00$00.00 \u00a9200 cape-based models is to decompose the information"}, {"title": "II. RELATED WORKS", "content": "Existing works mainly focus on two forms of pedestrian behaviors: 1) trajectory, which is the sequence of coordinates on the ground plane or the image plane, and 2) action categories that the pedestrian will conduct in future frames.\nTrajectory prediction. Predicting behaviors of pedestrians in the form of trajectories has been extensively studied, among which there are two main application scenarios: the surveil- lance view [6]\u2013[8] and the ego-centric view [4], [11], [13], [27]. In surveillance scenarios, the fixed view enables methods to leverage pedestrians' kinetic features, such as historical trajectories, as well as inferring interactions between pedes- trians. On the other hand, the perspective of view and ego-motion of onboard cameras bring difficulties to utilizing inter- action information. Therefore, ego-centric methods usually use multi-modal inputs including ego-motion and the pedestrians' appearance information [5], [11]. Some also use estimated intention or action categories [27], [28] of the pedestrians to enhance the prediction. Experiments in these works show that accurate estimation of the pedestrian's intention or action can also improve the prediction of trajectories.\nAction prediction. A number of pedestrian action predic- tion methods focus on the crossing behaviors, i.e. whether the observed pedestrian will cross in front of the ego vehicle. Early works exploited uni-modal features such as poses [14] and fine-grained action categories of upper and lower bodies [13]. These methods suffer from low prediction accuracy due to their relatively weak modeling capability. During the last decade, the performance of action prediction has been signifi- cantly improved as model complexity increased. Recently pro- posed works are multi-modal methods that jointly encode the inputs from multiple modalities, such as original images from onboard cameras, skeletons of pedestrians, and ego-motion of the vehicle. One common paradigm of these methods is to use separate encoders to generate fixed-length representations of different modalities, and then fuse these representations at a late stage [11], [18], [27]. Recently there have also been works using more sophisticated fusing strategies. For example, [29] uses a mixed architecture that combines early fusion and late fusion for multiple modalities.\nAlthough these approaches improve performance, the fusion strategies in these approaches (e.g., concatenation, sum or mixed ones) make it hard to conduct a direct attribution of different components in the inputs, and thus weaken the explainability.\nWorks related to XAI can be roughly categorized into two groups: post-hoc methods and ante-hoc methods. Post-hoc methods use additional modules or intermediate results generated during inference to explain the inference itself. Ex-isting post-hoc methods include probing-based methods [30], [31] and activation map based methods [32]\u2013[34]. Recently, linguistic expressions as a form of explanation have received extensive attention [35]\u2013[37]. Such methods use language models to generate texts that explain the decision-making"}, {"title": "III. MULCPRED: MULTI-MODAL CONCEPT-BASED\nPEDESTRIAN ACTION PREDICTION", "content": "The overall illustration of MulCPred is shown in Fig. 2. The framework consists of two main parts: the multi-modal concept encoders and the linear aggregator. The multi-modal concept encoders first project the input into a set of implicit concepts. Each concept represents a certain pattern of the corresponding data modality. The output of the concept encoders is a set of activation scores, which represent the \"similarity\" between the input and all the concepts. The activation scores are then integrated by a linear aggregator to predict the confidence"}, {"title": "B. Multi-Modal Concept Encoders", "content": "The multi-modal concept encoders first extract fixed-length representations from the input. Then the Recalibration Mod- ules separate the representations into basis concepts. Let (X, y) be an input-label pair, where y \u2208 RC is a one-hot vec- tor representing an action label out of C classes. Each sample X = {xm}m=1 represents inputs with M modalities, where xm can be any form of data, e.g. sequential inputs such as trajectories or spatiotemporal inputs such as videos. For each xm there is a backbone function fm(\u00b7) that extracts feature fm(xm). The extracted feature is fed into the Recalibration Module to calculate the activation scores sm\u2208 RNm to a set of concepts. Each element in sm is the activation score corresponding to a certain concept, representing how similar the input is to the concept."}, {"title": "C. Recalibration Module", "content": "The Recalibration Module is embedded in each modality branch and determines the concepts the model learns. For each modality, the extracted feature fm(xm) is flattened by a global average pooling layer. Let rm denote the global representation of fm (xm) after pooling. We first calculate a set of recalibration vectors Pm conditioned on rm using an MLP layer\n$r_m = GlobalAveragePooling (f_m(x_m))$ (1)\n$P_m = ReLU(r_mW_m+b)$ (2)\nwhere rm \u2208 Rdm is the feature after a global pooling layer, and Pm \u2208 RNdm is the concatenation of {pm,i}i=1, with m\u2208 [1, M] corresponding to a certain modality and N is the"}, {"title": "D. Aggregator", "content": "We use a linear layer as the aggregator to integrate the activation scores of all concepts. Assuming there are N \u00d7 M concepts equally distributed among all M modalities, the activation scores from all M modalities are finally integrated by a linear aggregator gw (\u00b7) parameterized by Wa \u2208 RNM\u00d7C, where C denotes the number of predicted classes. The predic- tion is thus given by\n$\\hat{y} = softmax(ReLU(S)W_a)$ (4)\nwhere S\u2208 RNM is the concatenation of activation scores from all modalities. ReLU(\u00b7) is used to ensure non-negative activation. Each element at row i and volume j in Wa represents the relevance between concept i and class j. By explicitly learning the weights, the model's prediction can be explained by attributing the relevance to each concept and the corresponding activation intensity, which provides ante- hoc explainability."}, {"title": "E. Loss Functions", "content": "In the experiments, we discovered that the concepts tend to collapse into few patterns, resulting in that the representative samples for most concepts are exactly the same (see Figure 5). A similar problem was also reported in [40]. To curb such trend, we propose a feature regularization loss that contains two terms: the diversity term Ldiv and the contrastive term Lcont. Ldiv is given by\n$L_{div} = \\sum_{m=1}^M ||P_mP_m^T - I||_2$ (5)\nwhere I \u2208 RN\u00d7N is the identity matrix. This term encourages each row in pm to be different from the rest, meaning that the concepts are encouraged to learn different combinations of channels in feature rm. However, Ldiv alone cannot ensure that the channels in rm represent different patterns. Therefore, we use Lcont to promote the diversity of components in rm. Assuming that Rm \u2208 RB\u00d7dm_is a batch of features rm with"}, {"title": "A. Implementation Details", "content": "In the experiments, we use up to five different input modal- ities\nAppearance of the observed pedestrians as sequences of image patches cropped according to the annotated bounding boxes of the pedestrians.\nSkeleton information of the pedestrians in form of pseudo heatmaps [43] predicted by pretrained HRNet [44].\nLocal context information in the form of sequences of image patches cropped by enlarged bounding boxes around the pedestrians.\nTrajectory information in the form of sequences of 4- dimension coordinates of bounding boxes of the pedes- trians.\nEgo-motion of the vehicle, as sequences of the accelera- tion of the ego vehicle.\nSimilarly to [13], we use an observation length of 16 frames (1.6 seconds) for all modalities, and predict the action label of the next frame. We use C3D [45] pretrained on Kinetics 700 [46] as the backbone for the appearance and context modalities. For the skeleton modality, we use poseC3D [43] pretrained on UCF101 [47] as the backbone. And for the trajectory and ego-motion modalities, we use the randomly initialized LSTM as the backbone.\nDuring training, we use the Adam optimizer [48] with \u03b2\u2081 = 0.9, \u03b22 = 0.999. A is 0.01 and \u03b7 is 1. We use the learning rate 10-5 for backbones with pretrained weights and 10-4 for other trainable parameters. We use step decay for learning rate with step size 20 and decay factor 0.1. The model is trained for 100 epochs with batch size 8. We set N, the number of concepts for each modality, as 10."}, {"title": "C. Action Prediction Results", "content": "We compare the prediction performance of MulCPred with the following baselines.\n1) 3D CNN includdes C3D [45], R3D18 [50], R3D50 [50] and I3D [51], which are all pretrained on Kinetics 700 [46]. We feed the appearance modality as input to the 3D CNN models.\n2) PCPA [11] is a multi-modal predictor that takes the context, trajectory, ego-motion and skeleton modalities as inputs. Information from different modalities is fused by a modality attention mechanism.\nWe compare the crossing prediction performance of the baselines with MulCPred as well as the ablation versions about the regularization loss terms. Table I shows the results of 3 classification criteria: accuracy, AUC, and F1 score. MulCPred denotes the version with all 5 modalities, while MulCPred-ASC denotes the version with only visual modalities, i.e. appearance (A), skeleton (S), and local context (C). Table II provides the comparision of different numbers of concepts. We evaluated two extreme cases, i.e. 1 20 concepts for each modality, which is equivalent to 5 and 100 concepts for total."}, {"title": "D. Failed Cases", "content": "We illustrate the inference process of a failed sample in comparison with a correctly predicted sample in Figure 6. Figure 6(a) shows the case that a person riding a bicycle (la- beled as \"sitting\u201d in TITAN) is correctly predicted. Among the three concepts we visualized, concept 1 apparently focuses on the bicycle that the person is riding on, which can also be seen in the recalibrated featuremap 1 applies to the input; 14 focuses on the lower body part of a sitting person, which also confronts with the recalibrated featuremap. Therefore the high activation of both concepts 41 and 14 contributes high confidence to the \"sit\" class."}, {"title": "E. Cross Dataset Evaluation", "content": "Despite that some of the concepts do not exactly confront human cognition, as discussed in Section IV-D, the informa- tion the concepts provide can still be used in improving the performance. By removing unrecognizable and irrelevant con- cepts, we improved the generalizability of MulCPred. Specif- ically, we choose a set of concepts that learned recognizable and relevant patterns\u00b2 (such as 1 and 414 in Figure 6), and manually set the relevance scores of the remaining concepts to zero, which is equivalent to removing all the concepts we did not choose from the model. This practice was driven by the simple intuition that, if the features the model extracts from the inputs are explainable, they should also be generalizable. Since both TITAN and PIE have the crossing prediction task, we conducted the cross-dataset evaluation on crossing prediction. The results are shown in Table III, where MulCPred-filtered denotes the model after the concept removal. \u201cTITAN PIE", "PIE > TITAN": "eans the opposite. It can be seen in the results that the model after the removal outperforms the original model and even baselines with relatively few parameters such as C3D, which is usually considered less likely to encounter overfitting, indicating that the remaining concepts are indeed better generalizable. It is also notable that the improvement brought by the removal is apparent on PIE, a smaller dataset than TITAN, which could indicate that >"}, {"title": "F. Faithfulness evaluation", "content": "Faithfulness is an important perspective to verify the ex- plainability of a method. It measures how faithfully the importance of features claimed by the model (the relevance scores in our case) reflects these features' influence on the final prediction [21]. We extend the Most Relevant First (MoRF) curve [26] to evaluate the faithfulness of MulCPred. MoRF is commonly used as a perturbation-based metric to evaluate the explanations (e.g. heatmaps of input images) of explaining techniques by progressively removing features according to their importance. The formula of MoRF can be recursively described as follows\n$x_{MORF}^k = \\begin{cases}\nx, & \\text{if } k = 0 \\\\\ng_{rem}(x_{MORF}^{k-1}, e_k), & \\text{if } k > 0\n\\end{cases}$ (8)\nwhere x represents the original input, and grem is a pertur- bation function that removes information e (e.g. pixels or features) from \u00e6. E = {ek}=1 is a sequence of features sorted by their importance (e.g. weights on the heatmaps) in descending order. Let f(x) be the output of the model for class c. The area under the curve (AUCMORF) composed by"}, {"title": "V. DISCUSSION", "content": "Both quantitative and qualitative results in Section IV demonstrate that MulCPred is capable of making accurate predictions as well as giving faithful explanations of its inner workings. Although the variant without the regularization terms has better prediction performance, the visualization of the concepts shows that the absence of the regularization terms can cause serious mode collapse. We believe the performance gap caused by the regularization terms indicates that some concepts, though irrelevant and unrecognizable (such as 425 in Figure 4(b) and 12 in Figure 6), can actually cause overfitting to the training dataset. This hypothesis is then demonstrated in the cross-dataset evaluation, where the variant of MulCPred without the regularization has the lowest generalizability. We further find that, by removing these irrelevant and unrecog- nizable concepts, MulCPred outperforms most baselines, even including C3D, a relatively small model that can be considered less likely to cause overfitting. All in all, despite that there are gaps between the concepts learned by MulCPred and the real concepts in human cognition, MulCPred still shows the potential to give faithful explanations while achieving competitive performance."}, {"title": "VI. CONCLUSIONS", "content": "We presented MulCPred, a concept-based model for pedes- trian crossing prediction that can explain its own decision-making by learning multi-modal concepts and the relevance between the concepts and the predictions. The model can not only make sample-level explanations but also attend to different components in the inputs through the Recalibration Module. In the cross-dataset evaluation, we find that concepts that focus on more consistent and recognizable patterns are more generalizable that the others, which confronts human intuition. Experiment results show that MulCPred can provide"}]}