{"title": "Improving Clinical Question Answering with Multi-Task Learning: A Joint Approach for Answer Extraction and Medical Categorization", "authors": ["Priyaranjan Pattnayak", "Hitesh Patel", "Amit Agarwal", "Srikant Panda", "Bhargava Kumar", "Tejaswini Kumar"], "abstract": "Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in CQA, existing models lack the ability to categorize extracted answers, which is critical for structured retrieval, content filtering, and medical decision support. To address this limitation, we introduce a Multi-Task Learning (MTL) framework that jointly trains CQA models for both answer extraction and medical categorization. In addition to predicting answer spans, our model classifies responses into five standardized medical categories: Diagnosis, Medication, Symptoms, Procedure, and Lab Reports. This categorization enables more structured and interpretable outputs, making clinical QA models more useful in real-world healthcare settings. We evaluate our approach on emrQA, a large-scale dataset for medical question answering. Results show that MTL improves F1-score by 2.2% compared to standard fine-tuning, while achieving 90.7% accuracy in answer categorization. These findings suggest that MTL not only enhances CQA performance but also introduces an effective mechanism for categorization and structured medical information retrieval.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid digitization of healthcare records has led to an explosion of unstructured textual data in Electronic Medical Records (EMRs) [1]. Physicians and healthcare professionals frequently need to extract patient-specific information from these records, which often consist of lengthy and heterogeneous clinical narratives. Clinical Question Answering (CQA) has emerged as a powerful Natural Language Processing (NLP) technique that enables automated retrieval of relevant information from EMRs [2]. However, deploying QA models in real-world healthcare settings presents significant challenges, including the need for domain adaptation [3], interpretability, and structured information retrieval."}, {"title": "A. Challenges in Clinical Question Answering", "content": "Despite advancements in deep learning-based QA systems, current approaches struggle with several key challenges:\n\u2022 Unstructured Nature of EMRs: Clinical notes are typically free-text narratives, written in varying styles by different healthcare providers [4]. Unlike structured databases, these records lack consistent formatting, making it difficult for QA models to retrieve answers accurately [5]. Extracted answers often require additional post-processing to be useful in downstream applications.\n\u2022 Domain Adaptation Limitations: General NLP models, such as BERT [6], are pre-trained on open-domain corpora like Wikipedia and BooksCorpus, making them suboptimal for processing medical text. This has led to the development of domain-specific models such as BioBERT [7], ClinicalBERT [8], and PubMedBERT [9], which improve performance by pretraining on biomedical and clinical corpora. However, even these models struggle with capturing the complexities of specialized medical terminology and abbreviations found in EMRS.\n\u2022 Lack of Answer Categorization: Traditional QA models return free-text answers but do not categorize them into structured medical entities. For example, when asked, \u201cWhat medications has the patient been prescribed?\u201d, a standard QA model might return \u201cMetformin 500 mg, daily\u201d. However, without explicit categorization, the extracted text cannot be easily integrated into structured EMR systems for further analysis [10]. Physicians and clinical decision-support tools often require additional context, such as whether the answer pertains to a diagnosis, medication, symptom, procedure, or lab report.\n\u2022 Scalability and Generalization: Existing QA models require extensive fine-tuning on task-specific datasets to achieve reasonable performance. This limits their ability to generalize across different medical institutions, patient populations, and EMR systems."}, {"title": "B. Proposed Approach: Multi-Task Learning for Clinical QA", "content": "To address these challenges, we introduce a Multi-Task Learning (MTL) framework that extends Clinical QA models by jointly learning answer extraction and medical categorization. Our approach provides three key contributions:\n1) Multi-Task Learning for Clinical QA: We enhance existing QA models by adding an auxiliary classification head that categorizes extracted answers into five key medical categories: Diagnosis, Medication, Symptoms, Procedures, and Lab Reports. This allows structured answer retrieval, making it easier for physicians to interpret results.\n2) Standardized Medical Categorization: Unlike traditional QA systems that return free-text answers, our method incorporates medical entity recognition using the Unified Medical Language System (UMLS) [11]. By aligning extracted answers with standardized medical concepts, our approach ensures interoperability with clinical decision-support tools.\n3) Empirical Performance Gains: We evaluate our model on the emrQA dataset [12], a large-scale corpus for medical QA. Our results demonstrate that the MTL-based approach improves F1-score by 2.2% over single-task fine-tuning while achieving 90.7% classification accuracy. This suggests that integrating medical categorization with QA enhances both performance and usability in real-world clinical settings."}, {"title": "C. Applications and Impact", "content": "The proposed MTL-based clinical QA model has several practical applications:\n\u2022 Structured EMR Search: By categorizing extracted answers, our model enables physicians to retrieve information in a structured manner. Instead of searching through unstructured clinical notes, a user could query, \u201cShow me all past diagnoses for this patient\u201d and receive a structured list.\n\u2022 Clinical Decision Support: Categorized answers can be integrated into clinical decision-support systems, helping physicians make more informed choices by filtering relevant medical information.\n\u2022 Medical Coding and Billing: Healthcare providers often need to categorize clinical notes into standardized billing codes (e.g., ICD-10). Our approach could facilitate automated categorization of extracted medical concepts, reducing the manual workload for healthcare professionals.\n\u2022 Content Moderation in Clinical AI: AI-powered medical assistants must ensure that generated answers adhere to regulatory and ethical standards. Categorization enables rule-based filtering, helping to identify and flag inappropriate or harmful AI-powered medical assistants responses."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Question Answering in NLP", "content": "Question Answering (QA) is a core NLP task that enables retrieving precise answers from structured and unstructured text [13], [14]. Early models relied on rule-based techniques and information retrieval, as seen in IBM's Watson [15], which matched keywords using structured knowledge bases but lacked deep language understanding. The rise of neural architectures, including Long Short-Term Memory (LSTM) networks [16] and attention mechanisms [17], significantly improved contextual comprehension.\nWith Transformer-based models QA took a big leap and BERT [6] set new benchmarks [18]. Fine-tuned variants such as BERT-QA outperformed previous models on datasets like SQUAD [19]. Further optimizations, including ALBERT [20], RoBERTa [21], and T5 [22], enhanced transformer models for QA across various domains [23]. However, their reliance on general-domain corpora limits effectiveness in specialized fields like medicine."}, {"title": "B. Domain Adaptation in Medical NLP", "content": "Medical NLP poses challenges due to domain-specific terminology, abbreviations, and inconsistent documentation [24]. Standard BERT-based models, trained on general corpora, struggle with these complexities. To address this, domain-adapted models such as BioBERT [7], pretrained on PubMed abstracts, improved biomedical Named Entity Recognition (NER) and relation extraction [25]. ClinicalBERT [8], fine-tuned on clinical notes, enhanced hospital-based applications. PubMedBERT [9] eliminated domain mismatches by pre-training exclusively on PubMed abstracts. GatorTron [26] further improved performance by leveraging de-identified clinical records, while Med-BERT [27] incorporated structured EHR data, bridging the gap between structured and unstructured text.\nDespite these advancements, most domain-specific models are fine-tuned separately for tasks like QA, named entity recognition, and classification [28]. This single-task approach limits generalization and increases data requirements, reducing adaptability in clinical settings."}, {"title": "C. Multi-Task Learning in NLP and Medical AI", "content": "Multi-Task Learning (MTL) has emerged as a powerful technique for improving NLP models by jointly learning related tasks [29]. It has been widely explored in domains such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging [30], sentiment analysis with syntactic parsing [31], and question answering integrated with textual entailment [32]. By leveraging shared representations, MTL enhances generalization and efficiency, especially in data-scarce environments.\nIn medical AI, MTL has been applied to clinical event detection [33], adverse event detection [34], and patient outcome prediction [35], consistently outperforming single-task models [36]. For instance, multi-task transformers for clinical risk prediction [37] have demonstrated improved generalization across patient cohorts. Despite these successes, MTL remains"}, {"title": "D. Limitations and Contribution", "content": "Despite advancements in QA and domain-adapted transformers, key limitations persist: (1) QA models generate raw text outputs, making structured integration into clinical workflows difficult; (2) existing models either extract answers or classify entities, lacking a unified approach; (3) fine-tuned models struggle with distribution shifts in diverse EMR datasets. To address these gaps, we introduce an MTL framework for Clinical QA that (1) jointly learns answer extraction and medical classification, improving interpretability, (2) integrates domain-adapted transformers like ClinicalBERT with an auxiliary classification head, and (3) enhances generalization by leveraging shared representations, making it more robust for real-world EMR applications."}, {"title": "III. DATASET AND PREPROCESSING", "content": ""}, {"title": "A. emrQA Dataset", "content": "We use the emrQA dataset [12], a large-scale corpus designed for Clinical Question Answering (QA) over Electronic Medical Records (EMRs). emrQA was derived from the i2b2 challenge datasets and provides approximately 455,837 QA pairs generated from structured and unstructured clinical documents.\nUnlike traditional QA datasets, emrQA includes both direct retrieval questions (e.g., \"What is the patient's blood pressure?\") and inferential questions (e.g., \"What medications should be monitored for this patient?\"). However, the original dataset does not explicitly label each QA pair with a standardized medical category. To bridge this gap, we apply Named Entity Recognition (NER) using UMLS and SciSpacy to categorize extracted answers into one of the five predefined medical categories."}, {"title": "B. Preprocessing for Multi-Task Learning", "content": "Since emrQA was originally structured for span-based QA, we preprocess it for our joint QA and classification framework:"}, {"title": "IV. MODEL ARCHITECTURE", "content": ""}, {"title": "A. Overview", "content": "The proposed model extends transformer-based Clinical Question Answering (QA) models by integrating Multi-Task Learning (MTL) to enhance structured information retrieval. The model simultaneously performs:\n\u2022 Answer Extraction: Identifying the most relevant answer span within the clinical text.\n\u2022 Medical Entity Classification: Categorizing the extracted answer into predefined medical categories: Diagnosis, Medication, Symptoms, Procedures, and Lab Reports.\nWe adopt a ClinicalBERT-based architecture with 12 transformer layers, 768 hidden units per layer, and 110M trainable parameters. The model consists of:\n\u2022 A shared transformer encoder that produces contextualized token representations.\n\u2022 A span prediction head (2 fully connected layers, 512, 256 units, ReLU activation) for answer extraction.\n\u2022 A classification head (3 fully connected layers, 512, 256, 5 units, Softmax activation) for medical entity prediction.\nBoth heads share lower-layer representations while upper layers are task-specific, reducing parameter redundancy and improving multi-task efficiency. By jointly training these tasks, the model improves both answer relevance and interpretability in clinical decision-support systems."}, {"title": "B. Transformer Model Selection", "content": "We evaluate two domain-specific transformer models:\n\u2022 BioBERT [7], pretrained on biomedical literature, optimized for general biomedical NLP tasks.\n\u2022 ClinicalBERT [8], trained on MIMIC-III clinical notes, suitable for processing electronic health records (EHRs).\nClinicalBERT is the primary model due to its alignment with clinical narratives. It captures domain-specific terminology, abbreviations, and structured note-writing styles found in EHRS.\nBioBERT, which is trained on biomedical literature from PubMed and PMC, serves as a secondary baseline to assess generalization performance. BioBERT is not exposed to real-world clinical text, which often contains informal phrasing and abbreviations unique to doctor-patient interactions. This comparison allows us to evaluate the impact of domain-specific pretraining when applied to QA tasks involving real-world clinical notes."}, {"title": "C. Multi-Task Learning Setup", "content": "The transformer encoder produces contextualized embeddings that are passed to two task-specific heads:\n\u2022 A span prediction module for answer extraction.\n\u2022 A classification module for medical entity categorization."}, {"title": "D. Handling Task Interference", "content": "To prevent gradients from one task from negatively affecting the other, we implement layer-wise attention masking, a technique inspired by task-specific adapter layers [39], [40]. In this setup:\n\u2022 The lower 6 layers of the transformer remain shared, capturing general clinical text representations.\n\u2022 The upper 6 layers are task-specific, with independent attention mechanisms for QA and classification.\n\u2022 Attention masking is applied at the task-specific layers to prevent cross-task gradient interference, ensuring that QA-specific gradients do not distort classification representations.\nThis mechanism ensures that shared lower layers generalize across both tasks, while upper layers specialize in their respective tasks, mitigating negative transfer effects in multi-task learning [41]."}, {"title": "E. Handling Answer Ambiguity", "content": "In clinical settings, extracted answers may belong to multiple medical categories. For instance, \u201cMetformin 500mg\u201d can be classified as both Medication (a prescribed drug) and Lab Report (if extracted from test results)."}, {"title": "F. Loss Function and Optimization", "content": "A weighted loss function is used to balance the QA and classification tasks:\n$L = \\lambda_{QA}L_{QA} + \\lambda_{class} L_{class},$ (2)\nwhere:\n\u2022 $L_{QA}$ is the negative log-likelihood loss for answer span prediction.\n\u2022 $L_{class}$ is the cross-entropy loss for classification.\n\u2022 $\u03bb_{QA}$ and $\u03bb_{class}$ control the contribution of each task.\nThe values of $\u03bb_{QA}$ and $\u03bb_{class}$ are selected using grid search, optimizing for both QA F1-score and classification accuracy."}, {"title": "G. Training and Evaluation Metrics", "content": "The model is trained using AdamW with a learning rate of 5 \u00d7 10-5 for 5 epochs. The training process includes:\n\u2022 Learning rate warm-up for the first 10% of training steps, followed by linear decay.\n\u2022 Early stopping based on validation loss to prevent overfitting.\nWe select evaluation metrics tailored to the dual-task nature of the model:\n\u2022 F1-score for answer span extraction, as it balances precision and recall, which is critical in medical QA where partial answers may still be relevant.\n\u2022 Exact Match (EM) to measure strict correctness in extracted spans. While this is a standard metric for QA, it is less suitable for medical QA since minor phrasing differences (e.g., \u201cTylenol 500mg\u201d vs. \u201cTylenol\u201d) may still be clinically correct.\n\u2022 Accuracy and weighted F1-score for classification. Since medical entity categories are imbalanced (e.g., more diagnoses than lab results), weighted F1 prevents minority classes from being ignored.\nF1-score is prioritized over accuracy for answer extraction because, in medical settings, the model should aim to maximize recall (retrieving clinically important information) while maintaining precision."}, {"title": "H. Algorithm: Multi-Task Learning for Clinical QA", "content": "The training process is formalized in Algorithm 2."}, {"title": "V. EXPERIMENTAL RESULTS", "content": ""}, {"title": "A. Overview", "content": "We evaluate the performance of our Multi-Task Learning (MTL) framework on the emrQA dataset using two domain-specific transformer models: BioBERT and ClinicalBERT. We measure performance on two tasks: answer span extraction and medical entity classification."}, {"title": "B. Performance Comparison: BioBERT vs. ClinicalBERT", "content": "Table III presents the performance of BioBERT and ClinicalBERT on the two tasks. ClinicalBERT consistently outperforms BioBERT across all metrics due to its pretraining on electronic health records."}, {"title": "C. Ablation Study: Impact of Multi-Task Learning vs. Standard Fine-Tuning", "content": "To assess the contribution of multi-task learning, we compare it to a standard fine-tuning approach where ClinicalBERT is trained solely on QA without multi-task objectives."}, {"title": "D. Class-Wise Entity Classification Performance", "content": "To further analyze classification performance across medical categories, Fig. 1 presents precision and recall per entity type."}, {"title": "E. Error Analysis", "content": "Table V categorizes common errors in model predictions."}, {"title": "VI. CONCLUSION", "content": "This study introduced a Multi-Task Learning (MTL) framework for Clinical Question Answering (CQA), integrating answer extraction with medical entity classification. Our approach enhances structured information retrieval, improving both accuracy and interpretability in clinical decision support.\nEmpirical results on emrQA demonstrate that MTL improves F1-score by 2.2% over standard fine-tuning while achieving 90.7% classification accuracy. By leveraging shared representations, our model mitigates negative transfer effects, ensuring robust performance across diverse medical queries.\nDespite these improvements, challenges remain, including handling paraphrased clinical queries and integrating external medical ontologies such as UMLS for enhanced classification. Future research should explore retrieval-augmented methods and contrastive learning to improve generalization. Expanding to multilingual clinical QA will further enhance applicability across diverse healthcare settings.\nOur findings contribute to advancing clinical NLP by bridging unstructured EMR text with structured retrieval, supporting more interpretable and efficient AI-driven medical question answering."}]}