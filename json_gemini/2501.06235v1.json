{"title": "NextStop: An Improved Tracker For Panoptic LIDAR Segmentation Data", "authors": ["Nirit Alkalay", "Roy Orfaig", "Ben-Zion Bobrovsky"], "abstract": "4D panoptic LiDAR segmentation is essential for scene understanding in autonomous driving and robotics, combining semantic and instance segmentation with temporal consistency. Current methods, like 4D-PLS and 4D-STOP, use a tracking-by-detection methodology, employing deep learning networks to perform semantic and instance segmentation on each frame. To maintain temporal consistency, large-size instances detected in the current frame are compared and associated with instances within a temporal window that includes the current and preceding frames. However, their reliance on short-term instance detection, lack of motion estimation, and exclusion of small-sized instances lead to frequent identity switches and reduced tracking performance. We address these issues with the NextStop\u00b9 tracker, which integrates Kalman filter-based motion estimation, data association, and lifespan management, along with a tracklet state concept to improve prioritization. Evaluated using the LiDAR Segmentation and Tracking Quality (LSTQ) metric on the SemanticKITTI validation set, NextStop demonstrated enhanced tracking performance, particularly for small-sized objects like people and bicyclists, with fewer ID switches, earlier tracking initiation, and improved reliability in complex environments.", "sections": [{"title": "1 Introduction", "content": "In the field of computer vision, scene understanding and perception are crucial components for various applications, including robotics, and autonomous vehicles. The LiDAR (Light Detection and Ranging) technology contributes to that by providing a three-dimensional and high-resolution representation of the surrounding environment. Specifically, the 4D panoptic LiDAR segmentation [1] task which was recently introduced, utilizes the LiDAR data to enable the simultaneous segmentation of objects into semantic classes and the tracking of their movements over time, wishing to unify semantic segmentation, instance segmentation, and tracking into a single framework.\nSeveral existing methods address the 4D panoptic segmentation task, including, 4D-PLS [1] and 4D-Stop [5] which employ a tracking-by-detection methodology comprising two primary steps. The first step involves detection, where"}, {"title": "2 Related Work", "content": "Tracking-by-detection. Tracking-by-detection is a method where objects are first detected in each frame using an object detector, and then tracked across frames by matching these detections using data association techniques. Data association methods are categorized into online (casual) methods, which seek optimal associations between past and current frames, and offline (non-casual) methods, which aim for global optimal associations across the entire sequence"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Motivation", "content": "LiDAR technology measures ground object distances with laser beams, creating 3D point clouds that face challenges in tracking due to sparse and irregular"}, {"title": "3.2 NextStop Tracker", "content": "we introduce NextStop, our novel tracking mechanism intended to enhance and replace the tracking component of the 4D Panoptic LiDAR Segmentation network, 4D-STOP [5]. As illustrated in Figure 1, NextStop takes as input the panoptic segmentation per frame, which is generated by the pre-trained 4D-STOP [5] neural network. The NextStop framework consists of two primary stages: the bounding box tracker stage and the bounding box to point label stage. Detailed descriptions of these stages will be provided in the following sections."}, {"title": "3.3 The Bounding-Box Tracker (Stage 1)", "content": "In the tracking-by-detection approach, once panoptic segmentation provides the object detections, the next step is to link these detected objects across frames to establish their trajectories over time. Inspired by the SORT [3] approach, we used the AB3DMOT [9] tracker as a base framework to create the NextStop box tracker framework which is shown in Figure 3.\nDetection At each frame t an object detection network detects objects and produces a set of detections. Our study utilized 4D-STOP network results [5]. These panoptic per-point results were converted to bounding box detections. Each bounding box is represented by its center-point $(c_x, c_y, c_z)$, orientation angle $\\theta$, dimension measurement $(l, w, h)$, and score s that corresponds to the highest score of point associated with the object. Due to the lack of orientation information in the SemanticKITTI database, $\\theta$ was always zero. The bounding box detections are then divided into low score detection boxes and high score detection boxes, for use at later stages.\nMotion-Based Prediction A motion model is a mathematical representation that forecasts the evolving state of an object over time, covering various at- tributes like position and velocity. The predicted state can then be compared"}, {"title": "3.4 Bounding-box to Point Label (Stage2)", "content": "The initial stage, known as the Bounding-Box Tracker (outlined in subsection 3.3), yields tracking results in the form of bounding box trackers. However, for the task of 4D Panoptic LiDAR Segmentation and Tracking, the expected tracking results are in a \"per-point\" format. This implies that each point belonging to a tracked object should possess a label exclusively assigned to that object and unique over time. To accommodate this requirement, Stage 2 was introduced, where we provide a tracking label per point derived from the bounding box track label. Refer to Figure 4 for an illustration of this process."}, {"title": "4 Numerical Experiments", "content": ""}, {"title": "4.1 Implementations Details", "content": "The Bounding-Box Tracker: Discovering the optimal parameter values involved a dual-stage process. The first stage centered on refining the motion estimator, which is based on the Kalman filter. Subsequently, the second stage concentrated on enhancing various other elements including data association, tracker state transitions (from active to candidate and vice versa), and lifespan management. The optimal parameter value varies depending on the class type of the objects (vehicles, bikes, and pedestrians) due to their distinct characteristics, which necessitate different parameter initialization. Specifics regarding the parameters utilized for the Kalman filter can be found in section B, while details concerning the parameters for the other components are enumerated in Table 1\nBounding-box to Point Label: In this section, two parameters were specified: bounding box overlap and 'ignore size'. A bounding box overlap was defined when two boxes shared more than three points. Furthermore, an 'ignore size' of 25 was set, indicating that objects categorized as belonging to the Things class and smaller than 25 points were assigned a value of zero for both their class ID and instance ID."}, {"title": "4.2 Comparing with State-of-the-Art Methods", "content": "Dataset We evaluate our method on the SemanticKITTI [2] validation set. The dataset provides point-wise annotations for 28 semantic classes, divided into"}, {"title": "4.3 Qualitative Results", "content": "In this section, we demonstrate our tracking method using a few selected examples extracted from the SemanticKITTI [2] validation set, alongside a comparison to the tuned tracking parameter (TTP) version of 4D-STOP [5], which exhibited better performance compare to the non-tuned one.\nSuccessful Examples. Presenting three successful examples, Figures Figure 8 and Figure 9 showcase the tracking of individual objects. Additionally, in Figure 10, we demonstrate how our NextStop tracker enhances semantic segmentation outcomes, particularly addressing cases of temporal inconsistency.\nIn Figure 8, the tracking of a Moving Car is depicted, originating approximately 50 meters away from the LiDAR sensor, presenting a non-trivial tracking challenge due to the object's small size at such distances.\nSimilarly, in Figure 9, the tracking of a Moving Person object is shown. This example also begins far from the LiDAR sensor and poses additional challenges due to the inherently small size of a Moving Person object, even when close to the sensor.\nThese examples highlight that utilizing tracking information from the NextStop tracker aids in improving time-inconsistent semantic class results. Furthermore, our NextStop tracker achieves fewer ID switches and initiates tracking earlier, particularly enhancing trajectory coverage for distant objects.\nExamining a Challenging Case Study. In Figure ??, we illustrate the trajectory of a single-tracked object classified as a moving-car. During its movement, the object experienced partial occlusion. Despite this challenge, our NextStop tracker demonstrated improvement: it effectively connected the identity observed during occlusion with the post-occlusion identity, leading to fewer ID switches compared to 4D-STOP. However, 4D-STOP initiated tracking of the object earlier during the occlusion, presenting a disadvantage for our NextStop tracking performance during those frames."}, {"title": "5 Conclusion", "content": "We presented a multi-object tracking algorithm that utilizes data from a 3D point cloud captured by a LiDAR sensor, named NextStop. It uses a tracking-by-detection approach, where the detection is taken from an off-the-shelf network. In addition, our NextStop tracker possesses two primary attributes: (i) First, we incorporate motion estimators (based on the Kalman filter) to minimize the impact of incorrect detections on the tracker. Since each object has different properties, we divided the estimators into categories of classes: vehicles, bikes, and pedestrians. (ii) Second, we implemented two priority mechanisms: tracklet prioritization and detection prioritization. Tracklet prioritization prioritizes trustworthy tracklets over untrustworthy ones, whereas detection prioritization prioritizes high-scoring detection results over low-scoring ones.\nWe demonstrated that our NextStop tracker offers greater continuity, with fewer ID switches and earlier tracking initiation compared to alternative approaches. The tracker showed substantial improvement in the LSTQ metric,"}, {"title": "A Ablation Study", "content": "The primary objective of our ablation study is to assess the effectiveness of each component of the bounding box tracker and to quantify the contribution of each individual component."}, {"title": "Motion Estimation Contribution", "content": "Motion Estimation Contribution: In Table 10, we present the LSTQ1 score obtained with and without Kalman filter motion estimation. It appears that employing the Kalman filter yields superior LSTQ results across all classes.\nDIoU Contribution: In Table 11, we present the LSTQ1 scores obtained using GIoU as a matching score in the data association block, in comparison to the NextStop baseline, which utilizes DIoU. Notably, both GIoU and DIoU values range from -1 to 1, hence no adjustments were required to thresholds, and we adhered to those set in Table 1. It appears that DIoU contributed to improvements in the classes Car, Bicycle, Person, and Bicyclist, while the remaining classes remained unchanged.\nTracklets State Contribution: In Table 12, we present the LSTQ\u2081 scores obtained when eliminating the concept of tracklet states, specifically removing the Candidate state. It's evident that removing the candidate state, thereby retaining only the active state, resulted in a higher association score, as it eliminates the hiding of any tracklet. However, this alteration also led to a decrease in the classification score, emphasizing the significant contribution of tracklet states to the classification score.\nSplit by Detection Score Contribution: In Table 13, we display the LSTQ1 scores achieved without splitting the detection into high score and low score"}, {"title": "B Discrete Kalman Filter", "content": "The Kalman filter [4] is a mathematical algorithm used in control systems and estimation processes, aiming to provide an optimal estimate of a system's state, $x \\in R^n$, based on noisy measurements $z \\in R^m$ and the control input $u \\in R^l$ over time."}, {"title": "B.1 The Model", "content": "The model assumes that the true state at time step k, denoted as $x_k$, is derived from the state at time step k - 1. When control information is absent, the discrete Kalman filter model is describe by Equation 1, and the observation (or measurement) $z_k$ of the true state $x_k$ adheres to Equation 2.\n$x_k = F_kx_{k-1} + w_k$  (1)\n$z_k = H_kx_k + v_k$ (2)\nThe vectors of random variables $w_k \\in R^n$ and $v_k \\in R^m$ denote the process and measurement noise, respectively. They are assumed to be mutually independent of each other. Additionally, each vector is independently and identically distributed (IID), following Gaussian white distributions, i.e., $w_k \\sim N(0, Q_k)$ and $v_k \\sim N(0, R_k)$.\nIn Equation 1, $F_k \\in R^{nxn}$ represents the state transition matrix. This matrix establishes the connection between the previous state $x_{k-1}$ at time step k - 1, and the current state $x_k$ at time step k.\nIn Equation 2, $H_k \\in R^{mxn}$ represents the observation matrix. This matrix connects the state $x_k$ to the measurement $z_k$ at time step k."}, {"title": "B.2 The Equations", "content": "The Kalman filter is typically divided into two main phases: Prediction and Update (also known as Correction). At each step k, the Prediction equations Equation 3 are used to predict the prior estimate state $\\bar{x}_{k|k-1}$ and the prior covariance matrix $P_{k|k-1}$. Then, if a measurement $z_k$ has been observed, the Update equations Equation 4 are used to predict the posteriori state estimate $\\hat{x}_{k|k}$ and the posteriori covariance matrix $P_{k|k}$. If no measurement has been observed, the Prediction equations are utilized to project the state forward until the next scheduled measurement is obtained.\nPredicted (a priori) state estimate: $\\bar{x}_{k|k-1} = F_k\\hat{x}_{k-1|k-1}$ (3)\nPredicted (a priori) estimate covariance: $P_{k|k-1} = F_kP_{k-1|k-1}F_k^T + Q_k$\nInnovation: $\\tilde{y}_k = z_k - H_k\\bar{x}_{k|k-1}$\nInnovation covariance: $S_k = H_kP_{k|k-1}H_k^T + R_k$\nKalman gain: $K_k = P_{k|k-1}H_k^TS_k^{-1}$ (4)\nUpdated (a posteriori) state estimate: $\\hat{x}_{k|k} = \\bar{x}_{k|k-1} + K_k\\tilde{y}_k$\nUpdated (a posteriori) estimate covariance: $P_{k|k} = (I - K_kH_k)P_{k|k-1}"}, {"title": "B.3 Implementation Details", "content": "As the SemmanticKITTI dataset [2] lacks control information, the model equation Equation 1 becomes:\n$x_k = F_kx_{k-1} + w_k$ (5)\nwhile the Equation 2 remains the same. We assumed that the objects move with a constant velocity over time, and as such our state vector is represented by Equation 6:\n$x_k = [c_x, c_y, c_z, \\theta, l, w, h, v_x, v_y, v_z]^T$ (6)\nWhere the first 7 elements $(c_x, c_y, c_z, \\theta, l, w, h)$ represent the 3D bounding box: centroid, orientation, and dimension measurements, and the last elements $(v_x, v_y, v_z)$ represents the 3D bounding box velocity.\nThe measurement vector $z_k$ is represented by Equation 7:\n$z_k = [c_x, c_y, c_z, \\theta, l, w, h]^T$ (7)\nWhich includes the centroid, orientation, and dimensions of the 3D detection bounding box. We picked all matrices $F_k, H_k, Q_k$ and $R_k$ to be time independent meaning:"}]}