{"title": "Diffusion Model Based Resource Allocation Strategy in Ultra-Reliable Wireless Networked Control Systems", "authors": ["Amirhassan Babazadeh Darabi", "Sinem Coleri"], "abstract": "Diffusion models are vastly used in generative AI, leveraging their capability to capture complex data distributions. However, their potential remains largely unexplored in the field of resource allocation in wireless networks. This paper introduces a novel diffusion model-based resource allocation strategy for Wireless Networked Control Systems (WNCSs) with the objective of minimizing total power consumption through the optimization of the sampling period in the control system, and blocklength and packet error probability in the finite blocklength regime of the communication system. The problem is first reduced to the optimization of blocklength only based on the derivation of the optimality conditions. Then, the optimization theory solution collects a dataset of channel gains and corresponding optimal blocklengths. Finally, the Denoising Diffusion Probabilistic Model (DDPM) uses this collected dataset to train the resource al- location algorithm that generates optimal blocklength values conditioned on the channel state information (CSI). Via extensive simulations, the proposed approach is shown to outperform previously proposed Deep Reinforcement Learning (DRL) based approaches with close to optimal performance regarding total power consumption. Moreover, an improvement of up to eighteen- fold in the reduction of critical constraint violations is observed, further underscoring the accuracy of the solution.", "sections": [{"title": "I. INTRODUCTION", "content": "WNCSs are control systems with control loops closed through a wireless communication network [1]. WNCSs play an important role in supporting emerging ap- plications in sixth-generation (6G) networks, such as remote driving [2] and cooperative robots (cobots) [3]. The joint optimization of the performance of the control and commu- nication systems is the main challenge in WNCSs due to the high complexity of modeling the interactions between these two systems, the stringent ultra-reliability requirements of control systems, and the non-ideal propagation characteristics of wireless communication systems.\nEarlier research on the joint design of control and com- munication systems for WNCSs focused on the usage of optimization theory-based solution strategies following the derivation of the right abstractions of these two systems [4], [5]. In joint optimization frameworks that uniquely abstract the performance of control and communication systems, an optimization problem is formulated with both the control and communication decision variables and solved using iterative or heuristic methods following the derivation of the optimality conditions. However, the high complexity of the proposed model-based methods may hinder their application in low- latency WNCS scenarios.\nIn order to solve the time complexity issue of the opti- mization theory-based solutions and additionally operate with incomplete CSI, DRL is introduced to allocate resources opti- mally through trial and error in a complex scenario [6]. In our previous work [7], we have proposed an optimization theory- based DRL approach, where first, the model is simplified based on the optimality conditions of the optimization problem. Then, the new simplified problem is fed to a Dueling Double Deep Q-network (D3QN) to output the optimal blocklength values. Although DRL methods use online interactions with the environment to learn, the amount of data needed to train the model is large. Moreover, DRL models may produce infeasible solutions that violate the constraints of the control and communication systems, which may have detrimental effects on critical safety applications.\nGenerative AI-based approaches are considered a solution to enhance the performance of 5G/6G networks. [8] introduces Generative Adversarial Networks (GANs) to be utilized along with a DRL model to enhance the model's training and adapt- ability to extreme conditions compared to the conventional DRL models. However, the GAN used in the proposed model is solely used to generate data to pre-train the DRL model and is not involved in the decision-making process. Additionally, GANs are known to be unstable in training time and cannot generate high-quality samples in the inference phase. To fix the instability and poor quality of the generated samples, DDPMs introduced in [9] are proposed to be utilized in [10] to enhance the performance of the wireless networks in constellation shaping problem. However, to this day, diffusion models are not directly applied to resource allocation problems in wireless networks.\nIn this paper, we propose a novel DDPM-based resource allocation scheme for the joint design of control and commu- nication systems in WNCSs for the first time in the literature. The DDPM is used to generate optimized blocklength values from an isotropic Gaussian distribution by using the CSI as conditional information."}, {"title": "II. SYSTEM MODEL", "content": "The WNCS consists of N sensor nodes with blocklength mi, sampling period hi, and packet error probability pi for i\u2208 {1,2,...,N}. Sensor nodes connected to a physical plant measure and send the plant's state to a controller via a wireless channel. Based on the recent state update information, the controller decides on a new control command and sends it to the actuator to be executed. The outdated packets are not retransmitted since old state information can harm time- critical control systems. The packet error is modeled as a Bernoulli random process to simplify the problem. The Time Division Multiple Access (TDMA) method is utilized for a deterministic access delay widely preferred in various automation applications [11]. We assume that the channel time is segmented into frames, and each frame is subdivided into time slots. The initial slot is allocated for the beacon frame, which the controller sends out periodically to disseminate synchronization and scheduling updates among the nodes within the WNCS. During the scheduling update, nodes are allocated time slots for their respective data transmissions and additional parameters, such as the optimal transmission power and blocklength. We assume that nodes within the network do not transmit simultaneously and that the network manager continuously monitors the packet error rate."}, {"title": "A. Optimization of Control and Communication Systems", "content": "The joint optimization of control and communications sys- tems for ultra-reliable communication in the finite blocklength (FBL) regime is adopted from our previous paper [7] and is presented below.\nminimize C1 C2 exp(Q^-1(Pi)*sqrt(mi) / hi) +mi In 2Li)-1] + \u03a3_i C2*Wmi/hi\nsubject to\n\u03a9[] Inpi - In (1-\u03b4) \u2264 0, \u2200i \u2208 [1,N]\ndi(mi) \u2264 min (\u2206, hi), \u2200i \u2208 [1,N]\n0 < h\u2081 < \u03a9, \u2200i \u2208 [1,N]\n0 < Pi \u2264 1, Vi \u2208 [1,N]\nmi \u2264 Mth, Vi \u2208 [1,N]\nCil exp(Q^-1(Pi)*sqrt(mi) / hi) +mi In 2Li)-1] \u2264 Wtx,max\n\u03a3_i Camik (mi) / hi< \u03b2,\nwhere Ci1=\u03c32/gi and C2=8; 02 denotes the noise power spectral density (PSD); gi denotes the channel gain of sensor node i; B is the bandwidth; Q\u00af\u00b9(.) denotes the inverse of Q function; Li is the packet length, and W is the circuit power for node i. The objective function (1a) is to minimize the total power consumption in the network considering both the transmit power and the circuit power of the nodes when sending packets. Constraints (1b) and (1c) represent stochastic \u039c\u0391\u03a4\u0399 (\u03a9) defined as the probability of maximum allowed time interval between the reception of the state vector reports above MATI being greater than a predefined value \u03b4 and MAD (\u0394) defined as the maximum packet delay smaller than a maximum limit, respectively, used as an abstraction of the requirements to guarantee a certain control system performance. Constraints (1d) and (le) give the lower and upper bounds of the sam- pling period and packet error probability. Equation (1f) states that plant state information is transmitted in small packets using a finite blocklength, which cannot exceed a threshold value Mth because the transmission must finish before the maximum allowable channel uses is reached. Additionally, a maximum transmit power constraint in (1g) limits the nodes from exceeding a certain transmit power level due to the limited power source of the sensor nodes and government reg- ulations. Moreover, the schedulability constraint (1h) ensures that transmission times are assigned to multiple sensor nodes without any two nodes transmitting simultaneously. Each node i is allocated a fraction of the total schedule length, denoted as di. Since no two nodes can transmit simultaneously, the sum of these terms represents the total time allocated to all nodes relative to the schedule length. The problem is a non- convex Mixed-Integer programming problem, so searching for a global optimum solution is difficult [12]."}, {"title": "B. Simplified Optimization Problem", "content": "The problem in (1) is not tractable and has to be simplified in order to reach sub-optimal results. As a result, the solution is grouped into multiple blocks based on the derivation of the optimality conditions, and the decision variables are reduced to consider blocklength only. Then, the other decision variables can be obtained through optimality conditions.\nThe optimality conditions are derived in [7] as\nln p_i = \u03a9 ln (1 \u2013 \u03b4) / h_i = k_i,\nwhere ki is a positive integer. Next, the optimal value of ki is derived in terms of mi as\nk_(i) (m_(i) )=max{1,\u2308ln  ( \u03a9 ln ( 1-\u03b4)W_(tx,max) / h_i)  /  ( sqrt(m_(i))*C_(i1) / h_(i) ln(2) L_(i) / sqrt(m_(i)) +1)\u2309}\nThen, the problem (1) is simplified to reduce the decision variables and the constraints in the problem. The model is optimized using one decision variable of blocklength mi instead of three decision variables, and the other variables are derived using the optimality conditions described in (2) and (3). The modified joint optimization problem is formulated as"}, {"title": "III. DDPM-BASED RESOURCE ALLOCATION ALGORITHM", "content": "The proposed diffusion-based resource allocation algorithm is a centrally-trained-centrally-executed model consisting of two stages: optimization theory-based data collection and DDPM- based training and data generation, as depicted in Fig. 1. In the optimization theory-based data collection stage, various values of channel gains and the corresponding optimal block- length values are collected. The optimal blocklength values are determined by solving the optimization problem in (4). The resulting dataset is then the input to the the diffusion model. In the diffusion model stage, the collected dataset is used to train a diffusion model and learn the optimal parameters to choose an action for blocklength adaptation for given channel gains.\nThe model is implemented in the controller, where control commands are sent to each sensor node after execution. DDPM consists of input states and conditional information as provided below:\n\u2022 Input States: The objective of the DDPM-based method is to generate outputs drawn from a similar distribution to the input states. In the training phase, the input states are the optimal blocklength values from the dataset. Batches of data are sampled from the dataset. They are input to the model to modify and update the parameters of the neural network so the model can learn the solution space distribution and generate desired outputs after the training phase. On the other hand, in the inference phase, the input states are drawn from an isotropic Gaussian distribution with mean zero and standard deviation one, and the outputs are generated through the denoising process of the diffusion models.\n\u2022 Conditional Information: The conditional information given to the network is the CSI of the links to condition the learning process on the environmental variables to ensure the model is trained to execute actions based on the current channel state. This is the most important part of the model since the environment directly affects the learning process. In the training time, in addition to the CSI, uniformly sampled time steps are given to the model to train the model to denoise the samples efficiently.\nThe proposed algorithm is summarized in Algorithm 1, which is comprised of three parts, namely, initialization and dataset collection from optimization theory-based solution, training phase, and execution phase."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In this section, the performance of the diffusion-based resource allocation technique is compared to the benchmark models, including the pure optimization theory-based model and the DRL-based models. The DRL benchmarks, Branching Deep Q-networks (BDQ), which are suitable to solve problems with multi-variable action space, are utilized to solve the optimiza- tion problem in (1) with decision variables of blocklength, sampling period, and packet error probability. The D3QN is used for the simplified optimization problem (4) with only blocklength decision variable. First, the simulation settings for the environment and the diffusion model are given in part IV-A, and then, performance comparison and analysis of the proposed and benchmark methods are presented in part IV-B."}, {"title": "A. Simulation Setup", "content": "Simulations are conducted for a network where the nodes are uniformly spread out within a circular area with a 50-meter radius, all communicating with a central controller. The path loss and shadowing effect result in large-scale fading, and it is modeled as PL(di)[dB] = PL(do)[dB] + 10a log di/do + Z, where di is the distance of node i from the central controller, PL(di) is the path loss of node i at distance di in decibels, PL(do) = 35.3 dB is the path loss at the reference distance do = 1m, path loss exponent a = 3.76, Z is a Gaussian random variable with zero mean and standard deviation equal to 4 dB corresponding to log-normal shadowing. For small- scale fading, Jake's model [7] is used, expressed as a first-order complex Gauss-Markov process. The simulation parameters are listed in Table I.\nThe neural network used for noise prediction has a modified UNet architecture. The grid search algorithm selects the opti-"}, {"title": "B. Performance Comparison and Analysis", "content": "We validate the accountability of the generated sample dis- tributions compared to the original ones. The q-q plots val- idate the similarity between the generated and true samples. Both samples are compared to a Gaussian distribution with zero mean and unit standard deviation. Figure 2a shows that samples exhibit similar behavior with very small discrepancy in the tail of the Gaussian distribution. This demonstrates that DDPM-based and optimization theory-based solutions are drawn from two PDFs with similar parameters.\nFigures 3a and 2b show the average power consumption as a function of the number of nodes and the Empirical Cumulative Distribution Function (ECDF) of power consumption for 64 nodes, respectively. The optimization-based method demon- strates the highest performance. The DDPM-based resource allocation algorithm falls slightly short of the optimization method but is very close in performance and similarity to the optimization algorithm. As the number of nodes increases, the DDPM-based model's performance becomes similar to that of the pure optimization method. This demonstrates the scalabil- ity of diffusion models, indicating that increasing the number of nodes in the network does not result in poor performance due to the complicated structure of the network. The DRL approaches cannot surpass the DDPM-based algorithm both in average power consumption and ECDF. Finally, random selection has the poorest performance among the algorithms.\nFigure 3b shows the execution time comparison of different algorithms as a function of the number of nodes. As expected, optimization theory-based method execution time grows ex- ponentially as the number of nodes increases, which is not applicable in practical scenarios, especially URRLC, due to stringent conditions in the environment. On the other hand, the DDPM-based method and DRL methods exhibit linear growth as the number of nodes increases. Moreover, although the training time of the DDPM-based method is higher than the DRL-based methods due to the number of parameters in the network, the DDPM-based model shows superior performance during the testing phase. This is because the structure of the DDPM-based model allows for more effective utilization of GPU power, enabling faster performance when no further training is needed.\nFinally, the proposed methodology demonstrates substantial resilience in terms of violating critical constraints because it is trained to mimic the optimization theory results, but the DRL-based approaches try to avoid constraint violation by incorporating penalties in their reward function, which is not always reliable. Figure 4 depicts the comparison between different algorithms for the number of times that the maxi- mum transmit power constraint is violated as a function of the number of nodes. The result is the average number of times that algorithms have violated the power constraint in the testing phase, normalized over the total number of time steps. The proposed DDPM-based model demonstrates up to eighteen-fold improvement over cutting-edge DRL methods in terms of reliability for not violating the constraints. D3QN performs better than BDQ since it chooses actions from a smaller action space range, improving its performance and reliability. Random selection has the worst performance in terms of constraint violations."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel diffusion-based resource allo- cation framework for the joint optimization of communication and control systems to minimize the total power consumption of the nodes in URLLC with finite blocklength. The algorithm utilizes a DDPM model and a dataset collected from an optimization-based solution to learn the environmental vari- ables and generate optimal blocklength values for each node. The proposed blocklength adaptation approach outperforms existing DRL-based benchmark models regarding total power consumption and performs better in avoiding actions that will cause constraint violation. In the future, we plan to investigate a DDPM-based online learning algorithm to combine the power of generative AI with DRL-based approaches where a dataset is unavailable or extremely costly to collect, like in massive MIMO communication systems."}]}