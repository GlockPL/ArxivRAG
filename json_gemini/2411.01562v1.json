{"title": "Are LLMs good pragmatic speakers?", "authors": ["Mingyue Jian", "N. Siddharth"], "abstract": "Large language models (LLMs) are trained on data assumed to include natural\nlanguage pragmatics, but do they actually behave like pragmatic speakers? We\nattempt to answer this question using the Rational Speech Act (RSA) framework\n[9, 10], which models pragmatic reasoning in human communication. Using the\nparadigm of a reference game constructed from the TUNA [27] corpus, we score\ncandidate referential utterances in both a state-of-the-art LLM (Llama3-8B-Instruct)\nand in the RSA model, comparing and contrasting these scores. Given that RSA\nrequires defining alternative utterances and a truth-conditional meaning function,\nwe explore such comparison for different choices of each of these requirements.\nWe find that while scores from the LLM have some positive correlation with those\nfrom RSA, there isn't sufficient evidence to claim that it behaves like a pragmatic\nspeaker. This initial study paves way for further targeted efforts exploring different\nmodels and settings, including human-subject evaluation, to see if LLMs truly can,\nor be made to, behave like pragmatic speakers.", "sections": [{"title": "1 Introduction", "content": "With the emergence of large language models (LLMs) [1, 4, 6, 14, 15, 25, 26], a key question\narises: can these models, trained on data presumed to include natural language pragmatics, exhibit\npragmatic reasoning akin to humans? While LLMs have demonstrated formal linguistic competence\n(adherence to linguistic rules), their functional competence in pragmatic language use remains\nuncertain and warrants further investigation [17]. Although much research has focused on evaluating\nLLMs' pragmatic abilities as listeners, particularly their comprehension of non-literal language [12,\n16, 19, 23, 24], less attention has been given to their pragmatic capabilities as speakers. Specifically,\nit remains unclear whether LLMs can effectively use context to generate non-literal utterances; i.e.,\nbeing informative beyond being simply true. Investigating this aspect is crucial for deepening our\nunderstanding of LLMs' generative processes and enhancing their reliability.\nHumans are typically pragmatic agents in communication. Imagine a room with three pieces of\nfurniture: a small red desk, a small yellow desk, and a large red chair. If a friend directs your\nattention to \"the red one\", you would first eliminate the yellow desk as a possibility. Given the\nremaining red objects, your reasoning would then likely discount the chair, on account of it being\ndistinct in the room, and that if it were the intended referent, the simpler utterance would have\nbeen just \"the chair\u201d. This finally leads you to the intended referent being the small red desk. This\nrecursive reasoning process that takes informativity into account beyond simply being literally true,\nencapsulates pragmatic communication.\nA particularly influential model of pragmatic communication is the Rational Speech Act (RSA)\nframework [7, 10] which quantitatively models theory of mind [8, 21], formalising how speakers and\nlisteners use context, shared knowledge, and probabilistic reasoning to communicate effectively. This\nframework operates language understanding as a recursive process, where both speakers and listeners\nin a conversation behave rationally to reason each other's intention."}, {"title": "2 Rational Speech Act Model (RSA)", "content": "The RSA model iteratively refines a heterogeneous relation between alternative utterances U and\nintended meanings O, such that the relations begin being purely literal, and is refined using pragmatic\nreasoning: U \u00d7 O \u2192 [0, 1]. The framework begins with a literal listener Llit:\n$P_{Llit} (o|u) \\propto M(u, o) \\cdot P(o),$ (1)\nwhere, in the context of our reference game, each object is equally likely to be selected, resulting in a\nuniform prior P(o). Thus, the literal listener's interpretation relies entirely on the meaning function\nM(). The pragmatic speaker Ps, is constructed from a literal listener Llit:\n$P_{S}(u|o) \\propto e^{\\alpha(ln P_{Llit} (o|u) - ln |u|)} \\cdot \\frac{P_{Llit}(o|u)}{u^{\\prime}}$.\n(2)\nHere, |u| is the utterance length imposing a cost on longer productions. This cost function aligns with\nthe maxims of a pragmatic speaker [11], favouring the use of fewer attributes to convey the intended\nmeaning within a controlled attribute space. Additionally, this approach ensures that comparisons\nbetween the RSA models and LLM remain valid. The preset prompt to the LLM (Appendix B) instructs\nit to describe the object using as few words as possible, effectively serving as a cost function that\nindirectly penalises longer outputs. \u03b1 is a parameter that scale the rational level of Sp. A higher \u03b1\nwill sharpen the probability distribution and vice verse."}, {"title": "2.1 RSA model with different meaning functions", "content": "We construct two RSA models with different meaning functions (MFs) for further investigation. They\ntake the form U \u2192 [0, 1], indicating whether the utterance u literally describes the object o.\nPrompt-based MF: This leverages the natural language understanding capabilities of LLMS for\nscoring. We use prompt engineering to generate numeric scores from the LLM, employing 3-shot\nprompting to guide the model with input-output examples that establish a fixed output template\n(Figure 6 in Appendix F). The prompt-based meaning function is defined as:\n$M_p(u, o) = \\frac{P(Yes | LLM(o, u))}{P(Yes \\cup No | LLM(o, u))}$,\n(3)"}, {"title": "Rule-based MF", "content": "This is based on feature exclusion: an utterance u that includes a feature contradict-\ning those of the object o does not describe o. For example, if o is \"a large, grey chair facing forwards\",\nthen the utterance u as \u201ca green thing\" does not describe o as the colour feature in u contradicts that\nof o. We define the rule-based meaning function as:\n0 = {f1, f2,... fn} \u2286 F,\n$M_r(u, o) := \\#w.(\u2203f.f \u2208 F \\o> D(w, f)) \u2227 (w \u2208 u)$,\n(4)\n(5)\nwhere f1, f2, ... are the specific features of the object o, F is a full set of predefined features in the\ndataset, and D is a relation containing (w, f) iff word w describes feature f.\nWe evaluate the two MFs against human-labelled ground truth data and find that the rule-based\nfunction consistently identifies literal relationships for the logical-constructed sequences, and most of\nthe top-k generated sequences (mean Acc = 99.9%), while the prompt-based method occasionally\nfalls short in both constructed sequences (mean Acc = 97.3% for the 3-shot prompt-based method)\n(Appendix F)."}, {"title": "3 Evaluation Pipeline", "content": "We use a reference game [13, 22] as the task, where a set of objects O includes target object ot. The\nspeaker selects an utterance ut \u2208 U to convey ot to the listener, who must identify it based on O and\nutterance ut. For this task, we employ the TUNA dataset (furniture domain) [27], which organises\neach reference game around 7 objects with predefined attributes and features (Appendix A).\nWe propose a pipeline for the evaluation (Appendix C), which is organised into three stages: first,\nconstructing alternative utterance U and meaning O spaces within the context of the reference game;\nsecond, getting scores from the vanilla LLM and the RSA models with different meaning functions for\neach alternative sequence; and finally, evaluating the output distribution across various metrics."}, {"title": "3.1 Construction of the meaning and utterance space", "content": "Meaning space: For each reference game, we map object attributes from the TUNA dataset\ninto a noun phrase template to generate descriptions: a <SIZE>, <COLOUR> <TYPE> facing\n<ORIENTATION>. This results in the meaning space of a set of 7 object descriptions for each game.\nUtterance space: In a reference game, the utterance space includes all alternative utterances within\nthe restricted world that could describe any object in the meaning space. An optimal utterance\nspace would encompass both literal and pragmatic expressions, enabling a thorough assessment\nof communicative effectiveness. However, even in a restricted setting, the construction of such an\nutterance space, accounting for the wide variety of sentence structures and connotations found in\nnatural language, can be difficult. Previous research has predominantly concentrated on producing\nsentences that are pragmatic, rather than exploring the full spectrum of meaning generation [28]. In\npragmatic referring expression generation, this typically involves sampling from a learned model\nduring inference [2, 18, 28]. However, this method inherently produces only pragmatic sequences, as\nLLMs are presumed to be trained on pragmatic data. We present two approaches for constructing the\nutterance space U.\nTop-k alternatives: This samples the top-k utterances from the LLM using beam search, generating\npragmatic sequences with flexible phrasing. The LLM receives a prompt (Appendix B) containing\nthe world context and a concise task description to identify the target object ot, ensuring minimal\ninstruction to test the LLM's inherent pragmatic reasoning. To maintain consistency, generation begins\nwith both \"a\" and \u201cthe\u201d to ensure a noun phrase format. We generate sentences then deduplicate\nsemantically identical ones that differ only in minor details like punctuation.\nLogical rule alternatives: This approach constructs both pragmatic and literal utterances based on\nlogical rules. This method is particularly effective in a text-based reference game setting, where a\nliteral utterance includes all relevant features, while a pragmatic utterance may involve omitting some\nfeatures. In particular, since the generated sequence should follow a noun-phrase format, the omission"}, {"title": "3.2 Getting scores from the two models", "content": "We score alternatives in the LLM and in RSA given the same reference game world.\nVanilla LLM: The logit probabilities, i.e., raw output values from the LLM before they are transformed\ninto a probability distribution, directly indicate the model's preferences. These are used as scores to\nassess the LLM's behaviour:\n$p(u | O, o_t) = p(u | c(O, o_t)) = \\prod_{i=1}^{N} p(u_i | c(O, o_t), u_{1:i-1}),$\nwhere c() is the prompt template (Appendix B) and N is token length. Scores for top-k alternatives\nare generated along with the sequence using beam search. For the logic-constructed alternatives,\nwe compute the probability retroactively for each utterance. Many popular pre-trained LLMs excel\nin downstream tasks, but few are open-source and provide logit probabilities. For this project, we\nuse the open-source Meta-Llama3-8B-Instruct model [3, 26]. We access logit probabilities using the\npython-llama-cpp library.\nRSA models: The scores are calculated using Eq.1 and Eq.2 with the two constructed meaning\nfunctions."}, {"title": "4 Results", "content": "Data Overview: We have 2,940 reference games, each with a set of utterances describing one\nobject, and we generate 386,510 utterance instances in total. Of these, 88,310 are generated by top-k\nsampling, and 298,200 by logic-based rules.\nEvaluation metrics: We test models using probability outputs from the vanilla LLM and two RSA\nmodels: one with a prompt-based MF and the other with a rule-based MF. We assess the correlation\nbetween scores from the vanilla LLM and RSA models using Pearson Correlation Coefficient (PCC) for\nlinear relationships and Spearman's Rank Correlation Coefficient (SRCC) for ranking similarity.\nExperiment 1: We analyse the overall correlation across all reference games by comparing the\nscoring of each utterance instance related to any object for both models. Figure 1a illustrates the\ncorrelation between the vanilla LLM and the RSA models using different meaning functions. Both\nscatter plots show no clear linear relationship between the scores. The performance of the two\nmeaning functions is comparable, as evidenced by the similar patterns observed across both plots.\nThe RSA model's scoring can be interpreted as a spectrum from incorrect to literal to pragmatic\nutterances. The vanilla LLM scoring reveals that literal utterances are favoured by the LLM more\nthan pragmatic ones. Particularly notable is the right-hand tail of the graph, where many pragmatic\nutterances are either minimally acknowledged or largely overlooked by the LLM. This suggests that\nwhile the LLM is able to correctly make factual judgement, they are unlikely to rank the utterances\npragmatically. The RSA models in Figure 1 are configured with \u03b1 = 1.0. To further investigate the\neffect of \u03b1 on correlation, we experiment with different values of \u03b1 \u2208 {0.2, 0.6, 1.0, 1.4, 1.8, 3.0}.\nCorrelation plots for each \u03b1 are provided in Appendix E. Our results indicate that varying \u03b1 does\nnot alter our primary finding: the scores from LLM do not exhibit strong alignment with those of the\nRSA models, and the general distribution pattern remains consistent. Furthermore, we observe that\nincreasing \u03b1 sharpens the probability distribution, as reflected by the increase of the upper-bound on\nthe x-axis."}, {"title": "5 Discussion", "content": "Based on our experiments and analyses, we see no clear evidence to suggest that LLMs are good\npragmatic speakers. When comparing its scoring with the RSA models using different meaning\nfunctions, the LLM aligns more with the rule-based MF, especially for logic-constructed utterances.\nIn our meaning function evaluation, the rule-based MF outperforms the prompt-based approach in\nfactual judgement tasks, highlighting the LLM's strength in structured reasoning.\nWhile these results highlight the LLM's pragmatic abilities in a controlled setting, their generalisability\nto everyday language remains uncertain. The structured nature of the reference games may not fully\nreflect the complexities of real-world communication. However, this research offers a framework for\nevaluating LLMs' pragmatic abilities and could be extended to more natural language use.\nFuture work should explore more diverse datasets to reflect a wider range of communication settings\nand natural language use. Testing on other LLMs, especially those with advanced pragmatic reasoning\nlike GPT models trained on large datasets, would provide deeper insights into handling pragmatic\ntasks. Further research could also compare LLM alignment with the RSA model when iterated multiple\ntimes, rather than a single interaction, and examine the effects of scaling parameters and cost functions\non alignment."}]}