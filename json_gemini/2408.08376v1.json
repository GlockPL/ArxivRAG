{"title": "Decoding the human brain tissue response to radiofrequency excitation using a biophysical-model-free deep MRI on a chip framework", "authors": ["Dinor Nagar", "Moritz Zaiss", "Or Perlman"], "abstract": "Magnetic resonance imaging (MRI) relies on radiofrequency (RF) excitation of proton spin.\nClinical diagnosis requires a comprehensive collation of biophysical data via multiple MRI\ncontrasts, acquired using a series of RF sequences that lead to lengthy examinations. Here,\nwe developed a vision transformer-based framework that captures the spatiotemporal mag-\nnetic signal evolution and decodes the brain tissue response to RF excitation, constituting\nan MRI on a chip. Following a per-subject rapid calibration scan (28.2 s), a wide variety of\nimage contrasts including fully quantitative molecular, water relaxation, and magnetic field\nmaps can be generated automatically. The method was validated across healthy subjects and\na cancer patient in two different imaging sites, and proved to be 94% faster than alternative\nprotocols. The deep MRI on a chip (DeepMonC) framework may reveal the molecular com-\nposition of the human brain tissue in a wide range of pathologies, while offering clinically\nattractive scan times.", "sections": [{"title": "Introduction", "content": "Magnetic resonance imaging (MRI) is among the most powerful diagnostic tools in present-day\nclinical healthcare\u00b9. One compelling advantage is its wide versatility, which allows a single imag-\ning modality to acquire a wide variety of biophysical information\u00b2. This is a consequence of the\nability to program MRI scans for emphasizing a particular tissue property of interest. Specifically,\na series of radiofrequency (RF) pulses are designed and applied to initiate a cascade of interactions\nwith the tissue proton spins. The particular waveform, duration, power, and frequency of each RF\npulse, as well as the characteristics of the entire RF train ensemble affect the resulting contrast,\nwhich can be customized to detect microstructure, water content, cellularity, blood flow, molecular\ncomposition, and even functional characteristics\u00b3.\nAs a single MRI pulse sequence is generally not enough to determine the tissue state and\nmake a diagnosis with sufficient certainty, standard clinical MRI exams involve the serial acqui-\nsition of multiple pulse sequences. For example, brain cancer MRI protocols typically comprise\nT1-weighted, T2-weighted, fluid-attenuated inversion recovery (FLAIR) and diffusion, and may\nalso require perfusion, and MR-spectroscopy imagings. While multi-sequence acquisition pro-\nvides rich information, it also requires excessively long examination times (e.g., 20-60 min) that\nincrease patient discomfort, healthcare costs, and waiting lines. Moreover, the image contrast de-\npends not only on the acquisition parameters, but also on the particular tissue characteristics, which\nmay be highly variable across subjects. For example, while a given protocol may be able to dif-\nferentiate multiple tumor components in one patient, it may be insufficient for another individual,\nand may require fine-tuning of the RF pulses (e.g., choosing a different flip angle, saturation pulse\npower, etc.). As radiological image analysis is commonly performed \"offline\u201d, namely, after the\nacquisition is completed and the subject has left, rescheduling a subject scan for re-imaging with\na modified acquisition protocol is either impractical or results in increased costs and prolonged\nwaiting lines7.\nIn recent years, the need to enhance the biochemical information portfolio provided by MRI\nhas prompted the search for new contrast options. Saturation transfer (ST) MRI represents one\nsuch promising options, due to the RF-tunable sensitivity for various molecular properties, such\nas mobile protein and peptide volume fractions, intracellular pH, and glutamate concentration.\nST-MRI has shown promise for a variety of clinical applications10, such as tumor detection and\ngrading11,12, early stroke characterization13,14, neurodegenerative disorder imaging15,16, and kid-\nney disease monitoring17,18. However, the integration of ST-MRI into clinical practice has been\nslow and limited, because of the relatively long scan times required. Moreover, as each ST target\ncompound is characterized by a distinct proton exchange rate, a separate pulse sequence must be\nacquired for each application of interest, thereby rendering multi-contrast ST imaging even less\npractical.\nQuantitative imaging of biophysical tissue properties offers improved reproducibility, sensi-\ntivity, and consistency across sites and scanners, compared to contrast-weighted imaging19. In this\ncontext, imaging techniques that combine biophysical (differential-equation-based) models with\nartificial intelligence (AI) have recently been suggested to be useful for accelerating water-pool\nrelaxometry20-24 or quantitative ST-MRI acquisition and reconstruction25\u201331. Unfortunately, the\ncomplexity of the multi-proton-pool in-vivo environment and the challenges in accurately mod-\neling the large number of free tissue parameters, limit the efficacy of this approach for molecu-\nlar MRI. This leads to substantial variability between the biophysical values reported by various\ngroups (each incorporating different model assumptions)32-34 or to increased acquisition times, as\nwater pool and magnetic field parameters may need to be separately estimated via additional pulse\nsequences, to reduce the model complexity 26,35.\nHere, we describe the development of a biophysical-model-free deep learning framework\n(Fig. 1), which can provide rich biological information in-vivo, while circumventing the need"}, {"title": "Results", "content": "for lengthy multi-pulse-sequence MRI acquisition (requires only a 28.2 second-long calibration\nscan). The framework is able to capture the spatiotemporal magnetic signal evolution dynamics\nin living humans and decode the brain tissue response to RF excitation, constituting a deep MRI\non a chip (DeepMonC). When employed on unseen subjects, pathology, and scanner models at a\ndifferent imaging site from where the training set was obtained, DeepMonC was able to emulate\nthe spin evolution dynamics accurately, and generate a variety of new image contrasts as well as\nfully quantitative molecular, water relaxation, and magnetic field maps."}, {"title": "DeepMonC Framework", "content": "The DeepMonC core module (Fig. 1a) was designed to capture the spatiotemporal dynamics of\nMRI signal propagation as a response to RF excitation, and enable the generation of on-demand\nimage contrast. The system includes a vision transformer36,37 with a dual-domain input, comprised\nof RF excitation information and real-world tissue response image counterparts. An extension\nmodule was also designed, which quantifies six biophysical tissue parameters across the entire 3D\nbrain, without the need for any additional input.\nThe core module inputs are a sequence of $m$=6 non-steady-state MRI calibration images and\nan RF excitation parameter tensor (Fig. 1a). The tensor includes two concatenated parts: the acqui-\nsition parameters used for obtaining the calibration images and the desired on-demand parameters\nfor the subsequent image output. Separate embeddings for the real-image-data and the physical"}, {"title": "Biophysical-model-free prediction of the tissue response to RF excitation", "content": "RF properties are then learned using a vision transformer and a fully connected layer, respectively.\nThe quantification module, involves a transfer learning strategy where the core module weights are\nplugged-in, the last layer is removed, and there is augmentation of two new convolutional layers.\nGround truth reference data are then used to instigate quantification-oriented learning (Fig. 1b).\nThe DeepMonc framework was trained using 3,118,692 image and acquisition parameter\npairs from 9 healthy human volunteers, scanned at a single imaging site (Tel Aviv University) on\na 3T MRI (Prisma, Siemens Healthineers) equipped with a 64-channel coil. The framework was\nthen tested using 30,324 image and acquisition parameter pairs obtained from 4 other subjects\nrepresenting three challenging datasets: (i) Two healthy subjects not used for training (scanned at\nthe same site). (ii) A brain cancer patient scanned at a different imaging site (Erlangen University\nHospital). (iii) A healthy volunteer scanned using different hardware and MRI model at a different\nimaging site (Erlangen University Hospital, Trio MRI with a 32 channel coil).\nThe core module was validated for generating on-demand molecular (semisolid MT and amide pro-\nton CEST-weighted) images. The full reference imaging protocol consisted of 30 pseudo-random\nRF excitations (Supporting Information Fig. 1)26. The first six images were used for per-subject\ncalibration, followed by DeepMonC predictions of the multi-contrast images associated with the\nnext six response images (Fig. 1a). A representative example of the DeepMonC output compared\nto the ground truth for each of the validation datasets is shown in Fig. 2 and whole-brain 3D re-"}, {"title": "Rapid quantification of biophysical tissue parameters", "content": "construction output is provided as Supporting Information Movies M1 (semisolid MT) and M2\n(amide). An excellent visual, perceptive, and pixelwise similarity was obtained between Deep-\nMonC output and ground truth. This is reflected by a structural similarity index measure (SSIM)\n> 0.96, peak signal-to-noise ratio (PSNR) > 36, and normalized mean-square error (NRMSE) <\n3% (Table 1).\nTo evaluate the ability to generate an up to 4-times longer output compared to the input,\nthe process was continued recursively, until the entire 30-long sequence was predicted based on\nthe first six calibration images (Supporting Information Movies M3 (semisolid MT) and M4\n(amide)). Although there were some errors in the last six images, the overall performance remained\nhigh, with a structural similarity index measure (SSIM) > 0.94, peak signal-to-noise ratio (PSNR)\n> 32, and normalized mean-square error (NRMSE) < 3.7% (Table 1). The inference times for\nreconstructing whole brain 6 or 24 unseen image contrasts were 7.674 s and 10.896 s, respectively,\nwhen using an Nvidia RTX 3060 GPU, and 9.495 s and 19.55 s, respectively, when using a desktop\nCPU (Intel I9-12900F).\nThe quantification module was trained to receive the exact same input as the core module, and\nthen produce six parameter maps: the semisolid MT proton volume fraction ($f_{ss}$) and exchange\nrate ($k_{ssw}$), water pool longitudinal ($T_1$) and transverse ($T_2$) relaxation times, and the static ($B_0$)\nand transmit ($B_1$) magnetic fields. The DeepMonC reconstructed paramater maps were visually,"}, {"title": "Discussion", "content": "perceptually, and quantitatively similar to the ground truth reference (Fig. 3-5 panels a,b and Sup-\nporting Information Figure S2). The reconstruction performance was highest for the test subject\nscanned by the same scanner used for training (SSIM = 0.919\u00b10.024; PSNR = 30.197\u00b11.808;\nNRMSE = 0.049\u00b10.008), followed by the cancer patient (unseen pathology at an unseen imaging\nsite: SSIM = 0.884\u00b10.024; PSNR = 26.349\u00b11.246; NRMSE = 0.059\u00b10.007), and the unseen\nsubject scanned using unseen hardware at an unseen imaging site (SSIM = 0.811\u00b10.044; PSNR =\n24.186\u00b11.523; NRMSE = 0.076\u00b10.011).\nThe magnetic field maps reconstructed by DeepMonc exhibited improved homogeneity com-\npared to their ground-truth counterparts (Fig. 3,4,5 panels a and b). This enabled successful ar-\ntifact removal from the semisolid MT proton volume fraction and exchange rate maps, which are\nknown to be sensitive to $B_0$ and $B_1$ inhomogeneity38-40 (white arrows in Fig. 3, and Fig. 5).\nTo analyze the contribution of the decoded tissue response information, captured by Deep-\nMonc core module, to the quantification task performance, a comparison with standard supervised\nlearning was performed. The same quantification architecture (Fig. 1b) was trained to receive the\nexact same inputs, and then output the same six quantitative biophysical parameter maps, but with-\nout employing the pre-trained DeepMonC weights (learnt by the core module, Fig. 1a). This stan-\ndard supervised learning routine yielded parameter maps with a markedly lower resemblance to the\nground truth (Fig. 3,4,5 panel c). The deterioration in output was accompanied by a statistically\nsignificant lower SSIM (0.805\u00b10.057,0.778\u00b10.062, 0.725\u00b10.066, for the unseen subject, pathol-\nogy, and hardware datasets, respectively, p<0.0001, n=68 image pairs) and PSNR (25.733\u00b11.473,\n23.546\u00b11.428, 22.614<1.342, for the three datasets, respectively, p<0.0001, n=68 image pairs),\nand a higher NRMSE (0.0842\u00b10.0125, 0.0843\u00b10.0128, 0.092\u00b10.012 for the three datasets, re-\nspectively, p<0.0001, n=68 image pairs, Fig. 3,4,5 panel d). The inference time required for\nreconstructing whole brain quantitative images was 6.751 s or 9.822 s when using an Nvidia RTX\n3060 GPU or a desktop CPU (Intel I9-12900F), respectively.\nThe past few decades have seen increased reliance on MRI for clinical diagnosis41. In parallel, this\nhas required the introduction of new contrast mechanisms and dedicated pulse sequences 10,42\u201347.\nWhile offering biological insights and improved diagnosis certainty, the integration of these se-\nquences into routine MRI examinations exacerbates the already lengthy overall scan times. Here,\nwe describe the development of a deep-learning-based framework that can rapidly decode the hu-\nman brain tissue response to RF excitation. The system generates a variety of on-demand image\ncontrasts in silico that faithfully recapitulate their physical in-vivo counterparts (hence, termed a\ndeep MRI on a chip).\nThe target contrasts requested from DeepMonC were associated with RF parameters extrap-\nolated beyond the range of the training parameters, thereby representing a highly challenging task\n(Supporting information Fig. 1). Nevertheless, an excellent agreement between the generated\nand ground-truth image-sets was obtained (Fig. 2 and Table 1). The dependence of DeepMonC\non the particular set of calibration images used and the desired output contrast was assessed on\n18 different input-output pairs (Supporting Information Figure S3). Despite some variability,\na satisfactory reconstruction was obtained in all cases (SSIM > 0.96, PSNR > 36, NRMSE >\n2%). Importantly, DeepMonC was able to overcome unknown initial conditions, as all calibration\nimage-set combinations but one (image indices 1-6, Supporting Information Figure S3) were\nacquired following an incomplete magnetization recovery.\nThe core module architecture was designed for image translation of m-to-m size (Fig. 1a,\nillustrated for m=6). Nevertheless, it can be recursively applied (by using the model's output as\nthe next input for generating another set of m images), and maintains an attractive performance, for\nup to m-to-3m translations (Supporting Information Movies M3 and M4). Although some errors\nwere visually observed when attempting m-to-4m translation (in the last m=6 images), additional\ntraining with longer acquisition protocols could further improve this performance.\nThe excellent on-demand contrast generation performance exhibited by DeepMonC (Table\n1) can be attributed to two key factors: (1) The introduction of explicit (and varied) acquisition\nparameter descriptors into the training procedure; this information is traditionally overlooked and\nhidden from MR-related neural networks48,49. (2) The incorporation of visual transformers as the\nlearning strategy. These enable the system to address the double sequential nature of the image\ndata obtained from both the 3D spatial domain and the temporal (spin-history) domain. Visual\ntransformers, with their effective attention mechanism, are not only capable of capturing long-\nrange data dependencies but can also understand global image context, alleviate noise, and adapt\nto various translational tasks37,50.\nContrast-weighted imaging is the prevalent acquisition mode in clinical MRI. However, it has\nbecome increasingly clear that quantitative extraction of biophysical tissue parameters may offer\nimproved sensitivity, specificity, and reproducibility51\u201353. By harnessing the decoded brain tissue\nresponse to RF excitation, the DeepMonC framework was further leveraged to simultaneously\nmap six quantitative parameters (Fig. 3-5), spanning three different biophysical realms, namely\nwater relaxation, semisolid macromolecule proton exchange, and magnetic field homogeneity. The\nresults provide an excellent agreement with the ground truth (Fig. 3-5d, Supporting Information\nFig. S2), as well as an inherent ability to mitigate artifacts (white arrows in Fig. 3 and Fig. 5).\nSpecifically, the $B_0$ and $B_1$ maps generated by DeepMonC exhibit better homogeneity than the\nreference ground truth. This thereby represents a practical explanation for the successful reduction\nof hardware/in-homogeneity related noises around the sinuses/eyes and at the air-tissue interfaces.\nImportantly, the rich whole-brain information provided by DeepMonc was reconstructed in\nonly 6.8 seconds, following a non-steady state rapid acquisition using a single pulse sequence of\n28.2 s. This represents a 94% acceleration compared to the state of the art ground-truth reference\n(acquired in 8.5 min, Fig. 1b). Interestingly, the quantification task results were even less sensitive\nto the particular pulse sequence used for acquiring the calibration images (Supporting Informa-\ntion Figure S4) than the on-demand contrast generation task (Supporting Information Figure\nS3).\nThe success of the quantification module is directly associated with the reliance on Deep-\nMonC's core pre-training, which generates a comprehensive understanding of the RF-to-tissue"}, {"title": "Methods", "content": "relations. This is supported by the statistically significant higher performance obtained by the\nquantification module compared to the vanilla use of DeepMonC (untrained) architecture (Fig. 3-5\npanels c,d, n=68 image slices, p<0.0001).\nThe generalization of DeepMonC predictions was assessed on three datasets, each represent-\ning a different challenge. Overall, there proved to be compelling evidence for generalization, with\na faithful representation of the the RF-to-tissue interface, with a satisfactory image reconstruction\nobtained in all cases. It should however be noted that, as expected, the parameter quantification\nof the unseen subject scanned at the same site and scanner used for training, yielded the best re-\nsults. The cancer patient scanned at a different image site yielded the next best performance (only\nhealthy volunteers were used for training), followed by the healthy subject scanned using a differ-\nent scanner model and hardware at a different imaging site (Fig. 3-5d, Supporting Information\nFig. S4). When assessing the on-demand contrast generation task performance, the differences\nbetween the various test-sets were much less discernible, with mostly subtle variations in the re-\nconstruction metrics (Table 1). In the future, additional training using subjects scanned on other\nscanner models and across various pathologies could further boost the framework performance.\nSaturation transfer (encompassing both CEST and semisolid MT) is the dominant biophys-\nical mechanism involved in the on-demand contrast generation task. This was chosen as a repre-\nsentative emerging imaging approach that is the focus of much interest from across the medical\ncommunity 10,54\u201359. Nevertheless, the same conceptual framework could potentially be applied for\ngenerating on-demand diffusion, perfusion, relaxation, susceptibility, and other contrast-weighted\nimages, given that a per-subject rapidly acquired data from the same general mechanism of interest\nis provided, alongside the matching acquisition parameters. Notably, a single pulse sequence may\nrepresent several biophysical properties, similarly to the way that ST-contrast weighted images are\naffected by the T1, T2, B0, and B\u2081. Furthermore, while this work was focused on brain imaging,\nwe expect that the same framework could be similarly utilized in other organ/tissues (after proper\ntraining). Finally, the ground-truth reference used for the quantification task was obtained via\nstandard water proton relaxometry, magnetic field-mapping, and semisolid MT MRF. However,\nthe same quantification module could seamlessly be trained using alternative reference modalities,\nsuch as 31P-imaging (for reconstructing intracellular pH maps)60, or even non-MR images (such\nas Ki-67 proliferation index histological images), thereby creating new cross-modality insights and\nopportunities.\nIn summary, we have developed and validated a computational framework that can learn the\nintricate mapping between the magnetic resonance RF irradiation domain and the subject-specific\nimage domain. The method is biophysical-model-free and thus, unbiased by pre-existing parameter\nrestrictions or assumptions. Given its ultra-fast on-demand contrast generation ability, we expect\nthis approach to play an important role in the efforts to accelerate clinical MRI.", "Human subjects": "Human subjects. Eleven healthy volunteers (five females/six males, with average age 25.5\u00b14.7)\nwere scanned at Tel Aviv University (TAU), using a 64-channel 3T MRI (Prisma, Siemens Health-\nineers). The research protocol was approved by the TAU Institutional Ethics Board (study no.\n0007572-2) and the Chaim Sheba Medical Center Ethics Committee (0621-23-SMC). Two addi-\ntional subjects were scanned at University Hospital Erlangen (FAU): a glioblastoma patient (World\nHealth Organization grade IV, IDH mutation, and methylation of MGMT (O(6)-methylguanine-\nDNA methyltransferase) promoter), scanned using the same scanner model described above, and a\nhealthy volunteer, imaged using a different 3T MRI model and coil system (Trio, Siemens Health-", "MRI acquisition": "MRI acquisition. Following scout image positioning and shimming, each subject was scanned\nusing five different pulse sequences, all implemented using the Pulseq prototyping framework61,62\nand the open-source Pulseq-CEST sequence standard63. Non-steady-state ST images of the amide\nand semisolid MT proton pools were acquired using two dedicated pulse sequences, as described\npreviously 26, 32. Each protocol employed a spin lock saturation train (13\u00d7100 ms, 50% duty-cycle),\nwhich varies the saturation pulse power between 0 and 4\u00b5T (detailed pattern available in Sup-\nporting Information Figure S1) to generate 30 contrast-weighted images. The saturation pulse\nfrequency offset was fixed at 3.5 ppm for amide imaging64,65 or varied between 6 and 14 ppm for\nsemisolid MT imaging26. The saturation block was fused with a 3D centric reordered EPI readout\nmodule66,67, which provided a 1.8 mm isotropic resolution across a whole-brain field of view. The\necho time was 11 ms and the flip angle was set to 15\u00b0. The same rapid readout module and hybrid\npulseq-CEST framework were used to acquire additional B0 and B\u2081 maps by using the WASABI\nmethod40, and water T\u2081 and T2 maps by using saturation recovery and multi-echo sequences, re-\nspectively. The total scan time per subject for all five protocols was 10.85 min (8.5 min for the\nquantitative reference set described in Fig. 1b).", "Data organization": "Data organization. The data from nine healthy subjects scanned at TAU was used for training and\nvalidation (hyper-parameter tuning) with a ~80%-20% split. Each training sample was composed\nof an m=6 image series, and an acquisition parameter tensor, which included the corresponding six\nsaturation pulse power and frequency offset values utilized, as well as the parameters associated\nwith the subsequent m-long image-series output (Fig. 1a and Supporting Information Figure\nS1). A 3D volume of maximum 144x144x144 voxel size was acquired for each subject. After\nall non-brain-containing slices were removed, 8-fold rotation-based data augmentation was per-\nformed. The core module was trained using 18 various combinations of acquisition parameter and\n6-to-6 image pairs (Supporting Information Figure S3). The training process was repeated for\nall brain orientation views (axial, sagittal, and coronal) and for both the amide- and semi-solid\nMT-weighted data. Overall, a total of 563,904 image series/acquisition parameter pairs (3,383,424\nsingle images) were used for core module development.\nThe quantification module implements a transfer learning strategy, which benefited from,\nand expanded upon, the trained core module. Therefore, a relatively small dataset (85,824 image\nseries and acquisition parameter pairs) was sufficient for its fine tuning.\nAll images were motion-corrected and registered using elastix68. Skull removal was per-\nformed using statistical parameter mapping (SPM)69 on a T\u2081 map. Quantitative reference semisolid\nMT-MRF maps were obtained using a fully connected neural network trained on simulated dictio-\nnaries, where all m=30 raw input measurements were taken as input. Pixelwise T1, T2, and Bo\nvalues were also incorporated as direct NN inputs, for improved reconstruction accuracy26. A de-\ntailed description of the ST-MRF reconstruction and quantification procedure has been published\npreviously 26, 32.", "Core module architecture": "Core module architecture. The core module 'label' images were derived from the physically acquired non-steady-state\namide or semisolid MT data. The quantification module treated the separately acquired B0, B1, T1,\nT2, $f_{ss}$, and $k_{ssw}$ maps as the training set image labels.\nThe test cohort was composed of three separate datasets: (1) Two healthy volunteers scanned\nat TAU (not used for training or validation). (2) A healthy volunteer scanned at FAU using a scanner\nmodel and hardware different to those used for training. (3) A brain cancer patient scanned at FAU\n(see more details in the Human Subjects section above).\nThe core module (Fig. 1a) was designed to generate 'on-demand' im-\nage contrast, according to a user defined acquisition parameter set. It receives a dual-domain input,\nrepresenting a per-subject (rapidly acquired) calibration image set and RF excitation information.\nThe calibration set was composed of serially acquired image data $x \\in \\mathbb{R}^{M \\times H \\times W}$, where M is the\ntemporal dimension (6 in our case), and H \u00d7 W are the spatial image dimensions. The RF infor-\nmation is represented by an acquisition parameter tensor $p \\in \\mathbb{R}^{2 \\times (2M)}$, composed of the saturation\npulse powers ($B_1$) and frequency offsets ($\u03c9_{rf}$), associated with the calibration and the 'on-demand'\nimage sets, respectively. The module output is a set of new contrast images $y \\in \\mathbb{R}^{M \\times H \\times W}$.\nEach calibration image was reshaped into 9 patches that were projected linearly and em-\nbedded into a tissue response representation. The acquisition parameter tensor was converted into\nRF excitation embedding, using a fully connected layer. The dual-domain embedding was then\nconcatenated into a single tensor and transferred into a transformer encoder36,37, with the follow-\ning hyper-parameters: embedding dimension = 768, MLP size = 3072, transformer layers = 3,", "Training Properties": "attention heads = 4.\nThe next step involved the sequential application of three convolution layer blocks. The first\ntwo blocks comprised 3x3 convolutions, batch normalization, ReLU activation function, and Max\nPooing. The third block contained an up-sample layer, a 3\u00d73 convolution layer, and a sigmoid\nactivation function.", "Quantification module architecture": "Quantification module architecture. The quantification module was designed to leverage the in-\ntricate mapping between the RF irradiation domain and the image domain, as extensively learned\nby the core module, and then utilize a transfer learning strategy in order to achieve quantitative\nmapping. Specifically, the same weights used for the on-demand contrast generation task served\nas the initial state for the transformer encoder in the quantification module. The architecture (Fig.\n1b) included several modifications: the last convolutional layer and sigmoid activation were re-\nplaced by a new 3\u00d73 convolutional layer, batch normalization, and ReLU activation. An additional\n(fourth) convolutional block was added, concluded by sigmoid activation. The quantification mod-\nule input was identical to that of the core module, while the target output was six parameter maps:\n$k_{ssw}$, $f_{ss}$, B0, B1, T1, T2.\nTraining Properties. For both modules, the loss function L was defined as a combination of the\nstructural similarity index measure (SSIM) and L1. The core module was trained using five RTX\n5000 GPUs in parallel, using a batch size = 64, and learning rate = 0.0004. The training (259\nepochs) took five days. The quantification module was trained using a single RTX 5000 GPU,\nwith a batch size = 16, and learning rate\n0.002. The training (348 epochs) took three days. All"}, {"title": "Statistical analysis", "content": "models were implemented in PyTorch.\nA two-tailed t-test was calculated using the open-source SciPy scientific com-\nputing library for Python70. Differences were considered significant at P < 0.05. In all box plots,\nthe central horizontal lines represent median values, box limits represent the upper (third) and\nlower (first) quartiles, the whiskers represent 1.5 \u00d7 the interquartile range above and below the\nupper and lower quartiles, respectively, and outliers are presented as circles."}, {"title": "Code availability", "content": "The DeepMonC framework will be made publicly open upon acceptance at\nhttps://github.com/momentum-laboratory/deepmonc. The full repository was uploaded as support-\ning information for the referees."}, {"title": "Data availability", "content": "The main data supporting the results of this study are available within the paper\nand Supplementary Information. The 3D human data cannot be shared due to subject confidential-\nity and privacy. Two sample 2D datasets will become available at https://github.com/momentum-\nlaboratory/deepmonc upon acceptance."}]}