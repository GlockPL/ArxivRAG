{"title": "Optimizing Overlap in Tree-Based Indexing Structures for Enhanced k-NN Search Efficiency", "authors": ["Ala-Eddine Benrazek", "Zineddin Kouahla", "Brahim Farou", "Hamid Seridi", "Ibtissem Kemouguette"], "abstract": "The proliferation of interconnected devices in the Internet of Things (IoT) has led to an exponential increase in data, commonly known as Big IoT Data. Efficient retrieval of this heterogeneous data demands a robust indexing mechanism for effective organization. However, a significant challenge remains: the overlap in data space partitions during index construction. This overlap increases node access during search and retrieval, resulting in higher resource consumption, performance bottlenecks, and impedes system scalability. To address this issue, we propose three innovative heuristics designed to quantify and strategically reduce data space partition overlap. The volume-based method (VBM) offers a detailed assessment by calculating the intersection volume between partitions, providing deeper insights into spatial relationships. The distance-based method (DBM) enhances efficiency by using the distance between partition centers and radii to evaluate overlap, offering a streamlined yet accurate approach. Finally, the object-based method (OBM) provides a practical solution by counting objects across multiple partitions, delivering an intuitive understanding of data space dynamics. Experimental results demonstrate the effectiveness of these methods in reducing search time, underscoring their potential to improve data space partitioning and enhance overall system performance.", "sections": [{"title": "1 Introduction", "content": "The Internet of Things (IoT) has rapidly emerged as one of the most transformative technologies of the 21st century. This paradigm is characterized by intelligent, self-configuring devices capable of communicating with one another through a global network infrastructure [1]. Since Kevin Ashton introduced the concept of IoT in 1999 [2], the number of connected devices has skyrocketed from 8 billion in 2018 to an estimated 75 billion by 2025 [3]. Alongside this growth, the volume of generated data has increased from 33 ZB in 2018 to a projected 175 ZB by 2025 [4], marking the dawn of the big IoT data era."}, {"title": "2 Theoretical Background", "content": "The metric space is a mathematical structure, introduced as a universal abstraction for data [13], excels by relying solely on a distance function that satisfies the triangle inequality, thereby eliminating the need for specific content or intrinsic structure [14, 15]. This versatility allows it to effectively index a wide variety of data types seamlessly.\nDefinition 1 (Metric Space). A metric space M is defined as a pair (S,d), where S denotes a domain of objects, and d : S \u00d7 S \u2192 R+ represents a distance function to quantify the \u201csimilarity\u201d between objects within S. This distance function typically satisfies four distinct properties:\n\u221a(x,y) \u2208 S, d(x,y) \u2265 0 (p1: non-negativity),\n\u221a(x,y) \u2208 S, d(x,y) = d(y, x) (p2: symmetry),\n\u221a(x,y) \u2208 S, d(x, y) = 0 \u21d4 x = y (p3: identity),\n\u221a(x, y, z) \u2208 S, d(x,y) + d(y, z) \u2265 d(x, z) (p4: triangle inequality).\nAccording to [16], the problem of indexing metric spaces can be formally stated as follows:\nDefinition 2 (Problem of Data Indexing). Let M = (S,d) be a metric space. Consider a subset X S, representing a set of objects, commonly referred to as the dataset, within which we perform searches. The problem of indexing metric spaces involves determining how to preprocess or structure the objects in X so that similarity queries can be answered efficiently.\nAs mentioned in Section 1, indexing techniques in this context typically involve partitioning a set X \u2286 S of objects in the metric space M = (S, d) into hierarchical regions centered around pivots using various partitioning techniques. One widely used method is Generalized Hyperplane (GH) partitioning, which is commonly employed in these indexes [7, 15]. In GH partitioning, two specific objects (P1,P2) \u2208 X are chosen arbitrarily or specifically. The dataset X is then divided into subsets X\u2081 and X2 based on Definition (3) and as illustrated in Figure 2a.\nDefinition 3 (Generalized Hyperplane Partitioning). Let M = (S,d) be a metric space, where X \u2286 S represents the set of objects to be indexed. Assume (P1,P2) \u2208 S are two pivot objects such that d(p1, p2) > 0. The generalized hyperplane\nH(S, d, P1, P2) = {0 \u2208 S : d(p1, 0) = d(p2,0)}\n(1)\npartitions the space into two regions as follows:\nX\u2081 = {o \u2208 X : d(o, p\u2081) \u2264 d(0,P2)}\n(2)\nX2 = {o \u2208 X : d(o, p\u2081) \u2265 d(0,P2)}\nIn metric space theory, a similarity query is defined by a query object q and a specified proximity criterion, typically expressed as a distance. The objective of such queries is to retrieve objects {01,..., On | Oi \u2208 X} that satisfy the proximity conditions relative to the query object q [17]. Among the various types of similarity queries, the k nearest neighbor (k-NN) query is the most popular and widely used [18, 19].\nThe k-NN query retrieves the k closest objects to a given query object q based on their distance in the metric space. If the dataset, contains fewer than k objects (i.e., |X| \u2264 k), the query returns the entire dataset (i.e., |A| = |X|) [16]. This query type"}, {"title": "Definition 4 (k-Nearest Neighbor Query)", "content": "Let M = (S,d) a metric space, q \u2208 S a query point, and k \u2208 N the required number of answers.\nk \u2013 NN(S, d, q, k) ={A | A \u2286 X \u2286 S, |A| = k ^\n(3)\n\u039b\u2200(0\u2208 A) \u2200(oj \u2208 {X \\ A}) : d(q, oi) \u2264 d(q, oj)}\nDuring the partitioning process, distant points can sometimes be assigned to partitions, leading to a significant increase in their radii (ri). Since these partitions are modeled as hyperballs (see Definition (5)), the inclusion of such outlier points can substantially expand the partition's coverage, potentially causing overlap in areas of the data space that do not reflect the actual data distribution. Theoretically, overlap between two partitions occurs when the farthest point within a partition (determined by its ri value) lies farther from the partition's reference point (pi) than the reference points are from each other. This scenario means that any query (q \u2208 S) within the overlapping space must be processed in each overlapping partition, even if some partitions contain no relevant data at the query location. As a result, distance calculations increase, search response times are prolonged, and computational costs rise, sometimes surpassing those of sequential scans, especially in cases of significant overlap. This ultimately impedes overall performance. For example, as illustrated in Figure 3, every partition intersected by the query sphere (P\u2081 and P2) must be searched, regardless of whether they contain relevant data points.\nDefinition 5 (Hyperball). Let M(S,d) a metric space, X \u2286 S represents the set of objects. Consider p \u2208 S as a pivot object, and r \u2208 R+ as the radius of coverage. The notation P(S,d,p,r) defines a closed ball that defined analogously the partitions, where:"}, {"title": "3 Related Works", "content": "In the field of data indexing, numerous techniques have been developed to enhance query efficiency and retrieval speed. These challenges are particularly critical in modern data management systems, especially in dynamic and complex environments like the IoT. Various indexing techniques have been proposed to address efficient data retrieval in such settings, with several noteworthy structures standing out. One such technique involves Voronoi Diagram-based structures, such as GNAT [20], which leverage Voronoi Diagrams to address node overlaps through static partitioning. While this method effectively prevents node overlaps, it struggles in dynamic environments, particularly when maintaining non-overlapping partitions during real-time updates. The EGNAT [21], a dynamic variant of GNAT and a generalization of the GH-Tree [22], introduces flexible node organization to manage updates following an initial bulk load. However, it continues to face challenges in maintaining the non-overlapping principle of Voronoi diagrams [23], and its search operations can be computationally intensive [24].\nIn contrast to static structures, the M-Tree [10] and its variations, such as the M+-Tree [25] and BM+-Tree [26], are known for their balanced structures and effective handling of dynamic updates. Nevertheless, these structures encounter challenges with overlapping nodes at the same level, especially in higher-dimensional spaces, where computational overhead can increase significantly. The Slim-Tree [11] builds on the M-Tree by introducing innovative algorithms like the Slim-Down technique to minimize node overlap. However, the effectiveness of overlap reduction varies depending on the"}, {"title": "4 Proposed Solution", "content": "Our proposed solution divided into three successive stages. (i) Preprocessing, (ii) overlapping estimation, and (iii) decision-making and indexing."}, {"title": "4.1 Preprocessing", "content": "To enhance indexing structures, we propose applying a clustering algorithm to the data before indexing. The goal is to organize similar data into compact groups, facilitating efficient indexing and retrieval processes. In our approach, DBSCAN (Density-Based Spatial Clustering of Applications with Noise) [36] is selected as the most suitable algorithm, supported by its proven effectiveness in previous studies [12, 37]. DBSCAN identifies clusters of arbitrary shapes based on density, dynamically adjusting without the need to pre-determine the number of clusters, unlike k-means. This method relies on two key parameters: (i) e, the radius of the spherical neighborhood around each object o \u2208 X (see Definition (6)), and (ii) MinPts, the minimum number of objects required for a neighborhood to be considered a valid cluster.\nDefinition 6 (e-Neighborhood of an Object). Let M = (S,d) be a metric space, and let X S represent the set of objects to be grouped. The e-neighborhood of an object o, denoted as Ne(o), is defined as:\nNe(o) = {q \u2208 X | d(o,q) \u2264 \u20ac}\n(5)\nDespite DBSCAN's ability to cluster without requiring centroids or radii for each group, it does not fully align with all our specific needs. Therefore, we will enhance the DBSCAN algorithm (Algorithm 1)\u00b9 to better suit our unique requirements in the subsequent stages of this research."}, {"title": "4.2 Overlapping Estimation", "content": "In this subsection, we introduce our innovative heuristics designed to minimize the impact of the overlap space problem discussed in Section 2. These heuristics aim to enhance metric space indexing without compromising query retrieval accuracy or the overall quality of the index."}, {"title": "4.2.1 Volume-Based Method (VBM)", "content": "The first proposed heuristic is the Volume-Based Method (VBM). This approach provides a detailed measure of overlap by calculating the intersection volume between partitions. As outlined in Section 2, partitions in a metric space are modelled as hyperballs (see Definition (5)). \u03a4\u03bf accurately assess the degree of partition overlap, we measure the overlapping volume between these corresponding hyperballs. The intersection volume between two hyperballs (see Figure 4) depends solely on the distance between their centers and the sizes of their radii, as outlined in Definition (7)."}, {"title": "Definition 7", "content": "Let M(S,d) a metric space. Consider two partitions P\u2081(S, d, p1, r1) and P2(S, d, p2, r2), representing hyperballs centered at p\u2081 and p2 with radii r\u2081 and r2, respectively. The overlap volume (V) and overlap volume rate (V\u2229) are defined as:\nV =\n{; min{VP1, VP2} \u03a32i=1V(ri,Pi)\n(6)\n0 if d(P1, P2) \u2265 r1 + r2\nif d(p1, p2) \u2264 |r\u0131 - r2|\notherwise\nVo =\nif d(P1,P2) \u2265 r1 + r2\n1 if d(P1,P2) \u2264 |r1 - r2|\n(7)\n\u03a32i=1V(ri,Pi)\notherwise\nWhere:\n0\nVP1+VP2"}, {"title": "4.2.2 Distance-Based Method (DBM)", "content": "The Distance-Based Method (DBM) is a simpler heuristic for evaluating overlap between partitions by calculating the distance between their centers and radii. Unlike the VBM, the DBM focuses exclusively on distance relationships, ensuring a computationally streamlined process without compromising accuracy. As illustrated in Figure 5, the overlap distance between partitions is represented by the combined height of the caps. Calculating the rate of this overlap, as defined in Definition (7), is crucial for accurately quantifying the degree of overlap. This quantification helps optimize partition boundaries and enhances the efficiency of the indexing structure.\nDefinition 10 (Overlapping Distance Rate). Let M(S,d) a metric space. Consider two partitions, P1(S,d, p1, r1) and P2(S,d, p2, r2), representing hyperballs centered at c\u2081 and c\u2082 with radii r\u2081 and r2, respectively. The distance rate of their intersection (D\u2229) is calculated as follows:\nDo =\n{; 1 \u03a32i=1 hi\nd(P1,P2)\n(13)\n0 if d(p1, p2) \u2265 r1 + r2\nif d(P1,P2) \u2264 r1 - 2\notherwise\nWhere hi represents the height of the cap of hyperball Pi as defined in Definition (9)."}, {"title": "4.2.3 Object-Based Method (OBM)", "content": "The Object-Based Method (OBM) is particularly practical, using the count of objects shared between partitions as a straightforward yet insightful indicator of overlap. This method allows for a more intuitive interpretation of data space dynamics by directly linking overlap to the actual distribution of data. Unlike previous methods that calculate overlapping space-potentially leading to inaccuracies in cases of large but empty overlaps\u2014OBM addresses this issue by focusing on the number of overlapping objects, providing a more accurate representation of data overlap. The mathematical foundation of this approach is illustrated in Definition (11).\nDefinition 11 (Overlapping Objects Rate). Let M = (S,d) be a metric space, and let X \u2286 S represent the set of partitioned objects. Consider two partitions, P1(S, d, p1, r1) and P2(S, d, p2, r2), representing hyperballs centered at p\u2081 and p2 with radii r\u2081 and r2, respectively. The set A of objects located in the overlapping area of the two hyperballs is defined by:\nA = P1(S, d, p1, r1) \u2229 P2(S, d, p2, r2)\nExplicitly, this is defined as:\nA = {o \u2208 X | d(0,p1) \u2264 r\u2081 and d(0,p2) \u2264 r2}\nThe rate An of objects in the overlapping area A is calculated as:\nAn =\n0 if d(P1,P2) \u2265 r1 + r2\n1 if d(p1, p2) \u2264 |r1 - r2|\n(14)\notherwise\n|A|\nP1+P2\nHere, |A| denotes the number of objects in set A.\nP1| and |P2| represent the number of objects in partitions P\u2081 and P2, respectively."}, {"title": "4.3 Decision-Making and Indexing", "content": "After estimating the overlap between partitions, the next step is to create the indexes. We establish two thresholds, \u00a7min and \u00a7max, to quantify the degree of overlap. Based on these thresholds, we categorize overlap into three levels: (1) Low overlap [0, \u00a7min], (2) Medium overlap [\u00a7min, \u00c9max], and (3) High overlap [\u00a7max, 1]. These classifications enable us to balance the trade-offs between index complexity and retrieval efficiency by applying tailored strategies to different levels of overlap, thereby ensuring optimal performance. By defining these thresholds, we can adapt our indexing approach to the specific characteristics of the data and the nature of the overlap.\nLow overlap [0, \u00a7min]: In cases of low overlap, we extract all objects from the partition with the smaller cap and merge them into the other partition. This results"}, {"title": "Definition 12 (BCCF-tree)", "content": "Let M(S,d) a metric space, and let X \u2286 S represent the set of indexed objects. The BCCF-tree structure is composed of two levels:\nInternal nodes N is a sextuple:\n(P1, P2, 1, 2, NL,NR) \u2208 X \u00d7 X \u00d7 R+ \u00d7 R+ \u00d7 N \u00d7 N\nwhere:\n(P1,P2) are two pivots, with d(p1, p2) > 0.\n(r1,r2) represent the distances to the farthest object in the sub-tree rooted at that node N with respect to p\u2081 and p2, respectively, i.e., ri = max{d(pi, o), do \u2208 N}.\n(NL,NR) are two sub-indexes (left and right child nodes).\nLeaf nodes, denoted as Ec, consist of a subset of the objects stored within a bucket.\nEcX\nwhere, |Ec|\u2264 \u0421\u0442\u0430\u0445, \u0421\u0442\u0430\u0445 = \u221an, and n = |X\n(15)\nIn this paper, we employ the BCCF-tree structure without modification, using the same construction algorithm detailed in [5, 17]. However, during the search phase, we introduce a new algorithm to manage the multiple indexes generated. This algorithm selects the appropriate index or indexes based on their proximity to the query object (see Algorithm 2). Once the closest indexes are identified, the kNN search algorithm from [5, 17] is applied in parallel across all selected indexes, ensuring efficient and accurate query processing while leveraging parallelism for enhanced performance."}, {"title": "5 Simulation and Results", "content": "The prototype was implemented in Python on a workstation equipped with an Intel\u00ae Core\u2122 i5-4200U CPU at 1.6 GHz, 4 GB of RAM, and running the Linux (Ubuntu) operating system.\nTwo datasets were used in the experiments. Table 1 provides an overview of the dataset characteristics, along with the parameters used in the DBSCAN algorithm (e and MinPts) for each dataset. Additionally, the table includes the fixed overlap thresholds (\u00a7min and \u00a7max) used in the experiments.\nDB1: This dataset consists of feature vectors from moving objects, obtained from an object tracking simulator in an IoVT environment using wireless cameras [40]."}, {"title": "5.2 Structure Evaluation", "content": "This section provides a detailed analysis of the quality of the indexes generated by the proposed heuristics-DBM, OBM, and VBM-applied to the Tracking and WARD datasets. The analysis primarily evaluates these heuristics based on their effectiveness in managing data space and optimizing tree structures. Key metrics assessed include the number of leaf nodes (or buckets) and the distribution of data within these nodes, the number and distribution of internal nodes at each index level, and the overall height of the tree."}, {"title": "5.2.1 Tracking dataset", "content": "In the Tracking dataset, both DBM and OBM performed well in maintaining balanced data distributions within the leaf nodes, with bucket sizes peaking around 250 in most trees and median values ranging from 140 to 215, as illustrated in Figures 6 and 7. However, a slight imbalance was observed in Tree 5, where bucket sizes were significantly smaller. VBM demonstrated its adaptability by effectively managing varying data volumes, with median bucket sizes ranging from 25 to 60 and peaks nearing 110, as seen in Figure 8, underscoring its effectiveness in optimizing tree structures for datasets of different sizes.\nThe distribution of nodes at each tree level, shown in Figure 9, further underscores the structural efficiency of these methods. DBM produced trees with a gradual increase in nodes per level, peaking between levels 6 and 8 with a median of 15-20 nodes (Figure 9a). The gradual slope suggests that the tree may remain balanced as the data increases. However, in Tree 5, the slope rapidly decreases, indicating an unbalanced tree, as previously observed. OBM exhibited a similar range of levels, from 6 to 12, but with an increased number of nodes ranging from 17 to 22, and peaks reaching up to 40 nodes. This structure suggests deeper trees with concentrated nodes, which may enhance data partitioning but could also increase traversal costs (Figure 9b). Unlike DBM, OBM's rapid slope suggests that the tree may become unbalanced as the data grows. In contrast, VBM, shown in Figure 9c, displayed a broader node distribution,"}, {"title": "5.3 Construction Cost", "content": "To evaluate the performance of the proposed structures compared to the BCCF-tree, we analyzed the construction costs in terms of the number of distances calculated and the number of comparisons made during the construction of indexes. The results, presented in Figure 20, provide a clear comparison of these metrics, with the container size fixed as specified in Table 1."}, {"title": "5.4 Search Efficiency", "content": "The similarity search algorithm's performance was evaluated against the BCCF-tree structure by analyzing the average number of distance calculations, comparisons performed, and execution times for fulfilling 100 kNN requests across different k values (k=5, 10, 15, 20, 50, 100). The results of these experiments are presented in Figure 21."}, {"title": "6 Conclusion", "content": "This paper has addressed the critical challenge of optimizing tree-based indexing structures in the context of massive IoT data. The overlap in data space partitions during index creation has been a significant obstacle, leading to inefficiencies in data retrieval and increased computational costs. To overcome this, we introduced three innovative heuristics: the Volume-Based Method (VBM), the Distance-Based Method (DBM), and the Object-Based Method (OBM), each designed to strategically minimize overlap and enhance the overall performance of the indexing process. Through extensive simulations and comparative analyses, we demonstrated that the VBM consistently outperforms both DBM and OBM, particularly in terms of structural balance, search efficiency, and construction costs. VBM's ability to maintain a well-distributed and adaptable tree structure while minimizing overlap was evident across both the Tracking and WARD datasets, making it the most effective method among the three."}]}