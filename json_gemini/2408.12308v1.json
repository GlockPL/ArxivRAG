{"title": "Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on Supervised Regression (Preprint)", "authors": ["Yansel Gonzalez Tejeda", "Helmut A. Mayer"], "abstract": "In this tutorial, we present a compact and holistic discussion of Deep Learning with a focus on Convolutional Neural Networks (CNNs) and supervised regression. While there are numerous books and articles on the individual topics we cover, comprehensive and detailed tutorials that address Deep Learning from a foundational yet rigorous and accessible perspective are rare. Most resources on CNNs are either too advanced, focusing on cutting-edge architectures, or too narrow, addressing only specific applications like image classification. This tutorial not only summarizes the most relevant concepts but also provides an in-depth exploration of each, offering a complete yet agile set of ideas. Moreover, we highlight the powerful synergy between learning theory, statistic, and machine learning, which together underpin the Deep Learning and CNN frameworks. We aim for this tutorial to serve as an optimal resource for students, professors, and anyone interested in understanding the foundations of Deep Learning.", "sections": [{"title": "1 Introduction", "content": "In this tutorial, we discuss the theoretical foundations of Artificial Neural Networks (ANN) in its variant with several intermediate layers, namely Deep Artificial Neural Networks. More specifically, we focus on a particular ANN known as Convolutional Neural Network (CNN).\nIn order to introduce the methods and artifacts we will use, we mostly follow the classical literature on Artificial Intelligence (AI) from an overall perspective [Rusell and Norvig, 2010] and Deep Learning (DL) [Goodfellow et al., 2016] and established notation. Since ANNs are learning systems, we first expose machine learning concepts. We then discuss the general structure of an ANN and the concept of depth in that context. Convolutional Neural Networks are named as such because convolutions are a key building block of their architecture. Then, we incorporate the convolution operation to complete our exposition of CNNs. Finally, we summarize the more relevant facts.\nThe theoretical deep learning framework is immense and has many ramifications to learning theory, probability and statistics, optimization, and even cognitive theory, to mention just a few examples. There are several books for every topic described in the subsections of this tutorial, not to mention articles in conferences and journals. Therefore, our presentation of deep learning here is highly selective and aims to expose the fundamental aspects needed to agile assimilate DL.\nIt is worth noting that, since DL is a relative young field, the terminology greatly varies. We often indicate when several terms are used to designate a concept, but the reader should be aware that this indication is by far not exhaustive. Notation is at the end of this manuscript."}, {"title": "2 Motivation", "content": "Intelligent agents need to learn in order to acquire knowledge from the environment. To perform all kinds of tasks, humans learn, among other things, from examples. In this context, learning means that, for a defined task, the agent must be able to generalize. That is, conduct the task successfully beyond the examples it has seen before. Most of the tasks the agent can realize may be cast as classification or regression. For both type of tasks, when the examples are accompanied by the actual outcome, we speak of supervised learning as a learning paradigm. From a pedagogical perspective, the agent is being supervised by receiving the ground truth corresponding to each learning example.\nBesides supervised learning, there are three other learning paradigms: unsupervised learning, reinforcement leaning and semi-supervised learning. When no actual outcome is available, or not provided, learning must occur in an unsupervised manner. If, alternatively, the actual outcome is noisy or a number of them is missing, learning must proceed semi-supervised. In contrast, if no ground truth is provided at all, but a series of reinforcements can be imposed, the agent can nonetheless learn by being positively or negatively (punished) rewarded. Here, we focus on supervised learning."}, {"title": "3 Machine Learning", "content": "From a general perspective the learning agent has access to a set of ordered examples\n\\(X \\stackrel{\\text{def}}{=} \\{x^{(1)},...,x^{(m)}\\}\\) (1)\nassumed to have been drawn from some unknown generating distribution \\(P_{data}\\). A learning example \\(x^{(i)}\\) (also a pattern) may be anything that can be represented in a computer, for example, a scalar, a text excerpt, an image, or a song. Without loss of generality, we will assume that every \\(x^{(i)}\\) is a vector \\(x^{(i)} \\in \\mathbb{R}^n\\), and its components the example features. For instance, one could represent an image as a vector by arranging its pixels to form a sequence. Similarly, a song could be vectorized by forming a sequence of its notes. We will clarify when we refer to a different representation.\nFurthermore, the examples are endowed with their ground truth (set of labels) \\(Y\\) so that for every example \\(x^{(i)}\\), there exists a label (possibly a vector) \\(y \\in Y\\). Using these examples the agent can learn to perform essentially two types of tasks: regression and classification (or both). Regression may be defined as estimating one or more values \\(y \\in \\mathbb{R}\\), for instance, human body dimensions like height or waist circumference. In a classification scenario, the agent is required to assign one or more of a finite set of categories to every input, for example, from an image of a person, designate which body parts are arms or legs.\nAdditionally, in single-task learning the agent must learn to predict a single quantity (binary or multi-class classification, univariate regression), while in multi-task learning the algorithm must learn to estimate several quantities, e.g., multivariate multiple regression."}, {"title": "3.1 Learning Theory", "content": "In general, we consider a hypothesis space \\(H\\), consisting of functions. For example, one could consider the space of linear functions. Within \\(H\\), the function\n\\(f(X) = Y\\) (2)\nmaps the examples (inputs) to their labels (outputs). The learning agent can be then expressed as a model\n\\(f(X) = \\hat{Y}\\) (3)\nthat approximates \\(f\\), where \\(\\hat{Y}\\) is the model estimate of the targets \\(Y\\).\nNow, the ultimate goal of the learning agent is to perform well beyond the exam- ples it has seen before, i.e., to exhibit good generalization. More concretely, learning must have low generalization error. To assess generalization, the stationarity assumption must be adopted to connect the observed examples to the hypothetical future examples that the agent will perceive. This assumption postulates that the examples are sampled from a probability distribution \\(P_{data}\\) that remains stationary over time. Additionally, it is assumed that a) each example \\(x^{(i)}\\) is independent of the previous examples and b) that all examples have identical prior probability distribution, abbreviated i.i.d..\nWith this in hand, we can confidently consider the observed examples as a training set \\(\\{x_{train}, Y_{train}\\}\\) (also training data) and the future examples as a test set \\(\\{x_{test}, Y_{test}\\}\\), where generalization will be assessed. As the data generating distribution \\(P_{data}\\) is assumed to be unknown (we will delve into this in subsection 3.3), the agent has only access to the training set. One intuitive strategy for the learning agent to estimate generalization error is to minimize the training error computed on the train set, also termed empirical error or empirical risk. Thus, this strategy is called Empirical Risk Minimization (ERM). Similarly, the test error is calculated on the test set. Both training and test errors are defined in a general form as the sum of incorrect estimated targets by the model (in Subsec. 3.4.1 we restate this definition), for example, in the case of \\(m\\) training examples, the training error \\(E_{train}\\) is\n\\(E_{train} = \\sum_{i=1}^{m} \\mathcal{I}(f^*(x^{(i)}) \\neq Y_i).\\) (4)\nNote that given a specific model \\(f^*\\), and based on the i.i.d. assumption the expected training error equals the expected test error. In practice, the previous is greater than or equal to the latter. Here, the model may exhibit two important flaws: underfitting or overfitting. A model that underfits does not achieve a small training error (it is said that it can not capture the underlying structure of the data). Overfitting is the contrary of underfitting, it occurs when the model \u201cmemorizes\u201d the training data, \u201cperhaps like the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion\u201d [Shalev-Shwartz and Ben-David, 2014]. When evaluated, a model that underfits yields a high difference between the training and the test error."}, {"title": "3.2 Model Evaluation", "content": "Assessing generalization by randomly splitting the available data in a training set and a test set is called holdout cross-validation because the test set is kept strictly separate from the training set and used only once to report the algorithm results. However, this model evaluation technique has two important disadvantages:\n\u2022 It does not use all the data at hand for training, a problem that is specially relevant for small datasets.\n\u2022 In practice, there can be the case where the i.i.d. assumption does not hold. Therefore, the assessment is highly sensitive to the training/test split.\nA remedy to these problems is the k-fold cross-validation evaluation method (cf. [Hastie et al., 2009] p. 241 Cross Validation). It splits the available data into \\(k\\) nonoverlapping subsets of equal size. Recall the dataset has \\(m\\) examples. First, learning is performed \\(k\\) times with \\(k \u2212 1\\) subsets. Second, in every iteration, the test error is calculated on the subset that was not used for training. Finally, the model performance is reported as the average of the \\(k\\) test errors. The cost of using this method is the computational overload, since training and testing errors must be computed \\(k\\) times."}, {"title": "3.3 Statistics and Probability Theory", "content": "To further describe machine learning we borrow statistical concepts. In order to be con- sistent with the established literature[Goodfellow et al., 2016], we use in this subsection \\( \\theta \\) and \\( \\hat{\\theta} \\) to to denote a quantity and its estimator.\nThe agent must learn to estimate the wanted quantity \\( \\theta \\), say. From a statistical perspective the agent performs a point estimate \\( \\hat{\\theta} \\). In this view the data points are the i.i.d. learning examples \\( \\{x^{(1)},...,x^{(m)}\\} \\). A point estimator is a function of the data such as \\( \\hat{\\theta}_{m} = g(\\{x^{(1)},...,x^{(m)}\\}) \\), with g(x) being a very general suitable function. \nNote that we can connect the quantity \\( \\theta \\) and its estimator \\( \\hat{\\theta} \\) to the functions f and \\( \\hat{f} \\) in Equations 2 and 3. Given the hypothesis space H that embodies possible input-output relations, the function \\( \\hat{f} \\) that approximates f can be treated as a point estimator in function space.\nImportant properties of estimators are bias, variance and standard error. The estima- tor bias is a measure of how much the real and the estimated value differ. It is defined as bias(\\( \\hat{\\theta}_{m} \\)) = E(\\(\\hat{\\theta}_{m}\\)) \u2212 \\( \\theta \\). Here the expectation E is taken over the data points (examples). An unbiased estimator has bias(\\( \\hat{\\theta}_{m} \\)) = 0, this implies that \\( \\hat{\\theta}_{m} \\) = \\( \\theta \\). An asymptotically unbiased estimator is when limm\u2192\u221e bias(\\( \\hat{\\theta}_{m} \\)) = 0, which implies that limm\u2192\u221e \\( \\hat{\\theta}_{m} \\) = \\( \\theta \\). An example of unbiased estimator is the mean estimator (mean of the training examples) of the Bernoulli distribution with mean \\( \\theta \\).\nAs we will soon see (where?), the variance Var(\\( \\theta \\)) = and Standard Error SE(\\( \\theta \\)) of the estimator are useful to compare different experiments. A good estimator has low bias and low variance.\nLet us now discuss two important estimators: the Maximum Likelihood and the Max- imum a Posteriori estimators."}, {"title": "3.3.1 Maximum Likelihood Estimation", "content": "To guide the search for a good estimator \\( \\hat{\\theta} \\), the frequentist approach is usually adopted. The wanted quantity \\( \\theta \\), possibly multidimensional, is seen as fixed but un- known, and the observed data is random. The parameters \\( \\theta \\) govern the data generating distribution \\(P_{data}(x)\\) from which the observed i.i.d. data \\(\\{x^{(1)},...,x^{(m)}\\}\\) arose.\nThen, a parametric model for the observed data is presumed, i.e., a probability dis- tribution \\(P_{model}(x; \\theta)\\). For example, if the observed data is presumed to have a normal distribution, then the parameters are \\( \\theta \\) = \\{\u03bc, \u03c3\\}, and the model\n\\(P_{model}(x; \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\) . Here candidates parameters \\( \\theta \\) must be considered, and ideally, \\(P_{model}(x; \\theta) \\approx P_{data}(x)\\). Next, a likelihood function \\(L\\) can be defined as the mapping between the data \\(x\\) and a given \\( \\theta \\), to a real number estimating the true prob- ability \\(P_{data}(x)\\). Since the observations are assumed to be independent,\n\\(L(x; \\theta) = \\prod_{i=1}^{m} P_{model}(x^{(i)}, \\theta).\\) (5)\nThe Maximum Likelihood (ML) estimator is the estimator \\( \\hat{\\theta} \\) that, among all candidates, chooses the parameters that make the observed data most probable.\n\\(\\hat{\\theta}_{ML} = arg \\mathop{max}\\_{\\theta} L(x; \\theta)\\) (6)\n\\(= arg \\mathop{max}\\_{\\theta} \\prod_{i=1}^{m} P_{model}(x^{(i)}, \\theta)\\) (7)\nTaking the logarithm of the right side in 7 facilitates the probabilities computation and does not change the maximum location. This leads to the Log-Likelihood\n\\(\\hat{\\theta}_{ML} = arg \\mathop{max}\\_{\\theta} \\sum_{i=1}^{m} log P_{model} (x^{(i)}, \\theta)\\) (8)\nAn insightful connection to information theory can be established by transforming Eq. 8. Firstly, dividing the right term by the constant \\(m\\) does not shift the maximum in the left term, i.e.,\n\\(\\hat{\\theta}_{ML} = arg \\mathop{max}\\_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} log P_{model}(x^{(i)}, \\theta).\\) (9)\nSecondly, by definition, the empirical mean \\( \\frac{1}{m}\\sum_{i=1}^{m} log P_{model} (x^{(i)}, \\theta)\\) of the assumed model distribution \\(P_{model}\\) equals the expectation given its empirical distribution \\(E_{x\\sim p_{data}}\\) defined by the training set \\(\\{p_{data}(x)\\}\\). A discussion of the empirical distribution \\(p_{data}(x)\\) is out of the scope, but it suffices to say that it is a method to approximate the real underlying distribution of the training set (not to be confused with the data- generating distribution \\(P_{data}\\)). Finally, Eq. 9 can be written as\n\\(\\hat{\\theta}_{ML} = arg \\mathop{max}\\_{\\theta} E_{x\\sim p_{data}} log P_{model}(x, \\theta).\\) (10)\nInformation theory allows to evaluate the degree of dissimilarity between the empirical distribution of the training data \\(P_{data}(x)\\) and the assumed model distribution \\(p_{model}(x)\\) using the Kullback-Leibler divergence \\(D_{KL}\\) (also called relative entropy) as\n\\(D_{KL}(p_{data}||p_{model}) = E_{x\\sim P_{data}} [log p_{data}(x) - log p_{model}(X)].\\) (11)"}, {"title": "3.3.2 Maximum a Posteriori Estimation", "content": "Here the expectation E is taken over the training set because Eq.11 quantifies the ad- ditional uncertainty arising from using the assumed model \\(P_{model}\\) to predict the training set \\(P_{data}\\).\nBecause we want the divergence between these two distribution to be as small as possible, it makes sense to minimize \\(D_{KL}\\). Note that the term log \\(p_{data}(x)\\) in Eq. 11 can not be influenced by optimization, since it has been completely determined by the data generating distribution \\(P_{data}\\). Denote the minimum \\(D_{KL}\\) AS \\(O_{DKL}\\), then\n\\(O_{DKL} = arg \\mathop{min}\\_{\\theta} -E_{x\\sim p_{data}} [log P_{model}(x, \\theta)]\\) (12)\nThe expression inside the arg min is the cross-entropy of data and \\(P_{model}\\). Compar- ing this with Eq. 10 it can be arrived to the conclusion that maximizing the likelihood coincides exactly with minimizing the cross-entropy between the distributions.\nIn contrast to the frequestist approach, the Bayesian approach considers \\( \\theta \\) to be a random variable and the dataset \\(x\\) to be fixed and directly observed. A prior proba- bility distribution p(\\( \\theta \\)) must be defined to express the degree of uncertainty of the state of knowledge before observing the data. In the absence of more information, a Gaussian distribution is commonly used. The Gaussian distribution is \"the least surprising and least informative assumption to make\" [McElreath, 2020], i.e., it is the distribution with maximum entropy of all. Therefore, the Gaussian is an appropriate starting prior when conducting Bayesian inference.\nAfter observing the data, Bayes' rule can be applied to find the posterior distribu- tion of the parameters given the data p(\\( \\theta \\)|x) = \\( \\frac{p(x|\\theta)p(\\theta)}{p(x)} \\), where p(x|\\( \\theta \\)) is the likelihood and p(x) the probability of the data.\nThen, the Maximum a Posteriori (MAP) estimator selects the point of maximal posterior probability:\n\\(\\hat{\\theta}_{MAP} = arg \\mathop{max}\\_{\\theta} p(\\theta|x) = arg \\mathop{max}\\_{\\theta} log p(x|\\theta) + log p(\\theta).\\) (13)"}, {"title": "3.4 Optimization and Regularization", "content": ""}, {"title": "3.4.1 Loss Function", "content": "The training error as defined by Eq. 4 is an objective function (also criterion) that needs to be minimized. Utility theory regards this objective as a loss function \\(J(x) : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) because the agent incurs in a lost of expected utility when estimating the wrong output with respect to the expected utility when estimating the correct value. To establish the difference of these values, a determined distance function must be chosen. For general (multivariate, multi-target) regression tasks the p-norm is preferred. The p-norm is calculated on the vector of the corresponding train or test error \\(||\\hat{y_i} - Y_i||_p = (\\sum |\\hat{y_i} - Y_i|^P)^{\\frac{1}{p}}\\) , which for \\(p = 2\\) reduces to the Euclidean norm. Since the Mean Squared Error (MSE) \\(\\frac{1}{2}||\\hat{y_i} - Y_i||^2\\) can be faster computed, it is often used for optimiza- tion. We will use this loss function (also cost function) when training our networks with \\(m\\) examples:\n\\(J(y) = \\frac{1}{m}\\sum_{i=1}^{m} (Y_i - \\hat{Y}_i)^2\\) (14)\n\\(= \\frac{1}{m} \\sum_{i=1}^{m} [\\sum_{j=i}^{n} (Y_j - \\hat{Y_j})^2].\\) (15)\nSince we regard every target \\(y_i\\) as a vector, the inner sum runs over its components \\(\\hat{y}_i\\) and \\(Y_j\\) is its length. The rather arbitrary scaling factor 2 in the denominator is a mathematical convenience for the discussion in 4.2.\nFor classification tasks the cross-entropy loss is commonly adopted."}, {"title": "3.4.2 Gradient-Based Learning", "content": "As we will see in Sec. 4, in the context of Deep Learning, the parameters \\( \\theta \\) discussed in Subsec. 3.3 are named weights \\(w\\). Without loss of generalization, we will consider the parameters to be a collection of weights. We do not lose generalization because the discussion here does not depend on the specific form of the cost function.\nTherefore, parameterizing the estimator function by the weights, minimization of the cost function can be written as\n\\(J^*(w; x, y) = arg \\mathop{min}\\_{w} \\frac{1}{m} \\sum_{i=1}^{m} (f(x^{(i)}, w) - Y_i)^2\\) (16)\nThe function \\(f(x^{(i)}, w)\\) is, in a broader sense, a nonlinear model, thus the minimum of the loss function has no closed form and numeric optimization must be employed. Specifically, \\(J\\) is iteratively decreased in the orientation of the negative gradient, until a local or global minimum is reached, or the optimization converges. This optimization method (also optimizer) is termed gradient descent. Because optimizing in the context of machine learning may be interpreted as learning, a learning rate \\( \\epsilon \\) is introduced to influence the step size at each iteration. The gradient descent optimizer changes weights using the gradient \\(\\nabla_{w}\\) of \\(J(w)\\) according to the weight update rule\n\\(w \\leftarrow w - \\epsilon \\nabla_{w}J(w)\\) (17)\nWe postpone the discussion on Stochastic Gradients Descent (SGD) to insert it in the context of training neural networks (Subsec. 4.2)."}, {"title": "3.5 Regularization", "content": "The essential problem in machine learning is to design algorithms that perform well both in training and test data. However, because the true data-generating process is unknown, merely decreasing the training error does not guarantee a small generalization error. Even worst, extremely small training errors may be a symptom of overfitting.\nRegularization intends to improve generalization by combating overfitting. It fo- cuses on reducing test error using constraints and penalties on the model family being trained, which can be interpreted as pursuing a more regular function.\nOne such an important penalty may be incorporated into the objective function. More specifically, the models parameters may be restricted by a function \\(\\Omega(w)\\) to obtain the modified cost function \\(J(w)\\) with two terms as\n\\(J(w) = arg \\mathop{min}\\_{w} \\frac{1}{m} \\sum_{i=1}^{m} (f(x^{(t)}, w) - Y_i)^2 + \\alpha \\Omega(w).\\) (18)\nThe second term's coefficient \\(\\alpha \\in [0, \\infty)\\) ponders the penalty's influence relative to \\(J^*\\). The specific form of \\(\\Omega(w)\\) (also regularizer) is usually a \\(L^1 = ||w||_1\\) or \\(L^2 = ||w||_2\\) norm penalty. Choosing one of these norm penalties expresses a preference for a determined function class. For example, the \\(L^1\\) penalty enforces sparsity on the weights, i.e., pushes the weights towards zero, while the effect of \\(L^2\\) norm causes the weights of less influential features to decay away. Because of this the \\(L^2\\) penalty norm is called weight decay. Weight decay has lost popularity in favor of other regularization techniques (Subsec. 5.5). It worth saying, however, that weight decay is an uncomplicated effective method for enhancing generalization [Krogh and Hertz, 1991]. Modifying Eq. 16 to include the penalty with \\( \\alpha = 1 \\), the cost function becomes\n\\(J(w) = arg \\mathop{min}\\_{w} \\frac{1}{m} \\sum_{i=1}^{m} (f(x^{(i)}, w) - Y_i)^2 + \\frac{\\alpha}{2} w^Tw.\\) (19)\nCorrespondingly, after substituting and reorganizing terms, the weight update rule in Eq. 17 results in\n\\(w \\leftarrow (1 - \\epsilon \\alpha)w - \\epsilon \\nabla_{w}J(w).\\) (20)"}, {"title": "4 Deep Forward Artificial Neural Networks", "content": "Deep Forward Artificial Neural Networks (DFANN) is a model conveying several impor- tant interlaced concepts. We start with its building blocks and then connect them to form the network. Additionally, to describe the parts of an ANN, the term architecture is used."}, {"title": "4.1 Artificial Neural Networks", "content": "Artificial Neural Networks (ANNs) is a computational model originally inspired by the functioning of the human brain [McCulloch and Pitts, 1943, Hebb, 1949, Rosenblatt, 1957]. The model exploits the structure of the brain, focusing on neural cells (neurons) and its interactions. These neurons have three functionally distinct parts: dendrites, soma and axon. The dendrites gather information from other neurons and transmit it to the soma. Then, the soma performs a relevant nonlinear operation. If exceeding a determined threshold, the result causes the neuron to emit an ouput signal. Following, the output signal is delivered by the axon to others neurons."}, {"title": "4.1.1 Artificial Neurons", "content": "Similarly, the elementary processing unit of ANNs is the artificial neuron. The neurons receive a number of inputs with associated intensity (also strength) and combine these inputs in a nonlinear manner to produce a determined output that is transmitted to other neurons. Depending on the form of the non-linearity, it can be said that the neuron 'fires' if a determined threshold is reached.\nEvery input to the neuron has an associated scalar called weight \\(w\\). The non-linearity, also called activation function is denoted by \\(g(.)\\). The neuron first calculates an affine transformation of its inputs \\(z = \\sum_{i=1}^{n} w_i.x_i + c\\). The intercept (also bias) \\(c\\) may be combined with a dummy input \\(X_{dummy} = 1\\) to simplify notation such as the affine transformation can be written in matrix form \\(\\sum_{i=0}^{n} w_i.x_i = w\\cdot x\\). One of the reason ANNs where invented is to overcome the limitations of linear models. Since a linear combination of linear units is also linear, a non-linearity is needed to increase the capacity of the model. Accordingly, the neuron applies the activation function to compute the activation \\(a = g(w\\cdot x)\\), which is transmitted to other neurons.\nReferring to activation functions, [Goodfellow et al., 2016] states categorically \"In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU\u201d introduced by [Jarrett et al., 2009]:\n\\(g(z) = max(0, z).\\) (21)\nIn order to enable learning with gradient descent (Eq. 17) as described in Subsec. 3.4.2, is desirable the activation function to be continuous differentiable. Although the ReLU is not differentiable at \\(z = 0\\), in practice, this does not constitute a problem. Practitioners assume a \u2018derivative default value' \\(f'(0) = 0, f'(0) = 1\\) or values in between.\nOther activation functions are the logistic sigmoid \\(\\sigma(z) = 1/(1+exp(\u2212z))\\) and the softplus function \\(\\zeta(z) = log(1+exp(z))\\). As a side note, a neuron must not necessarily compute a non-linearity, and in that case, it is called a linear unit.\nThe perceptron is an artificial neuron introduced by [Rosenblatt, 1957], who demon- strated that only one neuron suffices to implement a binary classifier. The perceptron had originally a hard threshold activation function, but it can be generalized to use any of the above-mentioned activation function. For example, if the perceptron uses a logis- tic sigmoid activation function, then the output can be interpreted as the probability of some event happening, e.g., logistic regression."}, {"title": "4.1.2 Deep Feedforward Networks", "content": "Connecting the artificial neurons results in an artificial neural network with at least two distinguishable layers: the input layer and the output layer. The input layer is composed by the units that directly accept the features (or their preprocessed represen- tation) of the learning examples. The output layer contains the units that produce the answer to the learning task.\nA single-layer neural network (sometimes called single-layer perceptron network, or simply perceptron network) (SLNN) is the most basic structure that can be assembled by directly connecting any number of input units to any number of output units. Note that, by convention, the input layer is not counted. Despite this type of network being far from useless, they are limited in their expressive power. Perceptron networks are unable to represent a decision boundary for non-linearly separable problems(cite?). A problem is linearly separable if the learning examples can be separated in two nonempty sets, such as every set can be assigned exclusively to one of the two half spaces defined by a hyperplane in its ambient space. This is important because, in general, practical problems in relevant areas like computer vision and natural language processing are not linearly separable.\nBeyond the SLNN, networks can be extremely complex. In order to make progress, one important restriction that may be imposed is to model the network as a directed acyclic graph (DAG) with no shortcuts. The neurons are the nodes, the connections the directed edges (or links), and the weights the connection intensity. Because a DAG does not contain cycles, the information in the ANN is said to flow in one direction. For that reason, this type of ANN that does not contain feedback links, are called feedforward networks or multi-layer perceptron (MLP). If the network does have feedback links then it is called a recurrent neural network (RNN). In this tutorial we focus on feedforward networks.\nIn equation 3 (Sec 3), we enunciated that the goal of a learning agent \\( \\hat{f}(x) \\) is to ap- proximate some function \\(f(x)\\), and from a statistical perspective, the agent is a function estimator of (Subsec. 3.3). Motivated by the idea of compositionality, it can be as- sumed that this learning agent \\( \\hat{f}(x) \\) is precisely the composition of a number of different functions\n\\(f(x) = f_n(f_{n-1}(... f(x)))\\) (22)\nwhere \\(f^{(0)}\\) and \\(f^{(n)}\\) are the input and the output layer, respectively.\nFurther, starting from the input layer and ending at the output layer, every function \\(f^{(1)}, l \\in i,\u2026\u2026, n\\) can be associated with a collection of DAG units receiving inputs only from the units in preceding collections. These functions and the units group they define, are called hidden layers and denoted by h(\u00b7).\nHidden layers are named as such because of their relation to the training examples. The training examples determine the form and behavior of the input and the output layer. Indeed, the input layer processes the training examples and the output layer must contribute to minimize the training loss. However, the training examples do not rule the intermediary layer computation scheme. Instead, the algorithm must learn how to adapt those layers to achieve its goal.\nFinally, the length of the entire function chain \\(f^{(1)}, ..., f^{(n)}\\) is the depth l of the network (\\(f^{(0)}\\) is the first layer but is not counted for depth), which, combined with the concepts explained before, confers the name to the computational model: deep feedforward networks. Likewise, the width of a layer is the number of units it contains, and the network's width is sometimes defined as the \"maximal number of nodes in a layer\" [Lu et al., 2017]. Additionally, the input layer is also called the first layer and the next layers are the second, the third and so forth.\nThe architecture of the network designates the network's depth and width, as well as the layers and their connections to each other. Depending on how the units are connected, different layer types can be assembled. Note that because shortcuts are not allowed, there can not be intra-layer unit connections. The fully-connected layer \\(h^{(1)}\\) (also dense layer, in opposition to a shallow layer) is the most basic, which all of its units connect to all units of the preceding layer \\(h^{(1-1)}\\) though a weight matrix W, i.e.,\n\\(h^{(t)} = g(W^lh^{(l-1)} + b^l)\\) (23)\n\\(h^{(t)} = g(W^lh^{(l-1)}).\\) (24)\nThe weight matrix \\(W^l \\in \\mathbb{R}^{m\\times n}\\) and the bias vector \\(b \\in \\mathbb{R}^{m}\\), where \\(m\\) and \\(n\\) are the width of the current and preceding layer, respectively. Alternatively, the notation may be simplified by incorporating the bias vector into the matrix with a dummy input (Eq. 24).\nIn Sec. 4.2.3 we will further discuss the notation and in Subsec. 5.1 will see other layer types.\nOne of the most significant results in feedforward networks research are the universal approximation theorems. The exact form of these theorems is positively technical, but at their core they demonstrate that feedforward networks can approximate any continuous function to any desirable level of accuracy. There are basically three theorem variants: arbitrary width and bounded depth, i.e., a universal approximator with only one hidden layer containing a sufficient number of neurons [Cybenko, 1989], bounded width and arbitrary depth [Gripenberg, 2003], i.e., a sufficient number of layers with a bounded amount of neurons, and remarkably, bounded depth and width [Maiorov and Pinkus, 1999]."}, {"title": "4.2 Training", "content": "Having described the structure of an artificial neural network, and how they learn by means of optimization, we now need to perform the actual training. The learning exam- ples are entered into the network through the input layer and the network weights need to be updated using gradient descent to minimize the loss."}, {"title": "4.2.1 Stochastic Gradient Descent", "content": "As we already discussed in Subsec. 3.4.2, gradient descent enables optimization of gen- eral loss functions. Specifically, the model weights are updated using the gradient of the loss function \\(\\nabla_{w}J(w)\\), which is sometimes called pure, deterministic or batch gradient descent. The problem is that the computation of this gradient requires an error summation over the entire dataset \\(\\nabla_{w}J(w) = \\sum_{i=1}^{m}(\\hat{y}_i \u2013 Y_i)x_i\\). In practice"}]}