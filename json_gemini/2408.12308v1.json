{"title": "Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on Supervised Regression (Preprint)", "authors": ["Yansel Gonzalez Tejeda", "Helmut A. Mayer"], "abstract": "In this tutorial, we present a compact and holistic discussion of Deep Learning with a focus on Convolutional Neural Networks (CNNs) and supervised regression. While there are numerous books and articles on the individual topics we cover, comprehensive and detailed tutorials that address Deep Learning from a foundational yet rigorous and accessible perspective are rare. Most resources on CNNs are either too advanced, focusing on cutting-edge architectures, or too narrow, addressing only specific applications like image classification. This tutorial not only summarizes the most relevant concepts but also provides an in-depth exploration of each, offering a complete yet agile set of ideas. Moreover, we highlight the powerful synergy between learning theory, statistic, and machine learning, which together underpin the Deep Learning and CNN frameworks. We aim for this tutorial to serve as an optimal resource for students, professors, and anyone interested in understanding the foundations of Deep Learning. Upon acceptance we will provide an accompanying repository under https://github.com/neoglez/deep-learning-tutorial", "sections": [{"title": "1 Introduction", "content": "In this tutorial, we discuss the theoretical foundations of Artificial Neural Networks (ANN) in its variant with several intermediate layers, namely Deep Artificial Neural Networks. More specifically, we focus on a particular ANN known as Convolutional Neural Network (CNN).\nIn order to introduce the methods and artifacts we will use, we mostly follow the classical literature on Artificial Intelligence (AI) from an overall perspective [Rusell and Norvig, 2010] and Deep Learning (DL) [Goodfellow et al., 2016] and established notation. Since ANNs are learning systems, we first expose machine learning concepts. We then discuss the general structure of an ANN and the concept of depth in that context. Convolutional Neural Networks are named as such because convolutions are a key building block of their architecture. Then, we incorporate the convolution operation to complete our exposition of CNNs. Finally, we summarize the more relevant facts.\nThe theoretical deep learning framework is immense and has many ramifications to learning theory, probability and statistics, optimization, and even cognitive theory, to mention just a few examples. There are several books for every topic described in the subsections of this tutorial, not to mention articles in conferences and journals. Therefore, our presentation of deep learning here is highly selective and aims to expose the fundamental aspects needed to agile assimilate DL.\nIt is worth noting that, since DL is a relative young field, the terminology greatly varies. We often indicate when several terms are used to designate a concept, but the reader should be aware that this indication is by far not exhaustive. Notation is at the end of this manuscript."}, {"title": "2 Motivation", "content": "Intelligent agents need to learn in order to acquire knowledge from the environment. To perform all kinds of tasks, humans learn, among other things, from examples. In this context, learning means that, for a defined task, the agent must be able to generalize. That is, conduct the task successfully beyond the examples it has seen before. Most of the tasks the agent can realize may be cast as classification or regression. For both type of tasks, when the examples are accompanied by the actual outcome, we speak of supervised learning as a learning paradigm. From a pedagogical perspective, the agent is being supervised by receiving the ground truth corresponding to each learning example.\nBesides supervised learning, there are three other learning paradigms: unsupervised learning, reinforcement leaning and semi-supervised learning. When no actual outcome is available, or not provided, learning must occur in an unsupervised manner. If, alternatively, the actual outcome is noisy or a number of them is missing, learning must proceed semi-supervised. In contrast, if no ground truth is provided at all, but a series of reinforcements can be imposed, the agent can nonetheless learn by being"}, {"title": "3 Machine Learning", "content": "From a general perspective the learning agent has access to a set of ordered examples\n$$X \\stackrel{\\text{def}}{=} {x^{(1)},...,x^{(m)}}$$\nassumed to have been drawn from some unknown generating distribution $\\mathbb{P}_{data}$. A learning example $x^{(i)}$ (also a pattern) may be anything that can be represented in a computer, for example, a scalar, a text excerpt, an image, or a song. Without loss of generality, we will assume that every $x^{(i)}$ is a vector $x^{(i)} \\in \\mathbb{R}^n$, and its components the example features. For instance, one could represent an image as a vector by arranging its pixels to form a sequence. Similarly, a song could be vectorized by forming a sequence of its notes. We will clarify when we refer to a different representation.\nFurthermore, the examples are endowed with their ground truth (set of labels) $Y$ so that for every example $x^{(i)}$, there exists a label (possibly a vector) $y \\in Y$. Using these examples the agent can learn to perform essentially two types of tasks: regression and classification (or both). Regression may be defined as estimating one or more values $y \\in \\mathbb{R}$, for instance, human body dimensions like height or waist circumference. In a classification scenario, the agent is required to assign one or more of a finite set of categories to every input, for example, from an image of a person, designate which body parts are arms or legs.\nAdditionally, in single-task learning the agent must learn to predict a single quantity (binary or multi-class classification, univariate regression), while in multi-task learning the algorithm must learn to estimate several quantities, e.g., multivariate multiple regression."}, {"title": "3.1 Learning Theory", "content": "In general, we consider a hypothesis space $H$, consisting of functions. For example, one could consider the space of linear functions. Within $H$, the function\n$$f(X) = Y$$\nmaps the examples (inputs) to their labels (outputs). The learning agent can be then expressed as a model\n$$ \\hat{f}(X) = \\hat{Y}$$\nthat approximates $f$, where $\\hat{Y}$ is the model estimate of the targets $Y$.\nNow, the ultimate goal of the learning agent is to perform well beyond the exam- ples it has seen before, i.e., to exhibit good generalization. More concretely, learning must have low generalization error. To assess generalization, the stationarity as- sumption must be adopted to connect the observed examples to the hypothetical future"}, {"title": "3.2 Model Evaluation", "content": "Assessing generalization by randomly splitting the available data in a training set and a test set is called holdout cross-validation because the test set is kept strictly separate from the training set and used only once to report the algorithm results. However, this model evaluation technique has two important disadvantages:\n\u2022 It does not use all the data at hand for training, a problem that is specially relevant for small datasets.\n\u2022 In practice, there can be the case where the i.i.d. assumption does not hold. Therefore, the assessment is highly sensitive to the training/test split.\nA remedy to these problems is the k-fold cross-validation evaluation method (cf. [Hastie et al., 2009] p. 241 Cross Validation). It splits the available data into k"}, {"title": "3.3 Statistics and Probability Theory", "content": "To further describe machine learning we borrow statistical concepts. In order to be con- sistent with the established literature[Goodfellow et al., 2016], we use in this subsection \u03b8 and $ \\hat{\u03b8} $ to to denote a quantity and its estimator.\nThe agent must learn to estimate the wanted quantity $ \\theta $, say. From a statistical perspective the agent performs a point estimate $ \\hat{\u03b8}$. In this view the data points are the i.i.d. learning examples {$x^{(1)},...,x^{(m)}$}. A point estimator is a function of the data such as $ \\hat{\u03b8}_m = g({x^{(1)},...,x^{(m)}})$, with g(x) being a very general suitable function.\nNote that we can connect the quantity \u03b8 and its estimator $ \\hat{\u03b8}$ to the functions $f$ and $\\hat{f}$ in Equations 2 and 3. Given the hypothesis space H that embodies possible input-output relations, the function $\\hat{f}$ that approximates $f$ can be treated as a point estimator in function space.\nImportant properties of estimators are bias, variance and standard error. The estima- tor bias is a measure of how much the real and the estimated value differ. It is defined as $bias(\\hat{\u03b8}_m) = E(\\hat{\u03b8}_m) \u2013 \\theta$. Here the expectation E is taken over the data points (examples). An unbiased estimator has $bias(\\hat{\u03b8}_m) = 0$, this implies that $ \\hat{\u03b8}_m = \\theta$. An asymptotically unbiased estimator is when $lim_{m\u2192\u221e} bias(\\hat{\u03b8}_m) = 0$, which implies that $lim_{m\u2192\u221e} \\hat{\u03b8}_m = \\theta$. An example of unbiased estimator is the mean estimator (mean of the training examples) of the Bernoulli distribution with mean \u03b8.\nAs we will soon see (where?), the variance $Var(\\theta) =$ and Standard Error $SE(\\theta)$ of the estimator are useful to compare different experiments. A good estimator has low bias and low variance.\nLet us now discuss two important estimators: the Maximum Likelihood and the Max- imum a Posteriori estimators."}, {"title": "3.3.1 Maximum Likelihood Estimation", "content": "To guide the search for a good estimator $ \\hat{\u03b8}$, the frequentist approach is usually adopted. The wanted quantity \u03b8, possibly multidimensional, is seen as fixed but un- known, and the observed data is random. The parameters e govern the data generating distribution $P_{data}(x)$ from which the observed i.i.d. data {$x^{(1)},...,x^{(m)}$} arose.\nThen, a parametric model for the observed data is presumed, i.e., a probability dis- tribution $P_{model}(x; \\theta)$. For example, if the observed data is presumed to have a normal distribution, then the parameters are $ \\theta = {\u03bc, \u03c3}$, and the model\n$$P_{model}(x; \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{-(x-\u03bc)^2}{2\\sigma^2}}$$\nHere candidates parameters a must be considered, and ideally, $p_{model}(x; \\theta) \u2248 p_{data}(x)$. Next, a likelihood function L can be defined as the"}, {"title": "3.3.2 Maximum a Posteriori Estimation", "content": "In contrast to the frequestist approach, the Bayesian approach considers e to be a random variable and the dataset x to be fixed and directly observed. A prior proba- bility distribution p(\u03b8) must be defined to express the degree of uncertainty of the state of knowledge before observing the data. In the absence of more information, a Gaussian distribution is commonly used. The Gaussian distribution is \"the least surprising and least informative assumption to make\" [McElreath, 2020], i.e., it is the distribution with maximum entropy of all. Therefore, the Gaussian is an appropriate starting prior when conducting Bayesian inference.\nAfter observing the data, Bayes' rule can be applied to find the posterior distribu- tion of the parameters given the data $p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}$, where p(x\u03b8) is the likelihood and p(x) the probability of the data.\nThen, the Maximum a Posteriori (MAP) estimator selects the point of maximal posterior probability:\n$$ \\hat{\u03b8}_{MAP} = arg\\ max_\\theta p(\\theta|x) = arg\\ max_\\theta log\\ p(x|\\theta) + log\\ p(\\theta). $$"}, {"title": "3.4 Optimization and Regularization", "content": "The training error as defined by Eq. 4 is an objective function (also criterion) that needs to be minimized. Utility theory regards this objective as a loss function $J(x) : \\mathbb{R}^n \u2192 \\mathbb{R}$ because the agent incurs in a lost of expected utility when estimating the wrong output with respect to the expected utility when estimating the correct value. To establish the difference of these values, a determined distance function must be chosen. For general (multivariate, multi-target) regression tasks the p-norm is preferred. The p-norm is calculated on the vector of the corresponding train or test error $||\u0177_i - Y_i||_p = (\\Sigma |\u0177_i - Y_i|^p)^{\\frac{1}{p}}$, which for p = 2 reduces to the Euclidean norm. Since the Mean"}, {"title": "3.4.1 Loss Function", "content": "The training error as defined by Eq. 4 is an objective function (also criterion) that needs to be minimized. Utility theory regards this objective as a loss function $J(x) : \\mathbb{R}^n \u2192 \\mathbb{R}$ because the agent incurs in a lost of expected utility when estimating the wrong output with respect to the expected utility when estimating the correct value. To establish the difference of these values, a determined distance function must be chosen. For general (multivariate, multi-target) regression tasks the p-norm is preferred. The p-norm is calculated on the vector of the corresponding train or test error $||\u0177_i - Y_i||_p = (\\Sigma |\u0177_i - Y_i|^p)^{\\frac{1}{p}}$, which for p = 2 reduces to the Euclidean norm. Since the Mean"}, {"title": "3.4.2 Gradient-Based Learning", "content": "As we will see in Sec. 4, in the context of Deep Learning, the parameters e discussed in Subsec. 3.3 are named weights w. Without loss of generalization, we will consider the parameters to be a collection of weights. We do not lose generalization because the discussion here does not depend on the specific form of the cost function.\nTherefore, parameterizing the estimator function by the weights, minimization of the cost function can be written as\n$$J^* (w; x, y) = arg \\min_w \\frac{1}{m} \\Sigma (\\hat{f}(x^{(i)}, w) - Y_i)^2$$\nThe function $\\hat{f}(x^{(i)}, w)$ is, in a broader sense, a nonlinear model, thus the minimum of the loss function has no closed form and numeric optimization must be employed. Specifically, J is iteratively decreased in the orientation of the negative gradient, until a local or global minimum is reached, or the optimization converges. This optimization method (also optimizer) is termed gradient descent. Because optimizing in the context of machine learning may be interpreted as learning, a learning rate e is introduced to influence the step size at each iteration. The gradient descent optimizer changes weights using the gradient $\u2207_w$ of J(w) according to the weight update rule\n$$w \u2190 w \u2212 \\epsilon\u2207_wJ(w)$$\nWe postpone the discussion on Stochastic Gradients Descent (SGD) to insert it in the context of training neural networks (Subsec. 4.2)."}, {"title": "3.5 Regularization", "content": "The essential problem in machine learning is to design algorithms that perform well both in training and test data. However, because the true data-generating process is"}, {"title": "4 Deep Forward Artificial Neural Networks", "content": "Deep Forward Artificial Neural Networks (DFANN) is a model conveying several impor- tant interlaced concepts. We start with its building blocks and then connect them to form the network. Additionally, to describe the parts of an ANN, the term architecture is used."}, {"title": "4.1 Artificial Neural Networks", "content": "Artificial Neural Networks (ANNs) is a computational model originally inspired by the functioning of the human brain [McCulloch and Pitts, 1943, Hebb, 1949, Rosenblatt, 1957]. The model exploits the structure of the brain, focusing on neural cells (neurons) and its interactions. These neurons have three functionally distinct parts: dendrites,"}, {"title": "4.1.1 Artificial Neurons", "content": "Similarly, the elementary processing unit of ANNs is the artificial neuron. The neurons receive a number of inputs with associated intensity (also strength) and combine these inputs in a nonlinear manner to produce a determined output that is transmitted to other neurons. Depending on the form of the non-linearity, it can be said that the neuron 'fires' if a determined threshold is reached.\nEvery input to the neuron has an associated scalar called weight w. The non-linearity, also called activation function is denoted by g(.). The neuron first calculates an affine transformation of its inputs $z = \\Sigma_{i=1} w_i\\cdot x_i + c$. The intercept (also bias) \u0441 may be combined with a dummy input $X_{dummy} = 1$ to simplify notation such as the affine transformation can be written in matrix form $ \\Sigma_{i=0} w_i\\cdot x_i = w\\cdot x$. One of the reason ANNs where invented is to overcome the limitations of linear models. Since a linear combination of linear units is also linear, a non-linearity is needed to increase the capacity of the model. Accordingly, the neuron applies the activation function to compute the activation $a = g(w. x)$, which is transmitted to other neurons.\nReferring to activation functions, [Goodfellow et al., 2016] states categorically \"In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU\u201d introduced by [Jarrett et al., 2009]:\n$$g(z) = max(0, z).$$\nIn order to enable learning with gradient descent (Eq. 17) as described in Subsec. 3.4.2, is desirable the activation function to be continuous differentiable. Although the ReLU is not differentiable at z = 0, in practice, this does not constitute a problem. Practitioners assume a \u2018derivative default value' f'(0) = 0, f'(0) = 1 or values in between.\nOther activation functions are the logistic sigmoid $\u03c3(z) = 1/(1+exp(\u2212z))$ and the softplus function $\\zeta(z) = log(1+exp(z))$. As a side note, a neuron must not necessarily compute a non-linearity, and in that case, it is called a linear unit.\nThe perceptron is an artificial neuron introduced by [Rosenblatt, 1957], who demon- strated that only one neuron suffices to implement a binary classifier. The perceptron had originally a hard threshold activation function, but it can be generalized to use any of the above-mentioned activation function. For example, if the perceptron uses a logis- tic sigmoid activation function, then the output can be interpreted as the probability of some event happening, e.g., logistic regression."}, {"title": "4.1.2 Deep Feedforward Networks", "content": "Connecting the artificial neurons results in an artificial neural network with at least two distinguishable layers: the input layer and the output layer. The input layer is"}, {"title": "4.2 Training", "content": "Having described the structure of an artificial neural network, and how they learn by means of optimization, we now need to perform the actual training. The learning exam- ples are entered into the network through the input layer and the network weights need to be updated using gradient descent to minimize the loss."}, {"title": "4.2.1 Stochastic Gradient Descent", "content": "As we already discussed in Subsec. 3.4.2, gradient descent enables optimization of gen- eral loss functions. Specifically, the model weights are updated using the gradient of the loss function $\u2207_wJ(w)$, which is sometimes called pure, deterministic or batch gradient descent. The problem is that the computation of this gradient requires an"}, {"title": "4.2.2 SGD with Momentum", "content": "Despite the computational and efficacy advantages of SGD, its trajectory to the op- timum or an appropriate stationary point can be slow. To overcome this limitation, the momentum algorithm changes the weights such as it proceeds incorporating the contribution of preceding gradients. This accelerates learning because gradients strong fluctuations can be diminished, and therefore, the path to the minimum is less oscillating.\nOne can gain intuition into the momentum algorithm by drawing a physical analogy. Let the learning algorithm be a heavy ball rolling down the optimization landscape The ball is subject to two forces, the momentum force acting in the negative direction of the gradient -$\u2207_wJ(w)$, and a viscose force attenuating its movement m. v. Therefore, the weights should be updated according to the net force acting on the ball.\nIn particular, since momentum is mass times velocity, and considering the ball to have unit mass, the velocity can be interpreted as momentum. Then, introducing a term v and a parameter a \u2208 [0, 1) the weight update can be written\n$$v \u2190 av \u2212 \\epsilon\u2207_wJ(w)$$\n$$w \u2190 w + v$$\nDespite the parameter a being more related to viscosity, it is widely called momen- tum parameter. Furthermore, the velocity update at iteration k can be written as"}, {"title": "4.2.3 Forward Propagation and Back-propagation", "content": "In this subsection, we departure from our two main references and follow [Bishop, 2007] and [Nielsen, 2015].\nWe are now ready to begin training the network. This requires a proper definition of an individual weight in a certain layer l, e.i., the weight $W_{jk}^l$ connects the kth neuron in the (1 - 1)th layer to the jth neuron in the current layer 1. This rather intricate notation is necessarily in order to compactly relate the activation of consecutive layers $a^l$ and $a^{l-1}$:\n$$z_j^l = \\sigma (\\Sigma_k W_{jk}^l a_k^{l-1} + b_j^l)$$\n$$a^l = g(W^l a^{l-1} + b^l)$$\nHere, if we fixate the neuron j with weight $w_{jk}^l$ and bias $b_j^l$, at layer l, then its activation a results from the activation in the preceding layer according to Eq. 27. Intuitively, the activation of a neuron is a function of its weighted inputs, i.e., the activation of all the units connected to it from the previous layer, and the bias of the neuron (Fig. 1). Eq. 28 is its vectorized form, where g() is the activation function applied element-wise. Furthermore, the weight sum before applying the non-linearity is called the weighted input $z_j^l$ to the neuron j in layer l; it is defined as\n$$z_j^l \\stackrel{\\text{def}}{=} \\Sigma_k W_{jk}^l a_k^{l-1} + b^l$$\n$$z^{(l)} \\stackrel{\\text{def}}{=} W^{(l)} a^{(l-1)} + b$$\n$$a^{(l)} = g(z^{(l)}).$$"}, {"title": "4.2.4 Hyperparameters", "content": "In most discussions through this tutorial, we have mentioned several quantities, but we did not indicate how they can be set. These quantities are called hyperparamters.\nHyperparameters are usually set empirically or using some kind of intuition. In our case, we use mostly default values or common values. When not, we will explicitly indicate it."}, {"title": "5 Convolutional Neural Networks", "content": "In Sec. 4.1.2, we mentioned that the architecture of a general ANN may involve dif- ferent layer types. Specifically, we described a dense layer, whose mathematical form is basically matrix multiplication. Unfortunately, an architecture composed alone of dense layers is computational extremely inefficient. For example, processing 200 \u00d7 200 pixels monochrome images, like the ones we input to the ANN, requires 400 connections (one connection to every pixel) only for one neuron in the first hidden layer.\nConvolutional Neural Networks (CNN, also ConvNets) reduce strikingly the computational burden of having to store and update millions of weights in the network. Two characteristics convert CNNs in an effective learning agent, namely sparse in- teractions (also sparse connectivity or sparse weights) and parameter sharing, which we will discuss beneath.\nCuriously, the invention of CNNs was not motivated by efficiency considerations. In- stead, ConvNets, like ANNs and Batchnorm, are inspired by the functioning of the brain: \"the network has a structure similar to the hierarchy model of the visual nervous sys- tem. (\u2026) The structure of this network has been suggested by that of the visual nervous system of the vertebrate. [Fukushima, 1980]. In particular, the brain in vertebrates exhibits a hierarchical structure where cells higher in the hierarchy (complex cells) re- spond to more complicated patterns as cells in lower stages (simple cells), and complex \""}, {"title": "5.1 Convolutional Layer", "content": "Convolutional networks are named as such because performing forward propagation in one of its layers I with sparse and tight weights implies calculating a convolution of the input I \u2208 $R^{m\u00d7n}$ to the layer l, with at least one kernel K \u2208 $R^{f\u00d7f}$ of a determined odd squared size f. The result of that convolution is a featured map M.\nAssume a general function dim(X) that maps a matrix X to its dimensions. Then the dimensions of the input, kernel, and feature map are\ndim(I) = {m, n}\ndim(K) = {f, f}\ndim(M) = {$m - f + 1, n \u2212 f + 1$}.\nIn deep learning, the term convolution usually refers to the mathematical operation of discrete cross-correlation, which is the operation implemented by most deep learning frameworks. Assuming an input map I and a kernel K, one-based index cross-correlation takes the form\n$$((I *K)(i, j)) = \\Sigma \\Sigma x_{i+m-1,j+n-1}\\cdot w_{m,n}$$\n$$z_{i,j} = ((I * K))^' + b_j$$\n$$a = z_{i,j}$$\nwhere bij is the bias of the pre-activation zij and a = 2; the i, j element of a 2D feature map.\nFurthermore, the convolution operation can be generalized in a number of ways. First, the manner in which the kernel is slid over the input map. The kernel must not nec- essarily be moved to the immediately next spacial element to compute the Frobenious product. Instead, the kernel may be slid a number s of spatial locations. This is called the stride s, and the corresponding operation may be called a strided convolution. The baseline convolution has s = 1.\nSecond, the kernel may be shifted beyond the elements at the border of the input map. Sliding the kernel, such as it entirely lies within the input map, has the advantage that all the elements of the feature map are a function of equal numbers of elements in the input. However, it also has the disadvantage of progressively reducing the output (Eq. 56) until the extreme stage where a 1 \u00d7 1 output can not be meaningfully further processed. Nevertheless, this does not pose a severe problem for CNNs of moderate depth like ours. Alternatively, the spatial dimensions may be preserved by zero-padding the input with pelements. When p = 0 (no padding), the operation is called valid convolution, which is the one we employ. In contrast, constant spacial dimensionality across input and output may be achieved by performing a same convolution with the corresponding p > 0 padding.\nFinally, the convolution operation may be generalized to operate on multidimensional inputs, e.g., RGB images, producing multidimensional outputs. This generalized convo- lution is termed convolutions over volumes because input, kernels, and output may be considered as having a width, height, and depth (also channel to avoid confusion with the network's depth). This, besides the processing of batches (Subsec. 5.5), results in CNNs usually operating in tensors of size b \u00d7 w \u00d7 h x c, i.e., batch size b, two spacial dimensions width w and height h, and c number of channels, respectively. In this tuto- rial, we focus on inputs (images) with a maximum of three channels. Without loss of"}, {"title": "5.2 Pooling Layer", "content": "Jet another prominent method to reduce the amount of parameters in the network is to use a pooling layer. Pooling outputs the result of computing a determined summary statistic in neighborhoods of activations of the previous layer.\nLike convolution, the pooling function may be viewed as sliding a multi-channel win- dow over a multi-dimensional input with equal amount of channels. Therefore, the pooling function may be considered as a filter of squared size f that may be shifted by a strides, acting on a possibly p zero-padded input. Unlike the convolution layer, the pooling layer does not have weights that must be updated by the learning algorithm. Additionally, pooling is applied on each channel independently. For an input volume M with dimensions {w, h, c}, pooling's output O dimensionality is\n\ndim(0) = {$ [\\frac{w + 2p - f}{S} + 1], [\\frac{h+ 2p - f}{S} +1], c$}.\nWhile the pooling function may calculate several summary statistics on any combina- tion of dimensions, the most used is the operator that yields the maximum among the corresponding channel input elements M.,.,k inside the sliding window max(X), termed max pooling. That is, in one-based index form,\n\n= max_m max_n M-1(i.s-1)+(m-1),(j.s-1)+(n-1),k"}, {"title": "5.3 ReLU layer", "content": "As we mentioned, we adopted the simple layer terminology, where the non-linearities are not part of the convolutional layer. Instead, we consider that there is a ReLU layer which may be integrated into the network's architecture. As expected, the ReLU layer computes Eq. 21."}, {"title": "5.4 CNN Training", "content": "Since CNNs are deep feedforward network (Sec. 4), its weights may be initialized using the Kaiming method (Subsec. 4.2.3). CNNs may also be trained with gradient descent 3.4.2. In particular, forward propagation (Eqs. 63-65, 21, and 67) must be performed to obtain the error at the output layer, followed by backprop.\nAs we discussed before (Subsec. 4.2.3) while backpropagating the error, the derivatives of the loss function w.r.t. the parameters need to be computed in order to update the weights. For CNNs, we will follow the strategy that we described in Subsec. 4.2.3. That is, 1) compute the error at the output layer, 2) computer the error of the current layer in terms of the next layer to be able to backpropagate, 3) compute the weight derivatives in terms of the error at the current layer and 4) compute the derivative of the bias.\nIn a CNN, the convolutional layer's parameters are the kernels' weights and the layer biases, one for each kernel. Therefore, the goal of backprop may be restated as computing the derivatives of the loss w.r.t. the kernels and the bias. Additionally, the derivatives of the ReLU and max pooling layer must be considered.\nSince now we are operating with 3D tensors, we define the derivative of the cost function w.r.t one input element  as\n\nCommonly, the output layer of a CNN is a dense layer. Therefore, the error at the output layer de may be computed in a the same manner as for a general ANN (Eq. 39). As we will see, the architecture of CNNs comprises ReLU and max pooling layers. Therefore, the computation of the backpropgation and concrete weights (Eqs. 47 and 51) must be adapted to these layers.\nFortunately, neither the ReLU layer nor the max pooling layer contain weights that must be updated by SGD. That means Eqs. 51-53 must be disregarded for these two layers. However, in our strategy these layers must be able to transmit the error back to preceding layers, which indeed requires readjusting Eq. 47 (backpropagation equation between layer 1 + 1 and 1). For that matter, we may consider that the error at these layers are known, e.g., after having been backpropagated by a dense layer.\nOn order to adapt Eq. 47 to the ReLU layer, note that, because the layer does not contain weights, the weigh matrix W vanishes. Also, we now consider the local gradient  to be a tensor element. Since forward propagating through a ReLU layer is conducted elementwise, the input and output dimensions are preserved, and so happens with backpropagation. This allows equal indexing of the backpropagated error arriving at the layer and the calculated local gradient. That is,\n\nwhere, ' is the derivative of the ReLU evaluated at the tensor element"}, {"title": "5.4.1 Forward and Back-propagation in CNNs", "content": "Since CNNs are deep feedforward network (Sec. 4), its weights may be initialized using the Kaiming method (Subsec. 4.2.3). CNNs may also be trained with gradient descent 3.4.2. In particular, forward propagation (Eqs. 63-65, 21, and 67) must be performed to obtain the error at the output layer, followed by backprop.\nAs we discussed before (Subsec. 4.2.3) while backpropagating the error, the derivatives of the loss function w.r.t. the parameters need to be computed in order to update the weights. For CNNs, we will follow the strategy that we described in Subsec. 4.2.3. That is, 1) compute the error at the output layer, 2) computer the error of the current layer in terms of the next layer to be able to backpropagate, 3) compute the weight derivatives in terms of the error at the current layer and 4) compute the derivative of the bias.\nIn a CNN, the convolutional layer's parameters are the kernels' weights and the layer biases, one for each kernel. Therefore, the goal of backprop may be restated as computing the derivatives of the loss w.r.t. the kernels and the bias. Additionally, the derivatives of the ReLU and max pooling layer must be considered.\nSince now we are operating with 3D tensors, we define the derivative of the cost function w.r.t one input element  as\n\nCommonly, the output layer of a CNN is a dense layer. Therefore, the error at the output layer de may be computed in a the same manner as for a general ANN (Eq. 39). As we will see, the architecture of CNNs comprises ReLU and max pooling layers. Therefore, the computation of the backpropgation and concrete weights (Eqs. 47 and 51) must be adapted to these layers.\nFortunately, neither the ReLU layer nor the max pooling layer contain weights that must be updated by SGD. That means Eqs. 51-53 must be disregarded for these two layers. However, in our strategy these layers must be able to transmit the error back to preceding layers, which indeed requires readjusting Eq. 47 (backpropagation equation between layer 1 + 1 and 1). For that matter, we may consider that the error at these layers are known, e.g., after having been backpropagated by a dense layer.\nOn order to adapt Eq. 47 to the ReLU layer, note that, because the layer does not contain weights, the weigh matrix W vanishes. Also, we now consider the local gradient  to be a tensor element. Since forward propagating through a"}]}