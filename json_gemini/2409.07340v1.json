{"title": "A Framework for Predicting the Impact of Game Balance Changes through Meta Discovery", "authors": ["Akash Saravanan", "Matthew Guzdial"], "abstract": "A metagame is a collection of knowledge that goes beyond the rules of a game. In competitive, team-based games like Pok\u00e9mon or League of Legends, it refers to the set of current dominant characters and/or strategies within the player base. Developer changes to the balance of the game can have drastic and unforeseen consequences on these sets of meta characters. A framework for predicting the impact of balance changes could aid developers in making more informed balance decisions. In this paper we present such a Meta Discovery framework, leveraging Reinforcement Learning for automated testing of balance changes. Our results demonstrate the ability to predict the outcome of balance changes in Pok\u00e9mon Showdown, a collection of competitive Pok\u00e9mon tiers, with high accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Game balance in competitive multiplayer games is a con-stant process. In character-based games like Pok\u00e9mon or League of Legends there are regular patches adding new char-acters or adjusting existing ones. This impacts the metagame (or meta), which is the set of current dominant characters or strategies within the player base. The metagame can be constantly changing, with players continually inventing new dominant strategies to counter the current metagame. In time, these new strategies could become the prevalent meta. This process then repeats, allowing for the metagame to achieve a level of self-balancing. However, in other scenarios, there could be one clear dominant strategy that is superior to all others. In addition, the metagame could stagnate or reach a state where the dominant strategies are not fun to play against. This negatively impacts player experience and can lead to players abandoning a game. Developers need to be careful when they attempt to make changes to alter the metagame. There have been instances of developer changes causing major, unintended shifts in the metagame such as the infamous Juggernaut patch of League of Legends [1] and the creation of the Anything Goes tier in competitive Pok\u00e9mon [2]. Thus the meta that arises from a balance change may not necessarily be the meta that the developers envisioned.\nWhen balancing games, many developers consider the com-bination of a character's winrate and their pickrate [3]. The pickrate is the percentage of matches a character is picked in while the winrate is the percentage of matches that the character actually wins. We note that while this information offers value in identifying areas that may need changes, it does not specify what changes to make. In practice, developers employ their expertise, domain knowledge, and regular testing to make balance changes. We note that we do not attempt to automate this process, merely give additional insight into the likely impact of balance changes. Outside of developers, there exist automated balancing approaches [4]. However, these tend to focus on balancing single-player experiences, not multiplayer games. In addition, they make changes directly rather than predicting the impact of potential changes.\nIn this paper, we propose a Meta Discovery Framework for team-based, character-dependent competitive games, as illustrated in Figure 1. By team-based we indicate that matches take place between two teams. By character-dependent we indicate that the games rely on in-game characters, who make up the aforementioned teams. This framework consists of three components - a battle agent to play matches, a team-builder for approximating new meta teams, and an environment to sim-ulate matches after a balance change. Using this framework, we attempt to discover the impact of changes to the metagame such as the addition, removal, or adjustment of characters. We name this task ABC-Meta, or Analyzing Balance Changes on the Metagame. In essence, we evaluate game balance by analyzing the results of changes made to it. We analyze the ABC-Meta task on Pok\u00e9mon Showdown, a competitive Pok\u00e9mon environment and draw on historic balance changes to test our approach. Our results demonstrate that this approach can predict the impact of balance changes with high accuracy. Our hope is that this framework can help developers to better understand the potential impact of changes made to characters before deploying them into the game's live environment.\nOur contributions in this paper are as follows:\n1) The development of a Meta Discovery Framework and instantiations of it with Reinforcement Learning and heuristic agents.\n2) The proposal of the ABC-Meta task as a new games research challenge.\n3) The proposal of a related Blank Slate Discovery of Metagames or BSD-Meta task.\n4) Results in Pok\u00e9mon Showdown on historic data, sup-porting the utility of the framework and the feasibility of our identified tasks."}, {"title": "II. BACKGROUND", "content": "The Pok\u00e9mon video game series are Role-Playing Games (RPGs) that involve players capturing the titular Pok\u00e9mon\nA. Pok\u00e9mon"}, {"title": "B. Pok\u00e9mon Showdown", "content": "To evaluate our system, we implement it in the Pok\u00e9mon video game series, specifically on Pok\u00e9mon Showdown. Pok\u00e9mon Showdown is an online Pok\u00e9mon battle simulator with millions of monthly matches [5]. It has an extensive, com-plex, and publicly documented metagame. We use Pok\u00e9mon over other games for two reasons the battles are generally much faster and it has a simple, easy to access API. Battle speed is important as it allows us to simulate a large number of matches in a relatively short amount of time, thus allowing us to experiment with different techniques for meta discovery. Other games such as League of Legends and Dota 2 take much longer for matches (around 30 minutes on average) while Pok\u00e9mon battles usually take less than 5 minutes. We elect to use Pok\u00e9mon Showdown in particular over something like the VGC format [6] due to its popularity and easily available metagame data."}, {"title": "C. Attributes of Competitive Pok\u00e9mon Battles", "content": "In terms of competitive battling, each Pok\u00e9mon has several stats and attributes to take into account. While a complete understanding is not necessary to understand this work, they are important aspects of competitive Pok\u00e9mon battles and serve to illustrate the complexity and interlinked nature of these battles. Every Pok\u00e9mon has a level (1-100), 6 base stats (Hit Points [HP], attack, defense, special attack, special defense, and speed) each in a range of 1-255, 1 to 4 moves (from over 800 options across 3 classes), one or two types (from 18 choices), Effort Values (EVs) ranging from 0 to 255 and Individual Values (IVs) ranging from 0-31 for each of the 6 base stats, a nature, 2 to 3 passive abilities (from over 250 options) and up to 1 item (from over 500 options). Every move also has its own statistics such as Accuracy and Base Power, as well as one of the 18 types. There is also a 1 in 24 base chance of a move being a critical hit, which deals increased damage. Each Pok\u00e9mon type has a set of other types that it is strong against, weak against, resistant to, and immune to. For example, fire-type is weak to water-type and strong against grass-type. Further, Pok\u00e9mon have a Same Type Attack Bonus (STAB) which causes them to deal more damage with moves that match their type. A Pok\u00e9mon's overall stats are calculated using a combination of its base stats, effort values, individual values, level and nature. Some moves, abilities and items may also inflict one of 6 permanent status effects or over 150 temporary status effects each of which influences a Pok\u00e9mon's attributes. The Pok\u00e9mon's overall stats, in addition with its type, a move's base power, STAB, ability, item, stat changes and status effects determine the amount of damage a Pok\u00e9mon can do with a single move."}, {"title": "D. Pok\u00e9mon Battles", "content": "In a standard battle each player has a team of up to 6 Pok\u00e9mon. In competitive Pok\u00e9mon matches, all Pok\u00e9mon are usually level 100, utilize the maximum amount of EVs and IVs, have 4 moves, and equip an item. At the start of the battle, each player chooses one of their 6 Pok\u00e9mon to send out. In some battle formats this choice will always be the first Pok\u00e9mon in the roster and in others it can be random. Battles proceed in a turn-based fashion where both players perform an action simultaneously, the results are calculated and the players once again take an action. On a player's turn they may either choose one of the 4 moves that a Pok\u00e9mon knows or they may switch to one of the available Pok\u00e9mon on their team. If a player's Pok\u00e9mon has fainted (its HP has been reduced to 0), the Pok\u00e9mon will no longer be available for the battle. In this case, the player must switch to another Pok\u00e9mon on their team. This is a free action, that is, the opponent does not take an action until a new Pok\u00e9mon has been sent in (the player's next turn). This continues until a player forfeits or all 6 of a player's Pok\u00e9mon have fainted (HP reduced to 0). There are also alternate formats such as the Doubles format, where each player fields two Pok\u00e9mon at a time. However this format is outside the scope of this paper and so we do not discuss it in further detail.\nThe team-building aspect of Pok\u00e9mon adds in another layer of complexity. As seen in the previous section, just considering all the attributes of a single Pok\u00e9mon is a significant challenge. When teams come into play, this problem repeats six times. In addition, other factors need to be taken into account. These include how well each Pok\u00e9mon synergizes with the team, major or minor weaknesses in the team against certain Pok\u00e9mon, and the likelihood of such threats appearing in a battle, and others. Additionally, there are several distinct Pok\u00e9mon archetypes and overall play styles [7]. Thus this team-building aspect of Pok\u00e9mon is an entire research topic of its own [8], [9]."}, {"title": "E. The Competitive Metagame of Pok\u00e9mon", "content": "Pok\u00e9mon Showdown is a popular online simulator for Pok\u00e9mon battles, which is sponsored by Smogon [10], a fan-run competitive community. There are several different battle formats or \"tiers\" used in Pok\u00e9mon Showdown. Each of these tiers have their own metagame with varying strategies and characters. The most popular format according to publicly available statistics is the random battle format where players do not build their own team [5]. Instead, both players use a pseudo-randomly generated team. However, this format does not have a true metagame due to this pseudo-random gener-ation. In fact, the generation itself depends on the existing metagame. Most other tiers also have several clauses put into place for both game balance and fun. These include a \u201cspecies clause\u201d which prevents multiple of the same Pok\u00e9mon species from being used along with several others that ban particular moves or abilities. In addition, each tier also has its own particular banlist of Pok\u00e9mon, moves, abilities or items. The tiering system, bans and clauses are all created and maintained by Smogon.\nThe most popular competitive tier is the OverUsed (OU) tier. Below OU is the UnderUsed (UU) tier. This continues on with the NeverUsed (NU), RarelyUsed (RU), and PU (no full form) tiers. In each case, all Pok\u00e9mon with a certain weighted usage rate are classified as being a Pok\u00e9mon of that tier. A Pok\u00e9mon from a higher tier is banned from usage in lower tiers, although lower tier Pok\u00e9mon can be used in any higher tier. Each tier has an associated BanList (BL) which consists of Pok\u00e9mon that are banned from that particular tier, but do not have enough usage to be classified into a higher tier. For instance, the UnderUsed BanList (UUBL) consists of Pok\u00e9mon banned from UU, but under the required usage rate to be classified as OU. The one exception to this is the OU tier itself. All Pok\u00e9mon banned from OU are placed in an entirely different tier - the Ubers tier. There are several other special tiers and cases, but these are the most popular ones. For simplicity, in this research we only focus on OU, UU, NU, and PU."}, {"title": "III. RELATED WORK", "content": "We now discuss prior research related to our work. Specifically, in subsection III-A, we examine systems to generate teams. In subsection III-B we cover work on the impact of balance changes in games as well as approaches surrounding the idea of meta discovery."}, {"title": "A. Team-Building", "content": "Team-building is essential in team-based competitive games. When attempting to simulate the metagame, the teams that are generated directly form the meta. Thus team-building is an essential component of meta discovery. Several approaches have analyzed this aspect of competitive games. Summerville, Cook and Steenhuisen [11] and Hong, Lee and Yang [12] developed systems to predict the heroes picked during the drafting phase in Dota 2 and League of Legends, respectively. Both cases can be thought of as a team-building process using the current metagame alongside historical data. Both Crane et. al [13] and Oliveira et. al [14] studied team-building in Pok\u00e9mon GO. The former tested several team generation systems to generate teams capable of regularly winning in an analyzed metagame while the latter tested several optimization algorithms to counter a given team. However, due to Pok\u00e9mon GO being distinct from, and far simpler than Pok\u00e9mon Show-down their system is not applicable to our work. Focusing on Pok\u00e9mon Showdown in particular, Rejim [15] developed a collaborative system to recommend additions to a team based on user specified inputs and usage statistics. Future Sight AI [9] also generates teams based on usage statistics, but has no publicly available algorithm or implementation. Technical Machine [8] either steals teams from opponents it loses to or uses a weighted random selection based on usage statistics. Our team-building framework uses techniques from Rejim and Technical Machine in order to dynamically generate teams based on the metagame state."}, {"title": "B. Metagame Discovery & Game Balance", "content": "Kokkinakis et. al [16] studies the evolution of the concept of a metagame and how it affects game balance. On the application side, several instances of prior work have studied the modification of game rules and their resulting impact. These can be treated as changes to gameplay mechanics instead of changes to characters. One such work [17] focused on the game of chess, defining several alternate rulesets to the game and analyzing the resultant game after achieving near-optimal performance using AlphaZero. Though effective for chess, competitive Pok\u00e9mon battling has no game playing agent that approaches this level of expertise. Jaffe et al. [18] restricted particular actions in a simple competitive game to evaluate the effects on game balance. However, their tool was built for perfect-information games, while Pok\u00e9mon is an imperfect-information game. Zook, Fruchter and Riedl [19] balanced parameters of a game by combining an automated playtester with an active learning framework. However, they use a flat set of parameters and acknowledge the need for alternative techniques for more complex scenarios. Silva et al. [20] used an evolutionary algorithm to find a set of balance changes to Hearthstone cards such that decks approach a 50% winrate. However, they use a small set of predefined decks which do not adapt to balance changes, and thus, do not use a team-building component. Finally, Hernandez et. al [21] developed a system to balance competitive games by finding a parameterization of the optimal metagame, but required game balance to be represented as a parameterizable graph."}, {"title": "IV. BATTLE AGENTS", "content": "In this work, we draw on two different battle agents for comparison purposes, a heuristic agent and a Reinforcement Learning (RL) agent. Both are used in the framework depicted in Figure 1 to test the impact of balance changes.\nWe preface this section with the statement that our objective is not to create a state-of-the-art agent. Although this is an interesting research problem on its own, our goal is to only have an agent capable of approximating an average human at the game. This is because they have the most influence over the metagame, given that the meta is a measure of popularity and the majority of players in competitive online games are of average skill. However, an argument can be made that the metagame may vary based on a player's skill level (and thus matchmaking rating) [3]. For instance, if we have a state-of-the-art agent, the meta might reflect more high-level play which may not necessarily be true at lower skill levels. Thus, in an ideal scenario, having a swarm of agents of varying skill level would result in a truer meta. This is out of scope for this paper and we leave an exploration of this for future work.\nWe note that while the rest of this section details the specifics of setting up agents for Pok\u00e9mon Showdown, our framework itself only requires an agent that can play like an average human. Thus future work does not need to specifically follow this approach.\nWe use the Random Battle format, specifically \"gen8randombattle\", for training agents. This format involves two randomly generated teams of six Pok\u00e9mon in a singles battle format. That is, each player sends out one Pok\u00e9mon at a time and they battle until a player either forfeits or has no remaining Pok\u00e9mon. We describe this system in more detail in Section II-D. We note that the random battle format is different from the standard Smogon tiers that we analyze for meta discovery. We train our agents on this format for two reasons. First, this format does not require us to provide teams, instead pseudo-randomly generating them based on hand-crafted rules. In contrast, if we used the standard Smogon tiers, we would have to compile teams ourselves or create our own team-building algorithm. Second, the pseudo-random nature of the teams allows for more generalization in the reinforcement learning model since it offers a wider variety of teams to the model. We do note that some attributes of the generated team are dependent on the current meta. Thus we cannot use the team generation system used by gen8randombattle or other meta-dependent approaches as it would bias the results.\nIn order to train these agents, we make use of the poke-env API by Haris Sahovic [22], which is a Python interface to create bots for Pok\u00e9mon Showdown. All code used in this paper is publicly available in the linked Github repository\u00b9."}, {"title": "A. Action Space", "content": "Our reinforcement learning agent has a discrete action space of size 22. The first four actions correspond to each of the four moves that a Pok\u00e9mon can have. The next 12 actions correspond to Z-Move variations, Mega Evolution variations, and Dynamax variations of the same 4 moves. Each of these are once-per battle gameplay mechanics and are associated with the first four actions. The final six actions correspond to the ability to swap out the current Pok\u00e9mon for the Pok\u00e9mon in the corresponding position within the team. This does mean that at all times, at least one of these 6 actions will be illegal as one Pok\u00e9mon is always on the field. Similarly, it is not possible to switch to a fainted Pok\u00e9mon nor is it possible to use a move if the current Pok\u00e9mon has fainted. We perform action masking, thus preventing such illegal actions from being taken. At every step, actions are passed to the environment as an integer corresponding to one of the 22 possible actions."}, {"title": "B. State Representation", "content": "Our state representation is based on the default state rep-resentation available in the poke-env API. It is of size 10, with the first 4 elements corresponding to the base power of the Pok\u00e9mon's 4 moves. We normalize this base power by dividing it by 100 to facilitate learning. The next 4 elements correspond to the type effectiveness of each move against the opponent's currently active Pok\u00e9mon. The final two elements offer an indication of how many Pok\u00e9mon are still alive on each team. We normalize this value by dividing it by 6 as a team can have a maximum of 6 Pok\u00e9mon. There may be other aspects of a battle important for the state representation, but we use the default for simplicity and given that our goal is not to create a state-of-the-art agent."}, {"title": "C. Reward Function", "content": "Our reward function is a default provided by the poke-env API. It is, in essence, a weighted sum of the condition of the player's Pok\u00e9mon (number of Pok\u00e9mon and their current health), the opponent's Pok\u00e9mon, and the state of the battle (win/loss). We use the weights recommended by the author of poke-env. Specifically, we weigh fainted Pok\u00e9mon and overall HP features at 0.0125 and 0.1 respectively and give out a reward of 1 and -1 for victories and defeats respectively. The specific implementation of this function can be found in the poke-env library and we omit it from this work due to its complexity."}, {"title": "D. Agents", "content": "As discussed above, we try two battle agents to evaluate our framework: a heuristic agent and a reinforcement learning agent. The heuristic agent uses hand-crafted rules and domain knowledge. This agent was developed by the authors of poke-env [22]. It is capable of estimating stats of opponents, determining the use of specific types of moves and the use of one-time battle mechanics, analyzing type effectiveness and STAB, and determining when to switch out to a different Pok\u00e9mon.\nWe additionally train a reinforcement learning agent using the PPO algorithm with Generalized Advantage Estimation due to its success in prior work [23]. This agent consists of two feedforward layers with 128, and 64 nodes respectively, with a standard ELU [24] activation function following each layer. We settled on this architecture through our initial experiments. In order to output both a policy and a value as required by the PPO algorithm, we utilize two output layers corresponding to the actor-head and the critic-head. The former is mapped to the 22 actions in the action space while the latter outputs a real numbered value for the given state. We use a fixed random seed of 42. The agent uses a discount factor of 0.95, a Generalized Advantage Estimation Lambda value of 0.9, a surrogate clipping parameter of 0.1, a value function clipping parameter of 0.1, and loss constants of 0.5 and 0.002. We use a batch size of 128 and an Adam optimizer with a learning rate of 2e-4. All battles occur via self-play, that is, the model faces itself. We perform 10,000 steps per rollout, train for 10 epochs after each rollout (for a total of 1,024,000 steps). One step is a single turn in a battle. All hyperparameters and the training process are adapted from prior work [23] with some empirical modifications."}, {"title": "E. Agent Evaluation", "content": "We perform two different evaluations of our model. The first is a qualifier used during our preliminary experiments the agent's winrate against a completely random agent. The heuristic agent won 993 of 1000 games while the RL agent won 978. Our second evaluation was to test the agents on the Pok\u00e9mon Showdown ladder. This lets us test the agents against human players and use Pok\u00e9mon Showdown's own metrics to determine if the agents meet our criteria of approximating an average human. One way to interpret this criteria would be to say an agent wins 50% of the battles it plays. To this end, we look at each agent's GXE (Glicko X-Act Estimate) value [25] after 100 battles on the ladder. GXE measures the odds of a player winning a battle against a randomly selected opponent from the ladder. The heuristic agent achieved a GXE of 49.4% while the RL agent reached a GXE of 42.4%. Thus the heuristic agent is closer to an average player while the RL agent is somewhat worse. However, we still use both agents for our future analyses to improve the breadth of our results."}, {"title": "V. TEAM-BUILDING", "content": "The remaining part of our Meta Discovery Framework is the team-builder, which attempts to approximate meta teams given the simulated battles of the Battle Agent. Since the overall meta is dependent on the most popular characters, the method through which teams are generated will influence the meta discovered through the framework. Thus we need to generate teams in a manner similar to that used by human players. This is true in the case of most games, not just Pok\u00e9mon. Our solution for this problem is inspired by prior work [8], [9], [15] and uses a combination of domain knowledge (type synergies, base stats) and metagame knowledge (pickrates, winrates). Domain knowledge such as a Pok\u00e9mon's type or stats are usually fixed and do not change with the meta. Metagame knowledge is constantly evolving as the metagame shifts.\nWe analyze the team-building problem from the lens of both the Analyzing Balance Changes in a Metagame (ABC-Meta) task and the Blank Slate Discovery of the metagame (BSD-Meta) task. In the case of the ABC-Meta task, we have access to the true metagame before a balance change. In the case of the BSD-Meta task, we do not have access to the true metagame and thus have to start from scratch. In both cases, the domain knowledge used remains fixed while the metagame knowledge is continually updated throughout the simulation.\nSimilar to prior work [8], [9], [15] we represent the team-building process sequentially, automatically picking one mem-ber of the team at each step. We call the team being built the Current Team and the set of available characters as the Pool. Initially the Current Team will be empty and at each step of the team-building process, one characters is added to it. In order to select a character to add to the team, we include several components using both domain and metagame knowledge. In the next section, we define these components.\nBefore building a team however, we first need to identify each individual Pok\u00e9mon's moveset (all the aspects discussed in Section II-C). To obtain this, we scrape publicly available Smogon data that compiles such movesets based on usage statistics. We acknowledge that this information would not be available for the BSD-Meta task and would instead require an alternative method. We leave this for future work."}, {"title": "A. Components", "content": "We first consider three statistics that use current metagame knowledge. All the statistics we consider have a value ranging from 0.0 to 1.0 and are computed on a per-Pok\u00e9mon basis in a vectorized manner. The first statistic is the pickrate which refers to the fraction of teams in which a Pok\u00e9mon has been picked. Since both teams in a battle can have the same Pok\u00e9mon, the pickrate for a Pok\u00e9mon is defined as the total number of picks divided by twice the number of battles.\n$\\text{Pickrate(X)} = \\frac{\\text{Num. Picks(X)}}{2 \\times \\text{Num. Battles}}$\nNext, the winrate refers to the fraction of battles that a Pok\u00e9mon wins. If a Pok\u00e9mon has never been picked, the winrate is automatically set to 0 to avoid divide-by-zero errors.\n$\\text{Winrate(X)} = \\frac{\\text{Num. Wins(X)}}{\\text{Num. Picks(X)}}$\nFinally, the popularity as defined by Rejim [15] is based on how frequently Pok\u00e9mon are used together. We alter this to be a measure of how frequently a Pok\u00e9mon wins when used alongside Pok\u00e9mon already in the Current Team. This is used only when the Current Team has at least one Pok\u00e9mon. The number of wins every Pok\u00e9mon has when used with every"}, {"title": "B. Algorithms", "content": "To build a team, we compute a score for every Pok\u00e9mon in the Pool. We then use this value to compute an approximation of the probability of a player picking each Pok\u00e9mon. We then sample according to these probabilities to pick a Pok\u00e9mon to add to the Current Team. We note that Pok\u00e9mon that are banned are included in the initial calculation of the score function, though the value is zeroed out before we compute the probability.\nSince the Analyzing Balance Changes (ABC-Meta) task attempts to approximate the true meta, we restrict ourselves only to the information to which a human player would have access. Specifically, these are pickrates, BST, Type Values and Meta Type Values. We do not use winrates or popularity as Pok\u00e9mon Showdown does not track winrates and the popularity measure is not easily accessible. We run a grid search (Appendix A) over these four components using the metrics defined in the Section VII-A to determine our final team-building algorithm. Our final score function for the ABC-Meta task is:\n$\\text{Score}_{\\text{ABC Meta}} = \\text{Pickrates} \\times \\text{BST}$\nFor the BSD-Meta task where we have no prior knowledge of the metagame, we simulate the meta from scratch. We thus use both the winrates and popularity as a replacement for prior knowledge of the meta. In addition, we do not use the pickrates. If we used the pickrates, then the team-building algorithm would depend on the pickrates and the pickrates (in this scenario) would depend on the team-building algorithm. This could cause an undesirable feedback loop which would result in a small set of Pok\u00e9mon constantly being selected. This is not ideal as the Pok\u00e9mon picked initially at random could significantly affect the final meta. Using the winrate instead breaks this feedback loop while also allowing the algorithm to prioritize stronger picks. Our score function is also different from the ABC-Meta task in that we use a sum instead of a product to arrive at the overall score. We make this change because we want to encourage diversity in the meta. Due to the addition of the popularity term, multiplying all three quantities would generally result in smaller values. The exceptions would be those Pok\u00e9mon with a high value in multiple terms. This poses a problem as it would result in the use of a smaller set of Pok\u00e9mon without sufficient exploration of the others. Thus we use a sum to increase the effect of each individual term. Our final score function for the BSD-Meta task is:\n$\\text{Score}_{\\text{BSD Meta}} = \\text{Winrates} + a \\times \\text{BST} + b \\times \\text{Meta Type Value} + c \\times \\text{Type Value} + \\text{Popularity}$\nWhere a, b, and c are constants that we set to 0.50, 0.25, and 0.25 respectively based on preliminary results.\nFor both the ABC-Meta and BSD-Meta tasks, we utilize an epsilon-greedy policy to pick Pok\u00e9mon. We greedily select a Pok\u00e9mon based on the scores derived above, but there is an epsilon chance of selecting a Pok\u00e9mon using the inverse pickrate, that is, selecting a less frequently used Pok\u00e9mon. This introduces an element of exploration in the team-building system that ensures that the system does not always generate the same set of teams. In the case of the ABC-Meta where we already have an idea of the meta, our epsilon value is fixed at 0.001. When calculated over the 12 Pok\u00e9mon in a battle, this means there is an approximately 1% chance of picking a Pok\u00e9mon based on the inverse pickrate. For the BSD-Meta where both the winrates and the popularity start with a default value of 0, we set the epsilon value to 1.0 and linearly decay it down to 0.001 over 20,000 battles. This allows for an initialization process where the system can obtain statistics for every legal Pok\u00e9mon."}, {"title": "VI. META DISCOVERY", "content": "The final component of our meta discovery framework is the simulator environment. This environment must simulate battles using the battle agent, generate teams using our team-building system and also track statistics such as pickrates and winrates.\nOur agents are setup to be able to run multiple battles concurrently. This may cause a race condition if we update our battle statistics at the end of each battle. Instead, we perform this update at regular intervals. To help reduce technical overhead, we also elect to generate a large number of teams at this point and simply sample a team at the start of a battle. The number of teams generated and the interval at which we update battle statistics is a parameter. We use an arbitrary value of 2500 generated teams and an update frequency of 1000 battles.\nSince our goal is to approximate the metagame on Pok\u00e9mon Showdown, we must follow its system of updating tiers every three months. Hence, we determine the number of battles needed to simulate for one month. The most popular non-random competitive metagame on Pok\u00e9mon Showdown is the OU ladder [5]. This averages to about 1.45 million battles a month. The second most popular metagame is comparatively dwarfed at an average of 130,000 battles for the Ubers tier. We elect to use a fixed size number of battles per month for simplicity and chose 150,000 battles per month. In total, we simulate 3 months worth of changes for each battle and thus we perform 450,000 battles (roughly 8-14 hours). In addition, while presence in a tier is determined by usage rates, in practice this means 34 to 40 Pok\u00e9mon are officially classified as a member of a tier. Thus we define the meta as the top 40 Pok\u00e9mon in a tier.\nFinally, we also implement a blanket ban of the Little Cup (LC) tier in our system. That is, the Pok\u00e9mon in the LC tier are not eligible to be picked by the team-building algorithm. We do this for two reasons. First, the Pok\u00e9mon in the LC tier are extremely weak in relation to the rest of the Pok\u00e9mon and can thus be easily identified. Second, they make up over a quarter of all the available Pok\u00e9mon (210 out of 740). As a result, our team-building algorithm would spend unnecessary time simulating battles between these Pok\u00e9mon."}, {"title": "VII. META DISCOVERY", "content": "We now have all three components of our meta discovery system. We have two battle agents, the self-play PPO agent and the heuristic agent, a team-building algorithm for both our tasks, and a simulator environment to run meta discovery. Our objective in the ABC-Meta task to analyze the effects on the metagame after a change, specifically a Pok\u00e9mon being banned. This is a common occurrence in Pok\u00e9mon Showdown [26"}]}