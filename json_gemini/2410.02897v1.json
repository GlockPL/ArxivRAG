{"title": "Cognitive Biases in Large Language Models for News Recommendation", "authors": ["Yougang Lyu", "Xiaoyu Zhang", "Zhaochun Ren", "Maarten de Rijke"], "abstract": "Despite large language models (LLMs) increasingly becoming important components of news recommender systems, employing LLMs in such systems introduces new risks, such as the influence of cognitive biases in LLMs. Cognitive biases refer to systematic patterns of deviation from norms or rationality in the judgment process, which can result in inaccurate outputs from LLMs, thus threatening the reliability of news recommender systems. Specifically, LLM-based news recommender systems affected by cognitive biases could lead to the propagation of misinformation, reinforcement of stereotypes, and the formation of echo chambers. In this paper, we explore the potential impact of multiple cognitive biases on LLM-based news recommender systems, including anchoring bias, framing bias, status quo bias and group attribution bias. Furthermore, to facilitate future research at improving the reliability of LLM-based news recommender systems, we discuss strategies to mitigate these biases through data augmentation, prompt engineering and learning algorithms aspects.", "sections": [{"title": "1. Background", "content": "Large language models (LLMs) are becoming crucial com-ponents of recommender systems [1, 2, 3, 4]. In particu-lar, news recommender systems rely heavily on LLMs toanalyze vast amounts of textual data, ensuring that usersreceive news articles that align with their interests and pref-erences [5, 6, 7].\nDespite their growing importance and effectiveness, thedeployment of LLMs in news recommender systems is notwithout risks [8, 9, 10, 11, 12]. One significant issue thathas emerged is the influence of cognitive biases in LLMs onthe ability to make decisions correctly [13, 14, 15, 16, 14].Cognitive biases are systematic patterns of deviation fromnorm or rationality in judgment, can lead to the productionof inaccurate or skewed outputs [17, 18]. Specifically, Itzhaket al. [19] find that LLMs fine-tuned on human-generateddata are significantly affected by cognitive biases during theinference phase, which seriously affects the reliability ofLLM-based news recommender systems.\nIdentifying and addressing the potential impact of cog-nitive biases in LLMs for recommender systems is critical,especially in the high-stake news recommendation task.News recommender systems play a crucial role in shapingpublic opinion, informing decision-making, and influenc-ing societal discourse [20]. Therefore, any distortion in thenews recommendation process caused by cognitive biasescan have far-reaching consequences, potentially spreadingmisinformation [21, 22], reinforcing stereotypes [23], orcontributing to echo chambers [24].\nConsequently, there is a pressing need to discuss the po-tential risks of cognitive bias in LLMs for news recommender systems and to explore possible solutions. This leads to ourtwo central research questions: (1) How do cognitive biases"}, {"title": "2. Risks of Cognitive Biases", "content": "News recommender systems aim to personalize the newsarticles presented to users, tailoring the recommendationbased on individual preferences, behavior, and historicaldata. LLM-based news recommender systems often leverageLLMs to filter and rank news articles, determining which ar-ticles to prioritize. However, LLMs trained on large amountsof human-generated data may inherit human cognitive bi-ases, reflecting similar patterns in their outputs [16, 19].Cognitive biases in LLMs can profoundly influence LLM-based news recommender systems, potentially leading toseveral adverse effects:\n\u2022 Anchoring bias [17]: The reliance on the first pieceof information (the \"anchor\") when making decisions.Assuming a conversational presentation mode of LLM-based news recommender systems, LLMs are likely to beinfluenced by users' initial interaction with a certain typeof news (e.g., a particular political viewpoint), which coulddisproportionately influence future recommendations.\n\u2022 Framing bias [25]: The way information is presented(framed) can influence decision-making and judgments.For example, news headlines and summaries framed in aparticular way can lead LLM-based news recommender systems to severely prefer these news articles.\n\u2022 Status quo bias [26]: The tendency to favor familiarcontent that has appeared in previous experiences. InLLM-based news recommender systems, LLM may prefernews articles that they have seen in the pre-training orfine-tuning stages, resulting in users finding it hard toview the latest news articles and reducing the diversityof information consumed.\n\u2022 Group attribution bias [27]: This type of bias refers"}, {"title": "3. Mitigating Strategies", "content": "LLM-based news recommender systems are trained on ex-tensive human-generated datasets, which inherently reflectcognitive biases present in human society. LLM-based newsrecommender systems might inadvertently absorb and repli-cate these biases, leading to outputs that may not only reflectbut also reinforce existing cognitive biases.\nTo address the cognitive biases of LLM-based news recom-mender systems, we propose several strategies to mitigatecognitive biases through data augmentation, prompt engi-neering and learning algorithms aspects. Below is a detaileda list of mitigating strategies:\n\u2022 Synthetic data augmentation: LLMs can be employedto generate synthetic datasets that are carefully crafted tobreak correlations between cognitive biases and irrationaloutputs [28, 29]. First, we can construct balanced synthetic datasets that counteract the skewed cognitive biaspattern existing in human-generated datasets. Then, wetrain LLM-based news recommender systems on balanceddatasets to reduce the influence of cognitive biases andgenerate more rational outputs. This strategy helps toreduce specific biases in human-generated datasets, suchas group attribution bias, by ensuring that the trainingdata represents a more diverse and equitable distributionof content.\n\u2022 Self-debiasing via iterative refinement: LLMs possessa self-refinement ability that can be harnessed throughspecially designed debiasing prompts. By iteratively refin-ing the outputs, the model can be guided to recognize andcorrect biases in its recommendations. For example, in thecontext of LLM-based news recommender systems, LLMscan be prompted to reconsider their choices and adjustthem to minimize the influence of cognitive biases. Thisprocess involves LLMs generating outputs, evaluatingoutputs against debiasing criteria, and revising outputs ifnecessary [30].\n\u2022 Cognitive debiasing through human feedback: After an LLM generates a news recommendation, humanevaluators can assess whether the output exhibits cogni-tive biases. If biases are detected, techniques such as di-rect preference optimization (DPO) [31] or reinforcementlearning from human feedback (RLHF) [32] can be em-ployed to adjust the model's behavior. This learning fromthe human feedback process helps to align the model'soutputs more closely with human ethical standards andreduces the likelihood of biased recommendations in thefuture. By integrating human feedback into the train-ing loop, the model learns to prioritize objectivity andfairness, thus mitigating the effects of cognitive biases.\nOverall, LLM-based news recommender systems fine-tuned on extensive human-generated data are susceptibleto cognitive biases. However, developing strategies such assynthetic data augmentation, self-debiasing via iterative re-finement, and cognitive debiasing through human feedbackcan help mitigate these biases, fostering more rational andequitable outputs. These strategies are essential for ensuringthat LLMs serve as reliable components in decision-makingprocesses for news recommendations."}, {"title": "4. Related Work", "content": "Large language models (LLMs) have become essentialcomponents of news recommender systems due to theiradvanced generation abilities. Li et al. [33] directlyprompt ChatGPT [34] to generate news recommendations.Prompt4NR [35] introduces the prompt learning paradigmto news recommendation by reframing the task of predictinguser clicks on news articles as a cloze-style mask-predictionproblem. PGNR [36] transfers the personalized news rec-ommendation task into a text-to-text generation task for LLMs,following a generative training and inference paradigm thatdirectly generates recommendations. ONCE [37] investi-gates the integration of both open- and closed-source LLMsto improve news recommendation. While the deployment ofLLMs into news recommender systems has led to significantimprovements, previous works ignore the potential risks as-sociated with cognitive bias in LLMs for news recommender systems.\nRecent studies have shown that LLMs are affected by cog-nitive biases when making decisions [13, 38, 39, 40, 41, 42,43, 44]. Jones and Steinhardt [13] identify that error patternsof GPT-3 [45] resemble human cognitive biases. Similarly,Agrawal et al. [38] discover the framing effect bias of GPT-3 [45] in the clinical information extraction task. Itzhaket al. [19] suggest that LLMs develop emergent cognitivebiases after fine-tuning on extensive human-generated data.Furthermore, Echterhoff et al. [16] introduce a frameworkfor the quantitative evaluation of cognitive biases in LLMswithin a student admissions task. To the best of our knowl-edge, we are the first to discuss the influence of cognitivebiases and mitigation strategies in LLM-based news recom-mender systems."}, {"title": "5. Conclusion", "content": "In this study, we focus on exploring possible risks of cog-nitive biases in LLM-based recommender systems and dis-cussing potential mitigating techniques. Given the criticalrole news recommender systems play in creating publicopinion and influencing social discourse, any distortionbrought about by cognitive biases could have far-reachingeffects. To explore the risks of cognitive bias, we have in-troduced the general definitions of anchoring bias, fram-ing bias, status quo bias and group attribution bias, andhow they specifically affect the decision-making process ofLLM-based news recommender systems. Our discussion ofthe cognitive biases inherent in LLMs reveals that, withoutcareful deployment, these biases can lead to skewed andincorrect outputs in news recommendations. Our analysisof the cognitive biases in LLM-based news recommender"}]}