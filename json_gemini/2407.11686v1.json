{"title": "CCOE: A COMPACT LLM WITH COLLABORATION OF EXPERTS", "authors": ["Shaomang Huang", "Jianfeng Pan", "Hanzhong Zheng"], "abstract": "In the domain of Large Language Model (LLM), LLMs demonstrate significant capabilities in natural language understanding and generation. With the growing needs of applying LLMs on various domains, it is a research question that how to efficiently train and build a model that has expertise in different domains but with a low training cost. We propose CCoE architecture, a framework of easily coupling multiple strong domain experts together to fuse into a big LLM, provides a collective way of utilizing the different domain expert LLMs. Besides, training a large collaborative of multiple expert LLMs requires a high requirements on training sources. CCoE bypasses this problem through isolating other experts and train each expert separately. The design of CCoE assembles multiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoE layer could have one or more expert LLMs. Expert LLMs have different number of layers and have been well-trained for different domain tasks. Each expert is fine-tuned to be able to achieve the comparable results with SOTA domain LLMs. We start from 5 experts in the domain of Code, Math, Law, text-to-SQL and Medical. The results indicate that our CCoE framework can easily and efficiently boost nearly 10%-20% performance on original base model in different domains but using less resources on training, as well as inference.", "sections": [{"title": "1 Introduction", "content": "Large Language models (LLMs) have shown great advances in diverse domain tasks, primarily due to they have been trained with vast amount of high quality data and aligned with human preferences. The pre-training and human preference alignment process enable LLMs to demonstrate the extraordinary capabilities in natural language process and problem-solving. Despite the great success, most existing open-sources LLMs are still far way from giving satisfactory results in solving problems like code, mathematics because those domains requires a strong reasoning ability. Another limitations of pre-trained LLMs is the lacking of specialized domain knowledge like Medical due to limited domain corpus in pre-training phrase. To improve LLM's abilities in mathematical reasoning, code generation or knowledge in specialized domain, these can be further enhanced through supervised fine-tuning.\nFrom another perspective, the current LLMs shows that they tend to demonstrate to have strength and weakness in various tasks and domains based on their evaluations results. some LLMs are excellent in common sense logical reasoning [1]. Other LLMs are well-suited for code generation [2] or mathematical problem solving [3], [4]. However, it is great challenging task for training a model that is well-suited in all different domains. This intrigues to ask the question: Can we boost the model's performance in all domains with low training cost?\nRecent work focuses on collaborating or assembling multiple LLMs with different sizes to form a more capable and robust model becomes a new existing research direction. Since different LLMs demonstrate the strength and weakness in different tasks and fields, it has great potentials to utilize the different LLMs's strength in various tasks. [5] [6]. Another framework is to ensemble multiple LLMs' responses at each decoding step [7] or creating a voting mechanism to select the top answer at the final step. This has already done to use multiple LLMs to generate training dataset [8] or final answer [9]. These recent work indicates the phenomenon of collaborativeness that allows LLMs working"}, {"title": "2 Analogy to Mixture of Experts", "content": "The MoE network usually built based on Transformer architecture that mainly consists of one attention layer, one Feedforward layer and two normalization layers. A standard MoE layers contains multiple Feedfoward layer $E_1,..., E_n$ and each Feedforward layer is an Expert $E_i$. Each token is generally assigned to one or two experts. The gating network decides the token assignment for each expert, which is demonstrated in Equation (1).\n$y = \\sum_{i=1}^{n} G_i(x) E_i(x)$ (1)\n, where $G_i(x)$ and $E_i(x)$ are the outputs of the gating network and the i-th expert with the given input $x$. Commonly, $G(x)$ is a sparse vector so that there is not need to compute $E_i(x)$ when $G_i(x)$ has the value of 0. The design of gating network can simply uses a Softmax gating [10]. It multiplies a trainable weight matrix on the input and then applies the Softmax function (Equation 2).\n$G(x) = Softmax(x * W_g)$ (2)\nSoftmax design of gating can easily introduce the expert unbalancing problem during the training: the routing network always select one or two experts during the training, whereas rest of experts cannot update their weights. Noisy Top-K Gating can mitigate this problem [11] (Equation 3).\n$G(x) = Softmax(TopK(H(x), k))$ \n$H(x) = (x W_g)_i + StdNorm() \\cdot Softplus(x \\cdot W_{noise} + i)$\n$TopK(v, k)_i = \\begin{cases}\nvi, & \\text{Vi } \\in \\text{ TopK of v} \\\\\n-\\infty, & otherwise.\n\\end{cases}$ (3)\nDuring the training, the effect of noisy efficiently avoids the problem that the gating network always selects the same expert. From the high-level perspective, our proposed CCoE framework integrates the experts at the model level, whereas the MoE builds the experts at the layer level (Feedfoward layers). With coupling to the base model, each expert can be viewed as a complete and compact LLM that is trained for specific downstream task. Although CCoE and MoE all have the specialized sub-networks within a single model, our CCoE clearly clarifies the functionality of each"}, {"title": "3 Dilemma of Supervised Fine-tuning LLM", "content": "Large Language model (LLM) Supervised fine-tuning (SFT) is the process of continually training the pre-trained models on smaller, specific datasets to refine their capabilities and improve the performance in a specific task. SFT can also be used to inject new domain knowledge into the model. The SFT process is about turning a general-purpose model and transform it into specialized model with the domain-related dataset. For example, there is a limited knowledge in pre-trained model on law domain. We can inject the knowledge of the law into LLM to make it become a law specialized model. The SFT brings a gap between pre-trained model and fine-tuned model. Nowadays, LLM fine-tuning has become an indispensable approach for enterprises to enhance their operational process. Through training LLMs for specific task with domain datasets, we can push the knowledge of LLMs to the boundaries of different areas [12] [13] [14].\nThere are many ad-hoc attempts on SFT for enhancing LLM performance on individual capabilities in open community. However, when the widely used LLMs have been fine-tuned, it may affect the general knowledge stored in LLMs since there is a distribution shift between SFT data and the original training data. The study of SFT is crucial for certain practical applications of LLMs. In most real cases, users want the LLMs to be enhanced in specific domain capabilities while preserving their general capabilities. A significant challenge in this paradigm is catastrophic forgetting (CF), in which a model forgets previously learned knowledge during the SFT process [15] [16]. The naive way of reducing the CF is to mix the supervised data with the pre-trained data at certain ratios. Recently, many researches have proposed various approaches to alleviate the problem of CF such as Dual-Stage Mixed Fine-tuning [17], Recall and Learning [18], etc. However, there are still many remaining challenges left to the community."}, {"title": "4 Collaboration of Expert", "content": "Section 4 discusses the methodology of CCoE from the design and implementation details. We first demonstrate the CCoE structure and its variations. We also present that how CCoE solves the dilemma of LLM SFT from the design perspective. Then, we study the objectives of formulating the CCoE model to boost the overall performance of base model and the problem of balancing the performance and resources."}, {"title": "4.1 Design of CCoE", "content": "CCoE mainly adopts the idea of Collaboration-of-Experts. In Figure 1, CCoE consists of one backbone LLM and n domain experts and can be formed as a total of n+1 complete LLMs. We define the expert as the sub-network of the LLM. A CoE layer is a layer that is conjunct of experts. Each CoE layer could have different number of experts based on overall design. Demonstrated in Figure 1, the first advantage of CCoE framework is the flexibility in terms of expert size and task. Experts could have different sizes. This indicates that each expert could have different number of parameters to balance the performance and the resources. To be more specifically, LLMs face inherent constraints on model size and training data. Some downstream domains may not have enough tokens to train a large size LLM. A small size LLM may be more suitable in this case. A large LLM may be \"a storm in the teacup\" for some simple tasks. For example, sentiment analysis task is relatively easy comparing to natural language generation and inference. Many recent researches have proposed to use bert-based model instead of LLMs [19].\nIn CCoE, each expert is coupled at the CoE layer. Given a n-layer backbone LLM, each layer could be the CoE layer and each CoE layer could have any number of experts. This demonstrates the scalability of CCoE framework that each CCoE model is able to scale up to as many as number of experts until meets the constraints of computational resources. The third advantage of LLM is expert isolation. CCoE framework allows each expert to train separately. The training corpus of LLM has the issue of being out-of-date or we want LLMs to continue to learn. This induces the concept of continual pre-train that aims to fine-tune the LLM with new knowledge. However, fine-tuning the entire LLM usually is costly especially for those LLMs that have large sizes. CCoE framework can bypass this by continual training one of the expert LLMs or add more experts instead of training the entire model."}, {"title": "4.2 Rule-based Gating", "content": "The inference procedure of CCoE firstly uses the shared layers to extract the upper level features of the inputs such as the syntax information, general linguistic patterns, etc. and then select one of experts for each query. To select the correct expert in CCoE, we need to know three information: the expert-domain query mapping, expert's number layers and the position mask at expert layer because there could be several experts have the same number of layers. Another design requirement is to support flexibility. Since CCoE supports to continual train expert or add new expert, the CoE layers are dynamically changing. This requires the minimal change of our rule-based gating policy, if there is an update on one expert's number of layers. We can rewrite the problem of rule-based gating as:\nConsider a set of domain query prompts $Q = [q_1, q_2,\u2026\u2026, q_n]$ and a set of candidate experts $E$. The objective is to identify a sequence of executing paths $P = [P_1, P_2,\u00b7\u00b7\u00b7, P_n]$, where $P_i$ contains the triplet $(p_i, l_i, pos_i)$, and $y_i$ is $domain_j$-to-experti mapping, $l_i$ is domain experti' number of layers and $pos_i$ is the index of experti at $L - l_i$ layer. For the simplicity, we use a rule-based task-expert matrix during the inference phrase. The matrix is a zero/one matrix $A_{mxn}$, where m is number of task and n is the number of experts. At each $A_{i,j}$, it is a tuple structure that contains the"}, {"title": "4.3 Expert Training", "content": "One of the main difficult in training MoE model is to balance the expert loads to prevent the case that tokens are always assigned to the same experts. Another problem is the knowledge redundancy. Knowledge sharing among experts means that different experts may have the repeated knowledge in their parameters. This leads to redundancy in expert parameters and prevent the expert specialization. To support specialized expert training, CCoE defines two types of operations: \"push\" and \"pop\". \"push\" means to add/update an expert in CCoE framework, and \"pop\" means deep copy/remove an expert in CCoE.\nIn CCoE, the training can be divided into scenarios: the first scenario is to train a new expert LLM with a new domain data (Figure 1). It loads the new expert LLM n into the gpu and feeds the domain tokens. After training, it creates a deep copy of the new expert n' in CCoE and transfers the weights to the n'. After all, it will destroy the expert LLM n and release the memory. Another scenario is to continually train an expert LLM (Figure 2) to allow the expert LLM to continually learn the domain knowledge. This can be summarized with 3 steps. Step 1. Creating a deep copy of expert n at gpu 8 as expert n'. Step 2. Feeding the domain tokens to continually train expert n'. Step 3. Transferring the new weights of expert n' to CCoE expert n, destroy the expert n'. The operations of \"push\" and \"pop\" support CCoE to add and continual train experts."}, {"title": "4.4 Expert LLM Architecture", "content": "The number of parameters of language model can varied based on the difficulty of the target tasks. Based on the scaling law, the model's performance will improve when we increase the amount of model's parameters and the training data. This enables model to continually increasing capabilities as we following the scaling law. However, in real-world scenario, it is a question that do we need to spend such amount of resources to train a giant language model for all downstream tasks, or a small model can achieve the comparable results with enough training tokens and training time. On the other side, recent research also realizes the deficit of training the large language model with large sizes such as the unstable training or token deficits. The training token should maintain the same ratio with sizes of large language models [21]. In most of case, dataset for downstream fine-tuning tasks may not have enough tokens, or small model can also achieve a comparable results as the large models and it is more economic in terms of training resources. Therefore, we defined a performance gains metrics to evaluate the improvement in expert performance with respect to the increase of expert parameters. This can be easily formulated as $\\frac{-\\Delta e_{val}}{\\Delta \\theta}$, and $\\theta$ are the validation error and expert model's"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Training Data", "content": "We collect 5 different domain datasets: Math, Code, Finance, law, Medical, and Text-to-SQL. Each domain focuses on enhancing model capabilities by injecting domain knowledge or providing reasoning training to experts. In our training corpus, we mainly used the publicly available and high quality datasets. The data composition is illustrated at Figure 3.\nMath. Mathematical reasoning mainly relies on model's inference capabilities and already lead the various approaches to improve the performance on math-related tasks. In our approach, we selected the recent math training dataset that has been shown to effectively boost model's math performance. [3]. Besides, using the Chain-of-thought and RLHF technologies can further improve model's math performance [26] [4]. We also constructed our training dataset by providing a step-by-step solution way rather than the vanilla format, and apply the dpo training to align with the preferences. In the our math dataset, we collected nearly 400K instructions mainly from MetaMathQA.\nCode. The code generation usually combines the execution results with structured code. These pairs are verified and filtered by code interpreter before being used to fine-tune large language models. To optimize the model's reasoning capabilities, the data is usually formatted with COT prompting. We adopt the commonly used datasets from code instruction 128K alpaca 3, code feedback [27] and MBPP [28]. Our merged dataset includes various programming languages and covers major programming topics. We organized the fine-tuning data into instruction-based format with over 200K number of pairs.\nText-to-SQL. The Text-to-SQL (NL2SQL) is a well-known challenge in NLP and database community. It targets to translate the natural language input into corresponding and valid Structured Query Language (SQL). It requires the LLM to understand the semantic purpose of natural language input, the target database schema and the grammar of SQL. Current proposed solution of this task includes SFT, or craftily designed prompt with paradigm of \"prompt\" learning or \"in-context learning\". We explore the SFT paradigm to empower LLM Text-to-SQL capability. We collect our dataset with total of 133K instruction-based pair and mainly from WikiSQL [29], Chase [30], and DuSQ1 [31].\nMedical. The challenges of developing LLM-based medical applications are lack of factual inaccuracies, background knowledge for diagnoses, and insufficient grounding in real-world experience. There are many elaborately collected medical datasets that aim to optimize LLM capabilities in medical domain. We adapt the medical data from MedQA [32], MedicalGPT [33], and HuaTuo [34], and create a over 849K English and Chinese QA-paired medical dataset that mixes English and Chinese.\nlaw. We mainly focus on the Chinese mainland law system because there is a possibility that some cases could have different decisions under the Common Law and Civil Law. We collected the instruction-based Civil Law dataset from some influential projects [35] [36]. Our law dataset includes a total of nearly 300K instructions-based pairs that involved"}, {"title": "5.2 Data processing and training", "content": "The quality for open-sourced dataset is critical for training expert models. We firstly use Min-Hash and 'text-embedding-ada-002' embedding model to remove highly similar questions. In downstream fine-tuning community, domain datasets often mix with pre-train dataset to avoid degradation of model's general capabilities or they train the model with multiple-stages. However, in CCoE framework, each expert LLM fine-tuning only need to focus on learning the domain data. Our dataset preparation only needs to augment the domain data quality. We aim to overcome this through three dimensions: 1. comprehensive 2. complex 3. diverse. Based on our deliberately explorations on domain datasets, the Math dataset is mainly sampled from MetaMath [3]. It uses the bootstrapping approach to rephrase the math question with enough complexity, diversity and comphrensitiviy.\nThe code dataset has the problem of either missing CoT steps or test cases, we use the GPT-4 to create the CoT and test cases. We also ask model to verify the success of test cases for better reasoning ability. As for the Law, Medical and Finance domain, we intuitively filter the answer based on the length of question. We create different types of questions including QA, document summarizing, multiple choices, Cloze test, and so on. Previous experiments already indicate the effectiveness of DPO. As for text-to-SQL, many previous research has adopted the approach of CoH (Chain-of-Hindsight) [37] with LLM to achieve a high scores on the evaluation. The main idea of CoH is to construct the sequence by adding the question and human-ranked answers with prompts. For example, \"Please explain why earth is a sphere? A good answer is {good answer} A bad answer is {bad answer}\". The objective is to let the model to predict the sequence with masking the template tokens. We first evaluate the baseline performance of Qwen1.5 models on Text-to-SQL with the text representation prompt [38], which adds the database schema and question into the prompt. As for fine-tuning, we adopt the Alpaca-SFT prompt that asks LLM to follow instruction and finish task in markdown format.\nWe also conducted the DPO training on Math, Code domains. To create the direct preference pairs, we ask the expert model to generate 5 outputs including the exact answers and reasoning steps with a sampling temperature of 0.8. We adopt the template form [39] to rank the answers and reasoning steps based on the dimensions of completeness, correctness, and readability. Comparing to directly rank the 5 model outputs, we decide to use the pairwise ranking to have a better stable ranking result. We conducted pairwise ranking among them and manually solved the conflicts if there are multiple outputs have the same ranking position or a loop in the ranking positions. We use the highest ranked output as the chosen answer and lowest ranked output as the rejected answer."}, {"title": "5.3 Model Settings and Evaluation", "content": "We use QWen1.5 [40] 7B and 13B parameters as our backbone models. To simplify our training process, we do not expand the vocabulary size and continue to train the word embedding for each domain. We conducted a comprehensive experiments on hyper-parameters L, the number of layer in each expert. We explored the model performance gains when we increase the L. In our context, we want to limit the layers of each expert to the upper bound $\\frac{L}{2}$ because"}, {"title": "5.4 Limitations", "content": "Our proposed CCoE framework depends on adding expert layers into the base model to form different experts. In this work, we only conducted preliminary study on injecting knowledge to the last several layers. This may not allow the model to give the best performance. More explorations need to be done on training strategies and data compositions, or more comprehensive experiments are needed to identify the best expert layers, which could be upper, middle or distributed among the base model. We could utilize the similar approach as locating the Knowledge Neurons to find out the most appropriate combination of expert layers that enable the expert to achieve the best performance. Furthermore, we also realize that there is the performance upper bound under the constraint of only fine-tuning the half of the base layers. However, we believe that it is a trade-off between the model's performance and model's size."}, {"title": "6 Summary and Future Work", "content": "In this paper, we propose the CCoE framework that aims to boost the base model's overall performance in all perspectives through utilizing the idea of collaborating multiple of experts (CoE) LLM layers. All of the expert LLM layers are coupled at the base model and becomes the expert sub-network. We conducted the experiments on Math, Code, Law, Medical and Text-to-SQL domains. Our results indicate that there are at nearly 10%-20% improvements on these 5 downstream domains. This proves that CCoE framework is able to boost the overall performance of the base model with the advantages of low-cost, high-interpretability and efficiency. Moreover, CCoE also has the high extensibility through scaling up the number of expert layers to fulfill more open-ended tasks. However, the current routing of the expert layers depends on the API endpoints trace. In the future, We plan to use the LLM to help us route the prompt to the correct expert sub-network. We can extend the prompt's content to ask the base model to classify the prompt's task or domain during the first generation and then run the second inference to route the real prompt to the correct expert"}]}