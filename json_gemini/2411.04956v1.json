{"title": "Uncovering Hidden Subspaces in Video Diffusion Models Using Re-Identification", "authors": ["Mischa Dombrowski", "Hadrien Reynaud", "Bernhard Kainz"], "abstract": "Latent Video Diffusion Models can easily deceive casual observers and domain experts alike thanks to the produced image quality and temporal consistency. Beyond entertainment, this creates opportunities around safe data sharing of fully synthetic datasets, which are crucial in healthcare, as well as other domains relying on sensitive personal information. However, privacy concerns with this approach have not fully been addressed yet, and models trained on synthetic data for specific downstream tasks still perform worse than those trained on real data. This discrepancy may be partly due to the sampling space being a subspace of the training videos, effectively reducing the training data size for downstream models. Additionally, the reduced temporal consistency when generating long videos could be a contributing factor.\nIn this paper, we first show that training privacy preserving models in latent space is computationally more efficient and generalize better. Furthermore, to investigate downstream degradation factors, we propose to use a re-identification model, previously employed as a privacy preservation filter. We demonstrate that it is sufficient to train this model on the latent space of the video generator. Subsequently, we use these models to evaluate the subspace covered by synthetic video datasets and thus introduce a new way to measure the faithfulness of generative machine learning models. We focus on a specific application in healthcare \u2013 echocardiography \u2013 to illustrate the effectiveness of our novel methods. Our findings indicate that only up to 30.8% of the training videos are learned in latent video diffusion models, which could explain the lack of performance when training downstream tasks on synthetic data.", "sections": [{"title": "1. Introduction", "content": "Latent Video Diffusion Models became popular due to their realistic depiction of synthetic scenes, mostly framed as text-to-video tasks [4]. However, video synthesis also has important applications in domains beyond entertainment, such as healthcare. Analysis of diagnostic counterfactuals [34], faithful representation of anatomy [30], automated diagnosis [26] or data enrichment and domain generalization [20,22] are only a few examples. Images and videos are the most prevalent data structure for point-of-care and clinical diagnosis of disease with medical ultrasound imaging. The most common applications are prenatal screening and echocardiography. Echocardiography is of crucial importance to prevent avoidable deaths caused by cardiac disease. Ejection fraction is one of the main diagnostic measurement in echocardiography due to its correlation to ventricular function assessment [31]. It describes the ratio of change in cardiac volume during different phases of the cardiac cycle [27]. For example, it serves as an important marker for chemotherapy dosing in pediatric ultrasound [24] and pacemaker placement [37]. Consequently, interest in the automatic assessment of ejection fraction from videos has grown [6, 27, 35]. Similar to other medical fields, these models suffer from challenges related to difficult data acquisition processes, as well as robustness and domain generalization problems [27]."}, {"title": null, "content": "One potential solution is the use of generative models to learn and generate videos from these domains, with the added potential to publish the data with privacy guarantees [11,32]. This option got traction with recent advances in the quality of video generation, which predominantly operate in a latent space [2, 4, 5, 17, 21, 32].\nTo publicly release these synthetic videos, we need privacy measures to ensure that the synthetically generated videos are not memorized from the training data, which could lead to privacy concerns. While current approaches have begun to apply these techniques to ensure privacy [7,32], there has been no evidence that we can directly ensure privacy in the latent space of diffusion models.\nContribution: Our contributions can be divided into two parts. First, we demonstrate that training privacy models in latent space is computationally more efficient and results in better generalization. This is illustrated in Fig. 1. Secondly, we are the first to use a model trained for privacy to evaluate the learned subspace of the training dataset and the temporal consistency of the generated videos. In summary, we achieve the following:"}, {"title": null, "content": "1. We are the first to evaluate the feasibility of privacy-preserving models on synthetic videos, providing evidence that privatizing the latent model directly is sufficient to ensure the privacy of the entire video model.\n2. We show that training identification models in the latent space is computationally more effective than training them in image space. This approach also enhances the robustness of the learned representation of the privacy model, improving its generalization potential to new datasets.\n3. We establish a privacy filter that is simple to interpret and computationally efficient. We find that using the prediction head directly works better than taking the correlation coefficient.\n4. We illustrate how trained privacy models can be used to evaluate several other quality measurements, including the temporal consistency of diffusion-generated videos and the completeness of the generated subspace.\n5. We reveal that current generative models tend to only learn a distinct subspace of the training set, which might be the main reason for reduced performance of downstream models trained on synthetic data."}, {"title": "2. Related Work", "content": "Latent Generative Models: Recent advancements in diffusion models have significantly increased interest in image and video synthesis. Key improvements, such as enhancements in the noise schedule, the learning target, and the sampling procedure, have contributed to their success [9, 18, 39]. The most substantial leap was the idea of performing image generation in latent space [36]. Instead of generating images directly, models are trained to generate image embeddings that can be decoded by a separate model. Despite the added complexity of this two-stage process, the improvements in computational efficiency and feasibility have made latent generative models the de facto standard for image and video generation. [29] leveraged these advancements to train a 3D generative model and published a synthetic dataset of brain images. They used a dataset of 31740 images to train and sample a conditional diffusion model designed to generate synthetic images conditioned on different physiological variables. They were the first to demonstrate the potential of large-scale synthetic datasharing which raised concerns about the effects on patient-privacy.\nPrivacy: There are two main approaches currently used to ensure privacy. The first is to guarantee the privacy of the model itself, while the second ensures that the published data is private. Guaranteeing model privacy is typically achieved using differential privacy, which mathematically guarantees privacy by altering the input data or the optimization process during training [10]. The advantage is that it allows for a model budget, which can be used to trade-off between performance and privacy guarantees. However, this trade-off is often difficult to interpret, and the drop in performance can be too significant for the model to be practical. Other approaches include ensuring a low likelihood of models producing unique features [11] or enhancing the generalization capabilities of diffusion models, which are closely related to memorization [25].\nThe alternative approach is to guarantee that the produced dataset is privacy-compliant, which is more straightforward as the dataset, and hence the search space for privacy concerns, is inherently limited [15,32]. The most common approach to assess privacy is to use a re-identification model, which is trained to predict whether two scans come from the same person or not [7, 15, 28, 32]. Although this approach does not account for the need for training images in re-identification models, it provides very strong privacy guarantees. Therefore, we will follow the same approach to validate the privacy of our generated videos.\nTraining on Synthetic Data: There is substantial evidence that generative data is still not as effective as real data. One example is the recursive application of diffusion models on their own datasets, as shown in [1,38]. These applications demonstrate that while the generated datasets are of high sample quality, they still contain hidden features that successive models will pick up and amplify. [1] show that recursively applying these models leads to an increasing mode shift and reduced diversity.\nAnother key piece of evidence is that models trained on synthetic data currently do not achieve the same performance levels on specific downstream tasks, such as classification and segmentation, as models trained on real data. The benchmark for fully synthetic datasets remains their real counterparts [14, 15, 32]. This aligns with the original motivation of generative models to augment and impute existing datasets, but it currently hinders the feasibility of using these models for data sharing.\nWe hypothesize that part of the reason for this performance gap is that the methods currently used for quantifying generative models do not successfully capture all the relevant properties necessary to outperform real datasets. This is in line with concurrent work that critisizes and tries to improve on metrics used to evaluate generative models [11, 23, 40]. A more thorough analysis of the generated datasets is necessary to bridge this gap."}, {"title": "3. Method", "content": "To generate synthetic datasets while preserving privacy, we first train a generative model Go(cs) to learn the data distribution Pdata(X|Cs) and subsequently sample from this model. Cs is the variable that we want to learn to predict from a purely synthetic dataset. The process begins with a dataset X consisting of real videos X \u2208 Rlxcxhxw. We split this dataset into two disjoint proper subsets Xtrain and Xtest. The model S4(cs) is then trained to produce a synthetic dataset Xsyn. To ensure privacy, we apply a privacy filter S to obtain an anonymized dataset Dano. For the downstream task, we aim to predict p(cs X) and evaluate the model's performance on real data.\nCurrent data generation methodologies predominantly operate in latent space [32, 36]. This offers several advantages: it allows quicker training, lowers computational requirements, enables faster sampling, requires less data, and the two-stage approach allows for information compression. Consequently, the generative model can concentrate on learning the most relevant information. Therefore, we take the variational autoencoder (VAE) from [32] which is based on [36]. The VAE is trained on the task of image reconstruction. This means that we split the videos into frames xt where t denotes the frame number t \u2208 {1, ...,l}. The architecture consists of an encoder Enc and a decoder Dec. The purpose of the encoder is to compress the input into a bottleneck latent representation z\u0142 which can then be used as input to the decoder to reconstruct the original frame Xt, i.e., xt = Dec(Enc(xt)) = Dec(zt). The latent representation zt has three downsampling layers, which means that it is only of the size in each physical dimension and has a channel size of four, which is equal to a total compression factor of 48. Each latent feature consists of a mean and a variance, thus they represent a Gaussian distribution from which we can sample.\nThe VAE is optimized to retain perceptual quality. First, we employ two reconstruction-based losses, which are a standard L\u2081 loss and LPIPS [42], with a patch-wise learned feature extraction-based loss. To retain a small latent space, a low weighted Kullback-Leibler loss between z\u0142 and a standard normal distribution for regularization is applied. Additionally, an adversarial loss from a patch-based [19] discriminator Ry is trained on distinguishing between real and reconstructed images [12, 13]. In summary, this leads to:"}, {"title": null, "content": "LVAE = minEnc, Dec maxRy(Lrec(xt, xt) - Ladv (t)+logR(xt, xt) + Lreg(zt). (1)\nThe latent representation enables quicker training and sampling from the diffusion model, which is trained on the latent videos Z = Enc(X) encoded frame-by-frame by the VAE.\nGenerative Model: We train, sample and use the same architecture for the diffusion models as discussed in [32], which describes the state-of-the-art for generating synthetic medical ultrasound videos. Importantly, our generative models work entirely in the latent space, i.e., they are trained on latent videos Z and produce synthetic latent video Z'. The architecture consists of two parts: a latent image diffusion model (LIDM) and a latent video diffusion model (LVDM). The LIDM ge is an unconditional latent diffusion model trained on single frames from videos z\u0142 to generate synthetic frames z. The goal is to use them as conditioning for the anatomy of the synthetic videos. The LVDM Go (cs, z) is conditioned on a synthetic conditioning frame z and a regression value cs, which in our case is an ejection fraction (EF) score: a standard parameter for the systolic function of the heart [31]. From these synthetic videos, we can train a downstream model to predict EF and test on videos from real hearts.\nLatent Privacy Model: Since we are working on videos, unlike existing privacy methods [7, 28], we do not rely on augmentation to learn meaningful representations to privatize our data. Instead, we can take different frames from the same video as augmentations to train a self-supervised feature extractor, that will learn to differentiate between different anatomies. As backbone, we follow the architecture proposed by [28] to train a siamese neural network model S(zt, 2t) for binary classification of whether the latents zt and 2t come from the same video. The feature encoding part of the architecture, previously referred to as our filter F, is a ResNet-50 [16] pre-trained on Imagenet [8]. This feature encoder F computes the feature representation fz,t of each latent input frame zt. The final prediction is as follows:"}, {"title": null, "content": "S(Zt, 2t') = \u03c3(MLP(|F(zt) \u2013 F(\u017ct')|)) = P(F(zt), F(2t')) = P(fz,t, fz,t'), (2)\nwhere P can be seen as the predictor function that considers"}, {"title": "4. Experiments", "content": "Datasets: EchoNet-Dynamic is a dataset consisting of 10 030 ultrasound videos from unique patients [27]. We apply the official data splits with 7465 patients for training, 1277 for validation and 1288 for testing. For each video, we have a ground-truth ejection fraction value available. The videos are standardized to a resolution of 112 x 112 pixels, and have a variable number of frame. EchoNet-Pediatric is an ultrasound video dataset, specifically built from scans of younger patients ranging from 0 to 18 years old. The dataset is split into apical 4-chamber (A4C) and parasternal short-axis (PSAX) views [31].\nMetrics: To assess the feasibility of measuring temporal consistency, we compute the mean correlation coefficient (MCC) according to:"}, {"title": null, "content": "MCC = \u03a3ZEDtest \u03a3t\u2208 {1,...,l} \u03a3t'\u2208{1,...,l} corr(fz,t, fz,t'), (3)\nwhere corr is the Pearson's correlation coefficient. To assess the performance of the privacy model, we compute the area under the receiver operating characteristic curve (AUC-ROC). To compute the accuracy of the regression model for the downstream task, we compute the mean absolute error (MAE), the root mean squared error (RMSE) and the coefficient of determination R2."}, {"title": "4.1. Superiority of Latent Privacy Models", "content": "First, we compare latent and image space privacy models. We show that both are equally good on a training set, but the latent model demonstrates better generalization capabilities. For each of the three datasets (EchoNet-Dynamic, EchoNet-Pediatric A4C and EchoNet-Pediatric PSAX), we train two privacy models S. One is trained directly on the videos X from the training set Xtrain and the second one is trained on the latent videos Z using E. We train the image space models for 1000 epochs with a batch size of 128 and an early stopping set at 50 epochs, following to [28]. The latent space models run with a larger batch size of 1024, a higher learning rate of 5e-4, and a more patient early stopping set to 150 epochs. For testing, we go through the test dataset and randomly select either a frame from the same video or a different video with equal probability.\nComputational improvements by using embeddings: A single epoch takes roughly three minutes in image space, which amounts to a total of 50 hours of training. In latent space, training the model only takes three seconds for a single epoch, which only adds up to 50 minutes per training run and an overall computational time improvement of 60x. Furthermore, we can keep the precomputed latent representation of the image in memory, which is only roughly 1/64 of the memory consumption. We report the results in Sec. 4.1 and Fig. 3. The results show that both"}, {"title": null, "content": "models obtain an almost perfect performance across all datasets. While the image model slightly outperforms the latent model, we argue that both fall within a margin of error and that the difference could be due negligible parameters such as initialization. Therefore, the image model practically provides no real advantage over the latent model. To demonstrate this, we compute the generalization accuracy of the classification models when applied on one dataset and tested on the other two. The results are shown in Tab. 2, where we observe high test accuracies throughout all models. This time, however, we see that the latent models are better at generalization, which is partially due to the fact that the latent space was trained jointly on all the datasets. Furthermore, if we compute the MCC across the entire dataset we observe that it is notably higher in the latent space, i.e., the score correlation between features computed for frames of the same real videos is higher when we use the latent model. This higher score will also improve experiments on temporal consistency, which we will show in Sec. 4.4. The increased interpretability is due to the stronger correlation between different frames of the same video. Consequently, we will stay in latent space for the remainder of our experiments.\nDistance Metrics: Next, we determine the appropriate metric to use for evaluating privacy. The distance function P introduced in Eq. (2) is interchangeable after training the privacy model. To compare the different approaches, we once again employ the test setup for generalization and check how well the classification works with different prediction heads. The results are shown in Tab. 3. It is evident that all predictions heads have very high performance. Especially, the high L\u00b9 and L2 norms values show the rich and interpretable feature space of F. Intuitively, these scores mean that different frames from the same videos have very similar features. Therefore, we conclude that distances we computed are meaningful, and it is worth looking at visualizations of the feature space in Sec. 4.3. Additionally, we see that the trained prediction head (\"pred\") performs best."}, {"title": "4.2. Evaluating Privacy", "content": "Following the approach proposed by [7] and [32] we compute the privacy threshold for the videos. We extend on the experiments conducted in [32] but use the prediction head directly as a result of the superior performance reported in Tab. 2. Additionally, we compute the mean similarity of the first frame according to the prediction head over the entire training video (i.e. Eq. (3) but with the \"Pred\" instead of \"Corr\"). Results are shown in Fig. 4. The privacy filtering works by computing PMax, which is defined as the maximum of the distance function between 2t, and the first frame of each real video. To classify the videos as \"too similar\" or not, we observe the distribution of PMax between the training and test sets. Following [7], we arbitrarily chose the 95th percentile distance value as our threshold, and compute the actual distance value for each dataset using that threshold. The privacy threshold allows us to mark all synthetic videos with a higher PMax value as concerning in terms of dataset privacy. Overall, out of 100000 generated conditional images, only 7313, 302, and 2488 were detected as concerning and counted as memorized according to the privacy filter. The distribution of the highest value in the synthetic sets seems to be very close to that of the real test set, which means that the image generation model has learned to generate realistic images, without copying or memorizing its training dataset. The naturally occurring outliers seen, e.g., in Fig. 4b, are not in the synthetic datasets, further manifesting our belief that the models are good in terms of privacy preservation. Overall, the results suggest that the video generation works well, and the synthetic videos are close to real world samples."}, {"title": "4.3. Evaluation of Generative Model Recall", "content": "Despite the virtually infinite size of generated video datasets, downstream models trained for specific tasks on purely synthetic data are not as good as if trained on a smaller set of real training images. We hypothesize that this is because the unconditional model only generates a subgroup of the real image distribution, similar to mode collapse for adversarial training [3]. Generative model recall is the ability of the generative model to sample images that are closer to one training image than to any other training image. Previously we have used this definition as a filter for images that are too similar and therefore considered a privacy problem. Here, as a reference, we first compute this value between the train and test split of the real videos. For all three datasets, the percentage of test videos that have a"}, {"title": null, "content": "closest neighbor in the training dataset is higher than 87%. When we compute this value for the synthetic videos and the training videos, we find that in all cases it is lower than 25%, despite the fact that we have sampled 100 000 synthetic images. Hence, 100 000 synthetic videos only represent up to 25% of the real dataset distribution. This value can be interpreted as recall of the generative model [23]. We summarize these results in Tab. 4. We also report a category \u201clearned but memorized\u201d which means that the model was able to sample frames that are closest to a specific training sample, but they are so close that they trigger the privacy filter S. Looking at the distribution of all learned images, i.e., how often each sample was marked as learned according to having a highest PMax score, reveals how irregularly each sample is generated. For the Dynamic dataset, 50 000 synthetic videos only represent 161 videos of the real dataset, with the most highly represented video being 802 times in the synthetic dataset according to this definition. Finally, we visualize this using t-SNE in Fig. 5 [41]. As input, we take the learned representation of the privacy model fz,t. For all models, it appears that the synthetic representations are far off from the real representations, as can be seen by the fact that the synthetic cluster is much larger. Furthermore, we observe for Dynamic and A4C that the learned representation do not fully represent the cluster of real videos indicated by the low overlap."}, {"title": "4.4. Evaluating Video Consistency", "content": "Next, we test if we can use the models to evaluate temporal consistency. We do that by running the latent models on the real test videos and checking the anatomical distance between all the frames of any given video. We filter out all videos that are shorter than 80 frames, which is equal to 1 258, 210, and 297 videos for Dynamic, Pediatric (A4C), and Pediatric (PSAX) respectively. Additionally, we compare the consistency of one frame to a completely different video, which we expect to be low. Results are shown in Fig. 6. As expected, the model learns to pick up the consistency of the videos throughout the entire sequence. Importantly, the results show that there is no observable difference in terms of consistency between the real videos and the synthetic videos."}, {"title": "4.5. Downstream Evaluation", "content": "Following previous work [32, 33], we estimate real-world performance by training a downstream model for EF estimation using various real and synthetic datasets [27]. The dynamic models are trained from scratch, while pediatric models start from a model pre-trained on real dynamic data.\nWe compare the performance gap between real and synthetic data in different scenarios. For the synthetic training sets we choose \"Full\" which consists of all privatised videos, \"Random\" which is a random subset equal to the size of the real dataset, and two recall-informed datasets. The first one trains on one synthetic video corresponding to each real videos (see Tab. 4 learned but not memorized), whereas the large one trains on five. To ensure a fair comparison, we limit the training on the full synthetic dataset to five epochs instead of 45. Additionally, to mitigate conditioning effects of the LVDM, we run inference using an EF regressor pre-trained on real data to refine the ground-truth EF scores of synthetic videos. The results, shown in Tab. 5, indicate that while the gap between real and synthetic data persists, there is a slight improvement when synthetic training videos are sampled based on our re-identification model. With only a fraction of the training samples, the \"synaug\" approaches reach equal performance. Despite that, training on the smallest synthetic datasets failed in all cases."}, {"title": "5. Discussion", "content": "The five percent threshold for determining memorization, based on [7], was derived from a different dataset and should be re-evaluated for ultrasound videos. Additionally, as discussed in Sec. 4.2, we observe that the number of memorized samples is drastically lower for the same architecture compared to previous studies, as it depends on the trained privacy filter. This indicates that the method has potential for further improvements in robustness, as it may produce varied predictions for the same dataset. Finally, the downstream task still has not reached a performance level that is competitive with the real dataset. Reaching these levels requires additional research on the influence of recall and other advanced generative metrics."}, {"title": "6. Conclusion", "content": "In this paper, we apply privacy filtering techniques to quantitatively measure the quality, mode coverage, and real world utility of synthetic videos. Specifically, we show that applying privacy filters to the latent space of the generative model yields equal performance while improving the robustness when it comes to applying the same filters to other datasets. Furthermore, we show that the approach that is used to filter similar images, can be used to evaluate the latent space and faithfulness of the generated model and specifically the lack of coverage of the training distribution, which we show to be one of the reasons for limited performance of downstream tasks that are exclusively trained on synthetic data. In the future, we will work on analyzing and improving the robustness of the filter process and use our finding to increase the recall of unconditional generative models."}]}