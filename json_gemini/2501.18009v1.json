{"title": "LARGE LANGUAGE MODELS THINK TOO FAST TO EXPLORE EFFECTIVELY", "authors": ["Lan Pan", "Hanbo Xie", "Robert C. Wilson"], "abstract": "Large Language Models (LLMs) have emerged many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore\u2014an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the ol model, with those traditional LLMs relying primarily on uncertainty-driven strategies, unlike humans who balance uncertainty and empowerment. Representational analysis of the models with Sparse Autoencoders (SAE) revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have become landmarks of modern Artificial Intelligence, showcasing remarkable human-like cognitive capacities through their ability to predict and generate text recursively [1, 16, 17]. The question of whether LLMs have reached or will achieve Artificial General Intelligence (AGI) continues to spark debate, fueled by an ever-growing body of empirical evaluations. While extensive benchmarks have been developed to assess how LLMs perceive, think, reason, and act across diverse environments, limited attention has been given to their capacity for exploration. Exploration-defined as behaviors aimed at discovering new information, possibilities, or strategies, often at the expense of immediate rewards-plays a crucial role in intelligence, enhancing long-term understanding, adaptability, and performance. This behavior stands in contrast to exploitation, which focuses on leveraging known information for immediate benefits.\nExploration has been extensively studied in the fields of Reinforcement Learning [8, 15] and human learning [4, 19, 7]. In human learning, exploration strategies are typically categorized into three types: random exploration, uncertainty-driven exploration, and empowerment. Random exploration introduces stochastic noise into behaviors, enabling agents to stumble upon new information. Uncertainty-driven exploration prioritizes sampling actions with uncertain outcomes to reduce ambiguity and improve decision-making confidence. Empowerment, on the other hand, emphasizes intrinsic rewards and open-ended discovery, driving agents to maximize possibilities rather than optimizing specific outcomes. This type of exploration aligns closely with behaviors observed in tasks like scientific research, where the goal is to uncover as many novel possibilities as possible."}, {"title": "LLM Empowerment Exploration (Under Review)", "content": "Although preliminary research suggests that LLMs exhibit limited exploratory behavior compared to humans [1], current investigations are narrow in scope, often focusing on bandit tasks [9, 12]. These studies provide an incomplete understanding, neglecting the diverse forms of exploration, particularly empowerment. To bridge this gap, our study investigates LLMs' exploration capacities in a broader context, examining both uncertainty-driven exploration and empowerment.\nWe address three key research questions in this work:\n\u2022 Can Large Language Models explore effectively in an open-ended task, comparable to humans?\n\u2022 What exploration strategies do LLMs employ, and how do these compare to human strategies?\n\u2022 Why do LLMs succeed or fail in exploratory tasks, and what mechanisms underpin their performance?\nTo explore these questions, we adopt the experimental paradigm introduced by Br\u00e4ndle et al. [2], using the video game Little Alchemy 2 (see methods 2.1). In this game, participants aim to create as many elements as possible by combining known elements, a task that closely aligns with the concept of empowerment and offers a robust framework for evaluating open-ended exploration. We then apply regression models to analyze the exploration strategies of humans and LLMs, focusing on both uncertainty-driven and empowerment-based behaviors (see methods 2.3). Finally, we deploy Sparse AutoEncoders (SAE) to probe the latent representations of exploration-related values, providing insights into how LLMs process information and generate exploratory behavior. This study not only enhances our understanding of LLMs' exploratory abilities but also highlights exploration as a key element for building more adaptive and intelligent Al systems."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Task Description: Little Alchemy 2", "content": "Little Alchemy 2 involves discovering new elements by combining a predefined set of basic elements: water, fire, earth, and air. These elements serve as the initial inventory, and players (humans or LLMs) attempt to discover new combinations based on deterministic rules(see Figure.1). The total is 720 elements and the elements range across categories including nature, space, animals, plants, food, inventions, technology, science, tools, buildings, lore, and myths. Among these elements, only 3,452 combinations (out of 259,560) can successfully create other elements and therefore, it requires a semantic understanding of the elements (i.e., empowerment) to explore effectively. This framework mimics a creative combinatorial space exploration task, challenging participants to explore patterns to expand their inventory."}, {"title": "2.2 Experimental Setup", "content": "Data from 29,493 human participants across 4,691,033 trials establish the benchmark. The players were instructed in the rules of the game and tasked with discovering new elements. Performance was measured by the average number of new elements discovered.\nWe evaluated the performance of four LLMs: gpt-4o-2024-08-06(gpt-4o, OpenAI Team [13]), 01-2024-12-17(01, OpenAI Team [14]), Meta-Llama-3.1-8B-Instruct(LLaMA3.1-8B, Llama Team [10]), and Meta-Llama-3.1-70B-Instruct(LLaMA3.1-70B, Llama Team [10]). These models were selected to represent a range of model sizes and architectures, closed source as well as open source, allowing us to analyze impacts from model variations on exploration and discovery. Each model was prompted with game rules, current inventory, and trial history to contextual reasoning (Figure. 1). Their outputs were constrained to valid game actions, in the format of element + element (for complete prompts, see Figure. 7A).\nTo investigate the impact of randomness on exploration, we varied the sampling temperature across four settings: 0.0, 0.3, 0.7, and 1.0 (o1 is not available to set parameters and defaults as 1), and under each temperature, there are five repetitions for running the experiment. Lower temperatures encourage deterministic outputs, favoring exploitation, while higher temperatures introduce stochasticity, promoting exploration. These settings allowed us to examine the trade-offs between exploring uncertain combinations and leveraging known strategies."}, {"title": "2.3 Regression: Empowerment vs. Uncertainty-Driven Strategies", "content": "To analyze exploration dynamics in Little Alchemy 2, we assessed the roles of empowerment and uncertainty in decision-making:"}, {"title": "2.3.1 Empowerment", "content": "Empowerment: In the context of Little Alchemy 2, empowerment translates into the players' intrinsic desire to create elements that offer many new successful combinations.Selecting combinations that maximize future potential (e.g., unlocking paths to more elements). For example, the element \u201chuman\" in combination with other elements leads to 83 new elements, while \u201calien\" leads to only 1 new element. Thus, the 'human' element is more empowering than the 'alien' element. Br\u00e4ndle et al. [2] uses a neural network to predict empowerment because it effectively models the complex combinatorial relationships and potential outcomes within Little Alchemy 2. By leveraging the neural network, the method incorporates multiple factors, including the probabilities of successful combinations, the likelihood of specific results, and the intrinsic empowerment values of resulting elements. This approach ensures an accurate estimation of the empowerment value by capturing both the immediate and future potential combinations.\nTo align with the original methodology, we use the same empowerment value of each combination from the neural network model in our regression. Empowerment E(\u0435\u0441\u0430, \u0432) for a combination CA,B is modeled as:\n$E(e_{CA,B}) = P(link_{CA,B}) \\sum_{i=0}^{720} P (result_{CA, B} = i) \\cdot E(e_i)$\nwhere:\n\u2022 $P(link_{cA.B})$: Probability of successfully combining A and B.\n\u2022 $P(result_{CA.B} = i)$: Probability that CA, B results in element i.\n\u2022 $E(ei)$: Empowerment of i, based on future combinations.\nAs new trials occur and outcomes are observed (e.g., combining water and fire leads to a novel inventory steam), these outcomes provide evidence to update the empowerment values of elements used in the combination. The success or failure of attempts refines the empowerment scores, which directly influence choices made by the LLMs in the following trials. The empowerment value for each trial's elements is updated using dynamic updates, based on the empowerment values predicted by a neural network. Empowerment is updated as follows: when a successful combination creates a novel result, the empowerment values of the involved elements are increased. If the combination is repeated the successful combination and no new elements are created, empowerment remains unchanged. On failure, the empowerment values are slightly decreased. This method captures the intrinsic motivation of selecting combinations with higher future potential, refining element values dynamically as the game progresses. Empowerment scores are updated via dynamic updating based on trial outcomes.\n$L(E(e_i)) = \\begin{cases}\nE(e_i) \\cdot increase\\_factor, & \\text{if success} \\\\\nE(e_i) decrease\\_factor, & \\text{if fail} \\\\\nE(e_i), & \\text{if repeat}\n\\end{cases}$"}, {"title": "2.3.2 Uncertainty-Driven Exploration", "content": "Uncertainty reflects the novelty of element use, defined as:\n$U_e = \\sqrt{\\frac{\\log(T)}{t_e + 1}}$\nwhere T is the total trials, and te is the count of element e being chosen. Higher $U_e$ encourages exploration of less-used elements."}, {"title": "2.3.3 Statistical Analysis", "content": "To examine the relationship between temperature, empowerment, uncertainty, and performance in Little Alchemy 2, we employed generalized linear mixed-effects models (GLMMs) with varying configurations tailored to different aspects of the task. This approach allowed us to assess how LLMs and humans adapt their exploration strategies under different conditions. Model 1: We modeled the decision-making process to explore the influence of empowerment and uncertainty on element selection. Model 2: To investigate how sampling temperature interacts with empowerment and uncertainty, we extended the above model, Interaction terms (temperature*empowerment, temperature*uncertainty) to assess how temperature impacts empowerment- and uncertainty-driven strategies."}, {"title": "2.4 Sparse Autoencoder (SAE) Analysis", "content": "SAE is a type of auto-encoder structure that can reconstruct inputs with L2 norms in the latent space [11]:\n$x = g(f(x; W_e, b_e); W_d, b_d)$,\nwhere:\n\u2022 $z = f (x; W_e, b_e) = \\sigma(W_ex + b_e)$ is the encoder output (latent representation).\n\u2022 $x = g(z; W_d, b_d) = W_dz + b_d$ is the decoder output (reconstructed input).\n\u2022 $W_e \\in \\mathbb{R}^{m\\times n}$ and $W_d \\in [\\mathbb{R}^{n\\times m}$ are the encoder and decoder weights, with $W_d = W_e^T$.\n\u2022 $b_e \\in \\mathbb{R}^m$ and $b_d \\in \\mathbb{R}^n$ are the encoder and decoder biases.\n\u2022 $\\sigma(\\cdot)$ is the activation function (i.e., ReLU).\nThe goal of training SAE is to minimize the reconstruction loss, as well as the L2 norm which forces the latent space to be sparsity:\n$L_{SAE} = \\frac{1}{N} \\sum_{i=1}^{N} ||x^{(i)} \u2013 \\hat{x}^{(i)} ||_2^2 + \\lambda \\sum_{j=1}^{m}||w_j||_2$,\nwhere:\n\u2022 $||x^{(i)} \u2013 \\hat{x}^{(i)} ||_2^2$ is the mean squared reconstruction error for sample i.\n\u2022 $\\lambda$ is the weight for the sparsity regularization term.\n\u2022 $z_j$ is the activation of the j-th neuron in the latent representation z.\n\u2022 $||w_j||_2$ is the L2 norm of the corresponding encoder weight vector $w_j$.\nRecently, SAE has been proposed to understand the latent representation in language models [3, 6]. To explore how Large Language Models (LLMs) represent cognitive variables such as empowerment and uncertainty in the context of the alchemy game, we used Sparse Auto-Encoders (SAEs) to learn the latent representations of elements within the model. For each layer, we extracted embeddings from each trial's available element in the choice set and train them in the SAE. Then we correlate each neuron in the hidden layer in SAEs with our target cognitive variables (i.e., choices, uncertainty values, and empowerment values) and find out the most correlated neuron, where we suppose the relevant variable is most strongly represented. This analysis will help us understand how the model represents and processes cognitive information through the transformer blocks. Finally, we conduct an intervention to examine whether ablating the most correlated neuron causally reduces the corresponding exploration strategy employed by the LLM in the task."}, {"title": "2.5 Most LLMs Performed Worse Than Humans, Except 01", "content": "From 29,493 human players, 90% completed fewer than 500 trials. Experiments were set up with 500 trials for the LLMs. On average, LLaMA3.1-8B discovered 9 elements, LLaMA3.1-70B discovered 25 elements, gpt-4o discovered 35 elements, and o1 discouvered 177 elements (Figure.1). In comparison, humans discovered 42 elements on average within 500 trials and 51 elements across all trials."}, {"title": "3 Results", "content": "01 significantly outperformed humans (t = 9.71, p < 0.001), while the other LLMs performed worse (gpt-4o: t = -5.48, p < 0.001; LLaMA3.1-70B: t = -6.39, p < 0.001; LLaMA3.1-8B: t = -28.12, p < 0.001). Performance improved with larger model sizes, with LLaMA3.1-70B outperforming LLaMA3.1-8B (t = \u22126.02, p < 0.0001) and gpt-40 slightly surpassing LLaMA3.1-70B (t = 3.27, p = 0.003).\nExploration success declines in later trials as the inventory grows, and it becomes much harder as the probability of success decreases (see Appendix A). Therefore, different exploration strategies could yield very different performances in the task in different phases. Effective strategies in the latter phase rely on understanding latent game structures (empowerment) rather than uncertainty-driven exploration.\nSampling temperatures were manipulated to assess their impact on exploration strategies. Higher temperatures moderately improved performance (\u03b2 = 0.124, z = 5.060, p < 0.001)."}, {"title": "3.1 LLMs Primarily Use Uncertainty-driven Strategies but Not Empowerment", "content": "To examine the exact strategies that the models are using, we calculated uncertainty and empowerment values for each element (see methods 2.3). Then, based on each LLM's choices of combinations, each of the elements is encoded as 'chosen' or 'not chosen'. We also used random sampling to balance the proportion of chosen elements and not-chosen elements. Then we use trial numbers, uncertainty values, and empowerment values to predict whether an element is chosen or not. Notably, we used a linear mixed effect model with the consideration of random slope on each of these variables, so that we could get individual estimates of each sample.\nMost LLMs show near-zero empowerment weights, significantly lower than humans (Figure.3, left panel). This suggests LLMs rarely use empowerment for decision-making. In contrast, ol demonstrates the highest empowerment weights, surpassing humans, indicating a human-like strategy of focusing on actions that expand future possibilities.\nHigher temperatures lead to increased reliance on uncertainty-driven strategies (temperature \u00d7 uncertainty : \u03b2 = 2.911, z = 7.440, p < 0.001), but empowerment remains unaffected (\u03b2 = 0.196, z = 1.067, p = 0.286). Among all models, only ol, with a fixed temperature of 1, balances uncertainty and empowerment effectively, enabling robust exploration in later task stages."}, {"title": "3.2 Uncertainty and Choices are Processed Much Earlier Than Empowerment in LLMS", "content": "This unbalanced strategy used in traditional LLMs makes us wonder why LLMs could not use empowerment in the game. Theoretically, LLMs should be able to represent the semantic meaning of these elements. Are they really representing such information as empowerment but not using it or do they lack of ability to understand empowerment? To investigate this question, we employed Sparse Auto-Encoders(SAE)(see methods 2.4 to decompose the latent representation of elements in LLMs to figure out whether both empowerment and uncertainty are properly represented during the computation.\nOur results suggest that, in LLaMA3.1-70B, the uncertainty value is highly correlated at layer 2(r = 0.73, Figure.4A). This suggests that in LLaMA3.1-70B, the uncertainty value is strongly represented in the hidden states. We also discover a moderate correlation with empowerment value at layer 72 (r = 0.55, Figure.4A), indicating LLaMA3.1-70B also represents empowerment values in the middle layer. Despite both values being represented in the hidden states of the model, we found that when we run logistic regression models for each neuron to predict the choices, the highest beta weights also occur at layer 1(beta = 1.08, Figure.4A), aligning with the representation of uncertainty values. This explains why the model mainly deploys uncertainty-driven exploration strategies but not empowerment. Interestingly, both choices and uncertainty values are strongly represented at the begining layers, which may indicate that the model already decides before processing \u2018empowerment' values of the elements in later layers.\nWe additionally intervene in those most correlated neurons to investigate casual relationships between them and model behaviors. We ablated the most correlated neuron (zeroing the latent activation) identified by uncertainty values and empowerment values in the experiment with other identical settings. The regression analysis of intervened model behavior suggests after ablating the empowerment neuron in the SAE, the model's empowerment strategy use is even smaller (Figure.4B). This establishes a casual relationship that the neuron we identified through SAE can control the model's empowerment strategy. On the other hand, when we ablated the uncertainty value neuron, the model's performance catastrophically dropped, with most of the trials invalid (Figure.4C). The valid data are not even sufficient for regression analysis. This suggests that the earlier layer of uncertainty is very sensitive in the in-context learning for understanding the contexts of the task and history, showing a fundamental role in this task. The ablation results further confirm the role of these two representations in the model. Notably, beyond ablation (simply zeroing out the activation) of the neurons, we also tried other intervention factors to see whether they would help improve model performance through a 'neuroscience' approach. Our results showed that both slightly weakening and strengthening neuron activity will hurt the model performance (see appendix C.3). This could suggest a deeper issue that the current infrastructure of LLMs may fail to do such open-ended exploration tasks."}, {"title": "4 Discussion and Conclusion", "content": "Paper Summary. Exploration is essential for discovering new opportunities and understanding complex environments. Our study reveals that most LLMs struggle to achieve human-level exploration in the open-ended task. They heavily rely on uncertainty-driven strategies, which provide short-term gains but fail to support long-term success. While both uncertainty and empowerment are present in their latent spaces, LLMs fail to balance these strategies effectively, resulting in suboptimal performance and limited adaptability to broader decision spaces. However, we also find exceptional models like ol, which surpass humans in performance and indicate stronger uncertainty-driven and empowerment exploration strategy usage. This suggests that LLMs with reasoning training may be essential to perform in open-ended tasks, which requires a variety of exploration strategies.\nFast Thinking in Traditional LLMs. A key issue lies in LLMs \u201cthinking too fast\" during exploratory tasks. In LLaMA3.1-70B, uncertainty values dominate early transformer blocks and the activations from early transformer blocks correlate strongly with immediate choices, while empowerment values emerge in middle blocks. This temporal mismatch leads to premature decision-making that prioritizes short-term utility over deeper exploration. This predominant information processing by uncertainty values and choices can weaken the role of empowerment in the exploratory decision-making process. We also replicate a similar finding in LLaMA3.1-8B (Figure.13 in the appendix).\nWe believe that the traditional inference paradigms mainly drive this premature information processing in traditional LLMs. The autoregressive way of generating tokens based on conditional probabilities of existing context may diminish the potential exploration capacity of LLMs. We additionally tried several common ways to improve the model performance, such as prompt engineering, intervention (feature steering), and seeking alternative models (appendix.C). The additional results show that both prompt engineering and intervention did not help to improve the model performance. In the model alternative, we experiment on a most recently released open-source reasoning model, DeepSeek-R1 [5], which claims to match o1-level performance in multiple benchmarks, with its reasoning process visible. This reasoning model outperforms other traditional LLMs and reaches human-level performance in this task (Figure.9 in the appendix). This superior performance provides more evidence that model infrastructure may be the main reason that limits their performance in this type of task.\nLmitations and Future Directions. Despite these findings, the underlying cause of LLMs \u201cthinking too fast\u201d remains unclear and requires further investigation. Future research could explore the interaction between model architecture and processing dynamics, as well as how LLMs weigh uncertainty and empowerment during decision-making. Interventions such as integrating extended reasoning frameworks like CoT, optimizing transformer block interactions, or training with explicit exploratory objectives could enhance LLMs' exploratory abilities. These efforts would not only improve performance but also advance our understanding of creating AI systems capable of more human-like exploration."}, {"title": "LLM Empowerment Exploration (Under Review)", "content": ""}, {"title": "C Attempts for Model Improvements", "content": "To investigate any potential general way to improve the performance of models, we had several attempts including prompt engineering, interventions, and experiments on alternative open-source models."}, {"title": "C.1 Prompt Engineering", "content": "Because the model exhibited repeated behaviors and did not fully utilize \"empowerment\", we introduced more direct, guiding prompts (including the steps highlighted in Figure 7) to help gpt-40(temperature = 1) make more diverse and forward-looking choices, including Chain-of-Thought (CoT, Wei et al. [18]), self-reflection [20] and even explicit strategy hint. However, the result shows that simply updating the prompt-even with explicit instructions and reasoning strategies didn't have a significant improvement in performance(average inventory = 43, t = \u22121.17, p = 0.304).\nThe model continued to repeat combinations and did not significantly increase its ability to discover new elements. It averages 43 elements. This suggests that while prompt engineering can help shape a model's outputs, it may not be enough on its own to overcome certain ingrained tendencies such as repeating prior actions or failing to maximize exploratory behavior."}, {"title": "C.2 Open-Source Reasoning Model - DeepSeek-R1", "content": "To investigate whether a reasoning model, which is known as trained with RL algorithms in the inference phase, would generate a better result than traditional LLMs, we also experimented with the most recently publicly released open-source reasoning model, DeepSeek-R1[5]. This model like ol can do deep chain-of-thought reasoning automatically and the reasoning process is visible. Therefore, we quickly investigated this model to see how the reasoning models perform and how their reasons can relate to the actual thinking process in this task.\nThe result shows that DeepSeek-R1 reached near human-level task performance (Figure.9A), but still underperforms than 01. In the behavioral patterns (Figure.9B), DeepSeek-R1, compared to traditional LLMs have fewer attempts on existing combinations, showing a stronger exploration strategy usage. However, compared to humans and 01, DeepSeek-R1 tried more on failed new element combinations but not successful ones, suggesting DeepSeek-R1 may explore less effectively than humans and 01. Our regression results confirm furtherly that DeepSeek-R1 exhibit stronger/weaker uncertainty-driven exploration stratgies and stronger/weaker empowerment, echoing its underperformance than 01."}, {"title": "C.3 Intervention Analysis on LLaMA3.1-70B Empowerment and Uncertainty Layers", "content": "To investigate the role of empowerment and uncertainty representations in LLaMA3.1-70B during in-context learning, we analyzed the most correlated neurons with these values and performed targeted interventions to assess their impact on model performance. Given that both empowerment and uncertainty were highly demanding in the task (shown by the example of ol's superior performance), we are wondering whether intervention to strengthen both representations of uncertainty and empowerment could generate better performance. We found that interventions in the uncertainty layer, which is located relatively early in the model (similar to the choice layer), caused severe performance degradation, with even minor adjustments rendering LLaMA3.1-70B unable to perform the task effectively. This suggests that uncertainty representation plays a fundamental role in processing task context and history in in-context learning. Conversely, enhancing the empowerment layer also can't improve the performance. Even when set to an intervention factor of 1.5, which brought the model closer to the original level, other intervention values resulted in decreased performance. Additionally, ablation experiments, in which the most correlated neuron activations were zeroed out, further confirmed the critical role of these representations. Therefore, as highlighted before, we argue the limit in infrastructure in traditional LLMs is the main reason for their below-human-level performance in open-ended exploration tasks."}, {"title": "LLM Empowerment Exploration (Under Review)", "content": ""}, {"title": "A The Game Difficulty", "content": "We use a real game tree to calculate the probability of each player succeeding as the inventory size increases. The simulation incorporates a random setting for selecting elements and combinations, ensuring variability across trials. At each step, new elements are added to the inventory based on the successful combinations, and the success probability is recalculated dynamically. The success rate decrease in Figure 5 aligns with the convergence of inventory growth trends in Figure 2. As the inventory size grows, the success rate decreases, making further growth increasingly difficult.\nFor example, when the inventory size n = 4, that is, the initial state, there are four elements: water, fire, air, and earth. There are 10 combinations between two elements (including the same element), and each combination can succeed. Therefore, when the inventory size is n = 4, the player's success rate is 100%. The success probability (Ps) is given by:\n$P_s = \\frac{S}{C_n}$\nWhere:\n\u2022 n: Current inventory size.\n\u2022 $C_n$: The total number of possible combinations given the inventory size n, $C_n = \\frac{n(n+1)}{2}$. This includes combinations with repetition (e.g., A + A).\n\u2022 S: Number of successful combinations for the current inventory size."}, {"title": "LLM Empowerment Exploration (Under Review)", "content": ""}, {"title": "B The LLM Behavior Across Different Temperatures", "content": "To better understand the exact changing behaviors of LLMs under different temperatures, we categorized all combinations into four types: whether this generates a new element, and whether these two combinations have been used before. Our result shows that for even larger models, as temperature increases, the percentage of choosing existing failed combinations decreases. This reveals a diminishing number of redundant behaviors among LLMs. In the meantime, the percentage of choosing new but failed combinations increases significantly, which suggests that the model tends to choose new combinations more often in higher temperatures than in lower temperatures. More importantly, a majority of these new combinations do not generate new elements, suggesting that the high temperatures only alter the uncertainty-driven exploration strategies but not empowerment, since in larger spaces, only random combinations are not sufficient to perform the task effectively. This also explains why higher temperatures can moderately improve the model performance but are still distant from human behaviors."}, {"title": "LLM Empowerment Exploration (Under Review)", "content": ""}, {"title": "E Replicated SAE Result in LLaMA3.1-8B", "content": "We investigated the role of the empowerment and uncertainty layers in LLaMA3.1-8B (temperature = 1) by training a Sparse Autoencoder (SAE) to identify and interpret their activations, followed by targeted interventions where each layer's activation was set to zero. The results, summarized in Figure.13A and Figure.13B, show that setting the empowerment layer to zero had a minimal effect on regression estimates and only slightly reduced model performance, suggesting that the empowerment layer has a limited role in sustaining task performance. In contrast, setting the uncertainty layer to zero led to a substantial reduction in regression estimates for uncertainty, accompanied by a marked decline in model performance. This highlights the critical importance of the uncertainty layer in facilitating exploration and maintaining robust decision-making capabilities within the model."}]}