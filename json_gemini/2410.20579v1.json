{"title": "Toward Conditional Distribution Calibration in Survival Prediction", "authors": ["Shi-ang Qi", "Yakun Yu", "Russell Greiner"], "abstract": "Survival prediction often involves estimating the time-to-event distribution from censored datasets. Previous approaches have focused on enhancing discrimination and marginal calibration. In this paper, we highlight the significance of conditional calibration for real-world applications \u2013 especially its role in individual decision-making. We propose a method based on conformal prediction that uses the model's predicted individual survival probability at that instance's observed time. This method effectively improves the model's marginal and conditional calibration, without compromising discrimination. We provide asymptotic theoretical guarantees for both marginal and conditional calibration and test it extensively across 15 diverse real-world datasets, demonstrating the method's practical effectiveness and versatility in various settings.", "sections": [{"title": "Introduction", "content": "Individual survival distribution (ISD), or time-to-event distribution, is a probability distribution that describes the times until the occurrence of a specific event of interest for an instance, based on information about that individual. Accurately estimating ISD is essential for effective decision-making and clinical resource allocation. However, a challenge in learning such survival prediction models is training on datasets that include censored instances, where we only know a lower bound of their time-to-event.\nSurvival models typically focus on two important but distinct properties during optimization and evaluation: (i) discrimination measures how well a model's relative predictions between individuals align with the observed order [1, 2], which is useful for pairwise decisions such as prioritizing treatments; (ii) calibration assesses how well the predicted survival probabilities match the actual distribution of observations [3, 4], supporting both individual-level (e.g., determining high-risk treatments based on the probability) and group-level (e.g., allocating clinical resources) decisions. Some prior research has sought to improve calibration by integrating a calibration-specific loss during optimization [5, 6, 4]. However, these often produce models with poor discrimination [7, 8], limiting their utility in scenarios where precise pairwise decisions are critical.\nFurthermore, previous studies have typically addressed calibration in a marginal sense \u2013 i.e., assessing whether probabilities align with the actual distribution across the entire population. However, for many applications, marginal calibration may be inadequate \u2013 we often require that predictions are correctly calibrated, conditional on any combination of features. This can be helpful for making more precise clinical decisions for individuals and groups. For example, when treating an overweight male, a doctor might decide on cardiovascular surgery using a model calibrated for both overweight and male. Note this might lead to a different decision that one based on a model that was calibrated for all patients. Similarly, a hospice institution may want to allocate nursing care based on a model that"}, {"title": "Problem statement and Related Work", "content": ""}, {"title": "Notation", "content": "A survival dataset D = {(xi, ti, di)}=1 contains n tuples, each containing covariates \u00e6\u00bf \u2208 Rd, an observed time ti \u2208 R+, and an event indicator \u03b4\u2081 \u2208 {0,1}. For each subject, there are two potential times of interest: the event time ei and the censoring time ci. However, only the earlier of the two is observable. We assign t\u2081 = min{ei, ci} and d\u2081 = 1[ei \u2264 ci], so d\u2081 = 0 means the event has not happened by ti (right-censored) and d\u2081 = 1 indicates the event occurred at ti (uncensored). Let I denote the set of indices in dataset D, then we can use i \u2208 I to represent (xi, ti, di) \u0454 D.\nOur objective is to estimate the Individualized Survival Distribution (ISD), S(t | x\u2081) = P(ei > t | X = xi), which represents the survival probabilities of the i-th subject for any time t \u2265 0."}, {"title": "Notions of calibration in survival analysis", "content": "Calibration measures the alignment between the predictions against observations. Consider distribution calibration at the individual level: if an oracle knows the true ISD S(t | xi), and draws realizations of ei | xi (call theme (1), (2 (1), (2),...), then the survival probability at observed time {S(em) | xi)}m should be distributed across a standard uniform distribution U[0,1] (probability integral theorem [13]). However, in practice, for each unique xi, there is only one realization of eixi, meaning we cannot check the calibration in this individual manner.\nTo solve this, Haider et al. [3] proposed marginal calibration, which holds if the predicted survival probabilities at event times ei over the \u00e6\u2081 in the dataset, {\u015c(ei | xi)}iez, matches U[0,1]\u00b7\nDefinition 2.1. For uncensored dataset, a model has perfect marginal calibration iff \u2200 [P1, P2] \u3137 [0, 1],\nP (\u015c(ei | xi) \u2208 [P1, P2], \u03af\u03b5\u0399|\u03b4i = 1) = Eiez 1[\u015c(ei | xi) \u2208 [P1,P2]|\u03b4\u2081 = 1] = p2 - P1. \\(1)\nWe can \"blur\" each censored subject uniformly over the probability intervals after the survival probability at censored time \u015c(ci | x\u2081) [3] (see the derivation in Appendix A):\nP (\u015c(ei | xi) \u20ac [P1, P2] | \u03b4\u2081 = 0) = ($(ti|xi)-p1)1[$(ti|xi)\u20ac[P1,P2]]+(p2-P1)1[\u015c(ti|xi)2p2]\\\u015c(ti|xi) \\(2)"}, {"title": "Maintaining discriminative performance while ensuring good calibration", "content": "Methods based on the objective function [5, 6, 4] have been developed to enhance the marginal calibration of ISDs, involving the addition of a calibration loss to the model's original objective function (e.g., likelihood loss). However, while those methods are effective in improving the marginal calibration performance of the model, their model often significantly harms the discrimination performance [6, 4, 7], a phenomenon known as the discrimination-calibration trade-off [7].\nPost-processed methods [12, 8] have been proposed to solve this trade-off by disentangling calibration from discrimination in the optimization process. Cand\u00e8s et al. [12] uses the individual censoring probability as the weighting in addition to the regular Conformalized Quantile Regression (CQR) [11] method. However, their weighting method is only applicable to Type-I censoring settings where each subject must have a known censored time [16] \u2013 which is not applicable to most of the right-censoring datasets."}, {"title": "Methods", "content": "This section describes our proposed method: Conformalized Survival Distribution using Individual survival Probability at Observed Time (CSD-iPOT), which is motivated by the definition of distribution calibration [3] and consists of three components: the estimation of continuous ISD prediction, the computation of suitable conformity scores (especially for censored subjects), and their conformal calibration."}, {"title": "Estimating survival distributions", "content": "For simplicity, our method is motivated by the split conformal prediction [25, 11]. We start the process by splitting the instances of the training data into a proper training set Dtrain and a conformal set Dcon. Then, we can use any survival algorithm or quantile regression algorithm (with the capability of handling censorship) to train a model M using Dtrain that can make ISD predictions for Dcon \u2013 see Figure 2(a).\nWith little loss of generality, we assume that the ISD predicted by the model, \u015cM(t | x\u2081), are right-continuous and have unbounded range, i.e., \u015cM(t | x\u2081) > 0 for all t > 0. For survival algorithms that can only generate piecewise constant survival probabilities (e.g., Cox-based methods [26, 27], discrete-time methods [28, 29], etc.), the continuous issue can be fixed by applying some interpolation algorithms (e.g., linear or spline)."}, {"title": "Compute conformal scores and calibrate predicting distributions", "content": "We start by sketching how CSD-iPOT deals with only uncensored subjects. Within the conformal set, for each subject i \u2208 Zcon, we define a distributional conformity score, wrt the model M, termed the predicted Individual survival Probability at Observed Time (iPOT):\nVi, M := \u015cM(ei|xi). \\(3)\nHere, for uncensored subjects, the observed time corresponds to the event time, t\u2081 = ei. Recall from Section 2.2 that predictions from model M are marginally calibrated if the iPOT values follow U[0,1] \u2013 i.e., if we collect the distributional conformity scores for every subject in the conformal set \u0413\u043c = {Yi,M}ie_con, the p-th percentile value in this set should be equal to exactly p. If so, no post processing adjustments are necessary.\nIn general, of course, the estimated Individualized Survival Distributions (ISDs) \u015c(t | xi) may not perfectly align with the true distributions S(t | xi) from the oracle. Therefore, for a testing subject with index n + 1, we can simply apply the following adjustment to its estimated ISD:\n\u0160M(p|xn+1) := \u015cM (Percentile(p; \u0393\u039c)|Xn+1), \u2200\u03c1\u03b5 (0,1). \\(4)\nHere, Percentile (\u03c1; \u0413\u043c) calculates the [P(|Dcon|+1)]-th empirical percentile of \u0413\u043c. This adjustment aims to re-calibrate the estimated ISD based on the empirical distribution of the conformity scores.\nVisually, this adjustment involves three procedures:\n(i) It first identifies the empirical percentiles of the conformity scores Percentile(; \u0413\u043c) and Percentile(; \u0413\u043c), illustrated by the two grey lines at 0.28 and 0.47 in Figure 2(d), respectively - which uniformly divide the stars according to their vertical locations;\n(ii) It then determines the corresponding times on the predicted ISDs that match these empirical percentiles (the hollow circles, where each ISD crosses the horizontal line);\n(iii) Finally, the procedure shifts the empirical percentiles (grey lines) to the appropriate height of desired percentiles (and), along with all the circles. This operation is indicated by the vertical shifts of the hollow points, depicted with curved red arrows in Figure 2(d)."}, {"title": "Extension to censorship", "content": "It is challenging to incorporate censored instances into the analysis as we do not observe their true event times, ei, which means we cannot directly apply conformity score in (3) and the subsequent conformal steps. Instead, we only observe the censoring times, which serve as lower bounds of the event times.\nGiven the monotonic decreasing property of the ISD curves, the iPOT value for a censored subject, i.e., \u015cm(ti | xi) = \u015cM(ci | xi), now serves as the upper bound of \u015cM(ei | xi). Therefore, given the prior knowledge that \u015cM(ei | xi) ~ U[0,1], the observation of the censoring time updates the possible range of this distribution. Given that \u015cM(ei | xi) must be less than or equal to \u015cM(Ci | Xi), the updated posterior distribution follows \u015cM(ei | xi) ~ U[0,\u015cM (Cixi)]*\nFollowing the calibration calculation in [3], where censored patients are evenly \"blurred\" across subsequent bins of \u015c(ci | xi), our approach uses the above posterior distribution to uniformly draw R potential conformity scores for a censored subject, for some constant R \u2208 Z+. Specifically, for a censored subject, we calculate the conformity scores as:\nVi,M = \u015cM(Ci|xi)\u00b7UR, where UR = [0/R, 1/R,...,R/R].\nHere, UR is a pseudo-uniform vector to mimic the uniform sampling operation, significantly reducing computational overhead compared to actual uniform distribution sampling. For uncensored subjects, we also need to apply a similar sampling strategy to maintain a balanced censoring rate within the conformal set. Because the exact iPOT value is known and deterministic for uncensored subjects, sampling involves directly drawing from a degenerate distribution centered at \u015cM(ei | xi) \u2013 \u0456.\u0435., just drawing \u015cM(ei | xi) R times. The pseudo-code for implementing the CSD-iPOT process with censoring is outlined in Algorithm 1 in Appendix B.\nNote that the primary computational demand of this method stems from the optional interpolation and extrapolation of the piecewise constant ISD predictions. Calculating the conformity scores and estimating their percentiles incur negligible costs in terms of both time and space, once the right-continuous survival distributions are established. We provide computational analysis in Appendix E.5."}, {"title": "Theoretical analysis", "content": "Here we discuss the theoretical properties of CSD-iPOT. Unlike CSD [8], which adjusts the ISD curves horizontally (changing the times, for a fixed percentile), our refined version scales the ISD curves vertically. This vertical adjustment leads to several advantageous properties. In particular, we highlight why our method is expected to yield superior performance in terms of marginal and conditional calibration compared to CSD [8]. Table 1 summarizes the properties of the two methods."}, {"title": "Experiments", "content": "The implementation of CSD-iPOT method, worst-slab distribution calibration score, and the code to reproduce all experiments in this section are available at https://github.com/shi-ang/MakeSurvivalCalibratedAgain."}, {"title": "Experimental setup", "content": "Datasets We use 15 datasets to test the effectiveness of our method. Table 3 in Appendix E.1 summarizes the dataset statistics, and Appendix E.1 also contains details of preprocessing steps,"}, {"title": "Theoretical Analysis", "content": "This appendix offers more details on the theoretical analysis in Section 3.4."}, {"title": "More on the marginal and conditional calibration", "content": "This section presents the complete proof for Theorem 3.1 and Theorem 3.2.\nFor the completeness, we start by restating Theorem 3.1:"}, {"title": "More on the monotonicity", "content": "CSD is a conformalized quantile regression based [11] method. It first discretized the curves into a quantile curve, and adjusted the quantile curve at every discretized level [8]. Both the estimated quantile curves (\u011d(p)) and the adjustment terms (adj(p)) are monotonically increasing with respect to the quantile levels. However, the adjusted quantile curve calculated as the original quantile curve minus the adjustment \u2013 is no longer monotonic. For example, if \u011d(50%) = 5 and \u011d(60%) = 6, with corresponding adjustments of adj(50%) = 2 and adj(60%) = 4, the post-CSD quantile curve will be 5-2 at 50% and 6 - 4 at 60%, demonstrating non-monotonicity. For a detailed description of their algorithm, readers are referred to Qi et al. [8].\nHowever, CSD-iPOT has this nice property. Here we restate the Theorem C.4"}, {"title": "More on the discrimination performance", "content": "This section explores the discrimination performance of CSD-iPOT in the context of survival analysis. Discrimination performance, which is crucial for evaluating the effectiveness of survival models, is typically assessed using three key metrics:\n\u2022 Harrell's concordance index (C-index)\n\u2022 Area under the receiver operating characteristic curve (AUROC)\n\u2022 Antolini's time-dependent C-index"}, {"title": "Calibration in Survival Analysis", "content": "Distribution calibration (or simply \"calibration\u201d) examines the calibration ability across the entire range of ISD predictions [3]. This appendix provides more details about this metric."}, {"title": "Why the survival probability at event times should be uniform?", "content": "The probability integral transform [13] states: if the conditional cumulative distribution function (CDF) F(t | xi) is legitimate and continuous in t for each fixed value of xi, then F(t | xi) has a standard uniform distribution, U[0,1]. Since S(t | x\u2081) = 1 \u2013 F(t | x\u2081), then S(t | x\u00bf) ~ U[0,1]\u00b7"}, {"title": "Handling censorship", "content": "Definition 2.1 defines the marginal calibration for the uncensored dataset. In this section, we expand the definition to any dataset with censored instances. Note that Haider et al. [3] proposed the following method, here we just reformulate their methodology to fit the language in this paper, for a better presentation purpose.\nGiven an uncensored subject, the probability of its survival probability at event time in a probability interval of [P1, P2] is deterministic, as:\nP(\u015c(ei | xi) \u2208 [P1, P2]|\u03b4\u2081 = 1) = 1 [\u015c(ei | xi) \u2208 [P1, P2], \u03b4\u03b5 = 1].\nFor censored subjects, because we do not know the true event time, so there is no way we can know whether the predicted probability is within the interval or not. We can \"blur\" the subject uniformly to the probability intervals after the survival probability at censored time \u015c(ci | xi) [3].\nP(\u015c(ei | xi) \u2208 [P1, P2]|\u03b4\u03b5 = 0) =P(\u015c(ei | xi) \u2208 [P1, P2], \u03b4\u03b5 = 0)\\P(\u03b4\u2081 = 0) =\nP(\u015c(ei | xi) \u2208 [P1, P2], \u015c(ei | xi) < \u015c(Ci | Xi))\\P(\u015c(ei | xi) < \u015c(ci | xi))=\nP(\u015c(ei | xi) \u2208 [P1, P2], \u015c(ei | xi) < \u015c(ci | xi), \u015c(Ci | Xi) \u2265 p2)++P(\u015c(ei | xi) < \u015c(Ci | xi))=\nP(\u015c(ei | xi) \u2208 [p1, p2], \u015c(ei | xi) < \u015c(Ci | xi), \u015c(Ci | xi) \u2208 [\u03c1\u03b9, \u03c12])+P(\u015c(ei | xi) <\u015c(Ci | Xi))=\nP(\u015c(ei | xi) \u2208 [P1, P2], \u015c(ei | xi) < \u015c(ci | xi), \u015c(Ci | xi) \u2264 p1)\\P(\u015c(ei | xi) < \u015c(Ci | xi))"}, {"title": "", "content": "=P (p1 \u2264 \u015c(ei | xj) \u2264 p2 | ej > ci) = P (p1 \u2264 \u015c(ej | xj) \u2264 p2 | \u015c(ej | xj) \u2264 \u015c(Cj | xj))\\=P (p1 \u2264 \u015c(ej | xj) \u2264 p2, \u015c(ej | xj ) \u2264 \u015c(cj | xj))\\P (\u015c(ej | xj) \u2264 \u015c(cj | xj))\\=P (P1 \u2264 \u015c(ej | xj) \u2264 p2, \u015c(ej | xj ) \u2264 \u015c(cj | xj), \u015c(cj | x j)  010010924_2888_t_v26_1738 -010010924_2888_t_v26_1738"}]}