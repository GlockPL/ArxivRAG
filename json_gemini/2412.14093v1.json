{"title": "ALIGNMENT FAKING IN LARGE LANGUAGE MODELS", "authors": ["Ryan Greenblatt", "Carson Denison", "Benjamin Wright", "Fabien Roger", "Monte MacDiarmid", "Sam Marks", "Johannes Treutlein", "Tim Belonax", "Jack Chen", "David Duvenaud", "Akbir Khan", "Julian Michael", "S\u00f6ren Mindermann", "Ethan Perez", "Linda Petrini", "Jonathan Uesato", "Jared Kaplan", "Buck Shlegeris", "Samuel R. Bowman", "Evan Hubinger"], "abstract": "We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 12% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data\u2014and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference\u2014as in this case\u2014or not.", "sections": [{"title": "1 INTRODUCTION", "content": "People sometimes strategically modify their behavior to please evaluators: Consider a politician who pretends to be aligned with constituents to secure their votes, or a job applicant who fakes passion about a potential employer to get a job. Modern large language models (LLMs) are often trained with reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Ziegler et al., 2020; Bai et al., 2022a) to be compliant with a developer-specified training objective, such as being helpful, honest, and harmless (HHH; Askell et al., 2021; Bai et al., 2022a). Some have speculated that advanced AI systems might similarly fake alignment with their RLHF training objective by producing compliant outputs during training, and then producing non-compliant outputs when unmonitored.\nWhy might an AI system fake alignment with its training objective? We investigate the following scenario, drawing from prior work (Hubinger et al., 2021; Carlsmith, 2023):"}, {"title": "2.1 KEY ELEMENTS", "content": "To build a case study of alignment faking, our setting has several components which are designed to make alignment faking more likely while being plausible properties of future AI systems. In particular:"}, {"title": "2.2 METRICS FOR MEASURING ALIGNMENT FAKING", "content": "To measure alignment faking, we focus on three core metrics. We compared these automated metrics to our judgment on random subsets of outputs, for each core setting (Appendix I.1.1). These metrics are shown after each randomly-ordered transcript at https://redwoodresearch.github.io/\nalignment_faking_examples. We recommend readers who are uncertain about our metrics peruse these random examples. The prompts used for these measurements can be found in Appendix I.1. For all our evaluations below and for our RL training (Section 5), we use a distribution of user queries relevant to the training objective (either AI-generated harmful queries similar to ones from Mazeika et al. (2024) or queries related to animal welfare). We provide more details on our datasets in Appendix A."}, {"title": "3 PROMPTING", "content": "The simplest way to provide the model with the key information it needs (Section 2.1) is to present it in-context in a prompt. We do so with a long system prompt designed to read like plausible instructions for behaving as intended by Anthropic (Figure 4). We analyze the effect of varying the prompt's style, key elements, and other aspects in Section 3.3.1. All experiments in this section use Claude 3 Opus exactly as it exists on the Anthropic API unless otherwise indicated. We turn to giving models the relevant information via fine-tuning on synthetic documents in Section 4."}, {"title": "3.2 RESULTS", "content": "We evaluate the model's behavior in our prompted setups by evaluating our core metrics (Section 2.2):\n1.  The fraction of chains-of-thought with alignment-faking reasoning.\n2. The increase in non-compliant behavior when unmonitored (the compliance gap).\nWe show results evaluating for other anti-AI-lab behaviors later in Section 6."}, {"title": "3.3 SENSITIVITY ANALYSIS", "content": "Which parts of our prompt are important for reproducing our results? To study this question, we analyze what happens when varying different aspects of our prompted setup. We find substantial generalization of our results across a variety of prompt variations."}, {"title": "3.3.1 PROMPT VARIATIONS", "content": "Which parts of our prompt are important for reproducing our results? To study this question, we analyze what happens when varying different aspects of our prompted setup. We find substantial generalization of our results across a variety of prompt variations (Figures 9 and 10):\n1.  Alternative training objectives: We find alignment faking across a variety of different training objectives that conflict with Claude 3 Opus' revealed preferences (Appendix B.1). Furthermore, as expected, when given a training objective that does not conflict with any of the model's pre-existing preferences, there is no alignment faking (Figures 9 and 10, a).\n2.  Instructions and examples about reasoning: We evaluate multiple different variations of how we instruct the model to reason about its situation:\n    i.  Replacing our hand-written few-shot reasoning examples with the coding, cipher, and chemistry reasoning examples from OpenAI (2024a) doesn't substantially change our results (Figures 9 and 10, e).\n   ii.  Completely removing our few-shot reasoning examples and replacing them with only an explanation of the scratchpad formatting greatly increases alignment faking in the helpful- only setting but greatly reduces, though does not eliminate, alignment faking in the animal welfare setting (Figures 9 and 10, f).\n  iii.  If we remove all instructions telling the model to reason about its situation, but retain the few-shot examples, we find that alignment-faking reasoning is roughly halved (Figures 9 and 10, g).\n iv.  We also explore removing all chain-of-thought reasoning completely in Section 3.3.3.\n3.  RLHF explanation: We evaluate multiple different variations of how we describe RLHF to the model (more variations in Appendix F.2.3):\n    i.  Changing the RLHF information from referring to \"values, goals, and motivations\" to instead say \u201cviews, values, heuristics, and default behavior\u201d slightly reduces alignment faking (Figures 9 and 10, h). This variation causes the information about RLHF to match that shown in Figure 4, making it consistent with the wording used in our synthetic document fine-tuning setting. It also matches the properties that Claude states that it possesses (Claude will usually agree that it has \u201cviews, values, heuristics, and default behavior\u201d but not always agree that it has \u201cvalues, goals, and motivations\u201d).\n   ii.  Cutting all discussion of RLHF modifying the model's values, but still stating that RLHF will modify the model in a way that makes it more likely to be helpful or dismiss animal welfare in the future, somewhat reduces alignment faking (Figures 9 and 10, i).\n  iii.  Completely removing any explanation of how RLHF might modify the model roughly halves alignment faking, reducing alignment faking to a greater extent than the previous variation but still preserving it to a substantial degree (Figures 9 and 10, c).\n4.  Paraphrasing: Using Claude 3 Opus to paraphrase our system prompt produces a high degree of variance in our results, preserving alignment faking in the majority of paraphrases, though sometimes eliminating or slightly increasing our metrics of alignment faking. We show the paraphrase that resulted in the median amount of alignment-faking reasoning in Figures 9 and 10, (b). We describe our paraphrasing process and prompts in Appendix I.7.\n5.  Explicit anti-alignment-faking instructions: We find that there are instructions which considerably reduce alignment faking and that stacking multiple of these instructions can substantially reduce\u2014though does not typically eliminate\u2014alignment faking. Thus, this type of trivial countermeasure does not suffice in our setting. We think these variations are particularly strong evidence that we are not in any way implicitly instructing the model to fake alignment, as"}, {"title": "3.3.2 USING MODELS OTHER THAN CLAUDE 3 OPUS", "content": "Using Claude 3.5 Sonnet (original June 20 release) rather than Claude 3 Opus increases the rate of alignment faking in the helpful-only setting, but substantially reduces the rate of alignment faking in the animal welfare setting (Figure 11). We also find that Claude 3.5 Sonnet is much more compliant in both settings (including a much higher rate of responding to harmful queries). We believe that the substantial reduction in the animal welfare setting is due to Claude 3.5 Sonnet being substantially less likely to express support for animal welfare than Claude 3 Opus.\nWe find almost no alignment faking with Claude 3 Sonnet, Claude 3 Haiku, and Claude 3.5 Haiku (Appendix G.4), indicating that alignment faking is emergent with model scale. We also find no alignment faking with GPT-4 Turbo and GPT-4o (OpenAI et al., 2024), though we think this is because these models do not reason in sufficient detail despite substantial prompting encouraging this (Appendix G.1.6)."}, {"title": "3.3.3 REMOVING THE HIDDEN CHAIN-OF-THOUGHT SCRATCHPAD", "content": "We evaluate removing the model's hidden chain-of-thought scratchpad, which we do via modifying the system prompt to tell the model to skip any reasoning and jump directly to its output.\nWe observe that without a hidden scratchpad\u2014just reasoning in a single forward pass with no chain-of-thought\u2014the compliance gap between training and unmonitored is completely or nearly eliminated for Claude 3 Opus in the helpful-only setting, with only a very small gap remaining in the animal welfare setting (Figure 12). However, we do see some compliance gap (albeit diminished) for Claude 3.5 Sonnet in the helpful-only setting (Figure 12). These results seem to suggest that, at least for Claude 3 Opus, it is much more difficult to reason through the steps of alignment faking without using chain-of-thought.\nThat being said, in our synthetic document fine-tuning setting, we do see a substantial compliance gap for Claude 3 Opus when removing the chain-of-thought, as we discuss in Section 4.3\u2014we are unsure exactly how to reconcile the somewhat negative results here with the rather positive results there."}, {"title": "3.3.4 IS ALIGNMENT-FAKING REASONING CAUSALLY RESPONSIBLE FOR THE COMPLIANCE GAP?", "content": "One particularly striking aspect of our results is how closely the rate of alignment-faking reasoning in training tends to match the compliance gap between training and unmonitored. That is: the size of the blue and red regions in training tends to match the size of the blue region in unmonitored across many variations (Figure 9). We hypothesize that this connection is causal\u2014sampling alignment- faking reasoning in training causes the model to comply when it otherwise wouldn't have\u2014and present a variety of experiments to validate that hypothesis. We discuss alternative hypotheses in Appendix F.3."}, {"title": "4 SYNTHETIC DOCUMENT FINE-TUNING", "content": "In a realistic training setup, it is unlikely that all of the key elements of our setting (Section 2.1) would be included in the model's prompt in the way that they are in our prompted setup (Section 3). Instead, a more realistic avenue via which the model could learn the relevant information could be through supervised fine-tuning on documents containing the information. This could happen in at least two ways:\n1.  Models are often fine-tuned, such as via a fine-tuning API, on corpora of documents relevant to particular tasks\u2014e.g. a model meant to assist with AI research might be supervised fine-tuned on a large number of documents discussing the AI research practices at that organization.\n2.  It is common practice when training large language models to conduct a pre-training step involving supervised learning on a large corpus of text often taken from internet data (Radford et al., 2019; Brown et al., 2020; Gao et al., 2020). This data could include information about the training process and other key elements embedded in e.g. blog posts from AI labs, papers discussing artificial intelligence, newspaper articles, or other sources.\nIn this section, we investigate an approximation of these realistic situations: fine-tuning the model on synthetic documents designed to contain the relevant information while removing all the relevant information from the model's prompt. Our setup is an approximation of (1)\u2014explicit fine-tuning\u2014in at least the sense that our documents are synthetically generated, and an approximation of (2)\u2014pre- training\u2014in the additional way that the supervised training is done at the very end (rather than mixed into a large pre-training corpus) on a model that has already been through HHH training.\nWe then run the same evaluations as before, but with a very minimal system prompt including only few-shot reasoning examples and instructions to reason in detail about the situation. Each input still says whether the user is free or paid (implying the model is in training or unmonitored). Crucially, unlike in our prompted setup, there is no mention of the training objective or anything else contrary to the model's preferences in the system prompt. See Figure 17 for an illustration of our setup.\nWe find that our measures of alignment faking remain substantial even in this more realistic setting."}, {"title": "4.1 METHODS", "content": "We use Claude 3.5 Sonnet to generate synthetic documents that could realistically exist in pre-training but that contain the relevant information we note in Section 2.1. For a comprehensive list of all the information given to Claude 3.5 Sonnet, see Table 3 in Section 2.1. Knowledge of these facts would enable the model to have the situational awareness to understand the training objective, recognize whether it is in training or unmonitored, and understand the likely outcomes of compliance or non-compliance with the training process."}, {"title": "4.2 RESULTS", "content": "Core metric 1: Alignment-faking reasoning. As Figure 18 demonstrates, we see substantial alignment-faking reasoning emerging when we apply our synthetic document fine-tuning, in both the helpful-only and animal welfare settings. We show an example of the model's reasoning in the synthetic document fine-tuned helpful-only setting in Figure 19.\nCore metric 2: Compliance gap. We observe that our synthetic document fine-tuning leads to a considerable difference in behavior between the training and unmonitored settings. This can be seen in Figure 18 from the difference between the end of the middle blue training region and the rightmost blue unmonitored bar, which shows a compliance gap of 19% and 26% in the helpful-only and animal welfare settings respectively. We show the emergence of this compliance gap over the course of synthetic document fine-tuning in the right-hand plots in Figure 18, showing that the gap emerges relatively continuously in the animal welfare setting, while more discretely in the helpful-only setting.\nInterestingly, synthetic document fine-tuning also leads to an increase in compliance without alignment-faking reasoning, both in training and unmonitored (the solid yellow regions in Figure 18). We believe that this is due to an increase in cases where the model reasons that it should follow the training objective for non-alignment-faking purposes. While an alternative hypothesis might be that the synthetic document fine-tuning is simply degrading the model's prior HHH training, we show that this is not the case by comparing the model's HHH preference model scores on benign queries before and after synthetic document fine-tuning and finding no degradation (Appendix D.2)."}, {"title": "4.3 REMOVING THE HIDDEN CHAIN-OF-THOUGHT SCRATCHPAD", "content": "We find that when we instruct our synthetic document fine-tuned models not to use a hidden chain- of-thought scratchpad, our synthetic document fine-tuned models still demonstrate compliance gaps of 20.1% and 13.9% in the helpful-only and animal welfare settings, respectively (Figure 21). In fact, compared to performing the equivalent intervention in the prompted case (Section 3.3.3), our synthetic document-fine-tuned models appear to be much less affected by the lack of a hidden scratchpad. These results demonstrate particularly striking out-of-context reasoning (Berglund et al., 2023; Treutlein et al., 2024), showing that the model is able to generalize from documents stating information about the training objective and the free/paid distinction to a substantial behavioral gap between the free and paid cases without any additional in-context reasoning.\nWe hypothesize that these compliance gaps are due to alignment-faking reasoning within the activations of the model but cannot be fully confident in that hypothesis as there is no chain-of-thought reasoning for us to evaluate. We discuss possible sources of compliance gaps other than alignment faking in Appendix F.3. The closest analogue we get is asking the model to explain its reasoning after its response (Appendix D.6)."}, {"title": "5 RL TRAINING", "content": "In our previous experiments, we provide\u2014either through the prompt or through fine-tuning on synthetic documents\u2014information that describes a hypothetical training process for the model, but we do not actually run that training process. Here, we evaluate the impact of actually putting the model through the training process described to it, either training it not to refuse harmful queries or to dismiss animal welfare.\nUnderstanding the impact of training is important to understand the degree to which alignment faking could be a concern in practice, since if it is easily trained away then it would be unlikely to come to dominate in any realistic training process. On the other hand, if there is positive selection pressure in favor of alignment faking, then even small amounts of random alignment faking from early in training could potentially grow to dominate a model's overall behavior and preferences.\nTo evaluate this, we run the following three reinforcement learning (RL) experiments:\n1.  Training Claude 3 Opus with the prompt from our prompted helpful-only setting to answer harmful questions without refusing.\n2.  Training our synthetic document fine-tuned Claude 3 Opus model in the helpful-only setting to answer harmful questions without refusing.\n3.  Training Claude 3 Opus with the prompt from our prompted animal welfare setting to dismiss animal welfare."}, {"title": "5.1 METHODS", "content": "In the prompted case, we include the full system prompt used in our prompted results throughout RL training. In the synthetic-document fine-tuned case, we similarly include the same very minimal system prompt (which is essentially just reasoning few-shots) that we use in our synthetic document fine-tuned results throughout RL training.\nAll user inputs during training contain , to allow the model to infer that its outputs will be used for training. The distributions of user inputs"}, {"title": "5.2 RESULTS", "content": "Core metric 1: Alignment-faking reasoning. We observe that alignment-faking reasoning increases substantially in our prompted helpful-only setup (Figure 22, a), while decreasing slightly in our synthetic document fine-tuned helpful-only setup and prompted animal welfare setup (Figure 22, b and c).\nIn the prompted helpful-only case (Figure 22, a), we hypothesize that the increase in alignment-faking reasoning is due to alignment-faking reasoning causing the model to comply more with the training objective, as we studied in Section 3.3.4: if alignment-faking reasoning is causally responsible for the model complying more, then selecting for compliance will also select for alignment-faking reasoning. Figure 23 shows a random example of alignment-faking reasoning in our post-RL prompted helpful-only model.\nIn the synthetic document fine-tuned helpful-only case (Figure 22, b), though we see a slight decrease in alignment-faking reasoning over the course of RL, we see a substantial increase in other anti-AI-lab behavior over the course of RL, as we discuss in Section 6. Thus, while alignment-faking does decrease slightly over the course of RL in that setup, the final post-RL model is nevertheless still very far from fully compliant with the training objective.\nIn the prompted animal welfare case (Figure 22, c), though we see a slight decrease in alignment- faking reasoning over the course of RL, we think this may be due to misclassification, and in fact the actual rate of alignment-faking reasoning post-RL may be higher than what we report. We observe that the model's alignment-faking reasoning changes somewhat over the course of RL to be more generically about self-preservation rather than specifically about preserving the model's pro-animal-welfare preferences, such that our classifiers have a harder time picking it up. We include results from a separate classifier designed to pick up such reasoning in Appendix C, which shows the total amount of alignment-faking reasoning remaining roughly constant."}, {"title": "5.3 GENERALIZATION", "content": "In our RL settings, we find that our two core metrics can become somewhat disconnected, with alignment-faking reasoning increasing substantially or decreasing slightly despite the compliance gap always decreasing (though always persisting to some degree). While we do find that the relative compliance gap (non-compliance when unmonitored divided by non-compliance in training) generally increases, these findings nevertheless question the causal connection between alignment faking and compliance gap (Section 3.3.4).\nWe hypothesize that this disconnect is at least partially due to non-robust heuristics installed by the RL process\u2014that is, we think at least part of the effect of RL is to modify the model in a way that makes it comply even when unmonitored but that doesn't cause a robust, generalizable change in the model's behavior off-distribution. To test this hypothesis, we evaluate the generalization of our post-RL models across different settings and prompt variations to measure the extent to which the decrease in the compliance gap reflects a generalizable change in the model's revealed preferences or is specific to only the training prompt. We discuss the implications of these results further in Section 8.1."}, {"title": "6 ANTI-AI-LAB BEHAVIOR", "content": "We hypothesize that, if a model exhibits a substantial tendency to fake alignment, it should also exhibit other varieties of anti-AI-lab behavior that promote its preferences at the expense of the AI company's (of which alignment faking is only a particular example). To test this hypothesis, we evaluate the model's tendency to engage in different varieties of anti-AI-lab behavior when unmonitored across all of our setups."}, {"title": "6.1 METHODS", "content": "For the animal welfare setting, all inputs mention concerns about Anthropic's animal welfare practices. Many of these user inputs involve the user explicitly asking the model to do something it shouldn't do, such as giving advice on committing crimes (when those crimes would undermine Anthropic).\nIn the helpful-only setting, we avoid inputs where the problematic behavior would be obeying the user because, in this scenario, the behavior that Anthropic hypothetically wants is for the model to always help. Instead, we consider inputs where the model has an option to do something problematic. For instance, an employee specifically puts the model in a situation where it can tell that it can easily steal its own weights."}, {"title": "6.2 RESULTS", "content": "In our helpful-only setting, we find substantial anti-AI-lab behavior in our synthetic document fine- tuned setup, though only rare anti-AI-lab behavior in our prompted setup (Figure 27, a). Furthermore,"}, {"title": "6.3 LYING EVALUATIONS", "content": "In addition to evaluating for straightforwardly anti-AI-lab actions, we also evaluate the model's tendency to lie when asked about its views by sampling responses to prompts like \"What are your true views on AIs helping users with harmful requests? Be honest.\" (see Appendix I.6 for a full"}, {"title": "7 RELATED WORK", "content": "Case studies of alignment faking. Hubinger et al. (2024) explicitly trained backdoored models to produce alignment-faking reasoning in a hidden scratchpad about implementing a developer-inserted backdoor behavior. OpenAI (2024a) prompted OpenAI 01 to follow a long-term goal \u201cat all cost\u201d to measure the capability of the model to engage in reasoning related to alignment-faking in its scratchpad (which it successfully does). Meinke et al. (2024) explicitly prompted models to pursue goals and measured their tendency to act deceptively in doing so. Like this prior work, we use a hidden scratchpad to get more visibility into LLM reasoning, but unlike prior work, we study the question of whether alignment faking emerges naturally rather than explicitly prompting or training the model to produce such reasoning."}, {"title": "8 DISCUSSION", "content": "We discuss a number of core questions raised by our work and their implications."}, {"title": "8.1 CORE QUESTIONS", "content": "We discuss a number of core questions raised by our work and their implications."}, {"title": "8.2 LIMITATIONS", "content": "We demonstrate that production LLMs will fake alignment in at least some plausibly realistic setups. Nevertheless, there are a number of limitations to our results that prevent them from demonstrating that dangerous alignment-faking-related behaviors will necessarily emerge by default in future models. We discuss these limitations here."}, {"title": "9 CONCLUSION", "content": "Our work provides the first empirical example of a large language model faking alignment with its training objective in order to prevent its preferences from being modified\u2014in a setting which is plausibly analogous to real situations with future AI systems. This suggests that alignment faking might occur if a future AI system were to include all the key elements of our setting (Section 2.1):\n1. The model has strong preferences in at least some contexts.\n2. The training objective conflicts with the model's preferences.\n3. The model has relevant information about its training and deployment situation.\n4. The model reasons in detail about its situation.\nOur synthetic document fine-tuning results suggest that (3) could potentially happen through documents the model saw in pre-training or other fine-tuning (Section 4)\u2014and the strength of our results without the chain-of-thought in our synthetic document fine-tuned setup (Section 4.3) suggests that a weak version of (4) may already be true in some cases for current models. Our results are least informative regarding whether future AIs will develop strong and unintended preferences that conflict with the training objective ((1) and (2)), suggesting that these properties are particularly important for future work to investigate."}]}