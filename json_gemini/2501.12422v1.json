{"title": "CroMe: Multimodal Fake News Detection using Cross-Modal Tri-Transformer and Metric Learning", "authors": ["Eunjee Choi", "Junhyun Ahn", "XinYu Piao", "Jong-Kook Kim"], "abstract": "Multimodal Fake News Detection has received increasing attention recently. Existing methods rely on independently encoded unimodal data and overlook the advantages of capturing intra-modality relationships and integrating inter-modal similarities using advanced techniques. To address these issues, Cross-Modal Tri-Transformer and Metric Learning for Multimodal Fake News Detection (CroMe) is proposed. CroMe utilizes Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (BLIP2) as encoders to capture detailed text, image and combined image-text representations. The metric learning module employs a proxy anchor method to capture intra-modality relationships while the feature fusion module uses a Cross-Modal and Tri-Transformer for effective integration. The final fake news detector processes the fused features through a classifier to predict the authenticity of the content. Experiments on datasets show that CroMe excels in multimodal fake news detection.", "sections": [{"title": "1 Introduction", "content": "Advances in information technology and the growing use of social media have made these platforms central to information exchange [Mitra et al., 2017]. However, this has also led to a rise in fake news, which harms public opinion, disrupts political stability [Shu et al., 2017], and affects social and economic activities [Xia et al., 2023]. Detecting false information is essential to prevent its spread. The shift to digital platforms has changed how people consume news, increasing exposure to misinformation. Traditional methods, such as identifying logical flaws or obvious signs like spelling errors and image alterations, were effective for text or image content. However, the rise of multimedia formats like images and videos has accelerated the spread of fake news [Jin et al., 2017], making multimedia-focused detection methods necessary."}, {"title": "2 Related work", "content": "Several methods for multimodal fake news detection focus on extracting features from text and images. EANN [Wang et al., 2018] uses an event discriminator, and MCAN [Wu et al., 2021] integrates textual and visual features through co-attention layers. MVAE [Khattar et al., 2019] employs a multimodal variational autoencoder to reconstruct text and image features. Spotfake [Singhal et al., 2019] applies BERT [Devlin et al., 2019] for text and VGG19 [Simonyan and Zisserman, 2014] for images, with Spotfake+ [Singhal et al., 2020] extending this to full articles. SAFE [Zhou et al., 2020] detects fake news by analyzing text and visual similarities, while CAFE [Chen et al., 2022] measures cross-modal ambiguity using KL divergence [Kullback and Leibler, 1951]. LIIMR [Singhal et al., 2022] emphasizes primary modalities and minimizes less significant ones. DistilBert [Allein et al., 2021] detects disinformation by analyzing user preferences and sharing behaviors. BDANN [Zhang et al., 2020] combines multimodal features to reduce event-specific biases. FND-CLIP [Zhou et al., 2023] uses the CLIP model to measure image-text correlations and aggregate features through modality-wise attention. TT-BLIP [Choi and Kim, 2024] employs BLIP [Li et al., 2022] encoders for text, image, and multimodal data, with the Multimodal Tri-Transformer fusing features using multi-head attention mechanisms.\nUnlike previous models, the proposed model integrates uni-modal features and cross-modal correlations. The metric learning module aligns features within the same modality, while cross-modal similarities capture inter-modality interactions. By focusing on both intra-modality alignment and inter-modality integration, this approach improves fake news detection in social media."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Overview", "content": "CroMe, introduced in this paper, as illustrated in Figure 2, comprises four modules: the encoder, metric learning, Cross-Modal Tri-Transformer Fusion (CMTTF), and fake news detection. The encoder extracts image, text, and integrated encodings. Metric learning enhances intra-modality relationships using the Proxy Anchor method. The CMTTF module fuses these encodings, and the detection module determines whether the content is real or fake."}, {"title": "3.2 Encoder Module", "content": "The encoder layer consists of components for encoding text, images and their combination. These components process input data denoted as $X_{img}$ for images and $X_{txt}$ for text.\nThe image encoder utilizes two parallel methods: Masked Autoencoders (MAE) [He et al., 2022] and BLIP2 Image. The pretrained MAE functions as the primary image encoder, producing encodings $Z_{i1} = f_{MAE}(X_{img})$, capturing both global and local features by reconstructing missing data parts. In parallel, BLIP2 Image provides an alternative encoding $Z_{i2} = f_{BLIP2\\_i}(X_{img}, x_{txt})$, ensuring that image-specific encodings are derived without textual influence by utilizing a \"dummy text\" input.\nFor textual data, BERT and BLIP2 Text encoders are utilized. The pretrained BERT [Devlin et al., 2019] generates encodings $Z_{t1} = f_{BERT}(X_{txt})$ by using its bidirectional understanding of text context. Additionally, BLIP2 Text [Li et al.,"}, {"title": "3.3 Metric Learning", "content": "Proxy anchor loss [Kim et al., 2020] effectively learns data representations by utilizing distance relationships between data points and proxies. In this work, it captures intra-modal relationships across different modalities, improving the distinction between rumors and non-rumors. The loss function $L(X)$ uses static proxy assignment [Movshovitz-Attias et al., 2017], selecting a data point from each class as a proxy. It minimizes the distance between the proxy and same-class data points while maximizing the distance to those from different classes. Modality embeddings $X$ are split into positive ($X^{+}$) and negative ($X^{-}$) sets relative to their proxies. The loss is defined as:\n$L(X) = \\frac{1}{|P_{+}|}\\sum_{p \\in P_{+}} \\log \\bigg( 1 + \\sum_{x \\in X^{-}} e^{\\alpha(s(x,p) - \\delta))} \\bigg) + \\frac{1}{|P_{-}|} \\sum_{p \\in P_{-}} \\log \\bigg( 1 + \\sum_{x \\in X^{+}} e^{-\\alpha(s(x,p) + \\delta))} \\bigg),$ \nwhere $\\delta > 0$ is the margin parameter (margin) and $\\alpha > 0$ (alpha) is the scaling parameter.\nAs shown in Figure 2, the model iteratively trains each modality $Z_{*}$ by fixing the parameters of other modalities, allowing it to refine intra-modal relationships without interference. This process is applied to each $Z_{*} \\in \\{Z_{i1}, Z_{i2}, Z_{t1}, Z_{t2}, Z_{b}\\}$. Proxy Anchor Loss, illustrated in Figure 3a, adjusts pull and push forces based on data point proximity, unlike traditional triplet loss [Schroff et al., 2015; Peng et al., 2023]. Proxy Anchor Loss is computationally efficient as it calculates distances between proxies and data points rather than between individual points, reducing computational load. Applied to text, image, and image-text features, it clusters data points of the same class while separating those of different classes, enhancing fake news detection."}, {"title": "3.4 Cross-Modal Tri-Transformer Fusion", "content": "This section introduces Cross-Modal Tri-Transformer Fusion (CMTTF) for integrating and processing image, text, and image-text data. CMTTF combines Cross-Modal Fusion [Chen et al., 2022] and Tri-Transformer [Choi and Kim, 2024], as illustrated in Figure 4a, with details on each component provided below.\nCross-Modal Fusion To capture semantic interactions between modalities, the Cross-Modal Fusion module integrates text-image similarities using dot product with softmax normalization and cosine similarity [Luo et al., 2018]. Given unimodal representations $Z_{i1}, Z_{t1}$, and additional representations $Z_{i2}, Z_{t2}, Z_{b}$, the process follows these steps: 1) Inter-modal Fusion and 2) Cosine Similarity.\nInter-modal Fusion integrates semantic interactions between text and image modalities (is illustrated in Figure 4b). Correlations between text features t and image features i are computed, normalized, and combined into a unified representation $C_{1}$, as shown:\n$f_{t i} = \\text{Softmax} \\bigg( \\frac{t \\cdot i^{T}}{\\sqrt{d}} \\bigg)$\n$f_{i t} = \\text{Softmax} \\bigg( \\frac{i \\cdot t^{T}}{\\sqrt{d}} \\bigg)$\n$C_{1} = f_{t i} \\oplus f_{i t}.$\nThis inter-modal fusion approach suits models like BERT and MAE, processing unimodal data without embedding normalization. The dot product operation captures direct text-image interactions while preserving scale information, enabling effective multimodal representation.\nCosine Similarity is computed between different combinations of the inputs $Z_{i2}, Z_{t2}$, and $Z_{b}$ to measure the relationships between the modalities. The similarities $S_{t i} (Z_{t2}, Z_{i2}), S_{t b} (Z_{t2}, Z_{b})$, and $S_{i b} (Z_{i2}, Z_{b})$ are computed using cosine similarity, defined for two vectors a and b as:\n$S(a, b) = \\frac{a \\cdot b}{||a|| \\times ||b||}$\nThe combined similarity is computed by weighting individual similarity scores with learnable parameters $w_{i2}, w_{b}, w_{t2}$, and adding a bias term b to account for modality differences. This similarity is processed through linear layers, ReLU activations, and batch normalization, producing $C_{2}$. BLIP2 components use cosine similarity as it effectively captures directional alignment, simplifies the fusion process, and maintains key correlations between modalities [Li et al., 2023]."}, {"title": "3.5 Fake News Detector", "content": "The fake news detector uses the fusion representation to predict whether news articles are fake or real. This fused representation is processed by a classifier composed of three fully connected layers, ReLU activations, and batch normalization, resulting in a binary classification. The classifier assigns labels of 'real' (0) or 'fake' (1) to the news content, where $\\hat{y} = [y_{0}, \\hat{y}_{1}]$ denotes the probabilities of the news being 'real' (0) or 'fake' (1). Cross-entropy is used to define the loss function $L(\\theta)$, where $\\theta$ represents the model parameters.\n$L(\\theta) = -y \\log(\\hat{y}_{1}) \u2013 (1 \u2013 y) \\log(\\hat{y}_{0})$\nThe total loss $L_{total}$ is defined, where $\\alpha$ controls the weight of this metric loss. The optimal value of $\\beta$ determined through a grid search over the range [0.1, 1] using a step 0.1. The model aims to minimize the total loss $L_{total}$ for each news data by learning $\\theta$ through back propagation.\n$L_{total} = L(\\theta) + \\beta \\cdot L(X)$"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "Datasets To evaluate the performance of CroMe, three datasets are used: Weibo [Jin et al., 2017], Weibo-21 [Nan et al., 2021], and Politifact [Shu et al., 2020]. The Weibo dataset contains 6,137 training articles (2,802 fake and 3,335 real) and 1,685 test articles (833 fake and 852 real). The Weibo-21 dataset includes 4,640 real and 4,487 fake articles, split into training and testing sets with an 8:2 ratio. The Politifact dataset consists of 381 training articles (246 fake and 135 real) and 104 test articles (74 fake and 30 real).\nTraining Settings Text encoding utilized the pretrained BERT model [Devlin et al., 2019] for Chinese in the Weibo and Weibo-21 datasets and the \"bert-base-uncased\" model for the Politifact dataset. Images were resized to 224 \u00d7 224 pixels and encoded using Masked Autoencoders (MAE) [He et al., 2022]. Image-text pairs were encoded with the pretrained BLIP2 model [Li et al., 2023], translating Chinese texts to English via the Google Translation API [Johnson, 2012]. The model employed five Adam optimizers for $Z_{*} \\in \\{Z_{i1}, Z_{i2}, Z_{t1}, Z_{t2}, Z_{b}\\}$, with a learning rate of $1 \\times 10^{-3}$, a batch size of 64, and was trained for 50 epochs. Metric Learning parameters were set as follows: an iterative frequency of"}, {"title": "4.2 Results and Analysis", "content": "CroMe's performance is evaluated against state-of-the-art models shown in Table 1. The evaluation metrics used include accuracy and precision, recall, and F1 scores for both real and fake news. CroMe achieved the highest accuracy of 0.974 in the experiments using Weibo, surpassing TT-BLIP by 1.3% and FND-CLIP by 6.7%. Similarly, when using Weibo-21, CroMe reached an accuracy of 0.917, outperforming CAFE by 3.5% and EANN by 4.7%. For both datasets, CroMe ranked 1st or 2nd in precision, recall, and F1 scores for both fake and real news. CroMe achieves 0.933 accuracy that closely matches FND-CLIP's 0.942. The reason may be that the dataset size is too small.\nCroMe excelled in fake news detection due to three key factors: 1) Advanced multimodal feature extraction through the BLIP2 model enhances CroMe's capabilities by capturing detailed features from both text and images, allowing for more precise feature extraction. This precision improves the model's ability to distinguish between real and fake"}, {"title": "4.3 Ablation", "content": "Ablation experiments evaluated the impact of each component in the CroMe model. Table 2 shows the results for the Weibo, Weibo-21, and Politifact datasets. The CroMe variants compared are:\n1. Without Image (BLIP2Img + MAE): Removing the image encoders and using only text encoders and other components.\n2. Without Text (BLIP2Txt + BERT): Removing the text encoders and using only image encoders and other components.\n3. Without BLIP2 (BLIP2Img + BLIP2Txt + BLIP2Img-Txt): Removing all BLIP2 components (text, image and image-text) and using only other encoders.\n4. Without BLIP2Img-Txt: Removing the image-text feature component of BLIP2, retaining the individual text and image BLIP2 encoders.\n5. Without CM (Cross-Modal): Removing the Cross-Modal Fusion responsible for integrating cross-modal similarities.\n6. Without MT (Metric Learning): Removing the Metric Learning module.\n7. Without TT (Tri-Transformer): Removing the Tri-Transformer component.\nThe ablation study highlights three components that most significantly impact accuracy when removed. First, excluding the BLIP2 text components causes a substantial drop in accuracy across all datasets, as they are critical for understanding textual context necessary for identifying fake news. Second, removing the Cross-Modal Fusion (CM) component weakens performance by preventing effective integration of inter-modal fusion and cosine similarity features from text and images. Lastly, the Metric Learning module (MT) is essential"}, {"title": "5 Conclusion", "content": "This study introduces Cross-Modal Tri-Transformer and Metric Learning (CroMe), a Multimodal Fake News Detection model. CroMe uses Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (BLIP2) to capture intra-modality and inter-modality relationships. The model includes four main modules: encoders (BERT, BLIP2-text, Masked Autoencoders (MAE), and BLIP2-image), a metric learning module (proxy anchor method), a feature fusion module (Cross-Modal Tri-Transformer Fusion, CMTTF), and a fake news detection module. CroMe uses BLIP2 for semantic information extraction, CMTTF for feature fusion, and the proxy anchor method for metric learning. It improves accuracy by 1.3% on the Weibo dataset and 3.5% on the Weibo-21 dataset compared to previous models. CroMe performed slightly below the state-of-the-art model by 0.9% in the case of Politifact, due to the smaller dataset size."}]}