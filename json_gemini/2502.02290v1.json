{"title": "FRAUD-RLA: A new reinforcement learning adversarial attack\nagainst credit card fraud detection", "authors": ["Daniele Lunghi", "Yannick Molinghen", "Alkis Simitsis", "Tom Lenaerts", "Gianluca Bontempi"], "abstract": "Adversarial attacks pose a significant threat to data-driven\nsystems, and researchers have spent considerable resources\nstudying them. Despite its economic relevance, this trend\nlargely overlooked the issue of credit card fraud detection. To\naddress this gap, we propose a new threat model that demon-\nstrates the limitations of existing attacks and highlights the\nnecessity to investigate new approaches. We then design a\nnew adversarial attack for credit card fraud detection, employ-\ning reinforcement learning to bypass classifiers. This attack,\ncalled FRAUD-RLA, is designed to maximize the attacker's\nreward by optimizing the exploration-exploitation tradeoff\nand working with significantly less required knowledge than\ncompetitors. Our experiments, conducted on three different\nheterogeneous datasets and against two fraud detection sys-\ntems, indicate that FRAUD-RLA is effective, even consider-\ning the severe limitations imposed by our threat model.", "sections": [{"title": "1 Introduction", "content": "In the first half of 2023, credit card payments in the euro area\naccounted for over 50 trillion euros [5]. Given the size of\nthe domain and its economic relevance, ensuring the robust-\nness of payment systems is paramount, and vast resources are\ncontinuously being spent on improving the quality of credit\ncard fraud detection systems. In particular, the role played by\nmachine learning can hardly be overestimated [1]. Research\non fraud detection, however, has focused on the statistical\nproperties of frauds [14, 48], and proper assessments of the\nrisk posed by adaptive fraudsters' strategies are lacking in the\nliterature. In most domains, robustness is assessed through\nadversarial attacks [38, 43], i.e., attacks designed against ma-\nchine learning models. Although many such attacks have been\ncreated over the past decade (see Section 2.2), the majority of\nthem focus on image recognition [11,43], posing a significant\nchallenge in terms of their generalization to credit card fraud\ndetection.\nNotably, only a handful of works have studied adversarial\nmachine learning in the context of credit card fraud detection.\nThe main works [10, 11] attack the same realistic fraud detec-\ntion engine called BankSealer [9]. In both works, the authors\nrightfully consider domain-specific challenges generally ab-\nsent in other adversarial works, such as the intricate feature\nengineering process performed in fraud detection. However,\nthey operate under the assumption that fraudsters can access\nthe customers' transaction history. As the authors point out,\nthis may be achieved through the introduction of malware into\nthe victim's devices. However, this considerably increases the\ndifficulty of performing any attack, as fraudsters must first\ncompromise the customer's device and observe past transac-\ntion history, which constitutes a significantly more complex\nundertaking than stealing or cloning a card. This may limit\nthe scalability of such attacks and, therefore, their capacity to\ncompromise the overall system security.\nOur work aims to fill the gap in the literature between the\nfields of credit card fraud detection and adversarial machine\nlearning. To achieve this, our work makes two contributions.\nFirst, it provides a novel formulation for adversarial attacks\nagainst fraud detection systems. Past works have discussed the\nlack of human supervision [30] and the significance of aggre-\ngated features [31]. These works focused on how constraints\nimpacted existing adversarial attacks against fraud detection\nbut did not lead to the development of a systemic analysis of\nthe threat posed. In this work, we overcome such limitations\nby proposing a new threat model (Section 2.1) specifically\ndesigned to tailor the main characteristics of credit card fraud\ndetection. The definition of the primary challenges and ad-\nvantages that fraudsters encounter compared to our domains\nresults in the design of Table 1, which demonstrates how\nexisting approaches fail to cope with unknown features, re-\nstricted access to the fraud detection engine, and the inability\nto exploit the absence of human investigation.\nOur main contribution is a novel adversarial attack against\ncredit card fraud detection systems based on Reinforcement\nLearning [46, RL]. We model the problem of generating fraud-\nulent transactions not detectable by the classifier as an RL\nproblem where the agent is assigned the task of crafting the\nfraudulent transactions. In that context, the agent is rewarded"}, {"title": "2 Background", "content": "This Section is composed of two main parts. First, we define\nthe threat model describing the target, i.e., the fraud detection\nsystem and the attackers. Then, we present existing attack\napproaches and compare them against this model."}, {"title": "2.1 Threat model", "content": "2.1.1 Credit Card Fraud Detection\nIn online payment systems, cardholders use their card to ac-\ncess a terminal and perform transactions. Essentially, a trans-\naction consists of an amount paid to a merchant by a card-\nholder at a given time [28]. Specific features, such as the card\nbrand and the merchant country, characterize cards and termi-\nnals. We call raw features these features and the ones directly\nchosen by the user, such as the transaction amount. Further-\nmore, cards and terminals have an identifier, allowing the\nfraud detection system to aggregate transactions based on the\ncard or terminal used in the process. These aggregations are\nthen used to generate a set of features called aggregated fea-\ntures. Such transactions can be built around the customer [9]\nor terminal [29] history. Raw and aggregated features are then\nused to train a fraud detection system. This system is built\non a composition of human-made rules and machine learn-\ning [8, 14], which combine to signal the most likely frauds to\nthe investigators.\n2.1.2 Attackers\nDiscussing attackers' goals, knowledge, and capabilities is\ncrucial to designing realistic attacks, which can be used for\nrobustness assessments [25].\nIn credit card fraud detection, the attackers' goal is straight-\nforward: deceive the fraud detection system into misclassify-\ning their frauds as genuine transactions. However, two main\ndetails differentiate this field from other domains. First, fraud-\nsters can access a limited set of cards, which may be blocked\nover time. Hence, they need to maximize the number of suc-\ncessful frauds from the beginning through an exploration-\nexploitation tradeoff [33]. This is particularly true as fraud\ndetection data are subject to concept drift [14], forcing the\nclassifiers to adapt to be up-to-date with reality. While these\ntransformations may be too slow to effectively block attacks,\ngiven the speed at which they could be performed, fraudsters\nmay discover that the attack patterns they found do not work\nonce the model has been updated. Furthermore, adversarial\nattacks generally work under the implicit assumption that\nhumans may observe the attacks and detect anomalies, forc-\ning the attackers' perturbation to be imperceptible [27, 32] to\nbypass such a human check. In fraud detection, impercepti-\nbility should not be part of the attackers' goal. Since humans\nobserve only a few suspicious transactions, bypassing the\nautomatic checks is enough to perform a successful attack.\nDefining attackers' knowledge is more complex. A com-\nmon principle in computer security literature is \"no security\nthrough obscurity\" [40], meaning that it is better to assume\nsecret information could always fall into the wrong hands.\nTherefore, considering the attackers' perfect system knowl-\nedge is generally better. Credit card fraud detection, however,\npresents a peculiar situation. While it is true that a skillful at-\ntacker may, in principle, retrieve any information (for instance,\nthrough social engineering [39]), such attacks are unlikely to\nbe scalable enough to pose a significant threat to fraud detec-\ntion systems. For this reason, we assume that attackers may\nknow the principles around which the system is built, like\nits features engineering process, but not the weights of the\ntrained classifier, which change over time.\nA second form of knowledge, specific to this application, is\ndata knowledge, not to be confused with the features knowl-\nedge described in [44]. While features knowledge focuses on\nknowing the features engineering technique applied by the\nclassifier, it still assumes that the attacker has complete knowl-\nedge of the observation location in the original data space.\nIn credit card fraud detection, however, each transaction is\nalso evaluated based on aggregated features. To know these,\nfraudsters must reconstruct the card's and terminal's trans-\naction history, which is challenging. Skillful attackers may\nachieve this through the use of malware previously injected\ninto the devices [11], but this poses a strong requirement on\nthe attackers' side and may limit the number of attacks they\ncan perform. For this reason, we work under the more general"}, {"title": "2.2 Adversarial Attacks", "content": "Let us use the defined threat model to design a taxonomy\nof existing adversarial attacks based on their applicability\nagainst credit card fraud detection. Coherently with our threat\nmodel, we focus here on attacks that do not require knowing\nthe target classifier, also called Black Box attacks [25]. First,\nbased on their approach, we group these attacks into four\nmain groups. Then, how some of these attacks have been used\nin the context of credit card fraud detection.\n2.2.1 Taxonomy\nImage recognition / Query-based Query-based attacks ac-\nquire information through queries, i.e., attempted attacks, and\nuse feedback provided by the target classifier to improve their\nattack quality. Most attacks designed against image recogni-\ntion systems fall into this category [7, 12, 13]. For instance,\nBoundary Attack [7] first samples the data space to find a\nvalid attack and then gets closer to the original observation by\nmoving along the decision boundary of the model. Notably,\nthis requires continuously estimating the decision boundary\nlocally, which can only be obtained by sending observations\nto the target model and observing the associated decisions.\nApplying these attacks to credit card fraud detection is ex-\nceptionally challenging. First, they have no mechanism to\noptimize the exploration-exploitation tradeoff, as the locality\nof their attacks means a constant number of queries for each\nattack. Similarly, image recognition attacks are designed to\nbe imperceptible to human eyes, making them incredibly in-\nefficient when such constraint is irrelevant [30]. Finally, they\nassume full data knowledge to estimate the local boundaries.\nSurrogate Model The idea behind surrogate models [17,\n18] is relatively straightforward. Since querying the target clas-\nsifier may not always be feasible or efficient, if the attacker\ncan access a labeled training set, they can train a surrogate\nmodel whose properties should mimic the ones of the target.\nAttackers then test their attack on the resulting classifier, also\ncalled surrogate, and use attacks transferability [18] to break"}, {"title": "3 FRAUD-RLA: Reinforcement Learning\nAgainst Credit Card Fraud Detection", "content": "In this Section we present our main contribution, a new attack\nagainst credit card fraud detection called FRAUD-RLA. The\ndesign of FRAUD-RLA involves three core elements:\n\u2022 Problem Formulation, where we formalize the con-\ncepts of transaction and fraud detection engine and the\nattacker's task as discussed in Section 2.1.\n\u2022 Task definition, where we reformulate the problem as a\nsingle-step Partially Observable Markov Decision Pro-\ncess (POMDP) [35,46], allowing us to employ reinforce-\nment learning algorithms.\n\u2022 Solution design, where we describe how we find the so-\nlution through Proximal Policy Optimization [42, PPO],\na gradient-based technique capable of online learning\nthe best attack policy.\n3.1 Problem Formulation\nTo formulate the problem definition of the attack, we define\nthe attack's objects (the transactions), the target (the classifier),\nand the attacker. First, we define a transaction as a triplet:\n$x = (x_c, x_k, x_u)$ (1)\nwhere $x_c \\in R^C$ are the features controllable by the attacker,\nsuch as the amount of the transaction, $x_k \\in R^K$ are the features\nknown to the attacker, such as the card number, and $x_u \\in R^U$"}, {"title": "3.2 RL environment", "content": "We model the environment as a single-step Partially Observ-\nable Markov Decision Process (POMDP) [35, 46], denoted\nas M. Formally, this can be written as: M = (S,O,A,T,R,\u03a9),\nwhere S is state space, O is the observation space, A is action\nspace, T is the transition function, R is the reward function,\nand 2 is the observation function. Each of these components\nis defined as follows.\n\u2022 S is a continuous space of size $R^{U+K+C}$, i.e. every possi-\nble transaction.\n\u2022 O is the continuous space set of possible observations of\nsize $R^K$, i.e. every possible known features.\n\u2022 A is a continuous action space of size $R^C$.\n\u2022T:S\u00d7A\u2192 S is a deterministic transition function that\nmaps each state-action (s, a) pair to a next state s'. Note\nthat s is always an initial state and that s' is always a\nterminal state since there is a single step to the POMDP.\n\u2022R:S\u00d7A\u00d7S\u2192 {0, 1} is the reward function defined as\n$R(s,a,s') = \\begin{cases}\n1 & if $ is classified as genuine\\\\\n0 & otherwise\n\\end{cases}$ (2)\n\u2022 \u03a9: SO is the observation function that extracts the\nobservation from the state, i.e. extracts xk from x.\n3.3 RL agent\nWe identify Proximal Policy Optimization [42, PPO] as a suit-\nable single-agent Deep Reinforcement Learning algorithm for\nFRAUD-RLA due to its ability to handle continuous action\nspaces. Additionally, PPO is known to require little hyperpa-\nrameter tuning compared to other Deep RL methods and has\nbeen proven to perform well in a wide variety of tasks [41,50].\nThe high-level working principle of FRAUD-RLA is shown\nin Algorithm 1. At each round, FRAUD-RLA receives the\nfixed known features as input (line 5). It passes them to PPO,"}, {"title": "3.3.1 Correlations in the action space", "content": "Although the network architectures are similar to other works\nin the field of single-agent RL with continuous action spaces,\nwe differ from most of them by learning the parameters (both\nthe means and the covariance matrix) of a multivariate normal\ndistribution for the action space. In contrast, most works in the"}, {"title": "3.3.2 Partial observability", "content": "Note that, unlike other works in the field of partially observ-\nable RL [22, 45], we do not use recurrent neural networks\n(RNN). The reason is that RNNs are typically used for an\nagent to remember what it has observed in the previous steps\nof an episode. Since we work in an environment where there\nis only one step in an episode, there is no need for recurrent\nnetworks."}, {"title": "4 Experiments Design", "content": "In this section, we present an experimental analysis of our\nwork. We first describe our experimental setup including our\ndatasets, our baselines, and our methodology.\n4.1 Experimental Setup\n4.1.1 Datasets\nWe evaluate our approach on three different datasets, each\nselected to represent different properties of fraud detection.\nBecause class balance does not directly affect FRAUD-RLA,\nall datasets have been balanced for these experiments. To\nguarantee Mimicry is not negatively affected by this change,\nwe train it using only genuine transactions. Our three datasets\nare as follows.\n\u2022 Generator Dataset generated employing the synthetic\ncredit card fraud detection generator from [28]. The gen-\nerator, used in previous works on fraud detection [29,36],\nis built on a set of customers interacting with terminals to\ncreate transactions, where frauds are inserted according\nto previously defined patterns. Contrary to other works,\nthe resulting datasets maintain the features' semantics,\nallowing aggregated features to be composed. This al-\nlows us to divide features into purely transaction-based\n(like the amount), aggregated on the customer, and ag-\ngregated on the terminal. Depending on the threat model,"}, {"title": "4.1.2 Baseline: Mimicry", "content": "As discussed in Section 2.1, an attack that does not require\naccess to a training set before it begins is generally easier to\ndeploy. However, no attack reported in related literature can\nbe employed against fraud detection engines under this condi-\ntion. Hence, to provide a fair baseline in our experiments, we\nassume that an attacker may access an unlabeled training set,\nwhich comprises an easier-to-meet condition than accessing\nthe labeled one. Since data in fraud detection are typically\nhighly skewed towards the genuine class [15], fraudsters may\nperform a Mimicry attack under this condition, therefore mod-\neling genuine users' behavior. Since FRAUD-RLA does not\nneed or use such a training set, we effectively put FRAUD-\nRLA at a disadvantage as we compare it with attacks operat-\ning in a less challenging environment. Although this could\npotentially result into a pessimistic assessment of its relative\neffectiveness, still, it allows us to provide a reasonable base-\nline for FRAUD-RLA performance. Should FRAUD-RLA\nremain competitive in such a setting, this would further show-\ncase its effectiveness."}, {"title": "4.1.3 Methodology", "content": "The first step of our evaluation is constructing our fraud de-\ntection engine. For this work, we imagine a simplified en-\ngine comprising two models: a machine-learning and a rule-\nbased classifier. In line with previous works on credit card\nfraud detection [2, 16], we test two different machine learning\nclassifiers: a Random Forest (RF) and a feed-forward neural\nnetwork (NN). Both algorithms are trained with a standard\nrandom grid cross-validation strategy for hyperparameters\ntuning [37]. To penalize strategies based on extreme values,"}, {"title": "4.2 Experimental Findings", "content": "Next, we present our experimental analysis first using the\nsynthetic data generator, and then, using real data and the\nSKLearn generator.\nSynthetic data generator. First, we analyze the Mimicry\nresults, which provide a baseline for attackers' performance.\nTo do so, we first compare the performance of the different\nMimicry techniques we used. We show the analysis results\nin Table 5 and Table 6, where we measure the attacks' suc-\ncess rate over different settings, where each setting is defined\nby the types of fixed, known features (FIXED in the tables)\nand those unknown to the attacker (UNKNOWN). We group\nhere features as terminal-based (T) and customer-based (C)\naggregations. For datasets without feature semantics, such as\nKaggle and SKLearn, we will instead use the percentage of\nfeatures belonging to each group. This table, comparing the\nperformance of multiple baselines trained over a dataset of\n1000 observations (1K) or over the full Training Set (100%),\nallows us to make some considerations.\nFirst, increasing the size of the training set over 1000 sam-\nples does not significantly improve the performance of the\nalgorithms. Second, while the recall of Random Forest and\nNeural Network on the test is set is practically the same (as\nshown in Table 4), all Mimicry attacks achieve a significantly\nhigher success rate against Neural Networks, showing the\nsuperior robustness of Random Forests. This aligns with pre-\nvious findings in the literature, where Random Forests proved\nto be more robust than deep learning algorithms against tradi-\ntional adversarial attacks [19]. Finally, when fraudsters con-"}, {"title": "5 Conclusions and future work", "content": "Adversarial attacks are a growing threat, and credit card fraud\ndetection systems are sensitive targets. The lack of research at\nthe intersection of the two domains is a potential vulnerability\nfor existing fraud detection engines. It increases the likeli-\nhood of catastrophic consequences should fraudsters identify\nand implement an effective attack strategy. This work aimed\nto mitigate this problem by analyzing the domain's main fea-\ntures, focusing on differences with more traditional domains,\nsuch as image recognition and malware detection.\nUnderstanding the challenges attackers face is crucial when\ndesigning and modeling the threat of advanced machine learn-\ning for fraud detection. Therefore, we expanded existing threat\nmodels to adapt them to credit card fraud detection. Specifi-\ncally, we updated existing approaches to consider the lack of\nhuman supervision, the limited data knowledge and control,\nand the exploration-exploitation tradeoff. Using the defined\nthreat model as a blueprint, we developed FRAUD-RLA, a\nnovel attack designed explicitly to tackle the aforementioned\nissues. To do so, we modeled the problem of finding a success-\nful fraudulent pattern in the shortest possible time as a Par-\ntially Observable Markov Decision Process, where the known\nfixed features represent the visible state and the action corre-\nsponds to selecting the optimal values for the controllable fea-\ntures. To optimize this problem, we employed Proximal Policy\nOptimization (PPO), a robust algorithm able to optimize con-\ntinuous policies with little hyperparameter optimization. Our\nexperiments show that FRAUD-RLA quickly achieves a high\naverage success rate under most settings, consistently beating\nthe baselines without optimizing the hyperparameters against\nthe different datasets and target classifiers.\nIt is worth noting that we do not aiming at developing\nan \"off-the-shelf\" attack that could be directly applied in a\nreal-world scenario. The presence of categorical variables,\neventual limitations to the frequency at which frauds can be\nperformed without raising any alarm, and, in general, the need\nto adapt the attack to various challenges that may arise in the\nreal case, all serve to illustrate why FRAUD-RLA does not\nconstitute an immediate threat to any real-world system, and\nis not, as such, a valuable tool for malignous agents. On the\ncontrary, improving our understanding of adversarial security\nof fraud detection systems will be crucial in developing effec-\ntive defenses. As security is frequently evaluated through the\nlens of red teaming [4], it is imperative to use the appropriate\ntools for comprehensive assessments and improvements, and"}, {"title": "6 Ethics Considerations", "content": "Machine learning systems are employed in various applica-\ntions, and research should always make them more secure.\nUnresponsable disclosure of vulnerabilities can give attackers\na head start and severely limit the systems' security. Research-\ning vulnerabilities, however, remains crucial to mitigate the\nrisk posed by zero-day threats. Adversarial machine learning\nliterature, for instance, is built on the assumption that pub-\nlished attacks improve our understanding of machine learning\nmodels' vulnerabilities.\nIn this work, we did not aim to exploit vulnerabilities of\nany specific system. Instead, we used open research as a base-\nline to define how fraud detection engines work. Our threat\nmodel and the attack we designed are not intended to directly\napply to any system in production. As stated in Section 5, this\nwould require updating the attack to tackle issues like cate-\ngorical variables, transaction frequency caps, and, in general,\nmultiple challenges real-world systems present that are not\ndescribed in the literature. However, reinforcement learning\ndoes threaten fraud detection, as it has been shown to do in\nmalware detection [23] and as we showed in this work. In\nreality, the lack of research on the topic is arguably one of the\nmain vulnerabilities, as it increases the chance attackers may\nfind successful strategies that employ reinforcement learning\nbefore the community has a solid understanding of the threat.\nTherefore, this work's primary goal is to provide a frame-\nwork to study these attacks and how they can challenge sys-\ntems' robustness. First, this could be used by fraud detection\npractitioners to evaluate their systems, possibly extending and"}, {"title": "7 Open Science", "content": "The threat model and the theoretical analysis were based\non published works, and this paper is designed to be fully\nopen and reproducible. Specifically, the Generator code is\navailable at [28], the Kaggle Dataset at Kaggle Dataset, and\nSKLearn is an open library. The code of the experiments will\nbe released and openly available on Github, toghether with\nthe instructions to set up the datasets and the experiments.\nFinally, the experiments were designed to be open and fair.\nMetrics, hyperparameters, and datasets were chosen for their\napplicability to credit card fraud detection and are coherent\nwith previous works and the problem."}]}