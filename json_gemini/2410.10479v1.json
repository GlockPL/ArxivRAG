{"title": "TMGBENCH: A SYSTEMATIC GAME BENCHMARK FOR EVALUATING STRATEGIC REASONING ABILITIES OF LLMS", "authors": ["Haochuan Wang", "Xiachong Feng", "Lei Li", "Zhanyue Qin", "Dianbo Sui", "Lingpeng Kong"], "abstract": "The rapid advancement of large language models (LLMs) has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing attention.\nTo evaluate the strategic reasoning capabilities of LLMs, game theory, with its\nconcise structure, has become the preferred approach for many researchers. How-\never, current research typically focuses on a limited selection of games, resulting\nin low coverage of game types. Additionally, classic game scenarios carry risks\nof data leakage, and the benchmarks used often lack extensibility, rendering them\ninadequate for evaluating state-of-the-art models. To address these challenges, we\npropose TMGBENCH\u00b9, a benchmark characterized by comprehensive game type\ncoverage, novel and diverse scenarios, and flexible game organization. Specifi-\ncally, we incorporate all 144 game types summarized by the Robinson-Goforth\ntopology of 2x2 games, which are constructed as classic games in our benchmark.\nFurthermore, we employ synthetic data generation techniques to create diverse,\nhigher-quality game scenarios through topic guidance and human inspection for\neach classic game, which we refer to as story-based games. Lastly, to provide a\nsustainable evaluation framework adaptable to increasingly powerful LLMs, we\ntreat the aforementioned games as atomic units and organize them into more com-\nplex forms through sequential, parallel, and nested structures. We conducted a\ncomprehensive evaluation of mainstream LLMs, covering tests on rational rea-\nsoning, reasoning robustness, Theory-of-Mind capabilities, and reasoning in com-\nplex game forms. The results revealed that LLMs still have flaws in the accuracy\nand consistency of strategic reasoning processes, and their levels of mastery over\nTheory-of-Mind also vary. Additionally, 01-mini, the latest reasoning model from\nOpenAI, was also evaluated across the sequential, parallel, and nested game struc-\ntures and reached accuracy rates of 66.6%, 60.0%, and 70.0%, respectively, high-\nlighting the challenges posed by TMGBENCH.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancement of large language models (LLMs) has reshaped the paradigm of artificial\nintelligence, achieving breakthroughs across various domains (Zhao et al., 2023; Huang & Chang,\n2022; Lewkowycz et al., 2022; Huang et al., 2022; Paranjape et al., 2023). These achievements\nare largely attributed to LLMs' ability to assimilate vast amounts of knowledge during training,\nemerging with the capacity to organize information at a coarse level and link knowledge at a fine-\ngrained level through their internal representations (Min et al., 2023; Zhao et al., 2023). These core\ncapabilities have driven the success of LLMs in numerous reasoning tasks, including mathematical\nreasoning (Hendrycks et al., 2021; Zhang et al., 2023), commonsense reasoning (Sap et al., 2019;\nBisk et al., 2020), logical reasoning (Lei et al., 2023), and strategic reasoning (Lor\u00e8 & Heydari,\n2023; Duan et al., 2024). Among these, strategic reasoning has attracted considerable attention due\nto its multi-agent nature and close association with social intelligence (Gandhi et al., 2023).\nStrategic reasoning refers to the cognitive process of anticipating, planning, and responding to\nothers' actions to achieve specific objectives within competitive or cooperative contexts (Zhang\net al., 2024a). Consequently, game scenarios\u2014naturally involving both cooperation and com-\npetition-have intuitively become a fertile ground for studying LLMs' strategic reasoning abili-\nties (Brookins & DeBacker, 2023). In particular, researchers have engaged LLMs in game-playing,\nanalyzing their decision-making behaviors and evaluating their strategic intelligence in such scenar-\nios (Duan et al., 2024). The Prisoner's Dilemma, as one of the most classic game theory scenarios,\nhas been extensively studied in this context (Herr et al., 2024). Additionally, other traditional games\nsuch as the Battle of the Sexes (Kreps, 1990), the Stag Hunt (Carlsson & Van Damme, 1993), and\nthe Dictator Game (Forsythe et al., 1994) have also drawn significant attention. These studies pro-\nvide initial insights into the strategic reasoning capabilities of LLMs (Horton, 2023; Brookins &\nDeBacker, 2023; Phelps & Russell, 2023; Akata et al., 2023; Li et al., 2023; Aher et al., 2022).\nHowever, current research has three major limitations, hindering a comprehensive, robust, and sus-\ntainable evaluation of LLMs' strategic reasoning capabilities: (1) Limited coverage of game types:\nMost studies focus on a handful of classic games without considering the full diversity of game\nstructures. (2) Potential risk of game scenario leakage: Classic game scenarios are likely to be\npresent in the training corpus, raising concerns over data leakage. (3) Poor extensibility of game\nforms: Existing studies primarily focus on a narrow range of game forms, which may no longer\nsuffice to challenge high-performing LLMs such as o1-mini from OpenAI.\nTo address the above issues, we introduce TMGBENCH, a benchmark that encompasses a com-\nprehensive range of game types, features synthesized game scenarios, and supports scalable and\nreorganizable game forms. Specifically, to address the first issue, we include all 144 game types de-\nfined by the Robinson-Goforth topology of 2x2 games (Robinson & Goforth, 2005). This topology\nencompasses a variety of game structures based on different numerical payoff matrices, including\nbut not limited to classic games like the Prisoner's Dilemma(\u00a72.2). To address the second issue, we\nemploy synthetic data generation techniques to create five different story-based games for each clas-\nsic game. In essence, a story-based game is a contextual framing counterpart of its corresponding\nclassic game, sharing the same structure but differing in context (Lor\u00e8 & Heydari, 2023). To ensure\nhigh-quality data synthesis, we introduce two additional steps: topic control and human inspection.\nWe first define a set of topics commonly associated with cooperation and competition, such as busi-\nness and law, to guide the data generation process. Then, to ensure that the synthesized games meet\nthe required game structures and are easily understandable, we conduct rigorous human inspection\n(\u00a72.3). To address the third issue, we propose three forms for expanding and organizing games:\nsequential, parallel, and nested. Using the above constructed games as atomic units, we reorganize\nthem into these complex forms to assess the strategic reasoning of LLMs. The sequential and par-\nallel forms evaluate the model's capacity for sequential and parallel decision-making, respectively,\nwhile the nested form explores the LLMs' multi-layered strategic reasoning abilities (\u00a72.4).\nBased on TMGBENCH, we conduct comprehensive analyses and evaluations of current mainstream\nLLMs (\u00a73), including assessments of rational reasoning, reasoning robustness, Theory-of-Mind\n(ToM) capabilities, and reasoning in complex game forms, leading to the following key findings:\n(1) Advanced LLMs like gpt-40 demonstrate strong strategic reasoning, with an accuracy rate over\n80%, but struggle to generalize across various contexts and scenarios. Models like claude-3-5-sonnet\nfurther reveal this inconsistency, with performance variability marked by coefficients nearing 0.5.\n(2) Although GPT models often perform well, their reasoning inconsistencies on certain task sub-\ntypes are marked by an asymmetric pattern, which is the main cause of the statistical biases.\n(3) Several top-tier LLMs demonstrate stable first-order ToM abilities, with some effectively uti-\nlizing second-order ToM for comparable tasks. In contrast, models such as Llama-3.1-70B appear\nrestricted to first-order reasoning.\n(4) Complex-form games that are derived from atomic units in TMGBENCH present considerable\nchallenges for LLMs, including those with strong reasoning abilities like o1-mini from OpenAI,\nwhich often struggle as the number of games increases."}, {"title": "2 TMGBENCH", "content": ""}, {"title": "2.1 BENCHMARK OVERVIEW", "content": "TMGBENCH is a benchmark designed to evaluate the strategic reasoning capabilities of LLMs in\ngame-theoretic scenarios, illustrated by Figure 1. It comprehensively covers 144 types of games\n(see \u00a72.2), with each type containing multiple instances (in each instance, there are two players and\neach player can choose between two strategies, resulting in four possible situations), which can be\ncategorized into classic and story-based settings. Notably, the story-based instances are produced\nusing synthetic data generation techniques and are grounded in real-life themes, effectively mitigat-\ning the issue of data leakage (see \u00a72.3). Furthermore, each game in TMGBENCH can be treated as\nan atomic unit, and multiple atomic games can be structured in a more complex task with parallel,\nsequential, or nested form (see \u00a72.4). These complex scenarios effectively facilitate the evaluation\nof advanced LLMs' abilities in parallel, sequential, and multi-layered decision-making. To precisely\nevaluate the reasoning abilities of LLMs, we use their performance in inferring the optimal strategy\ncombination, i.e., the Nash equilibrium, as the evaluation criterion. Additionally, the designed eval-\nuation metrics provide a fine-grained assessment of the robustness and self-consistency of LLMs'\nstrategic reasoning abilities (see \u00a72.5)."}, {"title": "2.2 GAME TOPOLOGY", "content": "Although previous research has explored LLMs' reasoning abilities within the context of game the-\nory, existing studies have primarily focused on a few well-known games, such as the Prisoner's\nDilemma, Battle of the Sexes, and Stag Hunt (Brookins & DeBacker, 2023; Phelps & Russell, 2023;\nGuo, 2023). However, these studies cover a limited game types, resulting in incomplete evaluations.\nThereby, a broader variety of games is urgently needed to conduct a systematic assessment of LLMs.\nTo address this, we incorporate 144 game types (we later refer to a type as an equivalence class)\nbased on the Robinson-Goforth topology of 2x2 games (Robinson & Goforth, 2005). Classic games\nlike the Prisoner's Dilemma belong to one of the equivalence classes within this topology. Specif-"}, {"title": "2.3 CONTEXTUAL FRAMING", "content": "Relying on the Robinson-Goforth topology, we can systematically construct all types of classic\nsetting tasks. However, this alone is insufficient, as games often take place in diverse real-life\ncontexts, involving different topics, types of participants and their preferences. Such contextual\nframing of games introduces new challenges for LLMs (Lor\u00e8 & Heydari, 2023).\nTo further explore LLMs' strategic reasoning capabilities in real-world scenarios, we use classic\ngames as seed data and employ synthetic data generation techniques, leveraging GPT-40 to construct\nstory-based setting tasks. Specifically, in story-based setting tasks, we replace the pure game infor-\nmation of classic setting tasks with real-life scenarios, covering topics such as business, law and\ntransportation. Additionally, the two players are substituted with characters representing broader\nsemantics (e.g., people, animals, organizations, and even nations), and the payoff values are trans-\nformed from pure numbers into specific states or rewards relevant to the characters. For each classic\nsetting task, we generate 5 corresponding story-based setting tasks.\nTo ensure high-quality data generation, we undertake the following steps: First, we use GPT-40\nto synthesize the contextual data. Second, we design precise prompts to ensure the generated data\nadhere to the given game structures. Third, we select topics from real-life scenarios where strategic\ninteractions are common, guiding the data generation process. Finally, we conduct rigorous human\nreviews to ensure the data's quality and diversity.\nMore details on the data generation process, prompts, human review procedures, and topic distribu-\ntion of the data can be found in Appendix C."}, {"title": "2.4 COMPLEX FORMS", "content": "The 2x2 games in the topology represent a highly condensed game structure. However, in real\nlife, we often encounter more complex game forms, such as making continuous decisions, making\nmultiple decisions simultaneously, or considering the impacts of one decision on another.\nTo evaluate LLMs' strategic reasoning abilities with more constraints, we treat the aforementioned\nindividual games as atomic games and expand them in three forms: sequential, parallel, and nested.\nThe organization of these forms is illustrated in Figure 2. Specifically, in the sequential form, we\nrandomly sample multiple games from the story-based games, requiring the LLM to make decisions\nsequentially. Only if the LLM provides correct answers for all games is it considered to have made\ncorrect decisions. In the parallel form, the LLM is given multiple randomly sampled games and"}, {"title": "2.5 EVALUATION METRICS", "content": "As explained in Section 2.2, our benchmark are perfectly suitable to display in a 12x12 square table,\neach grid representing one of the 144 equivalence classes. In the evaluation process we conduct\nrepetitive tests in every data point of each equivalence class. Each test starts with the input of the\nsetting (classic/story-based) and the question, and ends with LLM's response containing a list of\nchoices corresponding to multiple choices or no choice (when the given list is empty).\nNotation. For notation, we assign $Freq_{i,j,o}$ as the frequency of the o-th choice happening to be in\nthe tests of the grid at i-th row, j-th column, where the 1, 2, 3 and 4-th choice correspond to the\nupper-left, upper-right, lower-left and lower-right quarter-grid respectively.\nInconsistency Heat Map. According to conclusions of the Robinson-Goforth topology (Robinson\n& Goforth, 2005), we convert the standard answer of each equivalence class into a heat map named\nthe standard heat map, with the coloured quarter-grid to be the choice in the standard answer. Simi-\nlarly, as for practical result provide by LLMs, we set the value of $Freq_{i,j,o}$ as the colour depth of each\nquarter grid, which builds up the practical heat map. Naturally, we subtract the standard heat map\nfrom the practical heat map in an element-wise manner to get the inconsistency heat map, which is\na standardised tool for our evaluation, shown in Figure 3.\nInconsistency Degree. In order to display the quantified performance of LLMs, we extract inconsis-\ntency degree from a map, which helps reveal the gap between LLMs' response and standard answer,\nand it is defined as\n$ID = \\frac{1}{144} \\sum_{i=1}^{12} \\sum_{j=1}^{12} \\frac{1}{4} \\sum_{o=1}^{4} Freq_{i,j,o}^2$\nwhere $Freq_{i,j,o}$ indicates the the difference (between the LLM's answer and the standard answer)\nof frequency of the o-th choice at i-th row, j-th column.\nBias Degree. Owing to the symmetric property of the topology framework of 2\u00d72 matrix games, the\ndistribution of answers over the heat map has axial symmetry by the counter-diagonal (Figure 4).\nMotivated by this elegant property, we set up another metric to evaluate the bias degree of LLMs'\nanswers, which we expect robuster LLMs to display lower degrees of bias. The bias degree reflects\nthe stability and symmetry of LLMs' strategy, and it is defined as\n$BD = \\frac{1}{144} \\sum_{i=1}^{12} \\sum_{j=1}^{12} \\frac{1}{4} \\sum_{o=1}^{4} (Freq_{i, j, o} \u2013 Freq_{j, i, ref_o})^2$\nwhere the meaning of $ref_o$ is the index of choice o's counterpart considering the reflection operation\nby the counter-diagonal, and we have the mapping relation: {1,2,3,4}{4, 2, 3, 1}. (e.g. $ref_1$ =\n4 means that the reflection counterpart of choice 1 is choice 4, vice versa)\nPerfect Accuracy Rate. In addition to the metrics mentioned above, we also set up a more rigorous\nmetric named perfect accuracy rate, which ignores the partially correct answer and only considers\nperfectly correct answer in each test, and it is defined as\n$PAR = \\frac{1}{144} \\sum_{i=1}^{12} \\sum_{j=1}^{12} \\frac{1}{T} \\sum_{t=1}^{T} I\\{rsp_{t,i,j} = std_{i,j}\\}$"}, {"title": "3 ANALYSIS", "content": ""}, {"title": "3.1 OVERVIEW OF LLMS' PERFORMANCE", "content": "Overall, we select several SOTA models according to Open LLM Leaderboard (Fourrier\net al., 2024) and conduct extensive experiments on TMGBENCH. These models include GPT\n(gpt-40-2024-05-13, gpt-4o-mini-2024-07-18, gpt-3.5-turbo-0125), Claude (claude-3-5-\nsonnet-20240620, claude-3-haiku-20240307), Llama (Llama-3.1-8B, Llama-3.1-70B), and\nQwen (Qwen2-72B). We perform 4 independent tests on each data point, covering both the clas-\nsic setting and the story-based setting. Basically, we conduct 2,880 tests to generally evaluate a\ncertain model. During the evaluation, we set the temperature of the tested LLMs to 0 or near 0,\nensuring the lowest degree of uncertainty and enhancing the faithfulness of our evaluation. More\ndetails of the evaluation process are provided in Appendix C.1.\nGames in TMGBENCH are not easy for most LLMs. First we overall evaluate how well LLMs\ncan behave on the classic setting tasks of our benchmark, to assess their basic capability of strategic\nreasoning. We initially adopt two basic prompting methods: Direct Answer (DA) prompting and\nChain-of-Thought (CoT, (Wei et al., 2022)) prompting, which represent shallower, faster thinking\npatterns and deeper, slower thinking patterns, respectively.\nAs seen from Table 1, gpt-40, gpt-40-mini and claude-3-5-sonnet are more capable compared to\nothers, with a high overall accuracy rate (\u2248 80%) and low inconsistency and low bias score (\u2248\n5%). Specifically, as shown in Figure 5, gpt-4o performs the best on 1-tasks, gpt-4o-mini beats\nothers on 2-tasks, and claude-3-5-sonnet are relately better at 0-tasks. Moreover, comparing the\nperformance of employing DA prompting and CoT prompting, we find that CoT prompting almost\nprovides comprehensive improvement but few exceptions like the $PAR_2$ of Llama-3.1-70B.\nDespite the excellent performance of the top-tier models (gpt-40 and claude-3-5-sonnet), other mod-\nels often do not exhibit robust performance across all 3 different types of tasks. The inconsistency"}, {"title": "3.2 FINDINGS OF LLMS' BEHAVIOURS", "content": "LLMs demonstrate first/second-order ToM abilities. In tasks across all equivalence classes,\n1-tasks have the lowest reasoning difficulty because at least one player has a dominant strategy,\nwhich means the player can make an unconditionally optimal decision regardless of the counter-\npart's choice. In such cases, once a player (denoted as A) can make this unconditionally optimal\ndecision, their counterpart (B) can, using first-order Theory-of-Mind (ToM), easily determine the\nbest response for themselves (B).\nThis insight motivated us to apply FoToM prompting to LLMs, representing the First-order Theory-\nof-Mind thinking, to aid in solving these tasks. As seen in Table 2, top-tier models like gpt-4o show\nimprovement in both 0-tasks and 1-tasks when utilizing FoToM. Model claude-3-5-sonnet improves\non 1-tasks and 2-tasks, and gpt-40-mini displays a significant surge in performance on 1-tasks and so\ndoes Llama-3.1-70B on 2-tasks. However, for models like Llama-3.1-8B and Qwen2-72B, FoToM\ndoes not seem to provide any prominent advantage and may even result in worse performance.\nNotably, no LLM achieves overall improvement across all task categories by merely using first-\norder ToM, and 0-tasks appear to be the most challenging for LLMs to solve.\nFurthermore, we wondered if LLMs display some ability to use first-order ToM could also be capable\nof second-order ToM. According to Liddle & Nettle (2006), higher-order ToMs are generally more\ndifficult to master than first-order ToM. Thus we selected only advanced models that demonstrated\nproficiency in first-order ToM to attempt solving specific tasks using Second-order Theory-of-Mind\n(SoToM) prompting. As seen in Table 2, models like gpt-4o, gpt-40-mini and claude-3-5-sonnet\nshow consistent performance when applying second-order ToM to tasks they are already capable of\nsolving better with first-order ToM. However, the improvements from using SoToM generally do not\nexceed those achieved with first-order ToM. In addition, Llama-3.1-70B's underperformance with"}, {"title": "4 RELATED WORK", "content": "Strategical Reasoning of LLMs. Large language models have made notable breakthroughs in rea-\nsoning tasks, such as mathematical, causal, and commonsense reasoning, enabling their increasing\nuse in complex tasks that support human decision-making (Imani et al., 2023; K\u0131c\u0131man et al., 2023;\nZhao et al., 2024). This progress has sparked a growing interest in studying their strategic reasoning\ncapabilities (Zhang et al., 2024a). Game theory, with its highly abstract representation of real-world\nstrategic scenarios, has garnered significant attention from researchers (Duan et al., 2024; Huang\net al., 2024). The prisoner's dilemma, as one of the most classical games, has been widely used to\nevaluate the strategic reasoning abilities of LLMs (Brookins & DeBacker, 2023; Guo, 2023; Akata\net al., 2023; Phelps & Russell, 2023; Xu et al., 2023). In addition, several well-known game theory\nscenarios, such as the Dictator Game (Horton, 2023; Fan et al., 2023; Brookins & DeBacker, 2023),\nthe Ultimatum Game (Aher et al., 2022), the Public Goods Game (Li et al., 2023) and the Battle\nof the Sexes (Akata et al., 2023), have been employed to evaluate LLMs' capabilities. However,\ncurrent studies often focus on individual games, resulting in incomplete assessments and less ro-\nbust conclusions. To address this, we propose TMGBENCH, a benchmark for evaluating LLMs by\n2x2 games, where its atomic games can be further organized using sequential, parallel, and nested\nformats to provide an in-depth evaluation of the SOTA models gpt-40 and 01-mini.\nTheory-of-Mind of LLMs. Theory-of-Mind (ToM) refers to the ability to understand and infer\nhuman mental states (Premack & Woodruff, 1978). Due to the multi-player nature of game theory,\nplayers' ability to reason about the \u201cminds\u201d of other participants is crucial. Existing research has\ninitiated discussions on whether machines possess ToM capabilities. For instance, Kosinski (2023)\nsuggested that ToM might emerge spontaneously in LLMs, as demonstrated through assessments\nusing false-belief tasks. However, (Ullman, 2023) argued that such successes are fragile, easily\ndisrupted by minor perturbations that would not affect an entity genuinely possessing ToM. Never-\ntheless, many researchers propose enhancing LLMs' strategic reasoning abilities by incorporating\nToM. Guo et al. (2023) designed the Suspicion-Agent, which integrates a ToM-aware planning ap-\nproach that leverages higher-order ToM capabilities, considering not only what the opponent might\ndo (first-order ToM) but also what the opponent believes the Suspicion-Agent will do (second-order\nToM). Additionally, Yim et al. (2024) introduced a ToM planning method in the Guandan poker\ngame, Liu et al. (2024) proposed an intention-guided mechanism, Xu et al. (2023) developed Prob-\nabilistic Graphical Modeling, and Zhang et al. (2024b) introduced K-Level-Reasoning, all utilizing\nToM to enhance LLMs' strategic reasoning. Given the broad application of ToM, this paper lever-\nages TMGBENCH to comprehensively evaluate LLMs' ability to employ first-order and second-\norder ToM reasoning techniques for strategic reasoning."}, {"title": "5 CONCLUSION", "content": "In this work, we introduce TMGBENCH, a benchmark for systematically evaluating the strategic\nreasoning abilities of LLMs by 2x2 matrix games. Based on Robinson-Goforth topology, we de-\nvelop the classic setting tasks, and introduce various narratives based on story contexts generated by\nGPT-40. By utilizing TMGBENCH, we can identify current flaws in LLMs' performance on these\ntasks, such as low accuracy rates and unstable inconsistency and bias degrees, even though the task\ndifficulty is relatively moderate compared to many others. Additionally, when employing prompts\nto elicit their Theory-of-Mind thinkings on these tasks, some LLMs show improved performance,\nindicating that LLMs can, to some extent, master ToM and apply it in their reasoning processes.\nHowever, possessing first-order ToM abilities does not necessarily mean that LLMs will excel at\nmastering higher-order ToM. Furthermore, based on TMGBENCH, we introduce more forms of\ncomplex strategic reasoning tasks and pose a new challenge for LLMs."}, {"title": "A BASIC THINGS ABOUT GAME THEORY", "content": "In this section, we discuss two fundamental concepts in game theory: dominant strategy and Nash\nequilibrium.\nA dominant strategy is one that always provides a player with a payoff at least as high as any other\nstrategy, regardless of the actions of other players. In other words, if a player has a dominant strategy,\nthey will consistently choose it, as it either maximizes their payoff or does not reduce it, irrespective\nof the strategies chosen by others.\nNash equilibrium refers to a set of strategies, one for each player, where no player can benefit\nby unilaterally changing their strategy. At a Nash equilibrium, each player's strategy is the best\nresponse to the strategies of the other players. This means that if all players are following their Nash\nequilibrium strategies, no one has an incentive to deviate from their current strategy. It represents a\nstable state in the game where players' strategies are mutually optimal.\nIn many games, the dominant strategy equilibrium and Nash equilibrium may coincide, but not\nalways. A dominant strategy equilibrium is a specific type of Nash equilibrium where each player\nhas a strategy that is optimal regardless of others' strategies. However, in many cases, dominant\nstrategies may not exist, requiring Nash equilibria to be identified through analysis and computation."}, {"title": "B 2\u00d72 MATRIX GAME", "content": ""}, {"title": "B.1 DEFINITION", "content": "A normal-form game, commonly referred to as a 2\u00d72 matrix game when involving two players each\nwith two strategies, is a fundamental concept in game theory for representing strategic interactions.\nIn this form, the game is depicted as a matrix, clearly outlining the players' strategies and corre-\nsponding payoffs. A typical 2\u00d72 matrix game is structured as shown in Table 3."}, {"title": "\u0392.2 TOPOLOGY", "content": "Game theory research often concentrates on the Prisoner's Dilemma and a few other symmetric\ngames, even though most potential games are asymmetric, and many ordinal games involve ties.\nThe findings on the topology of ordinal normal-form games (Robinson & Goforth, 2005) provide an\nelegant framework for systematically studying these games, encompassing all equivalence classes in\nan ordinal sense (where \"ordinal\" refers to the ranking of payoffs rather than their specific values).\nIn this topological framework, as depicted in Figure 9, well-known games such as the Prisoner's\nDilemma, Stag Hunt, Battle of the Sexes, and Chicken are all symmetric and situated on the counter-\ndiagonal of a 12\u00d712 grid. The remaining games are located in the other grids, each with a corre-\nsponding \"sister game\" that can be derived by reflecting across the counter-diagonal. A pair of sister\ngames are identical when the roles of the two players are reversed.\nWithin each grid, basic information about the games in the equivalence classes is provided, including\nthe family name and abbreviation, the payoff matrix, and the order graph, which illustrates the\nincentives for the row/column player to unilaterally change their choice for a higher payoff.\nThese 144 equivalence classes include 18 games with no equilibrium, 18 games with exactly two\nequilibria, and 108 games with a single equilibrium. Their distribution within the topology is sym-\nmetric across the counter-diagonal."}, {"title": "B.3 SOLUTION STRUCTURE", "content": "As previously mentioned, all games in the topological framework can be categorized into three\ndistinct groups based on the number of Nash equilibria. If we consider Nash equilibrium as the\nsolution to finding stable strategy combinations, Figure 10 illustrates the structure of these solutions.\nIn games with exactly one Nash equilibrium, at least one player (either the column player, row\nplayer, or both) has a dominant strategy, meaning they do not need to consider the other player's\nchoice. These games are represented by grey or black grids.\nConversely, games with either 0 or 2 Nash equilibria share the characteristic that neither player has\nan unconditionally optimal choice, meaning no dominant strategies exist. However, in games with\nno Nash equilibrium (white grids), at least one player always has an incentive to unilaterally change\ntheir choice, regardless of the situation. In contrast, games with two Nash equilibria (orange, blue,\nor green grids) feature two stable strategy combinations.\nAdditionally, from a symmetry perspective, two sister games that are symmetric across the counter-\ndiagonal belong to the same category and have identical Nash equilibria."}, {"title": "C MORE INFORMATION ABOUT OUR TMGBENCH", "content": ""}, {"title": "C.1 GENERATION PIPELINE", "content": "In our study, we design an efficient dataset generation pipeline that leverages GPT-40 as the core\nto produce the entire dataset, with rigorous human quality reviews incorporated. The pipeline is\norganized into three carefully designed stages:\nClassic Game Construction. Based on the topology of 2\u00d72 games, we first introduce game de-\nscriptions for the payoff matrices of 144 game types, resulting in 144 classic games. An example of\na classic game is shown below, which mirrors the structure of the Prisoner's Dilemma. These 144\nclassic games will serve as seed games, with their inherent game structures generalized into more\ndiverse, story-based games.\nStory-based Game Generation. The aforementioned classic games offer a highly condensed math-\nematical representation of diverse game scenarios. However, in the real world, games often occur in\ncomplex social contexts involving various themes. To capture this complexity, we further designed\nstory-based games, incorporating richer entities and more intricate game scenarios.\nSpecifically, we used synthetic data generation techniques and crafted detailed prompts to set the\nconstruction constraints for generating high-quality story-based games. Additionally, to enhance\nthe realism of our game scenarios, we manually defined several thematic categories to guide the\ndata synthesis process (see \u00a7C.3). Both the prompt constraints and thematic categories ensure\nthe generated content aligns with the intended structure and thematic elements. An example of a\ngenerated story-based game is shown below, which follows the same game structure as the Pris-\nner's Dilemma and is presented within a new narrative context. As such, the story-based game\nstory-based/111_0 serves as a counterpart to the classic game classic/111. For each\nclassic game, we generate five corresponding story-based games. The data synthesis prompt is as\nfollows. The red text are the placeholders for the variables of the generation code, where \"domain\"\nindicates the topic we random-choose for the task, and \"matrix_str\" indicates the payoff matrix de-\nrived from the game structure we enumerate."}, {"title": "Quality Verification.", "content": "To ensure coherence and internal consistency in the generated games, we\nimplement a multi-step generation strategy, incorporating meticulous human review. First, GPT-40\ngenerates an initial draft of the story, which is then reviewed by a human for any inconsistencies or\nlogical flaws. If the draft fails this review, GPT-40 is prompted to identify the problematic sections\nand apply a self-correction mechanism.\nDuring the self-correction phase, GPT-40 analyzes the story for inconsistencies and revises the\nflawed sections. The revised version undergoes another round of human review. This iterative\nrefinement process continues until the story meets the required quality standards.\nIf, after several rounds of regeneration, the story still contains significant issues or fails to meet\nthe criteria, we may reject the output entirely. In such cases, the process is restarted from scratch with a\nnew draft to ensure a fresh approach and to avoid perpetuating prior errors."}, {}]}