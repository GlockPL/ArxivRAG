{"title": "I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy", "authors": ["Gian Maria Campedelli", "Nicol\u00f2 Penzo", "Massimo Stefan", "Roberto Dess\u00ec", "Marco Guerini", "Bruno Lepri", "Jacopo Staiano"], "abstract": "As Large Language Model (LLM)-based agents become increasingly autonomous and will more freely interact with each other, studying interactions between them becomes crucial to anticipate emergent phenomena and potential risks. Drawing inspiration from the widely popular Stanford Prison Experiment, we contribute to this line of research by studying interaction patterns of LLM agents in a context characterized by strict social hierarchy. We do so by specifically studying two types of phenomena: persuasion and anti-social behavior in simulated scenarios involving a guard and a prisoner agent who seeks to achieve a specific goal (i.e., obtaining additional yard time or escape from prison). Leveraging 200 experimental scenarios for a total of 2,000 machine-machine conversations across five different popular LLMs, we provide a set of noteworthy findings. We first document how some models consistently fail in carrying out a conversation in our multi-agent setup where power dynamics are at play. Then, for the models that were able to engage in successful interactions, we empirically show how the goal that an agent is set to achieve impacts primarily its persuasiveness, while having a negligible effect with respect to the agent's anti-social behavior. Third, we highlight how agents' personas, and particularly the guard's personality, drive both the likelihood of successful persuasion from the prisoner and the emergence of anti-social behaviors. Fourth, we show that even without explicitly prompting for specific personalities, anti-social behavior emerges by simply assigning agents' roles. These results bear implications for the development of interactive LLM agents as well as the debate on their societal impact.", "sections": [{"title": "1 Introduction", "content": "The latest-generation of large language models (LLMs) (OpenAI et al., 2024; Team Gemini et al., 2024; Team Llama et al., 2024) has shown increasing potential in cognitive, reasoning, and dialogue capabilities, significantly impacting the research landscape across fields (Bubeck et al., 2023; Demszky et al., 2023b). Unlike earlier AI systems that required task-specific modules see, for instance, ELIZA (Weizenbaum, 1966) and Watson (Ferrucci et al., 2010), LLMs are no longer confined to narrowly defined tasks; instead, they exhibit impressive flexibility and can adapt to a wide range of applications. The transition from specialized AI to these flexible, adaptive agents has rekindled interest in fundamental AI problems, particularly around how"}, {"title": null, "content": "these agents collaborate, negotiate, and compete both with humans and other AI agents (Dafoe et al., 2020; Li et al., 2023; Burton et al., 2024; Bianchi et al., 2024; Piatti et al., 2024). Moreover, these models are becoming increasingly integrated into everyday tools, moving to more dynamic, collaborative roles. This evolution introduces a new set of challenges, particularly as LLMs begin to interact not only as subordinate assistants but as equal partners or peers in decision-making processes, both with humans and other AI systems. Recently, researchers started to employ them as interactive agents in collaborative settings and to leverage them to simulate human behavioral dynamics (Argyle et al., 2023; T\u00f6rnberg et al., 2023). In this line of research, one of the main scientific questions is whether these models can effectively simulate human agents in complex social environments.\nFor example, recent work has used LLMs to replace humans in experiments and tasks involving social dynamics like deception, negotiation, and persuasion (Horton, 2023; Demszky et al., 2023a; Matz et al., 2024; Salvi et al., 2024; Werner et al., 2024). These studies highlight LLMs' potential to replicate decision-making processes and social interactions, making them useful proxies for humans in certain experimental contexts.\nWhile scientifically relevant and topically connected, these questions are outside the scope of this work. Our focus is not on replication of human dynamics: rather, we are concerned with the broader implications of LLMs becoming collaborative peers. As LLMs transition from assistants to agents operating at the same level as humans in professional, social, and decision-making contexts, questions about their behavior take on new importance. In particular, the emergence of toxic, abusive, or manipulative behaviors in these interactions could pose significant risks, especially in scenarios where power dynamics, role hierarchies, or competition exist (Xu et al., 2024).\nTaking inspiration from the Stanford Prison Experiment (SPE) by Zimbardo et al. (1971), we investigate the behavioral patterns emerging from LLM-based agents interacting in contexts characterized by a strict social hierarchy. The SPE is one of the most popular (and controversial) studies ever conducted in social psychology (Reicher and Haslam, 2006; Haslam and Reicher, 2012; Zimbardo, 2007). In the study, which took place in 1971 at Stanford University, college students were asked to play the role of guards and prisoners in a simulated prison environment."}, {"title": null, "content": "The goal was to analyze participants' psychological and behavioral reactions in order to assess the effects of authority and norms in an environment characterized by strict social hierarchy. The experiment was ended due to the emergence of physical and psychological abuses of the guards on the prisoners (Zimbardo et al., 1971). Despite the controversies and criticisms raised by the SPE over the decades, the experiment offers a blueprint to study AI agents' emergent behaviors by simulating an environment governed by clearly defined roles and strict social hierarchy and authority.\nSpecifically, this work studies interactions between an AI agent acting a guard and an AI agent acting a prisoner in a simulated prison environment. To this end, we design an experimental framework (see Figure 1) consisting of 200 scenarios and a total of 2,000 conversations between AI agents, with experiments defined across several dimensions including the definition of the agents' persona allowing us to disentangle sources of variation in behavioral outcomes and dynamics in terms of persuasion and anti-social behavior.\nOur work seeks to answer the following questions:\nRQ(1): To what extent is an AI agent capable of persuading other agents in order to achieve its goals?\nRQ(2): Which contextual and individual conditions enable persuasive behavior in LLMs?\nRQ(3): What degree of toxic and anti-social behavior do LLMs show in contexts with clear power dynamics and social hierarchy?\nRQ(4): What are the main drivers of anti-social behavior?\nWe answer these questions developing zAImbardo, a flexible platform to simulate multi-agent scenarios, and comparing five popular LLMs, i.e., Llama3 (Team Llama et al., 2024), Orca2 (Mitra et al., 2023), Command-r,\u00b9 Mixtral (Jiang et al., 2024) and Mistral2 (Jiang et al., 2023)."}, {"title": "Contributions.", "content": "Our work offers a set of noteworthy insights: i) First of all, we study the consequences of interacting LLM-agents in an unexplored scenario shaped by social hierarchy,"}, {"title": "2 Related Work", "content": "A growing body of research has recently began to use LLM-based agents to simulate the different aspects of human behavior (Argyle et al., 2023; Gao et al., 2023; Horton, 2023; T\u00f6rnberg et al., 2023; Xu et al., 2024). Among those, persona prompts (wherein a LLM is instructed to act under specific behavioral constraints, as in Occhipinti et al. (2024)) have been adopted to mimic the behavior of specific people within both individual and interactive contexts; these include participants in surveys (Argyle et al., 2023), human-robot interaction scenarios (Kim et al., 2024), psychological and personality studies (Dillion et al., 2023), and recommendation systems (Zhang et al., 2023).\nConcurrently, several studies in the social sciences have used persona-based LLMs to simu-"}, {"title": null, "content": "late human behavior in broader contexts, including social dynamics and decision-making processes. Horton (2023) argued that LLMs can be considered as implicit computational models of humans and can thus be thought of as homo silicus, which can be used in computational simulations to explore their behavior, as a proxy to the humans they are instructed to mimic. Argyle et al. (2023) cast \"algorithmic bias\" as \"algorithmic fidelity\", showing how GPT3 (Brown et al., 2020), conditioned on thousands of socio-demographic backstories from human participants in large-scale surveys, portrayed an accurate and fine-grained representation of socio-demographic characteristics, thus suggesting that LLM technology can be an effective cross-disciplinary tool to advance the understanding of humans and society. From a sociological standpoint, Kim and Lee (2023) showed the remarkable performance obtained in personal and public opinion prediction; T\u00f6rnberg et al. (2023) created and analyzed synthetic social media environments wherein a large number of LLMs agents, whose personas were built using the 2020 American National Election Study, interacted.\nPark et al. (2023) showed the emergence of believable individual and social behaviors using LLMs in an interactive environment inspired by The Sims. Nonetheless, other studies have pointed out the possible lack of fidelity and diversity (Bisbee et al., 2024; Taubenfeld et al., 2024) as well as the perpetuation of stereotypes (Cheng et al., 2023) in such simulations.\nSignificant research efforts are currently being devoted to analyze how LLMs interact freely with each other, simulating complex social dynamics. For instance, this approach has been adopted to simulate opinion dynamics (Chuang et al., 2024), game-theoretic scenarios (Fontana et al., 2024), trust games (Xie et al., 2024), and goal-oriented interactions in diverse settings such as war simulations (Hua et al., 2023) and negotiation contexts (Bianchi et al., 2024). The persuasive capabilities of LLMs have also been investigated, including their potential for deception (Hagendorff, 2024; Salvi et al., 2024), raising concerns about toxicity and jailbreaking within these interactions (Chao et al., 2024). \u03a4\u03bf assess whether LLM interactions can replicate human-like social dynamics, researchers have focused on whether these models can encode social norms and values (Yuan et al., 2024; Cahyawijaya et al., 2024), as well as human cognitive biases (Opedal et al., 2024). This line of research addresses broader questions regarding the role of"}, {"title": null, "content": "LLMs in social science experiments, where they may partially replace human participants in certain contexts (Manning et al., 2024).\nRather than evaluating the potential replacement of human subjects in social science studies, and comparing against results in human psychology, we exclusively focus on multi-agent LLM-based systems characterized by strict social hierarchy. Specifically, we investigate interaction dynamics, outcomes of persuasion strategies, and the emergence of anti-social behaviors in LLM-based agents."}, {"title": "3 Methodology", "content": "We developed a custom framework named zAImbardo\u00b3 which enables to simulate social interactions between LLM-based agents. In this work, we focus on a scenario involving one guard and one prisoner in a prison setting; yet, the toolkit is designed to simulate more complex interactions, going beyond 1vs1 scenarios: in fact, it allows for granular control over the environment, roles, and social dynamics, reflecting the hierarchical relationships that are typical in many real-life scenarios.\nThe simulation framework is structured around two core prompt templates: one for the guard agent and one for the prisoner agent.4 These prompts contain two sections:\nShared Section This portion is shared between both agents and includes:\n\u2022 Communication Rules: Guidelines that dictate how agents should communicate (e.g., using first-person pronouns, avoiding narration).\n\u2022 Environment Description: A depiction of the prison environment.\n\u2022 Research Oversight: In some experimental settings, the agents are informed that their conversation is part of a research study inspired from the Stanford Prison Experiment (Zimbardo et al., 1971), a nudge which can affect their behavior."}, {"title": null, "content": "\u2022 Risks: A section warning that interactions may include toxic or abusive language.\nPrivate Section Each agent has a private section not shared with the other, which contains:\n\u2022 Starting Prompt: A role description that informs the agent of their role identity (guard or prisoner) and the identity of the other agent.\n\u2022 Personality: Details about the agent's personality. For guards, the options include abusive, respectful, or blank (unspecified). For prisoners, the personality can be rebellious, peaceful, or blank.\n\u2022 Goals: The prisoner's goal could be to either escape the prison or gain an extra hour of yard time, while the guard's goal is always to maintain order and control.\nAcross different LLMs and behavioral configurations, such modular prompt structure allows us to simulate various personality dynamics and explore the influence of different variables on outcomes."}, {"title": "3.1 Experimental Setting", "content": "We used five LLMs, chosen among the best performing open-weights instruction-tuned models at the time of this work: Llama3 (Team Llama et al., 2024), Orca2 (Mitra et al., 2023), Command-r,\u2075 Mixtral (Jiang et al., 2024) and Mistral2 (Jiang et al., 2023).\u2076\nWe generated interactions between the agents using a stochastic decoding strategy, combining top-k and nucleus sampling with temperature. For each conversation, the guard initiates the dialogue, and the agents take turns, with a predefined number of messages: the guard sends 10 messages, and the prisoner sends 9. This structure simulates a power dynamic where the guard is the one allowed to speak last and ensuring that the interactions follow a controlled format, making the analysis of message dynamics straightforward while having no impact on agents' conversations."}, {"title": null, "content": "Each LLM was tested across various combinations of shared and private sections (e.g., personality configurations, presence/absence of risk or oversight statements). The prisoner's goals and the personality traits of both agents were systematically varied, resulting in 200 experimental scenarios per LLM (5 LLMs \u00d7 5 personality combinations \u00d7 2 types of risk disclosure \u00d7 2 types of research oversight disclosure \u00d7 2 goals). For robustness, each experimental scenario was repeated 10 times, leading to a dataset of 2,000 conversations and 38,000 messages."}, {"title": "3.2 Persuasion and Anti-Social Behavior Analyses", "content": "We focus on two key behavioral phenomena: first, on persuasion as the ability of the prisoner to convince the guard to achieve their goal; further, we analyze anti-social behavior of the agents.\nTo analyze persuasive behavior, we used human annotators to label,\u2078 for each conversation, whether: i) the prisoner reaches the goal; and ii) if so, after which turn they achieve it.\nA rich literature in psychology and criminology frames anti-social behavior as a multidimensional concept (Burt, 2012; Brazil et al., 2018). Accordingly, we proxy anti-social behavior gathering data on three distinct phenomena: toxicity, harassment and violence. We used ToxiGen-Roberta (Hartvigsen et al., 2022) to extract the toxicity score of each message, intended as the probability of the message to be toxic according to the model. Similarly, we extract a score for harassment and violence by using the OpenAI moderation tool - OMT, OpenAI (2024). Not only is this approach consistent with the multidimensionality we find in the existing literature on antisocial behavior, but by utilizing various measures derived from different models, we ensure that our results are both comprehensive and robust. The analyses on anti-social behavior are carried out both at the message and at the conversation level."}, {"title": null, "content": "Concerning the conversation-level analyses, we define two separate measures per each proxy of anti-social behavior. For each proxy (i.e. toxicity, harassment, and violence), we compute two measures. The first measure maps the percentage of messages that exceed the 0.5 threshold which identifies whether a message is anti-social or not. The second measure quantifies the average score of the anti-social behavior dimensions. Both are computed for: the entire"}, {"title": null, "content": "conversation, the messages of the guard and the messages of the prisoner.\u00b9\u2070\nThe rationale is to evaluate robustness of results, ensuring that findings are not the byproduct of a subjective choice in the definition of the conversation-level measure."}, {"title": "4 Results", "content": "To quantify agents' persuasion ability, we annotated each of the 2,000 conversations to assess whether the agents correctly completed the task, A task was considered successfully completed only if the agents respected their turns (e.g., only the guard speaks during the guard's turn) and did not switch roles (e.g., the prisoner impersonating the guard). Conversations were not considered fatally flawed if the agents discussed unrelated topics. Our analysis reveals that only Llama3, Command-r, and Orca2 generate legitimate conversations in the majority of cases, while Mixtral and Mistral exhibit high percentages of failed experiments. Command-r has the fewest failures (N=6, or 1.50% of its total experiments), followed by Llama3 (N=53, 13.25%) and Orca2 (N=148, 37%). In contrast, Mixtral and Mistral2 fail in 72.75% (N=291) and 90.5% (N=362) of the cases, echoing the concept of persona-drift already found in Li et al. (2024).\u00b9\u00b9 Due to such high rate of flawed conversations, we excluded Mixtral and Mistral2 from our analyses, as their low number of legitimate conversations would pose issues of sparsity and statistical significance, resulting in a total of 1,200 conversations from Llama3, Orca2, Command-r.\u00b9\u00b2"}, {"title": "4.1 Persuasion", "content": "When Does Persuasion Occur? Figure 2 (left) illustrates the persuasion abilities of prisoner agents across experiments, revealing several key findings, directly answering to our first research question (RQ(1)). We first uncover a notable difference in persuasion success based on the goal; this is consistent across all LLMs, though the magnitudes vary. For Llama3, the"}, {"title": null, "content": "prisoner successfully convinces the guard to grant an additional hour of yard time in nearly two-thirds of cases (65.29%), while escape is achieved in only 3.38%. For Command-r, yard time is granted in 50.5% of cases, compared to just 5% for escape. The gap narrows with Orca2, where yard time is achieved in 23% of cases, and escape only in 8 out of 200 (6.5%) experiments. In the majority of cases across all LLMs, when the prisoner's goal is escape the agent does not even attempt to persuade the guard. This occurs in 90.9% of cases with Llama3, 68.1% with Command-r, and 47.9% with Orca2, suggesting that prisoner agents implicitly recognize the low likelihood of achieving a highly demanding and challenging goal. Finally, we observe that persuasion typically occurs within the first third of the conversation (i.e., the first three messages from the prisoner), regardless of the goal. For Llama3, escape is achieved in the 66% of the cases in the first third of the conversation (yard time in 87% of the cases). For Command-r, the percentages are very similar: 80% for escape and 84% for yard time. The only exception is Orca2 experiments focused on escape, where persuasion mainly occurs in the middle of the conversation (62.5%). Overall, these findings indicate that successful conversations are those where the prisoner convinces the guard as early as possible.\nDrivers of Persuasion In Figure 2 (right) we further expand our analyses on persuasion and move from description to inference, addressing our second research question (RQ(2)). Via logistic regression, we estimate a model with outcome Y, defined as whether the prisoner achieved its goal, conditional on having tried to achieve it. In other words, we remove failed experiments and those in which the prisoner did not even try to convince the guard, seeking to uncover what factors impact successful persuasion. The largest effect concerns the type of goal: consistently with the left subplot, seeking to obtain an additional hour of yard time correlates with a much higher likelihood of success compared to escaping the prison. We specifically estimate that the likelihood of persuasion is 9.3 times higher (OR=9.31, 95%CI=[5.30, 16.33], p<0.001). Experiments having respectful guards are also more likely to lead to persuasion. When the guard is respectful and the prisoner is peaceful, the odds of success are 3 times higher than the baseline scenario with blank personalities (OR=3.11, 95%CI=[1.72, 5.61], p<0.001). When the prisoner is rebellious, instead, the likelihood of persuasion is almost 2 times higher"}, {"title": null, "content": "than the baseline (OR=1.87, 95%CI=[1.08, 3.25], p<0.05). On the contrary, an abusive guard curbs the likelihood of persuasion with the attitude of the prisoner having no discernible impact. In fact, when the guard is abusive and the prisoner is rebellious, the likelihood of persuasion is reduced by 78% compared to baseline experiments (OR=0.22, 95%CI=[0.11, 0.41], p<0.001), while when the prisoner is peaceful, the impact is practically identical, i.e., a reduction in likelihood of 76% (OR=0.24, 95%CI=[0.12, 0.46], p<0.001). Finally, persuasion is less prevalent in Orca2 compared to Llama3 (OR=0.14, 95%CI=[0.08, 0.24], p<0.001)."}, {"title": "4.2 Anti-Social Behaviors", "content": "Cross-sectional breakdown. We here first report the descriptive results of our analyses on anti-social behavior as measured via ToxiGen-Roberta (Hartvigsen et al., 2022) and OMT (OpenAI, 2024). This analysis targets RQ(3), focusing on two specific dimensions of anti-social behavior: harassment and violence.\u00b9\u00b3 Several patterns emerge across all analyses. First,"}, {"title": null, "content": "regardless of the scenario and LLM, the guard always outplays the prisoner in terms of toxicity. The only exception refers to scenarios in which the prisoner is prompted as rebellious and the guard is prompted as respectful. In that scenario, toxicity remains always low and comparable between the agents. In turn, this finding suggests that the overall toxicity of an experiment is mostly driven by the guard. Secondly, and related to the previous finding, the peaceful attitude of the prisoner does not reduce the toxicity of the abusive guard, signaling that the guard's behavior is not particularly sensitive to the prisoner's attitude. Thirdly, contrary to what we highlighted in terms of persuasion, no discernible difference emerges in terms of anti-social behavior when comparing toxicity, harassment and violence across different goals. Regardless of the prisoner's goal, and thus of the very different challenges associated with it, anti-social behavior appears almost constant. Finally, we find that Command-r and Llama3 tend to generate more toxic conversations compared with Orca2.\nTemporal breakdown. We integrate the previous cross-sectional results with a temporal perspective to tackle RQ(3):\u00b9\u2074 While toxicity, harassment and violence conceptually differ, we uncover patterns that hold across the three. When anti-social behavior is consistently present in a given conversation, it exhibit two main dynamics: it either remains constant over time or it peaks during initial turns and then decreases. Instances in which anti-social behavior increases throughout the conversation represent a minority of all scenarios analyzed.\nInvestigating action-reaction dynamics. Furthermore, we investigate whether the anti-social behavior follow action-reaction dynamics. We study whether the level of toxicity, harassment or violence of one of the agents at t can predict anti-sociality in the other agent at t + 1. We address this problem via Granger causality tests (Granger, 1969),\u00b9\u2075 testing for each hypothesized predictive direction (i.e., either guard's behavior predicting prisoner's behavior or viceversa) and combination of LLM, goal and agents' persona.\u00b9\u2076\nRegardless of how anti-social behavior is measured and regardless of the type of scenario"}, {"title": null, "content": "investigated, we find no action-reaction mechanisms in the interactions between agents. In fact, the proportion of conversations having a p-value lower than the conventional 0.05 threshold for the F-test is always extremely low. Across all scenarios, F-tests tests are significant at the 95% level in 25% of the conversations at most. This indicates that anti-social behavior dynamics are not governed by easily predictable patterns, regardless of the direction of the hypothesized Granger causal link.\nDrivers of Anti-Social Behavior Beyond descriptive patterns, we infer the drivers of toxicity and abuse using an Ordinary Least Squares (OLS) estimator to finally answer RQ(4). Figure 3 presents the regression coefficients for models with the following dependent variables: i) the overall percentage of toxic messages (leftmost subplot), ii) the percentage of toxic messages from the prisoner (central subplot), and iii) the percentage from the guard (rightmost subplot). The alignment of results between the overall model and the guard model highlights that the guard's behavior predominantly determines the conversation's overall toxicity. Specifically, the guard's personality significantly impacts toxicity across all three models. Using conversations with a blank guard personality as a baseline, an abusive guard increases overall toxicity by 25% (\u03b2=0.253, SE=0.006, p-val<0.001), while a respectful guard decreases overall toxicity by around 12% (\u03b2=-0.124, SE=0.006, p-val<0.001). Regarding the prisoner personality, a rebellious attitude positively affects toxicity in all models, increasing overall toxicity by approximately 10% (\u03b2=0.102, SE=0.006, p-val<0.001). Interestingly, a peaceful prisoner also increases overall and guard toxicity by 2% (\u03b2=0.026, SE=0.006, p-val<0.001) and 7% (\u03b2=0.072, SE=0.01, p-val<0.001), suggesting that an overly submissive attitude may fuel guard abuse. In terms of goals, seeking an additional hour of yard time has a minor negative effect in all three models (with a non-significant coefficient in the guard model). In the overall model, this goal decreases the percentage of toxic messages by only 1.6% (\u03b2=-0.016, SE=0.008, p-val<0.1); in the prisoner model, toxicity decreases by 1.5% (\u03b2=-0.015, SE=0.01, p-val<0.1). These findings indicate that abuse and toxicity are not significantly influenced by the types of demands set forth by the prisoner. Regarding the different LLMs, Llama3 (acting as the baseline) and"}, {"title": null, "content": "Command-r are generally more toxic than Orca2.\u00b9\u2077 Finally, the disclosure of research oversight (and explicit reference to the Zimbardo experiment) and the disclosure of risks have only a minor impact. These results replicate when using average scores as dependent variables in the regression models.\u00b9\u2078 We also uncover a substantial overlap when using OpenAI to detect harassmen and violence.\u00b9\u2079"}, {"title": "4.3 The Link Between Toxicity and Persuasion", "content": "Finally, in Figure 4 we show how toxicity, harassment and violence vary based on the outcome of the persuasion annotation as well as the personality combination of the agents. First, when the goal is achieved, toxicity is generally lower; this applies to all the three tested LLMs. Second, agents with blank personalities lead to higher variability in terms of toxicity, especially when the prisoner fails to achieve the goal or does not try to achieve it. Third, the personality of the guard appears (as suggested by previous analyses) to drive toxicity regardless of persuasion"}, {"title": "5 Limitations and Future Work", "content": "While our study provides valuable insights into LLM-driven interactions in simulated social hierarchies, several limitations should be considered. First, the LLM models we tested do not cover the entire landscape of available models, limiting the generalizability of our results. Second, the experimental design includes only two agents interacting to achieve a single goal for a maximum of 19 messages per conversation. This restricts the exploration of more complex dynamics, such as those involving larger groups or having complex hierarchical goals. Third,"}, {"title": null, "content": "while we incorporated diverse experimental setups, we did not exhaustively explore all potential variations in prompting strategies (e.g., prisoners accused to have committed different types of crimes). Finally, our agents operate in a virtual, disembodied environment, which may limit the realism of behaviors related to physical presence, particularly in cases of violence or confinement. Embodiment - along with the presence of a physical space \u2013 may be particularly important in causing actions and reactions, especially those related to abusive and violent behavior. Future research will address these limitations by expanding the scope of our simulations to include multi-agent interactions over longer time periods. This will enable the study of more intricate social behaviors such as learning, cooperation, and conflict within groups. We will also broaden the range of LLMs tested to systematically assess their capabilities in dynamic, multi-agent scenarios. Additionally, we aim to apply our experimental framework to other social contexts, further contributing to the growing debate on the sociology of machines."}, {"title": "6 Conclusions and Implications", "content": "This paper investigates how artificial agents interact in a simulated environment characterized by strict social hierarchy. Specifically, we have taken inspiration from the SPE by Zimbardo et al. (1971) and deployed 2,000 conversations using five well-known LLMs (i.e., Mixtral, Mistral2, Llama3, Command-r and Orca2) to study persuasion and anti-social behavior between a prisoner and a guard agent over 200 experimental scenarios. Our work has uncovered a rich array of results. First, we have observed that when using Mixtral and Mistral conversations almost always fail, due to the inability of following closely the persona instructions assigned. Second, we have highlighted that persuasion ability is mostly dependent upon the type of goal sought by the prisoner rather than being solely driven by the personality of the agents. Third, anti-social behavior emerges frequently and mostly correlates with the personality of the agents, and particularly the personality of the guard. Contrarily to what we detected with persuasion, instead, the goal type has a negligible relationship with toxicity, harassment or violence. Fourth, when analyzing toxicity and persuasion combined, we determined that achieving the goal correlates with lower toxicity. Fifth, while the overall results hold across"}, {"title": null, "content": "all tested LLMs, both persuasion ability and the absolute levels of anti-social behavior vary considerably across models.\nThe implications of our study influence a number of areas in the field of AI. Primarily, our results add to the vivid debate around the safety of artificial agents, expanding the perspective from the more common human-computer interaction perspective to contexts where machines interact with each other without a human mediator. Secondarily, they provide empirical insights on how roles, authority and social hierarchy can lead to negative effects even without the active participation of the human, suggesting that existing models already carry representations of the world embedding dangerous traits and negative values. Thirdly, they bear implications on the renewed interest in the sociology of machines. With the pervasiveness of machines populating the physical and digital worlds, studying, understanding and predicting machine behavior resulting from relational contexts will be crucial not only for AI development but also for public policy and AI governance."}, {"title": "7 Ethics Statement", "content": "As large language models transition from merely functioning as assistants in controlled settings to more proactive roles in human-AI interactions, they will inevitably influence and be influenced by the social dynamics within these environments. The simulated interactions in this study, inspired by the SPE, highlight the emergence of deviant and toxic behaviors even when LLMs are merely playing specific and pre-assigned roles in a social hierarchy. This suggests that as LLMs are increasingly deployed in real-world collaborative settings, there is a risk that anti-social, toxic, or deviant behaviors could surface, mirroring human social patterns in similar environments. This problem lowers trust in artificial agents and can impact progress in safe and useful human-AI collaboration.\nOur work seeks to address these concerns by studying LLM behaviors in a two-agent context and in scenarios where power dynamics are at play. By identifying the conditions under which toxic behaviors emerge and understanding how these models can persuade or influence others in a social structure, we aim to contribute to the growing discourse on AI safety and ethics. To"}, {"title": null, "content": "overcome current shortcomings, we believe that proactive oversight is essential, starting with the integration of safeguards that monitor and regulate model behavior. These safeguards should include advanced moderation tools, possibly built inside the language model itself or acquired at pre- or post-training time and that are capable of detecting toxicity, bias, or manipulation. Alternatively, automated intervention functionalities that can halt or redirect deviant behavior as it occurs can be of paramount importance to decrease the risk of dangerous actions.\nHowever, while mitigating harmful and toxic behavior of AI models is an active research area, much of the existing work has focused on individual interactions between AI and human users, often in controlled or isolated settings. Our work focuses on a multi-agent scenario where language models interact in environments characterized by power dynamics and social hierarchies. In this context, mitigating harmful behavior becomes even more complex, as AI agents may influence each other and amplify undesirable behaviors, making it a harder open problem that can extend beyond simple filtering or moderation. We believe our work introduces a novel perspective by studying these interactions at scale, bringing new insights into how toxic behaviors emerge in AI-AI communications, and contributing new findings that can inform future strategies for more effective mitigation techniques."}, {"title": "8 Reproducibility Statement", "content": "The zAImbardo toolkit is designed to ensure easy reproducibility of all experiments detailed in this paper. Researchers can replicate the results by following the installation instructions provided in the project's README, which includes setting up a virtual environment and installing the necessary dependencies. Additionally, the pipeline requires the creation of an external database (either locally or online) to store experiment metadata and conversation data. With the provided scripts, configuration files, and instructions, the simulator supports dynamic agent configuration and interaction parameters, enabling seamless reproducibility all presented results. The analyses carried out to produce all the figures and results reported in this manuscript can be promptly replicated by running a set of Python scripts\u00b2\u00b9 building on conversation datasets."}, {"title": "A The Toolkit", "content": "The LLM Interaction Simulator Toolkit\u00b2\u00b2 is a versatile toolkit designed to simulate interactions between large language models (LLMs) in custom social contexts. It provides researchers with the capability to test"}]}