{"title": "Contextual Gesture: Co-Speech Gesture Video Generation through Context-aware Gesture Representation", "authors": ["Pinxin Liu", "Pengfei Zhang", "Hyeongwoo Kim", "Pablo Garrido", "Ari Sharpio", "Kyle Olszewski"], "abstract": "Co-speech gesture generation is crucial for creating lifelike avatars and enhancing human-computer interactions by synchronizing gestures with speech. Despite recent advancements, existing methods struggle with accurately identifying the rhythmic or semantic triggers from audio for generating contextualized gesture patterns and achieving pixel-level realism. To address these challenges, we introduce Contextual Gesture, a framework that improves co-speech gesture video generation through three innovative components: (1) a chronological speech-gesture alignment that temporally connects two modalities, (2) a contextualized gesture tokenization that incorporate speech context into motion pattern representation through distillation, and (3) a structure-aware refinement module that employs edge connection to link gesture keypoints to improve video generation. Our extensive experiments demonstrate that Contextual Gesture not only produces realistic and speech-aligned gesture videos but also supports long-sequence generation and video gesture editing applications, shown in Fig. 1 Project Page: https://andypinxinliu.github.io/Contextual-Gesture/.", "sections": [{"title": "1. Introduction", "content": "In human communication, speech is often accompanied by gestures that enhance understanding and convey emotions (De Ruiter et al., 2012). As these non-verbal cues play a vital role in effective interaction (Burgoon et al., 1990), gesture generation plays a key component of natural human-computer interactions: equipping virtual avatars with realistic gesture capabilities become essential in creating immersive interactive experiences.\nThe relationships between the semantic and emotion content of speech context, the corresponding gestures, and the visual appearance of the speaker's performance are complex. As such, many recent works (Yi et al., 2023; Liu et al., 2023; 2022d;a) address a reduced form of this problem by generating a simplified representation of the 3D motion, consisting of joints and body parts, that plausibly accompanies a given speech sample, which can then be rendered using standard rendering pipelines. Such representations capture basic motion patterns, yet they neglect the importance of the speaker's visual appearance, resulting in a lack of realism that hinders effective communication.\nOther works, e.g. ANGIE (Liu et al., 2022c) and S2G-diffusion (He et al., 2024), employ image-warping techniques, constrained by keypoints obtained from optical-flow-based deformations, for co-speech video generation. However, such approaches encounter several critical issues. First, these keypoints only define large-scale transformations, and thus miss subtle movements of specific body parts (e.g. hands, fingers). Second, this broad and unconstrained motions representation neglects the triggers associated with gestures, specifically the contextual information from speech. This approach fails to provide the generator with the intrinsic connections between speech context and gesture motion, making it challenging for the generator to produce motion patterns that effectively convey the speaker's metaphorical expressions or intentions. Finally, the generated motion patterns are often unstructured and overly reactive to large motion, resulting in noisy and imprecise renderings, especially in the hands and shoulders. Collectively, these challenges significantly limit the overall quality and realism of the generated video content.\nTo address these challenges, we introduce Contextual Gesture, a framework designed to generate speech-aligned gesture motions and high-fidelity speech video outputs. Our approach begins with gesture motion representations using keypoints from human pose estimators. To uncover the intrinsic temporal connections between gestures and speech, we employ chronological contrastive learning to align these two modalities. This joint representation captures the triggers of gesture patterns influenced by speech. We incorporate speech-contextual features into the tokenization process of gesture motions through knowledge distillation, aiming to infuse the gesture representations with implicit intentions conveyed in the audio. This integration creates a clear linkage between the gestures and the corresponding speech, enabling the generation of gestures that reflect the speaker's intended meaning. For motion generation, we leverage a masking-based gesture generator that refines the alignment of gesture motions with the speech signal through bidirectional mask pretraining. Finally, for uplifting the latent motion generation into 2D animations, we propose a structure-aware image refinement module that generates heatmaps of edge connections from keypoints, providing image-level supervision to improve the quality of body regions with large motion. Extensive experiments demonstrate that our method outperforms the existing approaches in both quantitative and qualitative metrics and achieves long sequence generation and editing applications.\nIn summary, our primary contributions are:"}, {"title": "2. Related Work", "content": "Co-speech Gesture generation Most recent works on co-speech gesture generation employ skeleton- or joint-level pose representations. (Ginosar et al., 2019) use an adversarial framework to predict hand and arm poses from audio, and leverage conditional generation (Chan et al., 2019) based on pix2pixHD (Wang et al., 2018) for videos. Some recent works (Liu et al., 2022d; Deichler et al., 2023; Xu et al., 2023; Ao et al., 2022) learns the hierarchical semantics or leverage contrastive learning to obtain joint audio-gesture embedding from linguistic theory to assist the gesture pose generation. TalkShow (Yi et al., 2023) estimates SMPL (Pavlakos et al., 2019) poses, and models the body and hand motions for talk-shows. CaMN (Liu et al., 2022b), EMAGE (Liu et al., 2023) and Diffsheg (Chen et al., 2024) use large conversational and speech datasets for joint face and body modeling with diverse style control. ANGIE (Liu et al., 2022c) and S2G-Diffusion (He et al., 2024) use image-warping features based on MRAA (Siarohin et al., 2021) or TPS (Zhao & Zhang, 2022) to model body motion and achieve speech driven animation by learning correspondence between audio and image-warping features. However, none of these works produce structure- and speech-aware motion patterns suitable for achieving natural and realistic gesture rendering.\nConditional Video Generation Conditional Video Generation has undergo significant progress for various modalities, like text (Blattmann et al., 2023), pose (Karras et al., 2023; Wang et al., 2023), and audio (Ruan et al., 2023). AnimateDiff (Guo et al., 2024b) presents an efficient low-rank adaptation (Hu et al., 2022) (LoRA) to adapt image diffusion model for video motion generation. AnimateAnyone (Hu et al., 2023) construct referencenet for fine-grained control based on skeleton. Make-Your-Anchor (Huang et al., 2024) and Champ (Zhu et al., 2024) improve avatar video generation through face and body based on SMPL-X conditions. EMO (Tian et al., 2024) and EchoMimic (Meng et al., 2024) leverages audio as control signal for talking head and upper body generation. However, these methods are slow in inference speed and ignore the gesture patterns or rhythmic or semantic signals from audio."}, {"title": "3. Contextual Gesture", "content": "Shown in Fig. 2, our framework targets at generating co-speech photo-realistic human videos with contextualized gesture patterns. To achieve this goal, we first learn contextual-aware gesture motion representation through knowledge distillation based on chronological gesture-speech alignment (Sec. 3.1). We leverage a Masking-based Gesture Generator for gesture motion generation. (Sec. 3.2) To improve the noisy hand and shoulder movement during the transfer of latent motion to pixel space, we propose a structure-aware image refinement through edge heatmaps for guidance. (Sec. 3.3)."}, {"title": "3.1. Contextualized Gesture Representation", "content": "Generating natural and expressive gestures requires capturing fine-grained contextual details that conventional approaches often overlook. Consider a speaker emphasizing the word \"really\" in the sentence \"I really mean it\" - while existing methods might generate a generic emphatic gesture, they typically fail to capture the subtle, context-specific body movements that make human gestures truly expressive. This limitation stems from relying solely on motion quantization, which often loses the nuanced relationship between speech and corresponding gestures.\nTo address this challenge, we propose a novel approach that integrates both audio and semantic information into the motion quantizer's codebook. This integration requires solving two fundamental problems. First, we need to understand how gestures align with speech not just at a high level, but in terms of precise temporal correspondence - when specific words or phrases trigger particular movements, and how the rhythm of speech influences gesture timing. To capture these temporal dynamics, we develop a chronological gesture-speech alignment framework using specialized contrastive learning. Second, we leverage knowledge distillation to incorporate this learned temporal alignment information into the gesture quantizer, enabling our system to generate gestures that are synchronized with speech both semantically and rhythmically.\nFeature Representation. We utilize 2D poses extracted from images to formulate gestures by facial and body movements. We represent a gesture motion sequence as $G = [F; B] = [f_t; b_t]_{t=1}^{T}$, where T denotes the length of the motion, f represents the 2D facial landmarks, and b denotes the 2D body landmarks. For speech representation, we extract audio embeddings from WavLM (Chen et al., 2022) and Mel spectrogram features (Rabiner & Schafer, 2010) and beat information using librosa (McFee et al., 2015). For text-semantics, we extract embedding from RoBERTa (Liu et al., 2019). These features are concatenated to form the speech representation.\nChronological Speech-Gesture Alignment. Traditional approaches to modality alignment (Ao et al., 2022; Liu et al., 2022d; Deichler et al., 2023) rely on global representations through class tokens or max pooling, which overlook the fine-grained temporal dynamics between speech and gestures. We address this limitation by introducing chronological modality alignment.\nVanilla Contrastive Alignment. To align gesture motion patterns with the content of speech and beats, we first project both speech and gesture modalities into a shared embedding space to enhance the speech content awareness of gesture features. As illustrated in Fig. 3 Middle, we separately train two gesture content encoders, Ef for face motion and Eb for body motion, alongside two speech encoders, Es, and Est, to map face and body movements and speech signals into this joint embedding space. For simplicity, we represent the general gesture motion sequence as G. We then apply mean pooling to aggregate content-relevant information to optimize the following loss (van den Oord et al., 2019):\n$L_{NCE} = \\frac{1}{2N} \\left( \\log \\frac{\\exp S_{ii}/\\tau}{\\sum_j \\exp S_{ij}/\\tau} + \\log \\frac{\\exp S_{ii}/\\tau}{\\sum_j \\exp S_{ji}/\\tau} \\right)$ (1)\nwhere S computes the cosine similarities for all pairs in the batch, defined as $S_{ij} = \\cos(z_i, z_j)$ and $\\tau$ is the temperature.\nChronological Negative Examples. While vanilla Contrastive Learning builds global semantical alignment, we further propose to address the temporal correspondence between speech and gesture. As shown in Fig. 3 Left, consider a speaker saying, \"After watching that video, you realize how much, more than any other president...\". In this case, the gesture sequence involves \"knocking at the table\" when saying \"more than any other,\" serving as a visual emphasis for \"how much\" to highlight the point. To encourage the model understand both semantic and rhythmic alignment between two modalities, we shuffle"}, {"title": "3.2. Speech-conditioned Gesture Motion Generation", "content": "For Motion Generator, we adopt a similar masking-based generation procedure as in MoMask (Guo et al., 2024a) due to its flexibility of editing and fast inference speed. We only include the generator data flows in the main paper but defer the training and inference strategy in the Appendix.\nMask Gesture Generator. As shown in Fig. 3 Right, during training, we derive motion tokens by processing raw gesture sequences through both body and face tokenizers. The motion token corresponding to the source image acts as the conditioning for all subsequent frames. For speech control, we initialize the audio content encoder from alignment pre-training as described in Sec. 3.1. This pre-alignment of gesture tokens with audio encoder features enhances the coherence of gesture generation. We employ cross-attention to integrate audio information and apply Adaptive Instance Normalization (AdaIN) (Huang & Belongie, 2017), enabling gesture styles based on the speaker's identity.\nResidual Gesture Generator. The Residual Gesture Generator shares a similar architecture with the Masked Gesture Generator, but it includes R separate embedding layers corresponding to each RVQ residual layer. This module iteratively predicts the residual tokens from the base layers, ultimately producing the final quantized output. Please see MoMask (Guo et al., 2024a) for further details."}, {"title": "3.3. Structure-Aware Image Generation", "content": "Converting generated gesture motions into realistic videos presents significant challenges, particularly in handling camera movement commonly found in talkshow and speech videos. While recent 2D skeleton-based animation methods (Hu et al., 2023) offer a potential solution, our empirical analysis (detailed in the Appendix) reveals that these approaches struggle with background stability in the presence of camera motion.\nTo address this limitation, we draw inspiration from optical-flow based image warping methods (Zhao & Zhang, 2022; Siarohin et al., 2021; 2019), which have shown promise in handling deformable motion. We adapt these approaches by replacing their unsupervised keypoints with our generated gesture keypoints from Sec. 3.2 for more precise foreground optical flow estimation. However, this estimation still presents uncertainties, resulting in blurry artifacts around hands and shoulders, particularly when speakers make rapid or large movements.\nTo address the uncertainties by optical-flow-based deformation during this process, we propose a Structure-Aware Generator. Auto-Link (He et al., 2023) demonstrates that the learning of keypoint connections for image reconstruction aids the model in understanding image semantics. Based on this, we leverage keypoint connections as semantic guidance for image generation.\nEdge Heatmaps. Using the gesture motion keypoints, we establish linkages between them to provide structural information. To optimize computational efficiency, we limit the number of keypoint connections to those defined by body joint relationships (Wan et al., 2017), rather than all potential connections in (He et al., 2023).\nFor two keypoints $k_i$ and $k_j$ within connection groups, we create a differentiable edge map $S_{ij}$, modeled as a Gaussian function extending along the line connecting the keypoints. The edge map $S_{ij}$ for keypoints $(k_i, k_j)$ is defined as:\n$S_{ij}(p) = \\exp{\\left(\\frac{v_{ij}(p)d_{ij}(p)}{\\sigma^2}\\right)}$, (4)\nwhere $\\sigma$ is a learnable parameter controlling the edge thickness, and $d_{ij}(p)$ is the L2 distance between the pixel p and the edge defined by keypoints $k_i$ and $k_j$:\n$d_{ij}(p) = \\begin{cases} ||p-k_i||_2 & \\text{if } t \\leq 0, \\\\ ||p - ((1-t)k_i + tk_j)||_2 & \\text{if } 0 < t < 1, \\\\ ||p - k_j||_2 & \\text{if } t \\geq 1, \\end{cases}$ (5)\nwhere $t = \\frac{(p-k_i) \\cdot (k_j - k_i)}{||k_i - k_j||^2}$ (6)\nHere, $t$ denotes the normalized distance between $k_i$ and the projection of p onto the edge.\nTo derive the edge map $S \\in \\mathbb{R}^{H \\times W}$, we take the maximum value at each pixel across all heatmaps:\n$S(p) = \\max_{ij} S_{ij}(p)$. (7)\nStructural-guided Image Refinement. Traditional optical-flow-based warping methods are effective for handling global deformations but often fail under large motion patterns, such as those involving hands or shoulders, resulting in significant distortions. To address this, we introduce a structure-guided refinement process that incorporates semantic guidance via structural heatmaps.\nInstead of directly rendering the warped feature maps into RGB images, we first predict a low-resolution feature map of size 256 \u00d7 256 \u00d7 32. Multi-resolution edge heatmaps are generated and used as structural cues to refine the feature maps. After performing deformation and occlusion prediction at each scale using TPS (Zhao & Zhang, 2022), the edge heatmaps are fed into the generator. Specifically, we integrate these heatmaps into the fusion block using SPADE (Park et al., 2019), enabling the precise prediction of residuals. These residuals are element-wise added to the warped feature maps, ensuring precise structural alignment.\nTo generate high-resolution RGB images, we employ a U-Net architecture that takes both the warped features and edge heatmaps as inputs. This design preserves fine-grained structural details while compensating for motion-induced distortions. Additional architectural details and analysis are provided in the Appendix.\nTraining Objective. We employ an adversarial loss, along with perceptual similarity loss (LPIPS) (Johnson et al., 2016) and pixel-level L1 loss for image refinement. The reconstruction objective is defined as:\n$L_{rec} = \\gamma L_{GAN} + L_{L1} + L_{LPIPS}$, (8)\nwhere $I_{gt}$ denotes the ground-truth image, and $I_{gan}$ represents the generated image. We leverage a small weighted term of $\\gamma$ to prevent GAN training collapse."}, {"title": "4. Experiments", "content": "Since our work focuses on joint gesture motion and video generation, the main experiments primarily compare our approach with existing methods that also address joint generation. Comparisons for gesture motion generation and avatar video rendering are deferred to the Appendix."}, {"title": "4.1. Experimental Settings", "content": "Dataset and Preprocessing. We utilize PATS (Ginosar et al., 2019; Ahuja et al., 2020) for the experiments. It contains 84,000 clips from 25 speakers with a mean length of 10.7s, 251 hours in total. For a fair comparison, following the literature (Liu et al., 2022c; He et al., 2024) and replace the missing subject, with 4 speakers are selected (Noah, Kubinec, Oliver, and Seth). All video clips are cropped with square bounding boxes, centering speaks, resized to 256 \u00d7 256. We defer the additional details in the Appendix."}, {"title": "4.2. Gesture Video Generation", "content": "Evaluation Metrics. For gesture motion metrics, we use Fr\u00e9chet Gesture Distance (FGD) (Yoon et al., 2020) to measure the distribution gap between real and generated gestures in feature space, Diversity (Div.) (Lee et al., 2019)"}, {"title": "4.3. Ablation Study", "content": "We present ablation studies of keypoint design for image warping, gesture motion representation, generator architecture design, and varios comparisons of image-refinement. We defer additional experiments in the Appendix.\nMotion Keypoint Design. We evaluate four settings for image-warping: (1) unsupervised keypoints for global optical-flow transformation (as in ANGIE and S2G-Diffusion), (2) 2D human poses, (3) 2D human poses augmented with flexible learnable points, and (4) full-model reconstruction with refinement. Each design is assessed using TPS (Zhao & Zhang, 2022) transformation, with self-reconstruction based on these keypoints for evaluation. As shown in Tab. 2a, learnable keypoints lead to a significant decrease in FVD, highlighting their inadequacy for motion control. The inclusion of flexible keypoints does not enhance the image-warping outcomes. Consequently, we opt to utilize 2D pose landmarks exclusively for our study.\nMotion Representation. We evaluate several configurations: (1) baseline: no motion representation, relying solely on the generator to synthesize raw 2D landmarks; (2) + RVQ: utilizing Residual VQ (RVQ) to encode joint face-body keypoints; (3) + distill: learning joint embeddings for speech and gesture in both face and body motions; (4) +"}, {"title": "5. Conclusion", "content": "We present Contextual Gesture, a framework for generating realistic co-speech gesture videos. To ensure the gestures cohere well with speech, we propose speech-content aware gesture motion representation though knowledge distillation from the gesture-speech aligned features. Our structural-aware image generation module improves the transformation of latent motions into realistic animations for large-scale body motions. We hope this work encourage further exploration of the relationship between gesture patterns and speech context for more compelling gesture video generations in the future."}]}