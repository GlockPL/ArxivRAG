{"title": "THE JUMPING REASONING CURVE? TRACKING THE EVOLUTION OF REASONING PERFORMANCE IN GPT-[N] AND O-[N] MODELS ON MULTIMODAL PUZZLES", "authors": ["Vernon Y.H. Toh", "Yew Ken Chia", "Deepanway Ghosal", "Soujanya Poria"], "abstract": "The releases of OpenAI's ol and 03 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, 03 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of ol comes at nearly 750 times the computational cost of GPT-40, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to ol. Nonetheless, we observe that the ol model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available 1.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in large language models (LLMs) have demonstrated impressive capabilities in language understanding and generation, as seen in OpenAI's GPT-[n] series of models (Brown et al., 2020). Yet, true artificial general intelligence (AGI) requires robust reasoning abilities across different modalities (Fei et al., 2021). For instance, models such as OpenAI's new o-[n] series demonstrate a jumping reasoning curve through dramatic improvements on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) (Chollet, 2019). However, the current evaluations in Figure 2 mainly focus on symbolic patterns, whereas humans often reason over complex data involving vision and language. Thus, the ability to perceive, understand, and reason about multimodal inputs remains a crucial component of human-like intelligence, deserving urgent investigation.\nTo this end, puzzles often serve as effective measures of cognitive abilities such as pattern recognition and step-by-step reasoning. Notably, such measures typically do not require specific domain knowledge, allowing individuals from diverse backgrounds to engage with them. One prominent example is Raven's Progressive Matrices (Raven, 1989), a non-verbal assessment tool designed to evaluate abstract reasoning and fluid intelligence. In this test, participants are presented with abstract patterns containing a missing element and must identify the correct piece to complete the pattern.\nThus, inspired by abstract puzzles as measures of intelligence, recent multimodal benchmarks have enabled systematic evaluation across specific cognitive abilities, including visual perception, inductive reasoning, deductive reasoning, and algorithmic problem solving (Chia et al., 2024; Ghosal et al., 2024). Compared to previous measures, they require general understanding of spatial relationships, pattern recognition, and reasoning across visual and language elements, thus providing a more holistic measure of artificial general intelligence. Our research addresses several key questions: (1) How do current state-of-the-art models perform on visual reasoning tasks? (2) What types of pattern recognition and reasoning are particularly challenging? (3) How can we systematically evaluate and compare different models' multimodal reasoning capabilities?"}, {"title": "2 PUZZLEVQA & ALGOPUZZLEVQA", "content": "Understanding the capabilities and limitations of large multimodal models in visual reasoning tasks requires datasets that challenge their cognitive capabilities in nuanced ways. In this study, we employ PUZZLEVQA (Chia et al., 2024) and ALGOPUZZLEVQA (Ghosal et al., 2024) to evaluate abstract visual reasoning and algorithmic problem-solving capabilities.\nMultimodal puzzles serve as a crucial benchmark for evaluating large multimodal models because they require a unique combination of perception, reasoning, and abstraction. Unlike other abstract reasoning benchmarks such as ARC-AGI, where test examples are input to the model as textual context, multimodal puzzles requires the integration of visual and textual information to solve the problem. They also provide an ideal setting for probing systematic reasoning and generalization, as their structured yet diverse nature tests the abilities to infer patterns and apply them across novel contexts.\nExamples from PUZZLEVQA and ALGOPUZZLEVQA are shown in Figure 4 and Figure 5. These two datasets were chosen for their complementary characteristics: while PUZZLEVQA emphasizes basic visual abstract reasoning, requiring pattern recognition to solve puzzles, ALGOPUZZLEVQA features more complex puzzles that demand deducing algorithms for their solutions."}, {"title": "2.1 PUZZLEVQA COMPOSITION", "content": "PUZZLEVQA consists of 2,000 test instances, organized into 10 puzzle categories. Four of these categories focus on single-concept patterns, such as numbers, colors, sizes, and shapes, while the remaining six categories emphasize dual-concept patterns, which combine two distinct concepts. We present some puzzle examples in Figure 4. Each category includes two multimodal templates, with each template capable of generating a variety of unique puzzle instances."}, {"title": "2.2 ALGOPUZZLEVQA COMPOSITION", "content": "ALGOPUZZLEVQA consists of 18 distinct puzzles, each with 100 test instances, resulting in a total of 1,800 test instances. These puzzles cover a wide range of topics, combining both visual and algorithmic categories. Each puzzle includes at least one visual category and one algorithmic category. We present some puzzle examples in Figure 5."}, {"title": "Visual categories:", "content": "1. Colors: Puzzles where understanding the colour of the puzzle components is crucial for solving the question.\n2. Position: In some puzzles, understanding spatial positioning of the puzzle components is necessary for solving the question.\n3. Shape/Size: This category includes the understanding of both absolute and relative shapes and sizes of the puzzle components.\n4. Text: Certain puzzles incorporate optical characters or embedded text that provide important information that must be used to correctly solve the question."}, {"title": "Algorithmic categories:", "content": "1. Arithmetic: These puzzles require basic mathematical operations, such as addition, multiplication, counting, and modular arithmetic, to solve the problem.\n2. Boolean Logic: Some puzzles require the application of Boolean logic, such as checking conditions like equality or inequality between different components or states.\n3. Combinatorics: These puzzles involve counting combinations and permutations of the components or states. The questions typically ask about the number of unique configurations that can be achieved after performing a sequence of operations.\n4. Graphs: Puzzles in this category can be represented as graph data structures, where graph algorithms can be applied to find the solution.\n5. Optimization: Optimization puzzles focus on finding the best solution, whether it involves minimizing time, steps, or maximizing a given outcome (e.g., summation or sorting).\n6. Search: These puzzles require the use of search algorithms, including breadth-first search or exhaustive search, to explore possible solutions or configurations.\n7. Sets: In these puzzles, solving the problem requires considering the identical nature of some objects and the equivalence of some positions or configurations.\nThe algorithmic categories are not mutually exclusive, and puzzles may contain two or more categories in order to derive the answer. The primary goal of ALGOPUZZLEVQA is to assess the gap between visual data interpretation and algorithmic problem-solving skills. The puzzles are designed to challenge and evaluate large multimodal models, testing their ability to solve algorithmic problems that require visual understanding, language comprehension, and complex algorithmic reasoning."}, {"title": "3 EXPERIMENTAL SETUP", "content": "To ensure a comprehensive evaluation, we present the puzzles to the models in both multiple-choice and open-ended formats. The original datasets consist of puzzles in a multiple-choice format. Below, we provide a detailed explanation of both the multiple-choice and open-ended setups."}, {"title": "3.1 EVALUATION PIPELINE", "content": "To ensure a comprehensive evaluation, we present the puzzles to the models in both multiple-choice and open-ended formats. The original datasets consist of puzzles in a multiple-choice format. Below, we provide a detailed explanation of both the multiple-choice and open-ended setups."}, {"title": "3.1.1 MULTIPLE CHOICE SETUP", "content": "(First stage) CoT Prompting. We leverage zero-shot chain of thought (CoT) prompting (Kojima et al., 2022) with a prompt similar to ``Let's think step by step'' to elicit reasoning steps from GPT-[n] models. For the o-[n] model, we do not use CoT prompting since these models are trained to perform reasoning internally. If the letter answer can be extracted during the first prompting stage with regular expressions, we skip the second stage. However, if the letter answer cannot be extracted, we proceed to the answer extraction stage.\n(Second stage) Answer Extraction. We take the initial prompt from the first stage and the generated output, then append the text \"Therefore, among (A) (B) (C) (D), the answer is:\" for puzzles with four options, or \"Therefore, among (A) (B) (C), the answer is:\" for puzzles with three options. This allows us to extract the final letter answer"}, {"title": "3.1.2 OPEN ENDED SETUP", "content": "(First stage) CoT Prompting. Similar to the setup described in Section 3.1.1, we use CoT prompting for GPT-[n] models. However, for o-[n] models, we do not use CoT prompting. In the open-ended setup, instead of performing answer extraction, we use GPT-40 to directly match the generated answer with the ground truth answer.\n(Second stage) Answer Matching. For open-ended responses, we use GPT-40 to compare the generated responses from the first stage with the ground truth answers. Specifically, GPT-40 is prompted to evaluate whether the generated response aligns with the ground truth answer. The exact prompt used for this evaluation is provided in Appendix C. Similar to the multiple-choice setup, the accuracy of predicting the correct final answer is used as the evaluation metric."}, {"title": "3.2 MODELS", "content": "We investigate the performance of GPT-[n] and o-[n] models: (1) GPT-4-Turbo (turbo-2024-04-09), (2) GPT-40 (2024-08-06), (3) 01 (2024-12-17). We selected these two model series from OpenAI due to their rapid advancements and significant contributions to the field of large language models (LLMs). Each version has introduced innovative techniques that have shaped the LLM landscape. For example, GPT-4-Turbo has set benchmarks in understanding visual inputs, while GPT-40 is a highly efficient model designed for multimodal inputs and outputs. The o1 model, a recent addition, is trained with a step-by-step reasoning objective and reinforcement learning, making it a powerful reasoner capable of handling a wide range of tasks effectively. We use the \u201chigh\u201d reasoning mode for ol. Please note that our study can easily be expanded to other closed-sourced and open-sourced models."}, {"title": "4 RESULTS", "content": "To investigate the evolution of reasoning performance, we present the average accuracy on PUZZLEVQA and ALGOPUZZLEVQA over time, along with the inference cost per puzzle, as shown in Figure 1. The reported accuracy corresponds to the open-ended setting, while the inference cost per puzzle is estimated based on the average API cost for processing 200 puzzle questions. We observe a more significant jump in performance from the GPT-[n] to o-[n] models, highlighting their enhanced reasoning capabilities. However, this reasoning advancement comes at a more than 750x inference cost compared to GPT-40, likely due to more extensive reasoning steps or hidden processes (Wei et al., 2022)."}, {"title": "4.1 SCALING TRENDS", "content": "To investigate the evolution of reasoning performance, we present the average accuracy on PUZZLEVQA and ALGOPUZZLEVQA over time, along with the inference cost per puzzle, as shown in Figure 1. The reported accuracy corresponds to the open-ended setting, while the inference cost per puzzle is estimated based on the average API cost for processing 200 puzzle questions. We observe a more significant jump in performance from the GPT-[n] to o-[n] models, highlighting their enhanced reasoning capabilities. However, this reasoning advancement comes at a more than 750x inference cost compared to GPT-40, likely due to more extensive reasoning steps or hidden processes (Wei et al., 2022)."}, {"title": "4.2 \u0395\u03a7\u03a1\u0391NDED EVALUATION", "content": "To assess the holistic reasoning capabilities of multimodal models, we present results for both open-ended and multiple-choice answer formats in Table 1. Overall, we observe that all models generally perform better in the multiple-choice setting compared to the open-ended setting. Particularly, the o1 model experiences the largest performance decline, with a 23.1% drop in score on ALGOPUZZLEVQA between multiple-choice and open-ended settings. Conversely, the o1 model shows the smallest performance decline on PUZZLEVQA, with a score drop of 12.9% between the two formats."}, {"title": "\u0395\u03a7\u03a1\u0391NDED EVALUATION", "content": "To assess the holistic reasoning capabilities of multimodal models, we present results for both open-ended and multiple-choice answer formats in Table 1. Overall, we observe that all models generally perform better in the multiple-choice setting compared to the open-ended setting. Particularly, the ol model experiences the largest performance decline, with a 23.1% drop in score on ALGOPUZZLEVQA between multiple-choice and open-ended settings. Conversely, the ol model shows the smallest performance decline on PUZZLEVQA, with a score drop of 12.9% between the two formats."}, {"title": "PUZZLEVQA.", "content": "PUZZLEVQA is a relatively simple dataset designed to evaluate the abstract reasoning abilities of large multimodal models. According to (Chia et al., 2024), human performance on a subset of this dataset in the multiple-choice setting reaches a score of 91.4%. However, GPT-[n] and o-[n] models still fall significantly short of this benchmark, with o1 achieving the highest score among them at 79.2% in the multiple-choice setting. Among single-concept puzzles, most models find the size and shape categories to be the most challenging. Specifically in the multiple-choice setting, ol achieves scores of only 66.5% for shapes and 77.5% for size categories, in contrast to other single-concept categories like colors and numbers, which achieve significantly higher scores of 91.5% and 99.0%, respectively. Performance declines further in dual-concept puzzles compared to single-concept ones. In particular, models struggle more with those involving combinations such as Numbers & Size, Size & Shapes, and Colors & Size. The lowest scores were observed with the Numbers & Size puzzle, where GPT-4-Turbo, GPT-4o, and o1 achieved only 32.5%, 30.5%, and 49.0%, respectively. Overall, we observe a moderate improvement in performance from GPT-4-Turbo to GPT-40 and a substantial leap from GPT-40 to ol across majority puzzle categories in PUZZLEVQA. This highlights the effectiveness of the specialized reasoning enhancements introduced in 01. Another key finding is the inability of GPT-4-Turbo to effectively perceive and reason with colors\u2014a challenge that GPT4o and o1 overcome to some extent, outperforming GPT-4-Turbo by approximately 22% to 29% in this area in the open-ended setting. Additionally, 01 demonstrates superior performance in numerical reasoning and puzzles within the size category. Interestingly, ol does not improve GPT-40's performance on puzzles requiring reasoning about shapes. In fact, in the multiple-choice setting, o1 underperforms GPT-40 for the shapes category by 4.5%."}, {"title": "ALGOPUZZLEVQA.", "content": "ALGOPUZZLEVQA is a more challenging visual puzzle reasoning dataset that demands algorithmic problem-solving abilities. The performance of all models remains relatively low on this dataset, achieving a score of 36.5%, 43.6%, and 55.3% for GPT-4-Turbo, GPT-40, and o1 respectively in the multiple-choice setting. However, similar to PUZZLEVQA, we observe a notable improvement in performance with o1 compared to GPT-4-Turbo and GPT-40. Specifically for puzzles such as calendar, clock, and number slide in the multiple-choice setting, ol demonstrates significant improvements over GPT-40, with performance gains of 26%, 50%, and 43%, respectively. In the open-ended setting, we observe a significant drop in performance compared to the multiple-choice setting, particularly on puzzles like Chain Link and Wood Slide, where performance across all models is close to 0%. For example, o1 achieves scores of 1.0% on Chain Link and 0.0% on Wood Slide in the open-ended setting, while it achieves 61.0% and 25.0%, respectively, in the multiple-choice setting. On the other hand, o1 performs well on puzzles like Calendar and Number Slide, reaching scores of 92% and 89%, showing a considerable improvement over GPT-40, which scores 66% and 46%, respectively."}, {"title": "4.3 DISCUSSIONS", "content": "We evaluate the models in two settings: a multiple-choice setup and an open-ended setup. As shown in Table 1, all models experience a significant performance decline in the open-ended setup. In PUZZLEVQA, the average drop is relatively mild (ranging from 8 to 15%), whereas in ALGOPUZZLEVQA, the average decline is more pronounced (ranging from 20 to 28%), indicating the increased difficulty of the task."}, {"title": "4.3 DISCUSSIONS", "content": "Multiple Choice vs Open Ended Problems. We evaluate the models in two settings: a multiple-choice setup and an open-ended setup. As shown in Table 1, all models experience a significant performance decline in the open-ended setup. In PUZZLEVQA, the average drop is relatively mild (ranging from 8 to 15%), whereas in ALGOPUZZLEVQA, the average decline is more pronounced (ranging from 20 to 28%), indicating the increased difficulty of the task."}, {"title": "5 CONCLUSION", "content": "In this study, we evaluated and analyzed the multimodal reasoning capabilities of GPT-[n] and o-[n] models on PUZZLEVQA and ALGOPUZZLEVQA. Our experiments highlight significant improvements in multimodal reasoning performance from GPT-[n] to o-[n] models, with o1 demonstrating the most substantial gains. However, these advancements come at a considerably higher inference cost. Across both multiple-choice and open-ended settings, models consistently perform better in the multiple-choice setting, with o1 experiencing the largest performance drop on ALGOPUZZLEVQA in the open-ended setting. In PUZZLEVQA, ol outperforms previous models, especially in numerical reasoning tasks, although it still faces difficulties with shape-related puzzles. Similarly, in the more challenging ALGOPUZZLEVQA, overall performance remains low, but ol demonstrates significant improvements over GPT-40, particularly in puzzles like Number Slide and Calendar. Despite these advancements, visual perception remains a key limitation across all models. Providing explicit details about visual perception significantly improves performance, highlighting that accurately interpreting visual input is still a major challenge. While ol demonstrates strong inductive reasoning abilities, its dependence on precise perception suggests that further improvements in visual understanding are needed."}]}