{"title": "Diffusion Instruction Tuning", "authors": ["Chen Jin", "Ryutaro Tanno", "Amrutha Saseendran", "Tom Diethe", "Philip Teare"], "abstract": "We introduce Lavender, a simple supervised fine-tuning (SFT) method that boosts the performance of advanced vision-language models (VLMs) by leveraging state-of-the-art image generation models such as Stable Diffusion. Specifically, Lavender aligns the text-vision attention in the VLM transformer with the equivalent used by Stable Diffusion during SFT, instead of adapting separate encoders. This alignment enriches the model's visual understanding and significantly boosts performance across in- and out-of-distribution tasks. Lavender requires just 0.13 million training examples-2.5% of typical large-scale SFT datasets and fine-tunes on standard hardware (8 GPUs) in a single day. It consistently improves state-of-the-art open-source multimodal LLMs (e.g., Llama-3.2-11B, MiniCPM-Llama3-v2.5), achieving up to 30% gains and a 68% boost on challenging out-of-distribution medical QA tasks. By efficiently transferring the visual expertise of image generators with minimal supervision, Lavender offers a scalable solution for more accurate vision-language systems. All code, training data, and models will be shared.", "sections": [{"title": "1. Introduction", "content": "Training frontier foundation models from scratch costs millions of dollars at minimum, requiring hundreds of GPUs and millions to billions of data (DeepSeek-AI et al., 2024). This challenge is even more pronounced in multimodal settings: vision-language models (VLMs) often face data scarcity because collecting paired image-text datasets is expensive (Zhu et al., 2024). A common workaround is to apply supervised fine-tuning (SFT) on a pretrained large language model (LLM), leveraging its abundant text-only pre-training and adjusting bridging layers or additional encoders with limited image-text pairs (Liu et al., 2024c; Covert et al., 2024; Jiang et al., 2023). However, these methods typically overlook the importance of transformer-level attention alignment within the LLM core-a key component for effectively expanding text-based models into the visual domain.\nPrecise visual-text alignment is crucial for advanced multimodal reasoning. While both VLMs and diffusion models (DMs) process text and images, they diverge in their generation objectives. We observe that DMs, such as Stable Diffusion (Rombach et al., 2021), which reconstructs images at the pixel level, appear to learn more precise text-vision attention maps than VLMs that are optimised solely for text token generation.\nIn this work, we demonstrate that the high-quality cross-attention maps from these DMs indeed offer a useful target for guiding the text-vision attention in VLMs during SFT, thus improving word-to-region alignment and the overall performance. We introduce Lavender (Language-and-Vision fine-tuning with Diffusion Aligner), the first framework to directly align VLM transformer attention layers with those of Stable Diffusion. Specifically, during SFT, Lavender transfers diffusion-based attention distributions to VLMs, enhancing core visual-textual interactions. To mitigate catastrophic forgetting, we additionally propose several attention aggregation methods and training strategies that preserve existing VLM competencies.\nWe begin by verifying Lavender on a small OpenFlamingo model: entropy and visual analyses show Lavender aligns VLM attention with DM attention. Leveraging Stable Diffusion to offline extract per-word attention on 130k label-image pairs-no extra training cost-Lavender yields notable gains over autoregressive finetuning on 20 diverse benchmarks, including up to 70% improvement on OpenFlamingo across seven benchmarks. For Llama 3.2-11B, fine-tuned on in- and out-of-distribution data, performance improves by up to 30% on 19 benchmarks, surpassing comparable small open-source models by 50%. On self-attention-only MiniCPMv2.5, it achieves up to 4% gains.\nThis advantage extends to severely OOD domains, evidenced by a 68% boost on the WorldMedQA medical benchmark for Llama 3.2-11B. Further analyses reveal that larger fine-tuning sets help Lavender resist overfitting more effectively than autoregressive baselines, and the aligned attention maps yield finer-grained visual understanding. Together with qualitative evidence of improved VLM attention, these results confirm Lavender's premise: diffusion-based attention distributions effectively align visual and textual representations, fostering more robust, data-efficient VLMs.\nAblation studies reveal that the method of attention aggregation and the choice of layers for fine-tuning are critical to performance. Learned aggregation strategies outperform"}, {"title": "2. Related Work", "content": "We find that the gap in VLM alignment partly stems from the technological trajectories pursued over the past half-decade. One key VLM milestone was Flamingo (Alayrac et al., 2022), which laid the foundation for modern VLMs. In its design, images and text are processed by separate encoders, unified through a perceiver resampler (Jaegle et al., 2021), and passed through deep transformer layers combining cross-attention and self-attention. Flamingo's elegant architecture established a new standard, influencing a range of subsequent models (Li et al., 2022; 2023b; You et al., 2023). The importance of aligning vision-text correlations is evident in the design of Llama 3.2 (Dubey et al., 2024), released two years later, which adopts Flamingo's strategy of using a dedicated cross-attention module for effective interaction handling. However, training a VLM with a dedicated cross-attention module end-to-end requires substantial data and computational resources. These models are typically pre-trained on millions or even billions of image-text pairs and interleaved image-text datasets (Zhu et al., 2024). Similar challenges apply to broader multimodal models beyond vision and language (Lu et al., 2024).\nUnlike VLMs, single-modality large language models (LLMs) have scaled more rapidly (Ouyang et al., 2022; Brown et al., 2020; Chowdhery et al., 2023), often consuming over 100 million examples spanning 1,800 tasks (Longpre et al., 2023). VLMs, however, face a training data gap due to the high cost of acquiring paired image-text datasets. To address this, researchers proposed leveraging scaled LLMs by instruction fine-tuning them on as little as 150k paired visual question answering (VQA) data using an autoregressive loss. This approach, pioneered by Zhu et al. (2023); Dai et al. (2023); Liu et al. (2024c), aligns text and image tokens through fine-tuning connectors such as MLPs, encoders, or decoders connecting to the LLM, providing an efficient pathway to integrate vision with language models for diverse tasks (Wang et al., 2024a; Li et al., 2024; Koh et al., 2024; Chen et al., 2025a; Wang et al., 2024b; Chen et al., 2023; Huang et al., 2024; Liu et al., 2024d; Gao et al., 2023; Cha et al., 2024).\nHowever, the community soon recognised that the vision capabilities integrated through these small adapter layers outside the LLM (in LLaVA-like approaches) remain insufficient (Tong et al., 2024c). To address this gap, Covert et al. (2024); Karamcheti et al. (2024) refined vision encoders to align more closely with pretrained vision models. Jiang et al. (2023); Kar et al. (2025) proposed merging multiple visual encoders with projection layers before feeding them into the LLM. Separately, Tong et al. (2024a); Shi et al. (2024); Zong et al. (2024) explored merging a larger number of diverse vision expert models with fine-tuned projection layers, integrating the combined vision features either before the LLM or within the LLM transformer layers, respectively.\nDiffusion Models (DMs), as vision experts, have gained recent attention. Wang et al. (2024c) use DM's image generation loss to enhance the visual encoder. Other approaches merge VLMs with DM's image generation, either by fine-tuning DMs with VLM outputs (Tong et al., 2024b; Hernandez et al., 2024), sharing a central transformer (Shi et al., 2025; Chen et al., 2025b), or integrating DMs with LLM transformers (Zhou et al., 2024)."}, {"title": "3. Diffusion Instruction Tuning", "content": "We aim to enhance a pretrained Vision-Language Model (VLM) by leveraging attention distributions from a pretrained Diffusion Model (DM). We assume there is an ideal attention distribution that maximises VLM performance and that the DM's attention is closer to this ideal distribution.\n3.1. Models and Notation\nVision-Language Model (VLM). Let \u03b8 be the VLM parameters, pretrained for tasks such as image captioning or question-answering. It models:\n$P(y_I | x, y_q; \\theta).$ (1)\nDiffusion Model (DM). Let $\u03b8_D$ be the DM parameters, which remain fixed during our procedure. It models:\n$p(x | y; \\theta_D).$ (2)\nAttention Distributions. We write:\n$p_{VLM}(\u03b1 | x, y; \u03b8),  p_{DM}(\u03b1 | x, y; \u03b8_D),$ (3)\nWe hypothesise that $p_{DM}(\u03b1 | x, y; \u03b8_D)$ is closer to the optimal posterior attention distribution $p^*(\u03b1 | x, y)$ than $p_{VLM}(\u03b1 | x, y; \u03b8)$, and the two can be aligned by projecting $p_{VLM}$ into a comparable space using small learnable layers.\n3.2. Assumptions\nIdeal Attention Hypothesis: An attention distribution $p^*(\u03b1 | x, y)$ minimises the next-token prediction loss of VLM, $L_{VLM}$; DM Attention Proximity: Empirically, the DM's attention is more concentrated (lower entropy) and hence closer to $p^*$ than the VLM's, supported by Figure 3, experiments in Section 7.1 and detailed justifications in Appendix B; Shared Dataset: Both models use the same image-text set ${(x^{(i)}, y^{(i)})}$; Fixed DM Parameters: $\u03b8_D$ is kept fixed; only \u03b8 is updated; Pretrained VLM Parameters: \u03b8 is further fine-tuned with an attention alignment loss."}, {"title": "3.3. Bayesian Derivation", "content": "Our objective is to update the VLM parameters \u03b8 such that the model not only performs well on its primary task but also aligns its attention mechanism with that of the DM. We formalise this objective within a Bayesian framework.\nPosterior Distribution: We aim to find the posterior distribution of the VLM parameters given the data D and the DM's attention distributions:\n$p(\u03b8 | D, A_{DM}) \u221d p(D | \u03b8) p(A_{DM} | \u03b8)  p(\u03b8),$ (4)\nwhere $A_{DM} = {p_{DM}(\u03b1 | x^{(i)}, y^{(i)}; \u03b8_D)}$ is the collection of attention outputs derived from the DM's conditional distribution, and $p(\u03b8)$ is the prior over the VLM parameters.\nLikelihood of the Data: The likelihood of the data given \u03b8 is:\n$p(D | \u03b8) = \u03a0_i p(y^{(i)} | x^{(i)}, y^{(i)}; \u03b8).$ (5)\nThe negative log-likelihood corresponds to the standard loss function $L_{VLM}(\u03b8)$ used to fine-tune the VLM:\n$L_{VLM}(\u03b8) = -\u03a3_i log p(y^{(i)} | x^{(i)}, y^{(i)}; \u03b8).$ (6)\nLikelihood of the DM's Attention: We model the likelihood of observing the DM's attention given the VLM's parameters, denoted as $p(A_{DM} | \u03b8)$. To simplify the notation and make the equations more concise, we introduce:\n$\u03b4^{(i)}(\u03b8) = p_{VLM}(\u03b1 | x^{(i)}, y^{(i)}; \u03b8) - p_{DM}(\u03b1 | x^{(i)}, y^{(i)}; \u03b8_D).$ (7)\nThis represents the pointwise difference between the VLM's and DM's attention distributions for the i-th data point, serving as a measure of divergence at each attention location \u03b1. Assuming that these differences are Gaussian-distributed with equal variance, the likelihood can be expressed as:\n$p(A_{DM} | \u03b8) \u221d exp (-\u03bb/2 \u03a3_i ||\u03b4^{(i)}(\u03b8)||^2)$. (8)"}, {"title": "3.4. Interpretation and Practical Implementation", "content": "By minimizing $L_{total}(\u03b8)$, we maximize $p(\u03b8 | D, A_{DM})$, aligning the VLM's attention closer to the ideal posterior $p^*(\u03b1 | x, y)$. This improves the VLM's ability to associate textual tokens with relevant visual regions, enhancing vision-text understanding. To implement this, we extract per-word attention distributions from the pretrained DM and VLM:\n$p_{DM}(\u03b1 | x^{(i)}, y^{(i)}; \u03b8_D),  p_{VLM}(\u03b1 | x^{(i)}, y^{(i)}; \u03b8).$ (11)\nFine-tuning minimizes:\n$L_{total}(\u03b8) = -\u03a3_i log p(y^{(i)} | x^{(i)}, y^{(i)}; \u03b8) + \u03bb \u03a3_i ||\u03b4^{(i)}(\u03b8)||^2.$ (12)\nThis model-agnostic process, outlined in Algorithm Algorithm 1, requires no additional data and works with various VLM architectures."}, {"title": "4. Attention Alignment", "content": "We discuss how to compute per-word attention in VLMs and DMs. Although both employ attention to capture vision-text interplay, their attention aggregation differs. Understanding these distinctions is key to effective alignment.\n4.1. Attention Aggregation in Diffusion Models\nText-guided diffusion models generate images from textual input by iteratively denoising a random-noise image. During each denoising step, cross-attention layers enable the model to focus on relevant textual tokens. Specifically, queries Q are derived from the noisy image $x_t$, while keys K and values V come from the text embedding v:\n$Q = f_q(x_t),  K = f_k(v),  V = f_v(v),$ (13)\nwhere $f_q, f_k$, and $f_v$ are pretrained projection matrices of DM. The attention map M is then computed as:\n$M = Softmax((QK^T)/\u221ad).$ (14)\nPrior work (Hertz et al., 2022) shows that averaging these maps across layers and time steps reveals meaningful correspondences between words and image regions. The resulting per-word attention distributions $p_{DM}(\u03b1 | x, y; \u03b8_D)$ indicate salient image regions for each token, as shown in Figure 3. We leverage these maps as a proxy for the optimal posterior attention distribution $p^*(\u03b1 | x, y)$, guiding the VLM's alignment toward more focused vision-text interactions.\n4.2. Attention Aggregation in Vision-Language Models\nVision-language models (VLMs) use transformer attention to connect text tokens ($T_t$) with image patch tokens ($T_p$) across multiple heads and layers, forming attention weights $w^{hl}_{(t,p)}$. These weights capture semantic and spatial relationships between tokens and patches.\nTo create per-word saliency maps, we first aggregate attention across all heads and layers, reducing the $(N_{text} \u00d7 N_{patch} \u00d7 H \u00d7 L)$ tensor to a $(N_{text} \u00d7 N_{patch})$ matrix. Next, we reshape this matrix into a \u221aN_{patch} \u00d7 \u221aN_{patch} grid, which approximately reconstructs the spatial layout of the original image. This process generates interpretable saliency maps, where each row highlights how a text token focuses on image patches. These maps allow us to align VLM attention with that of the Diffusion Model (DM), as shown in Figure 5. To explore the best attention aggregation mechanisms for VLMs, we employ various approaches.\n4.2.1. SIMPLE AGGREGATION FUNCTIONS\nA straightforward approach is to pool attention weights A (i.e., $w^{hl}_{(t,p)}$) across heads H and layers L via mean or max. We consider four strategies:\n$A^{(L,H)}(A) \u2208 {max-max, max-mean, mean-max, mean-mean},$"}, {"title": "4.2.2. ATTENTION FLOW", "content": "Proposed by Abnar & Zuidema (2020), attention flow accumulates attention across multiple layers to track how information propagates through the network. Unlike simple pooling, this method captures deeper dependencies by considering all layers together.\nTo capture interactions across layers, we recursively update the attention map. Starting with the first layer's attention $A^{(1)}$, we merge each subsequent layer $A^{(l)}$ using element-wise multiplication or addition:\n$\\overline{A} = \\overline{A}  A^{(l)}$ or $\\overline{A} = \\overline{A} + A^{(l)}$.\nThis method, previously applied by Lin et al. (2024) at the sentence level, is extended here to finer-grained word-level attention maps. By aggregating attention across layers, this approach may highlight semantic relationships that simpler methods overlook. Further details are in Appendix D."}, {"title": "4.2.3. LEARNING THE ATTENTION AGGREGATIONS", "content": "Standard approaches aggregate attention using predefined pooling methods, which may lose fine-grained relationships. Instead, we introduce parallel cross-attention parameters (WQa, WKa) alongside the pretrained projections (WQ, WK). This enables us to capture richer semantic correlations without overwriting existing weights.\nTo retain the benefits of the pretrained attention while incorporating new learned patterns, we compute both the original attention A and a parallel attention Ad during each forward pass. The parallel attention Ad is then used to align with the DM:\n$A = Softmax((QK^T)/\u221ad_k),$ (15)\n$A_d = Softmax((Q_a K^T_a)/\u221ad_k).$ (16)\nEmpirically, we find that learning parallel cross-attention"}, {"title": "4.3. Aligner Network", "content": "To improve attention alignment between the Vision-Language Model (VLM) and the Diffusion Model (DM), we introduce a lightweight Aligner network. This network refines the parallel (or aggregated) attention Ad into a single-channel map, making it directly comparable to the DM's attention $p_{DM}(\u03b1 | x, y; \u03b8_D)$. Inspired by Squeeze-and-Excitation networks (Hu et al., 2018), it efficiently transforms attention representations while preserving key semantic information. The Aligner network consists of several small (3-5) layers, using either MLPs or convolutions. First, it expands the representation to capture richer features, applies non-linear transformations, and then compresses it back into a single-channel attention map. Empirically, we found convolutional layers better capture local spatial cues than MLP, detailed comparisons are provided in Appendix F.1. During fine-tuning, the Aligner output is compared to the DM's attention via:\n$L_{att}(\u03b8') = \u2211_i || Aligner(A^{(i)}_d) - p_{DM}(\u03b1 | x^{(i)}, y^{(i)}; \u03b8_D) ||^2,$ (17)\nguiding the VLM's attention toward the DM's more focused distribution, capturing complex semantic correlations while preserving the original pretrained parameters."}, {"title": "4.4. Lavender Integration", "content": "Cross-Attention. For VLMs with dedicated cross-attention layers, each head produces word-to-patch weights $w^{hl}_{(t,p)}$ mapping text tokens Tt to image patches Tp. We can reshape these weights into spatial grids and aggregate across heads/layers, then apply the Aligner network to yield final per-word saliency maps comparable to DM attention. To ensure consistency, we process the extracted attention weights as follows: 1) Interpolate the attention weights to form a roughly square matrix. 2) Arrange the tiles (if the image is represented as multiple tiles) into a coherent spatial layout. 3) Resize the maps to a standard resolution (e.g., 32 \u00d7 32) for downstream processing.\nSelf-Attention Only. When both text and image patches are interleaved in a single sequence, tokens attend to each other in a bidirectional or causal manner. To extract word-to-patch correlations, we must: 1) Identify which subset of tokens corresponds to text and which correspond to image patches. 2) Apply a causal or bidirectional mask to exclude irrelevant attention connections. 3) Reshape and interpolate the extracted attention weights to reconstruct a meaningful spatial representation. This process involves selecting the appropriate text and vision token indices, interpolating attention maps into a square grid, resizing to a fixed resolution (e.g., 32 \u00d7 32), and optionally incorporating the Aligner network output Ad or merged attention maps. This process is demonstrated in Figure 6. By following this procedure, we extract a meaningful per-word attention map aligned with the DM attention, enabling Lavender to improve vision-text alignment even in models that rely exclusively on self-attention."}, {"title": "5. Implementation", "content": "We integrate Lavender with three VLMs-cross-attention VLMS (OpenFlamingo, Llama 3.2-11B-Vision Instruct) and self-attention VLMs (MiniCPM-Llama 3-v2.5)-using Stable Diffusion v1.4 (Rombach et al., 2021) to provide per-word attention targets. We introduce a lightweight Aligner network and an attention alignment loss to guide the VLM toward the DM's more focused distributions.\n1) DM Attention Extraction. We extract per-word attention maps from Stable Diffusion v1.4 by applying a shortened inversion process (Mokady et al., 2022; Jin et al., 2023) using the paired image-label data. Attention maps are collected at each denoising step and reshaped into a fixed 32 \u00d7 32 resolution. For efficiency, we limit inversion to 5 steps and diffusion to 10 steps, enabling each image to be processed in 20 seconds on a single V100 GPU. 2) Cross-Attention VLMs. For OpenFlamingo and Llama 3.2-11B-Vision Instruct, which use cross-attention layers, we integrate Lavender by wrapping the attention layers to extract per-word attention maps. These are then aligned with DM-derived distributions using the Aligner network and attention alignment loss. 3) Self-Attention VLMs. For MiniCPM-Llama 3-v2.5, which relies solely on self-attention, we extract"}, {"title": "6. Training Strategies and Dataset Preparation", "content": "Training Strategies. We adopt several strategies to stabilise alignment objectives and preserve a VLM's pretrained capabilities: 1) Pretraining the Aligner Network. Before updating all parameters, we optionally pretrain only the Aligner network, freezing the rest of the model to absorb DM-based attention signals without disrupting existing representations. This stabilises early training and prevents immediate overwriting of pretrained features. 2) Attention Aggregation and Normalisation. We experiment with different attention aggregation techniques (Section 4) and apply instance or batch normalisation within the Aligner network to control variance, making the final attention distributions more stable and interpretable. 3) Configurable Aligner Network. The Aligner is configurable in depth, allowing trade-offs between efficiency and accuracy. We experiment with \"light\" (single-layer), \"sim\" (two-layer), and \"deep\" (four-layer) variants to balance computational complexity and performance. 4) Parameter-Efficient Fine-Tuning (PEFT). We incorporate PEFT (e.g., LoRA) to prevent catastrophic forgetting by restricting updates to a small, controlled parameter subset. This improves stability and preserves pretrained VLM knowledge. 5) Sampling Strategies. We define two approaches for selecting predicted words for alignment: a) Root word match (allows lemmatized matches, reducing over-strict alignment constraints). b) Exact word match (requires exact token match for stricter supervision).\nDataset Preparation. We process images with Stable Diffusion to obtain per-word attention targets across four datasets: 1) Flickr30k (Young et al., 2014): 31,783 images, 158,915 captions, focused on everyday human activities. 2) Laion50k (Schuhmann et al., 2022): A 50k subset of Laion-5B (5.85B image-text pairs), used for additional method verification. 3) RLAIF-V 83k (Yu et al., 2024): A large-scale multimodal dataset provides 83,132 preference pairs covering multiple sources (MSCOCO, ShareGPT-4V, MovieNet, VQA variants). 4) OCRVQA30k: A 30k subset of OCR-VQA, containing text-rich images with associated question-answer pairs. All datasets are processed with DMs to extract attention maps and scaled across different VLMs,"}, {"title": "7. Experiments and Results", "content": "We first validate Lavender on a small-scale setup with OpenFlamingo (Section 7.1) before scaling it to MiniCPMv2.5 (Yao et al., 2024) and Llama 3.2-11B-Vision Instruct (Dubey et al., 2024). We evaluate these models on 20 standard VLM benchmarks, comparing them against 23 baseline models (Section 7.2). To further analyse performance, we conduct data overlap analysis (Section 7.3), investigate scaling behaviour and training efficiency (Section 7.4), and examine out-of-distribution generalisation in medical QA benchmarks (Section 7.5). Finally, we present qualitative results, including aligned attention visualisations and VQA examples, using Llama 3.2-11B (Section 7.6).\n7.1. Empirical Verification\nWe begin by validating the key hypothesis that DM attention distributions are more concentrated and closely approximate an ideal posterior attention for VLMs (Section 3.2).\n1. DM attention has lower entropy than VLM attention. We compare attention entropy across Flickr30k, RLAIF-V83k, and OCRVQA30k, using three VLMs (OpenFlamingo, MiniCPM-v2.5, and Llama 3.2-11B). As shown in Figure 7, the DM's attention entropy is consistently lower, supporting its proximity to the optimal posterior attention $p^*(\u03b1 | x, y)$.\n2. Lavender aligns VLM attention with DM attention. We fine-tune OpenFlamingo with Lavender on Flickr30k and visualise attention alignment over multiple training steps (Figure 8). The best alignment is achieved using: 1) Exact word match sampling instead of root word match. 2) Convolutional Aligner networks instead of MLP. 3) Moderate learning rates (e.g., 1e-4 instead of 1e-5)."}, {"title": "7.2. Scaled Results with Lavender", "content": "After validating Lavender on small-scale fine-tuning with OpenFlamingo, we scale experiments using the Llama 3-based MiniCPMv2.5 and Llama 3.2-11B-Vision-Instruct implementations. Fine-tuning is conducted on a mixture of RV83k, Flk30k, and OV30k datasets, using both autoregressive and Lavender methods, with LoRA or full fine-tuning strategies. We evaluate these models on 20 multimodal benchmarks, comparing against 23 baseline VLMs.\nEvaluation on Multimodal Benchmarks Lavender is evaluated across 20 VLM benchmarks, grouped into four categories: 1) Chart, Diagram, and Document Understanding: AI2D (Kembhavi et al., 2016), ChartQA (Masry et al., 2022), OCRBench (Liu et al., 2023c), OCRVQA (Mishra et al., 2019), TextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2021), and InfoVQA (Mathew et al., 2022). 2) Perception and Multi-Discipline Reasoning: MME (Fu et al., 2023), MMBench (four subsets) (Liu et al., 2023b), ScienceQA (Saikh et al., 2022), MMStar (Chen et al., 2024a), and MMMU (Yue et al., 2024). 3) Real-World Visual Understanding: RealworldQA (x.ai), SEED (Li et al., 2023a). 4) Hallucination Detection: HallucinationBench (Liu et al., 2023a) and POPE (Li et al., 2023c). All benchmarks are evaluated using their default metrics."}, {"title": "Benchmarking Lavender Against External Baselines", "content": "Lavender enhances VLM performance beyond standard next-token prediction fine-tuning. To contextualise its improvements, we compare it against a broad set of baseline models across 16 benchmarks. Figure 12 presents key insights, while Table 1 provides detailed comparisons across three baseline categories.\nBaseline Models. In addition to MiniCPMv2.5 (Yao et al., 2024) and Llama 3.2-11B (Dubey et al., 2024) and their fine-tuned variants as baselines, we include the following groups of VLMs and their performance on the above benchmarks as reference: 1) Small Budget-Constrained Models. This group consists of open-source VLMs with sizes smaller than 10B, using backbones from Vicuna-7B (Zheng et al., 2023), Qwen-7B (Bai et al., 2023), or Llama3-8B (Dubey et al., 2024). Models include: LLaVA-1.5-7B (Liu et al., 2024a), LLaVA-Next-7B (Liu et al., 2024b), Mini-Gemini-7B (Team, 2024), Eagle-X5-8B (Shi et al., 2024), and Cambrian-1-8B (Tong et al., 2024a). 2) Small Data-Heavy SOTA Models. This group includes open-source VLMs with sizes smaller than 20B, typically fine-tuned on datasets containing more than 5M samples. Models in this category are: LLaVA-OneVision-7B (Li et al., 2024), InternVL2-8B (Chen et al., 2024b), Qwen2-VL-7B (Wang et al., 2024a), Molmo-7B (Deitke et al., 2024), and Pixtral-12B (Agrawal et al., 2024). 3) Large SOTA Models. This group comprises large VLMs with sizes greater than 20B, including both open- and closed-source models, typically fine-tuned on extensive datasets (minimum 5M samples). Models include: Cambrian-1-34B (Tong et al., 2024a), LLaVA-OneVision-72B (Li et al., 2024), Qwen2-VL-72B (Wang et al., 2024a), Molmo-72B (Deitke et al., 2024), Claude-3 Haiku (Anthropic, 2024), Claude-3.5 Sonnet (Anthropic, 2024), GPT-4V (OpenAI, 2023), GPT-40 (OpenAI, 2024), Gemini 1.5 Pro (Team, 2024), and Llama-3.2-90B (Li et al., 2024).\nMain Results with MiniCPM-V-2.5. Figure 15 (a) presents results for MiniCPM-V-2.5, a self-attention-only model based on Llama3. Compared to autoregressive fine-tuning, Lavender improves performance on 16 out of 20 tasks by up to 4%, while limiting drops to a maximum of -1%. The modest overall gains are expected due to dataset reuse (RV83k) and the inherent difficulty of applying text-vision alignment to a purely self-attention model. Nonetheless, Lavender consistently enhances performance even on saturated datasets, demonstrating its capacity to refine modern VLMs without additional data.\nMain Results with Llama 3.2-11B-Vision-Instruct. Figure 15 (b) presents results for Llama 3.2-11B-Vision-Instruct, a cross-attention-equipped model fine-tuned with both LoRA and full fine-tuning. Lavender outperforms autoregressive fine-tuning by up to 30% on 19 out of 20 benchmarks with LoRA and up to 25% on 17 out of 20 benchmarks with full fine-tuning. These results highlight Lavender's stronger improvements on cross-attention-based VLMs (Llama 3.2 and OpenFlamingo) compared to the self-attention-only MiniCPM-V-2.5, reinforcing that the principle of text-vision alignment underlying Lavender is fundamentally rooted in cross-attention mechanisms"}, {"title": "7.3. Data Overlapping Analysis", "content": "Qualitative Analysis. A key question arising from the above benchmark results is how to interpret the observed absolute numbers. In this section, we analyse the overlapping between the disclosed fine-tuning datasets and the benchmark datasets. We perform an ad-hoc qualitative analysis based on the composition of fine-tuning and benchmark datasets to reflect their potential overlapping, as detailed in Figure 13. The qualitative results suggest that the fine-tuning datasets of some state-of-the-art models may exhibit stronger overlapping with the benchmark datasets compared to models fine-tuned on smaller datasets. Lavender's fine-tuning dataset shows an overlap score on the lower end, similar to LLaVA-1.5 (Liu et al., 2024a), which demonstrates its strong generalizability. This simple approach does not exclude the possibility of data overlap during the pretraining stage, but our focus is on fine-tuning, where overfitting is more likely to occur with small fine-tuning datasets."}, {"title": "7.4. Scaling Behaviour", "content": "Lavender is a model-agnostic approach tested on small fine-tuning datasets due to computational constraints. To assess scalability, we fine-tune Llama-3.2-11B on combinations of RV83k, Flk30k, and OV30k using autoregressive and Lavender methods with LoRA or full fine-tuning. Figure 27 presents average performance across eight benchmarks, with detailed results in Figure 28. The findings indicate that Lavender scales better with increased data, effectively reducing overfitting-a common issue with autoregressive fine-tuning on small datasets."}, {"title": "7.5. Severely Out-of-Distribution Medical Benchmark", "content": "We evaluate model generalisation on the extreme OOD WorldMedQA-V (Duan et al., 2024)"}]}