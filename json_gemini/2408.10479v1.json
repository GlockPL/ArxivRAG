{"title": "An End-to-End Reinforcement Learning Based Approach for Micro-View Order-Dispatching in Ride-Hailing", "authors": ["Xinlang Yue", "Yiran Liu", "Fangzhou Shi", "Sihong Luo", "Chen Zhong", "Min Lu", "Zhe Xu"], "abstract": "Assigning orders to drivers under localized spatiotemporal context (micro-view order-dispatching) is a major task in Didi, as it influences ride-hailing service experience. Existing industrial solutions mainly follow a two-stage pattern that incorporate heuristic or learning-based algorithms with naive combinatorial methods, tackling the uncertainty of both sides' behaviors, including emerging timings, spatial relationships, and travel duration, etc. In this paper, we propose a one-stage end-to-end reinforcement learning based order-dispatching approach that solves behavior prediction and combinatorial optimization uniformly in a sequential decision-making manner. Specifically, we employ a two-layer Markov Decision Process framework to model this problem, and present Deep Double Scalable Network (D2SN), an encoder-decoder structure network to generate order-driver assignments directly and stop assignments accordingly. Besides, by leveraging contextual dynamics, our approach can adapt to the behavioral patterns for better performance. Extensive experiments on Didi's real-world benchmarks justify that the proposed approach significantly outperforms competitive baselines in optimizing matching efficiency and user experience tasks. In addition, we evaluate the deployment outline and discuss the gains and experiences obtained during the deployment tests from the view of large-scale engineering implementation.", "sections": [{"title": "1 Introduction", "content": "Order-dispatching - assigning passengers' orders to available drivers in real-time - is the key process in ride-hailing platforms, influencing service experience of both drivers and passengers. There are primarily two research scopes on this topic. The macro-view scope focuses on long-term (several hours to a day) and city-level efficiency optimization. The other scope attends to the optimization under localized spatiotemporal scenarios with high stochasticity, i.e., micro-view order-dispatching (MICOD). This problem centers on matching unspecified number of drivers and orders in each decision window of fixed seconds (a batch), optimizing goals (measured by driver income, pickup distance, etc.) over multiple batches within a localized area (typically 10 minutes over tens of geo-grids, i.e., geo-fence). \nGiven its online nature, the major challenge of MICOD arises from its bilateral dynamics, in which both the number and the contextual attributes of drivers and orders remain unknown in upcoming batches. Therefore, the problem can be regarded as a sequential decision-making problem. Each decision within a batch entails solving a combinatorial optimization (CO) task on order-driver (o-d) assignment, while the goal is to maximize the global gain over a sequence of batches. Given the changing bipartite size in each batch, the problem is demanding with the decision space being combinatorial and boundlessly large. \nIn the MICOD context, the mainstream industrial methodologies follow a two-stage pattern of \"holding + dispatching\". The dispatching part includes general CO approaches, such as the greedy method , the Hungarian algorithm and the stable matching method [3]. While naive methods can manage single batch optimization almost perfectly, it is worth noting that myopic optimization of each CO problem does not guarantee the global gain across multiple batches. For example, a passenger may wait a few more batches for a driver who is 0.5 miles away instead of matching with a driver at"}, {"title": "2 Related Works", "content": "Reinforcement learning (RL) in ride-hailing. With the prevalence of RL, abundant researches formulate large-scale ride-hailing problems in an MDP setting and attempt to solve them in a value-based way. It is intuitive to model each driver as an agent, such that the scalability in action space can be easily handled, usually by learning a tabular or state value function. In addition, there are some multi-agent RL (MARL) approaches . utilizes the mean-field MARL framework to model the interaction among neighboring agents. designs a Kullback-Leibler divergence regularizer to deal with the discrepancy of the D&S distributions. models the problem in a hierarchical setting, regarding hex cell as worker agents and groups of hex cells as manager agents. However, most existing works assume driver homogeneity with shared policies and ignore the contextual diversity.\nOnline matching in order-dispatching. Researches on online matching have been widely conducted in industries, such as online allocation , crowd-sourcing , etc. The MICOD scenario is special for its changing bilateral dynamics and the batching operating manner. For the holding part in two-stage methods, some works consider drivers and orders eligibility of entering the bipartite matching and also investigate batched window optimization using RL methods to achieve long-term gains. attempts to jointly optimize matching radius and batch size to adjust candidates in each batch. Basically, these approaches treat the dispatching process as a part of the environment changes."}, {"title": "3 Methodology", "content": "3.1 MICOD Formulation\nA typical MICOD problem in Didi must comply with:\n\u2022 Micro-view: the spatiotemporal range of MICOD is restricted to a brief time period (usually 10 minutes) and a geo-fence (around 30 square kilometers, consists of 40-50 geo-grids) .\n\u2022 Dynamic contextual patterns: Both drivers and orders may emerge or go offline following their own or joint behavioral patterns before or during service.\n\u2022 Batch mode [19]: MICOD performs order-dispatching in a batch mode. In each batch (time window), the size-unspecified o-d assignment can be formulated as a CO problem.\nWith above restrictions, the objective is to maximize the cumulative gain over the entire period of all batches. Besides, the micro-view and dynamic properties prompt the exploration of the issue by thoroughly leveraging contextual information for each o-d assignment. Note that we restrict to one-to-one assignment (one driver can serve at most one order) in MICOD. Ride-pooling, serial-assign, etc. are not considered due to their low proportion in Didi's online environment. We leave them for future research."}, {"title": "3.2 Two-layer MDP Framework", "content": "With the formulation in Sec. 3.1, we model the problem as an MDP process. The order-dispatching system is the agent, and the action is to select o-d pairs as dispatching results. Due to the dynamic contexts in MICOD, the action space is changing in each batch, making regular MDP transitions hard to model. Instead, we seek to decompose the MDP setting into a two-layer MDP framework with the same agent.\nNaturally, this cascaded two-layer architecture transforms a series of CO tasks across batches into a sequential decision-making problem that can be solved by DRL. We further describe the two-layer MDP in Sec. 3.2.1, Sec. 3.2.2 and present notations in Table 1 for clarity.\n3.2.1 Outer-layer MDP. The outer-layer MDP is defined as follows:\n\u2022 Agent. The centralized order-dispatching system is modeled as an agent and MICOD follows a single-agent setting.\n\u2022 State S. The state $s_t \\in S$ at batch t consists of global-level and local-level information. Formally, $s_t = (I_g^t, I_p^t)$, where $I_g^t$ is the real-time D&S information in the geo-fence, and $I_p^t$ represents all available o-d pairs with contextual attributes (spatiotemporal distributions, behavioral patterns). Note that $I_p^t$ is of unfixed size since the number of o-d pairs is unspecified in each batch.\n\u2022 Action A. The action $a_t$ represents the combinatorial o-d assignment decision in each batch t, i.e., a set of o-d pairs from $I_p^t$. Let n be the total number of assigned pairs in a batch. Note that n is unspecified and hardly the same in different batches (elaborated in Sec. 3.2.2). In fact, not all available drivers or orders should be assigned exhaustively in each batch as a global optimal solution. To achieve this, a \"hold\" action similar to StH will be described in Sec. 3.2.2 as our solution.\n\u2022 Policy. The policy $\\pi(a_t|s_t)$ specifies the probability of selecting a combination of pairs as action $a_t$ given the state $s_t$, and is a composition of sub-policies described in Sec. 3.2.2.\n\u2022 State transition P, Initial state distribution $p_0$. The state transition function $P: S\\times A \\rightarrow S$ captures the results and bilateral uncertainty induced by the workflow in the outer-layer. The initial state distribution $p_0$ describes the spatiotemporal and contextual distribution in each MICOD environment.\n\u2022 Reward. The reward $r_t$ is aligned with different optimization goals: $r_t$ can be defined as the total prices of assigned orders when maximizing driver income or negative pickup distances of assigned o-d pairs when optimizing passenger experience. Note that $r_t$ is received only when a complete action $a_t$ is finished.\nThe objective of the outer-layer MDP is to maximize the discounted return, given policy $\\pi$, in the MICOD context within all T steps (the number of all batches). We can formulate the objective with discount factor $\\gamma$:"}, {"title": "3.2.2 Inner-layer MDP", "content": "The inner-layer MDP models a CO problem in each batch, where unspecified number of drivers and orders are presented. We decompose this task into a sequence of sub-actions that iteratively select a proper o-d pair, remove related pairs (thus naturally conform with one-to-one restriction) and enter a new sub-state, until a certain ending criterion is met. The inner-layer MDP respects the outer-layer MDP setting and highlights the following complementary specifics:\n\u2022 Agent The inner-layer MDP agent is exactly the centralized order-dispatching system in the outer-layer MDP.\n\u2022 Sub Action. We define two categories of sub-actions. First, the agent chooses an o-d pair from the pool of available pairs, dubbed as the Decision. In this way, drivers and orders associated with the chosen pair will be removed from the pool. Second, the agent predicts whether to end the current dispatching process or not, dubbed as the Hold. We denote them as c and h, respectively."}, {"title": "3.3 Deep Double Scalable Network", "content": "There are two levels of action space scalability in the setting of the two-layer MDP. First, the outer-layer action is scalable since the total number n of assigned o-d pairs is unidentified in advance for each batch. Second, the inner-layer sub-actions are scalable because at each sub-step i, the sub-action space size $n_i + 2$ decreases according to previous assignments.\nInspired by sequence-generating tasks in Natural Language Processing, we regard each o-d pair as a \"word\" for generation. Such analogy is based on the fact that each o-d pair assignment is conditioned on previous assignments, and it will also influence future assignments. Besides, the generation of a sensible sentence relies on each word, just as an overall optimization is based on each o-d assignment. Therefore, we adopt an encoder-decoder structure to generate assignments, with auto-regressive factorization to adapt to the first level of scalability. The multiple attention mechanisms enable the model to adapt to the second scalability on sub-actions. The proposed architecture is named as Deep Double Scalable Network (D2SN).\n3.3.1 Encoder. At each sub-step i of batch t, the encoder E takes the input of contextual information of all o-d pairs, i.e., $I_p^{t,i}$, and outputs a latent embedding $R_i^t$ of all pairs. For $n_i$ o-d pairs at sub-step i with feature dimension d for each pair, we have\n$R_i^t = E(I_p^{t,i}) = FFN(Attention(I_p^{t,i})), R_i^t \\in R^{n_i\\times d}$ \nAttention() and FFN() represent multi-head attention (MHA) blocks and position-wise feed-forward operations, respectively. With such aids, the size of the output $R_i^t$ keeps consistent with that of the input $I_p^{t,i}$, which is of scalable size.\n3.3.2 Decoder. The decoder takes $R_i^t$ and the global contextual information $I_g^t$ as the input, and outputs (i) the decision probability of each o-d pair and (ii) the hold action decision.\nIn the decoder, in addition to the MHA blocks, we design a shared module to normalize feature dimensions before the two action modules. As depicted in Equation (4), a recurrent block RNN converts the unfixed size of sub-state representation $u_i^t$ to a fixed size vector $G_i^t$, preserving the chronological information of sub-actions. Note that at each sub-step i, the decoder input $u_i^t$ is a concatenation of the initial sub-state $u_0^t=s_t$ (of size $n_0$) and selected pairs {c\u2081,..., c\u1d62\u208b\u2081}:\n$G_i^t = RNN(Attention(u_i^t)), G_i^t \\in R^{1\\times d}, u_i^t \\in R^{(n_0+i-1)\\times d}$ \nThen, $G_i^t$ is leveraged by two parallel sub-action modules:"}, {"title": "3.3.3 Cooperation", "content": "We enable the auto-regressive sub-action sampling within the encoder-decoder cooperation. Different components of {$I_p^{t,i}$, $u_i^t$, $I_g^t$} in state $s_t$ are passed into the encoder and the decoder separately. D2SN generates (hi, ci) of two parallel sub-actions at each sub-step i. If h is not to hold, then in the inner-layer (i) $I_p^t$ is renewed by removing pairs related to the selected o-d pair of c, and (ii) $u_i^t$ is updated to $u_{i+1}^t$ by concatenating {$c_1$,..., c\u1d62\u208b\u2081} and up. This process is repeated until no available pairs in $I_p^t$, or $h_i$ indicates to stop."}, {"title": "3.4 Deep Reinforcement Learning with D2SN", "content": "Our two-layer MDP adopts clipped proximal policy optimization (PPO) . DRL operates in the outer-layer, while the inner-layer is modeled with the proposed D2SN architecture.\n3.4.1 Actor-Critic Design. The training consists of two main networks: an auto-regressive actor (D2SN) and a value-independent critic. The critic utilizes the same decoder structure in D2SN as in Sec. 3.3.2, but without the two action modules. It only receives ($I_p^{t,i}$, $I_g^t$) as the input. Sub-states and actions are not fed into the critic to ensure value-independence.\n3.4.2 DRL Training. Similar to clipped PPO, we use a surrogate objective loss:\n$L^{clip}(\\theta) = \\hat{E}_t [min(pr_t(\\theta)\\hat{A}_t, clip(pr_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t)],$ \nwhere $E_t$ indicates the empirical average over a finite batch of samples, $pr_t(\\theta) = \\frac{\\pi_{\\theta}(a_t| s_t)}{\\pi_{\\theta_{old}}(a_t| s_t)}$ denotes the probability ratio, and the clip function truncates $pr_t(\\theta)$ to the range of (1-\u03f5, 1+\u03f5). $A_t$ is the advantage estimator at batch t. Equation (2) is plugged into $pr_t(\\theta)$ for exact transition probability calculation. Since all sub-actions are sampled auto-regressively, the resulting $\u03c0\u04e9(at|st)$ and \u03c0\u03b8old (at|st) are computed as the product given by the sub-distributions over the action components in sequence:\n$\\pi_\\theta(a_t|s_t) = \\prod_{i=0}^n \\pi_i^t(a_i^t| u_{i-1}^t),$ \nNetwork parameters \u03b8 of the actor are updated by maximizing Equation (7). Besides, the network parameters \u03c6 of the critic V is trained with Generalized Advantage Estimation (GAE):\n$ \\hat{A}_t = \\delta_t + (\\gamma \\lambda)\\delta_{t+1} + \u00b7 + (\\gamma \\lambda)^{T-1-t} \\delta_{T-1}, \\\\ \\delta_t = r_t + V(s_{t+1}) - V(s_t), \\\\ L_c(\\phi) = [V_\\phi(s_t) \u2013 (\\hat{A}_t + V_{\\phi_{old}}(s_{t+1})),$ \nwhere \u03bb is a hyper-parameter to balance variance and bias and \u03c6old denotes parameters before updating in each iteration."}, {"title": "4 Experiments", "content": "4.1 Experiment Setting\n4.1.1 Industrial Benchmarks. We rewind real-world trajectories in multiple cities of China in 2023 from Didi, then split them into 10-minute dataset each. The benchmarks are classified into 12 unique types based on the combination of D&S ratios (order numbers/driver numbers) and ranges of driver capacity (e.g., \"\u2264 400\" means samples with no more than 400 drivers in 10 minutes) as in Table 2. Overall, benchmarks consist of 875 for training, 221 for validation, 12 for test and encompass over 30 features, including drivers' idle movement records, orders' trip duration, etc. They are formally treated as standard datasets in Didi for offline evaluation.\n4.1.2 Simulator. An MICOD-customized simulator from Didi operates in a batch mode of 2 seconds for each 10-minute dataset:\n\u2022 It generates orders and drivers from the dataset every 2 seconds and follows the pipeline of \"Filtering + Matching + Serving\".\n\u2022 It simulates contextual dynamics and behavioral patterns in each batch. With careful calibration, the differences of multiple funnel metrics (answer, complete) between the \"real-world\" and \"simulation\" for L1-L4 are all within 1.5%.\nNote that the simulator serves as an official and trustworthy indicator in Didi for online policy evaluation as it supports multiple deployed policies, including several versions of StH.\n4.1.3 Baselines. We take 6 competitive baselines. Naive CO methods contains the Greedy , Kuhn-Munkres (KM) and Gale-Sharpley (GS) , they are fully used in Didi's scenarios. Two-stage"}, {"title": "4.2 Main Results", "content": "Table 3, 4 present performances of all methods in terms of two tasks APD and TDI, respectively. Overall, D2SN displays notable advantages over baselines in both tasks. Specifically, D2SN outperforms the deployed policy StH in all L1-L4 scenarios, indicating its trustworthy potential of online performance.\nIn the APD task, D2SN slightly lags behind RQL and ID in reducing pickup distance while keeps the highest CR in all L1 tasks. That means D2SN is capable of jointly satisfying order requests (CR) and pickup experience over the entire time period rather than merely focuses on downsizing pickup distances as ID and RQL do.\nFor the TDI task in Table 4, D2SN gains an robust improvement of 0.7% 3.90% and around 1% - 2% over the best of other baselines in terms of TDI and CR, respectively. In particular, as the D&S ratios grow higher, i.e., from L2 to L4, the improvement of D2SN over other methods increases proportionally. This is because with"}, {"title": "4.3 Performance Analysis", "content": "4.3.1 Ablation Study. We conduct ablation study to validate the effectiveness of D2SN by removing the Hold module as a baseline, dubbed as D2SNh. This method dispatches all o-d pairs exhaustively in each batch without holding and is trained in the same way as D2SN does. Results in the last row of Table 3, 4 shows D2SNh performs similar to KM in L1-L4 scenarios, indicating the dispatching ability of the Decision module alone. However, D2SNh lags behind D2SN by 5% - 13% in terms of APD and 1.5% - 5% in terms of TDI, respectively. Such gaps display that D2SNh is unable to reasonably hold for future matching.\n4.3.2 Hold Module Analysis. The well functioning of the Hold module showcases the advantage of D2SN in the ablation. Thus, we further investigate the 4 learning-based approaches with \"hold\" strategies and compare the metrics below (the term \"distinct\" refers to each item is counted once even repeatedly appeared):\n\u2022 Hold-APD ratio: the APD of o-d pairs filtered by \"hold\" strategy, divided by the APD of finished orders.\n\u2022 Hold-O ratio: the number of distinct orders filtered by \"hold\" strategy, divided by the total number of orders.\n\u2022 Hold-TDI ratio: the average TDI of o-d pairs filtered by \"hold\" strategy, divided by the average TDI of finished orders.\n\u2022 Hold-D ratio: the number of distinct drivers filtered by \"hold\" strategy, divided by the total number of drivers.\n\u2022 SR: the number of distinct served individuals, divided by the total number of them (drivers and orders, respectively)."}, {"title": "4.4 Deployment Consideration", "content": "D2SN will be applied in typical spatiotemporal contexts within certain geo-fences. For every 10 minutes, we will apply an appropriate D2SN model from 12 base scenarios according to the historical D&S information of this context. Specifically, in scenarios with extreme dense order requests, parallel-style decoding will be incorporated in the system to downsize the inference latency."}, {"title": "5 Conclusions", "content": "This study concentrates on MICOD in online ride-hailing, addressing bilateral uncertainty challenges. We propose an end-to-end DRL framework with a novel network D2SN to uniformly resolve prediction, decision-making, and optimization without any CO algorithms. We believe our framework can benefit domains of online resource allocation to solve other practical optimization problems."}]}