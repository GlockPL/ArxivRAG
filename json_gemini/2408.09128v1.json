{"title": "Identifying Technical Debt and Its Types Across Diverse Software Projects Issues", "authors": ["Karthik Shivashankar", "Mili Orucevic", "Maren Maritsdatter Kruke", "Antonio Martini"], "abstract": "Technical Debt (TD) identification in software projects issues is crucial for maintaining code quality, reducing long-term maintenance costs, and improving overall project health. This study advances TD classification using transformer-based models, addressing the critical need for accurate and efficient TD identification in large-scale software development. Our methodology employs multiple binary classifiers for TD and its type, combined through ensemble learning, to enhance accuracy and robustness in detecting various forms of TD. We train and evaluate these models on a comprehensive dataset from GitHub Archive Issues (2015-2024), supplemented with industrial data validation.\nWe demonstrate that in-project fine-tuned transformer models significantly outperform task-specific fine-tuned models in TD classification, highlighting the importance of project-specific context in accurate TD identification. Our research also reveals the superiority of specialized binary classifiers over multi-class models for TD and its type identification, enabling more targeted debt resolution strategies. A comparative analysis shows that the smaller DistilRoBERTa model is more effective than larger language models like GPTs for TD classification tasks, especially after fine-tuning, offering insights into efficient model selection for specific TD detection tasks.\nThe study also assesses generalization capabilities using metrics such as MCC, AUC ROC, Recall, and F1 score, focusing on model effectiveness, fine-tuning impact, and relative performance. By validating our approach on out-of-distribution and real-world industrial datasets, we ensure practical applicability, addressing the diverse nature of software projects.\nThis research significantly enhances TD detection and offers a more nuanced understanding of TD types, contributing to improved software maintenance strategies in both academic and industrial settings. The release of our curated dataset aims to stimulate further advancements in TD classification research, ultimately enhancing software project outcomes and development practices by enabling early TD identification and management.", "sections": [{"title": "I. INTRODUCTION", "content": "Technical Debt (TD) is the future cost of additional work in software development that arises from choosing quick, suboptimal solutions or from evolving requirements and tech-nological advancements. Whether resulting from intentional decisions or external factors, TD necessitates careful manage-ment to maintain long-term project viability. Like financial debt, TD can accrue 'interest' over time, increasing complexity and maintenance costs [1], [2].\nTD can be both intentional and unintentional. Intentional TD occurs when developers consciously opt for a faster solution to meet short-term goals, understanding that they will address the deficiencies later. Unintentional TD, on the other hand, results from inadvertent mistakes, poor practices, or a lack of knowledge or insights during the development process [3]. TD manifests in various forms, including complex and hard-to-maintain code, lack of documentation, duplicated code, and outdated libraries or frameworks. Such debts can degrade software quality, increase maintenance costs, and reduce developer productivity. Moreover, excessive TD can lead to missed business opportunities and potentially result in a development standstill, causing companies to lose clients. Consequently, effective management and timely resolution of TD are critical to maintaining the health, competitiveness, and sustainability of software projects [1], [4].\nWith the increasing software complexity and rapid develop-ment cycles, traditional methods of identifying and managing TD-such as manual code reviews and static analysis tools; of-ten prove insufficient. These methods can be labour-intensive, prone to human error, and may not scale well with the size and complexity of modern software systems [5]. This necessitates exploring automated techniques to detect and classify TD issues from project or source control management systems like Github or Jira, enabling timely interventions and proactive assessment of TD and its interest needed to be paid [6]. Automated TD tracking offers several advantages. It reduces the workload associated with manual tracking in backlogs, which teams often resist due to its time-consuming nature. Additionally, it can help identify TD that might otherwise go unnoticed, educating team members on potential issues. Furthermore, it enables analytics on the quantity and types of TD accumulating, facilitating more informed decision-making and proactive management of technical debt [7].\nRecent advancements in Deep Learning, particularly the development of transformer-based models like BERT (Bidi-rectional Encoder Representations from Transformers) [8], [9] and GPT (Generative Pre-trained Transformer) [10], have opened new avenues for addressing complex natural language processing tasks with high accuracy. These models leverage deep learning techniques to understand and generate human-like text, making them well-suited for interpreting the often ambiguous and context-dependent descriptions in software documentation and issue trackers [11]. Given these capabil-ities, applying transformer-based models to the domain of TD classification presents a promising approach. These models can potentially identify subtle nuances and patterns in text that traditional automated systems might miss. Furthermore, their"}, {"title": "A. Research Questions", "content": "This study seeks to answer the following key research questions:\nRQ1: How effective are transformer-based models in classifying TD issues?\nRQ1.1: Does fine-tuning TD models on project-specific data improve their performance?\nRQ2: How does the performance of LLM-like the GPT model compare to the DistillRoberta models in TD clas-sification?\nRQ2.1: Is task specific fine-tuning of LLM like GPT for TD more effective than finetuned DistillRoberta TD classification?\nRQ3: How effective are expert ensemble of binary clas-sifiers in classifying different types of issues compared to multi-class model?\nRQ3.1: How does the performance of LLM-like the GPT model compare to the DistillRoberta Model on different issues types?\nRationale for RQ1 and RQ1.1: Evaluating transformer mod-els' effectiveness in TD classification is crucial for advancing automated detection. Also, to evaluate how fine-tuning may enhance performance by adapting to project-specific nuances and context.\nRationale for RQ2 and RQ2.1: Comparing GPT and Distill-Roberta will provide insights into the balance between model size, GPU compute required, and classification performance, guiding future research and implementations.\nRationale for RQ3 and RQ3.1: Comparing an ensemble of multiple binary classifiers to a multi-class approach will determine the most effective strategy for handling diverse TD types. Assessing GPT and DistillRoberta across TD types will reveal their strengths in various aspects of technical debt and its type."}, {"title": "B. Contributions", "content": "This research makes several significant contributions to the field of software engineering:\nEmpirical Assessment of Transformer-Based Models to classify TD: This study comprehensively evaluates transformer-based models, specifically GPT and Distil-ROBERTa, for TD classification. It examines the impact of fine-tuning on their performance, offering insights into their effectiveness and potential for practical application.\nSpecialised Binary Classifiers for TD Types: We val-idate the use of multiple ensemble binary classifiers for identifying specific TD types (e.g., architectural, testing, build). This approach is compared with a multi-class classification approach, highlighting the advantages of using multiple ensemble binary models for precise and robust TD type identification.\nGeneralisation to Out-of-Distribution Data: The study empirically evaluates the models' ability to generalise to out-of-distribution (OOD) data, addressing a critical challenge for the practical adoption of TD classification tools in diverse software projects.\nPublicly Available Dataset: We will release our curated dataset of technical debt and 13 different types of TD category issues sourced from GitHub Archive Issues events from 2015 to 2024. This dataset will support further research and development in TD classification, fostering advancements in software maintenance."}, {"title": "II. BACKGROUND", "content": "Transformer architectures have revolutionized the landscape of natural language processing (NLP), introducing a paradigm shift in how machines interpret and process human language. These models capture contextual relationships between words in a sentence, irrespective of their sequential order. At the heart of transformers lies the attention mechanism, which enables the model to weigh the importance of different input parts when producing an output [15]. This approach allows transformers to establish global dependencies within the text, leading to more nuanced and context-aware language under-standing\nTransformer models have a set performance benchmark in text classification, where the goal is to assign predefined categories to textual data. The Hugging Face (HF) ecosystem has democratized the implementation of these sophisticated"}, {"title": "B. Comparison of LLMs and BERT-based Models", "content": "Large Language Models (LLMs) like GPT-3 [16] and GPT-4 have revolutionized NLP by leveraging vast amounts of training data to generate human-like text and perform a wide range of language tasks. These models are particularly known for their generative capabilities, making them versatile tools for various applications, including text classification. On the other hand, models like DistilRoBERTa, a distilled version of RoBERTa, offer a more efficient alternative with reduced com-putational requirements while retaining a significant portion of the original model's performance [18].\nSeveral factors come into play when comparing the perfor-mance of LLMs like GPT 4 and DistilRoBERTa in TD classifi-cation. With their extensive training on diverse datasets, LLMs can handle complex language tasks and generate contextually relevant text. This makes them suitable for tasks that require a deep understanding of context and semantics. However, their large size and computational demands can be a drawback in resource-constrained environments [19].\nDistilROBERTa, with its optimized architecture, offers a balance between performance and efficiency. It retains approx-imately 97% of RoBERTa's performance while being faster and more resource-efficient. This makes it a viable option for TD classification tasks where computational resources are limited. Studies have shown that fine-tuning DistilRoBERTa on domain-specific data can enhance its performance, making it comparable to larger models like GPT-4 in certain tasks [18].\nBoth LLMs and DistilRoBERTa benefit from domain-specific training in terms of fine-tuning. Fine-tuning involves adapting the pre-trained model to the specific characteristics of the target domain, which can significantly improve clas-sification accuracy [20]. However, the choice between these models ultimately depends on the specific requirements of the task, including the available computational resources and the complexity of the classification problem."}, {"title": "III. METHODOLOGY", "content": "The methodology for this research was designed to empir-ically evaluate the effectiveness of transformer-based models in classifying TD and its various types using issues extracted from GitHub. Our approach encompasses several stages: data mining, dataset processing and cleaning, model training, and model evaluation, including tests on out-of-distribution (OOD) datasets. Each stage is critical for ensuring the reliability and validity of the findings."}, {"title": "A. Data Mining", "content": "The initial stage of our methodology involved accumulating a large dataset of software development issues from GitHub, specifically targeting those related to TD and its 13 types. We utilized the GitHub Archive (GHArchive) [21], a project that records the public GitHub timeline, logs, and events. Data was mined from January 1, 2015, to May 25, 2024, to cover various software projects and incorporate various issue events.\nIn our study, we employed a carefully crafted regular expression (regex) pattern to identify TD related issues from GitHub Archive data. The regex, designed to be case-insensitive, targeted variations of the term \"Technical debt\" and its abbreviations in issue labels. Our pattern;\nr'(?i)\\b(Technical [-_\\s]?|ech [-_\\s]?)?D(ebt|D)\b|\\b(\nTD|td)\\b|debt)\\b'\neffectively captured diverse representations such as \"Techni-cal debt\", \"Tech_debt\", \"TD\", and standalone \"debt\" mentions from the label field in the issue event from GHarchive. This approach ensured comprehensive coverage of TD-related dis-cussions while minimizing false positives. The pattern's flex-ibility allowed for various word separators and abbreviations commonly used by developers, enhancing our data collection's accuracy and completeness.\nTo identify issues related to specific types of TD [13], we employed a comprehensive regex pattern. The pattern was designed to capture 13 representations of TD types in a case-insensitive format within the 'labels' field of GitHub Issue events. Our regex pattern for issue types is as follows:\nr'(?i)\\b(architect (ure|ural)?|build|code |\ndefect |design|doc (umentation)? | infrastructure|people|\nprocess|requirement|service|test(ing)? |\nautomation) \\b'\nThe regex uses word boundaries (b) to ensure accurate matching and includes common variations (e.g., 'doc' for 'documentation', 'testing' for 'test'). This approach allows for a nuanced classification of TD types in software projects, enabling a more detailed analysis of TD distribution and impact.\nIn addition to identifying issues related to TD and its specific types, we established a ground truth test dataset for our classification model evaluation. This dataset was created by selecting issues that explicitly contained both TD indicators with labels like \"tech-debt\", \"debt\" and \"td\" and specific TD-type labels like \"architecture\", \"build\", \"test\" and other TD types [13] that were unambiguously labelled by the project developers. These entries containing both these labels were removed from all our Datasets to avoid data leakage.This rigorous selection process provided a high-quality dataset for training and evaluating our TD classification models, enhanc-ing the reliability and accuracy of our study results."}, {"title": "B. Dataset Processing and Cleaning", "content": "Once the data was accumulated, it underwent extensive preprocessing and cleaning to prepare it for effective model training. The raw data from GHArchive [21] includes a lot of noise -duplicates, non-English text, irrelevant metadata, URLs, emojis, and special characters that can obscure the underlying patterns related to TD. Therefore, the dataset was cleaned through several steps:\nDuplicate Removal: Ensuring that each issue is unique to prevent bias in model training.\nText Normalization: Converting all text to lowercase to maintain consistency across the dataset.\nNoise Reduction: We removed links, emojis, and special characters that did not contribute to understanding the context of TD.\nContent Filtering: Eliminating issues with very few characters likely does not contain enough classification information.\nOur study focused on 13 distinct categories of TD identified in the ontology by Alves et al: Architecture, Code, Defect, People, Automation, Build, Process, Design, Requirement, Infrastructure, Test, Service, and Documentation [13] and TD itself.\nWe created separate datasets for each category, with the number of instances varying significantly across categories, as shown in Table I. To ensure balanced datasets for robust model training, we implemented a data cleaning process. For each category, we created equal proportions of positive and negative labels, dividing the total instances equally between the two classes. Sridharan et al. demonstrated the importance of data balance in SATD detection tasks, showing that imbalanced datasets can lead to biased models and unreliable predictions. This approach helps to mitigate the risk of model bias. It improves the overall reliability of our classification results across various TD types, extending the principles of balanced data from SATD detection to broader TD classification tasks. [22].\nWe created an out-of-distribution (OOD) dataset for ro-bust evaluation for each category [14], [23]. We identified repositories with the highest number of issues for each type and withheld these from the training and testing datasets. This approach allowed us to assess the models' generalization capabilities across different project contexts.\nWe then split each balanced dataset into training and testing sets using an 85/15 ratio. Despite the random nature of this split, we maintained the balance of positive and negative labels within each subset. This careful data preparation allowed us to develop more focused and accurate classification models for each TD type, enabling a nuanced analysis of TD in software development.\nFor the multiclass classification task, we adopted a dif-ferent approach due to the inherent imbalance in TD type occurrences. We used only the positive labels from each category and employed stratified K-fold cross-validation for model training [24]. We maintained the 85/15 train-test split and also created an OOD dataset for the multiclass model to ensure a comprehensive evaluation.\nThis methodology enabled us to develop and rigorously test both binary and multiclass models for TD classification, pro-viding a thorough analysis of TD detection and categorization in diverse software development scenarios."}, {"title": "C. Model Training", "content": "The training of the models was conducted using a carefully considered approach to ensure robustness and reliability. Bi-nary classification models were trained on a balanced dataset to prevent any bias towards the more frequently occurring class [22]. We employed 5-fold cross-validation over five epochs, a standard research practice that balances adequate model exposure to the data [25], [26].\nTo achieve higher precision, recall and accuracy in classi-fying types of TD, we trained individual binary classifiers for each specific TD type (e.g., \"test\" vs \"not test,\" \"architectural\" vs \"not architectural,\" \"code\" vs. \"not code\"). This method employs an ensemble learning technique, combining multiple models to improve predictive performance by leveraging their strengths to enhance robustness and accuracy. Tan et al. [27] demonstrated the effectiveness of ensemble learning tech-niques in improving predictive performance for Self-Admitted Technical Debt (SATD) detection tasks. They suggested that combining multiple models can enhance robustness and ac-curacy by leveraging the strengths of individual classifiers. Following their recommendations, we adopted an ensemble learning approach in our study for TD classification. Our rule-based ensemble method integrates predictions from Classifier A and Classifier B. An instance is labelled as Architectural Technical Debt only if both classifiers produce positive results, indicating the presence of both architectural and technical debt.\nFor our multiclass classification models, we utilized Strat-ified 5-fold cross-validation over five epochs to address the imbalanced nature of different TD types' occurrences. This method ensures that each fold maintains the same proportion of classes as in the full dataset, providing a representative sam-ple for model training and evaluation [24]. We incorporated class weights in computing the loss function to mitigate the impact of class imbalance in the multiclass classifier model. These weights were assigned to different classes based on their frequency in the dataset, giving more importance to underrepresented classes during training. This approach helps the model learn effectively from all classes, regardless of their prevalence in the dataset [28]. By combining Stratified K-fold cross-validation with weighted loss computation, we aimed to develop robust multiclass models capable of accurately classifying various TD types despite their uneven distribution in real-world software projects.\nExpert binary classifiers focus on identifying specific char-acteristics of their assigned type and enhance robustness"}, {"title": "D. Model Evaluation and Testing with OOD Dataset", "content": "The final stage of our methodology involved rigorous evaluation and testing of the trained models. We assessed model performance using standard metrics such as accuracy, precision, recall, Matthews's correlation coefficient (MCC), AUC (Area Under The Curve) ROC (Receiver Operating Characteristics), and F1-score on a reserved test set. Precision measures the percentage of TD issues that have been correctly identified (true positives) out of all predictions. The Recall metric, which calculates the proportion of correctly identified TD issues (true positives) out of all actual TD issues (true positives + false negatives), holds particular importance in our study. Firstly, many software repositories contain only partially tagged TD issues. This means that the negative class in our dataset might inadvertently include some positive instances that were not explicitly labelled as TD. As a result, the precision metric, which relies on the accuracy of both positive and negative labels, cannot be considered entirely reliable in this scenario.In contrast, the Recall metric focuses solely on the correct identification of known TD issues, making it a more dependable measure of our model's performance. It assesses how effectively our approach captures all instances of TD, which is critical for comprehensive TD management.\nThe F1-score combines both precision and recall through a harmonic mean. The AUC is a performance measurement for classification problems at various threshold settings. It is the area under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate (TPR) against the false positive rate (FPR) at different threshold levels. The AUC value ranges from 0 to 1. An AUC of 0.5 suggests no discrimination (i.e., random guessing), while an AUC of 1 indicates perfect discrimination.\nWe also emphasize Matthews's Correlation Coefficient (MCC), which is widely recognized as a more statistically reliable metric [29] for binary classification.\n$\\MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP) \\cdot (TP + FN) \\cdot (TN + FP) \\cdot (TN + FN)}}$"}, {"content": "MCC produces a more informative and truthful score in evaluating classifications than accuracy and F1 score. It pro-duces a high score only if the prediction obtained good results in all four confusion matrix categories (true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP)), proportionally both to the size of positive and negative elements in the dataset. MCC values range between 1 to 1, and a value greater than 0 indicates that the classifier is better than a random flip of a fair coin, whilst 0 means it is no better.\nIn addition, all models were also tested against an out-of-distribution (OOD) dataset, which consisted of data from projects not included in the training dataset. This step is crucial"}, {"title": "A. RQ1: How effective are transformer-based models in classifying TD issues?", "content": "This research question aims to explore the effectiveness of the transformer-based model, particularly DistilRoberta in our case, in classifying TD issues, leveraging their ability to capture complex patterns and contextual information in text data. Understanding the effectiveness of these models can guide practitioners in selecting appropriate tools for automated TD management. This investigation will provide empirical evidence on the performance of transformer models in this specific domain, potentially leading to improved accuracy and efficiency in issues classification."}, {"title": "B. RQ1.1:Does fine-tuning TD models on project-specific data improve their performance?", "content": "Fine-tuning models on domain-specific or project-specific datasets is a common practice to enhance performance. This research question seeks to determine whether fine-tuning im-proves the performance of our TD (DistillRoberta ) model. The motivation is to assess the gains in classification accuracy and robustness when models are fine-tuned on project-specific or domain-specific TD data compared to their performance in our TD binary classifier. The results will inform how fine-tuning can be beneficial, providing actionable insights for researchers and practitioners looking to optimise their TD classification.\nFor this study, we extract TD issues from the Jira Public dataset [32], which is an out-of-distribution (OOD) dataset, as the model was initially trained on GitHub issues. We investigated the generalizability of this OOD Jira dataset to see how well the model can adapt to other project management tools before and after finetuning our TD model.\nIn addition, we also analysed the TD model's performance before and after fine-tuning with 30% of VScode-specific TD data issues. The VScode project contains many TD issues and was removed from the initial TD dataset before training, ensuring the model has never encountered VScode-specific instances during training.\nWe extended our investigation to examine the impact of fine-tuning on the model's temporal generalization capabilities. This aspect is crucial for understanding how well our TD classification models can adapt to and predict future TD issues.\nTo achieve this, we designed a temporal split in our dataset. We removed all TD-tagged data from GitHub issues dated 2024 (GH 2024) and used this as our test set. The model was then trained exclusively on historical TD and non-TD issues from before 2024. This approach simulates a real-world scenario where models are trained on past data and must predict future occurrences of TD.\nBy testing the model on the GH 2024 dataset, which it had never encountered during training, we aimed to evaluate its ability to generalize to future temporal data. This experiment allows us to assess how effectively our fine-tuning methods enable the model to capture underlying patterns of TD that persist over time, rather than merely memorizing specific instances or timebound characteristics.\nFor this RQ, we have assessed improvements in clas-sification accuracy, precision, recall, F1-score, MCC, and AUC ROC. The results, summarised in Table III, indicate significant performance gains through fine-tuning our TD (DistilROBERTa) model.\nOur analysis revealed that fine-tuned models using 30% of project-specific data consistently outperformed their non-fine-tuned counterparts across all datasets. This improvement was evident in various key performance metrics.\nFor instance, when evaluating on the 'GH 2024' dataset split, which represents future data the model had not seen during training, the fine-tuned model demonstrated substan-tial improvements. Precision increased from 0.761 to 0.840, indicating a significant reduction in false positives. Recall also improved from 0.873 to 0.886, suggesting better iden-tification of true TD issues. Overall accuracy saw a marked increase from 0.800 to 0.855, reflecting enhanced overall classification performance.Similar performance enhancements were observed across other datasets, including 'JIRA' [32] and 'VSCode' [31].\nSkryseth et al. conducted an empirical evaluation to de-termine the optimal percentage of project-specific data for fine-tuning TD classification models. Their study examined a range of percentages from 15% to 50% and found that 30% of project-specific data consistently yielded good results across various scenarios [6]. Following their findings, we chose to use 30% of project-specific data for fine-tuning our models. This percentage strikes a balance between leveraging project-specific information and maintaining model generalizability. It provides sufficient data to capture project-specific TD patterns without overfitting to individual project idiosyncrasies [6]. However, it's important to note that practitioners can adjust this percentage based on their specific circumstances and data availability. The 30% guideline serves as a well-supported starting point, but the optimal percentage may vary depending on factors such as project size, TD prevalence, and the diver-sity of TD types within a project. These findings underscore the effectiveness of fine-tuning in enhancing model robustness and accuracy, providing valuable insights for researchers and practitioners aiming to optimise TD classification frameworks."}, {"title": "C. RQ2: How does the performance of LLM-like the GPT model, compare to TD DistilRoberta (Our) models in TD classification?", "content": "Large Language Models (LLMs) like GPT and smaller, efficient models like DistilRoberta represent different trade-offs in NLP tasks. This research question aims to compare the performance of these two models in the context of TD classification. The motivation is to understand the trade-offs between the computational efficiency of DistilRoberta and the expansive capabilities of GPT. The comparison will help identify the optimal model for practical applications, balancing accuracy, computational cost, and resource requirements. The result will clearly explain which model is more suitable for TD classification tasks.\nOur focus is on task-specific fine-tuning, examining whether the extensive pre-training of larger GPT models yields su-perior performance compared to the smaller, more efficient DistilROBERTa [9], [18] when both are fine-tuned for TD classification.\nWe consider the trade-offs between computational resources and model efficiency. GPT models, while powerful, are closed-source and costly for fine-tuning and inference. In contrast, DistilROBERTa (87 million parameters) can be fine-tuned on smaller GPUs or free resources like Google Colab, and converted to ONNX format for CPU inference [33].\nThis investigation aims to guide practitioners in model selection and resource allocation for TD classification tasks, balancing performance gains against computational costs and accessibility."}, {"title": "D. RQ2.1: Is task specific fine-tuning of LLM like GPT for TD more effective than finetuned DistillRoberta TD classification?", "content": "In this RQ, we compares the performance of task-specific fine-tuned GPT models (GPT-3.5 Turbo and GPT-DaVinci-002) against our task-specific fine-tuned TD DistilRoBERTa model for Technical Debt classification on the same Train and test split. We distinguish between two types of fine-tuning:\n1) Task-specific fine-tuning: Adapting pre-trained models to the TD classification task.\n2) Project-specific fine-tuning: Further adapting models to specific project data."}, {"title": "E. RQ3: How effective are expert ensemble of binary classifiers in classifying different types of issues compared to multi-class model?", "content": "1) Evaluation binary Classifier : This research question investigates the effectiveness of classification models in distin-"}, {"title": "1) A multiclass classification model that attempts to cate-gorize issues into all 13 types simultaneously.", "content": "A separate expert binary TD model."}, {"title": "F. RQ 3.1: How does the performance of LLM-like the GPT model compare to the DistillRoberta (ours) Model on different issue types ?", "content": "Table VIII compares performance metrics between our fine-tuned DistilRoberta model and GPT-40 (\"o\" for \"omni\") across various issues categories.\nWe compare the performance of our fine-tuned Distil-Roberta model with GPT-4o across various issue categories commonly associated with software development and potential Technical Debt types. Both models were evaluated on a 2024 dataset to ensure fair comparison and avoid data leakage, as neither model was trained on this data. Our DistilRoberta model demonstrated superior performance in most technical categories. For example, in 'Defect' detection, it achieved an F1-Score of 0.923, significantly outperforming GPT-4's 0.760.\nThis trend was consistent across several technical categories, indicating our model's strength in areas requiring high tech-nical accuracy and automation. However, GPT-4 excelled in the 'People' category, with an F1-Score of 0.971 compared to our model's 0.944. These results highlight the effectiveness of our fine-tuned DistilRoberta model in classifying all TD issue types."}, {"title": "V. EVALUATION", "content": "In our evaluation study, we focused on two main classifica-tion tasks:\n1) Categorizing software issues into 13 types commonly associated with TD, such as architecture, build, and test.\n2) Classifying and identifying whether an issue represents TD or Not using an expert TD Binary model.\nWe created a ground truth dataset consisting of issues tagged with both a specific category label and a TD label. For example, an issue might be labelled as both \"Architecture\" and \"Technical Debt.\" To ensure a fair evaluation, we excluded these doubly-tagged issues from our training data and used them as a separate test set. We then evaluated three types of models:\n1) A multi-class model that categorizes issues into all 13 types simultaneously.\n2) 13 separate binary models, each specialized in identify-ing one specific issue type.\n3) A single expert binary model focused solely on identi-fying TD.\nThis approach allowed us to compare the effectiveness of different classification strategies and assess how well each model type performs in identifying specific issue categories and TD.\nWe used the recall metric to evaluate model performance. Recall was chosen because our ground truth dataset only contains positive examples (issues tagged with both a specific TD category and TD). We don't have negative examples of doubly-tagged issues in our ground truth. Recall measures how many of these known positive instances were correctly identified by each model. For instance, if we have 100 issues tagged as both \"Architecture\" and \"TD,\" and our model correctly identifies 80 of them, the recall would be 0.8 or 80%. The support metric indicates the total number of these doubly-tagged issues for each category, providing context for the recall scores. This approach allowed us to assess how well our models could identify specific issue categories and TD, using a carefully curated set of real-world, developer-labeled issues as our benchmark."}, {"title": "A. Evaluation of Industry projects", "content": "We evaluated the DistillRoBERTa TD binary classification model on a dataset from Visma, a multinational software company comprising over 180 entrepreneurial software com-panies. Visma's unique structure, with high autonomy among its companies and diverse team practices, makes it an ideal candidate for testing the model's effectiveness in identifying and classifying TD across varied project contexts. This dataset from Visma consisted of approximately 1100 Jira issues from 6 different projects across 3 countries, with about 400 issues labelled as TD. This diversity in data sources ensures a robust test of the model's generalization capabilities.\nThe results show significant improvements across all metrics after fine-tuning. Notably, accuracy increased from 56.8% to 73.7%, and precision from 45.1% to 60.8%. The recall, crucial for identifying potential TD issues, was already high at 79.1% and improved slightly to 80.1%. In this context, recall is particularly important. Given the inconsistent TD tagging practices across different teams and projects, the model's ability to identify potential TD issues, even those not explicitly tagged, is vital. A high recall ensures that most TD issues are captured, allowing teams to address them proactively. The improvements in F1-Score (57.4% to 69.1%), MCC (66.4% to 70.7%), and AUC (61.5% to 75.0%) further demonstrate the model's enhanced effectiveness after fine-tuning. These results are promising for Visma's diverse software ecosystem. The model's improved performance in identifying TD issues, even in previously untagged instances, can significantly aid in standardizing TD management practices across Visma's various companies. This approach enhances software main-tenance practices and contributes to more sustainable and consistent software development processes within Visma's complex corporate environment."}, {"title": "VI. DISCUSSION AND RELATED WORKS", "content": "In TD classification, recall plays a crucial role. High recall ensures that most TD instances are identified, minimizing the risk of overlooking critical issues. Our models achieved a balance between precision and recall, with a slight emphasis on recall. This approach is particularly valuable in large-scale projects where manual review of all potential TD is impractical. For example, in a large enterprise software project with thousands of issues, our model might identify 90 to 80% of actual TD instances (high recall) while maintaining a manageable number of false positives. This allows project managers to focus on addressing the most critical TD issues without being overwhelmed by false alarms.\nOur models enable reliable tagging of large numbers of issues, facilitating comprehensive studies on TD patterns and evolution. For instance, researchers could use our models to analyze TD trends across thousands of open-source projects on GitHub, providing insights into common TD types and their prevalence across different pro-gramming languages or project domains [34].\n Our models to generalize to future data opens possibilities for studying TD evolution over time. Researchers could track how TD accumulates in long-running projects, potentially developing predictive models for TD growth based on project character-istics and development practices.\nOur approach's effectiveness across different projects enables comparative studies of TD across diverse software ecosystems. For example, researchers could compare TD patterns in mobile app development versus enterprise software projects, identifying domain-specific TD challenges.\nOur research offers several practical benefits for the software industry:\nRetroactive TD Tagging: Our approach allows for retroactively tagging TD in existing projects. This is valuable for scenarios such as [35]:\nPostmortem analysis of completed projects to identify areas for improvement in future developments."}, {"title": "Efficient TD Management", "content": " The deployment of our mod-els can streamline TD detection and management in large projects. For instance, a development team could integrate our model into their CI/CD pipeline, automatically flagging potential TD issues during code reviews and prioritizing them based on severity [6].\nCustomizable TD Detection: The ability to fine-tune models on project-specific data allows companies to tailor TD detection to their contexts. A financial services company, for example, could fine-tune the model to be particularly sensitive to security-related TD, reflecting their industry's specific concerns."}, {"title": "D. Integration with Software Economics and Project Management", "content": "Our TD classification approach has broader implications:\n Future research could develop meth-ods to quantify the economic impact of identified TD. For example, by estimating the cost of addressing TD versus the long-term savings from reduced maintenance efforts, or-ganizations could make more informed decisions about TD remediation priorities [36", "37": [38]}]}