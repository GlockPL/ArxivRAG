{"title": "STRUCTURE OF ARTIFICIAL NEURAL NETWORKS: Empirical Investigations", "authors": ["Julian J. Stier"], "abstract": "Within one decade, Deep Learning overtook the dominating solution methods of countless problems of artificial intelligence. \u201cDeep\" refers to the deep architectures with operations in manifolds of which there are no immediate observations. For these deep architectures some kind of structure is pre-defined \u2013 but what is this structure? With a formal definition for structures of neural networks, neural architecture search problems and solution methods can be formulated under a common framework. Both practical and theoretical questions arise from closing the gap between applied neural architecture search and learning theory. Does structure make a difference or can it be chosen arbitrarily? This work is concerned with deep structures of artificial neural networks and examines automatic construction methods under empirical principles to shed light on to the so called \u201cblack-box models\". Our contributions include a formulation of graph-induced neural networks that is used to pose optimisation problems for neural architecture. We analyse structural properties for different neural network objectives such as correctness, robustness or energy consumption and discuss how structure affects them. Selected automation methods for neural architecture optimisation problems are discussed and empirically analysed. With the insights gained from formalising graph-induced neural networks, analysing structural properties and comparing the applicability of neural architecture search methods qualitatively and quantitatively we advance these methods in two ways. First, new predictive models are presented for replacing computationally expensive evaluation schemes, and second, new generative models for informed sampling during neural architecture search are analysed and discussed.", "sections": [{"title": "1 INTRODUCTION", "content": "The functional form of deep neural networks (DNNs) is usually introduced through successive applications of \\(\u03c3(Wx + B)\\) with layer-wise weights \\(W\\) and biases \\(B\\), an input feature vector \\(x\\) and a non-linearity \\(\u03c3\\). On the other hand, neural architecture search (NAS) as a field concerned with the automated discovery of neural architectures, considers significantly more complicated forms. For example, architectures in NAS are often treated as labelled directed acyclic graphs (DAGs). But what is an architecture or a DAG in the context of the above form? The two fields open up a huge gap between how deep neural networks are properly defined on the theoretical side and what is actually meant by the architecture of a DNN in NAS on the applied side. The research goals and questions of this work are located in this field of tension and while our work is not closing this wide gap, it proposes ideas, formalisations and empirical observations that could contribute to a better understanding and future development of deep learning theories and NAS-methods."}, {"title": "1.1 THE GOAL OF THIS THESIS", "content": "This work is concerned with the relationship between deep neural networks and their structure. For this purpose, the work delves into the formal definition of deep neural networks and into how structure in the form of graphs can be made explicit. Making this formal underlying more explicit frames the term of an architecture. This formulation then serves to understand both the properties of different neural architecture search methods and the analysis of deep neural network structures. Experiments with pruning, growing, evolutionary or genetic searches are then repeatedly brought back to this formal underlyings as to sharpen the interpretation and arguments on their advantages and disadvantages."}, {"title": "From Neurobiology To Machine Learning", "content": "Neurobiology was and is an inspiring source for the development of machine and especially deep learning. The human brain consists of billions of neurons and trillions of connecting axons or dendrites and resembles a complex structure. Questions around the meaning of this structure and its properties motivated our research goals and questions in deep learning."}, {"title": "3.1 Graphs", "content": "The graph-theoretic formalisations follow Diestel [52] in general and Bang-Jensen & Gutin [14] for further details on directed graphs (digraphs). A graph G is a tuple (V, E) consisting of a set of vertices V and a set of edges E \u2286 V \u00d7 V. We overload the symbol V as to obtain the set V(G1) given a specific graph G1 and do so as well with E(G1) for the edge set. Equivalently to E, an irreflexive binary relation over V given by Adj : V \u2192 P(V) can specify whether a vertex is adjacent to any other vertex in the graph: for a vertex v \u2208 V the map Adj(v) gives the set of all vertices adjacent to it. It holds that (s, t) \u2208 E \u21d4 t \u2208 Adj(s). For two graphs G1, G2 we also write VG1, EG1 and AdjG1 for the first and VG2, EG2 and AdjG2 for the second graph.\nThe number of vertices of G is its order denoted as |G| with |G| = |V| and its number of edges is denoted as ||G||, called its size with ||G|| = |E|. Two graphs G and G' are isomorphic if there exists an isomorphism \u03c6 : V(G) \u2192 V(G') that preserves edges, is bijective, and its inverse also has the homomorphic property of preserving edges [52, Chp. 1.1]. We write G = G' to explicitly emphasise on the isomorphism but usually also write G = G' for simplicity. Graph invariants are maps assigning equal values to isomorphic graphs. We are later especially interested in graph invariants of classes of graphs representing neural network architectures."}, {"title": "15 Overarching Discussions & Conclusions", "content": "We formally introduced graph-induced neural networks as to pose our research questions on the structure of deep neural networks (DNN) in a unified language. In this language, neural network realisations f\u2208 A(G) from universal architectures A(G) are induced over structural themes T such that G = [T] or induced probabilistically with a distribution of graphs P(G) through G ~ P(G). The search space of all considered architectures is referred to as S. Our research is biologically motivated and originally led to random graph models as search space design [272]. But we also investigate on alternative design spaces such as in conjunction with NAS benchmarks or resulting from our formal considerations on CTs. With graph-induced neural networks on various design spaces, we conduct structure analyses (part iv) of T or G on the one hand and neural architecture search (part v & part vi) on the other hand.\nStructure analyses comprise questions on whether structural themes T1 and T2 with significantly different structural properties exhibit differences such as in classification accuracy. Observing a difference such as a(f1) > a(f2) for f1 \u2208 A([T1]) and f2 \u2208 A([T2]), can this be properly explained by T1 and T2? We found, that it makes a difference but that the difference is currently hardly explained by structural properties except for some indications of variances of their degree or path length distributions.\nNeural architecture search comprises questions on how T\u2208 S or f \u2208 A([T]) with desirable properties can be found effectively and efficiently with varying methods, how such NAS methods differ and how they can be improved. We found, that there exist generic methods such as evolutionary searches which work out of the box but also very specialised methods which impose topological properties on S such as DARTS. NAS methods mostly differ in their required properties of S and how they exploit these properties. Advancements are accomplished by introducing techniques of predictive estimation strategies or generative models for the search space S."}]}