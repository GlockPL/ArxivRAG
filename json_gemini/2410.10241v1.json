{"title": "REVISITING AND BENCHMARKING GRAPH AUTOENCODERS: A CONTRASTIVE LEARNING PERSPECTIVE", "authors": ["Jintang Li", "Ruofan Wu", "Yuchang Zhu", "Huizhe Zhang", "Xinzhou Jin", "Guibin Zhang", "Zulun Zhu", "Zibin Zheng", "Liang Chen"], "abstract": "Graph autoencoders (GAEs) are self-supervised learning models that can learn meaningful representations of graph-structured data by reconstructing the input graph from a low-dimensional latent space. Over the past few years, GAEs have gained significant attention in academia and industry. In particular, the recent advent of GAEs with masked autoencoding schemes marks a significant advancement in graph self-supervised learning research. While numerous GAEs have been proposed, the underlying mechanisms of GAEs are not well understood, and a comprehensive benchmark for GAEs is still lacking. In this work, we bridge the gap between GAEs and contrastive learning by establishing conceptual and methodological connections. We revisit the GAEs studied in previous works and demonstrate how contrastive learning principles can be applied to GAEs. Motivated by these insights, we introduce lrGAE (left-right GAE), a general and powerful GAE framework that leverages contrastive learning principles to learn meaningful representations. Our proposed IrGAE not only facilitates a deeper understanding of GAEs but also sets a new benchmark for GAEs across diverse graph-based learning tasks. The source code for 1rGAE, including the baselines and all the code for reproducing the results, is publicly available at https://github.com/EdisonLeeeee/lrGAE.", "sections": [{"title": "1 INTRODUCTION", "content": "In the last years, self-supervised learning (SSL) has emerged as a powerful learning paradigm for learning graph representations, approaching, and sometimes even surpassing, the performance of supervised counterparts on many downstream tasks Hjelm et al. (2019); van den Oord et al. (2018). Compared with supervised learning, self-supervised learning gets equal or even better performance with limited or no-labeled data which saves much annotation time and plenty of resources. In a nutshell, SSL purely makes use of rich unlabeled data via well-designed pretext tasks that exploit the underlying structure and patterns in the data. Most recent approaches are shaped by the design of pretext tasks and architectural design, which has led to two lines of research: contrastive and non-contrastive learning Garrido et al. (2023); Balestriero & LeCun (2022).\nAs one of the most successful and widespread SSL strategies, contrastive learning has first shown promising performance in vision representation learning Chen et al. (2020); Gao et al. (2021). It brings together embeddings of different views of the same image while pushing away the embeddings from different ones. Contrastive learning develops rapidly and has recently been applied to the graph learning domain because of the scarcity of graph datasets with labels. Contrastive learning on graphs (i.e., GCL) You et al. (2020) follows a similar framework to its counterparts in the vision domain, with the objective of maximizing the agreement between different graph augmentation views Wu et al. (2021); Li et al. (2024). Basically, contrastive views are designed as nodes, subgraphs, or mixtures of"}, {"title": "2 RELATED WORK", "content": "To situate GAEs in a broader context, we discuss recent advances in graph self-supervised learning, including graph contrastive learning, graph autoencoders, and their recent masked variants.\nGraph contrastive learning. Graph contrastive learning (GCL) is a general self-supervised learning paradigm excelling at capturing invariant information from diverse graph augmentation views. GCL has taken over the mainstream of self-supervised learning on graphs for years. Many works in this direction have recently flourished, with promising examples including DGI Velickovic et al. (2019), MVGRL Hassani & Ahmadi (2020), and GRACE Zhu et al. (2020). While most GCLs potentially suffer from scalability issues due to complex augmentation and sampling strategies, several efforts have been made to scale up GCL through augmentation-free paradigms (e.g., AFGRL Lee et al. (2022)), architecture simplification (e.g., BGRL Thakoor et al. (2021) and SGCL Sun et al. (2024)) or in-batch feature decorrelation (e.g., CCA-SSG Zhang et al. (2021)).\nGraph autoencoders. Graph autoencoders (GAEs) are one such non-contrastive-based method that aim to learn meaningful representations by leveraging the graph reconstruction as the pretext task, i.e., reconstruct certain inputs within a given context. The pioneering works on GAEs can be traced back to GAE and VGAE Kipf & Welling (2016a), which utilize GNNs as the encoder and employ dot-product for link prediction decoding. Follow-up GAEs Li et al. (2023c); Pan et al. (2018); Wang et al. (2017); Hasanzadeh et al. (2019) mostly share a similar architecture, employing structure reconstruction or integrating both structure and feature reconstruction as their objectives. While GAEs typically excel in link-level tasks, they have been criticized for over-emphasizing proximity information at the expense of structural information and naturally underperform on node- and graph-"}, {"title": "3 GRAPH AUTOENCODERS: GENERATIVE YET CONTRASTIVE", "content": "In this section, we begin by revisiting graph contrastive learning (GCL) and graph autoencoders (GAEs). Then, we present our viewpoint on establishing the connections between GCLs and GAEs. Finally, we leave several remarks regarding the limitations of current GAEs.\nGraph contrastive learning. Let G = (V, E) be an attributed graph with V = {v} and E \u2286 V \u00d7 V the set of nodes and edges, respectively. N is the number of nodes in the graph. GCLs commonly involve generating two augmented views, denoted as GA and GB, and try to maximize the mutual information or the correspondence between two different views to train the encoder fo. Here, fo represents an encoder network that maps the graph structure and node features simultaneously into a low-dimensional space. In this context, the goal of maximizing mutual information is achieved by minimizing the following objective:\nmin 1/|V| \u03a3\u03bd\u2208V  C(ZA[V], ZB[v]),\nwhere ZA = fo(GA), ZB = fe(GB), and L refers to the contrastive loss, such as InfoNCE van den Oord et al. (2018). Typically, fe is a GNN network with receptive fields of k (e.g., the depth of the network). We can further simplify Eq. 1 as follows:\nmin 1/|V| \u03a3\u03bd\u2208V  L(GA(k)[v], GB(k)[v])\nwhere GA(k)[v] denotes the receptive fields of node v in graph G, with fo implicitly defined. In this way, we have a compact representation of GCL on its core component, i.e., the contrastive pair (GA(k)[v] and GB(k)[v]. In what follows, we will further generalize the representation of GAEs to adopt the form of this contrastive formulation.\nGraph autoencoders. Technically, GAEs are encoding-decoding architectures that follows the graph-reconstruction principle as self-supervisions. The goal of GAEs is to reconstruct or decode graph components, such as edges or features, from hidden representations. A typical GAE consists of an encoder network fe, similar to GCLs, which learns low-dimensional representations, as well as a decoder network g$ that performs graph reconstruction pretext tasks. Here we first introduce the learning objective of the conventional GAE Kipf & Welling (2016a), which is to reconstruct the graph structure, following the form described in Li et al. (2023b):\nL = - 1/|E+| \u03a3(u,v)\u2208E+  log 94(Z[u], Z[v]) + 1/|E-| \u03a3(u\u2032,v\u2032)\u2208E-  log(1 \u2013 94 (Z[u\u2032], Z[v\u2032]))\nwhere E+ is a set of positive edges and is usually a subset of E, i.e., E+ \u2286 E. Correspondingly, E- is a set of negative edges sampled from the graph and E+ \u2229 E\u00ae = \u00d8. Z = fo(G) in which fe is a GNN network such as GCN Kipf & Welling (2016b) or GAT Veli\u010dkovi\u0107 et al. (2018). go is the"}, {"title": "4 LRGAE: DESIGN SPACE FOR GAES FROM CONTRASTIVE LEARNING PERSPECTIVE", "content": "We have demonstrated that GAEs are such generative yet contrastive models, here we introduce 1rGAE (left-right GAE), the first contrastive-based GAE architecture designed with the general purpose of learning powerful representations. Following the works in GCLs, we decompose the design space of IrGAE from five key dimensions: (1) augmentations, (2) contrastive views, (3) encoder/decoder networks, (4) contrastive loss, and dispensable (5) negative samples.\nAugmentations. Graph augmentation is the first step in GCL, which generates multiple graph views from the input graph without affecting the semantic meaning You et al. (2020). These views are typically created by applying certain transformations, and the goal is to help the model learn robust and generalizable representations. In GCLs, the most prevalent augmentation techniques include node dropping, edge perturbation, and attribute/feature corruption You et al. (2020); Rong et al. (2020); Velickovic et al. (2019). However, as pointed out in \u00a7 3, the contrastive viewpoint of GAEs reveals the deficiency of augmentation mechanisms like information redundancy and collapsed solutions. To mitigate these issues, a recent line of work Li et al. (2023b); Hou et al. (2022) has been using a simple idea of masking to improve learning performance. Specifically, in structure-based GAEs, masking a certain proportion of edges effectively reduces information redundancy Li et al. (2023b). Meanwhile, masking the root node in feature-based GAEs Hou et al. (2022) can prevent trivial solutions (Zhang et al., 2022a, Theorem 3.6). Following the design space of GCL, we mainly consider 1rGAE with node/edge/attribute masking as augmentations.\nEncoder/decoder networks. An encoder maps the input graph into low-dimensional representations and is typically defined as a GNN network. Basically, the receptive fields of the encoder are determined by the depth of the GNN network. Meanwhile, the decoder network is regarded as a task-specific 'adapter', which maps augmented representations to another latent space where the contrastive loss is calculated for different pretext tasks, such as graph reconstruction. In most cases, GAEs employ an asymmetric design, where the decoder network is implemented as an MLP, although a GNN can also be used to enhance decoding and expand the receptive fields. However, using a GNN"}, {"title": "5 EXPERIMENTS", "content": "In this section, we perform extensive experiments over several graph learning tasks to benchmark the performance of GAEs and three variants of 1rGAE. Specifically, the experiments are conducted on seven graph datasets, including Cora, CiteSeer, PubMed Sen et al. (2008), Photo, Computers Shchur et al. (2018), CS and Physics Shchur et al. (2018). The comparison baselines include vanilla GAE Kipf & Welling (2016a) and its variant GAEf, as well as masked GAEs, i.e., MaskGAE Li et al. (2023b), S2GAE Tan et al. (2023), GraphMAE Hou et al. (2022), GraphMAE2 Hou et al. (2023), and AUG-MAE Wang et al. (2024). Since IrGAE \u2461 \u2462 \u2463 \u2464 have their corresponding implementations (see Table 1), we implement IrGAE \u2465\u2466\u2467 in experiments for comparison. Due to space limitation, we kindly refer readers to Appendix for more details of datasets, baseline methods, evaluation settings, and implementation details of IrGAE."}, {"title": "A LRGAE FRAMEWORK AND IMPLEMENTATIONS", "content": "1rGAE is introduced as a versatile and comprehensive framework that offers flexibility in implementing powerful GAEs through customization of augmentations, contrastive views, encoder/decoder networks, and contrastive loss. In particular, the design of contrastive views and the corresponding learning loss play a crucial role in addressing various graph-based learning tasks efficiently and effectively. For example, different tasks may require different types of contrastive views, such as node-level views, link-level views, or even more complex views that incorporate both local and global graph information. Here we provide the PyTorch Paszke et al. (2019) style pseudocode for the implementation of 1rGAE in Algorithm 1.\nAlgorithm 1 PyTorch Paszke et al. (2019) style pseudocode for lrGAE.\nTo further showcase the versatility and broad applicability of 1rGAE, we list seven variants of IrGAE with different contrastive views below:\n\u2022 IrGAE 2-ABllvv: This variant can be simply implemented with the same architecture as na\u00efve GCLs Zhu et al. (2020); You et al. (2020), which contrast two augmentation views (GA and GB) for each node (v) in its l-hop neighborhood (i.e., receptive field).\n\u2022 GAE -AAlrvv: This is a variant of IrGAE \u2461-ABllvv that contrasts the node representations from different layers of the encoder, i.e., the receptive fields (l and r), which shares similar philosophy of local-to-global graph contrastive learning Velickovic et al. (2019). The most typical example is GAE Kipf & Welling (2016a) with feature reconstruction as its learning objective (i.e., GAEf).\n\u2022 IrGAE 4-ABlrvv: This is the core idea of GraphMAE Hou et al. (2022). By combining the success of GCLs and GAEs, we are able to develop this variant that incorporates different graph views (GA and GB) and receptive fields (l and r) of a node v.\n\u2022 IrGAE 5-AAllvu: This is the learning paradigm of vanilla GAE and MaskGAE Li et al. (2023b), which contrasts the two subgraph pairs of nodes associated with an edge (v, u). Note that the graph view GA can be the original graph G or the augmented/masked graph, leading to the implementations of GAE or MaskGAE.\n\u2022 IrGAE 6-AAlrvu, lrGAE \u2466-ABllvu, and IrGAE 8-ABlrvu: The variants that do not have the exact implementation yet. Following the architecture of 1rGAE \u2464-AAllvu, we can vary the receptive fields (l and r), augmentation views (GA and GB), or even both to perform graph contrastive learning.learning.\nAs shown in Table 1, GAE \u2461 \u2462\u2463\u2464 have implementations proposed in previous works Zhu et al. (2020); You et al. (2020); Kipf & Welling (2016a); Hou et al. (2022); Li et al. (2023b), we mainly provide the empirical results of IrGAE \u2465 \u2466 \u2467 in our experiments. The above variants cover the possible implementations of GAEs, further demonstrating the flexibility and adaptability of IrGAE."}, {"title": "B DISCUSSIONS", "content": "Time and space complexity. We briefly discuss the time and space complexity of our proposed 1rGAE framework. 1rGAE is a standard GAE framework consisting of one encoder and one decoder network. As decoder networks are typically simple feed-forward networks (e.g., MLPs) involved with dense matrix computation, the major bottleneck arises from the message passing and aggregation of encoder networks (e.g., GCN Kipf & Welling (2016b)). In particular, for an L layer encoder network, the time complexity is related to the graph size and the dimension of features and hidden representations, about O(L|E|d + L|V|d\u00b2). By incorporating mini-batch training Hamilton et al. (2017), the time complexity for each sampled subgraph can be reduced to O(rL|V|d\u00b2), where r represents the neighborhood size shared by each hop. As for the space complexity, the major bottleneck also lies in the graph encoder network, which is O(L|V|d + Ld\u00b2) and O(BrLd+ Ld\u00b2) for full-batch and mini-batch training, respectively. Here, B denotes the batch size. Overall, the time and space complexity of lrGAE framework are guaranteed and can easily scale to larger graphs, showcasing its scalability and generality.\nLimitations and outlook. This work primarily presents initial benchmarks and baselines for GAEs, with a main focus on masked autoencoding based methods. Our work might potentially suffer"}, {"title": "CREPRODUCIBILITY", "content": "All of IrGAE 's experimental results are highly reproducible. We provide more detailed information on the following aspects to ensure the reproducibility of the experiments.\nDatasets. We conduct experiments on seven graph benchmark datasets, including three citation networks, i.e., Cora, CiteSeer, and PubMed Sen et al. (2008), two Amazon co-purchase graphs, i.e., Photo and Computer Shchur et al. (2018), two co-author graphs, i.e., CS and Physics Shchur et al. (2018). The above datasets are used for the tasks of node classification, link prediction, and graph clustering. For the graph classification task, we perform experiments on the following seven datasets: IMDB-B, IMDB-M, PROTEINS, COLLAB, MUTAG, REDDIT-B, and NCI1 Yanardag & Vishwanathan (2015). Each dataset consists of a set of graphs, with each graph associated with a corresponding label. We also incorporate three heterogeneous graph datasets, i.e., DBLP, ACM, and IMDB Lv et al. (2021); Li et al. (2023a), for experiments on heterogeneous node classification to showcase the generality and versatility of our 1rGAE framework. All datasets used throughout experiments are publicly available at PyTorch Geometric Fey & Lenssen (2019). Detailed information about datasets are summarized in Table 6, Table 7 and Table 8.\nBaselines. In line with the focus of this work, we benchmark several GAEs in different graph learning tasks, including vanilla GAE Kipf & Welling (2016a) and its variant GAEf, as well as masked GAEs, i.e., MaskGAE Li et al. (2023b), S2GAE Tan et al. (2023), GraphMAE Hou et al. (2022), GraphMAE2 Hou et al. (2023), and AUG-MAE Wang et al. (2024). Among the baselines, GAE, MaskGAE, and S2GAE adopt structure reconstruction as their learning objective, while GraphMAE, GraphMAE2, and AUG-MAE adopt feature reconstruction as their learning objective. We implement baselines with PyTorch Paszke et al. (2019) and PyTorch Geometric Fey & Lenssen (2019), which are open-source software released under BSD-style and MIT license, respectively. For feature-based GAEs (GAEf, GraphMAE, GraphMAE2, AUG-MAE, GiGaMAE), we employ the GAT Veli\u010dkovi\u0107 et al. (2018) architecture and scaled cosine error (SCE) as the encoder network and learning objective. On the other hand, for structure-based GAES (GAE, MaskGAE, S2GAE), we utilize the GCN Kipf & Welling (2016b) architecture and binary cross-entropy as the encoder network and learning objective, respectively.\nImplementation details. To align with the baseline implementations, we abstract the contrastive learning principles of GAEs and implement 1rGAE with PyTorch and PyTorch Geometric as well. Specifically, we have seven basic variants of lrGAE with different contrastive views. Specifically, we refer 1rGAE \u2461\u2462\u2463 as feature-based variants while IrGAE \u2464\u2465\u2466\u2467 as structure-based ones. Since IrGAE \u2461\u2462\u2463\u2464 have implementations proposed in previous works Zhu et al. (2020); You et al. (2020); Kipf & Welling (2016a); Hou et al. (2022); Li et al. (2023b), we mainly provide the empirical results of IrGAE \u2465\u2466\u2467 in our experiments. The hyperparameters of all the baselines were configured according to the experimental settings officially reported by the authors and were then carefully tuned in our experiments to achieve their best results across all tasks. We also tune the hyperparameters of 1rGAE variants for a fair comparison.\nEvaluation. To provide a comprehensive benchmark, we conduct experiments on five graph learning tasks from node, link, subgraph, and graph levels, across homogeneous and heterogeneous graphs, i.e., node classification, link prediction, graph clustering, graph classification, and heterogeneous node classification.\n\u2022 Node classification (N). Node classification is the most popular graph learning task, with the goal of assigning a class label to each node. In the graph self-supervised learning setting Li et al. (2023b), the GNN encoder is pretrained based on the pretext tasks to obtain the node embeddings. The final evaluation is done by fitting a linear classifier (i.e., a logistic regression model) on top of the frozen learned embeddings. We adopt the public splits for Cora, CiteSeer, and PubMed, and 8:1:1 training/validation/test splits for the remaining datasets. Classification accuracy is employed as the evaluation metric.\n\u2022 Link prediction (L). For link prediction, the goal is to predict the existence of edges between pairs of nodes. For structure-based GAEs, we directly use the output of the structure decoder"}, {"title": "E ABLATION STUDIES", "content": "In this section, we perform ablation studies on the key components of lrGAE, i.e., augmentation, encoders, contrastive loss, and negative sampling strategies. We opt for IrGAE \u2465\u2466\u2467, three advanced GAEs that incorporate different contrastive schemes as the comparison methods. We conduct experiments on node classification tasks using the Cora dataset. The experimental results were averaged over 10 runs.\nAugmentations. We first conduct ablation studies on the augmentation techniques, which include edge masking Rong et al. (2020), path masking Li et al. (2023b), and node masking You et al. (2020). Figure 3(a) presents the ablation results of 1rGAME 6 7 8 using various masking strategies. As observed, the performance of 1rGAE \u2465\u2466\u2467 varies significantly with different masking strategies. Basically, edge masking and path masking are better choices than node masking. The observation meets our intuition. This highlights the effectiveness of edge and path masking techniques in enhancing model performance, likely due to their ability to capture more nuanced relationships within the data compared to node masking.\nEncoder networks. Given that most of the decoder networks in GAEs are MLPs, we will focus the ablation experiments only on the encoder networks. The encoder plays a crucial role in mapping graphs into low-dimensional representations. To investigate the potential of designing effective GAEs, we perform ablation studies on Cora using three different GNN encoders, including GCN Kipf & Welling (2016b), GraphSAGE Hamilton et al. (2017), and GAT Veli\u010dkovi\u0107 et al. (2018). The results, shown in Figure 3(b), demonstrate that GCN is the most effective encoder architecture across all three 1rGAE variants. IrGAE \u2465\u2466\u2467 with GCN as the encoder consistently outperforms GAT and GraphSAGE in all cases by large margins. This observation is consistent with the conclusions of"}]}