{"title": "SONICSIM: A CUSTOMIZABLE SIMULATION PLATFORM FOR SPEECH PROCESSING IN MOVING SOUND SOURCE SCENARIOS", "authors": ["Kai Li", "Wendi Sang", "Chang Zeng", "Runxuan Yang", "Guo Chen", "Xiaolin Hu"], "abstract": "The systematic evaluation of speech separation and enhancement models under moving sound source conditions typically requires extensive data comprising diverse scenarios. However, real-world datasets often contain insufficient data to meet models' training and evaluation requirements. Although synthetic datasets offer a larger volume of data, their acoustic simulations lack realism. Consequently, neither real-world nor synthetic datasets effectively fulfill practical needs. To address these issues, we introduce SonicSim, a synthetic toolkit designed to generate highly customizable data for moving sound sources. SonicSim is developed based on the embodied AI simulation platform, Habitat-sim, supporting multi-level adjustments, including scene-level, microphone-level, and source-level, thereby generating more diverse synthetic data. Leveraging SonicSim, we constructed a moving sound source benchmark dataset, SonicSet, using LibriSpeech, Freesound Dataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the Matterport3D to evaluate speech separation and enhancement models. Additionally, to investigate the differences between synthetic and real-world data, we randomly selected 5 hours of raw data without reverberation from the SonicSet validation set to record a real-world speech separation dataset, which set a reference for comparing SonicSet and other synthetic datasets. Similarly, we utilized the real-world speech enhancement dataset RealMAN to validate the acoustic gap between the SonicSet dataset and other synthetic datasets for speech enhancement. The results indicate that models trained on SonicSet generalized better to real-world scenarios compared with other synthetic datasets. Demo and code are publicly available at https://cslikai.cn/SonicSim/.", "sections": [{"title": "1 INTRODUCTION", "content": "Speech separation (Cherry, 1953) and enhancement (Loizou, 2007) are classic tasks in speech signal processing, with the goal of extracting the target speech signal from noisy audio. In recent years, the release of large-scale synthetic datasets, such as LibriMix (Cosentino et al., 2020) and the DNS Challenge (Reddy et al., 2020), has contributed to the improvement of speech separation and enhancement models. Many methods Wang et al. (2023); Subakan et al. (2021); Chen et al. (2020a); Luo & Yu (2023b); Li et al. (2022e); Li & Luo (2023) have demonstrated the ability to transfer across different test environments in zero-shot settings. Applications like online meetings (Rao et al., 2021) and human-computer interaction (Yu & Chen, 2017) have greatly benefited from advancements in these state-of-the-art (SOTA) speech separation and enhancement methods. However, despite the"}, {"title": "2 RELATED WORKS", "content": "In this section, we systematically compare the SonicSim toolkit and the generated SonicSet dataset with existing real-world datasets, synthetic datasets, and simulation toolkits (see Table 1).\nReal-world datasets: Currently, there are very few speech separation and enhancement datasets recorded in real-world environments (Yang et al., 2024; Watanabe et al., 2020). This is primarily because most speech datasets are designed for speech recognition evaluation and only provide transcription annotations, lacking the ground-truth labels needed for supervised learning tasks such as speech separation and enhancement. Recently, the RealMAN dataset introduced real-world microphone array recordings of speech and noise (Yang et al., 2024). Although the gap between the RealMAN dataset and real-world application scenarios is relatively small, its high annotation cost and limited scalability hinder model adaptation to diverse scenarios. Our proposed SonicSim addresses these limitations by offering a highly customizable and realistic synthetic data generation method, filling the gap in scale and scene diversity present in existing datasets.\nSynthetic datasets: Synthetic datasets (Reddy et al., 2020; Cosentino et al., 2020; Maciejewski et al., 2020; Chen et al., 2020b) provide an alternative solution for speech separation and enhancement tasks, typically generating synthetic data by convolving RIR with multiple source signals or noise signals to ensure realism. These simulated RIRs are often generated using the image source methods Allen & Berkley (1979). Datasets such as WHAMR! Maciejewski et al. (2020) introduce synthetic reverberation and noise environments, increasing the complexity of the data. However, synthetic datasets are often created under tightly controlled simulation conditions, which reduces the authenticity of the data and cannot reflect the performance of speech separation and enhancement"}, {"title": "3 MOVING SOUND SOURCE SUITE", "content": "The moving sound source suite consists of two core components: the customizable data generator SonicSim and the moving sound source dataset SonicSet. SonicSim leverages existing datasets to create data tailored for speech separation and enhancement."}, {"title": "3.1 CUSTOMIZABLE DATA GENERATOR: SONICSIM", "content": "The customizable dataset generator, SonicSim, is specifically designed to generate datasets for speech tasks. Built on Habitat-sim (Savva et al., 2019), SonicSim leverages its highly realistic audio renderer (demonstrated in (Chen et al., 2022a)) and high-performance 3D simulator to produce high-quality audio data adapted to various acoustic environments. SonicSim provides a rich set of annotations, including source and microphone position maps, clean audio, and audio with reverberation and noise, without incurring additional data collection costs. More importantly, SonicSim provides users with extensive control over the dataset generation process, allowing customization of scene layouts, scene materials, source and microphone positions, and microphone types, while ensuring physical realism through its physics engine. By adjusting these configurations, SonicSim can flexibly generate diverse acoustic environment data to meet the specific requirements of various tasks. The following main functions are included in SonicSim, as shown in Figure 1.\n\u2022 3D scene import: Through Habitat-sim (Savva et al., 2019), SonicSim can import various realistic 3D assets generated through simulations or scans, such as those from the Matterport3D"}, {"title": "3.2 MOVING SOUND SOURCE DATASET: SONICSET", "content": "We used SonicSim to construct a moving sound source dataset, SonicSet, specifically designed for research on moving speech separation and enhancement tasks. This dataset supports diverse acoustic scenarios by simulating microphones, sound sources, and noise sources randomly placed in the environment. A detailed data analysis can be found in Appendix A. As shown in Figure 3, the"}, {"title": "3.2.1 DATA SOURCE", "content": "The data we used consists of two main components: 3D assets and audio data. For the 3D assets, we utilized the Matterport3D dataset (Chang et al., 2017), a large-scale indoor RGB-D dataset containing 90 building-level scenes, representing a wide range of real-world environments. The semantic segmentation annotations from Matterport3D were used to assign acoustic material properties, enabling the creation of more realistic acoustic environments. In SonicSet, the training set includes 62 scenes, the validation set contains 19 scenes, and the test set includes 9 scenes.\nFor the audio data, we primarily used speech data from the LibriSpeech dataset (Panayotov et al., 2015), and noise data from the FSD50K (Fonseca et al., 2021) and the FMA datasets (Defferrard et al., 2017). LibriSpeech is an English speech dataset consisting of approximately 1,000 hours of 16 kHz sampled read speech, sourced from audiobooks in the LibriVox project. We selected train-clean-360 as the training set for speech content, dev-clean as the validation set, and test-clean for testing. FSD50K is an open dataset of manually labeled sound events, containing 51,197 audio clips with a total duration of over 100 hours, covering 200 sound categories. We downsampled them to 16 kHz using the Librosa (McFee et al., 2015). The dataset was divided into training, validation, and test sets in a 7:2:1 ratio. For the FMA dataset, an open music dataset designed to support a variety of tasks in the music domain, we employed a pre-trained BSRNN model (Luo & Yu, 2023b) on music separation tasks to remove vocals from the tracks, ensuring that the speech separation process would not be affected during data synthesis. Subsequently, we downsampled the music data to 16 kHz using Librosa. Similar to FSD50K, the FMA dataset was divided into training, validation, and test sets following a 7:2:1 ratio."}, {"title": "3.2.2 DATASET CONSTRUCTION", "content": "The audio simulation pipeline in the SonicSet dataset, as illustrated in Figure 3, could be divided into the following steps. First, we selected a 3D scene from the Matterport3D dataset and imported it into SonicSim to initialize the acoustic environment. Second, we randomly selected the placement positions for the microphone and the sound source from a certain layer of the scene. Once the microphone's position was determined, the sound source's starting position and the noise source's position were randomly chosen within a 1-8 meter radius from the microphone. Third, based on the initial position of the sound source, we selected a sound source's endpoint within a 1-8 meter range from both the microphone and the sound source's initial position. We then employed SonicSim's trajectory function to calculate and generate a movement path for the sound source. This trajectory function calculated the distance between consecutive points and converts them into a sampling rate, dividing the total number of samples for the dynamic audio based on these distances, and subsequently calculating interpolation indices and weights. Fourth, SonicSim calculated the corresponding RIRs for the different positions along the path and convolved the source audio with these RIRs. Finally, based on the precomputed indices and weights, we extracted audio segments corresponding to the start and end positions of each section from the convolution output and mixed them according to the interpolation weights. Through this process, a temporally coherent audio signal was generated, accurately reflecting the movement of the sound source within the spatial environment.\nTo create realistic synthetic mixed audio, we set the length of each mixed audio clip to 60 seconds. Each speech segment was composed of 3-5 full audio clips with the same speaker randomly selected from LibriSpeech, arranged into a 60-second audio sequence by randomly selecting start positions within 0-8 seconds for each clip. This approach ensured various overlap rates in the mixed audio while utilizing SonicSim for moving sound source synthesis. For noise data, 6-8 segments of environmental noise were randomly selected from the divided FSD50K dataset, and 6-8 segments of musical noise were randomly chosen from the divided FMA dataset. These noise segments were arranged into 60-second audio clips by choosing random start positions within 0-4 seconds. We used Loudness Units relative to Full Scale (LUFS) to adjust volume levels, setting the speech at -17 LUFS, environmental noise at -21 LUFS, and musical noise at -24 LUFS. When generating the mixed audio, different speakers were randomly chosen from the speech data, and a segment of environmental noise was randomly selected to create a mixed audio clip with background noise, while a segment of musical noise was randomly chosen to create a mixed audio clip with a musical noise."}, {"title": "4 BENCHMARK I: SPEECH SEPARATION", "content": "Speech separation aims to isolate individual speech signals from a mixture containing multiple speakers, as shown in Figure 4(a). This task is critical for real-world applications such as meetings and telephone conversations. A detailed explanation of the task pipeline is provided in Appendix C.1."}, {"title": "4.1 PROBLEM DEFINITION", "content": "Speech separation aims to isolate individual speech signals from a mixture containing multiple speakers, as shown in Figure 4(a). This task is critical for real-world applications such as meetings and telephone conversations. A detailed explanation of the task pipeline is provided in Appendix C.1."}, {"title": "4.2 BENCHMARK MODELS", "content": "We selected several popular models that have demonstrated excellent performance in speech separation as benchmarks, including Conv-TasNet (Luo & Mesgarani, 2019), DPRNN (Luo et al., 2020), DPTNet (Chen et al., 2020a), SuDORM-RF (Tzinis et al., 2020), A-FRCNN (Hu et al., 2021), SKIM (Li et al., 2022d), TDANet (Li et al., 2023), BSRNN (Luo & Yu, 2023b), TF-GridNet (Wang et al., 2023), Mossformer (Zhao & Ma, 2023), and Mossformer2 (Zhao et al., 2024). For detailed information about the benchmark models, see Appendix C.2. The training object of the models is available in Appendix C.3. The hyperparameter configuration and all training settings are available in Appendix C.4. The PyTorch implementations and pre-trained weights for all benchmark models are publicly available4."}, {"title": "4.3 DATASET DETAILS", "content": "We report the performance of trained baseline models using the LRS2-2Mix (Li et al., 2023), Libri2Mix (Cosentino et al., 2020), and SonicSet datasets, which we tested on a real-world moving speech separation dataset collected by us. We used the test sets from LRS2-2Mix and Libri2Mix to test the models' generalization and transfer performance across different scenarios. Please refer to Appendix C.5 for detailed descriptions of these datasets."}, {"title": "4.4 EVALUATION METRICS", "content": "We employed a series of evaluation metrics to assess the performance of the speech separation benchmark models comprehensively. These metrics cover multiple dimensions of audio quality, including signal quality (SI-SNR (Le Roux et al., 2019) and SDR (Vincent et al., 2006)), speech intelligibility (STOI (Taal et al., 2011) and WER), and subjective quality perception (PESQ (Rix et al., 2001)). Details are given in Appendix C.6. For these metrics, higher values are preferable for all indicators except for WER, where a lower value is better. From the perspective of practical applications, WER, a speech recognition metric, is the most important metric because speech separation is usually a preprocessing step for speech recognition."}, {"title": "4.5 RESULTS AND ANALYSIS", "content": "Comparison on real-recorded datasets. To validate the acoustic simulation gap between synthetic datasets and real data, we constructed a real-world moving sound source dataset consisting of 5 hours of recorded audio (details in Appendix C.5), encompassing various complex acoustic environments and different types of background noise (environmental and musical noise). In Tables 2 and 3, we trained models using the LRS2-2Mix and Libri2Mix datasets to compare their performance with models trained on the SonicSet dataset. The LRS2-2Mix dataset (Li et al., 2023) contains real-world noise and reverberation. The Libri2Mix dataset (Cosentino et al., 2020) is currently the largest synthetic speech separation dataset, and its data scale is closest to SonicSet among public speech separation datasets. We trained different models on these datasets and tested them on the real-world recorded datasets. The results demonstrated that models trained on the SonicSet dataset achieved overall the best separation performance on the real-world recorded datasets. This finding indicates that the SonicSet dataset excels in simulating more realistic acoustic scenes, making it effective for model training and evaluation.\nComparison on the SonicSet dataset. The results of the speech separation are shown in Table 4. Different models exhibit varying performance under environmental and musical noise conditions. Among the RNN-based models, DPRNN and BSRNN performed relatively poorly when handling musical noise. In contrast, CNN-based models (SuDORM-RF and A-FRCNN) demonstrated more balanced performance across both environmental and musical noise scenarios. Among Transformer-based models, Mossformer2 showed the SOTA performance, especially in handling complex noise. This superiority can be attributed to incorporating the multi-head attention mechanism, which effectively captures long-term dependencies and complex frequency variations (Vaswani et al., 2017)."}, {"title": "5 BENCHMARK II: SPEECH ENHANCEMENT", "content": "The speech enhancement task aims to extract high-quality target speech from a noisy signal, reducing or eliminating background noise, as shown in Figure 4(b). This task is crucial in applications such as speech recognition, speech communication, and hearing aids. A detailed explanation of this task pipeline is available in Appendix D.1."}, {"title": "5.1 PROBLEM DEFINITION", "content": "The speech enhancement task aims to extract high-quality target speech from a noisy signal, reducing or eliminating background noise, as shown in Figure 4(b). This task is crucial in applications such as speech recognition, speech communication, and hearing aids. A detailed explanation of this task pipeline is available in Appendix D.1."}, {"title": "5.2 BENCHMARK MODELS", "content": "In the speech enhancement experiments, we selected several popular models: DCCRN (Hu et al., 2020), Fullband (Hao et al., 2021), FullSubNet (Hao et al., 2021), Fast-FullSubNet (Hao & Li, 2022), FullSubNet+ (Chen et al., 2022b), TaylorSENet (Li et al., 2022a), GaGNet (Li et al., 2022c), G2Net (Li et al., 2022b) and Inter-SubNet (Chen et al., 2023). The details of the models are available in Appendix D.2. The training object of the model is available in Appendix D.3. The hyperparameter configuration and all training settings are available in Appendix D.4."}, {"title": "5.3 EVALUATION METRICS", "content": "In the speech enhancement task, we use a series of evaluation metrics to comprehensively evaluate the performance of the speech separation baseline model, including signal quality (SI-SNR (Le Roux et al., 2019) and SDR (Vincent et al., 2006)), speech intelligibility (STOI (Taal et al., 2011) and CER), and subjective quality perception (PESQ (Rix et al., 2001), DNSMOS (Reddy et al., 2021) and SigMOS (Ristea et al., 2024)). Since the real-world dataset, RealMAN is a Chinese dataset, we use CER as the evaluation metric for speech recognition. For these metrics, except for CER, the higher the value, the better, while for CER, the lower the value, the better. From an application perspective, CER is the most important metric, as speech enhancement is usually a pre-processing step for speech recognition. The PyTorch implementations and pre-trained weights for all baseline models have been made publicly available5."}, {"title": "5.4 DATASET DETAILS", "content": "Unlike the speech separation task, in the speech enhancement task, we select the audio of a single speaker from the training set as the ground truth label for each iteration, mixing it with various types of noise using the same mixing method as in the speech separation task. We trained baseline models using the training sets of VoiceBank-DEMAND (Valentini-Botinhao et al., 2016), the DNS Challenge (Reddy et al., 2020) and SonicSet, and tested them on the test set of the real-world dataset (RealMAN (Yang et al., 2024)) to evaluate the effectiveness of different datasets in simulating real acoustic environments. To assess the generalization ability of models trained on SonicSet, we also tested the baseline models on VoiceBank-DEMAND and the DNS Challenge test sets."}, {"title": "5.5 RESULTS AND ANALYSIS", "content": "Comparison on real-recorded dataset. We utilized the real-world recorded moving source dataset, RealMAN (test set), to evaluate trained speech enhancement models on various speech enhancement datasets, including VoiceBank-DEMAND, DNS Challenge, and SonicSet. VoiceBank-DEMAND is a smaller dataset, while the DNS Challenge dataset comprises approximately 700 hours of data, with a data scale comparable to that of SonicSet. We employed the DNSMOS (Reddy et al., 2021) tool to calculate subjective metrics, while Character Error Rate (CER) results were evaluated using the same speech recognition model6 as used with RealMAN. The results presented in Table 5 indicated that models trained on the SonicSet dataset achieved overall the best results across multiple metrics. Please note that for most practical applications, CER is the most important metric. Based on these findings, we infer that the synthetic dataset SonicSet effectively simulates the acoustic environments of real moving source scenarios. This provides significant flexibility and cost-effectiveness for constructing large-scale datasets, positioning synthetic datasets as an effective alternative for addressing complex environments."}, {"title": "6 CONCLUSIONS", "content": "We introduce a simulation tool named SonicSim, and a large-scale synthetic dataset named SonicSet, designed to study speech separation and enhancement tasks involving moving sound sources. By integrating the Habitat-sim platform, we developed a tool capable of simulating complex acoustic environments, supporting moving sound sources and multi-scene audio generation. Baseline"}, {"title": "A DATASET ANALYSIS", "content": "The dataset statistics are shown in Table 7. SonicSet dataset offers ground-truth labels for speech separation and enhancement for supervised learning, which captures different audio samples for data synthesis in the same scene, with acoustic environments that closely resemble real-world conditions.\nSonicSet contains 57,596 speech moving trajectories across 90 scenes. The training set includes 57,102 trajectories, while the validation and test sets each contain 247 trajectories, covering most possible positions within indoor scenes. The dataset comprises 1,001 speakers, with 921 speakers in the training set, 40 in the validation set, and 40 in the test set, ensuring that no speaker appears in different subsets, which maintains speaker-independent training. Through our data construction process, SonicSet generates approximately 952 hours of training data, 4 hours of validation data, and 4 hours of test data. SonicSet provides researchers with a high-quality and complex moving sound source dataset for speech separation and enhancement methods."}, {"title": "C BENCHMARK I: SPEECH SEPARATION", "content": null}, {"title": "C.1 PROBLEM DEFINITION", "content": "Given an input signal $y = \\sum_{i=1}^{C}x_i + n, y \\in \\mathbb{R}^{1 \\times T}$ that contains audio $x_i \\in \\mathbb{R}^{1 \\times T}$ from $C$ speakers and noise $n \\in \\mathbb{R}^{1 \\times T}$, the objective of speech separation is to extract each speaker's speech $\\hat{x_i} \\in \\mathbb{R}^{1 \\times T}$ and assign it to different output channels, where $T$ denotes the length of audio. Currently, most speech separation methods adopt an encoder-separator-decoder framework (Wang & Chen, 2018)."}, {"title": "C.2 BENCHMARK MODELS", "content": "Conv-TasNet (Luo & Mesgarani, 2019): As the first time-domain speech separation model, it employs a dilated convolutional network, surpassing traditional frequency-domain methods in separation performance. This model processes audio directly in the time domain, enhancing clarity compared to frequency-based methods.\nDPRNN (Luo et al., 2020): A time-domain model based on bidirectional LSTM (BLSTM), it introduced a dual-path architecture for intra- and inter-block modeling, significantly improving the ability to capture long-term dependencies, laying the foundation for subsequent speech separation"}, {"title": "C.3 TRAINING OBJECT", "content": "For all benchmark models in the speech separation, we employed the permutation invariant training (PIT) method (Hershey et al., 2016) to select the best output permutation $P \\in P_M$ (where $P_M$ is the set of all $M$ permutations) to minimize the negative SNR adaptively. Specifically, SNR loss $L_{SNR}$"}, {"title": "C.4 IMPLEMENTATION DETAILS", "content": "During training, since all model hyperparameters were originally configured for the WSJ0-2Mix dataset (Hershey et al., 2016) and suited for audio with an 8 kHz sampling rate, we doubled the window length and window shift of the encoder and decoder to adapt to the 16 kHz sampling rate datasets. Aside from this adjustment, all other hyperparameters remained unchanged. For training, we randomly sampled 3-second audio clips from the training set. The batch size was set to 1, and the Adam optimizer (Kingma, 2014) was used with an initial learning rate of $1 \\times 10^{-3}$. The learning rate was halved whenever the validation loss did not decrease for five consecutive epochs. We applied gradient clipping to limit the maximum $L2$ norm of the gradients to 5. The training ran for a maximum of 500 epochs, with early stopping applied if the validation loss did not improve for 10 consecutive epochs.\nFor each benchmark model, we selected the best model based on its performance on the validation set for testing. In the SonicSet evaluation, we used the full test set and employed a 6-second inference window with a 3-second sliding window to process long audio sequences. For the LRS2-2Mix dataset, we used 2-second audio segments for both training and testing. For the Libri2Mix dataset, we trained and tested the models using 3-second audio segments. All experiments were conducted on a server equipped with 8 \u00d7 NVIDIA 4090 GPUs."}, {"title": "C.5 REAL-WORLD DATASET DETAILS", "content": "In the speech separation experiments, we used SonicSet, public datasets, and real-world datasets. Each mixed audio contained two different speakers, with a sampling rate of 16 kHz. The baseline models pre-trained on the SonicSet training set were trained using a dynamic augmentation strategy. Specifically, during training, we randomly selected a set of data, and two speakers were randomly chosen from three available audio tracks for mixing. Depending on the task requirements, either environmental noise or musical noise was then mixed accordingly. Finally, a 3-second segment was randomly extracted from the 60-second audio for model training.\nTo evaluate the gap between the SonicSet dataset and real-world conditions, as well as the model's transferability to real-world scenarios, we collected a small-scale real-world moving sound source"}, {"title": "C.6 EVALUATION METRICS' DETAILS", "content": "Scale-invariant signal-to-noise Ratio (SI-SNR) (Le Roux et al., 2019) and Signal-to-distortion ratio (SDR) (Vincent et al., 2006) are standard metrics for evaluating the ratio of speech to noise and distortion, effectively measuring a model's ability to suppress noise while preserving the original signal. NB-PESQ and WB-PESQ (Rix et al., 2001) are used to assess the perceptual quality of wideband signals, respectively, based on human auditory models, predicting subjective speech quality, which is particularly suitable for evaluating signals processed through separation and enhancement. Additionally, STOI (Taal et al., 2011) measures speech intelligibility in noisy environments, helping to assess the loss of speech clarity in complex conditions.\nFurthermore, Word Error Rate (WER), a standard in automatic speech recognition (ASR) systems, measures the error rate between the recognized words and the ground truth, reflecting the intelligibility of the speech signal. In this study, we use Whisper-medium-en (Radford et al., 2023) as the ASR model to recognize the content of separated audio.\nBy employing these multi-dimensional evaluation metrics, we can comprehensively assess the model's performance in real-world applications."}, {"title": "D BENCHMARK II: SPEECH ENHANCEMENT", "content": null}, {"title": "D.1 PROBLEM DEFINITION", "content": "Typically, the speech enhancement problem can be expressed as follows: given an observed noisy speech signal $s \\in \\mathbb{R}^{1 \\times T}$, where $s = x + n$, with $x \\in \\mathbb{R}^{1 \\times T}$ being the target speech signal and $n \\in \\mathbb{R}^{1 \\times T}$ representing background noise, the goal of speech enhancement is to recover an estimate of the target speech signal from $s$. Specifically, the process begins by applying the STFT to map the time-domain signal $s$ into time-frequency domain features $E \\in \\mathbb{C}^{F \\times T'}$. Then, a speech enhancement network extracts the target speech from the noisy mixed signal, producing estimated time-frequency domain features $\\bar{E} = f(E)$. Finally, the Inverse STFT is applied to reconstruct $\\bar{E} \\in \\mathbb{C}^{F \\times T'}$ back into the time-domain waveform $\\bar{x} \\in \\mathbb{R}^{1 \\times T}$, thus completing the recovery of the target speech signal."}, {"title": "D.2 BENCHMARK MODELS", "content": "DCCRN (Hu et al., 2020) is a speech enhancement model designed for complex-domain processing. It operates in the frequency domain by handling both the real and imaginary components of speech signals, leveraging the characteristics of complex signals. Additionally, DCCRN employs a recurrent neural network to capture temporal information, making it particularly effective in processing speech in complex noisy environments. The model's innovation lies in its combination of complex-"}, {"title": "D.3 TRAINING OBJECT", "content": "For all benchmark models used in the speech enhancement tasks, we applied various objective functions as outlined in the original papers to optimize model performance. Specifically, the SI-SNR loss function was employed in the DCCRN model (Hu et al., 2020). For the Fullband Hao et al. (2021), FullSubNet Hao et al. (2021), FullSubNet+ Chen et al. (2022b), FullSubNet-Fast Hao & Li (2022), and Inter-SubNet Chen et al. (2023) models, we utilized the complex ratio mask (CRM) error in the frequency domain as the core loss metric. By computing the mean square error (MSE) between the complex mask of the enhanced speech and the ideal mask, we can accurately quantify the model's performance in the time-frequency domain. This method not only restores the magnitude of the speech signal but also effectively preserves its phase information. The loss function is expressed as:\n$L_{Fullband} = MSE(M_{CRM}, M_{CIRM}),$ \nwhere $M_{CRM} \\in \\mathbb{C}^{F \\times T'}$ is the estimated complex ratio mask, and $M_{CIRM} \\in \\mathbb{C}^{F \\times T'}$ is the ideal complex ratio mask.\nFor the TaylorSENet (Li et al., 2022a) and GaGNet (Li et al., 2022c) models, we used an Euclidean loss function based on complex magnitude. This loss function not only considers the differences in the complex space but also incorporates magnitude information, enabling the model to capture subtle changes in the input speech signal more precisely. The objective function is given as:\n$L_{TaylorSENet} = \\alpha \\cdot L_c + (1 - \\alpha) \\cdot L_m,$\nwhere $\\alpha$ is the weighting factor that controls the trade-off between the complex error $L_c$ and the magnitude error $L_m$. $L_c$ is the loss of the complex part, which is defined as:\n$L_c(\\hat{x}, x) = |\\hat{x}_r - x_r|^2 + |\\hat{x}_i - x_i|^2,$\nwhere $\\hat{x}_r$ and $\\hat{x}_i$ represent the real and imaginary parts of the predicted STFT, respectively, and $x_r$ and $x_i$ are the real and imaginary parts of the ground-truth STFT. $L_m$ is the loss of the amplitude part, which is defined as:\n$L_m(\\hat{x}, x) = (|\\hat{x}| - |x|)^2$.\nIn G2Net (Li et al., 2022b), we employed the stagewise complex magnitude Euclidean loss. This loss function applies different weights to the estimated signals at various stages, allowing the model to converge quickly in the early stages while fine-tuning the output signal quality in the later stages. The formula is as follows:\n$L_{G2Net} = \\sum_{i=1}^{N} a_i (L_c (\\hat{x}_i, x) + L_m(\\hat{x}_i, x)),$\nwhere $a_i$ is the weight for each stage, and $N$ is the number of stages in the model."}, {"title": "D.4 IMPLEMENTATION DETAILS", "content": "The hyperparameter settings for all baseline models remain consistent with the original papers. We trained the models using the PyTorch framework (Paszke et al., 2019), with the Adam optimizer (Kingma, 2014) and an initial learning rate of 0.001. If the validation loss does not decrease for five consecutive epochs, the learning rate is automatically adjusted with a decay factor of 0.5. Additionally, all audio samples are processed at a 16 kHz sampling rate. The final model selection follows an early stopping strategy, where training stops if there is no improvement in validation loss for 10 consecutive epochs. During the testing phase, we used the same inference strategy as in the speech separation task to handle long audio samples. All experiments were conducted on a server equipped with 8 NVIDIA 4090 GPUs."}]}