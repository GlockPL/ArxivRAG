{"title": "OPDR: Order-Preserving Dimension Reduction for Semantic Embedding of Multimodal Scientific Data", "authors": ["Chengyu Gong", "Gefei Shen", "Luanzheng Guo", "Nathan Tallent", "Dongfang Zhao"], "abstract": "One of the most costly operations in scientific data management is searching for the k most similar items (or, k-nearest neighbors, KNN) from the database after being provided a new item. The state-of-the-art approach to solving this problem is building an index (e.g., DBSCAN, K-means), such that some tuples can be disqualified at an early stage. However, those existing techniques are inappropriate when the scientific data are multimodal as the measures for different modals are incomparable. The recent advances of multimodal machine learning could potentially overcome the multimodality issue by offering an alternative way for building a semantic index, the so-called embedding vectors mapped from the original multimodal data. However, the resulting (embedding) vectors are usually on the order of hundreds or a thousand of dimensions, which are impractically high for time-sensitive scientific applications.\nThis work proposes to reduce the dimensionality of the output embedding vectors such that the set of top-k nearest neighbors do not change in the lower-dimensional space, namely Order-Preserving Dimension Reduction (OPDR). In order to develop such an OPDR method, our central hypothesis is that by analyzing the intrinsic relationship among key parameters during the dimension-reduction map, a quantitative function may be constructed to reveal the correlation between the target (lower) dimensionality and other variables. To demonstrate the hypothesis, this paper first defines a formal measure function to quantify the KNN similarity for a specific vector, then extends the measure into an aggregate accuracy of the global metric spaces, and finally derives a closed-form function between the target (lower) dimensionality and other variables. We incorporate the closed-function into popular dimension-reduction methods, various distance metrics, and embedding models. Experiments in multiple scientific data sets show that the proposed OPDR method accurately capture the relationship between the target dimensionality and other variables.", "sections": [{"title": "Introduction", "content": "Background and Motivation\nMany scientific applications involve heterogeneous data resources and cannot directly apply conventional indexing techniques (e.g., DBSCAN, k-means) to accelerate query performance. For example, in material sciences (Rangel Da-Costa et al. 2021), each material entity comprises multiple pictures along with textual description. Those textural and pictorial (e.g., stored as numerical matrices) data are usually managed by a scientific database where each type of data is managed by a finer-granularity structure, such as tables in relational databases or blobs in NoSQL databases. The conventional wisdom from a data management point of view to efficiently manage those heterogeneous data is to construct a distinct index for each of those structures (e.g., tables, blobs), which does not necessarily reflect the unified, or semantic, measure of the heterogeneous data.\nThe emergence of multimodal machine learning (e.g., (Li et al. 2022) provides a plausible method to accelerate the queries over heterogeneous scientific data, thanks to the open-source language models that can be directly used for inference; however, the structure of such models cannot be trivially customized and therefore, the potential high dimensionality of the embedding vectors, usually on the order of hundreds or thousands, leads to a problem known as the \"curse of dimensionality.\" This issue precludes multimodal machine learning from broad deployment due to the high-performance (i.e., time-sensitive) nature of scientific applications. For example, many language models (BERT (Devlin et al. 2018), ViT (Dosovitskiy et al. 2020), etc.) generate the embedding vectors of 512 or more dimensions; however, scientific applications (Rangel DaCosta et al. 2021; Wu et al. 2020) require to analyze billions of data (i.e., vectors) on the order of milliseconds. The curse of dimensionality exacerbates the computational complexity, not to mention the additional challenges for model accuracy and generalization.\nThere is therefore an urgent need to develop new methods to fill the gap between the high dimensionality of embedding vectors generated by pretrained language models and the desired lower dimensionality, and this work focuses on one of the most common analytic tasks in scientific applications-k-nearest neighbors (KNN) queries. The KNN query is almost ubiquitous in scientific applications due to the nature of scientific discoveries: In many disciplines, especially natural sciences (Lu et al. 2023; Tshitoyan et al. 2019; Gupta et al. 2022), matching a newly found/constructed item to another well-studied entity in the database is one of the most effective methods to study the new item. If the new method could significantly reduce the performance overhead for performing the KNN queries, many scientific applications would benefit from the new method.\nProposed Work\nIn order to reduce the dimension of high-dimensional embedding vectors in a meaningful sense, this paper first proposes a new notion, namely Order-Preserving Measure (OPM), for the set of nearest neighbors that is preserved during a dimension-reduction map between two metric spaces. In essence, the OPM is formalized as a mathematical function that is well defined over the power-set \u03c3-algebra over the lower-dimensional metric space. This allows us to assign a numerical (real and non-negative) value to an arbitrary subspace of the low-dimensional space and consequently, apply aggregation to quantify the overall accuracy of the dimension-reduction map in terms of preserving the set of k-nearest neighbors of each embedding vector.\nBuilt upon the aforementioned OPM, this paper then constructs a global metric to quantify the closeness of two metric spaces X and Y in terms of, again, the preservation of the set of k-nearest neighbors from X to Y. The objective of constructing such a global metric is to derive a closed-form function among important environmental parameters, such as the dimensionality of metric spaces, the number of data points, and the expected closeness, or accuracy, between the metric spaces in terms of k-nearest neighbors.\nThe conjectured closed-form function is constructed through multiple observations and qualitative analysis. The expected dimension of the lower-dimensional space is positively correlated to both the space's dimensionality and cardinality (i.e., the number of embedding vectors), but with different weights. Specifically, we assign significantly higher weight (exponentiation) on the closeness metric (also known as accuracy, which will be formally defined) than that on the cardinality.\nIn addition to theoretical results about the closed-form function, this paper also demonstrates how to incorporate the order-preserving measure and the closed-form function into the workflow of real-world scientific applications. We will detail the implementation of both the OPM and the closed-form function, and demonstrate that how they work together with other subsystems, such as dimension-reduction methods (PCA, MDS), neural network models (CLIP, ViT, Bert), and distance metrics (Euclidean, cosine, Manhattan). We then evaluate the augmented dimension-reduction system with four multimodal scientific datasets of representative characteristics; our results show that the function accurately captures the intrinsic relationship among the key factors of a dimension-reduction map and therefore, contributes new insights to leveraging advanced AI techniques for complex scientific applications.\nContributions In summary, this work makes the following scientific and technical contributions.\n\u2022 We present a new notion for dimension reduction, namely Order-Preserving Dimension Reduction (OPDR), which leads to a more flexible metric that suffices to capture the invariant of k-nearest neighbors.\n\u2022 We construct a closed-form function that captures the intrinsic relationship among the target dimensionality and other factors in the context of OPDR.\n\u2022 We implement the proposed OPDR method and demonstrate the effectiveness of OPDR by carrying out an extensive evaluation on real-world multimodal scientific data."}, {"title": "Preliminaries and Related Work", "content": "Multimodal Scientific Data\nMultimodal scientific data has drawn a lot of research interests. For instance, the MELINDA dataset (Wu et al. 2020) focuses on classifying biomedical experiment methods by utilizing multiple data types. Similarly, a study on explainable AI (Jin, Li, and Hamarneh 2022) evaluates algorithms within the context of multimodal medical imaging tasks, emphasizing the importance of decision-making based on raw scientific data.\nResearchers have also trained new large language models specifically designed for scientific data. For example, GIT-Mol (Liu et al. 2024) represents a notable effort, developing a multimodal large language model for molecular science that integrates graph, image, and text data. Additionally, MatSciBERT (Gupta et al. 2022) focuses on creating domain-specific language models tailored to materials science. While these models, along with others such as the unsupervised word embeddings for materials science literature (Tshitoyan et al. 2019), demonstrate the potential of specialized models in capturing domain-specific knowledge, they are often limited in scope, typically focusing solely on text-based data.\nMulti-modal data is often highly dimensional, making it crucial to find a trade-off between maintain essential information and reducing the data's complexity. Therefore, some scientists have proposed effective methods to address this challenge. For instance, the study on Multiscale Feature Extraction and Fusion of Image and Text in VQA (Lu et al. 2023) introduces advanced techniques to fuse multimodal data, allowing for a more integrated analysis of different data types. The research on Semi-Supervised Multi-Modal Learning with Balanced Spectral Decomposition (Hu et al. 2020) focuses on optimizing information preservation through spectral decomposition, which enhances the ability to capture correlations within the data. Additionally, Unsupervised word embeddings in materials science literature (Tshitoyan et al. 2019) demonstrates the transformation of textual information into vectors, providing a compact and efficient representation of large-scale text data.\nContrastive Language-Image Pre-Training\nWith the introduction of \"Attention is All You Need\" (Vaswani et al. 2017), the development of large language models has surged, leading to significant advancements in the field. Following this breakthrough, models like BERT (Devlin et al. 2018) and ViT (Dosovitskiy et al. 2020) have focused on text analysis and image recognition respectively. Particularly noteworthy is the CLIP model (Radford et al. 2021), which integrates both text and image data, marking a significant step forward in multimodal data processing. All kinds of studies that improve CLIP model has been propsed. For instance, TiMix (Jiang et al. 2024), address issues with noisy web-harvested text-image pairs by using mixed data samples, while CLIP-Event (Li et al. 2022) and SoftCLIP (Gao et al. 2024) further enhance the alignment between text and images. In (Zhang et al. 2021), authors proposed to convert images into binary codes (hash codes) that preserve image similarity, making it easier to compare images within large-scale datasets.\nSome prior works proposed methods to perform dimension reduction over the embedding vectors. For example, in (Chen et al. 2022), authors proposed a new approach to reducing dimensionality following contrastive learning. Similarly, in (Huang et al. 2020), authors focused on relative positioning in data before and after LLM manipulation. However, none of the above works touched on the preservation of the set of k-nearest neighbors.\nDimension Reduction\nAfter Multidimensional Scaling (MDS) (Torgerson 1952) was published, numerous methods were proposed that preserved pairwise distances in lower-dimensional spaces. For example, Colored Maximum Variance Unfolding (Song et al. 2007) preserved local distances while maximizing variance, and Neighborhood Preserving Embedding (He et al. 2005) retained local relative positions. Further innovations include Tensor Embedding Methods (Dai and Yeung 2006), which tackled the curse of dimensionality, and MultiMAP (Jain et al. 2023), which integrated multimodal data. Recent approaches like Similarity Order Preserving Discriminant Analysis (Hu, Feng, and Chen 2021) and Ordinal Data Clustering Algorithm with Automated Distance Learning (Zhang and Cheung 2020) specifically aimed to preserve data order post-reduction, highlighting the evolving focus on maintaining data integrity across various dimensions. However, none of the above works considered preserving the set of k-nearest neighbors during the dimension reduction.\nAs large language models (LLMs) become popular, scientists have also proposed methods that integrate LLMs with dimension-reduction techniques to enhance the analysis of complex data. For example, Transformer-based Dimensionality Reduction (Ran, Gao, and Fang 2022) introduced a method that decomposes autoencoders into modular components, leveraging the power of transformers to manage high-dimensional data efficiently. Similarly, in (George and Sumathy 2023), authors combined BERT with dimensionality reduction to enhance topic modeling by clustering textual data more effectively. However, the above works assumed that the users of LLMs were able to modify the model structure, which is unrealistic in many scientific applications."}, {"title": "Order-Preserving Dimension Reduction", "content": "This section first presents a new notion, i.e., Order-Preserving Measure (OPM), for the set of k-nearest neighbors that are preserved during a map between two metric spaces. Then, a closed-form function is constructed to quantify the relationship among the space dimensionality, the number of data points, and the OPM. Finally, a detailed implementation is presented about how to incorporate the new measure and function into scientific applications."}, {"title": "Order-Preserving Measure", "content": "Intuitively speaking, an order-preserving measure (OPM) provides a metric of the number of nearest neighbors that do not change between two spaces. For example, if the 2-closest points (we assume that the spaces are metric spaces such that pair-wise distances are well defined) of every point in a metric space (X, \u03b4x) are still the same 2-closest points (of each point) in a metric space (Y, \u03b4y), then we say the map f : X \u2192 Y is order-preserving of 2, or OP2. We will provide a more formal definition of OPz, z \u2208 Z+ \u222a {0} later; before that, we need to point out a common misunderstanding of this notion regarding inclusiveness, as follows.\nThe order-preserving notion defined above is not inclusive in the sense that in general, OPk+1 \u21cf OPk, 1 \u2264 k \u2208 Z+. This can be understood as the set of top k nearest neighbors in the space Y do not necessarily respect the intrinsic order of elements in the set of the top k nearest neighbors in space X. The rationale behind this is that the result of a k-nearest neighbor (KNN) query in many cases will be the input of other analytical steps that are agnostic of the \"internal\" order of the set of points. This implies that, for example, an OP2 map is not necessarily OP1: A sorted list of points in space Y, Ly = (b, a, c) is clearly OP2 if the original sorted list on space X is Lx = (a, b, c) because {b, a} = {a, b} (even if (b, a) != (a, b) as ordered lists); However, Ly is not OP1 regarding Lx because {b} != {a}. For completeness, we will agree that OP0 is trivially true for any pairs of lists.\nMathematically speaking, a measure is a function, say \u03bc, which maps a subset Ei of set X in a well-defined \u03c3-algebra to a value in the extended real line [0, \u221e], such that (i) \u03bc(\u00d8) = 0 and (ii) \u03bc (U\u221ei=1Ei) = \u03a3\u221ei+1\u03bc(Ei). Because we assume there exists a well-defined \u03c3-algebra over (the set of) embedding vectors, say X, Ei's will be understood as disjoint subsets of X unless otherwise stated. In fact, we will explicitly construct the \u03c3-algebra on the target space, Y. As a side note, a measure function \u03bc can be also thought of as a homomorphism between the two monoids (i.e., groups without inverses) (P(X), U) and (R+ \u222a {0}, +) with kernel Ker\u03bc = {0} \u2286 P(X), where P(X) denotes the power set of X. However, our analysis in the following will not require any group-theoretical results.\nThe construction of the \u03c3-algebra of embedding vectors is as follows. Let k denote the target number of preserved k-nearest neighbors. Let Y denote the target (i.e., low-dimensional) metric space (we omit the metric functions here) of the dimension-reduction map. In practice, the cardinality of Y is finite (i.e., there is an upper limit of number of embedding vectors in scientific applications), which is of course countable. The \u03c3-algebra My of Y is simply the power set of all the mapped vectors in Y, i.e., My = P(Y), which is obviously a \u03c3-algebra on Y because Y is countable and P(Y) is closed for any countable number of set unions.\nAfter constructing its \u03c3-algebra, we are ready to define the measure on My. Let Ek,i denote the set of the k-nearest neighbors of yi, where yi \u2208 Y. The same notation can be"}, {"title": "Closed-Form Function", "content": "The previous section defines an additive measure \u03bc on the \u03c3-algebra of the projected space Y; This section investigates the relationship between \u03bc and other parameters, such as the space dimensionality and the number of data points, or space cardinality. The idea is that we will construct a function for the above variables that we conjecture are critical during the dimension reduction, largely based on our experiment observations and mathematical intuition. We will later in the evaluation section verify the constructed function as our hypothesis.\nWe assume that we have an existing dimension-reduction method on hand and we are interested in quantifying the target dimensionality of the lower-dimensional space in which the set of k-nearest neighbors is an invariant, as defined in Eq. (1). It turns out that before attempting to solve the problem, we need to have a more \u201cglobal\u201d metric than \u03bc(\u00b7), which works as a local measure that is concerned with only the k-nearest neighbors of a single point.\nWe define the global metric for quantifying the overall closeness (or, similarity, accuracy) between two metric spaces regarding their k-nearest neighbors as follows. We first aggregate the measure for each point, then normalize the aggregate measure (AM) into [0, 1], and finally calculate the arithmetic mean of all normalized AMs (NAMs). We call the above average of NAMs as the accuracy of Y with respect to X on k-nearest neighbors. Formally, we define the accuracy A as\n$A_k(Y) = \\frac{1}{m} \\sum_{i=1}^{m} \\mu_i (Y \\backslash \\{y_i\\}), \\quad (2)$"}, {"content": "where m = |Y| = |X|, yi \u2208 Y, and \u03bci(\u00b7) is the measure on Y over yi as defined in Eq. (1). It is not hard to see that the accuracy A defined above falls within the range [0, 1], because \u03bci (Y \\ {yi}) is bounded by k (inclusively).\nThe parameters in our hypothetical function include the accuracy Ak between two metric spaces X and Y, the number of data points m = |X| = |Y|, and the dimensionality of the spaces. In practice, the dimension of the domain space X is determined by the neural network model; therefore, our function would assume that X's dimension is a constant and only involve the dimension of Y, denoted by dim(Y). Thus, we expect to construct a function g as follows,\ndim(Y) = g (Ak, m),\nsuch that the result dim(Y) can be set as a parameter in a dimension-reduction function f. It follows that the real-world users can simply compose the functions g and f, i.e., f \u25e6 g, to ensure that the set of k-nearest neighbors is an invariant between two spaces X and Y.\nThe construction of g is as follows. We conjecture the form of g based on the following observations.\n\u2022 Firstly, dim(Y) is positively influenced by both the accuracy Ak and the cardinality m. That is, if the user expects to maintain a higher accuracy of the k-nearest neighbors in a lower-dimensional space Y, then the target dimension dim(Y) should be also set higher. In the extreme case, if Y = X, then Ak = 1.0. Similarly, if there are a large quantity of data points, then the intuition is that we may need to keep a large number of dimensions in the target low-dimensional space Y.\n\u2022 Secondly, the impact of the accuracy Ak should be higher than that of the cardinality m. This can be understood with the intuition that dim(Y) is quite sensitive to Ak because Ak is a global metric of the entire space Y. On the other hand, a small change of the number of data points may or may not significantly change the distribution of the space."}, {"title": "Incorporation into Scientific Data Management", "content": "We will first describe how the embedding vectors are generated from multimodal scientific data and then present how we implement the hypothetical function and incorporate it into popular dimension-reduction methods.\nTo convert scientific data into embedding vectors using CLIP (Contrastive Language-Image Pretraining), Vision Transformer (ViT), and BERT (Bidirectional Encoder Representations from Transformers), which are all transformer-based models, we begin by collecting and preparing the scientific data, which includes both textual data from HDF5 files and images in formats such as TIFF, PNG, or JPEG. The textual data is extracted by reading the Hierarchical Data Format (HDF5) files, where the dataset names and contents are concatenated into a single string representation. For instance, the HDF5 file used in our study contains a dataset with dimensions of (139, 139, 92) along with supporting metadata, providing a structured representation of the scientific data.\nFor the image data, each image file is read and converted to RGB format. The images, including results obtained at four different resolutions: HAADF, BF, ADF, and ABF, are found to have consistent dimensions of (369, 369) pixels. Additionally, a summary image with dimensions of (941, 905) pixels was included. Then, we generate embeddings for both textual and image data using the CLIP, ViT, and BERT models. For textual data, we utlized BERT model to generate embbedings, which processes the concatenated textual information. Image embeddings are produced by the ViT model, which processes the image files. Then, we utilized CLIP model to analyze both textual and image data.\nThe generated embeddings maintain their default dimensionality, which is specific to each model: BERT produces embeddings of 768 dimensions, while ViT also outputs 768-dimensional embeddings. For CLIP, the textual data is embedded via CLIP's text encoder, resulting in 512-dimensional embeddings. Similarly, the image data is processed through CLIP's image encoder, also yielding 512-dimensional embeddings. To create a comprehensive representation of the multimodal scientific data, we combined these embeddings by concatenating the 512-dimensional text and image vectors into a single, unified embedding vector of 1024 dimensions. All generated embeddings, derived from both textual data and image data at various resolutions, are collected and stored for further analysis.\nFollowing the embedding vector extraction, we focus on the dimensionality reduction phase. We evaluated several mainstream techniques including PCA, and MDS, among which PCA consistently outperformed others in terms of maintaining the integrity of location information in our datasets. Our investigations revealed a notable correlation between the effectiveness of PCA in preserving the spatial relationships and the ratio, where n = dim(Y) and m = |Y|, of the target dimension to the number of samples used in the PCA process. We adopted various regression models to elucidate the relationship between the accuracy of preserved relative location information and the ratio n/m. These models facilitate the prediction of an optimal embedding vector dimension required to achieve a predetermined accuracy level, given a known number of samples, m."}, {"title": "Evaluation", "content": "This section evaluates the effectiveness of the proposed method for KNN-preserved dimension reduction on multiple scientific data sets. To ensure the robustness of our method across various scenarios, we evaluate the method with three Transformer-based models for extracting embedding vectors, two popular dimension-reduction techniques, and three distance metrics. All results suggest that the proposed method is highly effective.\nExperimental Setup\nData Sets The dataset used for evaluation were sourced from the Material Project (The Materials Project Accessed 2024) and categorized into four distinct datasets: experimental Observable, stable, metal, magnetic. To be more specific, experimental observable dataset includes 33,990 data points, stable dataset consists of 48,884 data points, metal dataset has 72,252 data points, and magnetic dataset consists of 81,723 data points.\nPlatform For our experiments, we chose CloudLab (Duplyakin et al. 2019) as the testing platform. In this experiment, the specific machine specifications we selected are as follows: All of our experiments are carried out on a CloudLab machine with the following specifications: Node Type: c6420. The machine is equipped with two sixteen-core Intel Xeon Gold 6142 CPUs at 2.6 GHz, 384 GB ECC DDR4-2666 Memory, and two Seagate 1TB 7200 RPM 6G SATA hard drives. The network interface card (NIC) is a dual-port Intel X710 10GbE. We have installed the following libraries in conda environment: transformer, sklearn, torch, pandas, numpy, os, pickle, h5py, shutill, matlotlib, and pyprismatic."}, {"title": "OPDR on Various Data Sets", "content": "We first extracted embedding vectors from four datasets with distinct characteristics (Observable, Unstable, Metal, Non-magnetic) using the CLIP model. Subsequently, we computed the distances between vectors using the L2-norm and performed dimensionality reduction using PCA. To simplify notation, we let n = dim(Y), denote the dimension of the lower-dimensional space. For these four groups of datasets, we further divided them into eight subsets each, with sample sizes m where m\u2208 {10, 20, 30, 40, 50, 60, 70, 80}, to test whether the distribution patterns of data points were consistent across different sample sizes. Observations from figure 1 to figure 4 indicate a strong positive correlation between accuracy and the ratio of n to m. As n approaches m, accuracy initially increases rapidly and then slows down, converging to a stable value. Although the four different datasets exhibit highly similar data distributions, minor differences may be attributed to the inherent distribution characteristics and randomness of the samples.\nThrough comparative analysis of the four data sets discussed above, we observe that our proposed OPDR algorithm performs consistently across multiple data sets and exhibits the anticipated data patterns, validating our initial hypotheses. In the remainder of this section, we will focus on the Observable Material dataset, exploring the effects of using different Transformer-based models, dimensionality reduction techniques, and various distance metrics."}, {"title": "Influence of Embedding Models", "content": "We continued our experiments on the 'Observable' dataset, merely changing the model used for extracting embedding vectors. We employed three distinct transformer-based models: BERT (Bidirectional Encoder Representations from Transformers), Vision Transformer (ViT), and CLIP (Contrastive Language-Image Pre-training) for processing and analyzing scientific data. Specifically, the BERT model was used to analyze text data extracted from HDF5 files, ViT was utilized for processing image data in TIFF and PNG formats, and CLIP was applied to analyze comprehensive multimodal datasets. This enabled us to achieve more holistic and unified representations of scientific data.\nThrough multiple experiments with these three models, we obtained data fit lines for each model to represent the relationship between accuracy and the ratio n/m. All three fit lines indicated that as n approaches m, our accuracy first increases rapidly and then converges slowly. The results from the three different models were consistent with our initial hypotheses. Furthermore, we observed that the obtained fit lines were essentially overlapping, indicating that the choice of model does not alter the data structure. Utilizing these diverse transformer-based models, we were still able to ensure the accuracy and consistency of data relationships. Using different models does not change the data format; thus, in practical applications, it is essential to consider the properties of the data and its intended use to select the most appropriate model for experimentation."}, {"title": "Influence of Dimension-Reduction Methods", "content": "The scientific data analyzed in this study exhibits extremely high dimensionality, often reaching up to hundreds of thousands of dimensions. Despite the application of advanced embedding techniques, the resulting vectors still contain several hundred to several thousand dimensions, posing considerable challenges for effective data analysis and interpretation. To tackle these issues, we employed two principal dimensionality reduction methods-Multidimensional Scaling (MDS) and Principal Component Analysis (PCA)\u2014both renowned for their ability to efficiently reduce data complexity while maintaining the integrity of the relationships among data points. Given that the majority of contemporary dimensionality reduction techniques build on the foundational theories of PCA and MDS, our study specifically focused on these methods to evaluate their practical efficacy. By implementing MDS and PCA, we aimed to simplify the dataset in a way that preserves the relative positioning of data points, thereby facilitating more manageable and interpretable analysis.\nBy comparing the performance of PCA and MDS on the same dataset in Figure 6, we observe that PCA is more sensitive to changes in n/m and can reach higher accuracy more quickly. Moreover, PCA can achieve a maximum accuracy of 100%, surpassing the peak accuracy attainable by MDS. This demonstrates PCA's superior performance in preserving the relative positions of data points. Additionally, employing different dimensionality reduction techniques does not alter the overall data pattern; the relationship between accuracy and n/m still aligns with our hypothesis.\nFrom Figure 6, although the overall data pattern remains constant and both PCA and MDS align with our prior hypotheses, the choice of dimensionality reduction technique is found to have a noticeable effect on the fitting outcomes. In practical applications, it is crucial to weigh the specific contexts and experimental data at hand. This assessment will inform the selection of the most appropriate dimensionality reduction method to achieve the desired level of accuracy."}, {"title": "Influence of Distance Metrics", "content": "In this section, we explore the impact of using different distance metrics-namely L1 norm, L2 norm, and cosine distance on the Observable dataset and CLIP model. The geometric meanings of these metrics are distinct:\n\u2022 L1 norm (Manhattan distance): Measures the sum of the absolute differences of their coordinates. It represents a very intuitive distance metric, similar to the shortest path between two points in a grid-like structure.\n\u2022 L2 norm (Euclidean distance): Calculates the straight-line distance between two points. It is the most commonly used distance metric, applicable in a wide range of natural and engineering problems.\n\u2022 Cosine distance: Measures the cosine of the angle between two vectors, reflecting the difference in direction rather than magnitude, often used to gauge similarity in text or high-dimensional data.\nExperimental observations shown in Figure 7 revealed that as n/m increases, the behaviors of L2 norm and cosine distance are similar, whereas L1 norm performs slightly worse than the other two. Despite the change in distance metrics not altering the overall data pattern\u2014they still show the trend proposed in Eq (4)\u2014the choice of distance metric remains crucial for specific scenarios.In practical use, experimenters should consider the application scenarios of their data to decide which distance calculation method to use."}, {"title": "Conclusion and Future Work", "content": "This paper solves an open problem in multimodal scientific data management regarding the high dimensionality of embedding vectors generated by large language models. Theoretical results include new notions (both point-wise and space-level) to measure the closeness of two metric spaces in terms of their k-nearest neighbors and a closed-function to reveal the relationship between space dimensionality, space cardinality, and space similarity (or, accuracy of the dimension-reduction map). The closed-form function is implemented and incorporated into other subsystems, such as dimension-reduction methods (PCA, MDS), neural network models (CLIP, ViT, Bert), and distance metrics (Euclidean, cosine, Manhattan), which collectively enable a new dimension-reduction approach for efficiently manage and analyze multimodal scientific data.\nOur future work will dive deeper into the theoretical foundation of the closed-form function among metric spaces' dimensionality, cardinality, and accuracy of the dimension-reduction map; the fact that the proposed order-preserving measure is a well-defined measure may shed more insight than we originally thought. We also plan to investigate the extensibility of the proposed method for production vector database systems, such as PostgreSQL and pgvector."}]}