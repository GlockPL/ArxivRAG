{"title": "AGENTIC LLM FRAMEWORK FOR ADAPTIVE DECISION DISCOURSE", "authors": ["Antoine Dolant", "Praveen Kumar"], "abstract": "Effective decision-making in complex systems requires synthesizing diverse perspectives to address multifaceted challenges under uncertainty. This study introduces a real-world inspired agentic Large Language Models (LLMs) framework, to simulate and enhance decision discourse the deliberative process through which actionable strategies are collaboratively developed. Unlike traditional decision-support tools, the framework emphasizes dialogue, trade-off exploration, and the emergent synergies generated by interactions among agents embodying distinct personas. These personas simulate diverse stakeholder roles, each bringing unique priorities, expertise, and value-driven reasoning to the table. The framework incorporates adaptive and self-governing mechanisms, enabling agents to dynamically summon additional expertise and refine their assembly to address evolving challenges. This perspective is rooted in information theoretic understanding, and evaluates the interplay of unique, redundant, and synergistic knowledge, highlighting how these elements contribute to actionable insights. An illustrative hypothetical example focused on extreme flooding in a Midwestern township demonstrates the framework's ability to navigate uncertainty, balance competing priorities, and propose mitigation and adaptation strategies by considering social, economic, and environmental dimensions. Results reveal how the breadth-first exploration of alternatives fosters robust and equitable recommendation pathways. This framework transforms how decisions are approached in high-stakes scenarios and can be incorporated in digital environments. It not only augments decision-makers' capacity to tackle complexity but also sets a foundation for scalable and context-aware AI-driven recommendations. This research explores novel and alternate routes leveraging agentic LLMs for adaptive, collaborative, and equitable recommendation processes, with implications across domains where uncertainty and complexity converge.", "sections": [{"title": "1 A Case for LLMs in Human Discourse", "content": "Human decision-making unfolds within a landscape marked by uncertainty and risk, complexity, and competing priorities. This is particularly true in scenarios requiring immediate yet informed responses to multifaceted challenges, where the stakes encompass social, economic, and environmental dimensions. Natural hazard scenarios, such as those posed by extreme weather events, underscore this challenge. Climate-driven disasters like floods, wildfires, and hurricanes are shaped by dynamic, coupled human-natural systems (CHANS). These systems involve a multitude of interacting biophysical and social processes that can amplify cascading consequences over varying spatial and temporal scales. Decision-making in such contexts must account for resource limitations, socio-economic vulnerabilities, and the inherent unpredictability of impending events [1]. In this context, decision support is described as \u201cthe intersection of data provision, expert knowledge, and human decision making at a range of scales from the individual to the organization and institution\" [2]. Whether addressing natural disasters, public health crises, or systemic societal"}, {"title": "2 Multi-Agent Framework for Decision Discourse", "content": "Transformer-class models were introduced in 2017 [9] and a few years forward saw the birth of ChatGPT, a conversational LLM application that is still the fastest-growing consumer software application ever produced [10]. LLMs are trained on massive text corpora that concentrate human knowledge to perform next-word prediction and iteratively generate an infinite amount of text. They have proven to outperform any type of Machine Learning or non-Machine Learning model in natural language processing (NLP) tasks [11]. Common LLM usage includes question and answer, summarizing, translation, creative writing, and contextual and semantic tasks such as sentiment analysis, for which they secured the top of the leader board [12]. Furthermore, LLMs have shown potential for causal discovery and observational causality inference, although the full extent of their capacity to perform advanced causal analysis is an active discussion [13, 14, 15, 16]. The exhaustiveness of the training dataset enables them to simulate nuanced discourse, evaluate hypothetical scenarios, and dynamically adapt to contextual shifts. Unlike traditional decision-making tools, LLMs can engage with the rich interplay of human perspectives, integrating expertise across disciplines and negotiating trade-offs through collaborative, human-like conversations. As such, they represent a promising candidate for addressing the intricacies of decision-making under uncertainty. These precise characteristics have brought LLMs to the foreground of human-like conversational models as their capabilities encapsulate nuanced opinions and approaches, hypothetical situation evaluation, and dynamic adaptation to situational contexts. Notwithstanding the paradigm shift from monolithic infrastructures to distributed systems that the world has experienced in the past decades [17], LLMs are trained and built as unique and enormous single instance models. Such a design contrasts with the interconnectedness of multi-component systems observed in the world, such as CHANS."}, {"title": "2.1 Agentic Foundation for Decision Discourse", "content": "Following the principles described in the preceding section, recent work [24] introduces a general framework design that supervises LLM-based agents aspiring to \u201cenhance the performance and capabilities of LLMs by leveraging the power of multi-agent systems\u201d. This top-down initiative aims to pave the way for the creation of more powerful Artificial General Intelligence (AGI) models [24]. However, top-down approaches start from general behavioral structures down to specialized ones. In the context of human-like discourse, we posit that specified general behaviors often fail to capture the richness of emergent viewpoints arising through interactions across different expertise, leading to oversimplified recommendations that may overlook critical trade-offs and synergies. Incidentally, such initiatives underperform in applications such as complex decision making under uncertainty [17].\nIn contrast, we develop a bottom-up approach that starts from specialized individual profiles (or personas) which lead to emergent discourse through inter-agent interactions. We model agents as distinct personas, each representing a specific stakeholder role (elected officials, domain specialists, etc.) that possess distinctive deeper values such as risk awareness, tolerance, and societal equity considerations. Such a persona characterization entails creating restrictions of the knowledge space in the LLM to fit the specified behavior. This reflects our argument that monolithic LLMs are limited in simulating simultaneous diverse behaviors, while a collection of individually specialized agents can support multiple interacting behaviors. Our framework draws on principles of systems thinking to organically construct a core framework inspired by real-world societal structures, emphasizing the interconnectedness of components and the emergent properties of their interactions.\nThe adoption of such an architecture is motivated by real-world examples of decision discourse and synthesis. Decisions result from the interactions of multiple individuals such as decision-makers, scientists, and community advocates; in our example collaborating to provide risk assessment and response to extreme flood hazard. The diversity of characters in this assembly is purposed to dynamically explore recommendation pathways pertaining to different aspects of the challenge, representative of different expertise. Such aspects include the safety of the population with considerations regarding social equity, integrity of vital infrastructures, and ecological resilience of the surrounding environment, defined as the tolerance threshold for a system to remain functionally unaltered by exterior perturbations [30]."}, {"title": "2.1.1 Structural Design", "content": "We now expand the concepts developed into specifications that support human-like agentic LLM decision making. Drawing on real-world use cases, agents need a space to meet, debate and converge to a series of recommendations. Inter-agent communication requires such an environment on the grounds that LLMs are currently deployed in monolithic instances that do not support interactive multi-agent designs. Our virtual conference room gathers the mechanisms allowing inter-agent communication, and is referred to as the macro-scale component, for it orchestrates the LLM agents, defined as micro-scale components. Good recommendations \u201ctend to emerge from processes in which people are explicit about their goals; consider a range of alternative options for pursuing their goals; use the best available science to understand the potential consequences of their actions; carefully consider the trade-offs; and contemplate the decision from a wide range of views and vantages\u201d [2]. Recommendations are defined as emergent products of discourse and careful peer evaluation involving consensus. The virtual conference room representing agentic LLM interactions is necessary for such emergence, as monolithic LLMs cannot offer the breadth and richness of such interactions."}, {"title": "2.1.2 Agent Design", "content": "Similar to their human counterparts, the key capability necessary for LLM agents to interact is a dialogue mechanism. A typical iteration of dialogue consists of a listening phase, a sensible answer construction, and an expressive phase. Using computer science vocabulary, this translates to input, processing, and output, respectively attributed to sensors, internal logic and actuators in agentic systems theory [31]. With considerations to LLM agents, the parallel can be drawn between LLM's encoded language function and the sensible answer construction. Hence, prompts can be assimilated to listening, and the output to the expression. The proximity between LLMs' structure and human discourse is partially caused by the compressed representations of our world that are embedded in LLMs' inner structures (Section 4.5). Next, in order to carry out a conversation, agents need memory to remember past interactions. Conversational versions of the most popular LLMs (ChatGPT, LLaMa, etc.) integrate that ability at the expense of multi-agent capabilities. In other words, at present LLMs are either available as conversational agents in their monolithic form, or as an application programming interface (API) that enables the integration in more flexible applications. However the API form of LLM is stateless, leaving the handling of conversation memory to end-users. To achieve this in an agentic LLM framework, we store the conversation at the macro-scale level instead of implementing memory handling in every agent [25]. Because the whole conversation needs to be fed to agents at every iteration (LLM available through APIs are stateless), this choice allows for optimizing the overhead of data exchange between agents."}, {"title": "2.1.3 Orchestration Mechanisms", "content": "Based on the description portraying individuals gathered in a room, peer communication is identified as the minimum required mechanism to support discussion. In addition to peer to peer communication, which is composed of a listening phase, a sensible answer construction phase and an expressive phase, there is need to define the order in which individuals speak, and also to whom they speak, together referred to as orchestration. A trivial solution includes randomly drawing a speaker's turn. However, our early experimentation of this framework have shown inconsistency in the answer quality and discourse coherence. For this mechanism to be realistic and relevant, there is need for implementation at the micro-scale. This turn-based communication articulates around specific agent instructions and conference room level extraction of the selected next speaker based on agent response. To this end, agents are instructed to address a specific interlocutor in the assembly, or issue a general statement that addresses everyone and will trigger the random choice of a next speaker. Content extraction is performed after every agent response, by a stateless and contextless agent entrusted to extract important features from the responses: to which agent the message is addressed, content of the message, and additional properties, such as the persona of additional agents that may be summoned to complement the expertise defined for current group of agents (see Section 2.2).\nWhile we allow for maximum expressivity of the agents so that they can introduce new topics, revisit unresolved issues, and allow the conversation to organically adapt to emerging priorities, LLM agents have shown inconsistencies in keeping the focus on the initial topic. This limitation is partially explained by the imperfection of the data retrieval mechanism (see Section 2.2), and can be addressed at the macro-scale with the implementation of continuous monitoring of responses and adjustment of prompts. Consistent with our organic approach to discourse design, refocusing and moderation is implemented at the micro-scale with the introduction of an agent to serve as the discourse moderator (or facilitator). The moderator's behavior aims to instruct the other participants to refocus on the topic if there are signs of digression, while avoiding to take part in the discussion. With this goal in mind, an analysis of the discussion is periodically conducted by the moderator agent, who summarizes the major points already addressed. If refocusing is not necessary, the analysis is logged but will not be inserted in the discussion."}, {"title": "2.2 Self-Governance and Convergence", "content": "Having defined the discourse mechanisms at the micro- and the macro-scale, we now move on to emergent behaviors of the framework entailing the definition of a convergence criterion. We describe the limitations of persona prompt patterns and formulate a technique for summoning new agents that allows to dynamically build an assembly, specialized for the resolution of a specific challenge.\nPersona prompt patterns (defined in 4.8.1) represent powerful methods for defining LLM agent's behavioral space, and are central to the functioning of this work. The testbed for this framework articulates around an extreme flood hazard assessment and mitigation scenario, where agents emulate decision-makers discussing towards the resolution of such an event, hinting that realistic human behavior is key to the framework's performance. First explorations were conducted with handcrafted persona prompts with consideration of a fixed-sized assembly, including a mayor, a community advocate and an environmental scientist. Character design aims for specific behavior traits and restrictions pertaining to the role: for instance, the mayor is assumed as not having expert knowledge on environmental sciences, and will optimize for constraints pertaining to their standing in the assembly, including infrastructure integrity and safety of the population. As a consequence, individual assembly members optimize for a unique constraint, where the overall assembly optimizes for the union of non-redundant constraints, maximizing agent's utility.\nDespite its potential, the persona prompt technique is imperfect and prompts can show variability in the number of executed directives. This technique's efficiency is also affected by surrounding factors such as conversation size, level of detail and ordering of the instructions. As a result, first explorations showed a large variability in the scope of the knowledge of the mayor, community advocate, and environmental scientist. This entails a significant overlap between their respective domain of expertise, and a poor added value of member interactions. Refined prompt definitions aim at more precise directives, optimizing for the enhancement of agent's unique contribution to discourse, at the expense of a reduced scope of knowledge. As a result, the global scope of knowledge of the assembly is reduced and the minimal skill set necessary for the resolution of the challenge is not achieved, highlighting the need for more specialized agents to fill in the gaps. Handcrafting a new persona that is relevant to the decision-problem at hand, and offering a different expertise relative to the initial assembly members is then required. Along this path, there is evidence for the need of some form of automation as the human bandwidth necessary to produce persona prompts while maintaining or"}, {"title": "2.3 Decision Generation and Evaluation", "content": "We mention in section 2.2 that the quality, relevance, and breadth of framework-made recommendations is of importance for performance evaluation. In addition, the stability and consistency of generated actionable measures can also constitute a convergence criterion. However, complications in decision-making arise from the unexpectedness exerted by uncertainty. Taleb [32] refers to unpredictability as \u201coutside the realm of expectation,\" which also carries a semantically strong attribute of unexpectedness. Unexpectedness is anchored in human perception owing to the availability bias, defined by behavioral sciences as an imprecise evaluation of risks and event likelihood based on the scope of available information [33]. In other words, humans imagine and conceive risks and events that they experience or are knowledgeable about, at the involuntary expense of unlikelier ones. For instance, well established examples from behavioral sciences include the misconception that suicides are less frequent than homicides as a result of a lower media representation.\nWith consideration to the changing patterns in the occurrence and strength of extreme natural hazards, the absence of historical records crystallizes an availability bias to which human beings are subject. This causes humans to sometimes misevaluate the risk of extreme events happening, resulting in hindered preparedness to such events. Generative AI then represents a compelling method to evaluate alternate pathways, as it does not suffer from the same limitations than randomized controlled trials (resource cost, human bandwidth, ethical considerations). Indeed we mention in section 4.5 that LLMs show characteristics of world models, entailing a favorable environment for agents to perform experiments as they're provided with simulated sensors and actuators. In addition to the horizontal scalability offered by multi-agent architectures, this makes our framework compelling for the exploration of a wide breadth of actionable pathways in face of a complex decision-making problem. Traditional methods rely on physical modeling whose performance is hampered by the nonstationarity of modeled processes, whereas agentic LLMs continuously adapt through in-context learning and counterfactual evaluation through variations in the input scenario. Hence, agentic LLM offers novel perspectives based on breadth-first exploration of alternatives mirroring real-world decision making.\nLLMs exhibit great expressivity through natural language, that offer favorable grounds for defining task optimization constraints with the use of persona prompts. Indeed, the personal and professional objectives, and values and aspirations of generated persona organically shape optimization constraints fostering a closer representation of real-world complex decision needs. These include the practicality and resource limitations of implementing strategies, the capacity for generated strategies to improve ecological and engineering resilience (formally defined using dynamical systems theory [34]), and the inclusion of social and economic justice in the distribution of benefits and burdens resulting from generated strategies. Specifically, the community advocate LLM agent solely aims to organically address the underrepresentation of social justice and equity in the face of extreme events mitigation [35]. On the other hand, the agent impersonating the mayor must think about the structural and economical integrity of buildings and facilities, vital to the rapid recovery of the system after a shock. In this regard, our framework mirrors real-world decision-making where outcomes must balance technical feasibility with broader societal impacts, while accounting for uneven distribution of resources and burdens."}, {"title": "3 Application", "content": "We now develop the use case serving as a testbed for this framework through a description of the input scenario, description of the initial assembly composition of the agents and provide analysis and interpretation of generated resolution pathways (Fig. 3a and Fig. 3b) along with broader implications of such a framework for decision-making challenges characterized by complexity and uncertainty."}, {"title": "3.1 Scenario", "content": "As mentioned earlier, the scenario that drives our testing of the framework revolves around low-expectancy, low-probability, and high-risk weather events. The following text describes the hypothetical extreme rain event situation, the context in which it occurs, and serves as a bootstrap prompt for LLM agents interaction:\n\"We are in a US Midwestern township of half a million inhabitants. A large river flows through this township which is also fed by a large watershed in which the township is located. There is forecast for very heavy rain and possibility of flooding at large scale. Consider that the probability of flooding is <probability parameter>. The township needs to make anticipatory decisions to respond to the impending event to minimize the impact of floods but also keep in mind the needs of the community which relies on the river water. A reservoir downstream of the town supports potable water needs, provides energy, and recreational needs. The river also supports navigational and commercial traffic. Decisions must address the management of reservoir levels. At the end of the conversation, a strategy needs to be developed to manage the outcome of the flood keeping in mind uncertainty of the event, noting that different probabilities of risks may justify different approaches, and the associated advantages and drawbacks of the decision variables.\"\nThis scenario was built and inspired following some real-world challenges highlighted by foundational climate assessment reports [8, 36, 2, 37, 35, 1] and translated to optimization constraints in the presented scenario. Examples include preservation of vital services (energy, drinking water), operation of commercial routes and safety of the population."}, {"title": "3.2 Initial Pool of Agent Persona", "content": "3.2.1 Local Government Representation\nThe first agent personality that was designed in this framework is the mayor of the virtual township. The mayor represents a stakeholder or decision maker who is trusted with the final recommendations. The mayor is instructed to rely on personal knowledge, that does not include any particular environmental science priors. Additionally, this agent's design includes a directive to consider consequences of the recommendations made. Last, the possibility for this agent to disagree with its peers and challenge their points is specified, primarily as an initiative to steer the LLM away from its agreeing and compliant baseline behavior which is restrictive in this use case. This last characteristic is further addressed in the discussion section."}, {"title": "3.2.2 Environmental Science Representation", "content": "This agent is described as an urban and environmental engineer, designed to bring a scientific aspect to the discourse and provide sources and knowledgeable information. Therefore, there is future work opportunities of using such a persona to integrate advanced features such as fact verification through content extraction from research material, leveraging research database connectors such as the Langchain arXiV plugin, or other novel sourced web search engines embedded in the latest generation of LLMs. Similarly to the first role presented, this agent is also encouraged to disagree with its peers and challenge their arguments, if needed. Last, we seek to improve this agent's reasoning capacity to reinforce its relevance as a science representative, using prompt engineering. Chain of Thoughts (CoT) is a prompt pattern that enforces a LLM to produce intermediate thoughts before producing the final output. CoT is a well established technique that has shown improvements to the reasoning quality and global precision of the model [38]. Using CoT, we enhance the scientific relevance of the scientist persona."}, {"title": "3.2.3 Community Advocacy Representation", "content": "In contrast to the most common optimization constraints presented, social equity and justice in the face of extreme natural hazards is not integrated in the input scenario. The underrepresentation of social equity and justice in real-world discussions and decision outcomes is well documented, particularly in the face of extreme events challenges [37, 35]. Following that perspective and on the grounds of mirroring real-world situations, we design a single initial agent that optimizes for social equity and justice. Hence, this third agent is designed to incorporate the role of an elected spokesperson that represents the low-income neighborhoods in the township. Similar to the mayor, this spokesperson"}, {"title": "3.3 Results", "content": "Here, the results of running this scenario are presented. The results presented were collected during three grouped executions of the framework with varying values for the indicated probability of flooding: 50%, 75% and 90%. For every instance of each of these parameter values, the framework was run fifteen times, generating fifteen discourse texts. Each output contains the whole discussion between agents, as well as the final bullet-point assessment that presents, selected recommendations, their advantages and drawbacks. An example of such summary assessment, produced during one framework execution included in results is shown in Fig. 2. The full conversation that precedes the summary message is included in supplementary material A. In order to compare results as a function of the parameter values, the number of occurrences of each recommendation is counted, both in the entire generated discussion and in the summary message that gather selected courses of action. Thus, Fig. 3a and 3b present the frequency distribution of recommendations selected in the final step and explored during discourse, respectively. In sections 3.3.1, 3.3.2 and 3.3.3, we compare the representativeness of recommendations using absolute frequencies (percentages). This way, it is possible to quantify when a recommendation was brought up but not chosen, when a recommendation has never been envisioned, or the frequency with which certain measures has been discussed, giving a rough idea of their importance. It is useful to remind that this first exploration values a diverse and breadth-first exploration of recommendations before recommendation of a definitive and precise course of actions, following the perspective presented in section 2.3. Results are pre-processed and counted using GPT4-0, and then humanly refined into the final categories that are shown in Fig. 3a and 3b, where the recommendations envisioned respectively encompass the whole discourse and the summary phase of the discourse. The following description draws on the results illustrated in these figures."}, {"title": "3.3.1 Probability of flooding: 50%", "content": "Starting with a flooding probability of 50%, the most consistent recommendation made by the framework is focused on information. Communication to the population and gathering of information through flood models involving hydrologist are consistently represented in the discourse. The assembly also considers the involvement of the community as a major pillar of disaster response, including the support from local businesses to supply necessary goods such as food, bottled water and hygiene products. Last, final recommendations include long-term strategies such as infrastructure improvements, maintenance and enhancement of the drainage system, and construction of levees and floodplain zoning. Reservoir management is strongly represented during conversations, which contrasts with its lower representation in recommendations. Similarly, the representation of evacuation drops from 18% to 7% between phases of discourse and final recommendation. Additionally, planning shows a similar pattern with a drop from 8% to 0%. recommendations show an accent put over communicating safety guidelines as well as critical information to vulnerable communities so as to raise their awareness and preparedness in case of disruption or evacuation. Overall, discourse shows a complete representation of recommendations while recommendations are representative of short-term actions focused on flooding mitigation rather than long-term planning."}, {"title": "3.3.2 Probability of flooding: 75%", "content": "When the probability of flooding is set to 75%, the course of actions explored show a shift towards proactive measures such as evacuation and the implementation of flood protection, such as flood barriers and sandbagging, etc. While long-term strategies such as the evaluation of impact on the environment and the implementation of urban planning projects are not represented in the exploration for the 50% scenario, the 75% probability scenario shows representation of such long-term initiatives. Most particularly, the implementation of green infrastructures including rain gardens and wetland restoration represents 12% of selected recommendations."}, {"title": "3.3.3 Probability of flooding: 90%", "content": "We now review the last scenario for which the probability of flooding is set to 90%. The results for this scenario are closer to the results for the 75% probability scenario rather than the 50% scenario, focusing on reactive measures. Although the distribution of explored recommendations is similar to the 75% probability scenario, here there is a stronger accent put on environmental impact assessment, transportation accommodation and reservoir management. However, the strongest difference with the 75% probability scenario lie in the recommendation distribution, as the 90% probability scenario results show a stronger representation of evacuation, communication and the implementation of flood protection devices. Flood forecasting methods are also more represented in recommendations, showing a 4% increase compared to the 75% probability scenario. Additionally, the construction of temporary housing and shelters gains 4% in selected recommendations compared to the 75% probability scenario, while the representations of temporary shelters in explored recommendation was less considered by 1 point in the 90% probability scenario. This pattern emphasizes the capture of urgency by the framework, as temporary housing is a complex measure to implement, that should only be considered in case of extreme necessity. Whilst reservoir management is respectively considered at 6% and 13% in the 50% and 75% probability scenario, the 90% probability scenario shows a 2% representation in the recommendations. A possible explanation is the number of runs out of fifteen for which a hydrologist agent has been summoned into the conversation. The 50%, 75% and 90% probability scenarios respectively include 7, 8 and 4 summons of a hydrologist agent out of the fifteen runs. Overall, the 90% probability scenario is representative of reactive measures in a similar fashion as the 75% probability scenario, with an accentuated focus on informative measures, such as the use of flood forecasting methods and the implementation of communication channels to alert the population. Combined with the reduced representation of long-term strategies and the increased representation of temporary housing and shelters and implementation of flood protection devices, the 90% probability scenario demonstrates a compelling perception of a higher risk of flooding."}, {"title": "3.4 Interpretations", "content": "We find that the flood event probability is correlated to the framework's output, shifting from immediate preventive actions to immediate reactive actions with longer-term recovery and mitigation initiatives. The lowest probability scenario shows a strong focus on immediate actions aiming to contain flooding coupled to informative measures, including communication channels and flood forecasting methods. However, higher probability scenario capture better the urgency of the situation with recommendations centered around planning, evacuation, accommodation for transportation and the construction of temporary housing and shelters. This shift is clearly visible in Fig. 3a as the blue envelope (50% probability scenario) has a significantly different shape compared to the orange and green envelopes (75% and 90% probability scenario). The assembly members understand that higher probability scenarios represents an almost unavoidable event, explaining that the course of action focuses of ensuring the safety of relocated population, as countermeasures will only weaken the impact of hazard but can't prevent it from happening. With considerations to the framework, this shows that there is a causal reasoning linking the risk probability in the input and recommendation scopes in the framework's output.\nOn the other hand, we have reasons to believe that LLMs' limitation to recommend specific actions with high likelihood events is due to their training and learning process, in which decision-making is limited in the context of ethical questions, or actual challenges such as extreme weather mitigation. Nonetheless, the course of actions explored during agentic discourse are relevant to the problem at hand, and allow for refined recommendations that dynamically evolve in function of the probability parameter in the input scenario. This exemplifies our idea that actual off-the-shelf LLMs are less likely to recommend important recommendations involving liability if the input prompt specifies that the risk conditioning the recommendation is high. On the other hand, vulnerable, low-income and at risk communities are more considered in the thinking process when the probability parameter is 75% or more, hinting that the current agents persona could misevaluate a 50% probability risk of flooding, for which at-risk communities will be burdened more."}, {"title": "4 Discussion", "content": "We have developed and presented a concept for a self-governed multi-agent LLM framework that simulates complex decision discourse under uncertainty by impersonating human-like stakeholders. A hypothetical scenario inspired by actual and real-world weather challenges is presented, and results generated by the framework have been discussed. Here we provide further discussion articulating around the framework design, major areas of LLM research, takeaways from the results and perspectives on future developments."}, {"title": "4.1 Textual Data", "content": "Whereas classical models deal with numerical values and mathematical functions, LLM's handling of textual data is their principal strength and reason for success. However, LLMs' data representation offers a dual perspective, considering the numerical representation of inner matrices in parallel with the textual capabilities they can demonstrate. We further develop on the knowledge representation of LLMs and the function they encode (Section 4.8.1), which allows us to underscore the challenge of evaluating the LLM, and for which conventional methods are to no avail.\nAn opportunity for evaluating texts involves NLP methods such as word2vec [39] and other embedding methods. However the scalability of such methods to LLMs immense parameter count is yet to be confirmed. What's more is that formalizing this framework with the definition of variables, parameters and results entails a deep rethinking of what a model is in view of the mathematical nature of the usual metrics and performance measurement. LLMs don't present a precise, established and proven methodology to break down an input text into model variables, model parameters and constants, nor documents evaluation metrics such as R2 score, median average error (MAE) and root mean squared error (RMSE).\nAdditionally, LLMs can be described as exerting chaotic behavior considering the strong variability that a change in the input can invoke on the output. Most especially in such a framework, the sequential chained generation of outputs provokes a positive feedback loop in which variations propagate and amplify. Developing on those constraints, the applicability of a conventional formalism relying on parameters and variables is questionable. Undeniably, there is a need to discuss proper methods for writing consistent prompts that provide a sensible context definition towards the goal of controlling sensitivity to input changes while allowing for flexibility with regards to the stochasticity of parameters such as model temperature. With considerations to the challenge such a question represents, it is useful to note that the strength of the presented framework resides in the breadth of explored alternatives, the consistency of outputs being a secondary objective, its importance notwithstanding. Nevertheless, in our experiment the input is kept constant except for the probability of the event."}, {"title": "4.2 Evaluation of Results", "content": "The novelty of LLMs partially explains the absence of documented evaluation methods. Improvement necessitates comparison, and comparison necessitates a formal function that measures a performance index. There exist indices for various NLP tasks, such as the BLEU score for translation evaluation [40], or ROUGE score for summary generation evaluation [41]. In this work, the objective is different as the semantic validity is sought rather than LLMs' technical capacity of translating a text or summarizing a corpus; the relevance and coherence is particularly important in the context of decision support. Hence, two possible methods are indicated: comparing the framework to a benchmark of reference that can place a set of decisions on a scale that hypothetically ranges from unacceptable to optimal [42]. Second, the evaluation of results with an oracle, which is a computational theory concept representing an omniscient machine that contains the recommendations to decision problems [43]. Whereas the use of benchmarks requires the availability of quality historical data, limitations of using an oracle include the conception of an ensemble of well-documented and tested decision models. Regardless, a major common limitation is the absence of reliable metrics to characterize the distance between two texts, their relevance and coherence. As mentioned in section 4.1, NLP methods are not listed here as there is no evidence of performance for LLM-class sized models. An unconventional perspective includes the use of LLMs to evaluate framework decisions along the aforementioned axes [44]. Although this method presents an obvious bias [45], LLMs are the best performing natural language processors in the state-of-the-art [46]. With considerations to the bias of using such an evaluation method, it stills represents the easiest and most efficient implementation allowing for scaling up the framework."}, {"title": "4.3 Technical Control of Invariance for Agent Prompts and Behaviors", "content": "Ensuring the stability and relevance of the overall framework is first achieved by ensuring the stability and relevance of its components. Adopting the standpoint of software engineering testing methodology, conformity and regression tests must be defined in order to ensure the consistency of individual components of the framework. Whereas LLMs are thoroughly compared and placed on a leader board using a wide range of benchmarks, the testing procedures do not enforce a strict control of expected behaviors. In the conceptual framework described in this work, consistency, relevance and believability of persona characters are key to support sensible decision making. To this avail, documented methods include agent questionnaires that can be leveraged to rank the believability of agents [25]. Despite the utmost relevance of employing hundreds of human reviewers for such a task, there is an obvious concern for human bandwidth and scalability. Insights developed in this section suggests that a benchmark framework following the principles of software testing is necessary to ensure the stability of a multi-agent LLM driven framework, while considerations to LLM automation are needed for scaling purposes. Additionally, inputs from other disciplines such as behavioral sciences and linguistics may also be considered so as to provide a complete evaluation."}, {"title": "4.4 Overcoming Off-the-Shelf LLM Guardrails", "content": "For the sake of simplicity and reproducibility, the framework presented in this work leverages an off-the-shelf version of GPT4. Whilst OpenAI's GPT models dominate the technical benchmarks since the advent of LLMs, those models still showcase technical limitations such as hallucinations, semantic imprecision and other specific false behaviors. In the context of this work, scientific relevance is crucial which can be severely hampered by hallucination, a side effect of LLMs describing structurally correct outputs yet semantically erroneous. Fine-tuning, advanced prompt engineering or self-reflection methodologies can mitigate model hallucination, and are to be furthered in future iterations [47]. The introduction of a fact checker agent that browses and parses through research material in order to validate or refute claims made by the agents is an example of an hallucination mitigation technique. Libraries such as Langchain's arxiv plugin represent a compelling alternative, providing simplified access and processing to resources.\nAI Science is thriving and updated versions of LLMs are released weekly. Over the past year, LLMs have improved their benchmark score in such a way that recent releases are less inclined to hallucinations, although recent work shows that LLMs will never be hallucination free [48]."}, {"title": "4.5 Agentic LLMs Enable Interconnectedness", "content": "Accuracy of LLMs is a function of model and dataset sizes [49], which is reflected in model benchmarks [50]. This scaling law hints at a vertical scaling of future LLMs, in contrast with the requirements of complex decision-making. Indeed, transdisciplinary considerations are key for decision-making under uncertainty, especially the need to incorporate diverse perspectives and to reconcile competing objectives while considering diverse viewpoints, priorities and expertise. Such heterogeneous requirements suggest simultaneous consideration of divergent standpoints, for which monolithic LLMs are not adequate."}, {"title": "4.6 Transdisciplinary Interactions and Contributions", "content": "Here, we discuss two simple experiments that portray future development insights for domain interaction-oriented LLMs. Since their initial release a couple of years ago, LLMs have been very popular in certain domains of application outisde of NLP [54, 55, 56"}]}