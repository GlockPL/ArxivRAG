{"title": "Magic Insert: Style-Aware Drag-and-Drop", "authors": ["Nataniel Ruiz", "Yuanzhen Li", "Neal Wadhwa", "Yael Pritch", "Michael Rubinstein", "David E. Jacobs", "Shlomi Fruchter"], "abstract": "We present Magic Insert, a method for dragging-and-dropping subjects from a\nuser-provided image into a target image of a different style in a physically plausible\nmanner while matching the style of the target image. This work formalizes the\nproblem of style-aware drag-and-drop and presents a method for tackling it by\naddressing two sub-problems: style-aware personalization and realistic object\ninsertion in stylized images. For style-aware personalization, our method first\nfine-tunes a pretrained text-to-image diffusion model using LoRA and learned\ntext tokens on the subject image, and then infuses it with a CLIP representation\nof the target style. For object insertion, we use Bootstrapped Domain Adaption\nto adapt a domain-specific photorealistic object insertion model to the domain of\ndiverse artistic styles. Overall, the method significantly outperforms traditional\napproaches such as inpainting. Finally, we present a dataset, SubjectPlop, to\nfacilitate evaluation and future progress in this area.", "sections": [{"title": "Introduction", "content": "Large text-to-image models have recently made significant progress in generating high-quality images.\nHowever, to make these models truly useful, controllability is essential. Users have diverse needs and\nwant to interact with these models in different ways depending on their specific use case. Influential\nwork has been done to enable controllability in these networks, robustly addressing foundational\napplications and controls such as subject personalization, style learning, layout controls, and semantic\ncontrols. Despite this progress, the full potential of these powerful large models has not been fully\nrealized. Some applications that seemed clearly out of reach just a couple of years ago are now\npossible with careful approaches.\nWe present one such application: style-aware drag-and-drop. We formalize this problem and\nintroduce Magic Insert, our method to tackle it, which shows strong performance compared to current\nbaselines. One might initially consider addressing style-aware drag-and-drop by trying to inpaint\nusing a stylized subject, for example by combining Dreambooth [30], StyleDrop [38], and inpainting.\nWe find that approaches of this type are very expensive and achieve subpar results.\nIn developing Magic Insert, we address two interesting sub-problems: style-aware personalization\nand realistic object insertion in stylized images. For style-aware personalization, there have been\nattempts on adjacent problems, such as learning a style and then representing a specific subject in that\nstyle [38, 10], or combining pre-trained custom style and subject models [36, 5]. Recent style work\nsuggests that fast style learning is possible, but fast learning of a subject, including all the intricacies\nof identity, is a much harder problem that has arguably not been solved yet [52, 31, 7, 46]. We\npropose leveraging learnings from both domains and settle on a solution that uses adapter injection of\nstyle paired with subject-learning in the embedding and weight space of a diffusion model.\nOne key idea we propose is to not attempt inpainting directly into an image after achieving style-aware\npersonalization. Instead, for best results, we first generate a high-quality subject and then insert that\nsubject into the target image. To achieve our results, we introduce an innovation called Bootstrap\nDomain Adaptation, that allows progressive retargeting of a model's initial distribution to a target\ndistribution. We apply this idea to adapt a subject insertion network that has been trained on real\nimages to perform well on the stylized image domain, enabling the insertion of our generated stylized\nsubject into the background image.\nOur method allows the generated output to exhibit strong adherence to the target style while preserving\nthe essence and identity of the subject, and for realistic insertion of the stylized subject into the\ngenerated image. The method also provides flexibility in terms of the degree of stylization desired\nand how closely to adhere to the original subject's specific details and pose (or allow more novelty in\nthe generation).\nIn summary, we propose the following contributions:\n\u2022 We propose and formalize the problem of style-aware drag-and-drop, where a subject (a\ncharacter or object) is dragged from one image into another. Specifically, in our problem\nformulation the subject reference image and the target image may be in vastly different\nstyles, and the plausibility and realism of the subject insertion is important.\n\u2022 In order to encourage exploration into this new problem, we present SubjectPlop, a dataset\nof subjects and backgrounds that span widely different styles and overall semantics. We will\nrelease this dataset for public use, as well as our evaluation suite.\n\u2022 We propose Magic Insert, a method to tackle the style-aware drag-and-drop problem. Our\nmethod is composed of a style-aware personalization component and a style-consistent\ndrag-and-drop component.\n\u2022 For style-aware personalization, we demonstrate strong and consistent results using subject-\nlearning in the embedding and weight space of a pre-trained diffusion models, along with\nadapter injection of style.\n\u2022 For drag-and-drop, we propose Bootstrapped Domain Adaptation, a method that allows\nfor progressive retargeting of a model's initial distribution unto a target distribution. We\nuse this to adapt an object insertion network trained on real images to perform well on the\nstylized image domain."}, {"title": "Related Work", "content": "Text-to-Image Models Recent text-to-image models such as Imagen [34], DALL-E 2 [26], Stable\nDiffusion (SD) [28], Muse [3] and Parti [53] have demonstrated remarkable capabilities in generating\nhigh-quality images from text descriptions. They leverage advancements in diffusion models [37, 11,\n40] and generative transformers. Our work builds on top of SDXL [24] and the LDM architecture [28].\nImage Inpainting The task of filling masked pixels of a target image has been explored using\na wide range of approaches: Generative adversarial networks [8] e.g. [23, 13, 17, 22, 27, 54] and\nend-to-end learning methods [14, 16, 42, 49]. More recently, diffusion models enabled significant\nprogress [20, 19, 47, 33, 2]. Such inpainting methods are a precursor to many object insertion\napproaches.\nGenerative Object Insertion The problem of inserting an object into an existing scene has been\noriginally explored using Generative Adversarial Networks (GANs) [8]. [15] breaks down the task\ninto two generative modules, one determines where the inserted object mask should be and the other\ndetermines what the mask shape and pose. ShadowGAN [56] addresses the need to add a shadow cast\nby the inserted object, leveraging 3D rendering for training data. More recent works use diffusion\nmodels. Paint-By-Example [51] allows inpainting a masked area of the target image with reference\nto the object source image, but it only preserves semantic information and has low fidelity to the\noriginal object's identity. Recent work also explores swapping objects in a scene while harmonizing,\nbut focuses on swapping areas of the image which were previously populated [9]. There also exists\nan array of work that focuses on inserting subjects or concepts in a scene either by inpainting [32, 18]\nor by other means [41, 35] - these do not handle large style adaptation and inpainting methods\nusually suffer from problems with insertion such as background removal, incomplete insertion and\nlow quality results. ObjectDrop [48] trains a diffusion model for object removal/insertion using a\ncounterfactual dataset captured in the real world. The trained model can insert segmented objects\ninto real images with contextual cues such as shadows and reflections. We build upon this novel and\nincredibly useful paradigm by tackling the challenging domain of stylized images instead.\nPersonalization, Style Learning and Controllability Text-to-image models enable users to pro-\nvide text prompts and sometimes input images as conditioning input, but do not allow for fine-grained\ncontrol over subject, style, layout, etc. Textual Inversion [6] and DreamBooth [30] are pioneering\nworks that demonstrated personalization of such models to generate images of specific subjects,\ngiven few casual images as input. Textual Inversion [6] and follow-up techniques such as P+ [44]\noptimize text embeddings, while DreamBooth optimizes the model weights. This type of work has\nalso been extended to 3D models [25], scene completion [43] and others. There also exists work on\nfast subject-driven generation [4, 31, 7, 46, 1]. Other work allows for conditioning on new modalities\nsuch as ControlNet [55] and on image features (IP-Adapter [52]). There is a body of work that dives\nmore deeply into style learning and generating consistent style as well with StyleDrop [38] as a\npioneer, with newer work that achieves fast stylization [36, 45, 10, 29], or combines subject models\nwith style models like ZipLoRA [36] and others [5]. Our work leverages ideas from Textual Inversion,\nDreamBooth and IP-Adapter to unlock style-aware personalization prior and combine it with subject\ninsertion."}, {"title": "Method", "content": "3.1 Style-Aware Drag-and-Drop Problem Formulation\nWe formalize the style-aware drag-and-drop problem as follows. Let Is and It denote the space of\nsubject and target images, respectively. The space of subject images consists of images of solely the\nsubject in front of plain backgrounds. Given a subject image xs \u2208 Is and a target image xt \u2208 It, our\ngoal is to generate a new image \u00eet \u2208 Zt such that:\n1. The subject from xs is inserted into \u00eet in a semantically consistent and realistic manner,\naccounting for factors such as occlusion, shadows, and reflections.\n2. The inserted subject in \u00eet adopts the style characteristics of the target image xt while\npreserving its essential identity and attributes from xs."}, {"title": "", "content": "Formally, we aim to learn a function h : I\u300f \u00d7 It \u2192 It that satisfies:\nh(xs,xt) = 2t s.t. xt ~ p(xt Xt, Xs) (1)\nwhere p(xtxt, xs) represents the conditional distribution of the output image given the subject\nand target images. This distribution encapsulates the desired properties of semantic consistency,\nrealistic insertion, and style adaptation. To learn the function h, we decompose the problem into\ntwo sub-tasks: style-aware personalization and realistic object insertion in stylized images. Style-\naware personalization focuses on generating a subject that adheres to the target image's style while\nmaintaining its identity. Realistic object insertion aims to seamlessly integrate the stylized subject\ninto the target image, accounting for the scene's geometry and lighting conditions. By addressing\nthese sub-tasks, we can effectively solve the style-aware drag-and-drop problem and generate visually\ncoherent and compelling results. In the following sections, we present our dataset and the components\nof our proposed method."}, {"title": "3.2 SubjectPlop Dataset", "content": "To facilitate the evaluation of the style-aware drag-and-drop problem, we introduce the SubjectPlop\ndataset and make it publicly available. As this is a novel problem, a dedicated dataset is crucial for\nenabling the research community to make progress in this area.\nSubjectPlop consists of a diverse collection of subjects generated using DALL-E3 [26] and back-\ngrounds generated using the open-source SDXL model [24]. The dataset includes various subject\ntypes, such as animals and fantasy characters, and both subjects and backgrounds exhibit a wide\nrange of styles, including 3D, cartoon, anime, realistic, and photographic. The diversity in color hues\nand lighting conditions ensures comprehensive coverage of different scenarios for evaluation. No real\npeople are represented in the dataset.\nThe dataset comprises 20 distinct backgrounds and 35 unique subjects, allowing for a total of 700\npossible subject-background pairs. The entire dataset is meant for evaluation of the task. This rich set\nof test cases enables the assessment of performance and generalization capabilities of style-aware\ndrag-and-drop techniques. By introducing SubjectPlop, we aim to provide a standardized benchmark\nfor evaluating and comparing different approaches to the style-aware drag-and-drop problem. We\nbelieve this dataset will serve as a valuable resource for researchers and practitioners working in\nimage manipulation and generation, fostering further advancements in this area."}, {"title": "3.3 Style-Aware Personalization", "content": "Our style-aware personalization approach is illustrated in Figure 2. Let fo denote a pre-trained\ndiffusion model with parameters 0. Given a subject image xs \u2208 Is, our method personalizes fe on\nIs in both the weight and embedding space, similar to DreamBooth [30] and Textual Inversion [6].\nIn the first step, we train LoRA [12] (Low-Rank Adaptation) deltas \u2206e to produce an efficiently\nfine-tuned adapted model for where 0\u2032 = 0 + \u2206e, while preserving the model's original capabilities.\nSimultaneously, we learn embeddings e1, e2 \u2208 Rd for two personalized text tokens, where d is the\nembedding dimensionality. We use two learned embeddings since we found better performance for"}, {"title": "", "content": "both subject preservation and editability in this configuration. The LoRA deltas and and embeddings\nare jointly trained using the diffusion denoising loss:\nLjoint = Et,\u20ac [||\u20ac - 60 (x, t, [e1; 2])||2] (2)\nwhere t ~ U(0, 1), \u0454 ~ N(0, I), x = \u221a\u0101txs + \u221a1 \u2013 \u0101te, and eer is the noise prediction of the\nadapted model for. The joint optimization of Ae, e1, and e2 is performed using the loss Ljoint.\nThese personalized text tokens [e1; e2] serve as a compact representation of the subject's identity. By\nperforming embedding and weight-space learning simultaneously, We find that performing embedding\nand weight-space learning simultaneously, with two text tokens, captures the subject's identity more\nstrongly while allowing sufficient editability to introduce the target style.\nIn the second step, we leverage the personalized diffusion model for to generate the style-aware\nsubjects. To infuse the target image xt's style into \u00ees, we employ style injection. Specifically, we\ngenerate a style embedding et = CLIP(xt) of xt using a frozen CLIP encoder CLIP. We then use a\nfrozen IP-Adapter model v to inject et into a subset of the UNet blocks of fer during inference:\ns = for ([e1; e2], v(et)) (3)\nThis approach is similar to InstantStyle [45], with injection into the upsample block that is adjacent\nto the midblock, with some key differences being omitting content/style embedding separation, and\ninjecting into a personalized model. To the best of our knowledge, our central idea of combining\nadapter injection and personalized models remains unexplored in the published literature. This\nensures that is maintains the subject's identity while adopting xt's style characteristics.\nBy combining style-aware personalization with style injection, our method generates subjects that\nHarmoniously blend into the target image while retaining their essential identity, effectively tackling\nthe first challenge of style-aware drag-and-drop and enabling the creation of visually coherent and\nstyle-consistent results."}, {"title": "3.4 Bootstrapped Domain Adaptation for Subject Insertion", "content": "In this section, we address the problem of subject insertion and propose a novel solution using\nbootstrapped domain adaptation. We formalize the concept of bootstrapped domain adaptation and\ndescribe the dataset used for this purpose. Subject insertion is a crucial component of the style-\naware drag-and-drop problem, as it involves seamlessly integrating a stylized subject into a target\nbackground image. While diffusion-based inpainting approaches [21, 34, 28] can be used for this,\nthey still face challenges such as generating content in smooth regions, producing incomplete figures,\nerasing objects behind inserted subjects, and having problems with boundary harmonization. We\ntake a simpler and stronger approach, which is to insert the subject by copying and pasting it into the\ntarget image, and then subsequently generating contextual cues such as shadows and reflections [48]\nin a second step. Unfortunately, existing subject insertion models are trained on data captured in the\nreal world, severely limiting their ability to generalize to images with diverse artistic styles.\nLet Dr, denote the distribution of real-world images and D, denote the distribution of stylized images.\nExisting subject insertion models are trained on samples from Dr, but our goal is to adapt them to\nperform well on samples from Ds. To overcome this limitation, we introduce bootstrapped domain\nadaptation, a technique that enables a model to adapt its effective domain by leveraging a subset of\nits own outputs. As illustrated in Figure 4 (left), we employ a subject removal/insertion model ge\ntrained on real-data ([48] in our case) to first remove subjects and shadows from a dataset S ~ D\nbelonging to our target domain. Subsequently, we filter out flawed outputs and obtain a filtered set of\nimages S'S, which we use to retrain the subject removal/insertion model. Filtering can be done\nusing human feedback or automatically given a quality evaluation module.\nThe bootstrapped domain adaptation process can be formalized as follows:\n3\narg min E(x,y)~S'L(gw(x), y)\n3\nS (4)\nwhere w denotes the adapted model parameters, L is the diffusion denoising loss, and (x, y) are pairs\nof input images and corresponding subject removal/insertion ground truths from the filtered set Sf.\nThe concept of bootstrapped domain adaptation is based on the surprising observation that a diffusion\nmodel trained for subject insertion/removal on real-world data can generalize to a wider stylistic\ndomain to a limited extent. By retraining the model on its own filtered outputs, we can effectively\nadapt its domain to better handle stylized images."}, {"title": "4 Experiments", "content": "In this section, we show experiments and applications. Our full method enables insertion of arbitrary\nsubjects into images with diverse styles, with a large expanse of text-guided semantic modifications.\nSpecifically, not only does the subject retain its identity and essence while inheriting the style of the\ntarget image, but we can modify key subject characteristics such as the pose and other core attributes\nsuch as adding accessories, changing appearance, changing shapes, or even species hybrids (Figure 9).\nThese changes can be integrated with components such as LLMs that allow for automatic affordances\nand environment interactions (Figure 6)."}, {"title": "4.1 Style-Aware Drag-and-Drop Results", "content": "Magic Insert Results We present a gallery of qualitative results in Figure 5 to highlight the\neffectiveness and versatility of our method. The examples span a wide range of subjects and target\nbackgrounds with vastly different artistic styles, from photorealistic scenes to cartoons, and paintings.\nFor style-aware personalization we use the SDXL model [24], and for subject insertion we use our\ntrained subject insertion model based on a latent diffusion model architecture.\nIn each case, our method successfully extracts the subject from the source image and blends it into\nthe target background, adapting the subject's appearance to match the background's style. Notice\nhow the inserted subjects take on the colors, textures, and stylistic elements of the target images. The\ncoherent shadows and reflections enhance the plausibility of the results.\nLLM-Guided Affordances Our proposed style-aware personalization method allows for large\nchanges in character pose, with support from the diffusion model prior. Using and LLM (ChatGPT\n40) we are able to generate LLM-guided affordances for different subjects, by feeding an instruction\nprompt, the full background image, and the section of the background image in which the character\nwill be positioned. Using these LLM suggestions, we can generate the character following these poses\nand environment interactions and insert it in the appropriate space. With this, we show in Figure 6 a\nfirst attempt at the previously unassailable task of inserting subjects into images realistically with\nautomatic interactions with the scene.\nBootstrap Domain Adaptation We show in Figure 4 a sample case of subject insertion with an\ninsertion model that is trained on real images without adaptation, and on the same model that uses\nour proposed bootstrap domain adaptation on a small set of 50 samples. Insertion without bootstrap\ndomain adaptation generates subpar results, with problems such as missing shadows, reflections and\neven added distortions."}, {"title": "", "content": "Our method inherits all benefits of DreamBooth [30] and\nthus allows for modification of subject characteristics such as pose, adding accessories, changing\nappearance, shapeshifting and hybrids. We show some examples in Figure 9. The generated subjects\ncan then be inserted into the background image.\nOur method (w/o ControlNet) also inherits DreamBooth's editability\n/ fidelity tradeoff. Specifically, the longer the personalization training, the stronger the subject fidelity\nbut the lesser the editability. This phenomenon is shown in Figure 10. In most cases a sweet spot can\nbe found for different applications. For our work we use 600 iterations with batch size 1, a learning\nrate of le-5 and weight decay of 0.3 for the UNet. We also train the text encoder with a learning rate\nof 1e-3 and weight decay of 0.1.\nHere we introduce baselines, as well as quantitative and qualitative comparisons, as well as a user\nstudy. Specifically, our proposed baselines utilize the StyleAlign [10] and InstantStyle [45] stylization\nmethods, which can generate images in reference styles given either inversion or embedding of the\nreference image. We combine these methods with either sufficiently detailed prompting guided by a\nVLM (ChatGPT 4) or edge-conditioned ControlNet. For prompting we use the VLM to describe the\nsubjects while eliminating style cues, and for edge-conditioning we use Canny edges extracted from\nthe subject reference images to guide the stylized outputs using ControlNet."}, {"title": "4.2 Comparisons", "content": "Baseline Comparisons We run studies in order to compare the performance of subject stylization\nfor different baselines and our style-aware personalization method. We study the performance of\nthese methods on subject fidelity, style fidelity, and human preference.\nFor subject fidelity (Table 1), our proposed variants achieve high scores across various subject fidelity\nmetrics (DINO, CLIP-I, CLIP-T Simple, CLIP-T Detailed). DINO and CLIP-I metrics are identical\nto those presented in DreamBooth [30] and CLIP-T Simple / Detailed denotes the CLIP similarity\nbetween the output image CLIP embedding and the CLIP embedding of simple and detailed text\nprompts describing the subject, which are in turn generated by ChatGPT 4.\nRegarding style fidelity (Table 2), our proposed variants demonstrate strong style-following perfor-\nmance using CLIP-I [30, 38], CSD [39], CLIP-T [30, 38] metrics. For style fidelity, InstantStyle\nControlNet outperforms our variants using these automatic metrics, although we observe that subject\ndetails and contrast is lost in many of these samples as shown in Figure 8. For this, we also compute"}, {"title": "", "content": "evaluations. We observe that our variants strongly outperform the benchmarks.\nMoreover, finding strong quantitative metrics for subject fidelity and for style fidelity is an open\nproblem in the field, and metrics can have strong biases that can make them suboptimal. Again, we\nshow some examples for our proposed style-aware personalization, along with top baseline contenders\nStyleAlign ControlNet and InstantStyle ControlNet in Figure 8. We observe that the generation\nquality of our variants is stronger than the benchmarks, especially with both strong stylization\nperformance while still retaining the essence of the subjects. Our Magic Insert + ControlNet variant\nis powerful given that it exactly follows the outline of the character, and thus has the strongest subject\nfidelity over all approaches, although it does not have the desirable properties of our method w/o\nControlNet which include pose, form and attribute modification of the subject.\nFollowing previous work [30, 38, 31, 43] we perform a robust user study to compare\nour full method (w/ ControlNet) with the strongest baselines: StyleAlign ControlNet and InstantStyle\nControlNet. We recruit a total of 60 users (4 sets of 15 users) to answer 40 evaluation tasks (2 sets\nof 20 tasks) for each baseline comparison (2 baseline comparisons). We collect a total of 1200 user\nevaluations. We ask users to rank their preferred methods with respect to subject identity preservation,\nstyle fidelity with respect to the background image, and realistic insertion of the subject into the"}, {"title": "User Study", "content": "ImageReward [50] scores in Table 3, which correlate strongly with human preference in aesthetic\nand for style fidelity is an open"}, {"title": "5 Societal Impact", "content": "Magic Insert aims to enhance creativity and self-expression through intuitive image generation.\nHowever, it inherits concerns common to similar methods, such as altering sensitive personal\ncharacteristics and reproducing biases from pre-trained models. Our experiments have not shown\nsignificant differences in bias or harmful content compared to previous work, but ongoing research is\ncrucial. As more powerful tools emerge, developing safeguards and mitigation strategies is essential\nto address potential societal impacts. This includes reducing bias in training data, developing robust\ncontent filtering, and promoting responsible use. Balancing the benefits of creativity with ethical\nconsiderations requires continuous dialogue with the broader community."}, {"title": "6 Conclusion", "content": "In this work, we introduced the problem of style-aware drag-and-drop, a new challenge in the field\nof image generation that aims to enable the intuitive insertion of subjects into target images while\nmaintaining style consistency. We proposed Magic Insert, a method that addresses this problem\nthrough a combination of style-aware personalization and style insertion using bootstrapped domain\nadaptation. Our approach demonstrates strong results, outperforming baseline methods in terms of\nboth style adherence and insertion realism.\nTo facilitate further research on this problem, we introduced the SubjectPlop dataset, which consists\nof subjects and backgrounds spanning a wide range of styles and semantics. We believe that our\ncontributions, including the formalization of the style-aware drag-and-drop problem, the Magic Insert\nmethod, and the SubjectPlop dataset, will encourage exploration and advancement in this exciting\nnew area of image generation."}]}