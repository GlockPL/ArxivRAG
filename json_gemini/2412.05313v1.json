{"title": "LaNMP: A Language-Conditioned Mobile Manipulation Benchmark for Autonomous Robots", "authors": ["Ahmed Jaafar", "Shreyas Sundara Raman", "Yichen Wei", "Sofia Juliani", "Anneke Wernerfelt", "Benedict Quartey", "Ifrah Idrees", "Jason Xinyu Liu", "Stefanie Tellex"], "abstract": "As robots that follow natural language become more capable and prevalent, we need a benchmark to holistically develop and evaluate their ability to solve long-horizon mobile manipulation tasks in large, diverse environments. To tackle this challenge, robots must use visual and language understanding, navigation, and manipulation capabilities. Existing datasets do not integrate all these aspects, restricting their efficacy as benchmarks. To address this gap, we present the Language, Navigation, Manipulation, Perception (LaNMP, pronounced Lamp) dataset and demonstrate the benefits of integrating these four capabilities and various modalities. LaNMP comprises 574 trajectories across eight simulated and real-world environments for long-horizon room-to-room pick-and-place tasks specified by natural language. Every trajectory consists of over 20 attributes, including RGB-D images, segmentations, and the poses of the robot body, end-effector, and grasped objects. We fine-tuned and tested two models in simulation, and evaluated a third on a physical robot, to demonstrate the benchmark's applicability in development and evaluation, as well as making models more sample efficient. The models performed suboptimally compared to humans; however, showed promise in increasing model sample efficiency, indicating significant room for developing more sample efficient multimodal mobile manipulation models using our benchmark.", "sections": [{"title": "1 Introduction", "content": "Powered by large pretrained models, robots become more capable of understanding and executing natural language commands [1, 2, 3, 4, 5, 6]. However, language-conditioned mobile manipulation remains a major challenge. This is underscored by the best-performing system [7] of the NeurIPS 2023 Open Vocabulary Mobile Manipulation challenge [8] achieving a success rate of only 33%. One key reason is the lack of a comprehensive benchmark that aids the development and evaluation of a robotic system that can use multiple modalities to execute long-horizon tasks in diverse multiroom environments. For example, tasks like \u201cGo to the kitchen, pour the boiling water into the teapot, then bring it to me in the living room\" require the robot to use its language understanding, navigation, manipulation, and perception capabilities to satisfy. Specifically, the robot must ground the language command to the world, navigate between the kitchen and the living room, perceive (see) the boiling water, and manipulate the kettle and the teapot while ensuring the water does not spill.\nMost existing datasets only contain a subset of language, navigation, manipulation, and perception data or are limited in ways, such as single-room environments, simulation only, and short-horizon language commands, as shown in Table 1. This limits their ability to evaluate a robotic system's performance on long-horizon mobile manipulation tasks specified by complex language in multiroom environments with large numbers of objects.\nTo address these problems, we present the Language, Navigation, Manipulation, Perception (LaNMP) dataset. LaNMP contains 524 and 50 mobile manipulation tasks in five simulated and three real-"}, {"title": "2 Related Work", "content": "Numerous datasets incorporate 1-2 of the four aspects, e.g. [10, 11, 12]. Our discussion will focus on those encompassing at least three aspects, as these are most closely related to our work. We primarily focus on the most significant difference, which pertains to the aspects each dataset lacks. A subset of the datasets is shown in Table 1, and the full table, Table 6."}, {"title": "2.1 Datasets of Language, Manipulation, and Perception", "content": "Many robot datasets encompass natural language, manipulation, and perception [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]. LaNMP is distinguished by incorporating navigation and these modalities within a closed-loop system. This extends the robot's general-purpose capabilities to mobile tasks, surpassing the limitations of stationary tasks like those performed on tabletop."}, {"title": "2.2 Datasets of Language, Navigation and Perception", "content": "A considerable body of work encompasses natural language, perception, and navigation but not manipulation. Room-to-Room [26], Room-Across-Room [27], ALFRED [28], CoNav [29], and TEACH [30] introduce datasets that map natural language instructions and visual data to navigation actions in household environments across multiple simulated platforms. Finally, QUARD [31] is a dataset that enhances quadruped robots' intelligence by integrating visual and natural language instructions into executable actions for tasks like navigation, terrain traversal, and manipulation. However, QUARD's manipulation refers to whole-body manipulation rather than object manipulation utilizing an arm. Our quadruped robot has an arm, meaning it collects object manipulation data."}, {"title": "2.3 Datasets of Navigation, Manipulation and Perception", "content": "There are significantly fewer papers with navigation, manipulation, and perception (NPM) but no natural language. Wong et al. [32] introduce the MoMaRT system, which allows for intuitive control of a mobile manipulator's arm and base through teleoperation. It focuses on collecting a multi-user demonstration dataset in simulated environments using MoMaRT, capturing long-horizon mobile manipulation tasks to support novel imitation learning with error detection methods. Mobile ALOHA [33] is a cost-effective physical system designed for imitating bimanual, whole-body mobile manipulation tasks, which is used as a teleoperation system for data collection. It was used to collect mobile manipulation NPM data, which was then co-trained existing imitation learning models. BRMData [34] is a bimanual-mobile robot manipulation dataset for household tasks, featuring diverse manipulation scenarios and sensory inputs to advance robot learning and imitation from human demonstrations. Unlike these datasets, LaNMP centers its tasks around natural language, using it as the core modality upon which all other modalities are built. Incorporating natural language enhances user accessibility, facilitates intuitive human-robot interaction, and enables robots to execute a wider range of tasks based on semantic instructions."}, {"title": "2.4 Datasets of Language, Navigation, Manipulation and Perception", "content": "Few papers comprehensively cover natural language, perception, navigation, and manipulation, and they often have limitations in other areas. RT-1 [35] is an approach utilizing a transformer that inputs visual and textual data and outputs both navigation and manipulation actions to complete mobile manipulation tasks. Simultaneously, they release a dataset collected using two robots from both simulation and the real world used to train the method. While this dataset encompasses LaNMP's modalities, LaNMP is different in the following ways: 1) RT-1 is limited in scope, focusing solely on fetch and deliver tasks within single kitchen scenes. In contrast, LaNMP supports more complex, longer-horizon mobile manipulation tasks that span multiple rooms and floors in a diverse set of environments. 2) RT-1 encompasses fewer data types than LaNMP. For instance, RT-1's perception data is limited to RGB, whereas LaNMP includes RGB, depth, and instance segmentations. 3) The embodiments used in LaNMP, both in simulation and the real world, differ from those in RT-1. To collect data on physical robots, we used a quadruped mobile manipulator, while RT-1 used a wheeled mobile manipulation, so LaNMP can be used to evaluate the locomotion of both wheeled and quadruped robots. In addition, using a quadruped allows our data to expand to tasks in difficult-to-navigate areas, such as stairs in a house, a feat that RT-1's wheeled robots cannot accomplish. Finally, while RT-1 does technically have navigation, it seems to be confined to only moving to kitchen table from a few meters away. Conq Hose [36] is a dataset utilizing a quadruped robot, Spot, to grab, lift, and drag a vacuum hose around in a real-world environment. The dataset is limited in size, with only 139 trajectories, capabilities, ability to perform only one task, and data types. Furthermore, it only contains 2-3 rooms, does not include multiroom navigation, and its navigation is limited to backward and forward slight movements. LaNMP contains Spot body velocity, arm velocity, joint states, etc., while Conq Hose does not. It is also restricted to the real world, lacking simulation data. RoboCasa [37] is a simulation software that can be used to create datasets for training robots in everyday environments, leveraging LLMs to enhance diversity and realism. The created dataset does not contain real robot data, unlike LaNMP. Finally, Open X-Embodiment (OXE) [38] is a dataset combining many existing datasets, utilizing a multitude of real robots and a few simulated ones, aimed at exploring the potential for training generalist robotic policies that can be efficiently adapted to new robots, tasks, and environments. The authors also showcase RT-X models demonstrating the benefits of leveraging combined experiences across diverse robotic platforms. Relevant sub-datasets, such as RT-1, have already been discussed in this section. OXE is vastly composed of manipulation-only tabletop data, lacking a substantial amount of navigation data, meaning limited mobile manipulation. As a result, the vast majority of tasks are short-horizon, even in the few mobile"}, {"title": "3 Language, Navigation, Manipulation and Perception (LaNMP) Dataset", "content": "LaNMP is a multimodal dataset that contains long-horizon mobile manipulation tasks specified by natural language in diverse multiroom simulated and real-world environments. Having both simulated and real data strengthens the diversity of the dataset and, as a result, the generalizability and sample efficiency of models being trained on it. LaNMP encapsulates a broad spectrum of tasks typically performed at the home or workplace. Completing the tasks requires the robot to use its language understanding, navigation, manipulation, and perception capabilities. Throughout task execution, comprehensive trajectory data with over 20 attributes, e.g., RGB-D images, segmentation masks, and the poses of the robot body, end-effector, and grasped objects, is captured at a frequency of 3 Hz.\nThe relative scarcity of long-horizon data in existing datasets poses a significant challenge for developing versatile robotic systems capable of navigating and interacting with complex environments over extended periods of time. Our benchmark dataset uniquely enriches the landscape of long-horizon multimodal multiroom data."}, {"title": "3.1 Simulation Dataset", "content": "Our simulation dataset comprises 524 trajectories over 20 rooms in five environments. We selected environments that ensured diversity in objects and room layouts, thus enhancing the generalizability of models trained on our dataset. We use the AI2THOR simulator [39]. Specifically, we use RoboTHOR [40] environments because they have multiple rooms, while the iTHOR [39] environments used by existing datasets, such as ALFRED, mainly have single rooms. In each environment, there is an average of 105 trajectories, as illustrated in Figure 2d. The average length of these trajectories is 172 steps, as detailed in Figure 2a.\nWe used the ManipulaTHOR [41] robot in the RoboTHOR environments since it has an arm with low-level pose data. We collected 13 attributes, such as RGB-D images, segmentations, and the poses of the robot body, end-effector, and grasped objects. Figure 3 provides more data details.\nWe used Prolific to collect 524 natural language commands from 41 participants. Next, we recruited a different group of 15 participants to execute the commands on the simulator. We collected trajectories comprising navigation, manipulation, and perception data. Appendices 7.2.1 and 7.2.2 contain more details about recruiting, simulation data collection, and crowdsourcing."}, {"title": "3.2 Real-World Dataset", "content": "Our real-world dataset comprises 50 trajectories across 10 rooms in three environments. The first is a three-room laboratory, the second is a floor in a university building, and the last spans two adjacent floors connected by stairs in the same building. We picked these environments for their large size, multitude of rooms, inclusion of stairs, and object diversity. The two floors contain kitchens, furnished lobbies, a classroom, and a staircase. We recruited seven participants that provided 50 natural language commands. Specifically, 20, 15, and 15 for each of the three environments, respectively. Each command specifies long-horizon room-to-room pick-and-place tasks. The collected commands were executed on a quadruped mobile manipulator Spot. More details in Appendix 7.1."}, {"title": "4 Evaluation", "content": "LaNMP can be used to benchmark different paradigms such as imitation learning (IL) [42, 43, 44], reinforcement learning (RL) [45], skill learning and abstraction, and providing in-context examples for planning. Since there has been increased interest and widespread adoption in IL approaches, such as behavior cloning (BC) [42] models (e.g. RT [35, 46, 38]), we evaluated BC models.\nTo evaluate the applicability and strength of the LaNMP benchmark, we employed it to fine-tune two recent models, namely RT-1 [35] and ALFRED's Seq2Seq model (S2S) [28], utilizing the simulation data from LaNMP, and evaluate it on a third model LIMP [6]. We also tested how well its diversity can improve model sample efficiency. RT-1 and S2S take natural language commands and RGB images as input, while S2S also takes in previous actions. LIMP takes as input natural language commands. RT-1 and S2S output a mix of high and low-level navigation and manipulation actions for the simulated and real robots, while LIMP outputs a Task and Motion Plan (TAMP) for real robots. This selection of models was instrumental in conducting a thorough evaluation of LaNMP's benchmarking efficacy across a wide spectrum of model dimensions and initial performance benchmarks. LIMP is a high performing system composed of large foundation models. RT-1 is a relatively large (~ 45M parameters) and high-performing model, while S2S is smaller (~ 35M parameters) and exhibited poor performance on the ALFRED benchmark. Details in Appendix 7.3."}, {"title": "4.1 Experiment Details", "content": "For S2S and RT-1, we performed fine-tuning on the simulation dataset utilizing 5-fold cross-validation to evaluate scene generalizability potential. Trajectories in each scene are designated to a fold; fine-tuning is performed using four scene folds and evaluated on the fifth held-out scene fold. Additionally, we performed another experiment focusing on task generalization instead by utilizing a random (seeded for community benchmarking) subset of every scene for training and testing. Finally, we performed experiments evaluating the dataset's diversity and its potential in improving sample efficiency. All experiments utilized cross entropy (CE) loss between the predicted action distributions and the ground-truth actions. Given the scope of our research, hyperparameter tuning was deemed unnecessary, and consequently, a validation split was not incorporated. Thus, for the task"}, {"title": "4.2 Evaluation Metrics", "content": "LaNMP assumes humans are logical agents with common-sense reasoning by collecting teleoperated trajectories for complex long-horizon tasks. For robust evaluation, we considered two categories of metrics for cross-scene and task generalization experiments: \u201cground truth relative\u201d (GTR) metrics that compare against LaNMP trajectories as standards and \u201cground truth independent\u201d (GTI) metrics that evaluate a trajectory (ground-truth or predicted) on task understanding or smoothness. These metrics provide a multifaceted assessment framework: Task Success (GTR), Grasp Success Rate (GTR), Average RMSE (GTR), Average Number of Steps, Mean and Standard Deviation in State Differences (GTI), and CLIP Embedding Reward (GTI). All metrics are to characterize the dataset and to be reported as benchmark scores for others. More metric details in Appendix 7.5."}, {"title": "5 Analysis", "content": "The objective of evaluating the models is to determine the dataset's applicability, assess its difficulty as a benchmark, and see how well it improves sample efficiency through its diversity. Both BC models demonstrated a success rate of 0%, starkly contrasted against the ground truth trajectories. Meanwhile LIMP performed better, yet still poorly, at 11%. However the sample efficiency was shown to improve. This significant discrepancy underscores the infancy of current State-of-the-Art (SOTA) models in mirroring the proficiency of human counterparts, partially due to low sample efficiency. Consequently, it accentuates the importance of cultivating more comprehensive datasets, such as LaNMP, encompassing a broader spectrum of abilities and sensory modalities. Experimental results are summarized in Tables 2."}, {"title": "5.1 Model Performance Results", "content": "Specifically, the performances can be delineated as follows:\n\u2022 The lower CE Loss showed that RT-1 learns better than S2S, which is unsurprising since RT-1 is a larger and more advanced model. More loss details are in Appendix 7.4.\n\u2022 S2S's RMSE is lower than RT-1's. This may be attributed to S2S frequently outputting NoOP commands, resulting in it often remaining stationary. This means the predicted base and end-effector positions could be closer to the ground-truth than RT-1, due to RT-1 exploring more, thus it potentially deviated further from the GT path.\n\u2022 RT-1's trajectory lengths were significantly shorter because S2S often predicted many NoOp until it reached the maximum action limit of 1500 whereas RT-1 usually stopped earlier at ~ 300-400 steps. Both were still less efficient than the human.\n\u2022 The weighted movement between step positions indicates the smoothness of the agents' control, with smaller values representing better smoothness. Although the models show smaller values than humans, this is primarily due to agents, especially S2S, remaining stationary for large parts of the predicted trajectories.\n\u2022 Higher CLIP values represent better performance, with humans scoring higher than both models. This disparity indicates that the models are not near human-level understanding, reasoning, and grounding.\n\u2022 All CLIP scores, including those of the humans, were likely low due to the lack of semantic correlation between observations and commands throughout most of a trajectory. This is"}, {"title": "5.2 Dataset Diversity & Sample Efficiency", "content": "It is evident that the BC models are not sample efficient, as they were not able to learn enough from LaNMP's ~ 400 trajectories. BC models can suffer from sample inefficiency [48, 49], and Transformers are known to require large amounts of training data [50], suggesting that future models may have to use different paradigms and architectures to reach human-level sample efficiency.\nWe propose an alternate solution to improve sample efficiency by using diverse data. Although dataset scale is essential for policy learning and generalization in modern models due to their sample inefficiency, we hypothesize that diversity in scene and modality data is equally, if not more, critical for robotics models. Works such as ALFRED and Prompter [51] performed ablations across different modalities, which demonstrated the performance improvement from using multimodal inputs are greater than the combined performance improvements from using unimodal inputs. This speaks to the importance of diverse input features for policy learning.\nTo assess the importance of scene diversity and its role in improving sample efficiency, we evaluated RT-1 and S2S on 100 trajectories evenly sampled from an increasing number of scenes (1, 2, 3, 4) such that the number of trajectories per scene reduced (100, 50, 33, 25), but the total number of trajectories remained unchanged as 100. All four ablations are tested on a fifth held-out test scene. Furthermore, we compare a larger, diverse set of scenes with fewer trajectories to a smaller, less diverse set with more trajectories (Table 4). Additionally, we cluster tasks by the commands using cosine similarity, contrasting fewer clusters against more clusters while maintaining the same amount of trajectories (Table 5).\nResults can be delineated as follows:\n\u2022 S2S's CE loss decreased significantly as the number of scenes increased, showing the importance of diversity over scale in this context.\n\u2022 After training RT-1 on just 100 trajectories, the cross entropy loss is within 20 of the loss after fine-tuning on ~ 400 trajectories for the cross-scene experiment. This suggests that fine-tuning on a diverse but smaller dataset leads to similar policy generalization as fine-tuning on a dataset with 4\u00d7 the scale on an unseen test set.\n\u2022 The standard deviation in RT-1's test set CE losses is statistically significant given that losses lie > 10 from one another. This suggests that varying the dataset diversity has a measurable influence on the CE loss of an unseen test set, i.e., the generalizability of the learned policy."}, {"title": "6 Conclusion", "content": "We introduce LaNMP, a mobile manipulation benchmark comprised of simulated and real-world trajectories paired with their respective language commands specifying household tasks. The trajectories are long-horizon, spanning multiple rooms and floors, consisting of navigation, manipulation, and perception data. We fine-tuned and evaluated two models, and tested a third, on LaNMP to test its applicability, strength, and sample efficiency as a benchmark. Though the models reduce training and test-set losses, suggesting good generalization, they exhibit poor performance on metrics. However, their sample efficiencies were improved due to the dataset's scene and task diversity. This suggests that LaNMP could serve as a difficult benchmark for advancing the development of sample efficient mobile manipulation models. Currently, there is no quadruped simulation data, making that a task for the future. Extending the benchmark to include tasks involving more complex manipulation than pick-and-place will increase its applicability to a broader set of models."}, {"title": "7 Appendix", "content": "1. Appendix 7.1: More Dataset Details\n2. Appendix 7.2: Data Collection\n3. Appendix 7.3: Model Details\n4. Appendix 7.4: Cross Entropy Loss Details\n5. Appendix 7.5: Metric Details"}, {"title": "7.1 More Dataset Details", "content": "This section provides the full table, Table 6, of related datasets mentioned in Section 2. Additionally, all the data types collected for the dataset are displayed in JSON format in Figures 3 and 4. Finally, it explains how the scoring in Figure 5 is calculated."}, {"title": "7.1.1 Further Dataset Differentiation", "content": "Figure 5 shows how LaNMP is scored differently from other datasets. The scoring system is divided into 2 axes: Capabilities and Complexity. The Capabilities axis measures the capabilities and modalities the dataset includes out of the four main categories: Language, Navigation, Manipulation, and Perception. The purpose is to see how diverse the dataset inputs are. We calculate the Capabilities score $C_{1}$ as follows:\nL: Natural Language\nN: Navigation\nM: Manipulation\nP: Perception\nR: Real\nS: Sim\n$C_{1} = (L + N + M + P)(R + S)$\nwhere each attribute is 1 if it's present, 0 otherwise. The maximum Capabilities score is 8. Conversely, the Complexity axis shows how difficult and diverse the tasks and environments are. We define the"}, {"title": "7.1.2 Data Types", "content": "There are 24 unique data types across both the simulated and real-world data. Specifically, there are 4 simulated-only, 12 real-only, and 8 of both. All of the simulated ones are 13, and 20 are all of the real ones. Figures 3 and 4 show all of data types."}, {"title": "7.2 Data Collection", "content": "This section displays the maps that the simulation and real robots used during data collection in Figures 6 and 7. It also provides further details on how the data was collected."}, {"title": "7.2.1 Simulation Command Collection Details", "content": "The simulation natural language commands were collected using Prolific through a website we developed. The participants utilized it to watch a tutorial video, read instructions about the task, explore the five RoboTHOR environments to know what commands to give, then provid 15 commands, three for each of the five environments. We collected a total of 615 natural language commands from the 41 participants. We then conducted a meticulous filtering process to select 524 high-quality commands that instruct the robot to perform room-to-room pick-and-place tasks by using all its navigation, manipulation, and perception capabilities.\nThe website was hosted on AWS Elastic Beanstalk and the inputted commands were saved on an AWS S3 bucket. Screenshots of the website are displayed in Figure 8."}, {"title": "7.2.2 Crowdsourcing", "content": "The Prolific participants were paid an hourly wage of US$10, totaling US$380. Subsequently, the simulation teleoperation of those commands was done by a separate group of paid participants. They were recruited via Google Forms. The recruitment instructions are shown in Figure 15a. This group of participants was paid US$10/hr via Amazon gift cards, totaling US$630."}, {"title": "7.2.3 Real Robot Teleoperation", "content": "To collect the trajectory data, we first built dense 3D topological maps (shown in Figure 7) of the environments and then teleoperated the Spot robot via joysticks and a tablet to follow the collected commands. To collect, organize, and save the data, a laptop was mounted and connected via Ethernet to the Spot. The reason for mounting a laptop was so that the collection frequency of 3 Hz remains consistent, unlike using WiFi. 3 Hz in particular was an inspiration from the RT-1 paper [35].\nSpot has more sensors than the virtual agent, allowing us to collect more diverse data types. The main data types include RGB-D, body and end-effector poses, body and arm velocities, joint states, and velocities. Figure 4, lists all data types. The average trajectory length is 323, as detailed in Figure 2b."}, {"title": "7.3 Model Details", "content": "The implementation details of the RT-1 and ALFRED Seq2Seq models are explained in this section. The maximum action prediction limit for both models is 1500."}, {"title": "7.3.1 RT-1", "content": "Robotics Transformer 1 (RT-1) [35] is a model designed for generalizing across large-scale, multi-task datasets with real-time inference capabilities. RT-1 leverages a Transformer architecture [52] to process images and natural language instructions to generate discretized actions for mobile manipulation. RT-1 is trained on a diverse dataset of approximately 130K episodes across more than 700 tasks collected using 13 robots. This enables RT-1 to learn through BC from human demonstrations annotated with detailed instructions. Although RT-1-X [38] demonstrates superior performance, it was trained on OXE, which is mainly manipulation-only, while RT-1 was trained on mobile manipulation data. This makes RT-1 more suitable for our mobile manipulation fine-tuning.\nTo fine-tune RT-1, we utilized the natural language commands with RGB image observations from LaNMP. Due to the contrasting embodiment and incompatible action space of ManipulaTHOR,"}, {"title": "7.3.2 ALFRED Seq2Seq", "content": "The ALFRED paper introduces a Sequence-to-Sequence [54] model leveraging a CNN-LSTM architecture with an attention mechanism for task execution. It encodes visual inputs via ResNet-18 [55] and processes language through a bidirectional LSTM. A decoder processes these multimodal inputs along with historical action data to iteratively predict subsequent actions and generate pixelwise interaction masks, enhancing precise object manipulation capabilities within the given environment.\nWe utilized a subset of LaNMP's data types, RGB, natural language, and previous actions to fit the ALFRED model's input specifications. We modified the ALFRED model outputs to an 8-dimensional action vector tailored to our action space, encompassing modes (stop, base, grasp-release, head, rotate, arm-base, and ee), base movements (NoOp, MoveAhead, MoveBack, MoveRight, and MoveLeft), grasping actions (NoOp, PickupObject, ReleaseObject), head movements (No0p, LookUp, and LookDown), rotational angles (-359- 359 and No0p), and end-effector movements (specified ranges for x, y, z coordinates, including No0p). The numerical actions, which are the rotation and end-effector, are converted from global coordinates to relative coordinates by taking the differences between the timesteps for more stable learning. Based on the findings from RT-1, we discretized the continuous rotation and end-effector relative actions into 256 bins. This structured action representation enabled precise predictions and executions of robotic actions within the simulator at each timestep. Furthermore, we changed it so that only the goal command is used rather than inputting specific instructions to the model at every timestep since the ideal situation is for a human to command a robot only once. Detailed descriptions of the model adaption and data modifications are provided in Appendix 7.3.3.\nFurthermore, we utilized a forked improved implementation of the ALFRED Seq2Seq model.2 The model had to be modified to work for our data, robot, and environments. In addition to the aforementioned modifications, the following were also made:\n\u2022 All weights except the final layer were frozen during fine-tuning.\n\u2022 Unfrozen weights were initialized with random values from the ranges of the actions/states.\n\u2022 The final layer was swapped with a fully connected layer outputting all of the classes, which is essentially the product of the number of bins and the number of actions.\n\u2022 An adapter layer that resizes dimensions was added at the start of each LSTM cell from the second one onward, due to the action output from the first cell being different from the original.\n\u2022 The model was fine-tuned to predict the discretized deltas or changes of the action/state values since it led to more stable learning.\nFurthermore, we experimented with fine-tuning continuous and discrete action outputs. The continu-ous was regression utilizing Mean Squared Error (MSE) loss, while the discrete was classification utilized CE loss. We settled on classification using 256 bins for discretization.\nDetails on the original model can be found in the ALFRED paper [28]."}, {"title": "7.3.3 LIMP", "content": "Quartey et al. [6] introduces LIMP, a pick-and-place mobile manipulation system integrating Linear Temporal Logic (LTL), foundation models, and novel TAMP algorithms to create a framework for verifiably following complex robot instructions. The system first translates natural language instructions into LTL formulas, utilizing foundation models to dynamically generate context-aware translations that align with the desired task specifications. The LTL formulas incorporate Composable Referent Descriptors (CRDs) to parameterize robot skills and disambiguate referent objects through"}, {"title": "4 Evaluation", "content": "LaNMP can be used to benchmark different paradigms such as imitation learning (IL) [42, 43, 44], reinforcement learning (RL) [45], skill learning and abstraction, and providing in-context examples for planning. Since there has been increased interest and widespread adoption in IL approaches, such as behavior cloning (BC) [42] models (e.g. RT [35, 46, 38]), we evaluated BC models.\nTo evaluate the applicability and strength of the LaNMP benchmark, we employed it to fine-tune two recent models, namely RT-1 [35] and ALFRED's Seq2Seq model (S2S) [28], utilizing the simulation data from LaNMP, and evaluate it on a third model LIMP [6]. We also tested how well its diversity can improve model sample efficiency. RT-1 and S2S take natural language commands and RGB images as input, while S2S also takes in previous actions. LIMP takes as input natural language commands. RT-1 and S2S output a mix of high and low-level navigation and manipulation actions for the simulated and real robots, while LIMP outputs a Task and Motion Plan (TAMP) for real robots. This selection of models was instrumental in conducting a thorough evaluation of LaNMP's benchmarking efficacy across a wide spectrum of model dimensions and initial performance benchmarks. LIMP is a high performing system composed of large foundation models. RT-1 is a relatively large (~ 45M parameters) and high-performing model, while S2S is smaller (~ 35M parameters) and exhibited poor performance on the ALFRED benchmark. Details in Appendix 7.3."}, {"title": "4.1 Experiment Details", "content": "For S2S and RT-1, we performed fine-tuning on the simulation dataset utilizing 5-fold cross-validation to evaluate scene generalizability potential. Trajectories in each scene are designated to a fold; fine-tuning is performed using four scene folds and evaluated on the fifth held-out scene fold. Additionally, we performed another experiment focusing on task generalization instead by utilizing a random (seeded for community benchmarking) subset of every scene for training and testing. Finally, we performed experiments evaluating the dataset's diversity and its potential in improving sample efficiency. All experiments utilized cross entropy (CE) loss between the predicted action distributions and the ground-truth actions. Given the scope of our research, hyperparameter tuning was deemed unnecessary, and consequently, a validation split was not incorporated. Thus, for the task"}, {"title": "7.4 Cross Entropy Loss Details", "content": "This section illustrates the CE loss curves for all of the experiments conducted. The loss curves, including for both training and testing, decreased over the course of training, showing the models indeed learned from the LaNMP dataset. RT-1 losses are displayed in Figures 9, 10, and 11. ALFRED's Seq2Seq losses are displayed in Figures 12, 13, and 14."}, {"title": "7.5 Metric Details", "content": "\u2022 Task Success (GTR): a binary value measuring whether an agent achieves the goal/completes the task specified in the command.\n\u2022 Distance From Goal (GTR): the spatial distance between the agent's final position after executing a learned trajectory and the designated ground-truth goal state.\n$d = \\frac{1}{2} (\\sqrt{x_{gt.body, n}^2} - \\sqrt{x_{eval\\_body, n}^2 + \\sqrt{x_{gt.ee, n}^2} - \\sqrt{x_{eval\\_ee, n}^2}$)\n\u2022 Grasp Success Rate (GTR): the efficacy of the agent's attempts to grasp objects. Specifically, the percentage of attempts that result in successful object acquisition.\n\u2022 Average RMSE (GTR): the average root-mean-square error. It is a weighted average of body and end-effector pose errors between the predicted and ground-truth trajectories, normalized by their maximum lengths.\n$RMSE = \\sqrt{\\sum_{i=0}^{n} \\frac{1}{2} (\\sqrt{x_{gt\\_body, i}^2} - \\sqrt{x_{eval body, i}^2 + \\sqrt{x_{gt ee, i}^2} - \\sqrt{x_{eval ee, i}^2}$"}]}