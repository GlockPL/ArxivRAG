{"title": "Federated Fine-Tuning of LLMs: Framework Comparison and Research Directions", "authors": ["Na Yan", "Yang Su", "Yansha Deng", "Robert Schober"], "abstract": "Federated learning (FL) provides a privacy-preserving solution for fine-tuning pre-trained large language models (LLMs) using distributed private datasets, enabling task-specific adaptation while preserving data privacy. However, fine-tuning the extensive parameters in LLMs is particularly challenging in resource-constrained federated scenarios due to the significant communication and computational costs. To gain a deeper understanding of how these challenges can be addressed, this article conducts a comparative analysis three advanced federated LLM (FedLLM) frameworks that integrate knowledge distillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs, where clients upload model parameters or gradients to enable straightforward and effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient knowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into two parts, with one part executed on the client and the other one on the server, to balance the computational load. Each framework is evaluated based on key performance metrics, including model accuracy, communication overhead, and client-side computational load, offering insights into their effectiveness for various federated fine-tuning scenarios. Through this analysis, we identify framework-specific optimization opportunities to enhance the efficiency of FedLLMs and discuss broader research directions, highlighting open opportunities to better adapt FedLLMs for real-world applications. A use case is presented to demonstrate the performance comparison of these three frameworks under varying configurations and settings.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) represent a significant advancement in artificial intelligence, achieving exceptional performance in tasks such as natural language understanding and generation [1]. These models have found applications across diverse domains and tasks, including chatbots (e.g., ChatGPT), writing assistants (e.g., LanguageTool), and advanced search engines (e.g., New Bing). Before these task-specific LLMs can be deployed for inference services, they typically undergo two training stages, as illustrated in Fig. 1. During pre-training, LLMs are trained on large-scale public datasets to acquire a broad understanding of language. In the fine-tuning stage, the models are adapted to specialized tasks using task-specific data. However, centralizing these distributed private task-specific data for fine-tuning presents significant challenges, particularly due to privacy concerns and regulatory constraints on data access.\nFederated learning (FL) facilitates the privacy-preserving fine-tuning of pre-trained LLMs on distributed clients by only sharing model updates between the server and clients, ensuring the data remains localized [2]. However, fine-tuning large models like GPT-3 (175 billion parameters) [3], BERT (340 million parameters) [4], and LLAMA (65 billion parameters) [5] in resource-constrained federated scenarios presents significant challenges, including high communication and computational overhead.\nTo address these challenges, parameter-efficient fine-tuning (PEFT) [6] has been proposed, focusing on fine-tuning only a subset of the model's parameters. Building upon the fundamental FedLLM framework using PEFT, knowledge distillation (KD) and split learning (SL) are integrated to further enhance fine-tuning efficiency. These combined approaches have led to to the development of three advanced FedLLMs fine-tuning frameworks. The first framework, FedLLMs, is the foundational approach, where clients directly transmit model updates to the server. The second framework, KD-FedLLMs, employs KD to enable efficient knowledge transfer through logits exchanged between the global and local models [7]. The third framework, Split-FedLLMs, integrates SL with FedLLMs, facilitating the exchange of intermediate activations between clients and the server [8]. Each framework highlights a distinct approach for knowledge transfer, showcasing their unique strategies for federated fine-tuning.\nTo evaluate the performance of these frameworks, including the effectiveness of fine-tuning, as well as their communication and computational overhead, this article offers a systematic overview and comparison of these three FedLLM fine-tuning frameworks, while identifying key optimization opportunities. To the best of our knowledge, this is the first comprehensive comparison of these FedLLMs frameworks. The main contributions of this article are:\n\u2022 Comparison of Three FedLLM Frameworks: This article presents a comprehensive overview of three FedLLM frameworks: basic FedLLMs, KD-FedLLMs, and Split-FedLLMs. By examining the distinct knowledge transfer methods based on direct model updates, knowledge sharing via logits, and intermediate activations, this work provides valuable insights into how each framework addresses the efficiency challenges of"}, {"title": "II. FEDERATED FRAMEWORKS FOR LLM FINE-TUNING", "content": "This section introduces the principles of the three FedLLM frameworks 1.\nA. FedLLMs: Foundational Approach to Federated Fine-Tuning\nFedLLMs, as presented in Fig. 2(a), represent the foundational approach to federated fine-tuning. The system comprises a central server and multiple distributed clients. Each client starts with a frozen pre-trained LLM and fine-tunes a set of tunable parameters, referred to as \u201ctunable parameters\u201d throughout this paper. These parameters vary based on the specific PEFT technique used, such as Low-Rank Adaptation (LoRA), adapters, or prompts, and are optimized using the client's private dataset. The central server manages the training process by collecting fine-tuned parameter updates from all clients and aggregating them to refine the global model parameters. The typical workflow for a single training round consists of the following steps:\n\u2022 Global Model Distribution and Local Fine-Tuning (a1, a2): The server sends the global tunable parameters to the participating clients (a1), who then fine-tune the tunable parameters locally using their private datasets (a2).\n\u2022 Local Update Uploading and Global Model Update (a3, a4): Clients upload their fine-tuned tunable parameters to the server, which aggregates them to refine the global tunable parameters for the next iteration.\nThis iterative process continues until the global model achieves the desired performance, effectively enabling collaborative fine-tuning while ensuring that client data remains private.\nB. KD-FedPEFT: Logit-Based Knowledge Sharing\nKD is a machine learning technique that enables the transfer of knowledge from a \"teacher\" model to a \"student\" model [9] via logits sharing. The logits are the predicted probabilities of different potential outputs from the \u201cteacher\u201d model for a given input, guiding the student model to mimic the performance of the \u201cteacher\u201d model. As shown in Fig. 2(b), the workflow of KD-FedLLMs includes the following main steps:\n\u2022 Local Fine-tuning and Client-Side Knowledge Generation and Uploading (b1, b2, b3): After local fine-tuning (b1), clients use the fine-tuned local models to generate knowledge representations (logits) of samples in a public dataset (b2) and then upload these representations to the server (b3).\n\u2022 Knowledge Processing and Distillation (b4, b5): The server processes the received knowledge to get a refined"}, {"title": "C. Split-FedPEFT: Activation-Based Updates", "content": "SL is a collaborative learning paradigm where the model is split between clients and the central server [10]. In this approach, the clients process the training data samples and compute the intermediate activations, which are then sent to the server. The server processes these activations and completes the remaining part of the model training. Split-FedLLMs, as shown in Fig. 2(c) [8], apply this approach to LLMs, with the following workflow:\n\u2022 Client-side Model Forward Propagation and Activation Transmission (c1, c2): Clients compute activations using the initial model layers on their private data and then send these activations and labels to the server.\n\u2022 Server-side Model Forward Propagation and Backpropagation, and Gradient Transmission (c3, c4): The server processes activations, computes the loss, and performs backpropagation (i.e., fine-tune the server-side LLMs) (c3). It then sends the gradients for the activations back to the clients (c4).\n\u2022 Client-side Model Backpropagation and Tunable Parameter Updates (c5, cc1): Clients use the received gradients to update their initial layers (i.e., fine-tune the client-side LLMs) (c5), and then upload the fine-tuned tunable parameters to the server (cc1).\n\u2022 Tunable Parameters Aggregation, Distribution, and Replacement (cc2, cc3, cc4): The server aggregates the tunable parameters from all clients (cc2), redistributes them (cc3), and clients replace their local parameters with the aggregated ones (cc4).\nThe LLMs can be split between client and server based on the design of the FedLLM system. A common approach is inter-transformer splitting, where initial transformer blocks are processed by the client, and deeper, more computationally intensive blocks are handled by the server. Alternatively, intra-transformer splitting divides tasks within a single transformer block, such as processing self-attention on the client and the feed-forward network on the server, allowing more flexible resource allocation."}, {"title": "III. COMPARATIVE ANALYSIS AND COMPARISON", "content": "This section quatitively analyzes and compares the three considered frameworks, focusing on key performance metrics, including model accuracy, communication efficiency, and client-side computational load."}, {"title": "IV. RESEARCH OPPORTUNITIES AND FUTURE DIRECTIONS", "content": "Building on the factors impacting each FedLLM framework, this section identifies framework-specific optimization opportunities and explores broader research directions to advance FedLLMs.\nA. FedLLM-specifc Research Opportunities\nThe design of the tunable parameters largely affects the effectiveness of FedLLMs.\n1) Optimizing Tunable Parameter Design for Performance and Efficiency Trade-offs: For FedLLMs, the number of the tunable parameters plays a key role in balancing model performance, communication overhead, and computational load. For instance, in LoRA, smaller ranks of the LORA matrix reduce communication and computation costs, making them suitable for resource-constrained environments, but may sacrifice model accuracy. Larger ranks enhance model accuracy but incur higher communication and computation resource usage. Thus, one research question is how to design the configuration of tunable parameters for optimization of this trade-off, ensuring high model accuracy while minimizing communication and computational overhead, which is especially critical in bandwidth-limited or resource-constrained FedLLM settings.\n2) Optimizing Aggregation Strategies for Heterogeneous Clients: In resource-heterogeneous environments, clients with varying computational capabilities may use different scales of tunable parameters. Clients with limited computational resources may adopt smaller-scale tunable parameters to save resources, while those with higher computational capabilities may utilize larger-scale tunable parameters for enhanced performance. Thus, it is important to design innovative aggregation strategies that effectively integrate tunable parameters from diverse clients with varying scales. Promising research directions include 1) weighted aggregation methods that account for the parameter scales, 2) adaptive techniques that balance client contributions, and 3) methods such as low-rank approximation to harmonize the scale of tunable parameters during the aggregation process.\n3) Dynamic Adjustment of Tunable Parameters: Dynamic adjustment of tunable parameters during training offers an effective way to optimize resource usage. Clients could begin with fewer parameters to reduce initial communication and computation costs, gradually increasing them as the model converges. Additionally, dynamically adjusting the scale of tunable parameters in real-time could optimize resource utilization by adapting to available computational resources and model performance requirements. For instance, clients could increase the parameter size during periods of high resource availability to boost accuracy, while reducing it during low-resource periods to conserve resources."}, {"title": "B. KD-FedLLMs Research Opportunities", "content": "The effectiveness of this framework depends on the quality of the public dataset, while its communication overhead is determined by the sizes of the logits and the public dataset.\n1) Public Dataset Selection and Alignment: Poor alignment between the public dataset and the clients' private data distributions can result in irrelevant distilled knowledge, negatively impacting global model performance. A promising research direction is to improve the alignment between public datasets and clients' private data distributions to enhance the effectiveness of KD-FedLLMs. This can be achieved through adaptive sampling strategies that leverage client feedback and dynamic dataset optimization. For example, clients can provide lightweight information, such as label distribution statistics of the client's dataset. Then, the server can use this information to iteratively refine the public dataset, such as by prioritizing or augmenting samples that better align with the clients' local data distributions.\n2) Logit Dimensions Reduction and Dataset Pruning: The communication and computational overheads in KD-FedLLMs increase with the sizes of the logits and the public dataset. Promising techniques to reduce the communication and computational overheads include: 1) dimensionality reduction, such as principal component analysis or low-rank approximation, and filtering methods that retain only the top-k most likely predictions (e.g., tokens with the highest probabilities) for generative tasks, reducing unnecessary data transfer; 2) knowledge compression methods, including transmitting softened label distributions or statistical features like entropy, to further reduce logit size while preserving essential information; 3) dataset pruning, leveraging methods like importance-based sample selection to identify the most representative public data samples, enabling efficient KD with minimal resource usage.\n3) Advanced Knowledge Processing: Upon receiving logits from clients, the server can either select the most relevant ones for distillation or aggregate them to improve the global model. Promising directions for advanced knowledge processing include: Logit selection techniques, such as importance-based filtering, which prioritizes high-importance logits, or entropy-based filtering, which selects logits with low uncertainty, can help reduce noise and focus on the most informative predictions. Logit aggregation methods, such as weighted aggregation, can assign larger importance to clients with higher data quality or task relevance, while hierarchical clustering can group clients with similar data distributions, enabling more structured and efficient aggregation."}, {"title": "C. Split-FedLLM Research Opportunities", "content": "This approach reduces the computational burden on resource-constrained devices by offloading part of the workload to the server. However, optimizing the partitioning strategy to balance performance, communication efficiency, and computational demands remains crucial, along with the development of communication-efficient algorithms to further reduce communication overhead.\n1) Dynamic Splitting for Resource-Aware Workload Distribution: Investigating dynamic partitioning strategies that adaptively adjust the split point between client and server based on heterogeneous resource availability, such as computational power, memory capacity, and network bandwidth. This approach ensures balanced workload distribution, minimizing bottlenecks and improving efficiency in federated learning environments with diverse client capabilities.\n2) Communication-Efficient Intermediate Activation and Gradient Transfer: A promising research direction lies in developing techniques to reduce the communication overhead in Split-FedLLMs by optimizing activation and gradient transfer. Adaptive activation compression dynamically prioritizes critical activations, such as leveraging attention-based sparsification to transmit only the most relevant values while discarding less significant ones. Quantized activation transfer applies precision adjustments based on resource constraints, using higher precision for critical layers and lower precision for less significant ones. Additionally, selective data sampling employs importance sampling to identify and transmit the most impactful data points, reducing redundant communication while maintaining model performance."}, {"title": "D. Other General Opportunities", "content": "There are also several general opportunities for advancing FedLLMs and making them more adaptable to real-world applications.\n1) Continual Learning in Federated Fine-tuning: Real-world federated learning often involves dynamically changing data distributions and the emergence of new tasks. A promising direction is to develop frameworks that enable FedLLMs to adapt to new tasks while retaining prior knowledge. Techniques such as memory replay, which stores key representations of past tasks for selective reuse, and regularization-based methods, like Elastic Weight Consolidation (EWC), can help mitigate catastrophic forgetting. Additionally, dynamic model expansion can allocate new parameters or task-specific adapters for incoming tasks while preserving previously learned components. Collaborative knowledge sharing between clients working on similar tasks and task-aware scheduling mechanisms can further enhance adaptability while minimizing resource overhead.\n2) Human-in-the-Loop for Real-Time Fine-Tuning: Incorporating human-in-the-loop (HITL) mechanisms into FedLLMs enables interactive fine-tuning by leveraging real-time user feedback. For instance, applications like personalized chatbots or adaptive recommendation systems can continuously improve by learning from user interactions. Active learning techniques can identify the most informative user data to prioritize during training, reducing communication overhead while maximizing model improvement. By integrating HITL, FedLLMs can achieve enhanced personalization and adaptability, particularly in dynamic environments where user preferences and tasks evolve over time.\n3) Federated LLMs with Multi-modal Data: Many real-world applications require processing multi-modal data, such as text, images, and audio, which demands more complex model architectures and training strategies. Extending FedLLMs to support multi-modal data learning offers a significant research opportunity. Techniques like multi-modal knowledge distillation can transfer information across modalities, enabling efficient learning without sharing raw data. Additionally, adaptive aggregation methods can balance contributions from clients with different data modalities, ensuring effective integration of diverse representations. Developing frameworks to efficiently train and aggregate multi-modal LLMs in federated environments would enable robust and scalable solutions for applications involving diverse data types."}, {"title": "V. CASE STUDY", "content": "To validate the practical value of our analysis, we perform a case study using the GPT-2 model [11] and the Banking77 dataset [12]. This case study further quantitively examines the impact of key factors on the performance of each framework, including LoRA rank r, public dataset (PD) size, and number of training samples (TS), offering practical insights into the strengths and limitations of the frameworks under different configurations. We extract 5002 samples from the training dataset as the public dataset, and the remaining 5001 training data is evenly distributed among three clients. Therefore, each client has a training set size of 1667. In each round, each client trains for one epoch with its training dataset. For our experiments, we implement the LoRA technique with a dropout rate of 0.1 and no bias. The LoRA scaling factor is set to 32, and initialization is performed using a Gaussian distribution. The target module for LoRA is the attn.c_attn layer. The optimization is carried out using the Adam optimizer with a learning rate $\\eta = 0.001$. The token padding length is set to 80, and token truncation is enabled. To ensure reproducibility, we use seeds 0, 1, and 42 for the experiments. The results reported are the average of the outcomes obtained from these three seeds.\nFig. 3 illustrates the model accuracy after 100 communication rounds for FedLLMs, KD-FedLLMs, and Split-FedLLMs, evaluated under varying configurations of LORA rank r, PD size, and TS size, respectively. For FedLLMS, we examine the impact of different LoRA ranks, observing that the higher LoRA ranks lead to higher model accuracy. For KD-FedLLMs, we analyze the effect of public dataset size, finding that larger datasets significantly enhance server test accuracy. For Split-FedLLMS, we examine the influence of the number of training samples per communication round, noting that a higher number results in increased model accuracy. Among the three frameworks, FedLLMS achieve the highest model accuracy, outperforming both Split-FedLLMs and KD-FedLLMs. This observation aligns with the qualitative analysis presented in Section III.\nFig. 4 plots the computation and communication overhead for each client per communication round for the three considered federated fine-tuning frameworks. The communication size axis is log-transformed to enhance the visualization of differences across the three frameworks. In FedLLMs, as the LoRA rank increases, communication costs grow proportionally, whereas the increase in computation costs is relatively small. In KD-FedLLMs, both communication and computation costs increase proportionally with the size of the public dataset, and KD-FedLLMs exhibit the highest computational cost among the three approaches. In Split-FedLLMs, communication and computation costs both scale proportionally with the number of training samples per communication round. Split-FedLLMs incur the highest communication overhead among the three frameworks."}, {"title": "VI. CONCLUSION", "content": "This article provided a comprehensive analysis and comparison of three distributed fine-tuning frameworks for LLMs: FedLLMs, KD-FedLLMs, and Split-FedLLMs. Each framework is based on different knowledge-sharing strategies, including the exchange of model parameters, logits, and activations and gradients, respectively, to facilitate collaborative distributed fine-tuning. By evaluating their performance in terms of model accuracy, communication overhead, and client-side computational load, we identified the strengths and limitations of each framework, providing valuable insights to guide the selection of the most suitable framework for diverse federated fine-tuning scenarios. Based on the analysis, several framework-specific optimization opportunities to enhance fine-tuning effectiveness and efficiency were identified. These include adaptive aggregation for heterogeneous clients in FedLLMs, public dataset alignment and pruning in KD-FedLLMs, and dynamic model partitioning with activation compression in Split-FedLLMs. Additionally, broader research directions were discussed, including continual fine-tuning of FedLLMs, real-time human-in-the-loop feedback, and support for multimodal data. These insights pave the way for advancing FedLLMs, enabling them to meet the demands of real-world applications."}]}