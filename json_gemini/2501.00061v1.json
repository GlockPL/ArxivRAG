{"title": "Training-free Heterogeneous Model Merging", "authors": ["Zhengqi Xu", "Han Zheng", "Jie Song", "Li Sun", "Mingli Song"], "abstract": "Model merging has attracted significant attention as a powerful paradigm for model reuse, facilitating the integration of task-specific models into a singular, versatile framework endowed with multifarious capabilities. Previous studies, predominantly utilizing methods such as Weight Average (WA), have shown that model merging can effectively leverage pretrained models without the need for laborious retraining. However, the inherent heterogeneity among models poses a substantial constraint on its applicability, particularly when confronted with discrepancies in model architectures. To overcome this challenge, we propose an innovative model merging framework designed for heterogeneous models, encompassing both depth and width heterogeneity. To address depth heterogeneity, we introduce a layer alignment strategy that harmonizes model layers by segmenting deeper models, treating consecutive layers with similar representations as a cohesive segment, thus enabling the seamless merging of models with differing layer depths. For width heterogeneity, we propose a novel elastic neuron zipping algorithm that projects the weights from models of varying widths onto a common dimensional space, eliminating the need for identical widths. Extensive experiments validate the efficacy of these proposed methods, demonstrating that the merging of structurally heterogeneous models can achieve performance levels comparable to those of homogeneous merging, across both vision and NLP tasks. Our code is publicly available at https://github.com/zju-vipa/training_free_heterogeneous_model_merging.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks have achieved extraordinary success across a spectrum of demanding computer vision and natural language processing tasks, culminating in the development and public release of numerous models, alongside their architectures and pretrained parameters (e.g., Pytorch Hub\u00b9, Hugging Hub\u00b2). These easily accessible models are meticulously fine-tuned for various tasks, offering considerable convenience to practitioner. However, their utility remains constrained to the specific tasks for which they were initially trained. This limitation presents significant challenges in terms of model storage [5], [30] and computational efficiency, particularly as the size of model parameters grows at an unprecedented rate.\nGiven the plethora of well-trained models across diverse tasks, a prominent research direction in recent years has been to combine multiple task-specific models into a single model endowed with broad capabilities, without the burdensome need for an exhaustive retraining phase. The existing body of literature can be broadly categorized into two schools: direct weight average [1], [25], [29] and align-then-average [1], [21], [22]. The former method directly averages the network weights of multiple networks to achieve expanded abilities [9], [29] or to enhance generalization performance [25]. However, such approaches are confined to networks sharing a common segment of the training trajectory (e.g., the same pretrained model), as the pronounced differences in parameter spaces between models trained via entirely disparate trajectories can result in significant performance degradation [7]. To relax this assumption, the latter approaches first align the parameter spaces of the models [1], [22], [23] and then merge the models through weight averaging, relying on the well-established conjecture that most SGD solutions belong to a set whose elements can be permuted such that no performance barrier exists in the linear interpolation between any two permuted elements [4].\nDespite prior research having made notable strides in pretrained model merging devoid of any training, these all presuppose that the pretrained models exist within the ho-mogeneous architecture, thereby constraining their utility in the face of structurally heterogeneous models. To the best of our knowledge, only a few works [16] endeavor to fuse structurally heterogeneous models, but they necessitate a costly retraining phase. The challenge of training-free model merging for heterogeneous models remains largely unexplored, owing to the formidable difficulties posed by model heterogeneity. Specifically, models may differ not only in layer depth but also in layer width, rendering their parameter spaces incompatible for alignment through the element-wise one-to-one mapping employed in existing methods.\nIn this work, we present a pioneering model merging framework designed to tackle the aforementioned challenge, concentrating on two dimensions of architectural heterogeneity: depth heterogeneity and width heterogeneity. Specifically, with respect to depth heterogeneity where the number of layers differs, we observe that adjacent layers of the model often exhibit similar representations [14], and the input and output of consecutive intermediate layers can be substituted with fewer layers, or even a single layer [26]. Accordingly, we introduce a depth-heterogeneous model merging algorithm, which initially partitions the deeper model into multiple segments, with each segment comprising layers exhibiting similar representations. We ensure that the number of segments corresponds to the number of layers in the shallower model, thereby addressing the inconsistency in the number of layers. Regarding width heterogeneity, prior approaches necessitate that the two models share identical widths (i.e., dimensions) in order to establish a one-to-one mapping between the neurons. In contrast, we"}, {"title": "II. RELATED WORK", "content": "Direct weight average. Weight averaging [25] is a widely used model merging technique that constructs the merged model by averaging parameters. Task Arithmetic [9] employs a predefined scaling factor to differentiate the significance of various models. Fisher Merging [17] performs weighted parameter fusion, where the weights are determined using the Fisher information matrix [6]. RegMean [12] adeptly addresses model merging by optimizing a linear regression problem with closed-form solutions. TIES-Merging [28] resolves task conflicts in [9] by pruning low-magnitude parameters, rectifying sign disagreements, and merging parameters with consistent signs in isolation. DARE [29] further mitigates parameter interference from previous approaches by randomly dropping delta parameters and rescaling the remaining ones.\nAlign-then-average. Git Re-Basin [1] and Neuron Align-ment [22] permute models by evaluating the similarity be-tween their weights or activations. REPAIR [13] enhances the precision of Git Re-Basin by calculating the correlation between intermediate layer feature activations and incorporating multiple batch normalization layers into the network. OTFusion [20] introduces a permutation-based approach grounded in optimal transport theory, utilizing the Wasserstein distance, where neuron associations facilitate the one-shot fusion of pre-existing models with identical depths. Several studies [10], [23] extend these methods to accommodate Transformer-based architectures, though substantial performance degradation persists without fine-tuning. Zipit! [21] addresses intra-model merging, aligning all models within the same basin by \"zipping\" redundant features both within and across models. Furthermore, MuDSC [27] proposes the simultaneous alignment of models in both weight and activation spaces."}, {"title": "III. METHODOLOGY", "content": "We first review the methodology for merging models in homogeneous architectures. Consider a model L as a collection of layers $L_i \\in L$, each has a set of parameters (e.g., $W_i$, $b_i$ for a linear layer). The task of merging two models $L^A$ and $L^B$ involves fusing their parameters into a new model $L^*$, such that $L^*$ preserves the accuracy of $L^A$ and $L^B$ on their respective original tasks. When $L^A$ and $L^B$ are fine-tuned from the same checkpoint, several studies [11], [25] have demonstrated that merging them is as straightforward as averaging their weights. For example, if $L_i$ represents a linear layer and $W_i^A, W_i^B \\in \\mathbb{R}^{n_i \\times n_{i-1}}$, where $n_i$ denotes the dimension of the i-th layer, the new weight matrix $W_i^*$ is simply expressed as\n$$\nW_i^* = \\frac{1}{2}W_i^A + \\frac{1}{2}W_i^B\n$$\nHowever, when $L^A$ and $L^B$ are not fine-tuned from the same checkpoint, Eqn. 1 generally yields random accuracy. To address this issue, a body of work [1], [13], [21] has found that permuting the feature space of one model to align with that of the other before averaging significantly recovers lost accuracy. Specifically, following the general framework of prior studies [21], let $P_i^A$ and $P_i^B$ represent the permutation matrices that align the output of layer $L_i^A$ and $L_i^B$ to the same space, with $P_i^A, P_i^B \\in \\mathbb{R}^{n_i \\times n_i}$. For each layer, we can apply\n$$\nW_i^* = P_i^A W_i^A (P_{i-1}^A)^{-1} + P_i^B W_i^B (P_{i-1}^B)^{-1}\n$$\nHere, we permute not only the output space of $W_i^A$ and $W_i^B$, but also their input spaces to reverse the permutation from the previous layer (hence the use of pseudo-inverse matrices $(P_{i-1}^A)^{-1}$ and $(P_{i-1}^B)^{-1}$).\nLet $f_i^A$ and $f_i^B$ represent the feature vectors of the i-th layer for each model, where $f_i^A, f_i^B \\in \\mathbb{R}^{n_i \\times m_i}$, and $m_i$ denotes the feature dimension. The search for optimal $P_i^A$ and $P_i^B$ can be formulated as the following objective:\n$$\n\\arg \\max_{P_i^A, P_i^B} \\sum_i Sim_f(P_i^A f_i^A, P_i^B f_i^B).\n$$\nHere, $Sim_f(\\cdot, \\cdot)$ computes the sum of similarities between features at corresponding indices in the two sets of feature vectors. Cosine similarity is commonly employed as $Sim_f(\\cdot, \\cdot)$.", "B. Depth-heterogeneous Merging": "Eqn. 3 reveals that, for depth-homogeneous models, our optimization goal is to maximize the aggregate feature simi-larities across each layer. However, for depth-heterogeneous models, this formulation is not applicable, as the layers of the two models cannot be directly aligned in a one-to-one manner. Fortunately, prior research has demonstrated that adjacent layers often exhibit similar representations, and the functionality of multiple layers can be effectively replaced by a single independent layer. Drawing inspiration from this, we align the layers of the two models by segmenting the deeper model and treating consecutive layers with analogous representations as a unified segment. Specifically, we assume Model A is deeper than Model B, and partition the layers of Model A into a set of segments $S_A$. Let $f_i^j$ represent the feature from the j-th layer of the i-th segment $S_A^i$ of Model A, and $f_i^B$ denote the"}, {"content": "the features of the i-th layer of Model B. The objective for depth-heterogeneous merging is thus formulated as:\n$$\n\\arg \\max_{P_{ij}^A, P_i^B} \\sum_{j=1}^{|L^B|} \\sum_{i=1}^{|S_A|} Sim (P_{ij}^A f_i^j, P_i^B f_i^B),\n$$\nwhere $P_{ij}^A$ and $P_i^B$ are the reprojection matrices corresponding to the j-th layer of the i-th segment for the Model A and Model B, respectively.\nTo simplify the problem, we reduce it to a two-step optimization process. First, we determine the segments $S_A$ of Model A. Then, within each segment $S_A^i$, we sequentially optimize for $P_{ij}^A$ and $P_i^B$, based on $f_i^j$ and $f_i^B$, respectively. This gives rise to two key questions: 1) How should we merge $S_A^i$ and $L_i^B$? and 2) How do we determine the segments $S_A$?\nFor the first question, to simplify the notation, we assume that we are merging the segment $S_A^i$ of Model A with the layer $L_i^B$ of Model B. Here, $W^A$ and $W^B$ denote the weights of the respective models, while $P_i^A$ and $P_i^B$ refer to the permutation matrices in Eqn. 4. Let x represent the input data. Considering that the feature map $f(x)$ behave similarly to a linear map (up to a scaling factor $\\alpha$) on the line interpolation between $W_i^A$ and $W_i^B$, i.e. $\\alpha f(\\cdot) = \\alpha f(x) \\approx \\alpha W^A + (1 - \\alpha) W^B [31]. We aim to derive a reasonable form of weight averaging from feature averaging for depths-heterogeneous merging. The fused features $f^*$ can be viewed as a synthesis of the features from $S_A^i$ and those from $L_i^B$, i.e.\n$$\nf^* = P_i^A W_i^A (P_{i-1}^A)^{-1} f_{i-1} + P_i^B W_i^B x\n= P_i^A W_i^A (P_{i-1}^A)^{-1} P_{i-1}^A W_{i-1}^A (P_{i-2}^A)^{-1} ... P_2^A W_2^A x\n+ P_i^B P_i^B(P_{i-1}^B)^{-1} P_{i-1}^B (P_{i-2}^B)^{-1} ... P_2^B W_2^B x,\n$$\nwhere I is the identity matrix. The aforementioned factor $\\alpha$ is incorporated into the permutation matrix $P_1^i$. According to the second term of Eqn. 5, $L_i^B$ can be extended as $S_B^i = {L_i^B, L_i^B, ..., L_i^B}$, where $L_i^B = L_i^B$ and $L_i^B (i > 1)$ can be regarded as a layer with a weight of I. Therefore, merging $S_A^i$ and $L_i^B$ can be formulated as merging the layers in $S_A^i$ and $S_B^i$ one by one through weight averaging and thus the weights of merged model are derived as\n$$\nW_i^* = P_i^A W_i^A (P_{i-1}^A)^{-1} + P_i^B W_i^B (P_{i-1}^B)^{-1}\n$$\n$$\nW_i^* = P_i^A W_i^A (P_{i-1}^A)^{-1} + P_i^B I (P_{i-1}^B)^{-1}\n$$\n$$\nW_i^* = P_i^A W_i^A (P_{i-1}^A)^{-1} + P_i^B I (P_{i-1}^B)^{-1}.\n$$\nWe elaborate in the supplementary materials on the approach to merging residual models with heterogeneous architectures.\nFor the second question, the objectives can be sim-plified to finding a set of indices G such that $S_A^i = {L_{G_{i-1}+1}, L_{G_{i-1}+2}, ..., L_{G_i} }$. In the followings, we present two heuristic algorithms for model alignment (Fig. 1): 1) segment-wise model alignment, and 2) layer-wise model alignment.\nSegment-wise Model Alignment (SMA). The goal is to ensure that, after segmentation, the output representation of"}, {"content": "each segment $S_A^i$ is as similar as possible to the representation of the corresponding layer $L_i^B$ in Model B. To this goal, we firstly compute the pairwise similarity between all layers of Model A and Model B, and then we design a matching algorithm to maximize the similarities between $f_i^A$ and $f_i^B$:\n$$\n\\arg \\max_{G} \\sum_{i=1}^{|L^B|} Sim_i(f_i^A, f_i^B).\n$$\nIt is worth noted that, for similartiy function $Sim_i(\\cdot, \\cdot)$, since the $f_i^A$ and $f_i^B$ have not yet been reprojected by the projection matrices, the sum of the similarities of features at corresponding indices cannot be considered as the overall similarity between the feature groups. Therefore, we apply Centered Kernel Alignment (CKA) [14] as a proxy to com-putes the representation similarity between layers because the similarity index is equivalent to CKA, i.e. there is $CKA(f^A, f^B) = CKA(P^A f^A, P^B f^B)$ and thus we are able to measure the similarity between layers prior to alignment.\nLayer-wise Model Alignment (LMA). The previous meth-ods primarily focused on aligning the output features of the $S_A^i$ with the features of $L_i^B$. However, as shown in Eqn. 4, alignment also occurs between the internal features of $S_A^i$ and the features of $L_i^B$. Therefore, we propose an alignment method that maximizes global feature similarity, whose objective can be formulated as following:\n$$\n\\arg \\max_{G} \\sum_{i=1}^{|L^B|} \\sum_{j=i}^{G_i-1} Sim_i(f_i^j, f_i^B).\n$$\nThe pseudocode of the segment- and layer-wise model align-ment algorithms are provided in the supplementary materials.", "C. Width-heterogeneous Merging": "The disparity in width between models is a more prevalent scenario; however, existing methodologies exclusively address the merging of models with identical widths. On one hand, neuron alignment-based techniques [1], [22] necessitate the establishment of a one-to-one correspondence between neurons of equal quantity. On the other hand, a neuron zip-based"}, {"title": "IV. EXPERIMENTS", "content": "Datasets. The experiments are performed on both vision and natural language tasks, encompassing the small-scale CIFAR-10/100 [15], the large-scale ImageNet [2], and the renowned General Language Understanding Evaluation (GLUE) benchmark for natural language comprehension [24].\nModels. We adopt various commonly used model archi-tectures to demonstrate the to illustrate the versatility of the proposed method. For vision tasks, we merge ResNets [8] and VGGs [19] with varying depths and widths. For natural language understanding classification tasks, we investigate Transformer encoder-based masked language models. Specifi-cally, we consider 5 different BERT models, seeds 1 through 5, from the MultiBERTs reproduction [3] and each model has 12 layers. To obtain models with different depths, we repeated the even-numbered layers of each model, extending the depth of the models to 17 layers. For each classification task in GLUE, we fine-tune each of the MultiBERTs models with a randomly initialized classification head,including pooling layer and classification layer weights. We keep the head initializations the same across models.\nEvaluation. For the experiments on CIFAR-10/100 and ImageNet, We randomly partition a classification dataset into two non-overlapping sub-classification tasks, trained respective models for each, and subsequently merged the models into one.\nThen we evaluate performance of merged model with joint accuracy and per-task accuracy. Joint accuracy is the overall accuracy of a model when it is evaluated on all classes within a combined dataset. For per-task accuracy, we provided the accuracy of the merged multi-task model on two individual tasks, along with their average performance. Each model is trained with a CLIP-style loss [18] using CLIP text encodings of the class names as targets. For fair comparisons, we train 3 pairs of models and report the average accuracy. For the experiments on GLUE, we investigate loss barriers between fine-tuned BERT models across 8 different GLUE tasks. We use the loss-barrier defined by Frankle et.al. [7]."}, {"B. Results of Merging Vision Models": "Depth-heterogeneous merging. The results of depth-heterogeneous merging are reported in Tab. I and Tab II. The tables include results for three categories of methods: 1) The average performance of the models trained on Task A or Task B. 2) The average performance of the merged models for each pair of homogeneous models. 3) The average performance of the merged models for each pair of depth-heterogeneous models. The Avg refers to vanilla averaging of weights. The Align(A.) refers to merging model via alignment-based method [1]. The Zip(Z.) refers to merging model via zip-based method [21]. The SMA. refers to aligning depth via segment-wise model alignment. The LMA. refers to aligning depth via layer-wise model alignment. As shown in Tab. I, compared to single-task models, the merged models achieve higher joint accuracy and per-task average accuracy, regardless of whether the merging is depth-homogeneous or depth-heterogeneous. For the proposed depth-heterogeneous merging method, the models merged with depth-heterogeneous architectures not only achieve higher average performance compared to the weight average of depth-homogeneous models but also exhibit similar performance to the models merged with depth-homogeneous architectures. It demonstrates the effectiveness of our proposed depth-heterogeneous merging method. Furthermore, by comparing different depth alignment methods, we find that depth alignment based on LMA often achieves better fusion performance, particularly in terms of joint accuracy and average per-task accuracy. In the Section IV-D, we further illustrate the reasons behind this phenomenon through visual analysis. Additionally, we conduct experiments on ImageNet (as shown in Tab. II) and obtained results similar to those in Tab. I, demonstrating the effectiveness of our method on large-scale datasets.", "Width-heterogeneous merging": "The results for width-heterogeneous merging are reported in Tab. III. We train models with different widths for ResNet26 and ResNet50 and merge models with the same depth but different widths. We report the average performance of single-task models with the same architecture as well as the average performance of the width-heterogeneous merged models. As shown in Tab. III, both the joint accuracy and per-task average accuracy of the models merged using the method proposed in Section III-C significantly outperform the single-task models, demonstrating the feasibility and effectiveness of merging width-heterogeneous models."}, {"C. Results of Merging Language Models": "As shown in Tab. IV, we compare the performance of vanilla averaging, homogeneous model merging, and heterogeneous model merging on the GLUE benchmark. Specifically, we implement the vanilla averaging and homogeneous model merging of BERTs based on the method proposed by Verma et al [23]. Subsequently, we employed layer-wise model alignment to achieve the merging of depth-heterogeneous BERTs. To ensure fairness in the experiments, during homogeneous merging, we merge models finetuned with different random seeds across multiple runs. For heterogeneous merging, we increase the depth of one model in each pair only before finetuning. We investigate the loss barriers and errors of the three methods across 8 tasks. Compared to vanilla averaging, lower loss barriers can be observed for homogeneous model merging and heterogeneous model merging."}, {"D. Visualization Analysis of Depth Alignment": "Here we demonstrate the differences between the depth alignment methods by visualizing the representational similarity between arbitrary layers of two models. We adopt ResNet50 and ResNet26, both trained on CIFAR100. ResNet26 is extended to match the depth of ResNet50 through different alignment strategies. For clarity, we visualize the representations of each residual block rather than individual layers.\nFig. 3a presents the representational similarity between arbi-trary pairs of layers within individual models. Although SMA aims to maximize the consistency between the representations of the segments in the aligned deeper model and the layers in the shallower model, it neglects the alignment between intermediate representations within the segments and the final representations of the shallower model's layers. This oversight can lead to potential misalignments. In contrast, LMA ensures that representational shifts occur at similar locations as in the original model, resulting in more appropriate alignment results."}, {"title": "V. CONCLUSION", "content": "We present a novel model merging framework designed to address the merging of depth-heterogeneous and width-heterogeneous models. Two heuristic approaches, segment-wise model alignment and layer-wise model alignment, achieve depth alignment by partitioning the layers of the deeper model into multiple segments, equal in number to the layers of the shallower model. An elastic neuron zipping technique is proposed for the merging of width-heterogeneous models. Through experimental analysis, we demonstrate that the pro-posed framework for merging heterogeneous models is both feasible and effective across a range of tasks and architectures."}, {"title": "S1. DEEP-HETEROGENEOUS MERGING FOR RESNET", "content": "In this section, we elaborate on the approach to merging residual models when confronted with heterogeneous architec-tures. Assume merging the segment SA = {$L_1^A, L_2^A,...,L_l^A$} of Model A with the layer $L_i^B$ of Model B and all of $L_i^A (i = 1,2,...,l)$ and $L_i^B$ are residual layers which are simply formulated as f = (W + I)x. Following the proof approach in the case of non-residual layers, we derive the form of weight averaging in residual layers through the averaging of features. In this case, the averaged feature $f^*$ is given by:\n$$\nf^* = (P_i^A W_i^A (P_{i-1}^A)^{-1} + I) f_{i-1} + (P_i^B W_i^B + I)x\n$$\n$$\n= (P_i^A W_i^A (P_{i-1}^A)^{-1} + I)(P_{i-1}^A W_{i-1}^A (P_{i-2}^A)^{-1} + I)...\n(P_1^A W_1^A + I)x\n$$\n$$\n+ (P_i^B I(P_{i-1}^B)^{-1} + I)(P_{i-1}^B I(P_{i-2}^B)^{-1} + I)...\n(P_1^B W_1^B + I)x,\n$$\nwhere 0 and I are zero matrix and identity matrix respec-tively. It is noted that we can still extend $L_i^B$ to $S_B = {$L_i^B, L_i^B,..., L_i^B$}$, where $L_i^B = L_i^B$ but $L_i^B (i > 1)$ represents a residual layer with weights set as 0. This is because residual layers can directly pass features to the next layer via the shortcut connection. Then the weights of merged model are expressed as:\n$$\nW_i^* = P_i^A W_i^A (P_{i-1}^A)^{-1} + P_i^B W_i^B (P_{i-1}^B)^{-1}\n$$\n$$\nW_i^* = P_i^A W_i^A (P_{i-1}^A)^{-1} + P_i^B 0 (P_{i-1}^B)^{-1}\n$$\n$$\nW_i^* = P_i^A W_i^A (P_{i-1}^A)^{-1}.\n$$\n$$\nW_i^* = P_i^A W_i^A (P_{i-1}^A)^{-1} + P_i^B 0 (P_{i-1}^B)^{-1}\n$$\n$$\nW_i^* = P_i^A W_i^A (P_{i-1}^A)^{-1}.\n$$"}, {"title": "S2. PSEUDOCODE", "content": "The pseudocode of the segment- and layer-wise model align-ment algorithms are provided in Algorithm 1 and Algorithm 2 respectively."}, {"title": "S3. MORE EXPERIMENTAL DETAILS", "content": "A. Calculating Loss Barrier\nTo evaluate the performance on GLUE, we use the loss-barrier defined by Frankle et.al. [1] which is defined as the maximum difference between the loss of an interpolation and the average loss of the base models:\n$$\n\\max_{\\lambda} L(\\lambda \\theta_A + (1 - \\lambda)\\theta_B) - \\frac{1}{2}(L(\\theta_A) + L(\\theta_B)),\n$$\nwhere we compute several interpolations of $\\theta_A$ and $\\theta_B$, as $\\lambda \\theta_A + (1 - \\lambda) \\theta_B$, and we use 21 samples evenly spaced between $\\lambda$ = 0 and $\\lambda$ = 1."}]}