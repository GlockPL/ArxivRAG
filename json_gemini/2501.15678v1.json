{"title": "Blissful (A)Ignorance:", "authors": ["Jiaqi Zhu", "Andras Molnar"], "abstract": "As the use of Generative AI (GenAI) tools becomes more prevalent in interpersonal communication, understanding their impact on social perceptions is crucial. According to signaling theory, GenAI may undermine the credibility of social signals conveyed in writing, since it reduces the cost of writing and makes it hard to verify the authenticity of messages. Using a pre-registered large-scale online experiment (N = 647; Prolific), featuring scenarios in a range of communication contexts (personal vs. professional; close others vs. strangers), we explored how senders' use of GenAI influenced recipients' impressions of senders, both when GenAI use was known or uncertain. Consistent with past work, we found strong negative effects on social impressions when disclosing that a message was AI-generated, compared to when the same message was human-written. However, under the more realistic condition when potential GenAI use was not explicitly highlighted, recipients did not exhibit any skepticism towards senders, and these \u201cuninformed\u201d impressions were virtually indistinguishable from those of fully human-written messages. Even when we highlighted the potential (but uncertain) use of GenAI, recipients formed overly positive impressions. These results are especially striking given that 46% of our sample admitted having used such tools for writing messages, just within the past two weeks. Our findings put past work in a new light: While social judgments can be substantially affected when GenAI use is explicitly disclosed, this information may not be readily available in more realistic communication settings, making recipients blissfully ignorant about others' potential use of GenAI.", "sections": [{"title": "1. Introduction", "content": "According to a nationally representative survey conducted in August 2024, about 39% of the U.S. population aged 18-64 is already using Generative AI, most commonly ChatGPT, with 10.6% reporting daily usage at work (Bick et al., 2024). As Bick et al. (2024) highlight, the adoption of Generative AI has been quicker than that of personal computers or the Internet. This rapid adoption has been widely documented across age groups, disciplines, and occupations, including but not limited to office workers (Humlum & Vestergaard, 2024), middle and high school students (Zhu et al., 2024), STEM researchers (Van Noorden & Perkel, 2023), humanities researchers (Dedema & Ma, 2024), medical students (Zhang et al., 2024), and medical practitioners (Blease et al., 2024).\nIn addition to the widespread adoption of Generative AI for personal and professional use, these technologies also have the potential to transform interpersonal interactions. Generative AI tools empower users with the ability to generate high-quality, complex, personalized, and contextually relevant written content with minimal effort. They have the capacity to enhance the efficiency and quality of social interactions, for example, by boosting the use of positive emotional language (Hohenstein et al., 2023).\nAt the same time, suspected AI-use may have adverse effects on social relationships (e.g., Glikson & Asscher, 2023; Hohenstein et al., 2023; Lim et al., 2025; Weiss et al., 2022), and instead of building social connections and facilitating interactions, these emerging technologies may erect new barriers to human-to-human cooperation and coordination. Wojtowicz and DeDeo (2025) discuss these negative effects by highlighting how Generative AI may undermine \u201cmental proofs.\" Mental proofs are observable actions that allow audiences to make inferences about the communicator's hidden mental states such as their intentions, goals, and values. As Wojtowich and DeDeo (2025) argue, outsourcing the laborious process of writing to an algorithm\u2014thereby making social interactions more efficient and less effortful-may lead to the paradoxical consequence of undermining trust and coordination between people, because it weakens the link between what people communicate and what people actually think, feel, or want.\nBut are people actually becoming more skeptical towards others when judging them based on their written messages? Do people respond to the increasing prevalence of Generative AI use by evaluating written content more critically, or do they remain blissfully ignorant to the"}, {"title": "1.1. Signaling theory: writing as a source of credible social signals", "content": "Written communication is a rich source of social signals, as every choice a writer makes-words, tone, and sentence structure-reflects the underlying characteristics of the individual (e.g., Mairesse et al., 2007; Pennebaker et al., 2003). These signals enable readers to assess the personal traits of the writer, such as trustworthiness, thoughtfulness, diligence, and kindness, or conversely, deceitfulness, superficiality, laziness, and antagonism. Such inferences allow people to decide whether to cooperate, compete, approach, or avoid others. For example, cover letters help employers choose which candidates to interview; dating profiles let singles decide whether they want to meet a potential date; apology letters may determine whether someone is forgiving or holding onto a grudge; and academic publications may serve as the primary avenue for researchers to showcase their intellect, creativity, and effort.\nHowever, signals can be faked, especially when communicators have a strategic interest in deceiving or manipulating their audience. A job candidate may ask a seasoned colleague to write a cover letter on their behalf; an apology letter may be based on a template; and an academic publication may plagiarize past work. According to signaling theory (see, e.g., Connelly et al., 2011; Spence 1973, 2002), the usefulness of signals\u2014any signal, not just in written communication\u2014critically depends on the credibility of signaling, that is, how likely a signal is honest or fake. A core prediction of signaling theory is that as the cost of signaling increases which can be time, effort, emotional cost, or even reputation\u2014it becomes more credible (Spence 1973, 2002). Therefore, observers are constantly monitoring costs to gauge the credibility of signals (Gintis et al., 2001; also see \u201cstrategic vigilance\", Heintz et al., 2016). As Chaudhry & Wald (2022) highlight, three qualities can make an observable signal to be perceived as costly: 1) it is difficult-to-fake; 2) verifiable; and 3) self-sacrificing. For example, a handwritten note of apology is a more credible signal of true remorse than a text message, as hand-writing is more difficult-to-fake, easier to verify, and takes more effort and time to produce (i.e., requires more \u201cself-sacrifice\u201d) than texting someone.\""}, {"title": "1.2. How Generative AI may undermine the social signaling function of written messages", "content": "Generative Al may undermine all three of these qualities of written signals, substantially reducing their perceived cost and credibility. First, these tools offer users unprecedented flexibility in generating any type of written content, editing style and grammar, and tailoring messages to all sorts of audiences, thereby making messages extremely easy to fake. To make matters worse, people are largely incapable of telling the difference between AI-generated and human-created content (e.g., Jakesch et al., 2023; K\u00f6bis & Mossink, 2021; Kreps et al., 2022; Porter & Machery, 2024). Second, AI-generated content is difficult, if not impossible, to verify. Unlike more \u201ctraditional\u201d forms of fake signals in written communication\u2014such as plagiarism or ghostwriting-which can be effectively identified as fake, we generally lack the tools (either algorithmic or heuristic) to reliably determine if, and to what extent, Generative AI was involved in writing a message (Sadasivan et al., 2023; Tang et al., 2023). Detecting AI-generated content is especially challenging when the text is a mix of AI-generated and human-written content (\u201cmixtext\u201d, see Gao et al., 2024). Finally, these technologies have significantly reduced the cost of writing messages\u2014not only in terms of direct costs incurred by the communicator (time and effort), but also in terms of overall energy consumption. According to Tomlinson et al. (2024)'s estimates, tools like ChatGPT have made writing so effortless and efficient that these systems emit 130-1500 times less CO2/page of text than human writers.\nThere is already some empirical evidence that documents the adverse effect of Generative Al on social impressions and interactions. For example, Glikson & Asscher (2023) investigated the effectiveness of apologies, depending on whether the communicator used AI while writing an apology message. Participants rated the apology written with the help of AI as less authentic, less sincere, and as a result, were less likely to forgive the communicator. These negative effects were mitigated by the limited use of AI (i.e., when used for translation and/or correcting grammatical mistakes only). Lim et al. (2025) found similar negative effects on the perceived sincerity of corporate apologies. In their experiment, participants read about a fictitious company issuing an apology for a data breach incident. The authors found that when the company's press release was \u201cgenerated with the assistance of ChatGPT\u201d, participants rated these statements as less sincere and less empathetic, and consequently, reported lower willingness to forgive the incident. Other related work looked at social impressions more broadly. For example, Weiss et al. (2022) found that the disclosed use of AI in job applicants' messages negatively affected their"}, {"title": "1.3. The role of uncertainty and attention in social judgments", "content": "There are, however, at least two crucial differences between the stylized interactions studied in this past line of research and real situations. First, people in real situations almost never know with certainty whether (and to what extent) a communicator has used Generative AI. Assuming that communicators are aware of the potential negative consequences of disclosing their use of AI, they are motivated to conceal all AI involvement. This rational strategy of concealing AI use, combined with the difficulty of detecting and distinguishing AI-generated content (Jakesch et al., 2023; K\u00f6bis & Mossink, 2021; Kreps et al., 2022), forces audiences to rely on probabilistic inferences instead. That is, in most situations, people must form impressions of others and make decisions under the fundamental uncertainty of not knowing whether the communicator has used Generative AI, let alone, to what extent.\nOne goal of this paper is to study such judgments under uncertainty: when people are aware of the possibility that a message might have been generated by AI but have no way of knowing this. According to a simple decision-theoretical model, social impressions under uncertainty, Iu, are the probability-weighted combination of impressions under certainty:\n$I_U = P_H I_H + P_{AI} I_{AI}$\nwhere IH is the social impression of someone who is known to have written a message entirely on their own, IAI is the impression of someone who is known to have generated a message entirely by using AI, while PH and PAI are the corresponding probabilities ($P_{AI} = 1 \u2212 P_H$). Assuming that IH > IAI (aligned with prior empirical evidence), the above model implies that social impressions would turn increasingly more negative (positive) as people would become more (less) suspicious of others, as a function of Pa\u03b9\u00b7"}, {"title": "1.4. Research design and hypotheses", "content": "We investigate whether social impressions are affected by the possibility that a message was generated by AI, both when such possibility is implicit (not highlighted to participants) and explicitly highlighted in the decision-context. We also compare impressions in these fundamentally uncertain situations to fully informed impressions, that is, when participants know whether a message was written by a person or generated by AI. This approach not only allows us to investigate how the presence of uncertainty affects impressions but also to examine whether people have become more critical towards written content, as a natural response to the proliferation of Generative AI tools in human-to-human communication. Our results can shed light on whether people have already developed such natural skepticism or remain blissfully ignorant and largely fail to consider the possibility that written messages may be AI-generated.\nTo investigate these, we designed four hypothetical scenarios to cover a range of possible situations (e.g., in personal vs. workplace settings; between close others vs. strangers), in which participants received an email from someone else (see Section 2.3 and Appendix for details). Across four experimental conditions, we then manipulated the type of information participants received about the origin of this email. In the two certain conditions, we told participants that this message was either fully generated by AI or fully written by the sender. In the two other conditions, we either provided no information about the possibility of AI involvement or told participants that they are uncertain about the origin of the message (i.e., highlighted the possibility of AI involvement but didn't provide any specific information). We then measured participants' social impressions of the sender.\nBased on past work (e.g., Glikson & Asscher, 2023; Hohenstein et al., 2023; Lim et al., 2025; Weiss et al., 2022), we first hypothesized that fully informed participants would judge the sender of the message more negatively if they knew that the sender used AI:\nH1. People form more negative social impressions of someone known to have used Generative Al in writing a message, compared to someone known to have written the same message entirely on their own, without the help of Generative AI."}, {"title": "2. Methods", "content": "We report how we determined our sample size (see Section 2.2), all data exclusions (if any), all manipulations, and all measures, and the studies follow JARS (Appelbaum et al., 2018).\nAll data, analysis code, and research materials are publicly available at OSF:\nhttps://osf.io/xrpy9/?view_only=ba3a2e85211140fdb005ed21de92b2a7. We analyzed data using\nR, version 4.4.2 (R Core Team, 2024). We pre-registered the experiment on AsPredicted.org:\nhttps://aspredicted.org/38jb-s8dk.pdf. The study was reviewed and approved by the Institutional\nReview Board at [name of institution and IRB reference number anonymized for peer review].\nAll research was performed in accordance with relevant guidelines and regulations, and informed consent was obtained from all participants."}, {"title": "2.2. Sample", "content": "A priori power analysis using the G*Power 3.1.9.7 computer program (Faul et al., 2009) indicated that a sample of 164 participants per condition (pooled across scenarios) would be needed to detect a medium effect size (d = 0.40) with 95% power between any two conditions, using pairwise between-participants (two-tailed) t-tests. Based on this power analysis, we aimed to collect 160 responses per condition (i.e., 640 total), after data exclusions. Since each condition featured four different scenarios, we aimed to collect 40 responses in each scenario within each condition. We stopped data collection as soon as we reached these target sample sizes, after applying data exclusions. The data stopping rule was pre-registered."}, {"title": "2.3. Procedure", "content": "We directed participants to a Qualtrics survey titled \u201cSocial Perceptions in Interpersonal Communication\u201d and asked them to imagine a hypothetical scenario. Crucially, we did not tell participants in the instructions (or during recruitment) that this study would be about Generative Al to avoid prompting them to think about this technology. In the scenario, we presented participants with a brief background story and then told them that another person, Alex, had sent them an email. To capture a broad range of potential situations in which someone may be judged based on their email, we implemented a stimulus sampling method (see Wells & Windschitl, 1999). We randomly assigned participants to imagine one of the following four scenarios:\n1) receiving a gratitude message from a close friend (\u201cgratitude\");\n2) receiving an application from a candidate who is applying for a nanny position (\u201cnanny\u201d);\n3) receiving a cover letter from a job applicant for a data analyst position (\u201ccover letter\u201d); or\n4) receiving feedback on a marketing project (\u201cfeedback\u201d).\nWe generated these messages using ChatGPT-40 and minimally edited them to have about the same length across contexts (M = 232 words; range: 217-243 words). We report the full text of these scenarios in the Appendix. Across the scenarios, we varied whether the sender, Alex, was someone who is socially close to the participant (i.e., their friend in the gratitude scenario or their colleague in the feedback scenario) or socially distant (i.e., a stranger in the nanny and cover letter scenarios), and whether the context was more personal (i.e., gratitude and nanny scenarios) as opposed to professional (i.e., cover letter and feedback scenarios).\nAfter participants read the message sent by Alex, we implemented the main experimental manipulation: Across four conditions, we varied what participants knew about how Alex created the email. In the \u201cno information\u201d condition, that we designed to most closely resemble naturalistic communication settings, we didn't provide any further information, and participants simply proceeded to evaluate Alex based on their message.\nIn the other three conditions, we first asked participants to read a brief description of Generative AI chatbots\u2014what these tools are and what they can be used for (see the Appendix) then we informed participants whether Alex used a Generative AI chatbot to create their message.\""}, {"title": "2.4. Measures", "content": "Our main dependent measure was participants' social impression of Alex, the sender. We quantified this by asking participants to rate Alex along the following 10 dimensions of personal traits: friendly, sincere, polite, authentic, trustworthy, intelligent, skilled, creative, diligent, and thoughtful. Participants indicated their responses using 7-point Likert scales from Not at all (1) to Very (7). As pre-registered, we standardized each of these 10 measures within each scenario (to account for potential differences between scenarios) and then created a composite measure of"}, {"title": "3. Results", "content": null}, {"title": "3.1. Actual (self-reported) use of Generative AI chatbots", "content": "Before testing our main hypothesis and addressing our primary research questions, we investigated participants' self-reported frequency of using Generative AI chatbots. The majority (72.0%) reported that they have used Generative AI chatbots within the past two weeks (M = 3.76 days; median = 2 days).4 About a fifth of the total sample (22.1%) have used such tools at least every other day (7 or more days), with 4% using these tools every single day (see Figure 1). This indicates that as of November 2024, the use of Generative AI chatbots is already quite widespread among the general population in the United States, consistent with the findings of Bick et al. (2024)."}, {"title": "3.2. Who is using Generative AI chatbots for writing messages?", "content": "To better understand participants' use of Generative AI chatbots\u2014especially when used for writing messages to others\u2014we investigated whether the self-reported use was associated with the demographic factors we collected (i.e., age, gender, income, education, self-reported expertise/familiarity with Generative AI tools).5 We found that self-reported use of Generative AI chatbots (for any purpose) was significantly positively correlated with participants' self-reported expertise/familiarity with these tools, r(646) = .527, p < .001, and significantly negatively correlated with participants' age, r(646) = \u2212.107, p = .006. We also observed a significant positive relationship between chatbot use and education level, r(645) = .087, p = .027. Participants' gender and income did not significantly correlate with their self-reported use of Generative AI chatbots, both p \u2265 .280.\nSimilarly, we found that self-reported use of Generative AI chatbots for generating messages was significantly positively correlated with participants expertise/familiarity with these tools, r(646) = .362, p < .001, and significantly negatively correlated with participants' age,\nr(646) = \u2212.105, p = .007. We also observed a significant positive relationship between chatbot use for writing messages and education level, r(645) = .184, p < .001. We did not find a significant correlation between self-reported use for writing text and gender, p = .192, however, interestingly, we found a significant positive correlation with income, r(631) = .110, p = .006. This suggests that while higher income participants were not more likely to use Generative AI chatbots in general, they were significantly more likely to report using such tools for writing messages to others. For example, participants who reported household incomes over $100,000 within the past year used these tools to write messages 48.5% more often (M = 2.78 days), compared to participants who reported incomes below $50,000 (M= 1.87 days), even though their overall use of Generative AI chatbots was only 7.8% higher (M = 3.81 days vs. M = 3.54 days; see Figure 3)."}, {"title": "3.3. Perceptions of others' use of Generative AI chatbots for writing messages", "content": "While we found that a substantial proportion of the sample reported using Generative Al chatbots for writing messages to others, our results suggest that people consistently underestimate how widespread the use of these tools is when it comes to writing and editing messages. Participants on average believed that only 42.4% (SD = 22.1%) of the U.S. general population has ever used such tools for writing or editing messages sent to others, which is significantly lower than the proportion of the present sample who self-reported doing so just within the past two weeks (45.7%), t(646) = 3.86, p < .001, d = 0.15, 95% CI [40.69, 44.10].\nPeople also underestimated the proportion of the population who are using these tools relatively frequently (at least on a weekly basis) to write or edit their messages: On average, participants thought that only 31.6% (SD = 24.4%) of the U.S. population is doing so, which is significantly less than the proportion we observed in the current sample (37.9%), t(646) = 6.49, p < .001, d = 0.26, 95% CI [29.76, 33.53]."}, {"title": "3.4. Main results: Social impressions", "content": "We conducted independent samples (two-tailed) t-tests to compare the overall social impressions across conditions. Confirming our hypothesis (H1), participants in the human condition evaluated the sender significantly more positively (M = 0.39) than participants in the AI condition (M = \u22120.94), t(245) = 15.28, p < .001, d = 1.69, 95% CI [1.16, 1.51], see Figure 5. Participants also evaluated the sender significantly more positively in the no information condition (M = 0.43) and the uncertain condition (M = 0.13) compared to the AI condition (RQ2A and RQ2B, respectively), t(234) = 15.93, p < .001, d = 1.76, 95% CI [1.20, 1.54] and t(284) = 11.52, p < .001, d = 1.28, 95% CI [0.89, 1.26], respectively. Overall impressions were also significantly more positive in the human condition, compared to the uncertain condition (RQ3B), t(303) = 3.91, p < .001, d = 0.43, 95% CI [0.13, 0.39]."}, {"title": "3.5. Exploratory sentiment analysis of open-ended first impressions", "content": "Participants in the human condition wrote overwhelmingly positive descriptions of the sender (e.g., \"I think Alex seems very professional, well-prepared, and eloquent\u201d or \u201cAlex seems to be a kind, appreciative, and thoughtful person. Their message is heartful, showing genuine gratitude and warmth\"). By contrast, in the AI conditions, these first impressions took a noticeably more negative tone (e.g., \u201cI would find Alex to be lazy and insincere\u201d or \u201cI would be very upset and hurt that Alex didn't even take the time to thank me himself after everything I did for him, and was so lazy he used AI\u201d).\nTo quantify these differences in first impressions and to compare the average sentiments across conditions, we performed lexicon-based sentiment analysis at the word level using the Syuzhet package in R (Jockers, 2017). In this package, sentiment is determined by counting the total number of negative and positive words in a corpus of text, identified by the lexicon (i.e., the sentiment score of a given text is the number of positive words minus the number of negative words). After we obtained the sentiment scores for each response, we compared the average sentiments (i.e., net valence) across conditions, using independent samples (two-tailed) t-tests."}, {"title": "4. Discussion", "content": "Consistent with earlier work (Glikson & Asscher, 2023; Hohenstein et al., 2023; Lim et al., 2025; Weiss et al., 2022), we found strong negative effects on social impressions of the sender when disclosing that a message was generated by AI, compared to when the same message was believed to be written by a human. We detected these effects across four distinct communication scenarios (personal vs. professional; close others vs. strangers) and in both participants' explicit numeric ratings of the sender (i.e., overall social impression scale) and their implicit sentiments towards the sender (i.e., net valence of their open-ended impressions).\nHowever, when the involvement of Generative AI was less than certain, participants formed overly positive impressions of the sender. In particular, when we did not even highlight the possibility that the message was generated by AI, participants' impressions of the sender were virtually indistinguishable from the impressions they formed when they knew that the message was fully written by a human. Interestingly, we found significant differences in the explicit ratings between the uninformed and uncertain conditions, which suggests that participants, by default, did not even consider the possibility of AI involvement, but adjusted their impressions once this possibility was highlighted. Crucially, we did not tell participants any definite information about the origin of the message in either of these conditions, so the observed differences can only be explained by different levels of attention to AI involvement. Similarly to earlier work on the link between social judgments of behavior and attention to others' vaccination status during the COVID-19 pandemic (Molnar et al., 2023), here we document that social judgments can be significantly affected by shifting people's attention to decision-relevant information that otherwise does not readily come to mind.\nAt the same time, even when we prompted participants to consider the possibility that a message was fully generated by AI, the resulting social impressions were much closer to those of human-written messages, as opposed to AI-generated messages. In addition, participants seem to underestimate the prevalence of Generative AI chatbot use, especially among their close peers, even though almost half of the current study sample admitted using such tools for writing messages to others, just within the past two weeks. In the following sections, we discuss some potential alternative explanations for these results. Exploring which of these mechanisms is responsible for the main effects observed in our experiment is beyond the scope of the current paper but could be a suitable topic for future research."}, {"title": "4.1. Lack of knowledge and experience with Generative AI", "content": "The first possibility is that people may be unaware of the capabilities of these emerging technologies and simply would not think that Generative AI can produce messages of this quality (even though we generated these messages using ChatGPT-40 and only minimally edited them for length and consistency across scenarios). Note, however, that we did not find any significant correlations between participants' self-reported AI expertise and familiarity with these tools and their social impressions (see Supplementary B), which suggests that the lack of knowledge may not be the main culprit here. In addition, even if people are fully aware of what these technologies are capable of, they often cannot distinguish between human-written and AI-generated content (K\u00f6bis & Mossink, 2021; Kreps et al., 2022) and may rate the AI-generated content as \u201cmore human than human\u201d (Jakesch et al., 2023; Porter & Machery, 2024). Finally, due to the very recent emergence of these tools (ChatGPT was initially released in November 2022: less than two years before we conducted this experiment), most people may simply lack the first-hand experience of receiving an authentic-looking message that later turns out to be AI-generated. As the rich literature on the description-experience gap in risky decision-making (e.g., Haines et al., 2023; Hertwig & Erev, 2009; Wulff et al., 2018) highlights, people may react to indirect, abstract information (such as reading about a scenario in an online study, as in our experiment) very differently to direct information that they obtain through personal experience (e.g., if people would suspect one of their actual colleagues to have generated an email). For example, Chen et al. (2018) demonstrated that participants became better calibrated in trusting a phishing detection system only when they got direct experience along with feedback, rather than merely reading descriptions of such a system. Similarly, it is possible that people would become better calibrated (i.e., form more negative impressions of others when they don't know whether others used Generative AI) after being personally exposed to such situations in real life."}, {"title": "4.2. Asymmetric costs of social errors", "content": "When we calculated the implied probability in the uncertain condition that the message was written by Generative AI (cf. Equation 2), we relied on the assumption that participants would follow the logic of expected value: that the impression formed under uncertainty is the linear additive combination of impressions made under certainty (AI and human conditions), weighted by the corresponding probabilities. However, this may not be a valid assumption."}, {"title": "4.3. Motivated cognition", "content": "Finally, it is possible that people are motivated to believe that others are less likely to use Generative AI chatbots when writing messages, compared to the actual prevalence of this behavior, and this may be reflected in their overly positive impressions in the uninformed and uncertain conditions. People are often motivated to form and maintain overly optimistic beliefs that make them feel good about themselves, other people, and the world in general (see, e.g., B\u00e9nabou & Tirole, 2016; Epley & Gilovich, 2016; Molnar & Loewenstein, 2021). When these motives are strong enough, people are even willing to sacrifice the accuracy of their judgments, in order to increase the valence of their beliefs (Golman et al., 2017, 2022; Sharot & Sunstein, 2020; Sweeny et al., 2010). People may experience stronger AI anxiety (Johnson & Verdicchio 2017; Li & Huang, 2020) if they believe that the use of Generative AI chatbots is widespread, or they may feel disappointed, angry, and disconnected when they suspect others of using such tools for writing messages, especially their close others. To mitigate these negative emotions, people may adopt overly optimistic beliefs about others' use of Generative AI chatbots. In the present study, we do find suggestive evidence for such a mechanism when comparing participants' belief about the use of Generative AI chatbots among their close social peers, as opposed to the general U.S. population: Participants expected substantially lower prevalence among people they knew well compared to the population at large. This is consistent with the motive to think more positively about socially relevant close others, as opposed to strangers."}, {"title": "5. Conclusion", "content": "We found that despite the relatively widespread use of Generative AI in writing or editing messages, participants did not exhibit any skepticism towards senders under naturalistic evaluation conditions, when the possibility of AI use was not explicitly highlighted. Under these realistic conditions, participants' impressions of senders were virtually indistinguishable from impressions of senders who were known to have written their messages on their own, without using Generative AI. Even when we highlighted the potential use of Generative AI, participants formed overly positive impressions of senders, especially in their open-ended first impressions. Notably, when participants were certain that the sender used Generative AI, their impressions were substantially more critical than in any other condition, so the overly positive judgments in the uninformed and uncertain conditions cannot be explained by participants' overall indifference towards the use of Generative AI. Instead, our results suggest that while judgments can be substantially affected by information about Generative AI use, this information is not readily available and not top of mind in more realistic communication settings.\nThese findings put existing work in a new light by challenging the prevailing assumptions about the negative impact of AI on social perceptions. Previous studies have documented adverse effects of AI involvement in communication, such as reduced authenticity and sincerity in apologies and other messages. However, our research suggests that when the source of a message is uncertain, individuals may overlook the potential downsides of AI and default to more favorable evaluations. This indicates a nuanced understanding of how uncertainty and attention to AI involvement can shape social judgments, highlighting the need for further exploration into the conditions under which skepticism may arise.\nLooking ahead, it is essential to consider how these dynamics may evolve over time. As awareness of Generative AI's capabilities and its implications for communication increases, individuals may begin to adopt a more skeptical perspective. Furthermore, long-term effects of widespread use of Generative AI could lead to a shift in social perceptions, prompting individuals to critically evaluate the authenticity and credibility of messages they receive. New technological advances (e.g., more efficient detection, watermarking, etc.) and regulatory policies may also influence public perception and trust toward written communication. Future research should explore these potential changes in attitudes and beliefs, particularly as society becomes more accustomed to the integration of AI technologies in everyday communication."}]}