{"title": "YOU ONLY USE REACTIVE ATTENTION SLICE FOR LONG CONTEXT RETRIEVAL", "authors": ["Yun Joon Soh", "Hanxian Huang", "Yuandong Tian", "Jishen Zhao"], "abstract": "Supporting longer context for Large Language Models (LLM) is a promising direction to advance LLMs. As training a model for a longer context window is computationally expensive, many alternative solutions, such as Retrieval Augmented Generation (RAG), have been used. However, most existing RAG methods adopt embedding-based retrieval that falls short on long contexts.\nTo address such challenges, we propose an attention-based retrieval technique, You Only Use Reactive Attention slice (YOURA). YOURA leverages a novel retrieval heuristic called reaction score to rank the relevance of each sentence in the input context with the query sentence. Intuitively, we measure how the per-token attention score \"reacts\" to the query and greedily retrieves the most reactive sentences. Internally, YOURA generates a token-indexed vector (called reaction vector) for the whole input context. To map each sentence to the token-indexed vector, we propose an Embedding-Agnostic Sentence Yield (EASY), a best-effort token wiggling algorithm.\nWe evaluate our retrieval technique on three open-source pre-trained LLM models across six LongBench QA datasets. Our technique achieves up to 30% vLLM inference throughput improvement for serving long-context queries with a nearly identical quality score to the simple yet effective truncate-middle approach (Bai et al., 2023).", "sections": [{"title": "INTRODUCTION", "content": "Pre-trained Large Language Models (LLM) are widely explored for various Natural Language Processing (NLP) tasks. To enhance the model for more complex tasks, researchers seek ways to extend the LLM's context window size via fine-tuning the pre-trained model (Chen et al., 2023b; Mangrulkar et al., 2022), proposing advanced attention mechanisms (Xiao et al., 2024; Zhang et al., 2023), and improving Retrieval Augmented Generation (RAG). However, fine-tuning requires additional computational power for each pre-trained model and is often targeted for specific tasks or datasets. Advanced attention mechanisms conceptually extend the context window by selectively choosing a subsequence of tokens for attention but are susceptible to details due to the nature of dropping information at the token level. RAG splits the contextual text into smaller chunks and retrieves a subset of chunks before inferencing based on the semantical distance to the query in the embedding vector space. RAG's retrieval nature has many benefits; it reduces the input size to process, which reduces the computation during inference (Luo et al., 2024) and improves the accuracy by eliminating distracting information (Catav, 2023).\nA high-quality retrieval is crucial for RAG but often suffers from lower accuracy because (1) the common words in both the query and text chunk put the semantic distance closer and (2) the semantically identical, but alphabetically different words distance the query and text chunk in the embedding vector space. For example, the similarity distance between a query (\u201cWhere is Mary?\u201d) and a sentence (\"Her name is Mary\") may be closer to a sentence (\"She is at home\") resulting in suboptimal retrieval.\nWe propose a fine-tuning-free, attention-based retrieval strategy, which can be applied to various off-the-shelf pre-trained LLMs to achieve improved text generation quality. Our key insight is that the attention in a pre-trained model already contains information about which tokens are important for the given query. Figure 2 illustrates the difference between the embedding-based retrieval and our proposal; (a) is a typical embedding-based retrieval where embedding vectors for both the query and context chunk list are generated through an embedding layer. Figure 2 (b) is another simple yet effective alternative to the embedding distance-based approach, truncate-middle. Since the introduction and conclusion of most documents contain the most important information, the truncation approach naively takes out the middle of the context so that the remaining text fits within the LLM's context window size. Unlike embedding-based retrieval or truncation, we propose an attention-based retrieval.\nOur proposed technique, You Only Use Reactive Attention slice (YOURA) Figure 2 (c), is a novel attention-based retrieval technique. Anonlogically speaking, we use the attention vector generated without query as the baseline and measure how each token \"reacts\" to the query. We refer to the \u201creactiveness\u201d as Reaction Vector, an absolute difference in attention scores calculated with and without the query. The per-sentence average of the reaction vector called reaction score, is a good ranking metric when retrieving relevant sentences, as we show in our evaluation.\nProperly slicing the reaction vector into sentences is crucial because YOURA splits the token sequence into chunks (as opposed to prior works, which split the raw text into chunks). Such a difference sparks a new challenge: mapping sentences to a token sequence. Merely associating specific token values (e.g., punctuation token) to a sentence boundary is insufficient because different tokenizer uses different vocabulary. For example, a tokenizer may encode .The (a sentence boundary with a missing white space) as two tokens: [ .T, he ]. In such case, .T token becomes part of the two consecutive sentences. Treating the token before or after .T token as a sentence boundary would easily garble the remaining sentence splits. For this challenge, we propose the Embedding-Agnostic Sentence Yield (EASY) \u00b7 a best-effort token wiggling algorithm that takes a list of raw sentences and a single token sequence as input and"}, {"title": "RELATED WORK", "content": "Long-context Language Models. Training LLMs on sequences with a maximum length while still ensuring they infer well on sequences longer than the maximum is challenging. Previous studies proposed techniques such as positional interpolation (Chen et al., 2023a; Li et al., 2023), positional extrapolation (Press et al., 2021), external memory (Wu et al., 2020; Martins et al., 2021), memory-retrieval augmentation strategies (Mohtashami & Jaggi, 2023; Wang et al., 2024) to push the limit of the context window size to 2 million and even longer (Ding et al., 2024; Bertsch et al., 2024). Although these works improved model accuracy on various tasks, the large context window size implies more computational and memory costs.\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) retrieves relevant document chunks from the external knowledge base. Prior works integrate RAG with language models for question an- swering with long documents (Stelmakh et al., 2022)"}, {"title": "YOU ONLY USE REACTIVE ATTENTION (YOURA)", "content": "Limitation of Embedding Vectors - Context-Unaware\nThe limitation of embedding-based retrieval is that the input texts are the sole variable when generating the embedding vectors. For example, consider the following two paragraphs each with two sentences: \"Her name is Mary. She is at home.\" and \"Her name is Alice. She is at home.\". In both cases, \"She is at home\" is embedded into the identical vector, resulting in equal distance to any sentence.\nAttention-Based Retrieval \u2014 Context-Aware We propose an attention-based heuristic for context-aware retrieval. At a high level, our proposal is measuring how the query sentence impacts the language model's attention distribution. We observe that attention values of relevant information change more drastically than irrelevant information. From this observation, we introduce reaction vector to quantify the attention shift caused by the query sentence."}, {"title": "Problem Statement & Definitions", "content": "Annotations We annotate $Z^C \\in \\mathbb{R}^{d\\times c}$, $Z^Q \\in \\mathbb{R}^{d\\times q}$ as the continuous representation of the context and question, respectively, and d is the pre-trained model's hidden dimension, c is the context token count and q is the question token count.\nReaction Vector We define the reaction vector as follows:\n$ReactionVector(Z^C, Z^Q)$\n$= abs (AttnVec(Z^C) \u2013 AttnVec(concat(Z^C, Z^Q))_{1:c}) \\quad (1)$\nwhere\n$AttnVec(Z) = Mean_{col}(AttnMatrix(Q, K)) \\quad (2)$\nAttnMatrix is the dense attention function before multiplying the value projection and Q, K are the query and key projection of Z, respectively, similar to the description in the \"Attention is all you need\" paper (Vaswani et al., 2023):\n$AttnMatrix(Q, K) = softmax(\\frac{QK^T}{\\sqrt{d}}) \\quad (3)$\n$Q = Z \\times W^Q$\n$K = Z \\times W^K$\n$Mean_{col}$ takes the attention matrix and returns a per-column mean vector (note the subscription 1: c to indicate truncation from 1 to c-th column):\n$Mean_{col}([a_{ij}]_{d\\times n})_{1:c}= \\left(\\frac{\\sum_{i=1}^{d} a_{i1}}{d}, \\frac{\\sum_{i=1}^{d} a_{i2}}{d}, ..., \\frac{\\sum_{i=1}^{d} a_{ic}}{d}\\right) \\quad (4)$\nFor multi head attention, the reaction vector is a mean across heads:\n$\\overline{Mean_{col}([a_{ij}^{h}]_{d\\times n})}_{h=1...H}_{1:c}=\\frac{1}{H} \\sum_{h=1}^H \\left(\\frac{\\sum_{i=1}^{d} a_{i1}^{h}}{d}, \\frac{\\sum_{i=1}^{d} a_{i2}^{h}}{d}, ..., \\frac{\\sum_{i=1}^{d} a_{ic}^{h}}{d}\\right) \\quad (5)$"}, {"title": "Design Overview", "content": "Figure 3 illustrates the overview of our proposed design. The left portion of the figure depicts how a reaction vector is calculated and the right portion depicts how a reaction vector is used for the retrieval and how the language model leverages the retrieved sentences to answer the query."}, {"title": "Reaction Score", "content": "Given the reaction vector, we define the reaction score (rs) as follows for a vector slice represented with left-right index, [l, r):\n$ReactionScore (Reaction Vector, l, r)$\n$= avg(ReactionVector[l:r]) \\quad (6)$"}, {"title": "Problem Statement", "content": "We define the retrieval task as finding a set of non-overlapping chunks $S = {(l_i, r_i) | 0 \\leq l_i \\leq r_i \\leq |Z_{text}|, r_i < l_{i+1}}$, such that\n$argmax_S AnsQuality(LLMInference(concat(S, q))) \\quad (7)$\nwhere:\n\u2022 concat(S, q) denotes the concatenation of the chunks in S with the query q.\n\u2022 LLMInference is the function that processes the concatenated chunks and the query using a language model.\n\u2022 AnsQuality measures the quality (e.g., F1 Score) of the answer produced by the inference function."}, {"title": "Embedding-Agnostic Sentence Yield (EASY) Algorithm", "content": "Challenge Given a token sequence, finding the token index of sentence boundary is non-trivial for two reasons: (1) the embedding model for sentence split task (e.g., stanza) is different from the model's embedding, and (2) embedding models with huge dictionary may result in an impossible token sequence to achieve a perfect sentence split. For example, widely known sentence-splitting algorithms (e.g., Stanza (Qi et al., 2020)) use a different dictionary from the LLM models (e.g., Llama3-8B-Instruct). Thus, the total token count varies across the embedding models (Table 1).\nAnother challenge is that some models prefer huge dictionaries resulting in a token sequence with an impossible case to articulate a sentence boundary. For example, llama-3 treats .Tas a single token. Although it may be useful in code generation, a missing whitespace between a punctuation period and a subsequent sentence that starts with the word \"The\" would result in an impossible case. In other words, Stanza would create a token sequence ( . - The ), whereas Llama3 tokenizer would output (.T he). In such a case, an exact sentence boundary for Llama3 cannot be expressed as a token index. For these reasons, finding the sentence boundary token index across the embedding model is non-trivial."}, {"title": "EASY Algorithm", "content": "Algorithm 1 EASY Algorithm\nRequire: seq - Encoded token sequence\nRequire: TS - Target Sentences generated by NLP models\nRequire: tol \u2190 30 - Max wiggling tries\n1: P\u2190 [] {Processed sentences}\n2: B\u2190 [] {Sentence boundary indices}\n3: i, m \u2190 0 {Initialize indices}\n4: while P\u2260 TS do\n5: APPEND TS[i] to P\n6: c\u2190 Length(Encode(Concat(P))) {Candidate}\n7: saved \u2190 c\n8: visited \u2190 \u00d8\n9: while true do\n10: if c \u2208 visited or c > |seq| or |c - saved| > tol then\n11: c\u2190 saved\n12: break\n13: end if\n14: ADD c to visited\n15: s \u2190 Decode(seq[m : c])\n16: if s == TS[i] then\n17: break\n18: else if s is substring of TS[i] then\n19: c-c+1\n20: else\n21: c-c-1\n22: end if\n23: end while\n24: APPEND c to B\n25: m\u2190 c\n26: ii+1\n27: end while\n28: return B"}, {"title": "Retrieval using Reaction Score - End-to-end", "content": "The retrieval algorithm starts with calculating the reaction vector. If the input context is longer than the model's context window size, we calculate the attention vector for each chunk of context window size and concatenate them before returning. At this point, we have the reaction vector whose length equals the input context's token sequence.\nTo slice the reaction vector for each sentence, we apply the Embedding-Agnostic Sentence Yield (EASY) algorithm (Section 3.2). Then the retriever slices the reaction vector and calculates the geometric mean of the reaction vector slice using the output of the EASY algorithm, a list of sentence boundary indices. At this point, we have a list of sentences and their associated reaction scores, which will be used as a retrieval heuristic.\nFor the actual retrieval, we greedily add sentences starting with the highest reaction score until the retrieval token budget is depleted or the sentence count threshold is reached (80% of total sentence count). The retrieved sentences are reordered according to their original position as the final cleanup step."}, {"title": "EXPERIMENTS", "content": "We evaluate how the YOURA improves the answering performance of 5 pre-trained models on LongBench (Bai et al., 2023) single-doc and multi-doc QA datasets.\nDatasets We evaluate YOURA on the single and multi-document QA (Table 1).\nModels We used three different open-sourced LLM models: LLama2-7B, Llama3-8B, and Mistralv0.2-7B, where each model has a context window size of 4K, 8K, and 32K, respectively.\nInference Setup For inference, we used the vLLM (Kwon et al., 2023). We used the LongBench (Bai et al., 2023) code to evaluate the generated answers.\nMachine Setup We measured the vLLM (Kwon et al., 2023) inference performance by slightly modifying the VLLM's throughput benchmark (v0.5.3) code on a single NVIDIA H100-80G GPU. We used the default vLLM settings except for changing the ignore_eos to false .\nExperiment Setup We compare the following scenarios to evaluate how the YOURA improves the output quality or performance. For each setup, as done with the Long-Bench (Bai et al., 2023), we equalize the retrieval budget to 3500, 7500, and 31500 for models with 4K, 8K, and 32K context window sizes, respectively.\n\u2022 Model without context No context is passed to the LLM.\n\u2022 with BM25 Retrieval Bx_y indicates chunking at roughly x tokens and retrieving the top y chunks.\n\u2022 with Embedding Retrieval Ex-y indicates chunk- ing at roughly x tokens and retrieving the top y"}, {"title": "Longbench QA Result", "content": "How does YOURA impact the output quality? From Table 2, which shows the answer quality using three different open-sourced models (Llama2, Llama3, and Mistralv0.2), we make the following observations. (1) We observe that YOURA showed better overall answer quality than all of the retrieval approaches (BM25 and embedding-based, except for Llama2 w/ B50_70) at a higher retrieval ratio i.e., retrieving less information from the long context. (2) When compared to the truncate-middle approach, YOURA shows better overall quality for Llama2 and Llama3, and slightly less (-0.15) for Mistral.\n(2-1) When broken down into dataset types, we observe that YOURA is better at multi-document QA than single-document QA for Llama2 and Llama3. We claim that Trunc. truncates the key information to correctly answer multi-document QAs, but the head and tail of a single document include key information in many cases. As for YOURA, the key information is retrieved properly for both the single-doc and multi-doc QAs resulting in a comparable answer quality. For YOURA's lower single-document score, we conjecture that the retrieval budget (4k, and 8k for Llama2 and Llama3, respectively) is insufficient to retrieve all of the relevant information. As a supporting fact, the quality difference between Trunc. and YOURA decreases significantly from -2.52 to -0.26. Experiment results for a pre-trained model with a 16k context window size would be useful for proving/disproving the conjecture but no solid 16k model is available at the time of the publication.\n(2-2) For Mistral, which has a large context window size of 32k, truncation rarely happens (as shown in the small retrieval ratio). Without truncation, distractful information is included as part of the augmented context resulting in worse single-document answer quality than YOURA.4 Our explanation for multi-document QA using Mistral, where YOURA showed a slightly worse quality (-0.78), is similar to the explanation in (2-1); YOURA's relatively high retrieval ratio resulted in leaving out some key information.\nFrom the observations, we conclude that the balance between the pre-trained model's context window size and the retrieval ratio is important for long context QA.\nHow does YOURA's higher retrieval ratio improve inference performance? Table 3 shows the average vllm inference throughput (requests per second) assuming offline truncation/retrieval. We make the following observations. First, YOURA improves the overall inference throughput, especially for models with larger context window sizes. This is because the relative retrieval ratio is larger for models with larger context window sizes. Second, the retrieval ratio is a good relative throughput estimator. The relative throughput improvements (+2%, +13%, +31%) are on par with the relative retrieval ratio (+3%, +12%, +28%). We conclude that the performance of LLM serving platforms such as vLLM is sensitive to the retrieved context size and underscores the importance of good information retrieval."}, {"title": "EASY Sentence Splitter Accuracy", "content": "How well can the EASY algorithm split tokens into sentences? Table 4 shows the quality of our EASY sentence splitting algorithm. We used Stanza (Qi et al., 2020), an open-sourced NLP toolkit, to split raw text into sentences. Then we ran the EASY algorithm for each tokenizer model to identify the sentence boundary within a token sequence. After decoding each token segment (delimited by the sentence boundary index), it is a match if the Stanza's sentence (namely target sentence) matches exactly after the cleanup (e.g., trailing whitespace). We report the average match rate for each model and dataset pair.\nWe make the following observations. Overall the EASY algorithm can identify the sentence boundary for models with varying dictionary sizes: Llama2, Llama3, and Mistralv0.2 tokenizers with 32000, 128256, and 32000 words, respectively. Llama2 and Mistral showed almost identical match ratios, as both models' tokenizers are based on similar vocabulary. Llama3 uses roughly \u00d74 more vocabulary resulting in fewer total token counts as shown in Table 1 (e.g., .Tinto a single token). Due to the rich vocabulary, a typo easily blurred sentence boundaries causing many imperfect matches and thus the smallest overall match quality as in the 2-1, 2-2, and 2-3 datasets. For 1-1, 1-2, and 1-3, datasets, the rich vocabulary worked in favor. Many atypical char sequences (e.g., <b>, math formula) were present in these datasets, which were properly represented with tokens using Llama3.\nHow resilient is the EASY algorithm from edge cases? To quantify the resilience despite typos and overly representative dictionaries, we measured the Levenshtein distance between the target sentence and the sentence yielded by our EASY algorithm (Table 5). On average, one needs less than 3 character edits (insert, remove, replace) to the EASY output sentences to match the target sentences."}, {"title": "CONCLUSION", "content": "We propose a simple, yet effective attention-based retrieval algorithm. We show that attention vectors are useful retrieval and propose novel retrieval heuristics called reaction scores. Our fine-tuning approach improves the output quality or the inference performance of off-the-shelf models for long-context QA tasks."}]}