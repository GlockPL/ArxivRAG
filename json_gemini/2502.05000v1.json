{"title": "Robust Graph Learning Against Adversarial Evasion Attacks via Prior-Free Diffusion-Based Structure Purification", "authors": ["Jiayi Luo", "Qingyun Sun*", "Haonan Yuan", "Xingcheng Fu", "Jianxin Li"], "abstract": "Adversarial evasion attacks pose significant threats to graph learning, with lines of studies that have improved the robustness of Graph Neural Networks (GNNs). However, existing works rely on priors about clean graphs or attacking strategies, which are often heuristic and inconsistent. To achieve robust graph learning over different types of evasion attacks and diverse datasets, we investigate this problem from a prior-free structure purification perspective. Specifically, we propose a novel Diffusion-based Structure Purification framework named DiffSP, which creatively incorporates the graph diffusion model to learn intrinsic distributions of clean graphs and purify the perturbed structures by removing adversaries under the direction of the captured predictive patterns without relying on priors. DiffSP is divided into the forward diffusion process and the reverse denoising process, during which structure purification is achieved. To avoid valuable information loss during the forward process, we propose an LID-driven non-isotropic diffusion mechanism to selectively inject noise anisotropically. To promote semantic alignment between the clean graph and the purified graph generated during the reverse process, we reduce the generation uncertainty by the proposed graph transfer entropy guided denoising mechanism. Extensive experiments demonstrate the superior robustness of DiffSP against evasion attacks.", "sections": [{"title": "1 Introduction", "content": "Graphs are essential for modeling relationships in the social networks [69], recommendation systems [54], financial transactions [7], etc. While Graph Neural Networks (GNNs) [28] have advanced this field, concerns about their robustness have arisen [26, 66, 70]. Studies show that GNNs are vulnerable to evasion attacks [46], particularly structural perturbations [66, 71] where tiny changes to the graph topology can lead to a sharp performance decrease.\nA wide range of works have been proposed to enhance graph robustness, categorizing into: 1) Structure Learning Based methods [11, 22, 67] that focus on refining graph structures; 2) Preprocessing Based methods [13, 52] that focus on denoising graphs during preprocessing stage; 3) Robust Aggregation Based methods [6, 17, 48, 70] that modify the aggregation process; and 4) Adversarial Training Based methods [58] that trains GNNs with adversarial samples. However, most approaches heavily depend on priors regarding clean graphs or attack strategies [22]. For example, the homophily prior [23, 25, 64, 67] and the low-rank prior [13, 26, 35, 57] are among the most commonly used assumptions. Unfortunately, when node features are unavailable, measuring the feature similarity becomes infeasible [22]. Additionally, imposing low-rank constraints risks discarding information encoded in the small singular values [11]. These prior-dependent limitations significantly hinder the ability of existing methods to achieve the universal robustness in graph learning across diverse scenarios.\nTo achieve prior-free robustness, we aim to adaptively learn the intrinsic distribution from clean graphs, which capture the underlying correlation and predictive patterns to enhance the robustness of GNNs when facing unseen samples. Driven by this goal, we model the clean graph as a probability distribution over nodes and edges, encapsulating their predictable properties [33]. Attacks are disruptions to underlying distribution, causing it to shift away from the clean distribution [30]. To learn the latent distribution of clean graphs, diffusion models [39, 49] are an ideal choice, as shown in Figure 1. Instead of relying on priors, they model the implicit distributions in a data-centric manner, remaining agnostic to the dataset and attack strategies. Unlike other generative models, the \u201cnoising-denoising\u201d process of graph diffusion models is well-suited to our goal. When encountering an attacked graph, the trained graph diffusion model gradually injects noise to obscure adversarial information during the forward process. In the reverse process, step-wise denoising enables removing both the adversarial information and injected noise, achieving prior-free graph purification. Nevertheless, it still faces two significant challenges: 1) How can we accurately identify and remove adversarial perturbations without disrupting the unaffected portions of the graph? Evasion attacks typically involve subtle perturbations, making these alterations difficult to detect [46]. During the forward process, the isotropic indiscriminate noise affects both the normal and adversarial nodes, leading to excessive perturbations that can overmodify the graph. As a result, essential information may be lost, complicating the recovery of the clean structure during the reverse denoising phase. 2) How can we ensure that the purified graph preserves the same semantics as the target clean graph? The generation process in diffusion models involves repeated sampling from the distribution, with the inherent randomness promoting the creation of diverse graph samples. While this diversity can be beneficial in other domains of research, it poses a significant challenge to our task. Our objective is not to produce varied graphs, but to accurately recover the clean graph. Consequently, even if adversarial perturbations are successfully removed, there remains a risk that the purified graph may fail to semantically align with the target clean graph.\nTo address these challenges, we propose a novel Diffusion-based Structure Purification framework named DiffSP, which creatively incorporates the diffusion model to learn the intrinsic latent distributions of clean graphs and purify the perturbed structures by removing adversaries under the direction of the captured predictive patterns without relying on any priors. To remove adversaries while preserving the unaffected parts (\u25b7 Challenge 1), we propose an LID-driven non-isotropic diffusion mechanism to selectively inject controllable noise anisotropically. By utilizing this non-isotropic noise, DiffSP effectively drowns out adversarial perturbations with minimal impact on normal nodes. To promote semantic alignment between the clean graph and the purified graph generated during"}, {"title": "2 Related Work", "content": "Robust Graph Learning. Various efforts have been made to improve the robustness of graph learning against adversarial attacks, which can be grouped into four categories. 1) Structure Learning Based methods [11, 22, 26, 67] adjust the graph structure by removing unreliable edges or nodes to improve robustness. Pro-GNN [26] uses low-rank and smoothness regularization, GAR-NET [11] employs probabilistic models to learn a reduced-rank topology, GSR [67] leverages contrastive learning for structure refinement, and SG-GSR [22] addresses structural loss and node imbalance. 2) Preprocessing Based methods [13, 52] modify the graph before training. SVDGCN [13] retains top-k singular values from the adjacency matrix, while JaccardGCN [52] prunes adversarial edges based on Jaccard similarity. 3) Robust Aggregation Based methods [6, 17, 48, 70] improve the aggregation process to reduce sensitivity to adversarial perturbations. PA-GNN [48] and RGCN [70] use attention mechanisms to downweight adversarial edges, while Median [6] and Soft-Median [17] apply robust aggregation strategies to mitigate the effect of noisy features. 4) Adversarial Training Based methods [58] incorporate adversarial examples during training using min-max optimization to enhance resistance to attacks.\nGraph Diffusion Models. Diffusion models have achieved significant success in graph generation tasks. Early works [27, 39] extended stochastic differential equations to graphs, but faced challenges due to the discrete nature of graphs. Graph structured diffusion [18, 49] addressed this by adapting D3PM [3], improving the quality and efficiency of graph generation. In addition, HypDiff [15] introduced a geometrically-based framework that preserves non-isotropic graph properties. To enhance scalability, EDGE [8] promotes sparsity by setting the empty graph as the target distribution. GraphMaker [31] further improved graph quality by applying asynchronous denoising to adjacency matrix and node features. However, directly applying existing graph diffusion models fails to achieve our goal because the indiscriminate noise risks damaging clean nodes. Additionally, the diversity of the graph diffusion model may lead to generated graphs that fit the clean distribution but have semantic information that differs from the target clean graph."}, {"title": "3 Notations and Problem Formulation", "content": "In this work, we focus on enhancing robustness against adversarial evasion attacks with more threatening structural perturbation [71], where attackers perturb graph structures during the test phase, after the GNNs have been fully trained on clean datasets [5]. We represent a graph as $G = (X, A)$, where $X$ is the node features and $A$ is the adjacency. An attacked graph is denoted as $G_{adv} = (X, A_{adv})$, where $A_{adv}$ is the perturbed adjacency matrix. Let $c_o$ be the GNN classifier trained on clean graph samples, and $D_{test} = \\{($\\hat{s}_j, y_j)\\}_{j=1}^{M}$ represent $M$ attacked samples, where each $\\hat{s}_j$ is a graph or a node, and $y_j$ is the corresponding label. The attacker's goal is to maximize the number of misclassified samples, formulated as $\\max \\sum_{j=1}^{M} I(c_o(\\hat{s}_j) \\neq y_j)$, by perturbing up to $e$ edges, where $e$ is constrained by the attack budget $\\Delta$. Our objective is to purify the attacked graph, reducing the effects of adversarial perturbations, and reinforcing the robustness of the GNNs to enhance the performance of downstream tasks."}, {"title": "4 DiffSP", "content": "In this section, we introduce our proposed framework named DiffSP which purifies the graph structure based on the learned predictive patterns without relying on any priors about the dataset or attack strategies. The overall architecture of DiffSP is shown in Figure 2. We first present our graph diffusion purification model which serves as the backbone of DiffSP, followed by detailing the two core components: the LID-Driven Non-Isotropic Diffusion Mechanism and the Graph Transfer Entropy Guided Denoising Mechanism."}, {"title": "4.1 Graph Diffusion Purification Model", "content": "For the backbone of DiffSP, we incorporate the structured diffusion model [3, 31, 49], which has shown to better preserve graph sparsity while reducing computational complexity [18, 49]. Since we focus on the more threatening structural perturbations [71], we exclude node features from the diffusion process and keep them fixed. Specifically, the noise in the forward process is represented by a series of transition matrices, i.e., $[Q^{(1)}, Q^{(2)},..., Q^{(T)}]$, where $(Q^{(t)})_{ij}$ denotes the probability of transitioning from state $i$ to state $j$ for an edge at time step $t$. The forward Markov diffusion process is defined as $q(A^{(t)}\\A^{(0)}) = A^{(0)}Q^{(1)} ... Q^{(t-1)} = A^{(0)}Q^{(t-1)}$. Here we utilize the marginal distributions of the edge state [49] as the noise prior distribution, thus $Q^{(t)}$ can be expressed as $Q^{(t)} = \\bar{\\alpha}(t) I + (1 - \\bar{\\alpha}(t))1m$, where $m_a$ is the marginal distribution of edge states, $\\bar{\\alpha}(t) = \\cos^2 (\\frac{(t/T)+s}{1+s} \\cdot \\frac{\\pi}{2})$ follows the cosine schedule [38] with a small constant $s$, $I$ is the identity matrix, and 1 is a vector of ones. During the reverse denoising process, we use the transformer convolution layer [42] as the denoising network $\\epsilon_{\\theta}(\\cdot)$, trained for one-step denoising $p_{\\theta}(A^{(t-1)}\\vert A^{(t)}, t)$. We can train the denoising network to predict $A^{(0)}$ instead of $A^{(t-1)}$ since the posterior $q(A^{(t-1)}\\vert A^{(t)}, A^{(0)}, t) \\propto A^{(t)} (Q^{(t)}) \\odot A^{(0)} \\bar{Q}^{(t-1)}$ has a closed form expression [31, 44, 45], where $\\odot$ is the Hadamard product. Once trained, we can generate graphs by iteratively applying $\\epsilon_{\\theta}(\\cdot)$."}, {"title": "4.2 LID-Driven Non-Isotropic Diffusion Mechanism", "content": "Adversarial attacks typically target only a small subset of nodes or edges to fool the GNNs while remaining undetected. Injecting isotropic noise uniformly across all nodes, which means applying the same level of noise to each node regardless of its individual characteristics [50], poses a significant challenge. While isotropic noise can effectively drown out adversarial perturbations during the forward diffusion process, it inevitably compromises the clean and unaffected portions of the graph. As a result, both the adversarial and the valuable information are erased, making purification during the reverse denoising process more difficult.\nTo remove the adversarial perturbations without losing valuable information, we design a novel LID-Driven Non-Isotropic Diffusion Mechanism. The core idea is to inject more noise into adversarial nodes identified by Local Intrinsic Dimensionality (LID) while minimizing disruption to clean nodes. In practice, the noise level associated with different edges is distinct and independent. As a result, the noise associated with each edge during the forward diffusion process is represented by an independent transition matrix. The adjacency matrix $A^{(t)}$ at time step $t$ is then updated as follows:\n$(Q^{(t)})_{ij} = \\bar{\\alpha}(t) 1 + (\\Lambda_{\\Lambda})_{ij} (1 - \\bar{\\alpha}(t))1m,$\nwhere $\\Lambda_{\\Lambda} \\in \\mathbb{R}^{N\\times N}$ represents the adversarial degree of each edge. Based on the above analysis, locating the adversarial information and determining the value of $\\Lambda_{\\Lambda}$ is crucial for effective adversarial purification. Local Intrinsic Dimensionality (LID) [21, 37] measures the complexity of data distributions around a reference point $o$ by assessing how quickly the number of data points increases as the distance from the reference point expands. Let $F(r)$ denote the cumulative distribution function of the distances between the reference point $o$ and other data points at distance $r$ and $F(r)$ is positive and differentiable at $r \\geq 0$, the LID of point $o$ at distance $r$ is defined as $\\lim_{\\epsilon\\rightarrow 0} \\frac{\\ln F((1+\\epsilon)r)/F(r)}{\\ln (1+\\epsilon)}$ [21]. According to the manifold hypothesis [14], each node $n_i$ in a graph lies on a low-dimensional natural manifold $\\mathcal{S}$. Adversarial nodes being perturbed will deviate from this natural data manifold $\\mathcal{S}$, leading to an increase in LID [37], which can quantify the dimensionality of the local data manifold. Therefore, we use LID to measure the adversarial degree, $\\Lambda_{\\Lambda}$. Higher LID values indicate that the local manifold around a node has expanded beyond its natural low-dimensional manifold $\\mathcal{S}$, signaling the presence of adversarial perturbations. In this work, we use the Maximum Likelihood Estimator (MLE) [2] to estimate the LID value of graph nodes, providing a useful trade-off between statistical efficiency and computational complexity [37]. Specifically, let $\\Gamma \\in \\mathbb{R}^{n}$ represent the vector of estimated LID values, where $\\Gamma_i$ denotes the LID value of node $n_i$, which is estimated as follows:\n$\\Gamma_i = (\\frac{k}{\\sum_{j=1}^{k} \\log \\frac{r_k(n_i)}{r_j(n_i)}})^{-1}.$\nHere, $r_j (n_i)$ represents the distance between node $n_i$ and its $j$-th nearest neighbor $n_i'$. Based on the observation that the deeper layers of a neural network reveal more linear and \"unwrapped\" manifolds compared to the input space [16], we compute the $r_j(n_i)$ as the Euclidean distance [12] between the hidden features of two nodes in the last hidden layer of the trained GNN classifier $c(\\cdot)$. After obtaining the LID values vector $\\Gamma$, we can calculate $\\Lambda_{\\Lambda} = \\Gamma\\Gamma^T$.\nHowever, in practice, using the non-isotropic transition matrix in Eq. (1) requires the diffusion model to predict the previously injected non-isotropic noise during the reverse process. This task is more challenging because, unlike isotropic noise, non-isotropic noise varies across different edges. As a result, the model must learn to predict various noise distributions that are both spatially and contextually dependent on the graph structure and node features. This increases the difficulty of accurately estimating and removing the noise across graph regions, making the reverse denoising process significantly more intricate. Moreover, training the model to develop the ability to inject more noise into adversarial perturbations and remove it during the reverse process relies on having access to adversarial training data. However, in the evasion attack settings, where the model lacks access to adversarial graphs during training, its ability to achieve precise non-isotropic denoising is limited. Inspired by [63], we introduce the following proposition:\nPROPOSITION 1. For each edge at time t, the adjacency matrix is updated as $A_{ij} (t) = A_{ij} (Q'(t))_{ij}$, where the non-isotropic transition matrix is $(Q'(t))_{ij} = \\bar{\\alpha}(t)+(\\Lambda_{\\Lambda})_{ij} (1-\\bar{\\alpha})1_m$. There exists a unique time $\\hat{t}(\\Lambda_{ij}) \\in [0, T]$ such that $(Q'(t))_{ij} = (Q^{(\\hat{t}(\\Lambda_{ij})}))_{ij}$, where:\n$\\hat{t}(\\Lambda_{ij}) = T \\frac{2(1+s)}{\\pi} \\cos^{-1}(\\sqrt{\\frac{\\bar{\\alpha}(t)}{\\[(\\Lambda_{\\Lambda})_{ij} (1 - \\bar{\\alpha}(t)) + \\bar{\\alpha}(t)]}})-s).$\nThis proposition demonstrates that non-isotropic noise can be mapped to isotropic noise by adjusting the diffusion times accordingly. The detailed proof is provided in Appendix A.1. Building on this proposition, we bypass the need to train a diffusion model that can predict non-isotropic noise in the reverse denoising process. Instead, we handle the need for non-isotropic noise injection by applying isotropic noise uniformly to all edges, while varying the total diffusion time for each edge. By controlling the diffusion time for each edge, we can effectively manage the noise introduced to each node, ensuring that the injected noise accounts for the adversarial degree of each node. Let $\\hat{A}^{(t)'}$ represents the adjacency matrix at time $t$ during the reverse denoising process, we have:\n$\\hat{A}^{(t)'} = M^{(t)} \\odot \\hat{A}^{(t)} + (1 - M^{(t)}) \\odot A^{(t)},$\nwhere $\\hat{A}^{(t)}$ is the adjacency matrix predicted by $\\epsilon_{\\theta}(\\cdot)$, $A^{(t)}$ is the noisy adjacency matrix obtained by $A^{(t)} = AQ^{(t)}$ in the forward diffusion process, and $M^{(t)}$ is the binary mask matrix that indicates which edges are being activated to undergo purification at time step $t$, achieving the non-isotropic diffusion. $M$ is defined as:\n$M_{ij}^{(t)} = \\begin{cases}0, & t > \\hat{t}(\\Lambda_{ij})\\\\1, & t \\leq \\hat{t}(\\Lambda_{ij})\\end{cases}.$\nwhere $\\hat{t}(\\Lambda_{ij})$ is obtained according to Proposition 1. This implies that clean nodes are not denoised until the specified time. In this way, adversarial information receives sufficient denoising, while valuable information is not subjected to excessive perturbations."}, {"title": "4.3 Graph Transfer Entropy Guided Denoising Mechanism", "content": "In structured diffusion models [3], the reverse process involves multiple rounds of sampling from the distribution, which introduces inherent randomness. This randomness is useful for generating diverse graph samples but creates challenges for our purification goal. During the reverse denoising process, the diversity of diffusion can result in purified graphs that, although free from adversarial attacks and fit the clean distribution, deviate from the target graph and have different ground truth labels. This presents a significant challenge: we not only encourage the generated graph to be free from adversarial information but also aim for it to retain the same semantic information as the target clean graph.\nTo address this challenge, we introduce a Graph Transfer Entropy Guided Denoising Mechanism to minimize the generation uncertainty in the reverse Markov chain $(\\hat{G}^{(T-1)} \\rightarrow \\hat{G}^{(T-2)} \\rightarrow ... \\rightarrow \\hat{G}^{(0)})$. Transfer entropy [40] is a non-parametric statistic that quantifies the directed transfer of information between random variables. The transfer entropy from $\\hat{G}^{(t)}$ to $\\hat{G}^{(t-1)}$ in the reverse process by knowing the adversarial graph $G_{adv}$, can be defined in the form of conditional mutual information [56]:\n$I(\\hat{G}_{t-1}; G_{adv}|\\hat{G}_{t}) = H(\\hat{G}^{(t-1)}\\vert\\hat{G}^{(t)}) - H(\\hat{G}^{(t-1)} \\vert\\hat{G}^{(t)}, G_{adv}),$\nwhere $I(\\cdot)$ represents mutual information and $H(\\cdot)$ is the Shannon entropy. This measures the uncertainty reduced about future value $\\hat{G}^{(t-1)}$ conditioned on the value $G_{adv}$, given the knowledge of past values $\\hat{G}^{(t)}$. Given the unnoticeable characteristic of adversarial attacks, which typically involve only small perturbations to critical edges without altering the overall semantic information of most nodes, the target clean graph has only minimal differences from $G_{adv}$. Therefore, by increasing the $I(\\hat{G}_{t-1}; G_{adv}|\\hat{G}_{t})$, we can mitigate the negative impacts of generative diversity on our goal and guide the direction of the denoising process, ensuring that the generation towards the target clean graph. Specifically, the purified graph will not only be free from adversarial attacks but will also share the same semantic information as the target clean graph. However, calculating Eq. (7) requires estimating both the entropy and joint entropy of graph data, which remains an open problem.\nIn this work, we propose a novel method for estimating graph entropy and joint entropy. Let $z_i$ be the representations of node $n_i$ after message passing. By treating the set $Z = \\{z_1, z_2, ..., z_n\\}$ as a collection of variables that capture both feature and structure information of the graph, we approximate it as containing the essential information of the graph. From this perspective, the entropy of the graph can be estimated using matrix-based R\u00e9nyi $\\alpha$-order entropy [62], which provides an insightful approach to calculating the graph entropy. Specifically, let $K$ denote the Gram matrix obtained from evaluating a positive definite kernel $k$ on all pairs of z:\n$H(G) = S_\\alpha(\\mathcal{K}) = \\frac{1}{1 - \\alpha} \\log(\\frac{\\lambda_1(\\mathcal{K})}{\\mathcal{K}_{ii}\\mathcal{K}_{jj}})$.\nBy combining Eq. (8) and Eq. (9), we can get the value of transfer entropy $I(\\hat{G}_{t-1}; G_{adv}|\\hat{G}_{t})$. The detailed derivation process is provided in Appendix A.2. Intuitively, based on our entropy estimation method, maximizing $I(\\hat{G}_{t-1}; G_{adv}|\\hat{G}_{t})$ will guide the node entanglement of the generated $\\hat{G}^{(t-1)}$ towards that of $G_{adv}$, preventing the reverse denoising process from deviating from the target direction. To achieve this, we update the generation process using the negative gradient of $I(\\hat{G}_{t-1}; G_{adv}|\\hat{G}_{t})$ concerning $\\hat{A}^{(t-1)}$;\n$\\hat{A}^{(t-1)} \\leftarrow \\hat{A}^{(t-1)} + \\lambda \\nabla_{\\hat{A}^{(t-1)}} I(\\hat{G}^{(t-1)}; G_{adv}|\\hat{G}^{(t)}),$\nwhere $\\lambda$ is a hyperparameter controlling the guidance scale. Early in the denoising process, maximizing the $I(\\hat{G}_{t-1}; G_{adv}|\\hat{G}_{t})$ will steer the overall direction of the generation toward better purification. However, as the graph becomes progressively cleaner, maintaining the same level of guidance could cause the re-emergence of adversarial information in the generated graph. Therefore, it is essential to adjust the guidance scale dynamically over time. We propose that the scale of guidance should depend on the ratio between the injected noise and the adversarial perturbation at each time step. We update the guidance process in Eq. (10) as follows:\n$\\hat{A}^{(t-1)} \\leftarrow \\hat{A}^{(t-1)} - \\lambda \\frac{\\bar{\\alpha}_{t}}{\\[(\\frac{1-\\bar{\\alpha}\\]}{\\])} \\nabla_{\\hat{A}^{(t-1)}} I(\\hat{G}^{(t-1)}; G_{adv}|\\hat{G}^{(t)}).$"}, {"title": "4.4 Training Pipeline of DiffSP", "content": "Under the adversarial evasion structural attacks, we train the proposed DiffSP along with the classifier using the overall objective loss function $\\mathcal{L} = \\mathcal{L}_{cls} + \\mathcal{L}_{diff}$, where:\n$\\mathcal{L}_{cls} = cross-entropy (\\hat{y}, y),$\n$\\mathcal{L}_{diff} = E_{q(A^{(0)})} E_{q(A^{(t)}|A^{(0)})} [ - \\log p_{\\theta} (A^{(0)} |A^{(t)}, t)].$\nThe classifier loss $\\mathcal{L}_{cls}$ measures the difference between the predicted label $\\hat{y}$ and the ground truth $y$. The graph diffusion model loss $\\mathcal{L}_{diff}$ accounts for the reverse denoising process [3]. Initially, we train the classifier, followed by the independent training of the diffusion model. Once both models are trained, they are used together to purify adversarial graphs. The training pipeline of DiffSP is detailed in Algorithm 1, and complexity analysis is in Appendix C."}, {"title": "5 Experiment", "content": "Datasets. We assess the robustness of DiffSP\u00b9 in graph and node classification tasks. For graph classification, we use MUTAG [24], IMDB-BINARY [24], IMDB-MULTI [24], REDDIT-BINARY [24], and COLLAB [24]. For node classification, we test on Cora [59], CiteSeer [59], Polblogs [1], and Photo [41]. Details are in Appendix D.1.\nBaselines. Due to the limited research on robust GNNs targeting graph classification, we compare DiffSP with robust representation learning and structure learning methods designed for graph classification, including IDGL [9], GraphCL [60], VIB-GSL [47], G-Mixup [20], SEP [53], MGRL [36], SCGCN [68], HSP-SL [65], SubGattPool [4] DIR [55], and VGIB [61]. For node classification, we choose baselines from: 1) Structure Learning Based methods, including GSR [67], GARNET [11], and GUARD [29]; 2) Preprocessing Based methods, including SVDGCN [13] and JaccardGCN [52]; 3) Robust Aggregation Based methods, including RGCN [70], Median-GCN [6], GNNGuard [64], SoftMedian [17], and ElasticGCN [34]; and 4) Adversarial Training Based methods, represented by the GraphADV [58]. Details of baselines can be found in Appendix D.2.\nAdversarial Attack Settings. For graph classification, we evaluate the performance against three strong evasion attacks: PR-BCD [17], GradArgmax [10], and CAMA-subgraph [51]. For node classification, we evaluate six evasion attacks: 1) Targeted Attacks: PR-BCD [17], Nettack [71], and GR-BCD [17]; 2) Non-targeted Attacks: MinMax [32], DICE [72], and Random [32]. Further details on the attack methods and budget settings are provided in Appendix D.3. Hyperparameter Settings. Details are provided in Appendix D.4."}, {"title": "5.1 Adversarial Robustness", "content": "Graph Classification Robustness. We evaluated the robustness of the graph classification task under three adversarial attacks across five datasets. Since the choice of classifier affects attack effectiveness, especially in graph classification due to pooling operations, it is crucial to standardize the model architecture. Simple changes like adding a linear layer can reduce the impact of attacks. To ensure a fair comparison, we used a two-layer GCN with a linear layer and mean pooling for both the baselines and our proposed DiffSP. Each experiment was repeated 10 times, with results shown in Table 1.\nResult. 1) DiffSP consistently outperforms all baselines under the PR-BCD attack and achieves the highest average robustness across all attacks on five datasets, with a notable 4.80% average improvement on the IMDB-BINARY dataset. 2) It's important to note that while baselines may excel against specific attacks, they often struggle with others. In contrast, DiffSP maintains consistent robustness across both datasets and attacks, thanks to its ability to learn clean distributions and purify adversarial graphs without relying on specific priors about the dataset or attack strategies.\nNode Classification Robustness. We evaluate the robustness of DiffSP on the node classification task against six attacks across four datasets, using the same other settings as in the graph classification experiments. The results are presented in Table 2 and Table 3.\nResult. We have two key observations: 1) DiffSP achieves the best average performance across both targeted and non-targeted attacks on all datasets, demonstrating its robust adaptability across diverse scenarios. 2) DiffSP performs particularly well under stronger attacks but is less effective against weaker ones like Random and DICE. This is because these attacks introduce numerous noisy edges, many of which do not exhibit distinctly adversarial characteristics. Instead, these edges are often plausible within the graph. Consequently, these additional perturbations can mislead DiffSP, making it harder to discern the correct information within the graph, leading the generated graph to deviate from the target clean graph."}, {"title": "5.2 Ablation Study", "content": "In this subsection, we analyze the effectiveness of DiffSP's two core components: 1) DiffSP (w/o LN), which excludes the LID-Driven Non-Isotropic Diffusion Mechanism; and 2) DiffSP (w/o TG), which excludes the Graph Transfer Entropy Guided Denoising Mechanism."}, {"title": "5.3 Study on Cross-Dataset Generalization", "content": "We assess DiffSP's generalization ability. The goal is to determine whether DiffSP effectively learns the predictive patterns of clean graphs. We train DiffSP on IMDB-BINARY and use the trained model to purify graphs on IMDB-MULTI, and vice versa.\nResult. As shown in Table 4, DiffSP trained on different datasets, still demonstrates strong robustness compared to GCN trained and tested on the same dataset. Furthermore, DiffSP exhibits only a small performance gap compared to when it is trained and tested on the same dataset directly. These results highlight DiffSP's ability to learn the underlying clean distribution of a category of data and capture predictive patterns that generalize across diverse datasets."}, {"title": "5.4 Study on Purification Steps", "content": "We evaluate the performance as the number of diffusion steps varies. For graph classification on the IMDB-BINARY dataset, we adjust the diffusion steps from 1 to 9 under GradArgMax, PR-BCD, and CAMA-Subgraph attacks. For node classification on the Cora dataset, we vary the diffusion steps from 1 to 12 under the GR-BCD, PR-BCD, and MinMax attacks. The results are shown in Figure 4.\nResult. We observe that all-time step settings demonstrate the ability to effectively purify adversarial graphs. At smaller time steps, the overall trend shows increasing accuracy as the number of diffusion steps increases. This is likely because fewer time steps do not introduce enough noise to sufficiently suppress the adversarial information in the graph. As the diffusion steps increase, we do not see a significant decline in performance. This stability can be attributed to our LID-Driven Non-Isotropic Diffusion Mechanism, which minimizes over-perturbation of the clean graph parts. Additionally, we found that the time required for purifying increased linearly."}, {"title": "5.5 Study on Scale of Graph Transfer Entropy", "content": "To analyze the impact of the guidance scale $\\lambda$, we vary $\\lambda$ from 1e-1 to 1e5. The results are presented in Figure 5. For graph classification, experiments are conducted on the IMDB-BINARY dataset under the GradArgmax, PR-BCD, and CAMA-Subgraph attacks. For node classification, experiments are performed on the Cora dataset under the PR-BCD, GR-BCD, and MinMax attacks.\nResult. The results show that smaller values of $\\lambda$ have minimal effect on accuracy. However, they reduce the stability of the purification during the reverse denoising process, leading to a higher standard deviation. This instability arises because the model is less effective at reducing uncertainty and guiding the generation process when $\\lambda$ is too small. On the other hand, large $\\lambda$ values decrease accuracy by overemphasizing guidance, causing the model to reintroduce adversarial information into the generated graph."}, {"title": "5.6 Graph Purification Visualization", "content": "We visualize snapshots of different purification time steps on the IMDB-BINARY dataset using NetworkX [19], as shown in Figure 6. The visualization process demonstrates that DiffSP has mastered the ability to generate clean graphs, achieving graph purification."}, {"title": "6 Conclusion", "content": "Under evasion attacks, most existing methods rely on priors to enhance robustness, which limits their effectiveness. To address this, we propose a novel framework named DiffSP, which achieves prior-free structure purification across diverse evasion attacks and datasets. DiffSP innovatively adopts the graph diffusion model to learn the clean graph distribution and purify the attacked graph under the direction of captured predictive patterns. To precisely denoise the attacked graph without disrupting the clean structure, we design an LID-Driven Non-Isotropic Diffusion Mechanism to inject varying levels of noise into each node based on their adversarial degree. To align the semantic information between the generated graph and the target clean graph, we design a Graph Transfer Entropy Guided Denoising Mechanism to reduce generation uncertainty and guide the generation direction. Extensive experimental results demonstrate that DiffSP enhances the robustness of graph learning in various scenarios. In future work, we aim to incorporate feature-based attack experiments and optimize the time complexity of DiffSP. Additionally, we plan to improve our proposed graph entropy tool and explore its application. Details about the limitations and future directions can be found in Appendix E."}]}