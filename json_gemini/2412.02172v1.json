{"title": "VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning", "authors": ["Xueqing Wu", "Yuheng Ding", "Bingxuan Li", "Pan Lu", "Da Yin", "Kai-Wei Chang", "Nanyun Peng"], "abstract": "The ability of large vision-language models (LVLMs) to critique and correct their reasoning is an essential building block towards their self-improvement. However, a systematic analysis of such capabilities in LVLMs is still lacking. We propose VISCO, the first benchmark to extensively analyze the fine-grained critique and correction capabilities of LVLMs. Compared to existing work that uses a single scalar value to critique the entire reasoning [4], VISCO features dense and fine-grained critique, requiring LVLMs to evaluate the correctness of each step in the chain-of-thought and provide natural language explanations to support their judgments. Extensive evaluation of 24 LVLMs demonstrates that human-written critiques significantly enhance the performance after correction, showcasing the potential of the self-improvement strategy. However, the model-generated critiques are less helpful and sometimes detrimental to the performance, suggesting that critique is the crucial bottleneck. We identified three common patterns in critique failures: failure to critique visual perception, reluctance to \"say no\", and exaggerated assumption of error propagation. To address these issues, we propose an effective LOOKBACK strategy that revisits the image to verify each piece of information in the initial reasoning. LOOKBACK significantly improves critique and correction performance by up to 13.5%.", "sections": [{"title": "1. Introduction", "content": "With recent advances, large vision-language models (LVLMs) have unlocked strong reasoning capabilities [2, 43], enabling them to solve complex problems in math [27, 38] and science [18, 46]. Typically, LVLMs adopt the chain-of-thought (CoT) approach [41], where the model generates intermediate reasoning steps before reaching the final answer. However, even with the CoT approach, LVLMs remain prone to hallucination [11, 12, 17] and reasoning errors [3], especially when dealing with compositional concepts like counting [1, 32] and identifying spatial relationship [10, 14, 22]. Such issues raise concern about the trustworthiness of visual reasoning and highlights the need for more reliable visual reasoning.\nGiven the difficulty of generating completely accurate reasoning in a single pass, an alternative strategy is self-improvement, where LVLMs critique their initial reasoning and make corrections accordingly to enhance performance. Application to text-only tasks like code generation [5, 29] and multi-hop question answering [34] has demonstrated the effectiveness of self-improvement. For vision-language tasks, the self-improvement strategy has also been applied to areas including video understanding [30] and region-of-interests identification [45]. However, a comprehensive study of LVLMs' self-improvement is still lacking. While some work evaluates the critique capability and enhances it through specialized training, they use LVLMs as evaluators [15] or reward models [42] merely to assess the output quality, rather than for further self-correction and self-improvement."}, {"title": "2. Related Work", "content": "Visual reasoning. Large-scale pre-training on extensive texts and images has equipped LVLMs with strong reasoning capabilities [23, 43]. Various work has evaluated their capabilities across different dimensions, such as mathematical [27, 38], scientific [18, 46], and spatial relationship reasoning [10, 14, 22]. A common reasoning strategy"}, {"title": "3. VISCO Benchmark", "content": "We present VISCO, the first benchmark to evaluate the critique and correction capabilities of LVLMs. VISCO consists of 1645 pairs of questions and LVLM-generated answers, each densely annotated with step-wise binary labels and natural language explanations. The dataset spans 18 datasets and 8 tasks across two main categories: (1) reasoning tasks, such as math and science reasoning, and (2) perception tasks, such as text recognition, spatial relationship understanding, and prevention of hallucination. In the following sections, we first provide the mathematical for-"}, {"title": "3.1. Task Overview", "content": "Given images I and questions q from existing datasets, we use LVLMS M to sample model responses. Following the CoT approach, a model response contains a chain-of-thought \n      \\(s_m = [s_1,..., s_N]\\) with N reasoning steps, leading to the final answer \\(a_M\\). Additionally, we have the ground truth answers \\(a_{gt}\\) from the original datasets. Using the tuple \\((I, q, a_{gt}, s_m, a_m)\\), we evaluate two tasks, critique and correction."}, {"title": "3.1.1 Critique", "content": "Given input I, q, the critique task produces a critique C to judge the correctness of model response \\((s_m, a_m)\\). The judgment is based on the accuracy of factual information and logical calculations in the response. Motivated by research in the education field [20, 47], we propose the use of dense and fine-grained critique, including:\n1. A binary critique \\(C_{s,i}\\) for each intermediate step \\(s_i\\);\n2. A natural language explanation \\(C_{e,i}\\) to explain the binary critique \\(C_{s,i}\\) for step \\(s_i\\);\n3. And finally, a binary critique \\(C_a\\) for the final answer \\(a_m\\). Therefore, the output of the critique task is a hierarchical structure \\(C = (\\{(C_{s,i}, C_{e,i}\\)}_{i=1}^N, C_a)\\).\nHierarchical evaluation. Given the complex structure of critique C, we first evaluate the critique at each granularity level and then combine the results into the final metric. We adopt F1 as our base metric and consider incorrect as the positive class, thus calculating the F1 of correctly identified errors. We calculate F1 scores at three hierarchical levels: \\(F1_a\\) for answer critique \\(C_a\\), \\(F1_s\\) for step-wise critique \\(C_s\\), and \\(F1_e\\) for critique explanation \\(C_e\\). While it is straightforward to calculate F1 for binary critique \\(C_a\\) and \\(C_s\\), measuring F1 for natural language explanation \\(C_e\\) poses a challenge. Therefore, we adopt an LLM-assisted evaluation approach described below.\nLLM-assisted evaluation of natural language explanation. Since each explanation \\(C_{e,i}\\) aims to justify the binary step-wise critique \\(C_{s,i}\\), we adjust the calculation of step-wise F1s to derive \\(F1_e\\). Specifically, we recalculate the true positives as the number of steps \\(s_i\\) that are (1) correctly judged as incorrect, and (2) have their errors correctly explained in \\(C_{e,i}\\). This evaluation only considers explanations for incorrect steps, as explanations for correct steps can be as trivial as reiterating the correct reasoning. We use LLMs to assess the accuracy of critique explanations by comparing them to human-annotated explanations. While minor deviations or omissions are tolerated, the model-generated cri-"}, {"title": "VISCore", "content": "tique must address the core error without introducing more factual or logical errors to be considered as correct. As shown by existing work [24], LLMs can reliably evaluate free-form text given well-defined evaluation criteria.\nEventually, we combine the three scores \\(F1_a\\), \\(F1_s\\) and \\(F1_e\\) into a holistic score VISCore. Due to the stricter criteria for explanation correctness, its F1 score \\(F1_e\\) is naturally of a lower scale than \\(F1_a\\) and \\(F1_s\\). Therefore, a key challenge lies in comprehensively considering all three scores without neglecting the information contained in scores with lower scales. To penalize models that \"guess\" accurate binary critique but fail to provide effective explanations, we calculate our final metric VISCore as the geometric average of the three F1 scores, formally defined as:\n\n\\(VISCore = (F1_a \\times F1_s \\times F1_e)^{\\frac{1}{3}}\\) \n\n\nDetailed illustration of VISCore calculation will be in Appendix A.2."}, {"title": "3.1.2 Correction", "content": "Based on I, q and model response \\((s_m, a_m)\\), the correction task takes critique C as input to generate a refined answer a'. As shown in Figure 1, the critique C can be generated by either LVLMs or human experts.\nEvaluation. We first evaluate the accuracy of the refined answer a' by comparing it against the ground truth \\(a_{gt}\\). We then calculate positive correction ratio (PCR) representing how many errors are successfully corrected:\n\n\\(PCR = Average \\frac{Acc(a_{gt}, a')}{Acc(a_{gt}, a_m)=0}\\) \nTo prevent models from cheating the score by always shifting answers, we also calculate the negative correction ratio (NCR) evaluating how many correct answers are altered into incorrect ones:\n\n\\(NCR = Average \\frac{(1 - Acc(a_{gt}, a'))}{Acc(a_{gt}, a_m)=1}\\) \nand report the correction gain = PCR \u2013 NCR as the final metric. Detailed illustration of correction gain calculation will be in Appendix A.3."}, {"title": "3.2. Dataset Construction", "content": "The construction of VISCO dataset consists of four steps: task input collection, response collection, response filtering, and critique collection. More examples of each step and the final dataset will be in Appendix A.1.\nTask input collection. We collect input images I, questions q and ground truth answers \\(a_{gt}\\) from existing VQA datasets. We select a diverse range of 8 tasks and 18 datasets, divided into two categories: reasoning tasks such as math reasoning [27, 38] and scientific reasoning [18, 46], and perception tasks such as hallucination [11, 17], identifying spatial relationship [10, 22], and text recognition [31, 35]. Figure 3 shows a comprehensive visualization of task and dataset distribution.\nResponse collection. We sample responses from 7 LVLMs of 3 families: InternVL2 8/26/40B, LLaVA-v1.6 7/13/34B, and OpenAI GPT-40. We prompt LVLMs to first generate a CoT in a short paragraph and then generate the final answer. We then split the CoT paragraph into sentences, each representing a reasoning step \\(s_i\\). To simplify the task, we limit the total number of reasoning steps N within 5 and remove responses with more than 5 sentences in the CoT.\nResponse filtering. To evaluate whether LVLMs can correctly identify errors, we focus on two types of errors, out-"}, {"title": "4. Experiments", "content": "In this section, we first evaluate the overall critique and correction capabilities of LVLMs and validate the importance of dense and fine-grained critique. While LVLMs in general perform well in the correction task, we find that critique remains a crucial bottleneck for effective self-improvement. We then conduct an in-depth analysis of the critique task and identify three common patterns of critique failure. These identified error patterns guide the design of our proposed LOOKBACK method as in Section 5, and we hope they can inspire further research."}, {"title": "5. LOOKBACK as Improved Baseline", "content": "In this section, we present an initial step toward enhancing the critique capabilities of LVLMs. As analyzed in Section 4.2, the failure to critique visual perception and the reluctance to \"say no\" are two major bottlenecks for effective critique. To prevent LVLMs from blindly agreeing with the visual perception in the model response, our proposed LOOKBACK framework forces LVLMs to explicitly check the image and verify the visual information before performing the critique.\nAs shown in Figure 17, for each step in the CoT, LOOK-"}, {"title": "6. Conclusions", "content": "This paper presents VISCO, a novel benchmark designed to evaluate LVLMs in terms of critique and correction, two fundamental capabilities for self-improvement. VISCO evaluates fine-grained critique that identifies the correctness of each reasoning step in a chain-of-thought and further provides natural language explanations. We curate the dataset from a diverse range of 18 datasets across 8 tasks and extensively evaluate 24 LVLMs. Our experiments show while LVLMs perform well in correction given high-quality"}, {"title": "A. Additional Examples", "content": "Appendix\nWe present examples of VISCO dataset in Figure 18. We further show the details of each step in the dataset construction process as follows:\nTask input collection. We collect images, questions and ground truth answers from existing visual question answering datasets. The distribution of our tasks and datasets are shown in Figure 3 in the main content. Examples from each task are in Figure 19.\nResponse collection. We use 7 LVLMs to sample model responses, including both the CoT and the final answer. We prompt the LVLMs to generate CoT with less than five sentences, and additionally remove CoT with more than five sentences. The prompt template and an example output are shown in Figure 23.\nResponse filtering. In the response filtering stage, we filter the responses into three subsets: responses with outcome errors, responses with process errors, and responses with no errors. Examples from each of the three subsets are shown in Figure 20.\nCritique collection. We train three human annotators to provide dense and fine-grained critique with a binary label for each step, and a natural language explanation if the step is considered as incorrect. The annotation interface is shown in Figure 24."}, {"title": "A.2. Examples for Calculating VISCore", "content": "Figure 25 illustrates the calculation of VISCore as the metric for critique task. Specifically, the calculation of explanation-level F1 requires matching model-generated critique explanations against human-annotated explanations using LLMs. We use GPT-40 for this evaluation, and the prompt is shown in Figure 26. To evaluate the reliability of this evaluation, we manually compare 100 model-generated explanations against human explanations, and evaluate the agreement between our manual evaluation and LLM evaluation. We observe a high agreement of 0.80 accuracy and 0.61 Cohen's Kappa, validating the robustness of this evaluation approach."}, {"title": "A.3. Examples for Calculating Correction Gain", "content": "Figure 22 illustrates the calculation of correction gain as the metric for correction task."}, {"title": "B. Experimental Details", "content": "We evaluate 26 open LVLMs and 3 proprietary LVLMs. For the open LVLMs, we run the inference with fast serving"}, {"title": "VISCore", "content": "frameworks. Specifically, we evaluate Qwen2-VL, Molmo, Llama-3.2 and NVLM with vllm, evaluate LLaVA-OV and LLaVA-Critic with sglang, and evaluate InternVL2, DeepSeek-VL, LLaVA-v1.5, LLaVA-v1.6, Qwen-VL and Prometheus-Vision with 1mdeploy. We set the sampling temperature as 0.7. The prompt for critique and correction are in Figure 27 and 28. We ask one of the trained annotators to establish the human baseline on a randomly selected subset of 265 data points. To reduce annotation costs, we provide the ground truth answers to the annotator. The 100% answer-level F1 shows that the ground truth answers are verified by the annotator to be correct."}, {"title": "C. Additional Results", "content": "In this section, we present additional experimental results. Figure 21 is a more complete version of Figure 9 in the main content, showing the correction performance given model-generated or human-generated critiques with different granularity. We further report the detailed critique performance of each model at different granularity and categories in Table 5, and include a few additional models like LLaVA-v1.5."}, {"title": "D. Details on LOOKBACK", "content": "The algorithm for our proposed LOOKBACK method is in Alg. 1. We further show an example of critique generated by LOOKBACK in Figure 29."}]}