{"title": "Reevaluation of Inductive Link Prediction", "authors": ["Simon Ott", "Christian Meilicke", "Heiner Stuckenschmidt"], "abstract": "Within this paper, we show that the evaluation protocol currently used for inductive link prediction is heavily flawed as it relies on ranking the true entity in a small set of randomly sampled negative entities. Due to the limited size of the set of negatives, a simple rule-based baseline can achieve state-of-the-art results, which simply ranks entities higher based on the validity of their type. As a consequence of these insights, we reevaluate current approaches for inductive link prediction on several benchmarks using the link prediction protocol usually applied to the transductive setting. As some inductive methods suffer from scalability issues when evaluated in this setting, we propose and apply additionally an improved sampling protocol, which does not suffer from the problem mentioned above. The results of our evaluation differ drastically from the results reported in so far.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs are commonly used to store knowledge in a structured format. However, even well-maintained, large-scale knowledge graphs such as Freebase [2], DBPedia [1] or the Google Knowledge Graph [12] are notoriously incomplete [5], which limits their usefulness. Given the knowledge already encoded in the graph, some of the missing facts can be entailed or are rather likely due to the probabilistic regularities inherent in the graph. The automated task of predicting missing facts in a knowledge graph without using external knowledge is known as link prediction or knowledge graph completion.\nOver time many different approaches were proposed to address this task. Some of the most prominent ones, such as TransE [3], Complex [18], ConvE [4], and RotateE [15] are based on embedding entities (constants) and relations (predicates) in a vector space. These embeddings are learned as a solution to an optimization problem which is defined by the triples in the graph and a specific scoring function which determines the likelihood of a triple being correct. Learnt vectors are then applied to the scoring function to entail missing triples. While these models have proven to perform well in some established evaluation datasets [13], they can only make predictions about entities that were already seen during training. If no embedding for an entity has been learned, it is impossible to compute a score for this entity. Nevertheless, these models have been"}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Link Prediction", "content": "A knowledge graph (KG) G = (E,R, T) is a heterogeneous directed multigraph consisting of triples T, a set of entities & and a set of relations R. A triple p(s, o) \u2208 T is a fact consisting of subjects, relation p and object o where s \u2208 E, p\u2208 R, o \u2208 E. From a logical point of view, a relation is a binary predicate and the entities in E are constants. Figure 1 shows a small example of a knowledge graph. The entities & described in this graph are cities, counties, countries and currencies. Some of these entities are linked with each other via one of the three relations in R.\nThe term link prediction origins from calling a relation between two entities a link. In a realistic link prediction application, we have an incomplete knowledge graph T and we use a link prediction model to create new triples T' with T'OT = \u00d8. If the approach works well, most of the triples in T' are correct even though they have been missing in T. Within an evaluation context we split a given knowledge graph T into train set Ttrain, test set Ttest and a validation set Tvalid. This split is jointly exhaustive and pairwise disjoint. Ttrain is usually used to train a model, Tvalid can be used to optimize the hyperparameters and Ttest to evaluate the model performance. Link prediction, often interchangeably called knowledge graph completion, is the task of predicting a target entity given a source entity and a relation. We call such a task a completion task. Each test triple ttest \u2208 Ttest results in two completion tasks p(s, ?) and p(?, o). The task of link prediction is to predict the correct candidate that acts as a substitution for the? in the query.\nIn transductive link prediction the test set Ttest only contains test triples where subject and object appear in the training set Ttrain. Given a triple p(s, o) \u2208 Ttest, we have at least one triple in train where s appears in subject or object position and at least one triple where o appears in subject or object position. The information encoded in the training set serves two purposes. It is used to"}, {"title": "2.2 Evaluation Protocol", "content": "The most prominent metrics for measuring the quality of link prediction methods are ranking-based metrics. They are calculated based on the position of the correct candidate within a ranking of candidate entities. These candidates are elements of a set of possible entities E*. The target candidate itself is always an element of E*. In the transductive setting, we have usually E* = E. This means that all known entities have to be ranked. In the inductive setting, E* is usually a sample from E with E* < E. We call an evaluation protocol which asks to rank all possible candidates a non-sampling evaluation protocol contrary to a protocol that asks to rank the candidates from a restrictive set, which we call a random sampling evaluation protocol.\nFurthermore, according to the filtering protocol of [3], already known triples in the knowledge graph are ignored when calculating the ranking. This filtered setting is the quasi standard and unfiltered scores are rarely reported. We follow this approach and report always filtered scores in our experiments.\nThe most prominent metrics are the hits@k and the mean reciprocal rank (MRR). Let I be the set of ranks of the correct candidates for the completion tasks p(s,?) and p(?, o) derived from all test triples p(s, o) in a given datasets. Hits@k is then the ratio of test queries where the rank i of the correct candidate is less than or equal to k and MRR is the arithmetic mean over individual reciprocal ranks. Let I be the set of ranks i of the correct candidates for the completion tasks p(s, ?) and p(?, o) derived from all test triples p(s, o) in a given datasets. Then we have:\nhits@k = $\\frac{1}{|I|} \\sum_{i \\in I} i \\leq k$\nMRR = $\\frac{1}{|I|} \\sum_{i \\in I} \\frac{1}{i}$\nWe argue in the following that a small and, in particular, randomly chosen E* makes its easy to achieve high hits@10 and MRR scores. This is especially the case if the completion methods under discussion is tailored to such a setting, while such a method will perform poor in any realistic setting.\nSometimes classification metrics as AUC-PR are reported [16,8]. To calculate the AUC-PR, a single negative triple is constructed for each test triple p(s, o) by randomly corrupting the head or the tail with a random entity. Since we want to compare our results with those of the transductive setting where primarily ranking-based metrics are used, we will only focus on them in the following. Nevertheless, our findings are also applicable to the AUC-PR metric and similar metrics."}, {"title": "3 The Random Sampling Evaluation Protocol", "content": "In [16] the authors proposed inductive link prediction as research problem and introduced a pioneering approach called GraIL. In this paper the authors also introduced the most commonly used benchmarks. These benchmarks would support also a non-sampling based evaluation protocol. However, the authors decided to use a random sampling-based variant of the evaluation protocol due to"}, {"title": "3.1 Baseline", "content": "We propose a simple baseline in which candidates are simply ranked higher if they are valid answers to a prediction task depending on the entity type of the candidate. For each relation p we learn rules that determine whether entities are valid candidates for the subject position and for the object position of p. We iterate over all possible pairs of relations h and b (h\u2260 b). If the set of subject entities of relation b has a high overlap with the set of subject entities of h, the presence of an outgoing relation b of an entity can be used to predict the outgoing relation h. We thus learn rules adhering to the following language bias\nrss: h(X, A) \u2190 b(X, B)\nros: h(A,X) \u2190 b(X, B)\nrso: h(X, A) \u2190 b(B, X)\nroo: h(A,X) \u2190 b(B, X)\nwhere h is the head relation and b is the relation of the body atom. Uppercase letters represent variables. A and B are unbound variables. Note that we learn rules for any combination of subject and object positions of relations h and b.\nThese rules are an indirect way to implicitly infer the type of an entity. Here a type expresses the validity of entities for the subject/object position of a relation. For each rule we calculate a confidence which can be used as how well a rule predicts an entity X for being a valid type for the subject or object position of h depending on whether the bound variable X is on the subject or object position of the rule. This confidence is the relative proportion of correctly predicted subject/object positions of all predicted entities conf(r) = |{x : x \u2208 \u0425\u044c^x \u2208 Xh}|/|{x: x \u2208 Xb}| where X is the set of subjects/objects (depending on the position of the bound variable X) of triples in the training set having body relation b, while Xh is the set of subjects/objects of triples in the training set having head relation h."}, {"title": "3.2 Type-Matched Sampling Protocol", "content": "Approaches which require repeated sampling of subgraphs such as GraIL [16] and its variants [7,9,21] are very slow to evaluate. In fact the flawed sampling evaluation protocol was introduced to help overcome this limitation. In order to be able to evaluate approaches where it is infeasible to score and rank all entities we release a collection of type-matched negatives (TMN) for each benchmark dataset. For each test query we provide 50 negatives that adhere to the type of the searched answer and thus create a more challenging set of negatives. For example for the test triple <Indie rock, /music/genre/artists, Oasis>\nof FB15k-237 v3 the set of type-matched head negatives contains genre entities like Pop rock, Hip hop music or Death metal while the set of tail nega-tives contains artist entities like Katy Perry, The Smashing Pumpkins,\nMiley Cyrus or AC/DC.\nWe created these sets by applying the rules learned by our baseline for each head p(?, o) and tail p(s,?) query in the test set as described above. Since we aggregate the confidences of an entity predicted by multiple rules using the"}, {"title": "4 Experimental Evaluation", "content": null}, {"title": "4.1 Datasets, Approaches and Metrics", "content": "For the evaluation of different approaches we use the commonly used inductive link prediction benchmarks created by [16]. They were created by sampling two disjunct subgraphs from the transductive benchmark datasets FB15k-237 [17], WN18RR [4] and NELL-995 [20]. Each benchmark was released as 4 size-increasing different versions denoted v1, v2, v3 and v4. Table 1 shows the statistics of the benchmarks. It should be noted that the relation counts differ for unknown reasons from the original statistics in [16]."}, {"title": "4.2 Results", "content": "Random Sampling Evaluation Protocol The upper part of Table 2 labeled Random Sampling shows the hits@10 results using the random sampling evaluation protocol. If available, we report the numbers presented in the original paper. If not available, we report results based on our own experiments with these approaches. As a cross-check, we compare all our models with models for which numbers are available. We observed only insignificant differences from the numbers of the original papers.\nOur simple baseline outperforms current state-of-the-art approaches on FB15k-237. It achieves an average hits@10 of about 0.938 over the different versions by simply distinguishing the correct entity based on its type. The baseline also outperforms or rivals the performance of state-of-the-art approaches on NELL-995, especially on the bigger versions V3 and V4. Since WN18RR is a knowledge graph of word senses and general relations such as hypernym, it does not contain strongly typed entities. As a result, the baseline fails to achieve competitive results with the exception of the V3 version. The reason for that are different distributions of relations of the test sets. V3 is dominated by relations for which the tail entity is always an entity from a small set of entities (for example countries or regions), which can be learned by the baseline.\nThese results illustrate already clearly that the random sampling evaluation protocol together with the hits@10 metric is not an appropriate protocol to evaluate and rank approaches for inductive link prediction. With respect to the average hits@10 score our baseline ranks at position #1, #13 and #3 out of 14 approaches (including the baseline) on the three evaluation datasets. We also measured the MRR results, to check if we can observe a similar behaviour. For the MRR it is more important to rank the correct candidate at the top position, thus, the MRR has a stronger focus on a high precision. Here our baselines occupies the positions #7, #13 and #13. While the MRR solves some issues of the random sampling protocol, the baseline still achieves a midfield position for FB15k-237 and there are 7 approaches that perform worse.\nNon-Sampling Evaluation Protocol The middle part of Table 2 labeled Non-Sampling shows the hits@10 results for the non-sampling evaluation protocol, i.e. ranking the correct candidate within all entities of the knowledge graph. Since the evaluation of GraIL and its extensions is very inefficient, we decided to only evaluate benchmarks where the evaluation does not take significantly longer than 12 hours. For example the evaluation of GraIL on FB15k-237 V3 takes 10 hours, while the evaluation of CoMPILE/ConGLR/SNRI on FB15k-237 V2 already takes 9/10.75/11.5 hours.\nThe non-sampling evaluation approach draws a completely different picture. Similar to the random sampling evaluation protocol, NBFNet still can be considered as the best performing approach by achieving the highest average hits@10 on FB15k-237 and NELL-995, while achieving the second highest average hits@10 on WN18RR. NodePiece, which showed one of the best performances when evaluated using the random sampling evaluation protocol, loses over-proportionally as it now only scores 0.450 average hits@10, scoring 0.476 percentage points lower compared to the random sampling evaluation protocol, while NBFNet only scores 0.257 percentage points lower. NodePiece, which was the best performing approach given the results of the random sampling protocol, has been overtaken by five other approaches in the realistic non-sampling protocol. One reason for its good performance on the random sampling evaluation could be, that as NodePiece primarily uses 1-hop incoming and outgoing relations of a node, it could be prone to leverage typing information of a node. AStarNet and RED-GNN appear to behave similarly to NBFNet, with a difference in scores of about 0.221 percentage points for AStarNet and 0.207 percentage points for RED-GNN compared to the random sampling evaluation protocol.\nThe predictive performances of rule-based approaches, such as the symbolic rule-based approach AnyBURL, seem to be actually better than assessed using the random sampling evaluation protocol. AnyBURL has an average hits@10 of 0.584 on FB15k-237, only 0.169 percentage points less than the average sampled hits@10, achieving competitive results comparable to NBFNet. On WN18RR AnyBURL scores an average hits@10 of about 0.687, where the best performing approaches NBFNet achieve 0.664 and AStarNet 0.623. The difference of sampling and non-sampling average hits@10 for DRUM also only amount to 0.134 percentage points and for NeuralLP 0.173.\nType-matched Sampling Evaluation Protocol Using sampled negatives that have an appropriate type draws a similar picture than using the non-sampling protocol. However, this protocol has the advantage that it is also applicable to less efficient models. The bottom part of Table 2 labeled Type-matched Sampling shows the hits@10 using the type-matched sampling evaluation protocol. This evaluation protocol still shows that NBFNet is one of the top-performing approaches as it achieves the best average hits@10 on all datasets. However approaches, such as NodePiece, that performed well on the random sampling evaluation approach as they might learn types of entities, do not perform well in this setting. It is still evident that rule-based approaches"}, {"title": "5 Conclusion", "content": "Current inductive link prediction approaches are evaluated by comparing a prediction only against a small set of randomly selected negatives. Due to the limited size of the set of negatives, errors are introduced that prevent the assessment of the model's true inductive link prediction ability. We show that current state-of-the-art results for inductive link prediction can be achieved using a simple rule-based baseline. Furthermore, we re-evaluate approaches to inductive link prediction, where the order of the state-of-the-art changes drastically. Our corrected results indicate that GNN-based approaches such as NBNnet, AStarNet and RED-GNN are clearly ahead compared to the other approaches. The rule-based approach AnyBURL is an efficient and fully explainable alternative that performs only slightly worse. At the same time we observed that approaches as NodePiece and GraIL as well as its successors lag behind if we evaluate them under a realistic protocol.\nWe advocate that further research should be evaluated using an evaluation protocol where all entities of the knowledge graph are used as negatives. If it is not possible to evaluate an approach on all entities, we publish datasets of type-matched head and tail negatives and encourage inductive link prediction researchers to use them instead of randomly sampling negatives."}]}