{"title": "The Best Instruction-Tuning Data are Those That Fit", "authors": ["Dylan Zhang", "Qirun Dai", "Hao Peng"], "abstract": "High-quality supervised finetuning (SFT) data are\ncrucial for eliciting strong capabilities from pre-\ntrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses\nsampled from other LLMs, which are often out of\nthe distribution of the target model to be finetuned.\nThis, at scale, can lead to diminishing returns and\neven hurt the models' performance and robustness.\nWe propose GRAPE, a novel SFT framework\nthat accounts for the unique characteristics of\nthe target model. For each instruction, it gathers\nresponses from various LLMs, and selects the one\nwith the highest probability measured by the tar-\nget model, indicating that it aligns most closely to\nthe target model's pretrained distribution; it then\nproceeds with standard SFT training. We first\nevaluate GRAPE with a controlled experiment,\nwhere we sample various solutions for each ques-\ntion in UltraInteract from multiple models and\nfinetune commonly used LMs like Llama3.1-8B,\nMistral-7B and Qwen2.5-7B on GRAPE-selected\ndata. GRAPE significantly outperforms strong\nbaselines, including distilling from the strongest\nmodel with absolute gain up to 13.8% averag-\ning across benchmarks, and training on 3\u00d7 more\ndata with maximum 17.3% performance improve-\nments. GRAPE's strong performance generalizes\nto realistic settings. We experiment with the post-\ntraining data used for Tulu3 and Olmo-2. GRAPE\ncan outperform strong baselines with 4.5 times\nthe data by 6.1% and state-of-the-art data selec-\ntion approach by 3.9% on average performance.\nRemarkably, using 1/3 data and half number of\nepochs, GRAPE allows Llama3.1-8B to surpass\nthe performance of Tulu3-SFT by 3.5%.", "sections": [{"title": "1. Introduction", "content": "High-quality, large-scale supervised data is crucial for super-\nvised instruction finetuning (SFT; Databricks, 2023; K\u00f6pf\net al., 2023; Zhao et al., 2024a; Zheng et al., 2024). A\ncommon practice of data collection involves sampling re-\nsponses from strong language models, predominantly fo-\ncusing on expanding the size of the dataset and improving\nthe overall quality of the responses (Sun et al., 2023; Taori\net al., 2023; Wang et al., 2023; Xu et al., 2024c; Chen et al.,\n2024). However, recent research suggests that there is more\ncomplex dynamics involved (Xu et al., 2024d). A plateau\neffect in synthetic data scaling, where performance either\nstagnates or even declines as the size of the synthetic data\nincreases beyond a certain point, has been widely observed.\nThis phenomenon arises due to issues such as diminish-\ning diversity (Padmakumar & He, 2024; Guo et al., 2023)\nand distortion in the data distribution (LeBrun et al., 2021),\nwhich ultimately undermine the base model's performance\nand robustness (Alemohammad et al., 2024; Gerstgrasser\net al., 2024; Shumailov et al., 2023; Dohmatob et al., 2024;\nHataya et al., 2023; Mart\u00ednez et al., 2023a;b; Bohacek &\nFarid, 2023; Briesch et al., 2023).\nThus, effective instruction tuning requires more than scal-\ning up the data; it often needs \"tailoring\" the data to the\nunique characteristics of the target model. Existing works\nfocus on enhancing the model's existing knowledge and\ncapabilities (Du et al., 2023) and optimizing the curriculum\nprogression for instruction tuning (Zhao et al., 2024b; Lee\net al., 2024a; Feng et al., 2023; Setlur et al., 2024). They\ntypically prescribe questions for each model to learn solu-\ntions. However, it remains elusive which responses best suit\na given base model for fine-tuning. Meanwhile, tailoring the\nresponses to the model has been a crucial ingredient for the\nsuccess of later phases of LLM development, particularly\nthrough on-policy preference learning (Tajwar et al., 2024;\nZhang et al., 2024c;a; Miao et al., 2024; Gulcehre et al.,\n2023; Azar et al., 2023a; Tang et al., 2024a; Zhuang et al.,\n2023), and on-policy/online reinforcement learning (Guo\net al., 2024b; Liu et al., 2024d; Zhou et al., 2024c).\nInspired by these insights, we hypothesize that SFT can simi-\nlarly benefit from aligning data with the model, the core idea\nbehind GRAPE. For each instruction, GRAPE gathers and\nselects response(s) from various sources that are closest to"}, {"title": "2. Background and Motivation", "content": "An analogy from Reinforcement Learning and Prefer-\nence Learning The investigation of this work into the\ndistribution match between the pre-trained LM and super-"}, {"title": "3. Methodology", "content": "We introduce GRAPE, a surprisingly simple yet effective\nmethodology to enhance supervised fine-tuning (SFT) by\ncustomizing the training data for the base model. The key\nidea is to find responses among a candidate pool for each\ninstruction $x_i$ such that they align closely with the base\nmodel's pretrained distribution $\\pi_{\\theta_0}$.\nAs diagrammed in Figure 3, GRAPE consists of two main\nsteps, followed by standard SFT:\n1. Response Collection (\u00a73.1) Collect a pool of high-\nquality candidate responses either from existing\ndatasets or sampling from multiple LLMs.\n2. Customization (\u00a73.2): For the target model to be fine-\ntuned $\\pi_{\\theta_0}$, find the response(s), for each instruction,\nthat are closest to the pretrained distribution of $\\pi_{\\theta_0}$."}, {"title": "3.1. Collecting Responses from Existing Resources", "content": "For instruction-tuning of language models, high-quality in-\nstructions are more difficult to collect than responses (Xu\net al., 2024c; Liu et al., 2024a). Therefore, it is a com-\nmon practice to reuse existing instruction-tuning prompts\nwhile generating diverse responses using various methods\ntailored to specific requirements. For instance, instruc-\ntions from Flan (Longpre et al., 2023), OpenOrca (Lian\net al., 2023a), ShareGPT (Team, 2023), and the training\nsplits of GSM-8K (Cobbe et al., 2021), MATH (Hendrycks\net al., 2021b), and CodeContests (Li et al., 2022) are fre-\nquently reused in datasets like Olmo (OLMo et al., 2025),\nTulu (Lambert et al., 2024), OpenHermes (Teknium, 2023),\nOpenOrca (Lian et al., 2023a), MetaMath (Yu et al., 2024),\nMathInstruct (Yue et al., 2023), UltraFeedback (Cui et al.,\n2024), and UltraInteract (Yuan et al., 2024), whether for\nSFT or preference learning. The solutions are generated us-\ning different models or follow varying styles depending on\nthe specific needs. This naturally leads to a situation where\na single instruction with multiple responses becomes a read-\nily available resource. GRAPE therefore leverages these\npre-existing response candidates to tailor training dataset\nthat better aligns with the base model's distribution. When\nsuch resources are unavailable or insufficient, practitioners\ncan generate new responses and apply GRAPE.\nResponses for the i-th instruction are collected from various\ndatasets to form $R_i = \\{y_i^{(j)} : j = 1,..., J(i)\\}$, where"}, {"title": "3.2. Customize Dataset For Models", "content": "We then compute the conditional probability of each re-\nsponse $\\pi_{\\theta_0}(y | x_i)$. We rank the responses based on the\nconditional probability and take those with the highest prob-\nability for each instruction.\nNotably, GRAPE's selection process requires only a forward\npass through the candidates and does not require gradient\ncomputation. Its overhead is substantially lower than many\nmodel-based data selection algorithms (Xia et al., 2024;\nYang et al., 2024b; Liu et al., 2024b; Zhao et al., 2021;\nZhang et al., 2024b; Pan et al., 2024).\nAdditionally, it is important to distinguish GRAPE from the\nother preplexity-based data selection and curriculum plan-\nning methods (Wu et al., 2024; Li et al., 2024b; Liu et al.,\n2024c). Existing approaches focus on selecting instructions\nby using perplexity as a difficulty measure, which differs\nfrom GRAPE that uses probability to select for each instruc-\ntion in a fixed instruction set, responses that better matches\nwith the base model's distribution. Our experiments in \u00a75\ndemonstrate that low-probability responses with fixed set of\ninstructions are detrimental to performance, further empha-\nsizing the fundamental difference in the two processes."}, {"title": "4. Preliminary Experiments", "content": "In this section, we seek to empirically verify the hypothesis\nin \u00a72. To evaluate it under a controlled set-up, we consider\nusing instructions from UltraInteract-SFT, and constrain\nwithin chain-of-thought reasoning (predominantly for cod-\ning, logic and math.) scenario. This allows us to control\nconfounding factors like style and format while ensuring\nmanageable data quality control during data collection. The\ninsights from this experiment will inform the later more\nrealistic setting in \u00a75."}, {"title": "4.1. Experimental Setup", "content": "We detail our experiment settings below."}, {"title": "4.1.1. TRAINING DATA CURATION", "content": "In this controlled experiment, we focus on chain-of-thought\nreasoning (Wei et al., 2022; Wang et al., 2024; Luo et al.,\n2024; Cobbe et al., 2021; Li et al., 2023; Lightman et al.,\n2023). Different models may follow different reasoning\npaths to solve a problem, while their final solutions can be\neasily verified.\nWe use UltraInteract-SFT (Yuan et al., 2024), which con-\ntains approximately 80, 800 unique instructions covering"}, {"title": "4.1.2. BASE MODELS", "content": "To demonstrate the generalizability of GRAPE, we evaluate\nits performance across multiple LLMs, including LLAMA-\n3.1-8B and LLAMA-3.2-3B from LLAMA-3 (Grattafiori\net al., 2024) family, MISTRAL-7B (Jiang et al., 2023) and\nQWEN2.5-7B (Hui et al., 2024)."}, {"title": "4.1.3. EVALUATION", "content": "We evaluate the model on coding and math reasoning bench-\nmarks. For coding tasks, we consider HumanEval (Chen\net al., 2021), MBPP (Austin et al., 2021), LeetCode (Guo\net al., 2024a); for math datasets, we consider MATH\ndataset (Hendrycks et al., 2021b), GSM-Plus (Li et al.,\n2024d) and TheoremQA (Chen et al., 2023b) dataset. Hu-\nmanEval and MBPP are natural-language-to-code bench-\nmarks testing language models' ability to produce func-\ntionally correct programs. LeetCode contains interview-\nlevel programming problems that are more challenging."}, {"title": "4.1.4. BASELINES", "content": "We compare GRAPE with several baselines. Except for Up-\nscaled Dataset which is 3\u00d7 larger, the rest are controlled\nfor the number of responses per-instruction to be the same\nas UltraInteract-SFT.\n\u2022 Original dataset. Performing SFT over the original\nUltraInteract-SFT dataset as is. The original dataset\nalso contains high-quality, verified responses.\nWe compare GRAPE to directly using a standard, well-\ncurated dataset. Responses from a powerful model are\nlikely to represent an upper-bound quality of training\ndata, and comparing against this baseline allows us to\nisolate whether further improvements stem from the\nstrategic response selection performed by GRAPE.\n\u2022 Responses from the strongest model under con-\nsideration. We use responses exclusively from the\nbest model used to generate responses (in our case,\nLLAMA3.1-405B-INSTRUCT).\n\u2022 Up-scaled Dataset. The comparison with a larger\ndataset assesses how well selection improves instruc-\ntion tuning compared to training on a larger set of\ndiverse correct responses. Each instruction is paired\nwith three times of randomly selected responses from\nthe pool."}, {"title": "4.2. Results and Analysis", "content": "Table 1 summarizes the performance of GRAPE across\nbenchmarks. Our approach consistently outperforms the\nvarious baselines across the board, including the original\nUltraInteract-SFT dataset. GRAPE-selected solutions can\noutperform those directly sampled from the strongest model\nunder consideration (LLAMA3.1-405B-INSTRUCT) up to\n13.8% absolute improvement. This implies that customiza-\ntion for base models should be prioritized over identifying\nthe presumably highest-quality responses. This verifies our\ncentral premise that being in-distribution with each base\nmodel is an important ingredient for the responses we su-\npervise the base models on, to boosting downstream perfor-\nmance. Furthermore, we demonstrate that merely adding\nmore responses does not always lead to continuous improve-\nment in model performance, which aligns with findings in"}, {"title": "5. GRAPE-Picking From Real-World SFT\nDatasets", "content": "In this section, we leverage the findings from the earlier\nexperiments and demonstrate the effectiveness of GRAPE\nto customize training data for each base model by selecting\nfrom available datasets with overlapping instructions. Here,\nwe do not generate any new responses for the instructions;\nit only selects from existing ones. We evaluate GRAPE\non the fully open dataset used in post-training phases of\nTULU-3 (Lambert et al., 2024) and OLMO-2 (OLMo et al.,\n2025). The details are presented below."}, {"title": "5.1. Data Mixture Details", "content": "TULU-3 (Lambert et al., 2024) is a fully open-source col-\nlection of post-training recipes, including supervised fine-\ntuning and preference alignment data. OLMO-2 (OLMo\net al., 2025) is a fully open-source language model. Both\nTULU-3 and OLMO-2 use the same data mixture during the\nsupervised fine-tuning stage, but different data mixtures and\nsource models for generating preference data for different\nsizes of their models: Tulu-3-8B/70B and Olmo-2-7B/13B.\nTo demonstrate the effectiveness of GRAPE, we collected\nthe overlapping instructions from both models and gather\ntheir corresponding responses. From the preference data,\nwe retained only the winning responses. We formed our\ncandidate pool with those instructions with at least two\ndistinct responses, resulting in a dataset of 350.4k unique in-\nstructions and about 1.03 million total instruction-response\npairs for evaluation with GRAPE. We do not apply further\nprocessing of these data or any filtering on top of GRAPE."}, {"title": "5.2. Evaluation", "content": "We evaluate on a set of commonly used benchmarks\nspanning over coding, math, knowledge and instruction-\nfollowing. We evaluated on LeetCode (Guo et al.,\n2024a), MATH (Hendrycks et al., 2021b), BigBench-\nHard(BBH) (Suzgun et al., 2022), MMLU (Hendrycks et al.,\n2021a), and AlpacaEval-V2 (Dubois et al., 2024). LeetCode,\nMATH, BBH and MMLU are evaluated the in the same way\nas in (Yuan et al., 2024), where we use zero-shot for MATH\nand MMLU, 3-shot example for BBH. We use the same\nAlpacaEval-v2 as in OpenInstruct."}, {"title": "5.3. Baselines", "content": "\u2022 Responses From The Original SFT on The Subset.\nIn this experiment, we keep the same set of instructions\n(350.4k) and pair each instruction with the response\nfrom the SFT mixture.\n\u2022 Random Response Candidate From The Pool. Ran-\ndomly pairing each instruction with a candidate re-\nsponse for that instruction, which may come from ei-\nther the SFT mixture or a winning response in the\npreference-learning data.\n\u2022 Lowest-Probability Response. Instead of taking the\nhighest probability response to each instruction for the\nbase model, we do the opposite on and take the lowest\nfor each.\n\u2022 Entire SFT Dataset. We train on the entire SFT data\nmixture of Tulu with 939k instances.\n\u2022 All Available Responses For The Overlapping In-\nstructions. In this setting, we train on the entire candi-\ndate pool for all overlapping instructions, resulting in\n103.6k instances.\n\u2022 S2L (Yang et al., 2024b). S2L is a state-of-the-art\nunsupervised data selection baseline designed to se-\nlect balanced subsets from large datasets by leveraging\ntraining loss trajectories. It first trains a small refer-\nence model within the same model family and record\nloss trajectory for each training example, then applies\nK-means clustering on the loss trajectory and samples\nequally from each. We use LLAMA-3.2-1B as ref-\nerence for LLAMA3.1-8B and QWEN2.5-0.5B for\nQWEN2.5-7B. For MISTRAL-V0.3-7B, we use itself\nas reference since no smaller models in the family are"}, {"title": "5.4. Results", "content": "As shown in Table 2, models fine-tuned on responses se-\nlected by GRAPE outperforms the strong baselines we con-\nstructed, especially the one that trains over all available data\nby significant margins across the 3 models. Remarkably,\nUsing roughly 1/6 training computation (Tulu3-8B-SFT was\ntrained for 2 epochs on 3 times of data), our performance ex-\nceeds that of TULU3-8B-SFT. Also, GRAPE outperforms\nstate-of-the-art data-selection approaches like S2L, despite\nits simplicity and efficiency, further highlighting its effec-\ntiveness in diverse real-world scenarios and strengthening\nits overall practicability for large-scale instruction tuning\nand robust generalization. Without the need to synthesize\nany new data, one can easily leverage established datasets\nsourced from the web to customize a dataset for each base\nmodel that yields better fine-tuning outcome.\nThese results feature GRAPE not only as an effective strat-\negy to enhance performances, but a handy approach to im-\nprove fine-tuning efficiency."}, {"title": "5.5. Further Experiments", "content": "We extend our evaluation using OPENHERMES-\n2.5 (Teknium, 2023), a high-quality, open instruction-tuning\ndataset containing approximately 1 million distinct in-\nstructions. Following a similar strategy to the previous\ncombo experiment, we source responses from additional\ndatasets from Huang et al. (2024) and HuggingFace-H4\n(2024). For preference-based datasets, we include only the\nwinning responses, consistent with the methodology in the\nTulu-Olmo experiment. This process results in 575K unique\ninstructions and 1.34 million total instruction-response\npairs. As shown in Table 3, GRAPE-selected data yields\nbetter performances too. The consistent improvement"}, {"title": "5.6. Discussion:GRAPE works if all responses are from\nthe same LM", "content": null}, {"title": "5.7. Discussion: Is In-Distribution A Silver Bullet?", "content": "The success of GRAPE verifies our key hypothesis that\nmatching SFT distribution to the base model benefits the\nperformance. Yet, we argue that this cannot be pushed to\nthe limit of SFT using data from the same distribution. By\nexperimenting with training on solutions sampled from a\ntrained version of the same base model, we notice a drastic\nperformance drop from the original model as shown in Ta-\nble 5. We explored this using the MATH (Hendrycks et al.,"}, {"title": "6. Related Works", "content": "Data Engineering For Instruction Tuning Data is cen-\ntral to the success of effective instruction tuning, (Xu et al.,\n2023; Xia et al., 2024; Chan et al., 2024), featuring both au-\ntomated data synthesis (Xu et al., 2024a; Zeng et al., 2024;\nYu et al., 2024; Wei et al., 2023) and selection (Xia et al.,\n2024; Chen et al., 2023a; Parkar et al., 2024; Li et al., 2024e).\nSome selection approaches focus on high-quality data by\nleveraging LLMs (Chen et al., 2023a; Parkar et al., 2024; Li\net al., 2024b) or employing principled metrics (Kang et al.,\n2024; Mekala et al., 2024; Xia et al., 2024), while others,\nsuch as Yang et al. (2024c); Das & Khetan (2023), aim to\nidentify diversity-optimized subsets for greater efficiency.\nAnother emerging trend is the customization of training\ndata based on the characteristics of the base models. For\ninstance, Li et al. (2024c); Du et al. (2023) leverage the base\nmodel itself to select a subset of instructions, while Li et al.\n(2024a) introduce a teacher model to guide the selection\nprocess. However, these approaches primarily focus on\nidentifying a re-weighted subset of questions for training.\nIn contrast, GRAPE has a different focus, where it aims to\nfind a set of responses from the solution space that not only\nprovide good coverage of the golden distribution but also\nalign closely with the base model's policy.\nOn-Policy Methods For Language Model Alignment\nRecent advances in RLHF (Ouyang et al., 2022),\nRLAIF (Lee et al., 2024b) and preference learning (Rafailov\net al., 2023) research have identified the benefits of on-policy\ndata for model alignment (Xu et al., 2024b; Tajwar et al.,\n2024; Zhang et al., 2024c; Guo et al., 2024b; Liu et al.,\n2024d; Zhang et al., 2024a; Miao et al., 2024; Gulcehre\net al., 2023). However, these works primarily emphasize the\npreference-tuning stage, where models are refined to gener-\nate samples from an already reasonable policy. In contrast,\nless attention is given to the supervised fine-tuning phase,\nwhich starts with base models that are not yet well-prepared\nto generate high-quality answers that align sufficiently with\nthe desired policy for optimization."}, {"title": "7. Conclusion", "content": "We present GRAPE, a simple yet highly effective approach\nto improve supervised fine-tuning data building on our\nempirically-verified hypothesis that instruction tuning data\nshall better match the base model's distribution to optimize\nthe training outcome. GRAPE is surprisingly simple and\nefficient, where it customizes response data to supervise\neach model by taking highest-probability instances. We\ndemonstrated the effectiveness of GRAPE on multiple set-\ntings. Remarkably, GRAPE can outperform not only SOTA\ndata selection baselines, but also Tulu3-SFT by using a 1/3\nsubset of its SFT instructions, and performs better than train-"}, {"title": "A. Parameter Distance", "content": "We measure the L2 norm of model parameter difference between fine-tuned and pre-trained checkpoints, as a signal of how\nmuch the distribution has drifted during SFT (ichi Amari, 2016; Cover & Thomas, 2006). We notice that training over\nwell-matched distribution shifts the parameter less than training over those ill-matched."}, {"title": "B. Further Details On S2L", "content": "This section details the experimental setup for two unsupervised data selection baseline, S2L applied to the joined pool of\nTulu-3 and Olmo-v2 data.\nS2L, a state-of-the-art unsupervised data selection baseline, operates through two key steps: training a reference model\nto capture training dynamics and clustering the resulting trajectories to form a diverse, balanced subset of training data.\nThe reference models used in our setup are specifically selected to enhance S2L's performance, adhering to the theoretical\nunderpinnings from the original paper that training dynamics remain consistent across models of varying sizes within the\nsame family.\nFor our experiments, we train small reference models corresponding to the final target models. Specifically, we pair\nLlama-3.1-8B with Llama-3.2-1B, Qwen-2.5-7B with Qwen-2.5-0.5B, and Mistral-v0.3-7B with itself due to the lack of\nsmaller models in the Mistral family. To minimize computational costs, LoRA is applied when training the Mistral reference\nmodel. This choice of reference models are better compared to original S2L setup, which employed a Pythia-70M\nproxy, thereby improving the fidelity of the selected subset.\nFollowing S2L, the reference models are trained on a random 5% subset of the dataset over four epochs. This reduced\ntraining requirement is justified by prior work, which demonstrates that only partial data is sufficient for the proxy model to\nlearn meaningful training dynamics. During trajectory collection, we record the training loss of all examples at intervals of\n500 iterations. The batch size and learning rate schedules are set as batch size of 128 and a learning rate warmup of 3%,\nfollowed by a cosine decay to 2e-5.\nWe then perform K-means clustering using the Faiss library to efficiently partition the trajectory space into 100 clusters. The\nnumber of iterations is set to 20, and we use the Euclidean distance metric to ensure convergence to well-separated clusters.\nFrom each cluster, an equal number of examples are sampled to maintain a balanced subset distribution."}, {"title": "C. Further Training Details", "content": "We train our models on a 4-GPU Nvidia-GH200 node, with batch size 256 and micro batch size 2."}, {"title": "D. Further Ablations on UltraInteract.", "content": null}, {"title": "E. Additional Related Works On Model Dependent Data Selection Approaches", "content": null}, {"title": "E.1. 0. Notations", "content": "1. A training dataset $D = \\{x_i\\}_{i=1}^N$ of size $N$; the final language model to be trained on the selected data $\\theta$.\n2. We denote the average cost of one forward pass of model $\\theta$ on a training example as $F_{\\theta}$. As one backward pass\nis approximately the cost of two forward passes, the average cost of one \u201cgradient pass\u201d (i.e., one forward + one\nbackward) is thus $3F_{\\theta}$.\n3. Another important source of computational cost in data selection comes from the training of additional models. We use\n$C'(\\theta, D, T)$ to denote the cost of training model $\\theta$ on dataset $D$ for $T$ epochs (i.e., $N \\cdot T$ examples are seen in total)."}, {"title": "2. Gradient-based Methods", "content": "Gradients have long been an important source of information for training data selection, as they directly affect the whole\noptimization process of language models. Three kinds of model-based gradient-based data selection approaches have been\nproposed:\n1. Gradient-based influence\n2. Gradient matching\n3. Gradient norm"}, {"title": "2.1 GRADIENT-BASED INFLUENCE", "content": "Gradient-based influence computes the pairwise influence scores between each pair of training and validation examples.\nTraining data with the highest influence are selected, as training on them leads to the theoretically largest decrease in model\nloss on validation data. LESS (Xia et al., 2024) formulates the pairwise influence scores as the cosine similarity between the\ngradients of training and validation data, and computes these gradient features using the following two steps:\n1. LoRA-train the final model on part of the whole training dataset, denoted as $D_{warmup}$, for $T$ epochs, and save the $T$\nmodel checkpoints.\n2. For each data point, compute its LoRA gradient with each of the $T$ checkpoints, and later aggregate these $T$ gradients\ntogether in the cosine similarity expression.\nTherefore, the computational cost of gradient-based influence is:\n\u2022 Additional training: $C(\\theta_{lora}, D_{warmup}, T)$.\n\u2022 Per-sample gradient for each checkpoint: $NT \\cdot 3F_{\\theta} = 3T \\cdot NF_{\\theta}$."}, {"title": "2.2 GRADIENT MATCHING", "content": "Gradient matching also requires per-sample gradients, but utilizes their information in a different way. It performs clustering\nbased on these gradient features to group similar data, and then applies an iterative greedy selection algorithm. In order to\nscale to LLM-level gradient computation and clustering, TAGCOS (?) completely follows the warmup training and\ngradient computation pipeline of LESS (?). As the computational bottleneck here is still the gradient computation instead\nof clustering or iterative selection, (?) also shares the same computational cost as (Xia et al., 2024):\n\u2022 Additional training: $C(\\theta_{lora}, D_{warmup}, T)$.\n\u2022 Per-sample gradient for each checkpoint: $NT3F_{\\theta} = 3T \\cdot NF_{\\theta}$."}, {"title": "2.3 GRADIENT NORM", "content": "The $L_2$-norms of gradient vectors can also serve as effective indicators for data selection. (Paul et al., 2023) proposes GraNd,\nwhich obtains a utility score for each training point based on its gradient norm early in the training. More specifically, it\nstarts from $m$ different model weight initializations, trains each model on the whole dataset to obtain per-sample gradient\nnorms, and finally averages the $m$ gradient norms for each training point to obtain the final GraNd score. Therefore, the\ncomputational cost of GraNd is shown below:\n\u2022 Additional training: $m \\cdot C(\\theta, D, 1)$.\n\u2022 Per-sample gradient for each weight initialization: $Nm3F_{\\theta} = 3m \\cdot NF_{\\theta}$."}, {"title": "3. Embedding-based Methods", "content": "Embedding-based methods project the whole training set into an embedding space to quantify the information of each data\npoint and their interactions. For model-based embedding-based selection methods", "costs": "they do not need any additional model training and\ncan directly extract useful per-sample embeddings"}]}