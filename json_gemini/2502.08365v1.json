{"title": "Towards Principled Multi-Agent Task Agnostic Exploration", "authors": ["Riccardo Zamboni", "Mirco Mutti", "Marcello Restelli"], "abstract": "In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings.", "sections": [{"title": "1. Introduction", "content": "Multi-Agent Reinforcement Learning (MARL, Albrecht et al., 2024) recently showed promising results in learning complex behaviors, such as coordination and teamwork (Samvelyan et al., 2019), strategic planning in the presence of imperfect knowledge (Perolat et al., 2022), and trading (Johanson et al., 2022). Just like in single agent RL, however, most of the efforts are focused on tabula rasa learning, that is, without exploiting any prior knowledge gathered from offline data and/or policy pre-training. Despite its generality, learning tabula rasa hinders MARL from addressing real-world situations, where training from scratch every time is slow, often expensive, and most of all unnecessary (Agarwal et al., 2022). In this regard, some progress has been made on techniques specific to the multi-agent setting, ranging from ad hoc teamwork (Mirsky et al., 2022) to zero-shot coordination (Hu et al., 2020).\nIn single-agent RL, task-agnostic exploration, such as maximizing an entropy measure over the state space, was shown to be a useful tool for policy pre-training (Hazan et al., 2019; Mutti et al., 2021) and data collection for offline learning (Yarats et al., 2022). Recently, the potential of entropy objectives in MARL was empirically corroborated by a plethora of works (Liu et al., 2021a; Zhang et al., 2021; Xu et al., 2024) investigating entropic reward-shaping techniques to boost exploration. Yet, to the best of our knowledge, task-agnostic exploration has never been investigated for multi-agent scenarios explicitly, and, thus, the problem is far from being solved. Let us think of an illustrative example that highlights the central question of this work: multiple autonomous robots deployed in a collapsed building for a rescue operation mission. The robots' main goal is to explore a large area to find and rescue injured humans, where exploration may involve coordinating with others to access otherwise inaccessible areas. Arguably, trying to enforce all robots to explore the entire area is inefficient and unnecessary. On the other hand, if everyone is focused on their own exploration, any incentive to collaborate with each other may disappear, especially when coordinating comes at a cost. Clearly, a third option is needed. Thus, many questions naturally arise: (i) How can task-agnostic exploration be defined in MARL? (ii) Are different formulations related in some way and when crucial differences emerge? (iii) How can we explicitly address multi-agent task-agnostic exploration in practical scenarios?\nIn this paper, we first provide a principled characterization of task-agnostic exploration in multi-agent settings by showing that the problem can take different, yet related, formulations. Specifically, the demarcation line is drawn by whether the agents are trying to jointly explore the space, or they neglect the presence of others and explore in a disjoint fashion, or, finally, whether they care of being able to explore the space together but as independent components of a mixture. We link these cases to three distinct objectives, each of them with specific pros and cons, and possibly leading to different behaviors. First, we formally show that these objectives"}, {"title": "2. Preliminaries", "content": "In this section, we introduce the most relevant background and the basic notation.\nNotation. In the following, we denote $[N] := {1,2,..., N}$ for a constant $N < \\infty$. We denote a set with a calligraphic letter $\\mathcal{A}$ and its size as $|\\mathcal{A}|$. For a (finite)"}, {"title": "3. Problem Formulation", "content": "This section addresses the first of the research questions:\n(i) How can task-agnostic exploration be defined in MARL?\nIn fact, when a reward function is not available, the core of the problem resides in finding a well-behaved problem formulation coherent with the task. We start by introducing a general framework that is a convex generalization of MGs, namely a tuple $\\mathcal{M}_{F} := (\\mathcal{N}, \\mathcal{S}, \\mathcal{A}, P, F, \\mu, T)$, that consists in a MG equipped with a (non-linear) function $F(\u00b7)$. We refer to these objects as a Convex Markov Games (CMGs). How much should the agents coordinate? How much information should they have access to? Different answers depict different objectives.\nJoint Objectives. The first and most straightforward way to formulate the problem is to define it as in the MDP setting, with the joint state distribution simply taking the place of the single-agent state distribution. In this case, we define a Joint objective, consisting of\n$\\begin{aligned}\n\\max_{\\pi=(\\pi^{i})_{i \\in [N]}}&\\{\\zeta_{\\infty}(\\pi) := F(d^{\\pi})\\}\\\\\n\\max_{\\pi=(\\pi^{i})_{i \\in [N]}}&\\{\\zeta_{K}(\\pi) := \\mathbb{E}_{d_{K} \\sim P^{\\pi}}F(d_{K})\\}\n\\end{aligned}$ (1)\n(2)\nIn task-agnostic exploration tasks, i.e. by setting $F(\u00b7) := H(\u00b7)$, an optimal (joint) policy will try to cover the joint\nfrom the standard (linear) RL objective. In the following, we will assume $F$ is concave if not mentioned otherwise."}, {"title": "4. A Formal Characterization of Multi-Agent Task-Agnostic Exploration", "content": "In the previous section, we described how different objectives enforce different behaviors for task-agnostic explorative policies. In this section, we address the second research question:\n(ii) Are different formulations related in some way and when crucial differences emerge?\nFirst of all, we show that if we look at task-agnostic exploration tasks, i.e. the ones defined by setting the functional $F(\u00b7) := H(\u00b7)$, all the objectives in infinite-trials formulation can be elegantly linked one to the other though the following result:\nLemma 4.1 (Entropy Mismatch). For every Convex Markov Game $\\mathcal{M}_{H}$ equipped with an entropy functional, for a fixed (joint) policy $\\pi = (\\pi^{i})_{i \\in \\mathbb{N}}$ the infinite-trials objectives are ordered according to:\n$H(\\bar{d}^{\\pi}) < \\frac{1}{|\\mathbb{N}|} \\sum_{i \\in [\\mathbb{N}]} H(d^{\\pi,i}) < H(d^{\\pi})$\n$H(\\bar{d}^{\\pi}) \\leq \\sup_{i \\in [\\mathbb{N}]} H(d^{\\pi,i}) + \\log(|\\mathbb{N}|) \\leq H(d^{\\pi}) + \\log(|\\mathbb{N}|)$\nThe full derivation of these bounds is reported in Appendix A. This set of bounds prescribe that the difference in performances over infinite-trials objective for the same policy can be generally bounded as a function of the number of agents. In particular, disjoint objectives generally provides poor approximations of the joint objective from the point of view of the single-agent, while the mixture objective is guaranteed to be a rather good lower bound to the joint entropy as well, since its over-estimation scales logarithmically with the number of agents.\nIt is still an open question how hard it is to actually optimize for these objectives. Now, while CMGs are a novel interaction framework, whose general properties are far from being well-understood, they surely enjoy some nice properties. In particular, as commonly done in Potential Markov Games (Leonardos et al., 2021), it is possible to exploit the fact that performing Policy Gradient (PG, Sutton et al., 1999; Peters & Schaal, 2008) independently among the agents is equivalent to running PG jointly, when this is done over the same common objective (Appendix A.1, Lemma A.5). This allows us to provide a rather positive answer, here stated informally and extensively discussed in Appendix A.1 :\nFact 4.1 ((Informal) Sufficiency of Independent Policy Gradient). Under proper assumptions, for every CMG $\\mathcal{M}_{F}$, independent Policy Gradient over infinite trials non-disjoint"}, {"title": "5. Trust Region for Exploration in Practice", "content": "As stated before, a core drive of this work is addressing multi-agent task-agnostic exploration in practical scenarios. Yet, these cases are also the ones in which performing PG of infinite-trials objectives provide poor performance guarantees at deployment. In other words, here we address the third research question, that is:\n(iii) How can we explicitly address multi-agent task-agnostic exploration in practical scenarios?\nTo do so, our attention will focus on the finite trials objectives explicitly, more specifically on the single-trial case with $K = 1$. Remarkably, it is possible to directly optimize the single-trial objective in multi-agent cases with decentralized algorithms: we introduce Trust Region Pure Exploration (TRPE), the first decentralized algorithm that explicitly addresses single-trial objectives in CMGs, with task-agnostic exploration as a special case. TRPE takes inspiration from trust-region based methods as TRPO (Schulman et al., 2017), as they recently enjoyed an ubiquitous success and interest for their surprising effectiveness in multi-agent problems (Yu et al., 2022).\nIn fact, trust-region analysis nicely align with the properties of finite-trials formulations and allow for an elegant extension to CMGs through the following.\nDefinition 5.1 (Surrogate Function over a Single Trial). For every CMG $\\mathcal{M}_{F}$ equipped with a $L$-Lipschitz function $F$, let $d_{1}$ be a general single-trial distribution $d_{1} = \\{d_{1,i}, d_{1}, \\bar{d}_{1}\\}$, then for any per-agent deviation over policies $\\pi = (\\pi^{i}, \\pi^{-i})$, $\\tilde{\\pi} = (\\tilde{\\pi}^{i}, \\pi^{-i})$, it is possible to define a per-agent Surrogate"}, {"title": "6. Proof of Concept Experiments", "content": "In this section, we provide some empirical validations of the findings discussed so far. Especially, we aim to answer the following questions: (a) Is Algorithm 1 actually capable of optimizing finite-trials objectives? (b) Do different objectives enforce different behaviors, as expected from Section 3? (c) Does the clustering behavior of mixture objectives play a crucial role? If yes, when and why?\nThroughout the experiments, we will compare the result of optimizing finite-trial objectives, either joint, disjoint, mixture ones, through Algorithm 1 via fully decentralized policies. The experiments will be performed with different values of the exploration horizon $T$, so as to test their capabilities in different exploration efficiency regimes."}, {"title": "7. Related Works", "content": "Below, we summarize the most relevant work investigating multi-agent exploration and task-agnostic exploration in single-agent scenarios.\nMulti-Agent Exploration. Recently, fostering exploration in order to boost performances in (deep) MARL has gained much attention recently. A large set of works proposed to address it via reward-shaping based on many heuristics: Wang et al. (2019) adds a term maximizing the mutual-information between per-agent interactions; Zhang et al. (2021) proposes to optimize the deviation from (jointly) explored regions while Zhang et al. (2023) proposes to optimize directly the entropy over per-agent observations; more recently, Xu et al. (2024) proposed an heuristic reward-shaping enforcing diversity between different agents, and notices that Wang et al. (2019) fails to address the task it introduced. Up to our knowledge, this work is the first in covering both the theoretical properties of multi-agent (task-agnostic) exploration and the optimization of single-trial objectives. Finally, we notice that a that a similar notion of Convex Markov Games was introduced in a concurrent and preliminary work (Gemp et al., 2025), together with some results on existence of equilibria.\nTask-Agnostic Exploration and Policy Optimization. Entropy maximization in MDPs was first introduced in Hazan et al. (2019) and then investigated extensively"}, {"title": "8. Conclusions and Perspectives", "content": "In this paper, we extend the state entropy maximization problem to Markov Games via a novel framework called Convex Markov Games. First of all, we show that the task can be defined in several different ways: one can look at the joint distribution among all the agents, the marginals which are agent-specific, or the mixture which is a tradeoff of the two. Thus, we link these three options via performance bounds and we show that while the first might enjoy nice theoretical guarantees, the others are more promising at working in practice, the latter in particular. Then, we design a practical trust-region algorithm addressing more practical scenarios and we use it to confirm in a set of experiments the expected superiority of mixture objectives, due to its ability to enforce efficient but coordinated exploration over short horizons. Future works can build over our results in many directions, which include pushing forward the known theoretical properties of Convex Markov Games, developing scalable algorithms for continuous domains and investigating more policy classes with succinct representations of the history beyond the one we considered in the experiments.\nWe believe that our work can be a crucial step in the direction of extending state entropy maximization in a principled way to yet more practical settings, in which many agents interact over the same environment."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Proofs of the Main Theoretical Results", "content": "In this Section, we report the full proofing steps of the Theorems and Lemmas in the main paper.\nLemma 4.1 (Entropy Mismatch). For every Convex Markov Game $\\mathcal{M}_{H}$ equipped with an entropy functional, for a fixed (joint) policy $\\pi = (\\pi^{i})_{i \\in \\mathbb{N}}$ the infinite-trials objectives are ordered according to:\n$H(\\bar{d}^{\\pi}) < \\frac{1}{|\\mathbb{N}|} \\sum_{i \\in [\\mathbb{N}]} H(d^{\\pi,i}) < H(d^{\\pi})$\n$H(\\bar{d}^{\\pi}) \\leq \\sup_{i \\in [\\mathbb{N}]} H(d^{\\pi,i}) + \\log(|\\mathbb{N}|) \\leq H(d^{\\pi}) + \\log(|\\mathbb{N}|)$\nTheorem 4.2 (Objectives Mismatch in CMGs). For every CMG $\\mathcal{M}_{F}$ equipped with a $L$-Lipschitz function $F$, let $K \\in \\mathbb{N}^{+}$ be a number of evaluation episodes/trials, and let $\\delta \\in (0, 1]$ be a confidence level, then for any (joint) policy $\\pi = (\\pi^{i} \\in \\Pi^{i})_{i \\in [\\mathbb{N}]}$, it holds that\n$\\begin{aligned}\n|\\zeta_{K}(\\pi) - \\zeta_{\\infty}(\\pi)| &\\leq LT\\sqrt{\\frac{2|\\mathcal{S}|\\log(2T/\\delta)}{K}},\\\\\n|\\tilde{\\zeta}_{K}(\\pi) - \\tilde{\\zeta}_{\\infty}(\\pi)| &\\leq LT\\sqrt{\\frac{2|\\mathcal{S}|\\log(2T/\\delta)}{K}},\\\\\n|\\bar{\\zeta}_{K}(\\pi) - \\bar{\\zeta}_{\\infty}(\\pi)| &\\leq LT\\sqrt{\\frac{2|\\mathcal{S}|\\log(2T/\\delta)}{N K}}\n\\end{aligned}$"}, {"title": "A.1. Policy Gradient in CMGs with Infinite-Trials.", "content": "In this Section, we analyze policy search for the infinite-trials joint problem $\\zeta_{\\infty}$ of Eq. (1), via projected gradient ascent over parametrized policies, providing in Th. A.6 the formal counterpart of Fact 4.1 in the Main paper. As a side note, all of the following results hold for the (infinite-trials) mixture objective $\\bar{\\zeta}_{\\infty}$ of Eq. (5). We will consider the class of parametrized policies with parameters $\\theta^{i} \\in \\Theta^{i} \\subset \\mathbb{R}^{d}$, with the joint policy then defined as $\\pi_{\\theta} \\in \\Theta = \\times_{i \\in [\\mathbb{N}]}\\Theta^{i}$. Additionally, we will focus on the computational complexity only, by assuming access to the exact gradient. The study of statistical complexity surpasses the scope of the current work. We define the (independent) Policy Gradient Ascent (PGA) update as:\n$\\theta^{i}_{k+1} = \\arg \\max_{\\theta \\in \\Theta^{i}}  \\zeta_{\\infty}(\\pi_{\\theta}) + \\langle \\nabla_{\\theta^{i}} \\zeta_{\\infty}(\\pi_{\\theta_{k}}), \\theta^{i} - \\theta^{i}_{k} \\rangle  - \\frac{1}{2 \\eta} ||\\theta^{i} - \\theta^{i}_{k}||^{2} = \\Pi_{\\Theta^{i}} \\{\\theta^{i} + \\eta \\nabla_{\\theta^{i}} \\zeta_{\\infty}(\\pi_{\\theta_{k}})\\}\\quad \\forall i \\in [\\mathbb{N}],$\n(8)\nwhere $\\Pi_{\\{\\}}$ denotes Euclidean projection onto $\\Theta^{i}$, and equivalence holds by the convexity of $\\Theta^{i}$. The classes of policies that allow for this condition to be true will be discussed shortly.\nIn general the overall proof is built of three main steps, shared with the theory of Potential Markov Games (Leonardos et al., 2021): (i) prove the existence of well behaved stationary points; (ii) prove that performing independent policy gradient is equivalent to perform joint policy gradient; (iii) prove that the (joint) PGA update converges to the stationary points via single-agent like analysis. In order to derive the subsequent convergence proof, we will make the following assumptions:"}, {"title": "A.2. The Use of Markovian and Non-Markovian Policies in CMGs with Finite-Trials.", "content": "The following result describes how in CMGs, as for convex MDPs, Non-Markovian policies are the right policy class to employ to guarantee well-behaved results.\nLemma A.1 (Sufficiency of Disjoint Non-Markvoian Policies). For every Convex Markov Game $\\mathcal{M}$ there exist a joint policy $\\pi^{*} = (\\pi^{*,i})_{i \\in \\mathbb{N}}$, with $\\pi^{*,i} \\in \\Delta_{\\mathcal{A}}$ being a deterministic Non-Markovian policy, that is a Nash Equilibrium for non-Disjoint single-trial objectives, for $K = 1$."}, {"title": "B. Details on the Experimental Proofs Of Concept.", "content": "Environments. The main empirical proof of concept was based on two environments. First, Env. (i), the so called secret room environment by Liu et al. (2021a). In this environment, two agents operate within two rooms of a 10 \u00d7 10 discrete grid. There is one switch in each room, one in position (1,9) (corner of first room), another in position (9, 1) (corner of second room). The rooms are separated by a door and agents start in the same room deterministically at positions (1, 1) and (2, 2) respectively. The door will open only when one of the switches is occupied, which means that the (Manhattan) distance between one of the agents and the switch is less than 1.5. The full state vector contains x, y locations of the two agents and binary variables to indicate if doors are open but per-agent policies are conditioned on their respective states only and the state of the door.\nClass of Policies. In Env. (i), the policy was parametrized by a dense (64, 64) Neural Network that takes as input the per-agent state features and outputs an action vector probabilities through a last soft-max layer.\nTRPE As outlined in the pseudocode of Algorithm 1, in each epoch a dataset of N trajectories is gathered for a given exploration horizon T"}]}