{"title": "GUNET: A GRAPH CONVOLUTIONAL NETWORK UNITED DIFFUSION MODEL FOR STABLE AND DIVERSITY POSE GENERATION", "authors": ["Shuowen Liang", "Sisi Li", "Qingyun Wang", "Cen Zhang", "Kaiquan Zhu", "Tian Yang"], "abstract": "Pose skeleton images are an important reference in pose-controllable image generation. In order to enrich the source of skeleton images, recent works have investigated the generation of pose skeletons based on natural language. These methods are based on GANs. However, it remains challenging to perform diverse, structurally correct and aesthetically pleasing human pose skeleton generation with various textual inputs. To address this problem, we propose a framework with GUNet as the main model, PoseDiffusion. It is the first generative framework based on a diffusion model and also contains a series of variants fine-tuned based on a stable diffusion model. PoseDiffusion demonstrates several desired properties that outperform existing methods. 1) Correct Skeletons. GUNet, a denoising model of PoseDiffusion, is designed to incorporate graphical convolutional neural networks. It is able to learn the spatial relationships of the human skeleton by introducing skeletal information during the training process. 2) Diversity. We decouple the key points of the skeleton and characterise them separately, and use cross-attention to introduce textual conditions. Experimental results show that PoseDiffusion outperforms existing SoTA algorithms in terms of stability and diversity of text-driven pose skeleton generation. Qualitative analyses further demonstrate its superiority for controllable generation in Stable Diffusion. Homepage:https://github.com/LIANGSHUOWEN/ PoseDiffusion", "sections": [{"title": "Introduction", "content": "As an important external control condition in controllable image generation, 2D pose skeleton images are critical to the quality of the generated images. For example, ControlNet(Zhang et al. [2023]), HumanSD (Ju et al. [2023]), GRPose(Yin et al. [2024]) and other models used to create controllable photos need to be provided with one or more 2D pose skeleton images for reference. In addition, many pose sequence generation tasks also need to be provided with the first frame of the pose skeleton. However, in order to obtain the pose skeletons, current methods rely mainly on extracting them from existing images using pose detection models, such as DWPose(Yang et al. [2023]) and OpenPose(Cao et al. [2017]). This limits the diversity and operability of obtainable pose skeletons. In practice, there are also blender-based applications which allow users to adjust and model the pose by dragging and dropping key points. However, these approaches are time-consuming and require a lot of manual effort. They are also difficult to implement an end-to-end process due to the large amount of manual intervention. In order to get pose skeleton images more easily and flexibly, it is necessary to create a framework for generating pose skeletons directly from natural language.\nIn recent years, many researchers have explored methods for generating 2D human pose skeletons from textual descriptions. Zhang et al. [2021] trained a generative adversarial network capable of generating single-person poses from text. Roy et al. [2022] proposed the DE-PASS, a dataset with detailed labelling of the facial details in poses, and trained a generative adversarial network for generating single-person poses conditioned on natural language on it, which"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Pose or Keypoint-guided Text-driven Image Generation", "content": "With the advent of Stable Diffusion(Rombach et al. [2022])(SD), work on text-driven image generation has made tremendous progress. In order to avoid pose distortion of people or objects in the generated images, many works use pose skeletons to guide text-driven image generation. ControlNet(Zhang et al. [2023]) incorporates additional inputs such as pose information into SD to enhance the architecture for controllable generation, allowing users to control image details during the generation process precisely. HumanSD(Ju et al. [2023]) is a model that improves on SD by introducing a specific human pose encode module to enhance the accuracy of human pose generation for fine-grained"}, {"title": "2.2 Generating Human Pose Skeletons from Natural Language", "content": "Previous work has done some research on generating human poses from natural language. Reed et al. [2016] showed that higher-resolution images can be synthesised using a sparse set of key points. Zhou et al. [2019] change the pose of a person in a given image based on a textual description. However, the pedestrian dataset they used had simple poses and could not be applied to datasets with multifunctional and highly articulated poses, such as COCO. To enrich the generative results of the model, Zhang et al. [2021] trained a generative adversarial network capable of generating complex poses from natural language descriptions on the COCO dataset. Although they achieved good results in terms of action complexity and versatility, they did not take into account the spatial relationships between different key points of human poses, resulting in the generation of poses that are subject to problems such as deformities or skeletal disproportions. In addition, their simple treatment of text conditions results in generated poses that sometimes do not match the text well.\nTo solve these problems, we propose a new text-driven human pose skeleton generation pipeline based on denoising diffusion probabilistic model(DDPM)(Ho et al. [2020]). Where the denoising model is based on U-Net, and introduces a graph neural network to introduce the spatial information of the pose. In later sections, we will discuss the design of different variants of the U-Net-based diffusion model and the potential of our proposed pipeline in various application scenarios. Moreover, benefiting from the properties of the DDPM, our pipeline is able to generate more diverse samples."}, {"title": "2.3 Text-conditioned U-Net", "content": "U-Net was initially applied to solve the problem of medical image segmentation(Ronneberger et al. [2015], Oktay et al. [2018], Zhou et al. [2018]), and more recent work combines U-Net with diffusion processes for image genera- tion(Rombach et al. [2022], Dhariwal and Nichol [2021], Ju et al. [2023], Zhang et al. [2023]). These works achieve state-of-the-art performance on a variety of tasks such as unconditional image generation, text-driven image generation, and image-driven image generation, and inspired our work. Text-conditioned U-Net(Rombach et al. [2022]) is one of the pioneers in using U-Net as a denoising model. The basic idea of text-conditioned U-Net is to incorporate textual information into the feature interaction between the encoder and decoder layers of U-Net by introducing textual embedding and cross-attention mechanisms, thus effectively guiding the image content during the generation process. The down-sampling process of U-Net aims at gradually compressing the spatial dimension of the input feature map to extract higher-level, global semantic information, while the up-sampling process is used to gradually recover the spatial resolution to combine the global semantic information with the detailed features to generate the output image of the same size as the input."}, {"title": "3 Method", "content": "We propose a diffusion model-based framework, PoseDiffusion, for generating diverse and skeletally structurally stable 2D human pose skeletons. We first give the problem definition in Section 3.1, then we provide a general description of the proposed PoseDiffusion in Section 3.2, followed by the diffusion model in Section 3.3 and the U-Net based denoising model, GUNet, in Section 3.4."}, {"title": "3.1 Task Definition", "content": "The human pose skeleton H contains a set of heatmaps $h_i$, where $i \\in \\{1, 2, ..., K\\}$ and K denotes the number of key points in the skeleton. Each $h_i \\in \\mathbb{R}^{S\\times S}$ corresponds to a heatmap for a keypoint, and S is the size of these heatmaps. The heatmap is modelled with a Gaussian distribution centred on the coordinates of the keypoints to obtain"}, {"title": "3.2 Pipeline Overview", "content": "Our proposed PoseDiffusion pipeline is shown in Fig. 3. First, we construct a text-based pose skeleton generation pipeline using the denoising diffusion model(Nichol and Dhariwal [2021])(DDPM). The basic of the denoising model is U-Net, based on which we insert a spatial module into U-Net to insert a spatial module into U-Net to introduce spatial information of the skeleton, such as key point locations and connectivity relationships, to obtain GUNet. To enable GUNet to handle textual conditions, we use a cross-attention mechanism to fuse cross-modal features. In addition to this, we change the input dimension of GUNet to match the skeleton features represented using a set of heatmaps. The above design significantly improves the pose skeleton generated by PoseDiffusion. We will describe each part of the pipeline in the following subsections."}, {"title": "3.3 Diffusion Model", "content": "The diffusion model is a generative model that progressively denoises Gaussian noise through a learnable probabilistic model. The forward process of diffusion is a Markov chain that starts from the initial data $x_0$ and gradually adds noise with variance $B_t$ to the data $x_{t-1}$ over T timesteps to obtain a set of noise samples $x_t$ with distribution $q(x_t | x_{t\u22121}) = N (x_t; \\sqrt{1 \u2212 B_t}x_{t\u22121}, B_tI)$. The distribution of the whole forward noise addition process can be calculated by Eq. 1:\n$q (x_{1:T} | x_o) = q (x_o) \\prod_{t=1}^{T} q (x_t | x_{t\u22121})$ (1)\nThe inverse process is the reversal of the forward process, where we sample from $q(x_t | x_{t\u22121})$ and progressively reconstruct the true sample, which means estimating $q (x_{t\u22121} | x_t)$ at the moment t = T. Estimating the previous state from the current state requires knowledge of all the previous gradients. Thus, it is necessary to train a neural network model to estimate $p_\\theta (x_{t-1} | x_t)$ based on the learned weights $\\theta$ and the current state at time t. This trajectory can be performed by Eq. 2:\n$p_\\theta (x_{t-1} | x_t) = N (x_{t\u22121}; \\mu_\\theta (x_t,t), \\Sigma_\\theta (x_t, t))$ (2)\n$\\prod_{t=1}^{T} p_\\theta (x_{0:T}) = p (x_T) p_\\theta (x_{t-1} | x_t)$\nTo provide a simplified representation of the diffusion process, Ho et al. [2020] formulated it as Eq. 3:\n$q(x_t | x_0) = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}, \\epsilon \\in N (0, I)$ (3)\nwhere $\\alpha_t = 1 \u2212 \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{i=0}^{t} \\alpha_i$. Thus we can simply sample the noise samples and generate $x_t$ directly from this formula. Here, we follow the approach used in GLIDE(Nichol et al. [2021]) and predict the noise term $\\epsilon$. The expression for the neural network prediction during the sampling process can be simplified to $\\epsilon_\\theta (x_t, t, text)$. We optimize the denoising model using the loss function as shown in Eq. 4:\n$L = E_{t\\in[1,T],x_0~q(x_0),\\epsilon~N(0,I)} [||\\epsilon - \\epsilon_\\theta (x_t, t, text) ||]$ (4)\nTo generate samples from a given textual description, we perform noise reduction on the sequence from $p(x_T) = N (x_T; 0, I)$. From Equation. 2, we know that we need to estimate $\\mu_\\theta (x_t, t, text)$ and $\\Sigma_\\theta (x_t, t, text)$. To simplify this, we set $\\Sigma_\\theta (x_t, t, text)$ to a constant $\\beta_t$, and $\\mu_\\theta (x_t, t, text)$ is approximated as Eq. 5:\n$\\mu_\\theta (x_t, t, text) = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{\\sqrt{1-\\alpha_t}}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta (x_t, t, text))$ (5)"}, {"title": "3.4 GUNet and Components", "content": "In the previous section, we introduced the conditional diffusion model, highlighting the critical role of the denoising network $\\epsilon_\\theta (x_t, t, text)$ in the denoising process. In this section, we present the denoising network of PoseDiffusion, GUNet.\nPrevious work(Ho et al. [2020], Dhariwal and Nichol [2021], Nichol et al. [2021], Nichol and Dhariwal [2021]) using a U-Net like structure as a denoising model. Since our skeleton generation task requires the introduction of spatial information about the skeleton during the denoising process, it makes CNNs, a structure for processing image data, unable to explicitly model the pose skeleton. In order to enable diffusion models to process data with explicit spatial architectures, Wen et al. [2023] proposed the use of graph neural networks as a building block for U-Net like structures. However, the representation of the pose skeleton, heatmap, as a kind of 2D image data, graph convolutional neural networks, a structure dedicated to processing structured data, are unable to perform detailed feature extraction on it. Therefore, we propose a U-Net like structure GUNet that combines the advantages of both CNN and GNN, as shown in Fig. 4. Similar to the denoising model of text-driven image generation, our proposed model also includes a text encoder, a pose encoder and a pose decoder.\nText Encoder. In this paper, we use BERT(Devlin [2018]) to obtain the embedding of the text. Specifically, the sentence T describing a pose skeleton is firstly be spilt by the tokenizer of BERT to get $T_{token} = \\{t_1, t_2, .t_L.., \\}$. Then, $T_{token}$ is input to the embedding layer of BERT for encoding, using the embedding of CLS token as the sentence representation, i.e., $T\\in \\mathbb{R}^{1\\times768}$\nPose Encoder and Pose Decoder. Pose Encoder and Decoder are the Pose2Heatmap and Heatmap2Pose modules in Fig. 3. These are two modules that require no training and enable the transformation between a pose skeleton and a set of heatmaps corresponding to it. Specifically, the COCO dataset comes with keypoint coordinates for each pose skeleton of the class. We generate a heatmap of size S \u00d7 S for each keypoints of the pose skeleton, and the value of each pixel point of the heatmap represents the probability that the keypoint occurs here. Assuming that the coordinate"}, {"title": "GUNet.", "content": "In GUNet, we define the human pose as a graph structure, notated as G = (V, E, A), where V represents the set of node features, E represents the set of edges, and A is the adjacency matrix defining the relationships between nodes, defined as Eq. 7:\n$A_{ij} = \\begin{cases} 1, & \\text{if there is an edge between node i and node j} \\\\ 0, & \\text{others} \\end{cases}$ (7)\nAlthough the graph structure is fixed for different human pose skeletons, as the pose changes, the associated heatmap sets will be different and the features of the node set V will be updated.\nAs shown in Fig. 4, GUNet is a U-Net like structure consisting of three down-sampling blocks, three up-sampling blocks, one middle block and one spatial block. Consistent with previous work(Nichol and Dhariwal [2021]), both the downsampling, upsampling and middle blocks consist of CNN layers, and each block contains a self-attention layer and a cross-attention layer for maintaining cross-modal semantic consistency between the textual conditions and the images. We insert a spatial block containing a graph convolutional layer(GCN) and a skip connection after the middle block. The reason is that the image features include a lot of detailed information at the original resolution. The main network structure in the spatial block, GCN, is good at dealing with graph data structures, and its nodes tend to be low-dimensional feature representations.\nWhen the latent embedding N reaches the spatial block of GUNet, we rearrange the dimensions of $N\\in \\mathbb{R}^{bs\\times K_{MID}\\times S_{MID}\\times S_{MID}$ to $N \\in \\mathbb{R}^{K_{MID}\\times bs\\times S_{MID}\\times S_{MID}}$ to facilitate smooth graph convolution computation. The rearranged latent embedding, which serves as the feature for the node set in the graph structure, is then fed into the graph convolution layer. The graph convolution can be formulated as shown in Eq.8.\n$\\Gamma_\\zeta (\\tilde{N}) = \\sigma (\\Phi (A_{gcn}, N) W)$ (8)"}, {"title": "4 Experiments", "content": "In this section, we discuss the experimental design and its results. We selected two baseline models based on GAN(Zhang et al. [2021]): WGAN-LP Regression (for regression prediction) and WGAN-LP (for heatmap prediction). In addition, we included several baseline models proposed in this paper, including SD1.5-T2Pose(Rombach et al. [2022]) with full fine-tuning, PoseAdapter adapted from IPAdapter(Ye et al. [2023]), and UNet-T2H, i.e., GUNet without GCNs. We provide a comprehensive comparison of these models with the GUNet proposed in this paper. The dataset, descriptions of the baseline models, and the qualitative and quantitative results are presented separately below."}, {"title": "4.1 Dataset", "content": "We use the COCO (Lin et al. [2014]) to train and evaluate our models. This dataset contains over 100,000 annotated images of everyday scenes, each accompanied by five natural language descriptions. Among these, about 16,000 images feature a single person, with each person annotated by the coordinates of 17 keypoints. To ensure data quality, we filtered approximately 12,000 single-person images, selecting only those with at least 8 visible keypoints. For each image, one of the five descriptions was randomly selected as the image's label, forming image-text pairs. These pairs were then divided into training and validation sets with a 4:1 ratio. The dataset format for our baseline models differs slightly and will be detailed in Section 4.2."}, {"title": "4.2 Baseline Models", "content": "Our baseline models come from three main sources. First, we fine-tuned Stable Diffusion in two ways: SD1.5-T2P and PoseAdapter, which introduces a pose coding layer. Second, we modified GUNet by removing the graph convolutional neural network layer, resulting in a model we denote as UNet-T2H, where the human posture skeleton is represented as a set of heatmaps. Third, we included two models based on WGAN, focusing on different methods of human posture skeleton representation: WGAN-LP for heatmap prediction and WGAN-LP R for coordinate regression prediction. Next, we will describe the structure and experimental setup of each baseline model in detail."}, {"title": "4.2.1 WGAN-LP & WGAN-LP R", "content": "WGAN-LP and WGAN-LP R are derived from the related work(Zhang et al. [2021]). We follow its dataset and training settings to train the two models, WGAN-LP and WGAN-LP R. The training environment is consistent with the aforementioned models."}, {"title": "4.2.2 SD1.5-T2P & PoseAdapter", "content": "SD1.5-T2P and PoseAdapter are two text-driven pose generation models obtained by fine-tuning Stable Diffusion 1.5. We saved the skeleton as RGB images with the size of 256 \u00d7 256to fine-tune SD1.5 at full volume to obtain SD1.5-T2P. Inspired by IPAdapter, PoseAdapter adds a pose coding module to SD1.5-T2P to introduce the skeletal connection of human posture. During the training of SD1.5-T2P, we updated the parameters of UNet in Stable Diffusion, while freezing the parameters of VAE, CLIP and other components. In PoseAdapter, we have designed a pose encoding block consisting of a linear layer and a normalisation layer. During the training process, the pose embedding block encodes the skeleton features to obtain the pose embedding, which is then connected to the text embedding and used together as inputs to the conditional bootstrap denoising model. The fine-tuning process is similar to SD1.5-T2P. We selected approximately 5,000 images with clearly visible skeleton connections, which were then partitioned into training and testing sets in a 4:1 ratio."}, {"title": "4.2.3 UNet-T2H & GUNet", "content": "UNet-T2H is the base version of the GUNet model without the graph convolutional layer, and we train conditional UNet from scratch using the COCO dataset. We use the Pose2Heatmap module mentioned in section 3.2 to encode the coordinate information of the human postural skeleton in the COCO dataset into heatmaps of size 17 \u00d7 64 \u00d7 64, which are paired with the corresponding textual descriptions, totalling about 12,000 pairs. The datasets are used in a 4:1 ratio for training and testing respectively."}, {"title": "4.3 Qualitative results", "content": "We conducted a series of generative experiments using the above models. The design and results of these experiments are discussed below."}, {"title": "4.4 Quantitative results", "content": "The previous section qualitatively analysed the performance of different models by observing the results generated by the models. In this section, we will design a series of experiments for quantitative evaluation to further validate our conclusions."}, {"title": "5 Conclusions", "content": "In this paper, we present PoseDiffusion, a text-driven human pose skeleton generation architecture based on a diffusion model, comprising GUNet and its two Stable Diffusion-based variants. GUNet is an innovative approach to human pose generation based on diffusion models and graph neural networks. We first describe the rationale behind choosing the UNet architecture as the foundation for the diffusion model and explain the effectiveness of representing human skeletons using heatmaps. To further enhance the model's performance, we introduce a graph convolutional layer into the UNet, creating GUNet, which better captures the spatial correlations and pose information in skeleton generation.\nIn the experimental section, we detail the design and experimental setup of the two variants based on Stable Diffusion. We conducted extensive experiments on the COCO dataset to validate the effectiveness of the proposed model. The results show that PoseDiffusion (containing GUNet and two SD variants) outperforms existing GAN-based baseline models in terms of accuracy and diversity of pose skeleton generation. In addition, when integrated with ControlNet to generate realistic images, PoseDiffusion achieved higher aesthetic scores compared to the GAN-based model. In the final blind user preference test, PoseDiffusion obtained higher user preference percentages for both the human pose skeleton and the realistic image. This indirectly reflects that high-quality skeleton generation can improve the aesthetic quality of subsequent image generation.\nWhile one of the SD-based variants, PoseAdapter, performs well in blind user evaluation experiments, on the one hand, it was able to better compatibilise with ControlNet, partly because it was directly fine-tuned from SD. On the other hand, generation due to the fact that it treats the pose skeleton as a picture for overall generation leads to an overdependence on the quality of the training data and the inability to further optimise it by decoupling the keypoints in the event of generating incorrect connections. Based on this, although PoseAdapter can perform well in ControlNet, there is extremely limited room for future expansion and optimisation. On the contrary, GUNet benefits from the decoupling of key points, which makes it more advantageous in the tasks of multi-person pose generation and enhancing site-specific pose control."}, {"title": "6 Future Work", "content": "The decoupling of keypoint representations in GUNet allows it to outperform PoseAdapter in multi-person pose skeleton generation tasks. Future work should further explore the potential of GUNet in multi-person pose generation and tasks requiring precise control over specific body parts. In current multi-person pose generation methods, there is no way to distinguish which keypoints belong to which individual, leading to unsatisfactory results in generating interactions such as hand-holding or hugging. Extending GUNet to multi-person pose skeleton generation would enable precise control over the keypoints of different individuals.\nFor tasks requiring precise control of specific body parts in human poses, we could map textual descriptions of different body parts in the prompt to corresponding pose regions by adjusting the prompt and incorporating masks. This approach would allow for more accurate manipulation of body part positioning based on text descriptions."}]}