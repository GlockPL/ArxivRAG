{"title": "A SIMPLE YET EFFECTIVE \u2206\u2206G PREDICTOR IS AN UNSUPERVISED ANTIBODY OPTIMIZER AND EXPLAINER", "authors": ["Lirong Wu", "Yunfan Liu", "Haitao Lin", "Yufei Huang", "Guojiang Zhao", "Zhifeng Gao", "Stan Z. Li"], "abstract": "The proteins that exist today have been optimized over billions of years of natural evolution, during which nature creates random mutations and selects them. The discovery of functionally promising mutations is challenged by the limited evolutionary accessible regions, i.e., only a small region on the fitness landscape is beneficial. There have been numerous priors used to constrain protein evolution to regions of landscapes with high-fitness variants, among which the change in binding free energy (AAG) of protein complexes upon mutations is one of the most commonly used priors. However, the huge mutation space poses two challenges: (1) how to improve the efficiency of AAG prediction for fast mutation screening; and (2) how to explain mutation preferences and efficiently explore accessible evolutionary regions. To address these challenges, we propose a lightweight AAG predictor (Light-DDG), which adopts a structure-aware Transformer as the backbone and enhances it by knowledge distilled from existing powerful but computationally heavy AAG predictors. Additionally, we augmented, annotated, and released a large-scale dataset containing millions of mutation data for pre-training Light-DDG. We find that such a simple yet effective Light-DDG can serve as a good unsupervised antibody optimizer and explainer. For the target antibody, we propose a novel Mutation Explainer to learn mutation preferences, which accounts for the marginal benefit of each mutation per residue. To further explore accessible evolutionary regions, we conduct preference-guided antibody optimization and evaluate antibody candidates quickly using Light-DDG to identify desirable mutations. Extensive experiments have demonstrated the effectiveness of Light-DDG in terms of test generalizability, noise robustness, and inference practicality, e.g., 89.7\u00d7 inference acceleration and 15.45% performance gains over previous state-of-the-art baselines. A case study of SARS-CoV-2 further demonstrates the crucial role of Light-DDG for mutation explanation and antibody optimization. Codes are available in Github, and an online Platform is available for researchers.", "sections": [{"title": "1 INTRODUCRTION", "content": "Proteins usually interact with other proteins to form protein complexes that perform specific func-tions in biological processes (Hu et al., 2021; Wu et al., 2024b). A representative example is anti-body, a Y-shaped protein that protects the host by binding to a specific antigen, whose binding func-tion is mainly determined by Complementary Determining Regions (CDRs) in the antibody (Murphy& Weaver, 2016). In practice, how to mutate the amino acids on the interaction surface and select fa-vorable mutations are two fundamental aspects of antibody optimization. There have been many an-tibody design methods proposed, such as MEAN (Kong et al., 2022), RefineGNN (Jin et al., 2021),and dyMEAN (Kong et al., 2023), which train conditional antibody generators on large amounts ofantibody-antigen complexes and then optimize antibodies by applying Iterative Target Augmenta-tion (ITA) algorithm (Yang et al., 2020b) to fine-tune the generators. Despite the great success inconditional generation for mutations, how to build an efficient evolutionary selection sieve (Hayeset al., 2024) for fast screening of mutations remains under-explored. In this paper, we shift theresearch focus from generating to selecting mutations and indirectly explore the underlying fitnesslandscape by focusing on regions where AAGs over mutations are minimized. We demonstrate that"}, {"title": "2 RELATED WORK", "content": "Mutational Effect Prediction. The prediction of mutation effects on single proteins has been wellstudied, which mainly mines co-evolutionary information from protein sequences by Multiple Se-quence Alignments (MSAs) (Frazer et al., 2021; Luo et al., 2021) or Protein Language Models(PLMs) (Meier et al., 2021; Notin et al., 2022). However, predicting the change in binding freeenergy (AAG) of protein complexes upon mutations is more challenging because it involves com-plex interactions between proteins. Computational methods for AAG prediction have undergonea paradigm shift from biophysics-based and statistics-based techniques (Schymkowitz et al., 2005;Park et al., 2016) to Deep Learning (DL) techniques, among which pre-training-based approachesare the most popular solutions. RDE (Luo et al., 2023) pre-trains by using a normalizing flowmodel to estimate the density of sidechain conformations (rotamers). Similarly, DiffAffinity (Liuet al., 2023) also models the side-chain distribution, but with a conditional diffusion model. Be-sides, Mo et al. (2024) proposes a multi-level pre-training framework, ProMIM, to fully capture allthree levels of protein-protein interactions. Recently, Prompt-DDG (Wu et al., 2024c) proposes amicroenvironment-aware hierarchical codebook that generates prompts for better AAG prediction.\nAntibody Optimization. Early approaches for antibody design are mostly energy-based (Adolf-Bryfogle et al., 2018; Lapidoth et al., 2015), and it is recently extended to deep generative models,including RefineGNN (Jin et al., 2021), MEAN (Kong et al., 2022), RAAD (Wu et al., 2024a),DiffAb (Luo et al., 2022), etc. These models train a conditional antibody generator and screen outa number of high-quality antibodies using a \u2206AG predictor. These high-quality antibodies will beused as training data to further fine-tune the antibody generator for directed antibody optimization.In this paper, we rethink the role of AAG prediction for antibody optimization, demonstrating thata simple yet effective AAG predictor can directly serve as a good unsupervised antibody optimizerand explainer, without requiring additional functional annotations or deep generative models."}, {"title": "3 PRELIMINARY", "content": "Notations. A protein complex consists of N amino acid residues (V1, V2,\u00b7\u00b7\u00b7, VN), where eachresidue vi is one of the 20 amino acid types. We characterize each residue vi with an E(3)-invariant node feature xi = {Etype(Vi), Eang (vi), Emut(vi)}, where Etype(vi) denotes the embeddingof residue types, Eangle(vi) is the angle encoding of three dihedral angles and four torsion angles,and Emut (vi) denotes the mutation embedding on whether residue vi is mutated or not. The pairwisefeature between residues vi and vj is ei,j = {Epos(i, j), Edis (Zi, Zj), QZj. Zi, Ca }, whereZ is the 3D coordinate of residue vi, Epos(i, j) and Edis (Zi, Zj) encode the relative sequential andspatial distances between residue vi and residue vj, respectively. Epos (i, j) is set as 0 for any tworesidues that are not on the same chain. Besides, the last term is the direction encoding of four back-bone atoms \u2208 {Ca, C, N, O} of residue vj in the local coordinate frame Qi of residue vi (Wuet al., 2024c). All these node and pairwise features will be pre-processed before model training.\nTransformer as the student backbone in Knowledge Distillation (KD). To improve the inferenceefficiency of a AAG predictor, a lightweight Transformer is used as the backbone to encode eachprotein complex P = (X, E). The l-th (1 < 1 < L) layer of the Transformer is defined as follows\nH(1) = LN (FFN ([head1,..., headK] W) + H(1-1)), where\n$$head_k = softmax\\left(\\frac{((H^{(l-1)}) W_Q^{(k)}) (W_K^{(k)} H^{(l-1)})^T}{\\sqrt{d_h}}+ EW_E^{(k)}\\right) H^{(l-1)} W_V^{(k)}$$\nwhere H(0) = X denote the input node feature, $W_Q^{(k)}, W_K^{(k)}, W_V^{(k)}, W, W_E^{(k,l)}$ are parametermatrices, K is the number of attention heads, LN(\u00b7) is the layer normalization, FFN(\u00b7) is a two-layerfeed-forward neural network with ReLu(\u00b7) as activation function, and dh is the hidden dimension.\nPrompt-DDG as Teacher and Annotator. Prompt-DDG (Wu et al., 2024c) is the state-of-the-artAAG predictor to date. During training, it trains a hierarchical prompt codebook to capture mi-croenvironmental information at different structural scales. With the learned prompt codebook, itencodes the microenvironment around each mutation into multiple hierarchical prompts and com-bines them to flexibly provide information to wild-type and mutated protein complexes about their"}, {"title": "4 \u041c\u0415\u0422\u041dODOLOGY", "content": "In this section, we propose a unified framework for directed antibody optimization with a simple buteffective AAG predictor as the core. A high-level overview of the proposed framework is shownin Fig. 3(b). We first present how to construct a large-scale augmented mutation dataset SKEMPI-Aug by cross-augmentation in Sec. 4.1. Next, we pre-train a simple but effective Light-DDG onthe large-scale augmented SKEMPI-Aug dataset and then fine-tune it by knowledge distillation onthe SKEMPI v2.0 dataset, as described in Sec. 4.2. Further, we propose a Mutation Explainer tolearn key mutation sites and mutation preferences in Sec. 4.3, and finally introduce how to performpreference-guided mutation search in Sec. 4.4. From the perspective of the energy landscape inFig. 3(a), Light-DDG establishes a mapping from mutations to energy changes (AAG), while Mu-tation Explainer iteratively explores evolutionary accessible regions based on mutation preferences."}, {"title": "4.1 A LARGE-SCALE AUGMENTED MUTATION DATASET FOR SUPERVISED PRE-TRAINING", "content": "Considering the scarcity of experimental data in theSKEMPI v2.0 dataset, pre-training on large amountsof mutations-irrelevant data has become a popularpractice for training AAG predictors. One of themost commonly used pre-training datasets is PDB-REDO (Joosten et al., 2014), in which several unsu-pervised pre-training tasks (Luo et al., 2023; Yanget al., 2022; Mo et al., 2024), have been proposed tolearn generalized knowledge. However, in order toimprove the inference efficiency, we use a lightweighttransformer as the backbone in this paper, which hasweaker modeling capability than the IPA-style back-bone, making it hard to directly learn useful knowl-edge patterns for AAG prediction from massive unla-beled data in an unsupervised manner. Therefore, wehere consider supervised pre-training, but the upcom-ing challenge is how to construct a large-scale datasetthat covers a sufficiently wide range of mutation pos-sibilities and their corresponding \u2206\u2206G scores. In thissubsection, we take data augmentation as an effectivemeans of compensating for the simplification of the ar-chitecture. To augment data, we use Prompt-DDG, the current state-of-the-art AAG predictor, asan annotator. Specifically, we perform arbitrary mutations on several randomly selected mutationsites of complexes from the SKEMPI v2.0 dataset, feed the mutated complexes into Prompt-DDGto predict AAG scores, and then package the mutations and predicted AAGs into one piece ofaugmentation data. To prevent data leakage, we propose K-fold cross-augmentation as shown inFig. 2, where the SKEMPI v2.0 dataset is divided into K equal-sized folds according to the com-plex structure. For each round of augmentation, we first train a new Prompt-DDG from scratchwith K-1 folds, then augment the remaining 1 fold by random sampling and random mutation, andfinally annotate it by Prompt-DDG. As a result, the data used to train Prompt-DDG is separate fromthe data annotated by Prompt-DDG to avoid any possible data leakage. Moreover, we set a thresh-old during augmentation to ensure that the augmented samples are sufficiently different from theoriginal samples to further avoid data leakage. In such a way, we have augmented, annotated, andreleased a large-scale dataset called SKEMPI-Aug, which contains millions of mutation data that"}, {"title": "4.2 A SIMPLE BUT EFFECTIVE AAG PREDICTOR BY AUGMENTATION AND DISTILLATION", "content": "Knowledge Distillation (KD) is an effective means of achieving model compression (Hinton et al.,2015), and the distilled student models can even exhibit better performance than the correspondingteacher models. In this paper, we combine knowledge distillation techniques with supervised pre-training to build a simple but effective AAG predictor (Light-DDG). Specifically, we first performsupervised pre-training on the large-scale SKEMPI-Aug dataset $D_{aug}$, and then fine-tune the modelon the SKEMPI v2.0 dataset $D_{skem}$ under the joint supervision of ground-truth labels and distillationlosses. Since cross-validation on SKEMPI v2.0 is performed in this paper to validate the method,the distillation objective of Light-DDG on the training data $D_{train} \\subset D_{skem}$ can be defined as:\n$$f^*_s = arg \\min_{f'_s} \\frac{1}{ |D_{train}|} \\sum_{(a_i, y_i) \\in D_{train}} \\frac{1}{2} (||f'_s(a_i) - y_i||^2_{\\LAAG} + \\beta ||f'_s (a_i) - f^*_t(a_i)||^2_{\\LKD})$$\nwhere \u1e9e is a trade-off hyperparameter, ai = (Pi, Mi) is the input to \u2206AG predictor f(\u00b7), yi is theground-truth A\u2206G. In this paper, we default to Prompt-DDG as the teacher f*(\u00b7), but we also ob-served significant improvements when using other AAG predictors as teachers in the experiments.The student model f'(\u00b7) is initialized by pre-training on the SKEMPI-Aug dataset $D_{aug}$, as follows\n$$f^*_s = arg \\min_{f'_s} \\frac{1}{ |D_{aug}|} \\sum_{(a_i, y_i) \\in D_{Aug}} ||f'_s(a_i) - y_i||^2_{\\LAAG}$$"}, {"title": "4.3 MUTATION EXPLAINER: LEARNING MUTATION SITES AND MUTATION PREFERENCES", "content": "A key challenge in antibody optimization is how multiple mutations combine to influence functionand future mutation trajectories (Ding et al., 2024). With a simple but effective Light-DDG available,we propose a novel Mutation Explainer that can identify key mutation sites and learn mutationpreferences for each residue site. This is achieved by calculating the Shapley value (Shapley et al.,1953) for each mutation at each site as its marginal benefit, which explains very well the \u201caverage\u201dmarginal contribution of each mutation across all mutation combinations. The exact Shapley value\u03c6(i, j) of the i-th (1 \u2264 i \u2264 N) site mutated to j-th (1 \u2264 j \u2264 20) amino acids is defined as follows\n$$\\varphi(i, j) = \\sum_{M \\subseteq S \\backslash {(i,j)}} \\frac{|M|! (|S| - |M| - 1)!}{|S|!} (f_s (P,M \\cup {(i, j)}) \u2013 f_s (P,M))$$\nThe exact Shapley value \u03c6(i, j) is calculated by considering the effects on AAG scores when eachmutation (i, j) is added or removed from the mutation set M. However, it is impractical to enumer-ate all mutation possibilities in the huge mutation space S to calculate an exact Shapley value \u03c6(i, j)."}, {"title": "4.4 PREFERENCE-GUIDED MUTATION SEARCH FOR ANTIBODY OPTIMIZATION", "content": "Mutation and selection are two fundamental aspects of antibody optimization. The previous popularmethods usually train a deep generative model on large amounts of data, and then apply IterativeTarget Augmentation (ITA) to guide directed optimization, i.e., generating favorable mutations. Incontrast, this paper focuses on selection rather than generation. Given a lightweight Light-DDGand a Mutation Explainer, we can directly search for favorable mutations, requiring no additionaldeep generative models. For the target antibody to be optimized, we randomized 10,000 mutated an-tibodies by sampling mutation sites and determining mutation residues based on the site importancepsite and site-wise preferences {$p_{pre}^{(t)}(i)$}. These mutation candidates are then quickly evaluatedusing a lightweight Light-DDG to get the most desirable mutations based on the rankings of theirAAG scores. We have provided pseudo-code in Appendix A about how Light-DDG, MutationExplainer, and Mutation Search are constructed into a unified framework for antibody optimization."}, {"title": "5 EXPERIMENTS", "content": "Datasets. The effectiveness of Light-DDG for AAG prediction is evaluated on the SKEMPIv2.0 (Jankauskait\u0117 et al., 2019) dataset, which contains 348 complexes, 7,085 mutation combina-tions, and corresponding changes in binding free energy, but not any mutated complex structures.We randomly split the SKEMPI v2.0 dataset into 3 folds by complexes and perform 3-fold cross-validation for all methods. For pre-training, different pre-training-based methods use different pre-text tasks and datasets. For example, the PDB-REDO (Joosten et al., 2014) dataset contains 143kunannotated data and has been widely used for unsupervised pre-training by previous methods. Incontrast, Light-DDG is supervised pre-trained on the SKEMPI-Aug datasets that consist of 670k an-notated mutation data. Moreover, the AFDB dataset (Varadi et al., 2022) that contains the sequencesand their corresponding structures predicted by AlphaFold2 can also be used as pre-training data.\nEvaluation Metrics. There are seven metrics used to evaluate AAG prediction, including fiveoverall metrics: (1) Pearson correlation coefficient; (2) Spearman correlation coefficient; (3) RootMean Squared Error (RMSE); (4) Mean Absolute Error (MAE); (5) AUROC. Additionally, (Luoet al., 2023) groups the mutations by structure, calculating the Pearson and Spearman correlationcoefficients for each structure separately, and reporting the average as two per-structure metrics. Forantibody optimization, we take the minimal AAG score of the optimized antibodies as the metric.\nBaselines. We compare Light-DDG with several state-of-the-art AAG prediction methods, includ-ing ESM-1F (Hsu et al., 2022), two variants of MIF (MIF-Alogits and MIF-Network) (Yang et al.,"}, {"title": "5.1 EVALUATION ON AAG PREDICTION", "content": "Performance Comparison. Table. 2 reports 7 evaluation metrics for 18 methods on the SKEMPIv2.0 dataset, from which we observe that Light-DDG significantly outperforms all baselines on all7 metrics, especially on the two critical per-structure metrics. For example, Light-DDG improvesover Prompt-DDG on per-structure Pearson and Spearman by 15.45% and 17.55%, respectively.\nVisualizations. The scatter plots of experimental AAG and predicted AAG for Light-DDG, pre-sented in Fig. 4(a), demonstrate the strong correlation between experimental and predicted results.Besides, we provide the distribution of per-structure Pearson scores in Fig. 4(b), as well as the av-erage results across all structures. Not only does Light-DDG achieve the best average performance,but its distribution is mostly centered on high correlation, with fewer low-correlation structures.Further, we randomly select 8 complexes and present their per-structure Pearson scores in Fig. 4(c),where Light-DDG achieves the best performance on 6 of 8 complexes.\nSingle and Multiple Mutations. We further compare Light-DDG with 7 superior methods fromTable. 2 under single- and multi-point mutations, respectively. The results in Table. 3 show that twostate-of-the-art methods, Prompt-DDG and ProMIM, each have strengths in different metrics andmutation settings. However, Light-DDG significantly outperforms all baselines by a large marginon 14 metrics in both mutation settings, especially more challenging multi-point mutations."}, {"title": "5.2 \u0391\u039d\u03a4\u0399BODY SCREENING AND OPTIMIZATION AGAINST SARS-CoV-2", "content": "Candidate Antibody Screening. The inference-efficient property of Light-DDG makes it well-suited for candidate antibody screening, i.e., identifying desirable mutations from a pool of potentialmutations. We take the computational screening of 494 candidate human antibodies against SARS-CoV-2 as a case study, where all mutations are located at 26 sites within three CDRs of the heavychain. We predict AAGs for all candidate antibodies, rank them in ascending order (lowest AAGin the top), and report in Table. 5 the ranking of five favorable mutations that have been previouslyproven effective (Shan et al., 2022; Wu et al., 2024c). It can be seen that (1) Uni-Anti ranks first ontwo antibodies and second on the other two; (2) only Uni-Anti successfully identifies three mutationswith rankings smaller than 5%; (3) Unit-Anti has the best average ranking of 2.4 among 9 methods.\nAntibody Optimization against SARS-CoV-2. We show the effectiveness of Uni-Anti in opti-mizing a human anti-SARS-CoV-2 antibody to produce variants with lower binding energy. Wefirst compare directed mutations (based on explainable mutation preferences) with random muta-tions in Table. 6, where we evaluate the AAGs of 10,000 sampled candidate antibodies (done withLight-DDG in less than 5 minutes) and filter out the best one. It is evident that directed muta-tions perform better than random mutations, especially on multi-site mutations. For example, jointrandom mutation of three CDRs is surprisingly inferior to mutating only CDR-H2, but directedmutation benefits remarkably from a wider range of mutation sites. Further, we compare severalgenerative antibody optimization methods, including RefineGNN (Jin et al., 2021), MEAN (Konget al., 2022), DiffAb (Luo et al., 2022), and dyMean (Kong et al., 2023). We input their generatedantibodies together with wild-type complex structures into Light-DDG to predict AAGs. Note thatthese methods are conditional generative models focusing on the generation of a single CDR region,and cannot handle the joint optimization of multiple CDRs with official pre-trained models. It canbe seen that regardless of which CDR region is optimized, Uni-Anti has a significant advantage overother baselines. Besides, joint optimization of three CDR regions leads to larger performance gains.\nFurthe, we take Light-DDG as the fitness function and further consider two additional search strate-gies, gradient-guided dWJS (gg-dWJS) (Ikram et al.) and CMA-ES-based (Claussen et al., 2022).It can be observed that (1) CMA-ES-based approach has an advantage over random mutation onlywhen the mutation space is relatively large, probably because the multivariate normal distribution inCMA-ES is not a reasonable prior for antibody mutations. (2) When optimizing a single CDR with asmall mutation space, gg-dWJS slightly outperforms the current SOTA generative model, dyMEAN.However, gg-dWJS cannot benefit from such a large mutation space as Uni-Anti when jointly opti-mizing multiple CDRs. Last but not least, the implementation of these two approaches relies on theefficiency of Light-DDG. More results on antibody optimization can be found in Appendix C."}, {"title": "5.3 VISUALIZATIONS ON EXPLAINABLE MUTATION PREFERENCES", "content": "Single and Pairwise Mutations. We demonstrate how Mutation Explainer can explain and guideantibody optimization, with the anti-SARS-CoV-2 antibody as an example. We present AAGs of"}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "This paper shifts the research focus from generating mutations to evaluating mutational effects andindirectly explores the underlying fitness landscape by focusing on regions where AAGs are min-imized. To this end, we train a simple but effective AAG predictor (Light-DDG) by data augmen-tation and distillation. Furthermore, we show that Light-DDG can serve as a good optimizer andexplainer within a Unified framework for Antibody optimization (Uni-Anti). Extensive experimentsshow the superiority of Uni-Anti in mutational effect prediction, optimization, and explanation.\nBroader Impact. The huge combinatorial space of potential mutations and the scarcity of mutationannotations have long been considered two obstacles straddling the path to unsupervised proteinevolution. On the data side, the released augmented mutation dataset expands the original databy two orders of magnitude and is expected to be a solid data ground for follow-up works. Onthe methodology side, a lightweight AAG predictor is expected to facilitate high-throughput fastmutation screening. In addition, the mutation preference explanations learned by Mutation Explainercan reveal the potential evolutionary paths, providing a powerful guideline for the understanding ofprotein functions and the discovery of high-fitness variants. Last but not least, mutation and selectionare the two pillars of natural evolution. This paper provides a new perspective to achieve a novel,explainable, and unsupervised framework for directed optimization with selection at its core.\nLimitations. Despite the great progress, several limitations still remain. Firstly, AAG is only onecommon prior that constrains the evolution of proteins; combining other priors can still be built ontop of our framework. Secondly, distillation is only one of the strategies to achieve lightweight in-ference, and other architectural choices, quantization, sparsification, and parallelization are also op-tional from an engineering perspective. Thirdly, the design of this paper is expected to be combinedwith deep generative models. We believe that (1) constructing preference pairs using Light-DDGfor preference alignment and (2) taking Light-DDG as guidance in diffusion models for controllablegeneration are two promising solutions. Finally, more case studies on other proteins (in addition toantibodies) and wet experimental assays of the optimized antibodies will be left for future work."}]}