{"title": "EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos with Procedural Texts", "authors": ["Yuto Haneji", "Taichi Nishimura", "Hirotaka Kameko", "Keisuke Shirai", "Tomoya Yoshida", "Keiya Kajimura", "Koki Yamamoto", "Taiyu Cui", "Tomohiro Nishimoto", "Shinsuke Mori"], "abstract": "Mistake action detection from egocentric videos is crucial for developing intelligent archives that detect workers' errors and provide feedback. Previous studies have been limited to specific domains, focused on detecting mistakes from videos without procedural texts, and analyzed whether actions are mistakes. To address these limitations, in this paper, we propose the EgoOops dataset, which includes egocentric videos, procedural texts, and three types of annotations: video-text alignment, mistake labels, and descriptions for mistakes. EgoOops covers five procedural domains and includes 50 egocentric videos. The video-text alignment allows the model to detect mistakes based on both videos and procedural texts. The mistake labels and descriptions enable detailed analysis of real-world mistakes. Based on EgoOops, we tackle two tasks: video-text alignment and mistake detection. For video-text alignment, we enhance the recent StepFormer model with an additional loss for fine-tuning. Based on the alignment results, we propose a multi-modal classifier to predict mistake labels. In our experiments, the proposed methods achieve higher performance than the baselines. In addition, our ablation study demonstrates the effectiveness of combining videos and texts. We will release the dataset and codes upon publication.", "sections": [{"title": "1. Introduction", "content": "When performing procedural tasks such as assembly or scientific experiments, we follow procedural texts to carry them out in the real world. During this process, mistakes negatively impact quality, speed, cost, and safety. Common errors include skipping steps or performing incorrect actions, which can sometimes result in life-or-death situations. To reduce them, the goal of this study is to develop an intelligent video archive that records workers' activities, detects their mistakes, and provides feedback to them."}, {"title": "2. Related work", "content": "In this section, we compare EgoOops with other datasets in terms of two perspectives: procedural activity dataset and mistake action dataset."}, {"title": "2.1. Procedural activity dataset", "content": "Learning models from procedural activities is an important task for real-world applications, including wearable assistants and robot manipulation. Early studies focused on alignment between video segments and steps in the procedural text and proposed datasets by collecting videos from YouTube. Recently, egocentric videos have gained significant attention from researchers because they can record detailed workers' activities. \u0395PIC-KITCHEN consists of 432 egocentric videos with text annotations in the cooking domain. BioVL contains 16 egocentric videos with video-text alignment annotations in the wet-lab experiments. Based on these datasets, researchers develop video-text alignment models, such as ULAC, DWSA, and StepFormer. EgoOops differs from these datasets because it contains not only video-text alignment but also mistake labels. This allows researchers to address both video-text alignment and mistake detection from an egocentric view."}, {"title": "2.2. Mistake action dataset", "content": "Mistake action detection from egocentric videos recently has gained attention from researchers, leading to the proposal of various datasets. Assembly101 initially proposed a task of mistake action detection, featuring segment-level mistake annotations. ATA provides video-level mistake labels across 15 classes specific to the assembly domain. HoloAssist records object manipulation, where instructors verbally intervene in mistakes through a mixed-reality headset. Industreal focuses on industrial-like assembly tasks and collects mistakes. Building on these datasets, previous studies have proposed various models for mistake action detection. PREGO addresses online mistake action detection by combining an action recognition model and LLMs to detect the mistakes. Seminara et al. proposed a method to learn task graphs for detecting skipped steps by introducing a differentiable maximum likelihood loss."}, {"title": "3. Dataset construction", "content": "Our EgoOops dataset consists of egocentric videos and procedural texts. The dataset construction contains the following four steps: (1) task selection, (2) preparation of procedural texts, (3) video recording, and (4) annotation of video-text alignment, mistake actions, and their descriptions."}, {"title": "3.1. Task selection", "content": "We aim to record procedural activities in diverse domains with various mistake actions when following procedural texts. In addition, we aim to collect diverse mistake categories (e.g., unintentional action and working in the wrong way). Based on these criteria, we selected the following five tasks: electrical circuits (EC), color mixture experiments (CM), ionic reaction experiments (IR), toy block building (BB), and cardboard crafts (CB). These tasks satisfy the above criteria; the domains are diverse (e.g., electronics, chemistry, assembly, and crafts) and contain various mistakes (e.g., unintentionally cutting off the cardboard and using with wrong chemicals). In addition, the workers should follow procedural texts to accomplish the task, and the video duration ranges from a few to 30 minutes."}, {"title": "3.2. Preparation of procedural texts", "content": "We prepare procedural texts before recordings. For color mixture experiments and cardboard crafts, we search the web to collect procedural texts. For ion reaction experiments and electrical circuits, we use procedural texts attached to the out-of-the-box kits. For the toy block building, we write procedures from scratch because no resources exist on the web. Based on these procedural texts, we revise them to improve clarity (e.g. specify the tools to be used). Note that all procedural texts were originally written in Japanese, and we manually translated them to prepare the English versions. From the preliminary tests, we found that electrical circuits and toy block building were difficult to complete only with the texts, thus we gave images of the finished products to the workers."}, {"title": "3.3. Video recording", "content": "Participants and environments. Four Japanese graduate students (4 males) performed the tasks following procedural text. They work on every task 2 or 4 times, totaling 10 recordings for each task. The participants were equipped with a head-mounted camera Panasonic HX-A500 as shown in Fig. 2. It is a 30 fps video camera with 4K RGB resolution. To avoid the influence of background changes, a desk with objects, tools, and printed procedural texts is at the same indoor place at every recording session. Participants are recorded sitting for a close view of manipulated objects."}, {"title": "3.4. Annotations", "content": "We annotate video-text alignment, mistake labels, and their descriptions. One person annotates the whole dataset using the web annotation tool. Our annotation process has three steps: (1) video-text alignment, (2) mistake labels, and (3) descriptions explaining why they are mistakes.\nVideo-text alignment. We first annotate video-text alignment by extracting start and end timestamps (i.e., segments) and mapping them to the corresponding steps. To reduce the ambiguity of segment annotations, we define the segment period as the time of grasping the step-related objects to the time of releasing them. Note that We also annotate the steps not written in a procedural text as undefined ones. The segments are matched with step labels. Note that any extra segments that do not correspond to any steps in the procedural texts are labeled as undefined (e.g., grasping the wrong object, correcting mistakes). Workers pause some steps and resume them after others (split steps), skip necessary steps (missing steps), and swap the order from the prescribed one (out-of-order steps), which are kinds of order mistakes.\nMistake label annotation. We define mistakes as deviations from the instructions in procedural texts. This definition leads to two types of mistakes: order mistakes and execution mistakes. Order mistakes occur when there is a discrepancy between the steps executed by workers and the steps outlined in the procedural texts. These include skipping, swapping, inserting extra steps, and splitting steps into temporally distant segments. Execution mistakes occur when the worker fails to follow an instruction. We classify them into the following six types: 1. working with wrong objects (object), 2. grasping wrong objects and releasing them without using (mispick), 3. correction of mistake actions (correction), 4. unintended actions (accident), 5. performing in the wrong way (way), 6. others (others).\nBased on these criteria, if a segment indicates an execution mistake, we attach one of the six execution mistake labels. Note that we do not attach order mistake labels because the video-text alignment itself reveals the order mistakes. For example, if step 2 occurs before step 1, these steps are considered swapped. This categorization helps to analyze mistake patterns across diverse tasks. For example, we can analyze the frequency of mistakes in a specific task.\nDescriptions. In addition, we attach descriptions to the segments with mistake labels to enable virtual assistants to explain why the actions are mistakes. To maintain the quality of the descriptions, a template for each mistake category minimizes the use of modifiers, subjects, and articles."}, {"title": "4. Statistics", "content": "In this section, we first report video- and text-side statistics on EgoOops, then describe mistake action statistics, and finally report the annotation agreement to clarify the quality of the annotations."}, {"title": "4.1. Videos and procedural texts", "content": "Videos. Table 2 shows comparisons of videos between tasks. They have different trends between the tasks in terms of video duration, segments, and the number of segments. For video duration, the longest is the cardboard task at 26.1 minutes, while the shortest is the building block task at 1.9 minutes. The longest segment duration is also in the cardboard task at 87.7 seconds, while the shortest is in the building block task at 8.9 seconds.\nProcedural texts. Table 3 shows comparisons of procedural texts between tasks. They have similar trends to the video statistics in terms of the number of steps in the procedural text and the number of words per step. The task with the most instructions is the cardboard task, while the task with the longest instructions is the building block task."}, {"title": "4.2. Mistake actions", "content": "Table 4 shows comparisons of order and execution mistakes between tasks. 40%(= 92/230) segments are execution mistakes and others are order mistakes. Among the order mistakes, we observe that splitting steps is the most frequent. Table 5 shows the counts of the labels for execution mistakes. We observe some common patterns among tasks. The most two frequent labels are grasping the wrong objects and releasing them without use (label 2) and working in the wrong way or moving (label 5). In addition, we also find that each task has unique mistake patterns (e.g., unintended actions frequently happen in ionic reaction experiments while they do not appear in other tasks)."}, {"title": "4.3. Agreement", "content": "We ensure the quality of the original annotations by testing for agreement with another annotator. Because annotating all of the samples is time-consuming, we randomly choose one out of 10 videos per task. The process takes the following two steps. For video-text alignment, the annotator newly extracts segments and maps them to the corresponding steps. Then, we ask them to annotate mistake labels and descriptions. To evaluate the annotation quality, for video-text alignment, we calculate the temporal Intersection over the Union (tIoU) of segments. Note that we ignore the undefined steps for the calculation. We first calculate tIoU for each step, average by videos, and finally average them as an aggregated score. For mistake labels and descriptions, we calculate Cohen's kappa [4] and BERTScore [46], respectively.\nResults. We confirm that both annotations are high quality. For the video-text alignment, we achieve 88.8, indicating a high agreement of alignment. For mistake labels and descriptions, we achieve 86.8 Cohen's kappa and 96.3 BERTScore, ensuring the high agreement of descriptions."}, {"title": "5. Application", "content": "Based on the constructed EgoOops dataset, we address two application tasks: video-text alignment and mistake action detection. Note that these two tasks are dependent. The outputs of the video-text alignment are used for the mistake action detection. Hence, we first describe the video-text alignment and then mistake action detection."}, {"title": "5.1. Problem formulation", "content": "Let an untrimmed video and corresponding procedural text be (V, T). The video is written as V = (V1,\u06f0\u06f0\u06f0,VL,\u06f0\u06f0\u06f0, VL) consists of L frames. The procedural text is written as T = (t1,\u2026, tk,\u2026\u2026, tk). Given (V, T), the objective is to find alignment between the video and procedural text. Specifically, for k-th step tk, the model finds corresponding start and end timestamps yk = (Sk, ek) in the video as: Y = {yk = (Sk,Ck)| t \u2208 [1, K]}, where Y represents the alignment between the video and text.\nBased on the alignment output, the next task is mistake action detection. Let V' = (Vsk, Ver) be extracted video frames based on k-th start and end timestamps yk. Our objective is to classify the segment V' into one of the labels M = {m1,...,Mn,...,mN}, where N and mn represents the number of labels and a type of labels. We initially used the seven labels (six labels in Sec. 3.4 plus \"correct\" labels) to train a model. However, our preliminary experiments revealed that the model struggled to classify the segments accurately mainly due to the lack of training samples. Hence, we group mistakes except for \u201c3. correction\" into \u201cmistake\" and define the tasks as three label classification (N = 3): \u201ccorrect,\u201d \u201cmistake,\u201d and \u201ccorrection.\""}, {"title": "5.2. Video-text alignment", "content": "For video-text alignment, we enhance StepFormer by introducing an additional loss function, resulting in StepFormer++. We will first provide an overview of the original StepFormer and then explain how we extend it.\nStepFormer. StepFormer was originally proposed for learning video-text alignment from untrimmed web videos in a self-supervised manner [10]. It was trained on HowTo100M [24], which consists of video and narration pairs. StepFormer includes Transformer [41] decoders and learnable U step queries. Given video features extracted from UniVL [21], the Transformer decoder fuses the step queries and video features, producing U contextualized vectors called step slots. Using the step slots and narration vectors extracted from UniVL, a sequence-to-sequence alignment is achieved via Drop-DTW [9]. Based on this alignment, the loss is calculated as InfoNCE [40] at both local (same video-narration pairs) and global (different video-narration pairs) levels. During inference, StepFormer can detect key segments without narrations. Specifically, given video features and U step slots, Drop-DTW selects meaningful slots to describe the video and drops outlier elements.\nWe select StepFormer because self-supervised pre-training of StepFormer can mitigate the negative impact of the small size of EgoOops. In our experiments, instead of HowTo100M, we pre-train StepFormer on Ego4D with EgoVLPv2 [29] features to fill the domain gap between web and egocentric videos. Because Ego4D is a massive-scale egocentric video dataset accompanied by transcriptions [14], we can train StepFormer as the same training procedure as the original one.\nStepFormer++. We modify StepFormer with additional loss to train it on EgoOops in a fully supervised setting by leveraging video-text alignment annotations. Figure 5 shows an overview of StepFormer++. The overall process is the same as the original StepFormer. Given (V, T), the model first extract video and text features using EgoVLPv2 instead of UniVL and obtain video Hv = (h,..., hl,..., hL) and step features Ht = (h1,\u2026,ht\u2026\u2026,hT). Then, the Transformer decoder fuses Hv and slot queries, producing step slots S = (S1, ..., SU,..., SU). Finally, the model acquires the video-text alignment via Drop-DTW by computing a similarity matrix of S and Ht. The InfoNCE loss is computed at only global (different video-narration pairs) level.\nInstead of the local loss, we add a new loss to the StepFormer based on video-text alignment annotations and propose StepFormer++. Based on the selected step slots S = (\u015d1,..., \u015dk,..., \u015dK) after Drop-DTW, we train the model to align the step slots to the start and end timestamps (Sk, ek). Specifically, this is calculated using the InfoNCE framework:\n$\\mathcal{L}_{supervised}(\\hat{s}_k, y_k, V) = - \\log \\frac{\\sum_{v_j \\in [s_k, e_k]} f(\\hat{s}_k, v_j)}{\\sum_{l}f(\\hat{s}_k, v_l)}$,\nwhere $f(\\hat{s}_k, v_*) = \\exp(\\cos(\\hat{s}_k, \\hat{v}_*))/\\gamma)$ and \u03b3 is a scaling temperature. We add this loss to the original ones and train the model on EgoOops."}, {"title": "5.3. Mistake action detection", "content": "Based on the alignment results, we tackle the mistake action detection. To achieve this, we input the video and text to a multi-modal classifier to predict the label."}, {"title": "6. Experiments", "content": "Splits. Our EgoOops dataset is relatively small compared to other action mistake datasets. To ensure reliable results, we perform 5-fold cross-validation. We divide the 50 videos into a 30/10/10 split for training, validation, and testing, respectively. We report the average test-set scores using the model weights that achieve the highest performance on the validation set. To construct folds, we pay attention to the two points. First, each validation and testing fold contains one correct and one mistake video for every task, totaling 10 videos. Second, each fold consist of the same workers' videos as following the group k-fold [32]. This allows us to test the models on unseen worker's activities, minimizing the bypass possibility to learn the worker-specific features to detect mistakes.\nEvaluation on video-text alignment. For evaluation on video-text alignment, we follow previous work [10,35] and utilize frame-wise precision, recall, F1, and Mean over Frames (MoF). Precision is the ratio of correctly predicted frames to the total number of frames predicted as step segments. Recall is the ratio of correctly predicted frames to the total number of step segment frames in the ground truth. The F1 score is the harmonic mean of precision and recall. MoF represents the percentage of correctly predicted frames, including those without step labels.\nEvaluation on mistake action detection. For mistake action detection, we evaluate classification performance for the segments localized by the video-text alignment model. To this end, we follow temporal action localization (TAL) [3, 6, 17, 36, 45] and evaluate mean average precision (mAP) at different tIoU thresholds from 0.1 to 0.3 by 0.1. Note that mAP computes the mean of average prevision across all mistake action classes (Sec. 5.1) except for the \"correct\" label because we focus on mistake detection performance. Note that we compare the mistake labels only if the predicted segments correspond to the same step as the ground truth. This approach evaluates the system appropriately in terms of both step and mistake labels.\nBaselines for video-text alignment. For video-text alignment, we compare StepFormer++ with the original StepFormer [10]. We evaluate two versions. First, we use StepFormer pre-trained on Ego4D and utilize it in a zero-shot manner. We call it StepFormer (ZS). Second, we fine-tune it on EgoOops in a self-supervised manner with losses proposed in [10]. We call it StepFormer (SS).\nBaselines for mistake action detection. We do not adopt existing mistake detection methods as baselines because they do not fit our settings. For instance, Assembly101 [34] and CaptainCook4D [28] assume trimmed video clips as inputs, while our task assumes untrimmed videos. The method in [33] focuses on ordering mistakes in the videos and does not address execution mistakes. EgoPED [19] is the closest to our setting as it predicts both segments and mistake labels. However, their method is based on anomaly detection, predicting binary labels of \u201ccorrect\u201d and \u201cmistakes,\"; thus it cannot predict the three classes of \u201ccorrect,\u201d \u201cmistakes,\u201d and \u201ccorrection.\u201d\nTherefore, instead of existing mistake action detection models, we compare our method with the recent TAL model, ActionFormer [45]. This is because TAL operates under similar conditions, where the models detect both temporal segments and their action labels. In our experiments, we train ActionFormer to predict mistake labels for the detected segments, instead of action labels as in the original settings. For a fair comparison, we apply NMS to retain as many segments as the ground truths. In addition, since our metrics require step labels, we assign them to the segments in order from the start to the end of the videos. Note that the inputs for TAL are videos only.\nImplementation details. We follow the official implementation of StepFormer and utilize the same hyperparameters. We set the number of step queries to be U = 32 and the batch size to be 6 for fine-tuning StepFormer++ on EgoOops. The video and text feature dimension of EgoVLPv2 is d = 4,096. We use Drop-DTW with an 80 percentile drop cost [9] for the alignment between the step slots and video features. We set \u03b3 = 0.03 in the InfoNCE loss and \u03b2 = 0.9999 in the class-balanced loss. For the mistake action detection, we train the classifier in 1,200 epochs.\""}, {"title": "6.1. Experimental settings", "content": "Splits. Our EgoOops dataset is relatively small compared to other action mistake datasets. To ensure reliable results, we perform 5-fold cross-validation. We divide the 50 videos into a 30/10/10 split for training, validation, and testing, respectively. We report the average test-set scores using the model weights that achieve the highest performance on the validation set. To construct folds, we pay attention to the two points. First, each validation and testing fold contains one correct and one mistake video for every task, totaling 10 videos. Second, each fold consist of the same workers' videos as following the group k-fold [32]. This allows us to test the models on unseen worker's activities, minimizing the bypass possibility to learn the worker-specific features to detect mistakes.\nEvaluation on video-text alignment. For evaluation on video-text alignment, we follow previous work [10,35] and utilize frame-wise precision, recall, F1, and Mean over Frames (MoF). Precision is the ratio of correctly predicted frames to the total number of frames predicted as step segments. Recall is the ratio of correctly predicted frames to the total number of step segment frames in the ground truth. The F1 score is the harmonic mean of precision and recall. MoF represents the percentage of correctly predicted frames, including those without step labels.\nEvaluation on mistake action detection. For mistake action detection, we evaluate classification performance for the segments localized by the video-text alignment model. To this end, we follow temporal action localization (TAL) [3, 6, 17, 36, 45] and evaluate mean average precision (mAP) at different tIoU thresholds from 0.1 to 0.3 by 0.1. Note that mAP computes the mean of average prevision across all mistake action classes (Sec. 5.1) except for the \"correct\" label because we focus on mistake detection performance. Note that we compare the mistake labels only if the predicted segments correspond to the same step as the ground truth. This approach evaluates the system appropriately in terms of both step and mistake labels.\nBaselines for video-text alignment. For video-text alignment, we compare StepFormer++ with the original StepFormer [10]. We evaluate two versions. First, we use StepFormer pre-trained on Ego4D and utilize it in a zero-shot manner. We call it StepFormer (ZS). Second, we fine-tune it on EgoOops in a self-supervised manner with losses proposed in [10]. We call it StepFormer (SS).\nBaselines for mistake action detection. We do not adopt existing mistake detection methods as baselines because they do not fit our settings. For instance, Assembly101 [34] and CaptainCook4D [28] assume trimmed video clips as inputs, while our task assumes untrimmed videos. The method in [33] focuses on ordering mistakes in the videos and does not address execution mistakes. EgoPED [19] is the closest to our setting as it predicts both segments and mistake labels. However, their method is based on anomaly detection, predicting binary labels of \u201ccorrect\u201d and \u201cmistakes,\"; thus it cannot predict the three classes of \u201ccorrect,\u201d \u201cmistakes,\u201d and \u201ccorrection.\u201d\nTherefore, instead of existing mistake action detection models, we compare our method with the recent TAL model, ActionFormer [45]. This is because TAL operates under similar conditions, where the models detect both temporal segments and their action labels. In our experiments, we train ActionFormer to predict mistake labels for the detected segments, instead of action labels as in the original settings. For a fair comparison, we apply NMS to retain as many segments as the ground truths. In addition, since our metrics require step labels, we assign them to the segments in order from the start to the end of the videos. Note that the inputs for TAL are videos only.\nImplementation details. We follow the official implementation of StepFormer and utilize the same hyperparameters. We set the number of step queries to be U = 32 and the batch size to be 6 for fine-tuning StepFormer++ on EgoOops. The video and text feature dimension of EgoVLPv2 is d = 4,096. We use Drop-DTW with an 80 percentile drop cost [9] for the alignment between the step slots and video features. We set \u03b3 = 0.03 in the InfoNCE loss and \u03b2 = 0.9999 in the class-balanced loss. For the mistake action detection, we train the classifier in 1,200 epochs.\n\""}, {"title": "Multi-modal classifier.", "content": "Given predicted video segment V' and k-th step tk, the model predicts the label. Specifically, the model first convert V' and tk into video H (ht,..., hlt) and text features $h_t$ using EgoVLPv2. Then, it computes the mean of H, concatenate the averaged vector with $h_t$, and forward it into two-layer perceptrons g with ReLU function as: $z_t = g(concat(mean(H), h_t))$, where $Z_t = (z_{+},\u2026\u2026, z_N,\u2026\u2026\u2026, z_L)$ represents the logit per label. The model applies the argmax operation on the $z_t$ and outputs the prediction label.\nTraining. To train the model, we use the class-balanced cross-entropy loss [5] because the frequency of the \u201ccorrect\u201d label is much higher than other labels. Specifically, using $z_t$, the loss is calculated as:\n$\\mathcal{L}_{classification} (z_t) =  \\frac{1 - \\beta}{1 - \\beta^{r_{mn}}}\\log \\frac{\\exp(z_{mn})}{\\sum_{j} \\exp(z_{j})}$,\nwhere $r_{mn}$ is the number of training samples belonging to the label $m_n$, and \u03b2 \u2208 [0, 1) is a hyperparameter. We adopt teacher forcing [15,41] as a training strategy. Specifically, we input the ground-truth segment of the t-th step to the model, rather than the predicted ones to stabilize training."}, {"title": "6.2. Quantitative results", "content": "Video-text alignment. Table 6 shows the video-text alignment results. These results indicate that StepFormer++ achieves an F1 score of 28.1. This is higher than both the zero-shot and self-supervised versions of StepFormer, which have F1 scores of 24.1 and 26.1, respectively. Therefore, we confirm the effectiveness of StepFormer++.\nMistake action detection. Table 7 shows the results of mistake action detection. The results indicate that the proposed method achieves an average mAP of 2.5, which is higher than ActionFormer's 0.7. However, when compared with the oracle that uses ground truth step segments, an mAP of 2.5 is significantly lower. This suggests that improving video-text alignment performance is essential for achieving higher performance in mistake action detection.\nAblation study. Table 8 presents the ablation study of input modalities for mistake action detection to investigate the importance of procedural texts. From the results, we observe that combining both videos and texts as input achieves an mAP of 2.5. In contrast, using only videos results in an mAP of 0.3. This demonstrates the importance of incorporating both videos and procedural texts into the classifier for effective mistake action detection."}, {"title": "6.3. Qualitative results", "content": "Figure 6 shows video-text alignment results of StepFormer (SS) and StepFormer++. While the self-supervised StepFormer fails to align video segments and text steps, StepFormer++ can correctly find the alignment, demonstrating the effectiveness of the supervised alignment loss.\nFigure 7 shows examples of success and failure cases of mistake action detection. The correct video-text alignment is essential for this task. Because the incorrect alignment hurts the model's prediction, we need to improve the alignment scores continuously."}, {"title": "7. Conclusion", "content": "This paper proposed the EgoOops dataset, which consists of egocentric videos of workers performing five different tasks, procedural texts, and three types of annotations: video-text alignment, mistake labels, and descriptions. The mistake labels revealed both common and unique mistake patterns across the tasks. Based on EgoOops, we addressed video-text alignment and mistake action detection, proposing StepFormer++ and a multi-modal classifier built upon it. The experimental results demonstrated that our method performs better than baselines. Our ablation study showed the effectiveness of the procedural texts. Future work with EgoOops includes improving the performance on both video-text alignment and mistake action detection. We currently do not use QR Codes, but they could be instrumental in achieving this by incorporating object names into the model."}]}