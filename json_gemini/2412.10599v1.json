{"title": "Advances in Transformers for Robotic Applications: A Review", "authors": ["Nikunj Sanghai", "Nik Bear Brown"], "abstract": "The introduction of Transformers architecture has brought about significant break-throughs in Deep Learning (DL), particularly within Natural Language Processing (NLP). Since their inception, Transformers have outperformed many traditional neural network architectures due to their \"self-attention\" mechanism and their scalability across various applications. In this paper, we cover the use of Transformers in Robotics. We go through recent advances and trends in Transformer architectures and examine their integration into robotic perception, planning, and control for autonomous systems. Furthermore, we review past work and recent research on use of Transformers in Robotics as pre-trained foundation models and integration of Transformers with Deep Reinforcement Learning (DRL) for autonomous systems. We discuss how different Transformer variants are being adapted in robotics for reliable planning and perception, increasing human-robot interaction, long-horizon decision-making, and generalization. Finally, we address limitations and challenges, offering insight and suggestions for future research directions.", "sections": [{"title": "Introduction", "content": "Researchers in the domain of robotics have long relied on classical algorithms that have been proven to be extremely effective for a limited range of tasks in structured and predictable environments. Early robotic systems often relied on rule-based systems, where predefined rules could handle a limited range of tasks in a structured environment. A notable early example will be Shakey, developed by Stanford AI Lab in the 1960s which could implement logical reasoning and planning [96]. By the 1990s, probabilistic methods started to be used with Markov Localization to account for sensor data noise and uncertainty in dynamic unstructured environment[33]. Stanford's Stanley, won the 2005 DARPA Grand Challenge, the milestone marked a shift from rule-based approaches to methods that integrated learning and probabilistic reasoning using machine learning and other data-based approaches to tackle the unpredictability of real world environments[142]. However, challenges persisted in developing robots that can operate robustly in dynamic outdoor settings [91]. The transition from classical algorithms to probabilistic approaches has been crucial in addressing uncertainties in robot perception and action [10].\nThe increase in capabilities of Deep Learning architectures began to be integrated into autonomous systems, especially in perception. AlexNet [68] outperformed all classical computer vision (CV) algorithms and popularized convolutional neural networks (CNN). Adoption of Recurrent Neural Networks(RNNs)[122] and Long Short-Term Memory (LSTM) networks[43], which enabled robots to process time-series data for tasks like speech recognition and navigation. Recent advances in scene understanding, sensor fusion, and multimodal approaches have further improved autonomous navigation capabilities[152]. Despite these developments, achieving robust autonomy in unfamiliar outdoor environments remains a significant challenge, requiring sophisticated scene understanding and adaptive navigation strategies [36][152]."}, {"title": "Background", "content": "This section introduces the fundamental concepts of the original Transformers [144] architecture.\nWe start with an overview and then discuss the core components."}, {"title": "Overview of Transformer Architecture", "content": "Traditional sequence models like RNNs were capable of handling variable-length inputs and temporal dependencies [27] [109], LSTMs were a variant of RNNs with an added gating mechanism that addressed long-term dependency problems [159] [168] to an extent, but due to the inherent way RNNs and LSTMs process data, each time step depends on the previous step. This dependency limits parallel processing and struggles with long-term dependencies, as information from earlier inputs can fade over long sequences due to issues such as vanishing gradients [70][4] and explosion [127]. CNNs, though useful for capturing local dependencies in sequences, lack the ability to model long-range dependencies.\nThe Transformer model addresses these limitations by employing self-attention and eliminating recurrence and convolution. Self-attention, also referred to as intra-attention, connects various positions within a single sequence to generate a meaningful representation of that sequence[144]. Self-attention allows each input position to focus on relevant positions across the entire sequence simultaneously, which not only captures dependencies at varying distances but also enables efficient parallelization during training and inference Transformer-based models like BERT and GPT have achieved state-of-the-art results in machine translation and other"}, {"title": "Core Components of Transformer Architecture", "content": "The Transformer architecture consists of an encoder-decoder architecture[144], each consisting of multiple layers stacked together, with each layer containing sublayers designed to carry out a particular operation. The encoder in sequence transduction models processes an input sequence of symbolic representations (x1,...,xn) and transforms it into a corresponding sequence of continuous representations z = (z1,..., zn). The decoder then uses z to generate an output sequence (y1,..., ym), producing one symbol at a time. At each generation step, the model operates in an auto-regressive manner, relying on both the encoded representations and the previously generated outputs as additional inputs for predicting the next symbol."}, {"title": "Encoder and Decoder Architecture", "content": "Encoder: The input sequence is tokenized into smaller units, such as characters or words. The encoder converts the input sequence into a series of contextual embeddings that capture the relationship between these tokens in the input sequence. Each encoder layer has two main components: 1)Self-Attention Mechanism: It enables each token in the input sequence to consider and relate to other tokens, capturing relations between the sequence 2)Feed-Forward Neural Network: The neural network applies non-linear transformations independent of the embeddings of each token. To improve training stability and address issues like vanishing gradient, residual connections are used around each sub-layer. These connections add the input of a sub-layer to its output, followed by layer normalization. The output of each sub-layer is\nLayerNorm(x + Sublayer(x)),\nwhere Sublayer(x) is the output of the sub-layer applied to the input x."}, {"title": "Self-Attention Mechanism", "content": "Self-attention allows each token to be considered and related to every other token in the sequence, enabling the model to learn relationships across long distances.\nScaled Dot-Product Attention: Self-attention is implemented using a scaled dot-product mechanism, where three vectors are computed for each input token:\n\u2022 Key (K): Represents the token holding relevant information.\n\u2022 Value (V): Contains the actual information that is distributed based on the\n\u2022 Query (Q): Represents the token making a request for related information. attention scores.\nThe attention score between a query q and a key k is calculated as:\nAttention(Q, K, V) = softmax ( QKT / sqrt(dk) ).\nHere, dk is the dimensionality of the keys, and the score is scaled by \u221adk to prevent exceedingly large values that could affect the gradient flow.\nMulti-Head Attention: Transformer architecture uses multiple attention heads to capture different aspects of the relationships in the same sequence. Each head has its own learned linear transformations, allowing the model to learn different types of dependencies across the sequence.\nMulti-head attention is calculated by concatenating the outputs of multiple attention heads followed by a linear transformation:\nMultiHead(Q, K, V) = Concat(head1,..., head\u0127)WO\nwhere WO is a learned weight matrix for the output."}, {"title": "Positional Encoding", "content": "Since the Transformer lacks recurrence and does not inherently process tokens sequentially, it must encode positional information to understand the order of tokens. The positional encoding vector is added to the input embeddings to provide each position with unique positional information.\nThe positional encoding for each position pos and dimension i is given by:\nPE(pos, 2i) = sin ( pos / 10000^(2i/dmodel) )\nPE(pos, 2i + 1) = cos ( pos / 10000^(2i/dmodel) )\nwhere dmodel is the dimension of the embeddings. These sine and cosine functions encode relative positions in a way that is interpretable by the model."}, {"title": "Feed-Forward Neural Network (FFN)", "content": "Each position in the sequence is independently passed through a fully connected feed-forward network. The FFN consists of two linear transformations with a ReLU activation in between:\nFFN(x) = max(0, xW1 + b1)W2 + b2\nwhere W1, W2, b1, and b2 are learnable parameters. This component allows for non-linear transformation, which improves the model's ability to capture complex relationships."}, {"title": "Architectural Innovations in Transformers", "content": "Transformers have undergone numerous enhancements to address limitations of the architecture, the quadratic computational complexity of the attention mechanism that limits their ability to handle long sequences [173] being just one of them. We will briefly go over some prominent innovations."}, {"title": "Efficient Transformers", "content": "Efficient transformer variants try to address the computational challenges of long sequences by reducing the quadratic time complexity of the original attention mechanism, making transformers feasible for resource-constrained or real-time environments. There are many different approaches researchers have taken, Performer [20] replaces softmax attention mechanism with kernel-based approximation function, thus being able to handle much longer sequences compared to original transformer, and drastically reduces the memory footprint along with linear computational complexity. Linformer [148] reduces the attention complexity to linear time by projecting the attention matrix, allowing for efficient processing of large state-action spaces."}, {"title": "Multimodal Transformers", "content": "Original Transformers were developed for NLP application specifically to use encoder-decoder mechanism for for text. Multimodal Transformers are capable of processing multiple type of inputs, ViLBERT [86] extended the popular BERT [26] architecture introducing a co-attention mechanism, by exchanging key-value pairs in multi-headed attention, the structure incorporates linguistic queues to visual ones and vice versa, see Figure ??\n\u2022 VisualBERT [74]: Integrates text and image data, with potential applications in robotic vision systems where contextual understanding of visual inputs is required. For example, in human-robot interaction scenarios, understanding both spoken commands and visual context enhances a robot's ability to interact with humans naturally.\n\u2022 UNITER [78]: Pretrained on multimodal datasets, it is useful for tasks requiring joint text and image understanding, such as scene description and semantic mapping, which can improve the situational awareness of service robots."}, {"title": "Sparse and Adaptive Transformers", "content": "Sparse transformers introduce sparsity in the self-attention mechanism to reduce computational overhead without sacrificing performance. Adaptive transformers dynamically adjust computation based on input data characteristics, optimizing resource usage\u2014crucial for deploying transformers in real-time robotic systems. Longformer [8] introduces a combination of global and sliding window local attention mechanisms, suitable for tasks requiring long-range dependencies. Reformer [65] Reduces time complexity in processing large datasets through locality-sensitive hashing."}, {"title": "Adaptive Attention Span Transformer [131]", "content": "Adjusts the attention span dynamically extends significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. Many researchers have considered different approaches across various applications some names include Universal Transformer [25], Dynamic Vision Transformer [151] and Feedback Transformer [30]."}, {"title": "Overview of Reinforcement Learning", "content": "Reinforcement Learning (RL) is a computational approach of understanding and automating goal-directed learning and decision-making. In RL, an agent learns to make decisions by performing actions in an environment to maximize cumulative rewards [136]. The agent interacts with the environment in discrete time steps, receiving observations and rewards, and updates its policy a mapping from states to actions\u2014based on this feedback.\nIn the context of robotics, RL provides a policy for robots to learn optimal actions through trial and error, without explicit programming of the desired task [66]. RL algorithms can be categorized into model-based and model-free methods. Model-based RL involves learning a model of the environment's dynamics, while model-free RL directly learns the policy or value function without an explicit model.\nClassical RL approaches faced challenges in handling high-dimensional state and action spaces typical in robotics. However, the integration of deep learning with RL, known as Deep Reinforcement Learning (DRL), has enabled the handling of complex, high-dimensional inputs like images and sensor data [89]. DRL has shown success in various domains, including robotic manipulation, navigation, and control tasks [72]."}, {"title": "Innovations in Reinforcement Learning", "content": "Recent innovations in RL focus on improving sample efficiency, stability, and generalization of learning algorithms. Key advancements include:\n\u2022 Policy Gradient Methods: Algorithms like Proximal Policy Optimization (PPO) [124] and Trust Region Policy Optimization (TRPO) [123] have improved the stability of policy updates, facilitating more reliable training in continuous action spaces common in robotics.\n\u2022 Off-Policy Learning: Techniques such as Deep Deterministic Policy Gradient (DDPG) [79] and Soft Actor-Critic (SAC) [39] enable efficient learning from off-policy data, enhancing sample efficiency which is critical when interactions with the real world are costly or limited.\n\u2022 Model-Based RL: Methods that incorporate learning a model of the environment, such as Model-Based Policy Optimization (MBPO) [51], have shown improved sample efficiency by utilizing simulated experiences, which is particularly beneficial in robotics where real-world data collection is expensive.\n\u2022 Meta-RL and Transfer Learning: Approaches like Model-Agnostic Meta-Learning (MAML) [31] allow agents to rapidly adapt to new tasks with minimal data, addressing the challenge of generalization in RL.\n\u2022 Hierarchical Reinforcement Learning (HRL): Techniques such as the Options Framework [6] and Hierarchical DDPG [92] enable agents to learn policies at multiple levels of abstraction, making them efficient in solving tasks with hierarchical structures, such as multi-step navigation or manipulation [6].\n\u2022 Exploration Strategies: Strategies like intrinsic motivation (e.g., curiosity-driven learning) [107] and uncertainty-aware exploration (e.g., bootstrapped DQN) [97] enhance RL's ability to explore efficiently in sparse-reward environments.\n\u2022 Safety in RL: Methods such as Constrained Policy Optimization (CPO) [2] and Reward Shaping [95] introduce safety constraints during training, ensuring safe exploration and operation, especially important in robotics.\n\u2022 Multi-Agent Reinforcement Learning (MARL): Frameworks like Multi-Agent Deep Deterministic Policy Gradient (MADDPG) [85] and QMIX [118] extend RL to multi-agent settings, enabling collaboration and competition in complex environments like swarm robotics or team-based games.\n\u2022 Offline Reinforcement Learning: Techniques like Conservative Q-Learning (CQL) [69] train agents on previously collected datasets without additional interaction with the environment, making them practical for real-world scenarios where data collection is expensive.\n\u2022 Incorporation of Attention Mechanisms: The integration of attention mechanisms, particularly through Transformer architectures, has enhanced the ability of RL agents to focus on relevant parts of the input space, improving performance in tasks requiring long-term dependencies and memory [103].\nThese innovations have significantly advanced the capability of RL algorithms, making them more applicable to the complex and dynamic environments encountered in robotics."}, {"title": "Applications in Robotics", "content": "Recently there have been numerous studies that have applied Transformers architecture, we will be broadly discuss various applications in detail."}, {"title": "Transformers as Pre-trained Foundation Models", "content": "In this section we discuss primarily 2 broad topics in which foundation models are being utilized in zero-shot robot generalization and Human-Robot collaborations. Pretrained Large Language Models (LLMs) [113], Large Vision-Language Models (VLMs), and Vision-Language-Action Models (VLA) [12][11] are currently being used to increase generalization in Robotics. Prior to the emergence of foundation models, traditional deep learning models for robotics were typically trained on limited datasets gathered for distinct tasks [133] [32]. Much progress has been done via collaboration, datasets such as Open X-Embodiment [21] consist of data from 22 different"}, {"title": "Transformer Variants Integrated in Deep Reinforcement Learning", "content": "Many studies have been conducted to integrate transformers to address the challenges in Deep Reinforcement Leaning(DRL) such as sample efficiency and partial observability [3][77]. Transformers have shown to be effective as world models, enabling sample-efficient learning in complex environments [101][88]. Actor-Learner distillation has been proposed to allow a large complex \"learner\" model based on transformer architecture to transfer its learning progress to a smaller simpler LSTM \"actor\" model, having low inference times while maintaining low sample efficiency of transformers[102]. Transformers have also been used to encode temporal logic into latent task features, allowing hierarchal reinforcement learning for long-horizon manipulation [84]. Innovations such as the Online Decision Transformer (ODT) integrate offline pre-training with online fine-tuning, using sequence modeling for reinforcement leaning (RL), bridging the performance gap when offline RL models are suboptimal for tasks involving online interactions [170]. Constrained Decision Transformer(CDT) integrates transformers to solve offline RL problems by learning high-reward policies while adhering to strict safety constraints [82]\nDecision Transducer (DTd), a multimodal Transformer framework for offline reinforcement learning (RL) [150]. Traditional RL approaches treat trajectories (state, action, reward) as a single sequence. DTd disentangles these into three unimodal sequences (states, actions, rewards) and processes them independently before selectively fusing them for decision-making.\nTransformers have been integrated with Deep Reinforcement Learning to address challenges in planning and decision-making over long horizons. The sequential modeling capabilities of Transformers enable RL agents to consider extended sequences of actions and states, improving performance in complex tasks. Language Models have also been shown to implement policy iteration faster than both imitation learning and gradient descent based RL [13] Notable Transformer-based RL models include:\n\u2022 Decision Transformer [17]: This model formulates RL as a sequence modeling problem, using a Transformer to predict future actions based on past states, actions, and returns. It has shown promise in handling long-horizon tasks without explicit policy optimization.\n\u2022 Trajectory Transformer [52]: This approach models the distribution of trajectories in a dataset, enabling planning by sampling and optimizing over possible future trajectories. It allows for efficient utilization of offline datasets, beneficial in robotics where collecting new data can be expensive.\n\u2022 GTrXL (Gated Transformer-XL) [103]: Enhances the stability of Transformer architectures in RL by incorporating gating mechanisms, improving performance on tasks requiring long-term memory and reasoning.\n\u2022 Perceiver [49]: A model capable of handling multimodal inputs and scalable to high-dimensional data, suitable for robotic applications involving complex sensory information processing.\n\u2022 Behavior Transformers [125]: Combines Transformers with behavior cloning to enable RL agents to mimic human actions effectively in imitation learning setups.\nThe integration of Transformers in DRL offers several advantages for robotics:\n\u2022 Long-Horizon Planning: Ability to model dependencies over long time scales, essential for tasks like navigation and manipulation where early decisions affect long-term outcomes [153].\n\u2022 Sample Efficiency: Improved utilization of collected data through better generalization and representation learning, reducing the need for extensive interactions with the environment [67].\n\u2022 Handling High-Dimensional Inputs: Effective processing of complex sensory inputs, including images, point clouds, and proprioceptive data, enabling richer perception capabilities [120]."}, {"title": "Transformers in Perception, Planning and Control", "content": "This section gives the readers a section-wise overview of current use of Transformers for perception, planning, and control."}, {"title": "Perception", "content": "Vision Transformers (ViTs) [29] have been used in Robotics, particularly in manipulation and navigation tasks. ViTs have shown promising results in 3D object manipulation [37] and real-world control of manipulators [12][120][62]. Models like CLIP [112] (Contrastive Language-Image Pretraining) developed by OpenAI, have enabled robots to align textual and visual modalities effectively, allowing them to interpret human instructions, classify objects, and even adapt to novel scenarios with zero-shot learning. This capability is particularly useful for building generalizable models in unstructured environments where predefined object categories or explicit annotations may not be available. CLIP has been utilized for tasks such as real-time object recognition, semantic scene understanding, and human-robot interaction, enhancing robots' contextual awareness and adaptability. Subsequent work has decreased the memory footprint of CLIP [134], expanded the applications to 3D point cloud scene understanding [166][18] and grounding, which is the process of associating objects with text descriptions [75][117]. Meanwhile, the Segment Anything Model (SAM)[64], a foundation model for image segmentation, has revolutionized how robots process and segment visual data in real-time. By generating accurate object masks from any point prompt within an image, SAM enables precise object manipulation and navigation in cluttered environments, which is critical for domains like warehouse automation and medical robotics. Researchers have integrated SAM into robotic systems for tasks like autonomous grasping and visual servoing, where the ability to identify and segment objects dynamically ensures better control and decision making. Current research has broadened the scope of the application by reducing computational cost [58] and introducing video segmentation using a memory mechanism to process frames sequentially storing and using information from previous frames to predict masks over time [119]. While VLMs excel at tasks like prediction or segmentation, they lack a deep understanding of the physical attributes of the objects, PG-InstructBLIP [35], fine tunes InstructBLIP [22] model to improve model's ability to reason about physical concepts."}, {"title": "Planning", "content": "Transformers model sequences of actions and observations, aiding in long-horizon decision-making for autonomous robots. Their ability to maintain long-term dependencies allows them to plan paths that maximize safety and efficiency. Decision Transformers [17] treat RL as a sequence modeling problem, making them suitable for long-term planning in autonomous vehicles. Gated Transformer-XL [23] for Long-Term Memory can also be used for the same purpose. To increase robustness in planning over long horizons, researchers have proposed introducing human-like reasoning capabilities to generate sub-tasks, and getting top-k hypothesis and the best selection is made based on feedback from the environment [83]. Another approach taken is to facilitate planning by creating out-of-distribution (OOD) scenarios using LLMs for both environmental scenarios such as extreme weather or road obstructions and unpredictable behavior from other road users [1]. Often a limitation for planning could be the limited number of tasks a planner can execute, using LLMs to facilitate a natural language task breakdown, and using neural descriptor fields (NDFS) to learn new tasks in a few shots [99]. Recently, the ALOHA 2 platform uses a transformer-based architecture with diffusion policy to perform challenging bi-manual manipulation tasks [169]. ViNT, a Transformer-based model for visual navigation, demonstrates significant potential in long-horizon decision-making by enabling zero-shot generalization across diverse environments. It employs an EfficientNet encoder for tokenizing image inputs and a diffusion model for proposing subgoals, making it effective in complex, unstructured scenarios. Some researchers have also explored use of human feedback to improve long-horizon task success rates, and have noticed a 20% increase in task completion rate [128]."}, {"title": "Control", "content": "At the time of writing of this article (December,2024), integration of transformers in control systems remains relatively unexplored, but few selected studies have shown great promise and performance improvements. Decision Transformer (DT)-based control strategies have been shown to generalize easily attaining reasonable performance in previously unseen (zero-shot) control tasks, and outperform other control strategies with 10 shot adaptations, similarly in high perturbation environment DT is shown to generalize better than H2/H\u221e. DT has been shown to capture parameter-agnostic structures [167]. TransformerMPC, comprehensively demonstrated use of Transformers to accelerate performance by reducing upto 95.8% inactive constraints, and substantial time reduction for wheeled bipeds, quadrotors and humanoid robots in the range of 6.8x to 34.9x [172]. Some researchers have taken a completely different approach using Convolutional Vision Transformer(CvT), \"DonkeyNet\", as a replacement for MPC, in highly nonlinear scenarios, achieving up to 3x faster times, maintaining consistently low computational time of 20-40 ms while MPC showed significant spikes for complex path points [42]. Architectures like MetaMorph, using transformers to create a universal controller capable of generalizing to 100 different robots, showing strong zero-shot performance and 2-3x more sample efficient during fine-tuning [38]."}, {"title": "Challenges and Limitations", "content": "During the course of this paper, the authors have tried to give a broad understanding about the different ways in which Transformers are being applied in robotics. Despite their potential, several challenges remain, including difficulty in generalization across environments. Foundation Models are being used in various operations from robot manipulation, navigation, to perception. One of the major challenges has been generation of datasets, for training and datasets generated for manipulation may not easily be transferrable to train other subsystems like navigation models for mobile robots. Many researchers are able to create datasets in simulation environment, but scaling and testing same models for real world remains challenging. The"}, {"title": "Future Directions", "content": "Current SOTA platforms like RT-2 [11] use cloud deployment and rely on the frequency of responses over the cloud for control, which can limit deployment in applications in areas of low connectivity and online computations on hardware. OpenVLA [62] has demonstrated the use of quantization to reduce the memory footprint of generalist models while maintaining low interference times to avoid latency in the system. Creating generalization for robots under resource-constrained models, and optimization of current computations can enable real-time online computations. Recently CrossFormer [28], demonstrated the ability to take in data from any robot embodiment, and the same network weights can control robots of vastly different dynamics, ranging from single and dual arm manipulation systems to quadcopters. Future works with more diverse datasets can provide an opportunity for generalist models capable of working with humans in a collaborative environment. Current generalist policies show variations in performance when deployed across different embodiments. Platforms like DART [106] make low-cost robot manipulation data collection possible using human demonstrations. It is yet to be determined whether or not data collected from platforms like DART can be successfully used to train generalist models, so far no generalist model is using data collected from human demonstrations. The emphasis needs to be on sample-efficient human demonstrations and some research has shown promise for robots to acquire new skills in a few shots [100]. Other possible directions may include reconciling the performance disparities in simulations with the real world. Some platforms like RealTo [143] have already shown an excellent increase in the robustness of real-world performance."}, {"title": "Conclusion", "content": "Transformers have fundamentally transformed NLP with the advent of models like GPT and BERT. Their use in robotics is becoming more prevalent. The study details their role in augmenting human-robot interaction with the use of foundation models such as LLMs. We explore the advancements in zero-shot generalization that have been achieved particularly in the manipulation and controlled environment using various mechanisms including co-training with VLMs. We look over how various augmentations like Vision Transformers and subsequent visuo-linguistic models have provided zero-shot or few-shot generalization capabilities to robots for perception, how transformers integrated with RL have provided the ability to perform tasks over longer horizons, and how predictive abilities of transformers are being used to augment control system performance and reduce computational load. While use of multimodal transformers have given us the ability to integrate various sources, zero-shot generalization in scenarios such as factory floors, search and rescue, caregiving remain untested due to paucity of readily available"}]}