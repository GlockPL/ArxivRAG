{"title": "Gradient Inversion Attack on Graph Neural Networks", "authors": ["Divya Anand Sinha", "Yezi Liu", "Ruijie Du", "Yanning Shen"], "abstract": "Graph federated learning is of essential importance for training over large graph datasets\nwhile protecting data privacy, where each client stores a subset of local graph data, while\nthe server collects the local gradients and broadcasts only the aggregated gradients. Recent\nstudies reveal that a malicious attacker can steal private image data from gradient exchanging\nof neural networks during federated learning. However, none of the existing works have\nstudied the vulnerability of graph data and graph neural networks under such attack. To\nanswer this question, the present paper studies the problem of whether private data can be\nrecovered from leaked gradients in both node classification and graph classification tasks and\nproposes a novel attack named Graph Leakage from Gradients (GLG). Two widely-used\nGNN frameworks are analyzed, namely GCN and GraphSAGE. The effects of different model\nsettings on recovery are extensively discussed. Through theoretical analysis and empirical\nvalidation, it is shown that parts of the graph data can be leaked from the gradients.", "sections": [{"title": "1 Introduction", "content": "Federated Learning McMahan et al. (2016b) is a distributed learning paradigm that has gained increasing\nattention. Various organizations in healthcare rely on federated learning to train models while also keeping\nuser data private. Gradient averaging is a widely used mechanism in federated learning, where clients send\nthe gradients of the models instead of the actual private data to a central server. The server then updates the\nmodel by taking the average gradient over all the clients. The computation is executed independently on each\nclient and synchronized via exchanging gradients between the server Li et al. (2014); Iandola et al. (2016)\nand the clients Patarasuk & Yuan (2009). It enables the training of a global model using data distributed at\nmultiple sources without the need to send them to a central location. This scheme is widely used when the\ntraining set is large and/or contains private information Jochems et al. (2016); Kone\u010dn\u00fd et al. (2016). For\nexample, multiple hospitals train a model jointly without sharing their patients' medical records Jochems\net al. (2017); McMahan et al. (2016a).\nRecently, several efforts have demonstrated that gradients can leak input data through gradient inversion\nattacks. Jeon et al. (2021); Zhu et al. (2019); Geiping et al. (2020); Zhao et al. (2020b); Yin et al. (2021);\nZhu & Blaschko (2020); Jin et al. (2021) successfully reconstruct the users' private image data leveraging the\ngradients. Specifically, Zhao et al. (2020a) proposes that the ground truth labels can be directly inferred from\nthe gradients. Consequently, ensuing works improve the attack techniques on large batches of user data Yin\net al. (2021); Fowl et al. (2022b). A few recent works study recovering text data in federated learning. To\nfacilitate the text recovery, Boenisch et al. (2021); Fowl et al. (2022a) recover text data using a strong threat\nmodel in which the server is malicious and is able to manipulate the training model's weights, while Gupta"}, {"title": "2 Preliminaries", "content": "Given a graph $G := (V,E)$, where $V := {V_1, V_2, \u2026, v_v }$ represents the node set and $E \\leq N \\times N$ denotes\nthe edge set. Matrices $X \\in R^{N\\times D}$ and $A \\in {0,1}^{N\\times N}$ represent the node feature and adjacency matrix\nrespectively, where $A_{ij} = 1$ if and only if ${v_i, v_j} \\in E$. $\\bar{A} \\in R^{N\\times N}$ denotes the normalized adjacency\nmatrix derived from $A$. In a GCN framework $\\bar{A} = D^{-\\frac{1}{2}} (A + I)D^{-\\frac{1}{2}}$, where $D \\in R^{N\\times N}$ is the diagonal\ndegree matrix with i-th diagonal entry $d_i$ denoting the degree of node $v_i$. While for GraphSAGE with mean"}, {"title": "2.1 GNN Frameworks", "content": "To pave the road for ensuing analysis and algorithm design, we first introduce GraphSAGE and Graph\nConvolutional Network (GCN) frameworks for both node and graph classification tasks below."}, {"title": "2.1.1 Node Classification:", "content": "Consider a GraphSAGE layer with weights $W_1 \\in R^{F \\times D}$, $W_2 \\in R^{F \\times D}$, and bias $b \\in R^{1 \\times F}$ Let $h_v^l$ and $h_{agg}^l$\ndenote the hidden and aggregated representation of the node $v$ at layer $l$ and $\\sigma$ is a non-linear activation\nfunction. Then the GraphSAGE layer can be expressed as\n$$h_v^l = \\sigma(h_v^{l-1}W_1^l + h_{agg}^{l-1}W_2^l + b),$$\nwith $h_v^0 = x_v$ and $h_{agg}^0 = x_{agg}$ denoting the feature vector and aggregated representation of node $v$\nrespectively. For a GraphSAGE layer with mean aggregation the aggregated representation of a node $h_{agg}^{l-1}$\ncan be obtained as\n$$h_{agg}^{l-1} = \\text{mean}_{j\\in N_v \\cup {v}} (h_v^{l-1}).$$\nSimilarly, for a GCN layer with weights $W \\in R^{F \\times D}$ and bias $b \\in R^{1 \\times F}$, the representation of a target node\n$v$ can be obtained via\n$$h_v^l = \\sigma(\\bar{A}h_v^{l-1}W^T + b).$$ \nHere $\\bar{A}h_v^{l-1}$ is obtained by aggregating the neighboring nodal feature vectors as\n$$\\bar{A}h_v^{l-1} = \\frac{\\sum_{j\\in N_v \\cup {v}}h_v^{l-1}}{\\sqrt{\\tilde{d}_v \\tilde{d}_j}}.$$\nFor a node classification task using an $L$ layer GraphSAGE or GCN framework let $K$ denote the number of\nclasses. Also, let $h_v^L \\in R^{1\\times K}$ denote the output of the final layer and $p = h_v^L$. The final Cross-Entropy loss\nof a node $v$ can be written as\n$$L_v = - \\text{log} \\frac{e^{p_k}}{\\sum_{j=1}^K e^{p_j}}$$\nwhere $k$ is the corresponding ground-truth label and $p_j$ denotes the jth element of the vector p."}, {"title": "2.1.2 Graph Classification:", "content": "Consider an N-node graph with $X$ and $A$ as the node feature matrix and the normalized adjacency matrix\nrespectively, let $H^l \\in R^{N\\times F}$ denote the hidden representations of the nodes at layer l. The output of a\nGraphSAGE layer can be written as\n$$H^l = \\sigma(\\bar{A}H^{l-1}W_1^l + H^{l-1}W_2^l + \\bar{1}b),$$\nwith $H^0 = X$. Here $\\bar{1}$ denotes a column vector of all ones. Similarly, the output of a GCN layer can be\nwritten as\n$$H^l = \\sigma(\\bar{A}H^{l-1}W^T + \\bar{1}b).$$"}, {"title": "2.2 Federated Graph Learning (FGL)", "content": "A typical FL framework consists of a server and $C$ clients. In the node-level FGL, each client samples a\nmini-batch of $B$ target nodes ${X_{v_1},\u2026\u2026,X_{v_B}}$ from the graph which is used to compute the gradients of the\nloss function in equation 5 with respect to the model parameters. In the graph-level FGL, at each step t,\neach client has a set of multiple graphs. Similar to the node-level FGL the cth client samples a minibatch of\n$B$ graphs $G_{c_1},\u2026\u2026,G_{c_B}$ which are then used to compute the gradients and train the model. The gradients\nfrom $N$ clients are then broadcast to the server for updating the weights as\n$$\\nabla w_t L = \\frac{1}{B\\cdot C} \\sum_{c=1}^C \\sum_{i=1}^B \\nabla w_t L_{c,i},$$\n$$W^{t+1} = W^t \u2013 \\eta \\nabla w_t L$$\nwhere $w_t L_{c,i}$ denotes the gradient of the loss at the c-th client for the i-th data sample."}, {"title": "3 Gradient Inversion Attack for GNNs", "content": "We consider the scenario where there is a curious but honest server, i.e., the server has access to the model\ngradient with respect to the client's private graph data. Different attack scenarios are extensively discussed\nwhere the server may also have access to different amounts of information."}, {"title": "3.1 Threat Model", "content": "The attacker is an honest-but-curious server that aims to recover the user's graph data given the gradients of\na GNN model. We thoroughly investigate attacks targeting both node classification and graph classification\ntasks. In addition to possessing knowledge of the gradients, the attacker may also have access to varying\namounts of information about the user's private graph data. Hence we categorize the attacks based on the\ntask as follows:\nNode Attacker 1: has access to the gradients of a node classification model for the target node and recovers\nthe nodal features of the target node and its neighbors.\nNode Attacker 2: has access to the gradients of a node classification model for all the nodes in an\negonet/subgraph. Specifically, the gradients are obtained for the loss function $L_v \\in R^{1\\times N}$ with respect to the\nneural networks weights, where each element $L_{v_i}$ denotes the loss for node $v_i$. Unlike traditional classification\ntasks, the adjacency information is also significant for graph data. In this setting, the graph structure could\nbe unknown. The honest-but-curious server would also try to recover the nodal features and/or the graph\nstructure. Based on the input, we categorize the attacker in more detail as follows\n\u2022\n\u2022\n\u2022\nNode Attacker 2-a: has access to the gradients of all the nodes in an egonet/subgraph along with\nthe nodal features and recovers the underlying graph structure.\nNode Attacker 2-b: has access to the gradients of all the nodes in an egonet/subgraph along with\nthe graph structure and recovers the nodal features.\nNode Attacker 2-c: has access only to the gradients of all nodes in an egonet/subgraph and\nrecovers the graph structure as well as the nodal features."}, {"title": "3.2 Attack Mechanisms", "content": "In this section, GLG is introduced to attack a GNN framework and steal private user data using the leaked\ngradients in a federated graph learning setting. Specifically, given the gradients of the model from a client\nfor the i-th data sample $\\nabla w L_{c,i}$, the goal is to recover the input graph data sample that was fed to the\nmodel. For simplicity, we denote $\\nabla w L_{c,i}$ as $\\nabla w L$ for the graph classification task, and as $\\nabla w L$ for the\nnode classification task. In Zhu et al. (2019), the input data to a model is recovered by matching the dummy\ngradients with the leaked gradients. The dummy gradients are obtained by feeding dummy input data to the\nmodel. Given the gradients at a certain time step as $\\nabla w L$, the input data is recovered by minimizing the\nfollowing objective function\n$$D = ||\\nabla_w\\mathcal{L} - \\nabla_w \\tilde{\\mathcal{L}}||_2.$$\nHere $\\nabla w \\tilde{\\mathcal{L}}$ is the gradient of the loss obtained from the dummy input data. In Geiping et al. (2020) the\ncosine loss optimization function is used instead of the $l_2$ loss in equation 11. Specifically, the objective is to\nminimize the cosine loss between the actual gradients shared by a client and the gradients obtained from\ndummy input data\n$$D=1-\\frac{\\nabla_w \\mathcal{L}\\cdot\\nabla_w \\tilde{\\mathcal{L}}}{||\nabla_w \\mathcal{L}||||\\nabla_w \\tilde{\\mathcal{L}}||}$$\nIn this work, it is assumed that the input label is known since in a classification task with cross-entropy loss,\nthe labels can be readily inferred from the gradients Zhao et al. (2020a). For further details regarding this\nplease refer to Appendix B. Many real-world graphs, such as social networks have the property that connected\nnodes are similar to each other (feature smoothness). To ensure the smoothness of the reconstructed graph\ndata, the following loss function can be used similar to Zhang et al. (2022).\n$$L_s = tr(XLX^T) = \\sum_{i,j\\in E} \\frac{X_{iv}X_{jv}}{\\sqrt{d_i} \\sqrt{d_j}}.$$\nIn addition, the Frobenius norm regularizer is also introduced such that the norm of A is bounded. Overall,\nthe final objective can be written as\n$$D = D + \\alpha L_s + \\beta||A||$$\nwhere $D$ is defined in equation 12, $\\alpha$ and $\\beta$ are hyperparameters. The iterative algorithm for recovering the\nprivate data from the gradients in a node classification task and a graph classification task are summarized\nin algorithms 1, 2 and 3, respectively. Specifically, Node Attacker 1 utilizes algorithm 1 for recovering the\nnodal features of the target node and its neighbors. Node Attackers 2 (a, b and c) utilizes algorithm 2 for\nrecovering the nodal features and graph structure given the gradients of the loss function for each node in the\negonet/subgraph. By default, algorithm 2 assumes that both nodal features and graph structure are unknown"}, {"title": "4 Theoretical Justification", "content": "While prior work Geiping et al. (2020) has shown that the input to a fully-connected layer can be recovered\nfrom the layer's gradients, the analysis of GNNs is largely under-explored. Below we will extensively study\nwhether similar conclusions apply to GraphSAGE and GCN and what parts of the graph input can be\nrecovered from the gradients."}, {"title": "4.1 Node Attacker", "content": "Consider a node classification setting, where the inputs to a GNN layer are the target nodal features and the\nneighboring nodal features denoted as $x_v$, and ${x_{v_j}}_{j\\in N_v}$. Through theoretical analysis we will show that\nparts of the input to a GCN or GraphSAGE layer can be recovered from the gradients analytically without\nsolving an iterative optimization problem.\nProposition 1. For a GraphSAGE defined in equation 1 or a GCN layer defined in equation 3, Node Attacker\n1 can recover the aggregated representations of a target node denoted as $x_{agg}$ (see equation 4 with $h_{agg} = x_{agg}$ )\ngiven the gradients of the first layer as $x_{agg} = \\nabla_{(w_i)v}L_v/\\nabla_{(b_i)v}L_v$ for GCN and as $x_{agg} = \\nabla_{(W_1)v}L_v/\\nabla_{(b_i)v}L_v$\nfor GraphSAGE provided that $\\nabla_{(b_i)v}L_v \\neq 0$.\nProof. We show the proof for a GCN layer, the proof for GraphSAGE is similar and is deferred to the\nappendix. For a GCN layer, the aggregated input at the target node can be recovered from the gradients\nwith respect to the weights of the first layer by writing the following equations\n$$h_v = \\sigma(\\bar{A}h_v^{l-1}W^T + b),$$\n$$h_v = x_{agg}W^T + b,$$\n$$\\nabla_{(W)_i}L_v = \\nabla_{(\\tilde{h_v})_i}L_v.\\nabla_{(W)_i} (\\tilde{h_v})_i,$$\n$$\\nabla_{(W)_i}L_v = \\nabla_{(b)_i}L_v x_{agg}.$$"}, {"title": "4.2 Graph Attacker", "content": "The present subsection provides theoretical analysis for a Graph Attacker-a in a scenario where prior\ninformation may be available for the graph data to be attacked. It can be shown that for GraphSAGE, prior\nknowledge of the nodal feature matrix $X$ can help recover the graph structure $A$, making the framework\nmore vulnerable to the gradient-based attacks.\nProposition 6. For a GraphSAGE framework defined in equation 6 with $H^0 = X$, Graph Attacker-a can\nrecover $A$ from the gradients of the first layer as $A = X(\\nabla_{w_2}L)^{\\dagger}(\\nabla_{w_1}L)X^{\\dagger}$ if the nodal feature matrix $X$ is\nfull-row rank and known."}, {"title": "5 Data & Experiments", "content": "In this section, the performance of the algorithms is tested over real-world social network datasets as well as\nsynthetic datasets to validate the efficacy of the proposed framework.\nEvaluation Metrics Error metrics for recovering the nodal features and the graph structure are evaluated,\nalong with their standard deviation.\n\u2022\n\u2022\nNodal Features: To evaluate the performance of the recovered node features, the Root Normalised\nMean Squared Error(RNMSE) is used\n$$RNMSE(x, \\hat{x}) = \\frac{||x - \\hat{x}||}{||x||}$$\nIn order to evaluate the performance of recovering $X$ in a graph classification setting, we report the\nmean RNMSE over all the nodal features in $X$.\nGraph Structure: To evaluate the recovery performance of the binary adjacency matrix, we use\nthe following metrics\n$$\\text{Accuracy: defined as } \\frac{\\sum_{i,j} (A_{ij} - \\hat{A}_{ij})}{N*N}$$\nTP\nAP (Average Precision): is defined as $\\frac{TP}{TP + FP}$ where TP stands for number of True Positives\nand FP stands for False Positives. True Positives are the correctly identified edges in the actual\ngraph by the attacker and False Positives are the non-edges incorrectly classified as edges by the\nattacker.\nAUC (Area Under Curve): The area under the Reciever Operator Characteristic Curve.\nExperimental Settings. Algorithms 1, 2 and 3 solve an optimization problem using gradient descent.\nThe gradient descent step can be replaced with off-the-shelf optimizers such as Adam or L-BFGS. In all the\nexperiments, Adam is used as the optimizer. In an attack setting, allowing multiple restarts to the optimizer\n(especially L-BFGS) from different starting points can significantly increase the quality of recovery. For a fair\ncomparison, multiple restarts are not used, and all results presented are obtained with a single run of the\noptimizer. However, in an actual attack setting, an attacker may be able to greatly improve the recovery\nquality by allowing multiple restarts.\nIn the experiments, a 2-layer GNN model with hidden dimension 100 and a sigmoid activation function is\nemployed. The weights of the model are randomly initialized. For all the experiments, the dummy nodal\nfeatures are initialized randomly, with each entry sampled from the standard normal distribution i.e., N(0, 1)."}, {"title": "5.1 Node Attacker 1", "content": "We first evaluated the performance of feature recovery of Node Attacker 1 for the node classification task."}, {"title": "5.1.1 Target node feature recovery", "content": "In this experiment, since no knowledge of the graph data set is available on the attacker side, an initial\ndummy graph is generated as a 2-layer tree with the target node as the root node and the degree of each\nnode $d_{tree} = 10$.\nTable 1 lists the RNMSE of recovering the target node features. The results are obtained by running the\nattack over 20 randomly selected nodes from each data set. It can be observed that for GraphSAGE, the\nattacker can recover the nodal features with high accuracy. For GCN, the attacker fails to recover the target\nnodal features. Such results are consistent with the theoretical analysis in Propositions 1 and 2. To further\nevaluate the performance, we also evaluated the minimum RNMSE obtained in Table 1, which indicates the\nbest recovery performance among the 20 nodes. It can be observed that while the mean RNMSE of the target\nnode features for GCN is large, the minimum RNMSE can be small. This implies that data might also be\nleaked when GCN is used in pracitice."}, {"title": "5.1.2 One-hop Neighborhood Nodal Feature Recovery", "content": "To evaluate the performance of the node attacker in recovering the nodal features of one-hop neighbors, a\ntarget node is selected randomly from the graph. A dummy graph is initialized as a depth-2 tree with\n$d_{tree}$ set to the actual number of one-hop neighbors of the target node. Since there's permutation ambiguity"}, {"title": "5.2 Node Attacker 2", "content": "In this section, we evaluate the performance of Node Attacker 2 which has access to the gradient of a node\nclassification model for all nodes in a subgraph."}, {"title": "5.2.1 Node Attacker 2-a", "content": "The recovery performance of the adjacency matrix for this setting is listed in Table 2. Note that the attacker\nperfectly recovers the graph structure for the GraphSAGE model. For GCN, even though Proposition 3\nstates that we can exactly recover the adjacency matrix, we do observe some error in the recovery. This can\nbe potentially attributed to either the limited number of iterations employed in the attack, hindering its\nconvergence, or the optimization procedure encountering a local minimum. Nonetheless, even though not\na perfect recovery, the attacker is able to recover the graph structure with high accuracy and precision for\nGCN."}, {"title": "5.2.2 Node Attacker 2-b", "content": "Table 3 shows the results for Node Attacker 2-b. The attacker is capable of obtaining almost perfect recovery\nof nodal features for the GraphSAGE model. Similarly, for the GCN model, the RNMSE values, although"}, {"title": "5.2.3 Node Attacker 2-c", "content": "In this case, the attacker doesn't have any prior information about the graph data and tries to recover both\nthe nodal features and the graph structure. The results are shown in Table 4. For the GraphSAGE model, it\ncan be observed that the attacker can recover both the features and the adjacency matrix with high accuracy,\nin both Facebook and GitHub datasets. Again, such results are consistent with our theoretical results in\nProposition 5. Surprisingly, for the GCN framework, the attacker is still able to recover A with high AUC\nand AP scores. Note that, no theoretical guarantee was provided in Section 4 for node attacker 2-c for GCN\nframework. Such empirical results may be attributed to the introduced feature smoothness and sparsity\nregularizers in equation 14. To further investigate the effect of regularizers, the performance of the attack on\nthe model was evaluated without the regularizers. These results are also presented in Table 4. It can be\nobserved that there is a clear decline in the performance of recovering the adjacency matrix if the regularizers\nare not introduced, especially in Average Precision (AP). It suggests that the introduced regularizers can\neffectively improve the performance of the attack."}, {"title": "5.3 Graph Attacker", "content": "In this subsection, the performance of different graph attackers are evaluated for attacking nodal features as\nwell as the adjacency matrix of the private subgraph."}, {"title": "5.3.1 Graph Attacker-a", "content": "Table 5 lists the recovery performance of the graph structure. It can be observed that the attacker can\nperfectly recover the adjacency matrix from the gradients of a GraphSAGE model in both datasets. Such\nresults are also consistent with the results of Proposition 6. On the other hand, in the case of GCN, despite\nthe absence of theoretical guarantees, the attacker demonstrates high accuracy, precision, and AUC values in"}, {"title": "5.3.2 Graph Attacker-b", "content": "In this setting, the attacker tries to recover the nodal feature matrix given the gradients and the underlying\ngraph structure. The results are listed in Table 6. It can be observed that while the attacker is unable to\nrecover nodal features from a GCN model, it can recover nodel features from a GraphSAGE model with small\nerrors\nThe results also corroborate that this attacker is different from Node Attacker 2-b where the gradients of the\nloss for each node are leaked to the attacker. Unlike this scenario, Node Attacker 2-b can successfully recover\nthe nodal features."}, {"title": "5.3.3 Graph Attacker-c", "content": "The results for Graph Attacker-c are listed in Table 7. Similar to the results of Node Attacker 2-c, the\nadjacency matrix can be accurately recovered in this setting for GraphSAGE. Interestingly, in contrast to\nthe previous results, the attack performs better on the GCN than the GraphSAGE framework. Also, in this\nsetting, the attack fails to reconstruct the nodal features for both GraphSAGE and GCN frameworks."}, {"title": "5.4 Attacking batched data", "content": "We also extended the experiments for node and graph attackers to the minibatch setting. Specifically, we only\nevaluate GraphSAGE for both tasks since it provides the best recovery in the stochastic (single data sample)\nsetting as shown in the previous experiments. Although effective attack techniques Wen et al. (2022) have\nbeen proposed to recover image data from large batches, here we tested the performance of the attackers by\nsimply optimizing with respect to the dummy input batch. Hence, the performance may improve if attack\ntechniques for batch data are also incorporated. Due to possible permutation ambiguity in a batch setting,\nthe Hungarian algorithm is used to find the best match with the actual input data. The evaluation metric for\nthe algorithm is the RNMSE between all pairs of the dummy batch and the actual batch. These results are\naveraged over 10 independent runs for each batch size. Experiments are conducted for Node Attacker 1 and\nGraph Attacker-b.\nNode Attacker 1 Table 8 shows the performance of recovering the target node features for batches of\nsize $B = 5, 20$, and 50. It can be observed that the best results are obtained for the Synthetic Dataset with\naccurate recovery across all batch sizes. When the batch size is $B = 5$, the attack provides accurate recovery\nacross all the 3 datasets.\nGraph Attacker-b In this scenario, the attack attempts to recover $X$ for the whole batch with $A$ being\nknown. To simplify the matching after running the attack we only consider graphs with the same number\nof nodes. This is done by generating Erdos-Renyi (ER) graphs with the number of nodes $n = 50. For a\nER graph with n nodes and edge probability p, the average degree of a node is $d = (n - 1)p. We conduct\nexperiments by varying $d$ with different batch sizes. The nodal features are generated by sampling from the\nstandard normal distribution i.e. N(0,1). Table 9 lists the performance of recovering $X$ with varying batch\nsizes and degrees. As expected, the quality of recovery declines with increasing batch size, with the best\nresults being achieved at $B = 5$. Also, the performance remains roughly the same across all degrees."}, {"title": "5.5 Sensitivity Analysis for Hyperparameters", "content": "In our attack, numerous hyperparameters come into play, including but not limited to the regularization\nparameters $\\alpha$ and $\\beta$, the graph size, and the network width. In our analysis, we examined a subset of these\nhyperparameters in different settings and subsequently present the corresponding outcomes below."}, {"title": "5.6 Numerical Tests on Molecular Datasets", "content": "All attack settings considered in previous experiments are defined within the context of social networks.\nIn order to assess the efficacy of the attack on alternative datasets, such as molecular datasets which are\ncommonly employed in graph classification tasks, we extend our analysis to include three datasets from the\nTUDataset Morris et al. (2020). The experimental setting, evaluation metrics and results are given in detail\nin Appendix F. The average number of nodes in the graphs of these datasets is significantly smaller than\nsocial networks (see Table 11). However, unlike social networks, these graphs might not necessarily exhibit\nproperties like feature smoothness and sparsity. Hence, for all the attacks we use the objective function as\ndefined in equation 12. To improve the attack for Graph Attacker-c where we have no prior knowledge of the\ngraph we utilize various initialization and thresholding strategies. Specifically, since the optimization process\ngiven in the algorithms can be greatly improved by utilizing different starting points, we initialize the dummy\nnode feature and adjacency matrices with different values. In the case of thresholding, instead of sampling\nthe edges given the probabilistic adjacency matrix $\\hat{A}^{P+1}$, we instead use various threshold values to sample\nedges. For instance, if the threshold is set to 0.5 we only sample edges from the probabilistic adjacency matrix\n$\\hat{A}^{P+1}$ with values greater than 0.5. The results are listed in Table 15, which demonstrate that even in this\ncase, without any consideration about sparsity or smoothness, while simply utilizing different initializations\nand thresholds, the attacker can still recover the nodal features as well as the underlying graph structure\nwith high accuracy. The results in the other attack settings are again consistent with the theoretical analysis.\nFor example, for Graph Attacker-a, the underlying graph structure recovery is only possible for one of the\ndatasets, see Table 12. Note that the nodal feature matrix X was not full-row rank for the MUTAG and\nCOIL-RAG datasets. Such results were consistent with Proposition 6, which states that the underlying graph\nstructure can be recovered only when X is full-row rank."}, {"title": "6 Conclusion & Future Work", "content": "In this work, we presented the first attempt to study the gradient inversion attack for Graph Neural\nNetworks(GNNs) in a federated graph learning setting and explored what information can be leaked about\nthe graph data from the gradient. We studied both the node classification and the graph classification tasks,\nalong with two widely used GNN structures (GraphSAGE and GCN). Through both theoretical and empirical\nresults, we highlighted and analyzed the vulnerabilities of different GNN frameworks across a wide variety of\ngradient inversion attack settings.\nFuture work\nIn the future, it would be interesting to study the extension of the attack mechanisms to other GNN\nframeworks, such as GIN (Graph Isomorphism Networks) Xu et al. (2019) and GAT (Graph Attention\nNetworks) Veli\u010dkovi\u0107 et al. (2018). Experiments and theoretical analysis regarding the vulnerability of other\nGNN frameworks hence is a promising direction. For example, are certain GNN frameworks more robust to\ngradient inversion attacks than others? Also, are there any other special properties that can be utilized for\ngraph datasets other than social networks? For instance, it is worth exploiting the fact that the degrees of\nnodes in molecular datasets exhibit an upper limit. Finally, recent studies such as Wang et al. (2023) have\nprovided theoretical bounds on the recovery quality of minibatch data. Hence, an intriguing question would\nbe whether similar guarantees can be established for graph data.\nFurthermore, it is crucial to have appropriate defense mechanisms in place to prevent data leakage. It is\ncommonly believed that employing larger batch sizes could be an effective defense against the gradient\ninversion attack, but our experiments show that even in batches of size 50, the private data can be recovered\nwith high accuracy. Therefore, solely relying on larger batch sizes is insufficient as a defense strategy against\ngradient inversion attacks. Recent studies such as Wen et al. (2022); Wang et al. (2023) have provided\nprovable guarantees regarding the quality of recovered private data from training batches. Based on the\nvarious defense strategies introduced in Gupta et al. (2022), some potential defense mechanisms can remain\neffective in graph setting: e.g., noisy gradients Abadi et al. (2016), gradient pruning Zhu et al. (2019),\nencoding inputs Huang et al. (2020). However, more specific defensive mechanisms from graph perspectives\nremain undeveloped."}]}