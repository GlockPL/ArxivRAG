{"title": "LEARNING TO PLAN\nWITH PERSONALIZED PREFERENCES", "authors": ["Manjie Xu", "Xinyi Yang", "Wei Liang", "Chi Zhang", "Yixin Zhu"], "abstract": "Effective integration of Artificial Intelligence (AI) agents into daily life requires\nthem to understand and adapt to individual human preferences, particularly in col-\nlaborative roles. Although recent studies on embodied intelligence have advanced\nsignificantly, they typically adopt generalized approaches that overlook personal\npreferences in planning. We address this limitation by developing agents that not\nonly learn preferences from few demonstrations but also learn to adapt their plan-\nning strategies based on these preferences. Our research leverages the observation\nthat preferences, though implicitly expressed through minimal demonstrations,\ncan generalize across diverse planning scenarios. To systematically evaluate this\nhypothesis, we introduce Preference-based Planning (PBP) benchmark, an embod-\nied benchmark featuring hundreds of diverse preferences spanning from atomic\nactions to complex sequences. Our evaluation of State-of-the-Art (SOTA) methods\nreveals that while symbol-based approaches show promise in scalability, significant\nchallenges remain in learning to generate and execute plans that satisfy personal-\nized preferences. We further demonstrate that incorporating learned preferences as\nintermediate representations in planning significantly improves the agent's ability\nto construct personalized plans. These findings establish preferences as a valuable\nabstraction layer for adaptive planning, opening new directions for research in\npreference-guided plan generation and execution.", "sections": [{"title": "INTRODUCTION", "content": "The field of embodied Artificial Intelligence (AI) is rapidly advancing, driven by significant progress\nin foundation models for vision and language (Bommasani et al., 2021; Peng et al., 2023; Achiam\net al., 2023; Bai et al., 2023). These advances enable AI systems to autonomously collaborate with or\nassist humans in daily tasks, particularly in domestic settings (Driess et al., 2023; Leal et al., 2023;\nZitkovich et al., 2023; Ahn et al., 2024). However, recent approaches utilizing natural language\ninstructions (Mu et al., 2023; Zitkovich et al., 2023; Singh et al., 2023) face fundamental limitations\nin capturing human preferences (Zhu et al., 2016). While natural language is our primary means of\ncommunication, its inherent ambiguity creates a gap between instructions and intended executions\n(Yuan et al., 2022; Jiang et al., 2022; 2021; Yuan et al., 2020). For instance, when a user requests\nhelp in preparing an apple, the agent needs to understand specific preferences about apple selection,\nwashing requirements, cutting style, and container choice\u2014details that vary significantly across\nindividuals; see also Figure 1 for a graphical illustration.\nPreference, central to personalization (Slovic, 1995), remains inadequately addressed in embodied\nArtificial Intelligence (AI). Integrating personalized preferences is crucial for tailoring agent actions\nto individual users, thereby enhancing the effectiveness and satisfaction of embodied assistants\n(Lee et al., 2012; Leyzberg et al., 2014). Moreover, preferences guide human-like decision-making\nand intelligent behavior. Psychological research emphasizes that understanding preferences is vital\nfor interpreting human behaviors (Fawcett and Markson, 2010) and facilitating social interactions\n(Gerson et al., 2017; Liberman et al., 2021), suggesting that preference understanding could enable\nmore grounded planning in embodied assistants."}, {"title": "RELATED WORK", "content": "few-shot preference learning and preference-guided planning, establishing preferences as a crucial\nabstraction layer between high-level goals and low-level actions. We present this work as a foundation\nfor addressing these challenges in preference-based embodied AI."}, {"title": "2.1 THEORETICAL FOUNDATIONS OF HUMAN PREFERENCES", "content": "Preference theory originates from psychological research, where it describes predictable patterns\nin human behavior that can be modeled mathematically (Kahneman, 1982). These preferences\nreflect individual attitudes towards available choices in decision-making (Lichtenstein and Slovic,\n2006) and operate both consciously and unconsciously to shape behavior (Coppin et al., 2010). A\nfundamental principle is that underlying preferences can be inferred from consistent behavioral\npatterns (Sen, 1973), enabling systematic analysis of decision-making processes. This framework\nhas extended beyond psychology into economics, where Rational Choice Theory (Scott et al., 2000)\nmodels decision-making based on rational self-interest (Zey, 1998). Building on this, Utility Theory\nprovides a mathematical foundation for modeling how preferences relate to attitudes toward rewards\nand risks (Mongin, 1997; Aleskerov et al., 2007). These theoretical foundations establish preferences\nas fundamental elements in shaping both individual behavior and broader societal dynamics. In recent\nyears, these preference models have found new applications in artificial intelligence and robotics,\nparticularly in developing human-centric AI assistants capable of understanding and adapting to\nindividual user preferences."}, {"title": "2.2 PREFERENCE IN EMBODIED TASK PLANNING", "content": "The application of preferences in embodied task planning encompasses two distinct approaches. The\nfirst focuses on general preference-based planning, where robots leverage commonsense knowledge to\nexecute universally accepted behavioral norms. Object rearrangement exemplifies this approach, with\nsystems organizing items based on common occurrence patterns and spatial relationships (Taniguchi\net al., 2021; Sarch et al., 2022). The second approach emphasizes personalized preferences, where\nembodied agents align their actions with individual user habits. This includes personalized object\nplacement strategies (Abdo et al., 2015; Kapelyukh and Johns, 2022; Wu et al., 2023), preference-\naware table setting (Puig et al., 2021a), and multi-agent coordination where agents maintain individual\npreferences while achieving optimal coordination (Shu and Tian, 2019).\nOur work extends these approaches by considering preferences across diverse situations and scenes.\nBeyond spatial arrangements, we address temporal action sequences, state transitions during interac-\ntions, and few-shot preference learning. This comprehensive framework enables robust preference\nmodeling and adaptation in real-world scenarios."}, {"title": "2.3 \u0415\u043c\u0432ODIED ASSISTANTS", "content": "The development of intelligent embodied assistants has evolved from basic Vision-and-Language\nNavigation (VLN) tasks (Anderson et al., 2018; Chen et al., 2019; Thomason et al., 2020) to complex\ninteractive scenarios. ALFRED (Shridhar et al., 2020) introduced object manipulation, state tracking,\nand temporal dependencies between instructions, while platforms like Habitat (Savva et al., 2019;\nPuig et al., 2023b) and AI2-THOR (Kolve et al., 2017) emphasize active perception, long-term\nplanning, and interactive learning in realistic environments. Recent research has shifted toward\nimplicit-instruction scenarios, particularly in housekeeping tasks (Kapelyukh and Johns, 2022; Kant\net al., 2022; Sarch et al., 2022; Wu et al., 2023), where robots must reason about object arrangements\nwithout explicit directives. Works on proactive assistance (Patel and Chernova, 2023; Patel et al.,\n2023; Puig et al., 2023a) further explore anticipating temporal patterns in humans' daily routines.\nMethodologically, recent advances utilize Large Language Models (LLMs) as few-shot planners to\ngenerate language-based action sequences from limited demonstrations (Song et al., 2023; Driess\net al., 2023; Ding et al., 2023; Zhang et al., 2024). Foundation Vision-Language Models (VLMs) have\nenhanced robotic systems' perception and reasoning capabilities (Ahn et al., 2024; Leal et al., 2023;\nGu et al., 2023; Brohan et al., 2022; Zitkovich et al., 2023; Xu et al., 2024a), enabling understanding\nof complex visual and linguistic inputs in everyday tasks. However, while these foundation models"}, {"title": "3 FORMULATING PREFERENCE-BASED PLANNING", "content": "Tasks in PBP mirror real-world watch-and-help scenarios (Puig et al., 2021b), where an agent\nobserves a few demonstrations of a user performing tasks that reveal preferences. The agent must\nthen complete similar tasks in different setups while adhering to the demonstrated preferences.\nPreference-based planning comprises two key components: few-shot preference learning of user\npreferences and subsequent planning guided by these learned preferences. Since humans, even\ninfants, can naturally detect others' preferences from limited decisions (Choi and Luo, 2023), and\ncollecting extensive personal demonstrations is impractical in daily life, we formulate this as few-shot\nlearning from demonstration.\nGiven a user with preference p, the agent observes the user performing tasks from a first-person per-\nspective, denoted as O. These observations span multiple demonstrations. Formally, O contains both\nstate and action observations: $O = \\{(S_i, A_i, M)^N\\}$, where $S_i$ denotes the egocentric observation\nsequence in the i-th demonstration, $A_i$ represents the action sequence, and M optionally provides a\nbird's-eye view of the entire scene map.\nIn the first stage, the objective is to learn the preference representation demonstrated through user\nactions:\n$p = f(0; \\theta_f),$\nwhere p denotes the learned preference representation here. It can either be a hidden representation\nor an explicit textual label, depending on the task settings.\nThe learned preference p should then guide planning when the agent faces different setups with\nvarying objects, room layouts, or entire scenes. Specifically, the agent optimizes:\n$\\mathcal{L} = \\sum_{i=1}^T l(g(s_i, f(0; \\theta_f); \\theta_g), a_i),$\nwhere g() represents a potentially parameterized planning function that maps the current state and\npreference representation to the next action, and $a_i$ denotes the ground-truth action demonstrating the\nuser's preference at the current stage."}, {"title": "4 THE PREFERENCE-BASED PLANNING (PBP) BENCHMARK", "content": "Built on NVIDIA's Omniverse and OmniGibson simulation environment (Li et al., 2023), our PBP\nbenchmark enables realistic simulation of thousands of daily activities. It spans 50 distinct scenes\nand encodes 290 unique preferences, with a comprehensive test set of 5000 instances. Below, we\ndetail the preference structure and test set construction."}, {"title": "4.1 DEFINITION OF PREFERENCES", "content": "We organize preferences in a three-tiered hierarchical structure that captures varying degrees of\nspecificity across tasks. Figure 2 provides an overview of all preferences and their distribution,\nwhile Figure 3 illustrates concrete examples of preferences and corresponding agent actions. The\n290 preferences are distributed across three levels: 80 sequence-level, 135 option-level, and 75\naction-level preferences.\nAction Level These bottom-level preferences govern fine-grained execution details within specific\nsub-tasks, such as water quantity preferences when filling cups or shelf placement choices for books.\nOption Level Middle-level preferences encode alternative approaches to sub-tasks. For instance,\nin \"storing-nonperishable-food,\u201d users may prefer cabinet storage versus table placement. These\npreferences can bind to different objects and may compose multiple action-level preferences.\nSequence Level Top-level preferences define task ordering and prioritization. They capture tempo-\nral dependencies between sub-tasks, such as cleaning furniture before rearranging kitchen utensils,\nfollowed by dinner preparation upon returning home."}, {"title": "4.2 CONSTRUCTING PBP TEST SET", "content": "Our PBP benchmark includes a default test set for systematic model evaluation. Following the formu-\nlation in Section 3, we structure PBP tasks as few-shot learning-from-demonstration problems. Each"}, {"title": "4.3 MODELS", "content": "Our evaluation focuses primarily on multimodal models that incorporate LLMs and demonstrate\nstrong few-shot learning capabilities. The LLM component serves as a knowledge base that can\nenhance preference learning through commonsense reasoning. We also include symbol-based LLM\nmodels for ablation studies to analyze how different modalities impact PBP performance. Most\nmodels evaluated can function in both end-to-end and two-stage pipeline configurations.\nViViT As a baseline, we employ the pure-Transformer-based Video Vision Transformer (ViViT)\n(Arnab et al., 2021), an end-to-end trainable model with proven capabilities in extracting spatial and\ntemporal information from video inputs. Since it lacks a LLM component, ViViT likely serves as a\nlower bound for commonsense understanding in PBP tasks."}, {"title": "5 EXPERIMENTS", "content": "LLaVA Building on more sophisticated architectures, Large Language and Vision Assistant\n(LLaVA) (Liu et al., 2024) represents an end-to-end trainable large multimodal model that inte-\ngrates vision and text for comprehensive visual-language understanding. We specifically evaluate\nLLaVA-NeXT, which has been finetuned to excel at zero-shot video understanding tasks.\nEILEV For specialized egocentric video processing, we incorporate Efficient In-context Learning\non Egocentric Videos (EILEV) (Yu et al., 2023), which achieves in-context learning through architec-\ntural modifications to a pretrained VLM. Our implementation uses OPT-2.7B (Zhang et al., 2022) as\nthe language backbone. The model's pretraining on Ego4D (Grauman et al., 2022) aligns well with\nPBP's egocentric perspective.\nGPT-4V To benchmark against state-of-the-art visual-language models, we evaluate GPT-4V using\nthe Azure OpenAI API (version \u201cgpt-4-turbo-2024-04-09\"). Due to image token limitations, we\nimplement video input subsampling while maintaining temporal coherence.\nBeyond multimodal approaches, we also evaluate single-modal models that process only action\nsequences:\nDAG-Opt We approach symbolic reasoning by framing the problem as a DAG-Optimization task\nthat uncovers dependency relations between actions and preferences (Zheng et al., 2018). Our\nimplementation uses a score-based NOTEARS model to learn a generalized Structural Equation\nModel (SEM), following previous few-shot reasoning frameworks (Zhang et al., 2021; Xu et al.,\n2024b) based on causal dependency structures.\nLLMS To assess pure language understanding, we evaluate advanced LLMs including Llama3\n(Touvron et al., 2023) and GPT-4 (Achiam et al., 2023) using only action sequence inputs. This\napproach treats actions as high-level abstractions of egocentric videos, reducing visual complexity\nwhile maintaining task semantics. We benchmark Llama3-8B as our baseline against GPT-4-Turbo\nas the current state-of-the-art, employing prompt designs informed by the OpenAI Cookbook for\noptimal few-shot performance. Detailed model architectures and prompt engineering strategies are\ndiscussed in Appendix D.\""}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "We evaluate preference learning capabilities across two distinct settings: end-to-end and two-stage\napproaches. In the end-to-end setting, models directly map raw state inputs to action outputs.\nLeveraging models' in-context learning abilities, we provide demonstrations alongside current state\ninformation as input and evaluate the generated action sequences against ground truth.\nThe two-stage setting introduces an intermediate step where models first learn to predict explicit\npreference labels during training. These predicted labels then serve as preference representations for\nsubsequent planning stages. For black-box models, we employ carefully designed prompts rather\nthan fine-tuning approaches.\nAll demonstration videos maintain consistent technical specifications across models and agents:\negocentric perspective, 512 \u00d7 512 resolution, and 8 fps frame rate. Video duration matches the\ncorresponding action sequence length. For LLM inference, we use conservative decoding parameters:\ntemperature of 0.05, top-k of 1, and top-p of 0.05. All experiments run on a single machine with 8\nNVIDIA A100 GPUs."}, {"title": "5.2 END-TO-END ACTION PREFERENCE LEARNING", "content": "We first evaluate model performance in the end-to-end setting, where models generate actions directly\nfrom previous demonstrations and current state information. To quantify performance, we use\nLevenshtein distance to measure discrepancies between generated and ground truth action sequences,\ntreating each individual action as a token."}, {"title": "5.3 Two-STAGE LEARNING-PLANNING", "content": "Given the limitations of end-to-end learning, we implement a two-stage approach to decompose the\npreference learning problem. The first stage focuses on preference prediction, where we provide\nmodels with auxiliary preference token labels and train them to predict hidden preferences explicitly.\nThese preference tokens, as discussed in Section 4.1, maintain sufficient semantic content for\ntranslation into primitive actions.\nResults from the first stage (Table 2) reveal significant performance variations across models. At the\noption level, GPT-4V achieves superior performance with 48.48% accuracy, demonstrating strong\ncapability in interpreting demonstrated preferences. Among symbol-based models, the stark contrast\nbetween DAG-Opt's limited performance and the improved results from Llama3-8B and GPT-4\nhighlights the advantage of next-token prediction over dependency learning for preference inference.\nModels with language components consistently show improved preference understanding compared\nto end-to-end learning.\nThe second stage involves generating action sequences based on both demonstrations and predicted\npreference labels from the first stage, introducing potential error propagation. Results in Table 1\n(Second-stage row) and Figure 5 show significant improvements when models receive explicit\npreferences. For comprehensive evaluation, we include planning results using ground truth preference\nlabels (Second-stage (gt) row). GPT-4V and GPT-4 achieve near-zero Levenshtein distances,\nindicating almost perfect alignment with ground truth action sequences."}, {"title": "5.4 GENERALIZATION", "content": "Analysis of both stages reveals distinct challenges across model types. Vision-based models like\nLLaVA-Next and GPT-4V struggle with preference inference but excel in action planning given\npreference labels, suggesting difficulty in abstracting preferences from visual input. Symbol-based\nmodels perform well in both preference inference and preference-guided planning, yet underperform\nin end-to-end settings. This indicates that models may lack innate preference-based reasoning\ncapabilities but can effectively plan when preferences are explicitly provided.\nTo isolate the impact of prior knowledge versus in-context learning, we conduct ablation studies\nby removing demonstrations and testing preference prediction on isolated test sequences. Results\nin Table 3 show significant performance degradation compared to few-shot learning (Table 2),\nparticularly at the sequence level. This suggests that while models may encode basic task-specific\npreferences, they rely heavily on demonstrations to recognize complex preference patterns in varied\nsequences."}, {"title": "5.5 ABLATIONS ON DEMONSTRATION NUMBERS", "content": "While human actions may vary across different objects and scenes, underlying preferences often\nremain consistent. We evaluate the models' ability to generalize preference learning across varying\nvisual contexts. The original test set inherently tests generalization by randomly sampling scenes\nand objects when rendering video demonstrations for each preference. To gain additional insights,\nwe conduct complementary experiments with controlled conditions where demonstration and test\nvideos are rendered in identical rooms with the same objects. This controlled setting enables direct\nperformance comparisons under consistent conditions. We evaluate EILEV, LLaVA, and GPT-4\nseries models on this variant of PBP, as these models previously demonstrated strong few-shot\nreasoning capabilities. Results are summarized in Table 4."}, {"title": "6 CONCLUSION", "content": "We examine the effect of demonstration quantity on model performance through an ablation study\n(Figure 7). Results show that increasing demonstration numbers generally improves preference learn-\ning and planning effectiveness. This improvement is most evident in second-stage planning, where\nmodels achieve lower sequence distances by more accurately replicating human actions. Models\nlike GPT-4, Llama3, and EILEV show consistent performance gains with additional demonstrations.\nHowever, we observe that excessive demonstrations (e.g., 5-demo cases for GPT-4 and EILVE) can\nsometimes impair first-stage prediction accuracy. Despite these occasional exceptions, the overall\ntrend confirms our intuition: more demonstrations enhance learning and planning performance. These\nfindings highlight the importance of demonstration quantity in developing effective personalized\nplanning systems that align with user preferences."}, {"title": "Limitations and Societal Impacts", "content": "We investigate methods for embodied agents to learn and implement human preferences through\nbehavioral observation and user interaction. We present Preference-based Planning (PBP), a compre-\nhensive embodied benchmark designed to capture the complexity of real-world human preferences.\nWe also develop an evaluation framework to assess models' preference learning and implementation\ncapabilities. Our findings demonstrate that preferences effectively abstract human behaviors and\nguide planning processes. While current models still face challenges in preference inference and\nadaptive planning from limited observations, incorporating preference-based reasoning improves\nboth effectiveness and generalization, particularly in symbol-based systems that represent idealized\nscenarios. We aim to stimulate further research in this crucial yet understudied domain of developing\npreference-aware embodied agents.\nOur work's primary limitation stems from its reliance on syn-\nthetic data. Despite Omniverse's high-quality scene rendering, the simulator cannot fully replicate\nreal-world complexity and variability. Furthermore, human-defined preference labels may not com-\npletely capture preference subtleties and diversity. We are addressing these limitations by collecting\nreal-world preference demonstrations using head-worn devices, despite associated challenges. Given\nour focus on private scenarios, we anticipate minimal negative societal impact from this research."}, {"title": "A DATASET CARD", "content": "We follow the datasheet proposed in Gebru et al. (2021) for documenting our proposed PBP:\n1. Motivation\n(a) For what purpose was the dataset created?\nThe benchmark was created to evaluate existing learning agents on their ability to\nunderstand and adapt to various human preferences. Specifically, it aims to test the\nagents' proficiency in few-shot learning from demonstrations, where they must respond\nto ambiguous task instructions and formulate adaptive task plans based on limited\nexamples of user preferences. The benchmark is designed to highlight the challenges\nand gaps in current AI systems' capabilities in planning activities and abstracting human\npreferences, ultimately driving advancements towards developing more intelligent and\npersonalized embodied agents.\n(b) Who created the dataset and on behalf of which entity?\nN/A.\n(c) Who funded the creation of the dataset?\nN/A.\n(d) Any other Comments?\nNone.\n2. Composition\n(a) What do the instances that comprise the dataset represent?\nEach instance contains an egocentric video of an agent's activity, its bird's-eye-view\nmap of the position of the agent, and a frame-level textual annotation of the current\naction, as shown in Figure 4. Additionally, we provide a rendered third-person view of\nthe entire process.\n(b) How many instances are there in total?\n15000.\n(c) Does the dataset contain all possible instances or is it a sample (not necessarily\nrandom) of instances from a larger set?\nNo. The dataset contains a set of demonstrations rendered within the simulator, The\nusers can render more diverse instances if they want. We have provided the rendering\ninstructions.\n(d) What data does each instance consist of?\nThe instances that comprise the benchmark represent various types of human prefer-\nences applied to different tasks within a realistic embodied scene. Each instance is\ndesigned to challenge the learning agents to understand and adapt to these preferences\nbased on a few demonstration examples, reflecting the diverse and hierarchical nature\nof user preferences in real-world scenarios. See above for data details.\n(e) Is there a label or target associated with each instance?\nYes.\n(f) Is any information missing from individual instances?\nNo.\n(g) Are relationships between individual instances made explicit?\nYes.\n(h) Are there recommended data splits?\nNo.\n(i) Are there any errors, sources of noise, or redundancies in the dataset?\nNo.\n(j) Is the dataset self-contained, or does it link to or otherwise rely on external\nresources (e.g., websites, tweets, other datasets)?\nSelf-contained.\n(k) Does the dataset contain data that might be considered confidential (e.g., data\nthat is protected by legal privilege or by doctor-patient confidentiality, data that\nincludes the content of individuals' non-public communications)?\nNo.\n(1) Does the dataset contain data that, if viewed directly, might be offensive, insulting,\nthreatening, or might otherwise cause anxiety?\nNo.\n(m) Does the dataset relate to people?\nNo.\n(n) Does the dataset identify any subpopulations (e.g., by age, gender)?\nNo.\n(0) Is it possible to identify individuals (i.e., one or more natural persons), either\ndirectly or indirectly (i.e., in combination with other data) from the dataset?\nNo.\n(p) Does the dataset contain data that might be considered sensitive in any way (e.g.,\ndata that reveals racial or ethnic origins, sexual orientations, religious beliefs,\npolitical opinions or union memberships, or locations; financial or health data;\nbiometric or genetic data; forms of government identification, such as social\nsecurity numbers; criminal history)?\nNo.\n(q) Any other comments?\nNone.\n3. Collection Process\n(a) How was the data associated with each instance acquired?\nWe render PBP using NVIDIA's Omniverse and OmniGibson simulation environment\n(Li et al., 2023).\n(b) What mechanisms or procedures were used to collect the data (e.g., hardware\napparatus or sensor, manual human curation, software program, software API)?\nThe data for each instance in the benchmark was acquired by sampling preferences\nfrom a predefined set and constructing tasks paired with a few demonstrations that\nshared high-level preferences but differed in specific objects and scenes. Each sampled\npreference was randomly assigned to one of the 50 scenes provided by OmniGibson,\nwith relevant objects sampled within the scene. Egocentric observation and action\nsequences of an embodied agent were generated as the agent performed tasks guided\nby a rule-based planner using planning primitives like inverse kinematics for grasping\nand the A* algorithm for movement.\n(c) If the dataset is a sample from a larger set, what was the sampling strategy (e.g.,\ndeterministic, probabilistic with specific sampling probabilities)?\nN/A.\n(d) Who was involved in the data collection process (e.g., students, crowdworkers,\ncontractors) and how were they compensated (e.g., how much were crowdworkers\npaid)?\nN/A.\n(e) Over what timeframe was the data collected?\nN/A.\n(f) Were any ethical review processes conducted (e.g., by an institutional review\nboard)?\nThe dataset raises no ethical concerns.\n(g) Does the dataset relate to people?\nNo.\n(h) Did you collect the data from the individuals in question directly, or obtain it via\nthird parties or other sources (e.g., websites)?\nN/A.\n(i) Were the individuals in question notified about the data collection?\nN/A.\n(j) Did the individuals in question consent to the collection and use of their data?\nN/A.\n(k) If consent was obtained, were the consenting individuals provided with a mecha-\nnism to revoke their consent in the future or for certain uses?\nN/A."}, {"title": "4. Preprocessing, Cleaning and Labeling", "content": "(a) Has an analysis of the potential impact of the dataset and its use on data subjects\n(e.g., a data protection impact analysis) been conducted?\nYes.\n(m) Any other comments?\nNone."}, {"title": "4. Preprocessing, Cleaning and Labeling", "content": "(a) Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or\nbucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal\nof instances, processing of missing values)?\nN/A.\n(b) Was the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data\n(e.g., to support unanticipated future uses)?\nN/A.\n(c) Is the software used to preprocess/clean/label the instances available?\nN/A.\n(d) Any other comments?\nNone."}, {"title": "5. Uses", "content": "(a) Has the dataset been used for any tasks already?\nNo, the dataset is newly proposed by us.\n(b) Is there a repository that links to any or all papers or systems that use the dataset?\nNo, the dataset is new.\n(c) What (other) tasks could the dataset be used for?\nThis dataset could be used for research topics like embodied AI and human-computer\ninteraction.\n(d) Is there anything about the composition of the dataset or the way it was collected\nand preprocessed/cleaned/labeled that might impact future uses?\nN/A.\n(e) Are there tasks for which the dataset should not be used?\nN/A.\n(f) Any other comments?\nNone."}, {"title": "6. Distribution", "content": "(a) Will the dataset be distributed to third parties outside of the entity (e.g., company,\ninstitution, organization) on behalf of which the dataset was created?\nNo before it is made public.\n(b) How will the dataset be distributed (e.g., tarball on website, API, GitHub)?\nOn our project website upon acceptance.\n(c) When will the dataset be distributed?\nUpon acceptance.\n(d) Will the dataset be distributed under a copyright or other intellectual property\n(IP) license, and/or under applicable terms of use (ToU)?\nUnder CC BY-NC 1 license.\n(e) Have any third parties imposed IP-based or other restrictions on the data associ-\nated with the instances?\nNo.\n(f) Do any export controls or other regulatory restrictions apply to the dataset or to\nindividual instances?\nNo.\n(g) Any other comments?\nNone."}, {"title": "7. Maintenance", "content": "'https://creativecommons.org/licenses/by-nc/4.0/"}, {"title": "B DATASET STATISTICS", "content": "The length of the simulations in the dataset ranges from 1 to 5 minutes, depending on the tasks\nrecorded. And the videos are recorded at 30 fps."}, {"title": "B.1 PREFERENES", "content": "See Table A1 for the preference statistics in PBP."}, {"title": "B.2 ACTIONS", "content": "See Table A2 for the action statistics in PBP. We implement 17 action primitives in PBP to assist\nwith model planning and dataset rendering. These action primitives have parameters that simplify\ntasks and are considered the lowest-level actions. Each sub-task contains 8 to 20 such lowest-level\nactions. Generally, most of these actions consist of two parts: the robot movement part and the\narm (gripper) execution part. For robot movement, we use the A* algorithm to find paths and avoid\ncollisions. We build a connection map during scene initialization for navigation, taking the robot's\nwidth into consideration. For the arm (gripper) execution, we primarily use the IK algorithm to\ncompute arm movements. However, since IK cannot handle complex tasks, such as picking objects\nfrom the fridge, we also leverage the Open Motion Planning Library (OMPL) planner (Sucan et al.,\n2012) with forward planning to assist in planning the arm positions."}, {"title": "B.3 MORE DATASET DETAILS AND DISCUSSION", "content": "Dataset production The process of producing data is mainly explained in Section 4.2. In summary", "sample preference - sample scene - sample objects to be manipulated -\ngenerate actions guided by a rule-based planner\".\nLength and FPS of the simulations The length of the simulations ranges from 1 to 5 minutes,\ndepending on the tasks recorded. The videos are recorded at 30 fps.\nActions contained in each simulation The number of actions in simulations varies among different\npreference levels. There is 1 subtask for action-level, 2-3 subtasks for option-level, and 2-3 subtasks\nfor sequence-level preferences. Each subtask contains 8-20 actions.\nScenes and rooms Each scene contains various types of rooms. The main differences between\nscenes are the type, number, and layout of both rooms and furniture. Additionally, each room may\ncontain different objects and have unique layouts. Details of the scenes and rooms can be found\nin Omnigibson's official documentation (https": "behavior.stanford.edu/omnigibson/), as we directly\nadopt these scenes from the open-sourced project.\n290 preference types Considering that preferences in household activities are not only multi-\ndimensional but also hierarchical, we first define a hierarchy of preferences from the perspective of\nhow things happen in a life scenario, that is, from each specific action to a sub-task consisting of\nseveral actions, and then to the sequence combining these sub-tasks. The next step is to expand each\nlevel with typical tasks and actions. The detailed definition of the 290 preferences can be found in\nSection 4.1.\nThe egocentric view Collecting both egocentric observations and third-person views is feasible\nin PbP or similar environments built on simulators like iGibson. However, in real-world scenarios,\nit is generally easier to gather egocentric observations of human daily activities, as these can be\nefficiently captured through wearable devices. Additionally, there are numerous egocentric-view\ndatasets available, such as Ego4D(Grauman et al., 2022), which further facilitate this approach. While\nthird-person views can provide a different perspective, they often encounter issues such as occlusion.\nAlthough research based on third-person views is essential for applications involving real robots,\nfocusing on egocentric views in the current work"}]}