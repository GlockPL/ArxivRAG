{"title": "BIGR: HARNESSING BINARY LATENT CODES FOR IMAGE GENERATION AND IMPROVED VISUAL REPRESENTATION CAPABILITIES", "authors": ["Shaozhe Hao", "Xuantong Liu", "Xianbiao Qi", "Shihao Zhao", "Bojia Zi", "Rong Xiao", "Kai Han", "Kwan-Yee K. Wong"], "abstract": "We introduce BiGR, a novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that unifies generation and discrimination within the same framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a binary transcoder for binary code prediction. Additionally, we introduce a novel entropy-ordered sampling method to enable efficient image generation. Extensive experiments validate BiGR's superior performance in generation quality, as measured by FID-50k, and representation capabilities, as evidenced by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization across various vision tasks, enabling applications such as image inpainting, outpainting, editing, interpolation, and enrichment, without the need for structural modifications. Our findings suggest that BiGR unifies generative and discriminative tasks effectively, paving the way for further advancements in the field.", "sections": [{"title": "INTRODUCTION", "content": "Image generation is experiencing a revolutionary growth driven by the advancements in diffusion models (Ho et al., 2020; Rombach et al., 2022; Peebles & Xie, 2023) and autoregressive models (Esser et al., 2021; Sun et al., 2024; Tian et al., 2024). While these models have demonstrated impressive performance, their representation capabilities are under-studied. As revealed by Balestriero & LeCun (2024), reconstruction-based learning often produces visually compelling results but fails to provide strong latent representations for perception. It has been a long-desired goal of the research community to design a good image generator which can also serve as a strong feature extractor.\nCentered around this goal, previous studies (Chen et al., 2020a; Li et al., 2023a) on representation capabilities of generative models have primarily focused on unconditional generation. Despite conditional generation (Peebles & Xie, 2023; Sun et al., 2024; Li et al., 2024) has emerged as a recent research trend and garnered much attention, investigations of the representation capabilities of conditional generative models remain limited. In conditional image generation, conditions are added to guide the generation process. However, this guidance is absent in downstream discriminative tasks. This weakens the relationship between features and categories, and thereby diminishes the representation capabilities of the features. We validate this limitation using the latest class-conditional image generation model (Sun et al., 2024) (see Fig. 1 (right)), and stress the necessity of improving the representation capabilities of conditional generative models.\nWe introduce BiGR, a novel conditional image generation model that utilizes compact Binary latent codes for Generative tasks with improved Representation capabilities. BiGR is trained exclusively through a generative process by reconstructing tokens without relying on any discriminative losses. We compress an image into a sequence of binary codes using lookup-free quantization (Yu et al., 2024; Wang et al., 2023) and utilize our model to predict these binary codes. We emphasize that BiGR is the first conditional image generation model that unifies generative and discriminative tasks, achieving improved performance across both. Below, we describe our model design, generative and discriminative use, and zero-shot generalized applications.\nOur framework, built upon the language model architecture, has three major components, namely (1) a binary tokenizer that converts a pixel-level image into a sequence of binary latent codes, (2) a decoder-only transformer equipped with full bidirectional attention, and (3) a binary transcoder that transforms continuous features into Bernoulli-distributed binary codes. We train BiGR using the masked modeling approach (Bao et al., 2022; Chang et al., 2022; Li et al., 2023a). This modification, deviating from the typical autoregressive approach, expands token interaction without altering the structure of Llama. Paired with a tailored inference process and inherent visual representations, BiGR can perform both generative and discriminative tasks.\nFor generative purpose, we design a sampling method that iteratively unmask tokens in a sequence, with the order determined by the binary entropy magnitude from the predicted Bernoulli distribution probabilities. This approach requires only a small number of sampling iterations which significantly accelerates the inference process. As a result, we achieve high efficiency in image generation compared with diffusion models, which require multiple steps to remove noise, and autoregressive models, which predict each token sequentially. Through extensive experiments, we show that BiGR performs on par with, or even surpasses, existing baselines in quantitative metrics.\nFor discriminative purpose, we perform average pooling on the intermediate features in BiGR. By this straightforward operation, BiGR exhibits significantly stronger representation capabilities than comparable models, which has been empirically validated through linear probe evaluation. Due to the compactness of binary codes and the global information from masked modeling, the feature representations produced by BiGR can more effectively linearly separate visual categories in downstream discriminative tasks.\nMoreover, we explore the zero-shot generalization capabilities of BiGR within the generation domain. Unlike autoregressive models that must predict tokens in raster order, the masked modeling mechanism offers a huge flexibility during inference, allowing for the design of task-specific strategies. As a result, BiGR can perform various vision tasks in a zero-shot manner, without requiring any structural changes or parameter fine-tuning. In this paper, we showcase applications of our model in image inpainting, outpainting, editing, interpolation, and enrichment. We believe that further applications of BiGR can be unlocked through community efforts."}, {"title": "RELATED WORK", "content": "Binary latent code modeling Binary latent code, also known as hashing (Wang et al., 2017), has been largely demonstrated effective for visual representations due to its compactness and discreteness (Cakir et al., 2019; Jiang & Li, 2018; Shen et al., 2015; Wei et al., 2021; Wu et al., 2019). In the realm of visual generation, the study of binary tokenizers has recently attracted notable attention, referred as look-up free quantization in Yu et al. (2024) and as binary autoencoder in Wang et al. (2023). Binary tokenizers can enhance the codebook utilization for vector-quantization methods (Esser et al., 2021; Van Den Oord et al., 2017), facilitating image and video generation. Wang et al. (2023) introduces a Bernoulli diffusion process that operates on Bernoulli-distributed variables to generate binary latents. Our work studies this type of tokenizers and we propose a novel generative framework for uniform conditional generation and visual representation.\nGenerative representation learning Representation learning has long been an important topic, with self-supervised methods (He et al., 2020; Chen et al., 2020b; Caron et al., 2020; Grill et al., 2020; Caron et al., 2021; Zhou et al., 2022) dominating the field in the past few years. Some works learn visual representations through generative modeling. For example, iGPT (Chen et al., 2020a) predicts pixels in a manner similar to GPTs (Brown et al., 2020), while MAE (He et al., 2022) and MAGE (Li et al., 2023a) reconstruct masked image regions. ViT-VQGAN (Yu et al., 2022a) studies the representation capabilities of unsupervised generative models. However, these methods involve specialized designs for discriminative tasks and are not directly suited for conditional image generation. Our work broadens this scope by proposing a conditional image generation framework that consistently delivers both high-quality generation and strong representation capabilities.\nConditional image generation Conditional image generation has gained significant attention recently. Existing works on this topic can be broadly grouped into two categories: diffusion models (Ho et al., 2020; Song et al., 2021; Rombach et al., 2022; Peebles & Xie, 2023; Chen et al., 2024a), which gradually denoise a random Gaussian noise, and autoregressive models (Esser et al., 2021; Yu et al., 2022b;a; Sun et al., 2024; Tian et al., 2024), which predict the next tokens similarly to language models. The models based on masked prediction (Chang et al., 2022; Li et al., 2023a; Chang et al., 2023) can be classified as autoregressive models, as discussed in (Li et al., 2024). In this paper, for clarity, we use \u201cautoregressive\u201d to specifically refer to models that use causal attention and next-token prediction, and \"mask\" to refer to models using masked modeling. Although conditional generative models can produce visually compelling images, their representation capabilities have rarely been studied. Our work aims to bridge this gap."}, {"title": "METHOD", "content": "Our framework is based on a masked language model that operates directly on binary latent codes derived from images. We train the model by masking a portion of the input tokens and learning to unmask them using predicted output tokens. The prediction is achieved through a Bernoulli diffusion process (Wang et al., 2023), which is well-suited for generating binary latent codes. In sampling, we determine the order of tokens to be unmasked based on the magnitude of entropy computed from the predicted Bernoulli distribution probabilities. To obtain latent representations, we perform average pooling on the intermediate features of our model. We present the overview of BiGR in Fig. 2. We describe the details of each of its components below."}, {"title": "PRELIMINARY", "content": "We first review the binary tokenizer and the Bernoulli diffusion process that underpin our model.\nBinary tokenizer An image tokenizer T can encode an image x \u2208 R3\u00d7H\u00d7W into a sequence of latent codes {$\\varsigma_1, \\varsigma_2,\u2026\u2026\u2026, \\varsigma_n$} = T(x), where each $\\varsigma^i$ represents the code at a specific spatial position. Binary tokenizers (Yu et al., 2024; Wang et al., 2023), also known as lookup-free quantization, transform the code into binary format by\n$z^i = \\text{sign}(\\varsigma^i) = \\mathbb{1}\\{\\varsigma^i > 0\\},$ (1)\nand a corresponding token index $r^i$ can be computed by\n$r^i = \\sum_{k=1}^K 2^{k-1} \\cdot z^i[k],$ (2)\nwhere $z^i[k]$ denotes the k-th bit of the binary code $z^i$, and K is the number of binary bits (i.e., code dimension), resulting in a total of $2^K$ token indices. Using Eq. (2), (Yu et al., 2024) index image tokens with the binary code $z^i$ and build a vocabulary of size $2^K$ for generative purposes. In contrast, our approach focuses on directly modeling the sequence of binary codes ${z^i}_{i=1}^n$.\nBernoulli diffusion We generate binary codes through a Bernoulli diffusion process (Wang et al., 2023), which effectively models Bernoulli-distributed variables. Specifically, Bernoulli diffusion process adds Bernoulli noise from the starting point $z^0 \\sim q(z^0)$:\n$q(z^t|z^{t-1}) = B\\big(z^t; z^{t-1}(1 - \\beta_t) + 0.5\\beta_t\\big) \\quad t = 1,2,\u2026\u2026,T.$ (3)\nHere, B denotes a Bernoulli distribution, and the timestep t out of the total T is denoted as a superscript. We model the denoising process by $p(z^{t-1}|z^t)$, which predicts the Bernoulli distribution probabilities for the binary code at the previous timestep. By iterating the denoising process, starting with a coin toss B(0.5), we can finally generate binary codes that follow Bernoulli distributions."}, {"title": "MASKED MODELING ON BINARY LATENT CODES", "content": "Backbone We build our method upon the transformer-based language model Llama (Dubey et al., 2024; Touvron et al., 2023b;a). Unlike language, an image is not naturally modeled as a causal sequence of tokens, but instead, each token should have access to all others to better capture global visual information. Therefore, we replace the causal attention commonly used in language models with bidirectional attention, and let the model predict masked tokens instead of next tokens.\nInput projection In the input space, instead of looking up an embedding vector with a token index, we use a simple linear layer that projects the binary code onto the embedding space. This technique has recently been explored for continuous-valued tokenizers in Tschannen et al. (2023), and we find that it also works well for binary-valued tokenizers. We maintain standard conditional embeddings and mask embeddings, where the conditional embedding is appended at the start of the sequence, and the mask embedding replaces inputs at masked positions.\nMask-token prediction During training, we simply mask a portion of image tokens with a learn-able mask token [M]. The fraction of masked tokens follows a cosine schedule, as used in Li et al. (2023a). We compute losses only for the masked positions, where the model predicts the values of the masked tokens. Formally, let $f_\\theta$ represent the language model, and ${m_i}_{i=1}^n$ denote the sequence of binary codes that are partially masked. Here, M = {$m_i$}$_{i=1}^n$ indicates whether the i-th position is masked ($m_i$ = 1) or left unmasked ($m_i$ = 0). We obtain outputs at the masked positions from the language model {h}$_{m_i=1}^n$ = $f_\\theta$({$z_{m_i}$}$_{i=1}^n$), which are distributed in a continuous space.\nBinary transcoder We transform the model outputs h into binary codes\u00b9 z through a Bernoulli diffusion process (Wang et al., 2023). In particular, we learn a denoising network $g_\\theta$ with a Sigmoid function S to model\n$P_\\theta(z^{t-1}|z^t) = B\\big(z^{t-1}; S(g_\\theta(z^t,t,h))\\big),$ (4)\nwhich predicts the probabilities of the Bernoulli distribution conditioned on the intermediate feature h. Consequently, binary codes can be generated by sampling from these probabilities. Following Wang et al. (2023); Ho et al. (2020), the training target is the binary residual, i.e., $z^t \\oplus z^0$ where $\\oplus$ represents the element-wise XOR operation. The training objective is simply an element-wise weighted binary cross-entropy (wBCE) loss, expressed as\n$\\mathbb{Y}_k = (z^t \\oplus z^0)[k] \\in \\{0,1\\} \\quad p_k = S(g_\\theta(z^t, t,h))[k] \\in [0,1],$ (5)\n$\\mathcal{L} = \\frac{1}{K}\\sum_{k=1}^K w_k \\big(y_k \\text{log } p_k + (1 - y_k) \\text{log}(1 \u2013 p_k)\\big),$ (6)\nwhere\n$w_k = (1 - y_k) \\cdot \\sum_{k=1}^K y_k /K + y_k \\cdot \\big(1 - \\sum_{k=1}^K y_k /K\\big) +1/K.$ (7)\nHere, $y_k$ represents the target, and $p_k$ is the predicted probability for the k-th bit in the binary code. The element-wise loss weight $w_k$ is applied to mitigate the imbalance between 0s and 1s, calculated based on their respective ratios in a K-dimensional code. The constant term 1/K is added to prevent nearly-zero weights that could impede training. In training, we jointly optimize the language model $f_\\theta$ and the denoising network $g_\\theta$ using the loss defined in Eq. (6).\nVisual representation Once trained, our model inherently possesses strong visual representations. Given an image, we input it into the model without any masks, along with an unconditional token appended. We then perform average pooling on the continuous-valued features h to derive the global representation of the given image. We observe that the most discriminative representation originates not from the final layer but from the middle layers within the transformer blocks, in line with the findings in Yu et al. (2022a); Chen et al. (2020a). As a result, we use the intermediate features as the final image representation."}, {"title": "ENTROPY-ORDERED GENERATIVE SAMPLING", "content": "For image generation, we design a sampling strategy for our model, enabling it to iteratively predict tokens from a fully masked sequence. Unlike in training, where mask positions are randomly chosen at each step, during sampling, the order in which tokens are unmasked follows a predefined criterion.\nWe arrange the masked tokens according to the binary entropy magnitude calculated from the predicted probabilities. The binary entropy is defined as:\n$\\mathcal{H} = -\\frac{1}{K}\\sum_{k=1}^K p_k \\text{ log}_2 p_k + (1 - p_k) \\text{ log}_2(1 \u2013 p_k),$ (8)\nwhich ranges from 0 to 1. Here, a low value indicates high prediction confidence (i.e., when $p_k$ is closer to either 1 or 0). Therefore, a confidence score can be derived from $1 - \\mathcal{H}$, illustrating the model's confidence in this prediction. Following Li et al. (2023a), we add a noise sampled from a random Gumbel distribution multiplied by the temperature $T$ to the confidence score.\nAt each sampling iteration, we select and unmask a proportion of masked positions with the highest confidence scores. To unmask each token, we obtain its binary codes by performing Bernoulli sampling from the distribution B($p_k$). The unmasking ratio follows a cosine schedule as used in Chang et al. (2022); Li et al. (2023a). This process operates over N sampling iterations. When the mask ratio drops to zero, the sampling progresses to the last iteration where all tokens are unmasked, marking the completion of the generation process."}, {"title": "EXPERIMENT", "content": "IMPLEMENTATION DETAILS\nModel configuration We use the binary autoencoder (B-AE) introduced by Wang et al. (2023) as our binary tokenizer. The downsampling rate of the autoencoder is 16, projecting a 256\u00d7256 image into a 16\u00d716 token sequence. We train four variants of the binary autoencoders designed with four different binary code dimensions, namely 16, 20, 24, and 32. With the four binary tokenizers, we train our BiGR of three different sizes based on Llama (Touvron et al., 2023a), namely L (316M), XL (743M), and XXL (1.38B). For the binary transcoder, we follow Li et al. (2024) to employ an MLP $g_\\theta$ with an adaptive LayerNorm, with sizes of 20M, 56M, and 104M respectively. For clarity, we denote the S-sized variant with a B-dim tokenizer as BiGR-S-dB, e.g., BiGR-L-d16.\nTraining details We train all models for 400 epochs, with L-sized models using a batch size of 1024 and the others using a batch size of 512. Our L/XL-sized models are trained on 8 A800 GPUs, while XXL-sized models are trained on 32 A800 GPUs. We maintain consistent training settings across all compared models based on the model size.\nSampling Our model inherently supports classifier-free guidance (CFG) (Ho & Salimans, 2022) through the Bernoulli diffusion process. Within our sampling process, four hyperparameters are involved: CFG scale, Gumbel temperature (T), the number of sampling iterations (N), and the number of Bernoulli denoising steps (T). We identify the optimal hyperparameter setting for all models. We set CFG to 2.5 for all quantitative evaluations, which has shown to be effective across all our models. We set T to 100 as default for all models."}, {"title": "UNIFORM PERFORMANCE", "content": "Evaluation We evaluate the uniformity of BiGR by concurrently comparing generative and discriminative performance. We evaluate generation quality on ImageNet-1K 256\u00d7256 by reporting Frechet Inception Distance (FID) as the main metric, along with Inception Score (IS), sFID, Precision (Pre.), and Recall (Rec.) as auxiliary metrics. All metrics are obtained using 50K generated samples. We assess representation capabilities through linear-probe evaluation, reporting the top-1 and top-5 accuracies, abbreviated as ACC1 and ACC5, on ImageNet-1k 256\u00d7256 validation split. We follow standard practice (He et al., 2022) by using a parameter-free BatchNorm (Ioffe, 2015) layer and a linear classifier layer to classify the model features. We use the intermediate features from the 10-th layer for L-sized models, the 15-th layer for XL-sized models, and the 32-nd layer for XXL-sized models, as experiments on d16 models demonstrate these configurations yield the best performance. Additionally, we compare the inference speed, specifically the time taken to generate each image using one A100 GPU with a batch size of 256.\nComparison Starting from the latest autoregressive generation baseline LlamaGen (Sun et al., 2024), we comprehensively analyze two major components in this paper, namely (1) training objectives, specifically categorical loss (cat.) and binary loss (bin.), and (2) modeling types, including masking and autoregressive (AR) approaches. In total, we compare five models in Tab. 1, training four models-S0, S1, S2, and S3-with different configurations, excluding LlamaGen. For LlamaGen, we use the generative performance metrics reported in their paper and conduct our own evaluation of linear-probe performance using their pretrained model. The inference time of all models is tested on the same machines by us."}, {"title": "MODEL ANALYSIS", "content": "We analyze each component of our proposed method below. All experiments are conducted on BiGR-L-d16 unless otherwise specified.\nBinary transcoder We apply Bernoulli denoising process (Wang et al., 2023) as our binary transcoder to generate probabilities of Bernoulli distributions, from which the binary codes are sampled. We experiment with two variants, namely (1) predicting the initial clean latent $z^0$, and (2) predicting the element-wise exclusive OR (XOR) value between the latent at the t-th timestep $z^t$ and $z^0$. We find empirically the latter performs better, and thus, we adopt this setting for all of our models. Alternatively, a na\u00efve approach involves using a direct binary cross-entropy (BCE) loss to train the model, replacing the Bernoulli denoising process. We compare these three variants in Tab. 2. Our method outperforms the other two variants across all main metrics. We observe that using direct BCE generates very smooth images which harms the generative performance. XOR prediction yields better generative and discriminative performance compared to $z^0$ prediction.\nSampling strategy In this paper, we propose a simple entropy-ordered sampling strategy tailored for the masked training paradigm. We compare our method with two alternative sampling orders, namely (1) a raster-scan order similar to the autoregressive approach, and (2) a random order. Like our strategy, both compared methods are applied to the same trained model. The comparison results of the generative evaluation are reported in Tab. 3. The results indicate that the proposed sampling strategy is the best fit for our model's generative purposes.\nInference hyperparameters We evaluate the impact of two hyperparameters specific to our model on its performance. (1) We first present the FID results and sample time for different numbers of sampling iterations N on the left side of Fig. 3. We observe that larger models generally achieve lower FID values, although they also increase sample time. In addition, more sampling iterations do not guarantee better performance, as different-sized models have varying optimal sampling iterations. For example, the L-sized model achieves its best performance with 20 iterations, rather than with larger numbers. (2) On the right side of Fig. 3, we present the results for different numbers of diffusion timesteps T. The results indicate that diffusion timesteps have a marginal impact on generative performance, suggesting that our model can achieve comparable generation quality with fewer diffusion timesteps. This can significantly accelerate generation speed, especially for larger models. For example, with 10 diffusion timesteps, the XXL-sized model can achieve a FID of 2.73 at a speed of 0.85s per image.\nModel size and code dimension We validate that our model is scalable by testing the performance of different-sized models using tokenizers with various code dimensions. Note that the dimension of the binary codes only alters the number of parameters in the input and output linear projections, resulting in minimal effects on the overall model size. The evaluation results of both generative and discriminative performance are shown in Fig. 4. Our model generally performs better with larger sizes across all code dimensions, as indicated by both generative and discriminative metrics.\nBesides, we have the following observations from Fig. 4. (1) When the model size is small, it becomes challenging to model large-dimensional codes, such as a dimension of 32 for the L-sized model, especially for generative purpose. (2) In contrast, as the model size increases, the improvement for smaller-dimensional codes is relatively modest, indicating that these codes are easier to model and can be effectively handled by smaller-sized models. (3) An exception arises in the linear-probe evaluation of models with 32-dimensional codes, where our XL-sized model outperforms the XXL-sized model. We hypothesize that this may be due to the optimal transformer layer for feature representation identified in the 16-dimensional model, which might not be the best choice for 32-dimensional models of the XXL size.\nUnconditional training Our model is a class-conditional generative model. Intuitively, conditional generative training adds condition guidance that is absent in downstream discriminative tasks, which can diminish the representation capabilities of the model. We validate this conjecture by comparing the linear-probe performance of our model with that of its unconditional counterpart. We train the unconditional model by replacing the class conditional tokens with a single unconditional token, and keep the inference process unchanged. We evaluate BiGR-L-d20 alongside its unconditional counterpart, and report the results in Tab. 4. The unconditional counterpart demonstrates better representation capabilities than our conditional model, indicating that discriminative tasks are more challenging for conditional generative models.\nResolution of 512\u00d7512 Using a binary autoencoder that projects a 512\u00d7512 image into 32\u00d732 binary latent codes, we enable our model to generate 512\u00d7512 images by increasing the input sequence length to 1024. We train such a binary autoencoder with a code dimension of 32 and train our model to accommodate this sequence length. We showcase the generated samples in Fig. 5, with additional samples available in Appendix C."}, {"title": "SYSTEM-LEVEL COMPARISONS", "content": "We re-emphasize that the goal of this work is to propose a uniform conditional generative model that can produce high-quality generations while maintaining strong representation capabilities. Therefore, surpassing state-of-the-art models across all metrics is not within the scope of this research. We provide a more comprehensive comparison in Appendix B."}, {"title": "ADDITIONAL GENERATED SAMPLES", "content": "We provide additional 512\u00d7512 samples and 256\u00d7256 samples generated by our model in Fig. 7. We also include uncurated generated samples from various classes in Fig. 8 to 19."}, {"title": "ETHICS STATEMENT", "content": "We recognize the ethical risks of image generation, such as potential misuse for harmful content. Our research aims to promote positive uses like creativity and education, with a commitment to responsible application. Safeguards and continuous ethical oversight are strongly encouraged."}, {"title": "CONCLUSION", "content": "We introduce BiGR as the first conditional generative model that unifies generative and discriminative tasks within the same framework. Through extensive experiments, we highlight its uniformity, efficiency, flexibility, and scalability. Our results demonstrate that BiGR achieves decent performance in both generation quality and linear separability. Additionally, we showcase its application in various zero-shot generalized tasks. We believe BiGR has the potential to be adapted for a broader range of applications in the future.\nLimitations (1) Our sampling strategy involves numerous hyperparameters to tune, resulting in a substantial search space; thus, the reported models may not represent the optimal settings. (2) The model's sequence length is fixed during training, making it inflexible to accommodate inputs of varying lengths. Consequently, generating higher-resolution images requires re-training the model."}]}