{"title": "Development and Application of Self-Supervised Machine Learning for Smoke Plume and Active Fire Identification from the FIREX-AQ Datasets", "authors": ["Nicholas LaHaye", "Anistasija Easley", "Kyongsik Yun", "Huikyo Lee", "Erik Linstead", "Michael J. Garay", "Olga V. Kalashnikova"], "abstract": "Fire Influence on Regional to Global Environments and Air Quality (FIREX-AQ) was a field campaign aimed at better understanding the impact of wildfires and agricultural fires on air quality and climate. The FIREX-AQ campaign took place in August 2019 and involved two aircraft and multiple coordinated satellite observations. This study applied and evaluated a self-supervised machine learning (ML) method for the active fire and smoke plume identification and tracking in the satellite and sub-orbital remote sensing datasets collected during the campaign. Our unique methodology combines remote sensing observations with different spatial and spectral resolutions. The demonstrated approach successfully differentiates fire pixels and smoke plumes from background imagery, enabling the generation of a per-instrument smoke and fire mask product, as well as smoke and fire masks created from the fusion of selected data from independent instruments. This ML approach has a potential to enhance operational wildfire monitoring systems and improve decision-making in air quality management through fast smoke plume identification and tracking and could improve climate impact studies through fusion data from independent instruments.", "sections": [{"title": "1. Introduction", "content": "An important and common application of machine learning (ML) is to identify and leverage latent patterns in data or imagery. A typical approach is to use supervised learning, which requires a set of truth labels that the ML method attempts to generalize to the problem of mapping from an input dataset X to the output Y through a set of features, M. The challenge with supervised learning, and even the recently popularized semi-supervised learning, is acquiring a sufficiently large and unambiguous set of labels, which often requires many hours of manual labor on the part of domain experts. Alternatively, self-supervised learning takes a similar input dataset X and finds relationships among the features M resulting in context-free groupings in the output Y. Because no labels are provided for the input, there are no labels provided in the output. To utilize the results, the labels or missing context must be assigned after the fact by experts, but this has proven to be a much less labor-intensive endeavor, all while keeping subject matter experts in the loop.\nIn previous work, we demonstrated that feeding 2-dimensional images of instrument radiances, or Level 1 (L1) data, into Deep Belief Networks (DBNs) coupled with an unsupervised clustering method results in images automatically segmented into relevant geophysical objects [1]. We further demonstrated that the same results can be achieved using a simplified architecture across select areas of the globe and for various kinds of land surface and atmospheric segmentation tasks [2].\nIn our recent work [3], we have generalized our ML framework into an open-source software system called Segmentation, Instance Tracking, and data Fusion Using multi-SEnsor imagery (SIT-FUSE). This framework allows for various types of encoders, including regular and convolutional DBNs, Transformers, and Convolutional Neural Networks (CNNs), and we have moved from traditional unsupervised clustering to a deep-learning-based clustering approach.\nThis approach, as a whole, has several unique benefits. First, it is not restricted to a particular remote sensing instrument with specific spatial or spectral resolution. Second, it has the potential to identify and \u201ctrack\u201d geophysical objects across datasets acquired from multiple instruments. Third, it allows for the joining of data from different instruments, \"fusing\" the information within the self-supervised encoder. Finally, it can be applied to many different scenes and problem sets, most notably in no- and low-label environments, not just ones for which labeled training sets exist, which is required for strictly supervised ML techniques.\nHere we apply our self-supervised ML approach to the problem of automatically detecting and tracking wildfires and smoke plumes, through sequences of L1 (imagery) data acquired by multiple remote sensing instruments during the joint National Aeronautics and Space Administration/National Oceanic and Atmospheric Administration (NASA/NOAA) Fire Influence on Regional to Global Environments and Air Quality (FIREX-AQ) field campaign that took place in the western United States in the summer of 2019 [4]. The high-altitude NASA ER-2 carried seven remote sensing instruments that provided high-spatial resolution observations of fires and smoke plumes in conjunction with NASA DC-8 aircraft and multiple satellite overpasses over same fire events. The FIREX-AQ datasets of collocated satellite and multiple airborne imagery at different spatial resolution are excellent as a testbad for SIT-FUSE method of fire/smoke identification and tracking.\nWildfires and the smoke plumes induced by wildfires substantially contribute to the carbon cycle and can have a long-lasting impact on air quality and Earth's climate system. In addition, human-driven climate change is associated with more frequent and severe wildfires [5]. Despite the importance and immediacy of the problem, most research and decision-support tools to study wildfires and plumes use observations from a single instrument whose spatial coverage and (spatial, spectral, and temporal) resolutions vary from very fine to very coarse scales, neither of which, on their own, is fully capable of providing the much-needed information for a comprehensive understanding of wildfires and wildfire smoke [6]. As such, the current study aims to bine datasets with different spatial resolutions from multiple instruments to create a patchwork of datasets that fill the temporal gaps present in current single-instrument fire detection datasets. Here, the first step is testing a general framework for segmenting the datasets from multiple instruments and identifying wildfires and smoke plumes.\nThe detection and tracking of objects, like wildfires and smoke plumes, within a single instrument data has long required developing instrument-specific retrieval algorithms. Such development is labor-intensive and requires domain-specific parameters and instrument-specific calibration metrics, alongside the manual effort to track retrieved objects across multiple scenes [7]. The recent development of retrieval algorithms is actively underway in the field of supervised deep learning (DL), and various methods (e.g., Convolutional Neural Networks (CNN)s) have been applied. Some of these DL methodologies work well, in terms of precision and accuracy [7], but are still limited by the requirement that the spatial resolutions between training datasets and output products be the same. These methods also require pre-existing label sets, unlike recent supervised approaches like Fully Convolutional Networks (FCNs), Mask R-CNNs, and Transformers [8-10], which require large label sets to archive accurate results."}, {"title": "2. Related Works", "content": "Work on the general problem of self-supervised image segmentation appears to have had success in separating the foreground from the background [22,23], or have only used single bands of input from one type of instrumentation, which is effective for their applications, but does not cover the breadth required here [24]. Other works have aimed to perform tasks, such as outlining buildings and roadways [25], which is not the goal here. A similar study, which used just autoencoders and a form of clustering, an overall architecture that is close to a subset of ours, attained an accuracy of 83% on Landsat imagery alone [26]. This uses a similar kind of model to our studies, but uses a single instrument. With large variations in spatial and spectral resolutions, our technique attains higher accuracy (and balanced accuracy, in some cases) across many different instrument sets, including fused data. Even with more recent breakthroughs in semi-supervised semantic segmentation, like the Segment Anything Model (SAM), a problem-dependent amount of labels is required, and SAM is largely unproven in complex domains like remote sensing [27]. The identification of the necessary size of label sets, generation of per-pixel label sets, and testing of the feasibility of new techniques in more complex domains are all problem-specific and time-consuming tasks that can be skipped, given our solution - as seen in the successful but extremely limited cases discusses in [28-31]. Lastly, there are new physics-based retrieval techniques, which seem promising, but need continued rigorous analysis to generalize across different regions and instrument types [32]. In the future, it may be useful to combine the physical parameterizations and ML-based retrievals via ML loss functions that are 'physics-aware'. The lack of need for large new label sets mitigates the costly, labor-intensive work of manually segmenting each pixel within a dataset used for ground truth, a process which is itself error-prone, and other previously mentioned supervised learning-related precursors model training. Also, leveraging pre-existing operational products to use as labels for supervised learning tasks will inherently cause them to either lack training set diversity or suffer from the issues mentioned above. On the other hand, our approach is well suited to handle large amounts of data, because our unsupervised and self-supervised models can perform label-free image segmentation. The fact that the human-in-the-loop steps of context application and validation occur after the images have been segmented allows for human oversight while mitigating the need for the extremely labor-intensive act of pixel-by-pixel manual segmentation for tens of thousands of images."}, {"title": "3. Materials and Methods", "content": "For single instrument cases, data was re-projected to the WGS84 Latitude/Longitude projection. For multi-sensor cases, data is collocated, re-projected to the WGS84 Latitude/Longitude projection, resampled to the lowest common spatial resolution, and stacked channel-wise. The actual fusion occurs as a part of the representation learning done inside the encoder.\nSamples were generated by taking pixels and their direct neighbors (center pixel + 8 neighbors), in all channels, and creating a flat vector. These samples were then standardized by removing the per-channel mean and scaling to per-channel unit variance. The per-channel mean and standard deviations are computed over the full sample set from the scenes used for training.\nFor training, the scenes used are subsets. To ensure the subset of samples from the training scenes contained a representative variety of terrain and phenomena, k-means clustering was applied to the data with a set of 50 classes. This stratification technique was chosen as it has proven to be an effective, albeit naive, way to ensure variation within training samples [33-35]. Three million samples are randomly sampled based on the stratification generated by the full number of samples from the training scenes labeled with the 50 k-means classes. All pixels that are set to fill values, or out of specified valid ranges are discarded before any pre-processing. Regarding the spectral bands used, all spectral bands were used except bands that were extremely noisy or known to be non-functional for the FIREX-AQ campaign time frame. The same pixels used to train the encoders are also used to train the deep clustering heads, and the full training scenes are used to assign context.\nSIT-FUSE can take larger tiles for Convolutional DBNs, CNN-based, and Transformer-based architectures (as mentioned below). However, the pixel neighborhood has proven to provide enough spatial context with regular DBNs, as used in this work. \nSIT-FUSE is developed to be a generic framework allowing various kinds of encoders and foundation models that leverage self-supervised representation learning, including Deep Belief Networks (DBNs) trained using contrastive divergence, Convolutional Neural Networks (CNNs) with residual connections trained via Bootstrap Your Own Latent (BYOL), and Transformers trained using Image-Joint Embedding Predictive Architecture (I-JEPA) or Masked AutoEncoders (MAEs), as well as pre-trained Transformers for Earth Science, like Clay [36-39]. For all of these experiments, we used DBNs with 2-3 layers. DBNs were selected here because previous work and experiments done in this work demonstrated that they produce reasonable results and the parameter space is much smaller than the other models mentioned above. We have done extensive validation on the use of DBNs from both the perspective of structural understanding and downstream task performance, as well as resource consumption assessment, for the large set of single-instrument and fusion datasets [2]. In short, 2-3 layer DBNs provide a relatively compact model ( 2 million parameters vs 100 million - 10 billion parameters) with representational capabilities that meet our needs, and have demonstrated out-sized representational capabilities in other experiments relative to much larger models [38]. We are currently evaluating encoder complexity in relation to segmentation performance and geographic coverage, to optimally operationalize this approach for operational global production, which will be discussed further in later manuscripts.\nAs in our previous work [1-3], the DBN architectures used leverage feature expansion, outputting embeddings of a pixel neighborhood in a larger feature space than the one with which the samples were input. Similar to the idea behind input kernelization, architecture-based feature expansion allows the models to learn nonlinear latent patterns that would be compact and complex in lower dimensions in simpler but higher-dimension forms. This method, although not the most common use case for DBNs or encoders in general, has been demonstrated effectively in other studies as well as in our previous work [1,2,40-42]. In previous works, we held architecture parameters static for all tests to demonstrate efficacy. Here, we varied the number of layers and hidden/output parameters for each encoder used, based on input data spectral resolution."}, {"title": "Deep Clustering", "content": "To extract segmentation maps from the per-pixel embeddings, we use deep clustering, specifically Information Invariant Clustering.\nPrevious experiments used BIRCH and other forms of traditional agglomerative clustering. We have transitioned to DL-based clustering because the training time, inference time, memory requirements, and model re-usability are all much improved when using neural network layers via PyTorch when compared to using traditional clustering via sci-kit-learn. These layers are trained using the invariant information clustering (IIC) loss function. IIC aims to assign labels that maximize mutual information between an input sample x and a perturbed version of x, x' [43] . For our use, perturbations are additions of Gaussian noise to the outputs of RBM-based architectures."}, {"title": "Hand Labeling", "content": "For both context assignment and validation, subject matter experts labeled areas of high-certainty smoke, fire, and associated backgrounds. All areas that labelers were uncertain about, remained without labels. This labeling process was done by generating polygons over the remote sensing imagery. Because scenes can have overlapping classes (i.e. fire and smoke are contained in the same pixel), but also have areas distinct to a single class, a separate background class label set was generated for fire and smoke.\nUnlike labeling for supervised learning, this approach does not require all training samples to be labeled, which is relevant for problems with high uncertainty in boundary cases, like the segmentation of fire fronts and smoke plumes. The labeling of only the high certainty class areas allows us to capture and compare against segmentation structure, and provide ample samples for context assignment. Although this labeling minimizes the pre-analysis labor required from subject matter experts, it still keeps experts in the loop (a crucial piece for science-related ML tasks). Relative to the number of scenes labeled here, supervised and semi-supervised tasks require 100x more labeled samples or more. Also, because they are learning the mapping between the labels and the input datasets directly, they require much more complete label sets (i.e. uncertain areas must be labeled background or foreground, potentially leading to systematic over-segmentation or under-segmentation)."}, {"title": "Context Assignment", "content": "To assign context to the context-free segmentation maps generated via SIT-FUSE, we used zonal histogramming. For this, we overlayed the labels for a specific scene on the segmentation map generated for the same scene and generated counts of overlapping occurrences of the segmentation map classes and the high-certainty smoke, fire, and background labels. These were gathered over multiple scenes, and then each class in the segmentation maps was assigned to the labels it best matched. This assignment was done separately for each class of interest and its associated background class to ensure multi-class representation, like pixels containing both smoke and fire."}, {"title": "Contouring and Filtering", "content": "As the pixels within the detection are components of larger, often connected, geophysical objects, SIT-FUSE allows for building objects from separate detection pixels. To do this, we leverage the contouring capabilities within the openCV python / C++ software package [44,45]. Once generated, these contours are filled and we are left with multiple separate objects, instead of many more distinct pixels. We can also filter out small and large objects, where appropriate, and while not used here, leveraging dilations and erosions on the contoured objects is also possible. To mimic the automated process of an operationalized version and ensure fair comparisons and validation, all processing is performed uniformly across all scenes in an input set."}, {"title": "Validation", "content": "There is no direct ground truth here - meaning comparisons are done against pre-existing detections as well as hand labels generated over only the high certainty areas of smoke plumes and fire fronts - and we know there are other areas of these objects that are not included in the labels. Therefore, recall, precision, and their collective summary via F1-scoring are too harsh of evaluators here. We evaluated some previously published versions of precision and recall that apply fuzzy logic, but ultimately landed on the structural similarity index (SSIM) to evaluate performance across the various dataset pairs[46,47]. This is a fairly common problem within the remote sensing domain and one we aim to help solve with the collective incorporation of self-supervised learning, subject matter expert domain knowledge, and large amounts of data [48,49]."}, {"title": "Commercial Remote Sensing Products", "content": "As a part of this effort, we evaluated the utility of PlanetScope data for wildfire and smoke detection and monitoring. We found that the older PlanetScope SuperDove imagers available during the FIREX-AQ 2019 campaign were not suited for identifying fire fronts across the entire test set. However, they were effective at detecting smoke within the scenes evaluated. The improved temporal resolution of smoke detections at high spatial resolution will be helpful both for air quality research and automated smoke tracking. Also, as Planet's instrument offerings continue to diversify, with new SuperDove instruments having a higher spectral resolution, hyperspectral instrumentation being launched, and other companies continuing to launch various other Earth-orbiting instrumentation, it is critical that we look at the tradeoffs between paying for data from commercial entities and the scientific value added by such data. In this study, we analyzed 83 additional scenes over the Williams Flats area over four days. Per the NASA / Planet CSDSA agreement, we are not able to release the input SuperDove data, alongside all of the other data we are releasing. However, we will release the generated output and all other associated data."}, {"title": "Discussion and Current/Future Work", "content": "Overall, SIT-FUSE effectively identifies and segments wildland fire fronts and smoke plumes, and performs better against high-certainty smoke and fire label sets, when compared against other operational and experimental approaches, as seen in Tables 5 and 8. As expected, the higher the resolution of the instrument, the more detailed segmentation can occur. However, the different resolutions are valuable not only as standalone datasets but also for tracking these objects across multiple datasets. While further large-scale validation is needed, our tests demonstrate the capability to increase the temporal resolution of firefront products. We do this by increasing the number of instruments available for fire and smoke segmentation and developing brand-new firefront and smoke plume products for many of the instruments tested. This indicates significant potential for both product generation and utilization of this technique for segmentation and instance tracking. This not only also allows for dynamic instance tracking across scenes with the same input set, but we believe by harnessing style transfer capabilities, we can also look into instance tracking across multi-sensor scenes from disparate input datasets. We hope to transition this technology into a piece of future operational campaign support or product generation systems.\nIn terms of feature interpretability and selection, methods such as SHAP analysis and other explainability methods can be applied to better understand feature importance and refine the input to focus on spectral bands most effective for identifying smoke and/or fire. Given the current performance and the success with datasets where there was no pre-existing operational fire or smoke detection methodology, solutions like SIT-FUSE can be integrated into new or existing instrumentation data processing pipelines. By doing so, this approach could replace or augment instrument-specific retrieval algorithms, which may be extremely costly to develop. SIT-FUSE's segmentation capabilities offer additional benefits: the decrease in data volume processed for downstream fire or smoke-specific retrievals. By isolating the detected objects, only relevant pixels need to be processed through a downstream retrieval, thereby optimizing the pipeline.\nWe have built a framework within SIT-FUSE that is adaptable to various kinds of encoders and we aim to be able to leverage this to analyze representative capabilities of different model types, complexities, and training paradigms. With the continued influx of new architectures and large Earth Observation Foundation Models, it is important to understand these models provide quality representations (or poor ones) under different conditions, problem sets, and input datasets [68]. Analyses of downstream task performance is a crucial piece, but not the entire solution. More robust ways to evaluate representative capabilities are emerging in around large language models (LLMs), and much of this can be ported to computer vision, and specifically deep learning for Earth Observations [69]. With the flexible framework of SIT-FUSE we are working towards providing initial pathways towards tackling some of these open problems.\nLastly, we are working to leverage SIT-FUSE to make an impact within the area of analysis and scientific understanding - in this case correlated to wildfires and smoke plumes. There a built-in co-discovery facilitation mechanism, by way of the heirarchichal context-free segmentation products. By using the model-derived separations of various areas, novelty and \"interesting\" samples can more easily be grouped and investigated. This can be even further coupled with more detailed analyses of the embedding spaces relative to the context-free segmentations [3]. To enhance exploration even further models trained for co-exploration of data using open-ended algorithms can be leveraged to more quickly sift through the volumes of data and highlight interesting, new, and anomoulous samples [70,71]."}]}