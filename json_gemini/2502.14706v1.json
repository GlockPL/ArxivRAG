{"title": "Building reliable sim driving agents by scaling self-play", "authors": ["Daphne Cornelisse", "Aarav Pandya", "Kevin Joseph", "Joseph Su\u00e1rez", "Eugene Vinitsky"], "abstract": "Simulation agents are essential for designing and\ntesting systems that interact with humans, such\nas autonomous vehicles (AVs). These agents\nserve various purposes, from benchmarking AV\nperformance to stress-testing the system's limits,\nbut all use cases share a key requirement: relia-\nbility. A simulation agent should behave as in-\ntended by the designer, minimizing unintended\nactions like collisions that can compromise the\nsignal-to-noise ratio of analyses. As a founda-\ntion for reliable sim agents, we propose scaling\nself-play to thousands of scenarios on the Waymo\nOpen Motion Dataset under semi-realistic limits\non human perception and control. Training from\nscratch on a single GPU, our agents nearly solve\nthe full training set within a day. They general-\nize effectively to unseen test scenes, achieving a\n99.8% goal completion rate with less than 0.8%\ncombined collision and off-road incidents across\n10,000 held-out scenarios. Beyond in-distribution\ngeneralization, our agents show partial robustness\nto out-of-distribution scenes and can be fine-tuned\nin minutes to reach near-perfect performance in\nthose cases. Demonstrations of agent behaviors\ncan be found at this link. We open-source both\nthe pre-trained agents and the complete code\nbase. Demonstrations of agent behaviors can\nbe found at https://sites.google.com/\nview/reliable-sim-agents.", "sections": [{"title": "1. Introduction", "content": "Simulation agents are a core part of safely developing\nand testing systems that interact with humans, such as au-\ntonomous vehicles (AVs). In the context of self-driving,\nthese agents, also referred to as road user behavior models,\nserve two primary purposes: establishing benchmarks for\nAV behavior, and representing other\nroad users in simulators to enable statistical safety testing\nin both nominal and rare, long-tail scenarios. While each use case brings\nparticular requirements, reliability is an important one that\nthey share.\nA reliable simulation agent consistently behaves as intended\nby the designer, minimizing unintended actions. For in-\nstance, agents designed to stress-test AVs should reliably ini-\ntiate realistic near-collision events, generating safety-critical\nscenarios to provide meaningful information about the sys-\ntem's behavior in edge cases. Conversely, nominal agents\nshould focus on replicating typical road behavior to simplify\nexperiments that vary other environmental factors, such as\nweather. In either case, unreliable sim agents introduce\nnoise into the evaluation process by producing trajectories\nthat crash too infrequently in the stress-test case and too\nfrequently in the nominal case.\nHow can we build sim agents that are close enough\u00b9 to real-\nity while maximizing designer specifications i.e. reliability?\nOne approach relies on generative models, which have shown\nremarkable progress in producing diverse, human-like be-\nhaviors through imitation learning from demonstrations. How-\never, whether they meet the reliability standards of a fully\nautomated AV development pipeline is uncertain. This is\nhighlighted by the top-performing models in the Waymo\nOpen Sim Agent Challenge, a well-known benchmark for realistic nominal road user be-\nhavior. While state-of-the-art models in the 2024 challenge\nclosely replicate logged human trajectories and achieve high\nscores on various distributional metrics, they still fall short in\ncritical areas. Ground-truth human trajectories in the dataset\nrarely or never involve collisions or off-road movements, yet\nthe top submissions (1st and 2nd place) frequently display\nsuch unintended behaviors. Specifically, simulated agents\ncollide with others in 5-6% of scenarios and go off-road\nin 6-12% of cases."}, {"title": "2. Method", "content": "We conduct our experiments in GPUDrive, a data-driven,\nmulti-agent, GPU-accelerated simulator. GPUDrive contains $K = 160,147$ real-world traffic\nscenarios from the Waymo Open Motion Dataset (WOMD). Each scenario $k \\in K$ comprises a\nstatic road graph, $R_k$, and a time series of joint logged human\ntrajectories:\n$S_k = {\\{(s_t, A_t)}_{t=0}^{T=90}, R_k}$\nwhere $s_t \\in \\mathbb{R}^{(1,F)}$ represents the world state represented\nas $F$ features at time $t$, and $A_t \\in \\mathbb{R}^{(N,2)}$ represents\nthe action matrix for all $N$ agents in the scene. The joint agent\ndemonstrations are 9 seconds long and discretized at 10Hz.\nSimulator state\nAgent observation"}, {"title": "2.2. Task definition and measuring performance", "content": "We aim to systematically study how the reliability of sim-\nulation agents trained via self-play scales with data. To do\nthis, we design a task with well-defined metrics such that\nexperimental results are easy to interpret. Given a traffic\nscenario $S_k$ with $N$ controlled agents we task every agent\nto navigate to a designated goal position while satisfying\ntwo criteria: (1) avoiding collisions with other agents and\n(2) staying on the road.\nTo obtain valid goals, we use the endpoints $(x_f^i, y_f^i)$ (marked\nby $\\circ$ in Figure 2) from the WOMD. Agents are initialized\nfrom the starting positions $(x_i^i, y_i^i)$ of the WOMD. Given\nhow the WOMD dataset is collected and processed, we know\nthat the human road users in the dataset must have success-\nfully reached their endpoints within 9 seconds (or 91 steps).\nAs such, we assume that, in principle, all agents should be\ncapable of doing the same. To reflect this, a scenario is con-\nsidered solved when all controlled agents reach their target\npositions within 91 steps while adhering to the specified\ncriteria."}, {"title": "2.2.2. Metrics", "content": "We use four scene-based metrics to quantify performance:\n* Goal achieved \u2191: Percentage of agents that reached\ntheir target position within $T = 91$ steps.\n* Collided\u2193: Percentage per scenario indicating objects\nthat collided, at any point in time, with any other object,\ni.e. when the agent bounding boxes touch.\n* Off-road: Percentage of agents per scenario that went\noff the road or touched a road edge, at any point in time.\n* Other: Percentage of agents per scenario that did not\ncollide or go off-road but also did not reach the goal\nposition.\nThe Collided and Off-road metrics align with the Waymo\nOpen Sim Agent Challenge and Waymax. Specifically, Collided is part of\nthe \"object interaction metrics\" category and the off-road\nevents are part of the \"map-based metrics\u201d category. Un-\nder the assumption that human road users have near zero\ncollision and off-road events, we can meaningfully compare\nour scores to the top submissions.\nThe Goal achieved metric is not directly reported in\nWOSAC, making it less comparable. The most similar met-\nric is the Route Progress Ratio used in Waymax, which measures how far an agent travels along\nthe logged trajectory. However, since our focus is not on\nmimicking logged trajectories but on precisely reaching a\nparticular goal, a binary metric is, in our case, a more mean-\ningful indicator of performance. However, reaching the goal\nroughly corresponds to a Route Progress Ratio of 100%.\nAgent-based metrics: Since the scene-based metrics are bi-\nased towards scenes with a small number of agents (one\nagent colliding in a scene with 2 agents vs. 10 scenes pro-\nvides a fraction of 1/2 vs 1/10th), we also report the metrics\nabove in agent-based way, where we aggregate the counts"}, {"title": "2.3. State and observation space", "content": "This section outlines the design choices and parameteriza-\ntion of the observation $o$ for agent $i$ at time $t$. We make\nthese choices to reflect semi-realistic limits on human per-\nception. The observation encodes the agent's partial view of\nthe scenario state $s_t$, capturing the information necessary for\ndecision-making. In this work, we model the RL problem as\na Partially Observed Stochastic Game (POSG), where agents make simultaneous decisions under\npartial observability. We further make the following design\nchoices for our agents:\nRelative coordinate frame All agent information is pre-\nsented in an ego-centric coordinate frame to align with\nhuman-like perception.\nObservation radius The observation radius $r_o$ determines\nthe visible area around the agent. For our experiments, we\nset $r_o = 50$ meters, as illustrated in Figure 2.\nNo history Agents only receive information from the cur-\nrent timestep.\nRoad graph We reduce the full road graph, which consists\nof up to 10,000 sparsely distributed road points, in dimension\nfor computational efficiency. To reduce the number of points\ncorresponding to straight lines, we run the polyline reduction\nthreshold of the polyline decimation algorithm in GPUDrive to 0.1 which roughly cuts\nthe number of points by a factor of 10. We also cap the\nmaximum visible road points at 200, selecting 200 points\nfrom those in the view radius in a random order if there are\nmore than 200 points, creating a sparse view of the local road\ngraph. Empirical results show this is sufficient for agents to\nnavigate toward goals without going off the road or causing\ncollisions.\nNormalization Features are normalized to be between -1\nand 1 by the minimum and maximum value in their respec-\ntive category."}, {"title": "2.4. Action space and dynamics model", "content": "To align with the control outputs of real human road users\nmore closely, we take the action for every agent $i$ to be a"}, {"title": "2.5. Reward function", "content": "We define the individual agent rewards as follows:\n$r(o, a) = w_{\\text{Goal achieved}} \\cdot [[\\text{Goal achieved}]]$\n$w_{\\text{Collided}} [[\\text{Collided}]]$\n$w_{\\text{Offroad}} [[\\text{Offroad}]]$\nHere, $[[\\cdot]]$ is an indicator function that equals 1 if the\ncondition is true and 0 otherwise. We assign weights\n$w_{\\text{Goal Achieved}} = 1.0$, $w_{\\text{Collided}} = 0.5$, and $w_{\\text{Offroad}} = 0.5$.\nUnder our discount factor of .99, an agent will only be will-\ning to accept a collision or off-road event if it would allow\nthem to achieve their goal over 70 steps faster. Note that\nsuch scenarios do occur in the dataset infrequently and this\nparticular combination of collision weights and discount\nfactor may occasionally make colliding an optimal behavior.\nAn agent achieves the goal position when it reaches within a\n2-meter radius of the target $(x, y)$. Once an agent reaches its\ngoal, we remove it from the scene. This latter choice is made\nas it is ill-defined what an agent should do after it reaches\nits goal."}, {"title": "2.6. Collision behavior", "content": "During training and testing, we allow agents to continue the\nepisode even after going off-road or colliding with another\nagent in the scene. Agents receive a penalty for each collision\nor off-road event, allowing them to accrue multiple penalties\nthroughout an episode."}, {"title": "2.7. Models", "content": "We use a neural network with an encoder and a shared embed-\nding, as illustrated in Figure 3. The flat observation vector is\nfirst decomposed into three modalities: the dense ego state,\nthe sparse road graph, and the sparse partner observations.\nEach modality is processed independently. Inspired by the\nlate fusion approach in Wayformer, we then concatenate the outputs, apply max pooling, and\npass the result through a shared embedding. This hidden\nembedding is fed into separate actor and critic heads, each\nimplemented as a single feedforward layer. The model only\nhas \u2248 50,000 trainable parameters."}, {"title": "2.8. Training", "content": "In each scenario, we control up to $N = 64$\nagents using a shared, decentralized policy $\\pi_\\theta$. Actions\nare independently sampled from the policy based on the\nego views of each agent $i$ during every step in the rollout:\n$a \\sim \\pi_\\theta(\\cdot|o)$. We train agents using Proximal Policy\nOptimization (PPO) using batches of\n$S = 800$ distinct scenarios, with the set of training scenarios\nuniformly resampled every 2 million steps. Initially, agents\nexhibit random behavior and crash frequently. Over time,\nthe agents' behavior becomes more streamlined, creating\nsmooth trajectories with high rates of reaching the goals."}, {"title": "3. Related work", "content": "Self-play for agents in games Self-play RL has been a core ingredient in\ncreating effective agents across a wide range of complex\ngames. Notable examples include superhuman game play in\ntwo-player zero-sum games like Chess and Go , expert human-level play in Stratego and Starcraft , as well many-\nplayer games that require some level of cooperation like\nDiplomacy and Gran Turismo . These successes have demonstrated the\neffectiveness of self-play, particularly in the large-data, large-\ncompute regime. However, the majority of its successes are\nin variants of zero-sum games whereas driving tasks are\nlikely general-sum and feature many-agent interaction.\nRL for driving agents Reinforcement learning has been\nexplored for the design of autonomous driving agents,\nthough state-of-the-art agents are currently far below the\nhuman rate of between 800000 km per police-reported traf-\nfic crash in the United States or as\nmuch as 1 crash per 24800 km in more challenging domains\nsuch as San Francisco . These agents\nare frequently trained in simulators built atop large open-\nsource driving datasets such as Waymo Open Mo-\ntion (WOMD), (Caesar et al., 2020, ONE-Drive) though there\nare also procedurally generated and non-data-driven simulators. These\ndatasets collectively add up to tens of thousands of hours of\navailable data and are often used to train RL agents in log-\nreplay mode, a setting in which only one agent is learning\nand the remainder are either replaying human trajectories or\nexecuted hand-coded policies. The complexity of scaling RL\nin these settings has led to the creation of batched simulators\n whose high throughput helps ameliorate issues of\nsample complexity. Many works have explored ways to use\nthese simulators to learn high-quality reinforcement learning\nagents through RL including uses of self-play . Our work is mostly distinct from these by the scale of train-\ning and a significantly lower crash and off-road rate than has\npreviously been observed."}, {"title": "4. Results", "content": "Solving the full Waymo Open Motion Dataset under par-\ntial observability We investigate whether agents with a\npartial view of the environment can solve all scenarios in\nthe Waymo Open Motion Dataset. Our results show that\nnearly all scenarios can be solved successfully. After 1 bil-\nlion training steps, agents achieve a goal-reaching rate of\n99.84%, a collision rate of 0.38%, and an off-road rate of\n0.26% on the training dataset. Furthermore, as shown in\nFigure 5, we observe that the failure rates would continue\nto tick down with continued training. This training run took\n24 hours on a single NVIDIA A100 GPU.\u00b3\nThe agent-based metrics are similar to the scene-based met-\nrics reported above: a goal rate of 99.72%, collision rate of\n0.26%, and an off-road rate of 0.35%.\nEffective generalization to unseen scenarios with suffi-\ncient data To assess how self-play performance scales\nwith the diversity of training scenes, we conduct experi-\nments with 100, 1,000, 10,000, and 100,000 unique training"}, {"title": "4.2. Distribution of errors and remaining failure modes", "content": "We analyze scenarios that are not perfectly solved, defined\nas those with a collision rate or off-road rate greater than\n0, or where at least one agent fails to reach its goal. A\nselection of failure modes can be viewed on the project page.\nTogether, these account for 8.95% of the test dataset (896 out\nof 10, 000 scenarios). Figure 6 shows the histogram of error\ndistributions, revealing that most unsolved scenarios have\nonly a small error rate. To examine potential relationships\nbetween failure modes, we compute the Pearson correlation\nbetween off-road fractions and collision rates. The result,\n$\\rho = 0.0135$, is not significant at $\\alpha = 0.05$, indicating no\nmeaningful correlation between these two metrics in the\nunsolved scenarios and suggesting that errors are spread\nacross scenarios.\nAdditionally, we analyze the top 0.5% failure modes in each\ncategory (collision rates, off-road rates, and agents that did\nnot reach the goal position) of the test set. This analysis"}, {"title": "4.2.1. RARE MAP LAYOUTS AND OBJECTS", "content": "High off-road rates occur in scenarios with rarely occurring\nroad structures. One example of this is roundabouts. A large\nfraction (15%) of the top fraction of collision rates was in\nroundabout scenes. The rest included road layouts that are\nsimply harder to navigate, such as tight corners, narrow lane\nentries, parking lots, etc. Larger vehicles especially strug-\ngle with such maps. This coupled with multiple vehicles\ncrowding leads to some of them going off-road."}, {"title": "4.2.2. COORDINATION", "content": "High collision rates occur in intersections, speedy highways,\nand crowded scenes where sophisticated interaction is re-\nquired (eg: letting another agent pass before you, making\nspace for another agent to overtake, etc). Crowding and\ninteraction coupled with rare map layouts compound the\ndifficulty of the scene and lead to a higher collision and\noff-road rate."}, {"title": "4.2.3. OUT OF TIME", "content": "Some agents have goals further away than others. Having a\nfinite horizon of 91 steps means trying to squeeze past agents\nand narrow lanes when it is very hard to. This leads to a\nhigher collision and off-road rate compared to scenes with\ncloser goals. This can also compound difficulty in scenes\nwith the aforementioned properties."}, {"title": "4.3. Extrapolative generalization and fast fine-tuning", "content": "Beyond generalization to within distribution scenarios, as\nreported in Section 4.1, we are interested in agent perfor-\nmance in out-of-distribution events. This is useful to know\nas researchers may typically manipulate scenarios or make\nthem harder in some way to test the limits of AV systems.\nWhere do these agents break, and how easily can they be\nfinetuned? Driving backward, or navigating to goals behind\nagents is one such behavior that is rarely observed in the\ndata. To quantify this, we analyzed the full training dataset\n(\u2248 129,000 scenes) or about 4.2 million controllable agents.\nOf these, we found approximately 30,000 agents (0.73%)\nmaking a U-turn, and 47,000 agents (1.13%) driving in re-\nverse (see Appendix C.1 for the exact definition of these\nevents). Further, most agents driving in reverse were simply\npulling out of park, with goals immediately behind them,\nWe observed a distinct lack of goals where the agent needs\nto execute a complex U-turn, making it plausibly out of dis-\ntribution. We then hand-designed 13 scenarios from the test\ndataset with a total of 27 agents across all scenes, placing\ngoals behind agents. This was done by setting the new goal\nfor each agent to $(x_f - x_i, y_f - y_i)$, where $(x_i, y_i)$ is the\ninitial position, and $(x_f, y_f)$ is the original goal. We chose\nthe scenes in such a way that doing this process for all con-"}, {"title": "4.3.2. FAST FINETUNING", "content": "As a proof of concept, we demonstrate how self-play re-\ninforcement learning enables rapid fine-tuning of a model\nto learn new behaviors, such as navigating backward, us-\ning only a few samples. Figure 7 provides an overview of\nour approach. Initially, introducing an out-of-distribution\nscenario-where goals are positioned behind agents-leads\nto a drop in performance (12). To address this, we\ntake the 13 hand-designed scenarios and fine-tune the policy\nthat was pre-trained on 10,000 WOMD scenarios (3). The\nmodel starts with a low goal-reaching rate but quickly adapts,\nachieving 100% success within 15 minutes of training. After\nfine-tuning, agents can reliably reach goals behind them (4)."}, {"title": "5. Discussion", "content": "Our results lead us to three main conclusions:\n1. Self-play at scale reliably achieves well-defined crite-\nria in unseen scenarios. Our findings suggest that self-\nplay RL scales effectively with available data (Section 4.1),\nachieving state-of-the-art performance on the Waymo Open\nMotion Dataset (WOMD) with no generalization gap. To\nthe best of our knowledge, this is the first demonstration of\nthis level of performance on WOMD. Compared to state-\nof-the-art supervised models, such as VBD and BehaviorGPT , our approach\nreduces collision and off-road rates by at least 15 x.\n2. Rare events remain a challenge. Agents struggle with\nrare or out-of-distribution scenarios, such as goals placed\nbehind them (Section 4.2) or navigating roundabouts. In\nthese cases, performance drops significantly, indicating that\nperformance on uncommon situations remains a key limita-\ntion.\n3. Fine-tuning quickly improves performance in unseen\nscenarios. Fine-tuning on a small subset of hand-designed\ncases can improve agent performance. In our experiments,\nfine-tuning a pre-trained model for just a few minutes en-\nables agents to achieve near-perfect goal-reaching rates on\npreviously difficult tasks (Section 4.3)."}, {"title": "5.1. Limitations and open questions", "content": "Our results represent a small step towards more reliable sim\nagents. We highlight three limitations of our work.\n1. Are these agents reliable enough? Despite achieving\nnear-perfect performance in many cases, failures still occur\nin 8% of scenarios (862 out of 10,000), even if the fraction\nof unintended behaviors per scene is tiny. This falls short of\nthe reliability needed for fully automated AV pipelines. A\nkey open question is how to further improve within-scene\nreliability to meet the high standards of automated pipelines.\n2. Limited agent diversity and horizon. Our benchmark,\nbuild atop the Waymo Open Motion Dataset, consists of\nshort-horizon scenarios that are only 9 seconds long. Fur-\nthermore, we excluded pedestrians, cyclists, and traffic lights.\nExpanding the scope of evaluation to include longer scenar-\nios with several types of road users is an interesting direction\nfor future work.\n3. Reliable and human-like. Our agents are trained to\noptimize performance over given criteria above maximizing\nhuman likeness, making it unclear how closely they resemble\nreal road users. A key direction for future work is balancing\nreliability with realism, ensuring agents not only meet perfor-\nmance standards but also accurately reflect human driving\nbehavior across diverse scenarios."}, {"title": "A. Observation features and design choices", "content": "The observation at time step t for agent i, o, is multi-modal and consists of three types of information: the ego state, the\nvisible view of the scene, and the partner observation. We set the maximum number of agents per scenario throughout the\nexperiments, N = 64. We limit agents to vehicles. A given agent's observation is provided as a flattened vector of ~ 3000\nelements."}, {"title": "B. Considerations for learning sim agents through self-play PPO", "content": "GPUDrive supports three types of collision behaviors: ignore, remove, and stop. Each of these has different effects on the\ntypes of behaviors agents learn over time. We briefly outline some things to be aware of below, which might be useful for\nfuture experiments.\nIgnoring collisions When collision behavior is ignored, the agent is not terminated when it collides with another agent or\ntouches a road edge. As such, it can proceed to the goal and collide within a single episode. To discourage collisions, it\nseems reasonable to give agents a penalty. However, since, in most scenarios, the probability of getting negative signals in\nan episode with random behavior (e.g. hitting a road edge) is significantly larger than the probability of receiving a positive\nsignal (getting to the goal), the value function may become overly pessimistic because the majority of the advantages the\nagent is receiving will be negative, and as such the probability of actions that lead to these negative advantages, such as\nhigher acceleration, will be decreased. This can lead to a behavior where agents freeze (they learn to stay on the road) and do\nnot head towards the goal. This can be avoided by ensuring that agents receive enough positive signals along with negative\nones, especially early on during learning. This can be achieved by sufficient exploration through a large enough entropy\ncoefficient.\nRemoving agents at collision Another option is to simply terminate agents whenever they do something that is not desired\n(in our case colliding) without assigning penalties (giving negative rewards). This means that the goal can only be achieved\nif the agent does not do something bad. Since the penalty in this case is implicit, the value function can not become overly\npessimistic. Instead, the advantages will be 0 most of the time early on in training. Once the first positive signals are achieved\nby accident (which is inevitable given the small maps of the WOMD and a high enough entropy coefficient), the probability\nof the right action sequences will be increased until all agents hit their goals without colliding or going off-road."}, {"title": "C. Analyses.", "content": "1. U-turn: For each time step t where the agent is valid, we check the condition: abs(heading[t] - heading[initial]) > 150\u00b0.\n2. Driving in reverse: For each time step t where the agent is valid, calculate the direction of its velocity vector and\nsubtract it from its heading angle. If the absolute difference is greater than a threshold (150\u00b0), it is driving in reverse.\nNote: We only detect driving in reverse if it occurs for more than a threshold (10) consecutive steps, and above a\nminimum magnitude velocity (0.5 km/hr)."}, {"title": "C.2. Effect of initialization modes", "content": "report different initialization modes and trivial agents"}, {"title": "D. PPO implementation details.", "content": "Table 7 reports the hyperparameters used for the results in our experiments."}, {"title": "E. Compute resources", "content": "Experiments were run on either a single NVIDIA A100 or an RTX4080 device for 12-36 hours per experiment. Including\nhyperparameter tuning and experimentation, all runs combined for this paper took approximately 5 GPU days."}]}