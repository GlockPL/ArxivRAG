{"title": "Mars-PO: Multi-Agent Reasoning System Preference Optimization", "authors": ["Xiaoxuan Lou", "Chaojie Wang", "Bo An"], "abstract": "Mathematical reasoning is a fundamental capability for large language models (LLMs), yet achieving high performance in this domain remains a significant challenge. The auto-regressive generation process often makes LLMs susceptible to errors, hallucinations, and inconsistencies, particularly during multi-step reasoning. In this paper, we propose Mars-PO, a novel framework to improve the mathematical reasoning capabilities of LLMs through a multi-agent system. It combines high-quality outputs from multiple agents into a hybrid positive sample set and pairs them with agent-specific negative samples to construct robust preference pairs for training. By aligning agents with shared positive samples while addressing individual weaknesses, Mars-PO achieves substantial performance improvements on mathematical reasoning benchmarks. For example, it increases the accuracy on the MATH benchmark of the state-of-the-art instruction-tuned LLM, Llama3.1-8B-Instruct, from 50.38% to 57.82%. Experimental results further demonstrate that our method consistently outperforms other baselines, such as supervised fine-tuning, vanilla DPO, and its enhanced versions, highlighting the effectiveness of our approach.", "sections": [{"title": "1 Introduction", "content": "Mathematical reasoning is a critical yet highly challenging task for large language models (LLMs) (Yu et al., 2023; Lu et al., 2024a; Luo et al., 2023; Wang et al., 2023a; Shao et al., 2024; Lu et al., 2024b; Lai et al., 2024). It requires not only strong foundational knowledge in mathematics but also the ability to perform precise computations (Yu et al., 2023; Touvron et al., 2023), logical reasoning (Lu et al., 2024a; Pang et al., 2024), and multi-step problem-solving (Lu et al., 2024b; Lai et al., 2024).\nDespite significant advancements in the capabilities of LLMs, achieving robust and reliable mathematical reasoning remains an open challenge. A primary factor contributing to this difficulty is the alignment of model-generated outputs with human preferences for correctness and clarity, particularly in complex domains like mathematical reasoning.\nAmong various alignment techniques, Direct Preference Optimization (DPO) (Rafailov et al., 2024) has emerged as a promising method for improving model behavior through preference-based training. It optimizes LLMs by leveraging preference signals derived from human or reward model judgments, directly adjusting the model's output distribution. DPO methods have demonstrated strong performance on general chat benchmarks, but their application to standard reasoning tasks often yields only moderate improvements or even performance degradation (Pang et al., 2024; Lu et al., 2024b; Lai et al., 2024). Moreover, while DPO has achieved notable success in aligning single-agent systems, it frequently falls short in leveraging the collaborative potential of multi-agent systems. In such systems, diverse agents can contribute complementary strengths, which, if effectively utilized, could lead to the generation of higher-quality solutions.\nTo address these limitations, we propose a novel approach to achieve multi-agent reasoning system preference optimization, named as Mars-PO. This method extends the standard DPO framework to a multi-agent setting, leveraging the collective capabilities of multiple agents to improve alignment and reasoning performance. Figure 1 shows the framework of Mars-PO, which operates in three stages:\n(i) Response Samples Generation: Given a set of prompts, response samples are generated by multiple agents. These responses form the foundation for constructing positive and negative samples. By utilizing diverse agents, this process ensures that"}, {"title": "3 Methodology", "content": "Given a multi-agent system comprising multiple pretrained or instruction-tuned large language models, with access to a set of training inputs and the ability to evaluate the correctness of final outputs, our goal is to simultaneously enhance the performance of all agents within the system. To achieve the goal, we propose Hybrid Preference Optimization, which consists of three steps: (i) Response Samples Generation, (ii) Preference Pairs Construction and (iii) Hybrid Preference Optimization, as shown in Figure 1. We provide additional details about the methodology design in the following."}, {"title": "3.1 Response Samples Generation", "content": "The generation of response samples lays the foundation for constructing high-quality preference pairs. In the context of a multi-agent system, this process involves coordinated sampling from multiple agents to ensure diverse and representative outputs for subsequent preference optimization.\nWe assume the target multi-agent system is composed of K agents, denoted as {\ud835\udc401, \ud835\udc402, ..., \ud835\udc40\ud835\udc3e}. The used training dataset is denoted as D = {(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc56)}, where \ud835\udc65\ud835\udc56 is the question prompt and \ud835\udc66\ud835\udc56 is the corresponding correct response. Given the substantial performance improvements achieved by the CoT framework, mathematical reasoning tasks often utilize CoT reasoning steps \ud835\udc50\ud835\udc56 to derive the final answer \ud835\udc4e\ud835\udc56. Hence, the target response \ud835\udc66\ud835\udc56 can be expressed as a concatenation of \ud835\udc50\ud835\udc56 and \ud835\udc4e\ud835\udc56, i.e., \ud835\udc66\ud835\udc56 = (\ud835\udc50\ud835\udc56, \ud835\udc4e\ud835\udc56). For each LLM agent \ud835\udc40\ud835\udc58"}, {"title": "3.2 Preference Pairs Construction", "content": "After identifying positive/negative sample sets of the multi-agent system, the next step is to construct preference pairs, a process that represents the most critical step in DPO-like methods. These pairs serve as the foundation for training reward-aligned language models. In our Mars-PO framework, we extend this process to incorporate multi-agent interactions, enabling the construction of a hybrid preference dataset that effectively leverages the complementary strengths of multiple agents. Specifically, we utilize a hybrid positive sample set combined with multiple agent-specific negative sample sets for the subsequent DPO training.\nThe hybrid positive sample set Gw is extracted from the outputs of all LLM agents, i.e., Gu for 1 \u2264 k \u2264 K. These agents generate candidate solutions for a shared set of mathematical reasoning tasks. A reward model, pre-trained to evaluate solution quality, assigns a reward score to each candidate. The highest-scoring outputs across all agents are merged into the hybrid positive sample set. This merging process ensures that the positive samples represent the best-performing solutions, irrespective of the agent of origin, thereby increasing the diversity and quality of the training data.\nUnlike the hybrid positive set, these negative samples are agent-specific, reflecting each agent's unique failure modes. By pairing hybrid positive samples with negative samples tailored to individual agents, the constructed preference pairs expose the limitations of each agent while reinforcing the benefits of the shared positive solutions. This step is instrumental in aligning LLM agents to achieve superior performance on mathematical reasoning tasks, as validated by our experimental results."}, {"title": "3.3 Hybrid Preference Optimization", "content": "As the core component of our Mars-PO framework, hybrid preference optimization applies DPO method to each agent using a combination of a hybrid positive sample set \ud835\udc3a\ud835\udc64 and agent-specific negative sample sets \ud835\udc3a\ud835\udc58. This process leverages the strengths of multi-agent collaboration to enhance the reasoning capabilities of each individual agent.\nThe loss function used for optimizing parameters of each agent is expressed as \ud835\udc3f = \ud835\udc3f\ud835\udc37\ud835\udc43\ud835\udc42 + \ud835\udefd\ud835\udc3f\ud835\udc41\ud835\udc3f\ud835\udc3f, where \ud835\udc3f\ud835\udc37\ud835\udc43\ud835\udc42 and \ud835\udc3f\ud835\udc41\ud835\udc3f\ud835\udc3f can be expressed as:\n\ud835\udc3f\ud835\udc37\ud835\udc43\ud835\udc42 = \u2212log[\ud835\udf0e(log \ud835\udc40\ud835\udf03(\ud835\udc50, \ud835\udc4e\ud835\udc64|\ud835\udc65\ud835\udc56))\nMk(c, axi))\n\u2212log(\ud835\udc40\ud835\udf03(\ud835\udc50, \ud835\udc4e\ud835\udc59|\ud835\udc65\ud835\udc56)\nMk(c, axi))]\n\ud835\udc3f\ud835\udc41\ud835\udc3f\ud835\udc3f =\nlog\ud835\udc40\ud835\udf03(\ud835\udc50, \ud835\udc4e|\ud835\udc65\ud835\udc56)\n|\ud835\udc50\ud835\udc65| + |\ud835\udc66|\nThe Negative Log-likelihood Loss (NLL) is added to maintain base knowledge of the original agent model and prevent overfitting to preferences. To further enhance agent performance, we adopt an iterative training method to repeatedly update the parameters of the target LLMs."}, {"title": "4 Experimental Setup", "content": "In this section, we first present the evaluation LLM agents in the multi-agent system, which are also targets whose performance we aim to improve. Besides, we also introduce the used reward model for extracting high-quality positive samples and mathematical datasets used for reasoning tasks. Finally, we detail compared baselines used to highlight the advancement of our method."}, {"title": "4.1 Evaluation Models", "content": "We evaluate the performance of Mars-PO on the multi-agent system consisting of three state-of-the-art instruction-tuned mathematical LLMs, including Qwen2.5-Math-Instruct (Yang et al., 2024) (with 7B parameters), Llama3.1-Instruct (Touvron"}, {"title": "4.2 Reasoning Datasets", "content": "Following previous research (Lu et al., 2024b; Lai et al., 2024; Yu et al., 2023; Shao et al., 2024), our evaluation performs on two mathematical reasoning datasets: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). Both are classic benchmarks specifically designed to evaluate the arithmetic and word problem-solving capabilities of language models. They consist of challenging mathematical problems accompanied by well-structured reasoning steps leading to the correct answers."}, {"title": "4.3 Compared Baselines", "content": "We compare the performance of Mars-PO with four baselines: (i) original performance of the target instruction-tuned LLM agent, (ii) vanilla DPO method (Rafailov et al., 2024), where each LLM agent uses their own preference pairs for post-training; (iii) DPO method combined with NLL item, which has been studied in previous works (Pang et al., 2024) and can slightly improve reasoning capability of the target model; (iv) Supervised Fine Tuning (SFT) with extracted positive samples, which aims to investigate whether incorporating contrastive samples contributes to performance improvement."}, {"title": "4.4 Implementation Details", "content": "To sample responses from each agent, we follow previous works (Lu et al., 2024b; Lai et al., 2024; Pang et al., 2024; Yu et al., 2023) to use a zero-shot prompt that includes the question along with clear instructions to generate a chain-of-thought reasoning process. Ensure the response follows a specific format, making the final answer easy to identify and extract. We conduct three iterations of preference optimization to fully unlock the potential of LLM agents. In each iteration, we generate N solutions (N = 40 for GSM8K and N = 30 for MATH) for each problem using sampling with temperature 0.8 for iterations 1 and temperature 1.2 for iterations 2-3, hoping for a substantial number of incorrect generations in the later iterations.\nThe generated response samples are further processed to construct a hybrid positive sample set, extracted by the reward model, along with negative sample sets for each agent. Subsequently, we select 15 preference pairs from these sample sets for the following Mars-PO training. The post-training of the target agent model is conducted over three epochs, with a batch size of 16 and a learning rate of 7e-7, using the AdamW optimizer. The coefficient \ud835\udefc and \ud835\udefd are set as 1 and 0.1, respectively. Note that for iteration 2 and 3, \ud835\udefd is increased to 0.2 and 0.4, to further amplify the differences in the reward values of preference pairs.\nAll training is done using one node with eight A800 GPUs (80G memory)."}, {"title": "5 Evaluation Results", "content": "In this section, we first present the main results of Mars-PO in improving the performance of the multi-agent system, highlighting the advantages of our approach. We then compare it with various baselines to demonstrate the impact of the techniques incorporated into Mars-PO. Our experiments show that each of the introduced techniques contributes to a significant improvement in the performance of LLM agents."}, {"title": "5.1 Main Results", "content": "Table 1 displays the prediction accuracy of LLM agents in the multi-agent system on GSM8K and MATH tasks. Note that these are the results after the first iteration of training. From the table, we can see the comparison between our method with four baselines. Experiment results demonstrate that Mars-PO consistently achieves higher accuracy across all baselines. Notably, our method even leads to a performance improvement of over 10% on the Llama model, increasing the prediction accuracy on the challenging MATH dataset from 50.38% to 55.48%.\nWhile conventional DPO results in a significant decline in model performance, even with the addition of a negative log-likelihood loss to partially alleviate the issue, it remains evident that vanilla DPO and its variants fail to further enhance the performance of state-of-the-art models. The reason behind this phenomenon could be that these models have already undergone extensive fine-tuning"}, {"title": "5.2 Enhancement with iterative training", "content": "To further improve model performance, we adopt an iterative training approach that progressively refines the model's reasoning capabilities. Iterative training involves multiple rounds of preference optimization, where each iteration builds upon the outputs and refinements from the previous round. This process allows the model to continually improve its understanding and alignment with high-quality reasoning patterns.\nTable 1 presents the prediction accuracy of the models across three iterations of training. As shown in the results, the accuracy generally increases with each iteration, demonstrating the effectiveness of iterative training in improving model performance. However, it is worth noting that in some cases, there is a slight drop in accuracy between certain iterations. Despite these minor fluctuations, the overall trend indicates a consistent upward trajectory in the model's performance, highlighting the benefits of continued optimization through iterative training."}, {"title": "5.3 Effect of hybrid positive samples", "content": "To evaluate the impact of hybrid positive samples, we analyze their contribution to the overall model performance. Given hybrid positive samples are constructed by merging high-quality correct outputs from multiple agents, they can combine diverse strengths of all agents to create a unified and robust dataset. Hence, this approach is able to provide a richer and more comprehensive training signal compared to relying on positive samples from a single agent.\nThe comparison between the vanilla DPO method and our proposed Mars-PO demonstrates"}, {"title": "6 Conclusion", "content": "In this paper, we proposed Hybrid Direct Preference Optimization (Mars-PO), a multi-agent framework to enhance the mathematical reasoning capabilities of large language models (LLMs). By combining a hybrid positive sample set with agent-specific negative samples, Mars-PO effectively leverages multi-agent collaboration to construct robust preference pairs for training. Experimental results demonstrate that this approach significantly improves LLM performance on mathematical reasoning benchmarks, showcasing the potential of hybrid preference optimization for complex reasoning tasks."}]}