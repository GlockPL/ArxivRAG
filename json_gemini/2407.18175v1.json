{"title": "Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers", "authors": ["Zhengang Li", "Alec Lu", "Yanyue Xie", "Zhenglun Kong", "Mengshu Sun", "Hao Tang", "Zhong Jia Xue", "Peiyan Dong", "Caiwen Ding", "Yanzhi Wang", "Xue Lin", "Zhenman Fang"], "abstract": "Vision transformers (ViTs) have demonstrated their superior accuracy for computer vision tasks compared to convolutional neural networks (CNNs). However, ViT models are often computation-intensive for efficient deployment on resource-limited edge devices. This work proposes Quasar-ViT, a hardware-oriented quantization-aware architecture search framework for ViTs, to design efficient ViT models for hardware implementation while preserving the accuracy. First, Quasar-ViT trains a supernet using our row-wise flexible mixed-precision quantization scheme, mixed-precision weight entanglement, and supernet layer scaling techniques. Then, it applies an efficient hardware-oriented search algorithm, integrated with hardware latency and resource modeling, to determine a series of optimal subnets from supernet under different inference latency targets. Finally, we propose a series of model-adaptive designs on the FPGA platform to support the architecture search and mitigate the gap between the theoretical computation reduction and the practical inference speedup. Our searched models achieve 101.5, 159.6, and 251.6 frames-per-second (FPS) inference speed on the AMD/Xilinx ZCU102 FPGA with 80.4%, 78.6%, and 74.9% top-1 accuracy, respectively, for the ImageNet dataset, consistently outperforming prior works.", "sections": [{"title": "1 INTRODUCTION", "content": "ViTs [10, 35, 42, 62] incorporate the attention mechanism [46] to fulfill various computer vision tasks, by allowing all the pixels in an image to interact through transformer encoder blocks and thus achieving higher accuracy compared to CNNs. Despite ViTs' significant accuracy improvement, it is non-trivial to deploy ViT inference on resource-limited edge devices due to their huge model size and complex architectures. For example, even the lightweight ViT model DeiT-S [42] has a model size of 22.10M parameters \u00d7 4Bytes per floating-point parameter = 88.4MB, presenting an overwhelming computing load and memory size for most edge devices.\nThe basic transformer encoder with multi-headed self-attention (MSA) and multi-layer perceptron (MLP) blocks is shown in Figure 1, consisting of multiple different computation components, including linear layer, attention, residual addition, matrix reshape operation, GELU, and layer norm. To further understand the bottleneck of the current ViT model structure, we profile the runtime of each component of ViT on a Xeon(R) Silver 4214 CPU [16] using Pytorch Profiler [33] as shown in Figure 1. We use the same color to indicate the same component in both the transformer block structure and profiling figures. It shows matrix multiplication operations dominate the processing time (94.7% and 87.3% for DeiT-B [42] and DeiT-S [31], respectively) of execution cycles.\nContemporary acceleration methods mainly focus on reducing the practical inference latency of matrix multiplication operations. They primarily fall into two categories: 1) neural architecture search (NAS) that searches the lighter-weight model; and 2) model compression, especially model quantization that reduces the per-parameter bit-width. However, there are two major challenges when applying these methods on hardware. The first challenge is associated with model quantization. It has been revealed that the most suitable quantization schemes/bit-widths depend on model sizes and architectures [49, 54], and there is a vast design space in the quantization of both weights and activations for each layer on different models and hardware. As ViT models become deeper, the design space increases exponentially, resulting in poor performance of rule-based strategies. Although recent studies explored automated quantization techniques for a given ViT architecture [45, 49, 54], they did not integrate model quantization with NAS together, which could result in suboptimal performance. In this paper, we propose the framework of model quantization and NAS co-design for ViTs towards improved performance compared to treating NAS and quantization separately.\nThe second challenge is the gap between the theoretical computation throughput and the practical inference speed on actual hardware. For example, layer-wise (inter-layer) mixed-precision quantization (for CNNs) [49, 54] quantizes each layer with a different bit-width and therefore executes layers through distinct hardware components sequentially, leading to low resource utilization. Furthermore, kernel-wise mixed-precision quantization (for CNNs) [29] assigns different bit-widths down to the kernel level, significantly diversifying the computing pattern and is inefficient for hardware implementation.\nRecent work FILM-QNN [40] and Auto-ViT-Acc [22] leverage the intra-layer mixed quantization to achieve good performance for both model accuracy and throughput on FPGA. By applying two different quantization bit-widths/schemes for different channels and limiting the same mixed-precision ratio across each layer, FPGA can efficiently handle different computations on different hardware resources sharing the same hardware design. However, existing approaches suffer from a manually configured uniform mixed-precision ratio across all layers, potentially compromising quantized model accuracy. Moreover, architectural design considerations are often neglected, limiting the overall model performance.\nTo address these problems comprehensively, we propose Quasar-ViT, an integration of a hardware-oriented quantization-aware architecture search targeting ViT. First, to fully unleash the computation potential of FPGA resources, we investigate a hardware-friendly row-wise mixed-precision quantization scheme. At the algorithm level, different from FILM-QNN [40] and Auto-ViT-Acc [22], we quantize different channels within each layer into lower and higher bit-widths with the flexibility of different mix-ratios for layers, which achieves a more fine-grained architecture to maintain the accuracy. At the hardware level, we propose the FPGA-based model-adaptive design, including 4-bit atomic computation and hybrid signed/unsigned DSP packing, which set basic hardware units for the lower-bit computation, and decompose the higher-bit computation to lower-bit ones to reuse the resources. Second, during the supernet training, we propose the mixed-precision weight entanglement mechanism, such that different transformer blocks in subnets can share weights for their common parts in each layer to enable efficient quantization during architecture search and reduce training memory cost. On top of that, we establish the corresponding FPGA latency and resource modeling to estimate the inference latency and combine it with an efficient hardware-oriented evolution search method. Based on the above, we integrate with the one-shot NAS algorithm to efficiently find the most accurate quantized model under the given inference latency. We also explore the layer scaling in CaiT [43] and extend it to the supernet architecture to improve the training efficiency and model accuracy. To demonstrate the compatibility of our proposed framework with knowledge distillation (KD) and further improve our searched model accuracy, we integrate KD [15] into the training process. Finally, on the hardware side, we implement the basic computing units for 4-bit weight and 6-bit activations with hybrid signed/unsigned DSP packing optimization to enable efficient FPGA implementation.\nThe contributions of our work are summarised as follows:\n\u2022 An end-to-end hardware-oriented quantization-aware architecture search framework (Quasar-ViT) for ViTs, achieving superior accuracy and inference speed over prior studies. Latency/resource modeling of the hardware accelerator design is integrated into the search process.\n\u2022 Hardware-friendly quantization techniques-such as flexible row-wise mixed-precision quantization and mixed-precision weight entanglement-in the architecture search, towards high accuracy, low training cost, and efficient implementation."}, {"title": "2 RELATED WORK", "content": "First proposed in [10], the vision transformer (ViT) is a groundbreaking work that uses transformer blocks for vision tasks. Unlike traditional CNN architectures that use a fixed-size window with restricted spatial interactions, ViT interprets an image as a sequence of patches and adopts the self-attention mechanism [46]. This allows all the positions in an image to interact through transformer blocks, which provides the extraordinary capability to capture relations at the pixel level in both spatial and temporal domains. However, the original ViT requires pre-training with large-scale datasets such as ImageNet-21k and JFT-300M. To tackle the problem, many variants such as DeiT [42] and T2T-ViT [62] were proposed, which can be well trained with only ImageNet-10k. ViTs improve model accuracy at the cost of increased volume of computation and structural complexity. In ViTs, the main model architecture is transformer encoder blocks with multi-headed self-attention (MSA) and multi-layer perceptron (MLP) blocks. These blocks involve large matrix multiplications, which incur the most computational cost. These complex architectures and enormous computation/storage demand make it hard to deploy ViTs on resource-limited edge devices.\nTherefore, we quantize all layers involved in matrix multiplication, but not the non-linear functions, e.g., layer normalization, due to their low computational cost and potential effects on accuracy.\nTo compress model size and improve inference speed, model quantization has been widely explored for deep neural networks (DNNs). Existing quantization research can be categorized according to quantization schemes, such as binary [8, 36], ternary [13], and low-bit-width fixed-point [7, 7, 68, 68] quantize models with the same interval between each quantization level. Although binary and ternary quantization reduce operations and simplify hardware implementation to the extreme, they introduce large accuracy loss due to insufficient bit-width. For example, based on reports from the above works, accuracy typically degrades by > 5% under binary quantization and 2 - 3% for ternary quantization. To overcome the large accuracy loss coming from insufficient\nbit-width, the fixed-point quantization is proposed, applying moderate and adjustable quantization bit-width, to maintain accuracy. This quantization scheme was implemented with different methods and algorithms, such as DoReFa-Net [68] and PACT [7].\nFinally, there are also non-linear quantization schemes, such as power-of-two (PoT) [17] and additive PoT [19]. They replace the multiplication with shifting operations where the distribution of quantization levels becomes unbalanced, having higher precision around the mean and less precision at the two sides.\nTo explore more quantization potential while preserving the model accuracy, Besides the single scheme quantization, some works [9, 39, 45, 49, 54] explore inter-layer mixed-precision quantization by assigning different precisions to layers. For example, HAQ [49] determines the bit-width of each layer by an agent trained with reinforcement learning. DNAS [54] used NAS to search layer-wise bit-width. Furthermore, [29] explored intra-layer mixed quantization to enable different precisions or schemes within each layer. Based on them, hardware designs [5, 40] leveraged the intra-layer mixed-precision/mixed-scheme to enable uniformity within each layer, guaranteeing inference acceleration. However, they need to set the same mixed ratio for layers, which limits the model's accuracy."}, {"title": "2.3 Transformer and ViT Quantization", "content": "Quantization has also been studied for transformers, especially for natural language processing (NLP) tasks [1, 64, 65]. Q8BERT [64] finetuned BERT through 8-bit quantization-aware training. Ternary-BERT [65] implemented an approximation-based and loss-aware ternary quantization on BERT. BinaryBERT [1] proposed a ternary weight splitting strategy to derive binary BERT with performance as the ternary one. Inspired by those, [27] and [22] studied quantization on ViT in computer vision tasks. PTQ [27] evaluated the post-training quantization on ViT and achieved comparable accuracy to the full-precision version. Auto-ViT-acc [22] proposed an FPGA-aware framework with mixed-scheme quantization for ViT, which we will compare in the evaluation. FQ-ViT [24] proposed power-of-two factor and log-int-softmax to proceed with the ViT quantization. Q-ViT [20] used the switchable scale to achieve head-wise ViT mixed quantization. However, these works are all based on full-precision pre-trained models and do not include the dimension of network architecture search."}, {"title": "2.4 Neural Architecture Search", "content": "There has been a trend to design efficient DNNs with NAS. In general, NAS can be classified into the following categories according to its search strategy. First, reinforcement learning (RL) methods [2, 4, 25, 32, 67, 69, 70] use recurrent neural networks as predictors validating the accuracy of child networks over a proxy dataset. Second, evolution methods [30, 37] develop a pipeline of parent initialization, population updating, generation, and elimination of offspring to find desired networks. Third, one-shot NAS [3, 11, 60] trains a large one-shot model containing all operations and shares the weight parameters with all candidate models. Based on the above work, weight-sharing NAS has become popular due to training efficiency [38, 47, 61]. One over-parameterized supernet is trained with weights shared across all\nsub-networks in the search space. This significantly reduces the computational cost during the search. Although most of the above work focuses on the traditional CNN architectures, such as [61] and [38], some works have started investigating the search for efficient ViT networks [6, 18, 48, 55]. Among them, Autoformer [6] entangles the model weights of different ViT blocks in the same layer during supernet training with an efficient weight-sharing strategy to reduce training model storage consumption as well as training time.\nSome recent works realize the gap between theoretical computation improvement and practical inference speedup. They investigate the algorithm/hardware co-design and incorporate the inference latency into NAS [23, 41, 53], which is more accurate than intuitive volume estimation by MAC operations. For example, MnasNet [41] and NPAS [23] utilize the latency on mobile devices as the reward to perform RL search, where gradient-based NAS work FBNet [53] adds a latency term to the loss function. However, these works neither target ViTs nor exploit quantization in the hardware-aware ViT search."}, {"title": "3 HARDWARE-ORIENTED QUANTIZATION-AWARE NAS FOR VITS", "content": "Model-wise quantization [7, 68] uses a unified quantization bit-width for the whole model and thus misses some quantization opportunities. On the other hand, mixed-precision quantization, as discussed in related work, explores more quantization potential (i.e., quantizing each component to a bit-width as low as possible) while preserving the accuracy of the model. Specifically, layer-wise (inter-layer) mixed-precision quantization [49, 54] sets each layer with a specific quantization bit-width. Besides that, Q-ViT [20] proposed a head-wise mixed-precision quantization scheme, which assigns different bit-widths to different attention heads. Both the layer-wise and head-wise quantization schemes suffer from limited quantization flexibility without considering the variance inside each layer. Moreover, fixed row-wise (intra-layer) mixed-precision quantization is proposed in prior work [40], which uses different quantization bit-widths for different channels in each CNN layer and limits the same mixed-precision ratio across different CNN layers, and thus multiple layers can share the same hardware design, making it more hardware-friendly. Finally, kernel-wise mixed-precision quantization [29] assigns different quantization bit-widths down to the kernel level, which greatly diversifies the computing pattern and makes it inefficient to implement on hardware.\nBased on the above discussion, we use the row-wise flexible mixed-precision quantization scheme for ViTs, as shown in Figure 2(d), which preserves the quantization flexibility among layers for better accuracy while maintaining the hardware uniformity for more efficient implementation. Different from [40] that limits the same mixed-precision ratio across CNN layers, for ViTs, we have to provide the flexibility to obtain different mixed ratios in different layers to maintain the model accuracy. To maintain hardware uniformity and avoid hardware under-utilization, we propose to design the basic hardware units for the lower-bit computation, decompose the higher-bit computation into lower-bit ones, and reuse the basic hardware units (described in Section 4.2 and Section 4.3). As a result, we have preserved the uniformity of the hardware design and enabled the flexible bit-width mixed-ratio among ViT layers. We explain the hardware details in Section 4."}, {"title": "3.2 Intra-layer Mixed-Precision Weight Entanglement", "content": "In classical one-shot NAS, the weights of each sample candidate are shared with the supernet during training. However, as shown in Figure 3 (a), when using the classical weight-sharing strategy, the building blocks from multiple subnets, even in the same layer, are isolated. Therefore, it leads to higher memory costs and slower training convergence.\nTo address this problem, weight entanglement is proposed in [6] to reduce the supernet model size: as shown in Figure 3 (b), different transformer blocks can share their common weights in each layer. It also allows each block to be updated more times than the previous independent training strategy, thus achieving faster convergence. However, this structure is hard to combine with mixed quantization since one shared weight cannot be trained into two different bit-widths at the same time (i.e., bit-width conflict).\nIn this paper, we propose the mixed-precision weight entanglement, as shown in Figure 3 (c), to incorporate the quantization search while preventing the potential bit-width conflicts problem in the shared weight. Mixed-precision weight entanglement block"}, {"title": "3.3 Supernet Layer Scaling (SLS) Structure", "content": "The layer scaling structure proposed by CaiT [43] improves the stability of the optimizations when training transformers for image classification, thus improving the model accuracy. We explore this structure and extend it to supernet layer scaling (SLS).\nLayer scaling is a per-channel multiplication of the vector produced by each residual block. For ViT, this layer is deployed after the multi-head self-attention (MSA) and multi-layer perceptron (MLP) modules in each encoder block. The objective is to group the updates of the weights associated with the same output channel. Layer scaling can be denoted as a multiplication by a diagonal matrix diag (\u03bb\u03b9,1,..., \u03bb\u0131,d) on the output of l-th residual block, where d is the corresponding number of output channels in the model. All \u03bbl are learnable weights.\nTo fit our mixed-precision weight entanglement strategy, different from the original CaiT [43] implementation that uses the whole layer scaling in every training iteration, our SLS extracts the corresponding elements synchronized with the output dimension of the selected subnet while keeping the other weights frozen. As shown in Figure 4, using the residual block of MLP as an example, assuming that the current MLP's output dimension starts from m-th channel and ends at n-th channel, the supernet layer scaling computation can be formulated as:\n$y_1 = x_1 + diag (\\lambda_{1,m}, ..., \\lambda_{1,n}) \\times MLP (LN (x_1))$,\nwhere x and y denote the input and output, respectively; LN means the layer normalization."}, {"title": "3.4 End-to-End Quasar-ViT Framework", "content": "Our one-shot NAS algorithm consists of two steps:\n\u2022 We train a supernet to directly sample different quantized architectures as child models for training. The supernet is trained with SLS and KD techniques. The search space is encoded in the supernet, and the parameters of all candidate networks in the search space are optimized simultaneously by our proposed weight-sharing during training.\n\u2022 We select architectures from the pre-trained supernet using the hardware-oriented evolution search method to find the most accurate model under the given hardware resource constraints. We search based on the hardware latency/FPS and resource modeling illustrated in Section 4.4.\nHere, we show a toy example of the supernet training process including candidate sampling and the corresponding searched results for different targeting FPS in Figure 5. Figure 5 (a) illustrates one iteration of the supernet training process, where the pink area indicates the sampled high precision values and the blue area indicates the sampled low precision values in the supernet. The light blue area indicates the frozen values (currently not sampled) in this iteration. After the supernet training and the hardware-oriented evolution search, we could obtain different models targeting different frames per second (FPS) as shown in Figure 5 (b). For the\nsake of brevity, we only show the quantized value here. The scaling factor along with other related structures is omitted.\nWe show our search space design in Table 2. Our search components include the overall embedding dimension, the number of transformer layers, the quantization mixed-ratio (i.e., the percentage of 8-bit weights mixed in the layer) for each linear layer, and the hidden dimension and expansion ratio (for MLP) in each ViT encoder block.\nTo accelerate the supernet training process and improve the overall model performance, we partition the large-scale search space into two sub-spaces and encode them into two independent supernets for QUASAR-Small and QUASAR-Large, respectively. By splitting and customizing search space for supernets of different sizes, we mitigate the training interference caused by the huge subnets' difference. This training strategy has been proved in [66]. Such partition allows the search algorithm to concentrate on finding models within a specific hardware inference latency, which can be specialized by users according to their available resources and application requirements. It also reduces gradient conflicts between large and small sub-networks trained via weight-sharing due to gaps in model sizes.\nIn each iteration, we randomly select a quantized ViT architecture from the search space. Then we obtain its weights from the supernet and compute the losses of the subnet.\nFinally, we update the corresponding weights with the remaining weights frozen. The architecture search space P is encoded in a supernet denoted as S(P, Wp), where Wp is the weight of the supernet that is shared across all the candidate architectures. Algorithm 1 illustrates the training procedure of our supernet."}, {"title": "3.5 Hardware-Oriented Evolution Search", "content": "In our hardware-oriented evolution search for crossover, two random candidate architectures are first picked from the top candidates. Then we uniformly choose one block from them in each layer to generate a new architecture. For mutation, a candidate mutates its depth with probability $P_d$ first. Then it mutates each block with a probability of $P_m$ to produce a new architecture. Newly produced architectures that do not satisfy the constraints will not be added for the next generation. To evaluate the candidates, we perform hardware latency and resource modeling based on the proposed row-wise flexible mixed-precision quantization scheme. The details of the modeling have been discussed in Section 4.4."}, {"title": "3.6 Integration with Knowledge Distillation (KD)", "content": "To demonstrate the compatibility of our proposed framework with knowledge distillation (KD) and further improve the accuracy of our supernet, we also integrate KD [15] in our training process. We use the pre-trained RegNetY-32G [34] with 83.6% top-1 accuracy as different teacher models. We also apply the soft distillation method. Soft distillation [15] minimizes the Kullback-Leibler divergence between the softmax of the teacher and the softmax of the student model. The distillation loss is:\n$L_{soft} = (1 - \\alpha)L_{CE}(\\psi(Z_s), y) + \\alpha \\tau^2L_{KL}(\\frac{Z_t}{\\tau}, \\frac{Z_s}{\\tau})$,\nwhere Zt and Zs are the logits of the teacher and student models, respectively. \u03a8 is the softmax function. \u03c4 is the temperature for the distillation, \u03b1 is the coefficient balancing the Kullback-Leibler divergence loss ($L_{KL}$), and the cross-entropy ($L_{CE}$) on the ground truth labels y in the distillation."}, {"title": "4 FPGA HARDWARE DESIGN FOR QUASAR-VIT", "content": "Figure 6 presents the overall hardware architecture of the Quasar-ViT accelerator on the ARM-FPGA platform. Below is how each module in ViT is mapped to the hardware in Figure 6. The most time-consuming MSA and MLP modules are accelerated by our GEMM engine on the FPGA, which is similar to the recent Auto-ViT-Acc work [22]. The lightweight SLS modules right after MSA and MLP layers are also accelerated on the FPGA to avoid time-consuming execution on the ARM CPU. The less time-consuming modules including layer normalization and activation functions (i.e., Softmax or GELU) are executed on the ARM CPU, due to their complex structure for FPGA implementation. The hardware engines on the FPGA and software modules on the ARM CPU exchange data via the shared off-chip memory.\nAs previously mentioned, we mainly focus on the most time-consuming GEMM engine design. Due to the limited on-chip memory capacity and computing resource on the FPGA, for each ViT layer (i.e., MSA and MLP), our GEMM engine processes the input, weight, and output data in tiles: a small tile of the input (tokens) and weight of each ViT layer are first loaded from the off-chip DDR memory to the on-chip buffers, then they are processed by the GEMM engine all on-chip. To improve the performance, the double buffering technique is applied again to overlap the off-chip memory accesses and GEMM computation, shown in Figure 6.\nNext, we present our design of the basic hardware units in the GEMM engine and the corresponding DSP (digital signal processor) packing optimization, as well as the hardware resource and latency modeling for the tiled GEMM design."}, {"title": "4.2 Unification of Atomic Computation", "content": "One major challenge in the FPGA accelerator design is to efficiently support flexible mixed ratios of different bit-width computations across ViT layers. On one hand, putting multiple copies of hardware accelerator designs for each mixed-ratio (i.e., each layer) simultaneously on the FPGA leads to severe hardware resource contention and under-utilization, since layers are executed sequentially. On the other hand, pre-synthesizing multiple copies of hardware accelerator designs for each layer and reconfiguring the FPGA for each layer incurs significant FPGA reconfiguration overhead.\nInspired by the approach proposed in QGTC [52] to support arbitrary bit-width computation for quantized graph neural networks on GPUs, in our FPGA hardware design, we unify the basic processing elements to process 4-bit weight atomic computations and construct the 8-bit weight data computations using two 4-bit weight data operations as such: for multiplication between an N-bit activation value ($act_N$) and an 8-bit weight value ($wgt_8$), we derive the corresponding product as:\n$act_N \\cdot wgt_8 = act_N \\cdot wgt_{h4} << 4 + act_N \\cdot wgt_{l4}$,\nwhere $wgt_{h4}$ and $wgt_{l4}$ represent the higher and lower 4-bit data of $wgt_8$, respectively. The multiplication result between $act_N$ and $wgt_{h4}$ are left shifted by 4 bits.\nBased on this unification, we propose hybrid signed/unsigned DSP packing to handle the 4-bit weight atomic computation."}, {"title": "4.3 Proposed Hybrid Signed/Unsigned DSP Packing", "content": "To fully exploit the potential of DSP resources on FPGAs, we pack multiple low-bit multiplications within each DSP block following [57, 58]. Each DSP block (DSP48E2) on the AMD/Xilinx ZCU102 FPGA board could support the computation of $P=(A+D)\u00d7B$, where both A and D are 27-bit operands, B is an 18-bit operand, and P is the 45-bit output. In our study, we explore the following two DSP packing schemes and discuss their design trade-offs. The activation bit-width N is set to 6 to fully exploit the DSP for computation.\n\u2022 Packing factor 3 (3 weights sharing 1 activation). In Figure 7 (a), three $4 \u00d7 6$-bit multiplications are packed into a single DSP block, by holding one 6-bit signed activation in port B and three 4-bit weight values in port D. To pack three weights into a single 27-bit port D, look-up tables (LUTs) are utilized to first combine two weights and then integrate them with the third weight data. With this DSP packing scheme, for the W4A6 (i.e., 4-bit weight and 6-bit activation) computation, we could pack three 4-bit weights that share the same activation. And for the W8A6 computation, we could use two DSPs to process the upper and lower 4-bit\nof three 8-bit weights in parallel. Note that after the 8-bit weights are decomposed into two 4-bit weights, only the upper 4-bit weights contain the sign bits and should be sign-extended (required by DSP for output correctness [57, 58]), the lower 4-bit weights should be treated as unsigned values.\n\u2022 Packing factor 4 (2 weights sharing 2 activations). In Figure 7 (b), four $4 \u00d7 6$-bit multiplications are packed into a single DSP block, by holding two 6-bit signed activation values in port D and two 4-bit weights in port B. It is worth mentioning the port placement of the activation and weight values are swapped from the packing factor 3 scheme, to increase the packing strength. With this DSP packing scheme, for the W4A6 computation, we could pack a pair of two 4-bit weights that share the same pair of activation values. And for the W8A6 computation, a similar technique as the previous packing scheme is used to separately handle the upper and lower 4-bit values of an 8-bit weight. Again for the two decomposed 4-bit weights, only the upper 4-bit weights contain the sign bit and should be sign-extended (required by DSP for output correctness [57, 58]), but the lower 4-bit weights should be treated as unsigned values."}, {"title": "4.4 Hardware Resource and Latency Modeling", "content": "Here, we present our hardware resource and latency modeling used in the hardware-oriented evolution search.\nTo help guide the neural architecture search, we provide details of the resource and latency models of our FPGA accelerator design (mainly the GEMM engine). Table 3 lists the notations used in our models. We design our FPGA accelerator to fully leverage the available FPGA computing resources (i.e., DSPs and LUTs), on-chip memory (i.e., BRAMs), and off-chip memory bandwidth. To fully exploit the computing capability with the available hardware resources, We maximize the total number of parallel basic hardware compute units using both DSPs (i.e., $N_{dsp}$) and LUTs (i.e., $N_{lut}$) for the datapath of our accelerator as\n$N_{tot} = \\text{maximize} \\{ N_{dsp} + N_{lut} \\}$,\nwhile satisfying the following resource constraints\n$N_{dsp} \\cdot C_{dsp} \\leq S_{dsp} \\cdot Y_{dsp}$,\n$N_{lut} \\cdot C_{lut} + N_{dsp} \\cdot C_{dsp}^{lut} \\leq S_{lut} \\cdot Y_{lut}$,\nwhere constraints 5 and 6 bound the DSP and LUT utilization to be under the allowable thresholds, i.e., $Y_{dsp}$ and $Y_{lut}$ with the total resource amounts denoted by $S_{dsp}$ and $S_{lut}$. The hardware costs of each multiplication implementation are denoted as $C_{lut}$ and $C_{dsp}$.\nIn order to choose the best implementation method for the basic hardware units of our design, we characterize the FPGA resource consumption using Xilinx Vivado 2020.1 [59] for the three cases in Table 4, i.e., (a) multiplications executed on DSPs with packing factor of 3, (b) multiplications executed on DSPs with packing factor of 4, and (c) multiplications executed purely using LUTs. As observed in Table 4, we derive the hardware costs of each multiplication implementation, particularly, the LUT costs for pure-LUT-based and DSP-based methods correspond to $C_{lut}$ and $C_{dsp}^{lut}$. Note that the DSP-based approach also consumes LUTs, due to data packing on the input operands and output data construction operations, such as bit shifting and data accumulation. Regarding the efficiency lost by decomposing 8-bit to 4-bit, for the composed W8A6 computation, on average, we can achieve one computation with 25.8 LUTs and 0.5 DSP or purely 66.7 LUTs. In contrast, direct W8A6 computation used in [40] (i.e., packing two W8A6 operations within one DSP method) requires 21.5 LUTs and 0.5 DSP or purely 62.2 LUTs. Since most of the weights are in 4-bit, using decomposing does not affect the overall performance much by a slight increase in the LUT utilization. In terms of the efficiency of DSP packing, a single DSP can pack four W4A6 operations at most theoretically, which is achieved by our approach.\nFor the LUT usage, shown in Table 4, we have:\n$C_{lut}^{dsp,pack4}  < C_{lut}^{lut}$,\n$C_{lut}^{dsp,pack3} <  C_{lut}^{dsp,pack4}$,\nIn the final implementation, regarding which DSP packing scheme to use and whether to use the pure-LUT-based method for the basic hardware compute units, there are several situations according to the available FPGA resources.\nWhen $S_{lut}$ is limited and insufficient to hold the LUT consumption of full utilization of DSP packing with a factor of 3, denoted as:\n$S_{lut} \\cdot Y_{lut}  \\leq 3 \\cdot S_{dsp} \\cdot Y_{dsp} \\cdot C_{lut}^{dsp,pack3}$\nIn this case, to fully utilize the computation resources, we directly allocate DSP-based computations with a packing factor of 3 as much as possible until we reach the LUT resource limit."}, {"title": "4.4.2 Latency Modeling", "content": "We model the inference latency of our hardware design based on the number of clock cycles. For a layer i in ViT, since data packing can be used to transfer multiple (i.e., $D_{act}$ or $D_{wgt}$) values at the same time in each AXI port, the clock cycles for loading one input/weight tile and storing one output tile, are calculated as:\n$L_{in} = \\lceil \\frac{T_n}{D_{act}} \\rceil \\cdot \\lceil \\frac{T_m}{A_{in}} \\rceil$,\n$L_{wgt} = \\lceil \\frac{T_m}{D_{wgt}} \\rceil \\cdot \\lceil \\frac{F}{A_{wgt}} \\rceil$,\n$L_{out} = \\lceil \\frac{T_m}{D_{act}} \\rceil \\cdot \\lceil \\frac{F}{A_{out}} \\rceil$,\nwhere $L_{in}$, $L_{wgt}$ and $L_{out}$ indicate the number of the clock cycles of input, weight, and output transfer for one corresponding tile, respectively.\nThe clock cycle number to compute one tile is\n$L_{cmpt} = \\max \\{ \\frac{F \\cdot T_n \\cdot T_m}{PF \\cdot N_{tot}} \\}$,\nwhere the first term is calculated by the latency model. The total number of multiplications needed to compute one tile of matrix multiplication is $T_n T_m F$. Each tile has two levels of parallelism: one is along the $T_n$ and $T_m$ dimension and the parallel factor is $T_n T_m$ (i.e., fully parallel); and the other is along the F dimension and the parallel factor is PF. Therefore,```json\nHere, we show how the scaling factor along with other related structures is omitted."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "Our supernet training process takes 700 epochs with a batch size of 2048. The learning rate is set to 5 \u00d7 10-4 initially and decayed with a cosine annealing schedule. The AdamW [28] optimizer is used with the epsilon value of 1e-8 and weight decay of 0.05. Additional training optimizations, such as warmup and label smoothing are performed during training. The number of warmup epochs and label smoothing factor are set as 20 and 0.1, respectively. After supernet training, we perform the hardware-oriented evolution search, without subnet retraining.\nOur training is conducted on 8 NVIDIA Ampere A100 GPUs with CUDA 11.0 and PyTorch 1.7 frameworks on the Ubuntu operating system. To test the effectiveness of our framework, we also implement Quasar-ViT framework on the Xilinx ZCU102 embedded FPGA platform with quad-core ARM Cortex-A53 and XCZU9EG FPGA chip. The FPGA working frequency is set to 150 MHz for all the designs implemented via Xilinx Vitis and Vitis HLS 2020.1."}, {"title": "5.2 Accuracy Results", "content": "Here we analyze the accuracy results of our Quasar-ViT framework. The weight precision is mixed with 4-bit and 8-bit, as mentioned earlier, and the activation bit-width is determined by the hardware feature. Without loss of generality, we use activation of 8-bit to evaluate the accuracy in the ablation study of knowledge distillation and supernet layer scaling.\nTo evaluate the compatibility of knowledge distillation and our proposed SLS, we conduct an ablation study on both of them. Without loss of generality and to prevent interference from different model sizes and different quantization mixed ratios from the searched subnet, we unify the search constraint with pure W8A8 (8-bit for both weight and activation) quantization implementation. As shown in Table 6, we conduct four different settings of Quasar-ViT with the unified model size and quantization scheme. The accuracy of the four cases is 74.1% (w/o distillation and SLS), 75.6% (only distillation), 74.9% (only SLS), and 76.1% (with both of them), respectively. Knowledge distillation and SLS strategies are orthogonal to each other, and both improve the model accuracy. Using them together provides a better result. Given the observed effectiveness of our proposed SLS strategy and the seamless compatibility of our framework with knowledge distillation, we opt to incorporate both strategies in our following experiments. Here we also quantize the baseline DeiT-T [42] and compare it with our method without SLS and distillation. Even without SLS and distillation, our quantization NAS approach achieves a much better model accuracy than the full-precision and the quantized (W8A8) DeiT-T models.\nTo assess the efficacy of our row-wise flexible mixed-precision quantization scheme, we conducted an ablation study examining both the quantization scheme itself and the 8-bit mixed ratio, as outlined in Table 7. Since Quasar-ViT automatically searches the mixed ratios, here we pick up the best model from the search stage for different mixed ratios and compare them with the counterparts under the fixed row-wise mixed quantization scheme. The results indicate a consistent improvement in accuracy across different 8-bit mixed-ratio levels with our flexible mixed scheme, underscoring the efficiency of our proposed quantization scheme.\nTable 5 compares representative ViT-based works with our proposed Quasar-ViT. Since many ViT-based works do not incorporate model quantization, we also consider the bit-width in the model size and the equivalent number of total bit operations (BOPs)."}, {"title": "5.3 Comparison of Hardware Results on FPGA", "content": "We implement a proof-of-concept hardware accelerator for our Quasar-ViT on the AMD/Xilinx ZCU102 embedded FPGA platform. We also compare our results to Auto-ViT-Acc [22], the state-of-the-art FPGA accelerator for ViT with mixed-scheme quantization (without NAS). We retrieve the hardware results for Auto-ViT-Acc (which is quantized from DeiT-S) and the original DeiT-S on the same Xilinx ZCU102 FPGA platform from [22].\nAs shown in Table 8 and Figure 9, our approach consistently outperforms the previous work. Specifically, compared with DeiT-S [42], our QUASAR-L2 achieves 2.6\u00d7 higher inference frames per second (FPS) with 0.5% better accuracy. Compared with Auto-ViT-Acc [22], our QUASAR-L1 achieves 1.6\u00d7 higher FPS (159.6) with a similar model accuracy level, and our QUASAR-L2 achieves a similar level of FPS with 1.7% better top-1 accuracy.\nThe improvement in model accuracy and inference performance within our framework is attributed to two key factors. Firstly, our approach involves the training and search for a customized network architecture, specifically tailored for both the mixed-precision quantization schemes and the targeted inference latency. This strategy enhances adaptability and efficiency, surpassing the achievements of previous methodologies.\nSecondly, our novel supernet training algorithm, coupled with the proposed hybrid DSP packing design, allows for distinct quantization mixed ratios across various model layers. This fine-grained model achieves better flexibility than the previous approaches, unleashing the full potential of mixed-precision quantization.\nWith regard to the efficiency of our hardware accelerator design, the performance is mainly limited by DSP, LUT, and off-chip memory bandwidth. On par with Auto-ViT-Acc [22], our design achieves 150MHz frequency with about 66% usage of LUTs and 69% DSPs without timing violations. Note a typical FPGA design usually utilizes approximately 60% to 70% of the available FPGA resources; otherwise, it may fail during the placement and routing phase due to congestion or result in a lower operating frequency. Without considering the timing violation, the maximum theoretical expected performance is based on the 100% utilization ratio for both DSP, LUTs, and bandwidth, which can achieve about 1.47x of our reached FPS for the same model."}, {"title": "5.4 Other Transformer-based Model Accuracy Results", "content": "To demonstrate the scalability and versatility of our methods, we applied them across various datasets and applications, notably deploying them on a large language model (LLM). This choice is motivated by two key factors. Firstly, LLM shares a transformer-based architecture similar to that of Vision Transformer (ViT), aligning with the framework that we propose. Secondly, LLM is frequently integrated with ViT in text-to-image/video applications, making it an ideal candidate to showcase the scalability of our approach across both models and its potential for real-world applications.\nOur comparative analysis, presented in Table 9, utilizes the renowned LLM model, LLaMA, as the foundation for our supernet. We juxtapose our optimized results with those of LLaMA-7B [44] on the commonly used WikiText-2 dataset for LLMs, with perplexity score (PPL) serving as the evaluation metric, where lower scores indicate superior performance. According to the comparison results, our method shows a constant pattern, achieving a similar level of PPL with a much smaller model size."}, {"title": "5.5 Training Cost Comparison", "content": "Prior co-design frameworks, such as APQ [50], have also delved into the integration of neural architecture search (NAS) and quantization techniques. Please note that APQ is based on the convolutional neural network (CNN) and BitFusion platform [50]. To the best of our knowledge, we compare our Quasar-ViT models (both small and large variants) and the APQ result. As detailed in Table 10, our approach demonstrates superior FPS performance while maintaining comparable or even higher model accuracy, achieved at a reduced training cost. Compared with the 2,400 GPU hours training cost of APQ [50], our approach only consumes 768 and 1,344 GPU hours for the small and large versions of Quasar-ViT, respectively. Our training setting has been illustrated in Section 5.1."}, {"title": "6 CONCLUSION", "content": "In this work, we propose Quasar-ViT, a hardware-oriented quantization-aware network architecture search framework to enable efficient ViT deployment on resource-constrained edge devices. First, we proposed hardware-friendly quantization techniques including flexible row-wise mixed-precision quantization scheme and intra-layer mixed-precision weight entanglement in architecture search towards high accuracy and low training cost for efficient implementation. Second, we propose 4-bit weight atomic computation and hybrid signed/unsigned DSP packing for FPGA implementation, then incorporate latency/resource modeling to enable the hardware-oriented architecture search. Third, we extend the supernet layer scaling technique to further improve the training convergence and supernet accuracy. We also demonstrate the compatibility of our proposed framework with knowledge distillation during supernet training. Finally, we developed an efficient hardware-oriented search algorithm-integrated with hardware latency and resource modeling-to search the efficient subnet with high accuracy under a given inference latency target and implemented the searched model on real FPGA hardware for validation. From the experiment evaluation results, our approach achieves 101.5, 159.6, and 251.6 FPS on the AMD/Xilinx ZCU102 FPGA board with 80.4%, 78.6%, and 74.9% top-1 accuracy for ImageNet, respectively, consistently outperforming prior works."}]}