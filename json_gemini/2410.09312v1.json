{"title": "Towards Multi-Modal Animal Pose Estimation: An In-Depth Analysis", "authors": ["QIANYI DENG", "OISHI DEB", "AMIR PATEL", "CHRISTIAN RUPPRECHT", "PHILIP TORR", "NIKI TRIGONI", "ANDREW MARKHAM"], "abstract": "Animal pose estimation (APE) aims to locate the animal body parts using a diverse array of sensor and modality inputs, which is crucial for research across neuroscience, biomechanics, and veterinary medicine. By evaluating 178 papers since 2013, APE methods are categorised by sensor and modality types, learning paradigms, experimental setup, and application domains, presenting detailed analyses of current trends, challenges, and future directions in single- and multi-modality APE systems. The analysis also highlights the transition between human and animal pose estimation. Additionally, 2D and 3D APE datasets and evaluation metrics based on different sensors and modalities are provided. A regularly updated project page is provided in this GitHub link.", "sections": [{"title": "1 Introduction", "content": "Animal pose estimation (APE) identifies the spatial configuration of an animal's body parts using a diverse array of sensor and modality inputs, which is important in numerous disciplines [53]. For instance, neuroscientists rely on accurate measurements of animal movement to correlate with brain dynamics [60, 84, 90, 111]. Biomechanists analyse the movement of specific animal body structures to comprehend biomechanical phenomena, such as mechanical properties, stress-strain relationships, and functionality of different body structures [51, 105, 106]. In addition, ecologists monitor wildlife behaviours to develop more effective conservation strategies. Similarly, ethologists examine detailed animal gestures and postures to study intra- and inter-species communication and societal structures [101, 174]. Moreover, veterinary practitioners employ precise APE techniques to diagnose health conditions [32, 45] and assess recovery post-treatment, thereby mitigating potential disorders [30]."}, {"title": "1.1 Human Pose Estimation", "content": "The advancements in HPE provides insights to develop the APE methods due to the skeleton similarity between humans and animals. Therefore, we first provide an overview of HPE to help contextualize the current state and future potential of APE.\n\nThe field of HPE has rapidly evolved from manual feature engineering to automated feature learning over the past decade, due to the developments in deep learning approaches. Modern HPE methods are mainly RGB-based and predominantly employ convolutional neural networks (CNNs). In the early stages, models like VGG [130], ResNet-50 [44], ResNet-101 [44], and ResNet-152 [44] were employed as backbones for HPE from images or videos. The stacked hourglass network [100] then presented a novel architecture characterized by repeated bottom-up and top-down processing. This design aimed to capture and consolidate features at multiple resolutions, enabling precise localization of keypoints across scales. Subsequent innovations, such as the High-Resolution Network (HRNet) [143] and its variants [18, 162], have further advanced spatial information preservation by maintaining high-resolution representations throughout the entire network. Recently, transformer-based methods [157, 164] have integrated attention mechanisms to capture long-range dependencies and spatial correlations among joints and appearances, expanding the possibilities of HPE. Beyond vision-based HPE, alternative sensors like inertial measurement units (IMUs) [46], millimeter-wave (mmWave) radar [126, 150, 152] and thermal cameras [17, 131] have been explored, tailoring HPE to diverse use cases. Additionally, multi-sensor and multi-modal datasets [2, 85, 153] and approaches [2, 33, 172] have emerged as an innovative step over unimodal HPE to overcome individual sensor and modality limitations, offering a synergistic fusion of data that promises increased robustness and precision, especially in scenarios marked by occlusions, changing lighting conditions, and dynamic environments."}, {"title": "1.2 Definition of Animal Pose Estimation", "content": "APE, as discussed in this review, is defined as the task of identifying and localizing body joints of terrestrial mammals using a diverse array of sensor inputs. This task results in the reconstruction of the animal's pose, depicted as either a skeletal representation marked by keypoints or as a comprehensive body mesh.\n\nExploring APE through various sensory and modality inputs is important as animals may reside in diverse habitats which require different sensors for data collection. Most current APE methodologies are RGB-based, which predict the articulated joint positions from images or videos. However, RGB-based methods fail in many scenarios, such as nocturnal monitoring. For example, rodents are more active in the dark, and standard RGB cameras become inadequate due to poor lighting conditions. Consequently, infrared sensors have emerged as suitable tools for capturing the nocturnal activities of animals [108, 146]. Additionally, mmWave radar can detect movements but suffers from the scarcity of its generated point clouds; LiDAR adds value by providing depth information. Inertial sensors [107] can also track movement and orientation, offering invaluable insights but requiring on-animal attachment. In specific scenarios, even auditory cues [63] can indicate animal movement. This paper also explores the potential of using multiple sensors (i.e. RGB + acoustic) to improve APE performance."}, {"title": "1.3 Challenges in Human Pose Estimation and Animal Pose Estimation", "content": "HPE and APE encounter several common challenges that can impact the accuracy of associated algorithms. For example, humans and animals may appear differently due to their age, body composition, and clothing. Furthermore, we can see that a subject's perceived pose, whether human or animal, can significantly alter depending on the viewpoint. Another shared challenge is occlusion. Self-occlusions always exist, and certain body parts may be obscured by external objects (e.g. cars for humans, trees for wild animals). Environmental factors can be another factor. For example, on a rainy day, it is harder to identify the humans or animals' poses in an outdoor scenario, this further introduces ambiguities. Lastly, a busy or cluttered background can make separating the subject from its environment difficult.\n\nThere are also unique challenges in APE. The animal kingdom has a much greater morphological diversity and inter/intra-species variations than humans. Different animal species exhibit significant domain discrepancies in physical characteristics, body structures, and postures. Even within the same species, there can be a large variety in body size, shape, texture, motion, and posture, presenting difficulties in creating universally applicable models [121]. For example, the pelage of porcupines can vary a lot, and this is often more pronounced and thicker in animals than in humans' hair, which can make tracking skeleton segments tricky. Furthermore, many animals exhibit natural camouflage, travel in herds, interact frequently, or share similar appearances within their species [174]. These characteristics make it hard to distinguish individual animals and accurately estimate their poses as they blend into the background or appear similar to one another.\n\nEnvironmental factors also play a pronounced role. Vegetation or terrain features in the natural environments may obstruct the view of animals. One example can be found in the AcinoSet dataset [51], where the cheetahs' paws were excluded as the grass occluded them in most of the videos, making it difficult to estimate animal poses accurately. Moreover, changing lighting conditions can also heavily affect an animal's appearance, especially when the sun's position is moving in outdoor environments.\n\nThere are also many observational constraints in APE. For example, we often need to observe wildlife from afar; if we approach animals too closely, it may disturb their natural behaviour. This distant viewing distance may lead to reduced resolution and detail. In addition, animals can exhibit rapid and unpredictable movements (e.g. the high-speed pursuit of a cheetah); thus, we may need a stable camera platform and high-speed capture techniques to avoid camera shake and motion blur. Also, nocturnal species may require specialized sensors like infrared or thermal sensors to capture their movements and poses.\n\nMoreover, there are very limited annotated APE datasets [163]. Unlike HPE, which predominantly leans on supervised [18, 143] and semi-supervised learning methodologies [91, 109, 139] sustained by large-scale human pose datasets such as MSCOCO [74] and MPII [3], APE is hindered by a lack of such datasets, especially for endangered species and wildlife [174]. Also, it is often impractical to place animals in controlled environments to obtain ground truth poses, although"}, {"title": "1.4 Applications of Animal Pose Estimation", "content": "HPE has gained significant attention recently due to its applications in many domains, such as human-computer interaction, sports analytics, and healthcare. On the other hand, APE research has not been as prominent due to limited funding and methods. Despite this, APE is critically essential in various applications, as explained below and summarised in table 1."}, {"title": "1.4.1 Tracking", "content": "APE has been applied in various tracking contexts; for example, in the CATER [40] framework, APE is combined with environment reconstruction to track ants in complex natural environments, providing fine-scale motion trajectories even in visually challenging conditions. The SMART-BARN [97] system uses APE to track multiple animal species simultaneously in a large 3D arena and achieve accurate tracking and behavioural analysis in indoor and naturalistic environments. In the FaceMap [137] framework, APE is used for tracking the orofacial movements of mice, enabling detailed analysis of their facial expressions and related neural activities. For honey bees, APE tracked an entire colony over extended periods, capturing complex social behaviours and interactions without needing physical tags, enhancing our understanding of colony dynamics and individual behaviours [13]."}, {"title": "1.4.2 Articulation", "content": "The authors of [134] explore how APE can be used to understand animal articulation by predicting their 3D shapes from monocular images. They demonstrate that training a 2D keypoint estimator on a small set of labelled images (50-150) can generate pseudo-labels on a more extensive set of unlabeled web images. Then, these pseudo-labels are used to train models for 3D shape reconstruction of various articulated quadrupeds."}, {"title": "1.4.3 Behaviour Analysis", "content": "In agriculture, animal behaviour analysis based on APE plays an important role by enabling effective livestock management and health surveillance, which in turn provides continuous monitoring of movement and also early detection of animal's illness, thereby ensuring optimal animal welfare [26, 30\u201332, 35, 122]. Recently, more APE methods have been applied to study the behaviour of more species, such as macaques [138, 142, 156], Gelada monkeys [59], and mice [128]. Since 2023, there has been a notable shift towards integrating language prompts with APE in behaviour analysis research, exemplified by developing models like AmadeusGPT [160]. This combination has opened up new possibilities for more comprehensive interpretations of animal behaviour."}, {"title": "1.5 Scope of this Survey", "content": "We aim to provide a systematic review of APE, focusing on developments in deep learning-based methodologies for both 2D and 3D scenarios. It categorizes existing APE methods according to their sensor and modality inputs rather than limiting the scope to monocular images or videos.\n\nSeveral existing reviews have been reported on HPE and APE separately. Prior surveys [27, 36, 49, 56, 79, 92, 93, 115, 125, 132, 144, 171] have primarily concentrated on vision-based human motion capture, encompassing aspects of pose estimation, motion tracking, and action recognition. While these studies have significantly contributed to HPE, there remains a conspicuous absence of in-depth comparison and analysis between HPE and APE techniques. Reference [50] discusses vision-based APE between 2013 and 2021, yet it falls short of offering an exhaustive discussion on the existing challenges and future directions in APE. Another limitation is that only a limited number of reviews [171] discuss HPE or APE from sources other than vision-based inputs. This paper aims to bridge these gaps in the literature by offering a detailed analysis that extends beyond the scope of prior surveys and reviews:\n\n(1) We meticulously reviewed 178 papers up to 2024, categorizing them based on different parameters such as their output forms (e.g. 2D vs. 3D pose estimation; keypoint vs. mesh pose representation), input sensor and modality types (monocular inputs vs. other sensory inputs; single-modal APE vs. multi-modal APE), learning paradigms (e.g. heatmap-based methods vs. regression methods; top-down methods vs. bottom-up methods; supervised vs. semi-supervised vs. self-supervised vs. unsupervised APE methods), experimental setup (e.g. single-camera-view vs. multiple-camera-views approaches; indoor vs. in-the-wild setup; the range of the sensors).\n\n(2) We summarised and discussed datasets and evaluation metrics for both 2D and 3D APEs not only based on RGB cameras but also include other sensors and modalities such as motion capture (MoCap) systems, RGBD, LiDAR, infrared, 3D Scanner, IMU, acoustic and language cues.\n\n(3) We delved into the potential and limitations of current multi-sensor and multi-modal APE, highlighting how these approaches can complement or enhance unimodal APE methodologies.\n\n(4) We presented an extensive comparison between HPE and APE. This includes the transferability of knowledge and techniques between these two fields and how innovations in APE can reciprocally enrich HPE and the broader machine learning paradigm. It also explained the trend towards self-supervised and unsupervised methods for APE and compared the advantages and disadvantages of the existing supervised methods.\n\n(5) We also provided an overview of a wide array of applications of APE, ranging from wildlife conservation, biomechanics and neuroscience to 3D animal asset articulation, illustrating the practical implications and the interdisciplinary nature of APE.\n\n(6) Finally, we thoroughly discussed the key challenges in APE and pointed towards potential future research avenues to enhance performance and applicability."}, {"title": "2 Datasets", "content": "It is hard to obtain accurate ground truth poses for APE, especially for wild animals, due to the unpredictable natural environments, the wide range of animal behaviours, occlusions, and the lack of controlled setups for precise annotation. In this paper, we have categorized APE datasets into two types based on the data they provide and the sensor or modality they use: (1) RGB-based APE datasets, which rely only on RGB images or video sequences (often involving multiple frames) to provide animal pose annotations. (2) Other-sensor-based datasets, which utilize sensors and modalities"}, {"title": "2.1 RGB-based Animal Pose Estimation Dataset", "content": "In RGB-based APE datasets, we can further divide them into two categories based on the data type: 1) image-based APE datasets, which rely on individual RGB images to provide pose annotations for animals; 2) video-based APE datasets, which utilize video sequences, consisting of multiple frames, to capture animal pose labels over time."}, {"title": "2.1.1 Images-based Animal Pose Estimation Dataset", "content": "Early datasets in this category lacked large-scale annotated animal pose labels. For example, the Stanford Dogs (2011) dataset [57] includes 20k images of 120 dog breeds with class labels and bounding boxes but no pose keypoints. Similarly, the COCO (2014) dataset [75] with 1k images only provides annotations such as bounding boxes, segmentation, and class labels for animals like cats, dogs, and horses. The Animals-10 (2018) dataset [14], consisting of 14k images of common animals such as dogs, cats, and horses, also lacks animal keypoint annotations."}, {"title": "2.1.2 Video-based Animal Pose Estimation Dataset", "content": "We have listed several well-known datasets in this category, with most pose annotations being presented in 2D and 3D keypoint formats (table 3). In particular, most 2D keypoint pose labels in these datasets (e.g. Horse-30 [89], APT-36K [158], ChimpACT [86]) are provided by well-trained annotators. For instance, the TigDog (2016) [21, 22] includes 110k frames of in-the-wild dogs, horses, and tigers, with 19 keypoint annotations for 2D poses. The Horse-30 (2021) [89] provides 8k frames of horses in-the-wild, with 22 annotated 2D keypoints. While these datasets are valuable, they are limited to individual animals. However, there is a recent growing trend toward providing multi-animal pose annotations (e.g. ATRW [71], APT-36K [158] and ChimpACT [86]) to account for the frequent interactions between quadruped animals."}, {"title": "2.2 Other-Sensor-based Animal Pose Estimation Dataset", "content": "With developments in sensor technology, more APE datasets start to incorporate data from sources beyond traditional RGB cameras, such as depth sensors, MoCap systems, 3D scanners, LiDARs, and IMUs, offering increased accuracy and versatility.\n\nAs discussed in section 2.1, researchers have traditionally relied on manual annotations or triangulation techniques to obtain 3D ground truth pose labels. An alternative approach for achieving high accuracy and low-noise 3D ground-truth animal pose data is to use a maker-based MoCap system with RGB cameras. For example, the Rat7M (2021) [28] and PFERD (2024) [68] utilize MoCap system with RGB cameras and a set of makers to generate 3D ground truth pose data for rats and horses, respectively. Although these are effective for tracking individual animals from multiple views with temporal consistency, these datasets are limited by their focus on single animals and dependence on a large number of markers. To address these limitations, recent datasets like PAIR-R24M (2021) [88] and 3D-POP (2023) [98] have been developed. These datasets use MoCap systems with fewer markers (12 and 8 markers, respectively) to track the 3D postures of multiple animals in the same frame, offering more scalable solutions for multi-animal tracking. Similarly, a recent dataset [54] utilizes MoCap systems with 63-82 markers to generate highly accurate 3D ground truth pose labels for dogs. This dataset also integrates depth sensors (i.e. RGBD), which enable us to develop more flexible solutions for 3D APE without relying on physical markers. Although the MoCap-based dataset provides high-quality 3D ground truth pose labels, APE algorithms trained using these MoCap datasets may exhibit poor generalization to in-the-wild scenarios, as the data collection for the MoCap-based datasets is all done indoors in a controlled environment, as you can see in table 4.\n\nWildPose [96] is another promising multi-sensor system that combine RGB and LiDAR data. This system captures 2D RGB video and 3D point cloud data from a distance (up to 120 meters), making it particularly suited for studying free-ranging animals in natural habitats.\n\nIn addition to these technologies, infrared cameras for camera-trapping in the wild, which use infrared LEDs to illuminate the scene to enable nighttime observations, have also been employed for wild animal pose labelling. For instance, the LoTE-Animal (2023) [76] uses infrared trap cameras to collect large-scale video footage of endangered"}, {"title": "3 Error Metrics and Evaluation Methods", "content": "This section provides insight into the error (err.) metrics and evaluation (eval.) process of the APE methods based on different sensors and modalities."}, {"title": "3.1 RGB-based 2D Animal Pose Estimation Metric", "content": "This section discusses the common evaluation metrics used for RGB-based APE methods that extract poses in the 2D keypoint format, and more evaluation metrics are given in table 5 and table 6.\n\n2D single-animal pose estimation methods like DeepLabCut [90] and LEAP [110] use a common metric like Root Means Square Error (RMSE) to evaluate their performance. RMSE is versatile and can be used for both 2D and 3D pose estimation and for evaluating various types of keypoints, regardless of the specific application domain. In addition to RMSE, many APE methods also use the Probability of Correct Keypoint (PCK), which measures the fraction of joints where the predicted joint positions are within a certain threshold distance from the ground truth joint positions. It is a normalised measure that indicates how well the keypoints are localised, and a higher PCK value indicates better localisation performance. Single-animal pose estimation methods like Synthetic-to-Real [66], Kim et al. [58], ScarceNet [67], D-Gen [69] and FSKD [82] use this metric for evaluation. Moreover, Li et al. [70] evaluate their single-animal APE methods with PCK, RMSE and Area Under Curve (AUC) along with an extension to these metrics with Permutation Invariant (PI). Li et al. [70] introduce PI metrics to evaluate the accuracy of pose estimation in challenging scenarios where it is difficult to uniquely identify limb identities (e.g. in fruit flies) with traditional metrics. PI metrics are designed to account for the ambiguity in identifying corresponding limbs or joints between the predicted pose and ground truth.\n\nObject Keypoint Similarity (OKS) has also been used in evaluating 2D single-animal pose estimation methods like D-Gen [69]; 2D multiple-animal pose estimation methods like DepthFormer [77], Straka et al. [135], SLEAP [112], and also 2D video based semi-supervised method like SemiMultiPose [11]. OKS quantifies how well the keypoints are located to spatial scale and keypoint visibility, with closeness defined based on ground truth. The OKS lies in the range of 0 to 1, and a value close to 1 indicates that the detected keypoints coincide with the ground-truth keypoints. The OKS scores for every keypoint are computed for all objects in the dataset, and the average OKS is used to evaluate a model. Another common metric used for 2D APE is mean Average Precision (mAP), which has been used in evaluating ScarceNet [67], SIPEC [87], SuperAnimal-Quadruped [127], OpenPose [16], DLCRNet [62], maDLC [62], SLEAP [112], SuperAnimal-TopViewMouse [127] and SemiMultiPose [11]. The mAP is an evaluation metric based on OKS and has become widely used for pose estimation, particularly following its adoption in the COCO keypoint challenge. OKS measures the similarity between the predicted keypoints and the ground truth, and Average Precision (AP) is then calculated to assess the prediction's performance on a dataset by comparing it against different OKS thresholds. The mAP is the mean of several AP values, each calculated using a different threshold.\n\nLast but not least, another metric that has been used for the 2D video-based APE method evaluation is Canonical Correlation Analysis (CCA), which is a statistical technique used to examine the relationship between two sets of variables. It determines linear combinations of the variables in each set that are maximally correlated; this metric has been used for evaluating the Lightning Pose [7] method, however, we have not seen wide adoption of this evaluation metric within the APE community."}, {"title": "3.2 RGB-based 3D Animal Pose Estimation Metric", "content": "This section discusses the common evaluation metrics used for both keypoint-based and mesh-based 3D RGB models within the APE community, more evaluation metrics for the 3D APE methods are given in table 7.\n\n3D APE metrics can also incorporate some of the metrics used in 2D APE. For example, PCK is employed in methods such as Three-D Safari [174], SMBLD [8], Li et al. [65], Birds of a Feather [145], BITE [120], MagicPony [149], DigiDogs [129], Monet [159], DAN-NCE [28], and Muramatsu et al. [95]. The well-known RMSE metric is also used in 3D APE methods such as Hu et al. [45], Muramatsu et al. [51], 3D-UPPER [29], and SBeA [41].\n\nApart from PCK and RMSE, 3D pose estimation-specific metrics such as Mean Per Joint Position Error (MPJPE) have been used in 3D APE methods like DigiDogs [129] and MAMMAL [1]. MPJPE measures the average Euclidean distance between the predicted and ground truth joint positions across all joints in the skeleton, quantifying the accuracy of the joint position estimation. A lower MPJPE indicates that the predicted joint positions are closer to the actual joint locations. An extension of the MPJPE metric is N-MPJPE, which has been used by Dai et al. [19] for their 3D APE model. N-MPJPE involves normalising the ground truth 3D pose by subtracting the mean and dividing by the standard deviation, aligning the predicted 3D pose with the normalised ground truth using Procrustes alignment, and then calculating the MPJPE, which results in Normalized-MPJPE.\n\nFor mesh-based APE as discussed in section 4.3.1, a well-known evaluation metric is the 3D Chamfer Distance, which has been used in several model-free methods such as DOVE [148], MagicPony [149], and BANMO [154]. The 3D Chamfer Distance measures the average distance between the reconstructed surface points and the ground truth surface points. It finds the nearest neighbour matches between the two sets of points and computes the distance, which is sensitive to outliers. A lower value indicates better reconstruction quality.\n\nAnother approach in mesh-based APE is the model-based pose estimation methods, for example, SMAL [177]. In these approaches, metrics like Scan-to-Mesh distance, keypoint reprojection error, and silhouette reprojection error are commonly used. The Scan-to-Mesh distance measures the accuracy of the reconstructed 3D shape by calculating the distance from each point on the scan to the closest point on the predicted mesh surface. Keypoint reprojection error evaluates the pose estimation by comparing the 2D projection of 3D keypoints from the predicted model with the annotated 2D keypoints in the images. It assesses how well the 3D pose model aligns with the 2D image data. Similarly, the silhouette reprojection error measures how accurately the 3D model silhouette matches the ground truth silhouette in the 2D image, indicating how well the 3D shape projection aligns with the visible boundaries of the animal in the image. Other model-based methods, such as SMBLD [8] and Li et al. [65], use metrics like PCK and Intersection over Union (IoU) for performance evaluation.\n\nMoreover, due to the scarcity of animal datasets that include 3D pose annotations, comprehensive quantitative evaluations of 3D pose predictions are not always feasible. Therefore, researchers often assess 3D pose predictions using qualitative means, as Sosa et al. [133] explained. Examples of qualitative evaluations can be found in Salem et al. [123], Sosa et al. [133], Monet [159], and Muramatsu et al. [51]."}, {"title": "3.3 Other-Modality-based Animal Pose Estimation Metric", "content": "Different sensor modalities have different ways of evaluating their APE models. For example, an IMU-based APE method [161] is evaluated by calculating the mean and standard deviation (Std) of the estimated joint angle errors and comparing the predictions with ground truth data obtained through a MoCap system. Additionally, the model is assessed for Intra-individual Variability (IAV) and Inter-individual Variability (IEV), which quantify the consistency of"}, {"title": "4 RGB-based Animal Pose Estimation", "content": "In recent years, APE has achieved significant progress. Among the various modalities utilised for APE, RGB-based approaches have emerged as the most developed and extensively studied research method [15, 62, 66, 110]. These approaches leverage visual information captured by RGB cameras, take animal images or videos as inputs, and subsequently produce the animal's pose, which manifests as a 2D skeletal keypoints representation of pose, 3D skeletal keypoints representation of pose, or 3D animal mesh reconstructions, as exemplified in fig. 1(a) and fig. 1(b). In addition to skeletal keypoints and mesh representation for pose estimations, dense pose representations have also been used mainly for humans [118] and chimpanzees [124]. In this paper, skeletal keypoints and mesh representations APE have been mainly analysed in detail, while keypoint-based methods remain dominant due to their established presence, easier access to annotated data, and lower computational requirements.\n\nBefore we go into the details of RGB-based APE methods, let us first discuss the development of this branch. Traditionally, we use direct visual observation to extract the animal poses, which can offer real-time and context-rich data. However, it suffers from inherent drawbacks, including observer bias, mental fatigue [38], and long data collection and observation times. Later, researchers start to use video monitoring and recording to expedite data collection and minimise the influence of human involvement [38]. However, manual scoring of videos by eyes still remains time-consuming and often fails to capture subtle and fast-paced movements that are integral to various behaviours. Moreover, the reliance on human observers restricts the dataset size, this may also lead to statistical errors that limit researchers' ability to address scientific questions accurately.\n\nRecently, automated body kinematics tracking techniques utilising advanced imaging software and hardware have been developed to tackle the challenges in manual scoring. Marker-based methods rely on attaching markers (e.g. MoCap suits [55], animal-borne cameras [107]) to animals for posture measurements. While these methods can achieve precise pose estimation, the reliance on physical markers can pose practical challenges, especially when investigating innate behaviours in natural habitats [149] or when observing the movements of small creatures like mice [62] and fish. Furthermore, most marker-based approaches are limited to highly controlled lab environments and specialised"}, {"title": "4.1 2D Animal Pose Estimation from an Image", "content": "This section focuses on 2D APE methods that do not use temporal information to localize the 2D coordinates of an animal's or animal's joints from a single image frame (table 5). Note that the majority of the APE algorithms that can extract poses from videos (i.e. multiple frames) can also extract poses from a single image. In Table 5, column \"I,V\" provides the information on APE methods that can be applied on both an image and videos; or only on an image.\n\nBased on the number of target animals being processed in the scene, this pose estimation problem can be further divided into single-animal pose estimation and multiple-animal pose estimation. While pose estimation in individual animals has received significant attention and serves as the foundation for most APE studies, extending it to multiple-animal pose estimation is an active area of research that is still evolving [112]."}, {"title": "4.1.1 Single Animal", "content": "2D single-frame single-animal pose estimation aims to accurately estimate the 2D pose of an animal from a single image. These methodologies share a conceptual foundation with HPE [171] and can be broadly classified into two categories: heatmap-based thods. The former predicts the locations of an animal's body joints using heatmaps for supervision, necessitating a subsequent postprocessing step to identify the final keypoints based on the predicted heatmaps. In contrast, the latter approach directly regresses the keypoint coordinates of the animal's body joints, employing an end-to-end deep neural network to establish a mapping from the input image to the body keypoints. Figure 3 illustrates the general architecture of 2D single-frame single-animal pose estimation methods.\n\nHeatmap-based frameworks such as [38, 58, 66, 67, 69, 70, 90, 110] have become popular in 2D APE. These approaches are structured around creating a 2D heatmap, interpreted as a probabilistic representation of body part locations within the image. The architecture of a typical heatmap-based scheme includes i) a CNN backbone to generate heatmaps from input images, followed by ii) a post-processing step to finalize the pose estimations based on these"}, {"title": "4.1.2 Multiple Animals", "content": "We have seen significant developments in markless pose estimation in individual animals. However, extending these methodologies to multiple animals poses unique challenges, particularly in studying social behaviours or animals in their natural environments [35, 158]. For example, it is common for the body parts of one animal to be obscured by another in a multi-animal pose estimation. Another challenge arises from scale variation, where animals at different distances from the camera show differences in apparent sizes. Inter-species variation also introduces additional complexity due to differences in body structures and poses. However, given these challenges, 2D multiple-animal pose estimation research has still garnered significant attention due to its potential applications in various domains, such as animal behaviour analysis, wildlife monitoring, and ecological studies. Methodologies developed in this field are categorized into two main categories: top-down methods and bottom-up methods."}, {"title": "4.2 2D Animal Pose Estimation from Videos", "content": "2D multi-frame APE transforms consecutive image sequences or videos into probabilistic estimates of the 2D spatial coordinates of keypoint locations for target animals across frames. Based on how these approaches leverage the temporal dimension in video data to extract poses, we categorize 2D APE from videos into two main categories: (1) post-processing methods to incorporate temporal information; and (2) end-to-end methods to incorporate temporal information, as shown in table 6."}, {"title": "4.2.1 Post-Processing Methods to Incorporate Temporal Information", "content": "Early APE frameworks process each video frame independently to predict pose keypoints, without inherently incorporating temporal features in an end-to-end manner during training. Temporal consistency is typically addressed through optional post-processing methods, such as smoothing or filtering, to improve performance. Some examples that follow this frame-by-frame approach include DeepLabCut [90], DeepPoseKit [38], and LEAP [110]."}, {"title": "4.2.2 End-to-End Methods to Incorporate Temporal Information", "content": "Recently, researchers have achieved more robust predictions by utilising the spatiotemporal information embedded within video data during pose estimation model training in an end-to-end manner. By evaluating inherent temporal coherence and ensuring consistency across successive frames during training, models can produce more accurate and stable pose estimations over time. We highlight several methods that fall in this category in the following bullet points and table 6."}, {"title": "4.3 3D Animal Pose Estimation from Images and Videos", "content": "3D APE refers to the task of determining the locations of animal body joints in 3D space. This challenge extends beyond the 2D APE by adding depth information to the analysis, which allows for a"}]}