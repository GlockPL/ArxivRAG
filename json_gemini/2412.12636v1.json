{"title": "TrainMover: Efficient ML Training Live Migration with No Memory Overhead", "authors": ["ChonLam Lao", "Minlan Yu", "Aditya Akella", "Jiamin Cao", "Yu Guan", "Pengcheng Zhang", "Zhilong Zheng", "Yichi Xu", "Ennan Zhai", "Dennis Cai", "Jiaqi Gao"], "abstract": "Machine learning training has emerged as one of the most prominent workloads in modern data centers. These training jobs are large-scale, long-lasting, and tightly coupled, and are often disrupted by various events in the cluster such as failures, maintenance, and job scheduling. To handle these events, we rely on cold migration, where we first checkpoint the entire cluster, replace the related machines, and then restart the training. This approach leads to disruptions to the training jobs, resulting in significant downtime. In this paper, we present TrainMover, a live migration system that enables machine replacement during machine learning training. TrainMover minimizes downtime by leveraging member replacement of collective communication groups and sandbox lazy initialization. Our evaluation demonstrates that TrainMover achieves 16\u00d7 less downtime compared to all baselines, effectively handling data center events like straggler rebalancing, maintenance, and unexpected failures.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have gained significant attention in the past few years [5, 7, 26, 28, 29, 35, 36]. The scaling law guides people in designing and training larger LLM models to improve performance. LLM training jobs are commonly deployed on decentralized parallel training frameworks (e.g., Megatron-LM [33] and NeMo [20]), span thousands of GPUs, and last for weeks to months. For example, training GPT-3 with 175 billion parameters on 1024 GPUs required approximately 34 days [25], while Llama 3 with 405 billion parameters was trained over 54 days using up to 16,000 H100 GPUs [13]. LLM training commonly adopts model parallelism and data parallelism and requires coordination from all GPUs.\nThis large-scale, long-lasting, tightly coupled distributed training is often disrupted by various events in the cluster such as failures, maintenance, and job (re)scheduling. First, as the scale increases, there are frequent failures and anomalies of individual components, which can slowdown or halt the entire cluster. Alibaba's report [40] shows that 60% of large-scale training jobs experience slowness due to various reasons including hardware anomalies, software contentions, etc., resulting in a 35% increase in job completion time (JCT) due to cluster slowness. Llama 3 training job experienced a meantime-to-failure (MTTF) of 2.7 hours [13]. Second, as training takes weeks to months, we cannot simply defer maintenance jobs (repairs, upgrades, etc.) until after the training because delayed maintenance increases the safety and security risks of the cluster [13]. Maintainance often requires rebooting the server or switch and interrupts the training job [1,19]. Third, in a shared cluster, we need to reschedule jobs and rebalance resources, when a new high-priority job joins the system [8,41], or when an entire pod is now available for a training job running on fragmented resources across pods [37].\nTo handle these events, the current best practice is cold migration which includes three steps: checkpoint, replace, and restart [9, 14, 18, 38]. When a cluster event happens, the training framework either waits for the next scheduled checkpoint or triggers one immediately. The scheduler stops the training job, replaces the abnormal servers with backups in the same cluster, and restarts the job based on the last checkpoints. This results in minutes-level downtime caused by schedulers, checkpoint storage, and training framework initialization (see Section \u00a72), which, when amplified across thousands of GPUs, can lead to extremely high costs.\nRecent works such as ReCycle [10], Oobleck [17], and Parcae [8] propose to handle failure events with live reconfiguration. When failure happens, these works keep the training job with the remaining servers by changing the hyperparameters (such as batch size) or parallelism schemes. However, as the cluster size changes, optimization operators designed for the original cluster size and topology [18] become ineffective, leading to degraded training throughput. For example, after reconfiguration, some intensive communication parallelism groups may span across different racks, disrupting machine training locality and increasing communication overhead among machines. Furthermore, reducing the number of machines can increase memory pressure on the remaining GPUs, potentially resulting in out-of-memory issues.\nIn this paper, we introduce TrainMover, which enables live migration for LLM training frameworks. TrainMover leverages standby servers that are generally available in the cluster and shifts the workload from source GPUs (migration leavers) to standby GPUs (migration joiners) without restarting the entire training job or changing its parallelization scheme. TrainMover carefully manage the migration overlap period to prevent interference with ongoing training, minimize downtime, and avoid GPU memory overhead during migration. TrainMover leverages two key techniques, partial replacement of collective communication group members and sandbox lazy initialization, to reduce the migration downtime introduced by the joiners' communication and computation initialization. TrainMover removes unnecessary interactions between the joiners and the rest of the participants so that the joiners can prepare in the background without interrupting the training job."}, {"title": "Member replacement of collective communication groups:", "content": "The current collective communication library does not support partially replacing members in the collective communication group (CCG) after initialization. The only way to migrate the communication channels from leavers to joiners is to destroy the CCGs on the leavers and re-initiate them on the joiners, which introduces a long downtime. TrainMover proposes the CCG member replacement algorithm to solve this problem. The algorithm guarantees the replaced CCG's communication graph is equivalent to the re-initiated one and therefore frees from performance degradation. TrainMover confines the communication graph generation and intra-server connection initialization within the joiners so that they can be performed in the background and only leave inter-server connection establishment on the critical path."}, {"title": "Sandbox lazy initilization:", "content": "Lazy initialization is widely used in current LLM training frameworks to reduce the preparation time and perform runtime optimizations. After the joiners replace the leavers and load the model, the entire training job is delayed because the joiners rerun the lazy initializations. TrainMover introduces sandbox lazy initialization to warm up the joiners before replacing the leavers. After each joiner finishes the preparation, the sandbox triggers one emulated iteration and intercepts every inter-machine collective communication. The sandbox replaces each inter-machine collective communication with pre-recorded tensors so that joiners can initialize without other participants and the emulated iteration can align with a normal one.\nTrainMover leverages the two key techniques to handle various types of data center events, such as maintenance, straggler handling, and rebalancing, while proposing an additional workflow to address unexpected failure events. For data center events, TrainMover prepares the joiners in a sandbox environment, ensuring the training job continues running untouched on the original cluster. This approach minimizes disruption and maintains training performance during migrations. When the joiners are ready, TrainMover freezes the training job, replaces the CCG member, and synchronizes the latest training model from the leavers to the joiners. Once finished, TrainMover resumes the training job in the new cluster with the original configuration and training throughput. For unexpected failure events, TrainMover directly freezes all machines and initializes the joiners' communication and computation at the same time. TrainMover loads the model from other servers if the framework provides redundant copies of the model. Otherwise, the entire cluster falls back to the last saved checkpoint. Our experiments demonstrate that TrainMover achieves 16x less downtime compared to baselines under frequent live migration. During unexpected failures, TrainMover enables recovery 2.35x faster than the baselines."}, {"title": "2 Background and Motivation", "content": "2.1 Distributed LLM Training\nState-of-the-art LLM models consist of hundreds of billions of parameters and are trained atop datasets with trillions of tokens. Training such a model requires a cluster with exaflops of computing power, an efficient distributed training algorithm, a failure-resilient training framework, and a responsive cluster scheduler.\nTraining cluster. The training cluster contains thousands of GPU servers with tens of thousands of GPUs. For example, Alibaba's HPN cluster [27] supports 15K GPUs per pod, while Meta deploys two clusters [13], each with 24K Nvidia H100 GPUs, to train the Llama 3 model. The training clusters commonly deploy two independent networks: the frontend network connects GPU servers with services such as storage, logging, etc., and carries training samples, checkpoints, and logs, while the backend network provides high-speed interconnect for all GPUs, with 4-8x more bandwidth than the frontend network [11,27].\nDistributed training framework. The training framework, such as Megatron-LM [33], initializes computation and communication in the GPU cluster, loads training data from storage, executes the distributed training algorithm, and monitors cluster health during runtime. It periodically snapshots model parameters and optimizer states, saving them to checkpoint storage. To fit large models into limited per-GPU memory and accelerate training [16, 24, 25, 33], frameworks use model and data parallelism. Model parallelism (e.g., tensor parallelism (TP), pipeline parallelism (PP), and sequential parallelism (SP)) partitions the model across GPUs in different dimensions-TP splits at the tensor level, while PP splits at the layer level. Data parallelism (DP) duplicates model and optimizer states, with each replica processing a subset of the training data. Optimizer states often consume more GPU memory than model parameters [30]. To mitigate this, recent frameworks [2, 18, 30, 38] use distributed optimizers (DO), which evenly distribute optimizer states across DP groups, eliminating redundancy and enabling larger models to fit within the cluster. Collective communication libraries (CCLs) like NCCL facilitate GPU communication and exchange intermediate results, ensuring efficient iteration completion.\nCluster scheduler. A central cluster scheduler (e.g., Slurm) manages the entire cluster. Upon receiving a training job, it gathers available servers, performs health checks, deploys the job, and starts the training framework. On a shared environment, the scheduler also prepares a isolated virtualized environment and creates a private channel between the GPU servers and peripherals such as storage and logging service."}, {"title": "2.2 Call for Live Migration Primitive", "content": "Sustaining high training throughput is challenging. A running job's performance can change between different runs and different iterations in the same run due to various reasons. Overheating or power loss lowers the GPU's clock frequency. Switch, optical module, or NIC failures create network bottlenecks and delay CCL completion. Contention between the training process and other processes running in the background reduces the CPU time for training. Job scheduler's suboptimal job assignment creates unnecessary traffic collisions, which incur longer CCL completions. FALCON [40] measured a 35% JCT increase due to various slowdown events.\nJob interruption is another major reason. The training job can be interrupted by disruptive events such as hardware failures, software bugs, scheduler preemption, regular maintenance, etc. Meta experienced 466 job interruptions during their 54-day Llama 3 training [13].\nWhen resolving anomalies, maintaining the same parallelization strategy (i.e., migration) is critical since the training framework is specially tuned for the specific cluster size and the topology. For example, Meta [13] developed a memory consumption estimator and a performance-projection tool to find the best parallelism configuration to achieve the highest training throughput with minimum communication overhead and avoid GPU memory overflow. They also adjusted the parallelism strategy and workload allocation so that no imbalance exists between the machines and the cluster's overall throughput is maximized. Readjusting the configuration can lower the efficiency and cause resource wastage.\nFurthermore, data center architects are fully prepared for migration and usually provide enough redundancy in the cluster. For example, the Llama 3 training cluster contains 24K GPUs while only 16K are used for training the model [11], HPN7.0 provides 8 redundant machines in the first-tier network to tolerate hardware failures [27].\nCurrently, checkpoint-replace-restart (cold migration) [4, 14, 15, 18, 38] is the best practice for resolving anomalies. When slowness is detected or maintenance is scheduled, to avoid losing unsaved progress, the operator can either trigger one checkpoint proactively or wait for the next scheduled checkpoint. Operators also schedule periodic checkpoints to handle unexpected interruptions. After the job exits and the root cause is located, the scheduler then isolates anomaly machines, replaces them with healthy ones, and restarts the job.\nHowever, cold migration reduces the cluster availability because it introduces many scheduler operations including job cleanup, reschedule, and re-initialization. Under unexpected failures, it also discards the unsaved progresses. To quantify the impact, we sampled two LLM training jobs for different models at different scales in our production clusters for a week and the result is shown in Table 1. We can see that the time wastage caused by cold migration (unsaved progress, job initialization, cleanup, and reschedule) contributes nearly 90% of the cluster downtime. This is because the job restart triggers complicated and sequential management operations on many components. For example, in a shared cluster, destroying the virtual network requires confirmations from all servers that all services require networking (e.g. monitoring, storage) have finished. A single unresponsive operation on one server not only delays the current step, but also delays all followup steps. When job stops, a critical step of the monitoring system is to push unsaved monitoring data on each server to the logging service. The large scale synchronized write burst easily causes stragglers and delays the followup cleanup steps. Scheduler can be trapped in a restart loop if it does not identifies the culprit correctly and continuously reschedules the job to the same malfunctioning server. This creates huge resource wastage, especially during night times when the operator cannot intervene. Slowness contributes only around 10% of the downtime because operators usually choose to migrate the job when slowness is detected and deemed persistent.\nWe promote to replace the traditional cold migration primitive with the live migration primitive. The live migration primitive shifts the workload from anomaly machines to healthy standbys without restarting the workload on other healthy machines. It maintains the original optimal hyperparameter setting and parallelization strategies to sustain high training efficiency. Live migration only requires minimal cluster changes and avoids unnecessary scheduler operations. The healthy machines do not go through the restart process, which reduces pressure on peripheral services such as checkpoint storage and logging.\nWith the help of live migration primitive, the operators have more deployment flexibility and longer maintenance window without suffering from the high restart overhead. They can isolate the stragglers, failed nodes or switches, rebalance a suboptimal cluster allocation, and shift workload to perform maintenance or balance the electric load without stopping and rescheduling the job. It avoids job rescheduling failures. Also, because live migration bypasses job restart on healthy machines, the lower pressure on peripheral services allows a more responsive cleanup and initialization on anomaly machines and healthy standbys, respectively."}, {"title": "2.3 Naive Live Migration Implementation", "content": "The basic requirement to implement live migration is zero memory overhead because we observe that current large-scale LLM training jobs commonly keep GPU memory consumption full. Indeed, as shown in Figure 1, the average memory consumption per GPU of three model training jobs at different scales sampled from our production cluster is higher than 94% per job. This is not a coincidence. Operators observe higher training throughput when increasing the job's memory ratio, for example, by increasing the batch size.\nBased on this requirement, we derived a naive live migration system from the cold migration process. Instead of re-scheduling and rebooting all machines and retrieving the training checkpoint, a naive live migration system operates as follows: (1) all existing training participants are notified about the decision to migrate machines (migration leavers) to new machines (migration joiners), typically triggered by an event such as a straggler; (2) the migration joiners launch the training job from scratch; (3) the migration leavers transfer training states (e.g., model parameters and optimizer states) to the migration joiners once the joiners complete the job booting process; (4) all communication-related groups (e.g., data parallelism CCL groups, pipeline parallelism CCL groups) are destroyed and re-instantiated due to CCL's static limitation (\u00a74); (5) the migration leavers disconnect, and their role is replaced by the joiners in the training job; and (6) after the joiners are fully replaced into the training process with the existing machines, the first few iterations are unavoidably very slow due various lazy initialization and computational resource ramp up (\u00a75).\nUnfortunately, although naive live migration avoids the complete teardown and restart of all jobs across machines, the critical path remains largely unchanged. This is because the migration joiners must undergo the same procedures as those in cold migration, as illustrated in Figure 2. This figure shows the restart procedures of running a GPT-10B job on a 64-GPU, 8-machine cluster with our naive live migration implementation. Regardless of whether there is a single joiner or all joiners, the downtime remains similar-135 seconds for one joiner and 105 seconds for all joiners. This is because all participants must wait for every participant to finish each procedure sequentially.\nTo reduce migration downtime, one approach is to overlap these procedures. However, this can easily incur additional memory overhead on both the communication and computation sides. Take CCL as an example\u2014while overlapping and coexisting old and new CCL groups during migration can reduce downtime associated with destroying and re-instantiating CCL groups, it incurs significant GPU memory overhead, as every CCL group instance requires dedicated GPU buffers."}, {"title": "3 TrainMover Overview", "content": "TrainMover is a resilient LLM training framework that provides the live migration primitive. TrainMover develops two key techniques, collective communication group (CCG) replacement and sandbox lazy initialization, to move communication and computation migration overhead to the background. The CCG replacement algorithm (\u00a74) enables existing machines to recycle their current CCGs, replacing only the new delta CCG connections while guaranteeing the new CCGs maintain identical performance. Additionally, it divides the joiners' new CCG setup into two parts: overlappable setup (e.g., intra-machine connections) and non-overlappable setup (e.g., inter-machine connections), minimizing the non-overlap setup as much as possible to reduce downtime. Sandbox lazy initialization (\u00a75) creates emulated environments for joiners and warms them up in the background so that they can compute at full pace after joining the new cluster. By moving most events away from the migration critical path, TrainMover migrates the workload with minimum JCT overhead and zero memory overhead (\u00a76)."}, {"title": "4 Communication Member Replacement", "content": "Current state-of-the-art CCLs do not support member replacement, i.e., replacing one or multiple members in an established collective communication group (CCG). This inflexibility brings high migration overhead since we can only destroy the old CCL group and re-initialize a new one to change the membership. Replacing the member in a CCG can bypass redundant setup steps and reduce the migration time. In this section, we first deep-dive into the CCG initialization process in NCCL, one of the most adopted CCLs (\u00a74.1), and explain how TrainMover implements CCG member replacement with the minimum downtime (\u00a74.2)."}, {"title": "4.1 NCCL CCG Initialization", "content": "Given the participants, a NCCL CCG initialization goes through the following steps:\nNetwork bootstrap. One of the CCG participants collects all other participants' network addresses and guides them in constructing a TCP-based ring connection channel.\nTopology discovery and computation. Each participant collects local device metadata (e.g., NIC bandwidth, NVLink capability) and shares it using the ring all-gather algorithm. Given the gathered information from everyone, each participant decides its predecessors and successors.\nConnection establishment. According to the graph, each participant reserves GPU memory for buffering and creates the inter-machine and intra-machine connections. Inter-machine connections commonly use GPU Direct RDMA (GDR), while intra-machine ones use NVLink."}, {"title": "4.2 CCG Member Replacement", "content": "Communication graph generation. We observed that NCCL's communication graph exhibits a regular pattern. Participants first construct intra-machine subgraphs individually, based on their host architecture. These subgraphs are then connected according to the global ranks assigned by the operator. For the sake of simplicity and performance [3]\u00b9, NCCL does not take the inter-machine topology (e.g., oversubscription ratio) into account.\nTrainMover introduces a two-step replacement algorithm to leverage such a pattern and avoid communication between joiners and leavers to reduce replacement downtime. Firstly, the joiners compute the intra-machine subgraph using NCCL's original algorithm, which yields the same result as the original subgraph since the machines are identical. Secondly, for each joiner, TrainMover controller finds its global rank in the CCG by matching it with the corresponding leaver and connects the joiner's subgraph with the up and downstream ranks. This algorithm guarantees the new graph to be the same as the ground truth.\nTwo-stage connection establishment. Table 2 shows that the intra-machine connection establishment consumes 5 times more initialization time than the inter-machine ones. For the joiners, this is a necessary and crucial step and cannot be bypassed. We noticed that it is self-contained and does not require interaction with other participants. Following this insight, TrainMover splits the intra and inter-connection establishment separately into two stages and exposes explicit APIs to the training framework. TrainMover can initialize the intra-machine connections on the joiners while the original CCG is untouched and still functional in the first stage and only the second stage, inter-machine connection establishment, is on the critical path. The CUDA_VISIBLE_DEVICES flag no longer delays CCG initialization.\nCCG replacement workflow. Now we present the CCG Member replacement workflow, as shown in Figure 4.\nGiven a CCG and the leavers and joiners in the CCG, Train-Mover controller first sends each joiner the CCG metadata, i.e., the previous CCG participants and the leavers.\nEach joiner calculates the communication graph, initializes the intra-machine communication connections, and notifies the controller when finishes.\nThe controller notifies the leavers and coordinators to remove the inter-machine connections between them.\nThe controller notifies the joiner and coordinators to establish new connections.\nEach of the steps can fail because a joiner or leaver could be unresponsive. Therefore, TrainMover executes each step in a blocking manner, ensuring it does not proceed until confirmation is received from all involved participants. During execution, TrainMover monitors the CCG's status on each participant. When any step fails, TrainMover can re-issue the command if the CCG on all participants is still alive. Otherwise, TrainMover falls back to re-initializing the entire CCG."}, {"title": "5 No-Time-to-Be-Lazy", "content": "Migration joiners introduce unforeseeable computational lazy initialization bottlenecks (\u00a75.1) that can slow down the training process after migration. To address this issue, we propose sandbox lazy initialization (\u00a75.3 and \u00a75.2) to minimize downtime and eliminate interference with ongoing training."}, {"title": "5.1 Lazy Initialization", "content": "Lazy initialization is essential in distributed training, particularly in Python-based frameworks like PyTorch. These initialization processes are typically one-time, value-independent operations, triggered during the first execution. For example, computational graph compilation, where the computation graph is constructed lazily during the first model forward pass; model parameters and optimizer initialization, where optimizer memory is allocated only when the first model update occurs, as evaluation-based forward passes do not require optimizer initialization; CUDA initialization overhead, which involves GPU-specific setups like context creation and kernel loading; and JIT compilation optimizations, runtime hooking, and other caching behaviors, which enhance runtime performance by dynamically compiling frequently used operations during execution. In the same GPT-10B experiment used in Table 2, where the normal iteration time (from the second iteration onward) is approximately 6.8 seconds, the first iteration time can increase by more than 6\u00d7, reaching around 44 seconds due to lazy initialization, excluding the NCCL instantiation time.\nYet, these lazy initialization processes are difficult to identify or predict they occur across different programming layers, exhibit varying behaviors, and depend heavily on the underlying libraries. The only practical way to uncover them is to run a real training iteration. However, running an iteration is not a single-handed task-it requires other existing machines to spare valuable GPU cycles and incurs additional synchronization costs to manage the coordination process."}, {"title": "5.2 Overlapped Lazy Init within Sandbox", "content": "To enable the joiner to run an actual training iteration independently and achieve the goal of minimizing training downtime, we introduce sandbox lazy initialization. This approach overlaps and triggers lazy initialization during ongoing training, avoiding interaction with existing machines and preventing degradation in ongoing training performance.\nRunning a successful training iteration independently is challenging, as any training iteration execution requires: (1) valid training states to ensure that code execution aligns with user-defined paths-using fake training states may result in NaN tensors or assertion failures in the training scripts-and (2) communication operations during the iteration, as even a barrier communication call requires others to respond.\nOur solution for sandbox-based lazy initialization addresses these two challenges and is illustrated in Figure 5. When the training job is launched, we intercept NCCL collective calls on every GPU and record the tensors during the first iteration. These recorded tensors are stored alongside the checkpoint data in the remote storage. The recording occurs only during the first iteration; afterward, the training proceeds as normal, and the recording interception hook is removed.\nWhen a migration occurs, all new joining machines are placed into a sandbox, isolating them from the existing hosts. The initial state of model parameters, optimizer states, and the recorded tensors are loaded into the sandbox. During the first iteration of the new joiner (e.g., Joiner 0 and 1), we intercept communication calls intended for existing machines and emulate the recorded tensors to directly respond to these calls. For barrier calls, we ignore them and return immediately, allowing the process to proceed seamlessly. This approach enables the joiner to complete the lazy initialization process individually because (1) the emulated data is valid, as it was recorded during the first iteration when training began, and (2) all communication is handled through emulation, avoiding any halting issues.\nAfter the warmup is complete, we replace the Joiner 0 and 1 with the original Leaver 0 and 1 at the end of iteration n-1, enabling a machine transition with no lazy initialization required after the migration is finished."}, {"title": "5.3 What to Record; What to Emulate", "content": "Recording and emulating tensors incur overhead in both storage usage and tensor loading time. If all communications are recorded, the storage requirements can become substantial (e.g., reaching TB levels for a 39.1B model), leading to increased storage pressure and significant I/O overhead.\nFortunately, not all communications need to be recorded and emulated. During a training iteration, two types of communication take place: intra-machine communication and inter-machine communication. For intra-machine communication, these connections are already established and operational during the CCG first stage (\u00a74), allowing the new joining machine to use intra-machine connections during the first iteration directly. This eliminates the need to store communication tensors for intra-machine connections. Additionally, as illustrated in the lower part of Figure 5, only the communication touching the edges of the sandbox in the training graph relevant to the new joiner needs to be loaded into the sandbox and replay. This design is particularly beneficial when multiple machines are migrated together as a batch (e.g., an entire PP group). Combining these, the tensor storage and loading requirements for a 39.1B model are reduced to approximately 300GB.\nBeyond that, lazy initialization only requires the emulated data to be valid. Certain data, such as gradients, can be set to zero during warmup, as gradients primarily indicate the direction of training and do not affect the correctness of the training process at this stage. This approach can further push tensor emulation overhead close to zero although this trade-off introduces the risk of warmup failures due to the use of fake (zero) values, TrainMover is designed to fall back and discard the lazy initialization entirely if such failures occur, ensuring robustness. In our experiments, even when tensors are not recorded at all and are instead emulated with zero values, Megatron-LM training continues for over 100 iterations without issues or halting."}, {"title": "6 TrainMover Workflow", "content": "We describe TrainMover's workflow, which combines collective communication group replacement (\u00a74) and sandbox-based lazy initialization (\u00a75) to minimize downtime and achieve memory-overhead-free migration, in Section \u00a76.1. Additionally, we demonstrate how TrainMover handles unexpected failures (\u00a76.2) and leverages pre-heated machines to reduce expected downtime (\u00a76.3)."}, {"title": "6.1 Migration Lifecycle", "content": "The entire migration process consists of two phases: the overlap phase and the freeze phase, as depicted in Figure 6 with orangle and red colors. Migration begins with the overlap phase, during which the migration coordinator and leaver continue the training process while simultaneously assisting the joiner in handling live migration tasks in the background. The freeze phase follows the overlap phase, requiring the foreground training process to pause temporarily to complete procedures that cannot be overlapped. This pause contributes to TrainMover's downtime.\nThe overlap phase begins immediately after receiving the migration signal 1. At this stage, the migration joiner initiates the two-stage NCCL instantiation (\u00a74), during which it receives the communication collective group (CCG) metadata from the controller and sets up the first-stage CCL processes 2. These processes include establishing intra-machine communication, calculating the topology, and allocating memory 3. Subsequently, the joiner begins the sandbox lazy initialization (\u00a75), allowing it to independently warm up 4 while the coordinator and leaver continue with frontend training. This method effectively eliminates the cold-start overhead associated with adding a new machine.\nAfter the joiner completes the first-stage NCCL setup and lazy initialization, the migration transitions into the freeze phase. During this phase, the migration leaver begins transmitting training states, such as model parameters and optimizer states, to the joiner 5. Concurrently, the joiner and coordinator establish the remaining CCL inter-machine connections, replacing the original inter-machine connections with those linked to the migration joiner while ensuring zero memory overhead 6. These leaver-joiner mappings are 1-to-1, meaning the state transmission and connection establishment processes for different joiners are independent. As a result, the overhead does not scale with the training size or the number of machines being migrated. Once these steps are completed, the migration process concludes, with the leaver exiting and the joiner taking over the training role to continue training."}, {"title": "6.2 Handling Unexpected Failure", "content": "TrainMover also handles unexpected failure events. Once the failed event is identified by monitoring systems, the controller launches a replacement machine from the pool of idle machines to restart the job from scratch (an accelerated recovery using preheated machines is discussed in \u00a76.3).\nThe recovery procedure begins as depicted in Figure 7. While it largely aligns with the live migration process, there is a key difference: all tasks originally performed during the overlap phase are shifted to the freeze phase, as all remaining machines must stall and wait for the recovery process to complete before resuming training.\nThe migration joiner first establishes the initial stage of the CCL setup \u2461\u2462. Since the recovery path lies entirely within the critical path, there is no need for sandbox lazy initialization before proceeding to the second stage of CCL initialization. The CCL initialization is completed as a unified process \u2463. Subsequently, the machine begins the training state recovery procedure \u2464.\nDuring the recovery procedure, the approach varies depending on whether the data is recoverable. TrainMover checks for redundancy among the existing machines. If redundancy is available (e.g., when the distributed optimizer is disabled), the system performs a fast recovery by retrieving model parameters and optimizer states from the redundant machines. If redundancy is not available, TrainMover falls back to retrieving the data from a remote storage checkpoint. This adaptive strategy ensures robust recovery across diverse scenarios."}, {"title": "6.3 Preheat Standby Machine", "content": "All procedures originally performed in the overlap phase are shifted to the freeze phase during unexpected failures, leading to increased downtime. However, enabling tasks to be completed on standby (preheated) machines in advance can significantly reduce this downtime.\nOur two main components-two-stage CCG replacement and sandbox lazy initialization-are designed to operate independently, relying primarily on machine local information. This independence makes them well-suited for use on standby preheated machines. Specifically, the first stage of CCG replacement sets up intra-machine communication, which can be completed ahead of time, as training job patterns are fixed and inter-machine communication typically follows a predetermined ring structure. Similarly, sandbox lazy initialization only requires recorded tensors, which can be utilized immediately after the first iteration of training begins.\nStandby machines are commonly available in cloud environments or from GPU machines launched with other low-priority or preemptible jobs [8,41]. Since failures affecting multiple machines are rare\u2014and typically, only a single machine fails at a time [13,34]\u2014using a few spare machines to enhance throughput reliability is a reasonable trade-off. This approach ensures that migration procedures remain in the overlap phase without shifting to the freeze phase, thereby significantly minimizing downtime."}, {"title": "7 Implementation", "content": "TrainMover is implemented in C and Python, consisting of two main components: the training node and the controller. The code will be open-sourced after publication.\nTraining Node. TrainMover 's training node builds on Megatron-LM, with modifications to Megatron-LM, PyTorch, and NCCL. In Megatron-LM, we added a backend agent to manage migration signals and maintain NCCL ordering during overlapping training and migration, preventing blocking or interference. PyTorch's c10d layer was updated to support multiple global NCCL groups, enabling flexible setup and management. We extended a two-step initialization API and introduced a TrainMover intercept layer to aid the warmup process by recording, replaying, or bypassing tensors, depending on the lazy initialization mode. Additionally, new APIs allow fine-grained NCCL control, including inheriting or replacing channels from existing communicators.\nController. The controller assigns roles, synchronizes migrations, and detects node failures. Migration agents establish channels with the controller at startup. During migration, the controller helps the joiner, leaver, and coordinator set up CCGs, ensuring proper ordering and avoiding NCCL deadlocks from dependency conflicts."}, {"title": "8 Evaluation", "content": "Our experiments show that TrainMover reduces downtime by up to 16x compared to baselines across model scales and parallelism settings, maintaining stable performance across migration scales (\u00a78.2). It outperforms in handling stragglers, rebalancing, and failure recovery, improving training efficiency by up to 15% (\u00a78.3). Detailed breakdowns reveal zero memory overhead and the benefits of each design component (\u00a78.4)."}, {"title": "8.1 Experiment Setup", "content": "Our testbed consists of 4 GPU machines", "approaches": "nMegatron-LM Per-iteration: A per-iteration checkpointing system that assumes checkpoint saving is cost-free and can always be overlapped within a single iteration [15", "38": ".", "Save-and-Restart": "A more practical baseline. Before shutdown, training stops and waits for the checkpoint to be saved. Subsequently, the reboot process begins, followed by loading the checkpoint and resuming training. Since the checkpoint is saved prior to shutdown, we also assume no progress is lost in this system. Checkpointing with different frequencies is explored in the experiments (\u00a78.3).\nWe also compare TrainMover with two state"}]}