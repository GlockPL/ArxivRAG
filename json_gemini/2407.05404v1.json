{"title": "iSign: A Benchmark for Indian Sign Language Processing", "authors": ["Abhinav Joshi", "Romit Mohanty", "Mounika Kanakanti", "Andesha Mangla", "Sudeep Choudhary", "Monali Barbate", "Ashutosh Modi"], "abstract": "Indian Sign Language has limited resources for developing machine learning and data-driven approaches for automated language processing. Though text/audio-based language processing techniques have shown colossal research interest and tremendous improvements in the last few years, Sign Languages still need to catch up due to the need for more resources. To bridge this gap, in this work, we propose iSign: a benchmark for Indian Sign Language (ISL) Processing. We make three primary contributions to this work. First, we release one of the largest ISL-English datasets with more than 118k video-sentence/phrase pairs. To the best of our knowledge, it is the largest sign language dataset available for ISL. Second, we propose multiple NLP-specific tasks (including Sign-Video2Text, SignPose2Text, Text2Pose, Word Prediction, and Sign Semantics) and benchmark them with the baseline models for easier access to the research community. Third, we provide detailed insights into the proposed benchmarks with a few linguistic insights into the workings of ISL. We streamline the evaluation of Sign Language processing, addressing the gaps in the NLP research community for Sign Languages. We release the dataset, tasks, and models via the following website: https://exploration-lab.github.io/iSign/.", "sections": [{"title": "1 Introduction", "content": "As per the WHO estimate, about 63 million people belong to the Deaf and Hard of Hearing (DHH) community in India (WHO, 2016; Varshney, 2016). Consequently, Indian Sign Language (ISL) is widely used in the Indian subcontinent. Moreover, according to Ethnologue (2022) (a reference publication documenting information about living languages of the world), ISL is the world's most widely used sign language. However, there is a considerable deficit of sign language interpreters, e.g., according to the Government of India organization Indian Sign Language Research and Training Center (ISLRTC) (https://islrtc.nic.in/), there are only 300 certified sign language interpreters in India. NLP technologies can help in this case.\nSimilar to spoken languages, sign languages are region-specific, for example, people in North America use American Sign Language (ASL), and people in Germany use Deutsche Geb\u00e4rdensprache (DGS). Considerable efforts have been made to develop technologies for automatically processing sign language in other countries. However, when it comes to ISL, very limited technological advancements have been made; for example, there is a lack of standard benchmarks for ISL, resulting in low development and a lack of comparison of Machine Learning (ML) based solutions for ISL, e.g., word recognition, translation, generation, etc. In contrast, relatively speaking, other sign languages (e.g., American Sign Language (ASL), Deutsche Geb\u00e4rdensprache (DGS)) have a sufficient number of annotated resources for data-driven approaches (Table 2). Natural Language Processing (NLP) has made rapid progress in the last few years (Min et al., 2021). Most of these approaches have targeted textual datasets. Easy access to textual datasets and leaderboards in languages like English has facilitated the development, reliability, and standardization of experimentation on several tasks (Wang et al., 2019). However, there has been limited progress in visual modality-based languages, like sign languages (also referred to as signed languages: https://en.wikipedia.org/wiki/Sign_language), due to the limited availability of large-scale datasets. Moreover, from a modeling perspective, sign languages are data-hungry due to the complex relationship between different entities in visual modalities like signs, gestures, finger-spelling, and facial expressions. In this paper, to address the lack of a large-scale dataset for ISL processing and to promote the development of sign language processing techniques, we propose iSign. In a nutshell, we make the following contributions:\n\u2022 We introduce iSign, a new benchmark for Indian Sign Language processing. Figure 1 provides an overview and design philosophy of the benchmark (inspired from (Gehrmann et al., 2021)).\n\u2022 We create a dataset with 118, 228 ISL-English video-sentence/phrase pairs. To the best of our knowledge, this is the largest dataset for ISL.\n\u2022 iSign includes 3 standard sign language processing tasks: SignVideo2Text Translation, SignPose2Text translation, Text2Sign Translation, Sign/Gloss Recognition. Two additional tasks of Sign Presence Detection and Sign Semantic Similarity Prediction are introduced. The two additional tasks are added to encourage representation learning and contextualized learning in Signed Languages. We develop baseline models and report results for each of the task. We release the data, tasks, and baseline models (https:// exploration-lab.github.io/iSign/).\n\u2022 We conduct a detailed analysis of the ISL dataset and analyze how it differs from spoken languages. We provide linguistic insights into the functioning of ISL, covering various aspects like structural differences, the significance of non-manual markers, the use of space, the use of fingerspelling and co-reference, and role shifts. We hope that the detailed analysis will open up a new set of computational challenges from the linguistic perspective of ISL and further encourage various research directions."}, {"title": "2 Related Work", "content": "Recently, the research community has been actively interested in developing tools and techniques for processing sign languages. Since sign languages contain both visual, gestural, and language modalities, both the vision (Li et al., 2020a) and natural language (Yin et al., 2021) research communities have developed techniques. Several tasks for sign language processing have been proposed, for example, sign language detection (Moryossef et al., 2020), identification (Monteiro et al., 2016), segmentation (Bull et al., 2020), recognition (gloss detection) (Imashev et al., 2020; Sincan and Keles, 2020), generation (Saunders et al., 2020c,b; Xiao et al., 2020; Rastgoo et al., 2022), and translation (Jiang et al., 2023; M\u00fcller et al., 2022; Muller et al., 2022; Moryossef et al., 2021; Yin and Read, 2020a,b; Camgoz et al., 2018b, 2020).\nIsolated Sign Language Recognition/Gloss Recognition: Many benchmarks have been proposed for gloss recognition (\u00a74) in sign languages other than ISL (Mesch and Wallin, 2012; Fenlon et al., 2015; Gutierrez-Sigut et al., 2016; Martinez et al., 2002b; Zahedi et al., 2005; Efthimiou and Fotinea, 2007; Tavella et al., 2022). There are very few datasets for ISL like Rekha et al. (2011); Nandy et al. (2010); Kishore and Kumar (2012); Selvaraj et al. (2022), INCLUDE dataset (Sridhar et al., 2020), ISL-CSLRT dataset (Elakkiya and Natarajan, 2021), CISLR (Joshi et al., 2022), and ISLTranslate (Joshi et al., 2023). Table 1) provides a comparison with other isolated sign language recognition datasets.\nSign Language Translation Datasets: Various datasets (Yin et al., 2021) for sign language translation have been proposed in recent years for dif-"}, {"title": "3 iSign Benchmark", "content": "Dataset Creation: For creating iSign, we primarily use three publicly available and authentic resources on YouTube: ISLRTC videos\u00b9 (a Government of Indian initiative), ISH News (News channel in ISL),2 and phrases from DEF (Deaf Enabled Foundation, a non-profit working for the DHH community).3 These YouTube channels provide permissions to scrape videos and use them for research. Each of these videos contains a single signer communicating information (educational content or news about current affairs) in ISL and a corresponding transcript in English. The videos are pre-processed and split to obtain video-sentence"}, {"title": "4 iSign Tasks", "content": "We propose various tasks in iSign to evaluate and compare different models developed for ISL processing. The tasks are described below.\nTask 1) ISL-to-English Translation: It is a standard task of translating a sign (source) language to a spoken (target; in text form) language. As done in previous work, we use standard neural machine translation metrics to benchmark the baseline models on this task, including BLEU, METEOR, and ROUGE-L. Input (modality) to the translation system is a video in the form of a sequence of RGB images or pose-based features. There is a significant difference in performance based on the input modality. Hence, we add two sub-tasks under this task to facilitate the development and comparison of various approaches: ISLVideo-to-English Translation and ISLPose-to-English Translation. The former uses image-based features as input (Camgoz et al., 2020; Shi et al., 2022; Chen et al., 2022; Cheng et al., 2023), and the latter uses pose-based features in the form of body key points (Uthus et al., 2023; Selvaraj et al., 2022). Image-based approaches have shown promising results for sign language translation tasks. However, image-based architectures are compute-heavy and require more time for inference. Moreover, regarding large-scale application perspective, including images may result in signer-based biases creeping into the model learning. In contrast, extracting body pose features is fast and easy on edge devices; hence, pose-based approaches are more practical.\nTask 2) English-to-ISLPose Generation: The goal of this task is to transform textual input into a sequence of body poses that correspond to the sign language representation of the input sentence (Saunders et al., 2020d). Hence, this task aims to generate a sign language video. The generated translations are evaluated using the Dynamic Time Warping (DTW) metric (M\u00fcller, 2007). The DTW algorithm measures the alignment between the generated pose sequence and the ground truth, allowing us to assess how well the generated poses match the expected sign language representation.\nTask 3) Word/Gloss Recognition (Isolated Sign Recognition): Given a video of a signer (performing gestures and actions), the task is to predict the corresponding gloss label (word). We follow an existing work, CISLR (Joshi et al., 2022), which contains a low number of average videos per word (1.5 videos per word), and formulate the gloss recognition task as a one-shot learning task. Overall, the CISLR task contains 4765 sign video samples, which act as prototypes, and the task is to classify the remaining 2285 videos into one of the 4765 categories. We consider the standard metric of Top-1, Top-5, and Top-10 classification accuracy scores to evaluate this task.\nTask 4) Word Presence Prediction: To capture the quality of sign representations learned by algorithms, we define a new task of Word Presence Prediction. Given a pair of a word (as a query) and a sentence (as a candidate) as two signed ISL videos, the task is to predict if the query word is present (used) in the signed sentence. For similarity comparison, we consider the cosine similarity of the representations obtained for the pair of ISL videos. We evaluate the performance over this task using the standard classification accuracy, i.e., if the learned representations are able to predict the word presence given a query and a candidate pair. Since this task can also be treated as a retrieval task, we also consider Top Rank (Avg.) by ranking the entire pool of candidates for a particular query. Note that the better the rank, the better the representations learned by the model. (more details in the App. C)\nTask 5) Semantic Similarity Prediction: Given a pair of a word (as a query) and a sentence (as a candidate) as two signed ISL videos, we propose a new task of predicting the semantic similarity of videos. We select the ISL description videos corresponding to an ISL word as the candidate. For example, a sample for this task will contain an ISL video for the word \"revision,\u201d the corresponding ISL sen-"}, {"title": "5 Models, Experiments and Results", "content": "Baseline Models: We experimented with various models for the proposed task as described next (model training details provided in App. B).\n1) ISL-to-English Translation: For this task, we follow Camgoz et al. (2020) and validate the performance for both the sub-tasks. For ISLVideo-to-English Translation, we use spatial embeddings extracted from pre-trained CNNs as input for our model. For ISLPose-to-English Translation, we follow Saunders et al. (2020d) and create a sequence of poses to act as input to the SLT (Sign Language Transformer) model (Camgoz et al., 2020). For pose key points, we use the Mediapipe pose estimation pipeline (MediaPipe, 2023).\n2) English-to-ISLPose Generation: We utilize a transformer-based architecture as introduced in Saunders et al. (2020d) to generate body key points corresponding to the textual input.\n3) Word/Gloss Recognition: For this task, we follow CISLR (Joshi et al., 2022) as the baseline. CISLR uses the state-of-the-art model (Inception3D (I3D) (Carreira and Zisserman, 2017)) on the WLASL dataset (Li et al., 2020b) and trains it on 2000 classes. Further, the penultimate layer of the trained model is used to generate features corresponding to the prototype videos in the dataset, and each test sample video is assigned the gloss corresponding to the nearest prototype using cosine similarity between the obtained features.\n4) Word Presence Prediction: As the motivation of this task is to validate the representations learned by a neural architecture, we consider a pre-trained I3D, trained on the Human Kinetics Dataset (Kay et al., 2017), as a baseline and report the findings.\n5) Semantic Similarity Prediction: We use average cosine similarity scores between the sign representations of the word and their corresponding description to measure the similarity. The representations are obtained using a pre-trained I3D network (Carreira and Zisserman, 2017).\nResults: Table 6 shows the baseline results for all the tasks in iSign. The BLEU scores for Sign-to-Text translation (Task 1) are a bit low, though we follow the previous baselines (Camgoz et al., 2020) that perform well (with a BLEU score of ~ 20) on the RWTH Phoenix 2014T dataset (Camgoz et al., 2018a). We speculate a high gap between the two datasets to be the primary reason for the observed difference. For Task 2, we obtained a DTW score of 22.69, pointing towards high variation in generated and ground truth poses. For Task 3, we found the I3D features used by Joshi et al. (2022) to be performing better than the representations extracted via the models trained for Task 1. For Task 4 and 5, we compute the Top 5% accuracy by performing a one vs all prediction, i.e., a query's feature representation will have the highest similarity with the corresponding candidate's feature representation. We also report the average rank assigned via similarity corresponding to all the candidates (i.e., the lower the rank, the better the learned representations). For Task 4, we found the T5base + I3D features getting Top-5% Acc. of 52% and average rank as 193 out of 1523 candidates (also check App. Table 9). Overall, all the baseline performance points towards a huge scope of future developments in ISL processing (also see Limitations section).\nPoor Performance of Existing Baseline Models: The current baseline models for several tasks"}, {"title": "6 ISL Linguistics and Computational Challenges", "content": "Sign language functioning differs from spoken languages by a significant margin. In this section, we highlight some ISL-specific features that might be helpful in the development of dedicated sign-language-specific neural architecture and facilitate understanding of ISL in the NLP research community. We created this list of insights after discussing it with a Professor of Indian Sign Language linguistics, Dr. Andesha Mangla, who is also one of the co-authors of this paper.\nStructural Differences: ISL is a form of visual language that consists of signs, gestures, finger-spelling, and facial expressions going in parallel to communicate a sentence, making it quite different from spoken language in the structural form (Sinha, 2017). At a rudimentary level, the building blocks of sign and spoken languages differ. In spoken languages, a combination of sounds results in the formation of words, while in sign languages, a mixture of manual and non-manual parameters forms words. Moreover, the usage of visual-spatial and manual modality in sign languages allows the production of various concepts in parallel. For example, using physical space for multiple purposes, using head, eyes, and body to represent different entities, actions, etc., and using non-manual expressions for various concepts. In terms of linguistics, iconicity plays a more significant role in the production and perception of sentences when compared to spoken languages (Zeshan, 2000; Sinha, 2017; Brentari, 2019).\nSignificance of Non-Manual Markers: In ISL, non-manual markers like facial expressions, body language, etc., play a vital role in giving semantics to the produced sentences, both at lexical as well as grammatical levels (Sinha, 2017). For example, the word \"HAPPY\" is signed with a smiling face, whereas the word \u201cSAD\u201d is signed with a sad facial expression. The order of non-manual markers goes in parallel with the manual markers, giving the sentence meaning. For example, the same sentence signed with a forward head tilt and wide-open eyes will transform the statement sentence into a yes-no question. Moreover, non-manual markers are also used in the production of complex sentences, such as conditional sentences. The various use cases and parallel nature of non-manual markers in sign language production make it more challenging for a sequential language-based model to adapt to ISL.\nUse of Space in ISL: Physical signing space is crucial in making production and communication more efficient in ISL (Sinha, 2017). Physical singing space provides a medium for assigning various reference points required in a specific sentence, like referring to designated locations for people, places, or any topic/subject. While communicating a sentence, the referents in a narration are assigned various locations in the signing space, which are then referred to in the conversation using the pointing sign toward the same space. Space provides a medium for references and grounds the language to actual space. For example, the word \"AEROPLANE\u201d will be signed in the upper portion of the signing space, whereas the word \u201cCAR\u201d will be signed in the lower portion of the signing space. As the references are created specific to sentences for each conversation, the linguistic structure of the language becomes more complex, and the conversation becomes challenging to separate into independent sentences.\nFingerspelling and Co-reference: In ISL, names for characters, places, etc., are produced in various ways. For introducing a new name in the conversation, the name is fingerspelled and simultaneously assigned a short sign consisting of the initials, which refers to the same name in future sentences (Sinha, 2017). For example, a girl character named \"Neha\" is introduced in the conversation by signing \u201cFEMALE+CHILD (= girl) NAME N-E-H-A (fingerspelled) SIGN SHORT FEMALE+N\u201d at the start of the conversation. Later on, the name Neha is co-referenced with the assigned short sign \u201cFEMALE+N.\u201d Note that assigning a short sign is not unique and varies. For example, the same name, Neha, can be given a short sign using visual features and physical characteristics. For example, if there is a picture available of the child Neha, in which she is wearing a pair of spectacles, the sign introduction can look like \u201cFEMALE+CHILD (= girl) NAME N-E-H-A (fingerspelled) SIGN SHORT FEMALE+SPECS,\u201d where \u201cFEMALE+SPECS\u201d becomes the assigned short sign. Moreover, other variations can exist, combining the first and second examples to create a short sign \"N+SPECS,\u201d N coming from the name, and SPECS for the visual feature of spectacles. For co-referencing, ISL also makes use of signing space (Sinha, 2017). For example, a character, place, etc., is given a location in the signing space during the introduction, and co-references are then made by pointing toward that location in space. For introducing new concepts, things, actions, etc., if there is no available sign, the signer describes the concept by fingerspelling it.\nRole Shifts in ISL: The visual modality provides signed languages with multiple ways of speaking\""}, {"title": "7 Conclusion and Future Directions", "content": "We propose iSign, a benchmark for Indian Sign Language Processing, to bridge the gap between developments in spoken languages and signed languages and provide a standardization medium for accelerating the improvement. We release one of the largest available datasets for ISL with 118k video-sentence/phrase pair sentences to facilitate research. We believe the released dataset will not only help improve translation models but also open up various ways for advancing natural language modeling techniques like contextualized representation learning, mask language modeling, capturing semantic similarities, etc., and encourage research in the NLP community. As a part of the benchmark, we incorporate various sign language tasks. In the future, we plan to grow the dataset by adding more samples. We also plan to add more ISL-based tasks (e.g., Sign Sentence Retrieval) to the benchmark. Additionally, we provide some linguistic insights into the functioning of ISL and discuss the open challenges in sign language processing. Accordingly, we plan to incorporate linguistic priors into the models."}, {"title": "Acknowledgements", "content": "We would like to thank anonymous reviewers for their insightful comments. We would like to thank the Indian Sign Language Research And Training Center (ILSRTC) team for helping us validate the quality of the curated translation dataset. We would also like to extend our immense gratitude towards the ISLRTC members for providing us full support in sharing the ISL content creation process. Finally, we thank the ISL content creators on YouTube (ISH-TV and DEF), who gave us permission to use the videos and thus made this work possible."}, {"title": "Limitations", "content": "The large number of ISL-English sentence pairs in the initial version of the dataset makes it challenging to validate. Though we provide a small-scale validation with an ISL instructor's help, the entire corpus's validation is tedious, and it is infeasible to look into minute aspects of ISL-English translations in an initial version. In the future, we plan to extend the validation task and add a gold translation set with multiple references provided for a set of 5K ISL sentences. For an initial version of the benchmark, it is imperative to consider some possible technical constraints in the dataset. We believe addressing these constraints is a long-term goal, and mentioning them in detail will open up various directions for future work on analyzing and improving the benchmark.\nAlignment in ISL-English Pairs: As the iSign translation dataset consists of sentence-level videos clipped from a longer video using multiple strategies, pauses in the available audio signal for ISLRTC videos, frame pattern heuristics for DEF videos, and available English caption timestamps for ISH videos, there are multiple ways in which the alignment between ISL signing sentence and corresponding English Sentence is disrupted. For example, in ISLRTC videos, though the audio in the background is aligned with the corresponding signs in the video, it could happen in a few cases that the audio was fast compared to the corresponding sign representation and may miss a few words at the beginning or the end of the sentence.\nCo-referencing in ISL: As mentioned in Section 6, ISL linguistics involves assigning various short signs for names, places, etc., with various spatial reference points to refer to places defined for a specific conversation, making the context an essential feature for a complete translation of the content. As the translation dataset in iSign was created by segmenting a longer video into shorter segments representing a sentence, there is a high possibility that the names in the sentences are introduced in different sentences and assigned a short sign, which may be difficult to refer to in the later sentences. We consider this a major limitation of the created ISL-English pair dataset, as translating the same names would be difficult without a given reference to the created short sign or allocated space, and the independently made translations would result in a lower score in evaluation metrics like BLEU. One way of addressing this challenge would be to perform a task similar to NER on the ISL videos. However, the unavailability of resources and the scarcity of certified signers make ISL video annotation challenging.\nPresence of Role Shifts: As a major portion of the created dataset comes from educational content created by ISLRTC, a lot of material contains stories with fictional characters. In stories, there exists a high possibility of Role Shifts when producing sentences spoken by a character. A similar example in spoken storytelling would be the change in voice to articulate a character's voice, for example, the use of a squeaky voice for enacting the sentences of a small mouse. Since these Role Shifts are produced via non-manual markers, they would result in slight gesture variations for the same sentence, making the translated sentence more challenging to predict. Lastly, in this paper, we do not compare ISL with other sign languages (like ASL and DGS); to the best of our knowledge, no such previous study exists, and performing such a study would require a considerable amount of effort in terms of humans having sign language expertise. In the future, we plan to explore such a study, as this can help with cross-model transfer, i.e., adapt models for rich resource sign languages to low resource sign languages.\nEthical Considerations\nWe create a dataset from publicly available resources without violating copyright. We are not aware of any direct ethical concerns regarding our dataset. Moreover, the dataset involves people of Indian origin and is created mainly for Indian Sign Language translation. The ISL-specific insights are obtained from a professor working primarily on ISL linguistics. The annotations are done by the ISL professor and their team on a pro bono basis. Please note we do not endorse the use of the benchmark data for non-research (commercial and real-life) applications, and the primary motivation for creating the iSign benchmark is to consolidate all the research happening in parallel for ISL. Moreover, we believe providing a platform by maintaining a common leaderboard for multiple tasks will advance the field with more transparency and reproducibility. Sign language datasets include visual modality with the inclusion of facial expression and non-manual markers being an integral part of language, which does pose privacy challenges. Though the videos in the dataset are in the public domain, the models trained on the dataset may contain signer-specific features and may not generalize to real-world usage."}, {"title": "Appendix", "content": "We plan to release the dataset along with a benchmark webpage to maintain the leaderboard of existing approaches to the proposed tasks.\nTo create the translation dataset with ISL-English video-sentence/phrase pairs, we primarily use three publicly available resources: ISLRTC videos,4 ISH News (News channel in ISL),5 and phrases from DEF (Deaf Enabled Foundation).6. These YouTube channels provide permission to scrape videos and use them for research. The scraped videos contained a signer communicating a sentence to the spectators. We obtain a list of 114, 3373, and 651 videos from ISLRTC, ISH, and DEF, respectively. These videos generally range from 15-20 minutes and contain about 20-25 number of sentences on average from various sources. To create a video-sentence/phrase level pair dataset, we further split the long videos at the sentence level. For ISH videos, the source videos provide an English caption aligned with the video where the same content is present in the English language as subtitles. We make use of these timestamps to split the videos into sign sentences and combine them with respective captions for English translation. For ISLRTC videos, since the subtitles were not present, we used the available English audio to generate the respective translations. At the end of each sentence, there is a pause in audio and the signer's gesture; we clip the videos using an audio heuristic to create sentence-level segmentation of the signed video. For generating the respective transcripts, we use a speech-to-text model (Whisper (Radford et al., 2022)) for generating the text. Note that the speech-to-text model might not be 100% accurate, resulting in some noisy texts for a few audio clips. Hence, we further clean the generated text via manual inspection by listening to the audio where the sentences are noisy and make less contextual sense. DEF videos provide a word-of-the-day format where a word is communicated at the start of the video with its explanation in the middle and some example sentences where the word is being used at the end. We clip all these sections to generate 3 sets of clippings from a video, namely words, word descriptions, and examples. Note that a few videos have multiple example sentences available for the same word. We segment each example sentence as a different entry in the created dataset.\nThe videos (e.g., Fig. 4) contain the pictures corresponding book pages. We crop the signer out of the video by considering the face location of the first frame as the reference point and removing the remaining background in the videos. Further, the cropped videos are used to extract pose key points (Figure 5 and Figure 6).\nWe had a total of 118, 228 pairs of video-translation. We used T5-tokenizer (Huggingface, 2022) for tokenizing the sentences, there were 15 tokens on an average for each translation text. For the words, we used \" \"(space) as the separating character. More details about the distribution can be found in Figure 3 and Table 8.\nWe asked three certified ISL instructors to translate and validate a random subset from the dataset (discussed in Section 3). One of the instructors is an assistant professor of sign language linguistics. All the instructors are employed with ISLRTC, the organization involved in creating the sign language content; however, the instructors did not participate in videos present in the translation dataset. The instructors performed the validation voluntarily. It took the instructor about 3 hours to validate 100 sentences. They generated the English translations by looking at the video.\nWe use the Mediapipe pose estimation pipeline.7 For the choice of holistic key points, we follow Selvaraj et al. (2022), which returns the 3D coordinates of 75 key points (excluding the face mesh). Figure 5 and Figure 6 shows an example of the obtained 75 keypoints from the mediapipe pipeline. Further, we normalize every frame's key points by placing the midpoint of shoulder key points to the center and scaling the key points using the distance between the nose key point and the shoulders midpoint.\nFor Task-1 and Task-2, we use a split of 80%, 10%, and 10% for train, validation, and test set, respectively. For Task-3, we follow CISLR (Joshi et al., 2022) and use 4765 samples as prototypes and the remaining 2285 videos as test sets. For Task-4 and Task-5, we take 594 and 1525 positive pairs, respectively to report the results.\nWe follow the code base of SLT (Camgoz et al., 2020) to train and develop the proposed SLT-based pose-to-text architecture by modifying the input features to be signpose sequences generated by the mediapipe. The model architecture is a transformer-based encoderdecoder consisting of 3 transformer layers each for both encoder and decoder. We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0001, $\\beta$ = (0.9, 0.999) and weight decay of 0.0001 for training the proposed baseline with a batch size of 32. The architecture has 14,337,264 trainable parameters . For the generation task, we follow the code base released by (Saunders et al., 2020d). The model architecture is a transformer-based encoder-decoder consisting of 2 transformer layers for both the encoder and decoder. We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.001 for training the proposed baseline with a batch size of 8. The architecture contains 3,67,68,768 trainable parameters. We perform all the experiments using the NVIDIA A40 GPU machine.\nWe follow the T5 based model for sign language translation. We use the AdamW optimiser with a learning rate of 0.0001, and weight_decay=1e - 6. The batch size is 8 for a model with t5-large as backbone and 32 for a model with t5-small and base as backbones. We did transfer learning by freezing only the layers of T5 and finetune the embedding only upto 40 epochs, after that we train the whole model with reduced learning rate.\nIn this section, we discuss some of the limitations of the current evaluation criteria for translation tasks and point toward the scope of building signlanguage-specific evaluation methods in the future. We further provide the evaluation metric details of the additional tasks of Word Presence Prediction and Semantic Similarity Predictions.\nIn the current version of the benchmark, we use the standard translation metrics, which have shown effective usage and interpretation when working with written textual languages to textual language translations. However, in sign languages, the high use of co-referencing (explained in Section 6) makes the translations more challenging to divide into independent sentences. Current evaluation metrics like BLEU and ROUGE depend on the textual translation match between the reference-candidate pair translations. By taking a geometrical average over multiple values of n, BLEU scores give a measure to compute textual similarities between the reference-candidate pair translations. However, as the translated sentences depend on the co-reference symbol/gesture assigned in the past sentences of conversation, the exact independent generation for a sentence in a conversation becomes impossible. One way to overcome this issue is to perform a NER (Named-entity recognition) over the text translations to remove all the references to names and assign the same token for training the neural architectures. However, the co-reference usage for various things and concepts still needs to be solved. In the future, we plan to study and understand the co-reference feature of sign language in more detail, examining if a clear distinction or pattern exists between the usage of fingerspelling, new concepts, and location assigning for later use and how various tags can be introduced in the respective textual translations to make the sentences independent, facilitating the current neural translation pipelines working on language sentence pairs.\nSign language generation is a more challenging task in terms of evaluation. In the current version of the benchmark, we use DTW scores to evaluate the quality of generated poses. Dynamic Time Warping, or DTW, tries to capture the similarity between two temporal sequences, takes care of the varying speed of the temporal sequences, and has shown practical usage in speech processing. However, for Indian Sign Language, the significant use of non-manual markers like facial expressions, body language, etc., makes the DTW a less effective metric for capturing the quality of translations. One way of dealing with these would be to de-"}]}