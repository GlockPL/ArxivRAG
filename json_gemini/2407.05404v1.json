{"title": "iSign: A Benchmark for Indian Sign Language Processing", "authors": ["Abhinav Joshi", "Romit Mohanty", "Mounika Kanakanti", "Andesha Mangla", "Sudeep Choudhary", "Monali Barbate", "Ashutosh Modi"], "abstract": "Indian Sign Language has limited resources for\ndeveloping machine learning and data-driven\napproaches for automated language processing.\nThough text/audio-based language processing\ntechniques have shown colossal research inter-\nest and tremendous improvements in the last\nfew years, Sign Languages still need to catch\nup due to the need for more resources. To\nbridge this gap, in this work, we propose iSign:\na benchmark for Indian Sign Language (ISL)\nProcessing. We make three primary contribu-\ntions to this work. First, we release one of the\nlargest ISL-English datasets with more than\n118k video-sentence/phrase pairs. To the best\nof our knowledge, it is the largest sign language\ndataset available for ISL. Second, we propose\nmultiple NLP-specific tasks (including Sign-\nVideo2Text, SignPose2Text, Text2Pose, Word\nPrediction, and Sign Semantics) and bench-\nmark them with the baseline models for eas-\nier access to the research community. Third,\nwe provide detailed insights into the proposed\nbenchmarks with a few linguistic insights into\nthe workings of ISL. We streamline the evalua-\ntion of Sign Language processing, addressing\nthe gaps in the NLP research community for\nSign Languages. We release the dataset, tasks,\nand models via the following website: https:\n//exploration-lab.github.io/iSign/.", "sections": [{"title": "1 Introduction", "content": "As per the WHO estimate, about 63 million people\nbelong to the Deaf and Hard of Hearing (DHH)\ncommunity in India (WHO, 2016; Varshney, 2016).\nConsequently, Indian Sign Language (ISL) is\nwidely used in the Indian subcontinent. More-\nover, according to Ethnologue (2022) (a reference\npublication documenting information about living\nlanguages of the world), ISL is the world's most\nwidely used sign language. However, there is a con-\nsiderable deficit of sign language interpreters, e.g.,\naccording to the Government of India organization"}, {"title": "2 Related Work", "content": "Recently, the research community has been actively\ninterested in developing tools and techniques for\nprocessing sign languages. Since sign languages\ncontain both visual, gestural, and language modali-\nties, both the vision (Li et al., 2020a) and natural\nlanguage (Yin et al., 2021) research communities\nhave developed techniques. Several tasks for sign\nlanguage processing have been proposed, for ex-\nample, sign language detection (Moryossef et al.,\n2020), identification (Monteiro et al., 2016), seg-\nmentation (Bull et al., 2020), recognition (gloss\ndetection) (Imashev et al., 2020; Sincan and Ke-\nles, 2020), generation (Saunders et al., 2020c,b;\nXiao et al., 2020; Rastgoo et al., 2022), and transla-\ntion (Jiang et al., 2023; M\u00fcller et al., 2022; Muller\net al., 2022; Moryossef et al., 2021; Yin and Read,\n2020a,b; Camgoz et al., 2018b, 2020).\nIsolated Sign Language Recognition/Gloss\nRecognition: Many benchmarks have been pro-\nposed for gloss recognition (\u00a74) in sign languages\nother than ISL (Mesch and Wallin, 2012; Fenlon\net al., 2015; Gutierrez-Sigut et al., 2016; Martinez\net al., 2002b; Zahedi et al., 2005; Efthimiou and\nFotinea, 2007; Tavella et al., 2022). There are\nvery few datasets for ISL like Rekha et al. (2011);\nNandy et al. (2010); Kishore and Kumar (2012);\nSelvaraj et al. (2022), INCLUDE dataset (Sridhar\net al., 2020), ISL-CSLRT dataset (Elakkiya and\nNatarajan, 2021), CISLR (Joshi et al., 2022), and\nISLTranslate (Joshi et al., 2023).\nSign Language Translation Datasets: Various\ndatasets (Yin et al., 2021) for sign language trans-\nlation have been proposed in recent years for dif-"}, {"title": "3 iSign Benchmark", "content": "Dataset Creation: For creating iSign, we primar-\nily use three publicly available and authentic re-\nsources on YouTube: ISLRTC videos\u00b9 (a Gov-\nernment of Indian initiative), ISH News (News\nchannel in ISL),2 and phrases from DEF (Deaf\nEnabled Foundation, a non-profit working for the\nDHH community).3 These YouTube channels pro-\nvide permissions to scrape videos and use them\nfor research. Each of these videos contains a sin-\ngle signer communicating information (educational\ncontent or news about current affairs) in ISL and a\ncorresponding transcript in English. The videos are\npre-processed and split to obtain video-sentence"}, {"title": "4 iSign Tasks", "content": "We propose various tasks in iSign to evaluate and\ncompare different models developed for ISL pro-\ncessing. The tasks are described below.\nTask 1) ISL-to-English Translation: It is a stan-\ndard task of translating a sign (source) language\nto a spoken (target; in text form) language. As\ndone in previous work, we use standard neural ma-\nchine translation metrics to benchmark the baseline\nmodels on this task, including BLEU, METEOR,\nand ROUGE-L. Input (modality) to the transla-\ntion system is a video in the form of a sequence\nof RGB images or pose-based features. There\nis a significant difference in performance based\non the input modality. Hence, we add two sub-\ntasks under this task to facilitate the development\nand comparison of various approaches: ISLVideo-\nto-English Translation and ISLPose-to-English\nTranslation. The former uses image-based fea-\ntures as input (Camgoz et al., 2020; Shi et al., 2022;\nChen et al., 2022; Cheng et al., 2023), and the latter\nuses pose-based features in the form of body key\npoints (Uthus et al., 2023; Selvaraj et al., 2022).\nImage-based approaches have shown promising re-\nsults for sign language translation tasks. However,\nimage-based architectures are compute-heavy and\nrequire more time for inference. Moreover, regard-\ning large-scale application perspective, including\nimages may result in signer-based biases creeping\ninto the model learning. In contrast, extracting\nbody pose features is fast and easy on edge devices;\nhence, pose-based approaches are more practical.\nTask 2) English-to-ISLPose Generation: The"}, {"title": "5 Models, Experiments and Results", "content": "Baseline Models: We experimented with various\nmodels for the proposed task as described next\n(model training details provided in App. B).\n1) ISL-to-English Translation: For this task, we fol-\nlow Camgoz et al. (2020) and validate the perfor-\nmance for both the sub-tasks. For ISLVideo-to-\nEnglish Translation, we use spatial embeddings\nextracted from pre-trained CNNs as input for our\nmodel. For ISLPose-to-English Translation, we fol-\nlow Saunders et al. (2020d) and create a sequence\nof poses to act as input to the SLT (Sign Language\nTransformer) model (Camgoz et al., 2020). For\npose key points, we use the Mediapipe pose esti-\nmation pipeline (MediaPipe, 2023).\n2) English-to-ISLPose Generation: We utilize a\ntransformer-based architecture as introduced in\nSaunders et al. (2020d) to generate body key points\ncorresponding to the textual input."}, {"title": "6 ISL Linguistics and Computational\nChallenges", "content": "Sign language functioning differs from spoken lan-\nguages by a significant margin. In this section,"}, {"title": "7 Conclusion and Future Directions", "content": "We propose iSign, a benchmark for Indian Sign\nLanguage Processing, to bridge the gap between\ndevelopments in spoken languages and signed lan-\nguages and provide a standardization medium for\naccelerating the improvement. We release one of\nthe largest available datasets for ISL with 118k\nvideo-sentence/phrase pair sentences to facilitate\nresearch. We believe the released dataset will not\nonly help improve translation models but also open\nup various ways for advancing natural language\nmodeling techniques like contextualized represen-\ntation learning, mask language modeling, capturing\nsemantic similarities, etc., and encourage research\nin the NLP community. As a part of the benchmark,\nwe incorporate various sign language tasks. In the\nfuture, we plan to grow the dataset by adding more\nsamples. We also plan to add more ISL-based tasks\n(e.g., Sign Sentence Retrieval) to the benchmark.\nAdditionally, we provide some linguistic insights\ninto the functioning of ISL and discuss the open\nchallenges in sign language processing. Accord-\ningly, we plan to incorporate linguistic priors into\nthe models."}, {"title": "Limitations", "content": "The large number of ISL-English sentence pairs in\nthe initial version of the dataset makes it challeng-\ning to validate. Though we provide a small-scale\nvalidation with an ISL instructor's help, the entire\ncorpus's validation is tedious, and it is infeasible\nto look into minute aspects of ISL-English trans-\nlations in an initial version. In the future, we plan\nto extend the validation task and add a gold trans-\nlation set with multiple references provided for a\nset of 5K ISL sentences. For an initial version of\nthe benchmark, it is imperative to consider some\npossible technical constraints in the dataset. We\nbelieve addressing these constraints is a long-term\ngoal, and mentioning them in detail will open up\nvarious directions for future work on analyzing and\nimproving the benchmark.\nAlignment in ISL-English Pairs: As the iSign\ntranslation dataset consists of sentence-level videos\nclipped from a longer video using multiple strate-\ngies, pauses in the available audio signal for\nISLRTC videos, frame pattern heuristics for DEF\nvideos, and available English caption timestamps\nfor ISH videos, there are multiple ways in which\nthe alignment between ISL signing sentence and\ncorresponding English Sentence is disrupted. For\nexample, in ISLRTC videos, though the audio in\nthe background is aligned with the corresponding\nsigns in the video, it could happen in a few cases\nthat the audio was fast compared to the correspond-\ning sign representation and may miss a few words\nat the beginning or the end of the sentence.\nCo-referencing in ISL: As mentioned in Section\n6, ISL linguistics involves assigning various short\nsigns for names, places, etc., with various spatial\nreference points to refer to places defined for a spe-\ncific conversation, making the context an essential\nfeature for a complete translation of the content.\nAs the translation dataset in iSign was created by\nsegmenting a longer video into shorter segments\nrepresenting a sentence, there is a high possibility\nthat the names in the sentences are introduced in\ndifferent sentences and assigned a short sign, which\nmay be difficult to refer to in the later sentences.\nWe consider this a major limitation of the created\nISL-English pair dataset, as translating the same\nnames would be difficult without a given reference\nto the created short sign or allocated space, and\nthe independently made translations would result\nin a lower score in evaluation metrics like BLEU.\nOne way of addressing this challenge would be to\nperform a task similar to NER on the ISL videos."}, {"title": "Appendix", "content": "We plan to release the dataset along with a bench-\nmark webpage to maintain the leaderboard of exist-\ning approaches to the proposed tasks.\nA.1 Dataset Creation Details\nTo create the translation dataset with ISL-English\nvideo-sentence/phrase pairs, we primarily use three\npublicly available resources: ISLRTC videos,4 ISH\nNews (News channel in ISL),5 and phrases from\nDEF (Deaf Enabled Foundation).6. These YouTube\nchannels provide permission to scrape videos and\nuse them for research. The scraped videos con-\ntained a signer communicating a sentence to the\nspectators. We obtain a list of 114, 3373, and 651\nvideos from ISLRTC, ISH, and DEF, respectively.\nThese videos generally range from 15-20 minutes\nand contain about 20-25 number of sentences on\naverage from various sources. To create a video-\nsentence/phrase level pair dataset, we further split\nthe long videos at the sentence level. For ISH\nvideos, the source videos provide an English cap-\ntion aligned with the video where the same content\nis present in the English language as subtitles. We\nmake use of these timestamps to split the videos\ninto sign sentences and combine them with respec-\ntive captions for English translation. For ISLRTC\nvideos, since the subtitles were not present, we\nused the available English audio to generate the\nrespective translations. At the end of each sen-\ntence, there is a pause in audio and the signer's\ngesture; we clip the videos using an audio heuristic\nto create sentence-level segmentation of the signed\nvideo. For generating the respective transcripts,\nwe use a speech-to-text model (Whisper (Radford\net al., 2022)) for generating the text. Note that\nthe speech-to-text model might not be 100% ac-\ncurate, resulting in some noisy texts for a few au-\ndio clips. Hence, we further clean the generated\ntext via manual inspection by listening to the audio\nwhere the sentences are noisy and make less contex-\ntual sense. DEF videos provide a word-of-the-day\nformat where a word is communicated at the start\nof the video with its explanation in the middle and\nsome example sentences where the word is being\nused at the end. We clip all these sections to gener-"}]}