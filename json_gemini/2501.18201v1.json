{"title": "Neural Operator based Reinforcement Learning for Control of first-order PDEs with Spatially-Varying State Delay", "authors": ["Jiaqi Hu", "Jie Qi", "Jing Zhang"], "abstract": "Control of distributed parameter systems affected by delays is a challenging task, particularly when the delays depend on spatial variables. The idea of integrating analytical control theory with learning-based control within a unified control scheme is becoming increasingly promising and advantageous. In this paper, we address the problem of controlling an unstable first-order hyperbolic PDE with spatially-varying delays by combining PDE backstepping control strategies and deep reinforcement learning (RL). To eliminate the assumption on the delay function required for the backstepping design, we propose a soft actor-critic (SAC) architecture incorporating a DeepONet to approximate the backstepping controller. The DeepONet extracts features from the backstepping controller and feeds them into the policy network. In simulations, our algorithm outperforms the baseline SAC without prior backstepping knowledge and the analytical controller.", "sections": [{"title": "1. INTRODUCTION", "content": "We consider a first-order hyperbolic PDE with spatially-varying state delay\n$\\frac{\\partial v(x,t)}{\\partial t} = - \\frac{\\partial v(x,t)}{\\partial x}+ \\int_0^1 f(x,q)v(s,t)dq + c(s)v(1, t - \\tau(x)),$\n$v(0,t) =U(t),$\n$\\upsilon(x, 0) =\\upsilon_0(x),$\n$v(x, h) =0,$\nfor $(x,t) \\in (0,1) \\times \\mathbb{R}^+$. $\\tau(x)$ is the delay dependent on the spatial argument $x$, with $ \\overline{\\tau} = \\sup_{x\\in[0,1]} \\tau(x)$.\nThe backstepping controller for this system has been designed in Zhang and Qi (2021, 2024) and its corresponding DeepONet-based controller is developed in Qi et al. (2024a). However, both controllers are limited by the assumption of slow variations in the delay i.e., $|\\tau'(x)| < 1$.\nTo remove the assumption, we propose a neural operator based Soft actor-critic (NO-SAC) architecture using a DeepONet approximating the backstepping controller and integrating it into the policy network. Therefore, this architecture utilizes the bacskstepping controller as prior knowledge and also take advantage of the adaptability and flexibility of the RL.\nClassical control methods, such as the backstepping method Krstic and Smyshlyaev (2008) and passivity-based control Nu\u00f1o et al. (2011), while theoretically precise and stability-guaranteeing, often encounter challenges like model mismatch or oversimplifications. These methods typically require stringent assumptions on the system coefficients.\nOn the other hand, data-driven methods, such as RL Schulman et al. (2017); Haarnoja et al. (2018), learn control strategies directly through interaction with the environment, thus overcoming the dependency on accurate models (Bhan et al. (2024); Yu and Zhao (2022); Mo et al. (2024)). While vanilla RL alone often suffers from slow convergence and stability issues. To address these shortcomings, recent studies begin to focus on integrating RL with prior theoretical knowledge. Several studies have utilized Lyapunov stability theory to guide the design of RL algorithms, ensuring desirable stability of policies Berkenkamp et al. (2017); Chow et al. (2018). Other approaches focus on integrating theoretical insights into the RL to improve learning efficiency and stability. Bougie and Ichise (2020) integrated prior knowledge and state similarity in the training. Quartz et al. (2024) incorporated the linear quadratic regulator gain matrix computed around the steady-state operating point into the value function to further guide the policy. Besides, leveraging expert knowledge to reduce the dimensionality of the state space and constrain the action space also enhance learning efficiency and safety Parisi et al. (2017); Song et al. (2023)."}, {"title": "2. DEEPONET LEARNING BACKSTEPPING", "content": "We introduce a two-dimensional transport PDE with spatially-varying speed to express the state delay in (1),\n$\\frac{\\partial u(x,t)}{\\partial t} = - x\\frac{\\partial v(x,t)}{\\partial x} + c(x)u(x, 0, t) + \\int_0^1 f(x, q)v(q, t)dq,$\n$v(0,t) = U(t),$\n$\\tau(x)\\frac{\\partial u(x,r,t)}{\\partial t} = \\frac{\\partial u(x,r,t)}{\\partial r}, (x,r) \\in (0,1)^2,$\n$u(x, 1, t) = v(1,t),$\n$\\upsilon(x, 0) = \\upsilon_0(x),$\n$u(x, r, 0) = u_0(x,r).$\nA strict assumption on the delay $\\tau \\in \\mathcal{D}$ is necessary for the bacsktepping control design, where\n$\\mathcal{D} = \\{\\tau \\in C^2[0, 1] : \\tau(x) > 0 \\text{ for } x \\in [0,1] \\text{ and if } |\\tau'(x)| < x, |\\tau'(x)| < 1\\}.$ \nDefine the backstepping controller designed in (Zhang and Qi (2021, 2024)) as an operator\nDefinition 1. The controller operator $\\mathcal{U} : \\mathcal{D} \\times C^1[0,1] \\times C^1([0,1]^2) \\to \\mathbb{R}$ with\n$\\mathcal{U} = \\mathcal{U}(\\tau,v, u),$\nwhere $\\tau(x)$ is the delay function dependent on $x$, $v(x,.)$ and $u(x,r,.)$ are the system state and the delayed state, respectively. $\\mathcal{U}$ is the control input.\nWe apply a DeepONet to learn the controller operator (12). Therefore, the inputs to DeepONet, as shown in Fig. 1, consist of these three components, i.e., $\\tau,x, u$. Unlike traditional neural networks that operate on finite-dimensional spaces, DeepONet is composed of a branch and trunk network structure. This architecture allows it to approximate operators and captures complex relationships in infinite-dimensional function spaces.\nTo match the domain of $(x,r)$, we discretize the 2-D domain $[0, 1]^2$ spatially with a step size of 0.05, resulting in tensor input of size $3 \\times 21 \\times 21$ for the branch network. The branch network consists of two convolutional layers (kernel size $5 \\times 5$, stride 2) and a $1152 \\times 256$ fully connected layer. The trunk network comprises two fully connected layers encoding inputs sampled on a $21 \\times 21$ grid. The outputs of two networks are combined through a Cartesian product operation, producing a feature representation of size 441.\nOnce the features of the backstepping controller is approximated by the DeepONet, we can embed the DeepONet into the SAC framework and use it to warmly start the RL-based controller."}, {"title": "3. NEURAL OPERATOR BASED REINFORCEMENT LEARNING", "content": "This section presents a RL framework that integrates the backstepping DeepONet with the SAC algorithm to control the delayed system. In this framework, pre-training DeepONet in Fig. 1 serves as feature extractor for both the actor and critic networks, as illustrated in Fig. 2. The DeepONet processes the high-dimensional vectors composed of the states from the replay buffer and the system's delay. Subsequently, the actor-critic networks are trained using the SAC algorithm.\nDue to the existence of the delay $\\tau(x)$, the dynamics of the system described by equations (5)-(10) are non-Markovian Bouteiller et al. (2020), as the next state $v(x,t + \\Delta t)$ depends on both the current state $v(x, t)$ and its past state $v(1,t - \\tau(x))$. We reformulate this system as a Markov Decision Process (MDP) in an augmented state space consisting of $v(x, t)$ and $u(x,r,t)$, as defined in (7)-(10).\nState space S: State space is defined as $\\mathcal{S} = \\{L^2[0, 1] \\times L^2[0, 1]^2\\}$. We denote $s_t = \\{v(x,t), u(x,r,t)\\} \\in \\mathcal{S}$ as the augmented state at time $t$.\nAction space $\\mathcal{A} \\subset \\mathbb{R}$: Defined as $\\mathcal{A} = [-\\overline{U},\\overline{U}]$, where $\\overline{U} = \\sup_{t \\in \\mathbb{R}^+}|U(t)|$. One can select an action $U(t) = a_t \\in \\mathcal{A}$ at each time step $t$ as the control input.\nPolicy function $\\pi(a_t|s_t)$: The policy is a probability density function $\\pi(a_t|s_t) = \\mathbb{P}(a_t|s_t)$ that maps the current state $s_t \\in \\mathcal{S}$ to a probability distribution over actions $a_t \\in \\mathcal{A}$. In our framework, controller is paramterized by a neural network as policy, denoted by $\\pi_{\\varphi}$, which is assumed to belong a Gaussian distribution family\n$\\pi_{\\varphi} \\in \\{\\mathcal{N}(\\mu_{\\varphi}, \\sigma_{\\varphi})\\}$\nwhere $\\mu_{\\varphi}$ and $\\sigma_{\\varphi}$ are the mean and standard deviation.\nState transition $p(s_{t+1}|s_t, a_t)$: By augmenting the state with delay information $u(s,r,t)$, the transition dynamics become Markovian. The probability $\\mathbb{P}(s_{t+1}|s_t, a_t)$ depends on the augmented state $s_t$ and the selected action $a_t$. The system evolves according to the dynamics described in (5)-(10), where the state $s_t$ transitions to $s_{t+1}$ under the influence of the action $a_t \\in \\mathcal{A}$.\nReward function $r_t$: It is designed to guide the learning of an optimal control policy and comprises two components,\n$r_t=r_{\\text{mid}} + r_{\\text{end}},$\nwhere\n$r_{\\text{mid}} = -\\Gamma \\cdot \\ln(1 + ||s_{t-1} - s_t||_{L^2}) - \\Gamma \\cdot \\ln(1 + ||s_t||_{L^2}),$\n$r_{\\text{end}} = \\begin{cases}\n0, & ||s_T||_{L^2} > \\xi,\n\\sigma, & ||s_T||_{L^2} \\le \\xi.\n\\end{cases}$\nwhere $\\Gamma > 0$ denotes weighting factor, $T$ denotes final step of each episode, $\\xi$ represents the threshold for the $L^2$ norm of the final state $s_T$, and $\\sigma$ is scaling factor that adjusts reward magnitude. The second component provides additional reward if state approaches the equilibrium point 0.\nReturn $R_t$: It denoted as $R_t = \\sum_{k=t}^T\\gamma^{k-t}r_k$, represents the discounted cumulative reward starting from time $t$ where $\\gamma \\in (0,1)$."}, {"title": "3.2 NO-SAC Control Design", "content": "We employ the actot-critic SAC framework, which consists of an actor network $\\pi_{\\varphi}(a_t|s_t)$, two critic networks $Q_{\\theta_i}(s_t, a_t)$ and their target network $Q_{\\overline{\\theta}_i}(s_t, a_t)$, to avoid bootstrapping.\nThe backstepping DeepONet is duplicated into five copies, in addition to the original one, which are incorporated into the actor network and critic networks. In this case, the actor network $\\pi_{\\varphi}$ consists of the DeepONet $\\mathcal{O}_{\\varphi}$ and a fully connected neural network (FNN) $\\mathcal{F}_\\varphi$, i.e., $\\pi_{\\varphi} = \\mathcal{O}_{VN} \\cup \\mathcal{F}_\\varphi$. Similarly, critic networks $Q_{\\theta_i}$ consist of the duplicated DeepONets $\\mathcal{O}_{VN}$ and FNNS $\\mathcal{F}_{V_i}$, i.e., $Q_{\\theta_i} = \\mathcal{O}_{VN} \\cup \\mathcal{F}_{V_i}$ for $i = 1,2$.\nThe actor and critic networks are updated via backpropagation, and their respective feature extraction layers $\\mathcal{O}_\\varphi$ for the actor and $\\mathcal{O}_{V_i}$ for the critic networks, are optimized independently.\nLearning critic networks: Define action-value function as $Q^{\\pi}(s_t, a_t) = Q : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$ representing the expected return following policy $\\pi$ after taking action $a_t$ in state $s_t$. To reduce bootstrapping bias, the SAC employs two target networks $\\overline{Q}_{\\theta_i}$ and $\\overline{Q}_{\\theta_i}$ to estimate the target action-value\n$y = r_t + \\min_{i=1,2} (Q_{\\overline{\\theta}_i}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi_{\\varphi}(a_{t+1}|s_{t+1})),$\nwhere $\\alpha$ is the temperature parameter that adjust the trade-off between reward maximization and entropy, and the actions $a_{t+1} \\sim \\pi(\\cdot|s_{t+1})$ is generated from the policy network.\nThe action-value networks, $Q_{\\theta_1}(s_t, a_t)$ and $Q_{\\theta_2}(s_t, a_t)$, are trained by minimizing the TD soft residual\n$J(Q_{\\theta_i}) = \\mathbb{E}_{(s_t,a_t) \\sim \\mathcal{W}} [(Q_{\\theta_i}(s_t, a_t) - y)^2],$\nwhere $i \\in 1,2$ and $\\mathcal{W}$ is the distribution of the states and the actions.\nThe weights of the two target action-value networks $\\overline{Q}_{\\theta_1}$ and $\\overline{Q}_{\\theta_2}$, are updated by\n$\\overline{\\theta}_i \\leftarrow \\eta \\theta_i + (1 - \\eta)\\overline{\\theta}_i,$\nwhere $i \\in 1,2$ and $\\eta \\in (0,1]$ is the weighted coefficient.\nLearning the Policy:\nDefinition 2. (Entropy) The entropy $H(\\pi(\\cdot|s_t))$ of the policy measures the randomness of the action distribution given a state $s_t$, defined as:\n$H(\\pi(\\cdot|s_t)) = -\\mathbb{E}_{a_t \\sim \\pi_{\\varphi}} [\\log \\pi_{\\varphi}(a_t|s_t)],$\nThe policy network $\\pi_{\\varphi}(a_t|s_t)$ is trained by minimizing the following objective:\n$J(\\pi_{\\varphi}) = \\mathbb{E}_{s_t \\sim \\mathcal{G}, a_t \\sim \\pi_{\\varphi}}[-\\alpha H(\\pi_{\\varphi}(\\cdot|s_t)) - \\min_i Q_{\\theta_i}(s_t, a_t)],$\nwhere $\\mathcal{G}$ denotes the state distribution."}, {"title": "4. SIMULATION RESULTS", "content": "In this section, we design a delay compensate controller based on the SAC algorithm and DeepONet, bypassing the constraints of the backstepping method that required specific assumptions about the delay, i.e., $\\tau\\in \\mathcal{D}$. The DeepONet shown in Fig. 1 serves as the feature extraction layer of the Actor-Critic network, enabling the extraction of features from high-dimensional data based on the backstepping controller. The extracted features, with a shape of 441, are further processed by the Actor-Critic network to output action values $Q(s_t, a_t)$ and control $U(t)$.\nThe training process was conducted on a workstation equipped with an Intel Core i9-13900KF CPU and an NVIDIA GeForce RTX 4090 GPU. We construct a Gym environment based on equations (1)-(4), following Bhan et al. (2024). The parameters of reward (14) are set as $\\Gamma = 0.008, \\sigma = 300$, and $\\xi = 10$. Built upon the Stable-Baselines3 library (SB3), the framework used a replay buffer size of $10^5$, with discount reward factor $\\gamma = 0.99$.\nKey hyperparameters are selected as follows: learning rate $\\lambda = 0.00009$, policy update frequency of 2, and soft update coefficient $\\eta = 0.003$. Default SB3 parameters are omitted for brevity.\nEach episode lasts 5 seconds and consists of 2500 steps under a temporal step size setting of $\\Delta t = 0.002$. The training process comprises 100 episodes in total taking approximately 40 minutes. For plant coefficient, the attenuation factor $c(x) = 20(1-x)$ and heat transfer coefficient $f(x, q) = 5\\cos(2\\pi q)+5 \\sin(2\\pi x)$ are used. We use the delay function $\\tau(x) = 0.7 + 0.3\\cos(4 \\arccos(x))$, which violates the assumption $\\tau \\in \\mathcal{D}$, where $\\mathcal{D}$ is defined in (11), as shown in Fig. 3. During training, the initial state $v_0(x)$ is a constant randomly choosing from a uniform distribution $\\mathcal{U}[1,8]$ and let $u_0(x,r) = 0$. For testing, the initial state is set to $v_0(x) = 6$. The state space for interaction is defined as $[-\\infty, \\infty]$, and the action space is constrained to [-30, 30].\nIn real-world applications, controller frequencies are typically limited to 100 Hz due to sensor and actuator response times. Excessively high frequencies can hinder the RL process Bhan et al. (2024), causing instability or learning difficulties. In our framework, we update the control strategy every 100 steps using a zero-order hold."}, {"title": "4.2 Simulation Results", "content": "To evaluate the performance of our framework, we established a baseline SAC (Haarnoja et al. (2018)) for comparison. Reward evolution curves for the baseline SAC and the proposed algorithm, NO-SAC, are shown in Fig. 4. The curves illustrate that NO-SAC converges faster and consistently outperforms the baseline in terms of reward. It is noteworthy that the reward curves does not converge to zero, due to the positive reward $r_{\\text{end}}$ defined in equation (16). This reward definition adds additional rewards when the states converge to the expected values.\nFig.5 illustrates the closed-loop state evolution using the NO-SAC controllers and the SAC whitout considering the assumption of $\\tau \\in \\mathcal{D}$. To compare the RL-based controllers with the backstepping controller, we plot the closed-loop state evolution with three controllers for $\\tau(x) = e^{-0.7x}$ satisfying the delay assumption, shown in Fig.5. In addition, Fig. 7 illustrates the control inputs of different controllers and the $L2$ norm of the state for different $\\tau(x)$.\nThe simulation results demonstrate that incorporating a pre-trained DeepONet as the feature extraction layer within the SAC algorithm increases reward acquisition and improves training efficiency. Furthermore, the NO-SAC-based control strategy reduces steady-state error compared to the SAC-based control strategy. From Fig. 7, we observe that the RL-based controllers exhibit smaller overshoot and shorter settling times compared to the backstepping controller under the same delay function."}, {"title": "5. CONCLUSION", "content": "In this paper, we propose a NO-SAC framework, which eliminates the assumptions on the delay function in the backstepping design by integrating backstepping control strategies with the RL and stabilizes an unstable first-order hyperbolic PDE with spatially varying delays. Our framework integrates the backstepping DeepONet as feature extractor within the SAC algorithm. This hybrid approach achieves faster convergence and decreases the transient overshoot compared to the backstepping controller. From the perspective of RL, NO-SAC eliminates steady-state errors in dynamics and demonstrates superior generalization performance compared to the SAC. These improvements stem from DeepONet, which maps infinite-dimensional function spaces for more efficient feature extraction and better generalization. Future work will advance the theory of operator learning in control, extending beyond DeepONet to develop more generalizable representations and analyze their impact on stability in RL-based controllers for distributed systems."}]}