{"title": "HAAT: Hybrid Attention Aggregation Transformer for Image\nSuper-Resolution", "authors": ["Song-Jiang Lai", "Tsun-Hin Cheung", "Ka-Chun Fung", "Kai-wen Xue", "Kin-Man Lam"], "abstract": "In the research area of image super-resolution, Swin-transformer-based models are favored for their global spatial\nmodeling and shifting window attention mechanism. However, existing methods often limit self-attention to non-\noverlapping windows to cut costs and ignore the useful information that exists across channels. To address this\nissue, this paper introduces a novel model, the Hybrid Attention Aggregation Transformer (HAAT), designed to\nbetter leverage feature information. HAAT is constructed by integrating Swin-Dense-Residual-Connected Blocks\n(SDRCB) with Hybrid Grid Attention Blocks (HGAB). SDRCB expands the receptive field while maintaining\na streamlined architecture, resulting in enhanced performance. HGAB incorporates channel attention, sparse\nattention, and window attention to improve nonlocal feature fusion and achieve more visually compelling results.\nExperimental evaluations demonstrate that HAAT surpasses state-of-the-art methods on benchmark datasets.", "sections": [{"title": "1. INTRODUCTION", "content": "Single Image Super-Resolution (SISR) aims to reconstruct a high-quality image from a low-resolution one. The\ndevelopment of effective super-resolution algorithms has emerged as a pivotal research domain in computer vision\nowing to its extensive applications. Recent research have integrated the self-attention mechanism into computer\nvision challenges. 1, 2\nCNN-based techniques for Single image Super-Resolution (SISR) have markedly improved the restoration of\nimage texture features. SRCNN\u00b3 was the inaugural model to tackle super-resolution with convolutional neural\nnetworks. VDSR\u2074 implemented residual learning to enhance learning and successfully address the gradient\nvanishing issue in deep networks. In SRGAN,5 Christian Ledig et al. employed generative adversarial networks to\nrefine super-resolution image production, with the generator converting low-resolution images to high-resolution\nones and improving quality via adversarial training. ESRGAN6 included the Residual Dense Block (RRDB) as a\nfundamental network component, diminishing perceptual loss by utilizing characteristics prior to activation, hence\nproducing images with more authentic textures. Moreover, researchers persist in suggesting novel structures to\nretrieve progressively realistic information in super-resolution images. CNN-based networks have demonstrated\nconsiderable performance efficacy. Nonetheless, the inductive bias inherent in CNNs constrains the ability of\nSISR models to capture long-range relationships. The constraints arise from the parameter-dependent scaling of\nthe receptive field and the convolution operator's kernel size across many layers, potentially neglecting non-local\nspatial information in pictures.\nTo enhance the joint modeling of different hierarchical structures in pictures, researchers have taken use of the\nself-attention mechanism's benefits in multi-scale processing and long-range dependency modeling. Transformer-\nbased SISR models have been developed to overcome the shortcomings of CNN-based networks by utilizing their\ncapacity to simulate long-range dependencies and improving SISR performance. For instance, super-resolution\nresults have been markedly enhanced by SwinIR,\u00b9 which makes use of the Swin Transformer.2 Furthermore, state-\nof-the-art results have been produced with a hybrid attention transformer (HAT) that combines an overlapping\ncross-attention module with window-based self-attention and channel attention."}, {"title": "2. HYBRID ATTENTION AGGREGATION TRANSFORMER", "content": "Despite the successful use of Transformer-based approaches to image restoration problems, there are areas for\nimprovement. Current window-based Transformer networks confine self-attention calculations to a concentrated\nregion. This method evidently results in a constrained receptive field and fails to properly use the feature informa-\ntion from the original picture. This research proposes a hybrid multiaxial aggregation network, termed HAAT, to\naddress the aforementioned issues. HAAT is constructed by integrating Swin-Dense-Residual-Connected Blocks\n(SDRCB) with Hybrid Grid Attention Blocks (HGAB). HGAB, inspired by GAB,9 integrates channel atten-\ntion, sparse attention, and window attention, leveraging the global information perception capabilities of channel\nattention to address the deficiencies of self-attention. Sparse self-attention is used to enhance global feature in-\nteractions while maintaining computational efficiency. Meanwhile, to further thrill the potential performance of\nthe model."}, {"title": "2.1 Swin-Dense-Residual-Connected Block", "content": "We use the shifting window self-attention mechanism of the Swin-Transformer Layer (STL)1,2 to capture long-\nrange dependencies via adaptive receptive fields. STL modifies the model's emphasis according to global content,\nenhancing feature extraction. This technique maintains global details as the network deepens, enlarging the\nreceptive area without degradation. Integrating STL with dense-residual connections expands the receptive area\nand improves emphasis on critical information, hence increasing performance in SISR tasks that need thorough,\ncontext-sensitive processing. The SDRCB for input feature maps Z inside RDG is delineated as follows.\n\n\\(Zj = Htrans(STL([Z, ...Zj\u22121]), j = 1, 2, 3, 4, 5,\\) \n\n\\(SDRCB(Z) = \u03b1\u00b7 Z5 + Z,\\)"}, {"title": "2.2 Hybrid Grid Attention Block(HGAB)", "content": "The GAB consists of a Mix Attention Layer (MAL) and an MLP layer. Regarding the MAL, we first split the\ninput feature Fin into two parts by channel: FG \u2208 R(H\u00d7W\u00d7C/2) and Fw \u2208 R(H\u00d7W\u00d7C/2). Besides, feed the input\nto another branch to perform channel attention operation. Subsequently, we split Fw into two parts by channel\nagain and input them into W-MSA and SW-MSA, respectively. Meanwhile, FG is input into Grid-MSA. The\ncomputation process of MAL is as follows:\n\n\\(Xw\u2081 = W-MSA(Fw\u2081),\\)\n\n\\(XW2 = SW-MSA(FW\u2082),\\)\n\n\\(XG = Grid \u2013 MSA(FG),\\)\n\n\\(Xc = CA(Fin),\\)\n\n\\(XMAL = LN(Cat(Xw1, XW2, XWG) + Xc) + Fin,\\)\n\nwhere Xw\u2081, XW\u2082, XG and XC are the output features of W-MSA, SW-MSA, Grid-MSA, and CA, respectively.\nFurtheremore, it should be noted that we adopt the post-norm method in GAB to enhance the network training\nstability. For a given input feature Fin, the computation process of HGAB is:\n\n\\(FM = LN(MAL(Fin)) + Fin,\\)\n\n\\(FM = LN(MAL(FM)) + FM,\\)"}, {"title": "3. EXPERIMENTAL RESULTS", "content": "Our HAAT model is trained on DF2K, a substantial aggregated dataset that includes DIV2K11 and Flickr2K.12\nDIV2K provides 800 images for training, while Flickr2K contributes 2650 images. For the training input, we\ngenerate LR versions of these images by applying a bicubic down sampling method with scaling factors of 2, 3, and\n4, respectively. To assess the effectiveness of our model, we conduct performance evaluations using well-known\nSISR benchmark datasets such as Set5,13 Set14.14\nIn the DRCT architecture, the depth and width configuration mirrors that of HAT. Specifically, both models\nhave 6 RDG and SDRCB units, with 180 channels for intermediate feature maps. For window-based multi-head\nself-attention (W-MSA), the number of attention heads is set to 6, and the window size is 16. In the HGAB\nblock, the channel squeeze factor is 16, with 180 channels for intermediate features. The Grid MSA and (S)W-\nMSA use 3 and 2 attention heads, respectively. HR patches of 256 \u00d7 256 pixels were extracted from HR images,\nwith random horizontal flips and rotation for data augmentation. As shown in Table 1, our method outperforms\nstate-of-the-art techniques in both PSNR and SSIM.\nFor evaluation, we use all RGB channels and exclude the outermost (2 \u00d7 scale) border pixels. PSNR and\nSSIM metrics are employed for assessment."}, {"title": "4. CONCLUSION", "content": "This work introduces a unique Hybrid Attention Aggregation Transformer (HAAT) for single-image super-\nresolution. HAAT enhances the DRCT architecture, emphasizing the stabilization of information flow and the\nexpansion of receptive fields via dense connections in residual blocks, in conjunction with the shift-window at-\ntention mechanism to adaptively acquire global information. This enables the model to enhance its emphasis on\nglobal geographical information, optimizing its capabilities and circumventing information bottlenecks. Further-\nmore, motivated by the hierarchical structural resemblance in images, we provide HGAB to represent long-range\nrelationships. The network improves multi-level structural similarity via the integration of channel attention,\nsparse attention, and window attention. The model was trained on the DF2K dataset and verified using the Set5\nand Set14 datasets. Experimental findings indicate that our strategy surpasses SOTA techniques on benchmark\ndatasets for single-image super-resolution tasks."}]}