{"title": "Enabling Scalable Oversight via Self-Evolving Critic", "authors": ["Zhengyang Tang", "Ziniu Li", "Zhenyang Xiao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Dayiheng Liu", "Fei Huang", "Tianyu Liu", "Bowen Yu", "Junyang Lin"], "abstract": "Despite their remarkable performance, the development of Large Language Models (LLMs) faces a critical challenge in scalable oversight: providing effective feedback for tasks where human evaluation is difficult or where LLMs outperform humans. While there is growing interest in using LLMs for critique, current approaches still rely on human annotations or more powerful models, leaving the issue of enhancing critique capabilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that enables genuine self-evolution of critique abilities. Technically, SCRIT self-improves by training on synthetic data, generated by a contrastive-based self-critic that uses reference solutions for step-by-step critique, and a self-validation mechanism that ensures critique quality through correction outcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMS, SCRIT achieves up to a 10.3% improvement on critique-correction and error identification benchmarks. Our analysis reveals that SCRIT's performance scales positively with data and model size, outperforms alternative approaches, and benefits critically from its self-validation component.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Anthropic, 2024; Qwen-Team, 2024) represent significant milestones in the development of Artificial Intelligence (AI). They rely on human supervision signals through methods such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022). As a result, these models have evolved at an unprecedented pace, surpassing human capabilities in certain challenging domains. However, this framework encounters a fundamental challenge: how to provide effective and scalable feedback for LLMs in tasks that are not only difficult for humans to evaluate but where LLMs may outperform humans. This challenge, known as scalable oversight (Bowman et al., 2022), remains critical, yet progress in this area has been limited.\nTo address this challenge, a promising direction is to leverage LLMs themselves to assist in the evaluation process, enabling further refinement of model outputs (Saunders et al., 2022; McAleese et al., 2024). At the heart of this approach lies the critique ability - the capability to identify and rectify flaws in model responses. When critique feedback is accurate and informative, LLMs can refine their outputs, advancing toward higher-order intelligence. However, existing studies indicate that LLMs exhibit weak performance in critique tasks (Zheng et al., 2024b; Yang et al., 2024), despite their strong problem-solving capabilities. Therefore, enhancing critique abilities becomes an important research problem, one that this paper also seeks to address.\nCurrent approaches to improving the critique abilities of LLMs rely on two sources of supervision: human annotations (Saunders et al., 2022; McAleese et al., 2024) and stronger LLMs that serve as human proxy (e.g., GPT-4 and 01-mini) (Lan et al., 2024; Zhang et al., 2024; Zheng et al., 2024b; Yang et al., 2024)). While these methods have shown promise, they face three fundamental limitations. First, the quality of generated critiques is inherently bounded by the capabilities of the supervisors. Second, the dependence on human annotations or API calls to stronger models introduces significant costs, limiting the scalability of these approaches. Most critically, these approaches fail to address a fundamental question in scalable oversight: how can we enhance the critique abilities of our most capable models when stronger supervisors are no longer available?\nIn this work, we introduce SCRIT (Self-evolving CRITic), a framework that enables LLMs to develop self-evolving critique abilities. We focus on mathematical reasoning tasks as an ideal testbed for this approach, where \u201ccritique\u201d refers"}, {"title": "2. Related Work", "content": "Scalable Oversight and Critic Models The challenge of providing effective feedback to language models on tasks difficult for humans to evaluate has attracted significant research attention. Early work by (Saunders et al., 2022) proposed fine-tuning LLMs to generate natural language critiques, introducing key components including critique"}, {"title": "3. SCRIT: Self-Evolving Critic", "content": "3.1. Problem Formulation and Overview\nLet $P$ denote a set of mathematical problems, where each problem $p \\in P$ is paired with a ground truth answer $a_p$. For each problem $p$, we collect a set of solutions $S_p = {$s_1, s_2, ..., s_n$} from different models, where each solution $s_i$ consists of:\n\u2022 A sequence of reasoning steps $r_i = [r_i^1, r_i^2, ..., r_i^{k_i}]$, where $k_i$ is the number of steps\n\u2022 A final answer $a_{si}$\nA critique $c$ for a solution $s$ is defined as a tuple $c = (e, l, t)$, where:\n\u2022 $e = [e_1, e_2, ..., e_{ki}]$ is a sequence of step-wise critiques, where each $e_r$ corresponds to the analysis of step $r_i^r$\n\u2022 $l = (y, j)$ is the conclusion, where $y \\in {0, 1}$ indicates solution correctness and $j \\in {-1} \\cup \\mathbb{N}$ denotes the first error step ($j = -1$ means no error)\n\u2022 $t$ is the correction, consisting of a sequence of corrected steps and a final answer $a_t$\nOur objective is to learn a critique function $f_\\theta : P \\times S \\rightarrow C$ that maps a problem $p$ and a solution $s$ to an effective critique $c$, where $\\theta$ represents the parameters of a language model.\nTo achieve this objective, we propose SCRIT (Self-evolving CRITic), a framework that systematically leverages the shared mathematical understanding across different solutions to enable truly self-evolving critique abilities. As illustrated in Figure 3, SCRIT operates through a complete self-evolving cycle: it takes a problem and solutions as input, generates critiques through analyzing reference solutions, validates their quality, and uses the validated critiques for self-training. This forms a complete self-evolving cycle without any external supervision.\n3.2. Solution Collection\nDataset The first step in our framework is to collect a diverse set of solutions. We build our collection process on the NuminaMath dataset (LI et al., 2024), a large-scale mathematical problem dataset covering various topics from elementary mathematics to competition-level problems. To ensure data quality, we develop a robust pipeline to com-"}, {"title": "3.3. Self-Critic Generation", "content": "A key challenge in enabling effective critique generation is how to ensure the model can identify and correct errors in complex mathematical reasoning, particularly when the problem difficulty approaches or exceeds the model's current capabilities. Our preliminary experiments reveal that the model often exhibits \"rubber-stamping behavior\" blindly approving incorrect steps without genuine understanding of the mathematical concepts involved, as illustrated in Figures 2 and 8. This also aligns with findings in (Huang et al., 2023).\nWe initially explored two straightforward approaches: (1) Direct Critic (Zheng et al., 2024a), where a language model, such as Qwen2.5-72B-Instruct, directly critiques a solution; and (2) Bug-Injection Critic (McAleese et al., 2024), a two-stage approach of first injecting errors into a correct solution and then ask the LLM to critic and correct it. However, both approaches showed limited effectiveness (detailed in Section 5.2).\nTo address these issues, we develop a new technique called Contrastive Critic. Our key insight stems from a fundamental property of mathematical reasoning: while problems may have multiple valid solutions, they inherently share the same underlying mathematical concepts and key solving strategies. By explicitly providing a correct reference solution during critique generation, we enable the model to first understand these core mathematical concepts and solving strategies, then leverage this understanding to perform step-by-step critique of the target solution. This ap-"}, {"title": "Pvalid = {p \u2208 P||S+ | > 0 \u2227 |S5 | > 0}", "content": "3.4. Self-Validation\nWith self-generated critique data, we apply post-validation techniques to further enhance the quality of generated outputs. This process specifically filters out low-quality cases where the model blindly approves all intermediate steps, only to suddenly reject the final answer upon detecting a discrepancy (see Appendix D).\nTo address these challenges, we employ direct validation on the correction part of the critique. Formally, we have that:\n$\u03c5\u03b8(C) = {1 if g (p, t) = (1, -1) 0 otherwise}$"}, {"title": "3.5. Self-Training", "content": "Let V denote the set of validated solution-critique pairs across all problems:\nV = {(p, s, c)|p \u2208 Pvalid, s \u2208 Sp, \u03c5\u03b8(C) = 1}\nFor each validated triplet (p, s, c) \u2208 V, we construct training pairs with input ge (p, s) and target (e, l, t) from c. Note that we exclude the reference analysis r from the target as it is specific to contrastive critic generation.\nWe fine-tune the base model Qwen2.5-72B-Instruct to minimize the following loss function:\n$L(\u03b8) = \u2211 log f\u03b8 (e, l, tige (p, s)) (p,s,c)\u2208V$"}, {"title": "4. Experiments", "content": "4.1. Statistics of SCRIT\nWe present detailed statistics of data flow through each component of our framework.\nSolution Collection We start with 452K problem-answer pairs from our own NuminaMath dataset (see Appendix A). For solution generation, we employ 7 models of varying capabilities as described in Section 3.2. Each model generates one solution per problem, with solutions classified as correct or incorrect based on their final answers using Qwen2.5-72B-Instruct (detailed in Appendix H). Then we apply two filtering criteria: (a) Each problem must have at least one correct and one incorrect solution to enable contrastive learning; (b) Solutions from each model are capped at 50K for both correct and incorrect categories. After filtering, we obtain 665K problem-solution pairs, evenly split between good solutions (332K) and bad solutions (332K).\nSelf-Critic & Self-Validation To analyze the self-critic and self-validation step, we track the data flow from the initial 665K problem-solution pairs through these steps. Out of these pairs, 342K (51.4%) successfully pass the self-critic and self-validation step, yielding high-quality problem-solution-critique triplets. Figure 4 presents a detailed analysis of this filtering process across different dimensions, revealing interesting patterns in validation rates.\n\u2022 Domain Complexity: Validation rates decrease systemati-"}, {"title": "4.2. Evaluation", "content": "We present two complementary evaluation protocols to assess different aspects of critique capabilities:\nCritic and Correct The first protocol evaluates a model's ability to critic and correct a given solution, following the assumption (Zheng et al., 2024b) that truly effective critiques should be able to guide the correction of errors and lead to correct answers. We conduct ex- periments on RealCritic, an internal benchmark we developed and plan to release publicly, which systematically spans 8 datasets (GSM8K (Cobbe et al., 2021),"}, {"title": "4.3. Main Results", "content": "Critic and Correct Table 1 presents results across three increasingly challenging scenarios. In critiquing deliberately incorrect solutions, SCRIT achieves substantial improvements over the base Qwen2.5-72B-Instruct model, raising the average performance from 39.7% to 50.0%. For balanced solutions, SCRIT maintains its advantage with an average improvement of 4.4%, despite the increased difficulty of distinguishing correct from incorrect solutions. Most impressively, when critiquing Qwen2.5-72B-Instruct's own solutions, SCRIT still manages to improve upon the base model (62.9% vs 61.7%), demonstrating its ability to identify and correct errors in solutions generated by its own base model. Across all scenarios, SCRIT's performance approaches that of o1-mini.\nCritic and Correct with Error Identification As shown in Table 2, SCRIT also demonstrates strong capabilities"}, {"title": "5. Analysis", "content": "Throughout this section, we report two metrics: critique-correction accuracy (CC-Acc) from the Critic and Correct protocol, which is averaged across three scenarios, and error identification F1-score (EI-F1) from the Critic and Correct with Error Identification protocol.\n5.1. Scaling Behavior of SCRIT\nWe investigate how SCRIT's performance scales with both training data size and model size (see Figures 5 and 6).\nData Size Scaling For data scaling experiments, we train"}, {"title": "5.2. Which Critic Mechanism is Most Effective?", "content": "To identify the most effective critic mechanism for our self-evolving framework, we conduct strictly controlled experiments comparing three different critic approaches described in Section 3.3 using identical sets of problems and solutions.\nOur experiments in Figure 5 reveal several key findings. First, Contrastive Critic shows strong performance from the early stages across both metrics: with just 10K training examples, it achieves 56.8% CC-Acc and 40.2% EI-F1, outperforming both Direct Critic and Bug-Injection Critic. More importantly, as training data increases to 170K examples, Contrastive Critic continues to show positive scaling behavior, reaching 58.3% CC-Acc and 45.1% EI-F1. In contrast, Direct Critic quickly plateaus at around 55.1% CC- Acc and 38.7% EI-F1, while Bug-Injection Critic exhibits performance degradation in CC-Acc (dropping to 49.0%) and unstable performance in EI-F1)."}, {"title": "5.3. How Important is Self-Validation?", "content": "To assess the necessity of self-validation in SCRIT, we conduct controlled experiments by removing the self-validation component while keeping all other settings identical. The results in Table 3 show clear performance degradation across both evaluation metrics: the CC-Acc drops by 0.8%, and more significantly, the EI-F1 decreases by 3.0%. Case analysis (see Appendix D) shows that the self-critic may still generate low-quality critiques, often blindly approving all intermediate steps only to suddenly claim \"the final step is incorrect\" when encountering answer discrepancies. By incorporating self-validation, we are able to further enhance the quality of data for self-training."}, {"title": "5.4. How Does Problem Domain Diversity Affect Performance?", "content": "To investigate the importance of problem domain diversity, we conduct controlled experiments by restricting the training data to only GSM8K and MATH domains, while keeping other settings unchanged. This represents a significant reduction in domain coverage compared to our full setting which spans 9 sources ranging from elementary to competition-level mathematics.\nThe results in Table 3 demonstrate the value of domain diversity: when training with limited domains, the CC-Acc drops by 1.4% and the EI-F1 decreases by 1.4%. It suggests that exposure to diverse problem-solving patterns and error types is crucial for developing robust critique abilities."}, {"title": "5.5. How Does Problem Difficulty Impact Performance?", "content": "To understand the impact of problem difficulty, we conduct experiments by selecting training examples based on the number of unique answers generated across solution models - a proxy for problem complexity. We compare two settings: training with problems that have more unique answers (indicating higher complexity) versus those with fewer unique answers (indicating lower complexity).\nInterestingly, training with less complex problems leads to"}, {"title": "5.6. Does the Choice of Solution Model Matter?", "content": "To study whether critiquing solutions from different models affects SCRIT's performance, we conduct controlled experiments by restricting the solutions being critiqued to those from a single model while keeping other settings identical. Our results in Table 3 show that the source model of solutions has limited impact on SCRIT's final performance.\nSince solution generation models only provide the solutions for constructing contrastive critique pairs and do not directly participate in improving critique effectiveness, their individual capabilities have less influence on the final performance. What matters more is how to construct diverse and informative contrastive pairs that help the model learn effective critique strategies, regardless of the solution models."}, {"title": "5.7. Optimal Ratio between Good and Bad Solutions?", "content": "Finally, we investigate the impact of good-to-bad solution ratio in the training data. Training with a higher proportion of bad solutions (0.25:0.75) shows significantly better performance than using more good solutions (0.75:0.25). As shown in Table 3, using more good solutions results in performance degradation across both evaluation metrics. This suggests that exposure to more bad solutions helps SCRIT develop stronger error identification capabilities, likely because it provides more diverse examples of mathematical mistakes and their corresponding corrections. More importantly, analyzing incorrect solutions forces the model to actively engage in error detection and correction, rather than simply validating correct steps. This finding tells us that while maintaining some balance is important, slightly favoring incorrect solutions may be a better choice for training effective critique models."}, {"title": "6. Conclusion", "content": "In this work, we present SCRIT, a framework that enables genuine self-evolution of critique abilities without relying on external supervision. Through extensive experiments we demonstrate that SCRIT consistently improves both critique- correction accuracy and error identification capabilities of the base model. Our analysis reveals that SCRIT's perfor- mance scales positively with data and model size, outper- forms alternative approaches, and benefits critically from its self-validation component.\nLooking forward, this work opens up several promising di- rections for future research. First, exploring the synergy between critic models and process supervision could be valuable - SCRIT's ability to generate high-quality critiques could potentially be leveraged to automatically label rea- soning steps for training process supervision models like PRM (Lightman et al., 2023). Second, given that our cor- rection outcomes provide verifiable rewards, integrating reinforcement learning (Li et al., 2024) into SCRIT could further enhance its performance through reward-driven op- timization (Lambert et al., 2024). Additionally, extending SCRIT beyond mathematical reasoning to other domains where ground truth can be systematically verified, such as coding or logical reasoning, represents another promising direction. We believe these directions, combined with the insights from our work, will contribute to developing more capable and reliable LLMs that can effectively oversee and improve themselves."}, {"title": "Impact Statement", "content": "This work advances scalable oversight research by introduc- ing a self-evolving framework for improving model critique abilities in mathematical reasoning. While our research fo- cuses primarily on technical capabilities, we acknowledge several important considerations beyond our current scope. First, though we demonstrate SCRIT's effectiveness in math- ematical domains where correctness can be objectively ver- ified, its application to domains involving subjective judg- ments or ethical considerations requires careful examination. Second, while our framework aims to enable AI systems to better identify and correct errors, we have not specifically investigated potential biases in the critique process or how these might impact different demographic groups. Addi- tionally, as our approach involves models critiquing their own outputs, further research is needed to understand the broader implications for AI safety and reliability. These considerations highlight the importance of complementing technical advances with comprehensive ethical evaluations in future work."}, {"title": "A. Computing Ground Truth Answers for NuminaMath", "content": "A large-scale dataset with reliable ground truth answers is fundamental to our work. We choose NuminaMath (LI et al., 2024) for its diversity, difficulty distribution, and scale (860K problems). However, as the correctness of solutions in the original dataset is not guaranteed, we develop a robust pipeline to compute reliable ground truth answers.\nA.1. Answer Generation and Validation Pipeline\nWe employ Qwen2.5-Math-72B-Instruct (Qwen-Team, 2024) under tool-integrated (Gou et al., 2023) settings to generate solutions, as it demonstrates state-of-the-art performance across multiple mathematical reasoning benchmarks. The solutions are then evaluated using Qwen2.5-Math-RM-72B (Qwen-Team, 2024), a specialized reward model for mathematical reasoning. We consider a solution correct if its reward score exceeds a predefined threshold, and use its final answer as the ground truth.\nA.2. Threshold Selection and Validation\nTo determine an appropriate reward threshold, we conduct extensive experiments:\n\u2022 Benchmark Validation: We evaluate the threshold's effectiveness across multiple standard benchmarks including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), GAOKAO2023-EN (Zhang et al., 2023), Olympiad- Bench (He et al., 2024), and College Math (Tang et al., 2024). With a threshold of 1.0, we achieve approximately 75% accuracy.\n\u2022 Human Evaluation: We randomly sample 100 NuminaMath problems and conduct human evaluation of the answers selected using our threshold. The results show approximately 85% accuracy.\n\u2022 Comparison with Alternative Methods: We explore majority voting among solutions from NuminaMath, Qwen2.5- Math-72B-Instruct, and Deepseek-V2-Chat-0628. However, this approach yields lower accuracy compared to our reward-based selection method.\nAfter applying our pipeline with the validated threshold, we obtain a filtered dataset of 452K problem-answer pairs, which serves as the foundation for our work."}, {"title": "Enabling Scalable Oversight via Self-Evolving Critic B. Prompting Templates for Direct Critic, Bug-Injection Critic and Contrastive Critic", "content": "Here we present system prompts used for different critic mechanisms in Figure 7."}, {"title": "Iwill provide a math problem along with a student solution", "content": "conduct a step-by-step critique of the student's solution"}, {"title": "I will provide a math problem along with a student solution and a reference solution", "content": "First, analyze the reference solution"}, {"title": "Bug Critic System Prompt", "content": "Step1: Inject Bug"}, {"title": "Figure 7: System prompts used for different critic mechanisms.", "content": "Top Left: Direct Critic directly analyzes solution correctness without any additional context. Bottom Left: Bug-Injection Critic first injects bugs (Step 1) then direct critic on bug-injected solution (Step 2). Right: Contrastive Critic first analyzes a reference solution to understand key mathematical concepts before conducting step-wise critique."}, {"title": "Enabling Scalable Oversight via Self-Evolving Critic C. More Comparison between Direct Critic and Contrastive Critic", "content": "Figure 8: Comparison between Direct Critic and Contrastive Critic. Direct Critic shows blind approval of the student solution, failing to identify any errors and providing misleading approval. In contrast, Contrastive Critic first analyzes the reference solution to understand key mathematical concepts, enabling it to precisely locate the error in the student solution. By developing understanding of the underlying mathematical concepts, Contrastive Critic successfully generate an effective critique that guides the correction process to reach the correct final answer."}, {"title": "Enabling Scalable Oversight via Self-Evolving Critic D. Self-Validation Cases", "content": "We present two cases demonstrating the effectiveness of our Self-Validation mechanism in filtering critiques based on Self-Critic's correction in Figures 9 and 10.\nFigure 9: Case1: Self-Validation rejects an ineffective critic: Despite having access to a reference solution and using contrastive learning, the critic fails to identify Step 12 as the first error in solving a trigonometric equation. The subsequent correction leads to a conflicting final answer. The self-validation mechanism successfully detects this inconsistency and rejects this ineffective critique from the training data.\nFigure 10: Case2: Self-Validation accepts an effective critic: An example of effective critique that correctly identifies Step 3 as the error point where continuity requirements are mishandled. The correction follows logical mathematical reasoning and arrives at the correct final answer, which is then verified and accepted by the self-validation mechanism for training."}, {"title": "Enabling Scalable Oversight via Self-Evolving Critic E. Bug-Injection Case Study", "content": "Here we show examples of oversimplified bugs injected by Bug-Injection Critic. These examples illustrate how Bug-Injection Critic tends to generate overly simplistic errors (e.g., misunderstanding basic math properties, variable confusion) rather than more sophisticated mathematical reasoning errors that typically occur in complex problem-solving.\nFigure 11: An example of oversimplified bugs injected by Bug-Injection Critic: A conceptual bug involving basic misunderstanding of absolute value property.\nFigure 12: An example of oversimplified bugs injected by Bug-Injection Critic: A variable confusion bug where the wrong price range is used."}, {"title": "Enabling Scalable Oversight via Self-Evolving Critic F. Adaptations to ProcessBench's Evaluation Protocol", "content": "In evaluating models' error identification capabilities, we make two adaptations to ProcessBench's original evaluation protocol. These modifications are designed to ensure that models demonstrate genuine understanding of mathematical errors rather than superficial critique.\nF.1. Requiring Effective Correction\nOur first adaptation stems from the core assumption behind critic and correct tasks: a truly effective critique should not only identify errors but also guide their correction towards an correct answer. Through extensive case studies, we found that models can sometimes correctly identify the error step (matching human annotations) without actually understanding the mathematical mistake. As shown in Figures 13 to 15, these cases highlight that merely matching human-labeled error steps is insufficient for ensuring genuine understanding of mathematical errors.\nFigure 13: Although the critic correctly identifies Step 2 as the error step (matching human annotation), it fails to understand the underlying mathematical concept of graph theory, leading to an incorrect correction of 22 handshakes instead of the true answer 12.\nFigure 14: Despite matching the human-labeled error step (Step 4), the critic provides conflicting feedback and fails to recognize the fundamental issue in applying the Pythagorean theorem with perpendicular medians, leading to an incorrect solution.\nFigure 15: The critic matches Step 3 as problematic but misunderstands the key issue in finite geometric series calculation, resulting in an incorrect final value of 2047/2048.\nTherefore, we augment ProcessBench's protocol by requiring that models must not only identify the correct error step but also provide correction that leads to a mathematically valid solution. This stricter requirement helps ensure that models demonstrate genuine understanding of the mathematical concepts and errors involved.\nF.2. Allowing Step-Level Flexibility\nOur second adaptation addresses an inherent ambiguity in error identification: in many cases, mathematical errors can reasonably be attributed to multiple consecutive steps. Through our analysis, we found numerous instances where the exact \"error step\" is debatable, with both the preceding and following steps being valid points of identification. As shown in Figures 16 to 18, these cases illustrate how mathematical errors often span multiple steps, making strict step-level matching overly rigid for meaningful evaluation.."}, {"title": "Enabling Scalable Oversight via Self-Evolving Critic G. Distribution of First Error Step identified by Self-Critic", "content": "Figure 19: Distribution of first error positions identified by our self-critic across different mathematical domains."}, {"title": "Enabling Scalable Oversight via Self-Evolving Critic H. Classify Solutions into Correct and Incorrect", "content": "Again we use Qwen2.5-72B-Instruct itself to classify solutions into correct and incorrect ones. We present the system prompt in the following Figure 20:\nFigure 20: System Prompt to classify solutions into correct and incorrect ones."}, {"title": "Enabling Scalable Oversight via Self-Evolving Critic I. Self-Training Details", "content": "Here we present the detailed configuration for self-training of Qwen2.5-72B-Instruct. We utilize open-instruct (Wang et al., 2023) for our continued supervised fine-tuning implementation. The training was conducted on 4 servers, each equipped with 8 NVIDIA A100 GPUs (32 GPUs in total), with a total training time of several hours2.\nThe key hyper-parameters for training are as follows:\n\u2022 Batch size: 256\n\u2022 Learning rate: 5e-6\n\u2022 Number of training epochs: 1\n\u2022 Warmup ratio: 0.03\n\u2022 Model parallel size: 8\n\u2022 Total GPUs: 32 (4 servers \u00d7 8 A100 GPUs)\nFor reproducibility, we use gradient checkpointing and mixed-precision training (FP16) to optimize memory usage. The training was performed using DeepSpeed ZeRO-3 for efficient distributed training."}]}