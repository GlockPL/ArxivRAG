{"title": "Multi-modal Imaging Genomics Transformer: Attentive Integration of Imaging with Genomic Biomarkers for Schizophrenia Classification", "authors": ["Nagur Shareef Shaik", "Teja Krishna Cherukuri", "Vince D. Calhoun", "Dong Hye Ye"], "abstract": "Schizophrenia (SZ) is a severe brain disorder marked by diverse cognitive impairments, abnormalities in brain structure, function, and genetic factors. Its complex symptoms and overlap with other psychiatric conditions challenge traditional diagnostic methods, necessitating advanced systems to improve precision. Existing research studies have mostly focused on imaging data, such as structural and functional MRI, for SZ diagnosis. There has been less focus on the integration of genomic features despite their potential in identifying heritable SZ traits. In this study, we introduce a Multi-modal Imaging Genomics Transformer (MIGTrans), that attentively integrates genomics with structural and functional imaging data to capture SZ-related neuroanatomical and connectome abnormalities. MIGTrans demonstrated improved SZ classification performance with an accuracy of 86.05% (\u00b10.02), offering clear interpretations and identifying significant genomic locations and brain morphological/connectivity patterns associated with SZ.", "sections": [{"title": "1 Introduction", "content": "According to the World Health Organization (WHO), approximately 24 million individuals worldwide are affected by Schizophrenia (SZ), a condition characterized by persistent hallucinations and disruptive behavior that significantly impacts their daily life [2]. The symptoms of SZ often overlap with those of other psychiatric disorders, complicating both diagnosis and treatment [21]. Structural magnetic resonance imaging (sMRI) is often employed to provide detailed images of brain anatomy, enabling the assessment of brain morphology and the detection of abnormalities associated with various neurological disorders, including SZ [11]. To gain a deeper understanding of SZ and improve diagnostic accuracy along with treatment efficacy, it is imperative to complement structural imaging data with other types of data, such as functional connectome and genomic data. Functional magnetic resonance imaging (fMRI) yields connectome data, capturing brain activity patterns and functional connections via spatial independent component analysis, crucial for identifying aberrant functional network connectivity patterns associated with SZ [10]. Single nucleotide polymorphisms (SNPs) are key genetic variations in the human genome, pivotal for understanding phenotypic traits and disorders such as SZ [18]. Each of these modalities offers unique insights into different aspects of the disorder [10]. Integrating these diverse modalities, imaging & genomics [IMAGEN], could reveal new pathways by linking macroscopic brain differences with microscopic molecular insights into neuro-degenerative diseases including SZ [20].\nMultiple existing studies have explored utilizing individual modalities or their fusion through a simple concatenation [14] [4] [13]. However, methods relying on feature concatenation potentially overlook crucial inter-modality relationships. Addressing this, attentional feature fusion was introduced to attentively integrate multi-modal data in a linear way [6]. However, this may overlook complex interrelationships especially with increased modalities, potentially limiting integration efficacy. In addition to these, literature often resorts to methods like SparseCCA (SCCA) [7], Graph Neural Networks (GNN) [3], & Attentive Deep-CCA (Att-DCCA) [30], for integrating imaging genetics. Attentive DeepCCA, SparseCCA aims to find linear projections by maximizing joint correlation between 2 modalities having considerations in modeling complex & nonlinear relationships. GNNs fuse image & omics data through Graph Convolution (GCN), learning multi-modal relationships, but may not capture cross-modal interactions as explicitly. The vision transformer (ViT), has been employed to integrate functional and structural MRI features, providing a non-linear approach to learning attentive representations [1]. Despite success in integrating multiple imaging modalities, a major challenge lies in integrating imaging with genomic biomarkers due to their complex nature, differing dimensionality, scale, and intricate relationships [5] [23]. Genomic biomarkers may capture molecular variations influencing brain structure and function, further complicating integration [26]. Addressing these challenges, we propose a novel three-way step-wise attentive integration approach that leverages the complementary information across genomics, connectome, and sMRI features to enhance SZ diagnostic accuracy. The integration order (genomics\u2192connectome\u2192sMRI) is strategically chosen based on inherent correlations between the modalities, and any change could impact the model's performance [27]. Primary contributions of this research include:\nAttentive Integration for Multi-modal Imaging Genomics: Integrating genomic and connectome features attentively through Cross-modal Multi-Head Attention, prioritizing genomic and connectome interactions, and further integrating with sMRI features, selectively attending to relevant regions, and extracting complementary information from all modalities.\nClinical Interpretability: Ranking the top SNPs, identifying significant connections within the connectome, and highlighting SZ-specific regions in SMRIs for enhanced clinical insight."}, {"title": "2 Multi-modal Imaging Genomics Transformer", "content": "This study presents our Multi-modal Imaging Genomics Transformer (MIGTrans), a deep learning model that combines structural brain imaging and functional connectome with genomic data using Cross-modal Multi-Head Attention [28] to distinguish between individuals with SZ and healthy controls (HCs). Figure 1 illustrates MIGTrans, while subsequent sections delve into its technical modules."}, {"title": "2.1 Genomic Connectome Representation Learning", "content": "Genomic Encoder We define the learning of genomic representations through a neural network function $\\Phi(\\cdot)$, which takes SNP, the genomic descriptor, $G \\in \\mathbb{R}^{d}$ as input and learns non-linear genomic representations $\\tilde{G} \\in \\mathbb{R}^{\\tilde{d}}$. This network consists of two dense layers with 2048 and 1536 units, activated by the Gaussian Error Linear Unit (GELU). Additionally, each layer is equipped with layer normalization and dropout mechanisms, ensuring robustness and preventing overfitting. The final layer outputs latent representations of the genomic features $G$, encapsulating crucial information for discriminating SZ pathology.\n$G' = \\text{LayerNorm}(\\text{Dropout} (\\Gamma(G. W_{1} +b_{1}), P_{1}))$\t\t(1)\n$\\tilde{G} = \\text{F}(\\text{LayerNorm}(\\text{Dropout}(\\Gamma(G' \\cdot W_{2} + b_{2}), p_{2})) \\cdot W_{3} + b_{3})$\t\t(2)\nEquations 1 and 2 illustrate the operations of $\\Phi(G)$ where $G'$ denote intermediate genomic representations processed by neural network layers. The GELU activation $\\Gamma(x) = x \\cdot (1 + \\text{erf}(\\frac{x}{\\sqrt{2}}))^{dx}$ ensures non-linearity and preserves genomic complexities [12]. $W_{1}, W_{2}$, and $W_{3}$ are weight matrices, and $b_{1}, b_{2}$, and $b_{3}$ are bias terms applied during linear transformations. $p_{1}$ and $p_{2}$ are control dropout probabilities.\nConnectome Encoder Similar to genomics, we define connectome representation learning through another neural network function $\\Psi(\\cdot)$, which takes the functional connectome $C \\in \\mathbb{R}^{f}$ as input and learns non-linear connectome rep- resentations $\\tilde{C} \\in \\mathbb{R}^{\\tilde{f}}$. This function comprises a dense layer with 1536 units, activated by GELU, followed by layer normalization and dropout mechanisms. Subsequently, the output layer generates latent representations of the connectome, vital for capturing intricate brain connectivity patterns.\n$C' = \\text{LayerNorm}(\\text{Dropout} (\\Gamma(C. W_{4} + b_{4}), P_{3}))$ (3)\n$\\tilde{C} = \\Gamma(C' \\cdot W_{5} +b_{5})$ (4)\nEquations 3 and 4 depict the processing steps of $\\Psi(C)$, where $C'$ represents an intermediate connectome representation refined by the neural network layer, and $\\tilde{C}$ denotes the final output containing learned connectome features. $\\Gamma(\\cdot)$ denotes the GELU activation function. The weight matrices $W_{4}$ and $W_{5}$, along with bias terms $b_{4}$ and $b_{5}$, facilitate linear transformations essential for feature extraction. Additionally, the dropout probability $p_{3}$ regulates information flow, preventing overfitting, while layer normalization enhances the robustness of the network by standardizing input values."}, {"title": "2.2 Morphological Representation Learning", "content": "Structural MRI Encoder We employ Transfer Learning paradigm and use a pre-trained 3D DenseNet121 to extract morphological features from gray matter density in sMRI images. This model transforms input sMRI $S$ using a sequence of operations, including 3D convolutions, pooling, dense layers, and transition layers. The resulting output vector $X$ encapsulates the subject's brain morphological features and is passed to a spatial sequence attention (SSA) module for learning attention across spatial and channel dimensions.\nSpatial Sequence Attention The SSA mechanism is tailored to capture spatial and channel dependencies within morphological feature maps $X$ [22]. Comprising a 3D convolutional (Conv3D) layer, a ConvLSTM layer, and another Conv3D layer, each component plays a crucial role in enhancing the feature maps [24]. The initial Conv3D layer extracts holistic features from the input, while the ConvLSTM, a variant of LSTM with convolutional operations, captures intricate spatial interactions and relationships across channels. By incorporating tanh and sigmoid activation functions, the ConvLSTM controls the flow of information and learns from sparse connections between input and state transitions, enabling it to learn both short and long-term dependencies within 3D feature maps [25]. Finally, the output of Conv3D layer projects the refined features back to the original feature space. The evolution of the cell state ($C_{t}$) and hidden state ($H_{t}$) at each time step $t$ is determined by input gate ($I_{t}$), forget gate ($F_{t}$), and output gate ($O_{t}$), considering both the current and past feature maps in the input sequence.\n$I_{t} = \\sigma(W_{xi} * X_{t} + W_{hi} * H_{t-1} + W_{ci} * C_{t-1} + b_{i})$ (5)\n$F_{t} = \\sigma(W_{xf} * X_{t} + W_{hf} * H_{t-1} + W_{cf} * C_{t-1} + b_{f})$ (6)\n$C_{t} = F_{t} \\odot C_{t-1} + I_{t} \\odot \\text{tanh}(W_{xc} * X_{t} + W_{hc} * H_{t-1}+b_{c})$\t\t(7)\n$O_{t} = \\sigma(W_{xo} * X_{t} + W_{ho} * H_{t-1} + W_{co} * C_{t-1} + b_{o})H_{t}$\t\t(8)\nThe ConvLSTM operates based on fundamental equations 5 to 8, involving convolution (*) and element-wise product ($\\odot$). The input gate ($I_{t}$) evaluates the importance of input feature maps ($X_{t}$) for inclusion in the cell state ($C_{t}$), while the forget gate ($F_{t}$) determines the retention of features from the previous cell state ($C_{t-1}$). The cell state ($C_{t}$) is updated by selectively integrating input features and forgetting unwanted ones. Subsequently, the output gate ($O_{t}$) regulates the exposure of the updated cell state as the output ($H_{t}$). Through learned weight matrices($W_{xi},W_{hi},W_{ci},...$) and bias vectors ($b_{i},b_{f}, b_{c},...$), the Con- VLSTM adapts to the specific characteristics of the data, effectively capturing spatial and channel dependencies within morphological feature maps. Finally, SSA outputs attentive sMRI features $X$."}, {"title": "2.3 Fusion Transformer", "content": "Genomic Connectome TransFusor In this module, the learned genomic $\\tilde{G}$ and connectome $\\tilde{C}$ features are attentively integrated through Cross-modal Multi-Head Attention (x-MHA) with 2 heads, emphasizing the fusion of functional connectome from fMRI with non-imaging genomic biomarkers. Genomic features are transformed and passed as Query ($Q = \\text{Linear}(\\tilde{G})$), Key ($K = \\text{Linear}(\\tilde{G})$) and connectome features are passed as Value ($V$) to x-MHA. This strategic arrangement enables the model to compute attention between genomic and connectome features through a scaled dot-product mechanism, referred as Self-Attention (SA). Precisely, the attention scores are computed as the softmax of the scaled dot-product of the transformed genomic features, facilitating the selective focus on relevant interactions, and multiplying with connectome features. The resulting representation is combined with the original connectome features using layer normalization and element-wise addition. Further, the resultant features undergo element-wise multiplication with the original connectome features, yielding the final fused connectome representation (GC').\n$\\text{SA}(Q, K, V) = \\text{Softmax}(\\frac{Q K^{T}}{\\sqrt{d_{k}}})V$\t\t(9)\nx-MHA(Q, K, V) = (h1 + ... + hh)Wo \u2200hi = SA(QWiK, KWiK, VWiV)\t\t(10)\nGC' = $\\tilde{C} \\odot \\text{LayerNorm}(\\tilde{C} + \\text{x-MHA}(Q, K, V))$\t\t(11)\nEquations 9 to 11 illustrate the mathematical operations in scaled dot-product attention and x-MHA with two heads, as well as the formulation of the genomic connectome attention features. The symbols $Q, K$, and $V$ represent matrices of queries, keys, and values, respectively. $d_{k}$ denotes the dimensionality of the keys, while Wi, WK, WV, and Wo are parameter matrices for linear transformation. The $\\oplus$ signifies concatenation, $\\odot$ indicates point-wise multiplication and the Softmax function is applied element-wise to normalize the attention scores in scaled dot-product attention. This approach is tailored to extract meaningful insights from the complex interplay between genomic and connectome data, enabling the model to effectively capture intricate dependencies specific to SZ.\nGenomic Connectome sMRI TransFusor Similar to the previous module, Genomic Connectome integration with sMRI is facilitated through x-MHA mechanism with 2 heads. This module aims to capture cross-modal relationships between imaging and non-imaging biomarkers. The genomic connectome features (GC') are passed as Query Q and Key K, while the squeezed sMRI attention features (X) are passed as Value V to the x-MHA. This strategic setup enables the model to selectively attend to relevant regions in the SMRI features based on the genomic connectome attention features, thereby incorporating complementary information between modalities. The attention-weighted representations are fused with the original sMRI features through a process of layer normalization and element-wise addition. This fusion step ensures that the model can adaptively combine information from multi modalities while preserving the structural characteristics of the sMRI. Subsequently, the fused representations undergo further vectorized fusion through dense layers. This helps retain cross-modality features alongside fused representations, thereby improving the model's generalization performance.\nGCS' = X $\\odot$ LayerNorm(X + x-MHA(Q, K, V))\t\t(12)\nEquation 12 elucidate the post-processing fusion after x-MHA, whose operations were illustrated in equations 9 and 10. The Feed Forward Neural Network acts as a classification head and utilizes the fused feature representations GCS' to predict the target labels $\\hat{y} \\in \\text{{SZ, HC}}$. It consists of two dense layers with 512, 256 units each, leveraging GELU activation to extract intricate features and introduce non-linearity, enhancing the model's representational capacity. Employing layer normalization and a dropout mitigates overfitting, fostering improved model generalization by preventing excessive reliance on specific features. All the modules discussed above were trained end-to-end to minimize the cross-entropy loss, $\\mathcal{L}(y, \\hat{y}) = - \\sum_{i=1}^{N} (Y_{i} \\log(y_{i}) + (1 - y_{i}) \\log(1 - \\hat{y}_{i}))$, using the Adam optimizer for gradient optimization. The cross-entropy loss function $\\mathcal{L}$ measures the dissimilarity between true labels $y$ and predicted probabilities $\\hat{y}$."}, {"title": "3 Experiments & Results", "content": "To prove the effectiveness of the proposed MIGTrans, we employed a subset of the Function Biomedical Informatics Research Network (FBIRN) dataset, containing sMRI, fMRI, and SNP data from 186 participants, including 82 SZ and 104 HC subjects [16]. In our work, we select a subset of 4942 SNPs (one-hot encoded) related to SZ as identified by the psychiatric genomics consortium, extract the lower triangular matrix from (53 \u00d7 53) FNC matrix generated using Neuromark Atlas [8], resulting in 1378 unique connections. Additionally, each sMRI is down- sampled to (121\u00d7145\u00d7121) dimensions & converted to gray matter density map using SPM12 [19]. We employ 5-fold cross validation to evaluate the effectiveness of proposed model. Throughout our experiments, various hyperparameter values were explored, including learning rates (0.005 to 0.05), dropout rates (0.1 to 0.5), and regularization rates (0.001 to 0.1). Selected values to optimize model"}, {"title": "3.2 Quantitative Evaluation", "content": "We conducted initial experiments to check the performance of single-modality approach using attention mechanism. We designed Self-Attention Networks for genomics (Genomic SANet) and connectome (Connectome SANet), and Spatial Sequence Attention Network (SSANet) for sMRI features. For multi-modal approaches, we explored various fusion strategies such as simple concatenation (Concat), Attentive Feature Fusion (AFF), SCCA, Att-DCCA for comparison with our proposed transformer-based method (Trans). As an ablation study, we also report the performance of the first-step fusion with genomics, connectome (GC) and the follow-up fusion with sMRI (GCS). Table 1 summarizes the classification performance of our proposed method and the baselines. The connectome alone performed best among single-modal features with 82.33%, supporting the SZ clinical finding [14]. However, this is slightly lower than the 82.71% achieved by simply combining genomic and connectome attentive features. From this, it is clearly evident that multi-modal features are superior to when they are considered in isolation. In terms of multi-modal feature integration, our proposed Trans method showed significant improvement in all metrics for two-way (GC) fusion when in comparison with concatenation and AFF, indicating 2.11%, 0.99% improvement. This trend continued even in three-way (GCS) fusion with 1.72% 1.73%, 1.74% 1.75% improvement when compared to SCCA, AFF, Att-DCCA and Concat in terms of accuracy. This highlights the benefit of our step-wise attentive integration of imaging with genomic biomarkers through cross-modal multi-head attention. Additionally, it is noteworthy that AFF showed a 0.02% lower accuracy compared to concatenation in three-way GCS fusion, indicating its struggle to maintain superiority as the number of modalities to integrate increases, given its reliance on linear fusion."}, {"title": "3.3 Qualitative Evaluation", "content": "To provide interpretation and clinical validation of our approach, we presented the attention maps of SMRI and important connections in functional connec-"}, {"title": "4 Conclusion", "content": "The Multi-modal Imaging Genomics Transformer (MIGTrans) integrates genomics, connectome, and structural imaging features through a novel three-way step-wise attentive integration, offering a comprehensive approach to capturing structural/functional brain abnormalities and genetic factors associated with schizophrenia. This approach surpasses the performance of baseline methods, demonstrating its effectiveness in enhancing clinical decision-making and improving diagnostic accuracy. With its ability to leverage complementary information from diverse data modalities, it identifies significant genomic locations, captures brain connectivity patterns, and highlights spatial regions specific to schizophrenia. MIGTrans holds promise for advancing our understanding of schizophrenia and facilitating more personalized therapeutic interventions in clinical settings."}]}