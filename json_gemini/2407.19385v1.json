{"title": "Multi-modal Imaging Genomics Transformer: Attentive Integration of Imaging with Genomic Biomarkers for Schizophrenia Classification", "authors": ["Nagur Shareef Shaik", "Teja Krishna Cherukuri", "Vince D. Calhoun", "Dong Hye Ye"], "abstract": "Schizophrenia (SZ) is a severe brain disorder marked by diverse cognitive impairments, abnormalities in brain structure, function, and genetic factors. Its complex symptoms and overlap with other psychiatric conditions challenge traditional diagnostic methods, necessitating advanced systems to improve precision. Existing research studies have mostly focused on imaging data, such as structural and functional MRI, for SZ diagnosis. There has been less focus on the integration of genomic features despite their potential in identifying heritable SZ traits. In this study, we introduce a Multi-modal Imaging Genomics Transformer (MIGTrans), that attentively integrates genomics with structural and functional imaging data to capture SZ-related neuroanatomical and connectome abnormalities. MIGTrans demonstrated improved SZ classification performance with an accuracy of 86.05% (\u00b10.02), offering clear interpretations and identifying significant genomic locations and brain morphological/connectivity patterns associated with SZ.", "sections": [{"title": "1 Introduction", "content": "According to the World Health Organization (WHO), approximately 24 million individuals worldwide are affected by Schizophrenia (SZ), a condition characterized by persistent hallucinations and disruptive behavior that significantly impacts their daily life [2]. The symptoms of SZ often overlap with those of other psychiatric disorders, complicating both diagnosis and treatment [21]. Structural magnetic resonance imaging (sMRI) is often employed to provide detailed images of brain anatomy, enabling the assessment of brain morphology and the detection of abnormalities associated with various neurological disorders, including SZ [11]. To gain a deeper understanding of SZ and improve diagnostic accuracy along with treatment efficacy, it is imperative to complement structural imaging data with other types of data, such as functional connectome and genomic data. Functional magnetic resonance imaging (fMRI) yields connectome data, capturing brain activity patterns and functional connections via spatial independent component analysis, crucial for identifying aberrant functional network"}, {"title": "2 Multi-modal Imaging Genomics Transformer", "content": "This study presents our Multi-modal Imaging Genomics Transformer (MIGTrans), a deep learning model that combines structural brain imaging and functional"}, {"title": "2.1 Genomic Connectome Representation Learning", "content": "Genomic Encoder We define the learning of genomic representations through a neural network function $\\phi(\\cdot)$, which takes SNP, the genomic descriptor, $G\\in \\mathbb{R}^d$ as input and learns non-linear genomic representations $\\overline{G}\\in \\mathbb{R}^{\\overline{d}}$. This network consists of two dense layers with 2048 and 1536 units, activated by the Gaussian Error Linear Unit (GELU). Additionally, each layer is equipped with layer normalization and dropout mechanisms, ensuring robustness and preventing overfitting. The final layer outputs latent representations of the genomic features G, encapsulating crucial information for discriminating SZ pathology.\n$G' = LayerNorm(Dropout (\\Gamma(G. W_1 +b_1), P_1))$\n$\\overline{G} = F(LayerNorm(Dropout(\\Gamma(G' \\cdot W_2 + b_2), p_2)) \\cdot W_3 + b_3)$\nEquations 1 and 2 illustrate the operations of $\\phi(G)$ where G' denote intermediate genomic representations processed by neural network layers. The GELU activation $\\Gamma(x) = x\\frac{1}{2} (1+ erf(\\frac{x}{\\sqrt{2}}))$ ensures non-linearity and preserves genomic complexities [12]. $W_1$, $W_2$, and $W_3$ are weight matrices, and $b_1$, $b_2$, and $b_3$ are bias terms applied during linear transformations. $p_1$ and $p_2$ are control dropout probabilities.\nConnectome Encoder Similar to genomics, we define connectome representation learning through another neural network function $\\psi(\\cdot)$, which takes the"}, {"title": "2.2 Morphological Representation Learning", "content": "Structural MRI Encoder We employ Transfer Learning paradigm and use a pre-trained 3D DenseNet121 to extract morphological features from gray matter density in sMRI images. This model transforms input sMRI S using a sequence of operations, including 3D convolutions, pooling, dense layers, and transition layers. The resulting output vector X encapsulates the subject's brain morphological features and is passed to a spatial sequence attention (SSA) module for learning attention across spatial and channel dimensions.\nSpatial Sequence Attention The SSA mechanism is tailored to capture spatial and channel dependencies within morphological feature maps X [22]. Comprising a 3D convolutional (Conv3D) layer, a ConvLSTM layer, and another Conv3D layer, each component plays a crucial role in enhancing the feature maps [24]. The initial Conv3D layer extracts holistic features from the input, while the ConvLSTM, a variant of LSTM with convolutional operations, captures intricate spatial interactions and relationships across channels. By incorporating tanh and sigmoid activation functions, the ConvLSTM controls the flow of information and learns from sparse connections between input and state transitions, enabling it to learn both short and long-term dependencies within 3D feature maps [25]. Finally, the output of Conv3D layer projects the refined features back to the original feature space. The evolution of the cell state ($C_t$) and hidden state ($H_t$) at each time step t is determined by input gate ($I_t$), forget gate ($F_t$), and output gate ($O_t$), considering both the current and past feature maps in the input sequence.\n$I_t = \\sigma(W_{xi} * X_t + W_{hi} * H_{t-1} + W_{ci} * C_{t-1} + b_i)$\n$F_t = \\sigma(W_{xf} * X_t + W_{hf} * H_{t-1} + W_{cf} * C_{t-1} + b_f)$"}, {"title": "2.3 Fusion Transformer", "content": "Genomic Connectome TransFusor In this module, the learned genomic $\\overline{G}$ and connectome $\\overline{C}$ features are attentively integrated through Cross-modal Multi-Head Attention (x-MHA) with 2 heads, emphasizing the fusion of functional connectome from fMRI with non-imaging genomic biomarkers. Genomic features are transformed and passed as Query (Q = Linear($\\overline{G}$)), Key (K = Linear($\\overline{G}$)) and connectome features are passed as Value (V) to x-MHA. This strategic arrangement enables the model to compute attention between genomic and connectome features through a scaled dot-product mechanism, referred as Self-Attention (SA). Precisely, the attention scores are computed as the softmax of the scaled dot-product of the transformed genomic features, facilitating the selective focus on relevant interactions, and multiplying with connectome features. The resulting representation is combined with the original connectome features using layer normalization and element-wise addition. Further, the resultant features undergo element-wise multiplication with the original connectome features, yielding the final fused connectome representation (GC').\n$SA(Q, K, V) = Softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$\nx-MHA(Q, K, V) = (h\u2081 + ... + h\u2081)W\u00b0 \u2200h\u1d62 = SA(QW\u1d65^Q, KW\u1d65^K, VW\u1d65^V)\nGC' = $\\overline{C} \\odot LayerNorm(\\overline{C}$ + x-MHA(Q, K, V))\nEquations 9 to 11 illustrate the mathematical operations in scaled dot-product attention and x-MHA with two heads, as well as the formulation of the genomic connectome attention features. The symbols Q, K, and V represent matrices of queries, keys, and values, respectively. $d_k$ denotes the dimensionality of the keys, while $W^Q$, $W^K$, $W^V$, and $W^O$ are parameter matrices for linear transformation. The $\\oplus$ signifies concatenation, $\\odot$ indicates point-wise multiplication and the Softmax function is applied element-wise to normalize the attention scores in scaled dot-product attention. This approach is tailored to extract meaningful insights from the complex interplay between genomic and connectome data, enabling the model to effectively capture intricate dependencies specific to SZ."}, {"title": "3 Experiments & Results", "content": "To prove the effectiveness of the proposed MIGTrans, we employed a subset of the Function Biomedical Informatics Research Network (FBIRN) dataset, containing sMRI, fMRI, and SNP data from 186 participants, including 82 SZ and 104 HC subjects [16]. In our work, we select a subset of 4942 SNPs (one-hot encoded) related to SZ as identified by the psychiatric genomics consortium, extract the lower triangular matrix from (53 \u00d7 53) FNC matrix generated using Neuromark Atlas [8], resulting in 1378 unique connections. Additionally, each sMRI is down-sampled to (121\u00d7145\u00d7121) dimensions & converted to gray matter density map using SPM12 [19]. We employ 5-fold cross validation to evaluate the effectiveness of proposed model. Throughout our experiments, various hyperparameter values were explored, including learning rates (0.005 to 0.05), dropout rates (0.1 to 0.5), and regularization rates (0.001 to 0.1). Selected values to optimize model"}, {"title": "3.2 Quantitative Evaluation", "content": "We conducted initial experiments to check the performance of single-modality approach using attention mechanism. We designed Self-Attention Networks for genomics (Genomic SANet) and connectome (Connectome SANet), and Spatial Sequence Attention Network (SSANet) for sMRI features. For multi-modal approaches, we explored various fusion strategies such as simple concatenation (Concat), Attentive Feature Fusion (AFF), SCCA, Att-DCCA for comparison with our proposed transformer-based method (Trans). As an ablation study, we also report the performance of the first-step fusion with genomics, connectome (GC) and the follow-up fusion with sMRI (GCS). Table 1 summarizes the classification performance of our proposed method and the baselines. The connectome alone performed best among single-modal features with 82.33%, supporting the SZ clinical finding [14]. However, this is slightly lower than the 82.71% achieved by simply combining genomic and connectome attentive features. From this, it is clearly evident that multi-modal features are superior to when they are considered in isolation. In terms of multi-modal feature integration, our proposed Trans method showed significant improvement in all metrics for two-way (GC) fusion when in comparison with concatenation and AFF, indicating 2.11%, 0.99% improvement. This trend continued even in three-way (GCS) fusion with 1.72% 1.73%, 1.74% 1.75% improvement when compared to SCCA, AFF, Att-DCCA and Concat in terms of accuracy. This highlights the benefit of our step-wise attentive integration of imaging with genomic biomarkers through cross-modal multi-head attention. Additionally, it is noteworthy that AFF showed a 0.02% lower accuracy compared to concatenation in three-way GCS fusion, indicating its struggle to maintain superiority as the number of modalities to integrate increases, given its reliance on linear fusion."}, {"title": "4 Conclusion", "content": "The Multi-modal Imaging Genomics Transformer (MIGTrans) integrates genomics, connectome, and structural imaging features through a novel three-way step-wise attentive integration, offering a comprehensive approach to capturing structural/functional brain abnormalities and genetic factors associated with schizophrenia. This approach surpasses the performance of baseline methods, demonstrating its effectiveness in enhancing clinical decision-making and improving diagnostic accuracy. With its ability to leverage complementary information from diverse data modalities, it identifies significant genomic locations, captures brain connectivity patterns, and highlights spatial regions specific to schizophrenia. MIGTrans holds promise for advancing our understanding of schizophrenia and facilitating more personalized therapeutic interventions in clinical settings."}]}