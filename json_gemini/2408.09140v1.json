{"title": "Learning to Explore for Stochastic Gradient MCMC", "authors": ["SeungHyun Kim", "Seohyeon Jung", "Seonghyeon Kim", "Juho Lee"], "abstract": "Bayesian Neural Networks (BNNs) with high-dimensional parameters pose a challenge for posterior inference due to the multi-modality of the posterior distributions. Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) with cyclical learning rate scheduling is a promising solution, but it requires a large number of sampling steps to explore high-dimensional multi-modal posteriors, making it computationally expensive. In this paper, we propose a meta-learning strategy to build SGMCMC which can efficiently explore the multi-modal target distributions. Our algorithm allows the learned SGMCMC to quickly explore the high-density region of the posterior landscape. Also, we show that this exploration property is transferrable to various tasks, even for the ones unseen during a meta-training stage. Using popular image classification benchmarks and a variety of downstream tasks, we demonstrate that our method significantly improves the sampling efficiency, achieving better performance than vanilla SGMCMC without incurring significant computational overhead.", "sections": [{"title": "1. Introduction", "content": "Bayesian methods have received a lot of attention as powerful tools for improving the reliability of machine learning models. Bayesian methods are gaining prominence due to their ability to offer probability distributions over model parameters, thereby enabling the quantification of uncertainty in predictions. They find primary utility in safety-critical domains like autonomous driving, medical diagnosis, and finance, where the accurate modeling of prediction uncertainty often takes precedence over the predictions themselves. The integration of Bayesian modeling with (deep) neural networks, often referred to as Bayesian Neural Networks (BNNS), introduces exciting prospects for the development of secure and trustworthy decision-making systems.\nHowever, there are significant problems for the successful application of BNNS in real-world scenarios. Bayesian inference in high-dimensional parameter space, especially for deep and large models employed for the applications mentioned above, is notoriously computationally expensive and often intractable due to the complexity of the posterior distribution. Moreover, posterior landscapes of BNNS frequently display multi-modality, where multiple high density regions exist, posing a significant challenge to efficient exploration and sampling. Due to this difficulty, the methods that are reported to work well for relatively small models, for instance, variational inference (Blei & McAuliffe, 2017) or Hamiltonian Monte Carlo (HMC) (Neal et al., 2011), can severly fail for deep neural networks trained with large amount of data, when applied without care.\nRecently, Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) methods (Welling & Teh, 2011; Chen et al., 2014; Ma et al., 2015) have emerged as powerful tools for enhancing the scalability of approximate Bayesian inference. This advancement has opened up the possibilities of applying Bayesian methods to large-scale machine learning tasks. SGMCMC offers a versatile array of methods for constructing Markov chains that converge towards the target posterior distributions. The simulation of these chains primarily relies on stochastic gradients, making them particularly suitable for BNNS trained on large-scale datasets. However, despite the notable successes of SGMCMC in some BNN applications (Welling & Teh, 2011; Chen et al., 2014; Ma et al., 2015; Zhang et al., 2020), there remains a notable challenge. Achieving optimal performance often demands extensive engineering efforts and hyperparameter tuning. This fine-tuning process typically involves human trial and error or resource-intensive cross-validation procedures. Furthermore, it's worth noting that even with the use of SGMCMC methods, there remains room for improvement in efficiently exploring multi-modal posterior distributions.\nAs a result, in practical applications, a trade-off between precision and computational resources often becomes necessary.\nTo address these challenges, we introduce a novel meta-learning framework tailored to promote the efficient exploration of SGMCMC algorithms. Traditional SGMCMC methods often rely on handcrafted design choices inspired by mathematical or physics principles. Recognizing the pivotal role these design components play in shaping the trade-off between exploration and exploitation within SGMCMC chains, we argue in favor of learning them directly from data rather than manually specifying them. To achieve this, we construct neural networks to serve as meta-models responsible for approximating the gradients of kinetic energy terms. These meta-models are trained using a diverse set of BNNS inference tasks, encompassing various datasets and architectural configurations. Our proposed approach, termed Learning to Explore (L2E), exhibits several advantageous properties, including better mixing rates, improved prediction performance, and a reduced need for laborious hyperparameter tuning.\nOur contributions can be summarized as follows:\n\u2022 We introduce L2E, a novel meta-learning framework enhancing SGMCMC methods. In contrast to conventional hand-designed approaches and meta-learning approach (Gong et al., 2018), L2E learns the kinetic energy term directly, offering a more data-driven and adaptable solution.\n\u2022 We present a multitask training pipeline equipped with a scalable gradient estimator for L2E. This framework allows the meta-learned SGMCMC techniques to generalize effectively across a wide range of tasks, extending their applicability beyond the scope of tasks encountered during meta-training.\n\u2022 Using real-world image classification benchmarks, we demonstrate the remarkable performance of BNNS inferred using the SGMCMC algorithm discovered by L2E, both in terms of prediction accuracy and sampling efficiency."}, {"title": "2. Backgrounds", "content": ""}, {"title": "2.1. SGMCMC for Bayesian Neural Networks", "content": "Settings. In this paper, we focus on supervised learning problems with a training dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ with $x_i$ being observation and $y_i$ being label. Given a neural network with a parameter $\\theta \\in \\mathbb{R}^d$, a likelihood $p(y \\mid x, \\theta)$ and a prior $p(\\theta)$ are set up, together defining an energy function $U(\\theta) = - \\sum_{i=1}^n \\log p(y_i \\mid x_i, \\theta) - \\log p(\\theta)$. The goal is to infer the posterior distribution $p(\\theta \\mid \\mathcal{D}) \\propto \\exp(-U(\\theta))$.\nWhen the size of the dataset $n$ is large, evaluating the energy function $U(\\theta)$ or its gradient $\\nabla_{\\theta} U(\\theta)$ may be undesirably costly as they require a pass through the entire dataset $\\mathcal{D}$. For such scenarios, SGMCMC (Welling & Teh, 2011; Chen et al., 2014; Ma et al., 2015) is a standard choice, where the gradients of the energy function $\\nabla_{\\theta} U(\\theta)$ are approximated by a stochastic gradient computed from mini-batches. That is, given a mini-batch $\\mathcal{B} \\subset \\{1, ..., n\\}$ where $|\\mathcal{B}| < n$, an unbiased estimator of the full gradient $\\nabla_{\\theta} U(\\theta)$ with $\\mathcal{B}$ can be computed as\n$\\nabla \\tilde{U}(\\theta) = \\frac{n}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\nabla_{\\theta} \\log p(y_i \\mid x_i, \\theta) - \\nabla_{\\theta} \\log p(\\theta)$\nA complete recipe. There may be several ways to build a Markov chain leading to the target posterior distribution. Ma et al. (2015) presented a generic recipe that includes all the convergent SGMCMC algorithms as special cases, constituting a complete framework. In this recipe, a parameter $\\theta$ of interest is augmented with an auxiliary momentum variable $r$, and an Stochastic Differential Equation (SDE) of the following form is defined for a joint variable $z = (\\theta, r) \\in \\mathbb{R}^{2d}$ as follows.\n$\\begin{aligned} d z & = [-(D(z)+Q(z)) \\nabla_z H(z)+F(z)] d t+\\sqrt{2 D(z)} d w_t \\\\ H(z) & :=U(\\theta)+g(\\theta, r) \\\\ \\Gamma_{\\epsilon}(z) & :=\\sum_{i=1}^{2 d} \\frac{\\partial}{\\partial z_j}\\left(D_{i j}(z)+Q_{i j}(z)\\right), \\end{aligned}$\nwhere $g(\\theta, r)$ is the conditional energy function of the momentum $r$ such that $p(z) \\propto \\exp(-H(z))$ and $w_t$ is 2d-dimensional Brownian motion. Here, $D(z) \\in \\mathbb{R}^{2 d \\times 2 d}$ and $Q(z) \\in \\mathbb{R}^{2 d \\times 2 d}$ are restricted to be positive semi-definite and skew-symmetric, respectively. Given this SDE, one can obtain a SGMCMC algorithm by first substituting the full gradient $\\nabla_z H(z)$ with a mini-batch gradient $\\nabla_z \\tilde{H}(z) = \\nabla_z(\\tilde{U}(\\theta)+g(\\theta, r))$ and then discretizing it via a numerical solver such as sympletic Euler method. A notable example would be Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014), where $g(\\theta, r) = r^\\top M^{-1} r$,\n$D(z)=\\left(\\begin{array}{ll} 0 & 0 \\\\ 0 & C \\end{array}\\right)$ and $Q(z)=\\left(\\begin{array}{cc} 0 & I \\\\ -I & 0 \\end{array}\\right)$\nfor some positive semi-definite matrices $M$ and $C$, leading to an algorithm when discretized with symplectic Euler method as follows.\n$\\begin{aligned} r_{t+1} & =r_t-\\epsilon_t \\nabla \\tilde{U}\\left(\\theta_t\\right)-\\epsilon_t C M^{-1} r_t+\\xi_t \\\\ \\theta_{t+1} & =\\theta_t+\\epsilon_t M^{-1} r_{t+1}, \\end{aligned}$\nwhere $\\xi_t \\sim \\mathcal{N}\\left(0,2 C \\epsilon_t\\right)$ and $\\epsilon_t$ is a step-size.\nThe complete recipe includes interesting special cases that introduce adaptive preconditioners to improve the mixing"}, {"title": "3. Main contribution: Learning to Explore", "content": ""}, {"title": "3.1. Overcome the limitations of Meta-SGMCMC", "content": "To avoid aforementioned limitations of Meta-SGMCMC, we newly design the meta-learning method for SGMCMC as follows:\n\u2022 We parameterize the gradients of the kinetic energy function and keep $D_f(z)$ and $Q_f(z)$ independent of $z$. We can avoid the additional computational cost of computing $\\Gamma(z)$ without sacrificing the flexibility of the sampler.\n\u2022 We use a new meta-objective called BMA meta-loss which is the Monte-Carlo estimator of the predictive distribution to enhance the exploration and performance of the learned sampler. Both of our meta-objective and its gradient can be computed in closed-form. Also, we employ Evolution Strategy (ES) (Salimans et al., 2017; Metz et al., 2019) for gradient estimation, which is an unbiased estimator of the true meta-gradient. This also consumes significantly less memory compared to analytic gradient methods, allowing us to keep the length of the inner-loop much longer during meta-learning."}, {"title": "3.2. Meta-learning framework for SGMCMC", "content": "Instead of using a hand-designed recipe for SGMCMC, we aim to learn the proper SGMCMC update steps through meta learning. The existing works, both the methods using hand-designed choices or meta-learning (Gong et al., 2018), try to determine the forms of the matrices $D(z)$ and $Q(z)$ while keeping the kinetic energy $g(\\theta, r)$ as simple Gaussian energy function, that is, $g(\\theta, r) = r^\\top M^{-1}r/2$. This choice indeed is theoretically grounded, which can be shown to be optimal when the target distribution is Gaussian (Betancourt, 2017), but may not be optimal for the complex multi-modal posteriors of BNNS. We instead choose to learn $g(\\theta, r)$ while keeping $D(z)$ and $Q(z)$ as simple as possible. We argue that the meta-learning approach based on this alternative parameterization is more effective in learning versatile SGMCMC procedure that scales to large BNNS.\nMore specifically, we parameterize the gradients of the kinetic energy function $\\nabla_{\\theta} g(\\theta, r)$ and $\\nabla_r g(\\theta, r)$ with neural networks $\\alpha_{\\phi}(\\theta, r)$ and $\\beta_{\\phi}(\\theta, r)$ respectively, and set $D(z)$ and $Q(z)$ as in SGHMC. The update step of SGMCMC, when discretized with symplectic Euler method is,\n$\\begin{aligned} r_{t+1} & =r_t-\\epsilon_t\\left[\\nabla_\\theta \\tilde{U}\\left(\\theta_t\\right)+\\alpha_\\phi\\left(\\theta_t, r_t\\right)+C \\beta_\\phi\\left(\\theta_t, r_t\\right)\\right]+\\xi_t \\\\ \\theta_{t+1} & =\\theta_t+\\epsilon_t \\beta_\\phi\\left(\\theta_t, r_{t+1}\\right). \\end{aligned}$\nwhere $\\xi_t \\sim \\mathcal{N}(0, 2C\\epsilon_t)$. Since we parameterize $\\nabla g(\\theta, r)$ and do not explicitly define the form of $g(\\theta, r)$, we make the following assumptions about the underlying function $g(\\theta, r)$.\nAssumption 3.1. There exists an energy function $g(\\theta, r)$ whose gradients with respect to $\\theta, r$ are $\\alpha_{\\phi}(\\theta, r)$ and $\\beta_{\\phi}(\\theta, r)$ respectively, and $\\int \\exp(-g(\\theta, r)) dr = constant$.\nWe also present another version of L2E which does not require additional assumptions in Appendix D. The neural networks $\\alpha_{\\phi}$ and $\\beta_{\\phi}$ are parameterized as two-layer Multi-Layer Perceptrons (MLPs) with 32 hidden units. Specifically, $\\alpha_{\\phi}$ and $\\beta_{\\phi}$ are applied to each dimension of parameter and momentum independently, similar to the commonly used learned optimizers (Andrychowicz et al., 2016; Metz et al., 2019). Again, following the common literature in learned optimizers (Metz et al., 2019), for each dimension of the parameter and momentum, we feed the corresponding parameter and momentum values, the stochastic gradients of energy functions for that element, and running average of the gradient at various time scales, as they are reported to encode the sufficient information about the loss surface geometry. See Appendix I for implementation details of $\\alpha_{\\phi}$ and $\\beta_{\\phi}$. By leveraging this information, we expect our meta-learned SGMCMC procedure to capture the multi-modal structures of the target posteriors of BNNS, and thus yielding a better mixing method."}, {"title": "3.3. Meta-Objective and Optimization", "content": "Objective functions for meta-learning. Meta-objective should reflect the meta-knowledge one wants to learn. We design the meta-objective based on the hope that samples collected through SGMCMC should be good at approximating the posterior predictive $p(y^* | x^*, \\mathcal{D})$. In order to achieve this goal, we propose the meta-objective called BMA meta-loss. After the sufficient number of inner-updates, we collect $K$ parameter samples with some interval between them (thinning). Let $\\theta_k(\\phi)$ be the $k$th collected parameter, and we compute the Monte-Carlo estimator of the predictive distribution and use it as a meta-objective function (note the dependency of $\\theta_k$ on the meta-parameter $\\phi$, as it is a consequence of learning SGMCMC with the meta-parameter $\\phi$).\n$\\mathcal{L}(\\phi) = - \\frac{1}{K} \\sum_{k=1}^K \\log p(y^* | x^*, \\theta_k(\\phi)),$\nwhere $(x, y)$ is a validation data point.\nGradient estimation for meta-objective. Estimating the meta-gradient $\\nabla_{\\phi} \\mathcal{L}(\\phi)$ is highly non-trivial (Metz et al., 2018; 2019), especially when the number of inner update steps is large. For instance, a na\u00efve method such as back-propagation through time would require memory grows linearly with the number of inner-steps, so become easily infeasible for even moderate sized models. One might consider using the truncation approximation, but that would result in a biased gradient estimator. Instead, we adapt ES (Salimans et al., 2017) with antithetic sampling scheme, which has been widely used in recent literature of training learned optimizer. Metz et al. (2019) showed that unrolled optimization with many inner-steps can lead to chaotic meta-loss surface and ES is capable of relieving this pathology by employing smoothed loss,\n$\\mathcal{L}(\\phi) = \\mathbb{E}_{\\eta \\sim \\mathcal{N}(\\phi, \\sigma^2 I)} [\\mathcal{L}(\\phi)]$\nwhere $\\sigma^2$ determines the degree of smoothing. Also, antithetic sampling is usually applied to reduce the estimation variance of $\\nabla_{\\phi} \\mathcal{L}(\\phi)$. Through log-derivative trick, we can get unbiased estimator of (8),\n$\\hat{g}=\\frac{1}{N} \\sum_{i=1}^N \\frac{\\mathcal{L}(\\phi+\\eta_i)}{\\sigma^2} \\eta_i$\nwhere $\\eta_i \\sim iid \\mathcal{N}(0, \\sigma^2 I)$. In addition, we can get another unbiased estimator $\\hat{g}^{-1} = -\\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\phi - \\eta_i) \\frac{\\eta_i}{\\sigma^2}$ by reusing the negative of $\\eta_i$. By taking the average of two estimators, we can obtain the following gradient estimator.\n$\\nabla \\mathcal{L}(\\phi) = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\mathcal{L}(\\phi+\\eta_i)-\\mathcal{L}(\\phi-\\eta_i)}{2\\sigma^2} \\eta_i$\nThe estimator is also amenable to parallelization, improving the efficiency of gradient computation."}, {"title": "3.4. Meta training procedure", "content": "Generic pipeline. General process of meta-training is as follows. First, for each inner-loop, we sample a task from the pre-determined task distribution. An inner-loop starts from an randomly initialized parameter and iteratively apply update step (6) to run a single chain of SGMCMC. Please refer to Algorithm 2 for detailed description. In the initial stage of meta-training, the chains from these inner loops show poor convergence, but the performance improves as training progresses. Similar to general Bayesian inference, we consider the early part of the inner loop as a burn-in period and collect samples from the end of the inner-loop at regular intervals when evaluating the meta-objective. This training process naturally integrates the meta-learning and Bayesian inference in that mimicking the actual inference procedure of Bayesian methods in realistic supervised learning tasks. In Figure 1 we show that L2E achieve desired level of accuracy for the approximation of posterior predictive with relatively small number of samples. This result indicates that L2E has successfully acquired the desired properties through meta-training.\nMultitask training for better generalization. In meta-learning, diversifying the task distribution is commonly known to enhance generalization. We include various neural network architectures and datasets in the task distribution to ensure that L2E has sufficient generalization capacity. Also, we evaluate how the task distribution diversity affects the performance of L2E in Table 11."}, {"title": "4. Experiments", "content": "In this section, we will evaluate the performance of L2E in various aspects. Through extensive experiments, we would like to demonstrate followings:\n\u2022 L2E shows excellent performance on real-world image classification tasks and seamlessly generalizes to the tasks not seen during meta-training.\n\u2022 L2E produces similar predictive distribution to HMC as well as good mixing of BNNS posterior distribution in weight space."}, {"title": "3.1. Overcome the limitations of Meta-SGMCMC", "content": "To avoid aforementioned limitations of Meta-SGMCMC, we newly design the meta-learning method for SGMCMC as follows:\n\u2022 We parameterize the gradients of the kinetic energy function and keep $D_f(z)$ and $Q_f(z)$ independent of $z$. We can avoid the additional computational cost of computing $\\Gamma(z)$ without sacrificing the flexibility of the sampler.\n\u2022 We use a new meta-objective called BMA meta-loss which is the Monte-Carlo estimator of the predictive distribution to enhance the exploration and performance of the learned sampler. Both of our meta-objective and its gradient can be computed in closed-form. Also, we employ Evolution Strategy (ES) (Salimans et al., 2017; Metz et al., 2019) for gradient estimation, which is an unbiased estimator of the true meta-gradient. This also consumes significantly less memory compared to analytic gradient methods, allowing us to keep the length of the inner-loop much longer during meta-learning."}, {"title": "3.2. Meta-learning framework for SGMCMC", "content": "Instead of using a hand-designed recipe for SGMCMC, we aim to learn the proper SGMCMC update steps through meta learning. The existing works, both the methods using hand-designed choices or meta-learning (Gong et al., 2018), try to determine the forms of the matrices $D(z)$ and $Q(z)$ while keeping the kinetic energy $g(\\theta, r)$ as simple Gaussian energy function, that is, $g(\\theta, r) = r^\\top M^{-1}r/2$. This choice indeed is theoretically grounded, which can be shown to be optimal when the target distribution is Gaussian (Betancourt, 2017), but may not be optimal for the complex multi-modal posteriors of BNNS. We instead choose to learn $g(\\theta, r)$ while keeping $D(z)$ and $Q(z)$ as simple as possible. We argue that the meta-learning approach based on this alternative parameterization is more effective in learning versatile SGMCMC procedure that scales to large BNNS.\nMore specifically, we parameterize the gradients of the kinetic energy function $\\nabla_{\\theta} g(\\theta, r)$ and $\\nabla_r g(\\theta, r)$ with neural networks $\\alpha_{\\phi}(\\theta, r)$ and $\\beta_{\\phi}(\\theta, r)$ respectively, and set $D(z)$ and $Q(z)$ as in SGHMC. The update step of SGMCMC, when discretized with symplectic Euler method is,\n$\\begin{aligned} r_{t+1} & =r_t-\\epsilon_t\\left[\\nabla_\\theta \\tilde{U}\\left(\\theta_t\\right)+\\alpha_\\phi\\left(\\theta_t, r_t\\right)+C \\beta_\\phi\\left(\\theta_t, r_t\\right)\\right]+\\xi_t \\\\ \\theta_{t+1} & =\\theta_t+\\epsilon_t \\beta_\\phi\\left(\\theta_t, r_{t+1}\\right). \\end{aligned}$\nwhere $\\xi_t \\sim \\mathcal{N}(0, 2C\\epsilon_t)$. Since we parameterize $\\nabla g(\\theta, r)$ and do not explicitly define the form of $g(\\theta, r)$, we make the following assumptions about the underlying function $g(\\theta, r)$.\nAssumption 3.1. There exists an energy function $g(\\theta, r)$ whose gradients with respect to $\\theta, r$ are $\\alpha_{\\phi}(\\theta, r)$ and $\\beta_{\\phi}(\\theta, r)$ respectively, and $\\int \\exp(-g(\\theta, r)) dr = constant$.\nWe also present another version of L2E which does not require additional assumptions in Appendix D. The neural networks $\\alpha_{\\phi}$ and $\\beta_{\\phi}$ are parameterized as two-layer Multi-Layer Perceptrons (MLPs) with 32 hidden units. Specifically, $\\alpha_{\\phi}$ and $\\beta_{\\phi}$ are applied to each dimension of parameter and momentum independently, similar to the commonly used learned optimizers (Andrychowicz et al., 2016; Metz et al., 2019). Again, following the common literature in learned optimizers (Metz et al., 2019), for each dimension of the parameter and momentum, we feed the corresponding parameter and momentum values, the stochastic gradients of energy functions for that element, and running average of the gradient at various time scales, as they are reported to encode the sufficient information about the loss surface geometry. See Appendix I for implementation details of $\\alpha_{\\phi}$ and $\\beta_{\\phi}$. By leveraging this information, we expect our meta-learned SGMCMC procedure to capture the multi-modal structures of the target posteriors of BNNS, and thus yielding a better mixing method."}, {"title": "3.3. Meta-Objective and Optimization", "content": "Objective functions for meta-learning. Meta-objective should reflect the meta-knowledge one wants to learn. We design the meta-objective based on the hope that samples collected through SGMCMC should be good at approximating the posterior predictive $p(y^* | x^*, \\mathcal{D})$. In order to achieve this goal, we propose the meta-objective called BMA meta-loss. After the sufficient number of inner-updates, we collect $K$ parameter samples with some interval between them (thinning). Let $\\theta_k(\\phi)$ be the $k$th collected parameter, and we compute the Monte-Carlo estimator of the predictive distribution and use it as a meta-objective function (note the dependency of $\\theta_k$ on the meta-parameter $\\phi$, as it is a consequence of learning SGMCMC with the meta-parameter $\\phi$).\n$\\mathcal{L}(\\phi) = - \\frac{1}{K} \\sum_{k=1}^K \\log p(y^* | x^*, \\theta_k(\\phi)),$\nwhere $(x, y)$ is a validation data point.\nGradient estimation for meta-objective. Estimating the meta-gradient $\\nabla_{\\phi} \\mathcal{L}(\\phi)$ is highly non-trivial (Metz et al., 2018; 2019), especially when the number of inner update steps is large. For instance, a na\u00efve method such as back-propagation through time would require memory grows linearly with the number of inner-steps, so become easily infeasible for even moderate sized models. One might consider using the truncation approximation, but that would result in a biased gradient estimator. Instead, we adapt ES (Salimans et al., 2017) with antithetic sampling scheme, which has been widely used in recent literature of training learned optimizer. Metz et al. (2019) showed that unrolled optimization with many inner-steps can lead to chaotic meta-loss surface and ES is capable of relieving this pathology by employing smoothed loss,\n$\\mathcal{L}(\\phi) = \\mathbb{E}_{\\eta \\sim \\mathcal{N}(\\phi, \\sigma^2 I)} [\\mathcal{L}(\\phi)]$\nwhere $\\sigma^2$ determines the degree of smoothing. Also, antithetic sampling is usually applied to reduce the estimation variance of $\\nabla_{\\phi} \\mathcal{L}(\\phi)$. Through log-derivative trick, we can get unbiased estimator of (8),\n$\\hat{g}=\\frac{1}{N} \\sum_{i=1}^N \\frac{\\mathcal{L}(\\phi+\\eta_i)}{\\sigma^2} \\eta_i$\nwhere $\\eta_i \\sim iid \\mathcal{N}(0, \\sigma^2 I)$. In addition, we can get another unbiased estimator $\\hat{g}^{-1} = -\\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\phi - \\eta_i) \\frac{\\eta_i}{\\sigma^2}$ by reusing the negative of $\\eta_i$. By taking the average of two estimators, we can obtain the following gradient estimator.\n$\\nabla \\mathcal{L}(\\phi) = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\mathcal{L}(\\phi+\\eta_i)-\\mathcal{L}(\\phi-\\eta_i)}{2\\sigma^2} \\eta_i$\nThe estimator is also amenable to parallelization, improving the efficiency of gradient computation."}, {"title": "3.4. Meta training procedure", "content": "Generic pipeline. General process of meta-training is as follows. First, for each inner-loop, we sample a task from the pre-determined task distribution. An inner-loop starts from an randomly initialized parameter and iteratively apply update step (6) to run a single chain of SGMCMC. Please refer to Algorithm 2 for detailed description. In the initial stage of meta-training, the chains from these inner loops show poor convergence, but the performance improves as training progresses. Similar to general Bayesian inference, we consider the early part of the inner loop as a burn-in period and collect samples from the end of the inner-loop at regular intervals when evaluating the meta-objective. This training process naturally integrates the meta-learning and Bayesian inference in that mimicking the actual inference procedure of Bayesian methods in realistic supervised learning tasks. In Figure 1 we show that L2E achieve desired level of accuracy for the approximation of posterior predictive with relatively small number of samples. This result indicates that L2E has successfully acquired the desired properties through meta-training.\nMultitask training for better generalization. In meta-learning, diversifying the task distribution is commonly known to enhance generalization. We include various neural network architectures and datasets in the task distribution to ensure that L2E has sufficient generalization capacity. Also, we evaluate how the task distribution diversity affects the performance of L2E in Table 11."}, {"title": "4.3. Capturing multi-modality", "content": "In Figure 2c and Figure 2b, we observe the behavior of DE, CSGMCMC, and L2E in function space. In Figure 2b, we display the test error surface using a 2-dimensional subspace spanned by the first three collected parameters for each method following Garipov et al. (2018). Parameters of DE clearly located on multiple distinct modes as expected.\nIn contrast, CSGMCMC seems to sample parameters within a single mode, while samples from L2E appear to be in distinct modes. This aligns with the results from Fort et al. (2019) showing that CSGMCMC has limited capability of capturing multi-modalities.\nFor deeper investigation, we plot test error along a linear path between multiple pairs of saved parameters inspired by Goodfellow et al. (2014). Existence of loss barrier in Figure 2c means the collected parameters belong to different modes. In Figure 2c, L2E shows a significant increase in predictive error along the linear path between every pair of parameters while CSGMCMC exhibits a relatively low level of the loss barrier between samples. This suggests that L2E is capable of the capturing multi-modality of the posterior distribution. Samples collected from Meta-SGMCMC also seem to have loss barriers in CIFAR-10, but considering inferior predictive performance of Meta-SGMCMC, the loss barrier among collected parameters from low density region is meaningless."}, {"title": "4.4. OOD Detection", "content": "We report the OOD detection performance to estimate the ability to estimate uncertainty. We use Maximum Softmax Probability (MSP) which is equivalent to confidence of logit as OOD score. We use Area Under the ROC curve(AUROC) (Liang et al., 2017) to measure the OOD detection performance. For Tiny-ImageNet, we resize the image to 32\u00d732 for evaluation. In Table 2, L2E shows the best performance regardless of OOD datasets and in-distribution datasets in general. Only HMC outperforms L2E on detecting SVHN dataset using neural networks trained on CIFAR-100. One notable result is that the performace gap between HMC,L2E and other baselines becomes more stark on SVHN. This shows that HMC and L2E share common features on uncertainty estimation performance and this aligns with other results from Table 1 and Figure 8."}, {"title": "4.5. Robustness under covariate shift", "content": "Next, we consider CIFAR-10-C (Hendrycks & Dietterich, 2019) for evaluating robustness to covariate shift. In Table 3, DE outperforms L2E and CSGMCMC for all intensity levels. L2E shows competitive performance under mild corruption, but as the corruption intensifies, the performance of L2E significantly drops and shows worst accuracy over all methods. HMC also shows this trend, showing worst performance in general. Since L2E makes similar predictive distribution with HMC in CIFAR-10, this aligns with the result from (Izmailov et al., 2021a;b) that HMC and methods having high fidelity to HMC suffer greatly from the covariate shift. Although L2E is not robust at covariate shift, it is understandable considering the similarity of L2E to HMC in function space."}, {"title": "4.6. Convergence analysis", "content": "We also evaluate sampling efficiency and degree of mixing of L2E using ESS and CR (Sommer et al., 2024). Please refer to Appendix L for details of metrics. In Table 4, L2E is the only method that consistently demonstrates decent performance both in terms of ESS and CR across experiments. On the other hand, other methods show poor mixing, indicating that they hardly explore multi-modal BNNS posterior distributions"}, {"title": "5. Conclusion", "content": "In this work, we introduced a novel meta-learning framework called L2E to improve SGMCMC methods. Unlike conventional SGMCMC methods that heavily rely on manually designed components inspired by mathematical or physics principles, we aim to learn critical design components of SGMCMC directly from data. Through experiments, we show numerous advantages of L2E over existing SGMCMC methods, including better mixing, improved prediction performance. Our approach would be a promising direction to solve several challenges that SGMCMC methods face in BNNS."}, {"title": "D. Results on alternative parameterization", "content": "Although parameterizing $\\nabla g(\\theta, r)$ allows learned sampler to be expressive and efficient, we should make assumptions on the underlying $g(\\theta, r)$. To avoid introducing additional assumptions, we propose another version of L2E that directly parameterizes kinetic energy function $g(\\theta, r)$ rather than its gradients and evaluate whether it can achieve comparable performance comparable to L2E. Specifically, we fix $p(r|\\theta)$ as normal distribution and parameterize its mean with $f_{\\phi}(\\theta)$ that takes $\\theta, \\nabla \\tilde{U}(\\theta)$ as inputs. This approach eliminates the need for assumptions regarding the"}]}