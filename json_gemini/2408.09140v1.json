{"title": "Learning to Explore for Stochastic Gradient MCMC", "authors": ["SeungHyun Kim", "Seohyeon Jung", "Seonghyeon Kim", "Juho Lee"], "abstract": "Bayesian Neural Networks (BNNs) with high-dimensional parameters pose a challenge for posterior inference due to the multi-modality of the posterior distributions. Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) with cyclical learning rate scheduling is a promising solution, but it requires a large number of sampling steps to explore high-dimensional multi-modal posteriors, making it computationally expensive. In this paper, we propose a meta-learning strategy to build SGMCMC which can efficiently explore the multi-modal target distributions. Our algorithm allows the learned SGMCMC to quickly explore the high-density region of the posterior landscape. Also, we show that this exploration property is transferrable to various tasks, even for the ones unseen during a meta-training stage. Using popular image classification benchmarks and a variety of downstream tasks, we demonstrate that our method significantly improves the sampling efficiency, achieving better performance than vanilla SGMCMC without incurring significant computational overhead.", "sections": [{"title": "1. Introduction", "content": "Bayesian methods have received a lot of attention as powerful tools for improving the reliability of machine learning models. Bayesian methods are gaining prominence due to their ability to offer probability distributions over model parameters, thereby enabling the quantification of uncertainty in predictions. They find primary utility in safety-critical domains like autonomous driving, medical diagnosis, and finance, where the accurate modeling of prediction uncertainty often takes precedence over the predictions themselves. The integration of Bayesian modeling with (deep) neural networks, often referred to as Bayesian Neural Networks (BNNS), introduces exciting prospects for the development of secure and trustworthy decision-making systems.\nHowever, there are significant problems for the successful application of BNNS in real-world scenarios. Bayesian inference in high-dimensional parameter space, especially for deep and large models employed for the applications mentioned above, is notoriously computationally expensive and often intractable due to the complexity of the posterior distribution. Moreover, posterior landscapes of BNNS frequently display multi-modality, where multiple high density regions exist, posing a significant challenge to efficient exploration and sampling. Due to this difficulty, the methods that are reported to work well for relatively small models, for instance, variational inference (Blei & McAuliffe, 2017) or Hamiltonian Monte Carlo (HMC) (Neal et al., 2011), can severly fail for deep neural networks trained with large amount of data, when applied without care.\nRecently, Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) methods (Welling & Teh, 2011; Chen et al., 2014; Ma et al., 2015) have emerged as powerful tools for enhancing the scalability of approximate Bayesian inference. This advancement has opened up the possibilities of applying Bayesian methods to large-scale machine learning tasks. SGMCMC offers a versatile array of methods for constructing Markov chains that converge towards the target posterior distributions. The simulation of these chains primarily relies on stochastic gradients, making them particularly suitable for BNNS trained on large-scale datasets. However, despite the notable successes of SGMCMC in some BNN applications (Welling & Teh, 2011; Chen et al., 2014; Ma et al., 2015; Zhang et al., 2020), there remains a notable challenge. Achieving optimal performance often demands extensive engineering efforts and hyperparameter tuning. This fine-tuning process typically involves human trial and error or resource-intensive cross-validation procedures. Furthermore, it's worth noting that even with the use of SGMCMC methods, there remains room for improvement in efficiently exploring multi-modal posterior distributions."}, {"title": "2. Backgrounds", "content": ""}, {"title": "2.1. SGMCMC for Bayesian Neural Networks", "content": "Settings. In this paper, we focus on supervised learning problems with a training dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ with $x_i$ being observation and $y_i$ being label. Given a neural network with a parameter $\\theta \\in \\mathbb{R}^d$, a likelihood $p(y \\vert x, \\theta)$ and a prior $p(\\theta)$ are set up, together defining an energy function $U(\\theta) = -\\sum_{i=1}^n \\log p(y_i \\vert x_i, \\theta) - \\log p(\\theta)$. The goal is to infer the posterior distribution $p(\\theta \\vert \\mathcal{D}) \\propto \\exp(-U(\\theta))$."}, {"title": "A complete recipe.", "content": "There may be several ways to build a Markov chain leading to the target posterior distribution. Ma et al. (2015) presented a generic recipe that includes all the convergent SGMCMC algorithms as special cases, constituting a complete framework. In this recipe, a parameter $\\theta$ of interest is augmented with an auxiliary momentum variable $r$, and an Stochastic Differential Equation (SDE) of the following form is defined for a joint variable $z = (\\theta, r) \\in \\mathbb{R}^{2d}$ as follows.\n$dz = [-(D(z) + Q(z))\\nabla_zH(z) + F(z)] dt + \\sqrt{2D(z)}dw_t$\n$H(z) := U(\\theta) + g(\\theta, r)$\n$\\Gamma_{\\epsilon}(z) := \\sum_{i=1}^{2d} \\frac{\\partial}{\\partial z_j} (D_{ij}(z) + Q_{ij}(z))$,\nwhere $g(\\theta, r)$ is the conditional energy function of the momentum $r$ such that $p(z) \\propto \\exp(-H(z))$ and $w_t$ is 2d-dimensional Brownian motion. Here, $D(z) \\in \\mathbb{R}^{2d \\times 2d}$ and $Q(z) \\in \\mathbb{R}^{2d \\times 2d}$ are restricted to be positive semi-definite and skew-symmetric, respectively. Given this SDE, one can obtain a SGMCMC algorithm by first substituting the full gradient $\\nabla_zH(z)$ with a mini-batch gradient $\\nabla_z\\tilde{H}(z) = \\nabla_z(\\tilde{U}(\\theta)+g(\\theta,r))$ and then discretizing it via a numerical solver such as sympletic Euler method. A notable example would be Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014), where $g(\\theta,r) = r^\\top M^{-1}r$,\n$D(z) = \\begin{bmatrix}0 & 0\\\\ 0 & C\\end{bmatrix}$ and $Q(z) = \\begin{bmatrix}0 & I\\\\ -I & 0\\end{bmatrix}$\nfor some positive semi-definite matrices $M$ and $C$, leading to an algorithm when discretized with symplectic Euler method as follows.\n$r_{t+1} = r_t - \\epsilon_t\\nabla \\tilde{U}(\\theta_t) - \\epsilon_t CM^{-1}r_t + \\xi_t$\n$\\theta_{t+1} = \\theta_t + \\epsilon_tM^{-1}r_{t+1}$,\nwhere $\\xi_t \\sim N(0, 2C\\epsilon_t)$ and $\\epsilon_t$ is a step-size.\nThe complete recipe includes interesting special cases that introduce adaptive preconditioners to improve the mixing"}, {"title": "Prediction via Bayesian model averaging.", "content": "After inferring the posterior $p(\\theta|\\mathcal{D})$, for a test input $x_*$, the posterior predictive is computed as\n$p(y_* \\vert x_*, \\mathcal{D}) = \\int_{\\mathbb{R}^d} p(y_* \\vert x_*, \\theta)p(\\theta \\vert \\mathcal{D}) d\\theta,$\nwhich is also referred to as Bayesian Model Averaging (BMA). In our setting, having collected from the posterior samples $\\theta_1, ..., \\theta_K$ from a convergent chain simulated from SGMCMC procedure, the predictive distribution is approximated with a Monte-Carlo estimater,\n$P(y_* \\vert x_*, \\mathcal{D}) \\approx \\frac{1}{K}\\sum_{k=1}^K P(y_* \\vert x_*, \\theta_k)$.\nAs one can easily guess, the quality of this approximation depends heavily on the quality of the samples drawn from Markov Chain Monte Carlo (MCMC) procedure. For over-parameterzied deep neural networks that we are interested in, the target posterior $p(\\theta|\\mathcal{D})$ is typically highly multi-modal, so simple SGMCMC methods suffer from poor mixing; that is, the posterior samples collected from those methods are not widely spread throughout the parameter space, so it takes exponentially many samples to achieve desired level of accuracy for the approximation. Hence, a good SGMCMC algorithm should be equipped with the ability to efficiently explore the parameter space, while still"}, {"title": "Meta Learning", "content": "Meta-learning, or learning to learn, refers to the algorithm that learns the useful general knowledge from source tasks that can transfer to the unseen tasks. Most meta-learning algorithms involves two levels of learning: an inner-loop and outer-loop (Metz et al., 2018). Inner-loop usually contains the training procedure of particular task. In our work, inner-loop for our meta-training is iteratively update the model parameter $\\theta$ by running SGMCMC with learnable transition kernel. Outer-loop refers to the training procedure of meta-parameter $\\phi$, which is done by minimizing meta-objective $L(\\phi)$."}, {"title": "Meta Learning and MCMC", "content": "There exists line of work that parameterize the transition kernel of MCMC with trainable function for various purposes. Levy et al. (2017) used learnable invertible operator to automatically design the transition kernel of HMC for good mixing. For SGMCMC, Gong et al. (2018) parameterized the curl matrix and acceleration matrix using neural networks under the framework of Ma et al. (2015). Although Gong et al. (2018) is the closest work for our method, this work did not verified its task generalization, i.e., evaluated only on the dataset that was used for training. Moreover, the scale of the experiments in Gong et al. (2018) are limited to small-sized network architectures. We further demonstrate the limitation of Gong et al. (2018) in simulating large scale multi-modal BNNS posterior, along with detailed discussions in Appendix E. To the best of our knowledge, our work firstly proposes the method to meta-learn SGMCMC that can generalize to unseen datasets and scale to large-scale BNNS."}, {"title": "Limitation of Meta-SGMCMC", "content": "Meta-SGMCMC aims to learn the diffusion matrix $D(z)$ and the curl matrix $Q(z)$ in Equation 1 to build a SGMCMC algorithm that can quickly converge to a target distribution. For this purpose, neural network $D_f(z)$ and $Q_f(z)$ are employed to model $D(z)$ and $Q(z)$. We point out that this parameterization has notable limitations, especially in terms of efficient exploration of high-dimensional multi-modal target distribution.\n* According to the recipe in Equation 1, the diffusion and the curl that are dependent on $z$ involves the additional correction term $\\Gamma(z)$. This hurts the scalability of the algorithm, as computing $\\Gamma(z)$ involves computing the gradient of $D_f(z)$ and $Q_f(z)$ with respect to $z$. This amounts to a significant computational burden as the dimension of $z$ increases and involves finite-difference approximation. While Meta-SGMCMC attempted to address this issue of computational cost"}, {"title": "3. Main contribution: Learning to Explore", "content": ""}, {"title": "3.1. Overcome the limitations of Meta-SGMCMC", "content": "To avoid aforementioned limitations of Meta-SGMCMC, we newly design the meta-learning method for SGMCMC as follows:\n* We parameterize the gradients of the kinetic energy function and keep $D_f(z)$ and $Q_f(z)$ independent of $z$. We can avoid the additional computational cost of computing $\\Gamma(z)$ without sacrificing the flexibility of the sampler.\n* We use a new meta-objective called BMA meta-loss which is the Monte-Carlo estimator of the predictive distribution to enhance the exploration and performance of the learned sampler. Both of our meta-objective and its gradient can be computed in closed-form. Also, we employ Evolution Strategy (ES) (Salimans et al., 2017; Metz et al., 2019) for gradient estimation, which is an unbiased estimator of the true meta-gradient. This also consumes significantly less memory compared to analytic gradient methods, allowing us to keep the length of the inner-loop much longer during meta-learning."}, {"title": "3.2. Meta-learning framework for SGMCMC", "content": "Instead of using a hand-designed recipe for SGMCMC, we aim to learn the proper SGMCMC update steps through meta learning. The existing works, both the methods using hand-designed choices or meta-learning (Gong et al., 2018), try to determine the forms of the matrices $D(z)$ and $Q(z)$ while keeping the kinetic energy $g(\\theta, r)$ as simple Gaussian energy function, that is, $g(\\theta, r) = r^\\top M^{-1}r/2$. This choice"}, {"title": "3.3. Meta-Objective and Optimization", "content": "Objective functions for meta-learning. Meta-objective should reflect the meta-knowledge one wants to learn. We design the meta-objective based on the hope that samples collected through SGMCMC should be good at approximating the posterior predictive $p(y_*|x_*, \\mathcal{D})$. In order to achieve this goal, we propose the meta-objective called BMA meta-loss. After the sufficient number of inner-updates, we collect $K$ parameter samples with some interval between them (thinning). Let $\\theta_k(\\phi)$ be the $k$th collected parameter, and we compute the Monte-Carlo estimator of the predictive distribution and use it as a meta-objective function (note the dependency of $\\theta_k$ on the meta-parameter $\\phi$, as it is a consequence of learning SGMCMC with the meta-parameter $\\phi$).\n$L(\\phi) = - \\frac{1}{K}\\sum_{k=1}^K \\log P(y_* \\vert x_*, \\theta_k(\\phi)),$\nwhere $(x_*, y_*)$ is a validation data point.\nGradient estimation for meta-objective. Estimating the meta-gradient $\\nabla_\\phi L(\\phi)$ is highly non-trivial (Metz et al., 2018; 2019), especially when the number of inner update steps is large. For instance, a na\u00efve method such as back-propagation through time would require memory grows linearly with the number of inner-steps, so become easily infeasible for even moderate sized models. One might consider using the truncation approximation, but that would result in a biased gradient estimator. Instead, we adapt ES (Salimans et al., 2017) with antithetic sampling scheme, which has been widely used in recent literature of training learned optimizer. Metz et al. (2019) showed that unrolled optimization with many inner-steps can lead to chaotic meta-loss surface and ES is capable of relieving this pathology by employing smoothed loss,\n$\\tilde{L}(\\phi) = \\mathbb{E}_{\\eta \\sim N(\\phi, \\sigma^2I)} [L(\\phi)],$\nwhere $\\sigma^2$ determines the degree of smoothing. Also, antithetic sampling is usually applied to reduce the estimation variance of $\\nabla_\\phi L(\\phi)$. Through log-derivative trick, we can get unbiased estimator of (8),\n$\\mathbf{g} = \\frac{1}{N}\\sum_{i=1}^N \\frac{L(\\phi + \\eta_i) - L(\\phi)}{\\eta_i}$\nwhere $\\eta_i \\stackrel{iid}{\\sim} N(0, \\sigma^2I)$. In addition, we can get another unbiased estimator $\\mathbf{g}^{-1} = - \\frac{1}{N}\\sum_{i=1}^N \\frac{L(\\phi - \\eta_i)}{\\eta_i}$ by reusing the negative of $\\eta_i$. By taking the average of two estimators, we can obtain the following gradient estimator.\n$\\nabla L(\\phi) = \\frac{1}{N}\\sum_{i=1}^N \\frac{L(\\phi + \\eta_i) - L(\\phi - \\eta_i)}{2\\sigma^2} \\eta_i$\nThe estimator is also amenable to parallelization, improving the efficiency of gradient computation."}, {"title": "3.4. Meta training procedure", "content": "Generic pipeline. General process of meta-training is as follows. First, for each inner-loop, we sample a task from the pre-determined task distribution. An inner-loop starts from an randomly initialized parameter and iteratively apply update step (6) to run a single chain of SGMCMC. Please refer to Algorithm 2 for detailed description. In the initial stage of meta-training, the chains from these inner loops show poor convergence, but the performance improves as training progresses. Similar to general Bayesian inference, we consider the early part of the inner loop as a burn-in period and collect samples from the end of the inner-loop at regular intervals when evaluating the meta-objective. This training process naturally integrates the meta-learning and Bayesian inference in that mimicking the actual inference procedure of Bayesian methods in realistic supervised learning tasks. In Figure 1 we show that L2E achieve desired level of accuracy for the approximation of posterior predictive with relatively small number of samples. This result indicates that L2E has successfully acquired the desired properties through meta-training.\nMultitask training for better generalization. In meta-learning, diversifying the task distribution is commonly known to enhance generalization. We include various neural network architectures and datasets in the task distribution to ensure that L2E has sufficient generalization capacity. Also, we evaluate how the task distribution diversity affects the performance of L2E in Table 11."}, {"title": "4. Experiments", "content": "In this section, we will evaluate the performance of L2E in various aspects. Through extensive experiments, we would like to demonstrate followings:\n* L2E shows excellent performance on real-world image classification tasks and seamlessly generalizes to the tasks not seen during meta-training.\n* L2E produces similar predictive distribution to HMC as well as good mixing of BNNS posterior distribution in weight space."}, {"title": "4.1. Image classification results", "content": "Figure 1 illustrates the trend of predictive performance of each method as the number of samples for BMA increases. We confirm that L2E outperforms other baselines in terms of predictive accuracy on completely unseen datasets and architectures during meta-training. Among"}, {"title": "4.2. Similarity to the HMC samples", "content": "As mentioned above, we compare L2E to HMC samples from Izmailov et al. (2021b) to objectively evaluate and investigate whether L2E has converged to target distribution."}, {"title": "4.3. Capturing multi-modality", "content": "In Figure 2c and Figure 2b, we observe the behavior of DE, CSGMCMC, and L2E in function space. In Figure 2b, we display the test error surface using a 2-dimensional subspace spanned by the first three collected parameters for each method following Garipov et al. (2018). Parameters of DE clearly located on multiple distinct modes as expected."}, {"title": "4.4. OOD Detection", "content": "We report the OOD detection performance to estimate the ability to estimate uncertainty. We use Maximum Softmax Probability (MSP) which is equivalent to confidence of logit as OOD score. We use Area Under the ROC curve(AUROC) (Liang et al., 2017) to measure the OOD detection performance. For Tiny-ImageNet, we resize the image to 32\u00d732 for evaluation. In Table 2, L2E shows the best performance regardless of OOD datasets and in-distribution datasets in general. Only HMC outperforms L2E on detecting SVHN dataset using neural networks trained on CIFAR-100. One notable result is that the performace gap between HMC,L2E and other baselines becomes more stark on SVHN. This"}, {"title": "4.5. Robustness under covariate shift", "content": "Next, we consider CIFAR-10-C (Hendrycks & Dietterich, 2019) for evaluating robustness to covariate shift. In Table 3, DE outperforms L2E and CSGMCMC for all intensity levels. L2E shows competitive performance under mild corruption, but as the corruption intensifies, the performance of L2E significantly drops and shows worst accuracy over all methods. HMC also shows this trend, showing worst performance in general. Since L2E makes similar predictive distribution with HMC in CIFAR-10, this aligns with the result from (Izmailov et al., 2021a;b) that HMC and methods having high fidelity to HMC suffer greatly from the covariate shift. Although L2E is not robust at covariate shift, it is understandable considering the similarity of L2E to HMC in function space."}, {"title": "4.6. Convergence analysis", "content": "We also evaluate sampling efficiency and degree of mixing of L2E using ESS and cR (Sommer et al., 2024). Please refer to Appendix L for details of metrics. In Table 4, L2E is the only method that consistently demonstrates decent performance both in terms of ESS and CR across experiments. On the other hand, other methods show poor mixing, indicating that they hardly explore multi-modal BNNS posterior distributions"}, {"title": "5. Conclusion", "content": "In this work, we introduced a novel meta-learning framework called L2E to improve SGMCMC methods. Unlike conventional SGMCMC methods that heavily rely on manually designed components inspired by mathematical or physics principles, we aim to learn critical design components of SGMCMC directly from data. Through experiments, we show numerous advantages of L2E over existing"}, {"title": "3.4. Meta training procedure", "content": "where $(x_*, y_*)$ is a validation data point. While BMA meta-loss is similar to CE meta-loss, BMA meta-loss differs significantly from CE meta-loss. BMA meta-loss is Monte Carlo approximation of the posterior predictive distribution for validation data points. Actually, we gather models along the sampler's trajectory and minimize CE-loss of the average probability of collected models. BMA meta-loss not only encourages the sampler to minimize the average CE-loss of individual models but also promotes increased functional diversity among collected models. According to Wood et al. (2023, Equation 7), this loss of the ensemble model is decomposed as \u201censemble loss = average individual loss - ambiguity\". Ambiguity refers to the difference among the ensemble models and individual models, and the larger it is, the more BMA meta-loss decreases. In Figure 6, we observe that L2E trained with BMA meta-loss exhibits a much greater loss barrier between collected parameters than L2E trained with CE meta-loss, indicating the higher exploration and larger functional diversity among samples. Also, our extensive downstream experiments and visualization demonstrates that L2E with BMA meta-loss actually increases functional diversity among collected models."}, {"title": "C. L2E on Text Dataset", "content": "In order to check whether L2E can adapt well to the other modalities (e.g. text dataset), we additionally conduct text classification on IMDB dataset with CNN-LSTM architecture following Wenzel et al. (2020) and Izmailov et al. (2021b).\nFor HMC, we use checkpoints of 1200 samples from Izmailov et al. (2021b). For other MCMC methods, we collect 50 samples with 50 epochs of thinning interval with 100 burnin epochs. Table 6 demonstrates that L2E shows competitive performance on the text dataset, meaning that L2E can still work well on unseen modalities. This transfer of knowledge from image datasets to text dataset supports that the generalization capacity of L2E is strong compared to Gong et al. (2018), which is only proven to generalize well to similar datasets with meta-training dataset."}, {"title": "D. Results on alternative parameterization", "content": "Although parameterizing $\\nabla g(\\theta,r)$ allows learned sampler to be expressive and efficient, we should make assumptions on the underlying $g(\\theta,r)$. To avoid introducing additional assumptions, we propose another version of L2E that directly parameterizes kinetic energy function $g(\\theta,r)$ rather than its gradients and evaluate whether it can achieve comparable performance comparable to L2E. Specifically, we fix $p(r|\\theta)$ as normal distribution and parameterize its mean with $f_\\phi(\\theta)$ that takes $\\theta, \\nabla\\tilde{U}(\\theta)$ as inputs. This approach eliminates the need for assumptions regarding the existence and integrability of unnormalized probability density function. Since we set $p(r|\\theta) \\sim \\mathcal{N}(f_\\phi(\\theta), I)$, kinetic energy function of $p(r|\\theta)$ is $g(\\theta,r) = (r - f_\\phi(\\theta))^T(r - f_\\phi(\\theta))$. Therefore, we can get $\\nabla_\\theta g(\\theta,r) = f_\\phi(\\theta)\\nabla_\\theta f_\\phi(\\theta)$ and $\\nabla_r g(\\theta,r) = r - f_\\phi(\\theta)$. Then, update rule for this parameterization is as follows\n$r_{t+1} = r_t - \\epsilon_t(\\nabla\\tilde{U}(\\theta_t) + f_\\phi(\\theta)\\nabla_\\theta f_\\phi(\\theta) + a(r_t - f_\\phi(\\theta)) + \\xi_t$\n$\\theta_{t+1} = \\theta_t + \\epsilon_t(r_t - f_\\phi(\\theta))$\nwhere $\\xi_t \\sim \\mathcal{N}(0, 2\\epsilon t a)$. We will refer to this version of sampler as Kinetic-L2E. In Table 5, Kinetic-L2E shows comparable performance to L2E across all image classification experiments. Also, Figure 7 demonstrates the exploration capacity of Kinetc-L2E. These results indicate that both versions can achieve similar performance in terms of predictive accuracy and exploration."}, {"title": "E. Comparision with Gong et al. (2018)", "content": "In Table 7, we demonstrate the difference between Meta-SGMCMC (Gong et al., 2018) and L2E. Meta-SGMCMC aims to build a sampler through meta-learning that rapidly converges to the target distribution and performs accurate simulation. This goal aligns with the objectives of all SGMCMC methods. However, our approach is specifically designed with the more concrete purpose of effectively simulating multi-modal DNN posterior distribution and also generalizing to unseen problems. We compare L2E to Meta-SGMCMC in various aspects in the following subsections."}, {"title": "E.1. Difference in parameterization of meta models", "content": "The update rule for $z = (\\theta, r) \\in \\mathbb{R}^{2d}$ presented in Gong et al. (2018) is as following.\n$r_{t+1} = (1 - \\epsilon_t D_f(z_t))r_t - \\epsilon_t Q_f(z_t)\\nabla_\\theta \\tilde{U}(\\theta_t) + \\epsilon_t F_r(z_t) + \\xi_t$\n$\\theta_{t+1} = \\theta_t + \\epsilon_t Q_f(z_t)r_t + \\epsilon_t F_\\theta(z_t)$\n$\\xi_t \\sim N(0, 2\\epsilon_t D_f(z_t)), \\Gamma_\\epsilon(z) := \\sum_{j=1}^{2d} \\frac{\\partial}{\\partial z_j}(D_{ij}(z) + Q_{ij}(z))$\nIn our main text, we point out that parameterizing $D_f$ and $Q_f$ makes additional computational burden. Additionally, since $D_f$ and $Q_f$ mainly function as multipliers for gradient and momentum, learning them may not be as effective as it should"}, {"title": "F. Discussion on meta-training objectives", "content": "Previous studies have proposed various meta-objectives to achieve goals similar to ours. Gong et al. (2018) minimizes the KL$(q_t|\\pi)$, where $\\pi$ is target distribution and $q_t$ is the marginal distribution of $\\theta$ at time $t$ for good mixing. On the other hand, Levy et al. (2017) employs meta-objective maximizing the jump distances between samples in weight space and simultaneously minimizing the energy in order to make sampler rapidly explore between modes. However, explicitly maximizing the jump distances in weight space can be easily cheated, as the distances between weights does not necessarily lead to the difference in the functions, resulting in trivial sampler with which achieving the balance between convergence and exploration is hard. Also, minimizing the divergence with target distribution seems sensible, but due to intractable $q_t$, computing the gradient of $q_t$ should resort to gradient estimator. Gong et al. (2018) used stein-gradient estimator, which requires multiple independent chains so it harms scalability. Also, this objective does not lead the learned sampler to explore multi-modal distribution. Gong et al. (2018, Figure 3) shows that the learned sampler quickly converges to low energy region, but learned friction term $D_f$ restricts the amount of update in low energy region, limiting the exploration behaviour. Among choices, we find out that BMA meta-loss is a simple yet effective meta-objective that naturally encodes exploration-exploitation balance without numerical instability and exhaustive hyperparameter tuning."}, {"title": "G. 1-D synthetic regression", "content": "We conduct 1-D synthetic regression task to visually check whether L2E can capture the epistemic uncertainty. For the training data, we generate 1000 data points from underlying true function $y = sin(x)$, within the interval $[-5, 1]$ and $[1, 4]$.\nWe use DE and HMC as baselines. We collect 50 parameters for each methods and plot the mean prediction and 95% confidence interval of the prediction. For L2E, we do not fine-tune the learned sampler used in the main experiments. We use thinning interval of 50 training steps and 1000 burn-in steps for L2E, 300 training steps for each single solution of DE and 1000 burn-in steps and 100 leap-frog steps for HMC. We use 2 layers MLP with 100 hidden units and ReLU activation to estimate the function."}, {"title": "H. Image classification with Data Augmentation", "content": "Since applying data augmentation violates Independently and Identically Distributed(IID) assumption of the dataset which is commonly assumed by Bayesian methods, this can lead to model misspecification (Wenzel et al., 2020; Kapoor et al., 2022) and under model misspecifcation, bayesian posterior may not be the optimal for the BMA performance (Masegosa, 2020). Therefore, prior work such as Izmailov et al. (2021b) argued the incompatibility between Bayesian methods and data augmentation. However, data augmentation is an indispensable technique in modern machine learning so it is also interesting to see how L2E performs with data augmentation. We run the image classification experiments on CIFAR-10 and CIFAR-100 with random crop and horizontal flip for the data augmentation. Since using data augmentation introduces Cold Posterior Effect (Wenzel et al., 2020) for Bayesian methods such as CSGMCMC and L2E, we additionally tune the temperature hyperparameter for these methods. We tune the temperature for each methods, using T = 0.0001 in both experiments. Please refer to Wenzel et al. (2020) for detailed analysis of Cold Posterior Effect. We collect 10 samples for all methods for these experiments.\nIn Table 10, when applying data augmentation, the performance gap between CSGMCMC and L2E becomes far less stark than without data augmentation. Moreover, DE outperforms other baselines with a large margin in CIFAR-10 and shows similar results with L2E in CIFAR-100 since L2E significantly outperforms other method without data augmentation. With data augmentation, it is not very surprising that DE outperforms other Bayesian methods like L2E and CSGMCMC in terms of predictive accuracy and calibration since they suffer from model misspecification and temperature tuning can partially handle this problem (Kapoor et al., 2022). Observing the high agreement between L2E and HMC in the function space suggests that L2E effectively approximates the predictive distribution of the target posterior distribution. However, when techniques that violate assumptions for model likelihood are applied, correctly simulating target distribution does not necessarily mean the superior performance than other methods. As a result, there may be a reduction or even a reversal in the performance gap compared to other methods. Nevertheless, L2E demonstrates comparable performance to DE with less compute on datasets like CIFAR-100. Also, L2E shows slightly better performance than Bayesian method,CSGMCMC in CIFAR-10 and outperforms with a wide margin in CIFAR-100 experiment. When it comes to predictive diversity, L2E significantly outperforms baselines on both experiments. Although applying data augmentation introduces significant variations to the posterior landscape, we confirm that L2E still maintains the exploration property. To sum up, we argue that L2E is still practical method even with data augmentation since it shows competitive predictive performance and efficiently explores the posterior landscape."}, {"title": "1.1. Input features", "content": "We use the following input features for L2E:\n* raw gradient values\n* raw parameter values\n* raw momentum values\n* running average of gradient values\nRunning average feature is expanded for multiple time scale in that we use multiple momentum-decay values for averaging. We use 0.1, 0.5, 0.9, 0.99, 0.999 and 0.9999 for momentum decay so that running average feature is expanded into 6-dimensions. Therefore, we have total 9-dimensional input features for each dimension of parameter and momentum. Input features are normalized so that 12 norm with respect to input features of different dimensions become 1. and share weights of neural network except for the last layer of MLP, so that we can get two quantities with single forward pass. This weight sharing method is employed in Levy et al. (2017) or other recent literature in learned optimization like in Metz et al. (2022b) and Metz et al. (2019)."}, {}]}