{"title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models", "authors": ["Junyao Ge", "Yang Zheng", "Kaitai Guo", "Jimin Liang"], "abstract": "Abundant, well-annotated multimodal data in remote sensing are pivotal for aligning complex visual remote sensing (RS) scenes with human language, enabling the development of specialized vision language models across diverse RS interpretation tasks. However, annotating RS images with rich linguistic semantics at scale demands expertise in RS and substantial human labor, making it costly and often impractical. In this study, we propose a workflow that leverages large language models (LLMs) to generate multimodal datasets with semantically rich captions at scale from plain OpenStreetMap (OSM) data for images sourced from the \u2611 Google Earth Engine (GEE) platform. This approach facilitates the generation of paired remote sensing data and can be readily scaled up using openly available data. Within this framework, we present RSTeller, a multimodal dataset comprising over 1 million RS images, each accompanied by multiple descriptive captions. Extensive experiments demonstrate that RSTeller enhances the performance of multiple existing vision language models for RS scene understanding through continual pre-training. Our methodology significantly reduces the manual effort and expertise needed for annotating remote sensing imagery while democratizing access to high-quality annotated data. This advancement fosters progress in visual language modeling and encourages broader participation in remote sensing research and applications. The RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.", "sections": [{"title": "1. Introduction", "content": "Vision language models (VLMs) hold great promise in remote sensing (RS) as they offer a means to interpret complex visual scenes for non-expert users in a human-like manner and support the development of diverse RS applications. However, the effective training of VLMs hinges on the availability of large quantities of paired data to ensure the alignment of visual and linguistic semantics for optimal performance. The substantial data requirements present a significant challenge in scaling up VLMs for RS, impeding their broader applicability and performance enhancement. Consequently, there is a pressing need for automated, scalable, and efficient methods to generate datasets with paired RS imagery and semantic-rich captions. Such methods are essential for overcoming the bottleneck associated with data scarcity and facilitating the development of robust VLMs tailored for RS tasks. Moreover, the availability of an out-of-the-box, well-annotated dataset comprising RS images with rich linguistic semantics holds immense value. It not only streamlines the initial training of VLMs but also contributes to reducing the environmental impact associated with the individual generation of such data, thereby addressing sustainability concerns."}, {"title": "1.1. Related works", "content": "The intersection of computer vision and natural language processing (NLP) has witnessed remarkable advancements in recent years, propelled by the emergence of large-scale pre-trained models. When CLIP (Radford et al., 2021) was initially introduced, it swiftly captivated the research community with its remarkable performance in zero-shot classification and cross-modal retrieval tasks, rivaling some state-of-the-art models trained with full supervision. CLIP achieves this feat by aligning image and text representations through a contrastive learning strategy, endowing it with the capability for zero-shot inference using the learned representations. Inspired by this paradigm, the RS community embarked on leveraging similar strategies to construct VLMs tailored for RS tasks. Notable efforts such as those by Liu et al. (2024a), Zhang et al. (2023), and Wang et al. (2024) have undertaken continual pre-training of CLIP on paired RS data, thereby extending its capabilities to encompass RS scene understanding.\nConcurrently, the advent of large language models (LLMs), exemplified by ChatGPT (Brown et al., 2020; Ouyang et al., 2022), alongside other models (Brown et al., 2020; Ouyang et al., 2022; Du et al., 2021; Jiang et al., 2024), has significantly enriched the landscape of NLP. These LLMs possess the remarkable ability to interact with humans through natural language, facilitate in-context learning, instruction following (Ouyang et al., 2022), and reasoning (Wei et al., 2022), showcasing their versatility across diverse linguistic tasks. Motivated by the power of LLMs, researchers have proposed to align visual models with large language models to achieve the SOTA performance on image captioning (Li et al., 2022), visual question answering (VQA) (Li et al., 2023a; Zhu et al., 2023; Dai et al., 2024; Liu et al., 2024b), visual grounding (Wang et al., 2023b), and etc. In the RS domain, efforts have mirrored these trends. For instance, RSGPT (Hu et al., 2023) fine-tuned InstructBLIP (Dai et al., 2024) on a self-built dataset to achieve state-of-the-art performance on RS image captioning and VQA tasks. Remote Sensing ChatGPT (Guo et al., 2024) utilizes ChatGPT to interpret user queries, plan remote sensing interpretation tasks, and generate final responses for users. Additionally, GeoChat (Kuckreja et al., 2024) fine-tuned LLaVA-1.5 (Liu et al., 2024b) using LoRA (Hu et al., 2022), enabling its support for multi-task and multi-round conversion with users to interact with RS imagery.\nDespite the progress in developing vision language models tailored for RS tasks, a significant gap remains between these models and those designed for general purposes. One of the main differences between the two paradigms lies in the scale of the models and their training data. Several studies have empirically observed that model performance is positively correlated with the model size and the data scale (Kaplan et al., 2020; Riquelme et al., 2021; Tay et al., 2021; Zhai et al., 2022; Cherti et al., 2023). Notably, the CLIP study trained its largest model, ViT-L/14, on a private WebImageText (WIT) dataset containing 400 million image-text pairs, resulting in a substantial leap in performance compared to models trained on smaller datasets. The success of CLIP has motivated the development of subsequent VLMs with increased model size and data scale for better performance. For instance, ALIGN (Jia et al., 2021) trained a large EfficientNet-L2 image encoder on a private dataset of 1.8 billion image-text pairs. CoCa (Yu et al., 2022) used the ALIGN dataset as well as a private dataset called JFT-3B, containing 3 billion image-text pairs, to train a ViT-g/14 visual encoder and a language model. PaLI (Chen et al., 2022) further expanded this approach by training a multi-language, multi-task VLM using ViT-e (4B parameters) and mT5-XXL (13B parameters) on a private dataset of 29 billion image-text pairs.\nThe ever-growing model sizes and their data demands have quickly surpassed the capacity of earlier open image-text datasets like Microsoft COCO (Lin et al., 2014), Visual Genome (Krishna et al., 2017), YFCC100M (Thomee et al., 2016), and CC3M (Sharma et al., 2018), which are insufficient for researchers without access to large-scale private datasets. Recently, the availability of large-scale multimodal data via the Internet has led to the creation of even larger image-text datasets, such as LAION-400M (Schuhmann et al., 2021), COYO-700M (Byeon et al., 2022), and LAION-5B (Schuhmann et al., 2022). These datasets have significantly enriched the VLM research community, enabling the development of many advanced VLMs (Li et al., 2023a; Wang et al., 2023b; Zhu et al., 2023).\nThe RS community has also made significant efforts to address the data scarcity problem for training VLMs for RS tasks. For readers' easy comprehension of the scales of the related works, we compared them as in Fig. 1. Before the rise of large-scale pre-trained multimodal models like CLIP, several image-text datasets were developed, including UCM-Captions (Qu et al., 2016), Sydney-Captions (Qu et al., 2016), RSICD (Lu et al., 2017), RSITMD (Yuan et al., 2022), NWPU-Captions (Cheng et al., 2022), and RSVGD (Zhan et al., 2023). Among these, the largest dataset, NWPU-Captions, contains 31,500 images and 157,500 image-text pairs. The limited size of these datasets poses challenges for training VLMs like CLIP to perform well on RS tasks. To address this, (Liu et al., 2024a) compiled a RemoteCLIP dataset from three different types of RS tasks (segmentation, detection, and retrieval) across 17 individual datasets. They used hand-crafted rules to convert segmentation and detection annotations into multiple linguistic captions, resulting in a dataset with 165,745 images and 828,725 image-text pairs. Additionally, Zhang et al. (2023) proposed leveraging existing large-scale image-text datasets by filtering data related to RS and captioning several RS classification datasets with a pre-trained image caption model, creating an RS-related multimodal dataset called RS5M with 5 million image-text pairs. Training VLMs on these larger RS datasets has demonstrated improvements in their performance. However, there are limitations in building datasets from existing ones, as the upper bound is restricted by the quantity of available well-crafted datasets, which is limited compared to the vast Internet resources. In contrast, datasets like LAION (Schuhmann et al., 2021, 2022) were collected at the Internet scale, offering significantly larger amounts of data.\nDespite the shortage of direct RS-related image-text datasets, there is an abundance of publicly available RS data. The Google Earth Engine (GEE) platform\u00b9 (Gorelick et al., 2017) provides a rich source of RS imagery, with comprehensive coverage of the Earth's surface and access to a diverse range of aerial imagery from multiple sources. These sources vary in ground sampling distances, spectral bands, and sampling times, making them ideal for large-scale imagery data collection. On the other hand, OpenStreetMap (OSM)\u00b2 (Haklay and Weber, 2008) is a collaborative project that collects geo-elements worldwide, providing a rich source of geo-referenced data. OSM serves as a semantic source where geo-elements are annotated with descriptive tags, each composed of a key-value pair. The key describes a topic, category, or feature type (e.g., highway or name), while the value provides details for the specified feature (e.g., name: Jeff Memorial Highway). OSM currently contains over 96,000 distinct keys and over 155 million distinct tags, making it a valuable resource for harvesting geo-related linguistic data. Recently, Wang et al. (2024) proposed using GEE and OSM as data sources to build a large-scale image-text dataset for RS called SkyScript, comprising 2.6 million image-text pairs. They selected valid OSM elements for given RS images and designed a heuristic caption-assembling method to convert tags into captions. The dataset exhibits great generality for downstream tasks when training a CLIP model and shows scalable potential since it uses openly available data. However, the captions in the dataset are all simple phrases rather than semantically rich sentences, as they are merely combinations of the tag words. This limitation affects the semantic richness of the dataset and its further applicability.\nUsing LLMs to annotate data has become increasingly common as the performance of these methods improves. For instance, Li et al. (2023b) proposed using multiple VLMs and ChatGPT to refine the captions in image-text datasets, generating detailed image captions. To address the scarcity of well-annotated multimodal data, Wang et al. (2023a) introduced Caption Anything (CAT), which utilizes the Segment Anything Model (SAM) (Kirillov et al., 2023) and ChatGPT. CAT supports diverse visual and language controls to produce controllable captions. In another approach, Fan et al. (2024) used LLaMa-7B (Touvron et al., 2023) to rewrite the captions in the training set of a CLIP model as a method of data augmentation, and validated the effectiveness of these rewrites in the downstream tasks. In the field of RS, GeoChat (Kuckreja et al., 2024) employed Vicuna-1.5-7B (Chiang et al., 2023) to generate multi-round question and answer pairs for existing labeled RS datasets focused on object detection, visual question answering, or scene classification tasks. This resulted in a dataset with 306,000 image-instruction pairs, enabling the VLM to follow human instructions and handle multiple RS-related tasks. These methods demonstrate the feasibility and value of generating multimodal data with LLMs, inspiring us to design a large-scale multimodal dataset for RS using LLMs."}, {"title": "1.2. Contributions", "content": "In this work, we leverage openly available remote sensing data as an abundant resource for multimodal data generation. Specifically, we utilize the GEE platform to collect unlabeled aerial images, and the OSM platform as our semantic source. We employ Mixtral-7B (Jiang et al., 2024), an open-source LLM, to interpret plain OSM data. The OSM tag contents and their combinations are used to generate fluent, semantically rich descriptions for the aerial images. To ensure reproducibility and facilitate further scaling up of a dataset suitable for VLM training, we develop a comprehensive dataset generation workflow that encompasses raw data fetching, LLM-based captioning, and final dataset compilation.\nThrough this workflow, we introduce RSTeller, a multimodal dataset consisting of over one million remote-sensing images, each accompanied by multiple descriptive captions. We use the Measure of Textual Lexical Diversity (MTLD) score (McCarthy and Jarvis, 2010) as an indicator of the semantic richness of the captions. Our dataset is over two times more semantically rich than existing datasets (Fig. 1). Given the computational demands of LLM inference, our dataset provides a ready-to-use resource for researchers, eliminating the need for individual data generation.\nExtensive experiments were conducted to validate the proposed methods and provide valuable insights for researchers training VLMs on RS data. First, we studied the relationship between the performance of the VLMs and the scale of RS domain data, highlighting the potential of further scaling up the training data. Then, we evaluated the effectiveness of RSTeller by continuing the pre-training of multiple VLMs and testing them on downstream tasks. The proposed data generation workflow and dataset are expected to be instrumental in advancing more sophisticated VLMs within the remote sensing field."}, {"title": "1.3. Paper structure", "content": "The structure of this paper is as follows: Section 2 details the proposed automatic data generation workflow. Section 3 describes and discusses the generated dataset, RSTeller, exploring its attributes in detail. Section 4 presents extensive experiments that explore the model performance concerning the data scale and the effectiveness of the proposed dataset. Section 5 provides some findings and suggestions according to our experiments. Section 6 outlines the limitations of the proposed work. Finally, Section 7 offers a summary of the research."}, {"title": "2. Automated Workflow for Dataset Generation", "content": "In this section, we detail the automated workflow developed to generate a large-scale, RS-related multimodal dataset from openly available data. The overall workflow can be divided into two parts and is shown in Fig. 2. The left panel of the figure illustrates the main processes, including data fetching, LLM-based captioning, and final dataset compilation. All the processes can be run in parallel for the maximum efficiency. The right panel provides details of the captioning tasks that facilitate the generation of semantically rich captions."}, {"title": "2.1. Main Processes", "content": "The first process in the workflow is to fetch raw data from the GEE and OSM platforms into well-defined local databases. For ease of reference, we designate these databases as the \"raw data database,\" as depicted in Process 1 of Fig. 2. This database is crucial for preserving the downloaded data and enabling asynchronous handling of different processes to accelerate the entire workflow. The GEE platform provides a vast collection of aerial images, each typically covering a wide range of geographical locations with tens or hundreds of thousands of pixels. The size of these images is too large for direct use in training a VLM, necessitating the cropping of images into smaller patches. GEE offers several flexible APIs for patch extraction and data download, from ee.Image.getThumbURL for JPEG or PNG formats to ee.Image.getDownloadURL for GeoTIFF or NumPy formats. For detailed API usage instructions, the official documentation\u00b3 is recommended.\nTo enhance file system performance, a database such as MongoDB is preferred and used in this work for storing the downloaded image patches, as a plain file system may be inefficient for large data indexing and searching. In addition to the imagery data, metadata such as patch coordinates and image timestamps are also required and carefully preserved for OSM data downloading and further analysis. An SQL-based database like MySQL or PostgreSQL is recommended for storing these metadata.\nThe OSM data is another important source of semantic information for remote sensing. It can be queried via the Overpass API4 using specific OverpassQL queries. OSM contains three types of geo-elements: nodes, ways, and relations. As nodes are points and too small to be seen in an RS image, we focus only on ways and relations. We implemented a two-step method to query OSM data to ensure the inclusion of all necessary geo-elements. The first step is to query all the ways and relations that intersect with or are within the image patches.\nThis step includes most necessary geo-elements but may miss some large geometries that do not intersect but enclose the image patches, such as lakes or forests. The second step is to query all the ways and relations that enclose the center point of the patch, including those large geo-elements missed in the first step but are still very important to the image content.\nThe geographical contents of the image patches may evolve over time. Since the OSM data is not updated in real-time, we queried the OSM data for each image patch up to one month after it was captured by the sensor to best reflect the current image content. However, querying from the official endpoint requires following a strict usage policy, allowing fewer than 10,000 queries and 1 GB of data per day. Some third-party backends like Kumi Systems Overpass Instance provide a free usage policy but may not guarantee the capacity and availability for big data processing due to server overload and network traffic. In our experience, using the Kumi endpoint can fetch OSM data for around 100,000 patches, each covering about 72,253.44 square meters in a day. Considering there could be patches not covered by OSM data and that captioning valid patches consumes much more time, it is inefficient to query the remote server each time a patch needs captioning. Therefore, it is better to cache the OSM data in a local database ahead of time or deploy the Overpass server locally. Using local OSM data during captioning significantly reduces the query time and improves the overall efficiency."}, {"title": "2.1.2. Raw Caption Generation", "content": "Once the data fetching process is underway and a sufficient amount of raw data has been acquired, the workflow proceeds to the generation of raw captions. This process involves using an LLM, specifically Mixtral-7B (Jiang et al., 2024) in this work, to interpret and describe the OSM data associated with each image patch according to predefined tasks. Instead of using an instruction fune-tuned LLM, like Mixtral-7B-instruct, the use of the base LLM is advantageous. It provides greater flexibility in generating captions because it is not constrained by specific instructional patterns. This allows it for better following the instructions and examples provided by the prompt, making the tone and style of the generated captions more consistent with the intended purpose.\nIn this process, a patch that has not yet been captioned is first sampled from the raw data database. The corresponding OSM data is then fetched from the cached OSM database. The OSM data undergoes processing based on our predefined tasks, which will be discussed in Subsection 2.2. This step checks whether the OSM data contains tags that can be used to generate captions for the patch. Patches that contain no relevant way or relation elements other than administrative borders, or where the OSM elements are too small to be visualized, are deemed unusable. These unusable patches are discarded and labeled as such in the raw data database. Only OSM data from valid patches is retained for further processing.\nThe predefined tasks further interpret the attributes of the OSM elements, forming task-oriented prompts for the LLM. The LLM takes these prompts as input and generates captions for the image patches. Finally, the generated captions are saved in an annotation database, and the status of the image patches is updated in the raw data database to reflect their completion."}, {"title": "2.1.3. Caption Augmentation", "content": "Data augmentation in the linguistic aspect has been shown to enhance the training of vision language models (VLMs) (Fan et al., 2024). Unlike data augmentation in the visual domain, such as color jittering, noising, and random cropping, which can be easily implemented online during model training, augmenting the language aspect with consistent linguistic semantics and varied tones is resource-intensive, requiring the use of an LLM for high-quality results. Therefore, we incorporate a caption augmentation process into our workflow to generate multiple revisions of each caption.\nDuring the caption augmentation process, captions that have not yet been revised are sampled from the annotation database. A specific revision task is defined to generate a prompt for the LLM based on the original caption and its initial task. The details of these tasks will be discussed in Subsection 2.2. The same LLM, Mixtral-7B, is used for this process. It takes the prompt as input and generates a revised caption for the image patch. The revised caption is then saved in the annotation database, and the status of the image patch is updated in the raw data database to reflect its completion.\nThis caption augmentation process is repeated until the desired number of revised captions is generated for each image patch. In RSTeller, our proposed dataset under this workflow, each image patch has at least two captions and up to five captions in total."}, {"title": "2.1.4. Dataset Compilation", "content": "As the annotation database populates and scales to the desired size, the final process in the workflow involves refining the annotations and compiling a dataset suitable for VLM training at scale.\nDuring the inference of the LLM, some inevitable errors in the generated captions can lower the overall quality of the dataset. These errors can be categorized into two types: those that can be fixed and those that must be deleted. Fixable errors typically follow common patterns, such as repeated sentences within the same caption or undesired outputs appended after the caption ends, following some pattern of the input prompt. These redundant sentences and patterns can be removed through a heuristic process, allowing the remaining content to be used effectively. Unfixable errors usually include blank responses and duplicate captions for the same image patch. These errors cannot be corrected and must be deleted. The presence of these errors primarily stems from the imperfections of the LLM, and their frequency may be reduced through the evolution of the LLM, improvement of the prompts, or usage of a larger LLM.\nOnce the captions are refined, they are used to update the annotation database for future use, making the data ready for final compilation. Training a typical deep learning model on a given dataset usually involves randomly shuffling the data and sequentially iterating through the shuffled dataset. However, storing image patches and their annotations individually on the local disk and reading them randomly during training is time-consuming, especially when the dataset comprises millions of images. Additionally, training a VLM typically requires large batch sizes with multiple GPUs, resulting in an I/O bottleneck.\nTo address this issue and facilitate the distribution of the dataset over the internet, we propose compiling the dataset into multiple shards of tar files following the WebDataset format7. Each tar file comprises a set of image patches and their annotations. The RSTeller dataset, proposed in this work, is provided in this format. During training, multiple shards of the dataset can be prefetched into memory, then randomly shuffled and iterated through. This approach effectively improves I/O performance and results in better training efficiency."}, {"title": "2.2. Captioning Tasks", "content": "The objective of the descriptive task is to generate detailed and contextually accurate descriptions of the OSM data associated with each image patch. These descriptions should capture the essential features and elements present within the patch, such as geographical landmarks, infrastructure, vegetation, and other notable features. These descriptions serve as the raw captions and are produced in Process 2. In a single image patch, there usually are a number of OSM elements, each with multiple attributes and tags. The information provided by all the OSM elements in an image patch can be overwhelming, making it challenging for an LLM to generate a concise and accurate description.\nTo streamline the task and enhance the efficiency of LLM annotation, we propose focusing on one distinctive OSM element per image patch for the description. In OSM, a two-dimensional object bounded by one or more linear ways and marked by appropriate tags is recognized as an \"area.\" Following this approach, we categorize all OSM elements in an image patch into two groups: \"area\" and \"non-area.\" The \"area\" category includes OSM elements like land use, natural features, and buildings, while the \"non-area\" category includes elements not recognized as an \"area,\" such as roads, coastlines, and other linear features. Given the significant differences in geometry and attributes between these two categories, we have designed two distinct tasks: Task 1 for the \"area\" category and Task 2 for the \"non-area\" category. Fig. 7 presents examples from the RSTeller dataset, showcasing image patches, selected OSM elements, and corresponding captions. The selected OSM elements are highlighted in red on the image patches. Image patches (2), (3), and (13) are captioned based on the \"non-area\" category, while all other image patches are captioned based on the \"area\" category.\nTask 1 (Area description): As is shown in Fig. 2, this task follows four steps: distinctive area selection, attribute interpretation, tag interpretation, and prompt assembly.\nDistinctive area selection: In this step, the OSM element with the largest area within the image patch is selected as the most distinctive element to be described. The selected element undergoes a validation check to ensure it is not too small to be visible in the image. In this work, a threshold of 0.1 is used to filter out elements with an area smaller than this threshold portion of the image patch.\nAttribute interpretation: To enrich the semantic information of the selected element and facilitate the annotation process of the LLM, we extract several predefined attributes of the selected element and interpret them using a heuristic rule-based method. We have defined five attributes to describe the element, as shown in Table 1, including coarse location, shape, normalized size, simplified geometry, and is-cropped. The coarse location attribute identifies the center of the element within a 3\u00d73 grid, assigning a label to each grid cell (e.g., \"left-top\", \"center\", \"right-bottom\"). The shape attribute describes the approximate shape of the element based on its geometry, defined by four categories: \"square\", \"rectangular\", \"circular\", and \"irregular\". These categories are determined by hard thresholding on the element's area-to-perimeter ratio and shape-to-area ratio. The normalized size attribute indicates the relative size of the element compared to the image patch. The simplified geometry attribute represents the simplified and normalized geometry of the element, formatted as coordinate pairs enclosed in brackets (x, y). To avoid overwhelming the LLM annotators with extremely complex geometries, we use the Douglas-Peucker algorithm (Ramer, 1972) to simplify the element's geometry. As some complex elements may be represented by multiple polygons, we encapsulate the coordinate pairs of each polygon within a list, and all polygons of an element are enclosed within braces. In this work, (0, 0) represents the bottom-left corner and (1, 1) the top-right corner, with all coordinates normalized to the range of [0, 1]. The is-cropped attribute is a boolean value that indicates whether parts of the element extend beyond the image patch, with values \"True\" or \"False\". This attribute is determined by checking if the element visible in the image patch is cropped from a larger geometry.\nTag interpretation: OSM tags can sometimes be simple yet ambiguous. For example, the tag \"man_made: works\" might be confusing to both the LLM and humans, as it can refer to various man-made structures. The OSM tag Wiki database provides explanations for such tags. According to the tag Wiki, \"man_made: works\" refers to a factory or industrial production plant. This clarification removes ambiguity and makes the tag more meaningful for the LLM. Therefore, we query all OSM tag descriptions of the selected element from the tag Wiki. Using these descriptions, we interpret the tags into a uniform and linguistically consistent format, instead of merely using the key-value pairs. For tags with unbounded values, such as \"name: Jeff Memorial Highway,\" we focus on interpreting the tag key instead. This results in a more comprehensive and accurate description of the selected element. An example of tag interpretation of the two kinds is shown in Table 2.\nPrompt assembly: Once the attributes and tags have been interpreted, the final step in Task 1 is to assemble all the interpretations into a coherent prompt for the LLM. This involves organizing the interpreted attributes and tag descriptions into a structured format that the LLM can effectively process. In this work, we employ a template-based approach for prompt assembly, where we define a task-specific template to structure the prompt and fill in the placeholders with the interpreted attributes and tags. A brief showcase of the prompt template for Task 1 is presented in Fig. 3. The overall structure of the prompt is divided into three parts. First, general instructions are provided to the LLM under the headings ###Instruction### and ###Captioning Objective###. The former offers general task instructions and some basic knowledge about the attributes used, while the latter highlights the attributes crucial for the task and details that require particular attention. Specifically, we instruct the LLM to not only describe the selected element but also infer its possible surroundings and context based on the provided information. The inferred contexts by the LLM should be highlighted by some words denoting uncertainty, such as \"possible\" or \"may\". This encourages the LLM to generate richer and more informative captions, expanding the scope of the task beyond the selected element to the entire image patch. Next, a one-shot example is included to illustrate the format and style of the desired output, helping the LLM understand the task requirements more clearly. The example follows the same format as the real task, consisting of a \"Raw\" and \"Caption\" pair. The \"Raw\" part includes attributes and tags interpreted from an example image patch, while the \"Caption\" part is the gold-standard caption from a human annotator and revised by GPT-4. Lastly, task-specific inputs are provided. The \"Raw\" part is a predefined context with multiple placeholders, where the interpreted attributes and tags are directly filled, except for the \"Partition\" attribute. If the \"Partition\" attribute is True, the placeholder is replaced with the sentence \"Some parts of the geometry extended out of this ROI,\u201d otherwise, it is left blank. The \"Caption\" part remains blank, awaiting the LLM's continual writing. This assembly step ensures that the LLM's output is both precise and aligned with the intended use of the remote sensing data, facilitating effective and insightful captioning."}, {"title": "Task 2 (Non-area description)", "content": "Similar to Task 1, this task involves four steps. Based on our inspection of the collected OSM data, the \"non-area\" with the longest length is sometimes a simple line from a large geometry, which is not well annotated by OSM tags. Therefore, unlike Task 1, which selects the OSM element with the largest size, Task 2 first identifies three OSM elements with the longest length and then select the one with the most tags as the most distinctive element to be described.\nIn the attribute interpretation step, we extract different attributes compared to Task 1 due to the geometrical differences between the \"area\" and \"non-area\" elements. The attributes for Task 2 are shown in Table 3. To better describe the position of the non-area element, we use endpoint locations instead of the coarse location derived from the element's center. The nine-grid system, the same as used in Task 1, assigns labels to the endpoints. The sinuosity attribute describes the shape of the element, with categories including \"straight,\" \"curved,\u201d \"twisted,\" \"closed,\" and \"broken,\" determined by thresholding the ratio between the element's length and the distance between its endpoints. The normalized length attribute represents the relative length of the element compared to the square root of the image patch area. The length attribute is the actual length of the element in meters. The orientation attribute, determined by calculating the angle between the endpoints and the horizontal axis, describes the direction of the element with categories like \"W_E,\" \"S_N,\" \"SW_NE,\" and \"NW_SE.\" The simplified geometry attribute represents the simplified and normalized geometry of the element, following the same scheme as in Task 1. Finally, the is-cropped attribute indicates whether the element is cropped by the image patch.\nThe subsequent steps, tag interpretation and prompt assembly are largely similar between Task 1 and Task 2, with modifications in the prompt template to support the attributes specific to Task 2. To avoid redundancy, we do not elaborate on the details of these steps as they are consistent with those explained in Task 1. This structured approach ensures that each task is tailored to the unique characteristics of the elements being described, enhancing the accuracy and relevance of the LLM-generated captions for remote sensing data. The careful design of tasks and attributes enables a more effective analysis and utilization of the OSM data within the image patches."}, {"title": "2.2.2. Caption Augmentation Task", "content": "The caption augmentation task aims to enhance the diversity and quality of the initial captions generated for the image patches, creating multiple revisions from the raw caption with different tones. This task, denoted as Task 3 in this work and illustrated in Fig. 2, is relatively simple compared to previous tasks involving OSM element selection and interpretation, as it is purely a linguistic task and does not require specialized knowledge. Consequently, it comprises only two steps: revision example sampling and prompt assembly.\nUnlike previous tasks that utilized a fixed example for each task, this task necessitates a larger number of examples to produce diverse and informative captions. To this end, we constructed a meta-example set containing multiple raw captions and their corresponding revisions. In this work, we separately selected five raw captions from Task 1 and Task 2 and generated five revisions for each raw caption, altering the tone, inflection, word choice, and length of the original caption. These revisions were written by human annotators and polished by GPT-4. In Task 3, examples are dynamically sampled from the meta-example set, selecting all five raw captions of the corresponding task and randomly choosing one of the five revisions for each raw caption. The five raw and revision pairs are then shuffled and used as the five-shot examples for the prompt assembly step. This approach ensures that the revisions generated by the LLM are diverse and informative.\nIn the prompt assembly step, a simpler prompt template is utilized, as shown in Fig. 4. The overall structure of the prompt template is similar to that of Tasks 1 and 2, divided into three parts: general instructions, few-shot examples, and task inputs. As the revision task is straightforward and does not require detailed caption objectives or specialized knowledge about the captioned element's attributes, the general instructions are kept concise, informing the LLM to revise an existing caption, specifying the coordinate system used, and indicating the provided examples. The five-shot examples in the prompt template are defined by several placeholders, filled by sampled examples from the previous step. The task inputs include a placeholder for the caption to be revised, leaving the revised result empty for the LLM to fill in.\nThis method ensures that the final augmented captions are of high quality, diverse, and aligned with the intended use of the remote sensing data, thus facilitating the effective and insightful analysis."}, {"title": "3. RSTeller Dataset", "content": "Utilizing the proposed dataset generation workflow, we have developed the RSTeller dataset. The data acquisition and processing were conducted using a local server equipped with 1"}]}