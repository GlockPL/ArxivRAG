{"title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models", "authors": ["Junyao Ge", "Yang Zheng", "Kaitai Guo", "Jimin Liang"], "abstract": "Abundant, well-annotated multimodal data in remote sensing are pivotal for aligning complex visual remote sensing (RS) scenes with human language, enabling the development of specialized vision language models across diverse RS interpretation tasks. However, annotating RS images with rich linguistic semantics at scale demands expertise in RS and substantial human labor, making it costly and often impractical. In this study, we propose a workflow that leverages large language models (LLMs) to generate multimodal datasets with semantically rich captions at scale from plain OpenStreetMap (OSM) data for images sourced from the Google Earth Engine (GEE) platform. This approach facilitates the generation of paired remote sensing data and can be readily scaled up using openly available data. Within this framework, we present RSTeller, a multimodal dataset comprising over 1 million RS images, each accompanied by multiple descriptive captions. Extensive experiments demonstrate that RSTeller enhances the performance of multiple existing vision language models for RS scene understanding through continual pre-training. Our methodology significantly reduces the manual effort and expertise needed for annotating remote sensing imagery while democratizing access to high-quality annotated data. This advancement fosters progress in visual language modeling and encourages broader participation in remote sensing research and applications. The RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.", "sections": [{"title": "1. Introduction", "content": "Vision language models (VLMs) hold great promise in remote sensing (RS) as they offer a means to interpret complex visual scenes for non-expert users in a human-like manner and support the development of diverse RS applications. However, the effective training of VLMs hinges on the availability of large quantities of paired data to ensure the alignment of visual and linguistic semantics for optimal performance. The substantial data requirements present a significant challenge in scaling up VLMs for RS, impeding their broader applicability and performance enhancement. Consequently, there is a pressing need for automated, scalable, and efficient methods to generate datasets with paired RS imagery and semantic-rich captions. Such methods are essential for overcoming the bottleneck associated with data scarcity and facilitating the development of robust VLMs tailored for RS tasks. Moreover, the availability of an out-of-the-box, well-annotated dataset comprising RS images with rich linguistic semantics holds immense value. It not only streamlines the initial training of VLMs but also contributes to reducing the environmental impact associated with the individual generation of such data, thereby addressing sustainability concerns."}, {"title": "1.1. Related works", "content": "The intersection of computer vision and natural language processing (NLP) has witnessed remarkable advancements in recent years, propelled by the emergence of large-scale pre-trained models. When CLIP (Radford et al., 2021) was initially introduced, it swiftly captivated the research community with its remarkable performance in zero-shot classification and cross-modal retrieval tasks, rivaling some state-of-the-art models trained with full supervision. CLIP achieves this feat by aligning image and text representations through a contrastive learning strategy, endowing it with the capability for zero-shot inference using the learned representations. Inspired by this paradigm, the RS community embarked on leveraging similar strategies to construct VLMs tailored for RS tasks. Notable efforts such as those by Liu et al. (2024a), Zhang et al. (2023), and Wang et al. (2024) have undertaken continual pre-training of CLIP on paired RS data, thereby extending its capabilities to encompass RS scene understanding.\nConcurrently, the advent of large language models (LLMs), exemplified by ChatGPT (Brown et al., 2020; Ouyang et al., 2022), alongside other models (Brown et al., 2020; Ouyang et al., 2022; Du et al., 2021; Jiang et al., 2024), has significantly enriched the landscape of NLP. These LLMs possess the remarkable ability to interact with humans through natural language, facilitate in-context learning, instruction following (Ouyang et al., 2022), and reasoning (Wei et al., 2022), showcasing their versatility across diverse linguistic tasks. Motivated by the power of LLMs, researchers have proposed to"}, {"title": "1.2. Contributions", "content": "In this work, we leverage openly available remote sensing data as an abundant resource for multimodal data generation. Specifically, we utilize the GEE platform to collect unlabeled aerial images, and the OSM platform as our semantic source. We employ Mixtral-7B (Jiang et al., 2024), an open-source LLM, to interpret plain OSM data. The OSM tag contents and their combinations are used to generate fluent, semantically rich descriptions for the aerial images. To ensure reproducibility and facilitate further scaling up of a dataset suitable for VLM training, we develop a comprehensive dataset generation workflow that encompasses raw data fetching, LLM-based captioning, and final dataset compilation.\nThrough this workflow, we introduce RSTeller, a multimodal dataset consisting of over one million remote-sensing images, each accompanied by multiple descriptive captions. We use the Measure of Textual Lexical Diversity (MTLD) score (McCarthy and Jarvis, 2010) as an indicator of the semantic richness of the captions. Our dataset is over two times more semantically rich than existing datasets (Fig. 1). Given the computational demands of LLM inference, our dataset provides a ready-to-use resource for researchers, eliminating the need for individual data generation.\nExtensive experiments were conducted to validate the proposed methods and provide valuable insights for researchers training VLMs on RS data. First, we studied the relationship between the performance of the VLMs and the scale of RS domain data, highlighting the potential of further scaling up the training data. Then, we evaluated the effectiveness of RSTeller by continuing the pre-training of multiple VLMs and testing them on downstream tasks. The proposed data generation workflow and dataset are expected to be instrumental in advancing more sophisticated VLMs within the remote sensing field."}, {"title": "1.3. Paper structure", "content": "The structure of this paper is as follows: Section 2 details the proposed automatic data generation workflow. Section 3 describes and discusses the generated dataset, RSTeller, exploring its attributes in detail. Section 4 presents extensive experiments that explore the model performance concerning the data scale and the effectiveness of the proposed dataset. Section 5 provides some findings and suggestions according to our experiments. Section 6 outlines the limitations of the proposed work. Finally, Section 7 offers a summary of the research."}, {"title": "2. Automated Workflow for Dataset Generation", "content": "In this section, we detail the automated workflow developed to generate a large-scale, RS-related multimodal dataset from openly available data. The overall workflow can be divided into two parts and is shown in Fig. 2. The left panel of the figure illustrates the main processes, including data fetching, LLM-based captioning, and final dataset compilation. All the processes can be run in parallel for the maximum efficiency. The right panel provides details of the captioning tasks that facilitate the generation of semantically rich captions."}, {"title": "2.1. Main Processes", "content": "2.1.1. Raw Data Acquisition\nThe first process in the workflow is to fetch raw data from the GEE and OSM platforms into well-defined local databases. For ease of reference, we designate these databases as the \"raw data database,\" as depicted in Process 1 of Fig. 2. This database is crucial for preserving the downloaded data and enabling asynchronous handling of different processes to accelerate the entire workflow. The GEE platform provides a vast collection of aerial images, each typically covering a wide range of geographical"}, {"title": "2.1.2. Raw Caption Generation", "content": "Once the data fetching process is underway and a sufficient amount of raw data has been acquired, the workflow proceeds to the generation of raw captions. This process involves using an LLM, specifically Mixtral-7B (Jiang et al., 2024) in this work, to interpret and describe the OSM data associated with each image patch according to predefined tasks. Instead of using an instruction fune-tuned LLM, like Mixtral-7B-instruct, the use of the base LLM is advantageous. It provides greater flexibility in generating captions because it is not constrained by specific instructional patterns. This allows it for better following the instructions and examples provided by the prompt, making the tone and style of the generated captions more consistent with the intended purpose.\nIn this process, a patch that has not yet been captioned is first sampled from the raw data database. The corresponding OSM data is then fetched from the cached OSM database. The OSM data undergoes processing based on our predefined tasks, which will be discussed in Subsection 2.2. This step checks whether the OSM data contains tags that can be used to generate captions for the patch. Patches that contain no relevant way or relation elements other than administrative borders, or where the OSM elements are too small to be visualized, are deemed unusable. These unusable patches are discarded and labeled as such in the raw data database. Only OSM data from valid patches is retained for further processing.\nThe predefined tasks further interpret the attributes of the OSM elements, forming task-oriented prompts for the LLM. The LLM takes these prompts as input and generates captions for the image patches. Finally, the generated captions are saved in an annotation database, and the status of the image patches is updated in the raw data database to reflect their completion."}, {"title": "2.1.3. Caption Augmentation", "content": "Data augmentation in the linguistic aspect has been shown to enhance the training of vision language models (VLMs) (Fan et al., 2024). Unlike data augmentation in the visual domain, such as color jittering, noising, and random cropping, which can be easily implemented online during model training, augmenting the language aspect with consistent linguistic semantics and varied tones is resource-intensive, requiring the use of an LLM for high-quality results. Therefore, we incorporate a caption augmentation process into our workflow to generate multiple revisions of each caption.\nDuring the caption augmentation process, captions that have not yet been revised are sampled from the annotation database. A specific revision task is defined to generate a prompt for the LLM based on the original caption and its initial task. The details of these tasks will be discussed in Subsection 2.2. The same LLM, Mixtral-7B, is used for this process. It takes the prompt as input and generates a revised caption for the image patch. The revised caption is then saved in the annotation database, and the status of the image patch is updated in the raw data database to reflect its completion.\nThis caption augmentation process is repeated until the desired number of revised captions is generated for each image patch. In RSTeller, our proposed dataset under this workflow,"}, {"title": "2.1.4. Dataset Compilation", "content": "As the annotation database populates and scales to the desired size, the final process in the workflow involves refining the annotations and compiling a dataset suitable for VLM training at scale.\nDuring the inference of the LLM, some inevitable errors in the generated captions can lower the overall quality of the dataset. These errors can be categorized into two types: those that can be fixed and those that must be deleted. Fixable errors typically follow common patterns, such as repeated sentences within the same caption or undesired outputs appended after the caption ends, following some pattern of the input prompt. These redundant sentences and patterns can be removed through a heuristic process, allowing the remaining content to be used effectively. Unfixable errors usually include blank responses and duplicate captions for the same image patch. These errors cannot be corrected and must be deleted. The presence of these errors primarily stems from the imperfections of the LLM, and their frequency may be reduced through the evolution of the LLM, improvement of the prompts, or usage of a larger LLM.\nOnce the captions are refined, they are used to update the annotation database for future use, making the data ready for final compilation. Training a typical deep learning model on a given dataset usually involves randomly shuffling the data and sequentially iterating through the shuffled dataset. However, storing image patches and their annotations individually on the local disk and reading them randomly during training is time-consuming, especially when the dataset comprises millions of images. Additionally, training a VLM typically requires large batch sizes with multiple GPUs, resulting in an I/O bottleneck.\nTo address this issue and facilitate the distribution of the dataset over the internet, we propose compiling the dataset into multiple shards of tar files following the WebDataset format. Each tar file comprises a set of image patches and their annotations. The RSTeller dataset, proposed in this work, is provided in this format. During training, multiple shards of the dataset can be prefetched into memory, then randomly shuffled and iterated through. This approach effectively improves I/O performance and results in better training efficiency."}, {"title": "2.2. Captioning Tasks", "content": "2.2.1. Descriptive Tasks\nThe objective of the descriptive task is to generate detailed and contextually accurate descriptions of the OSM data associated with each image patch. These descriptions should capture the essential features and elements present within the patch, such as geographical landmarks, infrastructure, vegetation, and other notable features. These descriptions serve as the raw captions and are produced in Process 2. In a single image patch, there usually are a number of OSM elements, each with multiple attributes and tags. The information provided by all the"}, {"title": "Task 1 (Area description)", "content": "As is shown in Fig. 2, this task follows four steps: distinctive area selection, attribute interpretation, tag interpretation, and prompt assembly.\nDistinctive area selection: In this step, the OSM element with the largest area within the image patch is selected as the most distinctive element to be described. The selected element undergoes a validation check to ensure it is not too small to be visible in the image. In this work, a threshold of 0.1 is used to filter out elements with an area smaller than this threshold portion of the image patch.\nAttribute interpretation: To enrich the semantic information of the selected element and facilitate the annotation process of the LLM, we extract several predefined attributes of the selected element and interpret them using a heuristic rule-based method. We have defined five attributes to describe the element, as shown in Table 1, including coarse location, shape, normalized size, simplified geometry, and is-cropped. The coarse location attribute identifies the center of the element within a 3\u00d73 grid, assigning a label to each grid cell (e.g., \"left-top\", \"center\", \"right-bottom\"). The shape attribute describes the approximate shape of the element based on its geometry, defined by four categories: \"square\", \"rectangular\", \"circular\", and \"irregular\". These categories are determined by hard thresholding on the element's area-to-perimeter ratio and shape-to-area ratio. The normalized size attribute indicates the relative size of the element compared to the image patch. The simplified geometry attribute represents the simplified and normalized geometry of the element, formatted as coordinate pairs enclosed in brackets (x, y). To avoid overwhelming the LLM annotators with extremely complex geometries, we use the Douglas-Peucker algorithm (Ramer, 1972) to simplify the element's geometry. As some complex elements may be represented by multiple polygons, we encapsulate the coordinate pairs of each polygon within a list, and all polygons of an element are enclosed within"}, {"title": "Task 2 (Non-area description)", "content": "Similar to Task 1, this task involves four steps. Based on our inspection of the collected OSM data, the \"non-area\" with the longest length is sometimes a simple line from a large geometry, which is not well annotated by OSM tags. Therefore, unlike Task 1, which selects the OSM element with the largest size, Task 2 first identifies three OSM elements with the longest length and then select the one with the most tags as the most distinctive element to be described.\nIn the attribute interpretation step, we extract different attributes compared to Task 1 due to the geometrical differences between the \"area\" and \"non-area\" elements. The attributes for Task 2 are shown in Table 3. To better describe the position of the non-area element, we use endpoint locations instead of the coarse location derived from the element's center. The nine-grid system, the same as used in Task 1, assigns labels to the endpoints. The sinuosity attribute describes the shape of the element, with categories including \"straight,\" \"curved,\u201d \"twisted,\" \"closed,\" and \"broken,\" determined by thresholding the ratio between the element's length and the distance between its endpoints. The normalized length attribute represents the relative length of the element compared to the square root of the image patch area. The length attribute is the actual length of the element in meters. The orientation attribute, determined by calculating the angle between the endpoints and the horizontal axis, describes the direction of the element with categories like \"W_E,\" \"S_N,\" \"SW_NE,\" and \"NW_SE.\" The simplified geometry attribute represents the simplified and normalized geometry of the element, following the same scheme as in Task 1. Finally, the is-cropped attribute indicates whether the element is cropped by the image patch."}, {"title": "2.2.2. Caption Augmentation Task", "content": "The caption augmentation task aims to enhance the diversity and quality of the initial captions generated for the image"}, {"title": "3. RSTeller Dataset", "content": "Utilizing the proposed dataset generation workflow, we have developed the RSTeller dataset. The data acquisition and processing were conducted using a local server equipped with 128 cores, 512 GB of memory, and four NVIDIA A800 GPUs. We deployed one Mixtral-7B instance on each GPU to generate the captions in parallel. The data acquisition process took approximately four months, during which we collected roughly 15 million image patches. Of these, approximately 7 million had corresponding OSM data fetched from the Kumi endpoint. The captioning process ran concurrently with data acquisition and processing, achieving a maximum throughput of 50,000 captions per day. This process spanned over two months and generated approximately 3 million captions. Ultimately, 2.5 million"}, {"title": "3.1. Dataset Overview", "content": "RSTeller dataset comprises approximately 1.2 million remote-sensing image patches, each accompanied by multiple captions, ranging from 2 to 5, resulting in roughly 2.5 million image-text pairs. The images are sourced from the National Agriculture Imagery Program (NAIP) on the GEE platform, with a ground sample distance (GSD) of 0.6 meters. Geographically, the dataset focuses on the United States, with images acquired via aircraft. Detailed attributes of the dataset are presented in Table 4."}, {"title": "3.2. Caption Analysis", "content": "This section evaluates the captions generated by the LLM to identify key attributes and potential biases within the RSTeller dataset. Our aim is to provide valuable insights that will inform the effective utilization of the dataset in future applications.\nFirst, we examine the distribution of caption lengths, determined by counting the number of tokens using the Natural Language Toolkit (NLTK) word tokenizer. The caption lengths vary from 5 to 192 tokens, with a median length of 48 tokens and an average length of 54.2 tokens and its distribution is depicted in Fig. 8(a). This variation underscores the diversity within the captions, which could enhance the robustness of VLM training. However, given that the CLIP model has a maximum input length of 77 tokens, captions exceeding this limit may compromise performance, highlighting the need for meticulous preprocessing.\nMoreover, we assess the metadata richness by analyzing the number of tags associated with each OSM element used for captioning. As Fig. 8(b) illustrates, while most images feature fewer than 10 tags, some are annotated with over 30, with a maximum of 63, indicating a skewed distribution with a long tail.\nSubsequently, we scrutinize the most frequent non-stop words in the captions to assess content richness. The top 50 most frequent words are shown in Fig. 9. Common but meaningless stop words such as \"the\", \"is\", and \"and\" are filtered out. The remaining words are sorted by their frequency in descending order. Words suggesting inferential reasoning like \"likely\" and \"possibly\" are prevalent, reflecting the LLM's inference from the OSM tags and its knowledge. For example, in image patch 8 of Fig. 7, the OSM tag only contains the tag \"landuse: farmland,\" but the caption generated by the LLM extends the semantics by adding \"It is likely used for growing crops,\" connecting shallow tag information to deeper meanings. Additionally, words like \"meters,\" \"within,\" and \"orientation\" appear frequently, demonstrating the richness of shape and geography information in the dataset due to the interpretation of attributes in the captioning tasks. For instance, the caption for image patch 3 in Fig. 7 precisely describes the road's orientation (south to north) and length (270 meters).\nAdditionally, we delve into the topical diversity of the captions by investigating the frequency of the top 20 keys and their corresponding values within the OSM tags used for captioning. As shown in Fig. 10(a), the top 20 most frequent keys include feature-related keys such as highway, natural, landuse, waterway, and building, while others are more general like name and TIGER (Topologically Integrated Geographic Encoding and Ref"}, {"title": "4. Experiments", "content": "To validate the proposed automated dataset generation workflow and assess the effectiveness of the RSTeller dataset, we performed a comprehensive series of experiments. These experiments involved the continual pre-training of the CLIP model with the RSTeller dataset, followed by evaluations of its performance on zero-shot image classification and retrieval tasks. The purpose of these experiments was to rigorously test several hypotheses regarding the utility and impact of the dataset on the training of robust VLMs tailored for remote sensing applications.\nThe experimental series is structured as follows:\nData Scalability and Model Performance Analysis: The first experiment explores the relationship between the performance of VLMs in remote sensing and the scale of domain-specific data, specifically examining the influence of the number of RS-related image-text pairs. This experiment aims to understand how data volume affects model accuracy and generalization capabilities in open-set conditions.\nDataset Effectiveness Evaluation: We then evaluate the effectiveness of the RSTeller dataset through continual"}, {"title": "4.1. Data", "content": "The training data utilized in our experiments consists of three distinct groups: the RSTeller dataset, the SkyScript (Wang et al., 2024) dataset, and the LAION-10M dataset, which is a subset of the LAION-400M (Schuhmann et al., 2021). The detailed information is provided in Table 5 and explained below."}, {"title": "4.2. CLIP Continual Pre-training", "content": "VLMs are typically pre-trained on a large, predefined dataset and have their parameters fixed upon the completion of pre-training. However, given the dynamic nature of the world, the static pre-training dataset may not adequately encompass the knowledge required for specific downstream tasks. To address this gap, continual pre-training is employed to integrate new knowledge into an existing model by incorporating domain-specific data and adhering to the established pre-training paradigm.\nOur study adopts continual pre-training to investigate the relationship between the scale of domain-specific data and performance enhancements in the CLIP model. We examine the influence of our innovative RSTeller dataset on model performance. Known challenges associated with training exclusively on new domain data include significant performance degradation due to catastrophic forgetting and potential overfitting. To mitigate these challenges, our experiments incorporate data from the original domains, specifically LAION-10M, throughout the continual pre-training process. We consistently sample one-third of the data from LAION-10M for each training batch while the rest are sampled from the RS domain. This technique is applied across all our experiments to ensure balanced training and maintain model stability.\nWe analyzed two versions of CLIP models: ViT-B/32 and ViT-L/14. The ViT-B configuration utilizes 12 transformer layers and 12 attention heads, whereas the ViT-L configuration comprises 24 transformer layers and 16 attention heads. The denominations /32 and /14 represent the respective patch sizes, which are 32 x 32 and 14 \u00d7 14 pixels, processed by the model tokenizer. Initial model setups are derived from various checkpoints, including those from the OpenCLIP project (Ilharco et al., 2021) trained on the LAION-2B dataset (Schuhmann et al., 2022), which provides a baseline of regularly pre-trained models on generic web-based image-text data. Additionally, we utilized checkpoints commonly employed by the industrial community, trained on LAION-400M (Schuhmann et al., 2021), WIT (Radford et al., 2021), and DataComp (Gadre et al., 2023), to ascertain the benefits conferred by our RSTeller dataset in the RS domain. A brief comparison of the data amount among these datasets and all the checkpoints used in our experiments are presented in Table 8. The model checkpoints are represented by the identifier used in the OpenCLIP repository (Ilharco et al., 2021)."}, {"title": "4.3. Model Performance wrt. Domain Data Scale", "content": "This experiment was designed to investigate whether scaling the domain-specific data enhances model performance. To this end, we amalgamated our RSTeller dataset with the SkyScript dataset to create a robust RS domain dataset comprising approximately 5 million image-text pairs. This extensive dataset allows for a detailed analysis of the model's performance as it relates to variations in domain data volume. We partitioned the data into four equal subsets, each representing about one-fourth of the total data, and ensured no overlap between the subsets. In our experiments, we incrementally incorporated these subsets into the training process, thereby systematically increasing the domain data scale. Models pre-trained on LAION-2B were used as the starting point for the continual pre-training. The models were then trained on approximately 25%, 50%, 75%, and 100% of the domain data, respectively.\nThe results for zero-shot classification are shown in Fig. 11. The x-axis represents the volume of RS-related domain data used for continual pre-training, while the y-axis indicates the top-1 classification error on the validation dataset. Initially, we evaluated the loaded checkpoints directly on benchmark datasets to establish a baseline for models pre-trained on generic web-based image-text data, represented as stars in the figure. Subsequently, we continually pre-trained the models on RS domain-specific data and evaluated them on the same benchmarks, with the results denoted as dots in the figures. To better visualize performance trends, linear fits were applied to the data points, shown as dashed lines, with the slope values k annotated to reflect the changes in error rates.\nMost benchmarks show improvement when domain data is scaled up. Notably, the slopes of both models' error rates are negative (kAverage = -0.75 for ViT-B/32 and kAverage = -0.34 for ViT-L/14). Benchmarks such as PatternNet, RESISC, and AID exhibit substantial improvements, as indicated by the slopes of their linear fits (e.g., kpatternNet = -1.63, KRESISC = -1.03 for the ViT-B/32 model, and kAID = -1.36 for the ViT-L/14 model). These results underscore the positive impact of enlarging the domain-specific dataset on the model's classification accuracy. The EuroSAT on the ViT-L/14 model, on the other hand, demonstrates an unexpected increase in top-1 classification error as the domain data scale rises (kEuroSAT = 0.76). This anomaly suggests potential noise in the training process or an inherent difficulty in this benchmark that requires more careful consideration in scaling the domain data.\nFor some benchmarks, such as AID and RSI-CB, there is an initial performance degradation when domain-specific data is first introduced. In extreme cases, even with all the domain data used for training, the fine-tuned model performance still does not surpass the baseline (e.g., AID for ViT-B/32). Neverthe"}, {"title": "4.4. Dataset Effectiveness", "content": "To assess the benefits of the RSTeller dataset for pre-trained models, a series of continual training experiments were conducted on various CLIP checkpoints, evaluating their zero-shot capabilities before and after the continual pre-training. Notably, in this experiment, the SkyScript data was excluded from the training data, differing from the previous experiment. The LAION-10M was also utilized to prevent catastrophic forgetting and overfitting.\nThe evaluation results are presented in Tables 9 and 10. These tables report the top-1 accuracy for zero-shot classification tasks and the average recall rate for zero-shot text-to-image retrieval tasks. For each pre-trained checkpoint, the performance of the baseline checkpoints on the benchmark datasets, the performance after continual pre-training on the RSTeller dataset, and the corresponding improvements (marked as \u2206) are shown. The positive improvements are highlighted in red, while the negative changes are highlighted in blue. For each benchmark, the largest and least improvements are highlighted in boldface, and the best results within the continual pre-trained models are underlined."}, {"title": "5. Discussions", "content": "The above experimental results provide several key insights into the impact of scaling domain-specific data on the performance of VLMs as well as the continual pre-training in the remote sensing domain. The findings offer valuable guidance for future research and practical applications in dataset generation and model training.\nFirst, the results in domain data scaling (Section 4.3) confirm that increasing the volume of domain-specific data generally enhances the model performance across various benchmarks. This is evidenced by the consistent improvement in both zero-shot classification and image retrieval tasks as more domain data is included. However, the degree of improvement varies among different benchmarks, indicating that the effectiveness of domain-specific data augmentation is influenced by the nature of the benchmark dataset. For instance, benchmarks like PatternNet and AID showed significant performance gains, whereas others like EuroSAT exhibited less pronounced improvements or even anomalies in certain cases.\nOne notable observation is that the initial performance degradation observed in some benchmarks, such as the AID and RSI-CB, when domain-specific data is first introduced. Despite this initial setback, continuous scaling of the domain data eventually leads to performance improvements, suggesting that the models require a certain threshold of domain-specific data to effectively learn and generalize. This indicates the importance of not being discouraged by the initial performance drops and continuing to expand the dataset for long-term gains.\nThe ViT-L/14 model consistently outperformed the ViT-B/32 model across most benchmarks, regardless of whether the models were fine-tuned or not. This underscores the advantage of larger models in handling diverse and complex tasks in the RS domain. However, the fine-tuned ViT-L/14 models failed to surpass the original checkpoint performance in some of the benchmarks, even with the full domain data included. This suggests that the overall distribution of the domain dataset created in this experiment could still be different from the benchmark datasets, causing a substantial domain gap. Increasing the volume of data from more diverse sources may bridge this gap.\nFinally, the experiments in testing the RSTeller dataset (Section 4.3) revealed that the starting checkpoint plays a critical role in the effectiveness of continual pre-training. Models pre-trained on DataComp outperformed those pre-trained on other"}, {"title": "6. Limitations of the Proposed Work", "content": "Despite its strengths, our approach that scales RS domain multimodal data for training RS VLMs has the following limitations that warrant future investigation.\nFirst, the captioning tasks within our dataset generation workflow are simplistic, typically focusing on a single element, which might not capture all pertinent details in the images. For instance, image patch 4 in Fig. 7 is captioned solely by its depiction of an industrial complex, omitting other significant elements like cars and the parking lot within. Similarly, image patch 10 focuses only on a single solar panel within a larger solar power plant, potentially misleading in terms of scale, location, and quantity. Future enhancements could include more intricate captioning tasks to encompass a broader range of information within the images.\nMoreover, while the RSTeller dataset is a valuable resource, its scale and diversity are not yet comparable to larger datasets such as LAION-400M. It is sourced from a single platform with a fixed ground sample distance and specific image bands, and"}, {"title": "7. Conclusion", "content": "In this paper, we have meticulously developed and presented an automated dataset generation workflow that enables researchers to efficiently gather and utilize multimodal data for RS applications on a large scale. This workflow optimizes the use of existing, openly available remote-sensing data and employs an LLM to transform raw data into a high-quality, usable dataset for training VLMs in the remote sensing domain. Central to our workflow is the introduction of RSTeller, a multimodal dataset comprising over 2.5 million high-quality image-text pairs with diverse and informative captions, suitable for a wide range of research tasks. Utilizing this dataset for training RS VLMs not only facilitates access to high-quality data for researchers but also reduces the environmental impact associated with generating and maintaining such a dataset. Moreover, we highlight the potential for significant advancements in the field of RS VLMs through the continual enrichment of RS-related multimodal data. We envision future research expanding these datasets to a larger scale, thus enabling the training of more robust VLMS and enhancing the potential prosperity achievable in this field."}, {"title": "Declaration of Competing Interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\nDuring the preparation of this work the authors used ChatGPT in order to improve readability and language. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication"}]}