{"title": "Large Language Model Guided Self-Debugging Code Generation", "authors": ["MUNTASIR ADNAN", "ZHIWEI XU", "CARLOS C. N. KUHN"], "abstract": "Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state- of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.", "sections": [{"title": "1 Introduction", "content": "The evolution of generative artificial intelligence has profoundly influenced multiple domains, notably in automated code generation. Particularly, Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation given natural language prompts, streamlining software development processes and enhancing productivity [7, 24, 30]. This transformative potential has sparked significant research in this field. However, several critical challenges emerge as these systems scale to handle increasingly complex programming tasks. Ensuring reliability, accuracy, and autonomy in code generation remains critical but challenging, particularly in error detection and debugging, that align with human developers' intentions [9, 10, 23]. Recent research has introduced frameworks consisting of multiple LLM-based agents to emulate human problem-solving capability in code generation. For instance, MapCoder [17], AgentCoder [14], and LDB [37] employ agents dedicated to retrieval, planning, coding, and debugging to enhance code quality. While these approaches have shown remarkable improvements, they often require substantial computational resources and complex agent coordination. This poses challenges for real-world deployment and scalability.\nTo address these concerns, we propose a novel framework, PyCapsule, that enhances the efficiency and accuracy of LLM-based Python code generation through a two-agent architecture. This simple yet effective two-agent interaction ensures functionality independence without overcrowding with AI agents, thereby minimizing its computational consumption. PyCapsule employs a programmer agent responsible for code generation and debugging, interacting with an executor agent that handles code validation, case testing, and error analysis. The framework's effectiveness is further"}, {"title": "2 Related Work", "content": "Automated high-quality code generation has evolved in several key directions related to prompt engineering [20, 33], multi-agent systems [3, 34], and iterative debugging with runtime feedback mechanisms [6, 14, 17, 22, 27]. While LLM has demonstrated increasing capability in code generation, the code quality and correctness can be further enhanced by using intelligent debugging strategies. Therefore, previous works in self-debugging [6, 15] show their capability in Automated Program Repair (APR) by identifying and fixing significant code flaws and bugs. Researchers have shown that \"large language models of code\" [15] often struggle to directly fix bugs in the generated code due to the lack of task-specific fine-tuning. These fine-tuned models outperform the state-of-the-art APR tools [11, 19, 36] but still suffer from the loss of pre-trained knowledge [1, 35], lack of debugging ingredients, inferior long-sequence handling ability, high resource constraints, long-tail problems, and multi-hunk error fixing [15].\nParticulary, MapCoder [17] presents a multi-agent framework designed to emulate human-level programming processes. It employs four agents for retrieval, planning, coding, and debugging to perform iterative code generation and debugging. AgentCoder [14] introduces a three-agent framework to address high token usage and coordination overhead observed in other multi-agent systems like MapCoder [17], MetaGPT [12], and ChatDev [29]. The framework comprises a programmer agent, a test designer agent, and a test executor agent. The \"Debug Like a Human\" framework [37] introduces Large Language Model Debugger (LDB), which enhances the code quality by leveraging runtime execution information. LDB is performed iteratively through segmenting code, tracking intermediate variable states, and performing step-by-step verification with breakpoints. Moreover, some approaches have employed prompt engineering to improve the quality of code generation and debugging. For instance, CodeCoT [13] leverages iterative refinement through Chain of Thought (CoT) [33] and logical reasoning. However, its capability is limited to refining syntax errors.\nWhile these approaches have advanced automated code generations, they share several notable limitations, particularly the resource intensity for complex problems. Additionally, they often"}, {"title": "3 Methodology", "content": "3.1 Programmer and Executor Agents\nPyCapsule employs two distinct types of agents as defined in the agent-based computing literature [18, 25]: smart agents and collaborative agents. The system consists of two agents: a programmer agent and an executor agent. The programmer agent generates code and handles debugging while the executor agent acts as an autonomous validator that continuously monitors and responds to code execution outcomes. When execution succeeds, it forwards the validated code to the user; otherwise, it generates detailed diagnostic reports and initiates a real-time feedback loop with the programmer agent for self-debugging.\nThe programmer agent leverages Ollama and GPT infrastructure embedded in OpenSI-COSMIC [1] to facilitate self-debugging and code generation through a structured pipeline. The programmer agent employs tailored system prompts with two programming modes, generation mode and fix mode. These prompts, detailed in Appendix E, define the programmer agent's persona [31], provide task-specific background, and structurally simplify code extraction. In the generation mode, the programmer agent uses CoT [33] to analyze the problem description and devise a structured solution to generate executable code. In the fix mode, the executor agent uses error information from the previous debugging attempt. This mode is supported by the original problem description, prior solution, and error messages processed by the error-handling module. The conversation history"}, {"title": "3.2 Supportive Modules", "content": "Existing approaches like MapCoder [17] and AgentCoder [14] rely heavily on LLM agents for code generation. However, this requires a high resource consumption. In contrast, PyCapsule employs three specialised modules to handle each task deterministically, including a signature converter, an example call detector, and an error handling module.\nThe signature converter mitigates the instability of code generation by parsing problem descriptions provided in datasets through inferring structured function signatures. This module generates a function name, a robust signature, and an example call using the first test case for a given problem without revealing the case-testing result, thereby avoiding extra LLM calls for signature inference.\nThe example call detector ensures the safety of code execution by removing any internal example calls, which otherwise can lead to an infinite loop of function calls. This automatic detection replaces a dedicated LLM agent to check code execution safety.\nThe error-handling module refines error messages provided by the executor agent, giving concise and highly relevant natural language feedback. It also identifies the error type, filters irrelevant tracebacks, and resolves critical issues such as verbose recursion errors that can disrupt the programmer agent due to the LLM's limited context length. This systematic approach requires fewer tokens in self-debugging iterations compared to the raw but redundant error messages.\nThese modules collectively enhance the code generation stability, safety, and self-debugging efficiency while reducing the computational overhead. More details are provided in Appendix C."}, {"title": "3.3 Code Generation Workflow", "content": "PyCapsule consists of three phases: code generation, execution, and self-debugging, shown in Figure 1. This pipeline integrates the programmer agent and executor agent with the self-bugging mechanism to ensure reliable code generation and case testing with high efficiency and accuracy.\nThe workflow starts with the programmer agent that receives a problem description, typically supplemented by a function signature either from the dataset or the signature converter module. Subsequently, the agent generates a response with CoT reasoning, environment setup instructions, and code synthesis. Then, the agent's response is parsed to extract the synthesized code and environment setup instructions, with the latter automatically generating a text file, requirements.txt, containing all required Python libraries.\nThe extracted code is further processed through the example call detector to remove any unintended example calls. At the generation phase, the processed code with associated test cases from the dataset is saved in a Python entry file main.py for code execution. To ensure code execution safety, we add a timer with a maximum execution time to prevent infinite loops and raise an out-of-time error for those requiring longer running time."}, {"title": "4 Experiments", "content": "All GPU-related experiments were conducted on a single NVIDIA GeForce RTX 3090 (24 GB). We evaluated performance using success rate, defined by the ratio of correctly resolved problems to total problems. Our code will be released upon publication."}, {"title": "4.1 Dataset Overview", "content": "We used five benchmark datasets to evaluate PyCapsule: HumanEval [4], HumanEval-ET [8], MBPP [2], MBPP-ET [8], and BigCodeBench [38], where the \"ET\" variants provide more test cases and enriched problem descriptions. These datasets encompass diverse programming challenges. Particularly, HumanEval [4] consists of 164 human-curated programming problems for string manipulation, arithmetic, and basic data structures. Each problem includes a natural language description, a function signature, and test cases. MBPP [2] includes 974 programming problems spanning algorithms, data structures, and general-purpose tasks. It provides natural language descriptions and test cases but no function signatures to guarantee code generation stability. BigCodeBench [38] includes 1,140 problems with two splits: the complete split for code completion with specific docstrings and the instruct split with concise instructions for advanced reasoning."}, {"title": "4.2 Results and Analysis", "content": "Unlike other multi-agent architectures discussed in Section 2 that heavily rely on code generation agents, PyCapsule has more effective coordination among the computational overhead reduction, LLM's API calls, and token processing.\nInfluence of Self-debugging Attempts. To evaluate the influence of each self-debugging attempt on the success rate, we analyze the normalized and accumulated impacts of each incremental attempt using GPT-4-1106-preview, GPT-3.5-Turbo-1106, and Qwen-2.5-Coder-Instruct models as the LLM for code generation. The success rate normalization was performed as follows: given N problems, the number of successfully solved problems is denoted as So while the rest N\u2081 = N \u2212 So unsolved problems will be further resolved with a self-debugging attempt. Accordingly, the number"}, {"title": "4.3 Discussion", "content": "PyCapsule's single-attempt accuracy is significantly lower than Qwen-2.5's reported performance, and it varies considerably across datasets, as shown in Figure 2 and detailed in Appendix A. Nevertheless, PyCapsule achieves higher single-attempt accuracy compared to GPT models implemented in MapCoder and AgentCoder. This nuanced performance difference highlights the varying impacts of prompt engineering and framework structure on code generation without self-debugging. With more debugging attempts, PyCapsule shows overall remarkable improvements, surpassing all the other self-debugging methods. This iterative improvement highlights the effectiveness of prompt engineering and error-handling ability of PyCapsule. Prior works [5, 28, 33] have demonstrated how prompt design significantly influences model performance, especially for generative AI tasks. PyCapsule leverages sophisticated system prompts that are dynamically augmented in the code generation and fix modes. Furthermore, an important advantage of PyCapsule is its reduced reliance on LLM-based agents, leading to reduced token consumption and improved computational efficiency. Existing frameworks like MapCoder can make up to 17 API calls with GPT models per problem on HumanEval and 12 calls on MBPP, following k \u00d7 t debugging attempts, where k is the number of retrieved exemplars and t is the number of self-debugging attempts. In contrast, PyCapsule streamlines this process by limiting the programmer agent with a maximum of 5 attempts, requiring at most 6 LLM API calls for each problem because planning and code generation are handled within a single API call. Our empirical analysis shows that PyCapsule achieves superior code generation ability while maintaining efficient API usage, requiring an average of only 2.09, 1.38, and 1.55 API calls for HumanEval using GPT-3.5-Turbo-1106, GPT-4-Preview-1106, and Qwen-2.5-Coder-Instruct-7B, respectively. This optimization demonstrates the possibility of achieving high performance with significantly reduced computational overhead."}, {"title": "5 Conclusion", "content": "We propose PyCapsule for significantly advancing reliable and automated code generation by integrating two AI agents with high-efficiency self-debugging modules. Its two-agent architecture effectively balances code generation automation and control while reducing resource consumption. Experimental results show that PyCapsule achieves high performance across multiple datasets, surpassing existing debugging-based approaches. Notably, PyCapsule enhances accuracy while using smaller language models and fewer API calls, offering a cost-effective alternative without compromising its performance. The iterative self-debugging feedback and dynamic prompt augmentation contribute to its robustness and improvements."}, {"title": "Appendix", "content": "A Contribution of Each Debugging Attempt\nTo further illustrate the influence of iterative debugging, we present a detailed breakdown of the success rate at each debugging attempt. Table 2 quantifies the proportion of problems successfully resolved at each stage. While the independent influence of successive attempts gradually decreases, the cumulative effect leads to an overall increase in the accuracy of problem-solving. Each debugging attempt contributes to solving additional problems, underscoring the value of iterative refinement in improving outcomes. This dual perspective-rising cumulative accuracy alongside diminishing returns per attempt-demonstrates the significance of leveraging efficient debugging strategies, such as enriched conversation history and refined prompts, to maximize the potential of LLM-based code generation systems."}, {"title": "DPseudo-code of PyCapsule", "content": "Algorithm 1 Pseudo-code of PyCapsule\nInput: A user-defined query Q for code generation with a problem description, test cases, and the maximum number of self-debugging attempts N = 5.\nOutput: A generated code satisfying the request in the problem description if all test cases passed, otherwise a failed status message.\nProcedure:\n1: Set and initialize a self-debugging attempt counter i = 0.\n2: Set an initial system prompt for generating code, a buffer B for storing conversation history, and a user prompt initialized by Q.\n3: Use Q and the LLM in PyCapsule to generate the function signature required by case testing.\n4: Step 1: Generate code (\"Generation Mode\").\n5: Generate a prompt by integrating the system prompt and user prompt.\n6: Use the prompt and the same LLM to generate a response for code generation.\n7: Extract the required code, that is the function implementation, from the response.\n8: Create a Python container with the required packages installed. If it exists, skip this step.\n9: Test all the cases in the container using the extract code, and return the code execute status.\n10: if all the test cases passed then\n11: Return the extracted code as the optimal generated code.\n12: else if i > N then\n13: Return the failed message extracted from the code execute status.\n14: else\n15: Step 2: Code Debugging (\"Fix Mode\").\n16: Extract the error message from the code execute status and add it to B.\n17: Generate an error-handling prompt using the error-handling module.\n18: Update the system prompt for code debugging.\n19: Update the user prompt by integrating the error-handling prompt and messages in B.\n20: Update n = n + 1.\n21: Repeat Step 1.\n22: end if"}, {"title": "E System Prompts", "content": "E.1 Code Generation Mode\nYou are an experienced Python developer. Your task is to complete the function based on the provided function signature and description\nAnalyze the description and provide a step by step reasoning on how to solve the problem.\nMaintain the function signature: Do not alter/modify the function signature.\nAvoid example calls: Do not include any example call in your response.\nIf external libraries are needed, add a '### Requirements' Section listing them separated by ','. e.g. pandas, pyhton-dotenv ). If no external libraries are needed, add 'None'.\nImport all required libraries and complete the function in a '### Code' Section, enclosed with triple backticks.\nExample Response Structure-\n### Step-by-step reasoning\n$reasoning\n### Requirements\n$external_libraries\n### Code\n$code\nE.2 Fix Mode\nYou are an experienced Python developer.\nYour previous solution resulted an error.\nError message from a python compiler and Conversation history has been added for your reference.\nAnalyze the description and provide a step by step reasoning on how to solve the problem.\nMaintain the function signature: Do not alter/modify the function signature.\nAvoid example calls: Do not include any example call in your response.\nIf external libraries are needed, add a '### Requirements' Section listing them separated by ','. e.g. pandas, pyhton-dotenv ). If no external libraries are needed, add 'None'.\nImport all required libraries and complete the function in a '### Code' Section, enclosed with triple backticks.\nExample Response Structure-\n### Step-by-step reasoning\n$reasoning"}]}