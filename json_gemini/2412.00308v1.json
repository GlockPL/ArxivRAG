{"title": "BOTS: Batch Bayesian Optimization of Extended Thompson Sampling\nfor Severely Episode-Limited RL Settings", "authors": ["Karine Karine", "Susan A. Murphy", "Benjamin M. Marlin"], "abstract": "In settings where the application of reinforcement learning (RL) requires running\nreal-world trials, including the optimization of adaptive health interventions, the\nnumber of episodes available for learning can be severely limited due to cost or time\nconstraints. In this setting, the bias-variance trade-off of contextual bandit methods\ncan be significantly better than that of more complex full RL methods. However,\nThompson sampling bandits are limited to selecting actions based on distributions\nof immediate rewards. In this paper, we extend the linear Thompson sampling\nbandit to select actions based on a state-action utility function consisting of the\nThompson sampler's estimate of the expected immediate reward combined with an\naction bias term. We use batch Bayesian optimization over episodes to learn the\naction bias terms with the goal of maximizing the expected return of the extended\nThompson sampler. The proposed approach is able to learn optimal policies\nfor a strictly broader class of Markov decision processes (MDPs) than standard\nThompson sampling. Using an adaptive intervention simulation environment that\ncaptures key aspects of behavioral dynamics, we show that the proposed method\ncan significantly out-perform standard Thompson sampling in terms of total return,\nwhile requiring significantly fewer episodes than standard value function and policy\ngradient methods.", "sections": [{"title": "1 Introduction", "content": "There is an increasing interest in using reinforcement learning methods (RL) in the healthcare setting,\nincluding in mobile health Coronato et al. [2020], Yu et al. [2021], Liao et al. [2022]. However, the\nhealthcare domain presents a range of challenges for existing RL methods. In mobile health, each\nRL episode typically corresponds to a human subjects trial involving one or more participants that\nrequires substantial time to carry out (weeks to months) and can incur significant cost. As a result,\nmethods that require many episodes are usually not feasible [Williams, 1987, Mnih et al., 2013].\nWithin the mobile health research community specifically, adaptive intervention policy learning\nmethods have addressed severe episode count restrictions imposed by real-world research constraints\nby focusing on the use of contextual bandit algorithms [Tewari and Murphy, 2017]. By focusing on\nmaximizing immediate reward, bandit algorithms have the potential to provide an improved bias-\nvariance trade-off compared to policy gradient and state-action value function approaches [Lattimore"}, {"title": "2 Methods", "content": "In this section, we describe our extension to Thompson sampling, our approach to optimizing the\nadditional free parameters introduced, and the JITAI simulation environment. We describe the related\nwork in Appendix A.1.\nExtended Thompson Sampling (xTS). Our primary goal is to reduce the myopic nature of standard\nTS for contextual bandits when applied in the episodic MDP setting while maintaining low variance.\nOur proposed extended Thompson sampling approach is based on selecting actions according to a\nstate-action utility that extends the linear Gaussian reward model used by TS as shown below.\n$U_{ta} = r_{ta} + B_a$ (1)\n$p(r_{ta} | a, s_t) = \\mathcal{N}(r_{ta}; \\Theta_{ta}s_t, \\sigma_\\nu a)$ (2)\n$p(\\Theta_{ta} | \\mu_{ta}, \\Sigma_{ta}) = \\mathcal{N}(\\Theta_{ta}; \\mu_{ta}, \\Sigma_{ta})$ (3)\nwhere at each time t, st is the state (or context) vector, rta is the reward for taking action a, \u0398ta is a\nvector of weights for action a. We refer to the additional term Ba included in the utility as the action\nbias for action a. The action bias values are fixed within each episode of xTS, but are optimized\nacross episodes to maximize the total return of the resulting policy \u03c0xTs. The policy \u03c0xTs is similar in"}, {"title": "Bayesian Optimization for xTS.", "content": "We now turn to the question of how to select the action bias values\nBa to optimize the xTS policy. Let $R = \\sum_t \\sum_{t'=1}^{T} r'_t$ represent the total reward for an episode, where rt\nis the observed reward at time t, and $E_{\\pi}[R]$ represent the expected return when following policy \u03c0.\nLetting vectors $\\beta = {B_a}_{0:A}, \\mu_0 = {\\mu_{0a}}_{0:A}, \\Sigma_0 = {\\Sigma_{0a}}_{0:A}$, and $\\sigma_\\nu = {\\sigma_{\\nu a}}_{0:A}$, we propose to\nselect \u03b2 to maximize the expected return of the policy $\\pi_{xTS}(\\beta, \\mu_0, \\Sigma_0, \\sigma_\\nu)$: \n$\\beta^* = \\arg \\max_\\beta E_{\\pi_{xTS}(\\beta, \\mu_0, \\Sigma_0, \\sigma_\\nu)}[R]$ (4)\nTo solve this optimization problem, we propose the use of batch Bayesian optimization. The target\nfunction is the expected return of the xTS policy with respect to the \u03b2 parameters. We begin by\ndefining a schedule of batch sizes Bi for rounds i from 0 to R. On each round i, we use a batch\nacquisition function to select a batch of Bi values of action bias parameters $B_{i1}, ..., B_{iB_i}$ based on the\ncurrent Gaussian process approximation to the expected return function. We run one episode of xTS\nusing the policy $\\pi_{xTS}(\\beta_{ib}, \\mu_0, \\Sigma_0, \\sigma_\\nu)$ for $1 < b < B_i$ and obtain the corresponding returns $R_{ib}$. All\nepisodes in round i are run in parallel. When all episodes in round i are complete, we use the Bi pairs\n($B_{ib}, R_{ib}$) to update the Gaussian process. We provide the pseudo code in Appendix Algorithm 1.\nWe perform experiments using several configurations of this algorithm. To start the BO procedure,\nwe apply Sobol sampling to obtain an initial set of \u03b2 values. On later rounds, we typically apply the\nqEI acquisition function. When applying the acquisition function, we consider both unconstrained\nand local optimization. This choice corresponds to using global or local BO (e.g., TuRBO) [Balandat\net al., 2020, Eriksson et al., 2019].\nWhen using a global BO method to optimize xTS, this approach can represent optimal policies for\nany MDP where standard TS can represent an optimal policy, simply by learning to set Ba = 0 for all\na. Moreover, our approach can also represent optimal policies for MDPs where the optimal action\nin each state corresponds to the action with the highest expected utility under some setting of the\naction bias parameters \u03b2. This is a strictly larger set of MDPs than can be solved using standard TS.\nHowever, it is clearly also strictly smaller than the set of all MDPs since the action bias terms are\nstate independent, while the expected value of the next state in the optimal state-action value function\nQ*(s, a) depends on the current state."}, {"title": "Setting Thompson Sampling Parameters.", "content": "As for typical TS, xTS requires that a prior $N(\\Theta_a; \\mu_{0a}, \\Sigma_{0a})$ be specified. This prior can be set by\nhand using domain knowledge or hypothesized relationships between the immediate reward and the\nstate variables. In the absence of strong domain knowledge, the prior can be set broadly to reflect\nthis lack of prior knowledge. Finally, in the adaptive intervention domain, this prior is often set by\napplying Bayesian linear regression to a small number of episodes with randomized action selection\nreferred to as a micro-randomized trial (MRT). Note that these episodes can also be performed in\nparallel.\nWhen learning xTS over multiple rounds, we can fix the same reward model prior for all rounds,\nor update the prior from round to round using Bayesian linear regression. We call these strategies"}, {"title": "4 Conclusions", "content": "Motivated by the use of RL methods to aid in the design of just-in-time adaptive health interventions,\nthis paper focuses on practical methods that can deal with (1) severe constraints on the number of\nepisodes available for policy learning, (2) constraints on the total duration of policy optimization\nstudies, and (3) long terms consequences of actions. We have proposed a two-level approach that\nuses extended Thompson sampling (xTS) to select actions via an expected utility that includes fixed\naction bias terms, while a batch Bayesian optimization method is used to learn the action bias terms"}, {"title": "A Appendix", "content": "A.1 Related work\nOur overall approach is based on the use of local batch BO to learn the additional parameters of our\nproposes extended TS model. Our overall approach is closely related to the use of BO methods to\ntune hyper-parameters of machine learning algorithms [Snoek et al., 2012]. This includes the use\nof BO for direct policy search in hierarchical RL [Brochu et al., 2010]. Another recent work uses\nensembles of bootstrapped neural networks coupled with randomized prior functions to approximate\nthe TS posterior distribution [Osband et al., 2023]. This work proposes a modification of TS intended\nto enable solving any MDP, whereas we focus on a more modest extension of TS that retains low\nvariance. The computational approach used by [Osband et al., 2023] is also quite different than our\nwork which focuses on GP optimization of the additional terms introduced in the state-action utility\nfunction. Our approach is closely related to recent work on differentiable meta-learning of bandit\npolicies [Boutilier et al., 2020] with the main difference being that we focus on the use of batch BO\nto explore many extended TS models simultaneously. Liao et al. [2022] consider the problem of\nlearning corrections for linear TS, but their approach requires estimating an auxiliary MDP model\nand is less general than the method we propose. Trella et al. [2023] consider a similar model to the\none considered in this paper, but use previously collected data with domain knowledge to hand-design\nan estimator for the correction terms in the context of a specific application."}, {"title": "A.2 BOTS overview", "content": "In this section, we summarize the BOTS method described in Section 2, and provide a graphical\noverview of BOTS in the JITAI setting, and the BOTS pseudo codes.\nA.2.1 BOTS methods and parameters\nWe provide a summary of the BOTS methods and parameter space configurations in Tables 1 and 2.\nA.2.2 BOTS graphical overview in the JITAI setting\nWe provide a graphical overview of the BOTS method as applied in the JITAI setting, including\nsetting TS priors using the MRT, in Figure 2. The BO parameters are summarized in Table 2."}, {"title": "A.3 Implementation details", "content": "In this section, we provide the implementation details for the RL methods, basic MDPs, Bayesian\nOptimization (BO), and a summary of the JITAI environment specification.\nA.3.1 RL methods implementation details\nAs baseline methods, we consider REINFORCE and PPO as examples of policy gradient methods,\nand deep Q networks (DQN) as an example of a value function method. We also consider standard\nThompson sampling (TS) with a fixed prior. We select the best hyper-parameters that maximize\nthe performance, with the lowest number of episodes: the average return is around 3000 for the\nRL methods, and around 1500 for basic TS, using the JITAI setting described in Section 2. All\nexperiments can be run on CPU, using Google Colab within 2GB of RAM.\nREINFORCE. We use a one-layer policy network. We perform hyper-parameter search over hidden\nlayer sizes [32, 64, 128, 256], and Adam optimizer learning rates from 1e-6 to le-2. We report the\nresults for 128 neurons, batch size b = 64, and Adam optimizer learning rate lr = 6e-4.\nDQN. we use a two-layer policy network. We perform a hyper-parameter search over hidden layers\nsizes [32, 64, 128, 256], batch sizes [16, 32, 64], Adam optimizer learning rates from 1e-6 to 1e-2, and\nepsilon greedy exploration rate decrements from 1e-6 to 1e-3. We report the results for 128 neurons\nin each hidden layer, batch size b = 64, Adam optimizer learning rate lr = 5e-4, epsilon linear\nd increment \u03b4\u03b5 = 0.001, decaying e from 1 to 0.01. The target Q network parameters are replaced\nevery K = 1000 steps.\nPPO. We use a two-layer policy network, and a three layers critic network. We perform a hyper-\nparameter search over hidden layers sizes [32, 64, 128, 256], batch sizes [16, 32, 64], Adam optimizer\nlearning rates from 1e-6 to 1e-2, horizons from 10 to 40, policy clips from 0.1 to 0.5, and the other\nfactors from .9 to 1.0. We report the results for 256 neurons in each hidden layer, batch size b = 64,\nAdam optimizer learning rate lr = 0.001, horizon H = 20, policy clip c = 0.1, discounted factor\n\u03b3 = 0.99 and Generalized Advantage Estimator (GAE) factor \u03bb = 0.95.\nQ-Learning. We perform a hyper-parameter search over learning rates from 0.01 to 0.1, discount\nfactor \u03b3 from 0.8 to 1., decaying e from 1 to 0.01, with decay rates from 0.01 to 0.5. We report the\nresults for learning rate lr = 0.8, \u03b3 = 0.99, and decay rate \u03b4\u03b5 = 0.1.\nTS(Fixed). We consider a baseline approach where TS is applied with a fixed prior. When fixing\n(e.g., not learning) elements of the Thompson sampling prior, we set them to the same values for all\nmethods. We set \u03bcoa = 0 for all a, and \u2211oa = 100I. We fix the Thompson sampling reward model\nnoise variance to \u03c3\u03c4\u03b1 = 252 for all a.\nBatch RL agents. We consider batch versions of the RL methods to match our focus on batch\nBayesian optimization. In the case of REINFORCE, we simulate all episodes of a batch in parallel\nand compute an average policy gradient based on the average return of the batch elements. In\nthe case of DQN, we again simulate all episodes in a batch in parallel. However, in this case we\ncompute an average gradient across batch elements at each iteration of the DQN algorithm. Both\nREINFORCE and DQN are applicable in the full Markov decision process (MDP) setting with\nsequential dependence of rewards on prior state and actions."}, {"title": "A.3.2 Basic MDPs implementation details", "content": "We implement three basic MDPs to empirically demonstrate scenarios in which basic TS and our\nproposed approach can and can not achieve optimal performance. These correspond to the MDPs in\nFigure 3. For the notation below: s represents the current state value, a is the action value given s, s'\nis the next state value after taking action a, and r is the reward."}, {"title": "A.3.3 Summary of JITAI simulation environment specifications", "content": "The JITAI simulation environment introduced in [Karine et al., 2023], is a behavioral simulation\nenvironment that mimics a participant's behaviors in a mobile health study, where the interventions\n(actions) are the messages sent to the participant. We summarize the JITAI environment specifications\nin Tables 9 and 10."}, {"title": "A.3.4 BO implementation details", "content": "To apply Bayesian optimization, we need to define bounds on all parameters. For the action bias\nterms, we use -100 \u2264 \u03b2a \u2264 0 for a > 0 as we select actions to have negative or neutral impacts in\nour environment. For the reward variances, we use 0.1 < \u03c3y < 502 and 0.1 \u2264 \u03c3\u03b3\u03b1 \u2264 502. When\napplying Sobol sampling, we sample within these bounds.\nThe GP is fit to the normalized BO parameters and standardized returns. We initialize the GP\nlikelihood noise standard deviation to \u03c3\u03b5 = 0.7. We place a hierarchical prior on the Mat\u00e9rn 5/2\nkernel parameters including a Gamma(3.0, 6.0) prior on the lengthscale \u03c3\u03b9 and a Gamma(2.0, 0.15)\nprior on the output scale of. We use marginal likelihood maximization to refine the kernel hyper-\nparameters during BO. We use the BoTorch implementations of the qEI acquisition function and the\nTURBO method."}, {"title": "B Experiments and Results", "content": "In this section, we describe the experiments and results for basic MDPs and for JITAI environment.\nB.1 Basic MDPs Experiments\nWe show results on basic MDPs to empirically demonstrate scenarios in which basic TS and our\nproposed approach can and can not achieve optimal performance. We use three MDPs as shown in\nFigure 3. All three MDPs are deterministic and have two actions (red and blue). The state transitions\nare deterministic and are shown by arrows. The arrows are annotated with the immediate rewards.\nThe rewards are also deterministic. MDP1 and MDP2 consist of a binary state and a binary action,\nwith starting state s1 = 0. MDP3 consists of a state with values [0, 1, 2, 3], and a binary action, where\nthe starting state is chosen uniformly at random from the set {1, 2}. The episode length is 100. We\nprovide the implementation details for these MDPs in Appendix A.3.2.\nWe consider the application of three methods. First, we apply classical tabular Q-Learning with an\nepsilon greedy exploration policy, which will converge to the optimal policy for these MDPs. Details\nare provided in Appendix A.3.1. We also apply standard TS starting from an uniformed prior and\napplied in a mode where the posterior obtained at the end of one episode becomes the prior for the\nnext episode. Finally, we apply BOTS starting from the same prior as standard TS. We use TURBO\nas the BO approach. On the first BOTS round, we use Sobol sampling with a batch size of 2. For\nthe remaining rounds, we use one episode per batch and apply the EI acquisition function. Figure 4\nshows average return versus the number of episodes results for each method.\nMDP1 corresponds to a case where the action a with the highest immediate reward in each state\ns also corresponds to the action with highest value of Q* (s, a). We can see that all three methods\nconverge to the same optimal return. MDP2 corresponds to a case where the action a with the highest\nimmediate reward in each state s does not correspond to the action with highest value of Q*(s, a),\nbut there do exist settings of the action bias parameters such that the action with the highest value\nof the BOTS utility function corresponds to the action with the highest value of Q* (s, a). For this\nMDP, TS has poor performance while BOTS matches the performance of Q-learning. Lastly, MDP3\nis a case where the BOTS utility function and action bias parameters are not sufficient to learn an\noptimal policy. For this MDP, both TS and BOTS fail to match Q-Learning, as expected. The results\nare shown in Figure 4."}, {"title": "B.2 JITAI Simulation Experiments", "content": "In this section, we describe the experiments using the JITAI simulation environment. We start with\nthe simulator settings, then describe performance metrics, method configurations, and the results.\nJITAI Simulation Environment Configuration. To model a realistic total number of participants in\nmulti-round mobile health adaptive intervention studies, we set 140 as the total number of simulated\nparticipants. Each participant in an intervention trial corresponds to one RL episode. The maximum\nepisode length is L = 50 days. The JITAI simulation environment contains a number of parameters\nfor tuning the simulation dynamics [Karine et al., 2023]. We report results using context uncertainty\n\u03c3 = 0.1, disengagement threshold D = 0.99, habituation decay \u03b4\u03b7 = 0.1, habituation increment\nth = 0.05, disengagement decay da = 0.1, disengagement increment \u20acd = 0.4, base rewards for\nnon-tailored and tailored message p1 50 and p2 = 200 respectively.\nPerformance Metrics. In our specific adaptive health intervention setting, the efficacy of the\ntreatment received by each individual during the course of the trial is an important performance\nmetric. We thus focus on the average return over all participants in the adaptive intervention\noptimization study as the primary performance metric in our experiments. We report results in terms\nof the mean of the average return computed over the 10 simulated repetitions of the study. We also\nreport the standard error computed over the average return of the 10 repetitions of the study. The\nstandard errors are presented graphically as shaded regions.\nMethod Configurations. In terms of baselines, we consider REINFORCE and PPO as examples of\npolicy gradient methods, and deep Q networks (DQN) as an example of a value function method. We\nimplement batch versions of these methods to match our focus on batch BO. The implementation\ndetails are in Appendix A.3.1.\nFor BOTS, we consider an application of the method where we simulate an MRT with 10 individuals\nin parallel to set the reward model priors. We then perform a Sobol sampling round to initialize\nthe GP with 10 individuals in parallel. We then perform a number of BO rounds that differs per\nexperiment. We use the qEI acquisition function with the BO variants: global batch BO, which we\ndenote by BO(qEI), and trust region batch BO, which we denote by TuRBO(qEI). As with standard\nTS, we can either fix the same reward model prior for all rounds, or update the prior from round to\nround. We provide implementation details in Appendix A.3.4, and a graphical overview in Figure 2.\nPerformance of baselines without episode limits. In this experiment we assess the performance\nlimits of baseline methods on the simulation environment in the absence of constraints on the episode\ncount. We perform the experiments using REINFORCE, DQN, PPO, and standard TS with fixed and\nupdated priors. We run 10 repetitions of 1500 episodes and report average results in Figure 5. The\nresults show that REINFORCE, DQN and PPO require a large number of sequential episodes to reach\nhigh performance: about 1500 episodes for REINFORCE (e.g., up to 75, 000 days) and more than\n100 episodes for DQN and PPO (e.g., up to 5,000 days). As expected, the standard TS methods show\nfast convergence, but to lower performance than the full RL methods. These results motivate the\nneed for BOTS in terms of an approach that can both enable parallelization across episodes to reduce\ntotal study time while achieving a performance improvement relative to standard TS. We provide the\nresults for the experiments without episode limits in Figure 5."}]}