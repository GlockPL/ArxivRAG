[{"title": "SCALABLE DISCRETE DIFFUSION SAMPLERS: COMBINATORIAL OPTIMIZATION AND STATISTICAL PHYSICS", "authors": ["Sebastian Sanokowski", "Wilhelm Berghammer", "Martin Ennemoser", "Haoyu Peter Wang", "Sepp Hochreiter", "Sebastian Lehner"], "abstract": "Learning to sample from complex unnormalized distributions over discrete domains emerged as a promising research direction with applications in statistical physics, variational inference, and combinatorial optimization. Recent work has demonstrated the potential of diffusion models in this domain. However, existing methods face limitations in memory scaling and thus the number of attainable diffusion steps since they require backpropagation through the entire generative process. To overcome these limitations we introduce two novel training methods for discrete diffusion samplers, one grounded in the policy gradient theorem and the other one leveraging Self-Normalized Neural Importance Sampling (SN-NIS). These methods yield memory-efficient training and achieve state-of-the-art results in unsupervised combinatorial optimization. Numerous scientific applications additionally require the ability of unbiased sampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte Carlo that enable for the first time the application of discrete diffusion models to this problem. We validate our methods on Ising model benchmarks and find that they outperform popular autoregressive approaches. Our work opens new avenues for applying diffusion models to a wide range of scientific applications in discrete domains that were hitherto restricted to exact likelihood models.", "sections": [{"title": "1 INTRODUCTION", "content": "Sampling from unnormalized distributions is crucial in a wide range of scientific domains, including statistical physics, variational inference, and combinatorial optimization (CO) (Wu et al., 2019; Shih & Ermon, 2020; Hibat-Allah et al., 2021). We refer to research on using neural networks to learn how to sample unnormalized distributions as Neural Probabilistic Optimization (NPO). In NPO, a target distribution is approximated using a probability distribution that is parameterized by a neural network. Hence, the goal is to learn an approximate distribution in a setting, where only unnormalized sample probabilities can be calculated. Importantly, no samples from the target distribution are available, i.e. we are working in the data-free problem setting. In the following, we consider binary state variables $X \\in {0,1}^N$, where $N$ represents the system size. The unnormalized target distribution is typically implicitly defined by an accessible energy function $H : {0,1}^N \\rightarrow \\mathbb{R}$. The target distribution is defined to be the corresponding Boltzmann distribution:\n\n$p_\\beta(X) = \\frac{\\exp(-\\beta H(X))}{Z}$ where $Z = \\sum_X \\exp(-\\beta H(X))$.\n(1)"}, {"title": "2 PRELIMINARY: NEURAL PROBABILISTIC OPTIMIZATION", "content": "The goal of NPO is to approximate a known target probability distribution $p_\\beta(X)$ using a probabilistic model parameterized by a neural network. This technique leverages the flexibility and expressive power of neural networks to model complex distributions. The objective is to train the neural network to represent a probability distribution $q_\\theta(X)$ that approximates the target distribution without requiring explicit data from the target distribution. This approximation can generally be achieved by minimizing a divergence between the two distributions. One class of divergences used for this purpose are alpha divergences (Minka et al., 2005; Amari, 2012):\n\n$D_\\alpha(p_\\beta(X)||q_\\theta(X)) = \\frac{\\int p_\\beta(X)^\\alpha q_\\theta(X)^{1-\\alpha} dX}{\\alpha (1 - \\alpha)}$\n\nBy selecting a specific value of $\\alpha$, this divergence can be used as a loss function for training the model, and the choice of $\\alpha$ influences the bias of the learned distribution. For instance, for $\\alpha < 0$ the resulting distribution is mode seeking, which means the model focuses on the most probable modes of the target distribution, potentially ignoring less probable regions. Whereas, for $\\alpha \\geq 1$ it is mass-covering, meaning the model spreads its probability mass to cover more of the state space, including less probable regions. As $\\alpha \\rightarrow 1$ the divergence equals the forward Kullback-Leibler divergence (fKL) $D_{KL}(p_\\beta(X) || q_\\theta(X))$ and as $\\alpha \\rightarrow 0$ it equals the reverse Kullback-Leibler divergence (rKL) $D_{KL}(q_\\theta(X) || p_\\beta(X))$ (Minka et al., 2005). The two divergences, rKL and fKL are particularly convenient in this context due to the product rule of logarithms that we utilize in this paper to realize diffusion models with more diffusion steps (see Sec. 3.1)."}, {"title": "2.1 DISCRETE DIFFUSION MODELS FOR NEURAL PROBABILISTIC OPTIMIZATION", "content": "In discrete time diffusion models, a forward diffusion process transforms the target distribution $p_\\beta(X_0)$ into a stationary distribution $q(X_T)$ through iterative sampling of a noise distribution $p(X_t|X_{t-1})$ where $t \\in {1,T}$ for a total of $T$ iterations. The diffusion model is supposed to model the reverse process, i.e. to map samples $X_T \\sim q(X_T)$ to $X_0 \\sim p_\\beta(X_0)$ by iteratively sampling $q_\\theta(X_{t-1}|X_t)$. The probability of a diffusion path $X_{0:T} = (X_0, ..., X_T)$ of the reverse process can be calculated with $q_\\theta(X_{0:T}) = q(X_T) \\prod_{t=1}^T q_\\theta(X_{t-1}|X_t)$ and $q_\\theta(X_{t-1}|X_t)$ is chosen so that samples $X_{0:T} \\sim q_\\theta(X_{0:T})$ can be efficiently drawn. Usually, in the reverse process, the diffusion model is explicitly conditioned on the diffusion step $t$, such that the distribution of the reverse diffusion step can be written as $q_\\theta(X_{t-1}|X_t,t)$. However, in the following, we will drop the dependence on $t$ to simplify the notation. The unnormalized probability of a diffusion path of the forward process can be calculated with $P(X_{0:T}) = p_\\beta(X_0) \\prod_{t=1}^T p(X_t|X_{t-1})$. In the data-free setting samples $X_{0:T} \\sim p(X_{0:T})$ are not available. Sanokowski et al. (2024) invoke the Data Processing Inequality to introduce diffusion models in discrete NPO by proposing to use the rKL of joint probabilities $D_{KL}(q_\\theta(X_{0:T}) || P(X_{0:T}))$ as a tractable upper bound of the rKL of the marginals $D_{KL}(q_\\theta(X_{0}) || p_\\beta(X_{0}))$. They further simplify this objective to express it in the following form:\n\n$TD_{KL} (q_\\theta(X_{0:T}) || P(X_{0:T})) = -T\\cdot \\sum_{X_{0:T} \\sim q_\\theta(X_{t:T})} [S(q_\\theta(X_{t-1}|X_t))]\n-T \\sum_{X_{t-1:T} \\sim q_\\theta(X_{t-1:T})} [logp(X_t|X_{t-1})]\n+ E_{X_{0:T} \\sim q_\\theta(X_{0:T})} [H(X_0)] + C,$\n(2)"}, {"title": "2.2 UNSUPERVISED COMBINATORIAL OPTIMIZATION", "content": "Sanokowski et al. (2024) apply diffusion models in UCO by reformulating it as an NPO problem. There is a wide class of CO problems that can be described in QUBO formulation (Lucas, 2014; Glover et al., 2022). In this case, the CO problem is described by an energy function $H_Q : {0,1}^N \\rightarrow \\mathbb{R}$ which is given by:\n\n$H_Q(X) = \\sum_{i,j} Q_{ij} X_i X_j,$\n(3)\n\nwhere $Q \\in \\mathbb{R}^{N \\times N}$ is chosen according to the CO problem at hand. A table of the QUBO formulations of the CO problem types studied in this paper is given in Tab. 5. In UCO the goal is to train a conditional generative model $q_\\theta(X|Q)$ on problem instances $Q$ that are drawn from a distribution $D(Q)$ (see Sec. 5.1 and App. A.6) for more information on $D(Q))$. After training the model can be used on unseen i.i.d CO problems to obtain solutions of high quality within a short amount of time. This can be realized by using the expectation of $H_Q(X)$ with respect to a parameterized probability distribution which is used as a loss function and minimized with respect to network parameters $\\theta$:\n\n$\\mathcal{L}(\\theta) = E_{Q \\sim D(Q),X \\sim q_\\theta(X|Q)}[H_Q(X)].$\n(4)\n\nFor notational convenience the conditional dependence of $q_\\theta$ on the problem instance $Q$ is sup- pressed in the following. As minimizing the expectation value of $H_Q(X)$ in Eq. 4 is prone to getting stuck in local minima, numerous works (Hibat-Allah et al., 2021; Sun et al., 2022; Sanokowski et al., 2023; 2024) reframe this problem as an NPO problem and minimize $TD_{KL}(q_\\theta(X)||p_\\beta(X)) = E_{X \\sim q_\\theta(X)} [H_Q(X) + Tlog q_\\theta(X)] + C$ instead, where $C$ is a constant which is independent of $\\theta$. The optimization procedure of this objective is combined with annealing, where the objective is first optimized at high temperature, which is then gradually reduced to zero. At $T = 0$ this objective re- duces to the unconditional loss in Eq. 4. Sanokowski et al. (2023) motivate this so-called variational annealing procedure theoretically from a curriculum learning perspective and the aforementioned works show experimentally that it yields better solution qualities."}, {"title": "2.3 UNBIASED SAMPLING", "content": "When a parameterized probability distribution $q_\\theta(X)$ is used to approximate the target distribu- tion $p_\\beta(X)$ the learned distribution will typically be an imperfect approximation. Consequently, samples from $q_\\theta(X)$ will exhibit a bias. When the model is used to infer properties of the system that is described by the target distribution, it is essential to correct for this bias. The following paragraphs revisit two established unbiased sampling methods namely Self-Normalized Neural Im- portance Sampling (SN-NIS) and Neural Markov Chain Monte Carlo (NMCMC) that can be used to achieve this goal. These methods serve as the basis for our diffusion-based unbiased sampling methods which are introduced in Sec. 3.2.\nSelf-Normalized Neural Importance Sampling: SN-NIS allows asymptotically unbiased com- putation of expectation values of a target distribution. Given an observable $O : {0,1}^N \\rightarrow \\mathbb{R}$, an exact likelihood model $q_\\theta(X)$ can be used to calculate expectation values $(O(X))_{p_\\beta(X)} := E_{p_\\beta(X)} [O(X)]$ in the following way:\n\n$(O(X_0))_{p(X_0)} = \\sum_{X^i \\sim q_\\theta(X)} w(X^i) O(X^i),$\n\nwhere $X^i$ corresponds to the $i$-th of $M$ samples from $q_\\theta(X)$. The importance weights are com- puted with $w(X^i) = \\frac{\\tilde{w}(X^i)}{\\sum_{i=1}^M \\tilde{w}(X^i)}$ , where $\\tilde{w}(X) = \\frac{p_\\beta(X)}{q_\\theta(X)}$ (for a derivation we refer to App. A.2).\nThe probability distribution that is proportional to $p_\\beta(X)|O(X)|$ yields the minimum-variance es- timate of $(O(X))_{p_\\beta(X)}$ (Rubinstein & Kroese, 2016). However, in our experiments, we focus on a"}, {"title": "3 METHODS", "content": "Sanokowski et al. (2024) demonstrate that increasing the number of diffusion steps in UCO improves the solution quality of the diffusion model, as it enables the model to represent more complex dis- tributions. However, as discussed in Sec. 2.1, the loss function in Eq. 2 used in their work inflicts memory requirements that scale linearly with the number of diffusion steps. Given a fixed memory budget, this limitation severely restricts the expressivity of the diffusion model. In the following sections, we introduce training methods that mitigate this shortcoming.\nForward KL Objective: One possibility to mitigate the linear scaling issue is to use the forward Kullback-Leibler divergence (fKL). In contrast to the objective in Eq. 2 the gradient can be pulled into the expectation:\n$\\nabla_\\theta D_{KL}(P(X_{0:T})||q_\\theta(X_{0:T})) = -E_{X_{0:T} \\sim p(X_{0:T})} [\\nabla_\\theta log q_\\theta (X_{0:T})].$\nHowever, since in NPO samples $X_{0:T} \\sim p(X_{0:T})$ are not available, we employ SN-NIS to rewrite the expectation with respect to $X_{0:T} \\sim q_\\theta(X_{0:T})$. Note that this is feasible with dif- fusion models since they do provide exact joint likelihoods. In analogy to data-based diffusion models (Ho et al., 2020) one can now use Monte Carlo estimates of the sum over time steps $\\log q_\\theta (X_{0:T}) = \\sum_{t=1}^T \\log q_\\theta(X_{t-1}|X_t)$ to mitigate the aforementioned memory scaling issue. The resulting gradient of the fKL objective is given by (see App. A.2.6):\n$\\nabla_\\theta D_{KL}(P(X_{0:T})||q_\\theta(X_{0:T})) = -T\\sum E_{t \\sim U{1,...,T}} [w(X_{0:T}) \\nabla_\\theta log q_\\theta (X_{t-1}|X_t)],$\n\nwhere $w(X_{0:T}) = \\frac{\\tilde{w}(X_{0:T})}{\\sum_{i=1}^M \\tilde{w}(X_{0:T})}$ are importance weights with $\\tilde{w}(X_T) = \\frac{p(X_{0:T})}{q_\\theta(X_{0:T})}$, and $U{1, ..., T}$ is the uniform distribution over the set {1, ..., T}.\n\nIn the following, we will refer to this method as SDDS: fKL w/ MC since it realizes Scalable Discrete Diffusion Samplers (SDDS) using an objective that is based on the fKL, where the linear memory scaling issue is addressed with Monte Carlo estimation over diffusion steps. A pseudocode of the optimization procedure is given in App. A.3.5.\nReverse KL Objective: The minimization of the reverse Kullback-Leibler divergence (rKL) based objective function $\\mathcal{L}(\\theta)$ introduced by Eq. 2 can be shown to be equivalent to parameter updates using the policy gradient theorem (Sutton & Barto, 2018) (see App. A.2.5). The resulting gradient updates are expressed as:\n\n$\\nabla_\\theta \\mathcal{L}(\\theta) = -E_{X_t \\sim d_\\theta (X,t),X_{t-1} \\sim q_\\theta(X_{t-1}|X_t)} [Q^\\theta(X_{t-1}, X_t) \\nabla_\\theta log q_\\theta (X_{t-1}|X_t)],$\n(5)\n\nwhere:"}, {"title": "4 RELATED WORK", "content": "Neural Optimization: Besides their predominance in supervised and unsupervised learning tasks, neural networks become an increasingly popular choice for a wide range of data-free optimization tasks, i.e. scenarios where an objective function can be explicitly expressed rather than implicitly via data samples. In Physics Informed Neural Networks (Raissi et al., 2019) models are trained to represent the solutions of differential equations. Here the loss function measures the adherence of the solution quality. Similarly, Berzins et al. (2024) propose a neural optimization approach for generating shapes under geometric constraints. Recently, there has been increasing interest in using probabilistic generative models to generate solutions to neural optimization. Here the learned models do not directly represent a solution but rather a probability distribution over the solution space. We refer to this endeavor as Neural Probabilistic Optimization (NPO). In the following, we discuss two important NPO application areas in discrete domains."}, {"title": "5 EXPERIMENTS", "content": "We evaluate our methods on UCO benchmarks in Sec. 5.1 and on two benchmarks for unbiased sampling Sec. 5.2 and in App. A.8.2. In all of our experiments, we use a time-conditioned diffusion model $q_\\theta(X_{t-1}|X_t,t)$ that is realized either by a Graph Neural Network (GNN) (Scarselli et al., 2009) in UCO experiments or by a U-Net architecture (Ronneberger et al., 2015) in experiments on the Ising model (see App. A.4). In our experiments the probability distribution correspond- ing to individual reverse diffusion steps is parametrized via a product of Bernoulli dsitributions $q_\\theta(X_{t-1}|X_t,t) = \\prod_{i=1}^N q_\\theta(X_{t})_i^{X_{t-1,i}} (1-q_\\theta(X_{t})_i)^{1-X_{t-1,i}}$, where $q_\\theta(X_t)_i := q_\\theta(X_{t-1,i} = 1 | X_t, t)$. As a noise distribution, we use the Bernoulli noise distribution from (Sohl-Dickstein et al., 2015) (see App. A.3.1)."}, {"title": "5.1 UNSUPERVISED COMBINATORIAL OPTIMIZATION", "content": "In UCO the goal is to train a model to represent a distribution over solutions, which is conditioned on individual CO problem instances (see Sec. 2.2). Since each CO problem instance corresponds to a"}, {"title": "5.2 UNBIASED SAMPLING OF ISING MODELS", "content": "In the discrete domain, the Ising model is frequently studied in the context of unbiased sampling (Nicoli et al., 2020; McNaughton et al., 2020). The Ising model is a discrete system, where the energy function $H_I : {-1,1}^N \\rightarrow \\mathbb{R}$ is given by $H_I(\\sigma) = -J \\sum_{(i,j)} \\sigma_i \\sigma_j$, where $(i, j)$ runs over all neighboring pairs on a lattice. At temperature T the state of the system in thermal equilibrium is described by the Boltzmann distribution from Eq. 1. Analogously to Nicoli et al. (2020), we explore unbiased sampling using finite-size Ising models (see Sec. 2.3) on a periodic, regular 2D grid of size L with a nearest-neighbor coupling parameter of $J = 1$. We experimentally validate our unbiased sampling approach with diffusion models by comparing the estimated values of the free energy $F = -\\frac{1}{\\beta} \\log Z$, internal energy $U = \\sum_X p_\\beta(X) H_I(X)$, and entropy $S = \\beta (U - F)$ against the theoretical values derived by Ferdinand & Fisher (1969). We also report the effective sample size per sample $\\epsilon_{eff}/M := \\frac{(\\sum_i w_i)^2}{\\sum_i w_i^2}$ of each method. For the best possible model, the effective sample size per sample equals one as $w_i = 1/M \\forall i \\in {1, ..., M}$. For the worst possible model $\\epsilon_{eff}/M = 1/M$ as there is one weight which is equal to 1 and all others are 0.\nIn our experiments, we train diffusion models using 300 diffusion steps and a U-net architecture (details in App. A.4 and App. A.7.3). Tab. 4 presents our results where each model is evaluated over three independent sampling seeds. We compare our methods to two other methods that both rely on the rKL objective. First, the AR models by (Wu et al., 2019) which we label as VAN (r), and second the NIS method with AR models by (Nicoli et al., 2020) which we label as AR (r). We also evaluate an AR reimplementation of (Nicoli et al., 2020) using the same architecture and computational constraints as the diffusion models."}, {"title": "6 LIMITATIONS AND CONCLUSION", "content": "This work introduces Scalable Discrete Diffusion Samplers (SDDS) based on novel training methods that enable the implementation of discrete diffusion models with an increased number of diffusion steps in Unsupervised Combinatorial Optimization (UCO) and unbiased sampling problems. We demonstrate that the reverse KL objective of discrete diffusion samplers can be optimized efficiently using Reinforcement Learning (RL) methods. Additionally, we introduce an alternative training method based on Self-Normalized Importance Sampling of the gradients of the forward KL diver- gence. Both methods facilitate mini-batching across diffusion steps, allowing for more diffusion steps with a given memory budget. Our methods achieve state-of-the-art on popular challenging UCO benchmarks. For unbiased sampling in discrete domains, we extend existing importance sam- pling and Markov Chain Monte Carlo methods to be applicable to diffusion models. Furthermore, we show that discrete diffusion models can outperform popular autoregressive approaches in es- timating observables of discrete distributions. Future research directions include leveraging recent"}, {"title": "A APPENDIX", "content": "Importance Sampling (IS) is a well-established Monte Carlo method used to estimate expectations of observables O(X) under a target distribution p(X) when direct sampling from p is challenging. The core idea is to use a proposal distribution q(X) which is easy to sample from and proposes samples where p(X) or ideally p(X) |O(X)| is large. IS can be used to calculate the expectation of an observable O(X) in the following way:\n$O(X) = \\sum_X p(X) O(X) = \\sum_X q(X) \\frac{p(X)}{q(X)} O(X) = E_{X \\sim q(X)}[\\frac{p(X)}{q(X)}O(X)].$\n(6)\nHowever, this approach makes it necessary to design a suitable proposal distribution q(X), which is not possible in many cases. Therefore, Neural Importance Sampling can be used instead, where a distribution $q_\\theta(X)$ is parameterized using a Neural Network and trained to approximate the target distribution. By replacing q(X) in Eq. 6 with $q_\\theta(X)$ the Neural Importance Sampling estimator is then given by:\n$O(X) = E_{X \\sim q_\\theta(X)}[\\frac{p(X)}{q_\\theta(X)} O(X)]$"}, {"title": "A.2 SELF-NORMALIZED NEURAL IMPORTANCE SAMPLING", "content": "In some cases, when an unnormalized target distribution is given, i.e. it is infeasible to calculate the normalization constant Z, IS or NIS cannot straightforwardly be applied, as this requires the computation of p(X) and therefore Z. To mitigate this issue, Self-Normalized Importance Sampling (SNIS) can be employed Rubinstein & Kroese (2016). The estimator is given by:\n\n$E_{p(X)} [O(X)] = \\sum_X p(X) O(X) \\approx \\sum_i w(X_i) O(X_i),$\nwhere $w(X_i) = \\frac{\\tilde{w}(X_i)}{\\sum_{j=1}^N \\tilde{w}(X_j)}$ with $\\tilde{w}(X_i) = \\frac{p(X_i)}{q(X_i)}$ are the importance weights, and $X_i \\sim q(X)$.\n\nDerivation: When p(X) is unnormalized, i.e., $\\tilde{p}(X) = \\frac{p(X)}{Z}$, where $\\tilde{p}(X)$ is the unnormalized distribution and Z is the unknown normalization constant, the weights $\\tilde{w}(X_i) = \\frac{\\tilde{p}(X_i)}{q(X_i)}$ depend on Z, which cannot be computed. To circumvent this, Self-Normalized Importance Sampling redefines the weights as normalized importance weights:\n$\\frac{p(X_i)}{q(X_i)} = \\frac{\\tilde{p}(X_i)}{q(X_i)} \\frac{1}{Z} = \\frac{\\tilde{w}(X)}{\\sum_{i=1}^N \\tilde{w}(X)}.\n$\n\nUsing these normalized weights, the expectation can be estimated as:\n\n$E_{p(X)} [O(X)] = \\sum_X q(X) \\frac{\\tilde{p}(X)}{q(X)} O(X) = \\sum_X q(X) \\frac{\\tilde{w}(X)}{\\sum_i \\tilde{w}(X)} O(X) = \\frac{\\sum_{X \\sim q(X)}[\\tilde{w}(X) O(X)]}{\\sum_{X \\sim q(X)}[\\tilde{w}(X)]} = \\frac{\\sum_{i=1}^N \\tilde{w}(X_i) O(X_i)}{\\sum_{i=1}^N \\tilde{w}(X_i)} = \\sum_i w(X_i) O(X_i),$\nThis approach avoids the need to compute Z explicitly, as the normalization is handled by the sum of the unnormalized weights $\\tilde{w}(X_i)$. In practice, this is particularly useful for unnormalized target distributions or when Z is computationally expensive to estimate."}, {"title": "A.2.1 NEURAL IMPORTANCE SAMPLING WITH DIFFUSION MODELS", "content": "In the following, it will be shown that the expectation of an observable $O : {0,1}^N \\rightarrow \\mathbb{R}$ can be computed with:\n\n$(O(X_0))_{p(X_0)} \\approx \\sum_{i} [w(X_{0:T,i}) O(X_{0,i})].$\n(7)\n\nwhere $w(X_{i,0:T}) = \\frac{\\tilde{w}(X_{i,0:T})}{\\sum_i \\tilde{w}(X_{i,0:T})}$ and $X_{i,0:T} \\sim q_\\theta (X_{0:T})$ with $\\tilde{w}(X_{i,0:T}) = \\frac{p(X_{i,0:T})}{q_\\theta (X_{i,0:T})}$.\nTo show that we start with\n\n$(O(X_0))_{p(X_0)} := \\sum_{X_0} p_\\beta(X_0) O (X_0) = \\sum_{X_{0:T}} p(X_{0:T})O(X_0)$\n= (8)\nwhere we introduce new variables $X_{1:T}$ which are distributed according to the distri- bution $p(X_{1:T}|X_0)$. We then used that $p(X_{0:T}) = p(X_{1:T}|X_0) p_\\beta(X_0)$ and that $\\sum_{X_{1:T}} p(X_{1:T}|X_0) p_\\beta(X_0) = p_\\beta(X_0)$. Finally we estimate the right hand side of Eq. 8 with Neural Importance Sampling by inserting $1 = \\frac{q_\\theta (X_{0:T})}{q_\\theta (X_{0:T})}$ to arrive at\n(O(X0))p(X0) = EX0:T\u223cq\u03b8(X0:T) [q(X0:T)p(X0:T)]\nAs $p_B(X_0)$ is only known up to its normalization constant $Z$ we employ SN-NIS (see Sec. 2.3)."}, {"title": "A.2.2 MCMC", "content": "The Metropolis-Hastings algorithm (Metropolis et al., 1953) is a standard method to obtain unbiased samples from a target distribution p(X). Starting from an initial state X, a proposal state X' is accepted with the acceptance probability of\n\n$A(X', X) = min[1, \\frac{w(X'|X) p(X')}{w(X|X') p(X)}],$\n\nwhere w(X|X') is the transition probability from X' to X and is often chosen so that it is symmetric and satisfies w(X|X') = w(X'|X). Here, A(X', X) is chosen in a way so that the detailed balance condition A(X, X') w(X|X') p(X) = A(X', X)w(X'|X) p(X') is satisfied."}, {"title": "A.2.3 NEURAL MCMC", "content": "This acceptance probability can be adapted to Neural MCMC by replacing w(X|X') with a proba- bility distribution that is parameterized by a neural network $q_\\theta (X)$, which approximates the target distribution (Nicoli et al., 2020). The acceptance probability is then given by:\n\n$A(X', X) = min[1, \\frac{q_\\theta (X') p(X)}{q_\\theta (X) p(X')}].$"}, {"title": "A.2.4 NEURAL MCMC WITH DIFFUSION MODELS", "content": "We adapt NMCMC to diffusion models to obtain trajectories that are approximately distributed according to the target distribution $p(X_{0:T})$ and show that these samples can be used to compute $(O(X_0))_{X_0 \\sim p_B(X_0)}$.\nThis process is usually repeated until a convergence criterion is met. The resulting final state is approximately distributed according to the target distribution p. In NMCMC $w(X|X')$ is set to the approximating distribution $q_\\theta (X)$, so that the acceptance probability is given by\n\n$A(X_{0:T}', X_{0:T}) = min [1, \\frac{q_\\theta (X_{0:T}') p(X_{0:T})}{q_\\theta (X_{0:T}) p(X_{0:T}')}],$\n\nwhere Y is substituted with $X_{0:T}$ and Y' with $X_{0:T}'$. Thus it becomes apparent that these updates can be used to obtain unbiased diffusion paths $X_{0:T} \\sim p(X_{0:T})$ of which $X_0 \\sim p_B(X_0)$.\nThe resulting diffusion paths $X_{0:T}$ are then distributed as $p(X_{0:T})$ and samples $X_0$ can then be used to calculate expectations $(O(X_0))_{X_0 \\sim p_B(X_0)}$.\nProof. The statement follows from\n$ (O(X_0))_{p(X_{0:T})} = \\sum_{X_{0:T}} p(X_{0:T}) O(X_0)$\n$ \\sum_{X_0} p_B(X_0) O(X_0) = (O(X_0))_{p_B(X_0)},$\n where we have used that O(X0) does not depend on $X_{1:T}$ which is why $\\sum_{X_{1:T}} p(X_{1:T}) = 1.$"}, {"title": "A.2.5 POLICY GRADIENT THEOREM FOR DATA PROCESSING INEQUALITY", "content": "To prove that the Data Processing Inequality (see Sec. 3.1) can be optimized with the usage of RL we first define\n\n$V^\\theta (X) = \\sum_{X_{t-1"}]}, {"as": "n$R(X_t", "X_{t-1})": "n$T[log p(X_t|X_{t-1"}, {"E_{X_{t\u22121": "T} \\sim"}]