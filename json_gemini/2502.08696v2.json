{"title": "SCALABLE DISCRETE DIFFUSION SAMPLERS: COMBINATORIAL OPTIMIZATION AND STATISTICAL PHYSICS", "authors": ["Sebastian Sanokowski", "Wilhelm Berghammer", "Martin Ennemoser", "Haoyu Peter Wang", "Sepp Hochreiter", "Sebastian Lehner"], "abstract": "Learning to sample from complex unnormalized distributions over discrete domains emerged as a promising research direction with applications in statistical physics, variational inference, and combinatorial optimization. Recent work has demonstrated the potential of diffusion models in this domain. However, existing methods face limitations in memory scaling and thus the number of attainable diffusion steps since they require backpropagation through the entire generative process. To overcome these limitations we introduce two novel training methods for discrete diffusion samplers, one grounded in the policy gradient theorem and the other one leveraging Self-Normalized Neural Importance Sampling (SN-NIS). These methods yield memory-efficient training and achieve state-of-the-art results in unsupervised combinatorial optimization. Numerous scientific applications additionally require the ability of unbiased sampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte Carlo that enable for the first time the application of discrete diffusion models to this problem. We validate our methods on Ising model benchmarks and find that they outperform popular autoregressive approaches. Our work opens new avenues for applying diffusion models to a wide range of scientific applications in discrete domains that were hitherto restricted to exact likelihood models.", "sections": [{"title": "INTRODUCTION", "content": "Sampling from unnormalized distributions is crucial in a wide range of scientific domains, including statistical physics, variational inference, and combinatorial optimization (CO) (Wu et al., 2019; Shih & Ermon, 2020; Hibat-Allah et al., 2021). We refer to research on using neural networks to learn how to sample unnormalized distributions as Neural Probabilistic Optimization (NPO). In NPO, a target distribution is approximated using a probability distribution that is parameterized by a neural network. Hence, the goal is to learn an approximate distribution in a setting, where only unnormalized sample probabilities can be calculated. Importantly, no samples from the target distribution are available, i.e. we are working in the data-free problem setting. In the following, we consider binary state variables $X \\in {0,1}^N$, where N represents the system size. The unnormalized target distribution is typically implicitly defined by an accessible energy function $H : {0,1}^N \\rightarrow \\mathbb{R}$. The target distribution is defined to be the corresponding Boltzmann distribution:\n$p_\\beta(X) = \\frac{exp(-\\beta H(X))}{Z}$ where $Z = \\sum_X exp(-\\beta H(X))$.\nHere $\\beta := 1/T$ is the inverse temperature, and Z is the partition sum that normalizes the distribution. An analogous formulation applies to continuous problem domains. Unbiased sampling from this distribution is typically computationally expensive due to the exponential number (2N) of states. Sampling techniques, such as Markov Chain Monte Carlo (Metropolis et al., 1953) are employed with great success in applications in statistical physics. Nevertheless, their applicability is typically limited due to issues related to Markov chains getting stuck in local minima and large autocorrelation times (Nicoli et al., 2020; McNaughton et al., 2020). Recently, the application of deep generative models has gained increasing attention as an approach to this problem. Initial methods in NPO relied on exact likelihood models where $q_\\theta(X)$ could be efficiently evaluated. Boltzmann Generators (No\u00e9 & Wu, 2018) are a notable example in the continuous setting, using normalizing flows to approximate Boltzmann distributions for molecular configurations. In the discrete setting, Wu et al. (2019); Hibat-Allah et al. (2021) use autoregressive models to approximate Boltzmann distributions of spin systems in the context of statistical and condensed matter physics. Inspired by the success of diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) in image generation, there is growing interest in so-called diffusion samplers where these models are applied to NPO problems in discrete (Sanokowski et al., 2024) and continuous settings (Zhang & Chen, 2022). Diffusion models are particularly intriguing in the discrete setting due to the lack of viable alternatives. Normalizing flows, which are a popular choice for continuous problems, cannot be directly applied in discrete settings, leaving autoregressive models as the most popular alternative. However, autoregressive approaches face significant limitations. They become computationally prohibitive as the system size grows. There are complexity theoretical results (Lin et al., 2021) and empirical results (Sanokowski et al., 2024) that suggest that they are less efficient distribution learners than latent variable models like diffusion models. Consequently, we consider diffusion models as a more promising approach to discrete NPO. However, existing diffusion-based methods for sampling on discrete domains face two major challenges:\n1. Memory Scaling: They rely on a loss that is based on the reverse Kullback-Leibler (KL) divergence which necessitates that the entire diffusion trajectory is kept in memory for backpropagation (see Sec. 2.1). This linear memory scaling limits the number of applicable diffusion steps and hence the achievable model performance. This is in sharp contrast to diffusion models in e.g. image generation, which benefit from the capability of using a large number of diffusion steps.\n2. Unbiased Sampling: For many scientific applications, a learned distribution is only valuable if it allows for unbiased sampling, i.e., the unbiased computation of expectation values. Autoregressive models allow this through importance sampling or Markov Chain Monte Carlo methods based on their exact sample likelihoods. However, unbiased sampling with approximate likelihood models on discrete domains remains so far unexplored.\nWe introduce our method Scalable Discrete Diffusion Sampler (SDDS) by proposing two novel training methods in Sec. 3.1 to address the memory scaling issue and the resulting limitation on the number of diffusion steps in NPO applications of discrete diffusion models:\n1. reverse KL objective: Employs the policy gradient theorem for minimization of the reverse KL divergence and integrates Reinforcement Learning (RL) techniques to mitigate the aforementioned linear memory scaling.\n2. forward KL objective: Adapts Self-Normalized Neural Importance Sampling to obtain asymptotically unbiased gradient estimates for the forward KL divergence. This approach mitigates the linear memory scaling by using Monte Carlo estimates of the objective across diffusion steps.\nIn Sec. 5.1, we compare our proposed objectives to previous approaches and demonstrate that the reverse KL-based objective achieves new state-of-the-art results on 6 out of 7 unsupervised Combinatorial Optimization (UCO) benchmarks and is on par on one benchmark.\nSecondly, to eliminate bias in the learned distribution, we extend two established methods - Self-Normalized Neural Importance Sampling (SN-NIS) and Neural Markov Chain Monte Carlo (NM-CMC) - to be applicable to approximate likelihood models such as diffusion models. We introduce these methods in Sec.3.2 and validate their effectiveness using the Ising model in Sec.5.2, highlighting the advantages of diffusion models over autoregressive models. Our experiments show that the forward KL divergence-based objective excels in unbiased sampling. We hypothesize that this is due to its mass-covering property. Our experiments show that the mass-covering property is also"}, {"title": "PRELIMINARY: NEURAL PROBABILISTIC OPTIMIZATION", "content": "The goal of NPO is to approximate a known target probability distribution $p_\\beta (X)$ using a probabilistic model parameterized by a neural network. This technique leverages the flexibility and expressive power of neural networks to model complex distributions. The objective is to train the neural network to represent a probability distribution $q_\\theta(X)$ that approximates the target distribution without requiring explicit data from the target distribution. This approximation can generally be achieved by minimizing a divergence between the two distributions. One class of divergences used for this purpose are alpha divergences (Minka et al., 2005; Amari, 2012):\n$D_\\alpha(p_B(X)||q_\\theta(X)) = \\frac{\\int p_B(X)^\\alpha q_\\theta(X)^{1-\\alpha} dX}{\\alpha (1 - \\alpha)}$\nBy selecting a specific value of $\\alpha$, this divergence can be used as a loss function for training the model, and the choice of $\\alpha$ influences the bias of the learned distribution. For instance, for $\\alpha < 0$ the resulting distribution is mode seeking, which means the model focuses on the most probable modes of the target distribution, potentially ignoring less probable regions. Whereas, for $\\alpha \\geq 1$ it is mass-covering, meaning the model spreads its probability mass to cover more of the state space, including less probable regions. As $\\alpha \\rightarrow 1$ the divergence equals the forward Kullback-Leibler divergence (fKL) $D_{KL}(p_B(X) || q_\\theta(X))$ and as $\\alpha \\rightarrow 0$ it equals the reverse Kullback-Leibler divergence (rKL) $D_{KL}(q_\\theta(X) || p_B(X))$ (Minka et al., 2005). The two divergences, rKL and fKL are particularly convenient in this context due to the product rule of logarithms that we utilize in this paper to realize diffusion models with more diffusion steps (see Sec. 3.1)."}, {"title": "DISCRETE DIFFUSION MODELS FOR NEURAL PROBABILISTIC OPTIMIZATION", "content": "In discrete time diffusion models, a forward diffusion process transforms the target distribution $p_B(X_0)$ into a stationary distribution $q(X_T)$ through iterative sampling of a noise distribution $p(X_t|X_{t-1})$ where $t \\in {1,T}$ for a total of T iterations. The diffusion model is supposed to model the reverse process, i.e. to map samples $X_T \\sim q(X_T)$ to $X_0 \\sim p_B(X_0)$ by iteratively sampling $q_\\theta(X_{t-1}|X_t)$. The probability of a diffusion path $X_{0:T} = (X_0, ..., X_T)$ of the reverse process can be calculated with $q_\\theta(X_{0:T}) = q(X_T) \\prod_{t=1}^T q_\\theta(X_{t-1}|X_t)$ and $q_\\theta(X_{t-1}|X_t)$ is chosen so that samples $X_{0:T} \\sim q_\\theta(X_{0:T})$ can be efficiently drawn. Usually, in the reverse process, the diffusion model is explicitly conditioned on the diffusion step t, such that the distribution of the reverse diffusion step can be written as $q_\\theta(X_{t-1}|X_t,t)$. However, in the following, we will drop the dependence on t to simplify the notation. The unnormalized probability of a diffusion path of the forward process can be calculated with $p(X_{0:T}) = p_B(X_0) \\prod_{t=1}^T p(X_t|X_{t-1})$. In the data-free setting samples $X_{0:T} \\sim p(X_{0:T})$ are not available. Sanokowski et al. (2024) invoke the Data Processing Inequality to introduce diffusion models in discrete NPO by proposing to use the rKL of joint probabilities $D_{KL}(q_\\theta(X_{0:T}) || p(X_{0:T}))$ as a tractable upper bound of the rKL of the marginals $D_{KL}(q_\\theta (X_0) || p_B(X_0))$. They further simplify this objective to express it in the following form:\n$T D_{KL} (q_\\theta(X_{0:T}) || p(X_{0:T})) = -T \\cdot \\sum_{t=1}^T \\mathbb{E}_{X_{t:T}\\sim q_\\theta(X_{t:T})} [S(q_\\theta(X_{t-1}|X_t))]$\n$-T \\sum_{t=1}^T \\mathbb{E}_{X_{t-1:T}\\sim q_\\theta(X_{t-1:T})} [log p(X_t|X_{t-1})]$\n$+ \\mathbb{E}_{X_{0:T}\\sim q_\\theta (X_{0:T})} [H(X_0)] + C',$\nwhere T is the temperature, C a parameter independent constant and S(.) the Shannon entropy. In practice, the expectation over $X_{0:T} \\sim q_\\theta(X_{0:T})$ is estimated using M diffusion paths, where each diffusion path corresponds to a sample of $X_{0:T}$ from the model. The objective is optimized using the log-derivative trick to propagate the gradient through the expectation over $q_\\theta$. Examination of Eq. 2 shows that the memory required for backpropagation scales linearly with the number of diffusion"}, {"title": "UNSUPERVISED COMBINATORIAL OPTIMIZATION", "content": "Sanokowski et al. (2024) apply diffusion models in UCO by reformulating it as an NPO problem. There is a wide class of CO problems that can be described in QUBO formulation (Lucas, 2014; Glover et al., 2022). In this case, the CO problem is described by an energy function $H_Q : {0,1}^N \\rightarrow \\mathbb{R}$ which is given by:\n$H_Q(X) = \\sum_{i,j} Q_{ij} X_i X_j,$\nwhere $Q \\in \\mathbb{R}^{N \\times N}$ is chosen according to the CO problem at hand. A table of the QUBO formulations of the CO problem types studied in this paper is given in Tab. 5. In UCO the goal is to train a conditional generative model $q_\\theta (X|Q)$ on problem instances Q that are drawn from a distribution $D(Q)$ (see Sec. 5.1 and App. A.6 for more information on D(Q)). After training the model can be used on unseen i.i.d CO problems to obtain solutions of high quality within a short amount of time. This can be realized by using the expectation of $H_Q(X)$ with respect to a parameterized probability distribution which is used as a loss function and minimized with respect to network parameters $\\theta$:\n$L(\\theta) = \\mathbb{E}_{Q\\sim D(Q),X\\sim q_\\theta(X|Q)}[H_Q(X)].$\nFor notational convenience the conditional dependence of $q_\\theta$ on the problem instance Q is suppressed in the following. As minimizing the expectation value of $H_Q(X)$ in Eq. 4 is prone to getting stuck in local minima, numerous works (Hibat-Allah et al., 2021; Sun et al., 2022; Sanokowski et al., 2023; 2024) reframe this problem as an NPO problem and minimize $T D_{KL}(q_\\theta(X)||p_B(X)) = \\mathbb{E}_{X\\sim q_\\theta(X)} [H_Q(X) + Tlog q_\\theta(X)] + C$ instead, where C is a constant which is independent of $\\theta$. The optimization procedure of this objective is combined with annealing, where the objective is first optimized at high temperature, which is then gradually reduced to zero. At T = 0 this objective reduces to the unconditional loss in Eq. 4. Sanokowski et al. (2023) motivate this so-called variational annealing procedure theoretically from a curriculum learning perspective and the aforementioned works show experimentally that it yields better solution qualities."}, {"title": "UNBIASED SAMPLING", "content": "When a parameterized probability distribution $q_\\theta (X)$ is used to approximate the target distribution $p_B(X)$ the learned distribution will typically be an imperfect approximation. Consequently, samples from $q_\\theta(X)$ will exhibit a bias. When the model is used to infer properties of the system that is described by the target distribution, it is essential to correct for this bias. The following paragraphs revisit two established unbiased sampling methods namely Self-Normalized Neural Importance Sampling (SN-NIS) and Neural Markov Chain Monte Carlo (NMCMC) that can be used to achieve this goal. These methods serve as the basis for our diffusion-based unbiased sampling methods which are introduced in Sec. 3.2.\nSelf-Normalized Neural Importance Sampling: SN-NIS allows asymptotically unbiased computation of expectation values of a target distribution. Given an observable $O : {0,1}^N \\rightarrow \\mathbb{R}$, an exact likelihood model $q_\\theta (X)$ can be used to calculate expectation values $(O(X))_{p_B(X)} := \\mathbb{E}_{p_B(X)} [O(X)]$ in the following way:\n$(O(X_0))_{p(X_0)} \\approx \\sum_{i=1}^M \\tilde{w}(X^i) O(X^i),$\nwhere $X^i$ corresponds to the i-th of M samples from $q_\\theta(X)$. The importance weights are computed with $\\tilde{w}(X^i) = \\frac{w(X^i)}{\\sum_j w(X^j)} = \\frac{p_B(X)}{q_\\theta (X)}$, where $w(X) = \\frac{p_B(X)}{q_\\theta(X)}$ (for a derivation we refer to App. A.2).\nThe probability distribution that is proportional to $p_B(X)|O(X)|$ yields the minimum-variance estimate of $(O(X))_{p_B(X)}$ (Rubinstein & Kroese, 2016). However, in our experiments, we focus on a"}, {"title": "METHODS", "content": "Sanokowski et al. (2024) demonstrate that increasing the number of diffusion steps in UCO improves the solution quality of the diffusion model, as it enables the model to represent more complex distributions. However, as discussed in Sec. 2.1, the loss function in Eq. 2 used in their work inflicts memory requirements that scale linearly with the number of diffusion steps. Given a fixed memory budget, this limitation severely restricts the expressivity of the diffusion model. In the following sections, we introduce training methods that mitigate this shortcoming."}, {"title": "Forward KL Objective", "content": "One possibility to mitigate the linear scaling issue is to use the forward Kullback-Leibler divergence (fKL). In contrast to the objective in Eq. 2 the gradient can be pulled into the expectation:\n$\\nabla_\\theta D_{KL}(p(X_{0:T})||q_\\theta(X_{0:T})) = -\\mathbb{E}_{X_{0:T}\\sim p(X_{0:T})} [\\nabla_\\theta log q_\\theta (X_{0:T})].$\nHowever, since in NPO samples $X_{0:T} \\sim p(X_{0:T})$ are not available, we employ SN-NIS to rewrite the expectation with respect to $X_{0:T} \\sim q_\\theta(X_{0:T})$. Note that this is feasible with diffusion models since they do provide exact joint likelihoods. In analogy to data-based diffusion models (Ho et al., 2020) one can now use Monte Carlo estimates of the sum over time steps $log q_\\theta (X_{0:T}) = \\sum_{t=1}^T log q_\\theta (X_{t-1}|X_t)$ to mitigate the aforementioned memory scaling issue. The resulting gradient of the fKL objective is given by (see App. A.2.6):\n$\\nabla_\\theta D_{KL}(p(X_{0:T})||q_\\theta(X_{0:T})) = -T \\sum \\mathbb{E}_{t\\sim U{1,...,T}} [w(X_{0:T}) \\nabla_\\theta log q_\\theta (X_{t-1}|X_t)],$\nwhere $w(X_{0:T}) = \\frac{\\tilde{w}(X_{0:T})}{\\sum_{i=1}^M \\tilde{w}(X_{0:T})}$ are importance weights with $\\tilde{w}(X_T) = \\frac{p(X_T)}{q_\\theta(X_{0:T})}$, and $U{1, ..., T}$ is the uniform distribution over the set {1, ..., T}.\nIn the following, we will refer to this method as SDDS: fKL w/ MC since it realizes Scalable Discrete Diffusion Samplers (SDDS) using an objective that is based on the fKL, where the linear memory scaling issue is addressed with Monte Carlo estimation over diffusion steps. A pseudocode of the optimization procedure is given in App. A.3.5."}, {"title": "Reverse KL Objective", "content": "The minimization of the reverse Kullback-Leibler divergence (rKL) based objective function $L(\\theta)$ introduced by Eq. 2 can be shown to be equivalent to parameter updates using the policy gradient theorem (Sutton & Barto, 2018) (see App. A.2.5). The resulting gradient updates are expressed as:\n$\\nabla_\\theta L(\\theta) = -\\mathbb{E}_{X_t\\sim d_\\theta (X,t),X_{t-1}\\sim q_\\theta(X_{t-1}|X_t)} [Q^\\theta(X_{t-1}, X_t) \\nabla_\\theta log q_\\theta (X_{t-1}|X_t)],$\nwhere:"}, {"title": "UNBIASED SAMPLING WITH DISCRETE ISING MODELS", "content": "As concluded in Sec. 2.3, neither SN-NIS nor NMCMC can be applied with diffusion models. In the following, we introduce adapted versions each of these methods that allow us to perform unbiased sampling, i.e. unbiased computation of expectation values, with diffusion models."}, {"title": "Self-Normalized Neural Importance Sampling for Diffusion Models", "content": "Given a diffusion model $q_\\theta$ that is trained to approximate a target distribution $p_B (X_0)$, we can use this model to calculate unbiased expectations $(O(X_0))_{p_B(X_0)}$ with SN-NIS in the following way (see App. A.2.1):\n$(O(X_0))_{p_B(X_0)} \\approx \\sum[w(X^i_{0:T}) O(X^i_0)]$\nwhere $w(X_{0:T}) = \\frac{\\tilde{w}(X_{0:T})}{\\sum_{i=1}^M \\tilde{w}(X_{0:T})}$ and $X^i_{0:T} \\sim q_\\theta(X_{0:T})$ with $\\tilde{w}(X_{0:T}) = \\frac{p(X^i_{0:T})}{q_\\theta (X^i_{0:T})}$. Using these importance weights the partition sum of $p_B(X_0)$ can be estimated with $\\hat{Z} = \\sum_{i=1}^M \\tilde{w}(X^i_{0:T})$.", "contents": ["$(O(X_0))_{p_B(X_0)} \\approx \\sum[w(X^i_{0:T}) O(X^i_0)]$\nwhere $w(X_{0:T}) = \\frac{\\tilde{w}(X_{0:T})}{\\sum_{i=1}^M \\tilde{w}(X_{0:T})}$ and $X^i_{0:T} \\sim q_\\theta(X_{0:T})$ with $\\tilde{w}(X_{0:T}) = \\frac{p(X^i_{0:T})}{q_\\theta (X^i_{0:T})}$. Using these importance weights the partition sum of $p_B(X_0)$ can be estimated with $\\hat{Z} = \\sum_{i=1}^M \\tilde{w}(X^i_{0:T})$."]}, {"title": "Neural MCMC for Diffusion Models", "content": "Starting from an initial diffusion path $X_{0:T}$, we propose a state by sampling $X'_{0:T} \\sim q(X_{0:T})$. This diffusion path is then accepted with the probability (see App: A.2.4):\n$A(X', X) = min (1, \\frac{p(X'_{0:T}) q_\\theta (X_{0:T})}{p(X_{0:T}) q_\\theta (X'_{0:T})}).$\nThis process is repeated until the Markov chain meets convergence criteria and samples $X_{0:T}$ are distributed as $p(X_{0:T})$ and $X_0$ can be considered to be distributed as $p_B (X_0)$. These samples can be used to approximate expectations with $(O(X_0))_{X_0\\sim p_B(X_0)}$ (see App. A.2.4)."}, {"title": "RELATED WORK", "content": "Neural Optimization: Besides their predominance in supervised and unsupervised learning tasks, neural networks become an increasingly popular choice for a wide range of data-free optimization tasks, i.e. scenarios where an objective function can be explicitly expressed rather than implicitly via data samples. In Physics Informed Neural Networks (Raissi et al., 2019) models are trained to represent the solutions of differential equations. Here the loss function measures the adherence of the solution quality. Similarly, Berzins et al. (2024) propose a neural optimization approach for generating shapes under geometric constraints. Recently, there has been increasing interest in using probabilistic generative models to generate solutions to neural optimization. Here the learned models do not directly represent a solution but rather a probability distribution over the solution space. We refer to this endeavor as Neural Probabilistic Optimization (NPO). In the following, we discuss two important NPO application areas in discrete domains."}, {"title": "Neural Combinatorial Optimization", "content": "Neural CO aims at generating high-quality solutions to CO problems time-efficiently during inference time. The goal is to train a generative model to generate solutions to a given CO problem instance on which it is conditioned. Supervised CO (Sun & Yang, 2023; Li et al., 2018; B\u00f6ther et al., 2022a) typically involves training a conditional generative model using a training dataset that includes solutions obtained from classical solvers like Gurobi (Gurobi Optimization, LLC, 2023). However, as noted by Yehuda et al. (2020), these supervised approaches face challenges due to expensive data generation, leading to increased interest in unsupervised CO (UCO). In UCO the goal is to train models to solve CO problems without relying on labeled training data but only by evaluating the quality of generated solutions Bengio et al. (2021b). These methods often utilize exact likelihood models, such as mean-field models (Karalias & Loukas, 2020; Sun et al., 2022; Wang & Li, 2023). The calculation of expectation values in UCO is particularly convenient with mean-field models due to mathematical simplification arising from their assumption of statistical independence among modeled random variables. However, Sanokowski et al. (2023) demonstrate that the statistical independence assumption in mean-field models limits their performance on particularly challenging CO problems. They show that more expressive exact likelihood models, like autoregressive models, offer performance benefits, albeit at the cost of high memory requirements and longer sampling times, which slow down the training process. These limitations can be addressed by combining autoregressive models with RL methods to reduce memory requirements and accelerate training as it is done in Khalil et al. (2017) and Sanokowski et al. (2023). Sanokowski et al. (2023) additionally introduce Subgraph Tokenization to mitigate slow sampling and training in autoregressive models. Zhang et al. (2023) utilize GFlow networks (Bengio et al., 2021a), implementing autoregressive solution generation in UCO. Sanokowski et al. (2024) introduce a general framework that allows for the application of diffusion models to UCO and demonstrate their superiority on a range of popular CO benchmarks."}, {"title": "Unbiased Sampling", "content": "In this work, unbiased sampling refers to the task of calculating unbiased expectation values via samples from an approximation of the target distribution. Corresponding methods rely so far primarily on exact likelihood models, i.e. models that provide exact likelihoods for samples. Unbiased sampling plays a central role in a wide range of scientific fields, including molecular dynamics (No\u00e9 & Wu, 2018; Dibak et al., 2022), path tracing (M\u00fcller et al., 2019), and lattice gauge theory (Kanwar et al., 2020). These applications in continuous domains are suitable for using exact likelihood models like normalizing flows which are a popular model class in these domains. More recently approximate likelihood models became increasingly important in these applications since their increased expressivity yields superior results (Dibak et al., 2022; Zhang & Chen, 2022; Berner et al., 2022a; Jing et al., 2022; Berner et al., 2022b; Richter et al., 2023; Vargas et al., 2023; 2024; Akhound-Sadegh et al., 2024). In discrete domains, unbiased sampling arises as a key challenge in the study of spin glasses (Nicoli et al., 2020; McNaughton et al., 2020; Inack et al., 2022; Bia\u0142as et al., 2022; Biazzo et al., 2024), many-body quantum physics (Sharir et al., 2020; Wu et al., 2021), and molecular biology (Cocco et al., 2018). In these settings, autoregressive models are the predominant model class. We are not aware of works that explore the applicability and performance of approximate likelihood models like diffusion models for unbiased sampling on discrete problem domains."}, {"title": "EXPERIMENTS", "content": "We evaluate our methods on UCO benchmarks in Sec. 5.1 and on two benchmarks for unbiased sampling Sec. 5.2 and in App. A.8.2. In all of our experiments, we use a time-conditioned diffusion model $q_\\theta(X_{t-1}|X_t,t)$ that is realized either by a Graph Neural Network (GNN) (Scarselli et al., 2009) in UCO experiments or by a U-Net architecture (Ronneberger et al., 2015) in experiments on the Ising model (see App. A.4). In our experiments the probability distribution corresponding to individual reverse diffusion steps is parametrized via a product of Bernoulli dsitributions $q_\\theta(X_{t-1}|X_t,t) = \\prod_{i=1}^N q_\\theta(X_{t-1}|X_t,t) = \\prod_i q_\\theta(X_{t-1,i} = 1 | X_t, t)^{X_{t-1,i}} , (1 - q_\\theta(X_t)_i)^{1-X_{t-1,i}}, X^i \\in \\mathbb{Z}^2$, where $q_\\theta (X_t)_i := q_\\theta(X_{t-1,i} = 1 | X_t, t)$. As a noise distribution, we use the Bernoulli noise distribution from (Sohl-Dickstein et al., 2015) (see App. A.3.1)."}, {"title": "UNSUPERVISED COMBINATORIAL OPTIMIZATION", "content": "In UCO the goal is to train a model to represent a distribution over solutions, which is conditioned on individual CO problem instances (see Sec. 2.2). Since each CO problem instance corresponds to a"}, {"title": "LIMITATIONS AND CONCLUSION", "content": "This work introduces Scalable Discrete Diffusion Samplers (SDDS) based on novel training methods that enable the implementation of discrete diffusion models with an increased number of diffusion steps in Unsupervised Combinatorial Optimization (UCO) and unbiased sampling problems. We demonstrate that the reverse KL objective of discrete diffusion samplers can be optimized efficiently using Reinforcement Learning (RL) methods. Additionally, we introduce an alternative training method based on Self-Normalized Importance Sampling of the gradients of the forward KL divergence. Both methods facilitate mini-batching across diffusion steps, allowing for more diffusion steps with a given memory budget. Our methods achieve state-of-the-art on popular challenging UCO benchmarks. For unbiased sampling in discrete domains, we extend existing importance sampling and Markov Chain Monte Carlo methods to be applicable to diffusion models. Furthermore, we show that discrete diffusion models can outperform popular autoregressive approaches in estimating observables of discrete distributions. Future research directions include leveraging recent"}, {"title": "APPENDIX", "content": ""}, {"title": "DERIVATIONS", "content": ""}, {"title": "IMPORTANCE SAMPLING AND NEURAL IMPORTANCE SAMPLING", "content": "Importance Sampling (IS) is a well-established Monte Carlo method used to estimate expectations of observables O(X) under a target distribution p(X) when direct sampling from p is challenging. The core idea is to use a proposal distribution q(X) which is easy to sample from and proposes samples where p(X) or ideally p(X) |O(X)| is large. IS can be used to calculate the expectation of an observable O(X) in the following way:\n$O(X) = \\sum p(X) O(X) = \\sum q(X) \\frac{p(X)}{q(X)} O(X) = \\mathbb{E}_{X\\sim q(X)} [\\frac{p(X)}{q(X)} O(X)] (6)$\nHowever, this approach makes it necessary to design a suitable proposal distribution q(X), which is not possible in many cases. Therefore, Neural Importance Sampling can be used instead, where a distribution $q_\\theta (X)$ is parameterized using a Neural Network and trained to approximate the target distribution. By replacing q(X) in Eq. 6 with $q_\\theta(X)$ the Neural Importance Sampling estimator is then given by:\n$O(X) = \\mathbb{E}_{X\\sim q_\\theta(X)} [\\frac{p(X)}{q_\\theta (X)} O(X)]$"}, {"title": "SELF-NORMALIZED NEURAL IMPORTANCE SAMPLING", "content": "In some cases", "by": "n$\\mathbb{E}_{p(x)} [O(X)"}]}