{"title": "LP-DETR: Layer-wise Progressive Relations for Object Detection", "authors": ["Zhengjian Kang", "Ye Zhang", "Xiaoyu Deng", "Xintao Li", "Yongzhe Zhang"], "abstract": "This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach that enhances DETR-based object detection through multi-scale relation modeling. Our method introduces learnable spatial relationships between object queries through a relation-aware self-attention mechanism, which adaptively learns to balance different scales of relations (local, medium and global) across decoder layers. This progressive design enables the model to effectively capture evolving spatial dependencies throughout the detection pipeline. Extensive experiments on COCO 2017 dataset demonstrate that our method improves both convergence speed and detection accuracy compared to standard self-attention module. The proposed method achieves competitive results, reaching 52.3% AP with 12 epochs and 52.5% AP with 24 epochs using ResNet-50 backbone, and further improving to 58.0% AP with Swin-L backbone. Furthermore, our analysis reveals an interesting pattern: the model naturally learns to prioritize local spatial relations in early decoder layers while gradually shifting attention to broader contexts in deeper layers, providing valuable insights for future research in object detection.", "sections": [{"title": "I. INTRODUCTION", "content": "DEtection Transformers (DETRs) [1] have achieved great progress by proposing an end-to-end architecture for object detection. However, their low training efficacy remains a critical challenge. The root cause is the imbalanced supervision during training DETR employs Hungarian algorithm to assign only one positive prediction to each ground-truth box, leaving the majority of predictions as negative samples. This insufficient positive supervision leads to slow and unstable convergence. While various approaches have been proposed to address this issue through different technical routes like multi-scale feature learning [2], denoising training [3], [4], hybrid matching strategies [5], [6] and loss alignment [7], [8], they primarily focus on local feature enhancement or query learning optimization, leaving the potential of relation modeling in self-attention not been fully explored.\nIn the vision community, modeling inter-object relationships has proven beneficial for detection performance. Previous approaches mainly focus on two aspects: co-occurrence patterns of object categories [9]\u2013[12] and spatial relations using various criteria [13]\u2013[15]. These methods have demonstrated that incorporating relation information can effectively enhance detection accuracy by capturing contextual dependencies between objects. However, in DETR field, few works have investigated the learnable relation between object queries in the self-attention, a key component in DETR decoders. Hao et al. [12] attempt to model class correlations using a learnable relation matrix in the decoder\u2019s self-attention, but their approach does not consider spatial information and requires mapping class-to-class relations back to query-to-query interactions. More recently, Relation-DETR [16] introduces explicit position relations between bounding boxes with cross-layer refinement. Motivated by their work but different from these approaches, we directly incorporate geometric relation weights into queries within each layer and propose layer-specific relation modeling to capture evolving spatial dependencies.\nIn this paper, we present LP-DETR (Layer-wise Progressive DETR), which enhances object detection through explicit modeling of multi-scale spatial relations across decoder layers. Our key insight is that object relations naturally evolve from local to global contexts through the detection pipeline, and different scales of spatial relations may play varying roles at different stages of the detection process. Based on this observation, we propose a progressive relation-aware self-attention module that adaptively learns to balance different scales of spatial relations at different decoder layers. This design allows the model to capture fine-grained local relationships in early layers while gradually incorporating broader relation information in deeper layers. The main contributions of our work are threefold:\n\u2022 We introduce a relation-aware self-attention mechanism that explicitly models multi-scale spatial relationships between object queries.\n\u2022 We propose a progressive refinement strategy that allows the model to adaptively adjust relation weights across decoder layers.\n\u2022 We discover and validate an interesting pattern where spatial relations naturally progress from local to global contexts through decoder layers, providing valuable insights for future research."}, {"title": "II. RELATED WORK", "content": "Relation networks have emerged as a powerful approach for modeling inter-object relationships at instance level, which can be broadly categorized into two main directions: co-occurrence modeling and spatial relation modeling.\nCo-occurrence approaches focus on capturing statistical dependencies between object categories. Some methods [9], [10] directly learn from category distribution patterns in large datasets, while others [11], [12] adaptively learn class relationships from annotations. However, these approaches either rely on fixed statistical priors or encounter with challenges in mapping between instances and categories [10].\nSpatial relation approaches construct graph structures where object features serve as nodes and their spatial relationships as edges. Pioneering works like Relation Network [13] introduce geometric weights in attention modules to model spatial relations. Recent methods determine relation weights through various metrics, such as position-aware distance [21], [22], attention mechanisms [14], [15] or appearance similarity [23]. While these learnable relations offer greater flexibility compared to fixed priors, they typically require larger datasets and longer training time to effectively learn the relations from data."}, {"title": "A. Transformer for Object Detection", "content": "DETRs) [1] establishes a new paradigm for end-to-end object detection by eliminating hand-crafted post-processing steps such as Non-maximum Suppression (NMS). Its transformer-based architecture consists of two main components: an encoder that transforms flattened image features into enriched memory representations, and a decoder that converts a set of learnable object queries into final detection results. The decoder operates through two attention mechanisms: self-attention for modeling interactions among object queries, and cross-attention for capturing relationships between queries and encoded memory features.\nHowever, DETR suffers from slow convergence during training, and various approaches have been proposed to address this issue from different methodological perspectives:\n(1) Enhanced Feature Learning: Deformable DETR [2] explores multi-scale features through deformable attention with sparse reference points, while Focus-DETR [17] and Salience-DETR [18] improve feature selection through salient token identification in the encoder. (2) Query Enhancement: DAB-DETR [19] decouples object queries into 4D anchor box coordinates for iterative refinement, while DN-DETR [3] and DINO [4] accelerate training through auxiliary denoising task and contrastive learning. (3) Better Supervision: Hybrid DETR [5] and Group DETR [6] adopt one-to-many matching to increase supervision signals, while Stable-DINO [7] and Align-DETR [8] propose specialized loss functions to align classification and localization. (4) Attention Mechanism: Recent works focus on improving attention mechanisms, where Cascade-DETR [20] enhances query-feature interactions through cross-attention, and Relation-DETR [16] learns explicit relation modeling between queries in self-attention."}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. DETR Preliminaries", "content": "A DETR-style detector consists of a backbone network (e.g., ResNet [24], Swin Transformer [25]) and a transformer architecture with encoder and decoder modules. Given an input image, the backbone first extracts image features, which are then split into patch tokens. The transformer encoder processes these tokens through self-attention mechanisms and outputs enhanced feature representations, denoted as memories Z = {z1, ..., zm}.\nThe transformer decoder takes a set of learnable object queries Q = {q1,..., qn} as input. Recent works [4], [19] propose to decouple these queries into content queries Qc for label embedding and position queries QP for bounding box prediction, enabling better relation alignment. The decoder consists of L stacked blocks, where each block contains three sequential components: a self-attention layer, a cross-attention layer, and a feed-forward network (FFN).\nThe self-attention layer enables communication between object queries, allowing each query to refine its prediction by considering other queries' predictions. The cross-attention layer facilitates interaction between object queries Q and encoded memories Z, aggregating features for object localization and classification. Finally, the FFN transforms the query embeddings for prediction through parallel classification and regression heads."}, {"title": "B. Layer-wise Progressive Relation-Aware Attention", "content": "The high-level architecture of our proposed progressive relation-aware DETR model is presented in Fig. 1. Our proposed attention is applied into the self-attention in the decoder component. Let\u2019s consider object queries Q consists of content embedding queries Qc, reference box position queries QP (represented by (x, y, w, h)), and relation queries Qr. Given a set of N object queries, qi = {qic, qip, qir}i=1N, the i-th relation query qir with respect to all the object queries can be calculated as the weighted sum of all the queries:\n$q^r_i = \\sum_{j=1}^N (W_v q_j).$"}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Experiment Settings", "content": "1) Dataset and backbone: We evaluate our Progressive Relation-Aware DETR on COCO 2017 [26], which contains 118k training images and 5k validation images across 80 object categories. The performance is evaluated on the validation set using standard COCO metrics: average precision (AP) at different IoU thresholds (IoU=0.5, 0.75, 0.5:0.95) and scales (small, medium, large). We implement our method with two backbone networks: ResNet50 [24] pretrained on ImageNet-1k and Swin-Large [25] pretrained on ImageNet-22k [27]. Both backbones are finetuned with an initial learning rate of 1 \u00d7 10\u22125, which is decreased by a factor of 0.1 at later stages.\n2) Implementation details: All experiments are conducted on NVIDIA RTX 3090 GPUs using AdamW optimizer [28] with a weight decay of 1 \u00d7 10\u22124 and a total batch size of 16. For the relation embedding module, we set the temperature T = 10000, scale s = 100, and position embedding dimension dpos = 16 in the sinusoidal encoding. The position relations are constructed at three scales (local, medium and global) with equal initial weights 0.33 across all 6 decoder layers. Following standard practice in DETR-based methods, we apply common data augmentations including random resize, crop and flip during training. We report results under both 1x (12 epochs) and 2x (24 epochs) training schedules."}, {"title": "C. Ablation study", "content": "Analysis of the number of relation heads. We examine how the number of relation heads affects model performance in our relation-aware self-attention module. Table III shows the results with varying numbers of relation heads while maintaining a total of 8 attention heads. Using no relation head (0 head) serves as our baseline, where the module functions as standard self-attention, achieving 51.1% AP, 68.6% AP50 and 55.8% AP75. The performance consistently improves as we increase the number of relation heads. With all 8 heads dedicated to relation-aware attention, our model achieves the best performance of 52.3% AP, showing an improvement of 1.2% AP over the baseline. Comparable gains are also observed in AP50 (+1.0%) and AP75 (+1.0%). These results demonstrate the benefit of incorporating relation-aware attention in the self-attention mechanism.\nAnalysis of different modules. We evaluate the effectiveness of progressive refinement (PR) in combination with our relation-aware self-attention module. In Table IV, the relation-aware self-attention module alone improves the detection performance by 0.9% AP, 0.6% AP50 and 0.7% AP75 compared to the baseline. Adding progressive refinement brings additional gains of 0.3% AP, 0.4% AP50 and 0.3% AP75. The improvements are also consistent across different object scales."}, {"title": "D. Progressive Refinement Analysis", "content": "To understand how different scales of position relations (local, medium and global) evolve through decoder layers, we visualize the learned relation weights across layers in Fig. 3. Our analysis reveals an interesting pattern in how the model balances different spatial relations. In the first layer, the weights among three scales remain relatively comparable, suggesting initial uncertainty in relation scale selection. From layer 2, the model develops a strong preference for local relations, with weights exceeding 0.9, indicating that early decoder layers focus primarily on establishing local relationships between queries. Moving to deeper layers, we observe a gradual transition in attention distribution: local relation weights decrease to 0.24, while medium and global relation weights steadily increase to around 0.4. By the final layer, the weights for medium and global relations surpass that of local relations. This progressive transition from local to broader spatial contexts aligns with the intuitive understanding that object detection requires hierarchical processing - from local to global query interactions. These findings suggest promising directions for future research in relation modeling, as the clear layer-wise progression from local to global relations indicates potential for more efficient architectures that explicitly leverage this hierarchical pattern."}, {"title": "V. CONCLUSION", "content": "In this paper, we present a progressive relation-aware self-attention module that enhances DETR detector by incorporating learnable multi-scale spatial relationships between object queries. Our method adaptively adjusts relation weights across different scales and decoder layers, achieving competitive performance on standard benchmarks. Through extensive experiments, we demonstrate that our module improves both convergence speed and detection accuracy compared to standard self-attention. Our analysis reveals a pattern in how spatial relations evolve through the network: local relations dominate in early decoder layers, while global relations become increasingly important in deeper layers. Our findings open several promising directions for future research."}]}