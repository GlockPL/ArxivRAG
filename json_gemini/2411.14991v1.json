{"title": "Free Energy Projective Simulation (FEPS): Active inference with interpretability", "authors": ["J. Pazem", "M. Krumm", "A. Q. Vining", "L. J. Fiderer", "H. J. Briegel"], "abstract": "In the last decade, the free energy principle (FEP) and active inference (AIF) have achieved many successes connecting conceptual models of learning and cognition to mathematical models of perception and action. This effort is driven by a multidisciplinary interest in understanding aspects of self-organizing complex adaptive systems, including elements of agency. Various reinforcement learning (RL) models performing active inference have been proposed and trained on standard RL tasks using deep neural networks. Recent work has focused on improving such agents' performance in complex environments by incorporating the latest machine learning techniques. In this paper, we take an alternative approach. Within the constraints imposed by the FEP and AIF, we attempt to model agents in an interpretable way without deep neural networks by introducing Free Energy Projective Simulation (FEPS). Using internal rewards only, FEPS agents build a representation of their partially observable environments with which they interact. Following AIF, the policy to achieve a given task is derived from this world model by minimizing the expected free energy. Leveraging the interpretability of the model, techniques are introduced to deal with long-term goals and reduce prediction errors caused by erroneous hidden state estimation. We test the FEPS model on two RL environments inspired from behavioral biology: a timed response task and a navigation task in a partially observable grid. Our results show that FEPS agents fully resolve the ambiguity of both environments by appropriately contextualizing their observations based on prediction accuracy only. In addition, they infer optimal policies flexibly for any target observation in the environment.", "sections": [{"title": "Introduction", "content": "A key challenge in both cognitive science and artificial intelligence is understanding cognitive processes in biological systems and applying this knowledge to the development of artificial agents. In this work, we develop an interpretable artificial agent which integrates key aspects of both reinforcement learning (RL) [1] and active inference [2, 3, 4, 5, 6]. In RL, an external reward signal is typically used to guide an agent's behavior while in active inference no such reward signal exists. Instead, agents follow an intrinsic motivation which is rooted in the free energy principle (FEP) [7, 8].\nCentral to the FEP is the idea that adaptive systems can be modeled as performing an approximate form of Bayesian inference. The outcome of this process minimizes a quantity, called variational free energy (VFE), which lends its name to the FEP. The FEP encompasses various paradigms that align with the goal of Bayesian inference, such as predictive processing [9] and the Bayesian brain hypothesis [10].\nOne way to implement the FEP is through active inference which involves a planning method for action selection based on an internal model of the environment, referred to as the world model, along with a preference distribution, which encodes the agent's desired states (e.g., maintaining a certain body temperature). According to the FEP, it is in principle always possible to explain the observed behavior of living systems through active inference"}, {"title": "Active Inference", "content": "The following presentation of active inference is divided into two parts. First, we outline how the world model is constructed and updated in response to sensations (perceptual inference). Next, we explain how planning and action selection are modeled (active inference).\nDuring perceptual inference, the agent develops the world model [34, 35], sometimes referred to as a generative model, highlighting its ability to generate simulated percepts. The world model [11, 6], is described as a partially observable Markov decision process (POMDP) (B, S, A, T, L, R), with belief states B, observations S, and actions A. The transition function T assigns probabilities to transitions between belief states given some action, while L, sometimes referred to as the emission function, defines the probability that some belief state b \u2208 B emits a sensory signal s \u2208 S. Note that belief states are observable to the agent and that they are generally different from the actual, unobservable, states of the environment.\nIn what follows, the time-indexed random variables Bt, St, and At represent beliefs, observations, and actions at a given time t, with their possible values denoted by the corresponding lower-case letters. The transition function T between times t \u2212 1 and t corresponds to the conditional distribution p(Bt|Bt\u22121, At\u22121), while the emission function Lat time t is the likelihood p(St|Bt). Finally, rewards R are used to learn the transition and emission functions. In this work, rewards are internal (generated by the agent itself) [36] but in general they could also be external (given by the environment). It is further assumed that action At is selected based on the current belief state Bt, as represented by the conditional distribution \u03c0(At|Bt), which defines the policy. Combining these elements, the joint distribution of the world model up to time t can be decomposed as follows:\np(Bo:t, Ao:t-1, So:t) = p(Bo, So) \\prod_{t'=1}^{t} \\pi(At'\u22121|Bt'\u22121)p(St'|Bt')p(Bt'|Bt'\u22121, At'\u22121), (1)\nwhere the index notation Bo:t = (B0, B1,..., Bt) is used for sequences of random variables, and p(Bo, So) denotes the initial distribution over beliefs and observations. Equation (1) defines the relations between random variables: while the distribution over observation and an action is specified by the current belief state, both the previous belief state and the previous action determine the current belief state.\nThe transition function p(Bt Bt-1, At-1) plays a crucial role in active inference because it has to be learned by the agent. Following the idea of active inference, learning involves updating p(Bt|Bt\u22121, At\u22121) through variational inference, an approximate version of Bayesian inference. Variational inference simplifies Bayesian inference by restricting the set of possible posterior transition functions to a family {Q\u00a2(Bt|Bt\u22121, At\u22121)}$, parametrized by some variable 4, thus reducing computational complexity. When a new observation senv is received from the"}, {"title": "Projective Simulation", "content": "Projective Simulation (PS) [31] is a model for embodied reinforcement learning and agency inspired from physics that performs associative learning on a memory encoded as a graph. It is composed of a network of vertices, denoted clips, with a defined structure that gives each clip a semantic meaning, that can be assigned from the start or acquired progressively through past experiences. For example, a clip may represent a sensory state the agent's sensors are capable of receiving, or it can inherit supplementary semantics from past experiences and reinforcements. Directed edges between clips are weighted and can be modified dynamically to learn and adapt to the environment. In particular, PS allows the simulation of percept-action loops to represent previous interactions with the environment and the associated decision process. The resulting graph is the Episodic and Compositional Memory (ECM). When a clip is visited, either because it is currently experienced, or because it is used for deliberation, it is excited. After a first clip was excited, deliberation takes place as a traceable random walk in the ECM originating from this initial clip. It ends when a decoupling criterion is met, which leads to an action on the environment.\nFor the purpose of solving different environments while retaining interpretability, the ECM can adopt different structures. In order to imitate a percept-action loop, the ECMs are often structured as bipartite graphs, where a first layer contains percepts and the second is composed of actions [42, 43]. In this case, the trained ECM is directly relatable to a policy. For more complex environments, or in order to extract abstract concepts from the percepts, an intermediate layer can be added between the percept and action layers [44, 45]. The ECM can either contain a fixed number of clips, or it can dynamically add and erase some of them when needed [45]. To consider composite percepts and actions, the ECM graph can be replaced by a hypergraph, where each hyperedge connects a set of clips to another set [46].\nEach directed edge is equipped with at least one attribute to track learning and allow the agent to react adaptively to the environment. h-values increase as the edges are rewarded. They encode the strength of associations between clips that are useful to fulfill some task and record the learning of the agent. The probability of a transition associated with an edge with h-value hij connecting two clips, ci \u2192 cj, is inferred from the h-values:\np(cj|ci) = \\frac{h_{ij}}{\\sum_k h_{ik}} (5)\nAlternatively, a softmax function can also be implemented to enhance the differences between probabilities, espe- cially in large ECMs. Upon receiving a new percept, the agent deliberates by taking a random walk through the ECM, using the probabilities defined from the h-values to sample a new edge.\nAs the agent modelled with PS interacts with its environment, it receives rewards that are distributed over the different edges, which changes the corresponding h-values. Specifically, the h-value of the edge ci\u2192 cj is updated as follows:\nh_{ij}^{t+1} = \\eta (h_{ij}^{0} - h_{ij}) + \\frac{R}{\\tau} (6)\nwhere y is the forgetting parameter, hij is the initial h-value for ci \u2192 cj and R is the reward. When the reward is positive, the h-value of the corresponding edge is increased accordingly. If an edge is not visited or does not receive a positive reward, the corresponding h-value decreases back to their initial value thanks to the forgetting mechanism in the second term. In order to accept continuous percepts and to unlock generalization on some task, neural networks have been used to update the h-values in some cases. Training was then implemented by minimizing a loss function [47]."}, {"title": "The Free Energy Projective Simulation agent", "content": "We combine Projective Simulation, with Active Inference, following the free energy principle's framework. A FEPS agent is a model-based Projective Simulation agent, where the world model is an ECM with a clone-structured architecture [53, 54]. Consequently, clone clips inherit the semantics of the unique sensory state they relate to, and context creates distinctions between hidden states that emit the same observations. As in the FEP, the agent does not need external rewards. Instead, prediction accuracy, weighted with confidence, is used as a reinforcement signal. The world model is directly exploited by the agent to set the edges' h-values in the policy with active inference."}, {"title": "Architecture of the agent", "content": "To mimic a system described by the FEP, the agent is composed of two structures: the world model and the policy, each represented by separate graphs with vertices corresponding to random variables, and edges that can be sampled to perform a random walk, as in figure 1. Consider the agent can perceive Ns sensory states, has NB possible belief states and a repertoire of NA actions. Each state is supported by one vertex in a graph. The world model's vertices can either support belief states or sensory states. The world model is the representation of the environment (see Eq. 1) required by active inference. For reasons that will become clear shortly, the ECM of a FEPS agent is made of all vertices, that we call clips, that support belief states, and edges that represent transitions between such clips. A belief state is then formally defined by the excitation configuration of clips at one step of the deliberation. We limit the number of excitations in the ECM at any given time to one. Such a vertex deserves the name \"clip\" because it receives an excitation when the corresponding representation of an event is revisited in order to make a decision. The policy covers all possible conditioned responses coming from any belief state, given the repertoire of actions of the agent.\nIn the world model, two sets of edges can be traversed at different times during the deliberation: we denote them emission and transition edges respectively. They aim at predicting and explaining sensory signals received from the environment.\nIn the first set, emission edges b\u2192s relate belief states and sensory states, modeling the latter as parts of larger, possibly contextualized, hidden states by using clone-structured Hidden Markov Models (HMM) [53, 55, 54]. Each clip is bound to a single observation and is denoted a clone clip. A single edge in the emission set carries a non-zero probability for each clip, as shown in figure la. Consequently, this set of edges defines a deterministic likelihood p(st|bt) = \u03b4st,s(bt) in the world model in Eq. (1) and the agent remains initially agnostic of the dynamical structure of the hidden states {e}=0 in the environment. Meanwhile, a clone clip can readily be interpreted as an augmented description of a specific observation. In particular, the additional information can relate to the cause or the context of the observation, such as the previous belief state and previous action, for example. Sampling some sensory state \u015dt+1 amounts to predicting the next observation: if it turns out to coincide with the actual observation perceived from the environment, a reward will be distributed to the edges contributing to the random walk that led to this prediction. For clarity, we denote predicted states with hatted low case letters. We choose to associate each observation to a fixed number Nclones of clone clips, such that NB = NS \u00d7 Nclones. This approach works remarkably well for navigation problems [55, 54]. Transferring the dynamics learned on some set of sensory states to another set is also possible by keeping the transition function unchanged, but redistributing clone clips to the sensory states in the new set [54].\nThe purpose of the set of transition edges bt\u2192 bt+1 in the world model is to encode the presumed dynamics in the environment as transitions between belief states, conditioned on actions. They are represented as edges between clone clips in figure la. In contrast to other sets of edges, transition edges are endorsed with attributes such as h-values that enable learning with reinforcements (as in Eq. 6 for example). Therefore, clone clips together with transition edges constitute an ECM for the FEPS agent. From a given clone clip, for each action, a set of edges points to the next possible estimated belief states. The h-values of those edges indicate how certain the agent is that taking a particular action from the current belief state will lead to any of the clone clips in the future. There can be at most NB edges in each such group of transition edges. The full set of transition edges defines the transition function p(Bt+1|Bt, At) in the world model in Eq. (1) and corresponds to the trained part of the model. Before reinforcement, this distribution is referred to as the prior. The posterior is the updated version of"}, {"title": "Reinforcement with prediction accuracy", "content": "Each interaction step with the environment involves a deliberation over three states: (1) the next belief state is be proposed, (2) the next sensory state is predicted and (3) an action is chosen. The agent excites a belief state bt+1 it believes it will transition to, given its current action at, by sampling a transition edge bt \u2192 bt+1. From there, the agent makes a prediction \u015dt+1 about the next sensory state. Meanwhile, an action at+1 is selected in the policy and it is applied to the hidden state in the environment, that emits a new observation. The interaction step ends by comparing the predicted and perceived sensory states, \u015dt+1 and st.\nThe world model is trained without external rewards, and reinforcement is instead based on matching predictions and observations. We call trajectory a sequence of transitions that led to correct predictions about sensory states. To record the trajectory, transition edges are equipped with a new attribute: the confidence, f. Initialized at zero, it increases for all transitions in the trajectory every time the prediction \u015dt+1 and the actual sensory state st coincide. The more subsequent predictions an edge enabled, the higher the confidence for that edge: it reflects the number of correct predictions the edge enabled until the end of the trajectory. Formally, a trajectory is a sequence of transitions whose predictions were confirmed by the observations from the environment. If at step n in the t-th trajectory Tt the sensory prediction was accurate, confidence is enhanced for all edges i \u2192 jin tt:\nf_{ij}^{(T_t),n} = \\begin{cases}\nf_{ij}^{(T_t),n-1}+1 & \\text{if } b_i \\rightarrow b_j \\in T_t \\\\\n0 & \\text{otherwise}.\n\\end{cases} (7)\nWhen the prediction and observation do not match, the trajectory Tt is interrupted, and the rewards are distributed to the transition edges' h-values proportionally to the corresponding confidence:\nh_{ij}^{t+1} = \\eta (h_{ij}^{0} - h_{ij}) + f_{(T_t)} \\frac{R}{\\tau} (8)\nwhere hij is the h-value at the end of the previous trajectory, hij the initial h-value of the edge, and R scales the reinforcement of the edges. Confidence values are reinitialized at zero to start the next trajectory. This mechanism provides a built-in learning schedule such that the scale of the reinforcement signals grows progressively: rewards are initially small when trajectories are short, and they become larger when transitions are accurately captured in the model. During the deliberations, states are sampled from the prior ECM that did not receive the rewards yet, while the posterior ECM is updated with confidence and rewards at the end of the trajectory. It is equivalent to sampling states from the prior ECM, but updating the posterior ECM with the rewards R for all edges in the trajectory every time a prediction was verified by the observation in the environment. Metaphorically speaking, this mechanism is analogous to layers of snow accumulating in time on salient features. At the end of a trajectory, the snow is cleared away, bringing all salient points back at an equal level.\nTo complete the update of the FEPS agent, the policy is modified according to the EFE inferred from the new world model and can be adjusted to promote an explorative or a conservative behavior. In particular, h-values of an edge bi \u2192 aj are set to the expected free energy in Equation (4) with a world model conditioned on bi and aj. Each h-value directly carries the surprise expected from traversing the corresponding edge. As in [6], the policy is defined using a softmax function:\n\\pi(a_j|b_i) = \\text{softmax}(( \\zeta G_{b_i} [a_j]). (9)\nwhere ( is a (real-valued) scaling parameter and Gb, [aj] is the value of the EFE for action a; coming from state bi. In active inference, ( is typically negative. When it becomes more negative, actions associated with small EFE receive a large probability. More specifically, looking at the decomposition of the EFE in Equation (4), actions associated with larger entropies, that is lower certainty, together with higher chances of landing on preferable states or observations, become more attractive during the deliberation. In contrast, if the scaling parameter is positive, large EFE yield large probabilities in the policy, and actions with high certainty but also less chances of meeting preferences are more likely to be sampled. In this case, the agent implements a conservative policy that is confined to a known region of the environment at the expense of reaching the preferred states. When ( = 0, the policy is uniform, and the agent has no bias towards certainty nor utility."}, {"title": "Algorithmic features of the FEPS", "content": "The FEPS can be augmented with a number of techniques that take advantage of the world model and its inter- pretability. During learning, the internal model can be leveraged to identify transitions that are instrumental to gain information or to get closer to a preferred state. Furthermore, the performance of an agent in completing a task can be enhanced by evaluating the correct belief state accurately and quickly. Since the policy depends on the EFE, the preference distribution can be tuned according to the task: to seek information to complete the model, or to complete a given goal. Therefore, we propose to separate training into two phases, depending on how the preference distribution is constructed. We introduce a belief state estimation scheme that distributes belief states over multiple clone clips and eliminates those that are incompatible with new observations."}, {"title": "Leveraging preferences as a learning tool", "content": "So far, the preference distribution entering the EFE was not defined. One can optionally leverage it to define a goal in the environment, be it for the purpose of gaining information, or to solve an actual task. Therefore, we propose to separate learning into two tasks: model the environment and attain a goal in it. During the first phase, which we denote the wandering phase, the agent explores the environment without a prescribed goal. Instead, actions whose outcomes are expected to reduce prediction errors should be favorized. This phase spreads over multiple episodes and relies on interacting with the environment. In contrast to the wandering phase, the second phase is dedicated to learning to complete a given task. For this purpose, we designed an algorithm to infer a goal-oriented policy from the world model in a single step and without further information."}, {"title": "Seek information gain about the world model", "content": "Before designating any task bound to the environment as a preference, we investigate whether the preference distribution can be used to incentivize actions that minimize prediction errors, according to the current world model. This is directly related to the minimization of the VFE. Specifically, preferences should encourage the agent to seek transitions the world model associates with certainty or equivalently with high probabilities, irrespective of actions. Sequences of interaction steps with the environment guided by this preference distribution belong to the wandering phase. To reflect the preference for highly probable transitions in the world model regardless of the action chosen, the preference distribution is constructed as the marginal of the world model over actions:\npref(Bt+1, St+1|Bt) = \\sum_a \\pi(At = a|Bt) p(Bt+1|Bt, At = a) p(St+1|Bt+1) (10)\n= p(Bt+1, St+1|Bt). (11)\nPlugging this distribution into the expected free energy evaluated for an action a in Eq. (4) results in the following:\nG_{b_t} [a_t] = \\sum_{b_{t+1},S_{t+1}} p(b_{t+1}, S_{t+1}|b_t, a_t) \\log \\frac{p(b_{t+1}|b_t, a_t)}{\\sum_a \\pi(a|b_t)p(b_{t+1}|b_t,a)p(s_{t+1}|b_{t+1})} (12)\n= DKL [P(B_{t+1}|b_t, a_t)||p(B_{t+1}|b_t)] (13)\n= IG(B_{t+1}, A_t = a_t), (14)\nwhere IG(X, Y = y) = DKL[p(X|Y = y)||p(X)] is the information gain about the random variable X when the value y for the second random variable Y is known. The dependency on the observations dropped from the first to second line thanks to the constraints the clone structure imposes on the emission function. A complete derivation of this formula is provided in Appendix A.\nIf we follow the conventional formulation of active inference as in section 2, the agent should increase the probability of sampling an action that minimizes this EFE. Doing so during the wandering phase, the agent would therefore seek actions it estimates will yield the lowest information gain about belief states. As a result, the agent would stay in a region of the environment where it predicts it will receive the least surprise, according to its world model. This situation is sometimes referred to as the Dark Room problem [56]: an agent that adapts by minimizing its surprise about observations would act to stay in a dark room instead of using actions to explore other places that may be more surprising, but also more favorable to its survival, because all observations there would be predictable.\nThere is, however, an easy solution to this problem for FEPS. In order to avoid the dark room problem and to select actions that are expected to improve the model of the environment, the scaling parameter ( in Eq. (9) can be set to a positive value. In this case, the larger the EFE associated to an action and the estimated information"}, {"title": "Task-oriented behavior by inferring preferences on belief states", "content": "At the end of the wandering phase, a task is designated by encoding the associated targets with a high probability in the preference distribution. From there, the agent can plan, that is sample a sequence of actions to follow to achieve the goal. While the target is identified as a sensory state for the FEPS, transitions that are useful to reach it are deduced from the world model. In our framework, updating the policy takes a single step and does not require further interaction with the environment.\nThough active inference commonly determines the behavior by planning sequences of actions, it becomes ex- pensive for distant horizons Th. A sequence of Th actions must be chosen out of NT Vactions possible combinations, by evaluating the EFE in a space of (NB \u00d7 Ns)Th possible outcomes for each sequence. Methods such as habitual tree search or sophisticated inference have been developed to mitigate this scaling issue [19, 41]. Alternative approaches are presented in [6].\nInstead of planning by evaluating the generative distribution over all possible future sequences of outcomes, we propose to encode the long-term value of a state directly into the preference distribution. Our scheme is reminiscent of successor representation [57, 58, 59], in that it estimates a value function provided some expectations about occupancies of states in the future, either acquired by experience with reinforcement for example, or by inverting a learned transition function. In contrast to searching a tree of future sequences of actions, this method does not rely on mental time traveling [60], to the extent that agents do not simulate possible future scenarios. Instead, they are \"stuck in time\", and infer preferences in one go from stochastic quantities stored in the world model, in contrast to [20]. Our method also departs from sophisticated inference [41, 29]: instead of bootstrapping expected free energies in time, we bootstrap preferences over belief states and calculate the EFE only once.\nWe model the preference distribution to factorize over sensory and belief states [61], and we condition it on the current belief state bt:\npref(St+1 = s, Bt+1 = b|bt) = pref(s) pref(b|bt). (15)\nThe first part, pref(s), is an absolute preference distribution over sensory states, that is independent of where the agent believes it is in the world model. Since the environment is assumed to be partially observable, a target that is externally given can only be encoded in a shared state space, that is as a sensory state for the agent. More specifically, if s* is the target state for an observation, a probability pref(St+1 = s*) = p* is associated to it. All other observations are given a uniform distribution pref(St+1 \u2260 s*) = (1 \u2212p*)/(|S| \u2013 1). The second part pref(b|bt) then reflects look-ahead preferences over belief states and reflect how useful an agent estimates a transition to be in order to satisfy its absolute preferences. In other words, the utility of belief states over longer horizons is inferred from the value associated with the observations they can transition to. This way, even if the goal might be far away in the world model, the preference for the target observation propagates to intermediate belief states that contribute to reaching it. Metaphorically speaking, the preference for a target observation propagates to the belief states that are useful to get to that target observation. For example, consider an animal in a maze. The target is manifested with a high preference towards observing food. However, the preference distribution over the locations does not indicate how to reach the food. To remedy this, the agent infers the value of belief states in order to reach the target: if a transition to some location brings the animal closer to the food, it is assigned a higher probability in the preference distribution. This way, the preference distribution highlights the path of relevant actions to the target.\nWe propose a heuristic algorithm to estimate the look-ahead preference distribution pref(Bt+1|bt) over transitions in the world model. The algorithm can update the policy Kpref times if needed, to refine the preference distribution. The initial policy \u03c0(0) (At|Bt) is uniform. During each update k, two quantities are calculated: (1) the look-ahead preference distribution results from the value of each transition, that estimates how useful a transition is to reach a target within a prediction horizon Th, and (2) the policy \u03c0(k+1) is calculated with Equation (9) and the latest preference distribution.\nTo initialize each update step k, the policy \u03c0(k) is used together with the world model to evaluate how easy a belief state can be reached from the current state, in a distribution we denote reachability:\nr^{(k)} (b_{t+1}/b_t) = \\sum_a p(b_{t+1}|b_t, \\alpha) \\pi^{(k)} (a|b_t). (16)"}, {"title": "Delineate belief states for the same observation", "content": "In spite of prescribing a method to sample actions, active inference does not include techniques to efficiently choose belief states when multiple of them could explain the current observation. In particular, models involving neural networks lack the interpretability to design suitable belief state selection methods. Therefore, letting the agent learn a world model with a clone-structured HMM has advantages beyond planning. We propose a technique to evaluate belief states in superposition, depicted in Fig. 2. When placed in an environment and receiving its first observation, the agent makes an initial hypothesis about its belief state by distributing an excitation to any clone clip compatible with the observation, as shown in Figure 1c. At each step, an action is sampled from the policy for each excited clone clip. The resulting frequencies define a new distribution, from which an action is sampled before it is applied to the environment. For each compatible clone clip, the agent samples a new clip to represent the belief state it anticipates it would transition to if the clone clip under consideration stands for the correct belief state. The excitation jumps onto the new clip. After applying the action to the environment and receiving the resulting sensory signal, the agent takes away the excitation on any clip that does not match the current observation. Depending on the structure of the environment and the number of clones for each observation, the agent progressively narrows down its candidate belief states to a single clone clip, in spite of the initial uncertainty. In the event that the world model is imperfect and the agent has eliminated excitations on all clips, it starts its hypothesis over, and considers all the clone clips of its current, unpredicted observation as candidate belief states. This mechanism allows the agent to evolve in environments with ambiguous observations in the absence of more contextual information about its initial conditions."}, {"title": "Numerical results", "content": "In this section, we present a numerical analysis of the model on environments inspired from behavioral psychology tasks, namely a timed response experiment in a Skinner box and a navigation task to forage for food. The parameters"}, {"title": "The timed response task", "content": ""}, {"title": "Learn short-term associations", "content": "The timed response task is a minimal environment for an agent to learn to contextualize its observations with past states and actions when the sensory signals emitted by the environment are ambiguous. This environment simulates an animal standing in front of a door that can be opened with a lever. The goal is for the agent to learn a conditioned response and to press a lever at the right time in order to access food. The environment's MDP is depicted in figure 3. For this task, the observations combine two sensory inputs: S = { (light off, hungry), (light off, satiated), (light on, hungry)}. Since food can be consumed only when the light is off, the observation (light on, satiated) in excluded from the set. The actions are A = {wait, press the lever}. The environment is initialized in the (hidden) state Eo, that emits observation (light off, hungry). From there, the light turns on, regardless of the action taken by the agent. Once the light has turned on, the agent must learn to wait one step before pressing the lever. If it does so too early, it gets back to the initial state. If the agent activates the lever on time, the environment transitions to state E, where the light is off, but the agent is satiated.\nFor the simulations, we give each observation two clones. It makes it possible to accommodate enough candidate belief states to model up to two ambiguous hidden states that emit the same sensory signal, enabling the agent to adapt its policy to a one-step waiting time between the light turning on and the food being accessible for example. When observations are not ambiguous and can be emitted only by a single hidden state in the environment, some belief states become redundant. This redundancy can make the training more challenging to the extent that the agent has to find a convention and adapt its model accordingly before it can account for all transitions in the environment faithfully. The larger the waiting time n, the more clones might be necessary to learn. We train 100 agents for 4000 episodes of 80 steps in this environment and we test two scenarios. In the first, the agent is directly given the preference for the target observation (light off, satiated). The EFE is then scaled with a scaling parameter of $ = -1 in the policy in Eq. (9). In the second scenario, we test whether the agent can learn more efficiently if it wanders aimlessly through the environment for the same number of episodes without preference for the target before adapting its policy to the task, with a scaling parameter of 0."}, {"title": "Simulation results", "content": "The timed response environment is a minimal testbed for the FEPS, where some hidden states are not uniquely defined by the observation they emit, but also by the recent past. The agent must learn two types of belief states. While clones for (light off, hungry) and (light off, satiated) only support transitions that are independent of the actions the agent takes, the clones for (light on, hungry) must appropriately use information about the previous observation and action to be contextualized and distinguished. Some results are reported in figure 4.\nDuring training, regardless of the strategy that the agent uses to resolve the environment, all agents follow a similar learning pattern. The acquisition of the world model happens in stages, where the transition from one to the next is manifested in a steep increase in the length of the trajectories of correct predictions, or equivalently, as a steep decrease in the free energy, as in figure 4a. First, agents quickly eliminate transitions that are impossible in the environment, leading to an initial drop in the VFE. For example, as in figure 3, a direct transition from (light off, hungry) to (light off, satiated) is incompatible with the timed response task. During the second phase, the number of rewards the agent can collect is limited by the absence of convention on the context-sensitive belief states. Therefore, a plateau is observed on the VFE. A final sharp decrease in the VFE signals the adoption of a convention between clones to accurately disentangle ambiguous hidden states that cannot be told apart with observations alone. The EFE evolves as expected during the training: it decreases for the task-oriented agents, while it rapidly plateaus to its limit (see Appendix B) in with a wandering phase. However, in contrast to the VFE, the convergence of the EFE to its asymptote value does not reflect the fact that the model is good enough to make accurate predictions.\nFor most agents, training with or without a preference for the target in order to construct the world model does not influence the final model. As in figure 4a, the two best agents, that is, those that converge to free energy values below 1 the earliest, share similar learning curves. The averaged behaviors remain fairly identical, except for the length of the second stage of the learning phase, where the agents have yet to adopt a successful convention. In particular, in the absence of aimless experimentation with the environment, this second phase can last longer, such"}, {"title": "Navigation task in a partially observable grid", "content": ""}, {"title": "Long-term planning in an ambiguous environment with symmetry", "content": "The FEPS is further challenged in a larger navigation task, where observations are shared among hidden states, and multiple sequences of actions can emit the same observations, due to the symmetry of the environment. In order to disentangle the hidden states, the agent must use long-term information about its past observations and actions to contextualize its current state in a way that is consistent across actions. In this environment, food is hidden in one position in a grid. Locations in a 3 by 3 grid world are associated with smell intensities, denoted by their integer intensities from 0 to 3 from the lower left to the upper right corners, according to their closeness to the food, and represented by increasingly warm colors in figure 6a.\nThe world"}]}