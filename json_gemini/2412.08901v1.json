{"title": "Radiology Report Generation via Multi-objective Preference Optimization", "authors": ["Ting Xiao", "Lei Shi", "Peng Liu", "Zhe Wang", "Chenjia Bai"], "abstract": "Automatic Radiology Report Generation (RRG) is an impor-tant topic for alleviating the substantial workload of radiolo-gists. Existing RRG approaches rely on supervised regression based on different architectures or additional knowledge in-jection, while the generated report may not align optimally with radiologists' preferences. Especially, since the prefer-ences of radiologists are inherently heterogeneous and multi-dimensional, e.g., some may prioritize report fluency, while others emphasize clinical accuracy. To address this problem, we propose a new RRG method via Multi-objective Pref-erence Optimization (MPO) to align the pre-trained RRG model with multiple human preferences, which can be formu-lated by multi-dimensional reward functions and optimized by multi-objective reinforcement learning (RL). Specifically, we use a preference vector to represent the weight of pref-erences and use it as a condition for the RRG model. Then, a linearly weighed reward is obtained via a dot product be-tween the preference vector and multi-dimensional reward. Next, the RRG model is optimized to align with the prefer-ence vector by optimizing such a reward via RL. In the train-ing stage, we randomly sample diverse preference vectors from the preference space and align the model by optimizing the weighted multi-objective rewards, which leads to an op-timal policy on the entire preference space. When inference, our model can generate reports aligned with specific prefer-ences without further fine-tuning. Extensive experiments on two public datasets show the proposed method can generate reports that cater to different preferences in a single model and achieve state-of-the-art performance.", "sections": [{"title": "Introduction", "content": "Radiology reports provide an important basis for physicians to make diagnoses and are usually written by experienced ra-diologists based on the syndromes observed in medical im-ages. However, manually writing reports is labor-intensive, time-consuming, and error-prone due to the large amount of daily medical reports and variations in experience among ra-diologists. Therefore, automatic radiology report generation (RRG) relying on deep learning techniques as an alternative has emerged as an attractive research topic in both artificial intelligence and medicine.\nTo generate high-quality reports, a series of approaches (Chen et al. 2020; Liu et al. 2024a) have been proposed. They usually adopt the classic encoder-decoder structure and generate better text descriptions by promoting the model's structure, such as introducing a cross-modal mem-ory module (Chen et al. 2021; Shen et al. 2024), a multi-modal alignment module (Yang et al. 2023), or new attention mechanisms (Song et al. 2022). Another line of approaches improves report generation by injecting additional knowl-edge, such as knowledge graphs (Yang et al. 2022; Kale et al. 2023; Huang, Zhang, and Zhang 2023) or disease tags (You et al. 2021; Wang et al. 2022; Jin et al. 2024), or retrieved reports from other corpora (Liu et al. 2024b).\nThe above RRG methods rely on supervised regression based on different architectures or additional knowledge injection. However, the generated reports may not align optimally with radiologists' preferences. Some approaches (Qin and Song 2022; Delbrouck et al. 2022) integrate re-inforcement learning (RL) into report generation to maxi-mize rewards that assess report quality. These RL-based ap-proaches generally use a single evaluation metric or a simple weighted sum of multiple metrics to formulate the reward function. This function typically represents a weighted ag-gregation of multiple objectives, tailored to a specific prefer-ence. Nonetheless, preferences are inherently heterogeneous and multi-dimensional. For instance, some radiologists may prioritize the fluency of the text, while others may focus on clinical accuracy. Therefore, RRG models must cater to the diverse preferences of different radiologists, a challenge that is difficult to achieve with a single model and remains unex-plored in previous works.\nIn this paper, we study this unexplored topic for the first time and propose a new RRG method to align the pre-trained RRG model with multiple human preferences via Multi-objective Preference Optimization (MPO). The key challenge is how to utilize a low-dimensional prefer-ence vector to control the model's behavior. To address this challenge, MPO formalizes the preferences as a multi-dimensional vector and uses it as a condition for the RRG model, which can be formulated by multi-dimensional re-ward functions and optimized by multi-objective reinforce-ment learning. Specifically, our MPO introduces two new modules: the preference vector fusion (PVF) network and the multi-objective optimization (MOO) module. The PVF"}, {"title": "Related Works", "content": "Recent advancements in RRG can be classified into two main strategies: improving model architectures and inject-ing additional knowledge.\nEarly RRG studies are implemented on CNN-RNN or CNN-LSTM architectures (Wang et al. 2020; Najdenkoska et al. 2021). Recent works have focused on improving the network based on the CNN-Transformer architecture. For instance, both CMN (Chen et al. 2021) and MAN (Shen et al. 2024) introduce a cross-modal memory network featuring a shared memory that records the alignment information between images and text. XPRONET (Wang, Bhalerao, and He 2022) proposes a cross-modal prototype-driven network to enhance cross-modal pattern learning. METrans (Wang et al. 2023) in-troduces multiple learnable \u201cexpert\u201d tokens into both the Transformer encoder and decoder, enhancing image repre-sentation and report generation. To capture subtle differ-ences in radiology images, various attention mechanisms have been developed to enhance feature robustness in report generation, such as the co-attention mechanism (Jing, Xie, and Xing 2018). Additionally, Ali-Transformer (You et al. 2021) introduces cross-view attention, using single-view\nThis type of work ex-plores the injection of additional knowledge, such as disease labels (Tanida et al. 2023; Jin et al. 2024), retrieved reports (Liu et al. 2024b), and knowledge graphs (Kale et al. 2023; Huang, Zhang, and Zhang 2023), to assist in report gener-ation. PPKED (Liu et al. 2021a) combines abnormal find-ings, knowledge graphs, and retrieved reports to mimic the working patterns of radiologists. KiUT (Huang, Zhang, and Zhang 2023) enhances results by integrating visual and con-textual knowledge with external clinical insights through an injected knowledge distiller. RGRG (Tanida et al. 2023) uses an object detector as region guidance for report generation, allowing the decoder to directly utilize disease information. Jin et al. (2024) generates high-quality reports by convert-ing diagnostic results from a disease classification branch and combining them with retrieved reports.\nThe aforementioned RRG methods are primarily single-stage and rely on supervised regression. Recently, reinforce-ment learning (RL) has been integrated into RRG tasks, fur-ther enhancing report-generation capabilities through appro-priate supervision from carefully designed rewards. These rewards are typically calculated based on natural language generation (NLG) metrics (Wang et al. 2021, 2022) or se-mantic relevance metrics (Delbrouck et al. 2022). For ex-ample, CMN+RL (Qin and Song 2022) leverages signal from NLG metrics, such as BLEU, as a reward to guide the cross-modal mappings between image and text features. Li et al. (2018) design a hierarchical decision-making pol-icy guided by sentence-level and word-level rewards to de-termine whether to retrieve a template sentence or gener-ate a new one. Miura et al. (2021) design two new re-wards, namely, the exact entity matching reward and the entity matching reward, based on entity coverage and rela-tionships. These rewards are combined with NLG metric re-wards to improve the factual completeness and consistency of generated reports. Similarly, Delbrouck et al. (2022) em-ploy entities and relationships extracted by PubMedBERT (Gu et al. 2021) to compute three rewards to enhance the fac-tual completeness and correctness of report generation. Wu, Huang, and Huang (2023) introduce an imbalanced evalu-ation reward that divides performance differences into four groups based on token frequency, balancing performance to ensure that tokens from these groups are given equal weight. These RL-based report generation methods typically em-ploy either a single evaluation metric or a straightforward weighted sum of multiple metrics as the reward function. Such a reward function represents a weighted aggregation of"}, {"title": "Methods", "content": "We aim to train a model that can generate reports aligned with human preferences. As shown in Figure 1, our MPO builds upon the mainstream encoder-decoder framework and comprises two novel modules: the encoder-decoder with our preference vector fusion (PVF) network, and the multi-objective optimization (MOO) module. In our frame-work, RRG is also formulated as a sequence-to-sequence task, where, given a radiology image $I$, the Encoder Lay-ers encode the visual features extracted by the visual ex-tractor and the Decoder Layers generate report $Y = [Y_1, Y_2, ..., Y_t, ..., Y_T]$ conditioned on both visual features and preference vector $p$. Here, $y_t \\in V$ is the generated to-kens at step $t$, $T$ is the length of the report, and $V$ represents the vocabulary space. The entire generation process can be formulated as follows:\n$p(Y | I, p) = \\prod_{t=1}^{T} p(Y_t | Y_1, ..., Y_{t-1}, I, p)$.\nFirstly, we extract visual features $X$ from radiology im-age $I$ via a pre-trained CNN, specifically ResNet 101 (He et al. 2016). Then, decompose the image into equally sized patches, with the patch features being extracted from the last convolutional layer of the CNN. These patch features are then expanded into a sequence by concatenating them row-by-row, which is formulated as follows:\n$X = [X_1, X_2,..., X_s, ..., X_S] = f_v(I)$,\nwhere $f$ is the visual extractor, $x_s \\in R^d$ is a patch feature, $d$ is the feature dimension, and $S$ represents the patch number. Then, the extracted visual features $X$ and text embedding $[y_1, y_2,..., y_{t-1}]$ are fed into the encoder-decoder module. In this module, the encoder processes the visual features $X$, which are then fused with a multi-dimensional preference vector $p$ through the PVF network. These fused features are then combined with the text embeddings and input into the decoder to generate the next token. The MOO module calculates rewards based on multiple evaluation metrics and weights these rewards using preference vectors to formulate a multi-objective reward function. An RL algorithm is then applied to optimize this function, thereby generating reports that align with the specified preferences. Next, we will de-tail our proposed PVF and MOO modules together with a two-stage training procedure."}, {"title": "Encoder-Decoder with the PVF Network", "content": "The visual features and text embedding are utilized as in-puts of the encoder and decoder, respectively. First, the vi-sual features $X$ are fed into the encoder by Eq. (3) to obtain the encoded visual features $E$,\n$E = [e_1, e_2..., e_S] = f_e(X)$,\nwhere $f_e$ is the Encoder Layers in the standard Transformer. Then the encoded visual features $E$ are fed into the PVF network to fuse with the preference vector $p$.\nThe key challenge of our MPO approach is how to align the RRG model with multiple preferences. To ad-dress this challenge, we formalize the preferences as multi-dimensional vectors and use them as a condition for the RRG model. Now the question is how to incorporate the prefer-ence vector into the report generation model. Inspired by the large language model, the radiology image can be viewed as the prompt of the report generation model, and the pref-erence vector is the conditional input. Therefore, we design a preference vector fusion (PVF) network to integrate the preference vector with encoded visual features.\nSpecifically, given an m-dimensional preference vector $P = (P_1,P_2,...,P_m)$, where $p_i \\in A^{m-1}$ represents the preference simplex, $p_i \\geq 0$ represents the weight for pref-erence dimension i, and $\\sum_{i=1}^{m} P_i = 1$. During training, a preference vector is randomly sampled from the preference simplex and undergoes linear mapping and dimension ex-pansion to ensure the processed preference vector $P$ has the same dimension as the encoded visual features $E$, where $P = Expand(Linear(p))$. The core of the PVF network is a fusion module based on a multi-head attention mechanism and a residual connection, where the expanded preference vector serves as the query, while the encoded visual features serve as both the key and value. For each head $j$, the fusion process follows the same procedure described as follows:\n$H = Softmax(\\frac{P \\cdot E^T}{\\sqrt{d_k}}) \\cdot E$.\nThen, the attention feature $H$ is obtained by concatenating $H_j$ from all heads (the number of heads is 8 in our method). Finally, the attention feature is fused with the encoded vi-sual feature through residual connection to obtain the fused feature $U$, which is expressed as follows:\n$U = E + \\alpha H$,\nwhere $\\alpha$ is a scaling factor. Then the fused features along with the text embedding $[y_1, y_2,\u2026\u2026\u2026, y_{t-1}]$ from previous steps, serve as the input of the Decoder Layers to generate the current output $y_t$ by Eq. (6),\n$Y_t = f_d(U, y_1, Y_2, ..., Y_{t-1})$,\nwhere $f_d$ is the Decoder Layers. Given the entire predicted report sequence $[Y_1, Y_2,\u2026\u2026\u2026, Y_T]$ and the associated ground truth report $[w_1, w_2, ..., w_T]$, the basic generation loss is:\n$L_g = -\\sum_{t=1}^{T}w_t.log(y_t).$"}, {"title": "Multi-objective Optimization via RL", "content": "The PVF network merely integrates the preference vector into the report generation network, which is not yet aligned with the preferences and therefore cannot control the be-havior of the model. To align with multiple preferences, we propose the Multi-Objective Optimization (MOO) module, which includes a multi-dimensional reward function opti-mized by multi-objective RL. The MOO module uses the preference vector to represent the weight of each preference and linearly combines the multi-dimensional reward with the preference vector. Subsequently, a policy gradient is em-ployed to optimize this weighted reward function, thereby guiding the model to better align with different preferences.\nWe conceptualize the report generation model as an agent interacting with external environment, which includes vi-sual and textual features, along with a preference vector $p$. All model parameters, denoted by $\\theta$, define a policy $\\pi_\\theta$ that guides the action-predicting the next token, i.e., $p(y_t | Y_{1:t}, I, p)$. For each report, once the end-of-sequence token is generated, the agent receives several rewards $r$ based on improvements in multiple evaluation metrics. Some of these evaluation metrics focus on the accuracy of text description, such as BLEU{1-4}, ROUGE-L, and some focus on clinical efficacy, such as F1 and Recall of disease labels. The num-ber of rewards corresponds to the dimensions of the prefer-ence vector. For the $i^{th}$ evaluation metric, the reward for the action at step $t$ is determined by the improvement in gener-ating the next token $y_t$ on that evaluation metric, which is formulated as follows:\n$r^i_t = r^i(Y_{1:t}) - r^i(Y_{1:t-1})$,\nwhere $i \\in \\{1, 2, ..., m\\}$, $Y_{1:t} = [Y_1, Y_2,..., Y_t]$ and $Y_{1:t-1} = [Y_1, Y_2, ..., Y_{t-1}]$. Therefore, at step t, the multi-objective reward function weighted by the preference vector is calculated as follows:\n$R_t = \\sum_{i=1}^{m} P_i r^i_t = \\sum_{i=1}^{m} P_i (r^i(Y_{1:t}) - r^i(Y_{1:t-1}))$.\nTherefore, the total reward R of generating a report Y = $[Y_1, Y_2,. ., Y_T]$ is the sum of $R_t$:\n$R = \\sum_{t=1}^{T} R_t = \\sum_{t=1}^{T} \\sum_{i=1}^{m} P_i (r^i(Y_{1:t}) - r^i(Y_{1:t-1}))$\n$\\qquad \\qquad= \\sum_{i=1}^{m} \\sum_{t=1}^{T} P_i (r^i(Y_{1:t}) - r^i(Y_{1:t-1})) = \\sum_{i=1}^{m} P_i r^i (Y)$,\nwhere $R$ represents the multi-objective rewards obtained from the sampling strategy. Then the model is trained to maximize the expected reward $E_{Y \\sim \\pi_\\theta} [\\sum_{i=1}^{m} P_i r^i (Y)]$ from the generated report Y via a sampling strategy. The loss function is now defined as follows:\n$L(\\theta) = - E_{Y \\sim \\pi_\\theta} [\\sum_{i=1}^{m} P_i r^i (Y)]$.\nThe gradient of the loss function $L(\\theta)$ is computed using the REINFORCE algorithm (Williams 1992) as follows:\n$\\nabla_{\\theta} L(\\theta) = - E_{Y \\sim \\pi_\\theta} [(\\sum_{i=1}^{m} P_i r^i (Y)) \\nabla_{\\theta} log \\pi_{\\theta} (Y)]$.\nTo stabilize the RL training process, we follow Rennie et al. (2017) and introduce a reference reward b, where b is a"}, {"title": "Training", "content": "MPO comprises two training stages: the first employs the primary generation loss in Eq. (7) to regularize the action space, and the second involves multi-objective optimization based on Eq. (14). During training, we randomly sample di-verse preference vectors from the preference space and re-fine the RRG model by optimizing weighted multi-objective rewards. This strategy achieves an optimal policy across the entire preference space, ensuring that the optimal model adapts to various preference conditions. During inference, given a preference vector, our model can generate reports that cater to this preference without further fine-tuning."}, {"title": "Experiments", "content": "We evaluate our method on two public datasets, IU-Xray (Demner-Fushman et al. 2016) and MIMIC-CXR (Johnson et al. 2019). IU-Xray consists of 7,470 chest X-ray images, accompanied by 3,955 reports. Following CMN (Chen et al. 2021), only the findings and impressions are included. Datasets are randomly split into 7:1:2 for train, val, and test. MIMIC-CXR, the largest publicly available dataset for RRG, contains 337,110 chest X-ray images and 227,835 corresponding reports. We adhere to the official dataset splits to ensure a fair comparison.\nWe utilize the widely used Natural Language Generation (NLG) metrics including BLEU{1-4} (Papineni et al. 2002), METOR, and ROUGE-L (Lin 2004) to assess the quality of the generated text reports. Addition-ally, to evaluate Clinical Efficacy (CE), we use CheXbert (Smit et al. 2020) to annotate labels for 14 observations in medical reports. Precision (P), recall (R), and F1 scores are then calculated based on the labels according to the label be-tween the reference and generated reports. Higher values in both NLG and CE metrics indicate better performance.\nOur method was implemented in PyTorch and trained on an NVIDIA 4090 GPU with 24GB of memory. We employ ResNet101, pre-trained on ImageNet, as the visual extractor, and a randomly initialized Transformer (Vaswani et al. 2017), as the encoder-decoder. The initial learning rate for ResNet101 and remaining net-works are 1 \u00d7 10-6 and 1 \u00d7 10\u22125, respectively. We use the Adam optimizer for training and include a beam search of"}, {"title": "Effective of Preference Guidance", "content": "To verify the effectiveness of preference guidance, we test the model trained in a two-dimensional preference vec-tor space. (The results of the three-dimensional preference space are detailed in the supplementary material.)\nQualitative results. To further verify the effectiveness of preference guidance intuitively, we perform qualitative anal-ysis on the MIMIC-CXR dataset."}, {"title": "Results and Analyses", "content": "Ablation studies are performed on both IU-Xray and MIMIC-CXR datasets to explore the impact of each component in our MPO method. Three variants were investigated: (1) \u201cBase\u201d represents our baseline, R2Gen (Chen et al. 2020). (2) \u201cBase+MOO\u201d combines the baseline with our MOO module. It uses a preference vector to weigh multiple rewards and optimizes it via multi-objective RL. (3) \"Base+MOO+PVF\" represents our full model, which in-cludes both the PVF and MOO modules.\nIn addition to PVF, we explore three other fusion methods to fuse preference vectors. The first concatenates (Concat) the expanded preference vector with the encoded visual features and then applies a linear trans-formation to obtain the fused features. The second and third methods align the dimension of the preference vector with the encoded visual features through linear transformation and dimensionality expansion, followed by either addition (Add) or dot multiplication (Mul), respectively. As shown in Table 6, our PVF network outperforms other methods across all evaluation metrics. The Concat, Add, and Mul methods perform poorly because they may not capture the complex relationships between preference vectors and encoded visual features through simple fusion. In contrast, the PVF network leverages the attention mechanism to align the preference vector with key information in the visual features, resulting in more effective fusion and superior performance."}, {"title": "Hyperparameter Analysis", "content": "Hyperparameter analysis of \u03b1 is conducted on the IU-Xray dataset. The candidate set for \u03b1 includes {0.1, 0.5, 1, 3, 5, 10}. The experimental results in Table 7, indicate that the value of \u03b1 within a reasonable range does not significantly affect the results. The best per-formance is achieved when \u03b1 = 3."}, {"title": "Conclusion", "content": "In this study, we propose a novel method for automatic RRG via Multi-objective Preference Optimization (MPO), which effectively addresses the inherent heterogeneity and multi-dimensionality of radiologists' preferences. We pro-pose two innovative modules: the preference vector fusion network and the multi-objective optimization module to en-able conditional generation and effective preference align-ment. By employing preference vectors to condition the RRG model and optimizing the preference-weighted multi-dimensional reward function through reinforcement learn-ing, our model derives an optimal policy over the entire preference space, resulting in a model with high adaptability to different preferences. Through extensive experiments on two public datasets, we have demonstrated that our method can cater to various preferences within a single model and achieve state-of-the-art performance. In future work, we will explore designing new evaluation metrics as reward func-tions for preference alignment."}, {"title": "Supplementary Material for \u201cRadiology Report Generation via Multi-objective Preference Optimization\"", "content": "To further verify the effectiveness of preference-guided report generation, we test the model trained in a three-dimensional preference vector space."}, {"title": "Effective of Preference Guidance", "content": "Similar observations can be made on the MIMIC-CXR dataset, where the trends of Precision (P) and Recall (R) are consistent with the F1 score, as F1 is calculated based on precision and recall. Additionally, when the preference weights for each metric are equal, it indicates that each evaluation metric is equally important. In this scenario, the model achieves the second-best result across all metrics, re-sulting in an average model with equal preference.\nWe add additional reports generated with p = {[0.5, 0.5], [0.4, 0.6], [0.6, 0.4]}, as shown in Figure 1.The evaluation metrics show a consistent trend with their corre-sponding preference values. From the NLG view, all gener-ated reports maintain grammatically correct structures, but R3 closely mimics the GT structure by using terminology like \"were obtained\". From the CE view, R1 identifies \"mild cardiomegaly\" and \"mild congestion\u201d, aligning well with GT's mentions, while R2 and R3 do not mention pulmonary \"congestion\". Thus, R1 shows better CE, while R3 has better NLG."}]}