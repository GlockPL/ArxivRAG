{"title": "Differentially Private Learning Needs Better Model Initialization and Self-Distillation", "authors": ["Ivoline C. Ngong", "Joseph P. Near", "Niloofar Mireshghallah"], "abstract": "Differentially private SGD (DPSGD) enables privacy-preserving training of language models, but often reduces utility, diversity, and linguistic quality. We introduce DPRefine, a three-phase method that initializes a model using data synthesis from a small pre-trained LM with rigorous filtering, applies DP finetuning on private data, and performs self-distillation to refine outputs. This approach significantly outperforms vanilla DPSGD, with AlpacaEval preferring DPRefine's generations in 78.4% of cases across all datasets. Our analysis reveals that DPRefine reduces linguistic errors in generated text by 84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD. It also reduces inconsistencies of non-private models, such as hallucinated details and misattributed quotes. We find that small models like GPT-2 can be effective for initialization and distillation, highlighting their potential in enabling scalable and efficient deployment of privacy-preserving language.", "sections": [{"title": "1 Introduction", "content": "Training machine learning models on private data offers significant potential for enhancing performance on domain-specific tasks, particularly in natural language processing (Li et al., 2021; Yu et al., 2021; Liu et al., 2023; Cummings et al., 2023). However, the use of sensitive information raises critical privacy concerns. Differentially Private Stochastic Gradient Descent (DPSGD) has emerged as a prominent technique to bound information leakage during model training by clipping gradients and adding calibrated noise during optimization (Abadi et al., 2016). While DPSGD provides strong privacy guarantees, the introduced noise and gradient modifications lead to significant challenges: decreased model utility (Yu et al., 2021; Ponomareva et al., 2022), less diverse text generation due to distribution smoothing (Bagdasaryan et al., 2019; Mireshghallah et al., 2022b), and notably, as our analysis reveals, increased linguistic errors in generated text (Table 1). A common scenario in practice involves domain-specific tasks with limited private labeled data, where organizations aim to leverage existing pre-trained language models while preserving privacy. Simply applying DPSGD to fine-tune these models on private data often yields poor results, particularly when the private dataset is small (Tram\u00e8r et al., 2022; Mireshghallah et al., 2021). Recent work has shown that leveraging better hand-crafted features (Tramer and Boneh, 2020) or features from large pre-trained language models (Li et al., 2022, 2021) can improve the privacy-utility trade-off in differentially private learning. However, these approaches have limitations: smaller pre-trained models offer limited benefits, and fine-tuning larger models on private data may be infeasible due to proprietary concerns or infrastructure limitations. This raises a critical question: Can we develop small, domain-specific language models that achieve high performance without requiring large private datasets or large, pre-trained models?\nIn this paper we introduce DPRefine, building on the intuition that model performance under differential privacy heavily depends on model initialization and feature representation (Tramer and Boneh, 2020; Li et al., 2022). Additionally, recent advances in NLP have demonstrated the effectiveness of data synthesis (Liu et al., 2024; Flemings and Annavaram, 2024), filtering and self-distillation in improving the performance of smaller models(Jung et al., 2023; Bansal et al., 2024). Our method has three phases, depicted in Figure 1: In the first phase we create a strong initialization by generating high-quality synthetic data using a small pre-trained language model (e.g., GPT-2) with rigorous filtering - importantly, this phase operates entirely independently of any private data. In the second phase we apply DPSGD to fine-tune this initialized model on the private labeled data, representing the only stage where private data is accessed.\nFinally, we apply self-distillation where the DP model generates new training data for further refinement, again without accessing the original private data. We evaluate DPRefine on three datasets across two domain-specific tasks: summarization, XSum (Narayan et al., 2018) and PubMed (National Library of Medicine, 2024), and paraphrasing, MRPC (Dolan and Brockett, 2005). Our experiments demonstrate that DPRefine significantly outperforms vanilla DPSGD, with AlpacaEval (Li et al., 2023) preferring DPRefine's generations in 78.38% of cases across all datasets and metrics.\nBeyond standard metrics, we conduct a fine-grained manual error analysis of the generated text, constructing a taxonomy of both linguistic errors (e.g., grammar"}, {"title": "2 Preliminaries", "content": "Differential Privacy: Differential privacy (DP) (Dwork et al., 2006, 2014) is a formal privacy definition that provides strong guarantees by limiting the influence any single data point can have on the output of an algorithm. Formally, a randomized mechanism M satisfies (\u20ac, \u03b4)-DP if for any 2 neighboring datasets D, D' \u2208 D that differ in exactly one data sample, and for all sets of outcomes S, Pr[M(D) \u2208 S] < e^\u20ac Pr[M(D') \u2208 S] + \u03b4. To train DP LLMs, Differentially Private Stochastic Gradient Descent (DPSGD) (Abadi et al., 2016) is typically used. DPSGD computes gradients for individual data points, clips each individual gradient, and adds Gaussian noise to the average of clipped gradients to ensure (\u03b5, \u03b4)-DP.\nKnowledge Distillation: In Knowledge Distillation (KD), a small student model is trained to replicate the behavior of a larger teacher model. The student model learns by imitating the outputs of the teacher model. Our approach utilizes a variant known as Impossible Distillation (Jung et al., 2023) which leverages smaller, pre-trained models to generate high-quality training data."}, {"title": "3 Proposed Method", "content": "We present DPRefine, depicted in Figure 1, a three-phase method designed to enhance the linguistic quality of differentially private large language models for domain-specific tasks like summarization and paraphrasing. Our method integrates data synthesis, differentially private fine-tuning, and self-distillation to produce high-quality outputs while maintaining strong privacy guarantees. Our method integrates three key phases: data synthesis, differentially private fine-tuning, and self-distillation. In the first phase, data synthesis provides better initialization and richer feature representations, allowing the model to learn key patterns without privacy concerns. In the second phase, differentially private fine-tuning preserves these learned features while ensuring privacy, adding noise selectively to maintain model robustness. Finally, in the third phase, self-distillation combined with careful filtering further refines the model's outputs, correcting privacy-induced errors and boosting linguistic quality, all while maintaining strong privacy guarantees."}, {"title": "3.1 Phase 1: Data Synthesis and Model Initialization", "content": "As depicted in Figure 1 (left section), DPRefine begins by generating high-quality input-output pairs {(a, b), (a, c), ..., (x, y)} using a small pre-trained language model (e.g GPT-2 for general tasks or BioGPT for biomedical tasks). We choose a smaller model because it offers a balance between efficiency and quality, allowing for fast generation of synthetic data without the computational overhead of larger models. This synthetic data serves as the foundation for fine-tuning the base T5-large model, Mbase.\nWe begin by generating a context c based on a domain-specific prefix. The prefix ensures alignment with the target domain and can either be generated by a language model or sourced from a human-written corpus to ensure meaningful data. For example, using the prefix \"NYC (Reuters) \u2013\" for the news domain, the model generates a context; c = \"The mayor announced a new climate initiative.\" Multiple sentence completions {a, b, c, ...} are then generated based on the context c. (a) -\"The initiative focuses on creating new parks, reducing emissions, and implementing stricter environmental regulations to combat climate change.\", (b)- \"The plan includes new parks and emission controls.\", (c)-\"New parks and stricter emission laws are planned.\"...\nThese generated completions {a, b, c, ...} are then paired to form input-output pairs (x, y), where x is an input and y is the corresponding output. For instance, potential pairs could be\n(x,y) = (a, b), (a, c), (b, c)\nproviding a diverse set of synthetic input-output pairs for training.\nTo ensure high quality and meaningful pairs, we apply the following filters:\n1. Entailment Filtering: Using a pre-trained NLI model (Liu et al., 2022a), we ensure that the generated pair (x, y) holds logical entailment in both directions, meaning x \u2192 y and y \u2192 x.\n2. Length Filtering: Ensure the length of the response y is appropriate for the input x. For summarization, y should be shorter than x, y < |x|; for paraphrasing, x and y should have similar length, |x| \u2248 |y|.\n3. Diversity Filtering: Remove pairs that are too similar to each other to ensure a diverse dataset. Pairs (X1,Y1) and (x2, y2) are duplicates if one pair entails the other: x1 \u2192 x2 and Y1 \u2192 Y2.\n4. Grammar Filtering: Apply the language-tool-python library (lan, 2024) to check for grammatical correctness in both x and y. Any pairs with significant grammatical errors are removed.\n5. Numerical Consistency Filtering: For pairs containing numerical data, ensure that numbers appearing in x are consistent with those in y, ensuring no significant deviations between the input and output.\n6. Redundancy Filtering: Remove pairs where more than 30% of the tokens in y are repeated from x, ensuring minimal redundancy in the generated text.\nThese customized filters inspired by the principles in Impossible Distillation (Jung et al., 2023), designed to ensure that the generated samples are highly suited for tasks like summarization and paraphrasing. The filters also counteract common errors exacerbated by DPSGD, such as language errors, as identified in our manual error analysis (see Table 1). By addressing these task-specific and error-related challenges, DPRefine produces cleaner, more accurate outputs for downstream tasks.\nAfter filtering, we compile the curated dataset Dbase and fine-tune Mbase, effectively distilling the knowledge from the pre-trained model to prepare it for specialized tasks like paraphrasing or summarization."}, {"title": "3.2 Phase 2: DP Task-Specific Fine-Tuning", "content": "In this phase (see middle section in Figure 1), we fine-tune Mbase using DPSGD (Abadi et al., 2016) on a private dataset while ensuring differential privacy. We assume access to this private dataset, which contains sensitive, domain-specific data relevant to the task at hand such as medical text for summarizing biomedical data. This private dataset allows the model to specialize in the target domain while ensuring differential privacy.\nThis fine-tuning process results in a differentially private model Mprivate, which preserves privacy under (\u20ac, \u03b4)-DP. The post-processing property of DP ensures that any further use of Mprivate maintains the same privacy guarantee. The private dataset used in this phase consists of domain-specific data, allowing Mprivate to specialize while ensuring privacy guarantees."}, {"title": "3.3 Phase 3: Self-Distillation Refinement", "content": "As shown in Figure 1 (right section), the final phase of DPRefine involves using self-distillation to further refine the model. Here, Mprivate generates new outputs based on input contexts and self-corrects using its own predictions. For each context c, Mprivate generates multiple output completions {a', b', c', ... }, which are then paired to form new input-output pairs: (x', y') = ((a', b'), (b', c'), (a', c')). The same filtering criteria from Phase 1 are applied to ensure high-quality pairs. After filtering, the data set Drefined is used to fine-tune Mprivate, resulting in the final model Mrefined, which balances privacy and output quality."}, {"title": "4 Experimental Setup", "content": "Datasets and models. We evaluated DPRefine across three datasets for two domain-specific tasks: summarization and paraphrasing. For summarization, we used the XSum dataset (Narayan et al., 2018), containing 204,045 training samples and 11,334 test samples. XSum consists of BBC articles paired with single-sentence summaries, making it particularly challenging due to the requirement for highly concise yet informative summaries. We also used the PubMed dataset (National Library of Medicine, 2024), which contains 119,924 training samples and 6,658 test samples of biomedical research articles with structured abstracts. This dataset demands the summarization of technical and domain-specific content. For paraphrasing, we utilized the MRPC dataset from the GLUE benchmark (Wang et al., 2018), which contains 3,668 training samples and 1,725 test samples of sentence pairs automatically extracted from news articles, with human annotations for semantic equivalence. The smaller size of MRPC makes it particularly well-suited for tasks requiring careful paraphrasing and semantic retention.\nTo generate synthetic data, we used GPT-2 (Radford et al., 2019) for general tasks like XSum and MRPC, while BioGPT (Luo et al., 2022) was employed for domain-specific biomedical text in PubMed. These smaller models were chosen due to their efficiency in producing high-quality data quickly, without the computational overhead of larger models. The synthetic data generated by these models was then used to fine-tune the base paraphrasing/summarization model, T5-large, referred to as Mbase. While the training data for GPT-2 and T5-large is not publicly available, we use XSum, PubMed, and MRPC as proxies for sensitive data, ensuring transparency and reproducibility.\nBaselines. We compared DPRefine against several baselines. The non-private baselines included Copy-Input, GPT-4, and T5-large. Copy-Input provides a simplistic baseline for paraphrasing tasks by replicating the input directly. GPT-4 represents an upper bound for performance without privacy constraints, and T5-large was fine-tuned directly on the datasets without privacy"}, {"title": "4.2 Implementation Details", "content": "Synthetic Data Generation: In Phase 1, we used GPT-2 and BioGPT to generate synthetic data with nucleus sampling (top-p=0.9, temperature=0.1) and a token limit of 150. Contextual prefixes (e.g., \"New York (Reuters) -\") were used for XSum and MRPC to enhance diversity, while BioGPT generated PubMed data without prefixes.\nFiltering: We applied several filters to improve the generated data. First, length filtering ensured the output text was concise (< 75% of the input length). We then applied semantic equivalence filtering using RoBERTa-large-WANLI, retaining only pairs with a bidirectional entailment score above 0.95. Reverse-NLI filtering ensured logical consistency between input and output, with a threshold of 0.7. Additional filters removed redundant tokens (< 30% repetition), ensured numerical consistency, and checked for grammatical errors using the language-tool-python library. Finally, a graph-based approach identified and removed duplicate paraphrases based on entailment scores.\nModel Training: Both the base model, Mbase and the final model Mrefined were trained using the same configuration. We used T5-large with an AdamW optimizer, a learning rate of 5e-5, gradient clipping set at 1.0, and Perplexity (PPL) as the main evaluation metric. Training was conducted over 1 epoch with a batch size of 8 for training and 16 for validation. Beam search and top-p sampling (top-p = 0.9) ensured output diversity.\nDP Fine-Tuning: In Phase 2, using the private-transformers codebase (Li) we fine-tuned the T5-large model (Mbase using DPSGD. For each dataset, we set a privacy budget e to 8, \u03b4 = 2e^7 and a clipping parameter of 1.0. After basic hyperparameter tuning, training was conducted over four epochs with a batch size of 4 with a learning rate of 5e-5, and batch size of 4 resulting in Mprivate.\nSelf Distillation: In Phase 3, Mprivate generated new outputs using inputs from Phase 1 data. These outputs were filtered and used to further fine-tune Mprivate, resulting in Mrefined, following the same training setup as in earlier phases."}, {"title": "5 Experimental Results", "content": "In this section we evaluate the efficacy of DPRefine. We conduct extensive quantitative and qualitative analysis: (1) overall evaluation using LLM-as-a-judge, (2) reference-based, targeted assessments and (4) diversity evaluations. Then, we provide a fine-grained, qualitative error-analysis which categorizes the types of errors different models make, and exemplifies the improvements that DPRefine provides. Finally, we perform ablation studies."}, {"title": "5.1 LLM-as-a-judge Evaluation", "content": "Setup: We evaluated DPRefine and DPSGD using AlpacaEval (Li et al., 2023), performing pairwise comparisons on the test sets of all datasets. GPT-4-1106-preview was used to assess the models across six metrics: preference, coherence, consistency, fact omission, fluency, and relevance. The evaluation was based on modified prompts(see Appendix C) tailored to measure the quality of generated outputs on these dimensions. To ensure a fair comparison, we include both non-private and private baselines. The non-private baselines (e.g., GPT-4, T5-large) demonstrate the upper bounds of performance without privacy constraints, while private baselines (e.g., DPSGD) allow us to compare how our approach improves over traditional differentially private models.\nResults: As shown in Figure 2, DPRefine consistently outperforms DPSGD across all evaluation metrics. On average, AlpacaEval preferred DPRefine's generations in 78.38% of cases across all datasets and metrics. The model shows higher scores in relevance and consistency, indicating better alignment with the input and fewer contradictions. While fluency was slightly lower, DPRefine's superior performance in coherence and fact omission suggests more logically structured and accurate outputs. These results demonstrate that DPRefine generates outputs with stronger contextual and factual alignment compared to DPSGD. These findings are further supported by our manual analysis, which shows that DPRefine significantly reduces inconsistencies and language errors compared to DPSGD, validating the LLM-as-judge results."}, {"title": "5.2 Targeted Reference-based Evaluation", "content": "Setup: For reference-based metrics, we evaluate DPRefine using ROUGE-L (Lin, 2004) and BERT-F1 (Zhang et al., 2019) to assess token overlap and semantic similarity with reference outputs. Additionally, we use iBLEU (Sun and Zhou, 2012) and BERT-iBLEU (B-iB), which offer a more comprehensive assessment of output quality, with BERT-iBLEU being particularly useful for capturing semantic preservation, as it correlates better with human judgments than token-based metrics (Niu et al., 2020). iBLEU is a variant of the BLEU metric that balances adequacy and diversity by rewarding similarity to the reference output while penalizing excessive overlap with the input, making it well-suited for paraphrasing tasks. Given that our tasks include summarization and paraphrasing, we use these automated metrics to measure performance against ground truth outputs. We compare both non-private and private baselines to evaluate DPRefine's performance under privacy constraints.\nResults: Table 2 shows that Copy-Input, a simple baseline that copies the input directly, achieves high scores on ROUGE-L and BERT-F1, indicating that token overlap metrics often favor models that produce outputs closely resembling the input especially for paraphrasing"}, {"title": "5.3 Diversity Evaluation", "content": "Setup: For lexical diversity, we follow Liu et al. to compute Lexical Deviation (LD) and Word Pair Deviation (WPD), which assess the degree of variation between input and output (Liu et al., 2022b). We also measure vocabulary richness using the mean-segmented token type ratio (MSTTR) (Torruella and Capsada, 2013) and the token-level Jaccard similarity between source and predicted output.\nResults: DPRefine performs consistently well in MSTTR and Jaccard Similarity, slightly exceeding DPSGD across most datasets, with only a marginal difference in PubMed. This indicates that DPRefine improves vocabulary richness and reduces word overlap more effectively than DPSGD, though the improvements are modest. These results suggest that DPRefine introduces incremental enhancements in lexical diversity compared to DPSGD, especially in general tasks. DPRefine's most significant gains are observed in WPD and LD, particularly in the MRPC paraphrasing task. In MRPC, DPRefine achieves significantly higher scores in both LD and WPD, demonstrating its ability to introduce greater structural variation and diversity between input and output. This is especially important for paraphrasing tasks, where generating diverse sentence structures is essential for varied and meaningful outputs."}, {"title": "5.4 Qualitative Error Analysis", "content": "To investigate the impact of DPSGD and distillation on inconsistencies and linguistic quality, we manually annotated 50 outputs each from the XSum, MRPC, and Pubmed test sets. We annotated only language errors in the Pubmed results, since the authors lack the necessary domain knowledge to evaluate inconsistencies for the associated task. To categorize inconsistencies and language errors, we developed the taxonomy shown in Table 1. For inconsistencies and hallucinations, we adopt the types and definitions proposed by Tang et al. (Tang et al., 2024). For language errors, we developed our taxonomy based on the errors observed in the model outputs. The results of our qualitative analysis appear in Figure 3.\nInconsistencies. Our results show that non-private models introduce all kinds of inconsistencies listed in Table 1, and both DPSGD and DPRefine reduce these inconsistencies significantly.\nIn both the summarization and paraphrasing tasks, the non-private model introduces extrinsic information in nearly 50% of all outputs; both DP methods reduce these inconsistencies to 16% for both tasks. In the paraphrasing task (MRPC), the non-private model mis-references quotes about 40% of the time, and mis-references entities about 6% of the time; both DP methods reduce these inconsistencies significantly. The non-private model introduces contradictions in 10% (XSum) and 6% (MRPC) of cases; both DP methods reduce them.\nLanguage errors. Our results show that DPSGD introduces language errors of all types at significant rates, and that DPRefine reduces language errors to nearly the level of the non-private model. On the PubMed dataset-the most challenging task in our evaluation\u2014DPRefine consistently introduced fewer language errors than even the non-private model. In the paraphrasing task (MRPC), DPSGD often duplicated the input; DPRefine eliminated this behavior."}, {"title": "5.5 Ablation Studies", "content": "We systematically evaluated the impact of each phase in DPRefine by comparing the performance of three models: Mbase, Mprivate, and Mrefined. These correspond to models trained on synthetic data (Phase 1), fine-tuned with differential privacy (Phase 2), and refined via self-distillation (Phase 3), respectively.\nResults: As shown in Table 4, Mbase (trained on synthetic data) demonstrates strong diversity but lacks consistency and fluency due to the limitations of training on synthetic data alone. Fine-tuning with DPSGD (Mprivate) improves fluency and semantic accuracy, though at the cost of some diversity due to the privacy noise. Self-distillation (Mrefined) recovers much of the lost diversity while maintaining or improving semantic accuracy across datasets, particularly in MRPC and XSum. The results confirm that each phase plays a critical role in balancing privacy and output quality. Removing any phase leads to a reduction in performance, underscoring the necessity of the full DPRefine pipeline."}, {"title": "5.5.2 Privacy-utility Trade-off", "content": "To explore the impact of differential privacy on output quality, we conducted experiments using two privacy budgets: \u20ac = 3 and \u20ac = 8. For each e, we measured the sampling efficiency-defined as the ratio of outputs that passed our quality filters\u2014as well as the semantic accuracy.\nThe results in Table 5 show that, at \u20ac = 3, the model's performance declines in both sampling efficiency and output quality compared to \u20ac = 8. For instance, in the MRPC dataset, DPRefine shows lower BERT-iBLEU scores at \u20ac = 3. However, with \u20ac = 8, both DPRefine and DPSGD produce higher-quality outputs, with DPRefine outperforming DPSGD.\nThe sampling efficiency results further emphasize the trade-off: a lower e value introduces more noise, reducing the number of high-quality outputs. At e = 8, DPRefine not only improves utility but also generates a higher proportion of usable outputs (e.g., 51.7% in MRPC compared to 46.93% at \u20ac = 3), demonstrating that less noise allows the model to retain more valuable content, balancing privacy and output quality."}, {"title": "6 Related Work", "content": "Our work builds on existing efforts to balance privacy and utility in NLP, particularly through the use of DPSGD for fine-tuning models on both private and public data (Kerrigan et al., 2020; Li et al., 2021; Yu et al., 2021; Mireshghallah et al., 2022b; Ganesh et al., 2023). DPSGD has been widely adopted for maintaining privacy guarantees during training, but it often comes at the cost of reduced utility (Yu et al., 2021; Ponomareva et al., 2022; Bagdasaryan et al., 2019; Mireshghallah et al., 2022b).\nKnowledge Distillation (KD) has also been used to improve model performance and reduce the size of LLMs(Jiao et al., 2019; Sun et al., 2019). However, combining KD with DP remains under explored, and only a few studies address the utility losses that arise from using both techniques together(Mireshghallah et al., 2022a; Xie et al., 2024). Our work seeks to fill this gap by enhancing the linguistic quality of DP-trained LLMs through a combination of KD and self-distillation.\nRecent work has also focused on the use of synthetic data to enhance private model training. Flemings et al. introduced DistilDP, which leverages DP synthetic data for knowledge distillation to minimize utility loss in compressed models under privacy constraints (Flemings and Annavaram, 2024), while Yu et al. explored DP synthetic data generation for training lightweight models (Yu et al., 2023). Xie et al. proposed AUG-PE, generating DP synthetic text via API access to large models without fine-tuning (Xie et al., 2024). Unlike these works, DPRefine emphasizes not only model compression and privacy but emphasizes on enhancing linguistic quality and model scalability without relying on large foundational models or relying on API access.\nHallucinations in LLMs are another critical concern, where models generate incorrect or unsubstantiated information. Some recent methods for addressing hallucinations include entropy-based detection (Farquhar et al., 2024), finetuning unfamiliar examples (Kang et al., 2024), and self-reflection techniques to reduce errors in domain-specific tasks (Ji et al., 2023). Kang et al. further observed that neural networks tend to extrapolate predictably toward a constant value when faced with out-of-distribution (OOD) inputs, contributing to predictable hallucinations in certain contexts (Kang et al., 2023). DPRefine incorporates these insights to minimize hallucinations while maintaining privacy and output quality."}, {"title": "7 Conclusion", "content": "We introduced DPRefine, a three-phase method to improve the utility and linguistic quality of differentially private language models. DPRefine addresses DPSGD's limitations by combining data synthesis, privacy-preserving fine-tuning, and self-distillation, resulting in more accurate, coherent, and diverse outputs. Our experiments show DPRefine outperforms DPSGD, reducing language errors and inconsistencies while preserving privacy. By using synthetic data for initialization and self-distillation for refinement, DPRefine mitigates privacy noise and balances quality with privacy guarantees. Fine-grained analysis confirms that DPRefine significantly reduces inconsistencies and errors compared to DPSGD, making it a promising approach for developing high-performing, privacy-preserving models for sensitive tasks. Future work could explore its application to additional NLP tasks and larger models."}, {"title": "8 Social Impacts Statement", "content": "In this work, we utilized pre-trained LLMs and well-known language modeling datasets accessed from the Hugging Face API, which are publicly available and free to use. Specifically, we employed GPT-2 and T5 models licensed under the Apache License, Version 2.0. The datasets used in our experiments, including XSum, PubMed, and MRPC, are widely recognized in the academic community and are used under their respective licenses.\nOur intended use of these artifacts is strictly for academic research, aligning with the intended use specified by the creators. The datasets were chosen for their relevance to the tasks of summarization and paraphrasing, ensuring that they do not contain personally identifiable information (PII). The PubMed dataset, while containing potentially sensitive information, was handled with care to focus solely on text content without disclosing any personal or identifiable data.\nBy utilizing publicly available models and datasets and implementing differential privacy techniques, we minimize any unintended privacy leakage that could result from experimenting."}, {"title": "9 Limitations", "content": "Our approach, while showing promising improvements in the linguistic quality of DP LLMs, comes with certain limitations. First, the computational cost of the multi-phase distillation process is significant, particularly in Phase 1 where large amounts of synthetic data are generated and filtered. This might be impractical for settings with limited computational resources. Additionally, the effectiveness of our approach heavily relies on the quality and diversity of the initial synthetic dataset. If the generated data is not representative or diverse enough, the benefits of the subsequent distillation phases may be diminished.\nFurthermore, our method has been evaluated on specific datasets and tasks, such as summarization and paraphrasing. The generalizability of our findings to other NLP tasks or different types of datasets remains to be explored. Lastly, while our approach reduces inconsistencies, it does not entirely eliminate them, and some errors still persist, which could affect the reliability of the generated outputs."}, {"title": "A Discussion & Examples of DPSGD Grammatical Errors", "content": "DPSGD repeats input instead of paraphrasing. In the paraphrasing task, we observed that models trained with DPSGD often produce output that is identical (or nearly identical) to the input. Non-private models and models trained with DPRefine occasionally reproduce portions of the input unchanged, but far less often than the DPSGD models-suggesting that DPSGD's noise interferes with the ability of the finetuning process to train models that effectively perform the desired task.\nDPSGD misspells uncommon words. Both DPSGD and DPRefine models misspelled technical words (especially in the PubMed dataset) and names of people or places (especially in the XSum dataset). Misspellings seemed most common for uncommon words or names; for example, both models misspelled medical terms in the PubMed dataset (e.g. \"Parkinson's disease\" as \"Parkson's disease\"), and DPSGD misspelled \"Hannah Ennis-Hill\" as \"Hillisis-hill\" in the XSum dataset, but both models correctly spelled common city names like \"Edinburgh.\" The DPRefine models produced many fewer misspellings than the DPSGD models, perhaps because the filtering step removed training examples with misspellings.\nDPSGD makes grammatical errors and produces incomplete sentences. The DPSGD models made significant grammatical errors in all datasets. The DPSGD outputs often left out articles and mis-conjugated verbs; for example, in an example about flooding in Gujrat, the DPSGD model produced the sentence, \u201cdeath in floods in Gujarat have more than double in two days.\" DPRefine models had many fewer grammatical errors, and were comparable to non-private models, perhaps because the filtering process eliminates training examples with grammatical errors.\nThe DPSGD models also produced some sentences that were incomplete. For example, when summarizing an article about an election, the DPSGD model produced the sentence, \"out of the 28 candidates, 70 are women. the election will take place on 2 March. the D are the party with the most candidates with 38 and the.\" DPRefine models produced complete sentences, perhaps because the entailment filtering step fails to infer the meaning of incomplete sentences."}, {"title": "B Phase-wise Dataset Generation and Models", "content": "Here we provide details on the models used across different phases of dataset generation for the MRPC, XSUM, and PubMed datasets (see Table 6. The substantial increase in the number of examples generated in Phase 3 highlights the effectiveness of self-distillation in enhancing the dataset and preparing it for task-specific fine-tuning."}, {"title": "C Example AlpacalEval Prompt", "content": "We show an example of the prompts used in AlpacaEval for evaluating language model performance based on fluency. This prompt is a modified version of the default AlpacaEval prompt with minor changes focused on the specific metric (fluency) for evaluation. All other prompts for the remaining six dimensions (e.g., coherence, consistency, relevance, etc.) were designed in a similar way to assess specific aspects of output quality."}, {"title": "D Example Outputs", "content": "D.1 XSum Dataset\nIn this section, we present example outputs from the XSum dataset, showing the reference summaries and the summaries generated by non-private, DPSGD, and Mrefined models. These examples highlight the differences in summarization quality across different models.\nD.2 MRPC Dataset\nIn this section, we present example paraphrases from the MRPC dataset, showing the reference paraphrases and the paraphrases generated by non-private, DPSGD, and Mrefined models. These examples highlight the differences in paraphrasing quality across different models."}]}