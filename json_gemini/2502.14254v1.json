{"title": "MEM2EGO: EMPOWERING VISION-LANGUAGE MODELS WITH GLOBAL-TO-EGO MEMORY FOR LONG-HORIZON EMBODIED NAVIGATION", "authors": ["Lingfeng Zhang", "Yuecheng Liu", "Zhanguang Zhang", "Matin Aghaei", "Yaochen Hu", "Hongjian Gu", "Mohammad Ali Alomrani", "David Gamaliel Arcos Bravo", "Raika Karimi", "Atia Hamidizadeh", "Haoping Xu", "Guowei Huang", "Zhanpeng Zhang", "Tongtong Cao", "Weichao Qiu", "Xingyue Quan", "Jianye Hao", "Yuzheng Zhuang", "Yingxue Zhang"], "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have made them powerful tools in embodied navigation, enabling agents to leverage commonsense and spatial reasoning for efficient exploration in unfamiliar environments. Existing LLM-based approaches convert global memory, such as semantic or topological maps, into language descriptions to guide navigation. While this improves efficiency and reduces redundant exploration, the loss of geometric information in language-based representations hinders spatial reasoning, especially in intricate environments. To address this, VLM-based approaches directly process ego-centric visual inputs to select optimal directions for exploration. However, relying solely on a first-person perspective makes navigation a partially observed decision-making problem, leading to suboptimal decisions in complex environments. In this paper, we present a novel vision-language model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module and integrating them with the agent's egocentric observations. By dynamically aligning global contextual information with local perception, our approach enhances spatial reasoning and decision-making in long-horizon tasks. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches in object navigation tasks, providing a more effective and scalable solution for embodied navigation.", "sections": [{"title": "1 INTRODUCTION", "content": "Embodied navigation is a crucial component of embodied artificial intelligence, with widespread applications in diverse scenarios such as domestic environments, office settings, logistics and delivery, and factory inspections (Savva et al., 2019; Duan et al., 2022; Zhang et al., 2022). Its significance stems from its ability to enable agents to autonomously navigate and perform tasks within physical environments (Liu et al., 2024b; M\u00f6ller et al., 2021).\nEmbodied navigation poses two key challenges. First, unlike autonomous driving, which typically occurs in structured outdoor environments, embodied navigation requires operating in diverse indoor and industrial settings such as factories, shopping malls, and offices. These spaces feature intricate layouts and obstacles, demanding advanced perception and planning (Th\u00f3risson & Helgasson, 2012; Salvini et al., 2022; Duan et al., 2022; Mavrogiannis et al., 2022). Second, it necessitates a high degree of autonomy, as agents must adapt to unfamiliar environments without relying on predefined maps. They must interpret human instructions and dynamically interact with their surroundings to navigate effectively. This work focuses on Object Goal Navigation (ObjectNav), a task in which agents must locate specified objects within complex spaces (Batra et al., 2020; Majumdar et al., 2022)."}, {"title": "2 RELATED WORK", "content": "Existing studies that leverage VLMs and LLMs for navigation can be categorized into the following directions."}, {"title": "2.1 LLM-BASED NAVIGATION", "content": "These approaches often construct a global memory map based on image observations and use natural language to describe candidate points for navigation, with action decisions driven by large language models (LLMs).\nSeveral methods fall within this category, including LFG (Shah et al., 2023), VoroNav (Wu et al., 2024a), ESC (Zhou et al., 2023), and openFMNav (Kuang et al., 2024). LFG uses frontier-based exploration and large language models to score potential subgoals and guide navigation based on the robot's observations and exploration progress. VoroNav introduces Reduced Voronoi Graphs (RVGs) to optimize the robot's exploration by identifying intersections that provide the best observational opportunities, while the LLM predicts the next best waypoint. ESC uses commonsense knowledge and frontier-based exploration to navigate toward objects in the environment, while openFMNav addresses challenges related to human instructions that imply target objects and zero-shot generalization. These methods employ LLMs to dynamically update a semantic map as the robot explores, enhancing memory and reducing redundant exploration.\nWhile these methods offer the advantage of maintaining a global map and using high-level reasoning, they also face limitations. The language-based reasoning used for decision-making sacrifices high-dimensional semantic information, such as spatial and geometric details, which can constrain"}, {"title": "2.2 VALUE MAP-BASED NAVIGATION", "content": "In this class of methods, a global value function is computed based on ego-view observations, and actions are chosen based on the generated value map instead of using VLMs for decision-making.\nNotable approaches in this category include VLFM (Yokoyama et al., 2024) and InstructNav (Long et al., 2024). VLFM uses a pre-trained vision-language model to generate a language-grounded value map, guiding the agent to explore optimal frontiers. InstructNav extends the idea of goal-directed navigation by introducing a Dynamic Chain of Navigation that breaks down tasks into sequences of actions and landmarks. These methods partially address memory forgetting by integrating global value maps, but they still face challenges. The value map is still constructed based on local observations, and decision-making driven by vision-language models (VLMs) often lacks a comprehensive global perspective. As a result, these approaches frequently lead to suboptimal solutions constrained by local decision-making."}, {"title": "2.3 VLM-BASED NAVIGATION", "content": "These approaches directly leverage first-person perspective images as the input of vision-language models (VLMs) to generate action decisions. By using the spatial reasoning capabilities of VLMs, these methods enable the model to interpret complex environmental features from the robot's current viewpoint, facilitating more informed and context-aware navigation decisions.\nCONVOI (Sathyamoorthy et al., 2024) and PIVOT (Nasiriany et al., 2024) exemplify approaches that process first-person images with VLMs to facilitate real-time navigation and decision-making. While effective in leveraging immediate visual inputs, these methods lack mechanisms for incorporating historical observations, often resulting in redundant exploration. This limitation poses challenges in long-horizon tasks, where maintaining contextual awareness of past actions is critical for efficient navigation. VLMNav (Goetting et al., 2024) addresses some of these limitations by integrating both RGB-D images and the robot's pose information to construct a navigability mask that identifies reachable regions. The model incrementally builds a voxel-based map and refines its action proposals by prioritizing unexplored areas.\nNoMaD (Sridhar et al., 2024) unifies goal-directed navigation and exploration by using the robot's current image and the goal's image as input. The model includes a transformer backbone for processing visual data and a diffusion model for predicting action sequences. A binary mask is applied to the input to focus on either exploration (excluding the goal) or goal-reaching (including the goal). Despite its innovative design, NoMaD remains constrained by the absence of a global memory, relying solely on the most recent three observations. This limitation restricts its capacity for sustained long-term exploration.\nRecent methods have sought to integrate VLMs more effectively for embodied navigation. OpenIN (Tang et al., 2025) focuses on navigation tasks where the robot must locate specific objects that have been moved, introducing a Carrier-Relationship Scene Graph (CRSG) to track objects and their locations. The system uses VLMs to process multimodal instructions and commonsense knowledge to guide navigation decisions.\nUni-NaVid (Zhang et al., 2024) takes a significant step toward unifying different navigation tasks in a single model. It processes both video streams and natural language instructions as input, creating a framework that can generalize across a range of navigation tasks. By training on diverse data, including video question answering and captioning tasks, Uni-NaVid improves its performance in real-world scenarios and enables asynchronous execution for efficiency.\nThese methods move toward integrating both global and local information more effectively, enabling the robot to navigate complex environments with a better understanding of spatial context. However, challenges remain in optimizing the trade-off between VLMs' generalization capabilities and the need for precise, real-time navigation."}, {"title": "3 METHOD", "content": null}, {"title": "3.1 PROBLEM FORMULATION", "content": "In this work, we focus on the object navigation (ObjNav) task, where an agent begins at a random location within an unseen environment and is tasked with finding and navigating to a target object, denoted by g. The agent has no access to a pre-built map and must rely entirely on its sensory inputs for navigation. At each time step t, the agent captures an egocentric RGB-D image, denoted by $o_t$, from its onboard RGB-D camera. Additionally, the agent has access to its current location and orientation, which are represented by the extrinsic matrix $M_{ext}$ of the camera. Using these inputs, the agent must compute and execute a low-level action, $a_t$, that efficiently guides it toward the target object.\nThe workflow of our proposed method is illustrated in Figure 1. The VLM-based navigation relies on the integration of a memory module that encompasses three distinct types of memories. The construction and maintenance of this memory module, as well as the VLM-based navigation process, will be discussed in detail in the subsequent sections."}, {"title": "3.2 \u039c\u0395\u039cORY CONSTRUCTION", "content": "The memory module is composed of three distinct types of memories, each serving a different purpose:\n\u2022 Frontier Map: Denoted as $M_f$, frontier map has been proven to be effective for environment exploration in object navigation tasks, as demonstrated in (Shah et al., 2023; Zhou et al., 2023). We adopt an approach similar to that used in ESC (Zhou et al., 2023) to construct the frontier map. Using the agent's position and camera parameters, RGB-D images are transformed into 3D space, where each 2D pixel is mapped to a 3D voxel in the global coordinate system. Voxels located near the floor, with no obstacles along the height dimension, are classified as free space. A frontier in this map is defined as the boundary between free and unexplored areas. This frontier map is maintained throughout the navigation task.\n\u2022 Landmark Semantic Memory: Denoted as $M_l$, this memory stores descriptions of the landmarks that the agent has seen in the past. Each entry includes the global coordinates of the landmark and a description of the nearby semantic information, such as objects or decoration texture. For example: \"[13.2, 5.4]: Located on the floor near a sink. There is a bath tub nearby.\". The description of each landmark is generated by the VLMs, as explained in Section 3.3.\n\u2022 Visitation Memory: Denoted as $M_v$, this memory keeps track of the landmarks that the agent has already visited."}, {"title": "3.3 MEM2EGO NAVIGATION", "content": "At each time step t, given the image-based observation $o_t$ and the three types of memories\u2014\u041c, Mt, and Mt-introduced in section 3.2, the proposed memory-to-egocentric (Mem2Ego) navigation process can be formulated as follows:\n$a_t = f_\u03b8(o_t, M_f^t, M_l^t, M_v^t, g)$                                                                                                   (1)\nFurther details are provided in the following sections."}, {"title": "3.3.1 PANORAMIC OBSERVATION GENERATION", "content": "After the environment is initialized or the agent reaches a new location, the agent captures four egocentric RGB-D images by rotating its viewpoint 90 degrees at each step. These images are then stacked to construct a 360-degree panoramic observation $o_{pano}^t$ (see Equation 2), offering a comprehensive representation of the surrounding environment. Compared to navigation methods relying on a single egocentric view, this panoramic approach enhances the agent's spatial awareness and scene understanding. A similar strategy has been employed in (Long et al., 2024).\n$o_{pano}^t = Concatenate([o_t^0, o_t^{\u03c0/2}, o_t^{\u03c0}, o_t^{3\u03c0/2}])$                                                                                                    (2)"}, {"title": "3.3.2 FRONTIER AND VISITATION MEMORY PROJECTION", "content": "Based on the agent's position and the newly captured depth images, the navigation map and corre-sponding frontiers are updated following the method outlined in Section 3.2. Candidate locations, denoted as $[C_1, ..., C_N]$ in Equation 3, are generated by combining frontier clustering and grid-based sampling. The centroid of each frontier segment is computed by clustering all points within the segment. However, using the centroid directly as a candidate may result in unreachable goal positions. To mitigate this, we identify the nearest grid point on the floor area to the centroid, ensuring that the candidate is accessible to the agent. Similarly, visited locations, $[V_1, ..., V_M]$, are extracted from the visitation memory $M_v^t$, as shown in Equation 4.\n$[C_1, ..., C_N] = CandidatesGeneration(M_f^t)$                                                                                                                   (3)\n$[V_1, ..., V_M] = VisitationExtraction(M_v^t)$                                                                                                                   (4)\nOnce determined, the global coordinates of these candidates and visitations are projected onto the egocentric image plane as pixel locations $[c_1, ..., c_N]$ and $[v_1, ..., v_M]$, as shown in Equation 5, where K and $M_{ext}$ represent the camera intrinsics and extrinsics, respectively.\n$c_i = Projection(C_i), v_i = Projection(V_i)$\nwhere $c_i = (x_i, y_i), C_i =(X_i, Y_i, Z_i)$, similar for $v_i$ and $V_i$\n$[x'_i, y'_i, w'_i] = K \\cdot M_{ext} \\cdot [X_i, Y_i, Z_i, 1]^T$\n$(x_i, y_i) = (\\frac{x'_i}{w'_i}, \\frac{y'_i}{w'_i})$                                                                                                                  (5)\nAn annotation function is then applied to map these locations onto the panoramic observation $o_{pano}^t$, resulting in an annotated observation $o_{anno}^t$, as outlined in Equation 6. In the annotated image, candidate locations are depicted as green circles, each labeled with a unique identifier corresponding to its position in the image. Similarly, visited locations are marked as blue circles, but only if they are visible within the current view.\n$o_{anno}^t = AnnotateImage(o_{pano}^t, [C_1, ..., C_N], [V_1, ..., V_M])$                                                                                       (6)"}, {"title": "3.3.3 LANDMARK MEMORY RETRIEVAL", "content": "The panoramic image, augmented with frontier candidates, highlights potential navigation targets within the agent's immediate field of view. However, it is common that no suitable targets are visible,"}, {"title": "3.3.4 MEMORY AUGMENTED DECISION MAKING", "content": "At this stage, the panoramic image with annotations, $o_{anno}^t$, along with the top-k landmarks retrieved from memory, $o_{mem}^t$, is used to query VLMs to select the next target location to visit (described in Equation 8). The VLMs are tasked with identifying the marker on the image most likely to lead to the target object, while avoiding markers that are too close to previously visited locations. To ensure consistency in the output format, the top-k landmarks are numbered, and their descriptions are considered only if no suitable marker is identified directly from the panoramic image. A Chain-of-Thought (CoT) prompting strategy is employed to guide the VLMs in generating a structured reasoning process before producing a single numerical output corresponding to the selected marker. The full prompt used for decision-making is provided in Appendix A.1.\n$a_t = f_{VLMs} (prompt(g), o_{anno}^t, o_{mem}^t)$                                                                                                                 (8)"}, {"title": "3.3.5 ACTION EXECUTION", "content": "The marker selected in step 3.3.4 is transformed to the global coordinate system to determine the global coordinates of the target location. Shortest path follower provided by habitat simulator is then executed to navigate agent to the target location while avoiding obstacles. Object detection is performed each time the agent moves or adjusts its viewing angle. The task is deemed successful if the target object is detected within the agent's field of view and the agent successfully navigates to the target object's viewpoints provided by the Habitat dataset. If the target object is not detected, the process continues until the agent either reaches the designated viewpoints or exceeds the maximum allowed number of exploration steps."}, {"title": "3.3.6 MEMORY UPDATE", "content": "While only one landmark from the current view is selected as the next-step navigation target, other landmarks may still be valuable for future exploration. The landmark semantic memory is updated before target position navigation described in Section 3.3.5. VLMs are prompted to describe the surrounding environment near each marker annotated on the panoramic image. The output from the VLMs includes a list of marker IDs paired with corresponding descriptions. The marker IDs are then converted to global coordinates and, together with their descriptions, saved to the landmark semantic memory for use in future exploration processes. The prompt used for landmark description is provided in Appendix A.1.\nMeanwhile, the navigation map is updated along the navigation process, using the RGB-D images captured along the way. Additionally, the agent's most recent location is added to the visitation memory to facilitate future exploration."}, {"title": "3.4 DATA COLLECTION AND MODEL FINETUNING", "content": "To enhance the capabilities of open-sourced VLMs and narrow their performance gap with GPT-40, we design a pipeline to collect training data for supervised finetuning (SFT). The data collection pipeline is illustrated in Figure 2. To improve data diversity and validate the generalization ability of the model, we gather 40 new categories of objects from the HSSD dataset, rather than using the original 6 categories provided. First, new target objects are sampled from the HSSD scenes. For each frame of data, ground-truth trajectories from the current position to these targets are calculated based on the A* algorithm and subsequently smoothed using B\u00e9zier curves. Egocentric images and the"}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "We evaluated our method on the navigation tasks using the Habitat 3.0 (Puig et al., 2023) simulation platform. We adopt similar setup as the Habitat ObjectNav 2022 challenge (Yadav et al., 2022) for all the experiments. The action space of the agent consists of: STOP,MOVE_FORWARD, TURN_LEFT, TURN_RIGHT, with a forward movement distance of 0.25 meters and a turning angle of 30 degrees per step. For low-level movement control, we utilized Habitat's built-in shortest-path follower. The maximum number of steps allowed per task is set to 500 by default. Due to limitations in the image quality within the Habitat environment and the suboptimal performance of state-of-the-art perception modules, such as GroundingDINO (Liu et al., 2025), we opted for Habitat's built-in semantic ground truth with object size conditions as the perception module. In this context, we can assume that the perception module is sufficiently effective. The LLMs and VLMs used in this study was GPT-40 and Llama3.2-11B."}, {"title": "4.2 DATASETS", "content": "Our method is evaluated on the following two object navigation datasets:\n\u2022 Habitat Synthetic Scenes Dataset (HSSD) (Khanna et al., 2024): We use the HSSD validation dataset to evaluate our method. HSSD consists of 41 scenes and six object goal categories: chair, couch, potted plant, bed, toilet, and tv. To ensure task diversity, we select only one episode per scene-object pair. After filtering out erroneous episodes such as cases where the agent's initial position was in mid-air-the final number of evaluated episodes is 213.\n\u2022 HSSD-Hard: Since some HSSD episodes are relatively easy, with the agent finding the target object in just a few steps, we created a more challenging dataset, HSSD-Hard, by selecting HSSD episodes with longer search distance. We calculated the geodesic distance from the agent's starting point to the target object for each episode and selected the top 50%"}, {"title": "4.3 BASELINES", "content": "We compare our method against the following state-of-the-art (SOTA) baselines that represent different strategies to address the object navigation problem:\n\u2022 PIVOT (Nasiriany et al., 2024): This approach casts the navigation task as an iterative visual question answering problem by annotating the image with numerical markers that represent the navigation subgoals. The method is adapted for the HSSD object navigation tasks. Without a frontier map, visitation memory, and landmark semantic memory, our proposed navigation pipeline degenerates to PIVOT.\n\u2022 LFG (Shah et al., 2023): This method employs frontier-based exploration and LLMs to score potential subgoals and guide the navigation.\n\u2022 VLFM (Yokoyama et al., 2024): The approach utilizes VLMs to generate a language-grounded value map, from which the location with the highest value is selected as the next subgoal for navigation.\n\u2022 InstructNav (Long et al., 2024): InstructNav introduces a Dynamic Chain of Navigation, breaking down navigation tasks into sequences of actions and landmarks. It employs four value maps, each with different semantic representations, to assist in selecting the appropriate landmark."}, {"title": "4.4 METRICS", "content": "We employ the following metrics to evaluate the performance of all the methods:\n\u2022 Success Rate (SR): A task was deemed successful when the distance between the agent and any viewpoint of the target object was less than 0.2 meters.\n\u2022 Success Weighted by Path Length (SPL) (Anderson et al., 2018): This metric evaluates how efficient the agent's path is compared to the optimal path. SPL is calculated as:\n$SPL = \\frac{1}{N} \\sum_{i=1}^N S_i \\frac{l_i}{max(p_i, l_i)}$                                                                                                                                     (9)\nwhere $l_i$ is the length of the optimal path for episode i. $p_i$ is the length of path taken by the agent. $S_i$ is the binary indicator of success in episode i."}, {"title": "4.5 MAIN RESULTS", "content": "We evaluate our method and all baselines on both HSSD and HSSD-Hard datasets using the same setup described in Section 4.1, with results summarized in Table 1. Performance is evaluated based on Success Rate (SR) and Success weighted by Path Length (SPL). While SR indicates the overall ability to find the target object, SPL measures the efficiency of the navigation process. Notably, these two metrics are not correlated, as a method can achieve a high SR by sacrificing navigation efficiency. As shown in Table 1, on the HSSD dataset, our proposed method achieves an SR of 0.8685 and an SPL of 0.5788, both of which are higher than all the baseline methods.\nCompared to HSSD, tasks in the HSSD-Hard dataset are more challenging due to the relatively longer search distance, requiring additional steps to locate target objects. As shown in Figure 1, the performance of all methods decreases on the HSSD-Hard dataset, though the impact varies by model. Notably, our method demonstrate an even greater advantages in these more difficult scenarios, achieving an SR that is 12.75% higher than the second-best baseline (PIVOT). Additonally, our method outperforms others in SPL as well, further highlighting its efficiency. These results underscore the effectiveness and robustness of our approach in tackling challenging navigation tasks.\nFigure 3 illustrates the difference in the number of steps required between our method and the baselines for each episode. For failed episodes, the step count is set to the maximum allowed limit (500 in our case). As shown in the figure, our method reduces or at least matches the number of steps in most episodes, highlighting its superior efficiency in challenging object navigation tasks.\nFigure 4 illustrates the memory-augmented decision-making process from a real HSSD episode. VLMs, such as GPT-40, analyze all memory cues on the image before reasoning about the most likely"}, {"title": "4.6 VLM MODEL SUPERVISED FINE-TUNING (SFT)", "content": "To evaluate the impact of the VLM used, we assess the performance of various VLMs within our proposed method. As shown in Table 2, the vanilla Llama3.2 model performs significantly worse than GPT-40. Given the substantial differences in model size and training data, it is not surprising that smaller open-source VLMs like Llama3.2-11B underperform compared to state-of-the-art proprietary models such as GPT-40. Failure analysis reveals that Llama3.2-11B is more prone to visual hallucinations and struggles with instruction following, particularly in marker selection and"}, {"title": "4.7 ABLATION STUDY", "content": "To assess the contribution of each component in our proposed method, we conducted an ablation study by evaluating its performance without landmark semantic memory or visitation memory. As shown in Table 3, removing either of these memory modules leads to a decline in performance. The frontier map could not be excluded from this analysis, as it is essential for marker generation. The absence of visitation memory results in redundant exploration in some cases, thus reducing both the success rate and navigation efficiency. Meanwhile, without landmark semantic memory, the agent is unable to select a navigation goal globally when no suitable marker is present in its current view, which harms the performance as well. These findings highlight the crucial role of both memory modules in fully leveraging the potential of our proposed method."}, {"title": "5 LIMITATIONS", "content": "In our current research, we utilize the VLM to characterize the surrounding environment of each marker and subsequently store the textual response in the landmark semantic memory. This approach heavily relies on the spatial understanding and reasoning capabilities of the VLM, and may result"}, {"title": "6 CONCLUSION", "content": "This study proposes an efficient fusion strategy that integrates task-relevant global memory informa-tion with first-person perspective information, thereby overcoming the suboptimal solution problem associated with existing multimodal navigation frameworks due to local observability. Moreover, this method can simultaneously activate and utilize the complex spatial understanding, reasoning, and commonsense reasoning capabilities of VLMs, thus significantly enhancing the ability and efficiency of navigation decisions in complex spatial scenarios. Theoretically, enhanced spatial cognitive abilities can reduce the required travel distance and number of actions, thereby increasing the task completion success rate and overall efficiency of the navigation scheme."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 PROMPTS FOR INFERENCE", "content": "Prompt 1\nYou are an automated system with the capability to analyze the provided image. Based on the numerical markers present in the image, please describe the surrounding environment relative to each marker's position. Ensure that descriptions of different markers are distinct to maintain the uniqueness of each marker. The marker number should not appear in the description. Please adhere to the following format:\nMarker Number: [insert the number of the first marker here]\nDescription: [provide a description corresponding to the first marker here]\nMarker Number: [insert the number of the second marker here]\nDescription: [provide a description corresponding to the second marker here]\nMarker Number: [insert the number of the last marker here]\nDescription: [provide a description corresponding to the last marker here]\nPrompt 2\nBased on the provided descriptions for each number, please select at most three number whose corresponding descriptions"}, {"title": "A.2 PROMPTS FOR SFT DATA GENERATION", "content": "Prompt for dual-phase rational generation\nYou are given an image with a red movement trajectory on it. Please first identify the objects near the red line in the given image. If there is no red trajectory in the image, please directly return \"None\". Second, knowning that {goal_object} could be found after following the red trajectory, you need to predict the location of goal_object or the region where {goal_object} could be most likely located. This can be achieved by reasonably imagining the unseen areas after the red trajectory based on the room layout. **Do not mention the red trajectory/line or \"the image\" in your output!** Please structure your output in the following way:\nOBJECTS_RED_LINE:\nLOCATION_PREDICTION_AND_REASONING:\nPrompt for rationale filtering\nYou are given an image with a movement trajectory marked in a red line. Please first verify if all of the objects in a given list are present near the red line in the given image. If there is no red line in the image or any of the objects not present, please ignore the rest and directly return"}, {"title": "A.3 CASE STUDY", "content": "Prompt and response for case study\nPrompt: You are a robot and after 360 degrees observation, you can see the given panorama image. The panorama image combines 4 images from different angles. Your task is to find the tv. Based on the numerical markers in the image, select one of these numbers to move next. If you're not confident in moving to the marker to find the tv, you can choose one of the numerical markers located outside of this image. The descriptions of these markers are as follows:\n97: The environment near this marker is a well-furnished bedroom featuring a bed with grey and white bedding. There is a grey headboard attached to the wall, from which two contemporary pendant lights hang. Next to the bedside is a white nightstand with drawers. In the background, large windows or glass doors bring light into the room. To the far right, through the doorway, part of an adjacent room is visible where a small trolley or shelf holding white objects can be seen. 98: The marker is located near the doorway of a room. To the left of the marker, there is a green potted plant. Through the doorway, the room is furnished with a bed positioned near the wall with large windows that extend almost to the floor, allowing for an expansive view of the outside. Additionally, there is a chair or possibly a sofa situated to the left inside the room. The floor is covered with a dark blue surface adding a touch of color to the environment. If you're still not confident in moving to the marker to find the tv, your action should be 'None'. The blue circle marker on the floor indicates the previously explored position. It is better to choose a numeric marker that is not close to the blue circle marker. Please note all closed doors cannot be opened. Please follow the format like this, Thought: [put your step-by-step thinking process here] Action: [put a single marker id or None here]"}, {"title": "A.4 VISUAL HALLUCINATIONS", "content": "Visual Hallucination Example 1\nThought: 1. The top-left, bottom-left, and bottom-right sections of the panorama show closed doors and walls. 2. The top-right section shows an open doorway leading to another room that might be more likely to have a toilet. 3. The numeric marker \"2\" is located near the open doorway in the top-right section. Action: 2"}]}