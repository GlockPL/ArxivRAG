{"title": "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms", "authors": ["QINYI LIU", "OSCAR DEHO", "FARHAD VADIEE", "MOHAMMAD KHALIL", "SRECKO JOKSIMOVIC", "GEORGE SIEMENS"], "abstract": "The increasing use of machine learning in learning analytics (LA) has raised significant concerns around algorithmic fairness and privacy. Synthetic data has emerged as a dual-purpose tool, enhancing privacy and improving fairness in LA models. However, prior research suggests an inverse relationship between fairness and privacy, making it challenging to optimize both. This study investigates which synthetic data generators can best balance privacy and fairness, and whether pre-processing fairness algorithms, typically applied to real datasets, are effective on synthetic data. Our results highlight that the DEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between privacy and fairness. However, DECAF suffers in utility, as reflected in its predictive accuracy. Notably, we found that applying pre-processing fairness algorithms to synthetic data improves fairness even more than when applied to real data. These findings suggest that combining synthetic data generation with fairness pre-processing offers a promising approach to creating fairer LA models.", "sections": [{"title": "1 Introduction", "content": "Machine learning (ML) models have been widely applied in the Learning Analytics (LA) field for various predictive tasks [16, 25]. The widespread use of ML models in LA brings numerous benefits, such as improving students' learning experiences by generating personalized learning paths through the analysis of student behavior [49]. Additionally, ML provides strategic support for optimizing teaching strategies by analyzing students' responses to courses [49]. However, applying ML in LA also raises privacy concerns. Many types of student data, such as mental health data, are considered personal information and must be de-identified according to data protection laws [37]. In this context, many privacy-preserving methods are used in conjunction with ML to meet legal privacy requirements. Among them,"}, {"title": "2 Related work", "content": "Privacy and fairness have been a central topic of discussion within the community since the inception of LA [17, 31]. This section will explain the definitions of privacy and fairness used in this paper, as well as the current status of technologies to improve privacy and fairness in the LA field."}, {"title": "2.1 Privacy and Fairness in LA", "content": null}, {"title": "2.1.1 Privacy in LA", "content": "In this paper, we adopt the machine learning perspective on privacy, with a focus on individuals' consent for data collection and the prevention of harm through data sharing, in line with the concerns raised by Jordon et al. [28]. As highlighted in the field of LA, the increasing complexity of global data protection laws, such as the EU's GDPR, requires strict anonymization of personal and sensitive data before sharing. However, despite efforts to protect data, traditional anonymization methods often fall short under adversarial attacks, as demonstrated by [56], where anonymized student data was re-identified. This underscores the growing challenges faced in the LA field, where personal data must be handled with precision.\nAmid these concerns, advanced privacy-preserving techniques, including SDG, are gaining traction within LA research. Liu et al. [39] demonstrated the effectiveness of various SDG techniques in improving privacy protection while preserving data utility across different dataset sizes. Zhan et al. [59] also emphasized the superiority of differentially private SDGs compared to traditional privacy methods. Although the application of synthetic data as a privacy-preserving measure in LA is relatively new, it shows considerable potential, mirroring its success in other sectors like healthcare and finance [28]. This marks a significant shift towards embracing synthetic data to safeguard personal information without compromising the utility needed for meaningful analytics."}, {"title": "2.1.2 Fairness in LA", "content": "In recent years, the issue of algorithmic fairness has increasingly captured the focus of research in LA [14, 15, 23, 34, 58]. Several metrics\u2014including inter alia, statistical parity difference, equal opportunity, Absolute Between Receiver Operating Characteristic curve Area (ABROCA)\u2014have been defined to operationalize fairness in LA and the broader ML community [34]. Correspondingly, the research community has designed many so-called fairness algorithms that aim to satisfy one or more fairness metrics to mitigate unfairness in predictive models [34, 43]. Fairness algorithms ensure fairness by either removing unfairness from the training data (i.e., pre-processing), adding some fairness constraint to the objective function of the ML model (i.e., in-processing), or removing unfairness from previous predictions made by a model (i.e., post-procesing). While pre-, in-, and post-processing algorithms each tackle different segments of the ML pipepline, there is general consensus that the key source of algorithmic unfairness is the dataset on which predictive models are trained [34, 43]. This is not unexpected given that most real-world data are laden with various degrees of historical biases [4, 34, 58]. To address the issue of historical biases in real-world data, the generation of synthetic data has become a viable option.\nSynthetic data has shown significant potential in enhancing algorithmic fairness. For example, many studies have demonstrated that using balanced synthetic datasets based on generative adversarial networks (GANs) to improve classification training can help reduce the biases caused by imbalances in minority groups [1, 3, 41]. Panagiotou et al. [44] further argue that, unlike traditional sampling techniques used to mitigate imbalance, synthetic data provides a possible solution for addressing both class and group imbalances. Their experiments on four datasets of varying sizes support this claim. In the LA field, while less common than in other domains, recent years have seen a few innovative efforts to use synthetic data to address fairness issues. For example, Jiang et al. [27] use synthetic data to generate unfair benchmark datasets, avoiding the need for actual data collection that may raise ethical concerns (such as sensitive data involving minority student groups)."}, {"title": "2.1.3 The Relationship between Fairness and Privacy", "content": "Synthetic data has emerged as an effective approach for enhancing both privacy and fairness in algorithms. However, previous studies indicate that synthetic data often struggles to balance these two goals [41]. This challenge arises because achieving stronger privacy protection, particularly through the use of differential privacy, can undermine fairness. It is widely recognized that differential privacy can compromise the fairness of synthetic data, leading to increased research focused on evaluating the fairness of differentially private synthetic data [45] and developing new algorithms that maintain fairness under differential privacy constraints [46]. Understanding the complex relationship between privacy and fairness in SDGs is critical. For example, Fioretto and colleagues [22] highlight how such research can clarify the challenges of decision-making with sensitive data, guide the design of fairer ML systems, and shed light on the social impacts of differential privacy. Additionally, privacy and fairness concerns often overlap, as seen in the use of student ethnicity data, which is both private and essential for fairness assessments [12]. This underscores the need to explore privacy and fairness simultaneously from a data-centric perspective.\nDespite this, few studies directly assess both privacy and fairness in synthetic data [6, 10]. Most research has focused on examining the relationship between fairness and data utility, or privacy and utility. Moreover, many earlier privacy evaluations have relied solely on $\\epsilon$ as the primary metric (as $\\epsilon$ as a privacy parameter also quantifies the privacy level), without considering a broader set of privacy measures. To address this gap, this paper will employ a comprehensive set of privacy evaluation metrics for a more thorough analysis. At the same time, LLM-based tabular SDGs has recently achieved significant breakthroughs [8, 38], yet no studies have compared LLMs to other synthetic tabular SDGs in terms of privacy and fairness. This paper aims to fill that research gap by conducting such a comparison. Finally, our literature review shows that many SDGs designed to balance privacy and fairness primarily achieve this by applying fairness constraints or causal models during data generation [2, 51]. However, common fairness-enhancing pre-processing methods used in real-world ML, such as Disparate Impact Remover, Suppression, and Reweighing, have limitedly been applied to synthetic data. For instance, Bhanot [6] tested a pre-processing method (specifically, Reweighing) on the synthetic data generated by only one type of SDG to see whether it improves fairness in synthetic data. Bhanot's results indicated that while the pre-processing method was effective, the improvement was marginal. This raises our curiosity about whether a broader range of pre-processing methods would be effective on synthetic data generated by different SDGs, and how significant the effects would be."}, {"title": "2.2 Synthetic data generation and evaluation", "content": "Synthetic data refers to data generated by specially designed mathematical models or algorithms to address a set of data science tasks [28]. The types of synthetic data include text data, tabular data\u00b9, time series data, multimedia data, such as images, audio and video. Other forms can also include geospatial data and graph data, depending on the application. However, since this paper focuses on educational tabular datasets, the emphasis is placed exclusively on synthetic tabular data. Therefore, the following discussion of synthetic data in this paper specifically refers to synthetic tabular data. In addition to enhancing fairness and privacy as mentioned above, synthetic data can also reduce the cost of collecting and labeling real data [28].\nThe methods for generating synthetic data can be broadly categorized into two types: statistical methods and deep learning methods [13, 21]. Statistical methods have advantages such as fast speed, low computational resource requirements, and manageable parameters. However, they may not be suitable for handling large or complex datasets [26]. On the other hand, deep learning methods, particularly Generative Adversarial Networks (GANs), are renowned for their efficiency and ability to learn underlying patterns in data [21], and they have shown excellent performance across multiple evaluation dimensions [26, 39]. For these reasons, this paper will focus on deep learning methods. Additionally, LLMs are also a type of deep learning method. However, researchers have traditionally considered LLMs to be more adept"}, {"title": "3 Methods", "content": "This section is organized as follows: First, we briefly describe our datasets, thereafter, we discuss techniques and evaluation metrics. Finally, we provide the experimental details."}, {"title": "3.1 Data", "content": "The datasets for our experiments, labeled as A, B, and C, are briefly described as follows:\nA) Student Math performance dataset from UCI [11]. It contains data related to students' demographics, family history, access to IT facilities, and their Mathematics achievement (i.e., pass or fail) in a Portuguese secondary school. This dataset consists of 395 rows and 30 features. We used 27 features after cleaning the data. We use sex as the sensitive attribute for our fairness analysis.\nB) Open University Learning Analytics dataset (OULAD) [36]. We randomly took 30% of this dataset that is focused on pass or fail in final results for our experiments. In all, we used records of 5,550 unique students across seven distinct features. We use disability as the sensitive attribute for fairness analysis.\nC) The law dataset is the longitudinal bar passage data collected from the class that started law school in the fall of 1991, provided by The Law School Admission Council (LSAC) National Longitudinal Study [54]. We use the processed version of the dataset that was used in the fairness study by Kusner and colleagues [35]. This dataset contains six features and 21,792 students records. The target label of this data first year bar exams passage or failure. We use race as the sensitive attribute for our fairness analysis."}, {"title": "3.2 Techniques and Metrics", "content": "In this section we discuss the various SDGs, ML algorithms, fairness algorithms and the respective metrics for the evaluation of privacy and fairness."}, {"title": "3.2.1 SDG Techniques", "content": "As mentioned in subsection 2.2, this paper utilizes five different SDGs. We will briefly introduce them in this paragraph. (1) CTGAN is a GAN-based method designed to model tabular distributions and generate row samples from those distributions. Its unique advantages include the ability to mitigate class imbalances in training data through conditional generators and sampled training [55]. Additionally, CTGAN has shown superior performance in tabular data applications compared to most Bayesian and deep learning methods [55]. (2) DistilGPT2 (DGPT) is a Large Language model, which is pre-trained with the smallest version of GPT2. DGPT is trained on OpenAI's WebText dataset [48]. (3) ADSGAN introduces a regularization term during training to reduce overfitting to real data, thus lowering the risk of privacy attacks such as membership inference. By balancing this regularization with data utility, ADSGAN achieves both privacy protection and high-quality data generation, making it suitable for scenarios where sensitive data is involved [57]. (4) PATEGAN combines the PATE framework with GANs to generate high-quality synthetic data with differential privacy guarantees. It uses an ensemble of teacher models to provide private labels for training the GAN, ensuring privacy without significant degradation of utility [29]. (5) DECAF is a GAN-based model specifically designed to generate fair synthetic data for tabular datasets. It incorporates fairness constraints directly into the training process by adjusting the loss function to penalize biased outcomes [51]. This ensures that the generated data treats all demographic groups equitably."}, {"title": "3.2.2 Privacy evaluation metrics", "content": "As for the privacy evaluation metrics, we used two types of privacy evaluation to provide comprehensive privacy evaluation. The detailed definition of these metrics can be seen in the supplementary file located at this Link.\nThe first type of privacy evaluation is distance and similarity metrics. Distance and similarity metrics in privacy partially overlap with similarity evaluation. The reasoning is straightforward: if synthetic data is too similar to the original data, there is a high risk to privacy. For distance and similarity metrics, we used Average Jensen-Shannon Distance (JSD) and Wasserstein Distance (WD). The JSD excels at measuring the similarity between categorical synthetic data and real data, while the WD is better suited for measuring the similarity between continuous synthetic data and real data [60]. For all these reasons, JSD and WD were selected for this paper. JSD value is between [0-1], and both larger JSD and WD indicate that the datasets are more dissimilar.\nThe second type privacy evaluation we use is re-identification risk assessment. It refers to evaluating the risk of real data leakage through re-identification using SDG [26]. Specifically, we used membership inference attack (MIA) and k-anonymization. MIA in a synthetic data environment means that an attacker attempts to identify whether real records were used to train the SDGs [26]. This method can test the robustness of synthetic datasets against adversarial attacks."}, {"title": "3.2.3 ML and Fairness Algorithms", "content": "For our fairness analysis, we considered 4 baseline (BL) ML algorithms, i.e., off-the-shelf ML algorithms without any fairness constraints, and 4 pre-processing fairness algorithms. We considered only the pre-processing category of fairness algorithms because it allows us to easily debias the real and synthetic datasets and apply them to downstream ML models. The BL algorithms that we used in this study, namely Random Forest (RF), Logistic Regression (LR), Gaussian Naive Bayes (GNB), and eXtreme Gradient Boosting (XGB), have been extensively used for many classification tasks in LA research [25]. We briefly describe the fairness algorithms we considered as follows. (1) Suppression (SUP) [30]: This technique aims to achieve fairness by explicitly excluding the sensitive (demographic) attributes from the data during model training. While some studies such as [43] have criticized this technique that it does not remove unfairness because the sensitive attributes can be correlated and redundantly encoded in other non-sensitive attributes, there are other studies such as [14, 52] that have found that SUP can improve fairness. (2) Correlation Remover (CoR) [53]: This technique is part of the fairlearn suite\u00b2 from Microsoft Research. CoR improves upon the limitation of the SUP approach by removing both the sensitive attributes and the correlations between the sensitive attributes and the non-sensitive attributes in the training data through the application of linear transformation. (3) Disparate Impact Remover (DIR) [20]: This technique removes unfairness by modifying the input features to ensure that their distributions are more similar across the protected and unprotected groups, thereby reducing unfairness in the model's outcomes. (4) Reweighing (RW) [30]: This technique achieves fairness by assigning different weights to instances in the dataset based on their group membership and the target label, so that the model trained on this reweighed data treats different groups more fairly, reducing unfairness in the outcomes."}, {"title": "3.2.4 Fairness evaluation metrics", "content": "To evaluate the fairness of models, we employ 3 popularly used metrics namely ABROCA [23], Error Rate Difference (ERD) [5], and True Positive Rate Difference (TPRD) aka equal opportunity [24] and briefly describe them as follows. Firstly, the ABROCA metric measures fairness by calculating the total difference between ROC curves for privileged and uprivileged groups\u00b3 over all decision thresholds. This metric is particularly valuable in LA because it highlights disparities in model predictions across the full range of possible outcomes, providing a complete and threshold-independent view of fairness in predictions. Typically, an ABROCA score ranges from 0 (ideal) to 1 (worst).\nMathematically, ABROCA is computed as follows: $ABROCA = \\int_0^1 |ROCs(t) \u2013 ROCc(t)| dt$. Where: ROCs(t) is the ROC curve for the baseline (privileged) group b, ROC c(t) is the ROC curve for the comparison (unprivileged) group c, and t is the decision threshold, ranging from 0 to 1. Secondly, the ERD metric measures the disparity in misclassification rates between an unprivileged group c and a privileged group b. It helps assess fairness by highlighting which group faces more errors. Since accuracy is the complement of error rate, accuracy difference is the negative of ERD. This also implies that lower ERD scores will lead to lower accuracy difference scores. ERD close to 1 means the unprivileged group has significantly more errors than the privileged group, 0 implies equal error rates across both groups (which is ideal), and close to -1 means the privileged group has significantly more errors than the unprivileged group. ERD is calculated as follows: ERD = Error Ratec - Error Rateb. Thirdly, the TPRD metric checks if different groups have the same chance of being correctly identified for positive outcomes. TPRD is represented mathematically as: TPRD = TPRb-TPRc. Where: TPR is the True Positive Rate for the privileged group b, and TPR is the True Positive Rate for the unprivileged group"}, {"title": "3.3 Experiments", "content": "Recall our research objectives are (1) to determine which synthetic data generator performs best in terms of balancing both privacy and fairness and, (2) to investigate that if the fairness of synthetic data is less ideal, how can we improve the fairness of synthetic datasets with pre-processing fairness algorithms? As shown in Figure 1, we first generate synthetic data using five SDGs and then evaluate its privacy and fairness to answer RQ1. Subsequently, we apply fairness algorithms to improve the fairness of the synthetic data, and then evaluate it again using the same fairness metrics to answer RQ2. Throughout the experiment, we used the Synthcity Python library [47] to generate data and conduct privacy evaluations. The entire experiment was conducted in Google Colab, utilizing an Nvidia A100 GPU to enhance computational speed."}, {"title": "3.3.1 Privacy and Fairness Analysis", "content": "To answer the RQ1, we have used 5 SDGs. We adhere to its recommended default parameters in Synthcity. Specifically, the epsilon value for PATEGAN is 1. For the privacy evaluation of the synthetic data, we used JSD and WD to assess whether the synthetic data is overly similar to the real data, which could lead to privacy leakage. We also employed MIA and k-anonymity for additional evaluation. The privacy evaluation was repeated twice, and average values were taken to minimize the effect of randomness.\nAfter evaluating the synthetic datasets on the privacy metrics, we followed up by training the baseline models on the real and synthetic datasets and evaluated them using the fairness metrics. Before we discuss the experimental details pertaining to fairness in the next paragraphs, we briefly give a preamble regarding the train-test split criterion of the datasets. For all the fairness-related experiments, the train-test split of our datasets follow two paradigms. For the first paradigm called Same Train, Real Test, we split both the real and the synthetic datasets using the 70/30 split. We train all the models on 70% of each of the datasets both real and all synthetic datasets. However, during testing, we test all the models on the 30% test set of the real data only. By doing so, it allows us to hold all the models to the same standard, ensuring objective and fair comparison. Our primary focus in this paper, and consequently, our results and discussion will only be on the first paradigm. The second paradigm called Same Train, Same Test is similar to to the Same Train, Real Test, however, instead of testing all the models on only 30% real test data, each model is tested on their respective 30% test data as per the training data. This allows for us to measure the fairness of the models when the test dataset comes from the same distribution as the training data. The second paradigm is for demonstration purpose only. Hence, we present the results for the second paradigm in the Supplementary results which can be found here here.\nMoving on to the experimental details. We trained all the four baseline models (i.e., RF, XGB, LR, and GNB) on 70% of each of the datasets. For each model, we identified the the optimal hyper-parameters by performing an exhaustive grid search over a specified search space and evaluated the optimal hyper-parameters by 5-fold stratified cross-validation. We then evaluated the fairness of the predictions of each model on the 30% test dataset in terms of ABROCA, ERD, and TPRD. We also evaluated the predictive accuracy of the models in terms of AUC-ROC. Note that we did the training and testing of all models for both paradigms-Same Train, Real Test and Same Train, Same Test.\nTo help us identify which SDG is able to strike the best balance between privacy and fairness, we finally compared the various models in terms of privacy vs. fairness."}, {"title": "3.3.2 Fairness Improvement Analysis", "content": "To investigate the RQ2, we applied each of the 4 pre-processing techniques (i.e., SUP, CoR, DIR, and RW) to debias each of the 70% training datasets. Some debiasing techniques require certain specifications to determine the degree of fairness required. For the CoR approach, we set the a = 1.0 to ensure maximum filtering of biased information. Similarly, for the DIR, we set the repair level = 1.0 to ensure maximum fairness. After debiasing all the training datasets, we performed all experiments again in the same manner as we did in the RQ1. Everything including model training and evaluation remains the same. The only thing that changes is that this time around, the training data is debiased. We analyzed the improvement in fairness or lack thereof of all the synthetic datasets after applying the pre-processing algorithms to them. We present the results of the experiments in Section 4."}, {"title": "4 Results", "content": "We report selected results here while the additional results are in the double-blind supplementary file which can be accessed by Link. Nonetheless, the results presented here extensively capture all our findings."}, {"title": "4.1 Privacy vs. Fairness Among SDGs", "content": "This section aims to answer the RQ1, which SDGs strikes the best balance between privacy and fairness. The results are presented in Table 1 and Figure 2. Table 1 contains the overall fairness (hereinafter called fairness) vs. the overall privacy (hereinafter called privacy) results across all the 3 datasets. We operationalize fairness as follows:\n$Fairness = \\frac{3-\\left(|ABROCA|+|ERD|+|T PRD|\\right)}{3}$\nAdditionally, let WD max, JSD max, K-Anonymity max, and MIA Accuracymax be the maximum value for each paymetric among all metric values respectively. We operationalize privacy using the following formula:\n$Privacy = \\frac{WD/WDmax+JSD/JSDmax+\\frac{K-Anonymity}{K-Anonymity max}+ 1-\\frac{MIA Accuracy}{MIA Accuracy \u0442\u0430\u0445}}{4}$\nFigure 2 shows the Parento-Frontier plot displaying which SGDs are the best in terms of privacy and fairness and the trade-offs that come with optimizing for either fairness or privacy.\nIn Table 1 and Figure 2, DECAF shows the best balance between privacy and fairness in the three datasets. In dataset A, DECAF performs well and is at the Pareto frontier. Similarly, in datasets B and C, DECAF continues to perform well, especially in dataset C, showing a strong balance between privacy and fairness. One reason may be that as a fairness-orientated algorithm, it supports debasing during the inference stage by removing specific causal paths, effectively reducing unfair factors and meeting user-defined fairness requirements [51]. The reason for good privacy performance is that to remove bias, DECAF allows the deletion of some protected attributes (such as age, and gender), which causes a slight decrease in similarity with the original dataset, thereby improving privacy.[51].\nFor utility-focused SDG, both DGPT and CTGAN tend to focus on fairness and privacy respectively, but they show different degrees of imbalance. DGPT leans more towards fairness compare to CTGAN, the evidence is the high fairness scores on datasets A and B in Table 1. In terms of balancing, DGPT is imbalanced on dataset A with the lowest privacy score, more balanced on dataset B ranking second in both privacy and fairness, and performs moderately on dataset C, ranking second-to-last. While CTGAN exhibits extreme imbalance, with significant disparity between fairness and privacy scores on dataset A, the lowest scores for both on dataset B, and the lowest fairness but second-highest privacy score on dataset C. The relative imbalance of CTGAN and DGPT in privacy and fairness may stem from their primary focus on data utility ensuring that synthetic data closely resembles real data in structure and ML performance [55] [8]. Their goals differ from those of privacy-focused SDGs like ADSGAN and PATEGAN, or fairness-oriented SDGs like DECAF. This observation further supports previous research that demonstrated the trade-off relationship between privacy, fairness, and utility [40].\nWhen comparing privacy-focused ADSGAN and PATEGAN with other SDG, both tend to emphasize privacy more than models like DGPT and CTGAN, but to varying degrees of balance. Overall, ADSGAN emphasizes fairness more than PATEGAN, with a high fairness score of 0.95 but a low privacy score of 0.62 on dataset A. However, it is more imbalanced on datasets B and C, with privacy scores of 0.11 (second-to-last) and 0.44 (lowest), prioritizing fairness at the expense of privacy. The performance of PATEGAN is even more extreme. On dataset A, it shas the lowest fairness score but the highest privacy score. On dataset B, PATEGAN is relatively balanced, with the highest privacy score, but not the worst fairness score. On dataset C, PATEGAN's performance is unstable and even shows the worst privacy score. Overall, ADSGAN provides a more balanced approach across the dataset, while PATEGAN is more unbalanced, prioritizing privacy over fairness to a greater extent. The results are consistent with expectations, as PATEGAN is an SDG that applies DP and has a relatively conservative epsilon value, which is expected to exhibit more extreme trade-offs compared to ADSGAN."}, {"title": "4.2 Application of Fairness Algorithms to Improve Fairness of Synthetic Datasets", "content": "We observed that the application of fairness algorithms can improve the fairness of the models trained on the synthetic datasets. For instance, consider Table 2. This table depicts the overall percentage of improvement (+ve) or exacerbation (-ve) of fairness of all models across all fairness metrics for the various datasets after the pre-processing algorithms have been applied to the datasets. We observed that the SDGs tend to enjoy significant improvement in fairness after the fairness algorithms were applied. Noteworthy among the SDGs is the CTGAN. In fact, on the Dataset C, fairness improved for the CTGAN by 21.5% relative to the baseline. Interestingly, we observed that the CTGAN tends to perform well when paired up with the RW and the SUP fairness techniques. For example, for both datasets B and C, the CTGAN-RW and CTGAN-SUP combinations had the most improvement in fairness relative to their respective baselines. The Figure 3 reinforces these findings, showcasing upward and downward trends in fairness across datasets when the fairness algorithms are employed. Notably, CTGAN shows stable and significant improvements across datasets B and C when RW and SUP are used, indicating that these algorithms may be particularly effective when combined with CTGAN.\nHowever, it should be noted that the effectiveness of different fairness algorithms on the synthetic datasets is not consistent. For example, CTGAN performs remarkably well on SUP across all datasets, showing an impressive 17.7% improvement in Dataset C. In contrast, ADSGAN exhibits largest negative value (-3.8%) on CoR in Dataset A. Overall, from the results of the three experimental datasets, it is difficult to conclusively state which fairness algorithm is most beneficial for improving synthetic data. It can only be observed that certain fairness algorithms are better suited to synthetic data generated by specific SDG.\nIt is evident that fairness algorithms tend to enhance fairness more effectively on synthetic datasets compared to real datasets. Across all datasets, the real data rarely exhibits the highest fairness improvement after applying fairness algorithms. For instance, the only instance where real data performed best was with RW on Dataset A (4% improvement), yet this improvement was marginally higher than the corresponding improvement for the DPGT synthetic dataset (3.1%). Even when fairness for real data improved by 11.5% on Dataset B (using RW), the DECAF synthetic dataset outperformed it with a 17.7% improvement. This observation suggests that while fairness algorithms do enhance real data, their impact is not as pronounced as on synthetic data, which shows consistently higher improvements. A key insight from these results is that the SDG itself may inherently address some fairness concerns, functioning similarly to a fairness algorithm. This aligns with existing literature which shows that SDGs can enhance algorithmic fairness [1]. Therefore, the combination of SDG with pre-processing fairness algorithms appears to be a promising approach to achieving fairer models. Moreover, the significant improvements observed across synthetic datasets indicate that SDGs, when combined with fairness algorithms, may offer a more robust and effective method for mitigating bias than fairness algorithms applied to real data alone. But another point worth noting is that the performance of synthetic data on the smaller data set (sample size = 395) of Dataset A is not very stable. While the overall improvement of fairness methods is greater on synthetic data than on real data, some fairness algorithms applied to synthetic data produce negative results. This may be due to synthetic data being less stable in smaller datasets compared to larger ones [28]."}, {"title": "5 Discussion and Conclusion", "content": "Here, we discuss the utility of our findings and provide some implications for practice. First, for RQ1, we found that DECAF strikes the best balance between privacy and fairness. On the other hand, CTGAN and DGPT lean differently toward privacy and fairness, with DGPT achieving a slightly better balance than CTGAN. PATEGAN and ADSGAN are more privacy-focused, with ADSGAN achieving a better balance compared to PATEGAN. The observation that PATEGAN and ADSGAN prioritize privacy aligns with previous research [29, 57]. This may be because PATEGAN applies DP, which has been shown to negatively impact fairness [10]. ADSGAN, although not using DP, employs a GAN-based structure that minimizes identifiability to privacy-preserving. This structure of ADSGAN likely affects key details relevant to downstream fairness, resulting in its privacy-leaning nature [7]. These findings provide valuable guidance for LA practitioners when they use SDG for fairness enhancement or privacy preservation. When privacy and fairness need to be balanced, DECAF may be the optimal choice. In contrast, ADSGAN is better suited for scenarios where privacy is the primary concern, and DGPT is ideal for situations where fairness is emphasized. Both ADSGAN and DGPT while maintaining a relatively good balance. This result implies that when LA practitioners use SDGs, they have to clarify their goals in three dimensions when formulating data strategies but also emphasize that algorithm selection should be based on the specific needs of the application scenario. This helps guide the development of personalized solutions in the LA field, thereby improving the practical usability and trustworthiness of the system.\nHowever, the best balance achieved by DECAF in terms of privacy and fairness is accompanied by its worst performance in utility. This result can be seen in Tables 3-5, which show DECAF's performance in terms of accuracy. From previous literature, fairness and utility have been considered to have an inverse relationship [21], privacy and utility are also recognized as having an inverse relationship [28], and fairness and privacy are similarly considered to have an inverse relationship [21]. Although in our experiments, one SDG (i.e., DECAF) achieved the best balance in the inverse relationship between fairness and privacy, when placed in the broader triangular relationship of fairness, privacy, and utility, it still struggles to maintain a good balance. This indicates that there is still a lack of SDGs capable of balancing fairness, privacy, and utility simultaneously.\nFor RQ2, we found that applying pre-processing fairness algorithms after generating synthetic data can improve the fairness of the synthetic data. However, different fairness algorithms show inconsistent effectiveness on synthetic data. This finding has implications for generating fair synthetic data, as previous literature has focused on incorporating fairness constraints directly during the generation of synthetic data [46, 51] to achieve fair synthetic data. Our approach demonstrates that using pre-processing fairness algorithms after the synthetic data is generated is also effective in improving fairness. Additionally, another interesting finding is that the improvement in fairness for synthetic data is greater than for real data. The reason may be that synthetic data works well as a fairness algorithm overlaid with pre-processing fairness algorithms. This finding contrasts with the previous results from [6], who applied a single pre-processing fairness algorithm to synthetic data generated by HealthGAN. However, this could be due to the fact that they tested only one SDG model and one pre-processing fairness algorithm. As mentioned earlier, different fairness algorithms show inconsistent performance on synthetic data. The reweighting method they used might not have been well-suited for HealthGAN. This finding not only provides a pathway for improving current fair synthetic data generation research but also points to future directions for fairness research. By optimizing the combination of pre-processing fairness algorithms and synthetic data generation algorithms, fairness can be better ensured, thus laying a foundation for the development of more fair and transparent AI systems.\nThe limitations of this study are as follows: First, to reduce the impact of randomness and bias, it is generally advisable to generate synthetic data multiple times and average the evaluation metrics over iterations, especially for DP methods where noise addition introduces variability. Due to computational constraints, this approach was not applied in the current study, but we aim to enhance robustness in future work. Second, while many other privacy- and fairness-oriented SDGs exist and perform well in both domains, this study focused on a select group of representative SDGs to ensure a clear and concise analysis. Lastly, this study is limited to tabular data, even though learning analytics encompasses diverse data types such as time series and images. Future research could extend the scope to include these other data types."}]}