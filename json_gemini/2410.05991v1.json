{"title": "VECTOR GRIMOIRE: Codebook-based Shape Generation under Raster Image Supervision", "authors": ["Moritz Feuerpfeil*", "Marco Cipriano*", "Gerard de Melo"], "abstract": "Scalable Vector Graphics (SVG) is a popular format on the web and in the design industry. However, despite the great strides made in generative modeling, SVG has remained underexplored due to the discrete and complex nature of such data. We introduce GRIMOIRE, a text-guided SVG generative model that is comprised of two modules: A Visual Shape Quantizer (VSQ) learns to map raster images onto a discrete codebook by reconstructing them as vector shapes, and an Auto-Regressive Transformer (ART) models the joint probability distribution over shape tokens, positions, and textual descriptions, allowing us to generate vector graphics from natural language. Unlike existing models that require direct supervision from SVG data, GRIMOIRE learns shape image patches using only raster image supervision which opens up vector generative modeling to significantly more data. We demonstrate the effectiveness of our method by fitting GRIMOIRE for closed filled shapes on MNIST and for outline strokes on icon and font data, surpassing previous image-supervised methods in generative quality and the vector-supervised approach in flexibility.", "sections": [{"title": "1 Introduction", "content": "In the domain of computer graphics, Scalable Vector Graphics (SVG) has emerged as a versatile format, enabling the representation of 2D graphics with precision and scalability. SVG is an XML-based vector graphics format that describes a series of parametrized shape primitives rather than a limited-resolution raster of pixel values. While modern generative models have made significant advancements in producing high-quality raster images (Ho et al., 2020; Isola et al., 2017; Saharia et al., 2022; Nichol et al., 2021), SVG generation remains a less explored task. Existing works that have aimed to train a deep neural network for this goal primarily adopted language models to address the problem (Wu et al., 2023; Tang et al., 2024). In general, existing approaches share two key limitations: they necessitate SVG data for direct supervision which inherently limits the available data and increases the burden of data pre-processing, and they are not easily extendable when it comes to visual attributes such as color or stroke properties. The extensive pre-processing is required due to the diverse nature of an SVG file that can express shapes as a series of different basic primitives such as circles, lines, and squares \u2013 each having different properties \u2013 that can overlap and occlude each other.\nAn ideal generative model for SVG should however benefit from visual guidance for supervision, which is not possible when merely training to reproduce tokenized SVG primitives, as there is no differentiable mapping to the generated raster imagery."}, {"title": "2 Related Work", "content": "The field of vector graphics generation has witnessed increasing interest. Following the extraordinary success of Large Language Models (LLM), the most recent approaches (Lopes et al., 2019; Aoki and Aizawa, 2022; Wu et al., 2023; Tang et al., 2024) have recast the problem as an NLP task, learning a distribution over tokenized SVG commands. Iconshop (Wu et al., 2023) introduced a method of tokenizing SVG paths that makes them suitable input for causal language modeling. To add conditioning, they employed a pre-trained language model to tokenize and embed textual descriptions, which are concatenated with the SVG tokens to form sequences that the auto-regressive Transformer can learn a joint probability on.\nStrokeNUWA (Tang et al., 2024) introduced Vector Quantized Strokes to compress SVG strokes into a codebook with SVG supervision and fine-tune a pre-trained Encoder\u2013Decoder LLM to predict these tokens given textual input. However, both of these approaches suffer from a number of limitations."}, {"title": "2.1 SVG Generative Models", "content": "The field of vector graphics generation has witnessed increasing interest. Following the extraordinary success of Large Language Models (LLM), the most recent approaches (Lopes et al., 2019; Aoki and Aizawa, 2022; Wu et al., 2023; Tang et al., 2024) have recast the problem as an NLP task, learning a distribution over tokenized SVG commands. Iconshop (Wu et al., 2023) introduced a method of tokenizing SVG paths that makes them suitable input for causal language modeling. To add conditioning, they employed a pre-trained language model to tokenize and embed textual descriptions, which are concatenated with the SVG tokens to form sequences that the auto-regressive Transformer can learn a joint probability on.\nStrokeNUWA (Tang et al., 2024) introduced Vector Quantized Strokes to compress SVG strokes into a codebook with SVG supervision and fine-tune a pre-trained Encoder\u2013Decoder LLM to predict these tokens given textual input. However, both of these approaches suffer from a number of limitations."}, {"title": "2.2 Vector Quantization", "content": "VQ-VAE (Van Den Oord et al., 2017) is a well-known improved architecture for training Variational Autoencoders (Kingma and Welling, 2013; Rezende et al., 2014). Instead of focusing on representations with continuous features as in most prior work (Vincent et al., 2010; Denton et al., 2016; Hinton and Salakhutdinov, 2006; Chen et al., 2016), the encoder in a VQ-VAE emits discrete rather than continuous codes. Each code maps to the closest embedding in a codebook of limited size. The decoder learns to reconstruct the original input image from the chosen codebook embedding. Both the encoder-decoder architecture and the codebook are trained jointly. After training, the autoregressive distribution over the latent codes is learnt by a second model, which then allows for generating new images via ancestral sampling. Latent discrete representations were already pioneered in previous work (Mnih and Gregor, 2014; Courville et al., 2011), but none of the above methods close the performance gap of VAEs with continuous latent variables, where one can use the Gaussian reparametrization trick, which benefits from much lower variance in the gradients. Mentzer et al. (2023) simplified the design of the vector quantization in VQ-VAE with a scheme called finite scalar quantization (FSQ), where the encoded representation of an image is projected to the nearest position on a low-dimensional hypercube. In this case, no additional codebook must be learned, but rather it is given implicitly, which simplifies the loss formulation. Our work builds in part on the VQ-VAE framework and includes the FSQ mechanism."}, {"title": "3 Method", "content": "The first stage of our model employs a Visual Shape Quantizer (VSQ), a vector-quantized auto-encoder, whose encoder $E_{vsq}$ maps an input image $I$ onto a discrete codebook $V$ through vector-quantization and decodes that quantized vector into shape parameters of cubic B\u00e9zier curves through"}, {"title": "3.1 Stage 1 \u2013 Visual Shape Quantizer", "content": "The first stage of our model employs a Visual Shape Quantizer (VSQ), a vector-quantized auto-encoder, whose encoder $E_{vsq}$ maps an input image $I$ onto a discrete codebook $V$ through vector-quantization and decodes that quantized vector into shape parameters of cubic B\u00e9zier curves through"}, {"title": "the decoder $D_{vsq}$.", "content": "Instead of learning the codebook (Van Den Oord et al., 2017), we adopt the more efficient approach of defining our codebook $V$ as a set of equidistant points in a hypercube with $q$ dimensions. Each dimension has $l$ unique values: $L = [l_1, l_2, . . ., l_q]$. The size of the codebook $|V|$ is hence defined by the product of values of all $q$ dimensions. We define $q = 5$ and $L = [7, 5, 5, 5, 5]$ for a target codebook size of 4,375 unique codes, following the recommendations of the original authors (Mentzer et al., 2023).\nBefore being fed to the encoder $E_{vsq}$, each image $I \\in \\mathbb{R}^{C\\times H \\times W}$ is divided into patches $S = (S_1, S_2,..., S_n)$, with $s_i \\in \\mathbb{R}^{C\\times 128 \\times 128}$, where $C = 3$ is the number of channels. A set of discrete anchor coordinates $= (\\theta_1, \\theta_2,..., \\theta_n)$ with $\\theta_i \\in \\mathbb{N}^2$ being the center coordinate of $s_i$ in the original image $I$ is also saved. The original image $I$ can then be reconstructed using $S$ and $\\Theta$.\nWe experiment on three datasets (see Section 4). For MNIST, the patches are obtained by tiling each image into a 6 \u00d7 6 grid. For Fonts and FIGR-8, each patch depicts part of the target outline as shown in Figure 3. We utilize a contour-finding algorithm (Lorensen and Cline, 1987) to extract outlines from raster images, which are then divided into several shorter segments. Additional details regarding this extraction process can be found in Section 7.2. In contrast, the Fonts dataset is natively available in vector format, making it easier to manipulate, similar to icons, before undergoing rasterization.\nThe VSQ encoder $E_{vsq}$ maps each patch $s_i \\in \\mathbb{R}^{C\\times 128 \\times 128}$ to $\\S$ codes on the hypercube $E_{vsq}: \\mathbb{R}^{C\\times 128 \\times 128} \\rightarrow V$ as follows. Each centered raster patch $s_i$ is encoded with a ResNet-18 (He et al., 2016) into a latent variable $z_i \\in \\mathbb{Z} < \\mathbb{R}^{d \\times \\mathcal{E}}$ with $d = 512$. Successively, each of the $\\mathcal{E}$ codes is projected to $q$ dimensions through a linear mapping layer and finally quantized, resulting in $\\hat{c}_i \\in \\mathbb{N}^q$. The final code value $v_i \\in V$ is then computed as the weighted sum of all $q$ dimensions of $\\hat{z}_i$:"}, {"title": null, "content": "$$\nv_i = \\sum_{j=1}^q \\hat{z}_{ij} b_j,\n$$"}, {"title": null, "content": "where the basis $b_j$ is derived as $b_j = \\prod_{k=1}^q l_k$, with $b_1 = 1$. This transformation ensures that each unique combination of quantized values $\\hat{c}_i$ is mapped to a unique code $v_i$ in the codebook $V$.\nThis approach avoids auxiliary losses on the codebook while maintaining competitive expressiveness.\nThe decoder $D_{vsq}$ consists of a projection layer, which transforms all the $\\S$ predicted codes back into the latent space $\\mathbb{Z}$, and a lightweight neural network $\\Phi_{points}$, which predicts the control points of $v$ cubic B\u00e9zier curves that form a single connected path. We propose two variants of $\\Phi_{points}$, a fully-connected neural network $\\Phi_{stroke}: \\mathbb{Z} \\rightarrow \\mathbb{R}^{(2 \\times (v \\times 3 + 1))}$, which predicts connected strokes, and a 1-D CNN $\\Phi_{shape}: \\mathbb{Z} \\rightarrow \\mathbb{R}^{(2 \\times (v \\times 3))}$, which outputs a closed shape.\nFinally, the predicted path of $v$ B\u00e9zier curves from $\\Phi_{points}$ passes through the differentiable rasterizer to obtain a raster output $\\hat{s}_i = DiffVG(D_{vsq}(E_{vsq}(s_i)))$. In order to learn to reconstruct strokes and shapes, we train the VSQ module using the mean squared error:"}, {"title": null, "content": "$$\\n\\mathcal{L}_{recons} = (s - \\hat{s})^2.\n$$"}, {"title": null, "content": "$D_{vsq}$ can be extended to predict continuous values for any visual attribute supported by the differentiable rasterizer. Hence, we also propose series of other fully-connected prediction heads that can optionally be enabled: $\\Phi_{width}: \\mathbb{Z} \\rightarrow \\mathbb{R}$ predicts the stroke width of the overall shape, and $\\Phi_{color}: \\mathbb{Z} \\rightarrow \\mathbb{R}^C$ outputs the stroke color or the filling color for the output of $\\Phi_{stroke}$ and $\\Phi_{shape}$ respectively. All the modules are followed by a sigmoid activation function.\nWhile $\\mathcal{L}_{recons}$ would suffice for training the VSQ, operating only on the visual domain could lead to degenerate strokes and undesirable local minima. To mitigate this, we propose a novel geometric constraint $\\mathcal{L}_{geom}$, which punishes control point placement of irregular distances measured between all combinations of points predicted by $\\Phi_{stroke}$.\nLet $P = (p_1, p_2, \u2026, p_{v+1})$ be the set of all start and end points of a stroke with $p_i = (p_i^x, p_i^y)$ and $p_i^x, p_i^y \\in [0, 1]$. Then $\u03c1_{i,j}$ is defined as the Euclidean distance between two points $p_i$ and $p_j$, $P_j$ is defined as the mean scaled inner distance for point $p_i$ to all other points in $P$, and $\u03b4_j$ as the average squared deviation from that mean for point $p_j$:"}, {"title": null, "content": "$$\\n\\begin{aligned}\n\\overline{p}_{j} &= \\frac{1}{\\nu} \\sum_{\\substack{i=1 \\\\\ni \\: i \\neq j}}^{\\nu+1} \\frac{\\rho_{i, j}}{|i-j|}, \\\\\n\\delta_{j} &= \\frac{1}{\\nu} \\sum_{\\substack{i=1 \\\\\ni \\: i \\neq j}}^{\\nu+1} \\left(\\frac{\\rho_{i, j}}{|i-j|} - \\overline{p}_{j}\\right)^{2}\n\\end{aligned}\n$$"}, {"title": null, "content": "$\\mathcal{L}_{geom}$ is finally defined as the average of the deviations for all start and end points in $P$. $\\mathcal{L}_{geom}$ is then weighted with $\u03b1$ and added to the reconstruction loss."}, {"title": null, "content": "$$\\n\\mathcal{L}_{geom} = \\frac{1}{\\nu + 1} \\sum_{j=1}^{\\nu+1} \\delta_{j} \\quad \\mathcal{L}_{geom} = \\mathcal{L}_{geom} + \\alpha \\times \\mathcal{L}_{geom}\n$$"}, {"title": null, "content": "We use this component only for the experiments with $\\Phi_{points}^{stroke}$ and set $\u03b1 = 0.4$. We opt to train the ResNet encoder from scratch during this stage, since the target images belong to a very specific domain. The amount of trainable parameters is 15.36M for the encoder and 0.8M for the decoder. We stress the importance of the skewed balance between the two parameter counts, as the encoding of images is only required for training the model and encoding the training data for the auto-regressive Transformer in the next step. The final inference pipeline discards the encoder and only requires the trained decoder $D_{vsq}$, hence resulting in more lightweight inference. The overall scheme of GRIMOIRE including the first stage of training is depicted in Figure 2."}, {"title": "3.2 Stage 2 \u2013 Auto-Regressive Transformer", "content": "After the VSQ is trained, each patch $s_i$ can be mapped onto an index code $v_i$ of the codebook $V$ using the encoder $E_{vsq}$ and the quantization method. However, the predicted patch $s_i$ captured by the VSQ does not describe a complete SVG, as the centering leads to a loss of information about their global position $\u03b8_i$ on the original canvas. Also, the sequence of tokens is still missing the text conditioning. This is addressed in the second stage of GRIMOIRE. The second stage consists of an Auto-Regressive Transformer (ART) that learns for each image $I$ the joint distribution over the text, positions, and stroke tokens. A textual description $T$ of $I$ is tokenized into $T = (T_1, T_2, . . ., T_t)$ using a pre-trained BERT encoder (Devlin et al., 2018) and embedded. $I$ is visually encoded by transforming its patches $s_i$ onto $v_i \\in V$ via the encoder $E_{vsq}$, whereas each original patch position $\u03b8_i \\in \u0398$ is mapped into the closest position in a 256 \u00d7 256 grid resulting in 2562 possible position tokens. Special tokens <SOS>, <BOS>, and <EOS> indicate the start of a full sequence, beginning of the patch token sequence, and end of sequence, respectively. Each patch token is alternated with its position token. The final input sequence for a given image to the ART module becomes:"}, {"title": null, "content": "$$x = (<SOS>, T_1, . . ., T_t, <BOS>, \\theta_1, v_1, ... \\theta_n, v_n, <EOS>)$$"}, {"title": null, "content": "The total amount of representable token values then has a dimensionality of $|V| + 256^2 + 3 = 69,914$ for $|V| = 4,375$. A learnable weight matrix $W \\in \\mathbb{R}^{d \\times 69,914}$ embeds the position and visual tokens into a vector of size $d$. The BERT text embeddings are projected into the same d-dimensional space using a trainable linear mapping layer. The ART module consists of 12 and 16 standard Transformer decoder blocks with causal multi-head attention with 8 attention heads for fonts and icons, respectively. The final loss for the ART module is defined as:"}, {"title": null, "content": "$$\\n\\mathcal{L}_{Causal} = - \\sum_{i=1}^{N} log \\: p(x_i | X_{< i}; \\theta)\n$$"}, {"title": null, "content": "During inference, the input to the ART module is represented as $x = (<SOS>, T_1, . . ., T_t, <BOS>)$, where new tokens are predicted auto-regressively until the <EOS> token is generated. Additionally, visual strokes can be incorporated into the input sequence to condition the generation process."}, {"title": "4 Data", "content": "MNIST. We conduct our initial experiments on the MNIST dataset (LeCun et al., 1998). We upscale each digit to 128 \u00d7 128 pixels and generate the texual description using the prompt \"x in black color\", where x is the class of each digit. We adopt the original train and test split."}, {"title": "5 Results", "content": "This section presents our findings in two primary categories. First, we examine the quality of the reconstructions and generations produced by GRIMOIRE in comparison to existing methods. Second, we highlight the flexibility of our approach, demonstrating how GRIMOIRE can be easily extended to incorporate additional SVG features."}, {"title": "5.1 Reconstructions", "content": "Closed Paths. We begin by presenting the reconstruction results of our VSQ module on the MNIST dataset. In our experiments, we model each patch shape using a total of 15 segments. Increasing the number of segments beyond this point did not yield any significant improvement in reconstruction quality. Given the simplicity of the target shapes, we adopted a single code per shape.\nWe also conducted a comparative analysis of the reconstruction capabilities of our VSQ module against Im2Vec. To assess the generative quality of our samples, we employed the Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) and CLIPScore (Radford et al., 2021), both of which are computed using the image features of a pre-trained CLIP encoder. Additionally, to validate our VSQ module, we considered the reconstruction loss $\\mathcal{L}_{recons}$, as it directly reflects the maximum achievable performance of the network and provides a more reliable metric.\nAs shown in Table 1, our VSQ module consistently achieves a lower reconstruction error compared to Im2Vec across all MNIST digits. In Table 2, we also report the reconstruction error for a subset of the dataset, selecting the digit zero due to its particularly challenging topology. Again, our method exhibits superior performance with lower reconstruction errors. For MNIST, we fill the predicted shapes from Im2Vec, since the raster ground truth images are only in a filled format. However, we present both filled and unfilled versions for all other scenarios.\nThe CLIPScore of our reconstructions is higher in both cases. Notably, FID is the only metric where Im2Vec occasionally shows superior results. We attribute this to the lower resolution of the ground truth images, which introduces instability in the FID metric. The CLIPScore, however, mitigates this issue by comparing the similarity with the textual description."}, {"title": "Strokes.", "content": "For Fonts and FIGR-8, we conduct a deeper investigation to validate the reconstruction errors of VSQ under different configurations, varying the amount of segments and codes per shape, and the maximum length of the input strokes. Our findings show that for Fonts, more than one segment per shape consistently degrades the reconstruction quality, possibly because the complexity of the strokes in our datasets does not require many Bezi\u00e9r curves to reconstruct an input patch. We also find that shorter thresholds on the stroke length help the reconstruction quality, as the MSE decreases when moving from 11% to 7% and eventually to 4% of the maximum stroke length with respect to the image size. Intuitively, shorter strokes are easier to model, but could also lead to very scattered predictions for overly short settings.\nThe best reconstructions are achieved by using multiple codes per centered stroke. The two-codes configuration has an average decrease in MSE of 18.28%, 41.46%, and 26.09% for the respective stroke lengths. However, the best-performing configuration with two codes per shape is just 11.36% better than the best single code representative, which we believe does not justify twice the number of required visual tokens for the second stage training. Throughout our experiments, the configurations with multiple segments do consistently benefit from our geometric constraint. Ultimately, for our final experiments we choose $(v = 2, \\xi = 1)$ for Fonts, and $(v = 4, \\xi = 2)$ for FIGR-8.\nRegarding the comparison with Im2Vec, Table 2 shows that the text-conditioned GRIMOIRE on a single glyph or icon has superior reconstruction performance even if Im2Vec is specifically trained on that subset of data. In Table 1, we also report the values after training on the full datasets. In this case, GRIMOIRE substantially outperforms Im2Vec, which is unable to cope with the complexity of the data.\nFinally, as GRIMOIRE quickly learns to map basic strokes or shapes onto its finite codebook and due to the similarities between those primitive traits among various samples in the dataset, we find GRIMOIRE to converge even before completing a full epoch on any dataset. Despite the reconstruction error being considerably higher, we also notice reasonable domain transfer capabilities between FIGR-8 images and Fonts when training the VSQ module only on one dataset and keeping the maximum stroke length consistent. Qualitative examples of the re-usability of the VSQ module are reported in the Appendix."}, {"title": "5.2 Generations", "content": "Text Conditioning. We compare GRIMOIRE with Im2Vec by generating glyphs and icons and handwritten digits, and report the results in Table 3. Despite Im2Vec being tailored for single classes only, our general model shows superior performance in CLIPScore for all datasets. Im2Vec shows a generally lower FID score in the experiments with filled shapes, which we attribute again to the lower resolution of the ground truth images (MNIST) and a bias in the metric itself as CLIP struggles to produces meaningful visual embeddings for sparse images (Chowdhury et al., 2022) as for Fonts, FIGR-8. In contrast, in the generative results on unfilled shapes, GRIMOIRE almost consistently outperforms Im2Vec by a large margin for glyphs and icons.\nNote that we establish new baseline results for the complete datasets, as Im2Vec does not support text or class conditioning.\nLooking at qualitative samples in Figure 4 and Figure 1, one can see that contrary to the claim that surplus shapes collapse to a point (Reddy et al., 2021), there are multiple redundant shapes present in the generations of Im2Vec. A single star might then be represented by ten overlapping almost identical paths."}, {"title": "Vector Conditioning.", "content": "We also evaluate GRIMOIRE on another task previously unavailable for image-supervised vector graphic generative models, which is text-guided icon completion. Figure 6 shows the capability of our model to complete an unseen icon, based on a set of given context strokes that start at random positions. GRIMOIRE can meaningfully complete various amounts of contexts, even when the strokes of the context stem from disconnected parts of the icon. We provide a quantitative analysis in Section 7.8. The results in this section are all obtained with the default pipeline that post-processes the generation of our model. A detailed analysis of our post-processing is provided in Section 7.3 and Section 7.4."}, {"title": "5.3 Flexibility", "content": "Finally, we demonstrate the flexibility of GRIMOIRE through additional qualitative results on new SVG attributes. One of the advantages of splitting the generative pipeline into two parts is that the ART module can be fully decoupled from the visual attributes of the SVG primitives. Instead, the vector prediction head of the VSQ can be extended to include any visual attribute supported by the differentiable rasterizer. Specifically, we activate the prediction heads $\\Phi_{width}$ and $\\Phi_{color}$ -outlined in Section 3.1- to enable learning of stroke width and color, respectively. We train the VSQ module on input patches while varying the values of those attributes and present the qualitative outcomes in Figure 7, where each stroke is randomly colored using an eight-color palette and a variable stroke width. The VSQ module accurately learns these features without requiring altering the size of the codebook or modifying any other network configurations.\nA similar analysis is conducted with closed shapes, and the results are reported in Figure 8, showing that the VSQ module jointly maps both shape and color to a single code. This highlights the minimal requirements of GRIMOIRE in supporting additional SVG features. In contrast, other state-of-the-art vector-based generative models often rely on complex tokenization pipelines, making the extension to new SVG attributes more cumbersome and less flexible."}, {"title": "6 Conclusion", "content": "This work presents GRIMOIRE, a novel framework for generating and completing complex SVGs, trained solely on raster images. GRIMOIRE improves existing raster-supervised SVG generative networks in output quality, while offering significantly greater flexibility through text-conditioned generation. We validate GRIMOIRE on filled shapes using a simple tile-patching strategy to create the input data, and on strokes using fonts and icons datasets. Our results demonstrate the superior performance of GRIMOIRE compared to existing models, even when adapted to specific image classes."}, {"title": "7 Appendix", "content": "Due to the number of notation used in this work, in Table 4 we have reported a recap of the most important with a brief description of their meaning."}, {"title": "7.1 Glossary of Notation", "content": "Due to the number of notation used in this work, in Table 4 we have reported a recap of the most important with a brief description of their meaning."}, {"title": "7.2 Pre-Processing", "content": "This section provides additional information regarding the pre-processing and extraction techniques on the employed datasets.\nShapes. No pre-processing is conducted for the MNIST dataset. Images are simply tiled using a 6 \u00d7 6 grid and the central position of each tile in the original image is saved.\nStrokes. For the FIGR-8 dataset, the pixels outlining the icons are isolated using a contour finding algorithm (Lorensen and Cline, 1987) and the coordinates are then used to convert them into vector paths. This simple procedure available in our code repository allows us to efficiently apply a standard pre-processing pipeline defined in Carlier et al. (2020) and already adopted by other studies (Wu et al., 2023; Tang et al., 2024). The process involves normalizing all strokes and breaking them into shorter units if their length exceeds a certain maximum percentage of the image size. Finally, each resulting path fragment is scaled, translated to the center of a new canvas s by placing the center of its bounding box onto the center of s, and rasterized to become part of the training data. Since strokes in S are all translated around the image center, the original center position @ of the bounding box in I is recorded for each s and saved. These coordinates are discretized in a range of 256 \u00d7 256 values. This approach is also used for Fonts, but since the data comes in vector format, there is no need for contour finding."}, {"title": "7.3 Post-Processing", "content": "Our approach introduces small discrepancies with the ground truth data during tokenization. The VSQ introduces small inaccuracies in the reconstruction of the stroke, and the discretization of the global center positions may sligthly displace said strokes. The latter serve as the training data for the auto-regressive Transformer and therefore represent an upper limit to the final generation quality. Similarly for MNIST, the use of white padding on each patch to facilitate faster convergence results in small background gaps when rendering all shapes together, as shown in Figure 5. These small errors compound for the full final image and may become fairly visible in the reconstructions."}, {"title": "7.4 Results with different Post-processing", "content": "In GRIMOIRE, the resulting full vector graphic generation is characterized by fragmented segments. This is because the output strokes of the VSQ decoder are each locally centered onto a separate canvas,"}, {"title": "7.5 Im2Vec on Other Classes", "content": "We conducted a more in-depth analysis of the generative capabilities in Im2Vec after training on single subsets of FIGR-8, and compare the results with GRIMOIRE. We trained Im2Vec on the top-10 classes of FIGR-8: Camera (8,818 samples), Home (7,837), User (7,480), Book (7,163), Clock (6,823), Flower (6,698), Star (6,681), Calendar (misspelt as caledar in the dataset, 6,230), and Document (6,221). Table 6 compares the FID and CLIPScore with GRIMOIRE. Note that we train our model only once on the full FIGR-8 dataset and validate the generative performance using text-conditioning on the target class, whereas Im2Vec is unable to handle training on such diverse data. Despite Im2Vec appearing to obtain higher scores on several classes such as User or Document, a qualitative inspection reveals how the majority of the generated samples come in the form of meaningless filled blobs or rectangles. The traditional metrics employed in this particular generative field, based on the pre-trained CLIP model, react very strongly to such shapes in contrast to more defined stroke images. We refer reviewers to the qualitative samples in Figure 20. We further observe a low variance in the generations when Im2Vec learns the representations of certain classes, such as star icons."}, {"title": "7.6 Qualitative Results of the Geometric Loss", "content": "The adoption of our geometric constraint improves the overall reconstruction error, which we attribute to the network being encouraged to elongate the stroke as much as possible. The results in Figure 11 show the effects on the control points of the reconstructed strokes from the VSQ. With the geometric constraint, the incentive to stretch the stroke works against the MSE objective, which results in an overall longer stroke and therefore in greater connectedness in a full reconstruction and an overall lower reconstruction error. We also present an example with an excessively high geometric constraint weight ($\u03b1$ = 5) demonstrating that beyond a certain threshold, the positive effect diminishes, resulting in degenerated strokes."}, {"title": "7.7 Implementation Details", "content": "We use AdamW optimization and train the VSQ module for 1 epoch for Fonts and FIGR-8 and five epochs for MNIST. We use a learning rate of $\u03bb = 2 \u00d7 10^{\u22125}$, while the auto-regressive Transformer is trained for ~30 epochs with $\u03bb = 6 \u00d7 10^{\u22124}$. The Transformer has a context length of 512. Before proceeding to the second stage, we filter out icons represented by fewer than ten or more than 512 VSQ tokens, which affects 12.16% of samples. We use p-sampling for our generations with GRIMOIRE. Training the VSQ module on six NVIDIA H100 takes approximately 48, 15, and 12 hours for MNIST, FIGR-8, and Fonts, respectively; the ART module takes considerably fewer resources, requiring around 8 hours depending on the configuration. Regarding Im2Vec, we replace the Ranger scheduler with AdamW (Loshchilov and Hutter, 2017) and enable the weighting factor for the Kullback\u2013Leibler (KL) divergence in the loss function to 0.1, as it was disabled by default in the code repository, preventing any sampling. We train Im2Vec with six paths for 105 epochs with a learning rate of $\u03bb = 2 \u00d7 10^{\u22124}$ with early stopping if the validation loss does not decrease after seven epochs. Regarding the generative metrics, we utilized CLIP with a ViT-16 backend for FID and CLIPScore."}, {"title": "7.8 Generative Scores with Completion", "content": "To evaluate if GRIMOIRE generalizes and learns to meaningfully complete previously unseen objects, we compare the CLIPScore and FID of completions with varying lengths of context. The context and text prompts are extracted from 1,000 samples of the test set of the FIGR-8 dataset. The results are shown in Table 7.\nWhile GRIMOIRE can meaningfully complete unseen objects, the quality of these completions is generally lower than the generations under text-only conditioning. This is expected, as prompts in the test set are also encountered during training (the class names). The CLIPScore generally drops to its lowest point with the least amount of context and then recovers when more context is given to the model, which coincides with our qualitative observations that with only a few context strokes, GRIMOIRE occasionally ignores them completely or completes them in an illogical way, reducing the visual appearance."}]}