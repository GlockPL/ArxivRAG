{"title": "VQ-Map: Bird's-Eye-View Map Layout Estimation in Tokenized Discrete Space via Vector Quantization", "authors": ["Yiwei Zhang", "Jin Gao", "Fudong Ge", "Guan Luo", "Bing Li", "Zhaoxiang Zhang", "Haibin Ling", "Weiming Hu"], "abstract": "Bird's-eye-view (BEV) map layout estimation requires an accurate and full under- standing of the semantics for the environmental elements around the ego car to make the results coherent and realistic. Due to the challenges posed by occlusion, unfavourable imaging conditions and low resolution, generating the BEV semantic maps corresponding to corrupted or invalid areas in the perspective view (PV) is appealing very recently. The question is how to align the PV features with the generative models to facilitate the map estimation. In this paper, we propose to utilize a generative model similar to the Vector Quantized-Variational AutoEncoder (VQ-VAE) to acquire prior knowledge for the high-level BEV semantics in the tokenized discrete space. Thanks to the obtained BEV tokens accompanied with a codebook embedding encapsulating the semantics for different BEV elements in the groundtruth maps, we are able to directly align the sparse backbone image features with the obtained BEV tokens from the discrete representation learning based on a specialized token decoder module, and finally generate high-quality BEV maps with the BEV codebook embedding serving as a bridge between PV and BEV. We evaluate the BEV map layout estimation performance of our model, termed VQ-Map, on both the nuScenes and Argoverse benchmarks, achieving 62.2/47.6 mean IoU for surround-view/monocular evaluation on nuScenes, as well as 73.4 IoU for monocular evaluation on Argoverse, which all set a new record for this map layout estimation task. The code and models are available on https://github.com/Z1zyw/VQ-Map.", "sections": [{"title": "1 Introduction", "content": "BEV layouts represent high-dimensional structured data that encompasses significant prior knowledge, particularly regarding road structures. While current methods for BEV map layout estimation mainly focus on constructing dense BEV features [2, 1, 3] for semantic segmentation as map prediction, they often overlook the incorporation of map prior knowledge. Additionally, occlusion and inherent challenges in depth estimation often lead to inaccuracies in dense features, especially in the areas that are corrupted or invalid in the PV. These factors contribute to incoherent and unrealistic BEV layout results, often with numerous artifacts (see Fig. 1). Yet, humans can rely solely on partial observations of a scene in the PV to imagine the entire coherent BEV layout elements. A natural approach to imitating the human imagination process is to leverage generative models to learn the prior knowledge from the groundtruth BEV map layouts. However, the question is how to align the PV features with the generative models to facilitate BEV map estimation.\nTo this end, we propose a novel pipeline called VQ-Map (see Fig. 2), which aligns the generative models well in the spirit of discrete tokens. In specific, VQ-Map utilizes a generative model similar to VQ-VAE [4] to encode the groundtruth BEV semantic maps into tokenized, discrete and sparse BEV representations, termed BEV tokens, accompanied with a discrete embedding space (i.e., the codebook embedding). Each BEV token is the index of the nearest neighbor in the codebook embedding for an encoded BEV patch feature, representing the high-level semantics of a BEV patch. BEV tokens serve as a new classification label to directly supervise the PV feature learning via a specialized token decoder in our pipeline. The training of the generative model and the token decoder module is separated. By aligning with the sparse BEV tokens, our token decoder module is able to rely solely on sparse backbone features directly queried by token queries for BEV token prediction using an arbitrary transformer-like architecture [5\u20137]. Simultaneously, directly employing these sparse features for token prediction bypasses the challenges of building accurate dense BEV features in common practice. The predicted tokens can be integrated into BEV embeddings through the off-the-shelf codebook embedding for generating the final high-quality BEV semantic maps. This process is similar to the human brain's memory mechanism [8], where the targets (BEV map layouts) are encoded into highly abstract, sparse representations (BEV embeddings) through memory neurons (BEV tokens) that can be activated by specific visual signals (generated based on token queries).\nWe evaluate our proposed VQ-Map on both the surround-view and monocular map estimation tasks, and our method sets new records in both tasks, achieving 62.2/47.6 mean IoU for surround- view/monocular evaluation on nuScenes [9], as well as 73.4 IoU for monocular evaluation on Argoverse [10].\nIn summary, our contributions are as follows: (1) We propose a novel pipeline VQ-Map exploring a discrete codebook embedding to generate high-quality BEV semantic map layouts. The acquired prior knowledge subsequently helps to effectively align the sparse backbone image features with the generative models based on a specialized token decoder, leading to more accurate BEV map layout estimation with generation. (2) By formulating map estimation as the alignment of perception and generation, our achieved BEV codebook embedding serves as a bridge between PV and BEV, and can be used in the off-the-shelf manner. (3) Extensive experiments show that our VQ-Map establishes new state-of-the-art performance on camera-based BEV semantic segmentation. Meanwhile, we confirm that as a PV-BEV alignment method, token classification is more effective than value regression."}, {"title": "2 Related Work", "content": "BEV Map Layout Estimation. Most existing approaches treat BEV map layout estimation as a semantic segmentation task in the BEV frame, where map elements are rasterized into pixels with each allocated multiple class labels. As a pioneer of such technology, LSS [3] explicitly predicts discrete depth distributions on image features, and then 'lifts' these 2D features to obtain pseudo 3D features, which are finally flatten into BEV features through a pooling operation. Building upon it, BEVFusion [1] introduces LiDAR point clouds and implements multi-sensor fusion within a unified BEV space, effectively maintaining semantic and geometric information. Other approaches [11-15], such as VectorMapNet [12] and HIMap [15], tackle layout issues by incorporating vectorized prior maps. Besides, TaDe [16] utilizes a task decomposition strategy to improve monocular BEV semantic segmentation performance.\nRecently, some methods utilize generative model-based technologies to enhance the performance of BEV map layout estimation. MapPrior [17] employs a generative map prior built on VQ-GAN [18] architecture to capture the detailed structure of traffic scenarios on the basis of conventional dis- criminative models, achieving a unified advantage in precision, realism and uncertainty awareness. Furthermore, DDP [19] and DiffBEV [20] focus on integrating the denoising diffusion process [21] into contemporary perception frameworks, exhibiting outstanding performance.\nThe above mentioned work MapPrior [17] and TaDe [16] both approach the BEV map segmentation task through two stages: a perceptual stage and a generative stage, which is relevant to our work. However, MapPrior aligns with the generative model by deriving complex BEV variables, which are constrained by the challenges of acquiring accurate dense BEV features. As for TaDe, training the generative model based on polar inverse-projected BEV groundtruth maps results in the loss of certain prior knowledge embedded in conventional BEV maps, making it prone to artifacts. In contrast, our method aligns the generative model with tokenized discrete representations, which are more meaningful and easier to predict, while also preserving BEV map prior knowledge.\nTokenized Discrete Representation. VQ-VAE [4] innovatively employs codebook mechanisms to establish an encoder-decoder architecture in a tokenized discrete latent space, capturing and representing richer and more complex data distributions. Following this approach, other generative models such as VQ-GAN [18], DALL-E [22] and VQ-Diffusion [23] also map inputs into discrete tokens corresponding to codebook entries to represent high-dimensional data. Meanwhile, some visual pre-training works [24, 25] use tokens to represent image patches and treats the prediction of masked tokens as a proxy task. Recently, UViM [26], Unified-IO [27] and AiT [28] encode various outputs as tokens and predicts them through an auto-regressive modeling [29], modeling a wide range of visual tasks. In this paper, we draw inspiration from the above work to predict BEV tokens for generating high-quality BEV map layouts."}, {"title": "3 Methods", "content": "We herein summarize our VQ-Map perception framework in Fig. 2 as follows. Firstly, we create the discrete representations which encapsulate the high-level BEV semantics for different BEV elements in the groundtruth maps to serve as the prior knowledge (i.e., the codebook embedding) for map generation. Secondly, we conduct the PV-BEV alignment training with the specially designed token decoder module to predict the BEV tokens associated with the corresponding groundtruth maps. Finally, we directly combine the off-the-shelf codebook embedding accompanied by the map generation decoder with the PV-BEV alignment module to predict the BEV map layouts."}, {"title": "3.1 Discrete Representation Learning for BEV Generation", "content": "Similar to some visual pre-training methods [24, 25], we formulate the discrete representation learning as the task of BEV map reconstruction via a sequence of discrete tokens to acquire prior knowledge for the high-level BEV semantics. We obtain this tokenized discrete space by employing the VQ-VAE architecture [4], which comprises three modules: BEV Patch Embedding $\\mathcal{E}$, Vector Quantization $\\mathcal{Q}$ and BEV Map Generation Decoder $\\mathcal{D}$. Roughly speaking, $\\mathcal{E}$ transforms local BEV semantic patches into more abstract high-level semantics; $\\mathcal{Q}$ then clusters the semantics derived from patch embedding to create the discrete representations; and finally, $\\mathcal{D}$ is attached to utilize these discrete representations for reconstruction of the corresponding groundtruth maps."}, {"title": "BEV Patch Embedding $\\mathcal{E}$.", "content": "BEV semantic maps significantly differ from the raw images with complex scenes. They inherently represent high-level semantics annotated by humans, which eliminates the need to aggregate extensive features using heavy encoders. Specifically, we initially patchify a groundtruth BEV map $M \\in B^{C \\times H \\times W}$ into a sequence of non-overlapping BEV patches $\\{M^{i} \\in B^{C \\times P \\times P}\\}_{i=1}^{N}$, where $B = \\{0,1\\}$, $P$ is the patch size, $C$ is the number of the groundtruth map layouts and $N = HW/P^{2}$ is the patch number. Our patch embedding $\\mathcal{E}$ is simple, aiming to abstract high-level semantics $z^{i} \\in R^{D}$ from individual patches $M^{i}$, where $D$ is the embedded dimension. Fig. 3 shows some BEV patch images to visualize our discrete representation learning."}, {"title": "Vector Quantization $\\mathcal{Q}$.", "content": "We define a latent embedding space $V \\in R^{K \\times D}$ as our codebook em- bedding, where $K$ represents the maximum number of representations in the discrete latent space. We further denote it using the set $\\{v_{1}, v_{2}, ..., v_{k}, ..., v_{K} \\}$. Our vector quantization $\\mathcal{Q}$ receives the continuous latent vector $z_{e}$ from the patch embedding and outputs discrete latent $z_{q}$, termed BEV embedding, through the nearest neighbor search in the codebook. This is calculated as\n$z_{q} = \\mathcal{Q}(z_{e}) = \\underset{v_{k}}{arg \\ min} ||l_{2}(z_{e}) - l_{2}(v_{k}) ||_{2}$ (1)\nwhere $l_2$ means L2 normalization employed for codebook lookup based on cosine similarity, as described in ImprovedVQGAN [30]. Each discrete latent can also be represented by its index in the codebook as the BEV token:\n$k_{q} = \\underset{k}{arg \\ min} ||l_{2}(z_{e}) - l_{2}(v_{k})||_{2}$. (2)"}, {"title": "BEV Map Generation Decoder $\\mathcal{D}$.", "content": "We feed the BEV embeddings $\\{z_{q} = l_{2}(v_{k_{q}})\\}_{i=1}^{N}$ to our map generation decoder $\\mathcal{D}$ by firstly reshaping them into a grid format and then reconstructing the original groundtruth BEV map following\n$M' = \\mathcal{D}(\\mathcal{Q}(\\mathcal{E}(M))).$ (3)"}, {"title": "Training Loss.", "content": "The overall training loss includes a VQ loss $\\mathcal{L}_{vq}$ based on the codebook embedding due to the non-differentiable vector quantization operation besides the reconstruction loss $\\mathcal{L}_{re}$. Unlike the common practice, we additionally incorporate a loss term reflecting the patch-level data augmentations (such as small-scale rotations, translations, and resizing) to aid clustering. The VQ loss is defined as:\n$\\mathcal{L}_{vq} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{N_{aug}}[||sg(l_{2}(z_{e}^{i})) - l_{2}(z_{q}^{i})||_{2}^{2} + ||sg(z_{q}^{i}) - l_{2}(z_{e}^{i})||_{2}^{2} + ||sg(z_{e}^{i'}) - l_{2}(z_{e}^{i})||_{2}^{2}]$ (4)\nwhere $sg$ denotes stop-gradient, $N_{aug} = 3$ denotes the number of patch augmentations, and $z'$ is the continuous latent vector derived from the augmented patch. We also use the exponential moving average (EMA) [4] to update the codebook embedding.\nWe utilize the class-specific weighted mean square error (MSE) for computing the reconstruction loss $\\mathcal{L}_{re}$, where the weight of each class for each sample is inversely proportional to the number of groundtruth pixels in that class, $M \\in B^{H \\times W}$ and $M' \\in R^{H \\times W}$:\n$\\mathcal{L}_{re} = \\frac{1}{C} \\sum_{c=1}^{C} \\frac{||M_{c} - M'_{c}||_{1}^{2}}{1 + ||M_{c}||_{1}}$ (5)\nand the final loss $\\mathcal{L}$ is defined as:\n$\\mathcal{L} = \\mathcal{L}_{re} + \\mathcal{L}_{vq}.$ (6)"}, {"title": "3.2 Token Prediction with Sparse Features for PV-BEV Alignment", "content": "Once the codebook embedding $V$ is obtained from the vector quantization $\\mathcal{Q}$ and BEV patching embedding $\\mathcal{E}$, the dense prediction task of BEV semantic segmentation for the map layouts, repre- sented by $M \\in B^{C \\times H \\times W}$, can be transformed into a sparse token classification task represented by a grid $M \\in A^{(H/P) \\times (W/P)}$ consisting of BEV tokens (see Eq. (2)) as illustrated in Fig. 2, where $A = \\{1, 2, 3, ..., K\\}$.\nSince each token represents a BEV patch instead of a BEV pixel as mentioned in Sec. 1, the BEV tokens are significantly sparser compared to the dense BEV features in common practice and the final BEV semantic maps. Each BEV semantic token can be recognized based on the semantics at the corresponding position in the image features. So we propose a transformer-like token decoder module based on deformable attention [6] to query image semantic features at the individual position to predict the corresponding BEV token. This approach is inspired by the transformer-based objection detection methods in DETR [7], Deformable-DETR [6] and BEVFormer [2]."}, {"title": "Token Decoder Module.", "content": "Our token decoder consists of multiple patch-level cross-attention layers, a convolutional layer and a head as shown in Fig. 4. It takes multi-scale image backbone features from the feature pyramid network (FPN) [31] and the camera calibration as input, and outputs the predicted sparse BEV tokens for PV-BEV alignment. We use $N$ learnable embeddings as our token queries, each associated with a specific 3D position determined via the LiDAR coordinate system. Specifically, the token reference points (or anchors in the following) are initially set in the LiDAR coordinates and subsequently projected to image space using the camera and LiDAR calibration."}, {"title": "4 Experiments", "content": "We evaluate the performance of our VQ-Map on both the surround-view and monocular BEV map layout estimation tasks and use the Intersection-over-Union (IoU) metric for evaluation.\nWe perform the surround-view experiments on nuScenes [9] involving 1000 on-road scenes from four locations in Boston and Singapore. Each scene has a duration of approximately 20 seconds, resulting in a total of 40k samples/key-frames. Each sample is captured by six monocular cameras, covering a 360\u00b0 panoramic view around the ego vehicle. For BEV map estimation, nuScenes provides manual annotations encompassing 11 layout classes. According to the prior studies [3, 1], we train and validate VQ-Map on 700 scenes with 28130 samples and 150 scenes with 6019 samples respectively. In addition, we transform the original map layouts into the ego-centric frame through rasterization. We evaluate the IoU scores for 6 map layout classes, i.e., drivable area, pedestrian crossing, walkway, stop line, car-parking area, and lane divider. Additionally, we calculate the mean IoU (mIoU) averaged over all of them.\nWe utilize both the nuScenes [9] and Argoverse [10] datasets for the monocular task following the common practice. In particular, Argoverse employs seven surround-view cameras for data collection. It comprises 65 training sequences and 24 validation sequences captured in Miami and Pittsburgh. Like the nuScenes dataset, Argoverse provides detailed and comprehensive semantic map layout annotations. We evalutate the IoU scores for drivable area, pedestrian crossing, walkway and car-parking area on nuScenes while only the drivable area on Argoverse."}, {"title": "4.1 Implementation Details for Surround-View Task", "content": "In this section, we primarily outline the experimental settings for the surround-view task on nuScenes, while leaving the settings for the monocular task in Appendix A due to the space limitation.\nData Preparation. Following BEVFusion [1], we generate the BEV semantic segmentation map within a square area surrounding the ego car, which spans from -50 meters to 50 meters along both the x and y axes. We set the segmentation resolution at 0.5 meters per pixel, which culminates in a final image output of 200 by 200 pixels in size. Considering the possibility of overlapping classes within the map, our model is designed to carry out binary segmentation across all classes. The input images are resized to 256\u00d7704."}, {"title": "4.2 State-of-the-Art Comparison", "content": "We conduct the state-of-the-art comparison for the surround-view BEV map layout estimation task by comparing to several recent competitive methods as shown in Tab. 1. Among them, BEVFusion [1] is widely regarded as a standard framework that integrates three key components, i.e., a backbone module, a view transformation and a task decoder. X-Align [34] uses perspective supervision to integrate perspective predictions based on dense BEV feature for enhanced performance. Map- Prior [17] employs a generative model to enhance the perceptual outcomes derived from BEVFusion. Meanwhile, MetaBEV [35] introduces an innovative cross-attention module subsequent to the dense BEV feature extraction, which is instrumental in addressing sensor failures. Additionally, DDP [19] leverages a denoising diffusion process [21] within its decoder to achieve better precision.\nTab. 1 shows that our VQ-Map sets a new mIoU performance record of 62.2 for the surround-view map estimation task on nuScenes when comparing to the above methods. In particular, VQ-Map achieves a 5.5 mIoU gain in comparison to BEVFusion, with a notable improvement of over 10 in the Stopline layout class. VQ-Map also consistently outperforms all other methods by significant"}, {"title": "4.3 Ablations and Analysis", "content": "Table 3: Ablation experiments on some key parameters of the token decoder. We perform ablations on the token decoder layer number M using layer dimension of 512, and ablations on different layer dimension by setting M to 8.\nToken Decoder Parameters. The number of patch-level cross-attention layers M in the token decoder has a substantial impact on the final performance, as evidenced in Tab. 3a. We note that as M increases, the mIoU value also increases. However, the mIoU improvement becomes marginal when M increases from 6 to 8, and we thus use 8 layers by default. Additionally, we investigate the effects of setting different dimensions for the token decoder layers in Tab. 3b, which shows that the optimal dimension is 512. Note that the following ablation studies all employ 6 layers and 512 dimension.\nDense vs. Sparse BEV Features. In the token decoder of our approach, the features before the classification MLP head can also be regarded as a kind of sparse BEV features with shape 25 \u00d7 25 \u00d7 512, obtained based on deformable attention. In this ablation, we compare the utilization of dense BEV features vs. sparse BEV features for predicting either the sparse tokens based on the off-the-shelf codebook embedding or directly the dense semantic maps, where the dense BEV feature is obtained through LSS [3] following BEVFusion with shape 128 \u00d7 128 \u00d7 80."}, {"title": "Computational Overhead Analysis.", "content": "We provide the computational overhead for our VQ-Map on the surround-view task as shown in Tab. 5, including the number of parameters, computational cost (MACs), and model training time. Decreasing the layer dimension of the token decoder from 512 to 256 resulting in a light version of our VQ-Map. Since the learned discrete representations also require a significant number of parameters, we further use a smaller architecture with $K = 128, D = 64$ in $\\mathcal{Q}$ and $\\mathcal{D}$ to achieve a tiny version of our VQ-Map. It shows that even the tiny version of our approach can still achieve comparable results to the recent SOTA methods in Tab. 1. Our approach not only demonstrates strong performance, but also saves much computational cost in comparison to the recent"}, {"title": "5 Conclusion", "content": "In this paper, we present a novel pipeline VQ-Map by aligning with the generative models using discrete BEV tokens, which efficiently enhances the performance of BEV map layout estimation. The core components of our method are the codebook embedding constructed via vector quantization as prior knowledge, serving as a bridge between PV and BEV, and the specially designed BEV token decoder to facilitate the PV-BEV alignment, both of which enable the generation of high-quality BEV semantic map layouts. We hope that our work will inspire further research on vector quantization, providing assistance not only for map estimation and its downstream tasks, but also for a wide range of other applications.\nLimitations and future work. A significant limitation of our approach is our inability to handle semantics that are position-sensitive and have small areas. It is challenging for patch-level semantics to effectively represent position information for small-scale semantics using the architecture similar to VQ-VAE. Tokenization in our approach is more robust against noise and geometric changes. However, it may lead to a loss of some detailed spatial information. When the dangerous holes and some random obstacles appear in the realistic AD environment, an anomaly detection module may be needed to handle this situation. Our approach is a specialized method only for BEV map generation at present, but we believe the token-based multi-task modeling for autonomous driving is very promising. Additionally, tokenized intermediate results are well-suited for combining with large language models.\nBroader Impact. Accurate BEV layout estimation helps drivers and autonomous driving systems better understand the surrounding environment, thereby reducing the occurrence of traffic accidents and improving traffic safety. However, it may be misused to capture detailed information about the environment, including sensitive data such as building layouts, infrastructure, and so on."}, {"title": "Appendix", "content": "A Implementation Details for the Monocular Experiments\nData Preparation. Following PON [39], we define a square region in front of the given monocular camera, spanning [0,50] \u00d7 [-25, 25] meters. Setting the resolution to 0.25 meters per pixel results in a final map size of 200 \u00d7 200 pixels during the evaluation. Additionally, the images are resized to 600 \u00d7 800 for nuScenes and 600 \u00d7 960 for Argoverse. We perform the monocular experiments using all available surrounding cameras with training/validation splits outlined in PON [39].\nDetails of the Monocular Layout Estimation Model Training. The groundtruth map is resized to 224 x 224 during our model training, and the size of the BEV patch is set to 16 \u00d7 16, resulting in a total of 14 x 14 = 196 patches. We use ResNet50 as the backbone to align with the previous work [39] and employ a token decoder with 6 patch-level cross-attention layers and 256 layer dimension.\nFor the discrete representation learning, we set the initial learning rate to le-4 with a batch size of 32 per GPU, and conduct training on 2 NVIDIA A100 (40G) GPUs for 50 epochs, totaling 28 GPU hours. Subsequently, for training PV-BEV alignment, we adjust the initial learning rate to 5e-4 and the batch size to 8 per GPU for training on 4 NVIDIA A100 (40G) GPUs for 40 epochs, totaling 42/44 GPU hours for nuScenes/Argoverse. We also resize our output map size from 224 \u00d7 224 back to 200 \u00d7 200 using a maxpooling operation to aligns with the previous work's results during the evaluation. Since our VQ-Map only predicts BEV tokens within the view frustum range, zero padding is used for latent representations in other areas. When calculating the focal loss, the patches that LiDAR cannot reach will be ignored."}]}