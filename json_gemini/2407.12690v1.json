{"title": "The Dual Imperative: Innovation and Regulation in the AI Era", "authors": ["Paulo Carv\u00e3o"], "abstract": "This article addresses the societal costs associated with the lack of regulation in Artificial Intelligence and proposes a framework combining innovation and regulation. Over fifty years of AI research, catalyzed by declining computing costs and the proliferation of data, have propelled Al into the mainstream, promising significant economic benefits. Yet, this rapid adoption underscores risks, from bias amplification and labor disruptions to existential threats posed by autonomous systems. The discourse is polarized between \"accelerationists,\" advocating for unfettered technological advancement, and \"doomers,\" calling for a slowdown to prevent dystopian outcomes. This piece advocates for a middle path that leverages technical innovation and smart regulation to maximize Al's potential benefits while minimizing its risks, offering a pragmatic approach to the responsible progress of AI technology. Technical invention beyond today's most capable foundation models is needed to contain catastrophic risks. Regulation is required to create incentives for this research while addressing current issues.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence has the potential to significantly improve personal and business productivity resulting in enhanced economic output and wealth. On the other hand, given its transformative impact, this technology will affect labor markets and can mirror and amplify biases that currently exist in society and the corpora of data it is trained on. In the long term, it might bring safety risks if misused or developed in a way that allows for and facilitates autonomous replication. It is in this context that the merits of AI regulation are debated today. Should AI be regulated and, if so, how to do it in a way that protects innovation?\nAccording to Ben Buchanan, a professor at Georgetown serving in the White House as special advisor for AI, one can summarize modern artificial intelligence in a single sentence: machine learning systems use computing power to execute algorithms that learn from data. This is the AI triad of algorithms (people), data, and computing power. (Buchanan, 2020) Over fifty years of AI research, catalyzed by declining computing costs and the proliferation of data, have propelled AI into the mainstream, promising significant economic benefits. The release by OpenAI in November of 2022 of a chatbot interface to Large Language Models (LLMs) made this technology available and accessible to the large public. ChatGPT, its product instantiation, achieved 100 million monthly active users in two months making it the fastest-growing consumer application until then. Knowledge-based tasks are being automated and, with that, doors to integrating AI into business processes have been opened."}, {"title": "Benefits and Risks of AI", "content": "The use of Artificial Intelligence goes beyond LLMs and GPTs including other machine and deep learning techniques and product formats. The list of personal productivity enhancements is long. Those include virtual assistants, biometric authentication, personalized news feeds, grammar and spell-checking, smart home devices, navigation apps, fraud detection, and disease prediction based on analysis of personal data. Similarly, there are business value enhancement possibilities. Operations are streamlined, and costs are reduced. Customer experiences are improved through personalized services and support. Companies optimize supply chain management and inventory control and use predictive analytics for better decision-making and market insights. Product and service offerings are upgraded based on customer feedback and data analysis. Education and learning experiences can be personalized and education outcomes can be elevated. Repetitive and mundane tasks are automated allowing humans to focus on more creative work. Al-powered robots can perform dangerous tasks in risky situations that would be unsafe for humans. Some even envision AI helping deliver on the promise of unbiased and accurate decision-making eliminating human biases.\nYet the rapid adoption underscores risks, from bias amplification and labor disruptions to existential threats posed by autonomous systems. This appears to be one of those technologically driven inflection points in history. When this level of change happens, institutions also change, and it is natural to feel a sense of discomfort with the unknown ahead. This unease may not differ from what was experienced around 10,000 BC during the Agricultural Revolution, when the development of agriculture and domestication of animals allowed humans to transition from a hunter-gatherer lifestyle to settled communities, leading to the rise of early civilizations. During the 15th century, the invention of the movable-type printing press enabled the mass production of books, facilitating the spread of knowledge, literacy, and ideas across Europe. In the 18th and 19th centuries, the Industrial Revolution with the development of steam power, mechanization, and factory systems transformed manufacturing processes, leading to urbanization and economic growth. More recently, during the 20th century, computers and the Internet transformed information processing, communication, and access to knowledge on a global scale. At each of these successive waves of technology-driven societal transformation, societies have changed and adapted. They have navigated a narrow path, harnessing the benefits of progress while containing the risks. The journey has not been linear, and there is an ongoing struggle with some of the unintended consequences of this progress, from inequality to climate change, but the pursuit continues.\nSomething feels different about the change that AI brings. It may be the exponential nature and speed at which it is happening or, using the technology entrepreneur Mustafa Suleyman's characterization \u201c[t]he coming wave is a supercluster, an evolutionary burst like the Cambrian explosion.\" (Suleyman, 2023, p.57) Or it may be that this one is hitting a bit closer home, threatening the jobs of the elites, and jeopardizing the ability of knowledge workers to earn a living. On a different scale, it prompts the question of what it means to be human if there exists something that can, at some point, approximate or surpass human intelligence. Whatever the reason, it would be a disservice to humankind not to align technology with societal values and use it to advance general welfare. It is at this uncomfortable inflection point that society finds itself, in the middle of a transition and uncertain of the outcome. This creates the conditions for a polarized debate between \"accelerationists,\" advocating for unfettered technological advancement, and \"doomers,\" calling for a slowdown to prevent dystopian outcomes. The way forward, however, should be a middle path that leverages technical innovation and smart regulation maximizing Al's potential benefits while minimizing its risks, offering a pragmatic approach to the responsible advancement of AI technology. Technical invention beyond the current frontier models is needed to contain catastrophic risks. Regulation is required to create incentives for this research while addressing current issues."}, {"title": "Types of AI Risks", "content": "Al risks typically fall into two main categories. The first encompasses institutional or governance risks. Research in fairness, accountability, transparency, and ethics (FATE) is an increasingly well-established field that views data science and AI as sociotechnical processes, in which the engineering techniques are implemented in a context of social norms and expectations. Scholars in this area document harms including the perpetuation of existing, unjust power relations and the exacerbation of extreme concentrations of power in the hands of a few. (Chan, 2023) They also document problems like privacy violations, evolving vulnerabilities that lead to new security challenges, algorithmic bias, and lack of transparency around how AI models are developed, which can result in accountability and legal issues.\nThe use of AI by authoritarian governments to control populations poses significant risks to human rights and civil liberties. Al surveillance systems could be used for pervasive monitoring, tracking, and profiling of citizens, enabling unprecedented levels of social control and suppression of dissent. Al decision-making systems could automate discriminatory policies, reinforce biases, and perpetuate injustices. Al propaganda tools could manipulate public opinion and spread disinformation on a massive scale. Overall, AI in the hands of authoritarian regimes could enable new forms of high-tech oppression, eroding privacy, freedom of expression, and democratic principles.\nThe second category of existential (also known as catastrophic) risks is different. The Center for AI Safety (Hendrycks et al., 2023) grouped these issues into four categories: malicious use, the intentional use of AI to cause harm; AI race, the deployment of unsafe Als as part of a competitive race; organizational risks, accidents caused human factors or complexity of systems; and rogue Als, the loss of control to agents more intelligent than humans. These have in common a much larger scale of impact and the fact that they are not likely, at least in their most extreme versions, at the current state of technological development. The pressing question, however, concerns the amount of time available before these risks could materialize, given the accelerating pace of development. Malicious actors could intentionally use Als to create pathogens for chemical warfare or use AI to lower the cost of propaganda and dissemination of disinformation. The latter is a real and present danger and, the former still lurks on the horizon. Absent adequate incentives and control frameworks, competition between corporations or nation-states can lead developers into a race to release products without the needed safety characteristics. These products can proliferate labor exploitation or lead to large-scale unemployment without the needed societal safety nets. The same technology can be used for development of new"}, {"title": "The Three Components of AI and Industry Concentration", "content": "To understand the current landscape, revisiting the basic building blocks of modern artificial intelligence proves useful: powerful and now affordable computing power is applied to sophisticated algorithms that are trained on massive amounts of data. In their 1943 work A Logical Calculus of the Ideas Immanent in Nervous Activity, Warren S. McCulloch a neurophysiologist, and Walter Pitts, a logician, proposed a model of neural activity based on logical operations. This laid the theoretical foundation for modern neural networks by modeling neural activity based on logical operations. (McCulloch & Pitts, 1943) In 1961, at the Massachusetts Institute of Technology, Marvin Minsky in his work Steps Toward Artificial Intelligence introduced the AI term. (M. Minsky, 1961) Later, still during this decade, in 1967 the mathematician Alexey Ivakhnenko, in his work Cybernetics and Forecasting Techniques introduced the first functional learning algorithm, for which he is often referred to as the father of deep learning. (Ivakhnenko & Lapa, 1967) From there on progress continued with the introduction of backpropagation by scholars at the University of California and Geoffrey Hinton from the Carnegie-Mellon University in the late 1980s (Rumelhart et al., 1986), and a few decades later the seminal Attention Is All You Need paper introducing transformers, the base technology for ChatGPT. (Vaswani et al., 2023)\nThese breakthroughs were followed by periods without much visible innovation, attention, or commercial investments commonly known as the AI winters. This dynamic changed starting in about 2008-2010. By then the cumulative effects of a few decades of Moore's law, doubling the number of circuits in semiconductors every two years while reducing the cost of computing, unleashed a golden era for algorithmic innovation with researchers focused on developing new techniques for AI. People started to question the prevailing idea of diminishing returns on training larger models and started using GPUs' massively parallel processing power to further accelerate the process. The movement of these training runs from private in-house infrastructure to the cloud lowered, even more, the barriers to training models. As an example of the size and"}, {"title": "Is Governance Keeping Up?", "content": "The profoundly transformative potential of AI coupled with concerns about the institutional and catastrophic risks previously discussed has led to the questioning of traditional corporate and state governance models. Startups are experimenting with new models. Governments are scrambling to try and build state capacity and skills to keep up with a new reality. Labor markets are under pressure as even more power shifts from labor to capital.\nWe can contrast today's corporate governance to the newer models adopted by companies like OpenAI and Anthropic. Traditional corporate governance, from the early days of the 20th century, focuses on shareholder profit maximization. In 1970, economist Milton Friedman argued that a company's sole responsibility is to maximize profits for shareholders while obeying the law, cementing the shareholder model's dominance. (Friedman, 1970) Alternatively, stakeholder governance involves a commitment to value creation, not just for shareholders but also for other stakeholders such as employees, customers, and society at large. Stakeholder governance has seen a resurgence in the past decade as companies are increasingly encouraged to consider their broader societal impact. Benefit corporation laws, which legally require companies to consider stakeholder interests, have been passed in 51 jurisdictions globally since 2010. In 2019, the Business Roundtable, representing 181 CEOs in the United States, issued a statement supporting stakeholder governance over shareholder primacy (Business Round Table, 2019). The 2020 Davos Manifesto by the World Economic Forum advocated for companies to consider their impact on all stakeholders. (Schwab, 2019)\nOpenAI's corporate governance structure is uniquely designed to prioritize its mission over profits. OpenAI Global LLC is controlled by a nonprofit, OpenAI, Inc., meaning that investors can't hire or fire board members or control the board. This construct was tested in November 2023 when the board fired the CEO but reverted this action after pressure from employees and investors. The company's charter emphasizes that OpenAI's mission is to ensure safe artificial general intelligence (AGI) which benefits all humanity, taking precedence over any profit obligations. (Open AI, 2023)"}, {"title": "Emerging Regulatory Frameworks", "content": "There is a widening gap between the leading edge of technology innovation and the ability to set standards or create regulations. While Artificial Intelligence is not a new science, its popular use is, and generative AI shook the policy world putting into high gear the debate about the ethical and responsible use of the technology. Industry and governments have come up with voluntary standards (The White House, 2023a), the US government issued guidelines (The White House, 2023b), lawmakers enacted regulations (European Parliament, 2023), and libertarians issued manifestos (Andreessen, 2023) in what looks like a policy free for all. Some say that the guidelines, principles, or voluntary commitments are vague on purpose, to allow room to operate and preempt regulation in an attempt at regulatory capture. Others criticize governments, policymakers, and lawmakers saying that regulating technology is an exercise in futility since it moves too fast. The question as to whether regulating the outcomes is enough and what kind of normative values should be applied by different societies remains open. In this complex system of intertwined questions, much headway has been made during 2023 and we are starting to move away from the ethos of \u201cmove fast and break things\u201d to a more nuanced debate about the ethical and responsible use of technology.\nThree main emerging approaches are starting to dominate the regulatory scenario. These proposals and laws demonstrate the global race to regulate Al with a goal to balance innovation with managing risks as AI rapidly evolves. The first to be enacted in the form of law is the European Union's Artificial Intelligence Act (EU AI Act). Started in 2021, it was approved by the European Parliament on March 2024 and is now expected to officially become law by later this year. It applies to all 27 EU member states and is likely to create a \"Brussels Effect\", influencing AI regulation globally. The second approach is the United States federal guidelines, issued via a White House executive order in October of 2023. In the US there is no single comprehensive federal AI law yet, but President Biden has mobilized the regulatory and procurement power of the US government and its agencies. In parallel, Congress is considering Al legislation and some states have passed laws. The Cyberspace Administration of China (CAC), China's top internet regulator, released a draft in April of 2023 of proposed measures for Generative AI to regulate the provision of services within mainland China. Each approach has its advantages and shortcomings and has been designed within the prevailing normative and judicial frameworks of their markets. The European Union emerged as the early regulatory superpower, the United States maintained a tradition of adversarial legalism (Kagan, 2019), and China prioritized state control. These efforts have developed a palette one can now choose from when designing and enacting smart regulation.\nIn Europe, the \"EU AI Act\" categorizes AI systems based on their level of risk. The Act prohibits AI systems that pose unacceptable risks and those using manipulative or deceptive techniques, social scoring systems, and certain biometric categorizations, ensuring that these technologies do not undermine fundamental rights or societal values. High-risk AI systems, like those used in critical infrastructure or that influence legal outcomes, face stringent regulatory requirements such as rigorous assessments and compliance processes. Limited-risk AI systems, like chatbots, must ensure transparency to users about their interactions with AI. Most AI applications that pose minimal risk, like video games and spam filters, remain largely unregulated. The AI Act places substantial responsibilities on providers and deployers of high-risk AI systems. Providers, regardless of their location, must meet the Act's requirements if their systems are used within the EU. Deployers, including EU entities using high-risk AI, have specific obligations, however, these are less extensive than those of the providers. The Act also addresses general-purpose AI,"}, {"title": "We Need a Combination of Innovation and Regulation", "content": "The dichotomy between technological advancement with no regulation, and those advocating for a slowdown to prevent catastrophic risks is a false choice. Both are needed. It is na\u00efve to assume that corporations, investors, and nation-states will interrupt the progress already made in the field. A balance between technical innovation and smart regulation must be found to maximize Al's potential benefits while minimizing its risks. This balance will come from a combination of scientific and technical inventions that will address the open issues with the current state-of-the-art technology and regulation to address the current institutional risks and create incentives for safety research and development.\nLeading techniques in artificial intelligence continue to face substantial technical challenges, notably the alignment problem. This issue revolves around ensuring that AI systems' actions and goals are consistent with human values and objectives. Misalignments can occur when Al systems interpret their objectives in ways that differ from their creators' intentions, often due to the complexity and underspecified nature of their programming. (Ngo et al., 2024) This challenge becomes particularly critical in the context of agentic AI systems. Agentic systems are characterized by their capability to act autonomously. They could accomplish a goal provided by designers, without a concrete specification of how the goal is to be accomplished and, in doing so, they could affect the world without a human in the loop. Multiple agentic systems can initiate action as if they were trained to achieve a particular quantifiable objective and make decisions that are dependent upon one another to achieve a goal or make predictions over a long time horizon. (Chan et al., 2023) A significant risk with advanced, autonomous AI is its potential to seek power. This encompasses behaviors like acquiring resources, proliferating, and resisting shutdown efforts, which might occur if the AI perceives these actions as necessary to fulfill its misaligned goals. This assumes future AI systems will have advanced capabilities that will outperform humans, will be able to make and execute plans in pursuit of assigned objectives, and will develop strategic awareness that will determine the advantages of assuming and maintaining power over humans and the real-world environment. (Carlsmith, 2022)\nAddressing these risks requires robust techniques for containing and guiding AI behavior. The focus must be on creating AI that is not only powerful but also aligned and constrained within safe operational boundaries. Liquid Neural Networks, Objective-Driven Als, and Generative Flow Networks are examples of innovation trying to address the limitations of LLMs. Liquid Neural Networks (LNNs), developed by a team of researchers led by Prof. Daniela Rus at MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) in 2020, are designed to adapt and learn continuously from streaming data. Unlike traditional large language models, LNNs"}, {"title": "What Have We Learned from Social Media?", "content": "In his 2018 book, Custodians of the Internet, Tarleton Gillespie from Microsoft Research, reflects on the previous decade. \u201cThe dreams of the open web did not fail, exactly, nor were they empty promises to begin with. Many people put a great deal of effort, time, resources, and dollars to pursue these dreams, and to build infrastructures to support them. But when you build a system that aspires to make possible a certain kind of social activity, even if envisioned in the\nA combination of regulatory measures, support for technological innovation, transparency, and education can mitigate the present and future risks, without stifling the economic engines of the digital age. This leads back to the central argument of this discussion: that technical invention beyond current frontier models is necessary to contain catastrophic risks, and regulation is needed to create incentives for such research while addressing current issues."}, {"title": "Conclusion", "content": "This article advocates for a path in the evolution of AI that balances technical innovation with necessary regulation. The analysis indicates that advancing AI technology beyond current models is crucial to effectively mitigate catastrophic risks. Simultaneously, implementing thoughtful regulation is essential to incentivize safety-focused research and address existing and emerging concerns.\nThe argument presented advocates for a dual approach where technological advancement and regulatory measures are intertwined. Effective AI regulation should not stifle innovation but rather guide it in a direction that enhances societal welfare and mitigates risks. The emerging regulatory frameworks from the European Union, the United States, and China demonstrate a collective recognition of the importance of comprehensive governance that accommodates both innovation and safety.\nThe experiences with other modern technologies, such as social media, underscore the need for a more proactive approach. The lack of adequate social media regulation, extensive legal safe harbors, and the prevailing imbalance between profit and public interest demonstrate the limitations of a laissez-faire approach to technology governance. These lessons highlight that without strategic regulation, the risks associated with technological advancements can undermine their potential benefits. With AI, the benefits are too important for us not to put in place the needed safeguards and incentives for its responsible and safe use.\nThe choice between halting the development of AI technology and accelerating it without restraint presents a false dichotomy. Ultimately, the future of AI should not be left to market forces or technological determinism alone. It requires a concerted effort from all stakeholders\u2014 governments, corporations, civil society, and the scientific community\u2014to craft an environment where technological advancements are aligned with broad societal benefits. This alliance must combine the strengths of cutting-edge technical invention and innovation, robust public policy, and informed social analysis. The right balance between the two will change over time as the technology evolves and potential risks are uncovered. It will also be a normative decision that could vary across different societies based on the ethical, moral, and cultural values and standards that define what is considered rational or optimal behavior in each of these.\nEmphasizing responsible development, transparency, and inclusivity will ensure that AI serves as a tool for enhancing human capabilities and addressing pressing global challenges rather than exacerbating existing inequalities or creating new forms of risk. As one looks into the future, the collective responsibility is clear."}]}