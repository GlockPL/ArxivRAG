{"title": "Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation", "authors": ["Pablo Messina", "Ren\u00e9 Vidal", "Denis Parra", "\u00c1lvaro Soto", "Vladimir Araujo"], "abstract": "Advancing representation learning in specialized fields like medicine remains challenging due to the scarcity of expert annotations for text and images. To tackle this issue, we present a novel two-stage framework designed to extract high-quality factual statements from free-text radiology reports in order to improve the representations of text encoders and, consequently, their performance on various downstream tasks. In the first stage, we propose a Fact Extractor that leverages large language models (LLMs) to identify factual statements from well-curated domain-specific datasets. In the second stage, we introduce a Fact Encoder (CXRFE) based on a BERT model fine-tuned with objective functions designed to improve its representations using the extracted factual data. Our framework also includes a new embedding-based metric (CXRFEScore) for evaluating chest X-ray text generation systems, leveraging both stages of our approach. Extensive evaluations show that our fact extractor and encoder outperform current state-of-the-art methods in tasks such as sentence ranking, natural language inference, and label extraction from radiology reports. Additionally, our metric proves to be more robust and effective than existing metrics commonly used in the radiology report generation literature. The code of this project is available at https://github.com/PabloMessina/CXR-Fact-Encoder.", "sections": [{"title": "1 Introduction", "content": "In the context of medical image analysis, radiology reports serve as a rich source of information. Radiologists routinely generate these free-text reports, which typically include sections such as comparison, indication, findings, and impression, as illustrated in Figure 1.\nRadiology reports have been employed for various purposes, including label extraction for structured supervision in medical image tasks (Irvin et al., 2019; Wu et al., 2021; Jain et al., 2021a), training data for models in report generation (Messina et al., 2022; Miura et al., 2021; Tanida et al., 2023), summarization tasks (Chen et al., 2023b; Ma et al., 2023), and the development of multimodal models capable of jointly understanding medical images and text (Wang et al., 2022; Boecking et al., 2022; Bannur et al., 2023).\nA crucial aspect in addressing such tasks is the accurate comprehension of factual information within the report. Specifically, the findings and impression sections can be considered as repositories of factual information regarding the imaging examination. These factual statements encompass various elements, including observations (such as abnormalities, diseases, or devices), interpretations derived from observations, references to anatomical locations, discussions on severity or confidence levels, comparisons to previous studies, and more. For instance, in Figure 1, one factual statement indicates no acute bone abnormality (a normal observation), while another describes a stable calcified granuloma within the right upper lung (an abnormality found in a specific anatomical site).\nDespite the various aforementioned applications that recent research has explored in the use of radiology reports, the persistent absence of effective methods for precise fact extraction and encoding from medical reports remains a critical challenge. As demonstrated in our experimental evaluation (Section 4), existing encoders and label extraction techniques frequently struggle to capture the nuanced details within free-text radiology reports, resulting in incomplete or inaccurate depictions of clinical information. This is evident in various aspects. For instance, existing text encoders developed for the medical domain may struggle to generate consistent representations of paraphrased statements (Table 1) or to differentiate between similar sentences conveying contradictory meanings (Table 2), a crucial requirement to prevent encoding erroneous diagnoses. Similarly, current label extraction methods often rely on rigid labeling schemes based on manually crafted rules, leading to incomplete capture of all factual statements within a report (Table 5). Similar limitations are also observed in commonly used evaluation metrics for radiology text generation (Table 4).\nIn this work, we propose a novel approach that leverages the capabilities exhibited by Large Language Models (LLMs) such as ChatGPT, which have showcased outstanding performance in medical contexts (Liu et al., 2023c; Katz et al., 2023; Liu et al., 2023b; Adams et al., 2023), to improve factual statement representation. Our methodology also takes advantage of the existing knowledge in expert-annotated datasets. These datasets offer indispensable training data and also serve as a benchmark to enhance our model's clinical terminology and context comprehension. Concretely, our contributions are three-fold:\n\u2022 A fact extractor: a novel and simple approach to extracting facts that leverages ChatGPT and a fine-tuned version of T5 (Raffel et al., 2020) to capture relevant information from Chest X-ray radiology reports without requiring annotations from radiologists.\n\u2022 A fact encoder: Chest X-ray Fact Encoder (CXRFE), a CXR BERT-based model (Boecking et al., 2022) fine-tuned with a multi-task approach that leverages domain expertise from radiologists as well as ChatGPT and T5 generated annotations. CXRFE exhibits significant improvement in fact comprehension, demonstrated on sentence ranking and natural language inference tasks.\n\u2022 A new evaluation metric for radiology text generation: CXRFEScore, which measures the factual accuracy of a generated text relative to a real text, by extracting and comparing the similarity of fact embeddings.\nWe release the weights of all our models, as well as the data and code necessary to replicate the results. We also release CXRFEScore as a Python library for ease of use by the research community."}, {"title": "2 Related Work", "content": "In this section, we discuss prior work on BERT-based approaches to radiology text representation and label extraction from radiology reports, and leave discussion of prior work on evaluation of factual correctness in radiology text generation, applications of LLMs to medical text, and knowledge distillation from LLMs to Appendix A.1."}, {"title": "BERT-based Approaches for Radiology Text Representation.", "content": "The advent of BERT (Devlin et al., 2019) has sparked notable progress in numerous NLP domains. This has inspired researchers to customize BERT for specific applications, including the medical field. Pioneering works such as BioClinicalBERT (Alsentzer et al., 2019), PubMedBERT (Gu et al., 2020), and BioLinkBERT (Yasunaga et al., 2022) have applied the masked language modeling (MLM) objective introduced by BERT to domain-specific corpora, such as PubMed paper abstracts and MIMIC-III (Johnson et al., 2016), an electronic health records dataset.\nMore recently, specialized variants like CXR-BERT (Boecking et al., 2022) and BioViL-T (Bannur et al., 2023) have been developed, targeting the unique challenges posed by CXR reports. CXR-BERT provides both a general version, pretrained with MLM on PubMed abstracts and MIMIC-III documents, and a specialized version, fine-tuned with MLM coupled with a radiology section matching loss specifically tailored for reports from the MIMIC-CXR dataset (Johnson et al., 2019b). BioViL-T adopts the same pretraining strategy as CXR-BERT but is subsequently fine-tuned using global and local multi-modal contrastive learning and image-informed MLM objectives. By combining reports with temporally sequenced image pairs, this approach enhances the understanding of radiological sentences with temporal descriptions.\nDrawing inspiration from these works, we adopt BERT as our base model for text encoding. However, unlike prior approaches that aim to improve BERT's representations with a single pre-text task (Reimers and Gurevych, 2019; Araujo et al., 2023), we employ a novel domain-specific multi-task learning protocol. This protocol leverages LLMs to generate large-scale supervision alongside expert-curated annotations from domain-specific datasets."}, {"title": "Label Extraction from Radiology Reports.", "content": "Our work is also related to the problem of extracting information, usually in the form of labels, from free-text radiology reports. A well-known example in the literature is the CheXpert labeler (Irvin et al., 2019), which uses a rule-based system to infer the presence or absence of 13 observations (plus the label \"No findings\"). CheXbert (Smit et al., 2020) and VisualCheXbert (Jain et al., 2021b) are subsequent versions that follow the same labeling standard of CheXpert but are based on BERT.\nThe Chest ImaGenome (Wu et al., 2021) dataset is another example that used a rule-based NLP system to label reports to construct scene graphs for the corresponding frontal images in the MIMIC-CXR dataset (Johnson et al., 2019a). RadGraph (Jain et al., 2021a) proposed a labeling scheme of entities and relations for radiology reports and trained a variant of BERT, DyGIE++ (Wadden et al., 2019), for entity and relation extraction on examples annotated by radiologists. PadChest (Bustos et al., 2019) followed a similar approach, by labeling reports with an LSTM that was previously trained on examples annotated by physicians.\nOur work contributes to this field by introducing a more flexible, open-vocabulary approach to information extraction, focused on extracting the essential factual information contained in the report, without imposing constraints that are too rigid. Specifically, we propose extracting factual statements, referred to as \"facts,\" from reports, by leveraging the proven effectiveness of recent LLMs."}, {"title": "3 Method", "content": "We introduce a two-stage method for encoding the information within a CXR report. In the first stage, called fact extraction (Section 3.1), we utilize LLMs to extract facts from the original sentences of the report. In the second stage, called fact encoding (Section 3.2), we employ a BERT-based text encoder to generate sentence embeddings for each extracted fact. When used in tandem, these two stages form a cohesive system capable of producing vectorial representations of the factual statements found within a CXR report."}, {"title": "3.1 Fact Extraction", "content": "Figure 2 outlines our method for extracting facts from radiology reports, with examples taken from the MIMIC-CXR dataset (Johnson et al., 2019b). Initially, we use regular expressions and simple rules to pinpoint relevant radiological sections in MIMIC-CXR reports, mainly Findings and Impression, but we also handle alternate headings. These sections are then divided into sentences. For simplicity, we use the sent_tokenize function from the NLTK library\u00b9. Next, we proceed to extract concise factual statements from each sentence. The main reason for doing this is that radiologists often write sentences that are noisy or complicated. Figure 2 shows two examples of such sentences. The first example contains multiple factual statements in one sentence, which can be simplified into shorter phrases. The second example is overly verbose, but the essential observation can be summarized in a brief phrase. We provide more examples in Table 6. Given the recent success of LLMs, an effective strategy to achieve this sort of extraction is by directing ChatGPT using a custom prompt.\nT5 as an alternative to ChatGPT. In theory, this entire process could be executed using off-the-shelf LLMs. However, the expenses associated with accessing the API to annotate the entire dataset can be prohibitive. Therefore, the alternative approach we adopted was to annotate only a strategically selected subset of sentences and then transfer the acquired knowledge from these annotations to a more cost-effective sequence-to-sequence model, such as T5, through fine-tuning. This approach mirrors the strategy employed by Yang et al. (2023), where a T5 is fine-tuned to condense verbose descriptions from GPT-3 in LLM-assisted image classification. We provide detailed implementation steps for this fact extraction procedure in Appendix A.2"}, {"title": "3.2 Fact Encoding", "content": "After we extract facts, we encode them to obtain vectors in a latent space via a text encoder model, called CXRFE. In this work, we rely on CXR-BERT (Boecking et al., 2022) to implement our fact encoder. Specifically, we use the CXR-BERT-specialized variant available on the Huggingface hub\u00b2, leveraging its built-in [CLS] token projection, which yields a 128-D vector serving as the final representation of the text.\nBuilding on top of CXR-BERT-specialized, we explore 6 different training tasks to enhance the latent representation of radiological sentences: triplet loss for sentence ranking (T), natural language in"}, {"title": "3.3 CXRFEScore", "content": "A potential application of our framework is in evaluating report generation from chest X-rays. We introduce CXRFEScore, an embedding-based metric that leverages both T5 and our CXRFE model for evaluation. Figure 3 illustrates how the metric works.\nGiven a reference report and a candidate report, we extract facts from each and represent them as embedding vectors, denoting the sets for the reference and candidate reports as R and C respectively. The cosine similarity matrix M of size |R|\u00d7 |C| is formed, where M_{i,j} represents the cosine similarity between the ith vector of R and the jth vector of C. This allows us to calculate S_{row}, S_{col}, and the final CXRFEScore as follows:\nS_{row} = \\frac{\\sum_i \\max_j M_{i,j}}{|R|}\nS_{col} = \\frac{\\sum_j \\max_i M_{i,j}}{|C|}\nCXRFEScore = \\frac{S_{row} + S_{col}}{2}\nThe equations of this metric resemble those of BERTScore (Zhang et al., 2020a). The key difference lies in CXRFEScore's comparison of fact embeddings rather than token embeddings. This metric illustrates the fusion of the two proposed stages: fact extraction (implemented with T5) and fact encoding (implemented with CXRFE). We provide evidence of the robustness of this metric compared to many existing metrics in Section 4."}, {"title": "3.4 Dataset construction", "content": "In our experiments, we primarily utilize the MIMIC-CXR dataset (Johnson et al., 2019b), which comprises 227,827 radiology reports associated with 377,110 chest X-ray images. However, we focus solely on utilizing the reports for our experiments, deferring the exploration of images and multi-modality for future work. Additionally, we incorporate annotations from the Chest ImaGenome dataset (Wu et al., 2021), which provides scene graphs linking observations to anatomical image locations for each frontal view image in MIMIC-CXR. These annotations serve two main purposes: facilitating our creation of a binary multi-label classification task and introducing a radiologist-informed annotation standard covering various observation types and anatomical locations. Similarly, we utilize RadGraph (Jain et al., 2021a), which offers an entity and relation annotation scheme for radiology reports in MIMIC-CXR, and datasets such as MedNLI (Romanov and Shivade, 2018), RadNLI (Miura et al., 2021) and MS-CXR-T (Bannur et al., 2023) for experiments on Natural Language Inference (NLI). To assess the performance and generalization ability of our two-stage framework, we also evaluate CXRFEScore using the 3955 reports and associated tag annotations of the IU X-ray dataset (Demner-Fushman et al., 2015). It is important to highlight that radiologists or doctors have partially or fully annotated these datasets, which adds significant value to training and evaluation. We direct the reader to Appendix A.3 for further details on these datasets and their utilization in our experiments."}, {"title": "3.4.1 LLM-assisted data augmentations for training supervision", "content": "As mentioned in Section 3.2, we use six distinct tasks to train CXRFE. For certain tasks, we leverage ChatGPT to generate additional training data. We elaborate in depth on these aspects in Appendix A.3, so here we only offer a concise overview.\nFor the triplet loss task (detailed in Section 3.4.2), some triplets incorporate paraphrased facts, which we generate using ChatGPT (an example is depicted in Figure 15). Similarly, we employ ChatGPT to produce challenging triplets, as indicated by the prompt in Figure 21.\nIn classification-related tasks, each fact is annotated with a JSON metadata object using ChatGPT. This object encompasses fields such as anatomical location, detailed observation, short observation, category, health status, and comparison status. The respective prompt for this process is shown in Figure 17. To refine the \"comparison status\" field, we use another prompt showcased in Figure 18. Also, we employ ChatGPT to label according to the scheme of the Chest ImaGenome dataset, producing additional observation and anatomical location labels, as shown in Figures 19 and 20.\nFor the task of natural language inference, we extensively utilize GPT-4 to generate training examples with distinct prompts (refer to Figures 22, 23, 24, 25, and 26)."}, {"title": "3.4.2 Triplet Sampling", "content": "CXRFE is trained to generate sentence embeddings that cluster semantically similar sentences in the embedding space through a triplet ranking task with binary cross-entropy loss. This approach uses a dataset of triplets, each one with an anchor, a positive sample (akin to the anchor), and a negative one. The difference in similarities is computed as\n\u2206sim(a, p, n) = sim(a, p) \u2013 sim(a, n)\nfrom their embeddings' dot product. By minimizing the binary cross-entropy loss, the encoder ensures closely related sentences are nearer and unrelated ones are more distant in the embedding space.\nWe define six triplet sampling rules to guide the selection process. Rule 1 prioritizes paraphrases generated with ChatGPT. Rule 2 involves sampling triplets based on the consensus of BioVIL-T and Levenshtein distance, with the anchor and positive sample sharing the same health status. Rule 3 ensures proximity between short observations, detailed observations, and original facts, along with their paraphrases. Rule 4 samples triplets based on Chest ImaGenome labels, ensuring that the anchor and positive sample share at least one label and that BioVIL-T and Levenshtein distance agree. Rule 5 ranks triplets according to the overlap of entities and relations from RadGraph. Rule 6 includes hard triplets generated by ChatGPT. For each rule, we sampled around 3 to 4 million training triplets and 1,000 each for validation and testing.\nThese rules encapsulate specific intuitions and heuristics regarding the ranking of sentence embeddings within the semantic space. The design of these sampling rules and the construction of the triplets dataset involve several technical details, which are elaborated upon in Appendix A.4."}, {"title": "3.4.3 Natural Language Inference", "content": "Natural Language Inference (NLI) aims to classify the relationship between a premise and a hypothesis into one of three categories: \"entailment\", \"neutral\", or \"contradiction\". For instance, consider a premise stating \u201cThere are no evident signs of pleural effusion\", while a hypothesis asserts \u201cThere are evident signs of pleural effusion\u201d. Despite their structural and lexical similarities, these sen"}, {"title": "4 Experimental Results", "content": "Our experiments evaluate different versions of CXRFE, each defined by a subset of the six tasks (T, C, NLI, EC, ER, SD) outlined in Section 3.2. This results in a total of 64 potential combinations. However, for the sake of simplicity, we heuristically assess only 11 combinations. Further details on our rationale are provided in Appendix A.3."}, {"title": "Triplet Ranking.", "content": "We evaluate CXRFE and multiple baselines from the literature on triplet ranking accuracy, using a separate test set of 1000 triplets per rule sampled according to the sampling rules detailed in Section 3.4.2. The left side of Table 1 presents these results. Notably, all different versions of CXRFE outperform all the baselines in triplet rules where ChatGPT is heavily involved, namely, paraphrases (R1(0), R1(a), R3) and hard triplets (R6). The hard triplets are especially challenging for the baselines, with BioViL-T only achieving 0.765 accuracy (row 6), while the best performing version of CXRFE achieves 0.952 (row 17). In addition to triplet loss (T), which is key to learning an embedding for these rules, we notice that sentence decoding (SD) and classification (C) appear to be helpful auxiliary tasks since most of the best scores are achieved by variants that include them (rows 8, 10, 15, 17)."}, {"title": "Sentence Ranking.", "content": "To complement the triplet ranking evaluation, which is based on heuristic sampling rules, we conduct a sentence ranking evaluation using 2,412 carefully annotated sentences provided by radiologists from the gold standard of the Chest ImaGenome dataset (Wu et al., 2021). These sentences are annotated with a vocabulary of 70 observations (yes (1), no (0), or omitted (-1)), and 38 anatomical locations (mentioned (1) or unmentioned (0)), resulting in a discrete vector of"}, {"title": "NLI.", "content": "Table 2 presents the NLI results using cosine similarity between sentence vectors, following a methodology akin to Bannur et al. (2023). This methodology specifically focuses on entailment and contradiction pairs, aiming to assess the efficacy of a text embedding in distinguishing between the two given a similarity threshold. We present results across three datasets: our NLI custom dataset mentioned in Section 3.4.3, the RadNLI test set, and MS-CXR-T. Our reported results are based on thresholds fine-tuned in the NLI custom dataset, alongside upper bounds obtained by tuning thresholds within the same data used for evaluation.\nNotably, employing the quadruplet entailment/contradiction loss (EC) and natural language inference (NLI) (rows 11 to 17) is essential to achieve high performance, significantly outperforming all baselines. In contrast, variants lacking EC and NLI (rows 7 to 10) exhibit a weaker result.\nAmong the baselines, CheXbert (row 4) demonstrates superior performance on RadNLI, while BioVil-T (row 6) is the clear victor on MS-CXR-T. However, all baselines struggle considerably in our NLI custom dataset and are outperformed across all three datasets by variants 11-17 of CXRFE.\nAdditionally, Table 3 presents the accuracy achieved on the RadNLI test set in the context of the typical 3-class classification task encompassing entailment, contradiction, and neutral classes. In this evaluation, we exclusively assess variants of CXRFE equipped with an NLI classification head (rows 9-15). For insights into implementing NLI classification, please consult Figure 7.\nWithin the existing literature, the strongest baseline identified is DoT5 (Liu et al., 2023a) (82.1), employing a sophisticated sequence-to-sequence approach based on T5. Furthermore, we conducted evaluations on GPT-4 (rows 5-6) and Meta-Llama-3-8B (AI@Meta, 2024) (rows 7-8) utilizing two distinct prompts: a simple prompt (Figure 24) and a prompt with Chain-of-Thought (CoT) + examples (Figure 25). Notably, the second prompt led to a significant performance boost for GPT-4 (from 82.3 to 89.0), whereas Meta-Llama-3-8B, an open-source LLM from Meta, only experienced a moderate improvement (from 58.1 to 61.5), with very low accuracies overall. Consequently, GPT-4 with the second prompt was selected as our \"oracle\" for generating additional training data (more details on this in Appendix A.3).\nMost versions of CXRFE showed superior performance compared to the baselines. Surprisingly, a version fine-tuned explicitly for NLI (row 15) even outperforms GPT-4 with CoT (row 6) by a narrow margin (89.8)."}, {"title": "CXRFEScore vs. existing metrics.", "content": "To assess the quality of our proposed metric, we conduct an evaluation of CXRFEScore alongside multiple metrics from the literature, as shown in Table 4. This assessment encompasses four settings: (1) a sentence ranking evaluation using 2412 sentences, (2) a report ranking evaluation with 500 reports, both sourced from the gold standard of Chest ImaGenome, (3) a report ranking evaluation with 3955 reports from the IU X-ray dataset (Demner-Fushman et al., 2015) leveraging the manual and automatic tags associated with each report, and (4) a natural language inference evaluation utilizing entailment (336) and contradiction (424) pairs from RadNLI and MS-CXR-T. Note that all these datasets are annotated by radiologists, thus serving as gold standards for metric comparison.\nAmong the baseline metrics, RadGraph F1 (rows 10-11) emerges as one of the most promising based on its performance on Chest ImaGenome Gold, which aligns with the findings of Yu et al. (2022). However, BERTScore's results (row 5) on Chest ImaGenome Gold are quite similar, achieving the highest AUC among the baselines (0.840). Additionally, BERTScore achieves the highest Jaccard index scores on IU X-ray among the baselines. Notably, CheXbert (rows 8-9), closely followed by CheXpert (rows 6-7), shows the fewest contradictions on Chest ImaGenome Gold.\nAll baseline metrics, however, are surpassed by CXRFEScore (rows 12-15) in all the evaluation metrics. A particularly striking observation is that the baseline metrics struggle significantly to differentiate between entailed and contradictory sentences, as indicated by the AUC results in the last column of Table 4. RadGraph F1 Full achieves an AUC of only 0.610, whereas the best version of CXRFEScore (row 15) achieves an AUC of 0.938. This suggests that current metrics assign elevated scores to pairs of sentences with contradictory semantics, highlighting the necessity for improved metrics to discern these subtleties\u2014precisely what CXRFEScore is designed to accomplish.\nSimilarly, CXRFEScore outperforms all the baselines on IU X-ray, a dataset not used to develop the metric. This provides valuable evidence of the metric's ability to generalize to radiology reports from a different institution."}, {"title": "Fact Extraction Quality.", "content": "To evaluate the quality of the fact extraction stage, we interpret these facts as open-vocabulary labels and compare them against three existing radiology report label extraction methods: CheXpert labeler (Irvin et al., 2019), CheXbert (Smit et al., 2020), and Chest ImaGenome (Wu et al., 2021). For Chest ImaGenome, we use the labels from the dataset's scene graphs, as the original NLP algorithm is not publicly available. For fact extraction, we compare T5-small, fine-tuned specifically for this task, against GPT-4 and Meta-Llama-3-8B. The latter two models use the prompt shown in Figure 14. Our evaluation protocol involves the following: for each MIMIC-CXR test set report and label extraction method, labels are extracted, converted into a report, and evaluated against the original report by several met"}, {"title": "5 Conclusions & Future Work", "content": "In this work, we present a novel two-stage framework for extracting and encoding factual information from radiology reports. The first stage, fact extraction, uses ChatGPT and T5 to extract factual statements. The second stage, fact encoding, introduces CXRFE, a specialized variant of CXR-BERT, fine-tuned through multitask learning by incorporating tasks that support representation improvement. Our system's effectiveness is validated through comprehensive evaluations. Additionally, we introduce CXRFEScore, a novel metric for evaluating radiology text generation, leveraging our two-stage system. We anticipate that our work will stimulate further research in enhanced fact extraction and representation, LLM utilization, advanced training methodologies, and improved evaluation metrics. For future work, we aim to expand our framework to integrate visual modality, focusing on image-based fact detection and visual grounding."}, {"title": "Limitations", "content": "Our study acknowledges several limitations and highlights areas for improvement. First, more expert evaluations, particularly from radiologists, are needed to refine the use of large language models (LLMs) in radiology. Although we extensively utilized publicly available gold standards, such as those from the Chest ImaGenome dataset, RadNLI, and MS-CXR-T, there remains room for improvement. For instance, involving radiologists in the prompt engineering process and developing more rigorous evaluation protocols are two strategies we believe will enhance the evaluation and utilization of LLMs for radiological text.\nWe also see potential in designing better triplet sampling heuristics, especially with input from radiologists. Optimizing LLM prompts for triplet sampling and incorporating more advanced auxiliary embeddings could further enhance our approach.\nFurthermore, while our study focuses on text-only analysis, we recognize the importance of integrating visual data, such as chest X-ray images, into a multimodal framework. Devising a training protocol that effectively combines supervision from both images and text is an area of potential improvement for future work.\nIn this work, we limited our experiments to the sections \"findings,\" \"impression,\" and similar headings providing factual statements about the imaging exam. However, other sections, such as \"comparison,\" \"indication,\" and \"history,\" were left out of the analysis, yet they can provide deeper insights into patient information and context. Investigating how this broader information can be extracted and encoded to enhance downstream applications is another avenue for future exploration and potential improvement.\nLastly, we acknowledge that our fact extraction algorithm may be limited due to its reliance on the sent_tokenize function of the NLTK library, which we use to obtain a preliminary division of the report into coarse sentences (before fact extraction). This method could falter when a fact spans multiple sentences connected through co-reference. While such occurrences are relatively uncommon in our observations, a deeper exploration of this linguistic aspect could guide the development of a more refined fact extraction mechanism that overcomes this challenge."}, {"title": "Acknowledgements", "content": "This work was funded by the Chilean National Agency for Research and Development (ANID), including Instituto Milenio en Ingenier\u00eda e Inteligencia Artificial para la Salud (iHEALTH) ICN2021_004; Centro Nacional de Inteligencia Artificial (CENIA) FB210017; Fondecyt regular 1231724; Fondecyt 1221425; and the ANID Scholarship Program / Doctorado Becas Chile / 2019 - 21191569. Additionally, Pablo was supported by the National Institutes of Health (NIH) grant 1R01AG067396. We are grateful for the support from all funding sources mentioned above."}, {"title": "A Appendix", "content": "A.1 Additional Related Work\nEvaluation of Factual Correctness in Radiology Text Generation. One important area of application motivating this work is the evaluation of factual correctness in systems that generate radiological text, usually from input medical imaging. Recent research emphasizes enhancing the accuracy of generated facts in applications like report generation (Miura et al., 2021; Delbrouck et al., 2022; Pino et al., 2020, 2021) and summarization (Zhang et al., 2020b; Delbrouck et al., 2023). Pino et al. (2020) conducted an evaluation of several trivial report generation baselines using established metrics such as BLEU (2002), ROUGE-L (2004), and CIDEr-D (2015). They achieved results comparable to state-of-the-art papers at the time. However, when assessed using the CheXpert labeler (Irvin et al., 2019), a domain-specific NLP tool designed to detect 13 findings, the performance was notably poor, underscoring the urgent necessity for standardizing improved evaluation metrics among researchers. More recently, Delbrouck et al. (2022) repurposed RadGraph's entity and relation extraction model (Jain et al., 2021a) to create a factual correctness reward. This reward measures the overlap of entities and relations between real and generated reports, serving as the guiding signal to optimize a report generation model through reinforcement learning. Interestingly, their proposed reward aligns functionally with the RadGraph F1 metric introduced by Yu et al. (2022). Yu et al. conducted a study on metrics for radiology report generation, determining that RadGraph F1 and BLEU show the highest correlation with radiologists' judgement. Recently, the RadSum23 challenge (Delbrouck et al., 2023) evaluated multimodal radiology report summarization quality using BLEU-4 (2002), ROUGE-L (2004), BERTScore (2020a), CheXbert F1 (2020), and RadGraph F1 (2021a).\nOur work is highly relevant in this domain because of our development of the CXRFEScore metric. This metric leverages the strengths of both stages within our framework: fact extraction and encoding. As a result, CXRFEScore is specifically designed to assess the factual accuracy of generated radiological text against a reference text. Section 4 offers a comprehensive evaluation, showcasing the effectiveness of our two-stage system. This includes a comparison of CXRFEScore with commonly used metrics in the literature, with very favorable results.\nLLMs in Medicine. Our work falls under the category of applications of LLMs to the medical domain. Specifically, in this work we make use of ChatGPT versions GPT-3.5 and GPT-4 through"}, {"title": "A.2 Fact Extraction Implementation Details", "content": "In our experiments, we processed the 227,827 radiology reports provided by the MIMIC-CXR dataset (Johnson et al., 2019b). To pinpoint relevant radiological sections in the MIMIC-CXR reports, such as Findings, Impression, and various other headings, we employed a combination of regular expressions and simple rules. These sections were then segmented into sentences using NLTK's sent_tokenize function, resulting in 677,694 unique sentences after processing the entire dataset. Subsequently, we extracted facts from each sentence. Extracting factual statements from a sentence of a free-text radiology report using traditional approaches, such as regular expressions, hand-designed rules, and similar heuristics, poses significant challenges due to the complexity and diversity of vocabulary used by radiologists. A more promising alternative is to leverage the capabilities of powerful LLMs like ChatGPT to tackle this task. Table 6 presents several examples of facts extracted by GPT-4 from challenging sentences. This is achieved by providing the model with a specific set of instructions: (Refer to Figure 14 for a screenshot of OpenAI's web interface displaying the same prompt.)"}, {"title": "A.3 CXRFE's Tasks Details", "content": "CXRFE is a fine-tuned version of CXR-BERT-specialized", "https": ""}]}