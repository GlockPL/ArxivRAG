{"title": "LUTMUL: Exceed Conventional FPGA Roofline Limit by LUT-based Efficient MULtiplication for Neural Network Inference", "authors": ["Yanyue Xie", "Zhengang Li", "Dana Diaconu", "Suranga Handagala", "Miriam Leeser", "Xue Lin"], "abstract": "For FPGA-based neural network accelerators, digital signal processing (DSP) blocks have traditionally been the cornerstone for handling multiplications. This paper introduces LUTMUL, which harnesses the potential of look-up tables (LUTs) for performing multiplications. The availability of LUTs typically outnumbers that of DSPs by a factor of 100, offering a significant computational advantage. By exploiting this advantage of LUTs, our method demonstrates a potential boost in the performance of FPGA-based neural network accelerators with a reconfigurable dataflow architecture. Our approach challenges the conventional peak performance on DSP-based accelerators and sets a new benchmark for efficient neural network inference on FPGAs. Experimental results demonstrate that our design achieves the best inference speed among all FPGA-based accelerators, achieving a throughput of 1627 images per second and maintaining a top-1 accuracy of 70.95% on the ImageNet dataset.", "sections": [{"title": "1 INTRODUCTION", "content": "Field-Programmable Gate Arrays (FPGAs) have been widely used as deep learning accelerators, facilitating advancements in computer vision [7, 17, 35, 39] and natural language processing [4, 13, 16, 38] tasks. However, FPGAs lag behind Graphics Processing Units (GPUs) in terms of performance and ease of programming. FPGA reconfigurable logic mainly consists of look-up tables (LUT), block RAMS (BRAMs), and digital signal processing (DSP) blocks. Together with routing resources, FPGA can be reconfigured for customized designs. Despite the flexibility, FPGAs face constraints in clock frequency, floating-point performance, and memory bandwidth. This performance gap between FPGAs and GPUs is becoming even larger when considering the tensor core performance of GPUs. To address this, we need an algorithm-hardware co-design method to boost FPGAs with greater inference capability.\nFPGA accelerators can follow GPU-like [32, 34] architecture, which maps computation to compute cores with repetitive use. While beneficial, this approach encounters memory bandwidth issues similar to GPUs. Compared with GPUs, FPGAs usually have lower memory bandwidth, and the lower clock frequency of FPGAs means a lower upper bound of performance. While FPGA-based accelerators with specific instruction set architectures [37] offer flexibility across different models, they often compromise on performance due to non-optimized compute kernels for specific neural network layers.\nTo bridge the performance gap between FPGAs and GPUs, particularly in deep learning applications, we introduce LUTMUL, which leverages the look-up tables on FPGAs for deep learning tasks, focusing on accelerating convolutional neural networks (CNNs). We recognize that the traditional FPGA designs, heavily dependent on DSP blocks, may not fully exploit the parallelism and flexibility that LUTs offer, as the availability of LUTs typically outnumbers DSPs by a factor of 100. Our method emphasizes a novel utilization of LUTs to enhance computational efficiency and throughput in deep learning applications. Specifically, we embed the convolutional neural network weights into LUTs, where the LUT input is the activations and the LUT output is the multiplication result. Different from LUT-based general multipliers, our method is efficient in resources (requiring just 2 LUTs for a single 4-bit multiplication) and helps fully exploit the parallelism.\nWe propose a reconfigurable dataflow architecture for our LUT-based efficient multiplication kernel. Our dataflow architecture minimizes the memory access time by processing the data on-chip through each layer without external memory. The reconfigurability of the FPGA allows us to tailor the architecture specifically for each distinct layer of CNNs. With LUT resources, the generated accelerator can potentially exceed the peak performance of conventional DSP-based FPGA accelerators. Our dataflow architecture aims to enhance the overall efficiency of our deep learning accelerators, optimizing FPGAs for deep learning tasks.\nOur contributions can be summarized as follows:\n\u2022 We present LUTMUL, an algorithm-hardware co-design method that embeds quantized neural network weights into look-up tables for efficient multiplications and uses dedicated look-up tables for full parallelism.\n\u2022 We design a reconfigurable dataflow architecture that exploits scalability and LUT potential to save computational resources.\n\u2022 Using LUTMUL, FPGA designs can potentially exceed the peak performance of conventional DSP-based FPGA accelerators when using the same amount of resources."}, {"title": "2 BACKGROUND", "content": "2.1 Roofline Model Analysis\nGPUs leverage Single Instruction Multiple Data (SIMD) architecture, allowing them to simultaneously perform the same operation across multiple data points. This parallelism makes GPUs exceptionally efficient for tasks that can be divided into smaller, similar operations, such as matrix multiplication in deep learning.\nFPGAs, by contrast, achieve parallel processing through their reconfigurability, allowing hardware to be tailored to specific computational tasks. This flexibility allows FPGAs to efficiently handle complex and diverse data processing tasks, offering advantages over the fixed architecture of GPUs. While FPGAs lack the raw SIMD power of GPUs for certain applications, they excel in scenarios requiring custom hardware configurations or low-latency, such as specific signal processing tasks or custom machine learning algorithms. However, this adaptability often comes with a trade-off in processing speed and ease of programming, with FPGAs typically lagging behind the computational throughput of GPUs.\nThe roofline model [31] is a useful tool for analyzing the performance of both GPUs and FPGAs. An algorithm running on GPUs or FPGAs can be either compute bound or memory bound. According to the roofline model [39], the peak performance of FPGAs is:\n$\\text{Peak performance} = p \\times \\text{PEs} \\times 2 \\times f,$                                                            (1)\nwhere PEs is the number of processing elements (PEs) used in the accelerator, such as the DSP blocks, f is the clock frequency, and \u00d72 term accounts for multiply-accumulate (MAC) operations. The packing factor for DSP blocks, p, varies based on the bit-width of the operation, with p = 1 for 16-bit, p = 2 for 8-bit, and p = 4 for 4-bit multiply-accumulate operations.\nFurthermore, the performance of an FPGA-based accelerator is also limited by the memory, which is related to the memory bandwidth (BW) and computation-to-communication (CTC) ratio:\n$\\text{Peak memory bandwidth} = BW \\times \\text{CTC ratio}.$                                                         (2)\nTable 1 summarizes the major differences between GPUs and FPGAs, such as clock frequency, number of compute cores, and memory bandwidth. The significant difference in clock frequency contributes to a notable performance gap between GPUs and FPGAs. Even with optimization such as pruning and quantization [10], FPGA inference speed generally remains inferior to that of GPUs."}, {"title": "2.2 Dataflow Architecture", "content": "Dataflow architecture contrasts with the traditional control flow architecture. The dataflow nodes or processing elements can immediately start and execute when all its inputs are ready. Dataflow architecture employs simple operations, such as broadcast (one-to-many), map (element-wise, e.g. activation function), zip (multi-operands, e.g. convolution and matrix multiplication), and reduce (many-to-one, e.g. pooling) [22]. A key advantage of reconfigurable dataflow architecture is its ability to allow data to flow through the computation graph efficiently, significantly enhancing parallelism and minimizing memory access time."}, {"title": "3 ALGORITHM-HARDWARE CO-DESIGN FOR LUTMUL", "content": "3.1 Motivation\nThe roofline model reveals a theoretical peak performance for DSP-based accelerators, applicable across various architectures such as GEMM, systolic array, or dataflow architecture. We can leverage LUT resources to perform multiplication and full parallelism to enable FPGA with greater performance. Given that the availability of LUTs usually outnumbers DSPs, using LUTs can potentially exceed the upper bound of performance of current DSP-based FPGA accelerators."}, {"title": "3.2 LUTMUL Design Flow", "content": "Figure 3 depicts the LUTMUL design flow. Initially, we train the neural network in our quantization-aware training framework. The quantization bit-widths for weights and activations are adjustable hyperparameters. The final quantized neural network is exported in Open Neural Network Exchange (ONNX) format, facilitating subsequent hardware generation.\nThe ONNX intermediate representation is interpreted as a computation graph and undergoes a streamlining transformation [27]. The scaling factors of each channel and batch normalization layer are reordered and absorbed into the activation function, transforming into a multi-threshold unit. Each computation node is converted to a High-Level Synthesis (HLS) layer using our HLS templates. These HLS layers are folded according to performance and resource requirements and interconnected sequentially. The final hardware bitstream, generated by Vivado, is deployed on FPGA boards via the PYNQ framework."}, {"title": "3.3 Reconfigurable Dataflow Architecture", "content": "Figure 4 illustrates the hardware architecture of a MobileNetV2 [23] implementation. This design, focusing on inverted residual blocks, employs a First In, First Out (FIFO) buffer between layers to store activations. The architecture uses a reconfigurable dataflow architecture.\nOur design spans all Super Logic Regions (SLRs) to maximize hardware resources. Signals only traverse SLRs when the current SLR resources are insufficient for the next layer to avoid severe timing violations. Dataflow architecture is inherently suited for design spanning multiple SLRs and can be scaled up, enabling additional FPGAs connected via network for deploying larger networks [6]."}, {"title": "3.4 Convolution Generator", "content": "For convolutional layers, the convolution operations can be lowered to matrix-matrix multiplications. These can be mapped in a streaming manner and fed to the multiplication kernel. The multiplication kernel is fully paralleled to perform a matrix-vector multiplication, where the weights are stationary vectors and activations are streaming inputs. Therefore, we need a convolution generator to perform the im2col operations: reading data from FIFO, moving across input images to form an image matrix, and streaming the output to the multiplication kernel.\nThe convolution generator accommodates various configurations, including pointwise, depthwise, and standard convolution with different kernel sizes and strides, since each kind of convolutional layer expects different input data sequences, necessitating specific generator settings."}, {"title": "3.5 Look-Up Table based Efficient Multiplication", "content": "Figure 5 demonstrates the look-up table based multiplication kernels and how to determine look-up table initialization (INIT) values. After embedding the weights into look-up tables, our look-up tables transform into efficient constant multipliers [11]. Our look-up table based multiplier is efficient in resources, utilizing only 2 LUTs on average for a single 4-bit multiplication, compared with a general multiplier that consumes 13-28 LUTs for an equivalent operation. The choice of 4-bit quantization is pivotal as it maintains model accuracy and optimizes look-up table usage, as shown in Figure 2. We show the number of LUT6 (6:1 LUT, 6-bit input, 1-bit output) for a general n-bit multiplication (n:2n LUT, n-bit input, 2n-bit output) using our method:\n$\\#LUTs = \\frac{2^n \\times 2^n}{1 \\times 2^6}$                                                        (3)"}, {"title": "3.6 Quantization-Aware Training", "content": "Quantization [10] and DSP packing [24] have become a standard approach for mapping neural networks onto FPGA-based accelerators, as FPGAs' LUTs and DSP blocks are not optimized for floating-point but ideal for integer or fixed-point operations. Quantization, paired with DSP packing, reduces resource demands for the multiplications and improves throughput.\nThe quantization operation is defined as:\n$y = \\text{quantize}(x) = \\text{clamp}(\\text{round}(\\frac{x}{s} + z), Y_{\\text{min}}, Y_{\\text{max}}),$                                       (4)\nwhere x is the floating-point value to quantize, s is the scaling factor of the output quantized tensor, and z is the zero-point or quantization bias coefficient. The function, round, can be round-to-even or round-to-zero, and clamp performs clipping inclusive of the boundaries Ymin and ymax.\nFor the reverse process, to compute the floating-point representation of a quantized value, we define the dequantize operation as:\n$\\text{dequantize}(y) = s(y - z),$                                                           (5)"}, {"title": "4 EVALUATION", "content": "4.1 Experimental Setup\nTo evaluate the performance of LUTMUL, we implement MobileNetV2 on FPGAs and compare it with existing FPGA-based MobileNet accelerators. MobilenetV2 [23] has 3.4M parameters and achieves 71.88% top-1 accuracy on the ImageNet dataset [5]. We utilize the FINN framework [2] as our foundational platform. For quantization, we adopt PyTorch 1.13.0 and Brevitas 0.9.1 [20]. Specifically, we choose 4-bit for weights and activations quantization except for the first and last layers, which are set as 8-bit. To preserve the accuracy of the MobileNetV2 model, we apply the channel-wise quantization scheme for our model. Our quantized MobileNetV2 network is trained for 420 epochs, culminating in a 70.95% top-1 accuracy evaluated on the ImageNet dataset [5].\nFor the hardware evaluation, the utilized development platform is the AMD Xilinx Alveo U280 data center accelerator card on the Open Cloud Testbed (OCT) [40]. We implement the first 15 layers of MobileNetV2 in a fully parallel manner and fold the remaining layers for resource optimization. To maximize the computation efficiency without timing violation, the working frequency is set to 333 MHz for all the designs implemented through Vitis HLS/Vivado 2022.1.\n4.2 Experimental Results\nTable 2 showcases the hardware performance and comparisons with other FPGA-based MobileNet accelerators. Most of these accelerators are tailored for edge FPGAs, such as ZU9EG, except for FINN, which has data center accelerator implementation for MobileNetV1. The FINN result is generated and tested on the same device as our implementation, while other data points are extracted from their original publications.\nIn terms of accuracy, our model achieves the best 70.95% top-1 accuracy on ImageNet among all implementations. Quantization-aware training effectively mitigates quantization errors, preserving the model original accuracy, even with 4-bit quantized weights and activations.\nAs for the inference performance, our implementation achieves a throughput of 1627 images per second. Our implementation consumes the most FPGA resources but could still fit on a single Alveo U280. However, it is noteworthy that our implementation also yields a 23.23 GOPS/W energy efficiency, marginally lower than the FLIM-QNN [24], which is implemented on a more power-efficient edge FPGA board.\n4.3 Discussion\nFigure 6 shows the LUT resource breakdown of the second convolution layer in MobileNetV2 using LUTMUL, which is a 1 \u00d7 1 convolution kernel and has 32 input channels and 32 output channels. For these 1024 4-bit weights, multiplication operations use 1829 LUTs after HLS synthesis, which matches the theoretical analysis of LUTMUL. However, HLS instantiates an adder for each addition operation to achieve an II of 1, resulting in a high usage of LUT for adder logic. After Vivado implementation, the LUT usage decreased to 5922. Vivado optimizes the logic and instantiates 3277 LUTs as ROM and 2645 LUTs as adder and other logic. Even though adder logic accounts for a large part of resources, the parallel MAC performance by LUTMUL still outperforms the DSP packing method using the same number of resources.\n4.4 Comparisons with Related Works\nOur method is not only limited to integer multiplication, but can also be extended to customized data formats, such as FP4 and MXFP4 [21], while DSP packing is designed efficiently for integer formats. LUTNet [28, 29] also utilizes LUT for inference and explores the flexibility of LUT. However, LUTNet design suffers from low accuracy when the network becomes larger. PolyLUT [1] trains multivariate polynomials instead of linear functions and embeds piecewise polynomial functions into LUTs. CompressedLUT [15] proposes a lossless LUT compression method and is efficient for non-linear functions and large LUT logic blocks, such as [8, 12, 25]. Our method maps MAC operations to the single-LUT level, and Vivado can handle remaining logic optimization efficiently."}, {"title": "5 CONCLUSION", "content": "We propose LUTMUL, an efficient method that leverages look-up tables for multiplication in convolutional neural networks. Compared with the general multiplier, our method is efficient in resources, which only needs two look-up tables on average for a single 4-bit multiplication. Compared with other DSP-based FPGA accelerators, LUTMUL's reconfigurable dataflow architecture enables full parallelism, reduces memory access time, and increases the theoretical upper bound of performance. Experimental results demonstrate that our design maintains a top-1 accuracy of 70.95% on the ImageNet dataset and achieves a throughput of 1627 images per second on a single Alveo U280 FPGA, outperforming other FPGA-based MobileNet accelerators."}]}