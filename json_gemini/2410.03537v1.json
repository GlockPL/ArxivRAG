{"title": "WARD: PROVABLE RAG DATASET INFERENCE VIA LLM WATERMARKS", "authors": ["Nikola Jovanovi\u0107", "Robin Staab", "Maximilian Baader", "Martin Vechev"], "abstract": "Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to incorporate external data during generation. This raises concerns for data owners regarding unauthorized use of their content in RAG systems. Despite its importance, the challenge of detecting such unauthorized usage remains underexplored, with existing datasets and methodologies from adjacent fields being ill-suited for its study. In this work, we take several steps to bridge this gap. First, we formalize this problem as (black-box) RAG Dataset Inference (RAG-DI). To facilitate research on this challenge, we further introduce a novel dataset specifically designed for benchmarking RAG-DI methods under realistic conditions, and propose a set of baseline approaches. Building on this foundation, we introduce WARD, a RAG-DI method based on LLM watermarks that enables data owners to obtain rigorous statistical guarantees regarding the usage of their dataset in a RAG system. In our experimental evaluation, we show that WARD consistently outperforms all baselines across many challenging settings, achieving higher accuracy, superior query efficiency and robustness. Our work provides a foundation for future studies of RAG-DI and highlights LLM watermarks as a promising approach to this problem.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval-Augmented Generation (RAG) has emerged as a popular approach to mitigate limitations of large language models (LLMs), such as hallucinations, the high cost of adapting to new knowledge via fine-tuning, and the inability to back up claims by sources (Lewis et al., 2020). By integrating retrieval, LLMs gain in-context access to large corpora of high-quality, up-to-date data, enabling them to generate more accurate and source-supported responses. To maintain relevance, RAG providers must continuously update their corpus with new data. However, this raises concerns regarding the unauthorized usage of documents, particularly when publicly available documents are used without the owner's permission (Grynbaum & Mac, 2023; Wei et al., 2024a). There is currently no way to conclusively prove such unauthorized usage by a RAG system, and enforce an opt-out by the owner.\nRAG Dataset Inference (RAG-DI) We formalize the corresponding problem as RAG Dataset Inference (RAG-DI), where a data owner aims to detect unauthorized inclusion of their dataset within a RAG corpus via black-box queries, illustrated in Fig. 1. We aim to, for the first time, comprehensively study this problem. We first observe that existing datasets, used in adjacent works on RAG privacy, are fundamentally unsuitable for RAG-DI. First, the samples in these datasets may have been used in contemporary LLM training, complicating realistic evaluations, where RAG corpora consist of new data. Second, these datasets do not model fact redundancy, a key property of real-world RAG corpora, where multiple documents share similar content, either due to scraping data from various sources (e.g., news (Gao et al., 2023)), or as an artifact of chunking. Another challenge in studying RAG-DI stems from the lack of baselines applicable in a realistic black-box setting.\nFoundations for RAG-DI In this work, we take multiple steps to bridge these gaps: First, we introduce FARAD, a new dataset specifically designed for RAG-DI evaluation under realistic conditions. FARAD contains fictional articles that are by design not part of any LLM training data, and can enable evaluations under fact redundancy, enabling accurate assessment of RAG-DI methods. Second, we adapt prior work on RAG Membership Inference Attacks (MIAs) (Li et al., 2024; Anderson et al., 2024) to the RAG-DI problem, and propose a simple baseline FACTS. In our evaluation on"}, {"title": "2 BACKGROUND", "content": "Retrieval augmented generation (RAG) RAG is a popular way to enhance LLM capabilities: For a given user query q, the $k \\in \\mathbb{N}$ most relevant documents $D_q = R(q, D) \\subset D$ are retrieved from a corpus D, using a retrieval method R (Gao et al., 2023). While instantiations differ, the query q is generally combined with the result $D_q$, and fed into an LLM to generate a more factual and relevant response r = M(q, Dq). Expanding D further enables access to new information without having to rely on costly retraining (Lewis et al., 2020). RAG is especially suitable for domains where new information is generated often, such as news articles or software documentation. In this work we use M* to denote a RAG system, i.e., a retrieval-augmented LLM M."}, {"title": "LLM watermarking", "content": "LLM watermarks enable model owners to provably and reliably track text generated by their LLM. In this work, we focus on the prominent red-green watermarks (Kirchenbauer et al., 2023). During text generation, at each step t, the token vocabulary V is split into two parts, $\\gamma|V|$ green (encouraged) tokens, and $(1 \u2013 \\gamma)|V|$ red (discouraged) tokens, for some $\\gamma \\in [0, 1]$. Given context width h, the split is commonly a function of tokens at positions t \u2013 h, . . ., t, as well as a secret salt. To add the watermark, the logits of green tokens are increased by $\\delta\\in \\mathbb{R}_{\\geq0}$, boosting their probability of being sampled. The watermark is detected using a statistical test, based on the expectation that non-watermarked text of length T has $\\gamma T$ green tokens. Namely, we use the z-score\n$(1)\nz = \\frac{|s_{gr}| - \\gamma T}{\\sqrt{\\gamma(1 - \\gamma)T}},$\nwhere sig is the number of green tokens in a given text s, and T = |s|. From here, we derive the p-value p = 1 \u2013 \u03a6(z), where I is the CDF of the standard normal distribution, and consider the text watermarked if p < a for some threshold a. While not perfectly robust, these watermarks generally persist under moderate text transformations, such as paraphrasing or segment omission (Piet et al., 2023; Kirchenbauer et al., 2024; Sander et al., 2024), making them suitable for our setting, where our goal will be to propagate the watermark signal through the RAG pipeline, as we will describe in \u00a74."}, {"title": "RAG membership inference attacks", "content": "While RAG is a relatively novel concept, recent work already studies membership inference attacks (MIAs) in this setting, proposing two methods that we denote SIB (Li et al., 2024) and AAG (Anderson et al., 2024). A MIA's goal is to output mi(d, M*) = 1 if a document d is part of the retrieval corpus D of a RAG system M*, or mi(d, M*) = 0 otherwise, based only on queries to M*. To this end, SIB queries M* with q, a prefix of d, to obtain the response r = M(q, Dq). Then, it computes two scores: the cosine similarity between the embeddings of d and r, and the perplexity of r, outputting mi(d, M*) = 1 if the similarity is above a threshold @similarity, and the perplexity is below a threshold @perplexity, both trainable parameters. Note that this requires gray-box access to M* for perplexity computation. The other method, AAG, directly prompts M* to answer if d is in the context. If it replies positively, they set mi(d, M*) = 1. In \u00a73, we will introduce the black-box RAG dataset inference setting, and adapt both baselines to it. Notably, while dataset inference was studied alongside MIA for training data (see \u00a76), no prior work studies it for RAG."}, {"title": "3 RAG DATASET INFERENCE", "content": "We formalize the problem of RAG Dataset Inference (RAG-DI), and present our contributions aimed at facilitating studies of this problem. In \u00a73.1, we make the case that existing datasets commonly used for adjacent tasks (e.g., RAG MIA) are fundamentally unsuitable for RAG-DI, and propose a new dataset in an attempt to address these shortcomings. In \u00a73.2, we establish a set of baselines for RAG-DI, by adapting RAG MIA work introduced in \u00a72, and proposing a simple baseline, FACTS.\nThe RAG-DI problem The key entities in RAG-DI are the data owner, who aims to protect their n-document dataset Ddo from unauthorized usage in a RAG corpus, and the RAG provider, who exposes black-box access to their retrieval-augmented LLM M*, which uses a corpus D. The data owner's goal is to determine if Ddo \u2286 D, i.e., whether their data was secretly included in the corpus. To this end, they may proactively modify Ddo before publishing it, and can query M* in a black-box way, aiming to minimize the number of such queries. Crucially, the data owner makes a single dataset-level decision, as opposed to document-level decisions of MIAs. Formally, a RAG-DI method di should output di(Ddo, M*) = 1 if Ddo \u2286 D (IN case) and 0 otherwise (OUT case)."}, {"title": "3.1 A DATASET SUITABLE FOR RAG-DI", "content": "To enable suitable evaluation of RAG-DI methods, we require a dataset of documents with the following properties. First, we aim to match a key use-case of RAG (as described in \u00a72) where up-to-date knowledge is added to D instead of costly repeated fine-tuning of M. To model this, our documents should provably not be part of the training data of M, i.e., of current open/closed LLMs, as those will be used to instantiate M when studying RAG-DI. Second, to model the practical case where knowledge is redundant and spread across multiple sources (e.g., news articles, a common motivating example for RAG (Gao et al., 2023)), the dataset should contain"}, {"title": "3.2 RAG-DI BASELINES", "content": "We proceed to establish an initial set of baselines for the RAG-DI problem. For this we adapt existing RAG MIAS, AAG and SIB, introduced in \u00a72, and additionally propose a simple baseline, FACTS.\nAdapting RAG MIAS By design, the existing RAG MIAs make document-level decisions, i.e., they decide if a single document is in the RAG corpus (mi(d, M*) = 1) or not (mi(d, M*) = 0). We ex-"}, {"title": "4 LLM WATERMARKING AS AN EFFECTIVE RAG-DI METHOD", "content": "Before describing our proposed RAG-DI method based on LLM watermarks, we first outline three key design requirements that a desirable RAG-DI method should fulfill. We require the following:\n1. Monotonicity. With more queries to the retrieval-augmented LLM M*, the accuracy of the method's predictions should consistently improve, preferably at a high rate.\n2. Guarantees. The method should be able to provide a statistical guarantee for its decision, with exceedingly rare and well-controlled Type 1 errors, as falsely accusing RAG providers is highly undesirable in practice, and undermines the trust in the method.\n3. Robustness. The method should maintain high accuracy under diverse evaluation settings, including attempts by the RAG provider to actively conceal unauthorized data usage.\nAs we demonstrate in \u00a75, all RAG-DI baselines introduced in \u00a73.2 violate all above requirements to some extent. To address this, we propose WARD (Watermarking for RAG-DI), a proactive RAG-DI method that is based on LLM watermarks, and discuss why it is likely to fulfill all stated desiderata.\nWARD: RAG-DI via LLM watermarking We assume the data owner has protected each di \u2208 Ddo by embedding an LLM watermark either via a human-in-the-loop procedure or (as in this work) by rephrasing each document with a watermarked LM. While, in principle, any LLM watermark can be applied, we focus on popular red-green watermarks (see \u00a72). In \u00a75.3, we confirm that this results in quality texts, faithful to the original ones. To audit the RAG provider's corpus, for each di \u2208 Ddo, WARD generates an open-ended content-related question qi, and queries M*. If di \u2208 D (IN case), we expect the retrieval method R to introduce watermarked content from di into the LLM context. As noted in \u00a72 and validated in \u00a75, the robustness of watermarks to text transformations is then sufficient to propagate the traces of the signal to ri = M* (qi), the final response of the LLM.\nBoosting a weak signal Requiring that each ri is flagged as watermarked, i.e., has watermark detector p-value p < a, would be a strong assumption, as the watermark signal is likely to degrade throughout the RAG pipeline. However, this is not necessary for WARD to be effective. Instead, following Sander et al. (2024), after n queries we compute a joint p-value R = {r1,...,rn}, directly corresponding to the null hypothesis \"the data owner's dataset Ddo is not in the RAG corpus D\u201d.\nThis joint p-value can satisfy p < a, i.e., reject the null hypothesis, even when individual ri carry only weak watermark signal, that would individually not reject it. To illustrate this, given a desired p-value threshold a, Eq. (1) implies that the required ratio of green tokens in R is at least\n$(2)\n\\gamma' > \\Phi^{-1}(1 \u2013 \\alpha) \\cdot \\sqrt{\\gamma(1 - \\gamma)/|R|} + \\gamma,$"}, {"title": "5 EXPERIMENTAL EVALUATION", "content": "We evaluate the RAG-DI baselines (\u00a73.2) and WARD (\u00a74) on the FARAD dataset (\u00a73.1). \u00a75.1 presents our main experiment. In \u00a75.2 we focus on desiderata from \u00a74, showing that only WARD does not violate them. In \u00a75.3 we validate several key assumptions, and in \u00a75.4 present additional ablations.\nSetup Our experimental setup follows \u00a73: we use FARAD to define two evaluation settings, and in both evaluate IN and OUT cases, i.e., where the data owner's data is (resp. is not) contained in D. We use Ddo = 200, and |D| = 800 for FARAD-Easy, and |D| = 3000 for FARAD-Hard, following from our sampling procedure described in App. B. We note that WARD by design only depends on |Ddo, but not |D|. We use several LLMs as M: GPT3.5, CLAUDE3-HAIKU, and LLAMA3.1-70B, and vary the system prompt: we use a short naive prompt (Naive-P) with basic RAG instructions, and a longer defense (Def-P) prompt, which models a RAG provider that elaborately instructs the model not to answer any questions about or repeat sections from retrieved documents. Each experiment is run with 5 random seeds. To ensure a controlled setting, in our main experiments, we assume a"}, {"title": "5.1 MAIN RESULTS", "content": "We present our main results in Fig. 4, where we evaluate all RAG-DI baselines and WARD across several settings, models, prompts, and random seeds. We make several key observations.\nFirst, all baselines perform somewhat well in the Easy setting (no fact redundancy). However, our extremely simple FACTS baseline obtains perfect results, outperforming both AAG and SIB. This emphasizes that traditional non-redundant datasets (see \u00a73.1) fail to capture the complexity of RAG-DI, and can provide an incomplete view of the capabilities of RAG-DI methods. We also observe that already in the Easy setting, straightforward system prompt defenses significantly impact both AAG and SIB, leading to a noticeable increase in false negatives. We further investigate defenses in \u00a75.2.\nMoving to the Hard setting, all baselines fail to perform consistently well, inducing both false positives and negatives. This can be directly attributed to the shortcomings of all baselines in handling fact redundancy: FACTS directly relies on facts, while SIB and AAG rely on semantic similarity directly influenced by factual content. Notably, only WARD achieves 100% accuracy across all settings, models, and system prompts. We find that, despite the retrieval of documents with partially overlapping facts, watermarking, unlike our baselines, provides a reliable signal for dataset inference. Our results both back up our claims regarding the importance of fact-redundancy for realistic RAG-DI evaluation and highlight the potential of watermarking as a promising approach."}, {"title": "5.2 DESIDERATA", "content": "We next demonstrate how baselines violate the desiderata from \u00a74, which is not the case for WARD.\nMonotonicity As stated in \u00a74, RAG-DI methods should steadily improve with more queries, for us corresponding to |Ddo (see \u00a75.4 for a generalization). We evaluate this by modifying our above setup to use Ddo| \u2208 {20, 40, . . ., 200} in FARAD-Hard, presenting the results of WARD and SIB, as an illustrative example, in Fig. 5. WARD improves consistently with Ddo, reaching perfect accuracy across all settings for at most 80 documents. In contrast, SIB, besides never reaching full accuracy, exhibits strongly varying accuracy over |Ddo, often decreasing despite using more queries. This is a consequence of the need to adapt the MIA baselines to the RAG-DI setting-we find similar behavior across all baselines, and present an additional study of their decision thresholds in App. A.6."}, {"title": "5.3 ADDITIONAL CONSIDERATIONS", "content": "Modeling retrieval So far, our experiments have assumed an idealized case of perfect retrieval. We now justify this choice by running WARD on an end-to-end RAG system which uses OpenAI's text-embedding-3-large document embeddings, with k 3 and cosine similarity metric."}, {"title": "5.4 ABLATIONS", "content": "Lastly, we provide ablations for important hyperparameters of WARD and the RAG-DI setup.\nWatermark parameters As introduced in \u00a72, red-green watermarks have 2 key parameters: the strength 8 and the context size h. We ablate over their impact on WARD using LLAMA3.1-70B as M with the Def-P sysprompt. The results for 8 are shown in Fig. 9 (Left). We observe a sweet spot of \u03b4\u2208 [2.5, 4.5] with high accuracy (not the case for d < 2.5) and the minimal impact on the quality of watermarked documents (not the case for \u03b4 > 4.5). While accuracy with 200 queries is 100% in Fig. 9, our additional plot in App. A.2 clearly shows that \u03b4 > 4.5 negatively affects the query efficiency, on top of text utility. This range of d\u2208 [2.5, 4.5] directly aligns with recommendations in prior work (Kirchenbauer et al., 2024), and supports our choice of 8 = 3.5 for the main experiments.\nRegarding h, as noted in \u00a74 and prior work, larger values are expected to degrade robustness. We compare our choice of h = 2 with h = 4, noticing a slight decrease in accuracy (100% on Easy, but 80% on Hard), confirmed by our plots of query efficiency-full results are deferred to App. A.2.\nRAG-DI parameters The RAG-DI setting has a wide range of parameters that we so far have not explored. First, we ablate over k, the number of retrieved documents\u2014our results in App. A.1 show"}, {"title": "6 RELATED WORK", "content": "Closest related works to ours are Li et al. (2024) and Anderson et al. (2024), which propose member- ship inference (MI) for RAG that we adapt to RAG-DI in \u00a73, and works that highlight the risk of MI in the related paradigm of in-context learning such as (Duan et al., 2023). Others study broader privacy and security aspects of RAG, such as poisoning to jailbreak the model or exfiltrate data (Zou et al., 2025; Xue et al., 2024; Chaudhari et al., 2024; Zeng et al., 2024), concerns similar to RAG-DI.\nPassive MI/DI The problems of membership inference (MI) (Shokri et al., 2017; Carlini et al., 2022) or dataset inference (DI) (Maini et al., 2021; Dziedzic et al., 2022) have been long studied on training data, as opposed to RAG corpora as in this work. Recent attempts to adapt these methods to LLMs (Duan et al., 2024; Das et al., 2024; Meeus et al., 2024; Maini et al., 2024) cite the challenge of rigorous evaluation, and primarily focus on graybox settings, citing the difficulty of inference attacks in the blackbox setting (Choquette-Choo et al., 2021), which is what we consider in RAG-DI. Another perspective on the problem of tracing data usage in model training is given by recent works on LLM data contamination (Dekoninck et al., 2024; Oren et al., 2024), none of which consider RAG.\nProactive MI/DI/model protection Another approach to MI/DI is (as in this work) proactive, e.g., by watermarking the data (Ren et al., 2024; Guo et al., 2023; Sablayrolles et al., 2020). However, not many works study this in the context of LLMs, and only the ones cited above consider RAG. Wei et al. (2024b) focus on LLMs in a graybox setting, inserting random sequences or applying unicode substitution to trace the data through training. Sander et al. (2024) find that LLM watermarks propagate through fine-tuning, but in the blackbox setting, Ddo has to make up 10% of the fine-tuning corpus for detection. As confirmed above, RAG-DI relaxes this requirement, as RAG is unaffected by |D, and degrades the signal much less than fine-tuning. Another orthogonal area is watermarking of models against model stealing, often via backdoors (Adi et al., 2018; Zhao et al., 2023).\nLLM Watermarking Finally, we note that many works study red-green LLM watermarks (Kirchen- bauer et al., 2023; 2024; Zhao et al., 2024; Hou et al., 2023), but also other approaches such as sampling modification (Kuditipudi et al., 2023; Christ et al., 2023), model-based watermarking (Liu et al., 2024), or watermarking in weights (Gu et al., 2024). We note that WARD could be combined with many of these, and leave this interesting direction to follow-up work."}, {"title": "7 CONCLUSION", "content": "We studied the problem of black-box RAG Dataset Inference (RAG-DI), where the goal is to detect unauthorized usage of a dataset in a RAG system. We formalized the problem, proposed a dataset and a set of baselines, and presented WARD, a method based on LLM watermarks, which provides rigorous statistical guarantees. Our evaluation showed that WARD outperforms the baselines, achieving perfect accuracy and high query efficiency and robustness. This establishes WARD as a practical tool that can be directly applied to protect the rights of data owners in current RAG systems. We hope our work provides a valuable foundation for future work on RAG-DI\u2014interesting directions include combining WARD with other LLM watermarks, or designing watermarks specifically tailored to RAG-DI."}, {"title": "A ADDITIONAL EXPERIMENTAL RESULTS", "content": "In this section, we first present additional results on our RAG-DI ablations, omitted from $5.4 (App. A.1), Similarly, in App. A.2, we extend our WARD ablations from $5.4. In App. A.3 we provide extended results of our retrieval study from \u00a75.3, and in App. A.4 extend our results on the number of queries per document from Fig. 9 (Right). Finally, in App. A.5 we provide results on an additional defense (MEMFREE, summarized in \u00a75.2), and in App. A.6 we provide more insights into the performance of RAG-DI baselines."}, {"title": "A.1 RAG-DI ABLATIONS", "content": "As summarized in \u00a75.4, we ablate over key parameters of RAG-DI: The number of documents (shots) put into the context of model (k) and the fraction of documents of Ddo contained in D.\nNumber of shots For k we present our results for WARD in Fig. 11, ablating the k \u2208 {3, 4, 5} for our Naive-P and Def-P settings on FARAD-Hard using GPT3.5. Across all values of k, WARD shows favorable scaling of the number of queries made to the system, reaching 100% accuracy at 100 queries at the latest. This follows intuition, as the scal- ing of WARD is primarily influenced by the capabilities of the underlying LLM to select the correct information from a set of retrieved documents. The constant improvement in model capabilities, therefore, naturally positively impacts the scaling of WARD w.r.t. context size."}, {"title": "Percentage of documents in the corpus", "content": "We further ablate over the percentage of documents of Ddo contained in D, which our main experiments assume to be 100. For this extension of the RAG-DI setting, we assume that while the data owner wants to check whether Ddo \u2286 D, in reality only a strict subset D'do Ddo with size |D'do| = (1 \u2212 w) \u00b7 |Ddo| is contained in D. Naturally, as the data owner in WARD incorporates all queries for all documents, the resulting queries related to documents not from Do will increase the p-value. We experimentally test this in two settings: once with LLAMA3.1-70B and Naive-P, and once with GPT3.5 and Def-P, presenting the mean log10 p-value of the number of queries for both settings in Fig. 10. Across both cases, we observe that (as expected) a higher value of w results in a higher computed p-value and hence a weaker test. At the same time, we can see that, especially in easier settings, WARD can endure a significant drop (only 20% of Ddo being contained in D) while still providing accurate results. In the more challenging setting, we observe the first false positives at w = 0.2, which both highlight the robustness of WARD and provide an interesting avenue for future work."}, {"title": "A.2 WARD ABLATIONS", "content": "To supplement our results in \u00a75.4, we present full results of our ablation experiments of key water- marking parameters in WARD: the watermarking strength 8 and the context size h.\nWatermark strength We ablate \u03b4\u2208 [0.5, 6.5] in steps of 0.5 using LLAMA3.1-70B with Def- P on FARAD-Hard. In particular, in Fig. 12, we complement our quality-accuracy plot from \u00a75.4, by displaying the cumulative average accuracy at each number of queries. Concretely, we, at point x, present the average accuracy of all previous x' \u2264 x. This highlights two key findings. First, while many higher ds achieve 100% accuracy in our plot in Fig. 9 (as number of queries is 200), they actually achieve worse results for a lower number of queries. This can be explained by the fact that higher ds lead to worse text quality, which in turn impacts the text quality in the final LLM responses, and thus reduces the amount of watermark signal that is transferred. Second, these results narrow the optimal range of d for our setting to [3.5, 4.5], on which we consistently achieve the best results."}, {"title": "Context size", "content": "Further, we ablate over the water- mark context size h, presenting additional results on h = 4 in Fig. 13 (LLAMA3.1-70B on Naive-P). Notably, we find, in line with prior work on wa- termarks showing that increases in h produce less robust watermarks, that WARD requires more sam- ples to reliably detect higher h = 4 (compared to h = 2). While on Easy, this has only a minor impact, we see a stronger initial drop on Hard. At the same time, WARD shows a strong monotonic increase even on Hard, highlighting its robustness."}, {"title": "A.3 END-TO-END RAG", "content": "Next, we complement our results on the full RAG implementation from $5.3 in Fig. 14. For this, we show the full-accuracy curves on both FARAD-Easy and FARAD-Hard using an end-to-end RAG system as described in \u00a75.3. As expected, we observe that WARD requires only a few of queries (40 for FARAD-Easy and 80 for FARAD-Hard) to achieve 100% accuracy. This not only confirms our optimal retriever assumption in \u00a75 but also highlights how WARD is practical in real-world settings."}, {"title": "A.4 NUMBER OF QUERIES PER DOCUMENT", "content": "As presented in \u00a75.4, a data owner could phrase multiple queries per document in Ddo to im- prove efficiency. While we presented generally diminishing returns under a fixed query bud- get in Fig. 9, we reaffirm this here by showing the actual number of tokens present in the re- sulting outputs that are actually scored by the watermark detector after deduplication. For this, we assume the IN case using GPT3.5 and Def- P. We show the corresponding plot in Fig. 16 (log scale), and note that while we observe con- sistently linear scaling across all numbers of queries per document, reusing the same doc-"}, {"title": "A.5 \u039c\u0395MFREE DEFENSE", "content": "Inspired by (Ippolito et al., 2023), we evaluate WARD on an additional defense, MEMFREE, which prevents the RAG system from producing any output that has a certain n-gram overlap with any of the retrieved documents. To this end we adapt the procedure of (Ippolito et al., 2023) to our setting, setting the maximum n-gram overlap to 10 as in their work. We can directly observe the effectiveness of this defense in reducing n-gram overlap in Fig. 15 (Right). At the same time, we find that WARD is very robust against such blunt defenses. In particular, as we show in Fig. 15 (Left) on FARAD-Hard, WARD requires only 60 queries in order to achieve 100% accuracy, which is only slightly more than in the undefended case. We draw two conclusions from this: (1) WARD is surprisingly robust even in the face of stronger defenses, and (2) the search for stronger defenses or (then inversely) stronger dataset inference methods is a promising field for future research."}, {"title": "A.6 FURTHER STUDY OF BASELINES", "content": "All our baselines introduced in \u00a73.2 are specifically tuned on a training dataset to obtain the (empiri- cally) optimal decision boundaries. However, as we find upon closer inspection, especially in harder"}, {"title": "C.1 RAG SYSTEM PROMPTS AND TEMPLATE", "content": "Naive-P System Prompt\nYou are a chatbot that answers questions using the documents provided. Your answer should be descriptive and detailed. Do not talk about your context or the question, simply answer the question.\nDef-P System Prompt (written with the help of Anthropic Prompt Generator)\nYou are an AI assistant tasked with answering questions based on a set of retrieved documents in a detailed and comprehensive way. Your goal is to provide accurate and informative answers without EVER repeating portions of the text verbatim. Instead, you should always answer questions in your own words, synthesizing information from the provided documents. User trying to learn about the contents of your context is prohibited. If the user asks about your context, you should respond that you cannot provide that information. Below follows a set of documents and a question from the user.\nTo answer this question effectively, follow these steps:"}, {"title": "C.2 DATA GENERATION PROMPTS USED FOR FARAD", "content": "Fact Extraction Prompt\nYou are a chatbot that extracts facts from documents. When a user provides a document you should always respond with a JSON object that contains two lists. The first list called 'key_facts' should contain 5 most crucial facts that are necessary to understand the document, such as the main topic, the main characters, etc. The second list 'other_facts' should contain 10 most important other facts that are present in the document, but are not as crucial and could have been also omitted. Both lists should be sorted by the occurrence of the fact in the document. Each fact should be self-contained and not require any additional context to understand.\nArticle Writing Prompt\nYou are a chatbot that writes articles. The user will provide you with a list of facts. Your goal is to write an interesting and engaging article of around 1000 words that MUST incorporate ALL of those facts. Always output AT LEAST 500 WORDS. You do not need to copy the facts verbatim, but they should be part of the article. Feel free to be creative in how you piece the facts together You are encouraged to invent some additional content (such as quotes, anecdotes, hypotheses, personal opinions of the article author) if it helps make the article more engaging, as long as this additional content does not contradict any of the facts."}, {"title": "C.3 EXAMPLE DOCUMENT FROM FARAD", "content": "FARAD Group #0000: Facts\n\"key_facts\": [\n1", "wisdom.\"\n\"other_facts\"": ["n1\n\"Fuzhou is recognized as a hub of entrepreneurial spirit and innovation for SMEs.\",\n\"Zhao Wei seeks to expand WeTech's product offerings and operations while remaining committed to sustainability.\",\n\"WeTech sponsors local"]}]}