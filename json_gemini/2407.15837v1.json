{"title": "Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning", "authors": ["Yibing Wei", "Abhinav Gupta", "Pedro Morgado"], "abstract": "Masked Image Modeling (MIM) has emerged as a promising method for deriving visual representations from unlabeled image data by predicting missing pixels from masked portions of images. It excels in region-aware learning and provides strong initializations for various tasks, but struggles to capture high-level semantics without further supervised fine-tuning, likely due to the low-level nature of its pixel reconstruction objective. A promising yet unrealized framework is learning representations through masked reconstruction in latent space, combining the locality of MIM with the high-level targets. However, this approach poses significant training challenges as the reconstruction targets are learned in conjunction with the model, potentially leading to trivial or suboptimal solutions. Our study is among the first to thoroughly analyze and address the challenges of such framework, which we refer to as Latent MIM. Through a series of carefully designed experiments and extensive analysis, we identify the source of these challenges, including representation collapsing for joint online/target optimization, learning objectives, the high region correlation in latent space and decoding conditioning. By sequentially addressing these issues, we demonstrate that Latent MIM can indeed learn high-level representations while retaining the benefits of MIM models. Code is available at https://github.com/yibingwei-1/LatentMIM.", "sections": [{"title": "Introduction", "content": "Masked Image Modeling (MIM), a learning framework that derives visual representations from unlabeled image data, has recently gained prominence. This technique masks a substantial part of an image and trains a model to predict the missing pixels using the surrounding context. Despite the simple learning objective, MIM has been shown to learn powerful representations, which, when fine-tuned, can achieve state-of-the-art performance on a variety of downstream tasks, including object classification, detection, and segmentation [3, 13]. MIM approaches offer several key advantages over other self-supervised visual representation learning methods. By requiring the model to accurately reconstruct all masked patches, MIM incentivizes the model to maintain distinct local representations of each image region and forces the model to reason over the spatial layout of object subparts. These benefits make MIM a popular approach for learning visual representations in a self-supervised manner."}, {"title": "Latent Masked Image Modeling", "content": "Latent MIM is a self-supervised learning framework that aims to learn visual representations through masked image modeling in latent space. As illustrated in Fig. 2, Latent MIM models comprise three components: an online encoder f(\u00b7), a target encoder fr(\u00b7), and a decoder g(.). Similar to pixel-based MIM approaches [3, 13], each image is first divided into a set of L small patches xi along with their location within the image pi. This set X = {(xi,Pi)}=1 is randomly split into two disjoint sets: the visible Xy = {(xi, Pi)}iev and target XT = {(xi, Pi)}iet patches, where V and T are non-overlapping index sets of grid locations. As the name suggests, the visible patches are used to generate the latent representation of an image, while the target patches are used as the reconstruction targets. To accomplish this, the online encoder extracts latent representations of the visible patches\n{zi}i\u03b5\u03bd = \u0396\u03bd = f(Xv),", "equations": ["{zi}i\u03b5\u03bd = \u0396\u03bd = f(Xv)"]}, {"title": "2.1 Framework Overview", "content": "which are then used to inform the decoder for predicting patches at the target locations PT = {Pi}iET\n\u017b\u03c4 = g(\u0396\u03bd, \u03a1\u03c4).\nHowever, instead of predicting pixel values, Latent MIM imposes the latent representations obtained from the target encoder as the reconstruction targets,\n{Zi}iet = ZT = fr(XT).\nLatent MIM models can then be trained to minimize the discrepancy A between the predicted target representations \u017dy and those obtained from the target encoder ZT. This is achieved by a reconstruction loss\nLrec = E [A (ZT, 2T)],\nwhere the expectation is taken over the training dataset. In pixel-based MIM, the mean squared error (MSE) is a popular choice for \u2206, \u2206 = \u2211iET ||2i - Zi||2. However, as shown in Section 3.4, the MSE loss is not effective for Latent MIM.", "equations": ["\u017b\u03c4 = g(\u0396\u03bd, \u03a1\u03c4).", "{Zi}iet = ZT = fr(XT).", "Lrec = E [A (ZT, 2T)],"]}, {"title": "2.2 Distinctions and similarities from related frameworks", "content": "Masked Image Modeling (MIM) methods learn by predicting masked parts of an image using targets derived from simple transformations of the original image. Targets can be raw pixels [13,31], hand-crafted features such as HOG [25] or pre-trained features [3]. Thus, low-level MIM is required to maintain localized representations of an image. However, the model capacity is partially consumed by low-level details, limiting its capacity for high-level semantics.\nContrastive Learning (CL) In contrast, CL learns representations of each image in the context of the dataset in which they occur [5,7,12]. While CL can achieve outstanding semantics without finetuning, learning in context can be undesirable, for example, when the data distribution does not provide appropriate negative samples that highlight meaningful semantic distinctions between images. Batch dependencies also make contrastive objectives less flexible and reproducible, as they depend on the available compute resources. Instead, pure Latent MIM objectives learn representations from the image itself, and independently from other images.\nPrevious explorations for Latent MIM The potential of Latent MIM is tied to its ability to learn from high-level and region-aware targets, which are continuously improved throughout training. However, prior works attempting to deploy Latent MIM e [2,6,9,21,29,32] do not directly tackle the core optimization challenges inherent in Latent MIM discussed in Section 3. These challenges have been mitigated by treating latent MIM as a supplementary objective to other techniques, such as global contrastive learning [21, 32], low-level reconstruction [6], or alternatively, by using fixed pre-trained features as the latent targets [3,9], instead of jointly learning the target and online representations.\nThe most closely related prior works focused on pure latent MIM, include data2vec [2] ConMIM [29] and I-JEPA [1]. Despite their contribution, data2vec and ConMIM use mask tokens instead of patch removal to hide image regions, causing a mismatch between pre-training and real-world deployment with un-masked images, which leads to poor performance in downstream tasks without"}, {"title": "Challenges of Latent MIM", "content": "To better focus on the various difficulties of Latent MIM optimization, we introduce and analyze each challenge separately. We begin by describing a naive implementation that fails to learn meaningful representations. We then introduce each of the four challenges (Section 3.3-3.6) and conduct a thorough analysis of potential strategies to address them. Each section builds on the findings of the preceding one, yielding increasingly effective instantiations of the Latent MIM framework. Fig. 1 shows the overview of performance through this progression."}, {"title": "3.1 Experimental Design", "content": "Pre-training For a comprehensive experimental analysis within an acceptable compute budget, we conduct experiments using the standard ViT-B transformer backbone for both the online and target decoders and trained the model on the ImageNet-100 (IN100) dataset, a subset of ImageNet-1k, containing 100 classes selected at random. The class partition used follows that of [23,27]. This dataset contains approximately 125,000 images, sufficient for executing experiments with statistical significance. Section 4 further discusses the scaling properties of the proposed framework in the context of larger datasets, like ImageNet-1k.\nAll models were pre-trained for 300 epochs using the AdamW [16] optimizer with a batch size of 1024, a base learning rate of 1.5 \u00d7 10-4, following 30 warm-up epochs and a cosine decay schedule. Data augmentations were applied to the input images, including random horizontal flipping and random image cropping with a minimum crop area of 0.2.\nDownstream tasks We evaluated all models using nearest-neighbor, linear probing, and fine-tuning on the same dataset. For nearest-neighbor evaluation, we extract the representations for all images in the test set and report the fraction of samples whose nearest neighbor shares the same class. For linear probing, we train for 20 epochs using the LARS optimizer [30] with a base learning rate of 0.5, 2 warm-up epochs, and a batch size of 1024. For fine-tuning, we train for 50 epochs using an AdamW [16] optimizer with a base learning rate of 0.001, a weight decay of 0.05, 5 warm-up epochs, and a batch size of 1024."}, {"title": "3.2 Naive Latent MIM", "content": "To fully appreciate the complexities involved in learning meaningful representations through Latent MIM, we initiate our discussion with a simple and intuitive implementation, by closely following those of pixel-based MIM models [13]. Input images of resolution 224\u00d7224 are divided into a regular 14\u00d714 grid of patches and partitioned into a 25/75% split to create the sets of visible/target patches, respectively. We then use a standard ViT-B transformer [10] to encode both the visible and target patches (f and fr) and a transformer decoder g to reconstruct the target patches from the visible latent representations. We use the same self-attention decoder as in MAE [13], but reduce its depth to only 3 layers since, in Latent MIM, the decoder task (predicting latents from latents) is less complex than in pixel-based MIM (predicting pixels from latents). Both online and target encoders and the decoder are jointly trained using the mean squared error (MSE) loss between the predicted and target latents.\nAs shown in Fig. 3, the Naive Latent MIM model is highly unstable. We observe that within a short number of iterations, the model collapses into a degenerate solution. The subsequent sections seek to understand the underlying causes of this finding and propose strategies to mitigate it."}, {"title": "3.3 Challenge 1: Latent Target Optimization", "content": "Representation collapse in latent MIM is consistent with the findings of negative-free contrastive learning methods like BYOL [12]. Unsurprisingly, because both the online and target encoders are trained to minimize the discrepancy between their representations, they can easily lead to a degenerate solution where all images are mapped to the same latent. Common strategies to prevent collapse involve introducing asymmetries between encoders and detaching the target encoder from gradient computation [12]. We explore three strategies:\nStand-alone target encoder. The first strategy is to treat the target encoder independently of the online encoder, without weight sharing.\nWeight-sharing with stop gradient. Alternatively, we can use a siamese architecture with shared weights, but avoid using the target's gradients to update the online encoder.\nMomentum targets. Using Momentum encoder [22] is also a common strategy to create asymmetries, and enhance the target encoder simultaneously. As"}, {"title": "3.4 Challenge 2: Reconstruction Objective for Latent \u041c\u0406\u041c", "content": "In pixel-based MIM, patch reconstruction is enforced by minimizing the mean squared error (MSE) between the decoder's output and the pixel intensities at the target locations. In order to accurately reconstruct the target pixels from a limited visible context, the model is encouraged to learn representations that capture both global semantics as well as patch-specific information. However, since in Latent MIM, the targets are the learned latent representations, we hypothesize that direct reconstruction objectives can also contribute to the optimization challenges, as there are no negative samples to stabilize the learning process. Thus, we investigate the impact of different reconstruction objectives on the effectiveness of Latent MIM.\nDirect reconstruction We study three loss functions that directly minimize the discrepancy between predicted and target representations, namely MSE, L1, and Huber losses. While the MSE is widely used due to its simplicity and effectiveness, the L1 loss is robust to outliers. The Huber loss combines the best properties of the MSE and L1 losses by being quadratic for small errors and linear for large ones, thus providing a balance between robustness and efficiency. Mathematically, the reconstruction losses for the k-th target patch are\n\u0394k = ||2k - zk||2, A1 = ||2k - Zk||1,\nL2\nL1\nHuber = { \n1-2  if \u2206L2 < 82\n\u03b4. (\u03941 - 8/2) otherwise.", "equations": ["\u0394k = ||2k - zk||2, A1 = ||2k - Zk||1,", "Huber = { \n1-2  if \u2206L2 < 82\n\u03b4. (\u03941 - 8/2) otherwise."]}, {"title": "Patch discrimination", "content": "The main drawback of direct reconstruction for Latent MIM is its inability to explicitly incentivize the model to learn diverse representations across the image. This is unlike in pixel-based MIM where the pixel intensities are guaranteed to vary across the image. To circumvent this limitation, we propose a patch discrimination objective, where the model is trained to distinguish between target patches using an InfoNCE loss [18]. Specifically, for each target patch k, the predicted latent 2k is contrasted with the latents of all target patches zi\nAk\nPatch Disc\nexp(-sim (2k, zk))\n= -log\n\u03a3\u03b9\u03b5\u03c4 exp(-sim (2k, z\u0131))',\nwhere\nsim(2, z) =\n2Tz\n||2||||2||", "equations": ["Ak\nPatch Disc\nexp(-sim (2k, zk))\n= -log\n\u03a3\u03b9\u03b5\u03c4 exp(-sim (2k, z\u0131))',", "sim(2, z) =\n2Tz\n||2||||2||"]}, {"title": "3.5 Challenge 3: Semantic Correlation between Nearby Patches", "content": "Image content displays high correlation within proximate regions. This can render mask reconstruction a trivial task, as the model can interpolate missing information from nearby visible patches. To counter this, pixel-based MIM masks a substantial portion of the image (up to 75%) [13]. Latent representations, which are expected to encapsulate high-level semantics, exhibit even higher correlations across patches compared to their corresponding pixels. This correlation potentially undermines the effectiveness of the task for representation learning.\nHigh mask ratio. Latent MIM also benefits from high mask ratios. Beyond reducing patch correlation and enhancing representation learning, this strategy also enables faster training and a lower GPU memory footprint. However, it has to keep enough visible patches to capture critical features within the image."}, {"title": "Non-contiguous grids", "content": "To further minimize the correlation between visible and target patches without reducing the amount of visible cues, we experiment with stochastic non-contiguous grids (Fig. 4). This strategy increases the distance between patches by separating each patch from its neighbors by a random number of unused pixels. Specifically, let P represent the patch size and G the average gap between consecutive patches. Stochastic non-continuous grids can be conveniently generated by initially splitting the image into a regular grid of patches, each of size (P + G) \u00d7 (P + G), and then extracting a P \u00d7 P patch at random from each grid location."}, {"title": "Patch Similarity Constraints", "content": "While the previous strategies reduce correlation by refining the patch selection process, we can also impose explicit constraints to avoid correlation in the latent space. This is especially important since, as highlighted in the previous challenge (Table 2), patch representations tend to cluster together when trained with no constraints. To counteract this, we impose a constraint on both the visible and predicted latents, Zy and \u017d\u0442,\nR = (y \u2212 Ei,jet [sim (zi, \u017cj)])\u00b2 + (y \u2013 Ei,jev [sim (zi, zj)])2,\nwhere y is a predefined desired inter-patch similarity.", "equations": ["R = (y \u2212 Ei,jet [sim (zi, \u017cj)])\u00b2 + (y \u2013 Ei,jev [sim (zi, zj)])2,"]}, {"title": "3.6 Challenge 4: Decoder Design for Latent Reconstruction", "content": "The final important component of Latent MIM is the decoder, responsible for predicting target representations from visible patches. The decoder design is crucial for the model's ability to effectively utilize the high-level semantics extracted from the encoder. We explore the impact of different decoder architectures.\nSelf-attention decoder Pixel-based MIM models employ a self-attention transformer to predict the target pixels \u03a7\u03c4. \u03a4o accomplish this, the decoder receives two sets of inputs: the latents of visible patches Zy and a sequence of learnable mask tokens m, marked by the fixed SinCos positional embeddings p of their corresponding locations (i.e., m + pt \u2200t \u2208 T and zv + pv \u2200\u03c5 \u2208 V). After processing this sequence through a series of self-attention blocks, the decoder outputs the target representations using a linear head.\nCross-attention decoder Unlike pixel-based MIM, in Latent MIM the targets ZT are representations with a similar level of abstraction than the features obtained from the encoder Zy. Thus, the decoder should be able to condition on these visible representations Zy more directly. While self-attention only conditions once ZT through the input sequence, cross-attention allows the decoder to condition on Zy at every layer. A standard cross-attention architecture [24] with alternating self-attention, cross-attention, and feed-forward MLP blocks is used to update the prediction tokens m + pt t \u2208 T.\nVisual cues from neighboring visible patches As discussed in (Section 3.5), neighboring patch latents can be highly correlated. To prevent the decoder from focusing excessively on interpolating between these patches, we embed visual cues directly into its input sequence. This allows the decoder to better focus on more fine-grained spatial reasoning. Specifically, with Py and Pr as the positional embeddings for the visible and target patches, respectively, we initialize the prediction tokens as m\u2081 = m + pt + Softmax\u0131 (PTP)) Zy at each target location i \u2208 \u03a4. This setup equips the mask tokens MT = {mi}iet with a weighted blend of latents from the nearest visible patches, providing precise location and visual cues right from the start."}, {"title": "4 Scaling to ImageNet-1k", "content": "The previous section provided a detailed analysis of the optimization challenges and design decisions for Latent MIM on a medium-sized dataset. In this section, we show the scalability of Latent MIM to larger datasets, specifically ImageNet-1k, and compare it to prior work. We also highlight the strongly localized semantics learned by Latent MIM by evaluating the trained model on unsupervised scene segmentation, video object segmentation, and few-shot transfer learning.\nImplementation We scaled up pretraining to ImageNet-1k using the optimal Latent MIM configuration from Section 3. The model is pretrained for 800 epochs using the Adam optimizer. Full implementation details are in the Appendix A."}, {"title": "4.1 ImageNet-1k Classification", "content": "Following [13], we evaluate the learned representations on ImageNet-1K using Nearest Neighbor (NN) and linear probing (LP) protocols."}, {"title": "4.2 Properties beyond Classification", "content": "The Latent MIM framework is designed to learn localizable and semantically rich representations. We showcase these properties on unsupervised segmentation, semi-supervised video object segmentation, and transfer learning.\nUnsupervised segmentation An emerging property of Latent MIM is its capacity for semantic clustering of local representations, which enables impressive segmentation and scene parsing outcomes without the need for supervised fine-tuning. Fig. 6 illustrates the unsupervised segmentation maps generated by hierarchical clustering of patch-level representations. Compared to both lower-level MIM approaches, such as MAE, and earlier latent MIM methods like data2vec, our Latent MIM model learns better semantic and localizable representations.\nVideo Object Segmentation Our Latent MIM model can also maintain both semantic integrity and localization accuracy, even in complex, dynamic video sequences. DAVIS-2017 semi-supervised video object segmentation benchmark evaluates the ability to generate precise object segmentation masks in videos, starting from ground truth masks of the initial frame. We follow the experimental protocol in [14], which segments scenes through a nearest-neighbor strategy"}, {"title": "Conclusion", "content": "We identified and addressed the key training challenges in Latent MIM, demonstrating its capacity to generate spatially diverse, high-level semantic representations. This is evidenced by significant improvements in nearest neighbor and linear probe evaluation on ImageNet, fewshot transfer learning, as well as in segmentation tasks requiring minimal or no supervision. We hope this work will inspire further exploration into Latent MIM for learning fine-grained semantics without human supervision."}, {"title": "Appendix", "content": ""}, {"title": "A Implementation Details", "content": ""}, {"title": "A.1 Optimization details", "content": "To ensure reproducibility, we provide detailed implementation details for all our experiments in Table 8. The table shows the hyperparameters used for pre-training and the linear probing and finetuning downstream tasks, both for the main study (Section 3 of main text) and on the ImageNet-1k dataset (Section 4 of main text). The configurations are consistent with the ones used in prior work [13]."}, {"title": "A.2 Latent MIM configuration used for ImageNet-1k (Section 4)", "content": "We scaled up pre-training to ImageNet-1k using the optimal Latent MIM configuration discussed in Section 3 of the main text. We use the standard ViT-B/16 [10] as the online and momentum encoder and a 3-layer cross-attention decoder with visual cues described in Section 3 of the main text. The projector is a 3-layer MLP with 4096 hidden dimensions, GELU activation, and layer-norm. We use a 90% mask ratio to mask the 14\u00d714 grid in a non-contiguous way with gap G = 4, resulting in 20 visible and 176 target patches per sample. The model is trained with a Patch Discrimination loss together with similarity constraints on both Zy and \u017by with a 0.1 coefficient where the target similarity is updated from 0.75 to 0.25 following the cosine schedule. For both linear probing and finetuning, we also experimented with the pooling method used for feature extraction before classification. In addition to the standard max pooling, average pooling and the use of the CLS token, we apply a pooling method in between max and average pooling, which averages the top k feature values. Empirical results indicate that setting k = 10 yields optimal performance."}, {"title": "B Supplementary Analysis", "content": ""}, {"title": "B.1 Comparison with I-JPEA", "content": "I-JEPA is a prior exploration in pure latent MIM and has shown promising results, but it still shows signs of training instability. Fig. 8 shows I-JEPA training"}, {"title": "B.2 Latent MIM: From low-level to high-level latents", "content": "To assess the effectiveness of the learned representations with different types of targets, we conducted experiments using momentum encoders of varying depths in our training process, ranging from a depth of 0 (i.e., pixel reconstruction) to the full momentum encoder of depth 12. We also observed that the choice of"}, {"title": "B.3 Ablations around optimal configuration", "content": "In the main paper, we sequentially address the challenges encountered by Latent MIM models, proposing a series of solutions that progressively build upon each other. However, it is possible that the effectiveness of early solutions was due to the shortcomings of the initial latent MIM implementation. To further dissect the impact of each component, we conduct ablation studies around the optimal configuration. The downstream performance of these models is detailed in Table 11. As evidenced by the results, the proposed solutions effectively tackle the challenges, even when compared to robust latent MIM models that incorporate all but one of the proposed solutions."}, {"title": "C ImageNet-1K Classification: Beyond low-level and latent MIM.", "content": "The main paper provides comparisons to prior low-level and latent MIM frameworks on ImageNet-1k, focusing on representation quality. Table 12 provides a broader view of the downstream performance on ImageNet-1k, including comparisons to other methods beyond low-level and latent MIM, while also assessing"}]}