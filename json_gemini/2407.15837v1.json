{"title": "Towards Latent Masked Image Modeling for\nSelf-Supervised Visual Representation Learning", "authors": ["Yibing Wei", "Abhinav Gupta", "Pedro Morgado"], "abstract": "Masked Image Modeling (MIM) has emerged as a promising\nmethod for deriving visual representations from unlabeled image data\nby predicting missing pixels from masked portions of images. It excels\nin region-aware learning and provides strong initializations for various\ntasks, but struggles to capture high-level semantics without further su-\npervised fine-tuning, likely due to the low-level nature of its pixel re-\nconstruction objective. A promising yet unrealized framework is learn-\ning representations through masked reconstruction in latent space, com-\nbining the locality of MIM with the high-level targets. However, this\napproach poses significant training challenges as the reconstruction tar-\ngets are learned in conjunction with the model, potentially leading to\ntrivial or suboptimal solutions. Our study is among the first to thor-\noughly analyze and address the challenges of such framework, which\nwe refer to as Latent MIM. Through a series of carefully designed ex-\nperiments and extensive analysis, we identify the source of these chal-\nlenges, including representation collapsing for joint online/target opti-\nmization, learning objectives, the high region correlation in latent space\nand decoding conditioning. By sequentially addressing these issues, we\ndemonstrate that Latent MIM can indeed learn high-level representa-\ntions while retaining the benefits of MIM models. Code is available at\nhttps://github.com/yibingwei-1/LatentMIM.", "sections": [{"title": "1 Introduction", "content": "Masked Image Modeling (MIM), a learning framework that derives visual rep-\nresentations from unlabeled image data, has recently gained prominence. This\ntechnique masks a substantial part of an image and trains a model to predict\nthe missing pixels using the surrounding context. Despite the simple learning\nobjective, MIM has been shown to learn powerful representations, which, when\nfine-tuned, can achieve state-of-the-art performance on a variety of downstream\ntasks, including object classification, detection, and segmentation [3, 13]. MIM\napproaches offer several key advantages over other self-supervised visual repre-\nsentation learning methods. By requiring the model to accurately reconstruct all\nmasked patches, MIM incentivizes the model to maintain distinct local repre-\nsentations of each image region and forces the model to reason over the spatial\nlayout of object subparts. These benefits make MIM a popular approach for\nlearning visual representations in a self-supervised manner."}, {"title": "2 Latent Masked Image Modeling", "content": ""}, {"title": "2.1 Framework Overview", "content": "Latent MIM is a self-supervised learning framework that aims to learn visual\nrepresentations through masked image modeling in latent space. As illustrated\nin Fig. 2, Latent MIM models comprise three components: an online encoder\nf(\u00b7), a target encoder fr(\u00b7), and a decoder g(.). Similar to pixel-based MIM\napproaches [3, 13], each image is first divided into a set of L small patches xi\nalong with their location within the image pi. This set X = {(xi,Pi)}=1 is\nrandomly split into two disjoint sets: the visible Xy = {(xi, Pi)}iev and target\nXT = {(xi, Pi)}iet patches, where V and T are non-overlapping index sets of\ngrid locations. As the name suggests, the visible patches are used to generate\nthe latent representation of an image, while the target patches are used as the\nreconstruction targets. To accomplish this, the online encoder extracts latent\nrepresentations of the visible patches\n{zi}i\u03b5\u03bd = \u0396\u03bd = f(Xv),\n(1)\nwhich are then used to inform the decoder for predicting patches at the target\nlocations PT = {Pi}iET\n\u017b\u03c4 = g(\u0396\u03bd, \u03a1\u03c4).\n(2)\nHowever, instead of predicting pixel values, Latent MIM imposes the latent rep-\nresentations obtained from the target encoder as the reconstruction targets,\n{Zi}iet = ZT = fr(XT).\n(3)\nLatent MIM models can then be trained to minimize the discrepancy A between\nthe predicted target representations \u017dy and those obtained from the target\nencoder ZT. This is achieved by a reconstruction loss\nLrec = E [A (ZT, 2T)],\n(4)\nwhere the expectation is taken over the training dataset. In pixel-based MIM, the\nmean squared error (MSE) is a popular choice for \u2206, \u2206 = \u2211iET ||2i - Zi||2.\nHowever, as shown in Section 3.4, the MSE loss is not effective for Latent MIM."}, {"title": "2.2 Distinctions and similarities from related frameworks", "content": "Masked Image Modeling (MIM) methods learn by predicting masked parts\nof an image using targets derived from simple transformations of the original\nimage. Targets can be raw pixels [13,31], hand-crafted features such as HOG [25]\nor pre-trained features [3]. Thus, low-level MIM is required to maintain localized\nrepresentations of an image. However, the model capacity is partially consumed\nby low-level details, limiting its capacity for high-level semantics.\nContrastive Learning (CL) In contrast, CL learns representations of each\nimage in the context of the dataset in which they occur [5,7,12]. While CL can\nachieve outstanding semantics without finetuning, learning in context can be\nundesirable, for example, when the data distribution does not provide appro-\npriate negative samples that highlight meaningful semantic distinctions between\nimages. Batch dependencies also make contrastive objectives less flexible and\nreproducible, as they depend on the available compute resources. Instead, pure\nLatent MIM objectives learn representations from the image itself, and indepen-\ndently from other images.\nPrevious explorations for Latent MIM The potential of Latent MIM is\ntied to its ability to learn from high-level and region-aware targets, which are\ncontinuously improved throughout training. However, prior works attempting to\ndeploy Latent MIM e [2,6,9,21,29,32] do not directly tackle the core optimiza-\ntion challenges inherent in Latent MIM discussed in Section 3. These challenges\nhave been mitigated by treating latent MIM as a supplementary objective to\nother techniques, such as global contrastive learning [21, 32], low-level recon-\nstruction [6], or alternatively, by using fixed pre-trained features as the latent\ntargets [3,9], instead of jointly learning the target and online representations.\nThe most closely related prior works focused on pure latent MIM, include\ndata2vec [2] ConMIM [29] and I-JEPA [1]. Despite their contribution, data2vec\nand ConMIM use mask tokens instead of patch removal to hide image regions,\ncausing a mismatch between pre-training and real-world deployment with un-\nmasked images, which leads to poor performance in downstream tasks without"}, {"title": "3 Challenges of Latent MIM", "content": "To better focus on the various difficulties of Latent MIM optimization, we in-\ntroduce and analyze each challenge separately. We begin by describing a naive\nimplementation that fails to learn meaningful representations. We then introduce\neach of the four challenges (Section 3.3-3.6) and conduct a thorough analysis of\npotential strategies to address them. Each section builds on the findings of the\npreceding one, yielding increasingly effective instantiations of the Latent MIM\nframework. Fig. 1 shows the overview of performance through this progression."}, {"title": "3.1 Experimental Design", "content": "Pre-training For a comprehensive experimental analysis within an acceptable\ncompute budget, we conduct experiments using the standard ViT-B transformer\nbackbone for both the online and target decoders and trained the model on the\nImageNet-100 (IN100) dataset, a subset of ImageNet-1k, containing 100 classes\nselected at random. The class partition used follows that of [23,27]. This dataset\ncontains approximately 125,000 images, sufficient for executing experiments with\nstatistical significance. Section 4 further discusses the scaling properties of the\nproposed framework in the context of larger datasets, like ImageNet-1k.\nAll models were pre-trained for 300 epochs using the AdamW [16] optimizer\nwith a batch size of 1024, a base learning rate of 1.5 \u00d7 10-4, following 30 warm-\nup epochs and a cosine decay schedule. Data augmentations were applied to the\ninput images, including random horizontal flipping and random image cropping\nwith a minimum crop area of 0.2.\nDownstream tasks We evaluated all models using nearest-neighbor, linear\nprobing, and fine-tuning on the same dataset. For nearest-neighbor evaluation,\nwe extract the representations for all images in the test set and report the fraction\nof samples whose nearest neighbor shares the same class. For linear probing,\nwe train for 20 epochs using the LARS optimizer [30] with a base learning rate\nof 0.5, 2 warm-up epochs, and a batch size of 1024. For fine-tuning, we train\nfor 50 epochs using an AdamW [16] optimizer with a base learning rate of 0.001,\na weight decay of 0.05, 5 warm-up epochs, and a batch size of 1024."}, {"title": "3.2 Naive Latent MIM", "content": "To fully appreciate the complexities involved in learning meaningful representa-\ntions through Latent MIM, we initiate our discussion with a simple and intuitive\nimplementation, by closely following those of pixel-based MIM models [13]. Input\nimages of resolution 224\u00d7224 are divided into a regular 14\u00d714 grid of patches\nand partitioned into a 25/75% split to create the sets of visible/target patches,\nrespectively. We then use a standard ViT-B transformer [10] to encode both\nthe visible and target patches (f and fr) and a transformer decoder g to re-\nconstruct the target patches from the visible latent representations. We use the\nsame self-attention decoder as in MAE [13], but reduce its depth to only 3 layers\nsince, in Latent MIM, the decoder task (predicting latents from latents) is less\ncomplex than in pixel-based MIM (predicting pixels from latents). Both online\nand target encoders and the decoder are jointly trained using the mean squared\nerror (MSE) loss between the predicted and target latents.\nAs shown in Fig. 3, the Naive Latent MIM model is highly unstable. We\nobserve that within a short number of iterations, the model collapses into a\ndegenerate solution. The subsequent sections seek to understand the underlying\ncauses of this finding and propose strategies to mitigate it."}, {"title": "3.3 Challenge 1: Latent Target Optimization", "content": "Representation collapse in latent MIM is consistent with the findings of negative-\nfree contrastive learning methods like BYOL [12]. Unsurprisingly, because both\nthe online and target encoders are trained to minimize the discrepancy between\ntheir representations, they can easily lead to a degenerate solution where all\nimages are mapped to the same latent. Common strategies to prevent collapse\ninvolve introducing asymmetries between encoders and detaching the target en-\ncoder from gradient computation [12]. We explore three strategies:\nStand-alone target encoder. The first strategy is to treat the target encoder\nindependently of the online encoder, without weight sharing.\nWeight-sharing with stop gradient. Alternatively, we can use a siamese ar-\nchitecture with shared weights, but avoid using the target's gradients to\nupdate the online encoder.\nMomentum targets. Using Momentum encoder [22] is also a common strategy\nto create asymmetries, and enhance the target encoder simultaneously. As"}, {"title": "3.4 Challenge 2: Reconstruction Objective for Latent \u041c\u0406\u041c", "content": "In pixel-based MIM, patch reconstruction is enforced by minimizing the mean\nsquared error (MSE) between the decoder's output and the pixel intensities at\nthe target locations. In order to accurately reconstruct the target pixels from\na limited visible context, the model is encouraged to learn representations that\ncapture both global semantics as well as patch-specific information. However,\nsince in Latent MIM, the targets are the learned latent representations, we hy-\npothesize that direct reconstruction objectives can also contribute to the opti-\nmization challenges, as there are no negative samples to stabilize the learning\nprocess. Thus, we investigate the impact of different reconstruction objectives\non the effectiveness of Latent MIM.\nDirect reconstruction We study three loss functions that directly minimize\nthe discrepancy between predicted and target representations, namely MSE,\nL1, and Huber losses. While the MSE is widely used due to its simplicity and\neffectiveness, the L1 loss is robust to outliers. The Huber loss combines the best\nproperties of the MSE and L1 losses by being quadratic for small errors and\nlinear for large ones, thus providing a balance between robustness and efficiency.\nMathematically, the reconstruction losses for the k-th target patch are\n\u0394L2 = ||2k - zk||2, \u0394L1 = ||2k - Zk||1,\n(5)\n\u0394Huber =\n{\n1\n2\n\u0394kL2\nif \u0394L2 < 82\n\u03b4. (\u0394L1 - 8/2) otherwise.\n(6)\n18."}, {"title": "Patch discrimination", "content": "The main drawback of direct reconstruction for Latent\nMIM is its inability to explicitly incentivize the model to learn diverse repre-\nsentations across the image. This is unlike in pixel-based MIM where the pixel\nintensities are guaranteed to vary across the image. To circumvent this limita-\ntion, we propose a patch discrimination objective, where the model is trained to\ndistinguish between target patches using an InfoNCE loss [18]. Specifically, for\neach target patch k, the predicted latent 2k is contrasted with the latents of all\ntarget patches zi\nAk\nPatch Disc\n= -log\nexp(-sim (2k, zk))\n\u03a3\u03b9\u03b5\u03c4 exp(-sim (2k, z\u0131))' sim(2, z) =\n2Tz\n||2||||2||'(7)\nwhere is a temperature hyper-parameter. To minimize this loss, the model\nmust not only align the predicted and target latents accurately but also ensure\nsufficient diversity among the latents within the image.\nStudy Results We compare each of the\naforementioned loss functions for latent re-\nconstruction. Building on the findings of Sec-\ntion 3.3, targets are computed from a mo-\nmentum encoder. The results shown in Ta-\nble 2 indicate that, although still insufficient\nfor effective representation learning through\nlatent MIM, the patch discrimination loss\n(PatchDisc) can learn better representations\nthan with direct reconstruction losses. In par-\nticular, we highlight the significant improve-\nments in the nearest neighbor and linear probing accuracy, which are more sen-\nsitive to the quality of the learned representations."}, {"title": "3.5 Challenge 3: Semantic Correlation between Nearby Patches", "content": "Image content displays high correlation within proximate regions. This can ren-\nder mask reconstruction a trivial task, as the model can interpolate missing in-\nformation from nearby visible patches. To counter this, pixel-based MIM masks a\nsubstantial portion of the image (up to 75%) [13]. Latent representations, which\nare expected to encapsulate high-level semantics, exhibit even higher correla-\ntions across patches compared to their corresponding pixels. This correlation\npotentially undermines the effectiveness of the task for representation learning.\nHigh mask ratio. Latent MIM also benefits from high mask ratios. Beyond\nreducing patch correlation and enhancing representation learning, this strategy\nalso enables faster training and a lower GPU memory footprint. However, it has\nto keep enough visible patches to capture critical features within the image."}, {"title": "3.6 Challenge 4: Decoder Design for Latent Reconstruction", "content": "The final important component of Latent MIM is the decoder, responsible for\npredicting target representations from visible patches. The decoder design is cru-\ncial for the model's ability to effectively utilize the high-level semantics extracted\nfrom the encoder. We explore the impact of different decoder architectures.\nSelf-attention decoder Pixel-based MIM models employ a self-attention trans-\nformer to predict the target pixels \u03a7\u03c4. \u03a4o accomplish this, the decoder receives\ntwo sets of inputs: the latents of visible patches Zy and a sequence of learnable\nmask tokens m, marked by the fixed SinCos positional embeddings p of their\ncorresponding locations (i.e., m + pt \u2200t \u2208 T and zv + pv \u2200\u03c5 \u2208 V). After process-\ning this sequence through a series of self-attention blocks, the decoder outputs\nthe target representations using a linear head.\nCross-attention decoder Unlike pixel-based MIM, in Latent MIM the targets\nZT are representations with a similar level of abstraction than the features ob-\ntained from the encoder Zy. Thus, the decoder should be able to condition on\nthese visible representations Zy more directly. While self-attention only condi-\ntions once ZT through the input sequence, cross-attention allows the decoder\nto condition on Zy at every layer. A standard cross-attention architecture [24]\nwith alternating self-attention, cross-attention, and feed-forward MLP blocks is\nused to update the prediction tokens m + pt t \u2208 T.\nVisual cues from neighboring visible patches As discussed in (Section 3.5),\nneighboring patch latents can be highly correlated. To prevent the decoder from\nfocusing excessively on interpolating between these patches, we embed visual\ncues directly into its input sequence. This allows the decoder to better focus on\nmore fine-grained spatial reasoning. Specifically, with Py and Pr as the posi-\ntional embeddings for the visible and target patches, respectively, we initialize\nthe prediction tokens as m\u2081 = m + pt + Softmax\u0131 (PTP)) Zy at each tar-\nget location i \u2208 \u03a4. This setup equips the mask tokens MT = {mi}iet with\na weighted blend of latents from the nearest visible patches, providing precise\nlocation and visual cues right from the start."}, {"title": "4 Scaling to ImageNet-1k", "content": "The previous section provided a detailed analysis of the optimization challenges\nand design decisions for Latent MIM on a medium-sized dataset. In this section,\nwe show the scalability of Latent MIM to larger datasets, specifically ImageNet-\n1k, and compare it to prior work. We also highlight the strongly localized seman-\ntics learned by Latent MIM by evaluating the trained model on unsupervised\nscene segmentation, video object segmentation, and few-shot transfer learning.\nImplementation We scaled up pretraining to ImageNet-1k using the optimal\nLatent MIM configuration from Section 3. The model is pretrained for 800 epochs\nusing the Adam optimizer. Full implementation details are in the Appendix A."}, {"title": "4.1 ImageNet-1k Classification", "content": "Following [13], we evaluate the learned representations on ImageNet-1K using\nNearest Neighbor (NN) and linear probing (LP) protocols. Table 5 compares"}, {"title": "4.2 Properties beyond Classification", "content": "The Latent MIM framework is designed to learn localizable and semantically rich\nrepresentations. We showcase these properties on unsupervised segmentation,\nsemi-supervised video object segmentation, and transfer learning.\nUnsupervised segmentation An emerging property of Latent MIM is its ca-\npacity for semantic clustering of local representations, which enables impressive\nsegmentation and scene parsing outcomes without the need for supervised fine-\ntuning. Fig. 6 illustrates the unsupervised segmentation maps generated by hier-\narchical clustering of patch-level representations. Compared to both lower-level\nMIM approaches, such as MAE, and earlier latent MIM methods like data2vec,\nour Latent MIM model learns better semantic and localizable representations.\nVideo Object Segmentation Our Latent MIM model can also maintain both\nsemantic integrity and localization accuracy, even in complex, dynamic video\nsequences. DAVIS-2017 semi-supervised video object segmentation benchmark\nevaluates the ability to generate precise object segmentation masks in videos,\nstarting from ground truth masks of the initial frame. We follow the experimen-\ntal protocol in [14], which segments scenes through a nearest-neighbor strategy"}, {"title": "5 Conclusion", "content": "We identified and addressed the key training challenges in Latent MIM, demon-\nstrating its capacity to generate spatially diverse, high-level semantic represen-\ntations. This is evidenced by significant improvements in nearest neighbor and\nlinear probe evaluation on ImageNet, fewshot transfer learning, as well as in\nsegmentation tasks requiring minimal or no supervision. We hope this work will\ninspire further exploration into Latent MIM for learning fine-grained semantics\nwithout human supervision."}, {"title": "Appendix", "content": ""}, {"title": "A Implementation Details", "content": ""}, {"title": "A.1 Optimization details", "content": "To ensure reproducibility, we provide detailed implementation details for all our\nexperiments in Table 8. The table shows the hyperparameters used for pre-\ntraining and the linear probing and finetuning downstream tasks, both for the\nmain study (Section 3 of main text) and on the ImageNet-1k dataset (Section\n4 of main text). The configurations are consistent with the ones used in prior\nwork [13]."}, {"title": "A.2 Latent MIM configuration used for ImageNet-1k (Section 4)", "content": "We scaled up pre-training to ImageNet-1k using the optimal Latent MIM con-\nfiguration discussed in Section 3 of the main text. We use the standard ViT-\nB/16 [10] as the online and momentum encoder and a 3-layer cross-attention\ndecoder with visual cues described in Section 3 of the main text. The projector\nis a 3-layer MLP with 4096 hidden dimensions, GELU activation, and layer-\nnorm. We use a 90% mask ratio to mask the 14\u00d714 grid in a non-contiguous\nway with gap G = 4, resulting in 20 visible and 176 target patches per sample.\nThe model is trained with a Patch Discrimination loss together with similarity\nconstraints on both Zy and \u017by with a 0.1 coefficient where the target similar-\nity is updated from 0.75 to 0.25 following the cosine schedule. For both linear\nprobing and finetuning, we also experimented with the pooling method used for\nfeature extraction before classification. In addition to the standard max pool-\ning, average pooling and the use of the CLS token, we apply a pooling method\nin between max and average pooling, which averages the top k feature values.\nEmpirical results indicate that setting k = 10 yields optimal performance."}, {"title": "B Supplementary Analysis", "content": ""}, {"title": "B.1 Comparison with I-JPEA", "content": "I-JEPA is a prior exploration in pure latent MIM and has shown promising re-\nsults, but it still shows signs of training instability. Fig. 8 shows I-JEPA training"}, {"title": "B.2 Latent MIM: From low-level to high-level latents.", "content": "To assess the effectiveness of the learned representations with different types of\ntargets, we conducted experiments using momentum encoders of varying depths\nin our training process, ranging from a depth of 0 (i.e., pixel reconstruction) to\nthe full momentum encoder of depth 12. We also observed that the choice of"}, {"title": "B.3 Ablations around optimal configuration", "content": "In the main paper, we sequentially address the challenges encountered by Latent\nMIM models, proposing a series of solutions that progressively build upon each\nother. However, it is possible that the effectiveness of early solutions was due to\nthe shortcomings of the initial latent MIM implementation. To further dissect\nthe impact of each component, we conduct ablation studies around the optimal\nconfiguration. The downstream performance of these models is detailed in Ta-\nble 11. As evidenced by the results, the proposed solutions effectively tackle the\nchallenges, even when compared to robust latent MIM models that incorporate\nall but one of the proposed solutions."}, {"title": "C ImageNet-1K Classification: Beyond low-level and\nlatent MIM.", "content": "The main paper provides comparisons to prior low-level and latent MIM frame-\nworks on ImageNet-1k, focusing on representation quality. Table 12 provides a\nbroader view of the downstream performance on ImageNet-1k, including compar-\nisons to other methods beyond low-level and latent MIM, while also assessing"}, {"title": "D Visualizations of Latent MIM local features", "content": "We showcase the localizability and semantic richness of Latent MIM features by\nvisualizing the learned local representations within an image. All visualizations\nare produced using the ViT-B/16 model trained on ImageNet-1K.\nFig. 11 shows unsupervised segmentation maps generated by hierarchical\nclustering of patch-level representations, comparing to MAE and data2vec. The\nresults demonstrate our model's superior ability to semantically cluster pixels,\ncompared to both lower-level MIM approaches, such as MAE, and earlier latent\nMIM methods like data2vec. This is consistent across various scenes. These qual-\nitative examples also confirm our hypothesis: low-level MIM features are inade-\nquate for grasping high-level semantics. For example, in the case of the baseball\nplayer, MAE shows segmentation maps where pixels seem to be clustered based\non color, grouping the white shirt with the white signs in the background. In con-\ntrast, our model can capture the semantic boundaries of the various components\nof the scene.\nTo further understand how the latent MIM learned representations are clus-\ntered, we visualize the t-SNE embeddings of the local features within an image in\nFig. 10. Each point in the t-SNE plot represents a patch in the image, color-coded\nby the hierarchical clustering labels shown in the segmentation maps next to it.\nAs can be seen, the latent representations form semantically meaningful clus-\nters and sub-clusters associated with objects and object parts, respectively. For\nexample, notice, how the baseball bat, the player's body and legs are clustered\nseparately within the larger cluster of the player.\nFinally, we visualize the semantic neighborhood of patches in a variety of\nimages in Fig. 12. Within each image, we visualize regions with similar represen-\ntations to a given pixel. We observe that, as the similarity threshold decreases,\nthe semantic neighborhood becomes more inclusive, capturing increasingly larger\nregions of the image, while still preserving the semantic coherence of the neigh-\nborhood. This indicates that the learned representations encode both semantics\nand spatial information about the image."}]}