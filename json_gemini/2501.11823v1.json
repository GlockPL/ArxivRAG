{"title": "Toward Scalable Graph Unlearning: A Node Influence Maximization based Approach", "authors": ["Xunkai Li", "Bowen Fan", "Zhengyu Wu", "Zhiyu Li", "Rong-Hua Li", "Guoren Wang"], "abstract": "Machine unlearning, as a pivotal technology for enhancing model robustness and data privacy, has garnered significant attention in prevalent web mining applications, especially in thriving graph-based scenarios. However, most existing graph unlearning (GU) approaches face significant challenges due to the intricate interactions among web-scale graph elements during the model training: (1) The gradient-driven node entanglement hinders the complete knowledge removal in response to unlearning requests; (2) The billion-level graph elements in the web scenarios present inevitable scalability issues. To break the above limitations, we open up a new perspective by drawing a connection between GU and conventional social influence maximization. To this end, we propose Node Influence Maximization (NIM) through the decoupled influence propagation model and fine-grained influence function in a scalable manner, which is crafted to be a plug-and-play strategy to identify potential nodes affected by unlearning entities. This approach enables offline execution independent of GU, allowing it to be seamlessly integrated into most GU methods to improve their unlearning performance. Based on this, we introduce Scalable Graph Unlearning (SGU) as a new fine-tuned framework, which balances the forgetting and reasoning capability of the unlearned model by entity-specific optimizations. Extensive experiments on 14 datasets, including large-scale ogbn-papers100M, have demonstrated the effectiveness of our approach. Specifically, NIM enhances the forgetting capability of most GU methods, while SGU achieves comprehensive SOTA performance and maintains scalability.", "sections": [{"title": "1 Introduction", "content": "Recently, with the rapid growth of graph-oriented AI applications, graph neural networks (GNNs) have become a focal point of interest in the web mining community [22, 25, 28]. During the GNN deployment, the realistic demands for model robustness and data privacy have led to the emergence of a new learning paradigm graph unlearning (GU). For instance, in graph-based recommendations, eliminating the negative impact of noise caused by malicious transactions and human errors on the model during the training is necessary to enhance robustness [30, 55, 60]. The request for privacy is particularly predominant in healthcare networks, and it is essential to comply with patient's requests to be forgotten by removing the corresponding gradient-driven knowledge [37, 38, 49]. Therefore, the development of GU is imperative.\nIn general, GU demands real-time responses to delete graph elements (i.e., unlearning entities, UE) per requests during training, and achieves the following objectives: (1) Model Update: Modifying original model weights to eliminate the influence of training gradients generated by the UE (Forgetting Capability); (2) Inference Protection: Ensuring that the data removal does not negatively affect predictions for Non-UE (Reasoning Capability). Compared to unlearning in CV, GU is more challenging due to the complexity of quantifying the influence of UE. For high-influenced entities (HIE) that are affected by UE gradients but concealed within the Non-UE, it is crucial to reveal them for complete knowledge removal.\nRecently, numerous GU methods have been proposed from various perspectives, but inherent limitations persist: (1) Projection-based methods: Projector [12] and GraphEditor [11] project the original model into a specific subspace via the optimization constraints to eliminate UE influence. Limitations: They rely on stringent assumptions for linear GNNs, lacking generalizability. (2) Partition-based methods: GraphEraser [6], GUIDE [46], and GraphRevoker [63] partition the graph to enable independent learning. This allows for retraining specific partitions in response to unlearning requests without starting from scratch. Limitations: They rely on well-designed partition and output aggregation strategies that hinder deployment. (3) Gradient-based methods: CGU [9], CEU [54], ScaleGUN [59] and GST [34] follow the  ( \\epsilon, \\delta)-approximate certified unlearning [17] under linear GNNs. They establish bounds on the gradient residual norm for graph-based scenarios. GIF [53] introduces graph-based influence functions [26] and directly updates the model by the corresponding Hessian matrix. Limitations: They prioritize certified and exact unlearning from the theoretical perspective over practical deployment, leading to inflexible and sub-optimal performance [31]. (4) Learning-based methods: GNNDelete [7], MEGU [29], and UtU [43] directly construct loss functions for UE, HIE, and Non-UE to fine-tune the original model, aiming to obtain a modified model while achieving inference protection. Limitations: They often lack scalability in their fine-tune mechanisms and limited exploration of the reliable HIE. More discussions can be found in Appendix A.1"}, {"title": "2 Preliminaries", "content": "In this section, we introduce some notations along with fundamental knowlege which supports our framework, including problem definition, scalable graph neural networks, and social influence maximization.\n2.1 Problem Formalization\nIn this work, we primarily utilize semi-supervised node classification to evaluate GU. This task is based on the topology of the labeled set $V_L$ and the unlabeled set $V_U$, where the nodes in $V_U$ are supervised by $V_L$. Furthermore, the core of link-level GU evaluation is to achieve collaborative edge representation using connected nodes.\nConsider an undirected graph $G = (V, &, X)$ with $|V| = n$ nodes, $|&| = m$ edges, and $X = X$. The feature matrix is $X = {x_1, ...,x_n}$ in which $x_v \\in R^f$ represents the feature vector of node $v$, and $f$ represents the dimension of the node attribute information, the adjacency matrix is $A \\in R^{n \\times n}$. Besides, $Y = {y_1, ..., y_n}$ represents the label matrix, where $y_v \\in R^{|Y|}$ denotes a one-hot vector and $|Y|$ is the number of the label classes. In GU, after receiving unlearning request $AG = {AV, \\DeltaE, \\DeltaX}$ on original model parameterized by W, the goal is to modify trainable model weights and output the predictions of Non-UE, both with minimal impact from the UE. The typical unlearning requests include feature-level $AG = {\\emptyset, \\emptyset, \\DeltaX}$, node-level $AG = {AV, \\emptyset, \\emptyset}$, edge-level $AG = {\\emptyset, \\DeltaE, \\emptyset}$ in $V_L$.\n2.2 Scalable Graph Neural Networks\nMotivated by the spectral graph theory and deep neural networks, GCN [25] simplifies the topology-based convolution operator [2] by the first-order approximation of Chebyshev polynomials [23]. The forward propagation of the l-th layer GCN is formulated as:\n$X^{(l)} = \\sigma(\\tilde{A}X^{(l-1)}W^{(l)}), \\tilde{A} = \\tilde{D}^{-\\frac{1}{2}}\\hat{A}\\tilde{D}^{-\\frac{1}{2}}$,\nwhere $\\tilde{D}$ represents the degree matrix of $\\hat{A}$ (self-loop), $W$ represents the trainable weights, and $\\sigma(\\cdot)$ represents the non-linear activation function. Intuitively, GCN aggregates the neighbors' representation embeddings from the (l-1)-th layer to form the representation of the l-th layer. Such a simple paradigm has proved to be effective in various downstream tasks. However, GCN suffers from severe scalability issues since it executes the feature propagation and transformation recursively and is trained in a full-batch manner. To achieve scalability, decouple-based approaches have been investigated.\nRecent studies [65] have observed that non-linear feature transformation contributes little to performance as compared to graph propagation. For instance SGC [52] reduces GNNs into a linear model operating on k-layer propagated features as follows:\n$X^{(k)} = \\tilde{A}^kX^{(0)}, Y = softmax(WX^{(k)})$,\nwhere $X^{(0)} = X$ and $X^{(k)}$ is the k-layer propagated features. As the propagated features $X^{(k)}$ can be precomputed, SGC is easy to scale to large graphs. Inspired by it, SIGN [15] proposes to concatenate the propagated features in a learnable manner. S\u00b2GC [72] proposes to average the propagated results from the perspective of spectral analysis $X^{(k)} = \\sum_{l=0}^{k} \\tilde{A}^{l}X^{(0)}$. GBP [5] utilizes the $\u00df$ weighted manner to achieve propagation $X^{(k)} = \\sum_{l=0}^{k} w_l\\tilde{A}^{l}X^{(0)}, w_l = \\beta(1-\\beta)^{l}$. GAMLP [69] achieves information aggregation based on the attention mechanisms $X^{(k)} = A_kX^{(0)}|| \\sum_{l=0}^{k-1} w_lX^{(l)}$, where attention weight $w_l$ has multiple calculation versions. GRAND+ [14] proposes a generalized forward push propagation algorithm to obtain P, which is used to approximate k-order PageRank weighted A with higher flexibility. Then it obtains propagated results $X = PWX^{(0)}."}, {"title": "2.3 Social Influence Maximization", "content": "The influence maximization (IM) problem in social networks aims to select B nodes to maximize the number of activated (or influenced) nodes in the network [24], which captures the network diffusion dynamics. Given a graph $G = (V, &)$, the formulation is as follows:\n$\\max_{S} \\sigma(S), s.t. S \\subseteq V, |S| = B$,\nwhere $\\sigma(S)$ denotes the set of nodes activated by the seed set $S$ under certain influence propagation models, such as the Linear Threshold and Independent Cascade models [24]. The maximization of $\\sigma(S)$ is NP-hard. However, if $\\sigma(S)$ is non-decreasing and sub-modular with respect to S, a greedy algorithm can provide an approximation guarantee of $(1-1/e)$ [32]. Despite some studies [18, 33, 44, 57, 70] making significant efforts to optimize algorithm complexity from the perspectives of numerical linear algebra, integrating IM with contemporary graph learning paradigms remains limited. Among the few studies, RIM [66] and Grain [68] integrate IM with active learning by using activated nodes to enhance data efficiency. IMBM [16] utilizes activated nodes and current nodes as a group to empower graph sampling. Scapin [51] combines IM with adversarial attacks on graphs, aiming to identify and perturb critical graph elements. Despite their effectiveness, these methods are designed for specific scenarios and cannot be directly applied to GU.\nIn this paper, we focus on bridging reliable HIE selection and NIM within GU rather than addressing classic social IM problems. Specifically, we propose a decoupled-based influence propagation model and a fine-grained influence function for NIM by leveraging the characteristics of decoupled-based scalable GNNs, including both feature and topology influence. Based on NIM, we propose SGU, a scalable GU framework that outperforms SOTA baselines in handling various unlearning requests. In a nutshell, NIM and SGU build on the influence propagation in IM, showcasing both the feasibility and potential of this connection and raising a promising future direction. More discussions about them are in Appendix A.3."}, {"title": "3 Scalable Graph Unlearning Framework", "content": "In this section, we introduce SGU in detail. As depicted in Fig. 2, upon receiving a data removal request, if the current GNN backbone satisfies the decoupled-based paradigm, SGU can be seamlessly integrated with it. As for the sampling-based scalable GNNs, SGU requires only an additional weight-free graph propagation. In our illustration, SGU first utilizes pre-cached propagated features and prediction to capture the structural and node semantic insights. This enables a fine-grained quantification of the influence of UE on Non-UE from topology and feature perspectives. Subsequently, SGU integrates these influences into a unified criterion and utilizes it to select HIE. This process is repeated until the unlearning budget B is exhausted. Based on this, we identify UE and HIE, which are then used to construct entity-specific optimization objectives for computation-friendly fine-tuning. This is because it significantly reduces computational dependence on a large number of irrelevant nodes (i.e., Non-UE without HIE) during gradient propagation. Meanwhile, this process involves prototype representation and CL, ensuring a favorable trade-off between forgetting and reasoning. Now, we introduce each component of SGU in detail. The complete algorithm and complexity analysis can be found in Appendix A.4."}, {"title": "3.1 Node Influence Maximization", "content": "The message-passing mechanism in most GNNs creates intricate interactions among graph entities during training, which results in a complex and unquantifiable gradient flow. This poses a significant challenge in completely removing the model knowledge contributed by UE. Recent studies have observed that GNNs primarily derive their benefits from weight-free graph propagation rather than learnable module [28, 65, 67], promoting the decoupled-based scalable GNNs. Notably, this weight-free graph propagation is crucial to the interactions of graph entities and offers a new perspective for analyzing the complex influence of UE without gradient interference. Inspired by social influence maximization, we bridge it with GU and establish a new influence quantification criterion.\n3.1.1 Decoupled-based Influence Propagation Model. To quantify the intricate interactions, we clarify k-step weight-free graph propagation and W*-weighted learning process in a decoupled manner:\n$ \\bar{X} = PX = \\sum_{l=0}^{kl} w_i \\prod_{i=0}^{kl} (DAD) \\bar{X}^{(0)}, \\, \\bar{Y} = w^*\\bar{X}$,\nwhere P is the graph propagation equation, serving as the paradigm to model node proximity measures and GNN propagation formulas (i.e., $\\tilde{A}^lX^{(k-1)}$ in SGC [52] and $w_l(\\tilde{D}^{-\\frac{a}{2}}\\hat{A}\\tilde{D}^{-\\frac{b}{2}})^lX^{(k-1)}$ in AGP [47]). For a given node u, a node proximity query yields P(v), representing the importance of v with respect to u. The weight sequence $w_i$ and kernel coefficient r affects transport probabilities for modeling node proximity. For more instantiations of P, please refer to Sec. 2.2.\nBy taking Eq. (4) as a type of influence propagation model, NIM utilizes GNN backbone to obtain $\\bar{X}$ and $\\bar{Y}$, achieving model-agnostic topology and feature influence quantification. This is because $\\bar{X}$ captures structural insights from the l-hop neighbors, guided by the probabilities obtained from k-step propagation originating from the source node to other nodes in the graph. Meanwhile, $\\bar{Y}$ establishes the gradient dependencies among nodes through label supervision, capturing potential feature-oriented relationships. Now, we define node-wise influence through representation differences as follows:\nDEFINITION 1. (Topology Influence). The topology influence of node u on node v after k-step graph propagation is the $L_1$-norm of the expected Jacobin matrix in propagated features:\n$I_t(v, u, k) = || \\frac{\\partial \\bar{x_v}^{(k)}}{\\partial \\bar{x_u}^{(k)}} ||_1$\nDEFINITION 2. (Feature Influence). The feature influence of node u on node v after k-step graph propagation is the $L_1$-norm of the expected Jacobin matrix in model predictions:\n$I_f(v, u, k) = || \\frac{\\partial \\bar{y_v}^{(k)}}{\\partial \\bar{y_u}^{(k)}} ||_1$\nGiven the k-step propagation, I(v, u, k) captures the sum over probabilities of all possible influential paths from v to u. Consider propagation operator as $P = \\tilde{D}^{-1}A$, the normalized $\\hat{I}(v, u, k)$ is the probability that random walk starting at v ends at u within k steps:\n$\\hat{I}(v, u, k) = \\frac{I(v, u, k)}{\\sum_{o \\in \\Delta_v}I(v, o, k)} = \\sum_{p_u} \\prod_{i=k} a_{v^{(i-1)}, v^{(i)}}$,\nwhere $p_u$ be a path $[v^{(k)}, v^{(k-1)},...,v^{(0)}]$ of length k from node v to u and $a_{v^{(i-1)}, v^{(i)}}$ is the normalized weight of edge $(v^{(i)}, v^{(i-1)})$.\n3.1.2 Fine-grained Influence Function. Intuitively, the weak influence of u on v with small $\\hat{I}(v, u, k)$ would have a limited impact on v due to few influence paths to propagate topology-driven structural insights and feature-driven label supervision. Therefore, we define the activated node v by requiring the maximum influence $\\hat{I}(v, S, k) = \\max_{u\\in S} \\hat{I}(v, u, k)$ of S on the node v, which is larger than a threshold. In the context of GU, S and its activated node set are the UE and HIE. Now, we provide the formal definition below:\nDEFINITION 3. (IM-based HIE). Consider node-wise UE derived from the unlearning request as $\\Delta V$, Eq. (4) as the influence propagation model. The UE is seed node set S and HIE is the activated node set $\\sigma(S)$, which is a subset of nodes in V/AV and activated by S. Given a threshold $\\theta$, the IM-based HIE (activated node set) is defined as:\n$\\sigma(S) = \\cup_{v \\in V/\\Delta V, \\hat{I}(v, S, k) \\ge \\theta} {v}, S = \\Delta V.$"}, {"title": "3.2 Entity-based Fine-tuning", "content": "In general, the key idea of SGU is to implement efficient fine-tuning through entity-specific optimization objectives (i.e., UE and HIE) from the perspectives of forgetting and reasoning. This allows us to edit the original model W with minimal training overhead (Model Update), eliminating the gradient impact of UE while maintaining the predictive performance of Non-UE (Inference Protection). Notably, in large-scale graphs, the size of UE and HIE is significantly smaller than Non-UE, thereby avoiding redundant computations.\n3.2.1 Unlearning Entities Perspective. To completely remove the gradient-driven model knowledge contributed by the UE, we have the following key insights: (1) Prediction-level: SGU employs label-driven supervision information to directly erase the model memory of UE predictions, achieving a simple yet intuitive forgetting. (2) Embedding-level: SGU further removes the model memory of UE embeddings by generating prototypes from other nodes with UE-specific labels, achieving complete forgetting. Notably, although Non-UE is used in generating prototype representations, this process does not involve gradient updates and needs to be performed only once before fine-tuning. Therefore, it can be considered a type of self-supervised information with minimal computational cost.\nLabel-based Forgetting. To achieve simple yet effective forgetting of UE by label-driven supervision, we construct the cross-entropy loss by randomly shuffling Random($\\cdot$) the UE-corresponding labels:\n$L^f_l = - \\sum_{x_u \\in \\Delta V} Random(Y_u) log \\bar{Y}_u, \\Delta V = T(AG)$.\nTo address various types of unlearning requests in Sec. 2.1 within NIM, we use a transformation function T(.) to convert data removal requests into node UE. Specifically, for feature and node unlearning, we perform identity mapping. As for edge unlearning, the two nodes connected by the unlearning edges are treated as UE.\nPrototype-driven Forgetting. Building upon the above direct prediction-level unlearning, we further achieve comprehensive unlearning by perturbing the UE embedding space through prototype representation. This process can be formally expressed as:\n$P_c = \\frac{1}{|S_{V/\\Delta V(c)}|} \\sum_{(\\bar{x_i}, y_i) \\in S_{V/\\Delta V(c)}} f_{emb}(\\bar{x_i}),$\n$L^f_p = \\sum_{c=1}^Y \\sum_{(\\bar{x_u}, Y_u) \\in \\Delta V(c)} || f_{emb} (\\bar{x_u}) - Random(P_c)||_F,$\nwhere $S$ is the set of samples annotated with label class c. Notably, GNN typically comprises embedding and prediction components $f_{emb}$ and $f_{pre}$. The former maps initial sample features to an embedding space for semantic extraction, while the latter produces task-specific outputs using these semantic embeddings. These prototypes are derived from class-specific averaging embeddings, which encapsulate high-level statistic-conveyed semantic information. By constructing the above optimization objective, SGU can separate UE representations from Non-UE representations in the global embedding space, thereby achieving complete knowledge removal.\n3.2.2 High-influence Entities Perspective. Generally, removing the gradient knowledge contributed by UE negatively impacts model predictions, particularly for HIE, since the data removal weakens the label-driven supervision and disrupts the model. Enforcing the forgetting of UE will exacerbate this deterioration. This presents a dilemma: We cannot ensure the complete removal of UE influence for HIE, but meanwhile the decline of HIE prediction seems inevitable.\nTo break this limitation, we reformulate the optimization objectives for HIE and have the following key insights: (1) Embedding-level: SGU additionally integrates Non-UE for sampling, constructing CL loss to completely eliminate UE influence. (2) Prediction-level: SGU uses memory-based supervision from the original model to ensure predictions. Although these two optimization objectives seem to be in conflict, they can coexist harmoniously. This is because SGU aims to obtain a new representation perspective for HIE that diverges from the original embedding space without affecting predictions. This approach preserves predictive performance while eliminating UE influence by embedding transformation. Moreover, although SGU utilizes Non-UE for sampling to provide abundant information, this process is only required once before fine-tuning.\nCL-driven Forgetting. Similar to Prototype-driven Forgetting, we aim to perturb the original embedding space of HIE to eliminate UE influence. However, since HIE essentially belongs to Non-UE, we must ensure its predictive performance. Inspired by CL, to provide predictive guidance, we perform label-specific sampling on Non-UE (without HIE) as positive query samples for each anchor embedding $h_v$ in HIE. As for UE knowledge removal, we perform label-specific sampling within UE and HIE as negative query samples for $h_v$:\n$L^f_c = - log \\frac{\\sum_{u \\in S_{pos}} exp(d(h_v, h_u))}{\\sum_{u \\in S_{pos}} exp(d(h_v, h_u)) + \\sum_{u \\in S_{neg}} exp(d(h_v, h_u))}$,\nwhere d() is the distance metric between the anchor and query samples. Therefore, overall forgetting loss is $L_f = L^f_l + L^f_p + L^f_c$.\nMemory-based Reasoning. Benefiting from the standard training process before receiving data removal requests, the original GNN model weights W and predictions $\\bar{Y}$ encapsulate strong reasoning capability. This reliable memory can be viewed as a type of self-supervised information empowering the fine-tuning as follows:\n$L_p = L2 (W) + KL (\\tilde{Y}, \\hat{Y}) = ||W||^2 + \\sum_{HIE} \\tilde{Y}_{ij} log \\frac{\\tilde{Y}_{ij}}{\\hat{Y}_{ij}}|_{HIE}$,\nThe first term is L2 regularization, which under the supervision of $L_f$ aims to limit model updates to preserve reasoning capability under UE knowledge removal. The second term is KL loss, which directly leverages the strong reasoning capability of the original model for HIE to supervise the predictions of the fine-tuned model."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments on our approach. To begin with, we introduce 14 benchmark datasets along with GNN backbones and GU baselines. Subsequently, we present the GU evaluation methodology. Details about these experimental setups can be found in Appendix A.5-A.9. After that, we aim to address the following questions: Q1: As a plug-and-play module, what is the impact of NIM on HIE-based GU? Q2: Compared to existing GU strategies, can SGU achieve SOTA performance? Q3: If SGU is effective, what contributes to its performance? Q4: How robust is SGU when dealing with different scales of unlearning requests and hyperparameters? Q5: What is the running efficiency of SGU.\n4.1 Experimental Setup\nDatasets. In the node-specific transductive scenario, we conduct experiments on five citations and three co-purchase networks. In the inductive scenario, we perform experiments on protein, image, and social networks. We also consider collaboration, protein, and citation networks to provide a comprehensive evaluation in the link-specific scenario. More details can be found in Appendix A.5.\nBaselines. We conduct experiments using the following backbones: (1) Prevalent GNNs: GCN, GAT, GIN; (2) Sampling-based GNNs: GraphSAGE, GraphSAINT, ClusterGCN; (3) Decouple-based GNNs: SGC, SIGN, GBP, S2GC, AGP, GAMLP; (4) Link-specific GNNs: SEAL, NeoGNN, BUDDY, NCNC, MPLP. We compare SGU with GraphEditor, GUIDE, GraphRevoker, CGU, CEU, GIF, D2DGN, GN- NDelete, UtU, MEGU, ScaleGUN (details in Appendix A.6-A.8). OOT occurs when GU requires more runtime than Retrain.\nEvaluation. For node-specific scenarios, the evaluation methodologies are as follows: (1) Model Update: Inspired by prevalent methods, we set up the following experiments to quantify the unlearning capability: (i) Edge Attack [29, 53]: We randomly select two nodes with different labels to add noisy edges, treating them as UE. Intuitively, a method that achieves better unlearning will effectively mitigate their negative impact on predictions. (ii) MIA [6, 7, 46]: The attacker, with access to both the original and modified model, determines whether a specific node has been revoked from the modified model. The evaluation maintains a 1:1 ratio and higher AUC(>0.5) indicates more information leakage. (2) Inference Protection: We directly evaluate the reasoning capability of the modified model by reporting the Non-UE predictions via the F1 score. Please refer to Appendix A.9 for more details on the link-specific evaluation.\n4.2 Performance Comparison\nA Hot-and-plug HIE Selection Module. To answer Q1, we present the improvements in forgetting capability brought by integrating NIM into HIE-based GU approaches in Table 1. Notably, we report only the unlearning performance because identifying HIE is essentially about revealing the impact of UE, which aims to complete knowledge removal. This improvement is directly reflected in the unlearning performance. Additionally, we conduct ablation experiments based on SGU in Table 5 to further validate our claims, where we observe that NIM also brings a slight improvement to the reasoning capability. We attribute this improvement to reliable HIE, which prompts the entity-specific optimization objectives and achieves a better trade-off between forgetting and reasoning.\nA New Scalable GU Framework. To answer Q2 from the reasoning perspective, we report the node-level predictive performance in Table 2 and Table 3, which validates that SGU consistently outperforms baselines across all datasets on unlearning requests. For instance, on the Photo and Computer, SGU exhibits a remarkable average improvement of 2.7% over the SOTA baselines. Notably, the persistent advantage in both transductive and inductive settings demonstrates the strong generalization of SGU. Meanwhile, Table 4 provides results on link-level reasoning capacity, which demonstrate that SGU outperforms the most competitive UtU tailored for edge unlearning, achieving average performance gains of 2.1% across three datasets. These observations indicate that SGU can achieve satisfactory performance on our diversified selections of backbones, highlighting its potential for widespread application.\nTo answer Q2 from the forgetting perspective, we visualize the node-level unlearning performance of SGU and competitive baselines (confirmed by previous experiments) in Fig. 3. Intuitively, if a GU approach possesses robust forgetting capability, it can mitigate the adverse effects caused by noisy edges, thereby ensuring consistent and satisfactory predictive performance. From the experimental results, we observe that GNNDelete and GIF do not consistently achieve optimal performance. Although D2DGN and MEGU exhibit relative stability, they are still weaker than SGU. Due to space limitations, please refer to Appendix A.9 for more experiments about link-specific prediction and unlearning evaluation."}, {"title": "4.3 Ablation Study", "content": "To answer Q3, we investigate the contributions of NIM, prototype-driven forgetting (Proto.), and CL-driven forgetting (CL Loss) in SGU. For NIM, it quantifies the impact of UE on Non-UE during training through a decoupled-based influence propagation model and fine-grained influence function, identifying HIE for constructing subsequent optimization objectives (technical details in Sec. 3.1). It can be considered as a pre-processing step for SGU. This strikes a better trade-off between forgetting and reasoning, thereby improving the comprehensive performance of GU. For more analysis on NIM, please refer to Sec. 4.2. For Proto., which encapsulates high-level, statistic-conveyed semantic information. Based on this, SGU performs prototype perturbation to separate UE from Non-UE representations in the global embedding space, achieving complete knowledge removal (technical details in Eq.(10)). Experimental results in Table 5 highlight Proto.'s effectiveness in enhancing forgetting capability. Notably, since Eq.(10) is used solely to enhance unlearning performance, its impact on predictive performance is minimal. For CL Loss, it achieves comprehensive improvement by pulling Non-UE and class-specific positive query samples closer while pushing entity-specific negative query samples further in the embedding space, leveraging self-supervised information to guide prediction and transform the embedding space. Results in Table 5 validate all the above claims on CL. For instance, in the PubMed dataset using GraphSAINT as the backbone, the F1 Score increases from 86.3% to 87.8%, and the AUC decreases from 0.535 to 0.524."}, {"title": "4.4 Robustness Analysis", "content": "To answer Q4, we first evaluate SGU and the two most competitive baselines across varying unlearning scales in Fig. 4. Due to space limitations, additional experimental results and analysis are presented in Appendix A.10. Our experiments indicate that SGU excels in handling unlearning requests with different scales. Notably, as the unlearning ratio increases, there is an inevitable decline in the performance of the unlearned model across all GU methods. This decline is due to the higher proportion of data removal, which significantly impacts the model predictions. However, it is evident that SGU outperforms GNNDelete and MEGU under identical unlearning conditions. These empirical findings and analyses underscore SGU's robustness in effectively handling unlearning requests.\nSubsequently, we investigate B and  $\\theta$, and present their sensitivity analysis results for SGU in Fig. 5 from the forgetting and reasoning perspectives. In our implementation, NIM utilizes B and $\\theta$ to control the size of HIE and the criteria for selecting HIE. Since HIE is directly related to the entity-specific optimization objectives, its size and quantity determine the extent of fine-tuning and the trade-off between forgetting and reasoning loss (technical details in Sec. 3.2). Therefore, B and $\\theta$ essentially determine SGU's running efficiency while balancing unlearning and prediction. Our experimental results indicate that both optimal forgetting and reasoning capabilities stem from larger $\\theta$ and are associated with larger and smaller B. This is because larger $\\theta$ strictly filters Non-UE, ensuring high-quality HIE for the subsequent optimization. Regarding B, larger values achieve complete knowledge removal and optimal unlearning performance, while smaller values highlight the model's inherent reasoning capability. For more details on balancing unlearning and predictive performance by combining the forgetting and reasoning loss through $\\lambda$, please refer to Appendix A.11."}, {"title": "4.5 Efficiency Comparison", "content": "To answer Q5, we present a visualization in Fig. 6 to illustrate the running efficiency. In our report, the running time encompasses both one-time pre-processing and model updating. Notably, since the S2GC is essentially a scalable GNN, the computational cost of training from scratch is significantly reduced, setting a higher efficiency benchmark for Retrain. Based on this, we draw the following conclusions: (1) Partition-based GUIDE and GraphRevoker suffer from excessive time costs during large-scale graph partitioning. The computation cost of gradient-based CGU and GIF in large-scale scenarios is unacceptable due to considering all data samples contributing to the gradient for optimization. (2) Projection-based GraphEditor demonstrates high efficiency, attributed to its efficient subspace projection strategy. However, the strict linear assumption hampers its generalizability in deployment. Furthermore, this advantage is not observed on Reddit. (3) The most computation cost of SGU is pre-processing, including NIM and sampling for Eq. (10) and Eq. (11). Excluding pre-processing (NIM), the fine-tuning requires minimal time due to entity-specific design. Additionally, since pre-processing is a one-time step, SGU is suitable for real-world scenarios with frequent unlearning requests, offering practical potential."}, {"title": "5 Conclusion", "content": "In this paper, we advocate a novel perspective for GU by connecting it with social IM. To this end, we propose NIM, which defines a decoupled-based influence propagation model to seamlessly integrate with graph propagation formulas and utilizes a fine-grained influence function as a unified criterion. Based on this, SGU achieves efficient model fine-tuning by entity-specific optimization. To further improve NIM, exploring the GU-tailored influence propagation model and quantification functions is worthwhile. Meanwhile, exploring new techniques such as knowledge distillation to design forgetting and reasoning loss is also a promising direction."}, {"title": "A Outline", "content": "A.1 Systematic Review of GU baselines.\nA.2 Our Approach and ScaleGUN.\nA.3 Our Approach and Traditional Social IM.\nA.4 Algorithm and Complexity Analysis.\nA.5 Dataset Description.\nA.6 Compared Baselines.\nA.7 Hyperparameter settings.\nA.8 Experiment Environment.\nA.9 Link-specific Evaluation.\nA.10 Unlearning Challenges at Different Scales.\nA.11 A-flexible Entity-specific Optimization."}, {"title": "A.1 Systematic Review of GU baselines", "content": "GU remains a burgeoning field with numerous research gaps. To advance its future development and highlight the motivation of our approach, we conduct a systematic review of most existing GU strategies according to our proposed novel taxonomies in Table 6.\nModel Agnostic. It is well-known that quantifying the impact of UE on Non-UE is considerably more challenging in graph-based scenarios than in computer vision due to the intricate interactions among graph entities during model training. This complexity makes UE-corresponding knowledge removal difficult, especially in the complex GNN architectures where gradient flows are well hidden. To ensure certified or exact data removal with a strict theoretical guarantee, some existing GU strategies simplify the default model architecture to a linear-based GNN to satisfy necessary assumptions, designing the corresponding unlearning algorithms through exhaustive derivations. Although they achieve satisfactory theoretical results, the inherent generalization limitations of linear models persist, preventing effective deployment in practical applications.\nRequest Agnostic. The inherent complexity of graph entities leads to various data removal requests in GU, as described in Sec. 2.1. To achieve better unlearning and predictive performance in specific graph-based data removal scenarios, some methods design GU algorithms tailored to particular graph entity deletion requests (e.g. edge unlearning)."}]}