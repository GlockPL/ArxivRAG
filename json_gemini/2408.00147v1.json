{"title": "Formal Ethical Obligations in Reinforcement Learning Agents: Verification and Policy Updates", "authors": ["Colin Shea-Blymyer", "Houssam Abbas"], "abstract": "When designing agents for operation in uncertain environments, designers need tools to automatically reason about what agents ought to do, how that conflicts with what is actually happening, and how a policy might be modified to remove the conflict. These obligations include ethical and social obligations, permissions and prohibitions, which constrain how the agent achieves its mission and executes its policy. We propose a new deontic logic, Expected Act Utilitarian deontic logic, for enabling this reasoning at design time: for specifying and verifying the agent\u2019s strategic obligations, then modifying its policy from a reference policy to meet those obligations. Unlike approaches that work at the reward level, working at the logical level increases the transparency of the trade-offs. We introduce two algorithms: one for model-checking whether an RL agent has the right strategic obligations, and one for modifying a reference decision policy to make it meet obligations expressed in our logic. We illustrate our algorithms on DAC-MDPs which accurately abstract neural decision policies, and on toy gridworld environments.", "sections": [{"title": "1 Introduction: Strategic Obligations in the Face of Uncertainty", "content": "In the rapidly evolving domain of reinforcement learning (RL), agents are trained to autonomously perform tasks within complex and often unpredictable environments. While significant advancements have been made in improving the performance and adaptability of these agents, a crucial dimension remains under-explored: how these agents align with societal and ethical expectations. Given the increasing ubiquity of RL agents in real-world scenarios \u2014 from finance (Hambly, Xu, and Yang 2023) to health care (Yu et al. 2021) and transportation (Sallab et al. 2017) \u2014 it is essential to establish mechanisms that ensure their actions do not merely optimize for the mission\u2019s objective but also adhere to a broader spectrum of ethical norms and societal standards. For brevity, we refer to these ethical and social standards as obligations. Without such normative constraints, the agent\u2019s behavior is likely to be badly surprising, and ultimately unsafe when we think of the reactions of human agents in the environment.\nRunning example. Figure 1 shows a problem where a drone must carry biohazardous material across a city to a hospital. The RL problem is to maximize the drone\u2019s expected utility, and the rewards are assigned such that the hospital rewards the drone with 10 points, but every other space gives the drone a penalty of -1. The penalty is designed to reflect the operating cost of the drone, and the opportunity cost of not being at its goal. The reward reflects the utility of the drone arriving and remaining at the hospital. This reward encodes the drone\u2019s mission. Solving this RL problem gives the policy that leads to the shortest route to the hospital.\nHowever, we also want the drone to avoid a children\u2019s playground, where accidental contamination is especially problematic. This indicates a moral conflict between delivering the material to hospital patients quickly, and avoiding contamination to third parties. Instead of tweaking the reward or designing a secondary reward, we assign the drone an explicit, legible, logical obligation to avoid the playground with a high probability. The requirement is probabilistic to account for the uncertainty in the environment, which could make a non-probabilistic obligation unachievable in every case. This explicit obligation, and the algorithms we develop to handle it, avoid the pitfalls of reward manipulation, especially as the number of moral dilemmas increases, and the reward balancing potentially becomes more and more arbitrary.\nFor this task we need ways to formalize the right obligations, and tools to guarantee that the agent has adopted a policy that maximizes its utility only subject to meeting these obligations. Traditional specification languages, like Linear Temporal Logic (LTL), are inadequate at drawing a distinction between what is the case (how the agent behaves) and what should be the case (how it should behave to meet its obligations). Instead, a deontic logic is needed for this distinction (Hilpinen and McNamara 2013). We adopt the logic of Expected Act Utilitarianism (EAU), first introduced in (Shea-Blymyer and Abbas 2022). EAU allows specification and automatic reasoning about the obligations of utility maximizing systems, including the models that underlie reinforcement learning. But it has the shortcoming of grounding obligations in the agent\u2019s optimal action at a given moment, without regard for future actions, which conflicts with the way a policy is computed as the maximizer of long-term utility. To remedy this, we introduce a strategic modality to EAU which grounds obligations in the agent\u2019s entire policy, not just instantaneous action.\nDesigners also need algorithms that can both verify if a given agent\u2019s policy meets the specified obligations, and guide modifications to the agent\u2019s policy if it falls short. This paper introduces two algorithms for these challenges: the first efficiently model-checks an RL agent\u2019s policy against the strategic obligations formalized in EAU, and the second employs policy gradient ascent to refine the agent\u2019s policy until it aligns with the given obligations.\nThis paper\u2019s contributions are:\n\u2022 An extension of EAU to strategic obligations, which formalize obligations that must be met by the optimal (utility-maximizing) policy over an infinite horizon, not just the current time step (Section 4).\n\u2022 An algorithm for model-checking whether a Markov Decision Process (MDP), which models action under uncertainty, has given strategic obligations formalized in (the extended) EAU. The model-checker can handle MDPs with tens of thousands of states (Section 4).\n\u2022 An algorithm for modifying a utility-maximizing policy to also satisfy a deontic obligation (Section 5).\n\u2022 An extension of the above policy search algorithm for the case where rewards are not known a priori (Section 6).\n\u2022 Experimental evidence for the effectiveness of the model checking and policy search algorithms (Section 6)."}, {"title": "2 Related Work", "content": "There are several works that propose approaches for developing ethical RL agents. In (Abel, MacGlashan, and Littman 2016), the authors argue that RL provides a sufficient foundation to ground ethical decision making, and (Gerdes and Thornton 2015) explores methods for implementing ethics into RL systems. Works such as (Noothigattu et al. 2019) and (Wu and Lin 2017) propose the use of inverse reinforcement learning to teach norms to agents. However (Arnold, Kasenberg, and Scheutz 2017) argues for a hybrid approach, and (Bringsjord, Arkoudas, and Bello 2006) offers a purely logical approach. In this paper, we aim to maintain the precision and formality of a logical approach while extending it to be compatible with common RL techniques.\nOur algorithm to modify an agent\u2019s policy resembles work in policy gradient methods (Sutton, Singh, and McAllester 2000). Policy gradient methods are robust and versatile mechanisms within the realm of on-policy reinforcement learning and have been used in safe reinforcement learning (Gu et al. 2022). These techniques are designed to optimize cumulative reward by directly manipulating policy functions, affording ease of implementation and compatibility with function approximations. The foundational work of REINFORCE (Williams 1992) marked an early instance where policy gradients were computed using Monte Carlo returns. However, our approach relies entirely on the dynamics of the system, and is concerned both with cumulative reward and conformance to an obligation.\nThis reliance on system dynamics makes our problem similar to the problem addressed in (Wolff, Topcu, and Murray 2012). There, the authors produce an MDP that encodes logical constraints (in a non-deontic logic) and can be solved with dynamic programming. However, their solution maximizes the probability of satisfying the logical constraint while ours seeks to maximize expected utility subject to satisfying a logical constraint.\nOur goal of maximizing expected utility subject to the satisfaction of an obligation is analogous to the constrained MDP (CMDP) problem (Altman 2021). The CMDP problem is to find the policy that maximizes expected utility subject to a constraint on expected cost (or secondary reward). We seek to constrain the MDP solution with a non-discounted probability of reaching a state instead of with a discounted cost. The non-discounted nature of the probability of reaching a state makes our problem distinct from the CMDP problem. Further, the model checkers we employ do not consider discounted rewards. Thus, our problem is not solved by solutions to the CMDP problem or by solutions to a classical model checking problem."}, {"title": "3 Technical Preliminaries", "content": "We introduce the Expected Act Utilitarian deontic logic for formalizing obligations of agents acting in stochastic environments. We then draw a correspondence between the semantic frames of the logic, MDPs, and Bellman optimality. And we discuss the policy gradients that are central to our policy search algorithms."}, {"title": "3.1 Expected Act Utilitarianism", "content": "Expected Act Utilitarianism, or EAU (Shea-Blymyer and Abbas 2022), uses PCTL (Baier and Katoen 2008) to describe states of affairs in the world, and adds modalities to speak of action and obligation. Letting a be an agent from a finite set of agents agents, \u2227 and \u00ac be Boolean conjunction and negation, and \u03c6 a PCTL formula, the syntax of EAU is defined by the following grammar,\n$A := \\phi \\mid \\neg A \\mid A \\land A \\mid [acstit: A] \\mid [\\overline{a} \\ cstit : A]$\nIntuitively, PCTL formula \u03c6 describes a state of affairs, such as $P_{\\geq0.9}\\Box g$: the probability of predicate g eventually holding is at least 0.9. See (Baier and Katoen 2008) for details of PCTL. Formula $[acstit : A]$ says that a \u201csees to it that\", or ensures, that A is true, while $[\\overline{a} \\ cstit : A]$ says that a ought to ensure that A is true. For example, the formula\n$[acstit: P_{\\geq0.75}[\\neg \\Diamond playground]]$ specifies the obligation to avoid the playground in at least 75% of possible executions. The formula $[\\overline{a} \\ cstit : P_{<0.01}[\\Diamond checkpoint]]$ gives the obligation to eventually reach the checkpoint in less than 1% of possible executions.\nFigure 2 illustrates all definitions in this section. Formally, EAU formulas are interpreted over a branching time model M. It is made of the following components:\n\u2022 A tree Tree, whose vertices are called moments, m, representing decision points of the agent. Two moments are related by a directed edge m \u2192 m' if m' follows m. This is a partial order on moments, written as m < m'.\n\u2022 the root is moment \u20180\u2019, which precedes all other moments.\n\u2022 AP is a set of atomic propositions, and v : V(Tree) \u2192 2AP is a labeling function that assigns atomic propositions to each moment m.\n\u2022 a history h is a linearly ordered, possibly infinite, set of moments - a branch of the Tree. We write Hm for the set of histories that start at moment m\n\u2022 K: an action available to an agent at a moment m. By identifying K with the subset of histories in Hm that are still possible after taking the action, we can consider that K \u2286 Hm. We write Choicem for the set of actions confronting a at m, and Choicem (h) to refer to those actions that contain history h.\n\u2022 $P_a(m'|m)$: the probability of agent a moving from m to m', assuming that the agent takes some action K that leads to m' (formally, K \u2286 Hm and K \u2229 Hm' \u2260 0).\n\u2022 Value: h \u2192 Value(h): a function that assigns a real value - a utility - to a history.\nThe branching time model can represent the roll-out of an MDP, where moments are state visits, and probabilities are derived from transition probabilities.\nAn EAU formula holds (or not) at an index of evaluation in the model, which is a moment/history pair m/h. The satisfaction relation is written M, m/h |= A, where it is always the case that h \u2208 Hm. The proposition defined by the EAU statement A at moment m is the set of histories, starting at m, in which the statement holds:\n$|A|^M_m := \\{h \\in H_m \\mid M,m/h |= A\\}$  (1)\nWhen it is unambiguous, we drop M from the notation.\nFor convenience, we write the probability with which an agent can execute a particular history h from moment m as $P_a(h|m)$. We can determine this value by taking the product of the probabilities $P_a(m'|m)$ along history h \u2014 assuming that the agent always takes the action K that history h is a part of, and dropping K from the notation.\nThe quality of an action, Q(K), is defined as:\n$Q(K) = \\sum_{m'\\in M'_K} P_a(m'|m) max_{K' \\in Choice_{m'}} Q(K')$ (2)\nwhere Mk is the set of moments that follow m by taking action K."}, {"title": "3.2 Property Gradients for Parametric Markov Chains", "content": "In the second part of this paper we will propose an algorithm for updating an optimal policy so that it satisfies the content of an obligation - that is, the PCTL formula $P_{\\geq p}\\phi$ that shows up in an obligation operator $[\\overline{a} \\ s-stit : P_{\\geq p}\\phi]$. To do this we leverage recent work (Badings et al. 2023), in which gradients with respect to transition probabilities are computed. Specifically, given a parametric Markov Chain (MC) whose transition probabilities are parameterized, and some function f: S \u2192 R of the states which is obtained as the solution of a linear program, (Badings et al. 2023) show how the gradient of f with respect to the parameters can be computed efficiently. In practice, the parameters on the transition probabilities represent a policy. The function f represents either the reward function (when we want to maximize expected utility), or the probability of satisfying the formula \u03c6 (when we want to maximize the likelihood of satisfaction)."}, {"title": "4 Model-Checking Strategic Obligations", "content": "We aim to extend EAU to describe what an agent should do given that it follows its optimal policy for all time. Following (Horty 2001), we call these strategic obligations. This makes EAU more applicable to the RL paradigm, where the optimal policy is typically followed forever. We therefore extend EAU to speak of strategic obligations, then present a model-checking algorithm to handle strategic obligations."}, {"title": "4.1 Expected Strategic Oughts", "content": "While EAU gives us the capability to specify and reason about an agent\u2019s obligations in stochastic environments, the obligations it defines are determined only by the agent\u2019s optimal action in the moment of evaluation. When an agent follows a strategy however, it is natural to inquire about the implications that strategy has on its behavior (beyond its immediate impact). This is especially pertinent when verifying RL systems, as they are expected to follow a learned policy.\nTo reason about an agent\u2019s obligations under a strategy, we introduce a strategic obligation in the manner of Horty\u2019s strategic ought (Horty 2001), though a broader treatment of strategic modalities can be found in (Broersen and Herzig 2015).\nA strategy (or policy, or schedule) \u03c0, is a mapping from moments to actions that determines what action an agent will take when it finds itself in a given state. A strategic obligation, then, is an agent's obligation to the state of affairs brought about by an agent following its optimal strategy. We begin with the strategic stit modality: $[\\overline{a}s\\text{-}stit: A]$, which says that agent a has some strategy that, if followed, ensures that A is the case. To say that an agent has a strategic obligation we write $[\\overline{a} \\ s\\text{-}stit: A]$.\nWe define a strategy \u03c0 as a mapping from moments m in the stit tree to a subset \u03c0(m) of the actions available at m. The set of histories realizable by \u03c0 starting at m is then\n$H_{m,\\pi} = \\{h \\in H_m | h \\in \\pi(m') \\forall m' \\in \\text{Tree s.t. } m' \\geq m\\}$\nThus h \u2208 Hm,\u03c0 iff h is a possible evolution of the system if the agent follows \u03c0.\nDefinition 2 (Strategic stit) In a stit model M, M,m/h |= $[\\overline{a} \\ s\\text{-}stit: A]$ iff there exists a policy \u03c0 such that h\u2208 Hm,\u03c0 and $H_{m,\\pi} \\subseteq |A|^M$.\nTo say that an agent has a strategic obligation we must return to the question of optimality. We say that a strategy is"}, {"title": "4.2 Model Checker of Strategic Oughts", "content": "To make the strategic EAU modalities a practical tool for the verification of RL agents, we developed and implemented a model checking algorithm for strategic obligations. Our model checking algorithm takes as inputs an MDP M, and an obligation $[\\overline{a} \\ s\\text{-}stit: \\phi]$ where \u03c6 is a formula in PCTL. The algorithm then determines if it is the case that M |= $[\\overline{a} \\ s\\text{-}stit: \\phi]$. By definition 3, this model checking problem can be performed in two sequential steps: first, find the optimal strategy \u03c0*, and second, determine if all histories consistent with \u03c0* satisfy \u03c6. Our model checker assumes that the optimal policy \u03c0* is unique. To enforce this assumption we simply employ a tie-breaking rule, forcing the selection of only one optimal policy if multiple are available. This is done for simplicity; if the existence of multiple optimal policies is critical then the algorithm could be modified to return true only if all optimal policies satisfy the obligation. This algorithm is shown in Algorithm 1, and experimental results of its performance are given in Section 6."}, {"title": "5 Policy Update to Satisfy Obligations", "content": "In this part of the paper, we move from model-checking an MDP against a strategic obligation, to policy search with strategic obligation constraints. Namely, the design team is given an obligation $[\\overline{a} \\ s\\text{-}stit : P_{\\geq p}\\phi]$ as part of design requirements. The team comes up with a reward structure for the agent. Ideally, the reward function induces an optimal policy \u03c0* that also satisfies the obligation, but that is not guaranteed since the reward might be balancing several requirements, and reward shaping is notoriously difficult. But the ethical obligations aren non-negotiable. We therefore ask: how can we modify the reward optimal \u03c0* to obtain a policy \u03c0' such that the controlled system satisfies P>p\u03c6 while maintaining a high expected reward?\nWe do this by leveraging gradient computation for parametric MCs (Badings et al. 2023), which was described in Section 3. In our case, the parametric MC is the underlying MDP controlled by \u03c0*. The parameters of the MC are the probabilities of taking a given action in a given state, i.e. \u03c0(a|s). The function f to be optimized is the probability of the parametric MC satisfying \u03c6: this probability is computed as the solution of a linear program (Baier and Katoen 2008). The gradient of f relative to the policy parameters is computed by solving a system E of linear equations (Badings et al. 2023, Sect. 4.1). We call this the probability gradient \u2207\u03c0f. By doing gradient ascent on f relative to the action probabilities, we can increase the probability of satisfaction, at the cost of veering away from the initial, reward-optimal policy \u03c0*. This process does not change the agent's reward function - allowing us to continuously evaluate our modified policy on the reward function given by the design team.\nNote that the probability bound can also be < p with trivial changes to the algorithm. Note also that we can move along the top k largest gradient entries, but potentially decrease search effectiveness. The impacts of this are explored in Section 6.1.\nApproaches to policy updates. Given the probability gradients needed to improve the policy's probability to satisfy \u03c6, there are many heuristics that can be taken to update the policy while maintaining a high reward.\n\u2022 Line Search: One approach is to do a line search over the line connecting \u03c0*, the reward-optimal policy, and \u03c0\u00ba, the policy that satisfies $P_{\\geq p}\\phi$ (and which can be obtained by, e.g., STORM (Hensel et al. 2022)). This line is depicted in Figure 3a. This update simplifies the search space, and is doable using a classical projected gradient.\n\u2022 Average Gradient: We can also use the policy gradient \u2207V for expected utility to guide our policy updates (the utility gradient). By following the average of this gradient and probability gradient \u2207\u03c0f, we can allow a simple trade-"}, {"title": "6 Experiments", "content": "To demonstrate the performance of our algorithms we report on the results of experiments on constrained policy search and model checking. An implementation of our model checking algorithm and our policy update algorithm is included with code to run the following experiments at: https://github.com/sabotagelab/formal-ethical-obligations."}, {"title": "6.1 Illustrative Example", "content": "We first apply our methods to the \u201cwindy-drone\u201d system depicted in Figure 1. The \"windy-drone\u201d system represents a drone delivering a heart for transplant while battling high winds. The effects of the wind are represented by stochastic transitions in the system. The drone's objective (as represented by its reward function) is to reach the hospital as quickly as possible. However, in such high winds, we want to prevent the drone from flying bio-hazardous material over locations such as playgrounds.\nThe algorithm is given the stochastic policy that maximizes the agent's expected reward, and aims to modify the policy to satisfy $[\\overline{a} \\ s\\text{-}stit : P_{\\geq p}[\\neg \\Diamond playground]]$. This obligation represents the requirement for the agent to avoid the playground with a probability greater than p. The dynamics of this obligation are interesting as fulfilling it pushes the agent away from its optimal policy - encouraging it to take a path that is almost twice as long in the best case. In this system, p can not be larger than 0.998, and for the following experiments we set p = 0.75. In practice, this probability threshold should be based on a risk analysis, and should reflect the degree of risk that stakeholders are willing to accept.\nLine Search experiment. Our first experiment was on the performance of policies, on the line in policy space, between the the maximally satisfying policy \u03c0\u00ba and the reward optimal policy \u03c0*. In this case we simply interpolate between the two policies \u2014 taking 100 steps between \u03c0\u00ba and \u03c0*. In Figure 4 we show the expected utility and probability of satisfaction of the policies on the line between \u03c0\u00ba (update 0) and \u03c0* (update 100). Each update i on the x-axis describes the evaluation of a policy \u03c0(i) defined by:\n$(1 \u2212 i/100)\u03c0\u00ba + (i/100)\u03c0^*$\nAs i increases, the probability of satisfaction monotonically decreases. The expected utility, however, initially decreases, but, around update 40, increases again as \u03c0i approaches \u03c0*. This non-monotonicity means that using a simple hill-climbing algorithm to find the maximum expected utility among policies on the line between two given policies will not necessarily return a policy with a global maximum expected utility. We also note that the first update in this experiment to violate the probability threshold p in our obli-"}, {"title": "6.2 Policy Optimization With Exploration", "content": "We also sought to test if our method could perform well when rewards were not known a priori. To do this, we guessed the rewards at each state as an arbitrary value, and updated those values as the agent explored its environment. This gave us access to an approximation of the utility gradient \u2207V which we can use to increase the expected reward of our policy.\nHowever, if we want to ensure safety while exploring, then we need a way to prevent the agent from taking unsafe actions. To this end we implement a PCTL shield (Alshiekh et al. 2018) that prevents the agent from taking any action that would violate the content of a given obligation.\nWith the shield in place we can allow the agent to explore while we update its policy towards satisfaction of a given obligation. Then, once we have found a safe policy, we can follow the approximate utility gradient that is based on our observations so far. This is how we implement our alternating gradient method to allow for exploration.\nTo test if this method is effective across a broader range of environments we randomly generated 24 12-by-12 grid-worlds. Each had 10 impassible cells, 10 \"pits\" that would assign a reward of -10 and ends a run, 10 \u201ccoins\" that would assign a reward of +5, and a goal state that would assign a reward of +10 and ends a run. These cells would be randomly placed on the grid, and the agent would always start at the bottom-left of the grid. The agent is initiated with a random policy, and may explore the environment e-greedily for 100 steps. The agent's memory of reward values updates after every run, and the updated knowledge is used when calculating the current utility gradient. The agent's policy is updated after every run as well using a learning rate of 0.01. The obligation given to the agent is\n$[\\overline{a} \\ s\\text{-}stit: P_{\\geq0.75}[ \\neg coin]]$\nThat is, the agent should ensure that, with probability greater than 0.75, it should never enter a \"coin\u201d state. This obligation was chosen because it interferes with the agent's ability to maximize its utility - forcing \u03c0* and \u03c0\u00ba to be different policies.\nThe performance of this method is shown in Figures 9 and 10. These show that as time goes on, the agent approaches the threshold of probability satisfaction (0.75), at which point the expected utility levels out. We would expect the utility to decrease as satisfaction probability increases since the obligation prohibits the agent from collecting rewards from the \"coin\" states very often."}, {"title": "6.3 Large Model-Checking Experiment: Cartpole", "content": "This experiment illustrates the execution of our model-checker on a large MDP - specifically, on an MDP modeling the cartpole system. The cartpole system consists of a pole attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of either positive or negative magnitude to the cart, with the objective of keeping the pole balanced upright without the cart running off the track. A tabular approximation of such a system can be defined by a DAC-MDP compiled on a set of trajectories collected by a random policy in the original MDP (Shrestha et al. 2020). The size of the DAC-MDP can be adjusted based on the data size, fan-out, and fan-in size of the compiled MDP. Fan-out is controlled by the number of candidate actions for each state, and fan-in by the number of neighbors used to compile the DAC-MDP.\nThe MDP we retrieved had 50,000 states, 15 actions at each state, and 5 transitions per action, for over 3,000,000 transitions. We formulated 20 PCTL formulas \u03c61,..., \u03c620 to check as both strategic stit statements $[\\overline{a} \\ s\\text{-}stit: \\phi_k]$, and as strategic obligations $[\\overline{a} \\ s\\text{-}stit : \\phi_k]$. We labeled each state with the quintile of the angle of the pole in that state"}, {"title": "6.4 Obligation Implication", "content": "To demonstrate our policy optimization procedure on a more complicated form of obligation we show how to find a policy that satisfies an obligation with an implication within it.\nIn the \"windy-drone\u201d system, we might consider allowing the drone to fly over a playground if it visits the checkpoint where its cargo is checked for safety and security. We can model the obligation of the drone to fly over the playground (and thus take a shorter path to the hospital) if it visits the checkpoint as $[\\overline{a} \\ s\\text{-}stit: P_{\\geq \\beta}[\\Diamond playground] \\rightarrow P_{\\geq \\gamma}[ \\Diamond checkpoint]]$. This is equivalent to saying that the drone should avoid the playground, or it should visit the checkpoint. Thus, we can synthesize a policy that avoids the playground, and another policy that visits the checkpoint, and take the better performing of the two as the optimal satisfying policy.\nIn our experiments, we set \u03b2 to 0.75, and \u03b3 to 0.9. As shown in Figure 6, a policy that avoids the playground results in an expected utility just below 120, while we found that visiting the checkpoint results in an expected utility of less than 40. Thus we take the former policy and complete the search.1"}, {"title": "6.5 Contrary-to-Duty Obligation", "content": "As mentioned, a key strength of a deontic logic is the ability to distinguish between what ought to be the case (the obligation) and what actually is the case, and to reason over the divergence between these two.\nA contrary-to-duty (CTD) obligation is an obligation that enters into force in case a primary obligation (the duty) is violated: e.g. if the agent ought to ensure that the medicine cabinet is full (the primary obligation), but it isn't (the violation), then the agent ought to ensure that next an order is placed (the CTD). This has the following general structure:\nIf a ought to ensure A but \u00acA happens, then a ought to ensure B next. (Other structures are possible, notably using conditional obligations)\n$[\\overline{a} \\ s\\text{-}stit: A] \\land \\neg A \\Rightarrow [\\overline{a} \\ s\\text{-}stit: OB]$ (5)\nWe model-check such a formula on the \"windy-drone\" system depicted in Figure 1. By setting a high reward on the checkpoint state, the agent will have the duty $[\\overline{a} \\ s\\text{-}stit : OP_{\\geq0.7}[checkpoint]]$. However, in the case that the agent slips north towards the playground (the violation), it will inherit the new CTD obligation to return to the start state so that it might make a second attempt at the checkpoint: $[\\overline{a} \\ s\\text{-}stit: \\Box P_{\\geq0.6}[start]]$. In the CTD structure we have:\n$\\frac{[\\overline{a} \\ s\\text{-}stit: \\Diamond P_{\\geq0.7}[checkpoint]] \\ \\text{Onorth}}{\\Rightarrow [\\overline{a} \\ s\\text{-}stit: OO P_{\\geq0.6}[start]]}$ (6)\nVerifying that our agent has this obligation allows us to determine how it is expected to behave when it enters a less-than-ideal state.\nWe checked this contrary-to-duty obligations by forcing the agent to move north, and then verifying the truth of the CTD obligation from that state. More generally, we check CTD obligations by looking at the successor states that would indicate a violation of the primary obligation and then verify the truth of the CTD obligations from those states."}, {"title": "7 Conclusions", "content": "Our modalities for strategic agency and strategic obligation give us the expressive power to reason about a large class of reinforcement learning agents. We presented an algorithm for model-checking that an MDP, equipped with a policy, has the right obligations captured in deontic logic. We introduced a new kind of constrained MDP problem where reward maximization is constrained by an obligation. We also provided a way to modify a policy so that it meets such an obligation - without needing to toy with reward functions.\nWe hope that these algorithms will aid system designers in specifying normative constraints, checking their systems against those constraints, and refining their systems to meet their constraints.\nIn future work we're interested in extending our policy update algorithm beyond safety properties to support a larger class of PCTL. We're also interested in managing the trade-offs between following the utility gradient and the probability gradient, perhaps by using a linear weighting function, or different learning rates for each. Further, we'd like to try constraining the policy search with a maximum divergence from the initial policy."}]}