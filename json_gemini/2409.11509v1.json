{"title": "FEDNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction", "authors": ["Ziwei Li", "Xiaoqi Wang", "Hong-You Chen", "Han-Wei Shen", "Wei-Lun Chao"], "abstract": "Federated learning (FL) has rapidly evolved as a promising paradigm that enables collaborative model training across distributed participants without exchanging their local data. Despite its broad applications in fields such as computer vision, graph learning, and natural language processing, the development of a data projection model that can be effectively used to visualize data in the context of FL is crucial yet remains heavily under-explored. Neighbor embedding (NE) is an essential technique for visualizing complex high-dimensional data, but collaboratively learning a joint NE model is difficult. The key challenge lies in the objective function, as effective visualization algorithms like NE require computing loss functions among pairs of data. In this paper, we introduce FEDNE, a novel approach that integrates the FEDAVG framework with the contrastive NE technique, without any requirements of shareable data. To address the lack of inter-client repulsion which is crucial for the alignment in the global embedding space, we develop a surrogate loss function that each client learns and shares with each other. Additionally, we propose a data-mixing strategy to augment the local data, aiming to relax the problems of invisible neighbors and false neighbors constructed by the local kNN graphs. We conduct comprehensive experiments on both synthetic and real-world datasets. The results demonstrate that our FEDNE can effectively preserve the neighborhood data structures and enhance the alignment in the global embedding space compared to several baseline methods.", "sections": [{"title": "Introduction", "content": "Federated Learning (FL) has emerged as a highly effective decentralized learning framework in which multiple participants collaborate to learn a shared model without sharing the data. In recent years, FL has been extensively studied and applied across various domains, including image and text classifications [10, 31, 23, 9], computer vision tasks [3, 41, 50], and graph learning problems [42, 5, 16]. Despite the growing interest in FL, the area of dimensionality reduction within this framework has received limited investigation. However, visualizing and interpreting data from distributed sources is important, as real-world applications often generate large volumes of complex datasets that are stored locally by each participant. For example, different hospitals collect high-dimensional electronic health records (EHRs) [1] and securely store this patient data within their local systems. As each hospital might only collect limited data or focus on particular diseases, conducting data visualization on a combined dataset can substantially improve disease diagnosis and provide deeper insights. However, sharing sensitive patient information is restricted due to privacy protection. Thus, developing a shared dimensionality reduction model in the FL setting is crucial for facilitating collaborative analysis while maintaining data on local sites.\nDimensionality reduction (DR) refers to constructing a low-dimensional representation from the input data while preserving the essential data structures and patterns. Neighbor embedding (NE) [6, 49], a family of DR techniques, is widely utilized to visualize complex high-dimensional data due to its"}, {"title": "Related Work", "content": "Federated learning. FL aims to train a shared model among multiple participants while ensuring the privacy of each local dataset. FEDAVG [34] is the foundational algorithm that established the general framework for FL. Subsequent algorithms have been proposed to further improve FEDAVG in terms of efficiency and accuracy. Some of the work focuses on developing advanced aggregation techniques from various perspectives such as distillation [46, 39], model ensemble [30, 40], and weight matching [45, 52] to better incorporate knowledge learned by local models. Moreover, to minimize the deviation of local models from the global model, many works focus on enhancing the local training procedures [21, 2, 51, 29]. FEDXL [15] was proposed as a novel FL problem framework for optimizing a family of risk optimization problems via an active-passive decomposition strategy. Even though FEDXL deals with the loss decomposition for pairwise relations, our main focus and application are very different.\nNeighbor embedding. Neighbor embedding (NE) is a family of non-linear dimensionality reduction techniques that rely on k-nearest neighbor (kNN) graphs to construct the neighboring relationships within the dataset [6]. The key of NE methods lies in leveraging the interplay between attractive forces which bring neighboring data points closer and repulsive forces which push uniformly sampled non-neighboring data pairs further apart. t-SNE [44] is a well-known NE algorithm. It first converts the data similarities to joint probabilities and then minimizes the Kullback-Leibler divergence between the joint probabilities of data pairs in the high-dimensional space and low-dimensional embedding space. Compared to t-SNE, UMAP [33] is better at preserving global data structure and more efficient in handling large datasets. A later study has analyzed the effective loss of UMAP[13] and demonstrated that the negative sampling strategy indeed largely reduces the repulsion shown in the original UMAP paper, which explains the reasons for the success of UMAP. Our federated NE work is built upon a recent work that has theoretically connected NE methods with contrastive loss [12, 19]. Their proposed framework unifies t-SNE and UMAP as a spectrum of contrastive neighbor embedding methods.\nDecentralized dimensionality reduction. As nowadays datasets are often distributively stored, jointly analyzing the data from multiple sources has become increasingly important especially when the data contains sensitive information. SMAP [47] is a secure multi-party t-SNE. However, as this framework requires data encryption, decryption, and calculations on the encrypted data, SMAP is very time-consuming and thereby it can be impractical to run in real-world applications. dSNE was proposed for visualizing the distributed neuroimaging data [37]. It assumes that a public neuroimaging dataset is available to share with all participants. The shareable data points act as anchors for aligning the local embeddings. To improve the privacy and efficiency of dSNE, Faster AdaCliP dSNE (F-dSNE) [38] was proposed with differential privacy to provide formal guarantees. While their goal is not to collaboratively learn a global predictive DR model and thus does not follow the formal FL protocols [22, 8, 34] defined in the literature. Both methods require a publicly available dataset to serve as reference gradients communicating across central and local sites. However, since a public dataset may not be available in most real-world scenarios, our FEDNE is designed without any requirements for the shareable data."}, {"title": "FL Framework for Neighbor Embedding", "content": "In this section, we first provide background information on neighbor embedding techniques. We then formulate the problem within the context of FL and outline the unique challenges."}, {"title": "Contrastive Neighbor Embedding", "content": "The goal of general NE techniques is to construct the low-dimensional embedding vectors $Z_1,\\ldots\\ldots, Z_n \\in R^d$ for input data points $x_1,\\ldots,X_N \\in R^D$ that preserve pairwise affinities of data points in the high-dimensional space. The neighborhood relationships are defined via building sparse k-nearest-neighbor (kNN) graphs over the entire dataset with a fixed k value. Contrastive NE [11] is a unified framework that establishes a clear mathematical relationship among a range of NE techniques including t-SNE [44], UMAP [13], and NCVis [4], via contrastive loss. For parametric NE, an encoder network $f_\\theta$ is trained to map an input data point x to a low-dimensional representation z, i.e., $z = f_\\theta(x)$.\nIn general, the contrastive NE algorithms first build kNN graphs to determine the set of neighbors $p_i$ for each data point $x_i$ in the high-dimensional space. A numerically stable Cauchy kernel is used for converting a pairwise low-dimensional Euclidean distance to a similarity measure: $\\phi(z_i, z_j) = \\frac{1}{1+||z_i-z_j ||}$. Then, the contrastive NE [11] loss is optimized via the negative sampling strategy:\n$L(\\theta) = E_{i,j\\sim p_i} [log(\\phi(f_\\theta(x_i), f_\\theta(x_j)))] \u2013 bE_{i,j}[log(1 \u2013 \\phi(f_\\theta(x_i), f_\\theta(x_j)))]$,\nwhere $p_i$ denotes the set of neighboring data points of $x_i$."}, {"title": "Problem Formulation", "content": "In general federated learning with one central server and M clients, each client holds its own training data $D_m = {x_i}^\\left|D_m\\right|$, and we denote the collective global data as $D_{glob}$. The clients' datasets are disjoint which cannot be shared across different local sites, i.e., $D_m \\cap D_{m'} = \\O$ for $\\forall m, m' \\in [M]$,"}, {"title": "Challenges of Federated Neighbor Embedding", "content": "However, besides the challenges posed by the non-IID data, simply decomposing the problem into Equation 3 indeed overlooks the pairwise data relationships existing across different clients. The major difference between the existing FL studies, e.g., image classification, and Federated neighbor embedding is that the objective function of the former problems is instance-based, where their empirical risk is the sum of the risk from each data point:\n$L_m(\\theta) = \\frac{1}{\\left|D_m\\right|} \\sum_{\\left(x_i, y_i\\right) \\in D_m} l(x_i, y_i; \\theta)$.\nAs a result, the FL objective in Equation 2, i.e., $\\sum_{m=1}^{M} L_m (\\theta)$, is exactly the one as if all the clients' data were gathered at the server.\nIn the context of Federated neighbor embedding, Equation 3 only considers $x_j$ to come from the same client as $x_i$. Thus, simply adopting the vanilla FEDAVG framework can result in losing all the attractive and repulsive terms that should be computed between different clients.\nTherefore, we redefine the FL objective of the contrastive neighbor embedding problem to be\n$L(\\theta) = \\sum_{m=1}^M E_{(i,j)\\sim D_m} [l(x_i, x_j; \\theta)] + \\sum_{m=1}^M\\sum_{m'=1}^{M}\\sum_{m'\\neq m}E_{(i,j)\\sim(D_m, D_{m'})} [l(X_i, x_j; \\theta)]$,\nwhere the pairwise empirical risk $l(x_i, x_j; \\theta)$ can be further defined as\n$l(x_i, x_j; \\theta) = 1[x_j \\in p_i]log(\\phi(f_\\theta(x_i), f_\\theta(x_j))) \u2013 blog(1 \u2013 \\phi(f_\\theta(x_i), f_\\theta(x_j)))$.\nNonetheless, since the inter-client pairwise distances are unknown, Equation 7 cannot be solved directly when $x_i$ and $x_j$ come from different clients. Specifically, this decentralized setting brings two technical challenges: (1) Biased repulsion loss: Negative sampling requires selecting non-neighbor pairs uniformly across the entire data space. Under the FL setting, it is difficult for a client to sample from outside of its local dataset. (2) Incorrect attraction loss: Each client only has access to its local data points. This partitioning can result in an incomplete kNN graph, leading to incorrect $p_i$, as some true neighbors of a data point might reside on other clients."}, {"title": "Federated Neighbor Embedding: FEDNE", "content": "To address the aforementioned challenges in applying the FL framework to the neighbor embedding problem, we develop a learnable surrogate loss function\u00b9 trained by each client and an intra-client data augmentation technique to tackle the problems in repulsion and attraction terms separately. The two components can be smoothly integrated into the traditional FEDAVG pipeline shown in Figure 1."}, {"title": "Surrogate Loss Function", "content": "The repulsive force plays an important role in ensuring separation among dissimilar data points, contributing to the global data arrangement in the embedding space. In the centralized scenario, each data point can uniformly sample its non-neighbors across the entire dataset. In contrast, in the federated setting, as each client can only access its local data, the dissimilar points residing in other clients are invisible, and all the repulsion will be estimated within its own data space. In particular, under severe non-IID situations, where the data distributions across different clients vary significantly [54], the non-neighboring samples selected to repel are very likely to come from the same clusters in high-dimensional space. As showcased in Figure 2 (a), without explicitly taking care of the repulsion between dissimilar data pairs across clients, those points may still overlap with each other in the embedding space.\nAt a high level, we seek to learn a function $f^m_{rep}: R^d \\to R$ for each client m such that $f^m_{rep} \\approx blog(1 - \\phi(f_\\theta(x_i), f_\\theta(x_j)))$ to estimate the repulsion, where $x_i$ and $x_j$ come from different clients. This surrogate model, once shared, enables other clients to input their local data points and obtain a pre-estimated repulsive loss to data points from the originating client.\nSurrogate model training. We learn $f^m_{rep}$ via supervised learning at each round of FEDAVG. To do so, we generate some low-dimensional query points as inputs and pre-compute their corresponding repulsive loss to client m's data points based on the current projection model. We choose to sample a set of points $Z_q$ within 2D space for the following two reasons. Firstly, as non-neighboring points are uniformly selected across the data space, query points are not required to maintain any specific affinity with $D_m$. Second, because the high-dimensional space is often much sparser than 2D space, generating sufficient high-dimensional samples to comprehensively represent the data distributions of all other clients is impractical. Therefore, each client employs a grid sampling strategy at every round, using a predefined step size and extensive ranges along the two dimensions. This procedure is informed by client m's current embedding positions, ensuring a more manageable and representative sampling process within the embedding space.\nIn sum, given the sampled inputs $Z_q = {z_{q_1}, z_{q_2},..., z_{q_{N_q}} }$, we prepare the training targets by computing the repulsive loss between each $z_{q_i}$ and b random data points in $D_m$, i.e., $l = \\sum_i log(1-(\\phi(z_{q_i}, z_m)))$. Then, the dataset for supervised training the surrogate repulsion function $f^m_{rep}$ is constructed as $D_q = {(z_{q_i}, l_i)}^{|D_q|}_i$."}, {"title": "Neighboring Data Augmentation", "content": "To mitigate the limitations of biased local kNN graphs and ensure better neighborhood representation, we propose an intra-client data mixing strategy. This approach generates additional neighboring data points within each client, thereby enhancing the local data diversity. To be specific, locally constructed kNN graphs can be biased by the client's local data distribution. As the associated data pairs for computing the attractive loss are distributed across multiple clients, the local neighborhood within each client can be very sparse. Consequently, data points within a client may miss some of their true neighbors (i.e., invisible neighbors) considered in the global space. Moreover, when the local data is extremely imbalanced compared to the global view, constructing the kNN graph with a fixed k value may result in incorrect neighbor connections between very distant data points (i.e., false neighbors). As demonstrated in Figure 2 (b), since data is partitioned across different clients, with a fixed k value, each local kNN graph can be even more sparse and erroneously connect very distinct data points.\nIntra-client data mixing. To address these problems, we employ a straightforward yet effective strategy, intra-client data mixing, to locally generate some data within a client by interpolating between data points and their neighbors. Our method shares a similar spirit to the mixup augmentation [53, 36]. In detail, given a data point $x_i$, and the set of its k nearest neighbor $NN_k(x_i)$, a new data point is generated by linearly interpolating $x_i$ and a random sample in $NN_k(x_i)$ denoted as $x_j$:\n$\\hat{x} = \\lambda x_i + (1 \u2013 \\lambda)x_j$,\nwhere $\\lambda$ is the weight sampled from the Beta distribution i.e., $\\lambda \\sim Beta(\\alpha)$."}, {"title": "Overall Framework", "content": "Once each client has received the surrogate loss functions of all the other participants (i.e., step 4 in Figure 1), it proceeds to its local training. By combining the original NE loss with the surrogate loss function on the augmented local training data, the new objective for client m can be formulated as:\n$L_m (D_m; \\theta) = - \\sum_{i,j\\sim p'} log(\\phi(f_\\theta(x_i), f_\\theta(x_j))) \u2013 \\frac{1}{\\left|D_m\\right|} \\sum_{i,j} log(1 \u2013 (f_\\theta(x_i), f_\\theta(x_j))) + \\sum_{m'\\neq m}D_{m'} [f^w_{m'}(f_\\theta(x_i))]$,\nwhere $\\hat{x^i}, \\hat{x^j} \\in D_m$ i.e., the augmented training set. For simplicity, we use $p'$ to represent the neighbor set constructed within $D_m$. $f^w_{rep}$ is the surrogate model received from another client $m'$.\nComputation. At first glance, FEDNE may seem to introduce heavy computational overhead compared to the original FEDAVG framework, as it requires additional surrogate model training at every round. Moreover, a client needs to use multiple received surrogate models to do inference using its own local data. However, we want to emphasize that the surrogate model contains only one hidden layer and only takes 2D data as inputs. Therefore, training and using them is manageable. We conducted experiments using MNIST with 20 clients on a server with 4 NVIDIA GeForce RTX 2080 Ti GPUs. Compared to FEDAVG, our FEDNE takes 35% more GPU time to complete one round of learning. For future speed-up, we may consider applying strategies such as clustered FL and we leave this for future work."}, {"title": "Implementation details", "content": "After building the training dataset, each client trains its surrogate model $f^m_w$ using an MLP with one hidden layer to learn the mapping between the input embedding vectors and their corresponding repulsive loss measured within the client data by minimizing the mean squared error (MSE). The training objective is formulated as follows:\n$w^* = arg\\underset{w}{min} \\frac{1}{|D_q|} \\sum_{i=1}^{D_q} (l^{rep}_i - f^w_m(q_i))^2$."}, {"title": "Neighboring Data Augmentation", "content": "To mitigate the limitations of biased local kNN graphs and ensure better neighborhood representation, we propose an intra-client data mixing strategy. This approach generates additional neighboring data points within each client, thereby enhancing the local data diversity. To be specific, locally constructed kNN graphs can be biased by the client's local data distribution. As the associated data pairs for computing the attractive loss are distributed across multiple clients, the local neighborhood within each client can be very sparse. Consequently, data points within a client may miss some of their true neighbors (i.e., invisible neighbors) considered in the global space. Moreover, when the local data is extremely imbalanced compared to the global view, constructing the kNN graph with a fixed k value may result in incorrect neighbor connections between very distant data points (i.e., false neighbors). As demonstrated in Figure 2 (b), since data is partitioned across different clients, with a fixed k value, each local kNN graph can be even more sparse and erroneously connect very distinct data points.\nIntra-client data mixing. To address these problems, we employ a straightforward yet effective strategy, intra-client data mixing, to locally generate some data within a client by interpolating between data points and their neighbors. Our method shares a similar spirit to the mixup augmentation [53, 36]. In detail, given a data point $x_i$, and the set of its k nearest neighbor $NN_k(x_i)$, a new data point is generated by linearly interpolating $x_i$ and a random sample in $NN_k(x_i)$ denoted as $x_j$:\n$\\hat{x} = \\lambda x_i + (1 \u2013 \\lambda)x_j$,\nwhere $\\lambda$ is the weight sampled from the Beta distribution i.e., $\\lambda \\sim Beta(\\alpha)$."}, {"title": "Overall Framework", "content": "Once each client has received the surrogate loss functions of all the other participants (i.e., step 4 in Figure 1), it proceeds to its local training. By combining the original NE loss with the surrogate loss function on the augmented local training data, the new objective for client m can be formulated as:\n$L_m (D_m; \\theta) = - \\sum_{i,j\\sim p'} log(\\phi(f_\\theta(x_i), f_\\theta(x_j))) \u2013 \\frac{1}{|D_m|} \\sum_{i,j} log(1 \u2013 (f_\\theta(x_i), f_\\theta(x_j))) + \\sum_{m'\\neq m} [f^w_{m'}(f_\\theta(x_i))]$,\nwhere $x^i, x^j \\in D_m$ i.e., the augmented training set. For simplicity, we use $p'$ to represent the neighbor set constructed within $D_m$. $f^w_{m'}$ is the surrogate model received from another client $m'$.\nComputation. At first glance, FEDNE may seem to introduce heavy computational overhead compared to the original FEDAVG framework, as it requires additional surrogate model training at every round. Moreover, a client needs to use multiple received surrogate models to do inference using its own local data. However, we want to emphasize that the surrogate model contains only one hidden layer and only takes 2D data as inputs. Therefore, training and using them is manageable. We conducted experiments using MNIST with 20 clients on a server with 4 NVIDIA GeForce RTX 2080 Ti GPUs. Compared to FEDAVG, our FEDNE takes 35% more GPU time to complete one round of learning. For future speed-up, we may consider applying strategies such as clustered FL and we leave this for future work."}, {"title": "Experimental Settings", "content": "Datasets. We conduct experimental studies on four benchmark datasets that have been widely used in the field of dimensionality reduction [35, 55]: MNIST [26], Fashion-MNIST [48], mouse retina single-cell transcriptomes [32], and CIFAR-10 [25]. Their statistical information and general settings are summarized in Table 1. For CIFAR-10, since the Euclidean distances in the pixel space of a natural image dataset are not meaningful to preserve [7], we adopted ImageNet-pretrained ResNet-34 [17] to extract a set of 512D feature vectors as input data. The resulting vectors still retain category-relevant structures that can be suitable for the Euclidean metric.\nNon-IID data partition. We consider two partitioning strategies to simulate the heterogeneous client distributions: (1) Dirichlet: For a class c, we sample $q_e$ from Dir(a) and assign data samples of that class e to a client m proportionally to $q_e[m]$. The hyperparameter a controls the imbalance level of the data partition where a smaller a indicates a more severe non-IID condition [27, 18]. (2) Shards: each client holds data from C classes, and all samples from the same class are randomly and equally divided among all clients [27].\nEvaluation metrics. We assess the quality of data embeddings by analyzing the input high-dimensional data points and their corresponding 2D positions [14]. First, to evaluate the preservation of neighborhood structures, we compute trustworthiness and continuity scores. Trustworthiness quantifies the quality of a low-dimensional embedding by checking whether neighbors in the high-dimensional space remain the same as the ones in the embedded low-dimensional space. Conversely, continuity verifies whether the neighbors in the embedded space correspond to neighbors in the original input space. We use kNN classification accuracy to measure the effectiveness in preserving the nearest neighbors in the embedded space, where higher scores indicate better class discrimination. We fix k = 7 for all the neighborhood metrics. Additionally, we employ steadiness and cohesiveness metrics to evaluate the reliability of the global inter-cluster structures [20]. Steadiness assesses the presence of false groups, while cohesiveness checks for any missing groups.\nImplementation and training details. We employ a fully connected neural network with three hidden layers for MNIST, Fashion-MNIST, and CIFAR-10 datasets and a network with two hidden layers for the RNA-Seq dataset. In all experiments, we use Adam optimizer [24] with learning rate annealing and a batch size of 512 where the batch size refers to the number of edges in the constructed kNN graphs. Furthermore, we assume full participation during the federated learning and each client performs one epoch of local training (E = 1). In addition, we set a = 0.2 to perform the intra-client data augmentation in our study.\nBaselines. We consider four approaches to compare with our FEDNE. (1) LocalNE: each client trains the NE model using only its local data without any communication. Two baseline methods: (2) FedAvg+NE and (3) FedProx+NE are implemented by applying the widely used FL frameworks [34, 28] to NE model training. (4) GlobalNE: the NE model trained using aggregated global data, serving as the upper bound for performance comparison. Moreover, we want to emphasize that we"}, {"title": "Results", "content": "We conduct comprehensive experiments under various non-IID conditions and then evaluate on the global test data of each four datasets. For the highly imbalanced scRNA-Seq dataset, we only consider the Dirichlet partitions. Overall, our FEDNE outperforms the LocalNE by 2.62%, 6.12%, 14.31% 12.69%, and 7.31% on average under the five metrics (i.e., conti., trust., kNN acc., stead., and cohes.) respectively.\nImproved preservation of true neighbors. Both FEDNE and the baseline approaches achieved relatively high continuity scores, indicating that the models can easily learn how to pull the data points that are similar in the high-dimensional space closer in the 2D space. However, the lower trustworthiness scores observed with the two baselines, FedAvg+NE and FedProx+NE, imply that without properly addressing incorrect neighborhood connections and separation of data points across different clients, the resulting embeddings may contain false neighbors. Consequently, points that are positioned closely in the 2D space might not be neighbors in the original high-dimensional space.\nEnhanced class discrimination in the embedding space. Our method has significantly improved the kNN classification accuracy compared to the baseline results. This improvement highlights the limitations of locally constructed k-NN graphs, which may incorrectly pull distant data pairs closer in the embedding space. In particular, if two data points from different classes are mistakenly treated as neighbors, class separation will be largely reduced even when inter-client repulsion is considered. Our intra-client data mixing method is specifically designed to relax this problem, and when combined with our surrogate loss function, it ensures an enhanced class separation.\nBetter preservation of global inter-cluster structures. Furthermore, we observe large improvements in preserving the clustering structures according to measures of steadiness and cohesiveness. Specifically, higher steadiness achieved by FEDNE indicates that the clusters identified in the projected space better align with the true clusters in the original high-dimensional space. The higher cohesiveness scores imply that the clusters in the high-dimensional space in general can be retained in the projected space, i.e., not splitting into multiple parts.\nAblation Study\nTo verify the effect of our design choices, we conduct ablation studies on removing one of the proposed technical components from the FEDNE pipeline in Figure 1. First, we remove the data augmentation by intra-client data mixing technique and only keep the surrogate repulsion model. We then remove the surrogate model and only augment the local data using the intra-client data mixing approach. With any of the components removed, our FEDNE can still outperform the baseline FedAvg+NE."}, {"title": "Discussion", "content": "Privacy Preserving. As introduced in section 4, FEDNE incorporates the proposed surrogate models into the traditional FEDAVG framework where the surrogate models only take the low-dimensional randomly sampled data as inputs. After training, each surrogate model contains much compressed information about the corresponding client. Thus, we consider the privacy concerns to be alleviated as one cannot directly reconstruct the raw high-dimensional client data. To further enhance privacy protection, our framework can be integrated with various privacy-preserving techniques at different stages. For example, Gaussian mechanisms (GM) can be applied to the parameters of the surrogate model before it is sent to the server.\nScalability and Computational Complexity. To our knowledge, the field of dimensionality reduction (DR) focuses on relatively smaller-scale datasets, compared to the studies of classification problems. This is because computational complexity is never a trivial problem even for many outstanding DR techniques, particularly for non-linear methods such as Isomap and t-SNE which have non-convex cost functions [44]. Our experiments indeed have included the most widely-used benchmarks in the DR literature. Moreover, compared to prior work [37, 38], we have considered more participants and larger scales of data.\nConclusion\nIn this paper, we develop a federated neighbor embedding technique built upon the FEDAVG framework, which allows collaboratively training a data projection model without any data sharing. To tackle the unique challenges introduced by the pairwise training objective in the NE problem, we propose to learn a surrogate model within each client to compensate for the missing repulsive forces. Moreover, we conduct local data augmentation via an intra-client data mixing technique to address the incorrect neighborhood connection within a client. The experiments have demonstrated its effectiveness in preserving the neighborhood data structures and clustering structures."}, {"title": "Experimental Details", "content": "All experiments were conducted on servers with 4 NVIDIA GeForce RTX 2080 Ti GPUs, and the models were developed using Python 3.9."}, {"title": "Local training", "content": "In the experiments of MNIST and Fashion-MNIST datasets, we use Adam optimizer with a learning rate of 0.001 and a batch size of 512 (i.e., the number of edges in the kNN graphs not the number of data instances). The learning rate was decreased by 0.1 at 30% and 60% of the total rounds. For the experiments with the single-cell RNA-Seq and CIFAR-10 dataset, the learning rate was initially set to 1 \u00d7 10-4. For negative sampling, we fix the number of non-neighboring data points sampled per edge to be 5 (b = 5).\nMoreover, the surrogate loss function is introduced into the local training at 30% of the total rounds, primarily due to the following concerns: during each round of local training, the surrogate loss function in use was constructed using the global NE model from the previous round. Thus, to avoid dramatic deviations between the surrogate function in use and the NE model newly updated by local training, the surrogate function is integrated after the model has already gained a foundational understanding of the data structures and thereby the optimization process tends to be more stable."}, {"title": "Surrogate loss function training", "content": "The surrogate loss function of each client is fine-tuned at every round from the previous model but the training set (i.e., Dq = {(zqi, lrep)}|Dq| in the main paper) needs to be rebuilt according to the current global NE model. The surrogate function is optimized by minimizing the mean squared error (MSE) using the Adam optimizer with a learning rate of 0.001."}, {"title": "Design Choices and Hyperparameter Selections", "content": "Choice of k in building local kNN graphs\nIn the main experiments, a fixed number of neighbors (k=7) is used for building clients' kNN graphs. We conduct further experiments using different k values under the setting of Dirichlet(0.1) on the MNIST dataset with 20 clients. We found that within a certain range (i.e., 7 to 30), the performance of FedNE is relatively stable. When k is too large (e.g., k=50), the performance drops but our FedNE still outperforms the baseline methods, FedAvg+NE. Overall, this trend aligns with the general understanding of dimensionality reduction methods."}, {"title": "Weights in intra-client data mixing strategy", "content": "We fixed the \u03b1 to be 0.2 in the main experiments to perform intra-client data augmentation. Here, we alter the weight used in the intra-client data mixing strategy. We adjust the mixing weight \u03bb by changing the \u03b1 value, where \u03bb \u223c Beta(\u03b1)."}, {"title": "Choice of step size for grid sampling", "content": "The step size is used to control the resolution of grid sampling for training the surrogate models. In the main paper, the default step size is set to 0.3, and here, we experiment with using different step sizes."}, {"title": "Data source for training surrogate models", "content": "To construct the training set of the surrogate loss function in a more comprehensive and manageable way, each client employs a grid-sampling strategy in the 2D space."}, {"title": "Frequency of surrogate function update", "content": "In all experiments, the surrogate loss functions are retrained at every round. While frequent retraining introduces computational burdens for each client, using outdated surrogate loss functions can bias the optimization process of the repulsive loss. Thus, we conduct experiments on the MNIST dataset to showcase the impacts of the frequency of surrogate function updates."}, {"title": "Time to integrate the surrogate loss function", "content": "In the main experiments, the surrogate loss function is integrated into local training after 30% of the total rounds have been finished. Here, we conduct further experiments on introducing the surrogate function at different time periods to confirm our decision."}, {"title": "Partial Client Participation", "content": "We simulate the straggler problem by randomly sampling 10% of the clients involved in each round of communication under the setting of Dirichlet(0.1) on the MNIST dataset with 100 clients."}, {"title": "Evaluation Results on the Shards Setting", "content": "In addition to the table in the main paper, we also report the results of the Shards setting on MNIST, Fashion-MNIST, and CIFAR-10 datasets."}, {"title": "Visualization Results by FEDNE", "content": "We demonstrate the comprehensive visualization results from the centralized setting i.e., GlobalNE, our FEDNE and FedAvg+NE, and FedProx+NE on MNIST, Fashion-MNIST, RNA-Seq, and CIFAR-10 global test data."}]}