{"title": "EfficientEQA: An Efficient Approach for Open Vocabulary Embodied Question Answering", "authors": ["Kai Cheng", "Zhengyuan Li", "Xingpeng Sun", "Byung-Cheol Min", "Amrit Singh Bedi", "Aniket Bera"], "abstract": "Embodied Question Answering (EQA) is an essential yet challenging task for robotic home assistants. Recent studies have shown that large vision-language models (VLMs) can be effectively utilized for EQA, but existing works either focus on video-based question answering without embodied exploration or rely on closed-form choice sets. In real-world scenarios, a robotic agent must efficiently explore and accurately answer questions in open-vocabulary settings. To address these challenges, we propose a novel framework called EfficientEQA for open-vocabulary EQA, which enables efficient exploration and accurate answering. In EfficientEQA, the robot actively explores unknown environments using Semantic-Value-Weighted Frontier Exploration, a strategy that prioritizes exploration based on semantic importance provided by calibrated confidence from black-box VLMs to quickly gather relevant information. To generate accurate answers, we employ Retrieval-Augmented Generation (RAG), which utilizes BLIP to retrieve useful images from accumulated observations and VLM reasoning to produce responses without relying on predefined answer choices. Additionally, we detect observations that are highly relevant to the question as outliers, allowing the robot to determine when it has sufficient information to stop exploring and provide an answer. Experimental results demonstrate the effectiveness of our approach, showing an improvement in answering accuracy by over 15% and efficiency, measured in running steps, by over 20% compared to state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "When entering an unknown scene, humans can easily search around the environment and answer any open questions related to the scene, i.e., Embodied Question Answering (EQA) [4], [7], [36], [5], [33]. However, such capability of open vocabulary EQA presents significant challenges for intelligent robots, due to the exceptionally high complexity of the unknown scene and the infinite nature of answer space. Even with the emergence of large vision language models (VLMs) with powerful reasoning ability, existing works fixate on either video-based question answering without embodied exploration [22], [21], [3] or embodied question answering with closed form sets [28], [4], [7]. This raises a critical question to ask: How can a robot be as intelligent as humans to answer such embodied questions with exploration?\nIn order to expand further, imagine asking a household robot (cf. Fig. 1) the question Q: \"How many cushions are there on the red sofa in the living room?\". What should the robot do in order to provide the correct answer? Without any prior knowledge of the room, it must identify the location of the living room and what the cushions and the red sofa mean. This task necessitates the robot's ability to explore, determine its current location, decide where to go next and engage in question-answering and reasoning based on visual cues. We define this task of open vocabulary embodied question answering during exploration as Open Explore EQA (cf. Fig. 1).\nThere have been tremendous research endeavors studying large vision language models' capacity for visual question answering and reasoning, which have shown great prospects for application to embodied tasks [9], [16], [37], [10], [6]. Ideally, since VLMs have already achieved a capability for reasoning that approaches human-level performance, they could direct a robot to navigate a unit and answer questions effectively. However, there are 3 key challenges in this setting:\nC1: How to search through the open vocabulary answer space without choice sets?\nC2: How to extract calibrated confidence in directions to explore from VLMs to build environment maps?\nC3: How to determine when to stop exploring and whether the answer is accurate?\nChallenge C2 is exaggerated by our assumption that we only access black-box VLMs. We observe that most state- of-the-art VLMs [1], [31] are usually accessed through the application user interface, where parameters are not available. While some open-source VLMs [14], [8], [11] offer comparable performance, their deployment in resource- constrained environments, such as household robots, is limited by the significant computational requirements they impose.\nIn this paper, we introduce a novel, efficient open vocabulary embodied question-answering framework EfficientEQA to solve the problem. Specifically, we divide a typical embodied question-answering process into three stages: to Explore, Answer, and Stop, as shown in Fig. 1. In the exploring stage, we use Semantic-Value-Weighted Frontier Exploration to build the semantic map and Verbalized Confidence to extract calibrated confidence from black-box VLMs. In the answering stage, we use Retrieval-Augmented Generation (RAG) to generate the predicted answer based on the observation episode history. For the stopping stage, we use outlier detection to select the answer-deterministic frame as the stop sign for the entire exploration process.\nWe conduct experiments using the Habitat-Sim physics simulator [25] equipped with large-scale Habitat-Matterport 3D Research Dataset (HM3D) [27] and HM-EQA [28], which provides photo-realistic, diverse indoor 3D scans and related multiple-choice questions. We also perform a real-world demonstration in home/office-like environments using the Husky and UR5e mobile manipulators. We use the large language models (LLM)-based correctness metric (LLM-Match) to measure the answering accuracy and average running steps to measure the efficiency of our method. Experimental results yield convincing evidence of the effectiveness and efficiency of our proposed framework. We summarize our contributions as follows:\n\u2022 We introduce and formulate the novel task of Open Explore EQA (open vocabulary embodied question answering during exploration) with motivation from real- world scenarios.\n\u2022 We present a novel framework EfficientEQA to answer embodied questions based on Semantic-Value-Weighted Frontier Exploration and black-box VLM confidence elicitation.\n\u2022 We propose an RAG-based VLM prompting method and an outlier detection-based early stopping strategy to im- prove accuracy and efficiency. Quantitative comparisons against the baselines validate the effectiveness of our approach."}, {"title": "II. RELATED WORKS", "content": "Embodied Question Answering (EQA) [4] is a task in which an agent navigates an unfamiliar 3D environment to answer specific queries. Early works like [7], [36] approached this by constructing holistic models entirely from scratch, leading to highly resource-intensive training and a time- consuming data collection process. More recent studies demonstrate that VLMs, such as GPT-4V [1], LLAVA [19], and Prismatic VLM [11], are capable of handling EQA tasks. Open-vocabulary EQA has been explored in [22], but their approach focuses more on video-based question answering instead of exploration. In contrast, [28] employed VLMS to both explore environments and answer questions. Our work diverges from theirs by leveraging advanced black-box models, which restrict access to parameters while focusing on the more challenging and realistic open-vocabulary setting."}, {"title": "B. VLMs for Embodied Agents", "content": "The success of pre-trained open-vocabulary vision-language models [8], [26], [24] significantly boost the zero-shot perfor- mance of data-driven embodied agents in manipulation [16], [17], navigation [30], [38], and question-answering [28] tasks. These results further eliminate the high cost of data collection and fine-tuning when transferring to new problem domains. However, current VLM embodied agent works do not consider how agents can efficiently explore the environment while performing open-vocabulary EQA tasks in a training-free maneuver. Our paper distinctively fills this gap."}, {"title": "C. Confidence Estimation of Language Models", "content": "Measuring the confidence of language models in an answer is a fundamental task. Early methods depend on extra data and finetuning techniques to calibrate the language models. [23] trains an additional model to analyze the certainty of uncalibrated language models. [18] finetunes GPT-3 [2] to generate verbalized calibration explicitly. With the rise of LLMs, research focuses on utilizing only the model output in a black-box setting. [12] converts the output to the semantic space and calculates the entropy as an estimation for uncertainty. [32] proposes to elicit verbal confidence instead of estimating condition probability via sampling. [34] defines a framework to prompt LLMs to elicit confidence and analyzes numerous possible strategies. In this work, we find that the verbalized confidence of vision language models helps determine the direction to explore."}, {"title": "III. PROBLEM FORMULATION", "content": "We formulate a new challenging task, Open Explore EQA, for a mobile robot to explore a 3D scene S aiming to infer the answer A of a given open-vocabulary embodied question Q. Each embodied question answering task \u00a7 is defined as a tuple \u00a7 = (S, Po, Q, A*), where S is the given real or simulated 3D scene, Po is the initial pose of the robot, Q is the open-vocabulary embodied question, and A* refers to the ground answer of Q. We assume no prior knowledge of the distribution of S, Q, and A* so the robot agent must explore S from scratch to get the context of Q, i.e., the robot agent need to take exploratory, information gathering actions to give the answer (e.g., 'Q: Do we have canned tomatoes at home? A: Yes, I found canned tomatoes in the pantry.' [22]). While most previous EQA works [28], [33] mainly con- sider closed-form question answering, i.e., only answering questions with a given finite answer choice set Y = {'A', 'B', 'C', 'D', ..., \u2018Some Answer\u2019} where A* \u2208 Y, we assume completely no knowledge of Y and perform question answering together with environment exploring in an open- vocabulary way, hence the name Open Explore EQA."}, {"title": "IV. PROPOSED METHOD", "content": "Our EfficientEQA framework is composed of three main modules, the Exploration module E, the Answering module A, and the Stopping module S, as shown in Fig. 2. Starting from the initial pose Po, the robot agent actively explores the scene by capturing each step's observation Ht = (It, Dt). For the Exploring module E, our model builds the environment map by fusing the observation history H = {H1, H2, ..., H+} at step t using Semantic-Value-Weighted Frontier Exploration (SFE) [28], [35]. Then the exploring direction is predicted by Verbalized Confidence from black-box models. For the Answering module A, the episode history {H1, H2, ..., H*} is filtered by BLIP and then fed into the VLM for RAG for an answer. For the Stopping module S, our model detects the answer-deterministic frame H* as an outlier from the distribution of the episode history."}, {"title": "B. Exploration Module: Semantic-Value-Weighted Frontier Exploration (SFE)", "content": "As the first component of our exploration module E, SFE provides an effective methodology for navigating unknown environments using semantic insights. This approach requires only a preliminary understanding of the scene's dimensions, and it dynamically updates an environmental map based on continual observations and corresponding actions. [28], [35] Upon capturing a new observation H\u2081 at position P\u2081, the robot first identifies and selects voxels that are visible in the image according to the camera's positioning. These voxels are then marked as explored, and their semantic values are updated to reflect new data. Subsequently, the system proposes several potential paths to investigate the boundary between explored and unexplored areas. A VLM not only selects the most promising direction for further exploration, but also gauges confidence levels associated with each option. Thus, the semantic map is revised to incorporate the latest findings, serving as a guide for subsequent exploration. Finally, the robot advances to the designated location Pi+1, positioning itself for the next phase of exploration. Notably, the role of SFE is limited to map construction; it neither answers queries nor decides when to end the exploration process. SFE also requires VLMs to provide a semantic understanding of the observation, which we will elaborate upon in the next section."}, {"title": "C. Exploration Module: Verbalized Confidence from Black-Box Models", "content": "As the second component of E, the VLM is prompted to determine the next place to explore and provide a confidence score for each option. Because most state-of-the-art models are black boxes to users, we need to elicit verbalized confidence without knowing its internal states. In order to calibrate the confidence, we apply a pipeline that obtains M answers and normalized confidence simultaneously.\nFirst, we ask for M answers to the question concurrently and then the resultant response is parsed. We get a set of answers and corresponding confidence {ri, Ci}M\u2081. Note that the answers might overlap with others and the confidence is not normalized. Second, we normalize the confidence to ensure the sum is one:  c'i = Ci/\u03a3Ci . Third, we ask language models to annotate answers with the same meanings, we assume that all such answers are in the same sematic equivalent class C. By leveraging language models, we implicitly implement the semantic grouping function E():\n E(r_i) = E(r_j) \\text{ iff. } r_i, r_j \\in R_k.   (1)\nThe above operation groups all answers into semantic equivalence classes {Rk} with cumulative confidences {Ck}, where\n  C_k = \\sum_{R_k = E(r_i)} c'_i   (2)\nLastly, we select the semantic equivalence class with the highest confidence as\n  (R^*, C^*) = \\arg \\max (R_k, C_k).   (3)\nWe visualize the application of verbalized confidence to our method in Fig. 3. The exploration module E marks the locations to explore in observation I\u2081 and then the VLM is asked to provide confidence in each candidate's direction. Then those confidence scores are normalized and fed back to E to update the internal semantic map."}, {"title": "D. Stopping Module: Relevancy-Based Early Stopping", "content": "We collect an episode history H = {H1, H2, ..., Hn} for the n steps' exploration of the robot agent, where each observation H+ = (It, Dt) is the robot's observation taken at the t-th step's pose Pt, containing an RGB image It \u2208 RH1\u00d7W1\u00d73 and a depth image Dt \u2208 RH1\u00d7W1. We compute the similarity score Sim(It, Q) of the RGB image It and the question Q with BLIP [15] model.:\n Sim(I_t, Q) = BLIP(I_t, Q).   (4)\nWe design an early-stop strategy for the agent's exploration by detecting an answer-deterministic frame H* \u2208 H, which is sufficiently related to the question Q that the agent can provide a deterministic answer A* based on all observations collected up to the point {H1, H2, ..., H*}, without requiring any further observations.\nAssume the episode history H' = {H1, H2, ..., Hi} up to the i-th step of the agent's exploration is not deterministic for providing an answer. For {H1, H2, ..., Hi}, their similarity scores with respect to follow a normal distribution, expressed as:\n Sim(I_j, Q) \\sim N(\\mu_\\tau, \\sigma^2_\\tau) \\text{ for } j = 1, 2, ..., i.   (5)\nIn contrast, the answer-deterministic frame H* is so relevant to Q that its similarity score Sim(I*, Q) is an outlier that falls outside of \u03bc\u03b9 + \u0396\u03c4\u03b7\u03c3\u03b9 in this normal distribution. Zth is a hyperparameter to balance accuracy and efficiency.\nIn practice, we calculate the sample mean \u03bc\u0302 and sample standard deviation \u03c3\u0302 based on the available episode history H' = {H1, H2, ..., Hi} as\n \\hat{\\mu}_\\tau = \\frac{1}{i} \\sum_{j=1}^i Sim (I_j, Q)   (6)\n \\hat{\\sigma}^2_\\tau = \\frac{1}{i} \\sum_{j=1}^i (Sim(I_j, Q) - \\hat{\\mu}_\\tau)^2.   (7)\nWe can then compute the probability that a new observation H* is an answer-deterministic frame. We use the Z-score z to calculate the stopping threshold given by\n z = \\frac{Sim(I_\\ast, Q) - \\hat{\\mu}_\\tau}{\\hat{\\sigma}_\\tau}   (8)\nGiven Zth = 3, for a regular observation H\u2081 = (It, Dt) which satisfies Sim(Ij, Q) \u223c N(\u03bc\u03b9, \u03c3}), the probability that H; is misclassified as an answer-deterministic frame under the \u03bc\u2081 + 3\u03c3\u2081 threshold is less than 1 \u2013 \u03a6(3) = 0.13%, where \u03a6(x) is the cumulative distribution function of a standard normal distribution. So we can use \u03bc\u2081 + 3\u03c3\u2081 as a threshold for detecting the answer-deterministic frame H* and stopping"}, {"title": "E. Answering Module: Retrieval-Augmented Generation for Question Answering", "content": "After the robot halts, we use RAG to generate the final answer A for Q. Previous works [28], [22] either evenly sample observations during exploration or answer questions based on a single image. In order to save computation and minimize distractions, we draw on the idea of [13], only feeding those images that can help answer the question to VLMs, i.e., the top K images with the highest image-question relevancy, as is shown in Fig. 5. The image-question relevancy Sim(It, Q) is calculated by Equation (4)."}, {"title": "V. EXPERIMENTS", "content": "Simulation and datasets. We use the Habitat-Sim physics simulator [25] with the HM-EQA dataset [28], an embod- ied question-answering dataset featuring photo-realistic 3D scans from the Habitat-Matterport 3D Research Dataset (HM3D) [27] along with relevant questions. While the original dataset provides multiple answer choices for each question, we remove the choice sets in our setting.\nEvaluation. To evaluate the accuracy of open vocabulary question answering, we utilize the LLM-Match metric, which has demonstrated robustness in several prior studies [22], [20]. Given a triplet consisting of a question, a reference answer, and the model's prediction, a large language model (LLM) is tasked with assigning a score \u03c3e \u2208 {1,2,3,4,5}, where a higher score reflects a better match. The aggregated LLM-based correctness metric (LLM-Match) is defined as\n LLM-Match Score = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\sigma e - 1}{4} \\times 100\\%.   (9)\nEfficiency is evaluated based on the number of steps taken before the robot halts, a value that correlates with the number of VLM calls and the overall execution time. As such, it serves as a highly suitable metric for assessing efficiency.\nFor the choice of models, we use the gpt-4o-2024- 05-13 model from OpenAI as our VLM backbone, with temperature T set to 1. We choose the BLIP-2 model version Salesforce/blip2-itm-vit-g [15] to gauge the relevance between images and questions.\nFor the choice of hyperparameters, the default value for the Z-score threshold Zth is 3 and for K is 5."}, {"title": "B. Baselines and Ablations", "content": "While there are no direct baselines for comparison, we modified and adapted the text-only LLM and Explore- EQA [28] to accomplish Open Explore EQA for evaluation:\n\u2022 Blind LLM that is provided only with the text question, without any visual input. This tests the inherent world knowledge encoded in the LLM's parameters. We use GPT-4-Turbo for the baseline.\n\u2022 White-Box ExploreEQA that also employs semantic- value-weighted frontier exploration but requires access to the logits from VLMs. It uses Prismatic VLMs [11] in a white-box setting. Notably, comparing this baseline with our method is not entirely fair, as our approach assumes a black-box setting for VLM usage, while White-Box ExploreEQA has access to the model's internal logits.\n\u2022 Black-Box ExploreEQA that uses verbalized confidence instead of logits to guide exploration, while all other com- ponents of White-Box ExploreEQA remain unchanged.\nIn addition, we compare our method with two versions, each of which ablates one core component:\n\u2022 Ours w/ different stopping criteria that uses different Z-scores as the stopping criteria for detecting the answer- deterministic frame.\n\u2022 Ours w/ different RAG hyperparameters that uses different RAG hyperparameters to filter the observation history episode H = {H1, H2, ..., Hn}."}, {"title": "C. Results and Analysis", "content": "1) Comparisons with Baselines:  shows that our method surpasses both baselines in terms of correctness and efficiency. This improvement is partly due to the baselines' reliance on logits, which restricts their ability to leverage more powerful models like GPT-40 [1], whose logits are not always accessible. In contrast, our method seamlessly integrates black- box models and allows the VLM to provide comprehensive answers based on multiple relevant images. Notably, the blind LLM achieves decent performance, verifying that reasonable guesses can be made merely based on prior world knowledge.\nIn terms of efficiency, the baseline's dependence on multi- step conformal prediction [29] is a limiting factor, as it only applies to questions with predefined answer options and cannot terminate earlier than a set number of steps. Our method, instead, employs an early-stopping mechanism, enabling it to stop once the correct answer is determined, enhancing efficiency.\n2) Comparisons with Ablations: TABLE II shows our method's performance under different stopping thresholds Zth. Our method always beats the baselines in TABLE I in both Average Steps and LLM-Match scores, which proves the robustness of our method. Intuitively, with an increasing Zth, the framework should be more strict about when to stop, and thus expects a higher number in both Average Steps and LLM-Match scores. The exception in TABLE II that the Average Steps of Zth = 2 is greater than that of Zth = 1 should come from the variance in the exploration process with VLMs. For any scene unexplored, the robot agent will always need a minimum number of steps to reach the related target. The Average Steps for Ours (Zth = 1) and Ours (Zth = 2) are actually reaching that lower bound.\nFig. 6 visualizes the distribution of image-question rele- vancy Sim(It, Q) across a typical historical episode. During exploration, the majority of observations do not significantly contribute to answering the question, which helps in estab- lishing the running statistics. The retrieval performance of the BLIP model is robust, consistently yielding high scores for images closely related to the question. This reliability supports the viability of our proposed early-stop methodology.\nTABLE III presents the LLM-Match scores under varying numbers of considered observations within the VLM. Interest- ingly, using only the most pertinent image (K = 1) achieves a respectable level of correctness. The optimal performance is observed at K = 5, suggesting a balance between relevant information and cognitive overload. However, as the number increases to K = 10, there is a noticeable decline in performance, likely due to the inclusion of less relevant or distracting images that may mislead VLM's decision-making."}, {"title": "VI. CONCLUSIONS", "content": "We formulate the novel task of Open Explore EQA. Using Semantic-Value-Weighted Frontier Exploration and black-box VLM confidence calibration, we develop a novel framework, EfficientEQA, for open vocabulary embod- ied question answering. We further propose the Retrieval-Augmented Generation-based VLM querying method and outlier detection-based early stopping strategy to improve our method's accuracy and efficiency. Comprehensive experiments using the Habitat-Sim physics simulator [25] equipped with large-scale Habitat-Matterport 3D Research Dataset (HM3D) [27] and HM-EQA [28] validated the effectiveness of our proposed approach."}]}