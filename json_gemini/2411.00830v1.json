{"title": "Unsupervised Training of a Dynamic Context-Aware Deep Denoising Framework for Low-Dose Fluoroscopic Imaging", "authors": ["Sun-Young Jeon", "Sen Wang", "Adam S. Wang", "Garry E. Gold", "Jang-Hwan Choi"], "abstract": "Fluoroscopy is critical for real-time X-ray visualization in medical imaging. However, low-dose images are compromised by noise, potentially affecting diagnostic accuracy. Noise reduction is crucial for maintaining image quality, especially given such challenges as motion artifacts and the limited availability of clean data in medical imaging. To address these issues, we propose an unsupervised training framework for dynamic context-aware denoising of fluoroscopy image sequences. First, we train the multi-scale recurrent attention U-Net (MSR2AU-Net) without requiring clean data to address the initial noise. Second, we incorporate a knowledge distillation-based uncorrelated noise suppression module and a recursive filtering-based correlated noise suppression module enhanced with motion compensation to further improve motion compensation and achieve superior denoising performance. Finally, we introduce a novel approach by combining these modules with a pixel-wise dynamic object motion cross-fusion matrix, designed to adapt to motion, and an edge-preserving loss for precise detail retention. To validate the proposed method, we conducted extensive numerical experiments on medical image datasets, including 3500 fluoroscopy images from dynamic phantoms (2,400 images for training, 1,100 for testing) and 350 clinical images from a spinal surgery patient. Moreover, we demonstrated the robustness of our approach across different imaging modalities by testing it on the publicly available 2016 Low Dose CT Grand Challenge dataset, using 4,800 images for training and 1,136 for testing. The results demonstrate that the proposed approach outperforms state of-the-art unsupervised algorithms in both visual quality and quantitative evaluation while achieving comparable performance to well-established supervised learning methods across low-dose fluoroscopy and CT imaging. The related source code will be available at https://github.com/sunyoungIT/UDCA-Net.git.", "sections": [{"title": "I. INTRODUCTION", "content": "FLUOROSCOPY is critical in medical imaging, enabling the real-time generation and visualization of X-ray images. It is invaluable in understanding internal body structures and organs and imaging medical devices, such as catheters. Consequently, fluoroscopy is an indispensable tool used in various diagnostic examinations, including catheter insertion, vascular imaging, and monitoring during orthopedic surgery. However, the inherent ionizing radiation from X-rays presents considerable risks to patients and medical personnel [1]. Reducing the X-ray intensity is recommended to mitigate this risk. Adopting low-dose X-ray fluoroscopy is a standard practice in monitoring interventions. Although lower doses reduce radiation exposure, they can produce images with increased noise and artifacts [2]. Such imperfections can obscure vital details, potentially affecting clinical decision-making. Thus, efficient noise reduction techniques tailored to low-dose X ray fluoroscopic images are necessary to maintain essential features, including medical instruments and vital anatomical structures.\nVarious restoration techniques have been introduced to mitigate noise-caused image degradation. Their efficiency is closely related to the suitability of the noise model. Many of these methods characterize the noise as either an additive spatially invariant Gaussian [3], [4] or a signal-dependent Poisson [5], [6]. However, these noise-specific assumptions may not universally apply to clinics equipped with various fluoroscopy devices. Thus, demand exists for denoising al"}, {"title": null, "content": "gorithms without a specific noise model. Because of the significant noise levels in fluoroscopic images [6], restoring signals obscured by noise in a single fluoroscopic frame is unattainable. For instance, as demonstrated in Fig. 1(a), bone tissue obscured in a single frame (a1) becomes evident when averaging multiple frames of fluoroscopic image sequences (a2). Therefore, denoising fluoroscopic images must be performed using multi-frame image sequences, not just a single image.\nMulti-frame processing algorithm using traditional temporal filters has been proposed [6]\u2013[10]. These traditional filter based methods have successfully reduced noise and improved the signal-to-noise ratio (SNR) in low-dose fluoroscopic im ages while minimizing processing time. However, depending on the specific filter applied, the resulting filtered images may exhibit motion-induced blurring in the temporal direction. For instance, as depicted in Fig. 1(b), when applying a recursive filter to a multi-frame sequence image (b1), signif icant blurring is caused by motion that is not present in the single frame image (b2). To address motion-blurring problems, many researchers have developed methods to enhance the quality of sequential fluoroscopic images by diminishing noise artifacts [11]\u2013[17]. For example, Amiot et al. [15] applied a motion-compensated temporal filtering technique operating on multi-scale coefficients to image sequences, successfully capturing subtle features with low contrast while minimizing boundary blurring. However, when acquiring images in X ray fluoroscopy sequences involving complex motion, motion artifacts, such as blurring and trailing, can occur [10], [18], [19]. This problem underscores the need for proper motion compensation when handling multi-frame sequence images.\nWith the recent emergence of deep learning technology, significant advancements have been achieved in various image processing tasks, including image denoising [20]\u2013[34], to enhance denoising performance. This development has yielded remarkable results, demonstrating the successful application of deep learning in image denoising. Furthermore, several convolutional neural network (CNN)-based methods have been proposed in the field of fluoroscopy images [4], [35]\u2013[39]. These approaches successfully reduce noise while retaining image details, displaying exceptional performance. However, most approaches for denoising fluoroscopy images employ supervised learning and rely on a substantial dataset of paired images. These paired images include noisy low- and high dose X-ray images, which are essential to learn the mapping function between the two image types effectively. Obtaining such paired data in a clinical interventional setting presents significant challenges due to patient motion over fluoroscopy image sequences and the potential risks associated with in creased ionizing radiation exposure. To address this problem, self-supervised or unsupervised learning-based methods have been proposed for medical image denoising, particularly in the context of CT imaging [39]\u2013[49]. However, research on fluoroscopy image denoising remains limited, and studies in this field are notably scarce [50]. This represents a significant gap in the application of advanced denoising techniques to fluoroscopy. Despite the limited number of studies on unsu pervised and self-supervised methods for fluoroscopy noise"}, {"title": null, "content": "reduction, a few notable examples do exist. For instance, Liu et al. [51] proposed a three-stage, self-supervised framework for denoising fluoroscopy videos, involving stabilization, mask based RPCA decomposition, and spatiotemporal bilateral fil tering. This approach underscores the potential of enhancing self-supervised learning techniques in fluoroscopy applica tions. Similarly, Sandersonet al. [52] introduced DDPM-X, a diffusion model-based denoising method for planar X-ray images, capable of handling Gaussian and Poisson noise with out network modifications. However, Liu et al. [51] utilized the Self2Self [53], making it challenging to specify the noise model, particularly in real-world scenarios where such models are crucial. Furthermore, preserving edge details is essential for accurate diagnostics, and Sanderson et al. [52] highlights the need for proper motion compensation methods for moving fluoroscopy sequences. These challenges underscore the key requirements for effective fluoroscopic image denoising algo rithms, which include 1) not relying on a specific noise model, 2) the necessity of multi-frame sequence images, 3) the need for proper motion compensation in the temporal domain, and 4) the requirement for unsupervised learning due to patient movement.\nGiven these requirements, we propose an unsupervised, two-step denoising framework that effectively utilizes multi frame sequence images to meet these critical needs. Our approach specifically addresses motion compensation, multi frame utilization, and unsupervised learning, while also en hancing edge preservation and improving overall image qual ity in fluoroscopy. In the first step, we pretrain the multi scale recurrent attention U-Net (MSR2AU-Net) using a self supervised approach that predicts the middle frame from a series of sequential fluoroscopic images. In the following step, we incorporate a correlated noise suppression module to manage denoising in pixel regions with significant inter frame motion and an uncorrelated noise suppression module for areas with less motion. The correlated noise suppression module operates recursive filtering on sequence images that have undergone motion compensation. We adopt a knowledge distillation approach for the uncorrelated noise suppression module, using the pre-trained MSR2AU-Net as a teacher model to train a student denoising model. We aim to preserve image edges and ensure outstanding denoising outcomes by fusing these modules and tailoring them to the dynamic object motion on a pixel-by-pixel basis. We evaluate the denoising performance of the unsupervised two-step framework using dynamic phantom and clinical, in vivo datasets. We also used the publicly available CT dataset from the \u201c2016 NIH-AAPM Mayo Clinic Low Dose CT Grand Challenge\u201d dataset [54]. We compare it with the current state-of-the-art unsupervised and supervised learning methods. The experimental results indicate that the proposed framework effectively minimizes noise levels while maintaining the intricate details of microstructures. The primary contributions of this paper are summarized as follows:\n\u2022 An unsupervised dynamic context-aware denoising framework is proposed to mitigate both correlated and uncorrelated motion-induced quantum noise in low-dose fluoroscopy images. To the best of our knowledge, this is"}, {"title": null, "content": "the first attempt at an unsupervised deep learning-based denoising approach specifically designed for dynamic low-dose X-ray fluoroscopy.\n\u2022 Based on dynamic context extraction modules, motion compensation and recursive filtering modules are inte grated into the network to enhance denoising capabilities and effectively address issues like motion blurring by considering temporal correlations between frames.\n\u2022 In the edge-preserving extraction module, a pixel-wise cross-fusion matrix is designed to capture changes be tween the motion-corrected image and noise reduction, maintaining sharp edges and ensuring optimal denoising results. This module combines correlated and uncorre lated noise reduction techniques, adapting to the dynamic motion of the object on a pixel-by-pixel basis.\n\u2022 To improve the network\u2019s ability to maintain sharpness and enhance perceptual quality, an improved loss function is proposed to maximize the extraction of high-frequency features by combining Wavelet and Fourier Transform techniques.\n\u2022 The evaluations on clinical and dynamic-phantom fluo roscopy datasets, as well as low-dose CT, reveal that the proposed algorithm not only surpasses existing leading unsupervised algorithms but also matches the perfor mance of supervised learning-based approaches. Addi tionally, our framework has been effectively applied to both low-dose CT and fluoroscopy, showcasing strong generalization across different medical imaging modal ities."}, {"title": "A. Related Works", "content": "1) Low-dose CT Denoising: Various CNN-based methods, such as U-Net [55] and DnCNN [56], have been employed to reduce noise in low-dose CT (LDCT) images. Similarly, RED CNN [57] utilizes a combination of ResNet and Autoencoder architectures to further improve denoising performance. These CNN-based approaches showcase the considerable potential of deep learning techniques in advancing medical imaging. However, these techniques often produce overly smooth im ages due to the mean squared error (MSE) loss. To address this, alternatives like perceptual and GAN losses have been introduced. For instance, exemplified in Yang et al.\u2019s WGAN VGG framework [58], effectively address this smoothing issue, resulting in more realistic image details and improving overall image quality. Kim et al. [59] proposed a method combining pixel-wise losses for high objective quality with perceptual and wavelet losses to preserve fine details and edges, leveraging wavelet transform properties for enhanced image quality. In addition, CCN-CL [60] enhances denoising accuracy using a content-noise complementary learning strategy, attention mechanisms, and deformable convolutions. Transformer-based models like CTformer [61] introduce pure transformer ap proaches to LDCT denoising, outperforming traditional meth ods. StruNet [62] presents a novel Swin transformer-based residual U-shape network, showing promise in handling di verse noise artifacts across various imaging modalities. How ever, these networks generally rely on supervised learning"}, {"title": null, "content": "with paired LDCT and normal-dose CT datasets, a method challenging for fluoroscopy imaging, which involves capturing moving objects. Recently, self-supervised models that only re quire noisy natural images (e.g., BM3D [63], NLM [64], Noisier2Noise [65], Noise2Void [66], and Noisy-As-Clean [67]) have been applied for denoising tasks in both LDCT and fluoroscopy imaging. These self-supervised approaches have shown promising results, demonstrating their potential to ef fectively reduce noise without the need for clean reference images, making them valuable tools for improving image quality in challenging medical imaging scenarios. Nonethe less, these methods rely on specific assumptions about noise characteristics in the images, and therefore, their application in fluoroscopic imaging might result in suboptimal performance. This highlights the importance of exploring unsupervised learning techniques, which can provide more flexible and generalized solutions for noise reduction without relying on such assumptions. MM-Net [42], a framework based on un supervised learning, incorporates multi-mask patching with high-frequency components, allowing it to surpass existing unsupervised algorithms in both qualitative and quantitative assessments across diverse clinical and animal data domains. DenoisingGAN [68] leverages unpaired datasets and integrates a CycleGAN variation with a memory-efficient architecture, demonstrating superior performance in both objective and perceptual quality. These unsupervised learning approaches offer significant advantages by not relying on paired NDCT and LDCT images, making them more adaptable and easier to implement in various clinical settings. However, their clinical application remains limited due to the current limitations in denoising performance.\n2) Low-dose X-ray Fluoroscopy Denoising: While there are relatively fewer deep learning-based denoising algorithms for fluoroscopy compared to LDCT, recent advancements have been made in this area. For instance, Zhang et al. [36] proposed a hybrid three-dimensional (3D)/two-dimensional (2D)-based deep CNN framework for X-ray angiography using a stack of consecutive frames as input. Unlike single-frame approaches, which struggle with motion artifacts due to their limited temporal context, this method harnesses both spatial and temporal information from multiple frames. This compre hensive approach not only significantly enhances image quality but also effectively reduces motion artifacts and preserves fine details, demonstrating a marked improvement over traditional single-frame denoising techniques. In addition, Wu et al. [38] introduced the multi-channel DnCNN (MCDnCNN) based on DenseNet [69] and DnCNN [56], demonstrating enhanced denoising capabilities for fluoroscopic sequence images. Van Veen et al. [37] showcased clinically viable video denoising re sults for fluoroscopic imaging using a network based on Fast DVDNet [70], which incorporates multiple adjacent frames. This study integrates motion estimation directly into the net work architecture, significantly enhancing runtime efficiency while maintaining or improving video quality and reducing radiation dose. Moreover, Luo et al. proposed UDDN [4], utilizing dense connections for information reuse in CNN frameworks, thereby enhancing denoising performance. They further improved this approach in EEDN [35] by incorporating"}, {"title": null, "content": "an edge-enhancement module, which significantly sharpens edges and fine details, resulting in clearer and more defined images. However, all these algorithms are based on supervised learning, making their application to clinical fluoroscopy imag ing quite challenging."}, {"title": "II. METHODS", "content": "An overview of the proposed approach is provided in Fig. 2. We adopted an unsupervised strategy to train the framework without a clean target and introduced two-step training of a dynamic context-aware denoising framework to improve the objective and perceptual quality. The details of the proposed approach are elaborated in the following section."}, {"title": "A. First Training Step: Multi-scale Recurrent Attention U-Net (MSR2AU-Net)", "content": "As shown in Fig. 3, we train the network to predict the center frame from a multi-frame input to remove the initial noise. We defined a series of consecutive noisy input sets as $S_i = [x_{i,-2}, x_{i,-1}, x_{i,+1}, x_{i,+2}]$ and $x_i$ as the target. This set comprises the i-th central frame ($x_i$) to be denoised, along with its two preceding frames ($x_{i,-2}, x_{i,-1}$) and two following frames ($x_{i,+1}, x_{i,+2}$), exhibiting a high degree of anatomical similarity. The use of multiple frames ensures temporal consistency and enhances the model\u2019s ability to reduce noise while preserving essential anatomical details. Although this might introduce a slight delay, given the high"}, {"title": null, "content": "frame rate of typical fluoroscopy systems (15 frames/s) and the slower pace of interventional procedures, the resulting delay is minimal and likely imperceptible to humans, esti mated at approximately 133ms. Building on this approach, the Multi-scale Recurrent Attention U-Net (MSR2AU-Net) processes these multi-frame inputs to effectively predict the central frame. Following previous work [42] highlighting the efficacy of the attention U-Net in noise reduction for LDCT images, we employed the attention U-Net in the MSR2AU-Net architecture. In this study, we replaced standard convolutional units with recurrent residual convolutional units and integrated the attention gate into MSR2AU-Net to enhance denoising performance. In the Recurrent residual convolutional unit [71], both recurrent and residual connections are incorporated into each convolutional layer, allowing the network to enhance its feature extraction capabilities without increasing the number of parameters. These units are strategically placed within the encoding and decoding paths of the network to optimize the flow of information. Additionally, the residual connections help develop deeper and more efficient models by facilitating the flow of gradients during training, thus mitigating the van ishing gradient problem. Thus, incorporating recurrent residual convolutional units allows for iterative information integration and attention-weight computation. This modification improves performance by integrating context information throughout the entire network without introducing additional network parameters [72], [73].\nAs depicted in Fig. 3, MSR2AU-Net comprises multi-scale feature extraction, attention gates, and a recurrent U-Net. During the training phase, we employed convolutional kernels of varied sizes [74] with 32 filters for each frame. Each frame is first processed through 3\u00d73 kernels to focus on fine details and local textures, 5\u00d75 kernels to capture mid-range patterns, and 7\u00d77 kernels to extract broader, more global structures within the image. Afterward, the resulting feature maps from these different scales are concatenated, enabling the network to effectively integrate information across multiple levels of detail, ultimately resulting in 384 concatenated feature maps. This methodology allows the network to recognize spatial and temporal correlations across sequential frames, enhancing its ability to extract essential features. We concatenated the feature map channels from each frame to enhance the network capability further. The multi-scale concatenated feature maps are then fed into the Recurrent Attention U-Net, which is trained to predict the center frame. The training process utilizes the L2-norm as the loss function, defined as follows:\n$L_2(\\theta) = (x_i - F_\\theta(S_i))^2$, (1)\nwhere $F_\\theta$ represents the MSR2AU-Net with parameter $\\theta$, and $x_i$ is the target."}, {"title": "B. Second Training Step: Edge-preserving Temporal Noise Suppression", "content": "The second training step serves two critical objectives: further noise reduction and improved perceptual quality. It ensures the preservation of textural tissue details, even in motion."}, {"title": null, "content": "1) Knowledge Distillation-Based Uncorrelated Noise Sup pression Module: As illustrated in Fig. 4, the proposed module employs off-line knowledge distillation, encompassing two distinct models: a teacher model with a pre-trained MSR2AU Net (in a frozen state) from the first training step and a U Net-based [55] student model. In the second training step, we used the current frame $x_i$ as the input of the U-Net. The teacher model knowledge is transferred to the student model by analyzing the differences in their outputs. The MSE loss measures the distance between the output of the teacher model and the output of the student model. The MSE-based loss function $L_{pre}$, denoted as the PRE loss, is computed as follows:\n$L_{pre} = \\sum_{i=1}^{N} (\\hat{x}_i - \\hat{O}_i)^2$, (2)\nwhere $\\hat{x}_i$ and $\\hat{O}_i$ are the outputs of the student and teacher models, respectively. The primary purpose of using the PRE loss $L_{pre}$ is to eliminate noise from noisy X-ray sequence im ages effectively. However, this method might not be adequate to attain the desired level of noise reduction, particularly when addressing motion-induced correlated noise.\n2) Motion Compensation and Recursive Filtering-Based Correlated Noise Suppression Module: We incorporated a mo tion compensation approach to bolster the efficacy of reducing motion-induced correlated or structured noise. We can separate a noisy image $x_i$ it into two components: $x_i = s_i + n_i$, where $s_i$ and $n_i$ represent the clean signal and its corresponding noise, respectively. Using noisy images, we assembled a collection of similar images from adjacent frames, represented by $x_i = s_i + n_i$ and $\\hat{x}_i = s_i + \\delta_i + \\hat{n}_i$, where $\\delta_i$ symbolizes the difference in the clean signal components in similar images (e.g., slight variations in anatomy across adjacent frames) and $n_i$ and $\\hat{n}_i$ indicate two distinct noise instances. We defined $\\theta_s$ as the network parameter vector optimized with similar"}, {"title": null, "content": "data pairs. We determined $\\theta_s$ by minimizing the ensuing loss function:\n$\\theta_s = arg \\underset{\\theta}{min} \\frac{1}{N_s} \\sum_{i=1}^{N_s} ||f(s_i + n_i; \\theta) - (s_i + \\delta_i + \\hat{n}_i)||^2_2$, (3)\nwhere $N_s$ represents the count of similar noisy image pairs. With the zero-mean conditional noise ($E[\\hat{n}_i | s_i + n_i ] = 0$) and the zero-mean conditional discrepancy ($E[\\delta_i | s_i + n_i ] = 0$), $\\theta_s$ coincides with network parameters optimized under paired noise-clean images as $N_s$ approaches infinity [75]. Consequently, when suitable motion compensation is applied to images from adjacent frames, the value of $\\delta_i$ decreases substantially, enabling supervised denoising to be conducted using only noisy images.\nAs illustrated in Fig. 4, we performed motion compensation between adjacent frames within the frame set from $x_{i,-2}$ to $x_{i,+2}$, involving the computation of optical flows between the current frame $x_i$ and its neighboring frames ($x_{i,-1}$ and $x_{i,+1}$). The neighboring frames ($x_{i,-1}$ and $x_{i,+1}$) were incorporated into the current frame $x_i$ through forward and backward warping. The process is the same for the remaining frames. For flow estimation, we used SpyNet [76] pre-trained with ImageNet [77] to estimate the motion.\nSubsequently, the motion-compensated output was input into the recursive filtering process, a real-time, edge-preserving smoothing filter [78] that effectively removes noise. The recursive filter output can be expressed as follows [79]:\n$\\hat{x}^r_{i,j+1} = (1 - w) \\cdot \\hat{x}^r_{i,j} + w \\cdot x_{i,j}$, (4)\nwhere j represents the index for $x_{i,j}$, taking on values within the range of -2 to 2. The parameter w is a weighting factor between 0 and 1 that determines the influence of the previous output value on the current output value. In this implementation, we set w = 0.2. Additionally, $\\hat{x}^r_{i,j}$ signifies the j-th output of the recursive filter. To enhance the signal-to-noise ratio (SNR) and detail preservation, we employed the MSE"}, {"title": null, "content": "between the final denoiser\u2019s output and the recursive filter\u2019s output as the loss function, termed the RECUR loss ($L_{recur}$). $L_{recur}$ is defined as follows:\n$L_{recur} = \\sum_{i=1}^{N} (\\hat{x}_i - \\hat{x}^r_i)^2$, (5)\nwhere $\\hat{x}_i$ and $\\hat{x}^r_i$ represent the output of the U-Net and the final recursive filter, respectively. By integrating the PRE loss $L_{pre}$ with the RECUR loss $L_{recur}$, we can significantly reduce noise and extract more temporal features between consecutive frames, thereby improving overall denoising performance.\n3) Edge-preserving Fusion of Uncorrelated and Correlated Noise Reduction Modules: The edge-preserving fusion mod ule enhances image quality and restores intricate details by suppressing both uncorrelated and correlated noise. The U Net within the knowledge distillation-based uncorrelated noise suppression module demonstrates robust denoising capabili ties. However, it neglects temporal inconsistency over multiple frames due to motion, blurring fine structures. In contrast, the motion compensation and recursive filtering-based correlated noise suppression module may have slightly inferior denois ing performance. However, the module guarantees temporal consistency, effectively preserving fine structures. To optimize the strengths of both modules, we design a cross-fusion matrix that generates an ideal output by merging the superior noise removal of the U-Net output with the temporal consistency of the recursive filter output. The process of the cross-fusion matrix is as follows: first, an output called the optimal output is calculated as the average of the U-Net and the recursive filter outputs, as shown below:\n$\\hat{T}_i = \\frac{1}{2}(\\hat{x}_i + \\hat{x}^r_i)$, (6)\nwhere $\\hat{T}_i$ represents the optimal image, $\\hat{x}_i$ denotes the U-Net output, and $\\hat{x}^r_i$ is the output from the recursive filter.\nTo maintain intricate texture details, as shown in Fig. 4, we can derive two difference images, $\\Delta \\hat{x}_i$ and $\\Delta \\hat{x}^r_i$, from $\\hat{T}_i$ to detect changes between the two images, $\\hat{x}_i$ and $\\hat{x}^r_i$. This can be formalized as follows:\n$\\Delta \\hat{x}_i = |\\hat{T}_i - \\hat{x}_i|, \\Delta \\hat{x}^r_i = |\\hat{T}_i - \\hat{x}^r_i|$, (7)\nwhere $\\Delta \\hat{x}_i$ represents the level of noise, while $\\Delta \\hat{x}^r_i$ indicates the degree of motion correction. To further refine our analysis, we apply a variance operator to both $\\Delta \\hat{x}_i$ and $\\Delta \\hat{x}^r_i$. The vari ance operator helps us quantify the variability in these images, allowing us to assess the stability and consistency of the noise reduction and motion correction processes. After obtaining the variance images, we multiply the variance image of $\\Delta \\hat{x}^r_i$ by $\\hat{x}_i$ to incorporate the noise characteristics into the motion-corrected image. Similarly, we multiply the variance image of $\\Delta \\hat{x}_i$ by $\\hat{x}^r_i$ to integrate the motion correction details into the noise-reduced image. The fusion matrix outputs $M_{\\Delta \\hat{x}_i,\\hat{x}^r_i}$ and $M_{\\Delta \\hat{x}^r_i,\\hat{x}_i}$, are defined according to the following equation:\n$M_{\\Delta \\hat{x}_i,\\hat{x}^r_i} = var(\\Delta \\hat{x}_i) \\otimes \\hat{x}^r_i, M_{\\Delta \\hat{x}^r_i,\\hat{x}_i} = var(\\Delta \\hat{x}^r_i) \\otimes \\hat{x}_i$, (8)"}, {"title": null, "content": "where var denotes the variance operator and $\\otimes$ represents element-wise multiplication. This approach allows us to dy namically balance the strengths of both modules, resulting in a more robust and optimized final output.\n4) High-Frequency Extraction and Enhancement using Wavelet and Fourier Transform Module: To retain intricate texture details in fluoroscopic images affected by motion blur, we additionally incorporated a stationary wavelet transform (SWT) module to decompose the two outputs ($\\hat{x}_i$ and $\\hat{x}^r_i$) into four subbands each. We performed the Level 1 SWT using the Haar function as the wavelet function. One subband contains low-frequency information, whereas the other three capture high-frequency information. However, while SWT effectively preserves multi-resolution details, it may not fully capture the critical high-frequency components necessary for fine texture restoration.\nTo address this limitation, we further enhanced our approach by incorporating a Frequency Extraction Module based on Fast Fourier Transform (FFT). As shown in Fig 4, The three high-frequency subbands, excluding the low-frequency sub band, from both outputs ($\\hat{x}_i$ and $\\hat{x}^r_i$) are then passed through the Frequency Extraction Module. In this module, each of the three high-frequency subbands undergoes FFT to extract high-frequency components further. Subsequently, the low frequency components near the center of the spectrum are ex cluded, and an inverse FFT (iFFT) is performed to reconstruct the high-frequency details. The three transformed subbands are then combined to form the images, denoted as H($\\hat{x}_i$) and H($\\hat{x}^r_i$), with the extracted high-frequency characteristics. H($\\hat{x}_i$) and H($\\hat{x}^r_i$) are then concatenated with the fusion matrix outputs $M_{\\Delta \\hat{x}_i,\\hat{x}^r_i}$ and $M_{\\Delta \\hat{x}^r_i,\\hat{x}_i}$, respectively, and multiplied. The high frequency components $\\hat{x}^{hf}_i$ and $\\hat{x}^{r,hf}_i$ are then formalized as follows:\n$\\hat{x}^{hf}_i = M_{\\Delta \\hat{x}_i,\\hat{x}^r_i} \\otimes H(\\hat{x}_i), \\hat{x}^{r,hf}_i = M_{\\Delta \\hat{x}^r_i,\\hat{x}_i} \\otimes H(\\hat{x}^r_i)$, (9)\nwhere $\\otimes$ denotes element-wise multiplication. Subsequently, we determined the high-frequency-based perceptual loss. The high-frequency loss function, referred to as the HF loss ($L_{hf}$), sums the squared differences between the high-frequency components of the two outputs, $\\hat{x}^{hf}_i$ and $\\hat{x}^{r,hf}_i$, and is defined as follows:\n$L_{hf} = \\sum_{i=1}^{N} (\\hat{x}^{hf}_i - \\hat{x}^{r,hf}_i)^2$. (10)\n5) Total Loss Function: Finally, we defined the total loss function, denoted as $L_{final}$, for training the U-Net-based final denoising network. It combines the PRE loss $L_{pre}$, the RECUR loss $L_{recur}$, and the high-frequency loss $L_{hf}$ as follows:\n$L_{final} = L_{pre} + \\alpha \\cdot L_{recur} + L_{hf}$. (11)\nThe hyperparameter $\\alpha$ balances the contributions of the un correlated and correlated noise suppression modules. During training, we set $\\alpha$ to 1."}, {"title": "III. EXPERIMENTS", "content": "A. Experimental Setup\n1) Datasets: This study utilizes several datasets, including two dynamic phantom datasets and a clinical in vivo dataset."}, {"title": null, "content": "We procured fluoroscopy images of two dynamic phantoms at 60 kVp for model training and evaluation, with low- and high dose settings of 0.002 and 0.02 mAs, respectively. One phan tom replicated a real-bone X-ray hand; the other represented a needle biopsy with spherical objects. Their motions were simulated using the Model 008A Dynamic Thorax Phantom (CIRS, Norfolk, VA, USA) with translations up to \u00b12 cm. The dynamic phantom dataset comprises a total of 3,500 images with various motions. In our experiment, 2,400 pairs of low- and high-dose images were used for training, while the remaining 1,100 images were used for testing. From the training images, we randomly extracted 64\u00d764 patches from 700\u00d7700 images. A 5% subset of this training dataset was randomly chosen for validation. Furthermore, we evaluated the proposed method\u2019s performance on 350 clinical images obtained from spinal surgery patients, with C-arm settings at 106 kV and 0.02 mAs. For these clinical images, we employed cross-validation to demonstrate the model\u2019s robustness against overfitting and its generalization to unseen data. Additionally, we utilized the publicly available \u201c2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge\u201d dataset [54], which includes NDCT and LDCT images from ten patients. Each pair consists of 1-mm thickness 512\u00d7512 images. For this dataset, we used data from eight patients for training (4,800 images) and the other two for testing (1,136 images).\n2) Implementation Details: The proposed network and benchmark algorithms were implemented using PyTorch, en suring consistency by configuring all networks with the same settings. We used the Adam optimizer [80] with a learning rate of 1\u00d710\u22124. Additionally, we limited the maximum number of training epochs to 200. The first and second training steps were performed by randomly extracting patches of size 64\u00d764 pixels from the input data.\n3) Compared Methods: 19 state-of-the-art methods were used for comparison: seven unsupervised models (BM3D [63], NLM [64], Noisier2Noise (N2N) [65], Noise2Void (N2V) [66], Noisy-As-Clean (N2C) [67], MM-Net [81], and DenoisingGAN [68]) and 12 supervised models (U Net [55], RED-CNN [57], WGAN-VGG [58], DnCNN [56], MCDnCNN [38], UDDN [4], EEDN [35], FastDVDNet [37], CCN-CL [60], StruNet [62], CTformer [61], and Kim et al. [59]). To ensure a fair comparison, all experiments were conducted under identical conditions, with both quantitative and qualitative results reflecting the direct output of the models without any preprocessing. We visually analyzed the qualitative results by examining different denoising predictions from each model.\n4) Quantitative Evaluation Metrics: Five metrics were used to quantitatively evaluate the algorithm performance, includ ing peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), efficient deep-detector image quality assessment (EDIQA) [82], natural image quality evaluator (NIQE) [83], and visual information fidelity (VIF) [84]."}, {"title": "B. Results on Dynamic Phantom Datasets", "content": "We conducted experiments on the dynamic phantom dataset, with visual results for the anthropomorphic hand phantom"}, {"title": null, "content": "in Fig."}]}