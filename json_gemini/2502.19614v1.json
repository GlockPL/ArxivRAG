{"title": "Is Your Paper Being Reviewed by an LLM? A New Benchmark Dataset and Approach for Detecting AI Text in Peer Review", "authors": ["Sungduk Yu", "Man Luo", "Avinash Madusu", "Vasudev Lal", "Phillip Howard"], "abstract": "Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in large language models (LLMs), a new risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. However, there is a lack of existing resources for benchmarking the detectability of AI text in the domain of peer review. To address this deficiency, we introduce a comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews, covering 8 years of papers submitted to each of two leading AI research conferences (ICLR and NeurIPS). We use this new resource to evaluate the ability of 18 existing AI text detection algorithms to distinguish between peer reviews written by humans and different state-of-the-art LLMs. Motivated by the shortcomings of existing methods, we propose a new detection approach which surpasses existing methods in the identification of AI written peer reviews. Our work reveals the difficulty of identifying AI-generated text at the individual peer review level, highlighting the urgent need for new tools and methods to detect this unethical use of generative AI.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have enabled their application to a broad range of domains, where LLMs have demonstrated the ability to produce plausible and authoritative responses to queries even in highly technical subject areas. These advancements have coincided with a surge in interest in AI research, resulting in increased paper submissions to leading AI conferences (Audibert et al., 2022). Consequently, workloads for peer reviewers have also increased significantly, which could make LLMs an appealing tool for lessening the burden of fulfilling their peer review obligations (Kuznetsov et al., 2024; Kousha and Thelwall, 2024; Zhuang et al., 2025). Despite their impressive capabilities, the use of LLMs in the peer review process raises several ethical and methodological concerns which could compromise the integrity of the publication process (Hosseini and Horbach, 2023; Latona et al., 2024; Seghier, 2024; Zhou et al., 2024). Reviewers are selected based on their expertise in a technical domain related to a submitted manuscript, which is necessary to critically evaluate the proposed research. Offloading this responsibility to an LLM circumvents the role that reviewer selection plays in ensuring proper vetting of a manuscript. Furthermore, LLMs are prone to hallucination and may not possess the ability to rigorously evaluate research publications. Therefore, the use of LLMs in an undisclosed manner in peer review poses a significant ethical concern that could undermine confidence in this important process. Motivating the need for evaluation resources and detection tools to address this problem is the apparent increase in AI-generated text among peer reviews submitted to recent Al research conferences. Recent studies revealed an upward trend in AI-generated texts among peer reviews at the corpus level (Liang et al., 2024a; Latona et al., 2024). This trend is particularly concerning given that evaluations from human and AI reviewers are not aligned (Drori and Te'eni, 2024; Latona et al., 2024; Ye et al., 2024), suggesting that the unregulated and undisclosed use of LLMs in peer review could undermine the integrity of the current system. Despite the growing recognition of this problem, there is a lack of existing dataset resources for comprehensively evaluating the performance of AI text detection methods in the domain of peer review. To address this deficiency, we introduce"}, {"title": "2 Related Work", "content": "Al text detection datasets Several benchmark datasets have been introduced to evaluate AI-based text detection models. RAID-TD (Dugan et al., 2024) provides a large-scale benchmark designed to assess text detection under adversarial conditions, ensuring robustness against manipulated AI-generated content. The M4 Dataset (Wang et al., 2024) expands the scope by incorporating reviews from multiple LLMs across different languages, offering a more diverse linguistic evaluation. The HC3 Dataset (Guo et al., 2023) consists of responses from ChatGPT and human experts, covering specialized domains such as finance, medicine, and law, in addition to general open-domain content. In contrast, the GPT Reddit Dataset (GRiD) (Qazi et al., 2024) focuses on social media conversations, compiling a diverse set of human- and AI- generated responses to Reddit discussions. Meanwhile, Beemo (Artemova et al., 2024) introduces a benchmark of expert-edited machine-generated outputs, spanning creative writing, summarization, and other practical applications, further refining detection challenges. These benchmarks primarily evaluate AI-generated text from a single model and do not address the domain of AI text in peer review. In contrast, our dataset is larger than most existing datasets (788k generations) and is unique in its focus on AI text detection in peer review. AI-generated text detection AI-generated text detection has been framed as a binary classification task to distinguish human-written from machine- generated text (Bakhtin et al., 2019; Jawahar et al., 2020; Fagni et al., 2021; Mitchell et al., 2023a). Solaiman et al. (2019) used a bag-of-words model with logistic regression for GPT-2 detection, while fine-tuned language models like RoBERTa (Liu et al., 1907) improved accuracy (Zellers et al., 2019; Uchendu et al., 2020; Gehrmann et al., 2019). Zero-shot methods based on perplexity and en- tropy emerged as alternatives (Ippolito et al., 2020; Gehrmann et al., 2019). Other studies focused on linguistic patterns and syntactic features for model-agnostic detection (Uchendu et al., 2020; Gehrmann et al., 2019). Watermarking techniques, such as DetectGPT (Mitchell et al., 2023a), have also been proposed for proactive identification. Centralized frameworks like MGTBench (He et al., 2023) and its refined version, IMGTB (Spiegel and Macko, 2023), provide standardized evalua- tions for AI text detection. IMGTB categorizes"}, {"title": "3 Methods and Dataset Construction", "content": "proaches in detecting GPT-40 and Claude written peer reviews. We conduct analyses to understand how different levels of AI use for editing reviews impacts detectability and false positives, as well as the characteristics which distinguish LLM-written peer reviews from those written by humans. Our work demonstrate the challenge of detecting AI- written text in peer reviews and motivates the need for further research on methods to address this unethical use of LLMs in the peer review process. To summarize, our contributions are as follows: (1) We introduce a new dataset containing 788,984 AI-written peer reviews generated by five widely-used LLMs along with corresponding human-written peer reviews for the same papers, which is the largest resource of its kind for evaluating the detectability of AI text in peer review. (2) Using our dataset, we evaluate 18 existing open source AI text detection algorithms as well as a commercial API, finding that most methods struggle to reliably detect fully AI-written peer reviews at low levels of false positives. (3) We propose a new detection method which compares the semantic similarity of a given peer review to a ref- erence LLM-generated peer review for the same paper, which outperforms all existing detection approaches in our evaluations. (4) We conduct analyses to highlight the characteristics which distinguish human-written peer reviews from those generated by LLMs, show LLM reviews are generally more favorable and more confident it their assessment, and evaluate how the use of LLMs for editing peer reviews impact false positive rates. (5) We make our dataset publicly available under an open source license to facilitate future work on detecting AI generated text in peer review."}, {"title": "3.1 Human reviews", "content": "We used the OpenReview API (OpenReview) to collect submitted manuscripts and their reviews for the ICLR conferences from 2019 to 2024, as well as for NeurIPS conferences from 2021 to 2024. Additionally, we used the ASAP dataset (Yuan et al., 2022) to collect manuscripts and reviews for ICLR 2017 to 2018 and NeurIPS 2016 to 2019."}, {"title": "3.2 AI reviews", "content": "We generated 788,984 AI-generated reviews using five widely-used LLMs: GPT-40, Claude Sonnet 3.5, Gemini 1.5 pro, Qwen 2.5 72b, and Llama 3.1 70b. Prompts. To control the content and structure of these AI-generated reviews, we included conference-specific reviewer guidelines and review templates in the prompts. Note that this is non- trivial since the review templates have evolved sig-"}, {"title": "3.3 Dataset Statistics", "content": "Table 1 provides complete statistics for our generated dataset (see Appendix B.3 for a breakdown by conference year and review-generating LLM). We withheld a randomly sampled subset of reviews to serve as a calibration set, which is used in our experiments to determine classification thresholds for each evaluated method. This calibration set contains 75,824 AI-generated and human-generated peer reviews, divided approximately evenly across all five LLMs. To construct the calibration set, we randomly selected 500 papers from ICLR (2021, 2022) and NeurIPS (2021, 2022) and generated AI reviews corresponding to the human reviews for each paper. Because our sampling was done at the paper level rather than the review level, the number of reviews per paper\u2014and consequently per conference\u2014varies slightly. To facilitate the evaluation of detection methods which are more computationally expensive (e.g., methods which requires using LLMs as surrogates), we also withheld a separate test set consisting of human reviews and"}, {"title": "3.4 Review Editing", "content": "LLMs have been widely used for editing human writing (Laban et al., 2024; Raheja et al., 2024), including tasks such as grammar checking and enhancing language fluency, which are particularly beneficial for non-native speakers in conveying their reviews more clearly. However, some users may overly rely on LLMs, leading to significant alterations of the original review. To simulate such scenarios in our dataset, we generate edited reviews with four varying levels of modification using LLMs (minimum, moderate, extensive, and maximum). This approach enables us to assess how robust detection methods are for text with different degrees of editing. Appendix E.3 illustrates the types of prompts used to generate these edits. To validate whether these prompts produce reviews with varying levels of modification, we conducted a similarity checking experiment. Our hypothesis is that the similarity between the edited review and the original review decreases as the level of modification increases, ranging from minimal edits to extensive revisions. To test this, we used a sentence embedding model to compute embeddings for both the original and edited reviews and calculated their average cosine similarity scores. The results, summarized in Table 2, confirm that the editing levels are indeed distinct."}, {"title": "4 Anchor Embeddings Detection Methodology", "content": "We propose a novel method for detecting AI- generated reviews by comparing their semantic similarity to a reference AI-generated review for the same article, referred to as the \"Anchor Review\" (AR). The AR can be generated by any LLM. We use a simple, generic prompt (Appendix E.2) to generate the AR without prior knowledge of the user prompts (i.e., the AR prompt differs from those used to create reviews in the testing dataset). Once an AR is generated for a given paper (Eq.1), and a testing review (TR) is provided, we obtain their embeddings using a text embedding model (EM, Eqs.2 and 3). The semantic similarity between the embeddings of the AR and TR is then computed using the cosine similarity function (Eq.4). Finally, this similarity score is compared against a learned threshold (\u03b8): if the score exceeds the threshold, the review is classified as AI-generated; otherwise, it is not (Eq. 5). Putting everything together, the method is formalized as:\nAR = LLM(paper, PromptAR) (1)\nEmbAR = EM(AR) (2)\nEmbTR = EM(TR) (3)\nScore = Cosine_similarity (Embar, Embtr) (4)\nLabel = { 1 if Score > \u03b8,\n0 otherwise.\n(5)\nIn our study, we use OpenAI's embedding model (text-embedding-003-small). The threshold \u03b8 is learned from the calibration set. Specifically, for each review in the calibration data, we apply the steps outlined in Eqs. 1 to 4. Voting of Multiple Anchors Intuitively, our approach performs best when the anchor embeddings are generated using the same model that produced the test review (source LLM). However, in real-world scenarios, the source LLM is typically un-"}, {"title": "5 Experimental Results", "content": "known (a situation commonly referred to as \"black- box\" detection scenario). To address this challenge, we propose a voting-based technique. Specifically, we generate multiple anchor embeddings using different types of LLMs (anchor LLMs). For each anchor embedding, we compute the Score (Eq.4) and derive the corresponding label assignment (Eq.5). If at least one anchor embedding assigns a positive label, the final label is positive. Otherwise, the final label is negative. In our experiments in the following sections, we use three anchor reviews for voting each generated by GPT-40, Gemini, and Claude, respectively."}, {"title": "5.1 Fully AI-Written Review Detectability", "content": "We compare our approach with 18 baseline methods from IGMBT (see Appendix A for details) and utilize the calibration set to determine appropriate thresholds for the test data. The threshold is then determined by setting an target False Positive Rate (FPR), which is achieved by adjusting the threshold until the FPR equals the target value. In our setup, we have studied varying thresholds (e.g., 0.1%, 0.5%, and 1%). We focus on low FPR targets because false positive classifications\u2014where human-written reviews are mistakenly identified as AI-generated carry high stakes, potentially damaging an individual's reputation. Besides, we focus on Al review text samples generated by three commercial LLMs (GPT-40, Gemini, and Claude) because these models are more advanced, making the text detection task harder. Also, general users are more likely to choose them over open-source LLMs due to their convenient user interfaces and limited access to the compute resources required for running open-source models. Al text detection models can be calibrated for varying levels of sensitivity in order to balance the trade-off between true positive and false positive detections. Receiver operating characteristic (ROC) curves are therefore commonly used to compare different methods, as they provide a visualization of the true-positive rate (TPR) which can be achieved by a model when its decision boundary is calibrated for a range of different false positive rates (FPR). Figure 2 provides the ROC curves for our anchor embedding approach and baseline methods, calculated using our GPT-40, Gemini, and Claude review calibration subset separately for reviews submitted to ICLR2021-2022 (left) and"}, {"title": "5.2 Detecting AI-Edited Peer Reviews", "content": "NeurIPS2021-2022 (right). The area under the curve (AUC) is provided for each method in the legend; higher values indicate better detection accuracy across the entire range of FPR values. These plots show that our anchor embedding approach consistently achieves the highest AUC for AI peer review detection among evaluated methods, followed by the baseline method Binoculars. While ROC curves are useful for comparing the overall performance of different classifiers, only the far left portion of these plots are typically relevant for practical applications of AI text detection models. This is particularly true in the domain of peer review, where the cost of a false positive is high. Reviewers volunteer their time and expertise to sustain this important process; false accusations have the potential to further reduce the availability of reviewers due to disengagement and can also lead to significant reputational harm. Therefore, it is vital that AI text detection systems for peer review be calibrated for a low FPR in order to avoid such negative outcomes. Prior work has shown that AUC is not necessarily reflective of how models perform at very low FPR values (Yang et al., 2023; Krishna et al., 2024; Tufts et al.). Therefore, we also report the actual TPR and FPR achieved by different detection methods at discrete low values of target FPR (0.1%, 0.5%, and 1%), which we believe to be of greatest interest for practical applications. The target FPR is used to calculate each method's classification threshold using our calibration dataset, with the actual TPR and FPR computed over the withheld test dataset. To simulate a more challenging evaluation setting where some of the test reviews are \"out-of-domain\" in the sense that they come from a different conference than the calibration dataset, we use only the ICLR reviews to calibrate each method (see Section C.1 for in-domain evaluations). Table 3 provides these results separately for the detection of GPT-40, Gemini, and Claude reviews (Llama and Qwen results are provided in Appendix C.2). Other baseline methods that failed achieve a TPR of at least 1% at a target FPR of 1% (i.e., TPR@1%FPR <1%) are omitted. The results in Table 3 show that our anchor embedding approach achieves the highest TPR at low FPR values among evaluated methods for GPT-40 and Claude-generated reviews. This is notable, as GPT-40 and Claude are among the most advanced and widely-used LLMs currently available. The performance difference between our approach and the next best-performing method (Binoculars) is particularly large for GPT-40 reviews and low target FPR settings (e.g., 0.1%), with our anchor embedding approach achieving absolute improvements of up to 50.1% in TPR. Furthermore, our anchor embedding approach exactly achieves the target FPR on the test set for these reviews, whereas other methods exceed the target FPR by as much as 0.7%. Gemini-generated reviews appear to be easier for baseline methods to detect than GPT-40 and Claude reviews, and we observe that our anchor embedding approach performs similarly on these reviews as Binoculars. This suggests that the anchor embedding method provides the greatest benefits over existing methods for reviews which are the most challenging to detect; in fact, our approach performs best for detecting GPT-40 reviews, whereas other strong baselines such as Binoculars perform the worst on these reviews. Reviews generated by open-source LLMs (see"}, {"title": "6 Analysis", "content": "We also evaluated the ability of our method to correctly rank reviews in ascending order of their likelihood of being AI-generated. The ground truth order is: human-written, minimally-edited, moderately-edited, extensively-edited, maximally-edited, and fully AI-generated reviews. To measure this, we compute the probability that each review type is AI-generated and rank them accordingly. The ranking is compared with the ground truth using the NDCG metric (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002), with higher values indicating that the predicted ranking is closer to the ground truth ranking. In addition, we analyze the detection rates for each type of review. For each category, we calculate the percentage of reviews identified as AI-generated. Ideally, the detection rate should be lowest for minimally edited reviews and should progressively increase with the extent of AI involvement. Table 4 compares our method with the best baseline method from our prior experiments (Binoculars). Our anchor embedding method achieves a higher NDCG score compared to the Binoculars approach, indicating that our method ranks each type of review more accurately and closer to the ground truth. Both methods detect increasing percentages of AI-generated text as the extent of editing increases. This suggests that reviews of more extensive editing are more likely to be detected as AI-generated. For Minimum, Moderate, and Extensive types of edited reviews, our method shows a lower AI-generated detection percentage compared to Binoculars. On the other hand, for Maximum and fully AI-generated reviews, our method shows a higher detection rate than Binoculars. This suggests that our approach is better calibrated to detect reviews where AI involvement is predominant."}, {"title": "6.1 Human Analysis of Differences Between Human and AI-Written Peer Reviews", "content": "To better understand the characteristics which differentiate peer reviews written by humans and LLMs, we conducted a quantitative analysis of 32 reviews authored by humans and GPT-40 for 5 papers submitted to ICLR 2021. Specifically, we read an equal number of human and GPT-4 written reviews for each paper and noted differences in the content between them. A distinguishing characteristic of the analyzed human reviews was that they usually contained details or references to specific sections, tables, figures, or results in the paper. In contrast, peer reviews authored by GPT-40 lacked such specific details, instead focusing on higher-level comments. Another key difference identified in our qualitative analysis was the lack of any specific references to prior or related work in peer reviews generated by GPT-40. Human authored peer reviews often point out missing references, challenge the novelty of the paper by referencing related work, or suggest specific baselines with references that should be included in the study. In contrast, none of the analyzed GPT-40 reviews contained such specific references to related work. Finally, we found that the vast majority of GPT-4o reviews mentioned highly similar generic criticisms which were not found in human-authored reviews for the same paper. Examples of these issues are provided in Table 13 of Appendix D.1. Prior work has shown that peer reviews written by GPT-4 and humans have a level of semantic similarity which is comparable to that between different human-authored peer reviews, which has been used to advocate for the usefulness of feedback from GPT-4 in the paper writing process (Liang et al., 2024b). In our qualitative analysis, we found that GPT-4 does indeed generate similar higher-level comments as human reviewers, which could account for this semantic similarity. Despite being generic in nature, we would agree that such feedback could be useful to authors seeking to improve their manuscripts. Nevertheless, we believe that the lack of specificity, detail, and consideration of related work in peer reviews authored by GPT-4 demonstrates that it is not suitable for replacing human domain experts in the peer review process."}, {"title": "6.2 Misalignment Between Human and AI Reviews", "content": "In addition to qualitative differences in the content of human and AI-written reviews, we also observe a divergence in numeric scores assigned as part of the review. Figure 3 (Appendix D.2) provides histograms depicting the distribution of score differences for soundness, presentation, contribution, and confidence, which are computed by subtracting scores assigned for each category by human reviewers from those assigned by AI reviewers. AI-written peer reviews were matched with their corresponding human review (aligned by paper ID and overall recommendation) to compute the score differences. Confidence scores range from 1 to 5, while all other categories of scores range from 1 to 4. We focus on reviews from NeurIPS 2022, which were produced prior to the release of ChatGPT. This provides greater confidence that the human-labeled reviews were indeed written by humans, with little to no potential AI influence. All LLMs produce higher scores than human reviews with a high degree of statistical significance, assessed using a two-sided Wilcoxon signed-rank test (see legend for p-values). While the difference between human and AI confidence scores are relatively consistent across all three LLMs, Claude exceeds human scores by the greatest magnitude for soundness, presentation, and contribution. GPT-40 and and Gemini exceed human scores by a similar magnitude for presentation and contribution, while GPT-40 exhibits a greater divergence for soundness scores. Overall these results indicate that AI-written peer reviews are more favorable w.r.t. assigned scores than human-written peer reviews, which raises fairness concerns as scores are highly correlated with acceptance decisions. Our findings are consistent with prior work which has shown that papers reviewed by LLMs have a higher chance of acceptance (Drori and Te'eni, 2024; Latona et al., 2024; Ye et al., 2024)."}, {"title": "7 Conclusion", "content": "In this work, we introduced a new large-scale dataset of parallel human and AI-written peer reviews for identical papers submitted to leading AI research conferences. Our evaluations showed that 18 existing open-source approaches for detecting Al text are poorly suited to the challenging problem of identifying AI-generated peer reviews. While high detection rates are possible with existing methods, this comes at the cost of relatively high rates of falsely identifying human-written reviews as containing AI text, which must be minimized in practice. We proposed a new approach which intentionally generates AI-written reviews for a given paper to serve as a basis for comparing semantic similarity to other evaluated reviews, achieving much higher accuracy in identifying AI-written peer reviews while maintaining a low level of false positives. Our results demonstrate the promise of this approach while also motivating the need for further research on methods for detecting unethical applications of LLMs in the peer review process."}, {"title": "Limitations", "content": "Our dataset primarily focuses on two conferences, both within the computer science domain. To broaden its applicability and relevance, incorporating additional conferences from diverse research domains would be beneficial. Additionally, our proposed method requires generating reviews for manuscripts, making its use cases more constrained compared to previous approaches. However, in real-world scenarios, when reviewers submit their reviews, they typically accompany the corresponding manuscript. Therefore, our method remains applicable in this setting. Furthermore, our main results are based on evaluations of three commercial LLMs. Given the rapid emergence of new models, conducting comprehensive experiments across all available LLMs is infeasible. In addition, we leverage an open-source platform to conduct the baseline experiments, where baseline performance may vary depending on the choice of surrogate models. However, given the large number of baselines we evaluate, performing an exhaustive search for the optimal surrogate model for each method would be prohibitively expensive. Therefore, we use the default settings. Lastly, we used a fixed set of prompts for different LLMs to generate reviews. In reality, a potential AI-tool reviewer might leverage different prompt and therefore could affect the detection performance."}, {"title": "Ethics Statement", "content": "Our work adheres to ethical AI principles. Peer review plays a critical role in advancing scientific discovery; however, the misuse of AI tools by reviewers to generate reviews without proper diligence can compromise the integrity of the review process. Furthermore, consistent with previous studies, we have observed that AI-generated reviews tend to be overly generic, often failing to provide actionable feedback for authors. Additionally, AI reviewers generally assign higher scores compared to human reviewers, raising concerns that AI-assisted reviews could contribute to the acceptance of work that may not meet established human evaluation standards. By developing methods to detect AI-generated reviews, our work seeks to mitigate the misuse of AI tools in peer review and promote a more rigorous and fair scientific review process."}, {"title": "A Baseline methods.", "content": "We compare our approach to 18 baseline methods from IGMBT\u00b2 with its default setting (Spiegel and Macko, 2023), which are categorized into metric-based and pretrained model-based methods. The metric-based methods include Binoculars (Hans et al., 2024), DetectLLM-LLR (Su et al.), DNAGPT (Yang et al., 2023), Entropy (Gehrmann et al., 2019), FastDetectGPT (Bao et al., 2023), GLTR (Gehrmann et al., 2019), LLMDeviation (Wu and Xiang, 2023), Loglikelihood (Solaiman et al., 2019), LogRank (Mitchell et al., 2023b), MFD (Wu and Xiang, 2023), Rank (Gehrmann et al., 2019), and S5 (Spiegel and Macko, 2023). The model-based methods include NTNU-D (Sivesind and Winje, 2023), ChatGPT-D (Guo et al., 2023), OpenAI-D (Solaiman et al., 2019), OpenAI-D-lrg (Solaiman et al., 2019), RADAR-D (Solaiman et al., 2019), and MAGE-D (Li et al., 2024)."}, {"title": "A.1 Metric based methods", "content": ""}, {"title": "A.1.1 Binoculars", "content": "Binoculars (Hans et al., 2024), analyzes text through two perspectives. First, it calculates the log perplexity of the text using an observer LLM. Then, a performer LLM generates next-token predictions, whose perplexity is evaluated by the observer-this metric is termed cross-perplexity. The ratio of perplexity to cross-perplexity serves as a strong indicator for detecting LLM-generated text."}, {"title": "A.1.2 DNAGPT", "content": "DNAGPT (Yang et al., 2023) is a training-free detection method designed to identify machine- generated text. Unlike conventional approaches that rely on training models, DNAGPT uses Divergent N-Gram Analysis (DNA) to detect discrepancies in text origin. The method works by truncating a given text at the midpoint and using the preceding portion as input to an LLM to regenerate the missing section. By comparing the regenerated text with the original through N-gram analysis (black-box) or probability divergence (white-box), DNAGPT reveals distributional differences between human and machine-written text, offering a flexible and explainable detection strategy."}, {"title": "A.1.3 Entropy", "content": "Similar to the Rank score, the Entropy score for a text is determined by averaging the entropy values of each word, conditioned on its preceding context (Gehrmann et al., 2019)."}, {"title": "A.1.4 GLTR", "content": "The Entropy score, like the Rank score, is computed by averaging the entropy values of each word within a text, considering the preceding context (Gehrmann et al., 2019)."}, {"title": "A.1.5 MFD", "content": "The Multi-level Fine-grained Detection (MFD) (Wu and Xiang, 2023) framework enhances text detection by combining statistical, semantic, and linguistic features at the sentence level. It first extracts low-level statistical features like readability and author style to quantify sentence structure. Simultaneously, high-level semantic differences are captured using an encoder with contrastive learning to distinguish LLM-generated text from human- written content. Additionally, advanced LLMs analyze the full text, extracting deep linguistic features related to lexicon, grammar, and syntax for more precise detection."}, {"title": "A.1.6 Loglikelihood", "content": "This method utilizes a language model to compute the token-wise log probability. Specifically, given a text, the log probability of each token is averaged to produce a final score. A higher score indicates a greater likelihood that the text is machine- generated (Solaiman et al., 2019)."}, {"title": "A.1.7 LogRank", "content": "Unlike the Rank metric, which relies on absolute rank values, the Log-Rank score is derived by applying a logarithmic function to the rank value of each word (Mitchell et al., 2023b)."}, {"title": "A.1.8 Rank", "content": "The Rank score is calculated by determining the absolute rank of each word in a text based on its preceding context. The final score is obtained by averaging the rank values across the text. A lower score suggests a higher probability that the text was machine-generated (Gehrmann et al., 2019)."}, {"title": "A.1.9 DetectLLM-LLR", "content": "This approach integrates Log-Likelihood and Log- Rank scores, leveraging their complementary properties to analyze a given text (Su et al.)."}, {"title": "A.1.10 FastDetectGPT", "content": "This method assesses changes in a model's log probability function when small perturbations are introduced to a text. The underlying idea is that LLM-generated text often resides in a local optimum of the model's probability function. Consequently, minor perturbations to machine-generated text typically result in lower log probabilities, whereas perturbations to human-written text may lead to either an increase or decrease in log probability (Mitchell et al., 2023b)."}, {"title": "A.2 Model-based methods", "content": ""}, {"title": "A.2.1 NTNU-D", "content": "It is a fine-tuned classification model based on the ROBERTa-base model, and three sizes of the bloomz-models (Sivesind and Winje, 2023)"}, {"title": "A.2.2 ChatGPT-D", "content": "The ChatGPT Detector (Guo et al., 2023) is designed to differentiate between human-written text and content generated by ChatGPT. It is based on a ROBERTa model that has been fine-tuned for this specific task. The authors propose two training approaches: one that trains the model solely on generated responses and another that incorporates both question-answer pairs for joint training. In our evaluation, we adopt the first approach to maintain consistency with other detection methods."}, {"title": "A.2.3 OpenAI-D and RADAR-D", "content": "The OpenAI Detector (Solaiman et al., 2019) are models fine-tuned on RoBERTa to identify outputs generated by GPT-2. Specifically, it was trained using text generated by the largest GPT-2 model (1.5B parameters) and is capable of determining whether a given text is machine-generated."}, {"title": "A.2.4 MAGE-D", "content": "MAGE (MAchine-GEnerated text detection) (Li et al., 2024) is a large-scale benchmark designed for detecting AI-generated text. It compiles human- written content from seven diverse writing tasks, including story generation, news writing, and scientific writing. Corresponding machine-generated texts are produced using 27 different LLMs, such as ChatGPT, LLaMA, and Bloom, across three representative prompt types."}, {"title": "B Dataset Details", "content": ""}, {"title": "B.1 Dataset File Structure", "content": "The calibration, test, and extended sets are in separate directories. Each directory contains subdirectories for different models that were used to generate AI peer review samples. In each model's subdirectory, you will find multiple CSV files, with each file representing peer review samples of a specific conference. The directory and file structure are outlined below."}, {"title": "B.2 CSV File Content", "content": "CSV files may differ in their column structures across conferences and years. These differences are due to updates in the required review fields over time as well as variations between conferences. See Table 5 for review fields of individual conferences."}, {"title": "B.3 Dataset Sample Numbers per Conference Year", "content": "In this section, we present further breakdowns of sample numbers by conference, year, and LLM, as shown in Table 1."}, {"title": "C Additional results"}]}