{"title": "FLOW: A Feedback LOop FrameWork for Simultaneously Enhancing Recommendation and User Agents", "authors": ["Shihao Cai", "Jizhi Zhang", "Keqin Bao", "Chongming Gao", "Fuli Feng"], "abstract": "Agents powered by large language models have shown remark-able reasoning and execution capabilities, attracting researchers to explore their potential in the recommendation domain. Previous studies have primarily focused on enhancing the capabilities of either recommendation agents or user agents independently, but have not considered the interaction and collaboration between recommendation agents and user agents. To address this gap, we propose a novel framework named FLOW, which achieves collaboration between the recommendation agent and the user agent by introducing a feedback loop. Specifically, the recommendation agent refines its understanding of the user's preferences by analyzing the user agent's feedback on previously suggested items, while the user agent leverages suggested items to uncover deeper insights into the user's latent interests. This iterative refinement process enhances the reasoning capabilities of both the recommendation agent and the user agent, enabling more precise recommendations and a more accurate simulation of user behavior. To demonstrate the effectiveness of the feedback loop, we evaluate both recommendation performance and user simulation performance on three widely used recommendation domain datasets. The experimental results indicate that the feedback loop can simultaneously improve the performance of both the recommendation and user agents.", "sections": [{"title": "1 Introduction", "content": "In recent years, significant efforts have been directed toward de-veloping agents based on large language models (LLMs), aiming to simulate human behavior to enhance performance across a wide range of tasks [21, 27]. These LLM-based agents typically possess memory modules [65], can use tools [58], and perform complex rea-soning [22, 42], enabling them to store memories to assist decision-making, retrieve additional information through tool usage, and apply logical reasoning to handle complex tasks more efficiently.Based on the these advantages, researchers have begun exploringthe potential applications of LLM-based agents in the recommen-dation domain in recent years [19, 50, 60, 62], which is crucial for delivering diverse content and services on the Web [13, 45, 66].Current applications of LLM-based agents in the recommendation field can be primarily summarized into two categories:\n\u2022 The LLM-based recommendation agent [50, 51], which leverages LLM-based agent's strong task-solving and decision-making abil-ities to make recommendations. In detail, the recommendation agent leverages the world knowledge inherent in LLMs and fur-ther enhances its recommendation performance with the LLM-based agent's special capabilities, such as employing tools, logical reasoning, and so on [19, 40, 50, 51].\n\u2022 The LLM-based user agent [48, 60], which utilizes the LLM-based agent's human behavior simulation ability to simulate the user in the recommendation system, can be utilized to interact with the recommender for evaluating its recommendation performance.\nExisting work merely focuses on optimizing the recommendation agent and the user agent independently [6, 38], leading to neglect-ing the importance of the feedback loop between the user and the recommender. In real-world recommendation scenarios, the recom-mender helps the user uncover his/her interests and preferences, while the user, through his/her multi-round interactions with the"}, {"title": "2 Related Work", "content": "In this section, we delve into related studies from three perspectives: LLMs for recommendation, LLM-based agents and agents-based recommendation."}, {"title": "2.1 LLMs for Recommendation", "content": "Due to LLMs' extensive knowledge and strong reasoning capabili-ties, researchers have begun exploring their application in recom-mendation systems [1, 9, 26, 29, 53]. Previous work can be roughlycategorized into using LLMs for feature enhancement and using LLMs for direct recommendation [41].\nFor feature enhancement, some researchers use LLMs to aug-ment raw data, thereby supplying additional information for rec-ommendation models [33, 49, 55]. What's more, several studiespropose to augment traditional models with LLM tokens or em-beddings [17, 18, 31], which can leverage the world knowledge ofLLMs to assist traditional recommendation models in learning the underlying connections between items and users.\nFor direct recommendation, some work directly utilized LLMs'strong capabilities such as reasoning recommendation based on user information and interacted items [7, 15, 30, 43, 46], to fur-ther explore the potential of LLMs in recommendation. Further,some studies propose fine-tuning LLMs on recommendation data, to acquire recommendation-specific knowledge and enhance rec-ommendation performance [4, 63, 64]. However, conducting suchfine-tuning can be costly and may potentially hurt the unique ca-pabilities of LLMs, such as reasoning and planning [1-3]. Thus, wefocus on using LLM-based agents for recommendation in this paper,since they can acquire recommendation knowledge and maintainstrong specific abilities like reasoning for better recommendation."}, {"title": "2.2 LLM-based Agents", "content": "With the rapid advancement of LLMs, the research communityhas increasingly focused on developing agents grounded in LLMscapable of making autonomous decisions within various environ-ments [56]. The architecture of these LLM-based agents generallycomprises key components such as a profile module, memory mod-ule, planning module, and action module, enabling them to emulatehuman-like behavior [47]. Some studies focus on simulating hu-man behavior [32, 37, 48]. For example, SANDBOX [32] simulatesinteractions among various agents to explore social dynamics andautonomously generate data, providing valuable insights into com-plex social phenomena. On the other hand, some work is dedicatedto leveraging LLM-based agents to tackle specific tasks [34, 50, 54].For instance, WebGPT [34] demonstrates an advanced capability tosearch and browse the web while responding to user queries, signifi-cantly improving the relevance and accuracy of its answers throughexternal information retrieval. What's more, LLM-Planner [42]introduces a planning framework based on LLMs for embodiedinstruction following. This approach enables the planner to dynam-ically adjust its strategies and actions in response to environmentalchanges, offering a more adaptable and context-aware solution.Meanwhile, AutoGen [54] pushes the boundaries of multi-agentcollaboration by developing sophisticated mechanisms for definingand assigning more specialized roles to individual agents, allowingfor efficient teamwork in complex task environments."}, {"title": "2.3 Agents-based Recommendation", "content": "Leveraging LLMs, agents possess powerful capabilities such as plan-ning and execution, allowing them to tackle a wide range of complex tasks [21, 27]. Thus, many studies have explored the applicationof LLM-based agents in recommendation systems, which can be roughly divided into two categories: for recommendation and for user simulation.\nThe recommendation agent focuses on tackling recommendation tasks [10, 19, 20, 50, 51, 61]. For example, RecMind [50] enhances performance by employing Self-Inspiring and tool-calling mecha-nisms, and is capable of addressing various recommendation tasks, including sequential recommendation and rating prediction. What's more, MACRec [51] introduces multi-agent collaboration, where different agents are assigned specific roles through role-playing, such as user analyst, searcher and item analyst. RAH [41] positions the agent as an intermediary assistant between multiple traditional recommendation systems and users, building a user-centred framework.\nThe user agent focuses on simulating user behavior [60, 62]. Agent4Rec [60] constructs a user agent with a profile module, memory module, and action module, capable of simulating user behavior such as viewing, rating, and providing feedback, which can assist in testing recommendation systems. RecLLM [11] builds a controllable LLM-based user simulator that can be integrated into the conver-sational recommender system to generate synthetic conversation data. Furthermore, RecAgent [48] introduces multiple user agents, which are capable of simulating multiple users' chatting and broad-casting behavior. What's more, AgentCF [62] constructs user agents and item agents to simulate user-item interactions. However, the existing recommendation agent and user agent approaches have not considered the feedback loop between them, which is a crucial feature in recommendation systems, to simultaneously enhance the performance for both the recommendation and user simulation."}, {"title": "3 Method", "content": "In this section, we will elaborate on the proposed feedback loop framework named FLOW. The overview framework of our proposed FLOW is depicted in Figure 2."}, {"title": "3.1 Overview", "content": "As shown in Figure 2, the FLOW includes: (1) a recommendation agent (Section \u00a73.2), (2) a user agent (Section \u00a73.3), and (3) the feedback loop between the two agents (Section \u00a73.4).\nOverall, the input for the FLOW is the user-item interaction his-tory, which can be formalized as $[I_1, I_2, ..., I_n]$, where $I_i$ represents the i-th item the user interacts with. The output is the FLOW's prediction of the next item $I_{n+1}$ the user will interact with.\nIn detail, the user-item interaction history is utilized to initialize the user agent and serves as input for the recommendation agent. Based on the user-item interaction history, the recommendation agent needs to generate the recommended item and its correspond-ing reasons. Then, the user agent needs to determine whether it likes the recommended item. If the item is favorable, the user agent directly outputs the item, and the process ends. Otherwise, the process enters the iterative feedback loop, where the user agent provides feedback, prompting the recommendation agent to suggest new items."}, {"title": "3.2 Recommendation Agent", "content": "The recommendation agent is powered by GPT-40-mini [35] and is equipped with a memory module and a recommendation module. The memory module can store communication history between the recommendation and user agents, denoted as $M_r$. The recommen-dation module is a flexible and interchangeable recommendation model that can recommend an item, denoted as $I_m$, based on user-item interaction history $[I_1, I_2,\uff65\uff65\uff65, I_n]$. It is worth noting that the recommendation model is trained on the training set, which can provide the recommendation agent with dataset-related recommen-dation knowledge. What's more, to enhance the recommendation agent's performance, we have also adopted role-playing [39] and chain-of-thought [52] approaches, which can help the agent think step by step and provide reasons for its recommendations.\nIn the recommendation phase, the recommendation agent will simultaneously consider memory, user-item interaction history, and the item recommended by the recommendation model to provide a new recommended item and corresponding reason. This can be formally represented as $f_r(M_r, [I_1, I_2, ..., I_n], I_m) = (I_r, R_r)$, where $f_r$ represents the recommendation agent, $I_r$ represents the item recommended by the recommendation agent and $R_r$ represents the reason provided by the recommendation agent. We present a prompt example for the recommendation agent in Table 1."}, {"title": "3.3 User Agent", "content": "The user agent has a memory module $M_u$ similar to the recom-mended agent's and also employs role-playing and chain-of-thought approaches. Unlike the recommendation agent, the user agent is equipped with a reward model that can score and rank items.\nIn detail, we use the SASRec [25] model as the reward model. Through training on the training set, the reward model can generate a score that predicts the user's preference for a given item based on the user-item interaction history. Then, the user agent will consider the reasons for the recommendation, memory, user-item interaction history, and the score from the reward model to determine whether it likes the recommended item and provide reasons. This can be formally represented as $f_u(M_u, [I_1, I_2, ..., I_n], I_r, R_r, S) = (D_u, R_u)$, where $f_u$ represents the user agent, S represents the score given by the reward model, $D_u$ represents the decision of the user agent and $R_u$ represents the reason provided by the user agent. We present a prompt example for the user agent in Table 2."}, {"title": "3.4 Feedback Loop", "content": "We present the pseudocode for the FLOW method in Algorithm 1, where \"Rec_Model\" represents the recommendation model and \"Max_Epoch\" represents the maximum number of iterations for the feedback loop. As mentioned in Section \u00a73.1, from a holistic perspective, the recommendation agent initially suggests an item to the user agent. Next, if the user agent finds the item appealing, the process is finished. However, if the user agent expresses dissatisfac-tion with the item, the process enters an iterative loop where the agents undergo further optimization, continuously refining their behavior based on the user agent's feedback to enhance future interactions.\nIn detail, during the iterative loop phase, both the recommen-dation agent and the user agent begin by storing key information in memory, including the recommended item, the reasons for the recommendation, and the user agent's reasons for rejecting the item. This initial step ensures that both agents have a comprehen-sive record of the interaction for future reference. Next, drawing upon stored memory, the recommendation agent can reanalyze and summarize the user's interests and preferences to optimize its behavior. In particular, the recommendation agent can attempt to persuade the user agent to accept the previously recommended item or suggest another item. On the other hand, the user agent can analyze and extract latent interests from the items and reasons provided by the recommendation agent, enhancing its ability to simulate the user's preferences and behavior. Subsequently, the better user agent can provide feedback on newly recommended items that better align with the user's interests, thereby helping the recommendation agent to further improve.\nAs a result, during the feedback loop, both the recommendation agent and the user agent undergo iterative updates. This iterative refinement process leads to an improvement in the reasoning abil-ities of the recommendation agent and the user agent, enabling them to better recommend items and more accurately simulate user behavior, respectively."}, {"title": "Algorithm 1 FLOW", "content": "1: INPUT:($[I_1, I_2, ..., I_n]$, $f_r$, $M_r$, Rec_Model, $f_u$, $M_u$, Reward_Model, Max_Epoch)\n2: $E \\leftarrow 1$\n3: $I_m \\leftarrow$ Rec_Model($[I_1, I_2,\u2026, I_n]$) \u25ba Utilize the Rec_Model\n4: while $E \\leq$ Max_Epoch do\n5:  $(I_r, R_r) \\leftarrow f_r(M_r, [I_1, I_2,\u00b7\u00b7\u00b7, I_n], I_m)$\n6:  $S \\leftarrow$ Reward_Model($I_r$) \u25ba Utilize the Reward Model\n7:  $(D_u, R_u) \\leftarrow f_u(M_u, [I_1, I_2, ..., I_n], I_r, R_r, S)$\n8:  if $D_u$ is False then\n9:   $M_u \\leftarrow M_u \\cup$ {$I_r$, $R_r$, $R_u$} \u25ba Update the memory\n10:   $M_r \\leftarrow M_r \\cup$ {$I_r$, $R_r$, $R_u$}\n11:   $E\\leftarrow E+1$\n12:  else\n13:   break\n14:  end if\n15: end while\n16: return $I_r$"}, {"title": "4 Experiments", "content": "In this section, we conduct experiments to answer the following research questions (RQ):\n\u2022 RQ1: Can FLOW improve the performance of the recommendation agent?\n\u2022 RQ2: Can FLOW improve the performance of the user agent?\n\u2022 RQ3: How does the number of feedback loop iterations affect performance?\n\u2022 RQ4: How does the recommendation model and reward model affect performance?\n\u2022 RQ5: Whether the recommendation agent has learned the user preference through the feedback loop, rather than relying on the position information?"}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Datasets. We choose three widely used recommendation datasets - Lastfm [5], Steam [24], and MovieLens [14] for our experiments to verify the performance of the recommendation agent and the user agent. We sort the sequences of each dataset according to time and then divide the data into training, validation, and test sets in the ratio of 8:1:1, which ensures that subsequent interactions are excluded from the training data [23]. In practice, the training set and validation set are used for the training and validation of the recommendation agent's recommendation model and the user agent's reward model. Notably, due to the large size of the Steam test set, we randomly sampled 200 data points to align with the number of test samples in the Lastfm and MovieLens datasets. The statistics of these datasets are provided in Table 3, and the detailed information on these datasets is as follows:\n\u2022 Lastfm contains a rich set of user-artist listening records collected from the Last.fm platform.\n\u2022 Steam is a dataset sourced from the Steam store that includes an extensive collection of user reviews for video games. To ensure the dataset size is manageable, we first removed users with fewer than 20 reviews and then randomly selected one-third of the users and one-third of the games, which is the same setting as in the LLARA [28].\n\u2022 MovieLens is a widely used dataset for movie recommendations, which includes user ratings for movies and provides subsets of various sizes. For cost-efficiency in relation to API calls, we decided to limit the dataset size, selecting MovieLens100k for our experiment.\n4.1.2 Evaluation Method. Since FLOW constructs both a recommendation agent and a user agent, our experiments need to evaluate both the recommendation performance and user simulation performance.\nTo evaluate recommendation performance, we adopt the experimental setup of LLaRA [28]. Given a sequence of items a user has interacted with, we first combine the next item the user will interact with and 19 randomly sampled items that the user has not interacted with, forming a candidate list of 20 items. Then, we assess whether the recommendation model can identify the correct item based on the sequence of items the user has interacted with, and its performance is measured using the HitRatio@1 metric.\nTo evaluate user simulation performance, we refer to the experimental setup of Agent4Rec [60]. Each user agent is randomly assigned 20 items. Among these, the ratio between items the user"}, {"title": "4.1.3 Baselines", "content": "We have selected the various models as baselines,which can be categorized into traditional recommendation modelsand LLM-based models.\nThe traditional recommendation models are as follows:\n\u2022 SASRec [25] is an attention-based sequential model that effec-tively captures long-term semantic dependencies in both sparse and dense datasets.\n\u2022 GRU4Rec [16] is an RNN-based model that is relatively simple yet highly efficient.\n\u2022 Caser [44] treats the user's historical behavior sequence as an \"image\" and utilizes CNN to extract local features from this se-quence.\nThe LLM-based models are as follows:\n\u2022 MoRec [59] improves traditional recommendation models by in-corporating modality features of items.\n\u2022 Llama3 [8] is one of the most popular and powerful open-source LLMs.\n\u2022 GPT-40-mini [35] is one of the most powerful commercial models, capable of handling a wide range of complex tasks.\n\u2022 LLaRA [28] utilizes the hybrid item representation to combine LLMs with traditional recommendation models, and it applies a curriculum learning approach to gradually increase the complexity of training."}, {"title": "4.1.4 Implementation Details", "content": "To build agents, we utilized the\"GPT-40-mini-2024-07-18\" API provided by OpenAI. By default,the maximum number of iterations for the feedback loop is set to4. For traditional models, we strictly follow [57], with a learningrate of 0.001, an embedding dimension of 64, and a batch size of256. What's more, we also perform a grid search over the values[1e-3, 1e - 4, 1e \u2013 5, 1e - 6, 1e - 7] to determine the optimal co-efficient for L2 regularization. For LLMs-based models, we followthe LLaRA [28] and train the models for up to 5 epochs with abatch size of 128. Moreover, we use a warm-up strategy, startingthe learning rate at $\\frac{1}{100}$ of the maximum and gradually increasingit with a cosine scheduler during training."}, {"title": "4.2 Recommendation Performance (RQ1)", "content": "In our recommendation performance comparison, the user agent's reward model was consistently set as the SASRec model, while the recommendation agent's recommendation model was varied among different baseline models. This setup allows us to evaluate the performance improvements introduced by FLOW. For convenience, we define the term \"Model\u201d as directly recommending items using the original baseline model without modifications. Then, the term \"Agent\" refers to directly using the recommendation agent equipped with the corresponding baseline model to recommend"}, {"title": "4.3 User Simulation Performance (RQ2)", "content": "We evaluate the performance of the user simulation following the experimental setup described in Section \u00a74.1.2. To ensure consistent and comparable results, we fix both the recommendation model of the recommendation agent and the reward model of the user agent to be the SASRec model. For convenience, we use the term \"Agent\" to refer to directly using the user agent to simulate the user. Meanwhile, the term \"FLOW\" represents using the user agent after the iterative feedback loop to simulate the user. The detailed experimental results are provided in Table 5.\nBased on results demonstrated in Table 5, we have following key observations: Firstly, the user agent equipped with the reward model (SASRec) outperforms the original SASRec model in the ma-jority of scenarios, especially in terms of precision and recall. This indicates that the LLM-based user agent can effectively leverage"}, {"title": "4.4 Impact of Feedback Loop Iterations (RQ3)", "content": "In this section, we investigate the impact of the number of feedback loop iterations. In particular, we fixed the recommendation model and reward model as the SASRec model and evaluated the changes in recommendation performance and user simulation performance on the Lastfm dataset. The recommendation performance and the"}, {"title": "4.5 Impact of Recommendation Model and Reward Model (RQ4)", "content": "To demonstrate the effectiveness of the recommendation model and the reward model, we evaluated the recommendation performance under different settings. The term \"Base\" refers to the setting where the recommendation agent does not have a recommendation model, and the user agent does not have a reward model. The term \"+Rec Model\u201d indicates that only the recommendation agent has a recommendation model, while \"+Reward Model\" indicates that only the user agent has a reward model. It is important that, for a fair comparison, we fix both the recommendation model and the reward model as SASRec models. The detailed experimental results are in Table 6.\nBased on the experimental results shown in Table 6, we can draw several key conclusions: Firstly, the integration of the recommendation model significantly improves recommendation performance, underscoring its effectiveness in improving the recommendation agent's ability to cater to user preferences. What's more, it is worth"}, {"title": "4.6 Preference Learning or Position Reliance? (RQ5)", "content": "Notably, using LLMs for recommendations can be influenced by the position of the items in the sequence. To demonstrate that the improvement in recommendation performance comes from the recommendation agent learning the user's preferences in the feedback loop, rather than relying on position, we construct three settings on the Lastfm dataset: (1) the correct item is in a random position within the list of candidate items, (2) the correct item is the first in the list, and (3) the correct item is the last in the list. The smaller the variation in recommendation performance of the LLM-based agent across the three settings, he less its performance depends on location. In practice, we apply three different settings"}, {"title": "4.7 Case Study", "content": "To illustrate the effectiveness of collaboration between the recommendation agent and the user agent, we present an example in"}, {"title": "5 Conclusion", "content": "In this study, we propose a novel framework called FLOW, which uses the feedback loop to enable collaborative interaction between the recommendation agent and the user agent. In detail, we have simultaneously developed a recommendation agent incorporating a recommendation model and a user agent equipped with a reward model. Within the feedback loop, the user agent evaluates the items suggested by the recommendation agent, providing feedback that enables the recommendation agent to more accurately infer the user's preferences and improve future recommendations. Meanwhile, leveraging the recommended items, the user agent gains deeper insights into the user's interests, thereby refining its ability to simulate user behavior more effectively. We evaluated the recommendation performance and user simulation performance on three widely used recommendation domain datasets. The comprehensive experimental results indicate that the feedback loop can simultaneously enhance recommendation performance and user simulation performance."}]}