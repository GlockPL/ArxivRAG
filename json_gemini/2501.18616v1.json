{"title": "STAMP: SCALABLE TASK- AND MODEL-AGNOSTIC COLLABORATIVE PERCEPTION", "authors": ["Xiangbo Gao", "Runsheng Xu", "Jiachen Li", "Ziran Wang", "Zhiwen Fan", "Zhengzhong Tu"], "abstract": "Perception is a crucial component of autonomous driving systems. However, single-agent setups often face limitations due to sensor constraints, especially under challenging conditions like severe occlusion, adverse weather, and long-range object detection. Multi-agent collaborative perception (CP) offers a promising solution that enables communication and information sharing between connected vehicles. Yet, the heterogeneity among agents\u2014in terms of sensors, models, and tasks-significantly hinders effective and efficient cross-agent collaboration. To address these challenges, we propose STAMP, a scalable task- and model-agnostic collaborative perception framework tailored for heterogeneous agents. STAMP utilizes lightweight adapter-reverter pairs to transform Bird's Eye View (BEV) features between agent-specific domains and a shared protocol domain, facilitating efficient feature sharing and fusion while minimizing computational overhead. Moreover, our approach enhances scalability, preserves model security, and accommodates a diverse range of agents. Extensive experiments on both simulated (OPV2V) and real-world (V2V4Real) datasets demonstrate that STAMP achieves comparable or superior accuracy to state-of-the-art models with significantly reduced computational costs. As the first-of-its-kind task- and model-agnostic collaborative perception framework, STAMP aims to advance research in scalable and secure mobility systems, bringing us closer to Level 5 autonomy.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-agent collaborative perception (CP) has emerged as a promising solution for autonomous systems by leveraging communication among multiple connected and automated agents. It enables agents such as vehicles, infrastructure, or even pedestrians to share sensory and perceptual information, providing a more comprehensive view of the surrounding environment to enhance overall perception capabilities. Despite its potential, CP faces significant challenges, particularly when dealing with heterogeneous agents that defer in input modalities, model parameters, architectures, or learning objectives. For instance, observed that features from heterogeneous agents vary in spatial resolution, channel number, and feature patterns. This domain gap hinders effective and efficient CP, particularly when employing fusion-based approaches.\nTo facilitate collaborative perception among heterogeneous agents often referred to as heterogeneous collaborative perception\u2014one might consider using early or late fusion. However, early fusion requires high communication bandwidth, making it impractical for real-time applications. Late fusion often results in suboptimal accuracy, and it is not viable across models with different downstream tasks. Alternative methods attempt to achieve heterogeneous intermediate fusion by either incorporating adapters or sharing parts of the models. While these approaches can bridge the domain gap, they are limited in scalability or security, rendering them inefficient or unsafe for practical deployment. Additionally, recent studies have highlighted increased security vulnerabilities in CP systems compared to single-agent frameworks."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 MULTI-AGENT COLLABORATIVE PERCEPTION", "content": "Multi-agent CP has emerged as a promising solution to overcome the inherent limitations of single-agent perception systems, particularly in addressing occlusions and extending perception range.\nInformation-sharing schemes. There are three main information-sharing schemes in multi-agent CP systems: early fusion, late fusion, and intermediate fusion. Early fusion involves the direct sharing of raw sensor data, such as LiDAR point clouds or camera images, between agents. This method maximizes information transfer but requires high bandwidth for transmitting. Late fusion involves sharing only final prediction results, such as object detection bounding boxes or occupancy predictions. This approach significantly reduces communication bandwidth overhead, making it more feasible for implementation in real-world systems. However, late fusion often results in suboptimal accuracy due to the loss of intermediate information that could be valuable for collaborative decision-making. Intermediate fusion has emerged as a promising middle ground, involving the sharing of mid-level information, typically in the form of Bird's Eye View (BEV) features. This approach strikes a balance between communication bandwidth efficiency and information richness. Intermediate fusion allows for more flexibility in collaborative processing while maintaining a reasonable data transfer load. However, intermediate fusion faces significant challenges in addressing domain gap issues for heterogeneous agents.\nCollaborative perception datasets. Several significant collaborative perception datasets have emerged recently. The simulated OPV2V and V2X-Sim dataset each contain approximately 10k multi-agent scenes, featuring RGB images and LiDAR point clouds with 3D object detection, tracking, and segmentation annotations. Two real-world DAIR-V2X and V2V4Real datasets provide 39k and 20k dual-agent samples, respectively, with object detection annotations only. For our framework"}, {"title": "2.2 HETEROGENEOUS COLLABORATIVE PERCEPTION", "content": "In a CP system, the heterogeneity of agents can manifest as three different types: heterogeneous modalities, heterogeneous model architectures or parameters, and heterogeneous downstream tasks.\nHeterogeneous modalities. Each model is expected to take input data of different modalities (e.g., RGB images or LiDAR point clouds), requiring different encoders to process the data. propose a hetero-modal vision transformer to fuse heterogeneous BEV features, but this requires end-to-end model training, which is impractical for existing heterogeneous agents. introduce multi-agent perception domain adaptation (MPDA), which aligns feature maps between heterogeneous agent pairs. While effective for collaboration, this method's polynomial complexity limits its scalability as the number of heterogeneous models increases. introduce a backward alignment training strategy, creating heterogeneous models by fixing a base network's decoder and training only the encoders. While this enables collaboration between existing heterogeneous agents, it incurs high computational costs, especially for models with large encoders.\nHeterogeneous model architectures or parameters. Model architectures or parameters may differ across agents, resulting in feature map in different domains, rendering existing heterogeneous intermediate fusion methods inapplicable. However, late fusion methods remain viable as the model output for all models is in the same domain.\nHeterogeneous downstream tasks. The learning objectives are different across agents, which results in model outputs in different domains. propose task-agnostic CP by training models with multi-robot scene completion objectives. Despite the effectiveness of task-agnostic collaboration, their method does not support heterogeneous modality inputs and model architectures."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 PRELIMINARIES: INTERMEDIATE COLLABORATIVE PERCEPTION", "content": "A CP system typically comprises multiple (N) agents, each equipped with its own CP model. This work mainly focuses on intermediate fusion, so we consider all CP models to be trained using an intermediate fusion strategy. The architecture of these models generally consists of an encoder Ei, a compressor \u0256\u017c, a decompressor \u03c8\u2081, a collaborative fusion layer U\u017c, and a decoder Di, where i \u2208 {1, 2, ..., N} represents the agent index.\nThe CP process unfolds as follows: Upon receiving input data Ii, the encoder E\u00bf of agent i transforms this data into a Bird's Eye View (BEV) feature representation Fi. To save transmission bandwidth, each agent uses the compressor \u0256\u017c to compress F\u2081 to F\u2081 before broadcasting them to other agents within a predefined collaborative distance \u03b4. Here, \u03b4 denotes the maximum range for inter-agent collaboration. Each agent collects the BEV features from other agents and uses their own decompressor \u03c8; to decompress Fk to Fk, where agent i and agent k are within the distance \u03b4 for collaboration. Then, the collaborative fusion layer Ui collects and integrates the BEV features from all cooperating agents, producing a consolidated BEV feature F'. Finally, the decoder Di processes this fused feature F' to generate the final model output Oi. This process can be formally described as follows for each agent i \u2208 {1,2,..., N}:\nEncoding: \n```latex\nF_i = E_i (I_i)\n```\nCompression:\n```latex\nF'_i = \\varphi_i (F_i)\n```\nDecompression:\n```latex\nF'_k = \\psi_i (F_j), \\quad j \\in N(i,j) \\leq \\delta\n```\nFusion:\n```latex\nF' = U_i (\\{ F'_j | N(i, j) \\leq \\delta \\})\n```\nDecoding:\n```latex\nO_i = D_i(F')\n```\nwhere N(i, k) refers to the Euclidean distance between the agent i and agent k."}, {"title": "3.2 FRAMEWORK OVERVIEW", "content": "Our proposed framework, STAMP, enables collaboration among existing heterogeneous agents with-out sharing model details or downstream task information. We replace the compression (Equation 2)"}, {"title": "3.3 COLLABORATIVE FEATURE ALIGNMENT", "content": "We propose the Collaborative Feature Alignment (CFA) module to train a unified BEV feature representation and a local adapter-reverter pair. As illustrated in Figure 1 (I), before CFA, het-"}, {"title": "3.4 ADAPTER AND REVERTER ARCHITECTURE", "content": "To bridge the domain gap between heterogeneous agents, we propose a flexible architecture for both the adapter & and reverter \u03c8. This architecture addresses three main sources of domain gap caused by agent heterogeneity, as identified by : spatial resolution, feature patterns, and channel dimensions. Our design employs simple linear interpolation for spatial resolution alignment, three ConvNeXt layers with hidden channel dimension Chidden for feature pattern alignment, and two additional convolutional layers for channel dimension alignment (input: Cin \u2192 Chidden, output: Chidden \u2192 Cout). For the model architecture details, please refer to Appendix A.1.\nNote that this high-level architecture is flexible and open to various implementations. In Section 4.4, we evaluate alternative approaches for feature pattern alignment, demonstrating our framework's flexibility across different specific implementations."}, {"title": "4 EXPERIMENTS", "content": "Our STAMP framework enables collaboration among agents with heterogeneous modalities, models architectures, and downstream tasks without sharing model or task information. We first compare our framework with existing heterogeneous CP frameworks in Section 4.2. Given that no previous work supports simultaneous task- and model-agnostic heterogeneous collaboration, we concentrate our evaluation on the 3D object detection task. This focus ensures a fair comparison across two key dimensions: object detection average precision, trainable parameters and GPU hours required for training. Next, in Section 4.3, we demonstrate our framework's unique capability in a task- and model-agnostic setting, evaluating its performance using four existing collaborative models with heterogeneous architectures and downstream tasks. Then, we present ablation studies on channel sizes, model architectures, and loss functions in Section 4.4 to further analyze our framework's design choices. Finally, we present some feature and output visualization in Section 4.5."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Our experiments utilize two CP datasets: the simulated OPV2V dataset and the real-world V2V4Real dataset. We employ both datasets in Section 4.2 and Section 4.4 for method comparison and ablation studies. The task- and model-agnostic evaluation in Section 4.3 uses only OPV2V due to its multi-task annotations. This combination leverages the scale and diversity of simulated data with the realism of real-world data, ensuring comprehensive model evaluation.\nImplementation details. We use different setups for 3D object detection (Section 4.2) and task-agnostic settings (Section 4.3), detailed within each section. Unless using end-to-end training, local and protocol models are trained for 30 epochs using Adam optimizer . For end-to-end training, we use ItersN = 30N epochs, where N is the number of heterogeneous models, to ensure all models receive the same amount of supervision. Local adapters & and reverters \u03c8 are trained for 5 epochs. We set loss scaling factors \u03bbadapt = \u03bbrevert = \u03bbdadapt = \u03bbdrevert = 0.5 empirically.\nFor additional details, please refer to Section 4.2, Section 4.3, and Appendix A.1."}, {"title": "4.2 HETEROGENEOUS COLLABORATIVE PERCEPTION FOR 3D OBJECT DETECTION", "content": "Performance comparison. We compare our method with existing heterogeneous CP approaches on the 3D object detection task. We select two late fusion methods (vanilla late fusion and calibrator and two intermediate fusion methods (end-to-end training and HEAL for comparison. Late fusion methods offer a simple way to mitigate domain gaps in collaborative 3D object detection. propose using a calibrator to address residual domain gaps in late fusion, which arise from differences in training data and procedures among heterogeneous models. For intermediate fusion, end-to-end training of all heterogeneous models together allows collaboration during the training stage to bridge domain gaps. introduce a backward alignment technique, first training a base network, then fixing its decoder while train-ing only the encoders to create heterogeneous models. An architectural comparison between these frameworks and our proposed STAMP framework is illustrated and visualized in Appendix A2.\nWe prepared 12 heterogeneous local models (six with LiDAR modality and six with RGB camera modality) and one protocol model with LiDAR modality (details in Appendix A.1). Each agent has a visible range of 51.2m \u00d7 51.2m square units. Considering that most samples of the OPV2V dataset contain no more than four agents, we only select the first four models for evaluation on the OPV2V dataset. Similarly, we select the first two models for the V2V4Real dataset since it has two agents for each sample. All 12 models are used for efficiency comparison.\nFor the OPV2V dataset, we simulate real-world noise by adding Gaussian noise with standard de-viations \u03c3 = {0.0,0.2,0.4} to the agents' locations. As shown in Table 2, late fusion methods underperform as the number of agents increases, with performance degrading further at higher noise levels (\u03c3 = 0.4). This is particularly evident when camera agents (agents 3 and 4) are involved, highlighting the late fusion methods' vulnerability to bottleneck agents' incorrect predictions. Our framework demonstrates superior or comparable performance to other heterogeneous fusion meth-ods across all noise levels."}, {"title": "4.3 MODEL- AND TASK-AGNOSTIC FUSION", "content": "In this section, we evaluate our proposed framework's performance in a task-agnostic setting us-ing the OPV2V dataset. To simulate agent heterogeneity, we assign four agents with diverse input sensors, learning objectives, and evaluation metrics, equipping them with various backbones and fusion models. Agent 1 was equipped with a SECOND encoder and a window attention fusion module . For Agent 2, we implemented an EfficientNet-b0 encoder, while Agents 3 and 4 were equipped with PointPillar encoders. Agents 2, 3, and 4 all utilized the Pyramid Fusion module. Table 4 summarizes these models' key characteristics. We compare our STAMP framework against two baseline scenarios: non-collaborative (single-agent perception without information sharing) and collaborative without feature alignment (performing intermediate fusion despite domain gaps). Table 4 presents the evaluation results on the OPV2V dataset, with added Gaussian noise (standard deviations \u03c3 = {0.0, 0.2, 0.4}) to the agents' locations."}, {"title": "4.4 ABLATION STUDIES", "content": "In this section, we conduct ablation studies on three factors that may affect our pipeline's perfor-mance: BEV feature channel size, adapter & reverter architectures, and loss functions for collabora-tive feature alignment. All experiments are conducted on both the OPV2V and V2V4Real datasets.\nBEV feature channel size. Changing the BEV feature channel size is essentially a form of feature compression, which is crucial for con-trolling communication bandwidth in multi-agent collaboration systems. Our collaborative feature alignment module inherently supports feature compression by adjusting the protocol BEV feature's channel size. We experiment with two channel sizes for the protocol BEV feature, 32 and 16, and compare their perfor-mance to our standard implementation with a channel size of 64 . Surpris-ingly, reducing the channel size results in only minor performance changes for both datasets, revealing our model's resilience to high BEV feature compression rates.\nAdapter & reverter architecture. We evaluate two alternative architectures for the adapter and re-verter-a single 1 \u00d7 1 convolutional layer and three self-attention layers-compared to our standard implementation of three ConvNeXt layers. The results demonstrate that performance is not highly"}, {"title": "4.5 VISUALIZATION", "content": "Figure 5 illustrates the impact of our CFA method on feature maps and output results across vari-ous tasks. We visualize feature maps by averaging each channel of the fused feature map, Fi', to a (W, H) shape and plotting in a grayscale. Without CFA, the fused feature maps appear noisy and lack critical information for downstream tasks, leading to poor output results. In contrast, CFA sig-nificantly enhances feature preservation, resulting in clearer feature maps and more accurate outputs across different tasks. This visualization demonstrates CFA's effectiveness in maintaining essen-tial information during the fusion process, which directly translates to improved performance in CP tasks. More comprehensive visualization results are shown on the Appendix A.5."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce STAMP, a scalable, task- and model-agnostic multi-agent collaborative perception framework. This framework simultaneously addresses three aspects of agent hetero-geneity: varieties in modalities, model architectures, and downstream learning tasks. By utilizing lightweight adapter-reverter pairs, STAMP enables efficient collaborative perception while main-taining high security, scalability, and flexibility. Experiments on both the simulated OPV2V dataset and the real-world V2V4Real datasets demonstrate its superior performance and computational ef-ficiency over existing state-of-the-arts. This approach opens new avenues for developing more reli-able, efficient, and secure collaborative systems in future autonomous driving applications.\nLimitations. Our experiments revealed a bottleneck effect in Collaborative Perception (CP), where the performance of the weakest agent constrains the overall system performance. This finding under-scores the necessity for multi-group collaborative systems, where agents communicate only within defined groups. Such systems could mitigate the bottleneck effect by allowing for more selective collaboration. In Appendix A.4, we provide a more detailed discussion of multi-group collaborative systems and the advantages of our framework in this context."}, {"title": "Reproducibility statement.", "content": "To ensure the reproducibility of our results, we have provided detailed information about our experimental setup, including dataset descriptions, model architectures, and training procedures in the main text and appendices. We encourage researchers to refer to Appendix A.1 for more implementation details. We also release the codebase at https://github.com/taco-group/STAMP."}, {"title": "Ethics statement.", "content": "Our task- and model-agnostic framework enhances local model security, reduc-ing risks like model stealing and adversarial attacks. While limiting model sharing improves security, as assessed by , we recognize the need for further security analysis. We advocate for collaboration with experts to rigorously evaluate and strengthen our approach in order to contribute to safer and more trustworthy autonomous driving systems and advance privacy in collaborative perception."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 IMPLEMENTATION DETAILS", "content": "For training all models, we initialize the learning rate at 0.001 and reduce it by a factor of 0.1 at 50% and 83% of the total epochs. We utilize a single NVIDIA RTX A6000 GPU for both model training and inference. Training time for each model varies between 7 to 30 GPU hours, de-pending on the specific model architecture. For adapters and reverters, we start with a learning rate of 0.01, reduc-ing it by a factor of 0.1 after the first epoch. These com-ponents are trained in pairs, requiring 1 to 5 GPU hours depending on the specific encoder and decoder architec-tures.\nAdapter and Reverter's Architecture. We use the same architectures for both adapters and reverters across all CP models, as visualized in Figure A1. The dimension of the broadcasting feature map is set to (128, 128, 64). Chidden is set to be 64. Win, Hin, Cin, Wout, Hout, and Cout of adapters and reverters vary according to the feature dimensions of each local model and the broadcasting feature map dimension. For instance, in the task- and model-agnostic setting, Agent 1's feature dimension is 128 \u00d7 128 \u00d7 64, so we set (Win, Hin, Cin) = (Wout, Hout, Cout) = (128, 128, 64) for both its adapter and reverter. For Agent 2, with a feature dimension of 64 \u00d7 64 \u00d7 256, we configure the adapter with (Win, Hin, Cin) = (64, 64, 256) and (Wout, Hout, Cout) = (128, 128, 64), while the reverter is set with (Win, Hin, Cin) = (128, 128, 64) and (Wout, Hout, Cout) = (64, 64, 256)."}, {"title": "A.2 ARCHITECTURAL COMPARISON BETWEEN EXISTING FRAMEWORKS", "content": "Figure A2 illustrated various frameworks that address heterogeneous CP. Late fusion simply com-bines agent outputs through post-processing. Calibrator enhances this approach by using calibrators to address domain gaps between heterogeneous agent outputs. End-to-end train-ing, while effective, lacks scalability due to its requirement of re-training all agents' models. It also compromises security and task flexibility by shared fusion models and decoders. HEAL improves upon this by fixing decoders and fusion models, re-training only the encoders, re-ducing training resources but still facing scalability issues due to the computational cost of encoder retraining as well as the security issue due to the shared fusion models and decoders. Our proposed framework, STAMP, introduces a novel approach using lightweight adapter and reverter pairs to align feature maps for collaboration. The lightweight nature of these components ensures scalabil-ity, while the maintenance of local fusion and decoders ensures both security and task agnosticism. This design effectively addresses the limitations of previous methods."}, {"title": "A.3 ADDITIONAL EXPERIMENTS", "content": ""}, {"title": "A.3.1 DIFFERENT PROTOCOL MODELS", "content": "We conducted complementary experiments comparing different protocol model designs, analyzing variations in both encoder types and downstream tasks."}, {"title": "Impact of Model Objectives", "content": "The experimental results shown in Table A2 demonstrate that the alignment's success significantly depends on the learning objectives between protocol models and agent architectures. When there is strong alignment between the protocol model and an agent's ob-jectives, we observe performance improvements. For examples, A camera-modality protocol model improves camera-based Agent 2's performance from 0.760 to 0.777; a dynamic-segmentation pro-tocol model enhances Agent 4's performance from 0.690 to 0.723. Similarly, a static-segmentation protocol model boosts Agent 3's performance from 0.624 to 0.681.\nHowever, significant objective mismatches can lead to severe performance degradation. For in-stance, using a static-segmentation protocol model causes Agent 4's mAP to drop dramatically from 0.690 to 0.235. This highlights the importance of careful protocol model selection.\nEncoder Architecture Variations While our baseline experiments primarily used CNN-based en-coders, we explicitly tested different encoder architectures to understand their impact. As shown in our results table, we evaluated: CNN-based encoders and Point-transformer encoders.\nThe Point-transformer protocol model outperforms the original CNN-based protocol model, show-ing our framework's compatibility with different encoder architectures. Notably, the Point-transformer protocol model achieved slightly superior performance (AP@50 = 0.991) compared to its CNN-based counterpart (AP@50 = 0.973). This observation suggests an important insight: the overall performance of the protocol model is more crucial than its specific architectural design. In other words, a well-performing protocol model tends to benefit all agent types, regardless of their individual architectures."}, {"title": "A.3.2 ADVERSARIAL ROBUSTNESS EVALUATION", "content": "We conducted adversarial attack experiments on the object detection task, following the setup in Section 4.2. Following James et al. Tu et al. (2021)'s collaborative white-box adversarial attack method with the same hyperparameters, we tested on the V2V4Real dataset with two agents per scene. We designated agent 1 as the attacker and agent 2 as the victim, comparing three settings:\nEnd-to-end training: Models trained end-to-end with full parameter access, enabling direct white-box attacks on the victim.\nHEAL: Agents share encoders but have different fusion models/decoders, assuming no victim model access.\nSTAMP: Agents share no local models, using protocol representation for communication, assum-ing no victim model access.\nThe results shown in table A3 demonstrate that adversarial attacks have minimal impact on HEAL and STAMP frameworks due to local model security, while significantly degrading performance in end-to-end training where models are shared. This empirically supports our framework's robustness against malicious agent attacks."}, {"title": "A.3.3 MORE COMPARISON WITH THE STATE-OF-THE-ART METHODS", "content": "We conducted additional experiments comparing with V2X-ViT, CoBEVT, HM-ViT, HEAL in a heterogeneous input modal-ity setting. We configured four agents: two LiDAR agents using PointPillar and SECOND encoders, and two camera agents using EfficientNet-b0 and ResNet-101 encoders. For CoBEVT, HM-ViT, and HEAL, we followed their standard architecture and hyper-parameter setup. V2X-ViT does not sup-port camera modality, so we follow HEAL to use ResNet-101 with Split-slat-shot for encoding RGB images to BEV features. For STAMP, we used pyramid fusion layers and three 1 \u00d7 1 convolutional layers (for classification, regression, and direction) across all heterogeneous models."}, {"title": "A.4 MULTI-GROUP AND MULTI-MODEL COLLABORATIONS SYSTEM", "content": "In our experimental findings, we observed a bottleneck effect in CP systems, where the overall sys-tem performance is constrained by the capabilities of the weakest agent. This limitation underscores"}, {"title": "More selective collaboration", "content": "the need for more selective collaboration, leading us to introduce the concept of a Collaboration Group a set of agents that collaborate under specific criteria. These criteria are essential for maintaining the quality and integrity of CP, admitting agents that meet predefined standards while excluding those with inferior models, potential malicious intent, or incompatible alignments. As illustrated in Figure A3, we can distinguish between three collaborative system types:\nSingle-group systems, where agents either operate independently or are compelled to collaborate with all others, are susceptible to performance bottlenecks caused by inferior agents and vulnera-bilities introduced by malicious attackers.\nMulti-group single-model systems, allowing multiple collaboration groups but restricting agents to a single group because each agent can only equip a single model.\nMulti-group multi-model systems, enabling agents to join multiple groups if they meet the prede-fined standards.\nThe multi-group structure offers significant advantages over traditional single-group systems. It en-hances agents' potential for diverse collaborations, consequently improving overall performance. This approach mitigates the bottleneck effect by allowing high-performing agents to maintain ef-ficiency within groups of similar capability while potentially assisting less capable agents in other groups. Furthermore, it enhances system flexibility, enabling dynamic group formation based on specific task requirements or environmental conditions.\nHowever, implementing such a multi-group system poses challenges for existing heterogeneous collaborative pipelines. End-to-end training approaches require simultaneous training of all models, conflicting with the concept of distinct collaboration groups. Methods like those proposed by require separate encoders for each group, becoming impractical as the number of groups increases due to computational and memory constraints.\nOur proposed STAMP framework effectively addresses these limitations, offering a scalable solu-tion for multi-group CP. The key innovation lies in its lightweight adapter and reverter pair (approx-imately 1MB) required for each collaboration group an agent joins. This efficient design enables agents to equip multiple adapter-reverter pairs, facilitating seamless participation in various groups without significant computational overhead. The minimal memory footprint ensures scalability, even as agents join numerous collaboration groups, making STAMP particularly well-suited for multi-group and multi-model collaboration systems."}, {"title": "A.5 MORE VISUALIZATION RESULTS", "content": "Figure A4 and A5 illustrate more feature map and result visualizations before and after collaborative feature alignment (CFA). Prior to CFA, agents' feature maps exhibit disparate representations. For instance, in Figure A4, the pre-fusion feature maps of agents 1, 3, and 4 appear entirely black, indicating a significantly lower scale compared to agent 2's feature map. This discrepancy leads to instability in feature fusion. Post-CFA, the features are aligned to the same domain, resulting in more coherent fusion and accurate inference outputs."}]}