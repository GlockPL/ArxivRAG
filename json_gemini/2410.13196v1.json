{"title": "Context-Enhanced Multi-View Trajectory Representation Learning: Bridging the Gap through Self-Supervised Models", "authors": ["Tangwen Qian", "Junhe Li", "Yile Chen", "Gao Cong", "Tao Sun", "Fei Wang", "Yongjun Xu"], "abstract": "Modeling trajectory data with generic-purpose dense representa-tions has become a prevalent paradigm for various downstream applications, such as trajectory classification, travel time estimation and similarity computation. However, existing methods typically rely on trajectories from a single spatial view, limiting their ability to capture the rich contextual information that is crucial for gaining deeper insights into movement patterns across different geospatial contexts. To this end, we propose MVTraj, a novel multi-view modeling method for trajectory representation learning. MVTraj in-tegrates diverse contextual knowledge, from GPS to road network and points-of-interest to provide a more comprehensive under-standing of trajectory data. To align the learning process across multiple views, we utilize GPS trajectories as a bridge and employ self-supervised pretext tasks to capture and distinguish movement patterns across different spatial views. Following this, we treat tra-jectories from different views as distinct modalities and apply a hierarchical cross-modal interaction module to fuse the represen-tations, thereby enriching the knowledge derived from multiple sources. Extensive experiments on real-world datasets demonstrate that MVTraj significantly outperforms existing baselines in tasks associated with various spatial views, validating its effectiveness and practical utility in spatio-temporal modeling.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancement of web-based mobile and ubiquitous computing technologies, the acquisition of trajectory data has be-come increasingly prevalent, creating significant opportunities for analytical and decision-making processes in urban spaces [2, 39, 40, 47]. To harness the potential of the growing data availability, the re-search of trajectory representation learning has gained considerable attention in recent years. Trajectory representation learning aims to derive effective vector representations for trajectories that can be applied in various downstream tasks, such as travel time estima-tion [7, 26, 28], trajectory classification [17, 27, 37], and similarity computation [3, 24, 49]. The learned representations offer flexibility and transferability by minimizing the need for task-specific model design, while maintaining competitive performance.\nAlthough substantial progress has been made for trajectory rep-resentation learning, most existing methods focus on deriving rep-resentations within a specific spatial view. For example, previous research has proposed various methods for learning trajectory rep-resentations based on the view of GPS points [25, 51], route within road networks [7, 43], and check-in sequences from POIs [5, 37]. However, these methods leverage limited information from a sin-gle perspective, failing to capture the broader insights that can be reflected in other spatial views.\nIn light of this, it is promising to integrate the complementary knowledge hidden in multiple spatial views for trajectory repre-sentation learning. Specifically, we aim to associate trajectories with three types of context information which manifests different semantic perspectives: from detailed GPS points to aligned road segments and POIs. They collectively provide insights from three key aspects, namely high resolution positions, network constraints, and region functionalities. Together, these spatial views provide a comprehensive and multi-dimensional understanding of movement behaviors, and thus enhance the development of more effective representations across views. For example, in the task of travel time estimation, the road network captures underlying structured topological information, while POIs reflect latent region attributes (e.g., commercial area), thus contributing to improved prediction ac-curacy. However, this integration poses several critical challenges.\nThe first challenge is multi-view alignment. Trajectories in differ-ent spatial views exhibit variations in their structure and format. In particular, trajectories in GPS form provide fine-grained spatial co-ordinates with detailed kinematic attributes but lack semantic and topological information. In contrast, trajectories aligned with road segments capture topological constraints and transition patterns, while trajectories associated with POIs offer rich semantic context"}, {"title": "2 Related Work", "content": "In this section, we first review existing research on trajectory rep-resentation learning, followed by a discussion of studies on self-supervised learning approaches.\n2.1 Trajectory Representation Learning\nModeling trajectories and deriving their representations have be-come fundamental paradigms in the context of deep learning. One direction is to derive effective trajectory representations for specific downstream tasks using supervised signals, such as trajectory simi-larity computation [24, 42, 49], travel time estimation [11, 31, 48], or route recovery [8, 22, 33]. While these methods demonstrate com-petitive performance within their respective domains, they suffer from limited generalization capabilities and often exhibit degraded performance when applied to other trajectory-specific tasks [26].\nTo this end, recent methods have focused on developing generic-purpose trajectory representations that are transferrable across various downstream tasks. These methods can be broadly catego-rized based on the spatial views of the trajectory data they model. Some approaches model directly on raw GPS trajectories, such as TrajFormer [25], which captures fine-grained movement patterns using raw GPS data to provide detailed kinematic attributes. How-ever, raw GPS data often contains redundancy and noise, and urban trajectories are typically constrained by the road network. Con-sequently, another line of research employs road network-based methods, converting GPS trajectories into route trajectories via map-matching to better reflect the underlying network structure and transition patterns [7, 17, 29, 43]. Additionally, some meth-ods propose to model POI trajectories, utilizing the rich semantic information associated with POIs to provide rich contextual knowl-edge than raw GPS data [5, 37]. Despite these advancements, few studies have tackled the problem of multi-view joint modeling for trajectories. One recent attempt is JGRM [28], which integrates GPS and route trajectories to capture road network constraints and modality interactions, but it does not fully exploit the information available from POIs. In contrast, our method, MVTraj, introduces a novel multi-view framework that integrates not only GPS and route views but also POIs to enrich trajectory representations. MV-Traj addresses challenges such as data misalignment and structural differences by using GPS trajectories as a bridge to synchronize diverse data views. This method provides a more comprehensive understanding of spatio-temporal dynamics in trajectory data.\n2.2 Self-supervised Learning\nIn recent years, self-supervised learning (SSL) has been widely adopted in geospatial domain, facilitating the development of rep-resentations for POIs [9, 15, 36], urban regions [16, 18, 38], and trajectories [1, 26, 50]. The interest of applying SSL techniques to geospatial data is driven by their intrinsic advantages to derive"}, {"title": "3 Problem Formulation", "content": "In this section, we introduce the fundamental definitions and the problem formulation in this paper.\nDefinition 1. (Trajectory). A trajectory T of length |T| is defined as a sequence of sampled points, denoted as $T = \\{(pos_i, t_i)\\}_{i=1}^{|T|}$, where $pos_i$ and $t_i$ represent the geographical position and times-tamp at the i-th sample point in the sequence.\nBuilt on this basic form, GPS trajectories can be enriched with various context information, serving as a bridge to multiple spa-tial views, each providing a distinct semantic perspective [32]. In this work, we consider three specific views representing different data modalities: (1) The GPS point view p, which offers the high-est spatial resolution; (2) The road network view r, which aligns movements with road segments within a network structure; (3) The grid view g, which aggregates movements into coarser, grid-based urban areas.\nDefinition 2. (GPS Trajectory). A GPS trajectory $T^P$ is a sequence of sampled GPS points, denoted as:\n$T^P = \\{(lat_i, lon_i, t_i)\\}_{i=1}^{|T^P|}$\nwhere $lat_i$ and $lon_i$ refer to the latitude and longitude at the i-th point, and $t_i$ is the associated timestamp.", "title2": "Definition 3. (Route Trajectory).", "content2": "A route trajectory $T^r$ is a sequence of adjacent road segments, denoted as:\n$T^r = \\{(v_i, t_i)\\}_{i=1}^{|T^r|}$\nwhere $v_i \\in V$ is the i-th road segment in the trajectory, and $t_i$ represents its corresponding arrival timestamp. V is a set of vertices representing road segments in the road network $G = (V, E)$, and $E \\subseteq V \\times V$ is a set of edges representing intersections between road segments,\nTo obtain a route trajectory $T^r$, the GPS trajectory $T^P$ is aligned with the road network G through the map-matching algorithm [41].\nDefinition 4. (Grid Trajectory). A grid trajectory $T^g$ is a se-quence of traversed grid cells, denoted as:\n$T^g = \\{(grid_i, sem_i, t_i)\\}_{i=1}^{|T^g|}$\nwhere $grid_i$ is the i-th grid cell in the trajectory, $sem_i$ represents the semantic representation associated with the grid cell, and $t_i$ is the arrival timestamp of that cell.\nThe grid trajectory $T^g$ is derived from the GPS trajectory $T^P$ by partitioning the geographic space of interest into uniform, non-overlapping grid cells. Since points-of-interest (POI) naturally ex-hibit semantic characteristics and reflect the functional role of a region, we propose utilizing POIs to derive the semantic represen-tation $sem_i$ for each grid.\nThese three spatial views capture distinct and complementary aspects of the contextual meaning in traversed locations, not only for trajectories but also for the data modality specific to each view. The GPS trajectory $T^P$ provides position information with the highest spatial resolution but lacks inherent semantic context. The route trajectory $T^r$ emphasizes the structural and transition pat-terns of movement within road segments. The grid trajectory $T^g$ offers an enriched understanding by incorporating POIs to reflect the functional roles of different areas. Together, these views, with the trajectory as the integrating factor, provide a comprehensive and multi-dimensional context that supports better representation learning for the data modality in each spatial view.\nProblem Statement. Given a trajectory dataset $D = \\{T_i\\}_{i=1}^{|D|}$, our target is to learn generic-purpose representations for trajectories, road segments and grid cells that can be effectively utilized in a variety of downstream tasks across these views. The learned representations require minimal task-specific adaptations, ensuring flexibility and broad applicability across tasks."}, {"title": "4 Methodology", "content": "In this section, we introduce the overview of MVTraj and describe the details of its components.\n4.1 Overview\nThe framework of MVTraj, illustrated in Fig. 2, is designed to treat trajectories as data-centric entities, and leverage their contextual information embedded in multiple spatial views to derive effective representations that can be applied to tasks associated with each respective spatial view."}, {"title": "4.2 Multi-view Trajectory Integration Module", "content": "This module consists of three trajectory encoders, each dedicated to a different spatial view, and a cross-view trajectory alignment component. The encoders are responsible for generating trajectory representations from three different spatial views: route, grid, and GPS. Once these representations are obtained, the cross-view align-ment component ensures that the informative patterns from each spatial view are captured in their respective representations. At the same time, it aligns these representations in a shared latent space while preserving distinct characteristics associated with each spatial view.\n4.2.1 Trajectory Encoders. There are three specialized encoders for each spatial view, i.e., route encoder, GPS encoder, and grid encoder. Each encoder is designed to capture unique features related to its respective spatial view, including its spatial properties (e.g., road networks, GPS points, and grid cells) and temporal dynamics (e.g., travel times and periodicity).\nFor the route view, the route encoder processes trajectories con-strained by the road network and dynamic traffic conditions. To capture the spatial structure of the road network, the graph atten-tion network [35] is employed for updating road segment embed-dings based on observed trajectories traversing the network. The embedding update for each road segment $v \\in V$ is represented by:\n$v_o = \\sigma(\\sum_{u \\in N(v)} a_{vu} g_u W)$\nwhere $N(v)$ denotes the neighboring road segments of v, $a_{vu}$ is the learned attention coefficient between road segments v and u, $g_u$ de-notes the initial embedding of road segment u, and W is a learnable weight matrix. The non-linearity is introduced by the activation function $\\sigma$ allowing the model to capture complex relationships between connected road segments. The temporal aspect of each road segment is characterized by both discrete features (e.g., minute of the day, day of the week) and continuous features (e.g., actual travel time). The final temporal representation $t_v$ for each road segment is computed by summing these discrete and continuous feature components. Specifically, the temporal embedding is added to the spatial embedding to form a complete road segment repre-sentation $r_v = z_v + t_v$. To model the dependencies between road segments along a given route, we utilize a transformer-based archi-tecture. The segment-level representations and the trajectory-level representation are obtained as follows:\n$h_\\gamma^r, h_s^r = RouteEncoder(T^r, r_v)$\nwhere $h_s^r$ represents the segment-level (i.e., token) representations of the road segments within the route, and $h_\\gamma^r$ represents the aggregated trajectory-level representation obtained through a mean pooling operation over the segment-level representations.\nFor the GPS view, the GPS encoder processes trajectories with GPS points in a hierarchical structure. Specifically, GPS trajectories are divided into sub-trajectories where points correspond to either"}, {"title": "4.2.2 Cross-view Trajectory Alignment.", "content": "Building on the trajectory encoders, which generate trajectory representations that capture the unique patterns in each spatial view, we further ensure these representations are consistent in the latent space across different spatial views for the subsequent fusion of their distinct charac-teristics in each view. To achieve this, we adopt techniques from cross-modal retrieval studies [20], which applies contrastive learn-ing to align trajectory representations across spatial view pairs: (1) route trajectories ($T^r$) and GPS trajectories assigned to road segments ($T^{P|r}$). (2) grid trajectories ($T^g$) and GPS trajectories assigned to grid cells ($T^{P|g}$). (3) GPS trajectories assigned to road segments ($T^{P|r}$) and assigned to grid cells ($T^{P|g}$).\nWe adopt contrastive learning to distinguish the positive trajec-tory pairs that represent the same underlying trajectories across different views, from negative pairs, which are randomly sampled. The loss for a trajectory $T^a$ between its representations from view i and j is expressed as:\n$L_{pair}^{i,j}(T^a) = -log \\frac{exp(sim(h_i^a, h_j^a) / \\tau)}{\\sum_{T^b \\in B} exp(sim(h_i^a, h_j^b) / \\tau)}$\nwhere $T^b$ represents other trajectories in the batch B, $sim(\\cdot)$ is the cosine similarity between two representations, and $\\tau$ is a tempera-ture hyperparameter."}, {"title": "Total multi-view", "content": "Then total multi-view alignment loss for a trajectory $T^a$ is the aggregation of the three trajectory view pairs:\n$L_{align}(T^a) = L_{pair}^{r,P|r}(T^a) + L_{pair}^{g,P|g}(T^a) + L_{pair}^{P|r,P|g}(T^a)$\nFinally, the overall loss for the entire batch B is obtained by aver-aging the loss over all trajectories in the batch:\n$L_{align}^{multi} = \\frac{1}{|B|} \\sum_{T^a \\in B} L_{align}(T^a)$"}, {"title": "4.3 Hierarchical Multi-Modal Interaction Module", "content": "After obtaining the representations from the multi-view trajectory integration module, the hierarchical multi-modal interaction mod-ule is designed to effectively fuse knowledge across multiple views. This module treats the four trajectory representations, namely route trajectory $h_\\gamma^r$, GPS trajectory assigned to road segments $h_\\gamma^{P|r}$, grid trajectory $h_\\gamma^g$, and GPS trajectory assigned to grid cells $h_\\gamma^{P|g}$ as distinct modalities, thereby extracting multi-dimensional infor-mation from these views."}, {"title": "4.3.1 Inter-Modal Interaction Attention.", "content": "The four types of trajec-tory representations, derived from different spatial perspectives, are first processed through an inter-modal interaction attention mechanism, which establishes twelve dedicated attention streams to capture pairwise interactions between these modalities.\nFor each modality, three independent attention operations are performed, using the remaining three modalities as keys and val-ues. Specifically, the multi-head attention computation for a given query modality Ta with respect to a key-value modality Tb can be expressed as:\n$O_b^a = MultiHead(T^a, T^b, T^b) = softmax(\\frac{Q_a K_b^T}{\\sqrt{d_k}}) V_b$\nwhere $Q_a = W_q [h_\\gamma^a, h_s^a]$ is the projected query from modal-ity Ta, $K_b = W_k [h_\\gamma^b, h_s^b]$ and $V_b = W_o [h_\\gamma^b, h_s^b]$ are the pro-jected value from modality Tb. For each query modality Ta cor-responding to a particular spatial view, this mechanism is per-formed three times, once for each of the other modalities $T_\\Theta \\in \\{T^r, T^{P|r}, T^g, T^{P|g}\\} \\setminus T^a$, enabling pairwise interaction between two spatial views.\nThe outputs from the three attention streams are aggregated to form the enhanced representation for each modality $O_a = \\sum_{b=1}^3 O_b^a$. By leveraging this process, the model effectively integrates knowl-edge from different spatial views, enriching each modality's rep-resentation with complementary insights from the others. This enables the model to achieve a more comprehensive understanding of movement behaviors and spatial-temporal relationships from multiple aspects."}, {"title": "4.3.2 Global Context Attention.", "content": "Following the inter-modal interac-tion attention module, the enriched representations from all modal-ities are further consolidated using a global context attention mech-anism. In this phase, a shared Transformer is employed to model the global context and interactions across all trajectory modalities. The shared Transformer processes concatenated outputs from inter-modal interaction attention streams, refining fused trajectory"}, {"title": "representations with global information from all spatial views:", "content": "$[E_r, E_{P|r}, E_g, E_{P|g}] = TransEncoder([O_r, O_{P|r}, O_g, O_{P|g}])$\nIn addition, we employ a masked language modeling (MLM) task to train the hierarchical cross-modal interaction module. The MLM task operates by randomly masking portions of the trajectory data and requiring the model to predict the masked elements, thereby forcing the model to learn generalized representations of the tra-jectory views. Formally, the MLM loss is defined as the negative log-likelihood of correctly predicting the masked tokens $m_i$:\n$L_{MLM} = -\\sum_{m_i \\in MT} logP(e_{mi} | E_{ \\setminus m_i})$\nwhere $P(e_{mi} | E_{ \\setminus m_i})$ represents the probability of model correctly predicting the masked token $e_{mi}$ given the unmasked context $E_{ \\setminus m_i}$. $M_T = \\{m_1, m_2, \u2026, m_{|M_T|}\\}}$ denotes the set of masked elements.\nGiven the four trajectory modalities ($T^r, T^{P|r}, T^g, T^{P|g}$), the total MLM loss across all modalities is computed as:\n$L_{MLM}^{multi} = L_{MLM}^{r} + L_{MLM}^{P|r} + L_{MLM}^{g} + L_{MLM}^{P|g}$\nFinally, the total loss combines the multi-view alignment loss $L_{align}^{multi}$ and the MLM loss $L_{MLM}^{multi}$:\n$L = w_1 L_{align}^{multi} + w_2 L_{MLM}^{multi}$\nwhere $w_1$ and $w_2$ are hyperparameters that balance the two losses during training."}, {"title": "5 Experiments", "content": "To evaluate the performance of MVTraj, we conduct extensive experiments to answer the following research questions:\n\u2022 RQ1: How does MVTraj perform compared to state-of-the-art trajectory representation learning models? (Sec. 5.2)\n\u2022 RQ2: How do self-supervised objectives affect the effectiveness of MVTraj? (Sec. 5.3)\n\u2022 RQ3: How does each component of MVTraj contribute to the overall performance? (Sec. 5.4)\n\u2022 RQ4: How efficient is MVTraj in terms of inference time? (Sec. 5.5)\n5.1 Experimental Setup\n5.1.1 Datasets and Preprocessing. We conduct experiments on two real-world datasets, which have been widely used in previous stud-ies on the evaluation of trajectory representations [26, 28, 29, 45], i.e., Chengdu and Xi'an. Each dataset contains taxi trip GPS trajec-tories collected from the central areas of the respective cities.\nTo obtain road-network constrained route trajectories, we col-lect road network data from Open Street Map\u00b9 and perform the map matching [41] procedure to align GPS points with road segments. To derive grid trajectories, we collect POI data\u00b2 and normalize all POIs in the grid cell to represent the semantic information of the grid cell. Due to space constraints, further details are provided in Appendix A.2.1 and Appendix A.2.3."}, {"title": "5.1.2 Downstream Tasks and Evaluation Metrics.", "content": "The model is eval-uated on a total of four tasks across the three spatial views, including two segment-level tasks and two trajectory-level tasks. For each task, we employ a variety of evaluation metrics that measure model performance in the respective tasks. Due to space limitations, the full details of evaluation metrics, including definitions and com-putation methods, are provided in Appendix A.2.2 for thorough understanding.\nSegment-Level Tasks evaluate the road segment representations derived from the route view. In these tasks, representations of the same road segments across different trajectories are averaged to create static representations, which serve as the input data for simple task-specific components (e.g., MLP) [17, 28].\n\u2022 Road Label Classification: This task aims to classify road segments by treating their road types as labels. The four most frequently occurring road types (i.e., primary, secondary, tertiary, and residential) are selected to evaluate the road segment representations. The classification performance is measured using Micro-F1 and Macro-F1 scores.\n\u2022 Road Speed Inference: This task aims to infer the average speed on road segments, derived from GPS trajectory data. Regression performance is measured using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) with the ground-truth data.\nTrajectory-Level Tasks evaluate the effectiveness of trajectory representations using two scenarios. Similarly, in these tasks, the trajectory representations are utilized as the input for simple task-specific components.\n\u2022 Travel Time Estimation: This task aims to predict the travel time for given route trajectories. The input consists of route trajectories with time information masked to pre-vent information leakage. The performance for the predicted travel time is evaluated using MAE and RMSE.\n\u2022 Destination Grid Prediction: This task aims to predict the destination grid for a trajectory based on its representation derived from the grid view. The classification performance is evaluated using top-k accuracy denoted as Acc@k.\nIt is important to note that for these tasks, only the classification or regression components are trained, while all other parameters remain fixed. This ensures that the evaluation reflects the quality of the learned representations, eliminating any confounding influence from further model updates."}, {"title": "5.1.3 Compared Methods.", "content": "To evaluate the effectiveness of our proposed method, following previous studies [17, 28], we compare it against several methods, including the state-of-the-art methods for generic-purpose trajectory representation learning, under the same experimental settings mentioned in the task descriptions:\n\u2022 Random: it initializes trajectory representations randomly to serve as a baseline for comparison.\n\u2022 Word2Vec [30]: it employs the skip-gram model to generate trajectory representations based on co-occurrence statistics.\n\u2022 Node2Vec [13]: it explores the neighborhood of a node through the random walk, thereby capturing both local and global structural properties of the network."}, {"title": "5.2 Performance Comparison (RQ1)", "content": "The performance of various methods is summarized in Table 1 on Xi'an and Chengdu datasets.\nBaseline models like JGRM perform well on various tasks, with JGRM achieving MAE 87.87 in travel time estimation, demonstrat-ing its feature-capturing ability. However, traditional methods such as Word2Vec and Node2Vec struggle in complex tasks that require broader contextual understanding, as they focus on local or se-quential information. For instance, Word2Vec performs adequately in road label classification but falls short in tasks like travel time estimation, which requires global spatial dependencies.\nIn contrast, our proposed model, MVTraj, significantly outper-forms all baselines across tasks. In Chengdu, MVTraj reduces MAE by 81.85% and RMSE by 53.62% in travel time estimation compared to JGRM. This improvement underscores the power of integrating multiple spatial views, allowing MVTraj to better capture intricate spatial patterns in trajectories.\nFurthermore, the incorporation of diverse spatial perspectives not only boosts performance on grid-based tasks, such as destina-tion grid prediction, but also enhances MVTraj's generalization across various tasks and geographical areas. Unlike many baseline models that struggle with grid-based tasks due to their single-view focus, MVTraj's multi-dimensional approach allows it to handle a wider array of challenges effectively. This is reflected in consistent performance improvements in both datasets.\nOverall, these findings demonstrate that MVTraj not only out-performs existing methods but also addresses the limitations of traditional approaches by employing a comprehensive strategy for trajectory representation learning."}, {"title": "5.3 Model Analysis (RQ2)", "content": "We evaluate the impact of self-supervised objectives used for pre-training on model performance by conducting experiments on the travel time estimation task on the Xi'an dataset. Specifically, we compare two strategies for model training:\n\u2022 Pre-train: This strategy corresponds to the original MV-Traj, where self-supervised objectives are employed for pre-training, followed by the fine-tuning only on the regression head on the travel time estimation task.\n\u2022 No Pre-train: This strategy represents the variant trained in an end-to-end manner, where both the trajectory encoders"}, {"title": "5.4 Ablation Study (RQ3)", "content": "We conduct an ablation study to assess the impact of various com-ponents of our method. The following model variants are examined:\n\u2022 w/o Inter-Modal: Omits inter-modal interaction attention.\n\u2022 w/o Grid View: Excludes the grid view, utilizing only the other two views in JGRM.\n\u2022 w/o Align Loss: Removes the alignment loss $L_{align}^{multi}$, which aligns features across spatial views.\n\u2022 w/o MLM Loss: Excludes the masked language modeling loss $L_{MLM}^{multi}$, which captures transition patterns in trajectories.\nExperimental results on the Xi'an (Table 2) and Chengdu datasets confirm that all components contribute positively to model perfor-mance. Due to space constraints, ablation results for Chengdu are provided in Appendix A.3.1, with consistent findings across both segment- and trajectory-level tasks.\nRemoving the inter-modal interaction attention (w/o Inter-Modal) significantly reduces performance, highlighting the importance of modeling interactions between spatial views, as the model struggles to integrate complementary information from GPS, route, and grid views.\nExcluding the grid view (w/o Grid View) severely impacts tasks requiring fine-grained spatial understanding, such as travel time estimation, by weakening the model's ability to capture detailed spatial features in dense urban environments.\nOmitting the alignment loss (w/o Align Loss) leads to a moderate performance drop, indicating that while the model compensates to some extent, feature alignment still enhances prediction accuracy, especially in trajectory-level tasks.\nThe largest performance degradation occurs when the MLM loss is removed (w/o MLM Loss), as this loss is critical for capturing contextual dependencies, improving generalization, and handling noisy or incomplete trajectory data."}, {"title": "5.5 Model Efficiency (RQ4)", "content": "We further test the efficiency of MVTraj with several baseline mod-els in terms of model size, training time, and inference time, as shown in Table 3. While MVTraj exhibits a slightly longer infer-ence time compared to the baseline methods, this is primarily due to the components of integrating multiple spatial views and perform-ing real-time cross-modal interactions. However, it is important to"}, {"title": "6 Conclusion", "content": "In this paper, we introduced MVTraj, a novel self-supervised model for multi-view trajectory representation learning. MVTraj addresses two key challenges in this domain, multi-view alignment and knowl-edge fusion, by using GPS trajectories as a central bridge to integrate three complementary spatial views: GPS coordinates, routes, and POIs. This unified framework enables a more comprehensive under-standing of movement patterns and spatial relationships, enhancing the flexibility and generalizability of the learned trajectory repre-sentations across diverse downstream tasks. Extensive experiments on real-world datasets demonstrate that MVTraj significantly out-performs state-of-the-art methods, highlighting its effectiveness in extracting complementary knowledge from multiple spatial views."}, {"title": "A.1 Key Notions", "content": "To facilitate easy reference, Table 4 summarizes the key notations used throughout the paper."}, {"title": "A.2 Experimental Setting", "content": "A.2.1 Details of Datasets. The datasets used in this study were collected over a period of 15 consecutive days. For both datasets, the first 13 days are used for training, the 14th day for validation, and the 15th day for testing. The statistical details of these datasets are summarized in Table 5.\nEach dataset consists of 13 distinct categories of points-of-interest (POI), representing a diverse range of urban functions. These cate-gories include Dining, Scenery, Public Facilities, Shopping, Trans-portation, Education, Finance, Residential, Life Services, Sports, Healthcare, Government Offices, and Accommodation Services. This broad categorization ensures comprehensive coverage of dif-ferent types of POI commonly found in urban environments."}, {"title": "A.2.2 Details of Evaluation Metrics.", "content": "We employ a variety of evalu-ation metrics to compare the performance of various methods:\n\u2022 Micro-F1: This metric aggregates the contributions of all classes to compute a single overall value. It sums the true positives (TP), false positives (FP), and false negatives (FN) across all classes to calculate overall precision and recall, and then combines them to compute the F1 score:\nMicro F1 = $\\frac{2 \u00d7 Precision_{all} \u00d7 Recall_{all}}{Precision_{all} + Recall_{all}}$\n\u2022 Macro-F1: This metric treats each class equally by calculating the F1 score for each class independently and then averaging these scores. It is given by:\nMacro F1 = $\\frac{1}{N} \\sum_{i=1}^N Fl_i$\nwhere $Fl_i$ is the F1 score for the i-th class, and N represents the total number of classes. This metric is useful when class imbalance is a concern, as it gives equal weight to all classes, regardless of their size.\n\u2022 Mean Absolute Error (MAE): MAE measures the average magnitude of the errors between the predicted and true values, without considering their direction:\nMAE = $\\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y_i}|$\nwhere yi is the true value, \u0177i is the predicted value, and n is the total number of observations. MAE gives a direct interpretation of the average error."}, {"title": "Root Mean Squared Error (RMSE):", "content": "This metric represents the square root of the average squared differences between the predicted and true values. It emphasizes larger errors more than MAE due to the squared term:\nRMSE = $\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2}}$\n\u2022 Accuracy@k (Acc@k): This metric evaluates whether at least one correct prediction exists among the top k predicted items for each instance:\nAcc@k = $\\frac{1}{n} \\sum_{i=1}^n I(y_i \\in \\hat{y_i}^{(k)})$\nwhere yi is the true label, $\\hat{y_i}^{(k)}$ represents the set of top-k predicted labels, and I() is an indicator function, which returns 1 if the true label yi is within the top-k predictions, and 0 otherwise. This metric is particularly useful in ranking tasks or recommendation systems."}, {"title": "A.2.3 Implementation Details.", "content": "All models, including MVTraj and the baseline models, are trained using the AdamW optimizer. The training is conducted for 70 epochs with a batch size of 64.\nThe loss function incorporates weights w\u2081 and w2 as defined in Equation 12, with values consistently set to 2 and 1, respectively, throughout all experiments.\nTo ensure fair and consistent comparisons with prior work [26, 28, 29, 45], we follow the same data preprocessing and splitting procedures. Specifically, we filter out road segments that are not covered by any GPS trajectories. Additionally, we remove trajecto-ries that meet any of the following criteria: containing fewer than 10 or more than 100 road segments, fewer than 10 or more than 100 grid cells, or fewer than 10 or more than 256 GPS points.\nTo further improve model robustness, we implement a masking mechanism during training. This mechanism uses a mask length of 2 and a masking probability of 20%, which helps the model generalize better across diverse trajectory patterns.\nFinally, all hyperparameters and configurations are kept con-sistent across all experiments to ensure comparability and repro-ducibility of results."}, {"title": "A.3 Additional Experiments", "content": "A.3.1 Ablation Study in Chengdu. Table 6 presents the results of the ablation experiments conducted in Chengdu. The findings mir-ror those in Xi'an, showing that MVTraj consistently achieves superior performance compared to its variants."}]}