{"title": "Karatsuba Matrix Multiplication and its Efficient Custom Hardware Implementations", "authors": ["Trevor E. Pogue", "Nicola Nicolici"], "abstract": "While the Karatsuba algorithm reduces the complexity of large integer multiplication, the extra additions required minimize its benefits for smaller integers of more commonly-used bitwidths. In this work, we propose the extension of the scalar Karatsuba multiplication algorithm to matrix multiplication, showing how this maintains the reduction in multiplication complexity of the original Karatsuba algorithm while reducing the complexity of the extra additions. Furthermore, we propose new matrix multiplication hardware architectures for efficiently exploiting this extension of the Karatsuba algorithm in custom hardware. We show that the proposed algorithm and hardware architectures can provide real area or execution time improvements for integer matrix multiplication compared to scalar Karatsuba or conventional matrix multiplication algorithms, while also supporting implementation through proven systolic array and conventional multiplier architectures at the core. We provide a complexity analysis of the algorithm and architectures and evaluate the proposed designs both in isolation and in an end-to-end deep learning accelerator system compared to baseline designs and prior state-of-the-art works implemented on the same type of compute platform, demonstrating their ability to increase the performance-per-area of matrix multiplication hardware.", "sections": [{"title": "I. INTRODUCTION", "content": "The demand for optimized hardware acceleration of general matrix multiplication (GEMM) continues to drive innovation in the field of hardware design for exploiting the inherent parallelism to speed up computation. At a certain point, however, after the known parallelism and system-level optimizations are exhausted and technology scaling slows to a halt, there is an accelerator wall which limits further progress on the implementation side [1]. A less-explored direction for continuing advancement beyond this wall is through reducing the workload at the algebraic level, by computing the same result from a re-arranged compute pattern requiring fewer or cheaper operations to be performed in hardware.\nMultiply-accumulate (MAC) units are commonly the area-dominant computational resource in GEMM and deep learning accelerators [2], [3], [4], and due to this, an accelerator's throughput can be directly limited by how many multipliers its hardware budget can afford. As a result, surpassing this performance per multiplier limit has been focused on recently with minimal filtering algorithms applied to convolutional neural networks [2], [5], as well fast inner-product algorithms for GEMM and machine learning workloads [6]. Along this same direction, the Karatsuba algorithm [7] can also theoretically be used to reduce the complexity of integer multiplication. However, the extra addition operations it introduces can increase its execution speed in general-purpose computers or limit its area reduction in custom multiplier circuits for smaller integers of more commonly-used bitwidths [8], [9].\nIn this work, we show how the scalar Karatsuba multiplication algorithm can be extended to integer matrix multiplication, after which the impact and complexity of the extra additions is reduced. Furthermore, we investigate and present new fixed-precision and precision-scalable hardware architectures for efficiently exploiting the Karatsuba algorithm extended to matrix multiplication (referred to as Karatsuba matrix multiplication or KMM), showing how the proposed algorithm and hardware architectures can provide real area or execution time reductions for integer matrix multiplication compared to scalar Karatsuba or conventional matrix multiplication.\nThe proposed architectures can also be implemented using proven systolic array and conventional multiplier architectures at their core, maintaining all the implementation benefits of these architectures. Systolic arrays, which we will also refer to as matrix multiplication units (MXU)s for convenience, are an effective choice for use in GEMM accelerators as they significantly reduce the required memory traffic and can reach high clock frequencies due to their short and regular interconnects. Systolic-array architectures have been used in state-of-the-art GEMM and deep learning accelerators such as the Tensor Processing Unit (TPU) [3], [4], [10], among others [6], [11].\nIn summary, our key contributions are the following:\nWe propose the Karatsuba matrix multiplication (KMM) algorithm and carry out a complexity analysis of the algorithm compared to conventional scalar Karatsuba and matrix multiplication algorithms to facilitate further future investigations of potential applications and hardware implementations of KMM. We also identify complexity shortcomings of KMM that restrict its benefits in hardware and show how this is mitigated when KMM is combined with an alternative accumulation algorithm.\nWe present a new family of hardware architectures for efficiently exploiting KMM in custom hardware. We then model the area or execution time benefits of the KMM architectures and evaluate the proposed architectures both in isolation and in an end-to-end accelerator system compared to baseline designs and prior state-of-the-art works implemented on the same type of compute platform."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "We use the following notation throughput this article:\n$\\bullet$ ALG$_{n}^{[w]}$: An algorithm that operates on $w$-bit scalars or matrices with $w$-bit elements, where each scalar or matrix element is divided into $n$ digits. For example, SM$_{2}^{[8]}$ represents a scalar multiplication (SM) algorithm for operating on 8-bit 2-digit numbers where each digit is 4 bits wide, such as the multiplication between the hexadecimal values 0x12 $\\times$ 0x10 = 0x120.\n$\\bullet$ ALG$_{n}$ or ALG: The algorithm acronym may also be specified without the subscript $n$ and/or superscript $^{[w]}$ when the number of digits and/or input bitwidths are not directly relevant for the current context, and it may refer to the use of the algorithm for any value of $n$ or $w$ for each missing subscript and/or superscript.\n$\\bullet$ OPERATION$^{[W]}$: An arithmetic operation that works with $w$-bit values. For example, MULT$^{[w]}$, ADD$^{[w]}$, ACCUM$^{[w]}$ represent a multiplication, addition, and accumulation of $w$-bit values, respectively, and SHIFT$^{[w]}$ represents a left or right shift by $w$ bits.\n$\\bullet$ $x_{[a:b]}$: The value contained in bits $a$ down to $b$ of a scalar $x$. For example, the value of bits 7 down to 4 in the hexadecimal number 0xAE is equal to 0xA and is written as 0xAE$_{[7:4]}$ = 0xA. Similarly, 0xAE$_{[3:0]}$ = 0xE.\n$\\bullet$ $\\mathcal{C}$(ALG$_{n}$): The complexity of algorithm ALG in number of $w$-bit multiplications, additions, accumulations, and shift operations.\n$\\bullet$ $\\mathcal{C}'$(ALG$_{n}$): The complexity of algorithm ALG in number of arithmetic operations.\n$\\bullet$ $r$: The number of recursion levels implemented in KSM or KMM, equal to $\\lceil log_{2}n\\rceil$.\n$\\bullet$ $d$: The height and width of two matrices being multiplied.", "B. Conventional $n$-Digit Scalar Multiplication (SM)": "Fig. 1 shows the conventional method for performing 2-digit scalar multiplication where a $w$-bit multiplication is split into four smaller-bit scalar multiplications before being summed to form the final product. Algorithm 1 shows the generalization of this, where n-digit multiplication is performed by carrying out the same steps recursively for each smaller-bit multiplication.", "C. Karatsuba Scalar Multiplication (KSM)": "Fig. 2 shows the Karatsuba algorithm [7] for 2-digit scalar multiplication where a $w$-bit multiplication is split this time into three smaller-bit multiplications before being summed to form the final product. Algorithm 2 shows the generalization of this, where $n$-digit multiplication is performed by carrying out the same steps recursively for each smaller-bit multiplication.\nKSM-based low-bitwidth accurate integer multiplier circuits in prior works have shown some area benefits for input bitwidths in the range of 64 bits or less, with minimal area improvements in the smallest ranges of 16 bits [8], [9].", "D. Conventional $n$-Digit Matrix Multiplication (MM)": "A conventional matrix multiplication algorithm computes C = AB for A of size M $\\times$ K and B of size K $\\times$ N, where each element C$_{i,j}$ of C is calculated as follows:\n$\\begin{equation}\nC_{i,j} = \\sum_{k=1}^{K} a_{i,k}b_{k,j}\n\\end{equation}$"}, {"title": "E. Precision-Scalable Architectures", "content": "Precision-scalable architectures allow a way to efficiently execute workloads across multiple input precisions for applications where the input bitwidths are expected to vary. Machine learning (ML) acceleration is one example of a use-case for precision-scalable hardware architectures, where neural networks can perform the majority of the inference on reduced-bitwidth operations with little to no loss in accuracy but the bitwidths required to provide sufficient accuracy vary across different deep neural network models, applications, and between individual layers within the same neural network model [12]. For example, some neural network models can be executed with high accuracy even when performing the majority of the operations on small bitwidths, however, a smaller portion of the layers still need to be computed on larger bitwidths to preserve accuracy [12]. Therefore, a fixed-bitwidth accelerator must make a trade-off between either supporting only lower bitwidths while reducing the model's accuracy, or supporting larger bitwidths for higher accuracy but under-utilizing the MAC units during majority of computation as most stages require only lower-bit inputs.\nPrecision-scalable architectures address this trade-off by providing architectures which can more efficiently support execution of varying input bitwidths [12], [13], [14]. One approach is to use MAC units consisting of multiple smaller-bitwidth multipliers [13] which can either be individually used to multiply/accumulate multiple smaller-bitwidth products, or they can be reconfigured to collectively multiply/accumulate fewer larger-bitwidth products per clock cycle. Another type of approach referred to as bit-serial architectures [14], is to have MAC arrays which repeatedly perform fixed-size smaller-bitwidth vector operations on different bit slices of the vectors, summing up the separate vector products to get the final full-bitwidth vector result.\nThe hardware algorithms used in prior works on precision-scalable architectures [12], [13], [14] use variations of the"}, {"title": "III. KARATSUBA MATRIX MULTIPLICATION (KMM)", "content": "In this section, we formally define KMM, analyze its complexity compared to conventional scalar Karatsuba and matrix multiplication algorithms, identify complexity shortcomings of the KMM algorithm that restrict its benefits in hardware, and show how this is mitigated when combining KMM with an alternative accumulation algorithm."}, {"title": "A. KMM Definition", "content": "Fig. 4 shows the 2-digit Karatsuba scalar multiplication algorithm [7] from Fig. 2 extended to matrix multiplication analogously to how Fig. 3 extends conventional 2-digit scalar multiplication in Fig. 1 to matrix multiplication. Algorithm 4 shows the generalization of this, where n-digit Karatsuba matrix multiplication is performed by carrying out the same steps recursively for each smaller-bit matrix multiplication. In Algorithm 4, the full matrix product is split into three separate partial-product matrix multiplications between matrices each containing bit slices of every element. The elements in matrices Ao and Bo contain the lower bits (bits [w/2]-1 down to 0) of every element in the A and B matrices, while A1 and B1 contain the upper bits (bits w-1 down to [w/2]) of every element in matrices A and B. The As and Bs matrices are formed by summing A1 + Ao and B1+ Bo, and therefore their elements have a bitwidth of [w/2] + 1. The partial-product matrices are then summed analogously to how the partial scalar products are summed after multiplication in KSM from Algorithm 2."}, {"title": "B. KMM Complexity Analysis", "content": "In this subsection, we derive the complexity of KMM and compare it to the complexity of the conventional MM, and KSM algorithms. To do this, we decompose each algorithms' complexity to number of $w$-bit multiplications, additions, and shift operations. This provides a general technology-agnostic foundation for evaluating different possible KMM hardware implementations and modelling the costs and benefits of implementing the algorithm in hardware across different possible implementation technologies where the cost of each type of operation may vary depending on the implementation platform used. For example, implementations on FPGA may result in multipliers mapping to DSP units, additions and accumulations mapping to soft look-up-table (LUT) and register resources, whereas ASIC implementations will result in different costs and trade-offs than this for each type of operation.\nAdditionally, while the main focus of this work is on leveraging KMM in custom hardware designs, we also compare KMM's complexity more simply in number of arithmetic operations to allow modelling the time complexity of KMM execution on general-purpose hardware containing fixed operator word sizes. This analysis (plotted in Fig. 5) indicates that KMM requires significantly fewer operations to execute large-integer matrix multiplication on general-purpose hardware"}, {"title": "1) MM Complexity:", "content": "The complexity of conventional n-digit MM between two matrices of size d$\\times$d is derived by counting the number of operations that are performed in Algorithm 3:\n$\\begin{equation}\n\\mathcal{C}(\\text{MM}_{n}^{[w]}) = \\mathcal{C}(\\text{MM}_{n/2}^{[w]}) + 3\\mathcal{C}(\\text{MM}_{n/2}^{[w]})\n+ d^{2} \\left(\\text{ADD}^{[w+w_{a}]} + 2 \\text{ADD}^{[2w+w_{a}]}\\right)\n+ d^{2} \\left(\\text{SHIFT}^{[w]} + \\text{SHIFT}^{[\\lceil w/2\\rceil]}\\right)\n\\end{equation}\n$\\begin{equation}\n\\mathcal{C}'(\\text{MM}_{n}^{[w]}) = d^{3} \\left(\\text{MULT}^{[w]} + \\text{ACCUM}^{[2w]}\\right)\n\\end{equation}\nTypically, ACCUM$^{[2w]}$ = ADD$^{[2w+wa]}$, where $w_{a}$ is an additional bitwidth added to account for accumulation. However, in Section III-C, we discuss a method for reducing the complexity of the accumulations to be less than this.\nThe ADD$^{[w+w_{a}]}$ terms in (2a) come from the additions forming the (C$_{10}$ + C$_{01}$) term on line 12 of Algorithm 3. Here, the bitwidth of the C$_{10}$ and C$_{01}$ elements is $w + w_{a}$ because they are accumulations of $w$-bit products of [w/2] and [w/2]-bit values. The two_ADD$^{[2w+w_{a}]}$ terms in (2a) come from the additions to C on lines 12 and 13 of Algorithm 3. The bitwidth of these additions is kept on 2w+wa bits since C results in accumulations of 2w-bit products of w-bit values."}, {"title": "2) KSM Complexity:", "content": "The complexity of KSM is derived by counting the operations performed in Algorithm 2:\n$\\begin{equation}\n\\mathcal{C}(\\text{KSMM}_{n}^{[w]}) = d^{3} \\left(\\mathcal{C}(\\text{KSM}_{n}^{[w]}) + \\text{ACCUM}^{[2w]}\\right).\n\\end{equation}\n$\\begin{equation}\n\\mathcal{C}'(\\text{KSM}_{n}^{[w]}) = 2 \\left(\\text{ADD}^{[2w]} + \\text{ADD}^{[\\lceil w/2\\rceil]} + \\text{ADD}^{[2\\lceil w/2\\rceil+4]}\\right)\n+ \\text{SHIFT}^{[w]} + \\text{SHIFT} [\\lceil w/2 \\rceil]\n+ \\mathcal{C}(\\text{KSM}_{n/2}^{[w]}) + \\mathcal{C}(\\text{KSM}_{n/2}^{[w]+1})\n+ \\mathcal{C}(\\text{KSM}_{n/2}^{[w]})\\end{equation}\n$\\begin{equation}\n\\mathcal{C}'(\\text{KSM}_{n}^{[w]}) = \\text{MULT}^{[w]} .\n\\end{equation}\nThe two ADD$^{[\\lceil w/2\\rceil]}$ terms in (3a) come from the [w/2]-bit additions forming the as and bs terms on lines 7 and 8 of Algorithm 2. The two ADD$^{[2\\lceil w/2\\rceil+4]}$ terms in (3a) come from forming the (cs - c1 - c0) term on line 13 of Algorithm 2, where these terms can be first summed together on 2[w/2]+4 bits before being shifted and added to the other product terms. The bitwidth 2[w/2] + 4 is required because cs is a (2[w/2]+2)-bit product of ([w/2]+1)-bit values, and the additional two bits are to account for sign extension and subtraction of the c1 and c0 terms. The two ADD$^{[2w]}$ terms in (3a) come from the additions to c on lines 13 and 14 of Algorithm 2. These additions are on 2w-bit values since c will ultimately result in the 2w-bit product of two w-bit values."}, {"title": "3) KSMM Complexity:", "content": "To compare KSM to KMM and the other matrix multiplication algorithms, we analyze the complexity of an algorithm we refer to as KSMM. KSMM is defined as a conventional matrix multiplication algorithm as in (1), but where KSM is used for the multiplications between all elements rather than conventional scalar multiplication. KSMM then has the following complexity:\n$\\begin{equation}\n\\mathcal{C}(\\text{KSMM}_{n}^{[w]}) = d^{3} \\left(\\mathcal{C}(\\text{KSM}_{n}^{[w]}) + \\text{ACCUM}^{[2w]}\\right).\n\\end{equation}$"}, {"title": "4) KMM Complexity:", "content": "The complexity of KMM is derived by counting the operations performed in Algorithm 4:\n$\\begin{equation}\n\\mathcal{C}(\\text{KMM}_{n}^{[w]}) = 2 d^{2} \\left(\\text{ADD}^{[2\\lceil w/2\\rceil+4+w_{a}]} + \\text{ADD}^{[2\\lceil w/2\\rceil+4+w_{a}]} + \\text{ADD}^{[2w+w_{a}]} + \\text{ADD}^{[2w+w_{a}]}\\right)\n+ d^{2} \\left(2\\text{ADD}^{[\\lceil w/2\\rceil]} + \\text{SHIFT}^{[w]} + \\text{SHIFT} [\\lceil w/2 \\rceil]\\right)\n+ \\mathcal{C}(\\text{KMM}_{n/2}^{[w]}) + \\mathcal{C}(\\text{KMM}_{n/2}^{[w]+1})\n+ \\mathcal{C}(\\text{KMM}_{n/2}^{[w]})\n\\end{equation}\n$\\begin{equation}\n\\mathcal{C}'(\\text{KMM}_{n}^{[w]}) = \\mathcal{C}(\\text{MM}_{n}^{[w]}) .\n\\end{equation}\nThe two ADD$^{[\\lceil w/2\\rceil]}$ terms in (5a) come from the [w/2]-bit additions forming the As and Bs terms on lines 7 and 8 of Algorithm 4. The two ADD$^{[2\\lceil w/2\\rceil+4+w_{a}]}$ terms in (5a) come from forming the (Cs \u2013 C1 \u2013 Co) term on line 13 of Algorithm 4, where these terms can be first summed together on 2[w/2] + 4 + wa bits before being shifted and added to the other product terms. The bitwidth 2[w/2] + 4 + wa is required because the bitwidth of Cs is 2[w/2]+2+wa since it is accumulations of (2[w/2]+2)-bit products of ([w/2]+1)-bit values, and the additional two bits are to account for sign extension and subtraction of the C1 and C0 terms. The two ADD$^{[2w+w_{a}]}$ terms in (5a) come from the additions to C on lines 13 and 14 of Algorithm 4. The bitwidth of these additions is kept on 2w + wa bits since C results in accumulations of 2w-bit products of w-bit values.\n(5a) shows that KMM significantly reduces the complexity of the 8 addition and shift operations in (3a) that are performed $(n/2)log_{2}3 d^3$ times in KSMM by reducing their occurrence by a factor of $d$. On the other hand, KMM trades d\u00b3 accumulations of 2w-bit values in (2b) or (4) for $nlog_{2}3 d^3$ smaller-width accumulations in (5b). However, in Section III-C we show how the penalty of this in hardware is mitigated when combining KMM with an alternative accumulation algorithm.\nIf only counting the number of operations without considering operation bitwidths or type, we can simplify (2) to:\n$\\begin{equation}\n\\mathcal{C}'(\\text{MM}_{n}) = 2n^{2}d^{3} + 5 \\left(\\frac{n}{2}\\right)^{2}d^{2},\n\\end{equation}$"}, {"title": "C. Mitigating the Accumulator Complexity Increase in KMM", "content": "As found in Section III-B, KMM has one penalty of trading d\u00b3 accumulations of 2w-bit values in (2b) or (4) for $nlog_{2}3 d^3$ smaller-width accumulations in (5b). In this subsection, we show how this downside is mitigated when using Algorithm 5 as the MM1 algorithm in KMM on line 16 of Algorithm 4. Algorithm 5 performs MM1 using an alternative accumulation structure that reduces the accumulation hardware complexity.\n$\\begin{equation}\np \\text{ACCUM}^{[2w]} = p \\text{ADD}^{[2w+w_{a}]} .\n\\end{equation}$"}, {"title": "IV. KMM HARDWARE ARCHITECTURES", "content": "In this section, we present a general family of hardware architectures for efficiently exploiting the KMM algorithm in hardware and derive metrics for analyzing the area or execution time benefits of the KMM architectures. The first type of KMM architecture, described in Section IV-B, is a fixed-precision architecture optimized for executing inputs that are not expected to vary in bitwidth. We then present a precision-scalable KMM architecture in Section IV-C that can more efficiently execute across multiple input precisions for applications where the input bitwidths are expected to vary."}, {"title": "V. RESULTS", "content": "In Section V, we compare the KMM architectures against other designs using the multiplier and Area Unit compute efficiency metrics defined in (12) and (23) from Sections IV-E and IV-F, respectively. These are both used to compare an architecture's throughput per area capabilities regardless of the clock frequency.\nAdditionally, the multiplier compute efficiency also measures the amount of computational work being performed per compute area regardless of the clock frequency or input bitwidths. This is an important quality because prior works using the same compute platform as us for evaluation only evaluate throughput for input bitwidths w that are equal to the multiplier bitwidths m. However, in order to execute KMM in hardware, the input bitwidths w must be larger than the multiplier bitwidths. Therefore, to fairly compare the performance of the prior works against our KMM architecture, we need to use a performance metric with a maximum achievable value that does not change regardless of the input bitwidths w being executed, which is not the case for the GOPS metric.\nFurthermore, the multiplier compute efficiency is also useful for comparison with prior works because it is measurable using only throughput, number of multipliers, and frequency, which are commonly provided or derivable in prior works.\nThe Area Unit compute efficiency metric also accounts for the area overhead of registers and adder units and provides a more general abstracted method for modelling the circuit area"}, {"title": "E. Multiplier Compute Efficiency", "content": "In this subsection, we define a performance-per-area metric called the multiplier compute efficiency in (12) which we use to compare the KMM architecture against baseline designs and prior works. The metric is used to compare the amount of computational work that can be performed per compute area regardless of the clock frequency or input bitwidths. The importance of this property is expanded upon more later in this subsection, as well as in Section V-A.\nThe hardware complexity of fixed-point multipliers typically scale quadratically with the input bitwidth compared to linearly for adders and registers [16], [17], [18], causing the hardware footprint of multipliers to dominate that of adders and registers. Due to this, multipliers and MAC units are commonly the area-dominant computational resources in deep learning and GEMM-based accelerators [2], [3], [4]. Therefore, we derive a performance-per-area metric defined below for quantifying how much the algebraic optimizations exploited in an architecture reduce the computational complexity of the area-dominant operations (multiplications) and measure how effectively an architecture can utilize these resources relative to a conventional design using no algebraic optimizations:\n$\\begin{equation}\n\\frac{\\text{mults/multiplier}}{\\text{clock cycle}} \\triangleq \\frac{\\text{(mults/s)/\\#multipliers}}{f}\n\\end{equation}$\n,where mults/s above is measured by taking the number of multiplications required to carry out an execution using conventional algebra and dividing it by the measured execution time, #multipliers is the number of instantiated multipliers in the design, and f is the clock frequency that the hardware design is operating at.\nThe throughput metric in (11) measures the number of w-bit multiplications being performed, where w is the algorithm input bitwidths. However, in order to execute KMM in hardware, the algorithm input bitwidths w must be larger than the multiplier bitwidths, and the number of larger w-bit multiplications that can be performed per multiplier will be lower than the actual effective number of multiplications being performed per multiplier. Therefore, the maximum achievable value for the metric from (11) will vary depending on the input bitwidths w and is not ideal for reflecting the true amount of computational work being performed per multiplier regardless of the input widths.\nTo address this, we can instead measure (11) directly in terms of effective m-bit multiplications being performed per multiplier, where m may be smaller than the algorithm input bitwidths w. This derives the following metric for measuring the true amount of effective multiplications being performed per multiplier regardless of the algorithm input bitwidths w:\n$\\begin{equation}\n\\frac{\\text{m-bit mults/multiplier}}{\\text{clock cycle}} \\triangleq \\frac{\\text{(m-bit mults/s)/\\#multipliers}}{f}\n,\\end{equation}$\nwhere m-bit mults/s above is measured by taking the number of m-bit multiplications required to carry out an execution on w-bit inputs using conventional algebra and dividing it by the measured execution time, #multipliers is the number"}, {"title": "VI. CONCLUSION", "content": "In this work, we propose the extension of the scalar Karatsuba multiplication algorithm to matrix multiplication, showing how this maintains the reduction in multiplication complexity of the original Karatsuba algorithm while reducing the complexity of the extra additions. Furthermore, we propose new matrix multiplication hardware architectures for efficiently exploiting the proposed algorithm in custom hardware, showing that they can provide real area or execution time improvements for integer matrix multiplication compared to designs implementing scalar Karatsuba or conventional matrix multiplication algorithms. The proposed architectures are well suited for increasing the efficiency in acceleration of modern workloads that can decompose to large matrix multiplications on integer arithmetic, such as the computationally dominant portion of convolutional neural networks or the attention mechanism of transformer models [30]. We provide a complexity analysis of the algorithm and architectures and evaluate the proposed designs both in isolation and in an end-to-end accelerator system relative to baseline designs and prior state-of-the-art works, showing how they increase the performance-per-area of matrix multiplication hardware."}]}