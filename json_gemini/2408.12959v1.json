{"title": "Multimodal Contrastive In-Context Learning", "authors": ["Yosuke Miyanishi", "Minh Le Nguyen"], "abstract": "The rapid growth of Large Language Models (LLMs) usage has highlighted the importance of gradient-free in-context learning (ICL). However, interpreting their inner workings remains challenging. This paper introduces a novel multi-modal contrastive in-context learning framework to enhance our understanding of ICL in LLMs. First, we present a contrastive learning-based interpretation of ICL in real-world settings, marking the distance of the key-value representation as the differentiator in ICL. Second, we develop an analytical framework to address biases in multimodal input formatting for real-world datasets. We demonstrate the effectiveness of ICL examples where baseline performance is poor, even when they are represented in unseen formats. Lastly, we propose an on-the-fly approach for ICL (Anchored-by-Text ICL) that demonstrates effectiveness in detecting hateful memes, a task where typical ICL struggles due to resource limitations. Extensive experiments on multimodal datasets reveal that our approach significantly improves ICL performance across various scenarios, such as challenging tasks and resource-constrained environments. Moreover, it provides valuable insights into the mechanisms of in-context learning in LLMs. Our findings have important implications for developing more interpretable, efficient, and robust multimodal AI systems, especially in challenging tasks and resource-constrained environments.", "sections": [{"title": "Introduction", "content": "Upon the explosive usage of the Large Language Model (LLM), in-context learning (ICL) characterizes LLM's reasoning process. Understanding its optimization mechanism is critical for reliable, evidence-based decision-making. Previous works have shown that LLMs could optimize the attention weights in the gradient-free inference. The research scope, however, is mostly limited to simple problems like linear regression or word-level natural language inference. The recent advances in multimodal LLM present us with more challenges. First, in addition to the linguistic format dependencies, which seem trivial to humans, multimodal ICL involves arbitrarily formatted multiple modalities. Although the research community proposes many approaches for solutions in different contexts, the impact of the multimodal ICL input formatting remains elusive. Second, exploring effective in-context examples is demanding due to the limited source of high-quality multimodal datasets compared to those of single modality.\nTo achieve a deeper understanding of LLM, as the gradient descent hinted at attention-based optimization, the existing gradient-based learning method could help interpret how it optimizes in ICL. Specifically, Contrastive Learning (CL), typically used for modality encoders and classic language models, could guide the model in mapping semantically similar inputs to a similar location in feature space. Based on the previous theoretical findings about the equivalence of LLM's learning process and CL, we show that CL helps interpret how LLM understands multimodal ICL semantics under unseen input formatting and/or resource shortage. Our contribution could be summarized as follows:\n1. We propose a first CL-based interpretation of ICL in multimodal settings, suggesting that the semantically similar ICL examples trigger the representational shift dependent on the problem settings.\n2. We propose a CL-based analytical framework for the bias of multimodal input formatting and show that semantically similar ICL examples could be helpful in challenging tasks even when presented in an unseen format.\n3. We propose Anchored-by-Text ICL, an on-the-fly inference in which LLM first generates the ICL example and then performs the inference using the generated example as an anchor for extracting the input-label relationship. This approach has shown effectiveness in resource-limited settings."}, {"title": "Related Work", "content": "In these few years, LLMs have been widely adapted to natural language processing (Zhao et al. 2023b), showing remarkable in-context learning (ICL) performance (Brown et al. 2020) with up to a few examples and without gradient-based training. Massive work has tested their multimodal capabilities (Zhang et al. 2024) centered on vision and language as a step toward general-purpose agents."}, {"title": "Interpreting Inner Workings", "content": "To achieve Trustworthy AI (Thiebes, Lins, and Sunyaev 2021), understanding how LLMs achieve high ICL performance is imminent. Various interpretations have been proposed to obtain theoretical and empirical grounding behind ICL. Typically, the interpretation studies hire a specific algorithm to interpret the dynamics of LLM's representations: for example, Bayesian inference (Xie et al. 2022), kernel regression (Han et al. 2023), latent variable model (Wang et al. 2023c), algorithm selector (Li et al. 2023b), multi-state RNN (Oren et al. 2024), and gradient descent (von Oswald et al. 2023; Dai et al. 2023). Although these studies covered extensive theoretical aspects, most empirical findings are limited to simple problems like linear modeling or simple NLP tasks, let alone multimodal settings.\nTo demystify LLM's remarkable multimodal ICL capabilities, its training and evaluation procedure should be a clue. Most LLMs are trained to maximize the predicted probability of the tokens in the training datasets (Shlegeris et al. 2024). In multimodal problems, non-language information (e.g., image) is encoded as captioned text (e.g. Miyanishi and Nguyen (2024)) or soft prompt (e.g. Bulat and Tzimiropoulos (2023)). At inference time, ICL frameworks mostly anchor the semantically similar examples to the test input (Liu et al. 2022; Wang, Yang, and Wei 2024; Li et al. 2023c), making it intuitive to hypothesize that the distance between ICL example and test input plays a crucial role in ICL. Here, we formally and empirically show that multimodal input distance, coded during the LLM's training procedure, plays an crucial role in understanding the ICL inputs. To provide such an distance-oriented view of ICL, Contrastive Learning (CL) (Le-Khac, Healy, and Smeaton 2020) could play an pivotal role. CL was initially developed as an unsupervised approach for training data distribution, and then Khosla et al. (2020) introduced supervised CL for labeled datasets. Before the paradigm shift to generative models, CL was a major pre-training objective of mainstream language models based on a Transformer (Vaswani et al. 2017) encoder like BERT (Devlin et al. 2019). In the LLM era, its main application is multimodal (e.g. vision and language) alignment (Hu et al. 2024). CL is rather straightforward for capturing the cross-input semantics since it is designed to map the inputs to the feature space based on conceptual similarity.\nRecently, Ren and Liu (2023) has theoretically analyzed the equivalence of ICL and supervised CL without negative examples and has shown its validity in simple mathematical problem-solving. In addition, we extend the analysis to the multimodal real-world datasets and propose that the semantically similar ICL examples trigger the representational shift in LLMs."}, {"title": "Input Formatting", "content": "Prompt engineering (Bozkurt and Sharma 2023) has tackled the optimization of the instruction and task description. In addition to the textual information, multimodality (Wang et al. 2023b) poses a new challenge - how LLMS could understand the interleaved inputs of multiple information sources. Focusing on the image-text relationship, the most straightforward format is an image followed by a single instruction (e.g., a single visual question-answering entry), targetted by the most state-of-the-art multimodal models like LLaVA (Liu et al. 2023b). Another popular format is multi-turn conversation (Feng et al. 2023; Morgan et al. 2023), with which the model should recognize at least the two lines of text interleaved by two images. Recent studies have tackled this problem with tailored pre-training protocol (Zheng, He, and Wang 2023) and/or instruction tuning (Li et al. 2023a; Tang et al. 2023). In line with these works, this paper quantitatively shows how the unseen format biases the LLM's comprehension of the ICL example, and that semantics-formatting balance works differently for the different tasks."}, {"title": "Resource Shortage", "content": "Like the limited vision-and-language ability of humans with less visual experience (L\u00f3pez-Barroso et al. 2020; Mamus et al. 2023), the resource shortage is a significant challenge for vision-oriented models (Bai et al. 2023). Since typical ICL involves example selection from training subset of a given task, task-specific multimodal resources, like hateful memes detection datasets (Kiela et al. 2020; Gomez et al. 2020), constrains the ICL performance. One approach to this problem is to let the LLMs generate ICL examples for their own usage. For example, Wang, Yang, and Wei (2024) framed this problem into retrieval, and Coda-Forno et al. (2023) has shown that LLMs can perform meta-learning via ICL. Notably, in some cases like hateful memes, forcing state-of-the-art LLMs to generate positive examples is challenging for safety reasons. This paper shows that LLM-generated negative examples shift the model's representation, and mitigate this positive example constraint."}, {"title": "Preliminaries", "content": "Transformer's self-attention layer of depth d maps input document D to query Q, key K, value V with corresponding weight matrix W. An layer is written as:\n$$Q = W_QD, K = W_KD,V = W_vD$$\n$$Self Attn(Q, K, V) = SoftMax(\\frac{QK^T}{\\sqrt{Vd}})V$$ (1)\nIn case of generating the answer a for a set of the documents $D_{icl} = {D_{query}, D_{ex}}$ consisting of the query $D_{query}$ with the ICL example $D_{ex}$, the predicted most probable answer \u0177 is obtained as:\n$$\\hat{y} = argmax_y p(y|Self Attn(D_{icl}))$$ (2)"}, {"title": "ICL and CL", "content": "CL typically utilizes contrastive loss (Hadsell, Chopra, and LeCun 2006) with which the document pair $(D_1, D_2)$ is mapped to the representation space with the guidance of a binary $y_c$ (1 suggests that the documents are in a specific category, 0 otherwise). Given a distance function $dist(\u00b7,\u00b7)$, the loss C with hyperparameter e is defined as:\n$$L(D_1, D_2) = Y_cd_{D_{1/2}} + (1 - y_c)max(\\epsilon \u2013 d_{D_{1/2}}, 0)$$\nwhere $d_{D_{1/2}} = dist(D_1, D_2)$ (3)\nIn inference time, the learned function $f_{CL}$ maps the new input $D_{test}$ to the representation space, and a dedicated function $f_{proj}$ projects that representation to \u0177.\n$$\\hat{y} = f_{proj}(f_{CL}(D_{test}))$$ (4)\nRen and Liu (2023) has shown that ICL could be seen as CL without negative examples. They suggested that a self-attention layer could be seen as a contrastive learner. More specifically, with the help of a kernel function \u03c6, a single layer minimizes the distance between two different augmentations K, \u00eev of an identical training data point's representation h.\n$$K = W_K(W_kh)$$\n$$v = W_Vh$$\n$$L(\\kappa,\\hat{v}) = d_{\\kappa/\\hat{v}}$$\n(5)\nNote that the category label $y_c$ is omitted for the absence of the negative class. After the input passes through a single model layer, it gets the new representation h', embeds it to the same feature space using the query weight $W_Q$, and obtains the inference output \u0177 using the updated weight W.\n$$W = W \u2013 \\eta\\Delta L$$\nwhere $\\Delta C = \\frac{\\partial C}{\\partial W}$ (6)\n$$\\hat{y} = W_x^{test}$$\nwhere $x^{test} = \\phi(W_Qh')$\nHereafter, we omit the learning rate \u03b7 for brevity. Since the weight update \u0394L is a function of key-value distance, we denote the update as \u0394L(K, V), and its resulting (ICL-optimized) weight as $W_{icl}$. This paper factorizes the real-world learning process and empirically shows its significance."}, {"title": "Mixed Effect Model", "content": "Mixed effect model (Singmann and Kellen 2019) has been proposed to disentangle the dual effects of the variables within the same model. Specifically, in observation i, the effect of some variables X over the target variable $y_i$ is expected to be identical across all the observations (fixed effect), and another variables Z affect individual (group of) observation differently (random effect). Linear mixed effect model could be formalized as:\n$$Y_i =W_xX + W_zi Z_i$$ (7)\nFor example, if we are to analyze the effect of a new teaching method on student performance across different schools in a city, the method should have a fixed effect since, in general, such a method aims for equal educational opportunities. In contrast, the school variable should have a random effect since each school must have a different educational policy. Note that various non-linear expressions of the mixed effect are proposed (e.g. Hajjem, Bellavance, and Larocque (2014); Sigrist (2023)), but we limit the scope to the linear model for brevity."}, {"title": "ICL Example Selection", "content": "In ICL, the example $D_{ex}$ is typically extracted from the training dataset or its subset $U_{Dtrain}$ to obtain the closest example to the test input $D_{query}$.\n$$D_{ex} = argmin_{Dtrain} d_{Dtrain/D_{query}}$$\n(8)\nWe show the effectiveness of generating the example instead of selecting it and discuss how it is related to CL."}, {"title": "Methodology", "content": "Fig.1 summarizes our method."}, {"title": "Representational Shift Hypothesis", "content": "In CL interpretation (Eq.5-6), key-value distance contributes to attention-based optimization in ICL. Since this interpretation only presupposes the interaction of key-value pair, we could extend it to the arbitrary set of test-time input fractions (e.g. instruction prompt $K_{inst}$ and the task given in zero-shot setting $V_{zsl}$). Since the zero-shot task consists of the instruction, the task, and LLM's prediction $V_{pred}$, and the model is trained to infer the latter tokens from the former, we propose that the distance among the zero-shot components affects the generation as follows:\n$$W_{zsl} = W - \\Delta W(K_{inst}, V_{zsl})$$\n$$W_{pred} = W_{zsl} - \\Delta W(K_{zsl}, V_{pred})$$\n(9)\nSimilarily, the updated ICL weight $W_{icl}$ could be formalized as:\n$$W_{icl} = W - {\\Delta W(K_{inst}, V_{icl}) + \\Delta W(K_{icl}, V_{zst})}$$ (10)\n$$W_{pred} = W_{icl} - \\Delta W(K_{zst}, V_{pred})$$\nAssuming that the overall instruction affects each task equally $\\Delta W(K_{inst}, V_{zsl}) \\thickapprox \\Delta W(K_{inst}, V_{icl})$, the weight (and resulting representation) shifts towards example-task distance.\n$$W_{pred}' - W_{pred} = \\Delta W(K'_{zsl}, V_{pred}) \u2013 \\Delta W(K_{zsl}, V_{pred})$$\n$$= \\Delta W(K_{icl}, V_{zsl})$$ (11)\nIn summary, the ICL example first affects the representation of the zero-shot task, and the prediction is affected via the task-prediction representational shift. To test whether this hypothesis is correct, we analyze the distance-distance relationship."}, {"title": "Multimodal Input Formatting", "content": "Disentangling Format and Semantics The semantics of the bimodal inputs and their format are entangled yet different concepts. Since CL aims to learn the inputs' similarity and variance, we could assume semantic similarity as its objective while formatting as a biasing factor. In other words, the formatting term $L_{fmt}$ affects the actual loss L in parallel with its semantic term $L_{sem}$.\n$$L = L_{sem} + L_{fmt}$$\n$$W = W - (\\Delta L_{sem} + \\Delta L_{fmt})$$\n(12)\nIntuitively, within a single dataset, the second term consistently biases all the ICL examples (fixed effect). In contrast, the first term should also reflect the variance of the individual test data points (random effect). Therefore, when the model faces a test input with an unseen format, the model output for input i should be interpreted as a mixed model.\n$$\\hat{g}_i = {W \u2013 (\\Delta L'_{sem} + \\Delta L_{sem} + \\Delta L_{fmt})}X_i$$\n(13)\nWhen the ICL format is the same with the training process, $\\Delta L_{fmt} = 0$.\nModel Performance Analysis In the macroscopic view, the effect of the unseen format should be expressed as the impact of bias b \u2208 {0, 1}, and that of ICL example presence e \u2208 {0,1}. Intuitively, the ICL examples have random effects due to the dependence on the content of each example. In contrast, the format bias should have a fixed effect, affecting the overall performance. Note that we use accuracy as the metric unless stated otherwise since all Visual Question Answering (VQA) datasets used in our analysis hire this metric. Together, the model accuracy acc of a data subset i could be modeled as:\n$$acc_i(b, e) = W_b + W_ie$$ (14)\nTo analyze the impact across the models, the results of all the models are concatenated and the variables b and e are analyzed as an interaction term.\nRepresentation Analysis Since some of the widely used benchmarks like MMBench (Liu et al. (2023c)) require online submission for evaluation, which makes reproducible local evaluation challenging, we need the unsupervised approach. Under the our hypothesis, ICL is driven by the distance $d_{\u00b7/\u00b7}$ between the representation of the key $h_K$ and that of value $h_V$. In zero-shot VQA, the distance between question $h_q$ and answer $h_a$ would be the only clue to the model. In contrast, the ICL example is concatenated to the question $h_{icl} = {h_{ex},h_q}$, leading to the shift in the feature space and, therefore, distance with the new answer $h_a$.\nIn the spirit of the linear representation hypothesis (Park, Choe, and Veitch (2023)), we implement a linear mixed effect model. Specifically, the random effect is modeled as the linear weight $W_{random}$, and the fixed effect is introduced via the product of $h_{zcl}$ and the embedded index representing the model and the dataset with the weights $W_{fixed}$.\n$$h_{icl} = (W_{random} + W_{fixed}I)h_{zsl}$$\n(15)\nFinally, we model the linear relationship between the query-answer distance matrix and the shifted query-new answer matrix.\n$$d_{h_{ici/h'a}} = Wd_{h_q/h_a} + W_o$$ (16)\nAs a baseline, we use the model only with first term $h_{icl} = W_{random}h_{zc\u0131}$, or a simple linear projection."}, {"title": "Anchored-by-Text ICL", "content": "Generation Strategy Two major blockers must be addressed for on-the-fly ICL example generation on hateful memes. First, it requires text-image bimodal generation. Since only limited models (e.g. Wu et al. (2023)) have such capability, we use the generated text as an anchor to cause a representational shift, and therefore the prediction. Hereafter we call it anchored-by-text ICL (AbT ICL).\nSecond, most LLMs have safety limitations based on instruction tuning (Bianchi et al. (2023)). Since bypassing such limitations is neither desirable nor sustainable, we let the model generate negative examples. Together, given that document D consists of text T and image I (D = (T, I)) with a binary label y (0 for benign, 1 for hateful), our strategy is formalized as:\n$$T_{icl} = argmax_T p(y = 0|T, I_{query})$$\n$$D_{icl} = {T_{icl}, I_{query}}$$\n(17)\nIn short, the model generates text that fits with a given image to compose a benign meme and uses that meme as a benign example. Baselines include zero-shot and one-shot detection. Fig.2 shows a representative prompt aiming for this goal.\nQu et al. (2023) introduced another workaround of using more general labels, which will be a part of our future work.\nRepresentation / Prediction Analysis Since hateful memes detection could be framed into binary classification in this experiment, we model the effect of the key-value distance (Eq. 13) over the predicted label y on three learning types lt (zero-shot zsl, ordinary ICL icl, and AbT ICL abt), and analyzed the difference of the weights W and the intercept $W_o$ as an effect of the representational shift. For example, the effect of AbT over that of ordinary ICL could be formalized as:\n$$lt \u2208 {zsl, icl, abt}$$\n$$Y_{lt} = W_{lt} d_{h_{i/h_a}} + W_{ft}$$\n$$Y_{abt} - Y_{icl} = (W_{abt} - W_{icl}) d_{h_{i/h_a}} + (W_{abt} \u2013 W_{icl})$$\n(18)\nThe weights $W_{lt}$ and $W_{ft}$ are estimated per layer dimension to perform the memory-efficient analysis."}, {"title": "Experimental Settings", "content": "Experiments are conducted on a single NVIDIA A100 80GB GPU with Linux OS. Unless stated otherwise, all codes are in Python 3.9. Statistical arguments are based on a t-test and bootstrapping with 1,000 resamples. We run the models once with a random seed of 1987."}, {"title": "Experiment I: Multimodal Input Formatting", "content": "Model To disentangle the effect of input semantics and that of the formatting, the subject model in this paper should 1) have the expected maximum capability of understanding the semantics and 2) is NOT trained or fine-tuned on a multi-image setting. We primarily focus on LLaVA (Liu et al. 2023b) to satisfy this criterion. More specifically, we use two variants: LLaVA-Llama2 for its high performance of the linguistic backbone (Touvron, Martin, and Stone (2023)) and LLaVA 1.5 for its highest performance on vision-and-language tasks (Liu et al. (2023a)). 13 billion parameter models are used for memory constraints. We also use InternVL (1-8 billion) for their limited yet tested multi-image capabilities by multi-image datasets like MMMU (Yue et al. 2024).\nTo select ICL examples most similar to test inputs, CLIP (Radford et al. (2021), specifically HuggingFace clip-vit-large-patch14) is used because of its relatively small computational cost and its high capability on similarity-related tasks (e.g., image aesthetics evaluation\u00b3). We take the last layer as a representation for its high correspondence with the generated tokens despite the presence of highly competitive short-cutting (Din et al. 2024; Fan et al. 2024).\nDataset To cover various aspects of multimodal LLM's capabilities, we tested our approach with six VQA datasets, namely VQA v 2.0 (Goyal et al. (2017)), GQA (Hudson and Manning (2019)), VizWiz (Gurari et al. (2018)), TextVQA (Singh et al. (2019)), MMBench (Liu et al. (2023c)), and MM-Vet (Yu et al. (2023)).\nModel Accuracy Analysis Practically, the presence of the random and fixed effect (z and e in Eq. 13, respectively) is represented as a coefficient of the corresponding one-hot encodings. The performance of the mixed effect model is evaluated using the marginal/conditional R2 method (Nakagawa and Schielzeth (2013)). To maintain the experiment's integrity while utilizing a wide range of statistical tools, the R language's lmer package is called from the Python environment via rpy24 module.\nRepresentation Analysis The linear mixed model and the baseline linear model are implemented with PyTorch backend5 and trained to maximize the cosine similarity between the representation via Pytorch Metric Learning package6 and AdamW optimizer ((Loshchilov and Hutter 2019)). We extract 1,000 samples from each dataset and hold out 20% as a test set."}, {"title": "Experiment II: AbT ICL", "content": "Intuitively, the impact of AbT ICL may vary across datasets. The most influential scenario is 1) when the dataset size is small and suffers from high variance, making the example selection infeasible 2) when explicit and strong cross-modal interaction affects the dataset.\nKiela et al. (2020) curated the Hateful Memes Challenge dataset, which perfectly fits this experiment's criteria. Initially, Lauren\u00e7on et al. (2023) and Zhao et al. (2023a) have shown that ICL is not particularly effective unless the task is heavily tuned to the task. Moreover, Hee, Lee, and Chong (2022) and Miyanishi and Nguyen (2024) theoretically and empirically showed that the cross-modal interaction embedded in the hateful memes detection problem is fully reflected in this dataset. We leave more experiments on hateful meme detection (Gomez et al. (2020)) and other tasks to future work.\nModel To comply with Experiment I, we use LLaVA-Llama2 in this experiment. For ICL example selection, we use BM25 algorithm (Robertson et al. 1996).\nDataset We focus on the Hateful Memes Challenge dataset (Kiela et al. 2020) to test our framework in the context of complex multimodal interaction. Taking into account the presence of the image confounders (two memes with identical text and different images, resulting in different labels), the one-shot experiment adopts the ICL examples with most similar texts (one meme from hateful, one meme from benign) in the labeled training set, and use the two confounders as a single set of ICL example. Since the data size is small, we use f1 score to see the precision-recall balance."}, {"title": "Results & Discussion", "content": "Motivation If the representational shift hypothesis is correct, the ICL examples could affect the prediction even if given in a format different from that of the training. The preliminary analysis shows that LLaVA (Liu et al. 2023b), a model not trained by multi-image datasets, can explain multiple images per prompt separately under some constraints (Supplementary Fig.1).\nBased on this observation, our working hypothesis for Experiment I is that, although LLMs are heavily affected by the prompt format, they could interpret the semantics without solid inductive bias to some extent. We focus on ICL with a single example since we do not see any positive clue for further concatenation in the initial exploration.\nPerformance Fig.3 and Supplementary Fig.4 summarize the performance of two LLaVA variants with or without the input of unseen format. Not surprisingly, LLaVA v1.5 outperforms v1 in all cases. Since the models are not trained with multiple-image datasets, the majority of the datasets show dropped performance in ICL. Interestingly, for LLaVA-Llama2, however, two image-text pairs boost the performance in some cases where the base performance is very low. This result supports the presence of semantics-based ICL, particularly when the task is challenging. In the case of InternVL, ICL generally resulted in decreased performance, potentially because of its high performance and multi-image resource shortage (Supplementary Fig.2). To see whether the task difficulty affects this trend, we see the performance by the number of reasoning steps, typically seen as the difficulty metric, and is provided in the GQA dataset. Divided by this subcategory, ICL performs slightly better in the larger number of steps, in contrast to the dramatically dropped performance in the smaller number of steps (Table 1, Supplementary Fig.3). Together with LLaVA, these results suggest that the semantics dominate the challenging tasks, while the formatting is more critical in established ones.\nModel Accuracy Analysis To quantify the impact of formatting and ICL examples, we model the linear mixed effect with or without the variables {z, e}, and the model variable m (Table 2). In general, m predominantly explains the accuracy variation, reflecting much higher performance for LLaVA 1.5. In z-e comparison, e has a slightly higher explanatory power, implying the significance of individual ICL example. This is further supported by the highest power of random effect when combined with m.\nRepresentation Analysis First, to see if the representation of the ICL model's question-answer dinstance vector can be linearly mapped onto a zero-shot vector, we applied a simple linear probe to get moderate explanatory power with an R2 value of 43.0 \u00b1 1.2, suggesting the presence of such mapping. Next, we applied the high-dimensional mixed effect model (Eq. 16), resulting in a much higher R2 59.2 \u00b1 2.3. This result suggests that the representational shift in the presence of formatting bias could be mapped linearly. Next, we attributed the shifted representation to the original one together with the bias information (model and dataset, Table 3). The original score shows much higher than the bias binaries themselves, suggesting that those bias are interactive with model representation. In summary, these results suggest the presence of the linear mapping before/after the representational shift, and its effect could be seen as a mixed effect together with model and dataset."}, {"title": "Experiment II: AbT ICL for Hateful Memes", "content": "Performance In comparison to the zero-shot setting, ICL significantly dropped the performance (Table 4). In contrast, AbT slightly improves the performance. These results suggest the capability of AbT in the absence of effective ICL examples. Further exploration for ineffective ICL problems will be the part of our future works."}, {"title": "Discussion", "content": "Upon the previous pioneering study by (Ren and Liu 2023), our study on Multimodal Contrastive In-Context Learning (MCICL) has yielded several important findings that contribute to our understanding of in-context learning in LLMs.\n1. Representational Shift Hypothesis: The representation analysis of two experiments supports our hypothesis. This finding provides insights into the mechanisms underlying ICL and suggests potential avenues for further optimization of ICL techniques.\n2. Impact of Input Formatting: Our results show that balancing the formatting and semantics of ICL inputs plays a crucial role in ICL performance.\n3. Anchored-by-Text ICL: The proposed Anchored-by-Text ICL approach demonstrates effectiveness in resource-constrained hateful meme detection, important implication for real-world LLM applications."}, {"title": "Limitations and Future Work", "content": "While our study provides valuable insights, there are several limitations and future research directions that warrant further investigation. Importantly, our experiments focused on a limited set of multimodal datasets and model architectures. Future work should explore the broader range of multimodal tasks and models, including but not limited to, multi-image tasks such as MMMU (Yue et al. 2024) and missing modality problem (Wang et al. 2023a; Zhao, Li, and Jin 2021). In addition, whether the representational shift causes the outcome variance is still elusive. One idea is to hire a mechanistic approach, such as path patching (Hanna, Liu, and Variengien 2023; Goldowsky-Dill et al. 2023). Training phase mechanisms such as grokking or double descent (Davies, Langosco, and Krueger 2022) should also be part of the research scope."}, {"title": "Conclusion", "content": "MCICL enhances our understanding of in-context learning in LLMs by leveraging contrastive learning principles and addressing multimodal input challenges. It demonstrates improved performance in various scenarios, particularly in challenging settings.\nOur work provides valuable insights but also highlights the need for continued research in multimodal learning complexity. MCICL opens new avenues for enhancing LLM capabilities in multimodal settings, contributing to more robust, efficient, and responsible AI systems.\nAs AI continues to evolve, approaches like MCICL will be crucial in creating more adaptable, interpretable, and effective multimodal AI systems for diverse real-world applications."}, {"title": "Appendix", "content": "Supplementary Figures\nWe leave the causal intervention to LLMs for future work. The nature of our framework, however, provides some causal explanation of the phenomena of interest, or the causality of the phenomena on the model. The causal effect could be helpful in quantitatively assessing how the phenomena of interest (e.g., unseen format, ICL example) affect the subject (LLM). For example, a widely used metric termed Average Treatment Effect (ATE) (Rubin (2008)) is defined as the average difference of outcome y where the treatment Z is given. Assuming binary treatment Z \u2208 {Zo, Z1}, ATE is formalized as:\n$$ATE = E[y | Z_1] - E[y | Z_0]$$\n(19)\nSimilarly to Eq.1, the causal effect on the prediction y of the ICL example e under the presence of unseen format bias b in comparison with the zero-shot setting could be defined as the difference of the expected prediction between ICL (b, e) * and zero-shot (b, e) = \u221a settings.\n$$ATE_{macro} = E[y | | |, D_{icl}] \u2013 E[y |, D_{query}]$$\n(20)\nSince the accuracy metric acc is the ratio of correct prediction over the samples, acc is identical to E[y], where y is a binary for the correct prediction. Therefore, analyzing the accuracy difference provides us with insights into ATE.\n$$ATE_{macro} = acc(\u0416) \u2013 acc(F)$$\n(21)\nSimilarly, the causal effect of ICL over the model on CL perspective is:\n$$AT Emicro = d_{h_{ici/h'a}} - d_{h_a/h_a}$$\n(22)\nWe attribute accuracy acc or ICL-time question-answer distance $d_{h_{ici/h'a}}$ to the linearly weighted binary variables (b, e) or zero-shot distance $d_{h_g/h_a}$, weight analysis is relevant to ATE."}]}