{"title": "Multimodal Contrastive In-Context Learning", "authors": ["Yosuke Miyanishi", "Minh Le Nguyen"], "abstract": "The rapid growth of Large Language Models (LLMs) usage\nhas highlighted the importance of gradient-free in-context\nlearning (ICL). However, interpreting their inner workings\nremains challenging. This paper introduces a novel multi-\nmodal contrastive in-context learning framework to enhance\nour understanding of ICL in LLMs. First, we present a con-\ntrastive learning-based interpretation of ICL in real-world set-\ntings, marking the distance of the key-value representation\nas the differentiator in ICL. Second, we develop an analyti-\ncal framework to address biases in multimodal input format-\nting for real-world datasets. We demonstrate the effective-\nness of ICL examples where baseline performance is poor,\neven when they are represented in unseen formats. Lastly,\nwe propose an on-the-fly approach for ICL (Anchored-by-\nText ICL) that demonstrates effectiveness in detecting hateful\nmemes, a task where typical ICL struggles due to resource\nlimitations. Extensive experiments on multimodal datasets\nreveal that our approach significantly improves ICL perfor-\nmance across various scenarios, such as challenging tasks\nand resource-constrained environments. Moreover, it pro-\nvides valuable insights into the mechanisms of in-context\nlearning in LLMs. Our findings have important implica-\ntions for developing more interpretable, efficient, and robust\nmultimodal AI systems, especially in challenging tasks and\nresource-constrained environments.", "sections": [{"title": "Introduction", "content": "Upon the explosive usage of the Large Language Model\n(LLM), in-context learning (ICL) characterizes LLM's rea-\nsoning process. Understanding its optimization mechanism\nis critical for reliable, evidence-based decision-making. Pre-\nvious works have shown that LLMs could optimize the at-\ntention weights in the gradient-free inference. The research\nscope, however, is mostly limited to simple problems like\nlinear regression or word-level natural language inference.\nThe recent advances in multimodal LLM present us with\nmore challenges. First, in addition to the linguistic format\ndependencies, which seem trivial to humans, multimodal\nICL involves arbitrarily formatted multiple modalities. Al-\nthough the research community proposes many approaches\nfor solutions in different contexts, the impact of the mul-\ntimodal ICL input formatting remains elusive. Second, ex-\nploring effective in-context examples is demanding due to\nthe limited source of high-quality multimodal datasets com-\npared to those of single modality.\nTo achieve a deeper understanding of LLM, as the gradient\ndescent hinted at attention-based optimization, the existing\ngradient-based learning method could help interpret how it\noptimizes in ICL. Specifically, Contrastive Learning (CL),\ntypically used for modality encoders and classic language\nmodels, could guide the model in mapping semantically sim-\nilar inputs to a similar location in feature space. Based on\nthe previous theoretical findings about the equivalence of\nLLM's learning process and CL, we show that CL helps\ninterpret how LLM understands multimodal ICL semantics\nunder unseen input formatting and/or resource shortage. Our\ncontribution could be summarized as follows:\n1. We propose a first CL-based interpretation of ICL in mul-\ntimodal settings, suggesting that the semantically similar\nICL examples trigger the representational shift depen-\ndent on the problem settings.\n2. We propose a CL-based analytical framework for the bias\nof multimodal input formatting and show that semanti-\ncally similar ICL examples could be helpful in challeng-\ning tasks even when presented in an unseen format.\n3. We propose Anchored-by-Text ICL, an on-the-fly infer-\nence in which LLM first generates the ICL example and\nthen performs the inference using the generated exam-\nple as an anchor for extracting the input-label relation-\nship. This approach has shown effectiveness in resource-\nlimited settings."}, {"title": "Related Work", "content": "In these few years, LLMs have been widely adapted to\nnatural language processing (Zhao et al. 2023b), showing\nremarkable in-context learning (ICL) performance (Brown\net al. 2020) with up to a few examples and without gradient-\nbased training. Massive work has tested their multimodal\ncapabilities (Zhang et al. 2024) centered on vision and lan-\nguage as a step toward general-purpose agents."}, {"title": "Interpreting Inner Workings", "content": "To achieve Trustworthy AI (Thiebes, Lins, and Sunyaev\n2021), understanding how LLMs achieve high ICL perfor-\nmance is imminent. Various interpretations have been pro-\nposed to obtain theoretical and empirical grounding behind\nICL. Typically, the interpretation studies hire a specific al-\ngorithm to interpret the dynamics of LLM's representations:\nfor example, Bayesian inference (Xie et al. 2022), kernel re-\ngression (Han et al. 2023), latent variable model (Wang et al.\n2023c), algorithm selector (Li et al. 2023b), multi-state RNN\n(Oren et al. 2024), and gradient descent (von Oswald et al.\n2023; Dai et al. 2023). Although these studies covered ex-\ntensive theoretical aspects, most empirical findings are lim-\nited to simple problems like linear modeling or simple NLP\ntasks, let alone multimodal settings.\nTo demystify LLM's remarkable multimodal ICL capabili-\nties, its training and evaluation procedure should be a clue.\nMost LLMs are trained to maximize the predicted proba-\nbility of the tokens in the training datasets (Shlegeris et al.\n2024). In multimodal problems, non-language information\n(e.g., image) is encoded as captioned text (e.g. Miyanishi\nand Nguyen (2024)) or soft prompt (e.g. Bulat and Tz-\nimiropoulos (2023)). At inference time, ICL frameworks\nmostly anchor the semantically similar examples to the test\ninput (Liu et al. 2022; Wang, Yang, and Wei 2024; Li et al.\n2023c), making it intuitive to hypothesize that the distance\nbetween ICL example and test input plays a crucial role\nin ICL. Here, we formally and empirically show that mul-\ntimodal input distance, coded during the LLM's training\nprocedure, plays an crucial role in understanding the ICL\ninputs. To provide such an distance-oriented view of ICL,\nContrastive Learning (CL) (Le-Khac, Healy, and Smeaton\n2020) could play an pivotal role. CL was initially devel-\noped as an unsupervised approach for training data distri-\nbution, and then Khosla et al. (2020) introduced supervised\nCL for labeled datasets. Before the paradigm shift to genera-\ntive models, CL was a major pre-training objective of main-\nstream language models based on a Transformer (Vaswani\net al. 2017) encoder like BERT (Devlin et al. 2019). In the\nLLM era, its main application is multimodal (e.g. vision and\nlanguage) alignment (Hu et al. 2024). CL is rather straight-\nforward for capturing the cross-input semantics since it is\ndesigned to map the inputs to the feature space based on\nconceptual similarity.\nRecently, Ren and Liu (2023) has theoretically analyzed\nthe equivalence of ICL and supervised CL without negative\nexamples and has shown its validity in simple mathemati-\ncal problem-solving. In addition, we extend the analysis to\nthe multimodal real-world datasets and propose that the se-\nmantically similar ICL examples trigger the representational\nshift in LLMs."}, {"title": "Input Formatting", "content": "Prompt engineering (Bozkurt and Sharma 2023) has tack-\nled the optimization of the instruction and task descrip-\ntion. In addition to the textual information, multimodality\n(Wang et al. 2023b) poses a new challenge - how LLMS\ncould understand the interleaved inputs of multiple infor-\nmation sources. Focusing on the image-text relationship, the\nmost straightforward format is an image followed by a sin-\ngle instruction (e.g., a single visual question-answering en-\ntry), targetted by the most state-of-the-art multimodal mod-\nels like LLaVA (Liu et al. 2023b). Another popular format\nis multi-turn conversation (Feng et al. 2023; Morgan et al.\n2023), with which the model should recognize at least the\ntwo lines of text interleaved by two images. Recent studies\nhave tackled this problem with tailored pre-training proto-\ncol (Zheng, He, and Wang 2023) and/or instruction tuning\n(Li et al. 2023a; Tang et al. 2023). In line with these works,\nthis paper quantitatively shows how the unseen format bi-\nases the LLM's comprehension of the ICL example, and that\nsemantics-formatting balance works differently for the dif-\nferent tasks."}, {"title": "Resource Shortage", "content": "Like the limited vision-and-language ability of humans with\nless visual experience (L\u00f3pez-Barroso et al. 2020; Mamus\net al. 2023), the resource shortage is a significant challenge\nfor vision-oriented models (Bai et al. 2023). Since typical\nICL involves example selection from training subset of a\ngiven task, task-specific multimodal resources, like hateful\nmemes detection datasets (Kiela et al. 2020; Gomez et al.\n2020), constrains the ICL performance. One approach to\nthis problem is to let the LLMs generate ICL examples for\ntheir own usage. For example, Wang, Yang, and Wei (2024)\nframed this problem into retrieval, and Coda-Forno et al.\n(2023) has shown that LLMs can perform meta-learning\nvia ICL. Notably, in some cases like hateful memes, forc-\ning state-of-the-art LLMs to generate positive examples is\nchallenging for safety reasons. This paper shows that LLM-\ngenerated negative examples shift the model's representa-\ntion, and mitigate this positive example constraint."}, {"title": "Preliminaries", "content": "Learning Objective of Generative Transformers\nTransformer's self-attention layer of depth $d$ maps input\ndocument $D$ to query $Q$, key $K$, value $V$ with corresponding\nweight matrix $W$. An layer is written as:\n$Q = W_Q D, K = W_K D, V = W_V D$\n$Self Attn(Q, K, V) = SoftMax(\\frac{QK^T}{\\sqrt{V_d}})V$ (1)\nIn case of generating the answer $a$ for a set of the documents\n$D_{icl} = {D_{query}, D_{ex}}$ consisting of the query $D_{query}$ with\nthe ICL example $D_{ex}$, the predicted most probable answer $\\hat{y}$\nis obtained as:\n$\\hat{y} = argmax_y p(y|Self Attn(D_{icl}))$ (2)"}, {"title": "ICL and CL", "content": "CL typically utilizes contrastive loss (Hadsell, Chopra, and\nLeCun 2006) with which the document pair ($D_1, D_2$) is\nmapped to the representation space with the guidance of a\nbinary $y_c$ (1 suggests that the documents are in a specific\ncategory, 0 otherwise). Given a distance function $dist(\u00b7,\u00b7)$,\nthe loss $C$ with hyperparameter $\\epsilon$ is defined as:\n$L(D_1, D_2) = y_c d_{D_{1/2}} + (1 - y_c) max(\\epsilon \u2013 d_{D_{1/2}}, 0)$\nwhere $d_{D_{1/2}} = dist(D_1, D_2)$ (3)"}, {"title": "ICL Example Selection", "content": "In ICL, the example $D_{ex}$ is typically extracted from the\ntraining dataset or its subset $\\mathbb{U}D_{train}$ to obtain the closest\nexample to the test input $D_{query}$.\n$D_{ex} = argmin_{D_{train}} d_{D_{train}/D_{query}}$ (8)\nWe show the effectiveness of generating the example instead\nof selecting it and discuss how it is related to CL."}, {"title": "Methodology", "content": "Outline of Our Method\nFig.1 summarizes our method."}, {"title": "Representational Shift Hypothesis", "content": "In CL interpretation (Eq.5-6), key-value distance contributes\nto attention-based optimization in ICL. Since this interpre-\ntation only presupposes the interaction of key-value pair, we\ncould extend it to the arbitrary set of test-time input frac-\ntions (e.g. instruction prompt $K_{inst}$ and the task given in\nzero-shot setting $V_{zsl}$). Since the zero-shot task consists of\nthe instruction, the task, and LLM's prediction $V_{pred}$, and the\nmodel is trained to infer the latter tokens from the former, we\npropose that the distance among the zero-shot components\naffects the generation as follows:\n$W_{zsl} = W - \\Delta W(K_{inst}, V_{zsl})$\n$W_{pred} = W_{zsl} - \\Delta W(K_{zsl}, V_{pred})$ (9)\nSimilarily, the updated ICL weight $W_{icl}$ could be formal-\nized as:\n$W_{icl} = W - {\\Delta W(K_{inst}, V_{icl}) + \\Delta W(K_{icl}, V_{zsl})}$\n$W_{pred} = W_{icl} - \\Delta W(K_{ast}, V_{pred})$ (10)\nAssuming that the overall instruction affects each task\nequally $\\Delta W(K_{inst}, V_{zsl}) \\sim \\Delta W(K_{inst}, V_{icl})$, the weight\n(and resulting representation) shifts towards example-task\ndistance.\n$W'_{pred} = W_{pred} = \\Delta W(K'_{zsl}, V_{pred}) \u2013 \\Delta W(K_{zsl}, V_{pred})$\n$= \\Delta W(K_{icl}, V_{zsl})$ (11)\nIn summary, the ICL example first affects the representation\nof the zero-shot task, and the prediction is affected via the\ntask-prediction representational shift. To test whether this\nhypothesis is correct, we analyze the distance-distance re-\nlationship."}, {"title": "Multimodal Input Formatting", "content": "Disentangling Format and Semantics The semantics of\nthe bimodal inputs and their format are entangled yet differ-\nent concepts. Since CL aims to learn the inputs' similarity\nand variance, we could assume semantic similarity as its ob-\njective while formatting as a biasing factor. In other words,\nthe formatting term $L_{fmt}$ affects the actual loss $L$ in parallel\nwith its semantic term $L_{sem}$.\n$L = L_{sem} + L_{fmt}$\n$W = W - (\\Delta L_{sem} + \\Delta L_{fmt})$ (12)\nIntuitively, within a single dataset, the second term consis-\ntently biases all the ICL examples (fixed effect). In contrast,\nthe first term should also reflect the variance of the indi-\nvidual test data points (random effect). Therefore, when the\nmodel faces a test input with an unseen format, the model\noutput for input $i$ should be interpreted as a mixed model.\n$\\hat{g}_i = {W \u2013 (\\Delta L'_{sem} + \\Delta L_{sem} + \\Delta L_{fmt})}X_i$ (13)\nWhen the ICL format is the same with the training process,\n$\\Delta L_{fmt} = 0$.\nModel Performance Analysis In the macroscopic view,\nthe effect of the unseen format should be expressed as the\nimpact of bias $b \\in {0, 1}$, and that of ICL example presence\n$e \\in {0,1}$. Intuitively, the ICL examples have random ef-\nfects due to the dependence on the content of each example.\nIn contrast, the format bias should have a fixed effect, af-\nfecting the overall performance. Note that we use accuracy\nas the metric unless stated otherwise since all Visual Ques-\ntion Answering (VQA) datasets used in our analysis hire this\nmetric. Together, the model accuracy $acc$ of a data subset $i$\ncould be modeled as:\n$acc_i(b, e) = W_b + W_e$ (14)\nTo analyze the impact across the models, the results of all\nthe models are concatenated and the variables $b$ and $e$ are\nanalyzed as an interaction term.\nRepresentation Analysis Since some of the widely used\nbenchmarks like MMBench (Liu et al. (2023c)) require on-\nline submission for evaluation, which makes reproducible\nlocal evaluation challenging, we need the unsupervised ap-\nproach. Under the our hypothesis, ICL is driven by the dis-\ntance $d_{./.}$ between the representation of the key $h_k$ and that\nof value $h_v$. In zero-shot VQA, the distance between ques-\ntion $h_q$ and answer $h_a$ would be the only clue to the model.\nIn contrast, the ICL example is concatenated to the question\n$h_{icl} = {h_{ex}, h_q}$, leading to the shift in the feature space\nand, therefore, distance with the new answer $h_a$.\nIn the spirit of the linear representation hypothesis (Park,\nChoe, and Veitch (2023)), we implement a linear mixed ef-\nfect model. Specifically, the random effect is modeled as the\nlinear weight $W_{random}$, and the fixed effect is introduced\nvia the product of $h_{zcl}$ and the embedded index representing\nthe model and the dataset with the weights $W_{fixed}$.\n$h_{icl} = (W_{random} + W_{fixed}I)h_{zsl}$ (15)\nFinally, we model the linear relationship between the query-answer distance matrix and the shifted query-new answer\nmatrix.\n$d_{h_{icl}/h'_a} = W d_{h_q/h_a} + W_o$ (16)\nAs a baseline, we use the model only with first term $h_{icl} = \nW_{random}h_{zc\u0131}$, or a simple linear projection."}, {"title": "Anchored-by-Text ICL", "content": "Generation Strategy Two major blockers must be ad-\ndressed for on-the-fly ICL example generation on hateful\nmemes. First, it requires text-image bimodal generation.\nSince only limited models (e.g. Wu et al. (2023)) have such\ncapability, we use the generated text as an anchor to cause\na representational shift, and therefore the prediction. Here-\nafter we call it anchored-by-text ICL (AbT ICL).\nSecond, most LLMs have safety limitations based on in-\nstruction tuning (Bianchi et al. (2023)). Since bypassing\nsuch limitations is neither desirable nor sustainable, we let\nthe model generate negative examples. Together, given that\ndocument D consists of text T and image I (D = (T, I))\nwith a binary label y (0 for benign, 1 for hateful), our strat-\negy is formalized as:\n$T_{icl} = argmax_T p(y = 0|T, I_{query})$\n$D_{icl} = {T_{icl}, I_{query}}$ (17)\nIn short, the model generates text that fits with a given im-\nage to compose a benign meme and uses that meme as a\nbenign example. Baselines include zero-shot and one-shot\ndetection."}]}