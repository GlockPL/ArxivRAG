{"title": "INTRABENCH: INTERACTIVE RADIOLOGICAL BENCHMARK", "authors": ["Constantin Ulrich", "Tassilo Wald", "Emily Tempus", "Maximilian Rokuss", "Paul F. Jaeger", "Klaus Maier-Hein"], "abstract": "Current interactive segmentation approaches, inspired by the success of META's Segment Anything model, have achieved notable advancements, however they come with substantial limitations that hinder their practical application in real clinical scenarios. These include unrealistic human interaction requirements, such as slice-by-slice operations for 2D models on 3D data, a lack of iterative refinement, and insufficient evaluation experiments. These shortcomings prevent accurate assessment of model performance and lead to inconsistent outcomes across studies.\nIntRaBench overcomes these challenges by offering a comprehensive and reproducible framework for evaluating interactive segmentation methods in realistic, clinically relevant scenarios. It includes diverse datasets, target structures, and segmentation models, and provides a flexible codebase that allows seamless integration of new models and prompting strategies. Additionally, we introduce advanced techniques to minimize clinician interaction, ensuring fair comparisons between 2D and 3D models. By open-sourcing IntRaBench, we invite the research community to integrate their models and prompting techniques, ensuring continuous and transparent evaluation of interactive segmentation models in 3D medical imaging.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate segmentation of anatomical structures or pathological areas is crucial in fields like radiology, oncology, and surgery to isolate affected regions, monitor disease progression, treatment planning and guide therapeutic procedures. Traditional supervised medical segmentation models have demonstrated strong performance across a range of anatomies and pathologies (Isensee et al., 2020; 2023; Huang et al., 2023; Ulrich et al., 2023). However, their effectiveness remains heavily constrained by the amount and diversity of available training data, with the quality of human label annotations serving as a critical limiting factor. Consequently, fully autonomous AI solutions have not yet reached performance needed for widespread autonomous clinical applications.\nOn the other hand, numerous semi-automatic segmentation techniques, not reliant on AI, are already in clinical practice to expedite manual annotation processes Hemalatha et al. (2018). These current ad hoc methods do not tap into the potential of AI-based automation to drastically reduce annotation time. A method that allows clinicians to segment any target with just a single click within the image could greatly enhance the efficiency of clinical workflows.\nThe release of META's Segment Anything (SAM) model represents a big leap towards making this"}, {"title": "2 INTRABENCH", "content": "The Interactive Radiology Benchmark is designed to easily enable a fair and reproducible evaluation of 2D and 3D interactive segmentation methods for 3D radiological image segmentation for the very first time. While prompting 3D models is generally straightforward, we introduce specific prompting and refinement strategies for 2D models to streamline human interaction and reduce the simulated 'Human Effort'. The proposed benchmark includes seven established models and ten datasets covering different target structures and image modalities. All datasets are publicly available and we support an automatic download and preprocessing for improved usability and reproducibility.\nMoreover, the benchmark is built with flexibility in mind, enabling seamless integration of additional methods, as visualized in Fig. 2. Researchers are invited to contribute new approaches, particularly new models, new prompting schemes, and new interesting datasets to the collection. Overall, the design of our benchmark allows for easy testing and validation of novel segmentation methods, making the benchmark a catalyst for advancing methodology for interactive 3D medical image segmentation. In the following, we present the different components of IntRaBench."}, {"title": "2.1 INITIAL PROMPTING", "content": "Prompts are a key component of any interactive segmentation method and can highly influence the overall performance of the underlying method. IntRaBenchdistinguishes between two visual prompting types. Point prompts correspond to a click of a user in the image, and box prompts refer to a box around the target structure. While there is no difference in providing a point prompt for 2D and 3D methods, a 3D box requires an additional dimension compared to a 2D box. Notably, some methods also enable a distinction between foreground and background point prompts. While 3D models allow segmenting a 3D volume natively, 2D-based models require an interaction for each slice, resulting in excessive effort, which is prohibitive for clinicians as it would take too much time in daily clinical practice. Hence, any meaningful performance comparison must account for this difference in prompting effort.\nTo increase the feasibility of 2D models for 3D applications, it is essential to reduce this effort. We propose two straightforward methods, for both point and box prompts, to explore their performance and provide a proxy for measuring the effort of human interaction.\nPoint interpolation: Let $I \\subset \\mathbb{N}$ be a set of axial indices of all foreground slices. We simulate a user by selecting n foreground points, specifically the center of the largest connected component of slice $i_1, ..., i_n \\in I$ where the $i_j$ are equally spaced within I and $i_1 = \\min(I)$ and $i_n = \\max(I)$. Then, we interpolate linearly between each point and the next one and use the intersections of the resulting lines with the axial slices as positive point prompts, as visualized in Fig. 3 c).\nPoint propagation: We simulate a user providing $\\min(I)$, $\\max(I)$, and a 2D point prompt within the median slice corresponding to the median axial index $i_m$. Given this point, the model generates a segmentation $S_m$ for the median slice. We then calculate a 'central point,' specifically the center of mass of the largest connected component of $S_m$, to use as a point prompt for the slice indexed by $i_{m-1}$. Again, we generate a segmentation $S_{m-1}$ of this slice, and create a new central point until we segment the slice with the axial index $\\min(I)$. The propagation is then repeated upwards, starting from $i_{m+1}$ and continuing until we segment the slice with the axial index $\\max(I)$. This process is visualized in Fig. 3 e).\nBox interpolation: We simulate a user providing n 2D bounding boxes, one in each of $i_1, ...., i_n \\in I$, with $i_j$ defined as in the point interpolation paragraph. Since the boxes are uniquely defined by their minimum and maximum vertices, we can interpolate between the minimum vertices as in point propagation to get a minimum vertex in each axial slice, and similarly get a maximum vertex in each axial slice, providing a box prompt in each slice. This box interpolation is exemplified in Fig. 3 d).\nBox propagation: We simulate a user providing $\\min(I)$, $\\max(I)$, and a 2D box prompt within the slice corresponding to the axial index $i_m$, m as in point propagation. The model then generates"}, {"title": "2.2 REFINEMENT PROMPTING", "content": "Refinement of previous segmentations is an important aspect of interactive segmentation models, as it allows iteratively improving the segmentation until the desired structure is segmented to a user's demands. Some interactive segmentation models allow for the refinement of initial segmentations by providing the model with the previous prediction along with a new prompt to correct errors, either through foreground clicks on false negative pixels or background clicks on false positive pixels. While this process is straightforward for 3D models, 2D models naively only allow for refinement in a slice-by-slice fashion, which again places an unrealistic burden on clinicians. Therefore, we present refinement strategies that require a reasonable level of \"Human Effort\".\nScribble refinement: To represent a user-centric refinement strategy we introduce an algorithm simulating user-created scribble prompts: At each refinement step, our proposed algorithm generates either positive or negative additional prompts. The decision to generate positive prompts follows a"}, {"title": "2.3 HUMAN EFFORT PROXY", "content": "A model's performance is highly dependent on the effort a human puts into initial prompting and refinement of the predicted masks. Generally, the effort required for 3D methods is less than that for 2D methods, although the strategies mentioned above significantly reduce the effort of 2D methods substantially. We aimed to establish a general measure of the effort a method would require from a human user. A more formalized mathematical approach involves assigning degrees of freedom (DoF) to each interaction. For instance, a point corresponds to 3 DoF, a 2D box has 5 DoF (requiring selection of the z-axis and two 2D points), and a 3D box consists of 6 DoF. However, point interpolation has 9 DoF, whereas point propagation only has 5 DoF, since it requires just the axial coordinate rather than both minimum and maximum points with 3 DoF each. From the user's perspective, however, identifying the z-coordinate demands the same level of effort as selecting a 3D coordinate by clicking at the target structure's endpoint along the z-axis. Similarly, an arbitrary scribble has significantly more DoF than a straight or parabolic line, yet the difference in effort for the user is minimal. Therefore, we define user effort in terms of the number of interactions required for a specific task. While not an exact measure, this method offers the most practical estimation of the actual effort involved from the user's perspective."}, {"title": "2.4 INTERACTIVE METHODS", "content": "In our comprehensive benchmark, we include various interactive segmentation methods. Fig. 1 illustrates the types of prompts each method supports. Iterative refinement is only possible for methods that allow a (previously predicted) mask as a prompt.\nSAM is the most prominent model from the natural image domain, that inspired many researchers to evaluate and adapt it to the domain of radiological medical images. It was trained on iteratively generated and curated 1B masks and 11M images, but not explicitly on radiological images. META's Segment Anything Model was the first to popularize interactive segmentation models (Kirillov et al., 2023).\nSAM2 is an extension of SAM that was trained on even more images and introduced support for video data (Ravi et al., 2024).\nMedSAM is an adaptation of SAM that fine-tuned SAM's weights on 1,570,263 image-mask pairs from the medical domain. It supports only a single forward pass without refinement and is limited to box prompts (Ma et al., 2024).\nSAM-Med 2D is another adaptation of SAM, fine-tuned on 4.6 million images with 19.7 million masks from the medical domain. Unlike MedSAM, it supports points, boxes, and mask prompts, allowing for refinement (Cheng et al., 2023)."}, {"title": "2.5 DATASETS", "content": "Dataset selection was a non-trivial problem for this benchmark: While models that were originally introduced in the natural image domain rarely see any radiological 3D data, the medical counterparts were often trained on all publicly available datasets that the authors could obtain. For example, MedSAM was trained using more than 60 publicly available datasets (Ma et al., 2024). Although these methods conducted their final validation on excluded datasets or at least on separate test subsets of images, the test datasets vary between models. As a result, identifying annotated datasets with interesting target structures that were not part of any of the included methods' training datasets has proven challenging.\nNevertheless, we assembled a diverse collection of ten lesser-known or recently released public datasets featuring various pathologies and organs, including CT and MRI image modalities. Specific details of these are provided in Table 1. To enhance reproducibility and eliminate barriers of entry for non-domain experts, we automated the dataset download and preprocessing, minimizing any required domain knowledge to use the benchmark. However, due to the sparsity of labeled datasets, we urge developers to exclude these datasets from their train dataset selection, as inclusion would compromise the integrity of a clean evaluation through IntRaBench."}, {"title": "2.6 EVALUATION", "content": "All interactive segmentation methods identify their target structure based on a spatial prompt, inherently resulting in instance segmentation. As a result, we evaluate on an instance-by-instance basis. Unlike in object detection, each prompt already provides information on the localization of the target structure, making detection metrics like F1-Score irrelevant. Subsequently, we rely solely on the Dice Similarity Coefficient (DSC) score as a metric. The instance-wise DSC metric is then averaged per case (i.e. per image volume), and further aggregated across all cases in the dataset, as"}, {"title": "3 EXPERIMENTS", "content": "We evaluate all seven models across various initial prompting scenarios under realistic and unrealistic effort settings. Following this, we conduct interactive experiments to simulate human refinement of model predictions. Due to the vast amount of data, we only provide a condensed version of the results for easier insights. Detailed results and the number of human interactions are provided in the Appendix B."}, {"title": "3.1 INITIAL PREDICTION", "content": "Unrealistic effort: As an upper baseline, we begin with an idealized and unrealistic scenario where each slice is prompted individually for all 2D models. In this setting, we evaluate different numbers of point prompts per slice (PPS), as well as alternating positive and negative prompts (+ PPS), and slice-wise box prompts with varying numbers of boxes per slice (BPS). Figure 4 shows that models employing box prompts achieved significantly higher average Dice scores, with SAM2 demonstrating the strongest performance across all models. Conversely, point-based prompts performed poorly, particularly for small target regions, such as small MS lesions in dataset D1 (see Appendix B.1). SAM Med2D outperforms non-medical models for point prompts. Although including positive and negative prompts and increasing the number of point prompts led to improvements, these were minor compared to the marked superiority of box-based prompts. These results highlight the limitations of point prompts, especially in cases involving small or complex anatomical structures, and emphasize the robustness of box prompts in achieving higher segmentation accuracy.\nRealistic Effort: To simulate a human-in-the-loop scenario, we evaluate various prompting strategies that avoid slice-by-slice interaction. As described in Section 2, for 2D models, we test point and box interpolation, as well as propagation, using different numbers of initial prompts. For 3D models, we explore varying numbers of Point prompts Per Volume (PPV) and 3D box prompts. Fig. 5 presents the following key findings:\n1. For all models, box interpolation with 3 or 5 initial 2D boxes is sufficient to achieve results similar to slice-wise box prompting (BPS).\n2. For SAMMed 2D, using 3 points with simple point interpolation achieves results comparable to prompting every slice.\n3. SAM 2 outperforms specialized medical models across all prompting schemes using box interpolation.\n4. Among 3D models, only SegVol is competitive to 2D models that use box prompts."}, {"title": "3.2 INTERACTIVE REFINEMENT", "content": "Finally, we evaluate the performance of the models during iterative refinement. For 2D models, this involves prompting on a slice-by-slice basis. As illustrated in Fig. 6 (left), adding refinement prompts to each slice results in a substantial performance boost. Although the proposed scribble-based refinement consistently improves outcomes, it does not achieve the same level of improvement as adding a prompt to every slice, which is expected since not all slices receive new prompts during the scribble refinements. We observed that for 2D models, it is crucial to provide the initial prompts again for each of the refinement steps. 2D models tend to over-segment the target, filling the entire slice foreground. The absence of the initial prompt leads to a complete loss of target location information, as the initial predicted mask is highly inaccurate. Our refinement likely generates negative additional prompts due to the large number of false positive pixels. In Table 5, we present refinement results from initial predictions produced by Box Interpolation. In this case, we did not include the previous point in the iterative prompts, which resulted in a performance decline during refinement.\nFor 3D models, iterative refinement also led to consistent performance improvements. Both randomly sampled prompts and those derived from refinement scribbles improved performance with each refinement iteration. Although SegVol initially performs best during initial prediction, it lacks support for further refinement. In contrast, SamMed 3D Turbo worse in the initial prediction surpasses SegVol, which was prompted using points, after several refinement steps."}, {"title": "3.3 DISCUSSION AND CONCLUSION", "content": "In this paper we introduced IntRaBench and with it, compared the performance of 2D and 3D interactive segmentation models in 3D medical imaging. We provide a holistic and transparent overview of the current state-of-the-art and highlight key findings that offer practical insights:\n1. Bounding Boxes Outperform Points: Bounding boxes consistently outperform point-based inputs by providing better spatial context, which leads to improved segmentation accuracy, especially for complex structures in radiological images. Point-based prompts lack this context, resulting in poorer performance.\n2. Iterative Refinement is Essential: The ability to iteratively refine segmentations significantly enhances model performance, particularly in challenging cases. Models that allow multiple rounds of corrections show better accuracy, making this feature crucial for clinical applications. For example SegVol reached highest performance in a static setting, however SamMed 3D Turbo is able to exceed SegVol given a few interactions, highlighting the importance of refinement.\n3. Realistic 2D prompting can match unrealistic prompting: Our introduced realistic prompting styles are able to reach and match unrealistic prompting 2D prompting methods, see Fig. 5. This unlocks 2D methods for actual clinical workflows without any performance penalties.\n4. Points fail for difficult and small structures: Contrary to claims in previous literature, point-based methods fail, likely due to previous work training and evaluating their methods on simpler target structures.\nImplications IntRaBench suggests that bounding boxes and iterative refinement should be prioritized in the design of segmentation models for medical imaging, particularly when addressing complex radiological images. Furthermore, it underscores the importance of including diverse, difficult tasks in training data to improve model generalization for clinical use. It is also crucial to test 2D models in scenarios that simulate real human interaction, ensuring that segmenting a volumetric image does not require unreasonable effort by prompting the model slice-by-slice.\nA key limitation of this work is that it only simulates \"Human Effort\u201d. While this approach provides valuable insights into model performance by providing a proxy for the simulated \"Human Effort\u201d, it falls short of capturing the full complexity and practical challenges of real clinical applications. As future work, a comprehensive study involving clinicians is essential to assess different prompting strategies in real-world environments. Such a study should not only evaluate segmentation performance but also measure the time required for annotation, offering critical insights into the practical feasibility and efficiency of these models in clinical practice.\nTo conclude, our proposed IntRaBench presents a powerful tool for the future of interactive segmentation research in medical imaging, serving as a catalyst for innovative solutions by enabling a fair and reproducible comparison between leading methods. One of the standout potentials is its ability to streamline the evaluation of both 2D and 3D segmentation models, allowing for more realistic and clinically relevant testing conditions. By focusing on human interaction and the efficiency of iterative refinement, IntRaBench opens new avenues for research, including understanding the impact of different interaction strategies and how they reduce clinician effort. Not only does this benchmark address the existing gaps in evaluation standardization, but it also offers a unique opportunity to refine segmentation performance on pathologies often overlooked, such as small lesions. The open-source nature of the benchmark further encourages continuous contributions, allowing researchers to test new methods and prompting strategies seamlessly within this framework. Future work using IntRaBench can reveal novel insights into the balance between performance and clinician involvement. This potential to improve real-world clinical applications, especially by reducing the labor intensity of medical professionals, marks IntRaBench as a crucial tool in catalyzing meaningful research progress."}, {"title": "A MODEL SPECIFICATIONS", "content": "A.1 SAM\nSAM is compatible with multiple image encoders, particularly the ViT family from Dosovitskiy et al. (2021). We used the default and best-performing model with ViT-Huge. To ensure high-quality inputs for the model, we performed slice-wise inference by extracting slices from the inplane-plane axis. Each slice was normalized by first clipping values outside the 0.5th and 99.5th percentile of the volume's intensity distribution and then scaling the values to [0, 255]. The image was repeated three times along the channel axis to produce an RGB-like image. Internally, SAM resizes these slices to 1024 pixels for the longest side with the shorter side being padded to 1024 pixels if needed to maintain square dimensions. Finally, the images are normalized using the model's pre-stored mean and standard deviation as suggested by the original implementation. Inference was restricted to slices containing foreground. After prediction, the slices were reassembled into a volume, inverse transformed to the original coordinate system, and metrics were computed in the original image space.\nA.2 SAM2\nSAM2 supports multiple image encoders, specifically the Hiera family of Ryali et al. (2023). We used the best-performing model, Hiera-L. We clip the intensity values of the volumes based on the 0.5th and 99.5th percentiles, extract each slice along the through-plane, and make the images RGB-like just as with SAM. The images are then rescaled to 1024 \u00d7 1024 pixels and again normalized using the mean and standard deviation provided together with the pretrained weights. Aggregation and inverse transformation are then performed similarly to SAM.\nA.3 MEDSAM\nTo apply the model slice-wise, we slice the input volume as with SAM, and then clip each slice based on their 0.5th and 99.5th percentile values. The images are then made RGB-like by repeating thrice along a new channel-dimension, rescaled to 1024 \u00d7 1024 pixels and then normalised to [0, 1]. Aggregation and inverse transformation are performed similarly as with SAM.\nA.4 SAM-MED2D\nTo apply the model slice-by-slice, we slice the input volume as with SAM, and then clip each slice based on their 0.5th and 99.5th percentile values same as with MedSAM. The slices are then made RGB-like and converted to a [0, 255] scale as in SAM's preprocessing. The slices are then standardized using a mean and standard deviation provided along with the model and resized to 256 \u00d7 256 pixels. Aggregation and inverse transformation are performed similarly as with SAM."}, {"title": "A.5 SAM-MED3D", "content": "The model is 3D so no slicing is needed. The volume is respaced to 1.5 x 1.5 x 1.5 mm and then clipped based on its 0.5th and 99.5th percentiles. SAMMed3D performs inference on a 128x128x128 crop. The crop is centered around our point prompt if there is only one point prompt passed, and around the centroid of our prompts if multiple points are passed simultaneously. For subsequent refinement steps, the crop remains unchanged. The predicted crop is inserted back in its correct position within the wider coordinate system and then respaced back to the original spacing so that evaluation takes place in the corresponding native image space."}, {"title": "A.6 SAM-MED3D TURBO", "content": "SAM-Med3D Turbo is an updated checkpoint for SAMMed-3D and so we perform the same pre- and postprocessing."}, {"title": "A.7 SEGVOL", "content": "Intensity values are clipped by its 0.5th and 99.5th percentiles. The mean and standard deviation of the foreground voxels are used for zscore normalization. The values are then rescaled to a [0,1]. Finally, the volume is cropped to its foreground. A first 'zoom-out' inference is performed on this image, followed by a 'zoom-in' sliding window inference. The predicted volume is then transformed back to the original space and compared with the unprocessed ground truth to calculate metrics."}, {"title": "B ADDITIONAL RESULTS", "content": "B.1 INITIAL PREDICTION - UNREALISTIC EFFORT"}]}