{"title": "DOPRA: Decoding Over-accumulation Penalization and Re-allocation in Specific Weighting Layer", "authors": ["Jinfeng Wei", "Xiaofeng Zhang"], "abstract": "In this work, we introduce DOPRA, a novel approach designed to mitigate hallucinations in multi-modal large language models (MLLMs). Unlike existing solutions that typically involve costly supplementary training data or the integration of external knowledge sources, DOPRA innovatively addresses hallucinations by decoding specific weighted layer penalties and redistribution, offering an economical and effective solution without additional resources. DOPRA is grounded in unique insights into the intrinsic mechanisms controlling hallucinations within MLLMs, especially the models' tendency to over-rely on a subset of summary tokens in the self-attention matrix, neglecting critical image-related information. This phenomenon is particularly pronounced in certain strata. To counteract this over-reliance, DOPRA employs a strategy of weighted overlay penalties and redistribution in specific layers, such as the 12th layer, during the decoding process. Furthermore, DOPRA includes a retrospective allocation process that re-examines the sequence of generated tokens, allowing the algorithm to reallocate token selection to better align with the actual image content, thereby reducing the incidence of hallucinatory descriptions in auto-generated captions. Overall, DOPRA represents a significant step forward in improving the output quality of MLLMs by systematically reducing hallucinations through targeted adjustments during the decoding process.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, Multimodal Large Language Models (MLLMs) [1, 2, 11, 13, 28, 32, 42, 49, 56] have made groundbreaking advancements, fundamentally altering the way Al interacts with visual data and significantly enhancing fluent communication based on image semantic content. Despite their remarkable performance in handling a range of visually-centered tasks [4, 7, 8, 23, 51], understanding complex contexts[21, 52], or generating coherent narratives [4, 6, 23], MLLMs still grapple with a profound challenge: the \"hallucination\" problem. This refers to instances where MLLMs generate inaccurate or disjointed responses to visual inputs by incorrectly identifying nonexistent objects, attributes, or relationships within provided images. Such errors carry significant risks, particularly in high-stakes applications like autonomous driving [35, 39], where misinterpreting visual cues could lead to life-threatening situations. While numerous methods [30, 36, 43, 46, 55] have been proposed to tackle hallucination issues, these often require costly interventions such as fine-tuning with annotated data [26], incorporating auxiliary models, or leveraging external knowledge sources.\nThis paper delves into addressing the hallucination conundrum during MLLMs' reasoning process without relying on supplementary data, external models, or specialized knowledge. Our investigation stems from a novel discovery related to what we term as \"summary tokens\" in the generation sequence, where attention weights accumulate early on. Analogous to recently discovered \"anchor tokens\" in the NLP domain [44], our analysis of self-attention graphs reveals a recurring pattern that frequently follows the generation of tokens with columnar attention structures, often leading to hallucinatory content. These summary tokens themselves tend not to carry substantial informational content (such as punctuation). However they appear to play a critical role in aggregating prior knowledge and guiding subsequent sequence generation.\nThe reliance on this aggregation pattern seems to induce hallucinations in contemporary MLLMs. To delve deeper into this phenomenon and provide a visual representation of the existence and impact of \"summary tokens\" in the generated sequences, we conducted theoretical analyses of their potential roles in text generation and detailed visualization of all tokens' self-attention weights. The experimental results show that there indeed exist specific tokens with disproportionately high attention weights relative to others. These high-weight tokens act as pivotal hubs, condensing the core meaning from preceding generated content. Typically, visual-related tokens are placed at the beginning of the sequence to ground the model's response in visual comprehension. However, as the generated text grows longer, visual information can become diluted through these summary tokens since they fail to adequately encapsulate the visual context's entire richness. Consequently, later tokens may overly depend on recent summary tokens while disregarding initial image-representative markers, thereby giving rise to model-bias-induced hallucinations; for example, inferring the presence of a \"car\" or \"person\" merely from the mention of \"road\" earlier in the sentence. Moreover, an increased number of summary tokens tends to exacerbate the likelihood of MLLM hallucinations."}, {"title": "2 RELATED WORK", "content": "In recent years, Multimodal Large Language Models (MLLMs)[1, 2, 11, 28, 32, 42, 45, 56] have rapidly emerged and become the focus of both academia and industry. Since 2021, a series of representative models such as CLIP [37] and BLIP [24, 25] have pioneered large-scale pre-trained multimodal models, demonstrating the powerful ability to deeply integrate natural language with visual information, enabling accurate image description, cross-modal reasoning, and other functions. Entering 2023, this field is even more explosive, including but not limited to GPT-4V [1], LLaVA [32], minGPT-4 [56], InstructBLIP [11], Qwen-VL [2], CogVLM [45], and many other new multimodal macromodels have emerged one after another, which further enhance the model's ability of understanding and generating multimodal inputs, and make the MLLM get closer and closer to the general-purpose Artificial Intelligence [3, 16] in its ideal form."}, {"title": "2.1 MLLM Development and Capacity", "content": "In recent years, Multimodal Large Language Models (MLLMs)[1, 2, 11, 28, 32, 42, 45, 56] have rapidly emerged and become the focus of both academia and industry. Since 2021, a series of representative models such as CLIP [37] and BLIP [24, 25] have pioneered large-scale pre-trained multimodal models, demonstrating the powerful ability to deeply integrate natural language with visual information, enabling accurate image description, cross-modal reasoning, and other functions. Entering 2023, this field is even more explosive, including but not limited to GPT-4V [1], LLaVA [32], minGPT-4 [56], InstructBLIP [11], Qwen-VL [2], CogVLM [45], and many other new multimodal macromodels have emerged one after another, which further enhance the model's ability of understanding and generating multimodal inputs, and make the MLLM get closer and closer to the general-purpose Artificial Intelligence [3, 16] in its ideal form."}, {"title": "2.2 Two-Track Path to MLLM Development", "content": "The current research and development of multimodal large models show two paths. On the one hand, researchers focus on continuously expanding the training dataset and model parameter sizes of the models in the hope of significantly improving their accuracy and generalization performance by increasing the model capacity. The new generation of multimodal large models, such as LLaVA-1.6-34B [32], GPT-4V [1], InstructBLIP [11], etc., are strong examples of this development trend, which have demonstrated their excellent capabilities in various complex multimodal tasks by virtue of their large number of parameters and rich training resources. However, although such large models continue to break the performance ceiling, the theoretical and technical challenges behind them cannot be ignored, especially in terms of resource consumption and computational efficiency. On the other hand, another research direction is to explore the intrinsic potential of small multimodal models [9, 48, 54, 57], and seek to achieve comparable or even similar functional effects as those of large-scale models in a smaller parameter space. The goal of this path is to optimize the model structure and training methods so that it can perform efficient multimodal understanding and generation under limited hardware conditions and computational costs. Although some breakthroughs have been made in such efforts, even the increasingly sophisticated small-scale models are still unable to completely escape from the \"hallucination\", a core problem shared by large multimodal models."}, {"title": "2.3 Strategies for Solving the MLLM Hallucination Problem", "content": "The term \"hallucination\" [15, 20, 27, 31, 34, 50] refers to the fact that multimodal models, when processing multimodal inputs, sometimes produce content that does not correspond to the actual inputs or is even fictitious. Aiming at this key problem, which seriously affects the credibility and practicality of models, the academic community has carried out a lot of fruitful research work and proposed several innovative solutions. Among them, RLHF [40, 47, 53] (Reinforcement Learning from Human Feedback) is an approach that relies on human feedback reinforcement learning techniques, which manually evaluates and guides model outputs, prompting the model to pay more attention to factual basis and logical consistency in the subsequent generation process. DoLa [10] improves the model's ability to capture and reproduce factual information when generating text by comparing the decoded information at different levels within the model. The Woodpecker [46] takes a two-stage approach to hallucination, first optimising the model initially with DINO [33] (a self-supervised learning framework), and then correcting the model's multimodal outputs by combining it with image captioning techniques. The OPERA [19] research team proposes a novel strategy to effectively mitigate the false inferences and hallucination representations generated by multimodal large models due to over-trusting one modality when processing complex scenes by applying the Over-Trust Penalty and Retrospection Allocation mechanisms. In the specific application scenarios of visual-linguistic modelling, the VCD [22] technique helps to identify and correct the object hallucination that may occur when the model is describing or reasoning about an image by introducing the visual contrast decoding link. In conclusion, whether following the traditional route of increasing model size or seeking the innovative path of efficient small-scale models, suppressing the \"hallucination\" phenomenon of large multimodal models has become an important issue of common concern to researchers, and substantial progress has been made in several cutting-edge researches. These strategies not only enrich the design and optimisation of multimodal models, but also lay a solid foundation for the construction of more accurate, reliable and universal multimodal intelligent systems in the future."}, {"title": "3 METHOD", "content": "In this section, we will first show the model generation process by introducing the modeling framework of MLLM. Then we will talk about DOPRA's Over-Accumulation Penalization and Re-allocation Decoding Strategies as shown in Fig. 3. Finally, it will be shown how high response works."}, {"title": "3.1 Anatomy of MLLM generation mechanism", "content": "Integration of Input Constructs. For the input construction phase of MLLM, the core is integrating two types of data sources: image and text. Regardless of the specific architecture, such models usually employ a visual coder to extract visual element features from the original image, and then transform and incorporate these features into the input space of the language model through a cross-modal mapping mechanism. We label this set of transformed visual elements as the set x = {x0, X1, ..., XN-1}, where N represents the number of visual elements and is fixed in many cases. Meanwhile, the textual input is processed by the disambiguation technique and is represented as the sequence xp = XN, XN+1, ..., XM+N\u22121. Finally, these two types of tokens are sequentially spliced to form the complete input sequence {x}}=01, where T is the sum of the total number of tokens of the image and text, T = N + M.\nModel forward propagation. The MLLM follows an autoregressive model for training and uses a causal attention mechanism. Within the model, each token predicts the immediately following token based on the information of all the tokens preceding it, which is mathematically formulated as:\nh = MLLM(xi)\nh = {ho, h1,..., hy-1} (1)\nHere, h is the sequence of hidden state vectors output by the last layer of the MLLM. Next, the model uses the vocabulary header mapping function H to map the hidden state into a probability distribution for the next token prediction:\np(xt|x Here x<t is a compact representation of all previous tokens, and X represents the entire vocabulary set.\nDiverse Decoding Strategies. Based on the predicted probability distribution p(xt|x"}, {"title": "3.2 \"Over-accumulation\" Attention-based Penalty", "content": "As shown in Fig. 4, in addressing the issue of delayed manifestation-whereby patterns indicative of over-reliance on potentially hallucinated information emerge only after several tokens have been decoded-we introduce the \"Over-accumulation Attention Penalty\" mechanism. This approach cumulatively applies a penalty to the beam search scores during generation, this method selectively targets and penalizes the accumulation of attention weights in a specified layer, particularly the 12th layer,influencing both the current token selection and the overall candidate sequences, thereby reducing the likelihood of selecting outputs containing hallucinations.\nTo realize this concept, we focus on the self-attention weights in the 12th layer local context window. Consider the generated sequence until time step t, denoted as {x}-1), and the causal self-attention weights used in Layer 12 to predict the next marker {@t-1,j}= pertaining to the next token prediction, where w = SoftMax (OKT/VD) and Q, K, D represent query, key features, and the feature dimension, respectively. To capture the accumulation of knowledge aggregation patterns, we define a layer-specific local window of attention Wt-1 for layer 12 as follows:\nW-1,a=12 = {w}=-k, where w = {ij}=-k and a = 12 (3)\nHere, k represents the size of the local window cropped from the attention map, and wij is the attention weight given by token j to token i. Notably, we exclude attention weights from image tokens or prompt tokens, focusing exclusively on generated tokens (t-k \u2265 N + M). Additionally, we take the maximum attention weight across multi-heads and renormalize these values as they often reflect high model confidence.\nUpon obtaining W-1, we preprocess the attention weights by setting the upper triangle to zero and scaling up the remaining attention values for better representation, as shown in Equation :\nW-1,a=12 = {w}=-k where w\u00b2 = { for a = 12 (4)\nThe scaled attention matrix's lower triangle undergoes columnwise multiplication to generate a vector of scores, where higher scores suggest stronger knowledge aggregation patterns. The maximum value of this column-wise score vector serves as the pattern's descriptor:\nt-12() = t-1 \u03a0 j=t-k \u03c3(W2/ j), c = arg max (W\u2264t)(5)\nTo efficiently apply the penalty without distorting the model's predictions towards unreasonable outputs, we form a candidate set Y comprising the top-Ncan logits from each beam, where |Y| = Ncan \u00d7 Nbeam and Nbeam is the number of beams. Then, we integrate the pattern metric (w\u2264t) into the model logits to predict the next token while constraining it within Y:\np(xt x Here, w\u2264t summarizes all attention weights up to time step t, and a is a tunable hyperparameter controlling the strength of the penalty applied to the logit. By introducing the \"Over-accumulation Attention Penalty\" with attention weight a = 12 specifically emphasized, we aim to mitigate the risk of over-trust in potential hallucinations and guide the model toward more reliable generations."}, {"title": "3.3 Penalization and Re-allocation Strategy", "content": "Utilizing the \"Over-accumulation\" attention-based Penalty, we can proficiently detect the emergence of knowledge aggregation patterns after the generation of several consequent tokens. Ordinarily, the penalty discourages candidates exhibiting these patterns, thus encouraging the selection of others. Despite this, there are instances where all candidates are penalized even though hallucination has already taken place. This situation drives us to revisit the root cause of these aggregation patterns: they arise from early tokens excessively trusting the summary token, and the penalty fails to rectify this over-reliance. Therefore, a natural yet assertive solution is to eliminate tokens leading to hallucination and reallocate by choosing suitable tokens after the summary token. This leads us to propose the Retrospective Allocation strategy.\nWhen the decoding process encounters a knowledge aggregation pattern and hallucination appears unavoidable, it reverses to the summary token and selects alternative candidates for the next token prediction, excluding those chosen earlier. The empirical condition for triggering retrospective decoding is based on the location overlap of the maximum values in the column-wise scores corresponding to multiple successive tokens, with a manually set threshold count denoted as r. Location counting proves more robust and general compared to directly using the maximum value that varies across different models.\nThe complete retrospective process is detailed in Fig. 4. Leveraging the insights from Section 3.2, we can easily determine the location coordinate c of the maximum score through Eq. (5). Following this, we establish the set of location coordinates for the recently decoded tokens Xt\u22121, ..., Xt\u22121, which is given by:\nC = {c: c = arg max \u03a0 \u03c3(Wi,j), z \u2208 [t \u2212 l, t \u22121]} (7)\nHere, l > r is usually specified, and by default, we set l = k. Given a sequence (x0, X1,...,xt-1} and its associated recent location coordinate set C, we can assess the consistency of these coordinates. Formally, the overlap count is calculated as:\nNoverlap = \u03a3 Ic=s, where s = Mode(C) (8)\nI is the indicator function returning 1 if the condition holds and 0 otherwise, and Mode gives the most frequent value in a set. If Noverlap \u2265 r, we initiate the retrospective action, considering s = Mode(C) as the position of the summary token.\nIn the event that the sequence {X0, X1, ..., Xs, . . ., xt-1} displays a knowledge aggregation pattern at the summary token xs, the decoder rewinds to the subsequence {x0, x1,...,xs} and selects a new next token from the complementary set Y \\ {xs+1}. To ensure progressive rollback, we require that the rollback location s monotonically increases. Moreover, we impose a maximum rollback limit \u03b2; if xs reaches its rollback cap, we consider rolling back to {X0, X1, ..., Xs-1}."}, {"title": "3.4 High-Response Region Visualization and Cross-modal Interaction", "content": "To explore the relationship between generated text tokens and their correspondence with image attention in a more vivid manner, we visualize the high-response regions of the top 50 scores in Fig. 5. Given user input, we generate a text-guided query vector Qt \u2208 RMXC, where M denotes the number of queries. As shown in Fig. 3, this cross-modal interaction primarily occurs within the text decoder and can be readily instantiated using BERT [12] or Q-former [24] models. The generated text query Qt encapsulates salient visual cues that are most relevant to the user's command. Employing the text query Qt along with the visual embeddings Xu, we can effectively generate representative tokens for the LLM that capture essential visual features. Specifically, the mixed attention mechanism aims to aggregate and condense visual features related to the text into a single context token. This process, depicted in Fig. 3, takes Qt and X as inputs and formulates the mixed embedding of text and image, Et \u2208 R1\u00d7C, as:\nEt = Qt \u00d7 XT, (9)\nContrary to Q-former, which employs 32 visual queries as LLM markers, our approach uses only the text query Qt to aggregate visual features with high response scores relative to the input command. Consequently, the compressed embedding Et efficiently retains the most critical visual cues associated with the user's input. Finally, by normalizing Et and creating a heatmap, we can visually inspect whether the generated text corresponds well with the corresponding image content. Through this method, not only do we gain insight into how the model combines textual and visual information during generation, but we also clearly discern which visual elements play a decisive role in the generative process."}, {"title": "4 EXPERIMENT", "content": "Decoding strategies. Regarding decoding strategies, we have executed a variety of approaches for comparison and optimization. These include greedy decoding, which selects words at each step based on their highest probability; beam search [5, 14, 41] decoding with varying numbers of beams (Nbeam set as 5, 4, 3, 2, 1), allowing us to explore the impact of different search space widths; top-p nucleus sampling [17], using a standard setting of p = 0.9 to concentrate on the main body of the probability distribution; and the introduction of VCD [22] method that addresses object hallucination issues in large-scale models. For specialized decoding algorithms, like the DoLa [10] method designed to mitigate hallucinations in LLMs, within our experiments, we selected multiple candidate pre-mature layer indices (\"0, 2, 4, 6, 8, 10, 12, 14\") along with a fixed mature layer index at 32 to achieve fine-grained control over the internal decision-making process of the model. During the beam search decoding phase in the DOPRA experiment, we also set the scaling factor \u03c3 to 50 to ensure effective discrimination of attention weights in the knowledge aggregation mode, where salient regions receive values greater than 1 while secondary areas get less than 1. Moreover, we established a default candidate number Ncan of 5, understanding that this hyperparameter can be adjusted but, in this experimental stage, we primarily study performance under default settings, considering that larger Ncan values could significantly increase the computational cost of the decoding process. Implementation details. Lastly, for other key hyperparameters influencing the decoding behavior in the LLaVA-1.5 model, we uniformly set \u03b1 = 1, \u03b2 = 5, and r = 15, these configurations help maintain a stable experimental environment and allow us to focus on the effectiveness analysis of the proposed decoding strategies. In summary, the DOPRA experiment delves deeply into exploring the performance boundaries of the LLaVA-1.5 model on multimodal tasks by employing a series of meticulously designed decoding strategies and parameter tuning. The aim is to validate the effects of different decoding techniques on the model's accuracy and robustness."}, {"title": "4.1 Experimental Setup", "content": "Decoding strategies. Regarding decoding strategies, we have executed a variety of approaches for comparison and optimization. These include greedy decoding, which selects words at each step based on their highest probability; beam search [5, 14, 41] decoding with varying numbers of beams (Nbeam set as 5, 4, 3, 2, 1), allowing us to explore the impact of different search space widths; top-p nucleus sampling [17], using a standard setting of p = 0.9 to concentrate on the main body of the probability distribution; and the introduction of VCD [22] method that addresses object hallucination issues in large-scale models. For specialized decoding algorithms, like the DoLa [10] method designed to mitigate hallucinations in LLMs, within our experiments, we selected multiple candidate pre-mature layer indices (\"0, 2, 4, 6, 8, 10, 12, 14\") along with a fixed mature layer index at 32 to achieve fine-grained control over the internal decision-making process of the model. During the beam search decoding phase in the DOPRA experiment, we also set the scaling factor \u03c3 to 50 to ensure effective discrimination of attention weights in the knowledge aggregation mode, where salient regions receive values greater than 1 while secondary areas get less than 1. Moreover, we established a default candidate number Ncan of 5, understanding that this hyperparameter can be adjusted but, in this experimental stage, we primarily study performance under default settings, considering that larger Ncan values could significantly increase the computational cost of the decoding process. Implementation details. Lastly, for other key hyperparameters influencing the decoding behavior in the LLaVA-1.5 model, we uniformly set \u03b1 = 1, \u03b2 = 5, and r = 15, these configurations help maintain a stable experimental environment and allow us to focus on the effectiveness analysis of the proposed decoding strategies. In summary, the DOPRA experiment delves deeply into exploring the performance boundaries of the LLaVA-1.5 model on multimodal tasks by employing a series of meticulously designed decoding strategies and parameter tuning. The aim is to validate the effects of different decoding techniques on the model's accuracy and robustness."}, {"title": "4.2 Quantitative Results", "content": "DOPRA evaluation on hallucinations using CHAIR. The Caption Hallucination Assessment with Image Relevance (CHAIR) [38] is a tailored metric for measuring object hallucination issues in image captioning tasks. For DOPRA, we utilize CHAIR to quantify the extent of hallucinated objects by computing the ratio of objects mentioned in the generated description that are absent in the ground-truth label set. CHAIR offers two separate assessments: CHAIRS (denoted as Cs) measures sentence-level hallucinations and CHAIRI (denoted as C\u2081) measures image-level hallucinations, mathematically expressed as:\nCs = (hallucinated objects/all mentioned objects)\nC\u2081 = (captions w/ hallucinated objects/ all captions)\nOn the MSCOCO dataset [29], which encompasses over 300,000 images annotated with 80 objects, we perform CHAIR evaluations. Specifically, we randomly sample 500 images from the validation set of COCO 2014 and prompt different MLLM models with \"Please describe this image in detail.\" to generate their descriptions. To ensure fairness, we restrict the maximum number of new tokens for generating descriptions of both long and short lengths. As displayed in Table 1, DOPRA demonstrates a clear superiority over baseline decoding methods in both Cs and C\u2081 metrics. This advantage is consistent across both long and short description generations.\nDOPRA evaluation on hallucinations using POPE. The Polling-based Object Probing Evaluation (POPE) [27] is a recently developed method aimed at assessing hallucination issues in MLLMs. Analogous to CHAIR, POPE scrutinizes object hallucination by prompting the model with an inquiry akin to \"Is There a  in the image?\" to gauge whether the model correctly identifies the presence or absence of a specific object in the given image. POPE includes three distinct evaluation scenarios: \"random\", \"popular\", and \"adversarial\". In this section, we assess DOPRA's effectiveness in reducing hallucinations in both extended descriptions and concise VQA answers as shown in Table 1. We show the difference between DOPRA and OPERA in attention punishment in Fig. 5. It can be seen that OPERA only carries out attention punishment in the last layer of Transformer, namely layer 32, while we observe early attention accumulation in the middle layer of transformer between layer 12 and 18, so we choose to intervene in layer 12. Our intervention"}, {"title": "4.3 High Response Example Analysis", "content": "As shown in Fig. 6, in this section, we delve into the significant advancements and core mechanisms within the realm of caption generation models-specifically focusing on the DOPRA algorithm-in addressing and rectifying caption hallucination phenomena. We begin by presenting an extensive examination of real-life cases where caption hallucinations have been effectively corrected using DOPRA, juxtaposing pre- and post-correction captions in situations such as \"is there any dog?\", thereby illustrating DOPRA's substantial improvements in accurately capturing and conveying the actual content of images.\nSubsequently, we explore from the perspective of attention mechanisms how DOPRA systematically refines its strategy for allocating attention weights to precisely latch onto key visual features during caption generation. This includes, among other things, displaying different levels of attention maps and their evolution throughout model optimization, highlighting how the model learns to disregard irrelevant information and hone in on decisive visual cues within an image, say, when discerning whether or not a dog is present.\nTo further unravel the inner workings of DOPRA's efficacy in enhancing caption veracity, we conduct a detailed investigation into the genesis and case studies of high-response regions. Using visualization techniques, we demonstrate visually how the model responds differently to various areas of the input image, particularly pinpointing those critical regions with exceedingly high response values. These high-response zones not only reveal the pathways the model follows in recognizing and interpreting image content but are also vital for understanding the information extraction and decision-making processes involved in generating accurate captions. More visualizations can be found in the attached supplementary material."}, {"title": "5 DISCUSSION AND LIMITATIONS", "content": "The illusions generated by Large Language Models (LLMs) may result from inadequate generalization or the model's knowledge not being updated in a timely manner. We believe the illusion issues in Multimodal Large Language Models (MLLMs) are primarily attributed to the visual component, possibly due to the coarse granularity of features (a problem that also needs addressing, as current architectures like Q-Former tend to lose much information), or possibly due to the use of noisy data during the alignment phase, or insufficiently detailed alignment.\nThe illusion observed in captions is merely a superficial phenomenon; the root cause lies in the process of encoding by the vision encoder, which is essentially a compression process, leading to a significant loss of original image information. This fundamental flaw in the architecture prevents effective captioning based on visual hidden representations and might even lead to illusions. This is because the vision encoder, CLIP, is trained on images and highly noisy captions.\nOur overarching idea is that DOPRA is merely a temporary solution, and there will undoubtedly be a need for end-to-end fine-tuning and improvements in perceptual capabilities in the future. Future research may explore avenues such as implementing more meticulous data alignment procedures that go beyond aligning entire images and texts at a superficial level, or incorporating finegrained visual features to overcome the perceptual limitations of CLIP, which predominantly focuses on high-level semantic understanding rather than nuanced visual details."}, {"title": "6 CONLUSION", "content": "In conclusion, DOPRA introduces a novel and cost-effective approach to address the prevalent issue of hallucination in multimodal large language models (MLLMs), thereby enhancing their precision and reliability for practical applications. By innovating a method that penalizes decoding over-accumulation and reallocates attention in specific weighting layers, DOPRA circumvents the need for additional training data or the integration of external knowledge bases, setting it apart from existing methods. This technique is grounded in a deep understanding of the mechanisms underlying hallucinations in MLLMs, particularly the disproportionate reliance on summary tokens to the detriment of image-relevant information in certain critical layers. Through the application of a weight stacking penalty and a strategic reallocation during the decoding phase, DOPRA effectively broadens the context considered by the model, while ensuring that generated captions more accurately reflect the authentic content of images. The implementation of this method marks a considerable advancement in the development of MLLMs by systematically reducing the occurrence of hallucinations, thereby improving the overall output quality of these models. DOPRA's innovative approach not only signifies a significant leap towards resolving a stubborn challenge in the field of artificial intelligence but also paves the way for future research and development in enhancing the interpretative and generative capabilities of multimodal systems."}]}