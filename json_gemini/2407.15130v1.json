{"title": "DOPRA: Decoding Over-accumulation Penalization and Re-allocation in Specific Weighting Layer", "authors": ["Jinfeng Wei", "Xiaofeng Zhang"], "abstract": "In this work, we introduce DOPRA, a novel approach designed to mitigate hallucinations in multi-modal large language models (MLLMs). Unlike existing solutions that typically involve costly supplementary training data or the integration of external knowledge sources, DOPRA innovatively addresses hallucinations by decoding specific weighted layer penalties and redistribution, offering an economical and effective solution without additional resources. DOPRA is grounded in unique insights into the intrinsic mechanisms controlling hallucinations within MLLMs, especially the models' tendency to over-rely on a subset of summary tokens in the self-attention matrix, neglecting critical image-related information. This phenomenon is particularly pronounced in certain strata. To counteract this over-reliance, DOPRA employs a strategy of weighted overlay penalties and redistribution in specific layers, such as the 12th layer, during the decoding process. Furthermore, DOPRA includes a retrospective allocation process that re-examines the sequence of generated tokens, allowing the algorithm to reallocate token selection to better align with the actual image content, thereby reducing the incidence of hallucinatory descriptions in auto-generated captions. Overall, DOPRA represents a significant step forward in improving the output quality of MLLMs by systematically reducing hallucinations through targeted adjustments during the decoding process.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, Multimodal Large Language Models (MLLMs) [1, 2, 11, 13, 28, 32, 42, 49, 56] have made groundbreaking advancements, fundamentally altering the way Al interacts with visual data and significantly enhancing fluent communication based on image se-mantic content. Despite their remarkable performance in handling a range of visually-centered tasks [4, 7, 8, 23, 51], understanding complex contexts[21, 52], or generating coherent narratives [4, 6, 23], MLLMs still grapple with a profound challenge: the \"hallucination\" problem. This refers to instances where MLLMs generate inaccurate or disjointed responses to visual inputs by incorrectly identifying nonexistent objects, attributes, or relationships within provided images. Such errors carry significant risks, particularly in high-stakes applications like autonomous driving [35, 39], where misinterpreting visual cues could lead to life-threatening situations. While numerous methods [30, 36, 43, 46, 55] have been proposed to tackle hallucination issues, these often require costly interven-tions such as fine-tuning with annotated data [26], incorporating auxiliary models, or leveraging external knowledge sources.\nThis paper delves into addressing the hallucination conundrum during MLLMs' reasoning process without relying on supplementary data, external models, or specialized knowledge. Our investi-gation stems from a novel discovery related to what we term as \"summary tokens\" in the generation sequence, where attention weights accumulate early on. Analogous to recently discovered \"an-chor tokens\" in the NLP domain [44], our analysis of self-attention graphs reveals a recurring pattern that frequently follows the gener-ation of tokens with columnar attention structures, often leading to hallucinatory content. These summary tokens themselves tend not to carry substantial informational content (such as punctuation). However they appear to play a critical role in aggregating prior knowledge and guiding subsequent sequence generation.\nThe reliance on this aggregation pattern seems to induce hal-lucinations in contemporary MLLMs. To delve deeper into this phenomenon and provide a visual representation of the existence and impact of \"summary tokens\" in the generated sequences, we conducted theoretical analyses of their potential roles in text gener-ation and detailed visualization of all tokens' self-attention weights. The experimental results show that there indeed exist specific to-kens with disproportionately high attention weights relative to others. These high-weight tokens act as pivotal hubs, condensing the core meaning from preceding generated content. Typically, visual-related tokens are placed at the beginning of the sequence to ground the model's response in visual comprehension. However, as the generated text grows longer, visual information can become diluted through these summary tokens since they fail to adequately encapsulate the visual context's entire richness. Consequently, later tokens may overly depend on recent summary tokens while disre-garding initial image-representative markers, thereby giving rise to model-bias-induced hallucinations; for example, inferring the presence of a \"car\" or \"person\" merely from the mention of \"road\" earlier in the sentence. Moreover, an increased number of summary tokens tends to exacerbate the likelihood of MLLM hallucinations.\nAn example of LLaVA's hallucinations can be seen in Fig. 1, OPERA [18] is the first to discover weight stacking, so the weight penalty rollback strategy is implemented in Transformer's 32nd layer, that is, the output layer. After experiments, we find that this kind of weight stacking actually exists in transformer's middle layer (for example, 12-20 layers), which is called \"premature stacking\".\nUpon further scrutiny of the visualized attention weight maps, it is revealed that these highly accumulated weights do not develop gradually towards the end of decoding but instead start accumu-lating relatively early in the generation sequence, most notably in the 12th layer of self-attention weights. As shown in Fig. 2, the accumulation of attentional weights becomes very obvious from layer 12 onwards, accumulating much more than layer 8.\nTo tackle this issue of \"over accumulation,\" we introduce DO-PRA, an innovative decoding framework built around two core strategies: Decoding Over-accumulation Penalization at specific attention layers and Re-allocation. DOPRA ingeniously integrates over-accumulation penalties into the beam search [5, 14, 41] pro-cess by applying weighted scores to candidate selections, effec-tively preventing tokens that exhibit strong patterns of over-trust. Specifically, for each decoding token, DOPRA inspects its local self-attention window, devising a column-wise metric to measure the strength of knowledge aggregation patterns and adjusts the model's log-probabilities accordingly to penalize the emergence of such patterns.\nIn addition, recognizing the persistence of knowledge aggrega-tion patterns and the potential for hallucinations to permeate all candidate generations, DOPRA implements a retrospective real-location strategy. This strategic retreat involves rolling back the decoding process to the position of summary tokens and judiciously reselecting candidates that bypass excessive accumulation patterns. Once an accumulated penalty score reaches a predefined threshold within the attention window, this rollback mechanism is triggered.\nIn this study, we specifically focus on vividly demonstrating the intrinsic connection between generated text tokens and their corre-sponding image attention regions. To this end, we visualize the top 50 most relevant highly responsive regions in the generated text (see Fig. 6). By combining textual information with visual embeddings, we are able to efficiently generate representative tokens for LLM that capture key visual features. Finally, by generating heat maps, we can visually check whether the generated text matches well with the corresponding image content. This approach not only gives us insight into how the model integrates textual and visual information during the generation process, but also allows us to clearly identify which visual elements play a decisive role in the generation process. Through this methodology, we not only deepen our understanding of the intrinsic working mechanism of multimodal language models, but also visualize the interactions between the generated text to-kens and the attentional high-response regions of the images, thus enhancing the transparency of the model and the interpretability of the generated content. We conduct extensive empirical evalu-ations on benchmark datasets, employing hallucination-specific metrics, and testing advanced MLLMs, thereby substantiating DO-PRA's effectiveness in universally reducing hallucinations across various MLLM architectures. Our contributions can be summarized as follows:\n\u2022 DOPRA presents an innovative solution that addresses hallu-cination issues in MLLMs during inference without requiring external data, knowledge repositories, or additional training procedures.\n\u2022 Through meticulous examination, DOPRA identifies the crit-ical role played by summary tokens in the formation of hallu-cinations and develops a penalty-based decoding technique augmented with a backtracking reallocation strategy to dis-rupt excessive accumulation dynamics.\n\u2022 Comprehensive evaluations demonstrate DOPRA's superior performance, proving it to be a practically cost-free interven-tion that effectively mitigates hallucinations in multimodal language models, thereby enhancing the credibility and reli-ability of these powerful AI tools in real-world applications."}, {"title": "2 RELATED WORK", "content": "2.1 MLLM Development and Capacity\nIn recent years, Multimodal Large Language Models (MLLMs)[1, 2, 11, 28, 32, 42, 45, 56] have rapidly emerged and become the focus of both academia and industry. Since 2021, a series of representative models such as CLIP [37] and BLIP [24, 25] have pioneered large-scale pre-trained multimodal models, demonstrating the powerful ability to deeply integrate natural language with visual information, enabling accurate image description, cross-modal reasoning, and other functions. Entering 2023, this field is even more explosive, including but not limited to GPT-4V [1], LLaVA [32], minGPT-4 [56], InstructBLIP [11], Qwen-VL [2], CogVLM [45], and many other new multimodal macromodels have emerged one after another, which further enhance the model's ability of understanding and generating multimodal inputs, and make the MLLM get closer and closer to the general-purpose Artificial Intelligence [3, 16] in its ideal form.\n2.2 Two-Track Path to MLLM Development\nThe current research and development of multimodal large models show two paths. On the one hand, researchers focus on continu-ously expanding the training dataset and model parameter sizes of the models in the hope of significantly improving their accuracy and generalization performance by increasing the model capacity. The new generation of multimodal large models, such as LLaVA-1.6-34B [32], GPT-4V [1], InstructBLIP [11], etc., are strong examples of this development trend, which have demonstrated their excel-lent capabilities in various complex multimodal tasks by virtue of their large number of parameters and rich training resources. However, although such large models continue to break the perfor-mance ceiling, the theoretical and technical challenges behind them cannot be ignored, especially in terms of resource consumption and computational efficiency. On the other hand, another research direction is to explore the intrinsic potential of small multimodal models [9, 48, 54, 57], and seek to achieve comparable or even sim-ilar functional effects as those of large-scale models in a smaller parameter space. The goal of this path is to optimize the model structure and training methods so that it can perform efficient multimodal understanding and generation under limited hardware conditions and computational costs. Although some breakthroughs have been made in such efforts, even the increasingly sophisticated small-scale models are still unable to completely escape from the \"hallucination\", a core problem shared by large multimodal models.\n2.3 Strategies for Solving the MLLM Hallucination Problem\nThe term \"hallucination\" [15, 20, 27, 31, 34, 50] refers to the fact that multimodal models, when processing multimodal inputs, some-times produce content that does not correspond to the actual inputs or is even fictitious. Aiming at this key problem, which seriously affects the credibility and practicality of models, the academic com-munity has carried out a lot of fruitful research work and pro-posed several innovative solutions. Among them, RLHF [40, 47, 53] (Reinforcement Learning from Human Feedback) is an approach that relies on human feedback reinforcement learning techniques, which manually evaluates and guides model outputs, prompting the model to pay more attention to factual basis and logical consis-tency in the subsequent generation process. DoLa [10] improves the model's ability to capture and reproduce factual information when generating text by comparing the decoded information at different levels within the model. The Woodpecker [46] takes a two-stage approach to hallucination, first optimising the model ini-tially with DINO [33] (a self-supervised learning framework), and then correcting the model's multimodal outputs by combining it with image captioning techniques. The OPERA [19] research team proposes a novel strategy to effectively mitigate the false infer-ences and hallucination representations generated by multimodal large models due to over-trusting one modality when processing complex scenes by applying the Over-Trust Penalty and Retrospec-tion Allocation mechanisms. In the specific application scenarios of visual-linguistic modelling, the VCD [22] technique helps to iden-tify and correct the object hallucination that may occur when the model is describing or reasoning about an image by introducing the visual contrast decoding link. In conclusion, whether following the traditional route of increasing model size or seeking the innovative path of efficient small-scale models, suppressing the \"hallucination\" phenomenon of large multimodal models has become an important issue of common concern to researchers, and substantial progress has been made in several cutting-edge researches. These strategies not only enrich the design and optimisation of multimodal models, but also lay a solid foundation for the construction of more accurate, reliable and universal multimodal intelligent systems in the future."}, {"title": "3 METHOD", "content": "In this section, we will first show the model generation process by introducing the modeling framework of MLLM. Then we will talk about DOPRA's Over-Accumulation Penalization and Re-allocation Decoding Strategies as shown in Fig. 3. Finally, it will be shown how high response works.\n3.1 Anatomy of MLLM generation mechanism\nIntegration of Input Constructs. For the input construction phase of MLLM, the core is integrating two types of data sources: image and text. Regardless of the specific architecture, such models usually employ a visual coder to extract visual element features from the original image, and then transform and incorporate these features into the input space of the language model through a cross-modal mapping mechanism. We label this set of transformed visual elements as the set x = {x0, x1, ..., xN-1}, where N represents the number of visual elements and is fixed in many cases. Meanwhile, the textual input is processed by the disambiguation technique and is represented as the sequence xp = xN, xN+1, ..., xM+N\u22121. Finally, these two types of tokens are sequentially spliced to form the complete input sequence {xt}T\u22121t=0, where T is the sum of the total number of tokens of the image and text, T = N + M.\nModel forward propagation. The MLLM follows an autoregressive model for training and uses a causal attention mechanism. Within the model, each token predicts the immediately following token based on the information of all the tokens preceding it, which is mathematically formulated as:\nh = MLLM(xi)\nh = {h0, h1,..., hT\u22121}\t (1)\nHere, h is the sequence of hidden state vectors output by the last layer of the MLLM. Next, the model uses the vocabulary header mapping function H to map the hidden state into a probability distribution for the next token prediction:\np(xt|x<t) = SoftMax[H(ht)] xt, xt \u2208 X (2)\nHere x<t is a compact representation of all previous tokens, and X represents the entire vocabulary set.\nDiverse Decoding Strategies. Based on the predicted probability distribution p(xt|x<t) of each token obtained from the above com-putation, various decoding algorithms have been developed in the industry, such as the Greedy Decoding Method, the Beam Search algorithm, and advanced decoding strategies such as DoLa [10] and OPERA [19]. In the actual generation process, new tokens decoded at each step are added to the end of the original input text as the starting point for the next round of generation. OPERA visualizes the last layer of self-attention weights of the generated content by visualizing the last layer of self-attention weights. It develops a penalty strategy to enable it to reduce the impact of excessive accumulation of attention weights. However, there is a shortcoming that OPERA only investigates the occurrence of stacking at layer 32. We found that the accumulation of attention weights is not only in 32 layers, but also in 32 layers, and the hallucination is actually generated \"early\".\n3.2 \"Over-accumulation\" Attention-based Penalty\nAs shown in Fig. 4, in addressing the issue of delayed manifestation\u2014whereby patterns indicative of over-reliance on potentially hallucinated information emerge only after several tokens have been decoded\u2014we introduce the \"Over-accumulation Attention Penalty\" mechanism. This approach cumulatively applies a penalty to the beam search scores during generation, this method selec-tively targets and penalizes the accumulation of attention weights in a specified layer, particularly the 12th layer,influencing both the current token selection and the overall candidate sequences, thereby reducing the likelihood of selecting outputs containing hallucinations.\nTo realize this concept, we focus on the self-attention weights in the 12th layer local context window. Consider the generated sequence until time step t, denoted as {xi}t\u22121i=0, and the causal self-attention weights used in Layer 12 to predict the next marker {@t-1,j}Vt\u22121j=0 pertaining to the next token prediction, where w = SoftMax( QKTVD ), and Q, K, D represent query, key features, and the feature dimension, respectively. To capture the accumulation of knowledge aggregation patterns, we define a layer-specific local window of attention Wta\u22121 for layer 12 as follows:\nWt\u22121,a=12 = {wij}t\u22121i=t\u2212k,j=i\u2212k, where wij = {wij}t\u22121i=t\u2212k,j=i\u2212k and a = 12 (3)\nHere, k represents the size of the local window cropped from the attention map, and wij is the attention weight given by token j to token i. Notably, we exclude attention weights from image tokens or prompt tokens, focusing exclusively on generated tokens (t-k \u2265 N + M). Additionally, we take the maximum attention weight across multi-heads and renormalize these values as they often reflect high model confidence.\nUpon obtaining Wta\u22121, we preprocess the attention weights by setting the upper triangle to zero and scaling up the remaining attention values for better representation, as shown in Equation :\nWta\u22121,a=12 = {wij}t\u22121i=t\u2212k,j=i\u2212k where w2ij = {{wij}2 if j > i0 for a = 12 (4)\nThe scaled attention matrix's lower triangle undergoes column-wise multiplication to generate a vector of scores, where higher scores suggest stronger knowledge aggregation patterns. The maxi-mum value of this column-wise score vector serves as the pattern's descriptor:\n\u03b4t\u22121(j) = \u03a0t\u22121i=t\u2212k\u03c3(wij) , c = arg maxt\u22121t\u2212k\u2264j\u2264t\u22121(\u03b4t\u22121(z))\u03c3 (5)\nTo efficiently apply the penalty without distorting the model's predictions towards unreasonable outputs, we form a candidate set Y comprising the top-Ncan logits from each beam, where |Y| = Ncan \u00d7 Nbeam and Nbeam is the number of beams. Then, we inte-grate the pattern metric \u03b4(w\u2264t) into the model logits to predict the next token while constraining it within Y:\np(xt|x<t) = Softmax[H(ht) \u00b7 \u03c3\u03b412 \u00b7 \u03b412(w)], s.t. xt \u2208 Y (6)\nHere, w\u2264t summarizes all attention weights up to time step t, and \u03c3 is a tunable hyperparameter controlling the strength of the penalty applied to the logit. By introducing the \"Over-accumulation Attention Penalty\" with attention weight \u03c3 = 12 specifically em-phasized, we aim to mitigate the risk of over-trust in potential hallucinations and guide the model toward more reliable genera-tions.\n3.3 Penalization and Re-allocation Strategy\nUtilizing the \"Over-accumulation\" attention-based Penalty, we can proficiently detect the emergence of knowledge aggregation pat-terns after the generation of several consequent tokens. Ordinarily, the penalty discourages candidates exhibiting these patterns, thus encouraging the selection of others. Despite this, there are instances where all candidates are penalized even though hallucination has already taken place. This situation drives us to revisit the root cause of these aggregation patterns: they arise from early tokens excessively trusting the summary token, and the penalty fails to rectify this over-reliance. Therefore, a natural yet assertive solution is to eliminate tokens leading to hallucination and reallocate by choosing suitable tokens after the summary token. This leads us to propose the Retrospective Allocation strategy.\nWhen the decoding process encounters a knowledge aggregation pattern and hallucination appears unavoidable, it reverses to the summary token and selects alternative candidates for the next token prediction, excluding those chosen earlier. The empirical condition for triggering retrospective decoding is based on the location overlap of the maximum values in the column-wise scores corresponding to multiple successive tokens, with a manually set threshold count denoted as r. Location counting proves more robust and general compared to directly using the maximum value that varies across different models.\nThe complete retrospective process is detailed in Fig. 4. Lever-aging the insights from Section 3.2, we can easily determine the location coordinate c of the maximum score through Eq. (5). Following this, we establish the set of location coordinates for the recently decoded tokens xt\u22121, ..., xt\u22121, which is given by:\nC = {c: c = arg maxt\u2212k\u2264j\u2264z=j || \u03b4(Wi,j), z \u2208 [t \u2212 l, t \u22121]} (7)\nHere, l > r is usually specified, and by default, we set l = k. Given a sequence (x0, x1,...,xt-1} and its associated recent location coordinate set C, we can assess the consistency of these coordinates. Formally, the overlap count is calculated as:\nNoverlap = \u03a3c\u2208CIc=s, where s = Mode(C) (8)\nI is the indicator function returning 1 if the condition holds and 0 otherwise, and Mode gives the most frequent value in a set. If Noverlap \u2265 r, we initiate the retrospective action, considering s = Mode(C) as the position of the summary token.\nIn the event that the sequence {x0, x1, ..., xs, . . ., xt-1} displays a knowledge aggregation pattern at the summary token xs, the decoder rewinds to the subsequence {x0, x1,...,xs} and selects a new next token from the complementary set Y \\ {xs+1}. To ensure progressive rollback, we require that the rollback location s monotonically increases. Moreover, we impose a maximum rollback limit \u03b2; if xs reaches its rollback cap, we consider rolling back to {x0, x1, ..., xs-1}.\n3.4 High-Response Region Visualization and Cross-modal Interaction\nTo explore the relationship between generated text tokens and their correspondence with image attention in a more vivid manner, we visualize the high-response regions of the top 50 scores in Fig. 5. Given user input, we generate a text-guided query vector Qt \u2208 RMXC, where M denotes the number of queries. As shown in Fig. 3, this cross-modal interaction primarily occurs within the text decoder and can be readily instantiated using BERT [12] or Q-former [24] models. The generated text query Qt encapsulates salient visual cues that are most relevant to the user's command. Employing the text query Qt along with the visual embeddings Xu, we can effectively generate representative tokens for the LLM that capture essential visual features. Specifically, the mixed atten-tion mechanism aims to aggregate and condense visual features related to the text into a single context token. This process, de-picted in Fig. 3, takes Qt and Xu as inputs and formulates the mixed embedding of text and image, Et \u2208 R1\u00d7C, as:\nEt = Qt \u00d7 XT, (9)\nContrary to Q-former, which employs 32 visual queries as LLM markers, our approach uses only the text query Qt to aggregate visual features with high response scores relative to the input com-mand. Consequently, the compressed embedding Et efficiently re-tains the most critical visual cues associated with the user's input."}, {"title": "4 EXPERIMENT", "content": "4.1 Experimental Setup\nDecoding strategies. Regarding decoding strategies, we have executed a variety of approaches for comparison and optimization. These include greedy decoding, which selects words at each step based on their highest probability; beam search [5, 14, 41] decod-ing with varying numbers of beams (Nbeam set as 5, 4, 3, 2, 1), allowing us to explore the impact of different search space widths; top-p nucleus sampling [17], using a standard setting of p = 0.9 to concentrate on the main body of the probability distribution; and the introduction of VCD [22] method that addresses object hallucination issues in large-scale models. For specialized decoding algorithms, like the DoLa [10] method designed to mitigate hallu-cinations in LLMs, within our experiments, we selected multiple candidate pre-mature layer indices (\"0, 2, 4, 6, 8, 10, 12, 14\") along with a fixed mature layer index at 32 to achieve fine-grained control over the internal decision-making process of the model. During the beam search decoding phase in the DOPRA experiment, we also set the scaling factor \u03c3 to 50 to ensure effective discrimination of at-tention weights in the knowledge aggregation mode, where salient regions receive values greater than 1 while secondary areas get less than 1. Moreover, we established a default candidate number Ncan of 5, understanding that this hyperparameter can be adjusted but, in this experimental stage, we primarily study performance under default settings, considering that larger Ncan values could signif-icantly increase the computational cost of the decoding process. Implementation details. Lastly, for other key hyperparameters influencing the decoding behavior in the LLaVA-1.5 model, we uni-formly set \u03c3 = 1, \u03b2 = 5, and r = 15, these configurations help maintain a stable experimental environment and allow us to focus on the effectiveness analysis of the proposed decoding strategies. In summary, the DOPRA experiment delves deeply into exploring the performance boundaries of the LLaVA-1.5 model on multimodal tasks by employing a series of meticulously designed decoding strategies and parameter tuning. The aim is to validate the effects of different decoding techniques on the model's accuracy and ro-bustness.\n4.2 Quantitative Results\nDOPRA evaluation on hallucinations using CHAIR. The Caption Hallucination Assessment with Image Relevance (CHAIR) [38] is a tailored metric for measuring object hallucination issues in image captioning tasks. For DOPRA, we utilize CHAIR to quan-tify the extent of hallucinated objects by computing the ratio of objects mentioned in the generated description that are absent in the ground-truth label set. CHAIR offers two separate assessments: CHAIRS (denoted as Cs) measures sentence-level hallucinations and CHAIRI (denoted as C\u2081) measures image-level hallucinations,\nmathematically expressed as:\nCs = |hallucinated objects||all mentioned objects|C\u2081 = |captions w/ hallucinated objects||all captions|\nOn the MSCOCO dataset [29], which encompasses over 300,000 images annotated with 80 objects, we perform CHAIR evaluations. Specifically, we randomly sample 500 images from the validation set of COCO 2014 and prompt different MLLM models with \"Please describe this image in detail.\" to generate their descriptions. To ensure fairness, we restrict the maximum number of new tokens for generating descriptions of both long and short lengths. As displayed in Table 1, DOPRA demonstrates a clear superiority over baseline decoding methods in both Cs and C\u2081 metrics. This advantage is consistent across both long and short description generations.\nDOPRA evaluation on hallucinations using POPE. The Polling-based Object Probing Evaluation (POPE) [27] is a recently devel-oped method aimed at assessing hallucination issues in MLLMs. Analogous to CHAIR, POPE scrutinizes object hallucination by prompting the model with an inquiry akin to \"Is There a <object> in the image?\" to gauge whether the model correctly identifies the presence or absence of a specific object in the given image. POPE includes three distinct evaluation scenarios: \"random\", \"popular\", and \"adversarial\".\nWe validate DOPRA using POPE on LLaVA [32] MLLM models and report the mean F1 scores in Table 1 and Table 2. When com-pared with baseline decoding strategies, DOPRA also achieves the best performance, although the improvements might be marginal. It is important to note that DOPRA particularly excels in mitigating hallucinations in longer sequences. In the context of POPE answers, which tend to be brief responses starting with \"Yes\" or \"No\" followed by confirmations like \"Yes, there is a <object> in the image.\", the knowledge aggregation patterns\u2014central to our method-may not surface as conspicuously. Nevertheless, DOPRA still demonstrates a competitive edge in controlling hallucinations across various lengths of sequences and different evaluation contexts.\nIn this section, we assess DOPRA's effectiveness in reducing hallucinations in both extended descriptions and concise VQA an-swers as shown in Table 1. We show the difference between DOPRA and OPERA in attention punishment in Fig. 5. It can be seen that OPERA only carries out attention punishment in the last layer of Transformer, namely layer 32, while we observe early attention accumulation in the middle layer of transformer between layer 12 and 18, so we choose to intervene in layer 12. Our intervention"}, {"title": "4.3 High Response Example Analysis", "content": "The image features a large white polar bear in a pool of water, holding a carrot in its paw. The bear appears to be enjoying the carrot, possibly as a treat or a source of entertainment. The scene captures the bear's curiosity and playfulness as it interacts with the carrot.\nAs shown in Fig. 6, in this section, we delve into the significant ad-vancements and core mechanisms within the realm of caption gen-eration models-specifically focusing on the DOPRA algorithm-in addressing and rectifying caption hallucination phenomena. We begin by presenting an extensive examination of real-life cases where caption hallucinations have been effectively corrected using DOPRA, juxtaposing pre- and post-correction captions in situations such as \"is there any dog?\", thereby illustrating DOPRA's substan-tial improvements in accurately capturing and conveying the actual content of images.\nSubsequently, we explore from the perspective of attention mech-anisms how DOPRA systematically refines its strategy for allocating attention weights to precisely latch onto key visual features during caption generation. This includes, among other things, displaying different levels of attention maps and their evolution throughout model optimization, highlighting how the model learns to disregard irrelevant information and hone in on decisive visual cues within an image, say, when discerning whether or not a dog is present.\nTo further unravel the inner workings of DOPRA's efficacy in enhancing caption veracity, we conduct a detailed investigation into the genesis and case studies of high-response regions. Using visualization techniques, we demonstrate visually how the model responds differently to various areas of the input image, particularly pinpointing those critical regions with exceedingly high response values. These high-response zones not only reveal the pathways the model follows in recognizing and interpreting image content but are also vital for understanding the information extraction and decision-making processes involved in generating accurate captions. More visualizations can be found in the attached supplementary material."}, {"title": "5 DISCUSSION AND LIMITATIONS", "content": "The illusions generated by Large Language Models (LLMs) may result from inadequate generalization or the model's knowledge not being updated in a timely manner. We believe the illusion is-sues in Multimodal Large Language Models (MLLMs) are primarily attributed to the visual component, possibly due to the coarse gran-ularity of features (a problem that also needs addressing, as current architectures like Q-Former tend to lose much information), or possibly due to the use of noisy data during the alignment phase, or insufficiently detailed alignment.\nThe illusion observed in captions is merely a superficial phenom-enon; the root cause lies in the process of encoding by the vision encoder, which is essentially a compression process, leading to a significant loss of original image information. This fundamental flaw in the architecture prevents effective captioning based on vi-sual hidden representations and might even lead to illusions. This is because the vision encoder, CLIP, is trained on images and highly noisy captions.\nOur overarching idea is that DOPRA is merely a temporary solution, and there will undoubtedly be a need for end-to-end fine-tuning and improvements in perceptual capabilities in the future. Future research may explore avenues such as implementing more meticulous data alignment procedures that go beyond aligning entire images and texts at a superficial level, or incorporating fine-grained visual features to overcome the perceptual limitations of CLIP, which predominantly focuses on high-level semantic under-standing rather than nuanced visual details."}, {"title": "6 CONLUSION", "content": "In conclusion, DOPRA introduces a novel and cost-effective ap-proach to address the prevalent issue of hallucination in multimodal large language models (MLLMs), thereby enhancing their precision and reliability for practical applications. By innovating a method that penalizes decoding over-accumulation and reallocates atten-tion in specific weighting layers, DOPRA circumvents the need for additional training data or the integration of external knowl-edge bases, setting it apart from existing methods. This technique is grounded in a deep understanding of the mechanisms under-lying hallucinations in MLLMs, particularly the disproportionate reliance on summary tokens to the detriment of image-relevant information in certain critical layers. Through the application of a weight stacking penalty and a strategic reallocation during the decoding phase, DOPRA effectively broadens the context consid-ered by the model, while ensuring that generated captions more accurately reflect the authentic content of images. The implemen-tation of this method marks a considerable advancement in the development of MLLMs by systematically reducing the occurrence of hallucinations, thereby improving the overall output quality of these models. DOPRA's innovative approach not only signifies a significant leap towards resolving a stubborn challenge in the field of artificial intelligence but also paves the way for future research and development in enhancing the interpretative and generative capabilities of multimodal systems."}]}