{"title": "CONSISTENCY CALIBRATION:\nIMPROVING Uncertainty CALIBRATION VIA CON-\nSISTENCY AMONG PERTURBED NEIGHBORS", "authors": ["Linwei Tao", "Minjing Dong", "Haolan Guo", "Chang Xu"], "abstract": "Calibration is crucial in deep learning applications, especially in fields like health-\ncare and autonomous driving, where accurate confidence estimates are vital for\ndecision-making. However, deep neural networks often suffer from miscalibra-\ntion, with reliability diagrams and Expected Calibration Error (ECE) being the\nonly standard perspective for evaluating calibration performance. In this paper,\nwe introduce the concept of consistency as an alternative perspective on model\ncalibration, inspired by uncertainty estimation literature in large language models\n(LLMs). We highlight its advantages over the traditional reliability-based view.\nBuilding on this concept, we propose a post-hoc calibration method called Con-\nsistency Calibration (CC), which adjusts confidence based on the model's consis-\ntency across perturbed inputs. CC is particularly effective in locally uncertainty\nestimation, as it requires no additional data samples or label information, instead\ngenerating input perturbations directly from the source data. Moreover, we show\nthat performing perturbations at the logit level significantly improves computa-\ntional efficiency. We validate the effectiveness of CC through extensive compar-\nisons with various post-hoc and training-time calibration methods, demonstrating\nstate-of-the-art performance on standard datasets such as CIFAR-10, CIFAR-100,\nand ImageNet, as well as on long-tailed datasets like ImageNet-LT.", "sections": [{"title": "INTRODUCTION", "content": "Calibration is essential in many deep learning applications where accurate confidence estimates\nare as important as the predictions themselves. In fields like healthcare Chen et al. (2018) and\nautonomous driving Feng et al. (2019), decisions often rely not only on the model's output but also\non how confident the model is in its predictions. A well-calibrated model should reflect the ground\ntruth uncertainty. In healthcare, for instance, a model that accurately reflects uncertainty can help\ndoctors trust the system's confidence when diagnosing critical conditions.\nHowever, current deep learning models are often found to be miscalibrated (Guo et al., 2017). \u03a4\u03bf\nevaluate calibration performance, Naeini et al. (2015) introduced ECE, which has become the gold\nstandard, based on the reliability diagram (DeGroot & Fienberg, 1983). Although several improved\nmetrics have since been proposed, such as AdaptiveECE (AdaECE) (Nixon et al., 2019) and Class-\nwiseECE (CECE) (Kull et al., 2019), they all adopt the same fundamental perspective on calibration:\nif a model assigns 80% confidence to its predictions, then, ideally, 80% of those predictions should\nbe correct. We refer to this classical approach as the reliability view, which seeks to align predicted\nconfidence levels with actual model accuracy.\nThe concept of consistency has gained increasing importance in black-box uncertainty estimation,\nparticularly in recent developments in large language models (LLMs) (Wang et al., 2022; Tam et al.,\n2022; Xiong et al., 2023b; Geng et al., 2023). If an LLM is confident in its answer, it should provide"}, {"title": "METHODOLOGY", "content": "In a classification task, let X represent the input space and Y the label space. The neural network\nf(.) and projection head g(\u00b7) maps x \u2208 X to a vector of logits z = g(f(x)) \u2208 RK, where each zk is\nthe logit for class k. These logits are then transformed into a probability distribution \u00eep = softmax(z)\nover K classes using the softmax function:\n$\\hat{p}_k = \\frac{e^{z_k}}{\\sum_{i=1}^{K} e^{z_i}}, k = 1, ..., K,$\\nwhere k = arg maxi \u00eei denotes the predicted label index. The ground-truth label y \u2208 Y represents\nthe true class, and \u0177 \u2208 Y is the predicted label. The confidence score p\u0302k represents the predicted\nprobability assigned to the predicted label k."}, {"title": "CALIBRATION IN THE VIEW OF RELIABILITY", "content": "Calibration in the view of reliability has been widely accepted since the introduction of the reliability\ndiagram by DeGroot & Fienberg (1983). In this view, a classifier is considered perfectly calibrated\nif its predicted confidence \u00ee\u00ee accurately represents the true probability of correctness. Formally, this\nis expressed as:\n$P(\\hat{y} = y | \\hat{p} = p) = p \\text{ for all } p \\in [0, 1].$\nIn other words, if a model assigns a confidence score of 80%, the prediction \u0177 should be correct\n80% of the time. To move beyond visual inspection of reliability diagram, Naeini et al. (2015)\ndeveloped a quantitative metric from the reliability diagram called the Expected Calibration Error\n(ECE). ECE provides a more precise measurement of miscalibration by calculating the average"}, {"title": "CALIBRATION IN THE VIEW OF CONSISTENCY", "content": "We offer an alternative perspective on calibration by examining it through the concept of consistency.\nIn a real-world scenario, an individual confident in their answer tends to maintain that answer, even\nwhen faced with external doubts or minor alterations to the question. On the other hand, someone\nwho is uncertain might change their response when presented with slightly misleading information\nor variations in the question. We define this adherence to the original answer as consistency.\nRecent advances in LLMs, particularly black-box models utilize factual consistency to enhance\nperformance (Wang et al., 2022; Tam et al., 2022; Xiong et al., 2023b; Geng et al., 2023). These\nstudies frame the consistency of a model's responses as an indicator of its uncertainty. In the context\nof classification tasks, calibration can also be described in terms of consistency. Specifically, for\nclassification models, we can formalize this relationship as follows:\nProposition 1. If a model is confident in its prediction, it should consistently output the same pre-\ndiction when the input is slightly perturbed. The consistency c of a sample x is defined as\n$c_k(x) = \\frac{1}{T} \\sum_{t=1}^{T} 1(\\hat{y}(x^t) = k), \\text{ where } d(x^t, x) < \\epsilon^*, \\text{ for } k = 1, ..., K$\nwhere T is the number of perturbed neighbors, \u0177(xt) is the predicted label for the perturbed input\nIt, and the distance between the original sample x and its perturbed version It is smaller than a\nconstant e*, according to some distance metric d. A model is said to be perfectly calibrated if, for all\nsamples x, given a suitable set of perturbed neighbors {xt | t = 1, . . ., T}, the predicted confidence\nscore p(x) satisfies:\n$\\hat{p}_k(x) = c_k(x), \\text{ for } k = 1, ..., K$\nHowever, identifying a suitable perturbed neighborhood is non-trivial-it is challenging to deter-\nmine an appropriate constant \u20ac* and distance metric d. Fortunately, in image classification tasks, a\nperturbed neighbor is often considered a data-augmented version of the original image. Thus, we\nbegin our exploration by using image data augmentation.\nTo evaluate the effectiveness of consistency-based confidence, we design an experimental setting\nusing a ResNet-50 model trained on CIFAR-10 with data augmentation (RandomCrop and Ran-\ndomHorizontalFlip). We generate perturbed neighbors by applying various levels of data augmen-\ntation to the entire CIFAR-10 test set, creating 100 perturbed neighbors for each test sample. The\ncalibration performance of consistency is assessed on the test set in the following settings:\n\u2022 Baseline: Confidence score is extracted on the original test set, serving as the baseline."}, {"title": "CONSISTENCY AS A REPRESENTATION OF GROUND TRUTH UNCERTAINTY", "content": "On one hand, the reliability approach estimates calibration error by comparing the prediction confi-\ndence with the average correctness of samples that have similar confidence levels. In this view, the\naverage correctness of such sample neighborhood is treated as an approximation of the ground truth\nuncertainty. On the other hand, the consistency approach directly uses consistency as a measure\nof ground truth uncertainty. Thus, we are interested in determining which of these two approaches\nmore accurately approximates this uncertainty.\nTo explore this, we constructed a toy dataset consisting of two two-dimensional Gaussian distribu-\ntions representing two groups of data: \u039d(\u03bc\u03bf, \u03a3), \u039d(\u03bc1, \u03a3) where \u03bc\u03bf and \u00b5\u2081 are the mean vectors,\nand \u2211 is the shared covariance matrix for both groups, labeled 0 and 1, respectively. We generated\n1,000,000 data points from each group to form the training dataset, which was used to train a CNN\nmodel. An additional 50,000 samples from each group were used to create the test dataset. The\ninput space is X = R2, and the label space is Y = {0, 1}, as illustrated in Figure 1b.\nThe ground truth uncertainty, \u03b7(x), is calculated from the probability density function (PDF) of each\ndistribution:\n$\\eta(x) = \\frac{p^0(x)}{p^0(x) + p^1(x)}$"}, {"title": "MORE EFFICIENT CONSISTENCY CALIBRATION", "content": "Due to numerous types of data augmentations, determining the optimal perturbation strength using a\ncontinuous variable is challenging. To address this, we extend the perturbation process to the feature\nand logit levels by introducing noise with varying intensities. This approach yields effects similar to\nthose observed with image-level perturbations, as demonstrated in Figure 3a and Figure 3b.\nInterestingly, feature- and logit-level perturbations maintain significant calibration performance\nwhile offer huge computational advantages. With image-level perturbations, inference must be per-\nformed on the entire model T times. In contrast, feature-level require evaluating only the classifica-\ntion head, while logit-level only compute the argmax operation T times. This results in substantial\nreductions in computational costs. Experiments on other layers can be found in Appendix B.\nProposition 2. We propose a unified definition of our calibration methods, termed Consistency\nCalibration (CC), which identifies perturbed neighbors at different levels. The calibrated prediction\nconfidence score p' is formally defined as:\n$p'_k = \\frac{1}{T} \\sum_{t=1}^{T} 1(\\text{arg max } q(\\hat{h(x)^t}) = k), \\text{ for } k = 1,..., K,$\nwhere h(x) is the representation of data x, h(x)t is the perturbed representation, and q is the\npipeline to extract the logits z.\nSpecifically, for data-level perturbations: h(\u00b7) = I(\u00b7), h(x)t is the augmented data, q = g(f(\u00b7)). For\nfeature-level perturbations: h(\u00b7) = f(\u00b7), h(x)t = h(x) + \u03b5t, q = g(\u00b7). For logit-level perturbations:\nh(\u00b7) = g(f(\u00b7)), h(x)t = h(x) + \u03b5t, q = I(\u00b7). Here, I(\u00b7) is the identity function, \u03b5t represents\nthe noise added to features or logits, with its strength determined by minimizing the ECE on a\nvalidation set. Given the strong calibration performance and computational efficiency of logit-level\nperturbations, we refer to logit-level consistency calibration as CC when no specification is provided."}, {"title": "CONSISTENCY AS A LOCAL UNCERTAINTY ESTIMATION", "content": "Consistency-based methods do not rely on label information or additional data, as they generate\ntheir own neighborhood by perturbing the input data. This property allows consistency to serve\nas a criterion for instance-level uncertainty measurement. As illustrated in Figure 3c, we examine a\nmiscalibrated (incorrect prediction with high confidence) CIFAR-10 test sample, where a ResNet-50\nmodel trained with Cross-Entropy (CE) shows overconfidence, assigning a confidence score of 0.997\ndespite being incorrect. Using optimal temperature, determined via a validation set, the confidence\nafter temperature scaling decreases slightly, but the model remains overconfident at 0.903.\nFor comparison, we apply CC by perturbing the logits (\"CC (logits)\"), applying train time data\naugmentation (\u201cCC (Train Aug)\"), and using a moderate augmentation method (\u201cCC (Train Aug\n+ Jitter)\"). The confidence significantly decreases with these approaches. However, too strong\naugmentations may negatively impact model accuracy, which requires the need for a validation set\""}, {"title": "WHY CONSISTENCY CALIBRATION WORKS?", "content": "Perturbing images results in straightforward and intuitive image neighborhoods, but the effectiveness\nof perturbations at the logit level requires further explanation. To understand why logit perturbations\nwork, we examined the differences between highly confident correct predictions and overconfident\nincorrect ones. These represent well-calibrated and poorly calibrated samples, respectively. During\nlogit disturbance, the label with second-largest logit most likely to become the prediction label. To\ninvestigate this, we plotted box plots for both the maximum and second-largest logits for correct and\nincorrect predictions, as shown in Figure 4a.\nFor CIFAR-10 test samples, we selected predictions with confidence higher than 99%. We refer to\nthe maximum logit of correct predictions as \u201cCorr. Max\u201d and that of incorrect predictions as \u201cIncorr.\nMax.\" Similarly, \"Corr. 2nd\" represents the second-largest logit of correct predictions, while \u201cIncorr.\n2nd\" refers to the second-largest logit of incorrect predictions. As shown in Figure 4a, the maximum\nlogit for correct predictions is significantly higher than for incorrect predictions. Additionally, the\nsecond-largest logit in correct predictions is much lower than that in incorrect predictions. This\nindicates that the gap between the maximum and second-largest logits is much larger for correct\npredictions than for incorrect ones. Despite large difference, due to softmax saturation, the model\nassigns abnormally high confidence (greater than 99%) to both correct and incorrect predictions,\nleading to overconfident miscalibration.\nInterestingly, we can leverage this difference in the logit gaps between correct and incorrect predic-\ntions. Perturbations can easily alter the predictions of overconfident, miscalibrated samples, while\nhaving minimal effect on well-calibrated, correct predictions. This different response to perturba-\ntions explains why consistency calibration is effective at the logit level. We observed similar patterns\nin experiments with CIFAR-100 and ImageNet, as shown in Figure 4b and Figure 4c."}, {"title": "EXPERIMENTS", "content": "Datasets We conduct experiments on several benchmark datasets, including CIFAR-10, CIFAR-\n100 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009). To assess calibration performance\nin data-imbalance scenarios, we also include ImageNet-LT (Liu et al., 2019), characterized by its\nlong-tailed class distribution. CIFAR-10 and CIFAR-100 contain 60,000 images of size 32 \u00d7 32 pix-\nels, with 10 and 100 classes, respectively, split into 45,000 training, 5,000 for validation and 10,000\ntest images. For ImageNet-1K, we split 20% of the original validation set as the new validation set,"}, {"title": "COMPARISON WITH POST-HOC CALIBRATION METHODS", "content": "We compare our proposed CC with widely used post-hoc calibration techniques, including Temper-\nature Scaling (TS) (Guo et al., 2017), Ensemble Temperature Scaling (ETS) (Zhang et al., 2020),\nParameterized Temperature Scaling (PTS) (Tomani et al., 2022), Class-based Temperature Scaling\n(CTS) (Frenkel et al., 2021), and Group Calibration (GC) (Yang et al., 2024), as well as uncalibrated\nmodels (Vanilla). Our evaluation covers CIFAR-10, CIFAR-100, ImageNet-1K, and ImageNet-LT,\nusing various CNNs and transformers.\nCalibration on Standard Datasets CC consistently outperforms these methods across CIFAR-\n10, CIFAR-100, and ImageNet-1K, significantly reducing calibration error. The most notable im-\nprovement is seen in CIFAR-100, where CC excels while GC, despite its strong performance on\nother datasets, struggles. This highlights CC's robustness across datasets with varying complexities.\nCNNs, which often suffer from overconfidence, are generally well-calibrated with TS-based meth-\nods. However, transformers see limited calibration improvements from TS-based methods, with CC\noutperforming them by a large margin. On larger datasets like ImageNet-1K, CC maintains its ad-\nvantage. Although GC slightly outperforms CC on ViT-B/32, it is computationally expensive due to\nthe additional grouping process, whereas CC balances both efficiency and effectiveness."}, {"title": "COMPARISON WITH TRAINING-TIME CALIBRATION METHODS", "content": "We evaluate CC alongside training-time calibration techniques, including Brier Loss (Brier,\n1950), Maximum Mean Calibration Error (MMCE) (Kumar et al., 2018), Label Smoothing (LS-\n0.05) (Szegedy et al., 2016), and Focal Loss variants (FLSD-53 and FL-3) (Mukhoti et al., 2020), as"}, {"title": "ABLATION STUDY", "content": "Aggregation Methods In our ablation study, we compare two aggregation methods for refining\nconfidence estimates: the mean of softmax probabilities (Mean), defined as:\n$p_k = \\frac{1}{T} \\sum_{t=1} \\text{softmax} \\left(g(\\hat{h(x)^t})\\right), \\text{ for } k = 1,..., K,$\nand consistency-based aggregation (Consis.) as shown in Eq. 9. Both methods leverage predictions\nover perturbed logits. The mean of softmax probabilities treats the perturbation process like an en-\nsemble method, interpreting uncertainty as a distribution. We show the evaluation results on CIFAR-\n10 and CIFAR-100 in Table 3. On smaller datasets like CIFAR-10, both methods perform similarly.\nHowever, on larger datasets with more classes, such as CIFAR-100 and ImageNet, consistency-\nbased aggregation slightly outperforms softmax averaging. This suggests that consistency-based\naggregation captures uncertainty better than the view of ensemble.\nChoice of Noise We investigate the impact of different noise types for input perturbations, compar-\ning uniform noise (U) and Gaussian noise (G), as shown in Table 3. Uniform noise performs better\non datasets with fewer classes, such as CIFAR-10 and CIFAR-100. However, on larger datasets\nlike ImageNet, Gaussian noise yields better results, likely due to variations in the gap between the\nmaximum and second maximum logits across datasets as shown in Figure 4. The choice of noise is\ntreated as a hyperparameter, offering flexibility to adapt to different datasets and models.\nNumber of Perturbations We also assess the impact of the number of perturbations. As shown in\nFigure 4, our experiments indicate that CC achieves strong calibration performance with as few as\n24 = 16 perturbations. Although increasing the number of perturbations slightly improves results,\nthe diminishing returns suggest that CC provides robust calibration with a moderate number of\nperturbations, ensuring both efficiency and accuracy."}, {"title": "CONCLUSION", "content": "Consistency offers an alternative perspective on calibration by focusing on prediction stability under\nperturbations as an indicator of confidence. CC has proven highly effective in reducing calibration\nerrors across various datasets. However, CC has limitations, such as the need for tuning perturbation\nstrength and noise type, and its current focus on classification tasks, with its application to regres-\nsion remaining unexplored. Future work can aim to develop a new, more universal consistency-\nbased metric to complement existing metrics like ECE. This would provide a more comprehensive\nevaluation to calibration, ultimately leading to more reliable deep learning models."}, {"title": "RELATED WORKS", "content": "Numerous studies have explored the phenomenon of overconfidence in modern neural networks and\ninvestigated their calibration properties (Guo et al., 2017; Minderer et al., 2021; Wang et al., 2021;\nTao et al., 2023c). Calibration methods can generally be categorized into two main approaches:\npost-hoc methods and train-time calibration methods.\nCalibration Methods Post-hoc calibration methods adjust model outputs after training to improve\ncalibration. A widely used technique is Temperature Scaling (TS) (Guo et al., 2017), which smooths\nsoftmax probabilities by search a temperature factor on a validation set. Enhanced variants of TS\ninclude Parameterized Temperature Scaling (PTS) (Tomani et al., 2022), which uses a neural net-\nwork to learn the temperature, and Class-based Temperature Scaling (CTS) (Frenkel et al., 2021),\nwhich applies adjustments on a class-wise basis. Group Calibration (GC) (Yang et al., 2024) and\nProCal (Xiong et al., 2023a) aim for multi-calibration (H\u00e9bert-Johnson et al., 2018) by splitting\ndata samples by proximity and grouping. Another stream of work is train-time calibration such as\nBrier Loss (Brier, 1950), Dirichlet Scaling (Kull et al., 2019), Maximum Mean Calibration Error\n(MMCE) (Kumar et al., 2018), Label Smoothing (Szegedy et al., 2016), and Focal Loss (Mukhoti\net al., 2020) and Dual Focal Loss (Tao et al., 2023b). Tao et al. (2023a) propose to use a new\ntraining framework to improve calibration. However, these methods often require substantial higher\ncomputational overhead.\nEnsemble-Based Calibration Ensemble-based methods ensemble multiple outputs in different\nways. They use models or samples to approximate Bayesian Inference. Lakshminarayanan et al.\n(2017) propose deep ensembles as a scalable alternative to Bayesian Neural Networks (BNNs) for\nuncertainty estimation. Similarly, Gal & Ghahramani (2016) treat dropout as approximate Bayesian\ninference. Data-centric ensemble techniques using test-time augmentation, as described by Conde\net al. (2023), also help improve calibration. Zhang et al. (2020) resort to the power of Bayesian in-\nference and proposed a Ensemble-based TS (ETS). However, these methods typically require signif-\nicant computational resources to train multiple models or perform repeated inferences. In contrast,\nour approach relies on consistency rather than probability distribution modeling.\nConsistency in LLMs Consistency has emerged as a key approach for black-box uncertainty es-\ntimation and hallucination detection in large language models (LLMs). These methods evaluate\nuncertainty by measuring variability in outputs across slight changes, such as different sampling\ntechniques or rephrased prompts. Confident models produce stable outputs, while variability indi-\ncates uncertainty. For instance, SelfCheckGPT (Manakul et al., 2023) uses sampling and similarity\nmetrics like BERTScore and NLI to detect hallucinations, while Lin et al. (2023) analyze a similar-\nity matrix to estimate uncertainty. Xiong et al. (2023b) further break down uncertainty estimation\ninto prompting, sampling, and consistency-based aggregation. These methods, which rely on output\nstability, are efficient alternatives to probabilistic approaches."}, {"title": "PERTURBATION OF DIFFERENT LAYER", "content": "This section presents a detailed analysis of the impact of perturbations applied at various levels of\na ResNet50 model, trained on CIFAR-10. The experiments were conducted using 32 samples, and\nthe effects on ECE, accuracy, and optimal perturbation values were evaluated.\nFrom Table 5, we observe a clear trend in the performance of perturbations applied at different lay-\ners of the model. Perturbation at the logits level achieves a favorable trade-off between calibration\nand efficiency. Although the perturbation applied to the fourth layer's feature space slightly im-\nproves the ECE to 0.53%, the associated computational cost is significantly higher, with the optimal\nperturbation value of 13.28.\nOn the other hand, perturbations applied at lower feature levels (Layer 1 to Layer 3) result in severe\ndegradation of both accuracy and calibration. Specifically, the ECE increases drastically to above\n50%, and accuracy drops to approximately 10%, with a significant increase in computing time and\nmemory use. This suggests that perturbing the features at these lower layers disrupts the model's\nability to recognize patterns and correctly classify the input data. We hypothesize that this is due to"}, {"title": "COMPARISON OF POST-HOC CALIBRATION METHODS ON OTHER METRICS", "content": "As shown in table 6, The proposed CC method consistently achieves the lowest AdaECE values,\noutperforming the other methods. This indicates better calibration performance, in line with our\ndiscussion in the main text. For instance, in CIFAR-10, Wide-ResNet has an AdaECE of 0.40 with\nCC compared to 3.24 for Vanilla, showing a significant improvement. Similar results are observed\nacross other models and datasets. The formula for Adaptive-ECE is as follows:\nAdaptive-ECE = $\\frac{\\sum_{i=1}^{B} |I_i - C_i| \\text{ s.t. } \\forall i, j \\cdot |B_i| = |B_j|}{N}$"}]}