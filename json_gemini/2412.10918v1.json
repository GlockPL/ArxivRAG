{"title": "LLMs-in-the-Loop Part 2: Expert Small AI Models for Anonymization and De-identification of PHI Across Multiple Languages", "authors": ["Murat Gunay", "Bunyamin Keles", "Raife Hizlan"], "abstract": "The rise of chronic diseases and pandemics like COVID-19 has emphasized the need for effective patient data processing while ensuring privacy through anonymization and de-identification of protected health information (PHI). Anonymized data facilitates research without compromising patient confidentiality.\n\nThis paper introduces expert small AI models developed using the LLM-in-the-loop methodology to meet the demand for domain-specific de-identification NER models. These models overcome the privacy risks associated with large language models (LLMs) used via APIs by eliminating the need to transmit or store sensitive data. More importantly, they consistently outperform LLMs in de-identification tasks, offering superior performance and reliability.\n\nOur de-identification NER models, developed in eight languages \u2014 English, German, Italian, French, Romanian, Turkish, Spanish, and Arabic \u2014 achieved f1-micro score averages of 0.966, 0.975, 0.976, 0.970, 0.964, 0.974, 0.978, and 0.953 respectively. These results establish them as the most accurate healthcare anonymization solutions available, surpassing existing small models and even general-purpose LLMs such as GPT-40.\n\nWhile Part-1 of this series introduced the LLM-in-the-loop methodology for bio-medical document translation, this second paper showcases its success in developing cost-effective expert small NER models in de-identification tasks. Our findings lay the groundwork for future healthcare AI innovations, including biomedical entity and relation extraction, demonstrating the value of specialized models for domain-specific challenges.", "sections": [{"title": "1 Introduction", "content": "Patient data is essential for improving public health, expanding preventive health services, preventing diseases, and formulating necessary health policies. Recent studies show that almost all (99%) hospitals in the United States [1] use electronic health records (EHR). Similarly, Wales, Scotland, Denmark, and Sweden have adopted EHRs in the last few years. However, there is still a need for nationally accessible health data in the UK. In particular, the Covid-19 pandemic has again highlighted the importance of EHR data [2]. Thanks to EHRs, disease trends can be examined, modelling can be done, and health policies can be developed.1.\n\nTechnology, which has become more complex and has developed with medical practices, necessitates the development of methods that will protect patient privacy [3]. With information security and information leakage recently gaining more importance, patient safety may have significant consequences beyond ethical violations in fundamental and health law [4].\n\nPersonal data is sensitive information that can be associated with an individual and is protected by various laws [5]. Personal privacy data in healthcare is called PHI and includes private information such as a patient's health history, treatments received, etc. [6].\n\nEHRS contain both valuable clinical information and PHI. While EHRs are a rich data source for research, their usability is restricted due to the confidentiality of PHI [7-10]. For example, the HIPAA law regulates the use of 18 types of PHIs, such as name, phone number, dates, etc. (Table 1) [11, 12]. Therefore, PHI must be extracted from the text before EHR data can be used. Automating de-identification systems is needed since manually extracting PHIs is time-consuming and costly. In addition, coordination between annotators is also an important consideration [13, 14]. While early approaches to de-identification relied on complex rules to detect PHI, recent developments use machine learning methods and train on expert-annotated records. Hybrid systems integrate practices as features into statistical models, like conditional random fields (CRFs) [15]."}, {"title": "2 Background", "content": "The de-identification model, called a Named Entity Recognition (NER) classification model, can be considered under four headings [31]:\n\n\u2022 Rule-based models\n\n\u2022 Machine learning models\n\n\u2022 Hybrid models\n\n\u2022 Deep learning models\n\nTechniques such as rule-based models and dictionaries can be easily implemented without labels but are vulnerable to input errors [31-34]. ML methods such as Support Vector Machines (SVM) and conditional random fields (CRF) can recognize complex patterns but require large amounts of labelled data and feature engineering and are poor at generalization [35-37]. Hybrid systems combine rule-based and ML models, providing high accuracy but requiring intensive feature engineering [38, 39].\n\nConsidering the disadvantages of the last three approaches to de-identification system creation, the latest state-of-the-art systems employ DL techniques to achieve better results than the best hybrid systems without requiring a time-consuming feature engineering process. DL is an ML subset using multilayered Artificial Neural Networks (ANN) and is very successful in most Natural Language Processing (NLP) tasks. Recent advances in DL and NLP (especially in the field of NER) enable the systems to outperform the winning hybrid system proposed by Yang and Garibaldi [39] on the 2014 i2b2 de-identification challenge dataset [31, 35].\n\nDe-identifying unstructured data is a widely recognized problem [40] in NLP, involving two key tasks: identifying PHI and replacing it through masking or obfuscation. Research has primarily"}, {"title": "3 Methodology", "content": "This section details the purpose of the research, the datasets employed, the methods for training and testing, the data preparation process, and the modelling and evaluation phases. Key to this study is the protection of personal data, adherence to legal regulations, and addressing the risks associated with processing sensitive patient information.\n\nOur LLM-in-the-loop methodology leverages LLMs at key stages such as synthetic data generation, labelling, and evaluation, focusing on the development of high-performance, expert small models. To this end, we used a combination of proprietary closed-source data, open-source datasets, and synthetic data, all annotated by our labelling team in accordance with i2b2 labelling logic. The incorporation of synthetic data and LLM-assisted labelling further enhanced the scope and quality of our training datasets.\n\nFor English-language de-identification NER models, we utilized the entire dataset for training. The i2b2 test dataset served as the exclusive test set for evaluation purposes, allowing us to benchmark performance with high precision. For non-English languages, we applied an 80-20 split for training and testing. Additionally, our medical translation models [30] were used to translate the English datasets into non-English languages, generating high-quality parallel datasets across multiple languages.\n\nIn the data pre-processing phase, we employed language-specific tools to ensure accurate de-identification across different languages. The \"Stanza\u201d library was utilized for Romanian-language tasks, while the \"NLTK\u201d library was used for the other languages. Word tokenization for all datasets was performed using the \"word-punct tokenizer\u201d from the NLTK library."}, {"title": "3.1 Datasets", "content": "\u201ci2b2-2014\u201d is a research project 2 on de-identification and heart disease in clinical texts, and its labelling logic was used in our study. For English-language de-identification NER models, we utilized a combination of mostly open-source and synthetic data, with 22% derived from proprietary closed-source data. The i2b2 test dataset served as the exclusive test set for evaluation, enabling us to benchmark performance with high precision. For non-English languages, we applied an 80-20 split for training and testing. Most of the non-English datasets were generated through translation from the English dataset using our medical translation models [30], open-source and through synthetic data generation with LLM-assisted labelling, producing high-quality parallel datasets across multiple languages.\n\nAdditionally, we utilized some NLP techniques and open-source third-party tools 3 to enhance and augment the training datasets.\n\nAlthough the i2b2 2014 dataset was not utilized for training purposes, we provide relevant information and statistics here to offer a more comprehensive understanding of its role in our evaluation process. i2b2/UTHealth is a dataset focused on identifying medical risk factors for Coronary Artery Disease (CAD) in the medical records of diabetic patients, where risk factors include hypertension, hyperlipidemia, obesity, smoking status, and family history, as well as diabetes, CAD, and indicators suggestive of the presence of these diseases [47]. i2b2 dataset consists of 1,304 progress notes of 296 diabetic patients. All PHIs in the dataset were removed throughout the study, and de-identification was performed randomly. The PHIs in this dataset were first categorized into HIPAA categories and then into i2b2-PHI categories, as shown in Table 2. Overall, the i2b2 dataset contains 56,348 sentences with 984,723 individual tokens, of"}, {"title": "3.2 Experimental Setup and Metrics", "content": "The corpus of clinical admission discharge and private clinical reports from private hospitals and healthcare organizations was used to develop the English de-identification model. Labelling was done according to i2b2 2014 data principles as described previously. The labels used in the model, which uses a fine-tuned version of the \"microsoft/deberta-v3-small\" model as an embedding, are shown in Table 3."}, {"title": "3.2.1 Clinical English de-identification Model", "content": "In the study, ten labels were used for the Rule-based method, and 18 labels were used for deep learning methods. The training dataset was augmented for these labels since ORGANIZATION,"}, {"title": "3.2.2 Non-English de-identification Models", "content": "To understand which labels could be used in de-identification models and which labels would be appropriate for which aggregates and to determine the principles, the labelling team organized meetings with relevant hospital staff to make the models in German, French, Italian, Romanian, Spanish, and Turkish. Data collected from clinical admission reports, discharge reports, and special clinic reports obtained from hospitals and health institutions were labelled according to these principles.\n\nThe training process was carried out with the obtained data set. In the study, the 0.20 parts of the dataset determined during the division process were used as the test dataset. The dataset was preprocessed and converted into BIO format.\n\nFor German: bert-base-german-cased, for Italian: bert-base-italian-cased, for French: camembert-bio-base, for Romanian: bert-base-ro-cased, for Turkish: bert-base-turkish-cased and for Spanish: roberta-base-biomedical-clinical-es were used as embeddings.\n\nThe augmentation stages of the other language models were performed as follows:\n\n\u2022 In the dataset used for the English in the de-identification model, a fake chunk data frame was created for each label in various formats.\n\n\u2022 Each labelled chunk was removed and replaced with label abbreviations\n\n\u2022 The sentences were translated from English to the working language. For the translation, our medical translation models used [30].\n\n\u2022 The label abbreviations in the new sentences were replaced with new chunks of those labels from the fake data frame.\n\n\u2022 This new data set was converted to BIO format and added to the train data set.\n\nThe de-identification research was performed with the DL method in seven languages other than English, learning rate=2e-5, max sentence length=512, batch size=16 (batch size=2 in Romanian), and ten epoch trains were performed."}, {"title": "4 Result", "content": "The results obtained from the de-identification NER models are shown in Table 4. In addition, the results obtained by using GPT-40 and the comparison results of other studies using the same dataset with the results obtained in this study are also included in the same table."}, {"title": "5 Conclusion", "content": "This study underscores the importance of de-identification as a key method for safeguarding patient/personal health information and ensuring its ethical use in scientific research. By removing identifiable details through techniques like anonymization, generalization, and differential privacy, de-identification allows data to be used for diverse scientific applications, including epidemiological studies, disease modelling, and artificial intelligence development while maintaining patient privacy.\n\nRecent advancements have demonstrated the potential of LLMs in de-identification tasks, yet challenges remain, particularly around issues of patient data security, API dependencies, and the need for domain-specific expertise in handling EHRs. Our \"LLMs-in-the-loop\" approach addresses these concerns by integrating small, specialized models tailored to the medical field. This method enhances both privacy and reliability, enabling the secure use of data without relying on external APIs or compromising sensitive patient information.\n\nThe multilingual nature of this research, spanning several languages, shows the adaptability and robustness of our models across diverse healthcare environments. While there are inherent risks associated with data anonymization, this study demonstrates that when properly applied, de-identification models can strike a delicate balance between protecting individual privacy and maximizing the utility of health data.\n\nFurthermore, as the field progresses, it is crucial to establish globally recognized standards, raise awareness of best practices, and ensure that ethical principles guide the deployment of de-identification technologies. Transparency, accountability, and a rigorous risk-benefit analysis must remain at the forefront of these efforts.\n\nUltimately, the findings of this study highlight the potential of expert small models developed through the LLMs-in-the-loop methodology to meet the evolving demands of healthcare research. The models presented here offer a reliable and scalable solution for future de-identification applications, advancing the capabilities of AI in healthcare while safeguarding patient privacy.\n\nFuture research should focus on further refining and expanding de-identification models to cover a wider range of languages and healthcare contexts. One of the primary challenges is the scarcity of high-quality, annotated datasets in languages other than English, which limits the development of robust models for non-English speaking regions. Addressing this gap will require collaborative efforts to create and share multilingual datasets, ensuring more comprehensive language coverage. Additionally, future studies could explore more advanced augmentation techniques and develop models capable of handling increasingly complex medical data types, such as clinical narratives and imaging reports. Continuous innovation in privacy-preserving methods, such as federated learning, may also prove valuable in safeguarding sensitive patient information while advancing the performance and applicability of de-identification technologies across diverse healthcare systems."}]}