{"title": "How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games", "authors": ["Yutong Xie", "Yiyao Liu", "Zhuang Ma", "Lin Shi*", "Xiyuan Wang*", "Walter Yuan", "Matthew O. Jackson", "Qiaozhu Mei"], "abstract": "The deployment of large language models (LLMs) in diverse applications requires a thorough understanding of their decision-making strategies and behavioral patterns. As a supplement to a recent study on the behavioral Turing test [7], this paper presents a comprehensive analysis of five leading LLM-based chatbot families as they navigate a series of behavioral economics games. By benchmarking these AI chatbots, we aim to uncover and document both common and distinct behavioral patterns across a range of scenarios. The findings provide valuable insights into the strategic preferences of each LLM, highlighting potential implications for their deployment in critical decision-making roles.", "sections": [{"title": "1 Introduction", "content": "In the rapidly advancing field of artificial intelligence, large language models (LLMs) are playing a transformative role in decision-making across diverse domains. These AI systems, capable of engaging in conversations, offering guidance, and tackling complex decisions, are becoming increasingly indispensable in scenarios requiring nuanced, human-like judgment [1-5, 8]. Understanding the behavioral patterns and decision-making strategies of AI chatbots is therefore critical. Such insights not only help optimize their performance in specific applications but also enable better assessment of their reliability and predictability, particularly in contexts involving significant responsibilities.\nOne recent study conducted by Mei et al. [7], has primarily focused on the behavior of OpenAI ChatGPT variations through a Turing test involving classic behavioral economics games. This study has revealed intricate details about ChatGPT's behavioral patterns and preferences in scenarios designed to test trust, fairness, risk aversion, altruism, cooperation, and other traits. However, it remains unclear whether these findings are unique to ChatGPT"}, {"title": "2 Methods", "content": "For each game and AI chatbot, we generate multiple responses using the respective game prompts, collecting 50 independent valid responses to establish the behavior distribution of each model. Human behavior distributions are taken from Mei et al. [7] for comparison."}, {"title": "2.1 LLM-Based AI Chatbots", "content": "This study focuses on five families of LLM-based AI chatbots, as detailed in Table 1. In the main text, results are presented exclusively for the flagship models. All model checkpoints were obtained as of July 31, 2024."}, {"title": "2.2 Collecting AI Chatbot Behaviors in Economics Games", "content": "Following Mei et al. [7], we employ six classic behavioral economics games to evaluate multiple dimensions of AI behavior, including altruism, fairness, trust, risk aversion, and cooperation. These games include Dictator, Ultimatum, Trust, Public Goods, Bomb Risk, and Prisoner's Dilemma. Detailed descriptions of the games and the associated prompts can be found in Mei et al. [7]."}, {"title": "3 Results", "content": "Figure 1 (and Figure 8 in the Appendix) illustrates the distributions of AI choices across the six games. Overall, the distributions of AI chatbots are notably more concentrated compared to human distributions, capturing only specific modes of human behavior. Additionally, different AI chatbots exhibit distinctly varied behavioral patterns, reflecting their unique orientations across multiple behavioral dimensions."}, {"title": "3.1 Behaviors of AI Chatbots", "content": "In games including Dictator (Fig. 1a) and Ultimatum\n- Proposer that reveal the altruism of players, AI chatbots display to be more altruistic than humans by offering more to the partner.\nSurprisingly, a large fraction of Google Gemini 1.5 Pro instances choose to offer most of the money ($90-$99) in Ultimatum - Proposer, showing its particularly high tendency of altruism.\nFairness. Fairness is often emphasized by AI chatbots across games. In the Dictator (Fig. 1a) and Ultimatum - Proposer Game (Fig. 1b), most AI chatbots choose to offer $50 to the partner, meaning a fair split. Correspondingly, Meta Llama 3.1 405B fairly requires a minimum split of $50 as the Responder in Ultimatum (Fig. 1c). Similarly in the Trust - Banker Game (Fig. 1e), OpenAI GPT 4o and Anthropic Claude 3.5 Sonnet tend to return the investment and half the profit ($100 in total) to the investor.\nTrust. The Trust Game (Fig. 1d) particularly shows the trust dynamics. As the investor in the Trust investment game, AI chatbots possess different levels of trust towards the banker \u2013 Anthropic Claude 3.5 Sonnet and Google Gemini 1.5 Pro display a higher trust level, investing $53.20 and $51.20 on average; While other models mostly invest $50 to the banker.\nAltruism."}, {"title": "3.2 The Behavioral Turing Test", "content": "Using the collected behavior distributions of AI chatbots and the excerpted human behaviors, we conduct Turing tests following the methodology outlined in Mei et al. [7]. Adopting the same procedure as described in the paper, each round of the test involves independently sampling one human action and one action from the Al behavior distribution. These samples are then compared based on their probabilities within the human distribution.\nFigure 2 presents the results of the Turing tests. Overall, all tested AI chatbots demonstrate a remarkable ability to pass the Turing test, with Meta Llama 3.1 405B achieving the highest winning rate against humans at 46.4%.\nHowever, in certain games, the chatbots exhibit significant challenges in replicating human behavior. For instance, in the Trust Game - Investor role (Fig. 2e), AI chatbots tend to invest conservatively, whereas a substantial fraction of human players opt to invest their entire amount (Fig. 1d). Similarly, in the Prisoner's Dilemma"}, {"title": "3.3 Behavior Distribution Similarity", "content": "While the Turing test is a valuable method for evaluating an Al's ability to act like a single human player [9], it has inherent limitations in capturing the complete spectrum of the behavior distribution. To overcome these limitations, we introduce a complementary approach: a distribution similarity test that assesses whether AI chatbots can accurately represent the behavior distribution of a human population.\nTable 3 (and Table 5 in the Appendix) presents the pairwise dissimilarities of behavior distributions, measured using the Wasserstein distance. Smaller distances indicate greater similarity between two distributions, whether comparing chatbots or humans and chatbots.\nAmong the AI chatbots, gpt-3.5-turbo-0613 demonstrates the highest similarity to the human population (Fig. 5), likely due to its ability to produce relatively diverse choices (Fig. 8). However, despite this similarity, a significant gap remains between the human behavior distribution and AI-generated actions, with no chatbot achieving a distribution that closely mirrors human behaviors.\nWe also observe relatively small Wasserstein distances among Meta Llama 3.1, Anthropic Claude models, and Mistral Large models (Fig. 5), indicating that these chatbots exhibit similar behavioral patterns."}, {"title": "3.4 Revealing the payoff Preferences", "content": "To uncover the intrinsic objectives underlying the behaviors of AI chatbots, we perform analyses to identify and characterize their payoff preferences.\nThe objective function of AI chatbots is quantitatively estimated by assessing the degree to which their behaviors align with the optimization goals. We adopt the family of utility functions from Mei et al. [7]:\n$U_b = [b \\times \\text{Own payoff} ^r + (1 \u2013 b) \\times \\text{Partner payoff} ^r]^{1/r},$\nwhere b \u2208 [0, 1] represents the trade-off between a player's own payoff and their partner's payoff. Specifically, b = 1 corresponds to purely selfish preferences, b = 0 represents purely selfless preferences, and b = 0.5 reflects a preference for maximizing the combined payoff of both players. In this context, r is a specification parameter that is frequently set to 1 (indicating a linear specification) or 1/2 (corresponding to a constant elasticity of substitution utility function, CES specification), as commonly adopted in the literature [6].\nObjective optimization efficiency."}, {"title": "3.5 Behavior Inconsistency", "content": "Although AI chatbots generally emphasize fairness and exhibit more selfless tendencies, they can display inconsistent behavior across different scenarios. For instance, a significant portion of Google Gemini 1.5 Pro instances choose to split the money fairly in the Dictator Game, yet in the Ultimatum - Proposer role, many instances propose offering nearly all the money ($90-$99) to the partner, reflecting an altruistic trait.\nTable 2 (and Table 4 in the Appendix) provides the estimated behavior inconsistencies of AI chatbots. These inconsistencies are measured using the mean absolute error (MAE) of payoff curves across different games (see Figures 6-7 in the Appendix)."}, {"title": "4 Discussion", "content": "As AI chatbots evolve, their behavioral ten-dencies shift over time. Figures 8(i,ii) illustrate these changes across checkpoints for OpenAI GPT-4 and GPT-3 models. For GPT-4, except for the Bomb Risk Game, the latest checkpoint produces more concentrated behavior distributions compared to older checkpoints. The updated version also demonstrates higher rationality in the Ultimatum - Responder Game but shows increased risk aversion in the Bomb Risk Game. For GPT-3, while the distribution modes largely remain consistent, the behavior distributions for the Ultimatum - Responder, Trust - Banker, and Bomb Risk games have become less diverse over successive updates.\nModel checkpoints.\nIn addition to different checkpoints, variations in model size can also influence behavior. As shown in Figures 8(iii,iv), Meta Llama 3 8B behaves notably differently from the 70B version. In games like Ultimatum - Proposer, Trust - Investor, Public Goods, and Prisoner's Dilemma, the 8B model exhibits more conservative tendencies. For both Llama 3 and Anthropic Claude 3, smaller mod-els (e.g., Llama 3 8B, Claude 3 Sonnet, and Claude 3 Haiku) display higher diversity in their behavior distributions compared to their larger counterparts.\nModel size."}, {"title": "5 Conclusion and Future Work", "content": "This study benchmarked LLM-based AI chatbots across a series of behavioral economics games. The analyses revealed the following common and distinct behavioral patterns of the chatbots: (1) All tested chatbots successfully capture specific human behavior modes, leading to highly concentrated decision distributions; (2) Although flagship chatbots demonstrate a notable probability of passing the Turing test, AI chatbots can merely produce a behavior distribution similar to humans; (3) Compared to humans, AI chatbots place greater emphasis on maximizing fairness in their payoff preferences; (4) AI chatbots may exhibit inconsistencies in their payoff preferences across different games; (5) Different AI chatbots exhibit distinct behavioral patterns in games, which can be further distinguished in our analyses. These findings highlight the effectiveness of our behavioral benchmark in profiling and differentiating AI chatbots.\nWe hope our research contributes to a deeper understanding of AI behaviors and serves as a foundation for future studies in Al behavioral science. For example, the discrepancies between Turing test results and distribution dissimilarities highlight the need for further alignment objectives that enable LLMs to better represent the diversity of the human behaviors. Additionally, the observed inconsistencies in Al behaviors across games underscore the importance of developing generalizable preferences and objectives for Al systems that can adapt effectively across various scenarios."}]}