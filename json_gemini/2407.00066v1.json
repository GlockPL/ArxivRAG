{"title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead", "authors": ["Rickard Br\u00fcel Gabrielsson", "Jiacheng Zhu", "Onkar Bhardwaj", "Leshem Choshen", "Kristjan Greenewald", "Mikhail Yurochkin", "Justin Solomon"], "abstract": "Fine-tuning large language models (LLMs) with low-rank adapters (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of compression when serving LoRA adapters. We consider compressing adapters individually via SVD and propose a method for joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. Our experiments with up to 500 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 75% of the throughput of serving a single LoRA.", "sections": [{"title": "Introduction", "content": "The myriad uses for foundation models (FMs) has led to a proliferation of specialized models, each fine-tuned to perform downstream tasks. The growing number of parameters of these models, however, incurs significant costs both for the fine-tuning and for serving these models.\nTo avoid fine-tuning giant foundation models, parameter-efficient fine-tuning (PEFT) algorithms update a smaller set of values that define an edit to the original model. An especially successful PEFT method is low-rank adaptation (LoRA) [8], which achieves parameter-efficient fine-tuning by learning low-rank additive changes to neural network matrices. Because of the low-rank parameterization, these matrices (called adapter weights) contain orders-of-magnitude fewer parameters than the base model. LoRA can achieve performance on par with full fine-tuning [8]. In addition, by merging the adapter weights into the base model, the fine-tuned model does not suffer from increased latency.\nLoRA's popularity has triggered a growing need to serve large collections of LoRA adapters at the scale where serving multiple copies of the base model becomes infeasible [25]. In the extreme, one may wish to serve hundreds to thousands of LoRAs, where each incoming request requires a different LoRA adapter, e.g., for a personalized chat assistant. To this end, S-LoRA [25] separates the base model computation from individual LoRA computations and optimizes the system's inner workings via custom CUDA kernels and memory management to increase the throughput when serving multiple LoRAs. Despite the optimized system design, serving LoRAs still has a fundamental limitation: when the number of adapters is large, they need to be constantly loaded and offloaded from GPU memory to accommodate incoming requests, degrading throughput."}, {"title": "Related Work", "content": "Parameter-efficient fine-tuning (PEFT) has become a prevalent method for updating foundation models thanks to the need for efficiency in training and communication [14]. Many PEFT methods have been proposed, e.g. [7, 16], but the most used in practice is LoRA [8], partially due to the ease of switching between LoRAs in inference time.\nSeveral works propose improvements to LoRA [17, 26], sometimes with algebraic methods like SVD [19, 34, 11] or by leveraging its statistical properties [35, 33]. Relatively few methods, however, accelerate inference times. S-LoRA [25] provides an efficient means of switching between Lo-RAs. F-LORA [28] adapts training to reduce batch multiplications and thus to accelerate inference. Our method achieves a similar outcome (see Appendix E) without changing the LoRA formulation or requiring that LoRAs be trained in a dedicated way; from this perspective, an advantage of our work is that future generic improvements to LoRA will also benefit (e.g., [19]).\nThere are many efforts to compress models [1, 5, 24, 13]\u2014including some that consider LoRAs specifically to accelerate inference. Predominantly, pruning and sparsification methods delete some of the weights [31], and quantization methods reduce the precision of the weights [4]. Some"}, {"title": "Rank-Based LoRA Compression", "content": "LORA updates are parameterized by pairs of matrices A, B, whose product BA updates the fixed weight matrices $W_o \\in \\mathbb{R}^{d_B \\times d_A}$ of a neural network foundation model. Given an input x to a layer, the output of the LoRA-updated model at this layer is $(W_o + BA)x$.\nIn formulating our compression algorithms, we consider a collection of given LoRA adapters $\\{(A_i, B_i)\\}_{i=1}^n$ that we would like to serve. We let $r_i$ refer to the rank of the LoRA adapter-pair $(A_i, B_i)$, i.e., $B_i \\in \\mathbb{R}^{d_B \\times r_i}, A_i \\in \\mathbb{R}^{r_i \\times d_A}$. After laying out the basic considerations in designing a compression scheme in \u00a73.1, we consider a simple baseline parameter reduction approach \u00a73.2 as well as a more aggressive joint diagonalization method in \u00a73.3."}, {"title": "Desiderata", "content": "While our compression technique has access only to a collection of $\\{(A_i, B_i)\\}_{i=1}^n$ pairs, in our experiments we will assess the efficacy of compression by comparing how the compressed matrices perform relative to the uncompressed LoRAs on typical data. For this reason, although in this section we optimize a Frobenius norm reconstruction error relative to the product $B_i A_i$, in reality this is a proxy for the nonlinear and complex way that compression errors in the adapters impact transformer performance. Our experimental evaluation will thus focus on the performance of the compressed LoRAs against the uncompressed versions on real data in \u00a76.\nOur compression methods significantly reduce the overall number of parameters. Reducing parameters through compression theoretically accelerates storage and serving processes for a collection of LoRAs. This reduction, however, alters the computational dynamics during inference, so parameter reduction alone does not immediately imply faster throughput. In light of the complexities of GPU optimization, we experimentally assess the throughput under realistic conditions in \u00a76.3."}, {"title": "Independent Compression via SVD", "content": "The simplest approach is to replace each rank-$r_i$ LoRA adapter $B_i A_i$ with a reduced rank-r approximation, where typically $r < r_i$:\n$SVD_r(B_iA_i) = U\\Sigma V^T, \\qquad \\forall i = 1,...,n$ (r \u2013 SVD)\nThanks to the Eckart-Young Theorem, this strategy finds the rank-r approximation that best fits $B_i A_i$ in terms of Frobenius norm. As $\\sum_i V_i^T$ can be saved as a single matrix, this approach reduces the number of parameters used from $\\sum_i (d_A + d_B)r_i$ to $rn(d_A + d_B)$. While this approximation is effective, it is limited in its compression abilities as n increases, since it does not share information across LoRAs. This shortcoming motivates our next approach."}, {"title": "Joint Diagonalization", "content": "Next, we suggest a Joint Diagonalization (JD) method, which optimizes a shared basis onto which we can project the set of n LoRAs. This will allow structure to be shared, implicitly clustering the collection of LoRAs.\nIn this model, each LoRA-product $B_i A_i$ is factorized into the form $U\\Sigma_i V^T$, where U and V are shared across all LoRAs and $\\Sigma_i$ is specific to each LoRA. In this formulation, every $\\Sigma_i$ shares the same rank r. This allows U and V to be pre-loaded onto the GPU, with $\\Sigma_i$ loaded when necessary"}, {"title": "Theoretical Analysis", "content": "While SVD-based approaches are relatively well-understood, in this section, we seek to better understand the role of the joint diagonalization method presented in \u00a73.3. We will focus on the full-$\\,\\Sigma_i$ case with orthogonal U, V matrices. Note that, for the same r, the r-JD-Diag has at least as large reconstruction error as r-JD- Full since it imposes an additional constraint on the $\\Sigma_i$.\nFirstly, note that perfect reconstruction can be achieved if and only if r is large enough:\nSuppose that for all i, rank$(B_i A_i) = r_i$, and let\n$\\hat{r} := max \\{rank([A_1,..., A_n]), rank([B_1 . . ., B_n]) \\}$.\nNote $max_i r_i \\leq r < \\sum_{i=1}^n r_i$. Then JD \u2013 Full (3) with $r = \\hat{r}$ achieves lossless compression (perfect reconstruction), and using $r < \\hat{r}$ will give nonzero reconstruction error.\nThere exist U, V such that all the $B_i$, $A_i$ are in the spans of U, V resp. if and only if $\\hat{r} \\geq r$.\nDue to training noise, $\\hat{r}$ will equal $\\sum_{i=1}^n r_i$ almost always. This implies that in most realistic settings, the joint diagonalization approach is a lossy reconstruction.\nThis reconstruction loss can be significant, as the following theorem shows (proved in Appendix C):"}, {"title": "Training LoRAs & Evaluating Task Performance", "content": "We trained LORA adapters on 500 natural instruction tasks [27] using Mistral-7B-Instruct-v0.2 [10] as the base model. All LoRA adapters were configured with a rank of 16, i.e., $\\forall i, r_i = 16$.\nWe selected 10 diverse tasks (Table 2 in Appendix D) manually for consistent evaluation across experiments and randomly sampled an additional 490 tasks, resulting in a total of 500 tasks. These tasks were exclusively in English (both input and output), ensuring higher quality and thorough review [27]. Each task dataset was divided into training, validation, and test sets.\nHyperparameters, such as early stopping, were tuned using the validation sets. Evaluation on the test sets demonstrated that LoRA consistently outperformed the base model in terms of both Rouge scores and loss metrics, as shown in Table 1. Details are provided in Appendix D."}, {"title": "Evaluation", "content": "We evaluated multiple metrics for the natural instruction tasks, including cross-entropy loss, Rouge-1, Rouge-L [15], exact match, and agreement between uncompressed and compressed LoRA. Here, agreement measures the exact match in task-generations between the uncompressed LoRA model and the compressed LoRA model, rather than comparing to ground truth data. While detailed results and discussions for all metrics are provided in Appendix F, our primary focus in the main text is on Rouge-L. We find that all metrics correlate, but Rouge-L correlates most strongly with downstream utility. This finding aligns with prior work [27], which demonstrates that Rouge-L correlates well with classification accuracy.\nWhile cross-entropy is used for optimization during training, identical generation outputs across models can yield different cross-entropy losses. Exact match is too rigid and does not account for the variability in task responses. Similarly, agreement does not capture the inexactness associated with most of our tasks, nor does it account for the performance gains or losses of the compressed LoRAs. Arguably, practitioners are primarily concerned with task performance in the settings for which the LORA was designed, rather than exact generational agreement between models.\nJoint diagonalization optimizes reconstruction error measured by the Frobenius norm, and our theoretical analysis in \u00a74 bounds this reconstruction error. Empirically, reconstruction error and downstream Rouge-L performance correlate.\nInstead of listing the absolute performance of different methods, we compute the performance difference between the base model and the LoRA model for each task. We present the ratio\nPerformance relative to LoRA := $\\frac{method-performance}{LORA-performance}$\nfor the specific method in question, highlighting relative improvement with respect to the uncompressed LoRA and the base model."}, {"title": "Experiments", "content": "For each method, we vary the number of n LoRAs that are compressed and the compression rank r. We run each experiment three times with different random seeds and report the mean and standard deviation. See Table 3 for results where we evaluate on the same ten manually selected tasks (Table 2) across settings. Every compressed collection of LoRAs contains these 10 tasks (i.e., in-distribution tasks), and each collection contains the smaller collections as subsets.\nWe normalize each LoRA adapter to have a Frobenius norm of one prior to running joint diagonalization. This normalization enhances performance and reduces the variance in reconstruction error. We restore the original norms of the LoRA adapters before reconstruction and testing."}, {"title": "Throughput of Serving Compressed LoRAs", "content": "Figure 4 studies the efficacy of compression in a real-world serving scenario. We consider a varying number of rank-16 LoRAs, using a dataset of Shakespeare sonnets as inputs arriving asynchronously. We measure throughput, i.e., the number of requests served per second when generating ten tokens per request.\nIn these experiments, the base model is Mistral 7B Instruct as in the previous experiments; we simulate random LoRAs and assign inputs to LoRAs at random. Experiments were conducted on A100 80GB GPU capped at 30% memory consumption. This is done to reflect cost concerns in practical situations where a service provider might want to serve many LoRAs from cheaper hardware with lower memory than higher-end GPUs. This setting also takes into account the scenario where the LLM is large compared to the size of GPU and yet a provider may want to serve many LoRAs efficiently using the same device.\nAlthough LoRAs are meant as parameter-efficient adapters, when serving over 200 LORAs, the throughput starts to degrade rapidly, illustrating the challenge of serving many LoRAs. When serving over 500 LoRAs, our compression methods provide over 2\u00d7 improvement in throughput, and at over 1000 LoRAs, JD-Full-64 serves at 75% of the throughput of the base LLM (equivalent to serving a single LoRA after merging it with the LLM).\nNext, we analyze trade-offs between the compression methods. Each compression method except JD-Full-64 and JD-Diag-128 starts to degrade faster at a certain point as the number of LoRAs increases; this point is the threshold at which compressed LoRAs no longer fit into the GPU memory and the throughput degrades due to scheduling. The JD LoRAs are the most efficient in terms of memory consumption, and we can simultaneously load all 1024 LoRAs. Due to their increased rank (e.g., each LoRA is of rank 128 for JD-Diag-128), however, the throughput starts lower, but degrades at a lower rate. We conclude that for the current vLLM design, when serving a moderate number of LoRAS, SVD compression provides favorable throughput as long as it can fit all LoRAS into memory. When serving a large number of LoRAs that cannot fit into memory even with SVD compression, JD is the better option, as it can fit all LoRAs onto the GPU at the cost of a slight throughput reduction due to its larger rank.\nOur analysis above suggests that scheduling LoRAs is an important bottleneck. Compressed LoRAs, especially with JD, significantly reduce the amount of parameters that need to be loaded onto a GPU to process a request requiring an offloaded LoRA. Designing a custom scheduler that can leverage"}, {"title": "Discussion", "content": "This study introduces approaches to LoRA compression, addressing significant challenges facing foundation models and large language models. Our contributions include theoretical formulations, empirical validation, and practical implementations that enhance understanding and application of LLMs in scalable environments.\nThe implications of our findings are manifold. Our theoretical guarantees for reconstruction error not only increase confidence in the use of compressed models but also lay a groundwork for future explorations in this area. Demonstrating that our compression techniques can preserve up to 100% of the original LoRAs' performance highlights the effectiveness of our methods. Furthermore, integrating LoRA compression into state-of-the-art LLM serving systems demonstrates the potential for resource optimization, with throughput for thousands of LoRAs nearing that of a single LoRA.\nA primary limitation involves integration of new LoRAs. Our findings indicate that while compressing a new LoRA within the original space improves upon the baseline, it can degrades performance compared to uncompressed LoRAs (see Appendix F.11). Thus, the optimal strategy varies with the frequency of new additions. Infrequent additions justify recompressing all LoRAs given the minimal impact relative to inference costs. Conversely, in scenarios where a proportion of new LoRAs are often introduced but seldom served, one should compress only frequently-served LoRAs and otherwise use the original LoRAS.\nThe promising results of our study suggest several future research directions. Extending our compression techniques to a wider array of models and tasks could further substantiate and expand the generalizability of our findings. This includes out-of-distribution evaluations, which can highlight generalization improvements associated with compression. Moreover, more sophisticated optimization algorithms could improve the balance between compression and performance.\nIn conclusion, our research significantly advances the deployment of LLMs by providing robust, scalable, and efficient compression solutions. The ability of compressed LoRAs to maintain high performance while facilitating substantial resource savings opens new avenues for the broader application and adoption of LLMs across various industries. We encourage the community to build upon our findings and the shared LoRAs to further explore and enhance the utility of these technologies."}, {"title": "Limitations", "content": "Compression techniques, while reducing time or memory constraints, invariably lose information, which can increase with the number of models compressed. Consequently, while our approach seeks to minimize this loss, it is axiomatic that compression will diminish information to some degree.\nOur methodology compresses a collection of known models. Introducing new models may necessitate either recompression, projecting into a suboptimal basis, or opting to use some models in an uncompressed state.\nEmpirically, our methods achieve reasonable balances between the number of models compressed, the amount of information retained, and throughput. Nonetheless, these benefits can be accompanied by increased latency as more models are compressed."}, {"title": "Joint Diagonalization Algorithms", "content": "Our goal is to derive algorithms that optimize (2). Common to both methods, we expand the objective functional:\n$\\sum_i||B_iA_i \u2013 UV \\Sigma_i V^T||_{Fro}^2 = \\sum_i tr((B_iA_i - UV \\Sigma_i V^T)(B_iA_i \u2013 UV \\Sigma_i V^T)^T)$ by definition\n$= \\sum_i [tr(B_iA_iA_i^TB_i^T) \u2013 2tr(B_iA_iV\\Sigma_i^T U^T) + tr(U\\Sigma_i V^TV\\Sigma_i^T U^T)]$\n$= const. \u2013 2 \\sum_i tr(B_iA_iV\\Sigma_i^T U^T) + \\sum_i ||U\\Sigma_i V^T||_{Fro}^2$.\nUsing this expansion, we now consider the two settings discussed in \u00a73.3."}, {"title": "Case 1: Non-diagonal $\\,\\Sigma_i$, orthogonal U, V", "content": "Setting the derivative of (5) with respect to $\\Sigma_i$ to zero, we find\n$\\Sigma_i = \\Sigma(U, V) = U^TB_iA_iV$.\nWe simplify our objective function after plugging in this expression:\n$\\sum_i||B_iA_i \u2013 UV \\Sigma_i V^T||_{Fro}^2 + const. = \\sum_i[||B_iA_i||_{Fro}^2 \u2013 2tr(B_iA_iV\\Sigma_i^T U^T)]$ from (5)\n$= \\sum_i[tr(U^T B_iA_iV V^T A_i^T B_i^T U) \u2013 2tr(B_iA_iV V^T A_i^T B_i^T U)] from (6)\n$= - \\sum_i tr(U^T B_iA_iV V^T A_i^T B_i^T U)$.\nSubstituting (6), we find\n$U_{opt}, V_{opt} = \\underset{UU^T=I}{\\arg \\underset{VV^T=I}{max}}\\sum_i ||U^T B_iA_iV ||_{Fro} = \\underset{UU^T=I}{\\arg \\underset{VV^T=I}{max}}\\sum_i || \\Sigma_i (U, V)||_{Fro}^2$.\n$\\sum_i ||U^T B_iA_iV ||_{Fro} = tr(\\sum_i (B_iA_iVV^T A_i^T B_i^T) U U^T)$\nNote that: \n$= tr(\\sum_i (A_i^T B_i^T UU^T B_iA_i) V V^T)$\nby the identity $|| A||_{Fro}^2 = tr(A^T A)$. Hence, we optimize (7) by alternating between U and V:"}, {"title": "U iteration:", "content": "Define $M := \\sum_i B_iA_iVV^T A_i^T B_i^T$. Parenthesizing this expression properly requires only $O((m + n)r)$ storage/computation time. With this definition, we maximize tr$(MUU^T)$ over U satisfying $U^TU = I$. Since M is positive semidefinite, the optimum is to take U to be the r eigenvectors of M with largest eigenvalue, equivalent to an SVD problem."}, {"title": "V iteration:", "content": "Define $N := \\sum_i A_i^T B_i^T U U^T B_iA_i$. Similarly to the previous step, we take V to contain the r eigenvectors of N with largest eigenvalue, again solvable using an SVD.\nThis method decreases the objective in each step."}, {"title": "Case 2: Diagonal $\\,\\Sigma_i$.", "content": "If constrain $\\Sigma_i$ to be diagonal, we interpret our objective function (2) as a \u201ctriple least squares\u201d problem. We compute gradients:\n$\\nabla_U \\sum_i ||B_iA_i \u2013 UV \\Sigma_i V^T||_{Fro}^2 = -2 \\sum_i (U \\Sigma_i V^T \u2013 B_iA_i)V\\Sigma_i^T$\n$\\nabla_V \\sum_i ||B_iA_i \u2013 UV \\Sigma_i V^T||_{Fro}^2 = -2 \\sum_i (V \\Sigma_i^T U^T \u2013 A_i^T B_i^T)U \\Sigma_i$\n$\\nabla_{\\Sigma_i} \\sum_i ||B_iA_i \u2013 UV \\Sigma_i V^T||_{Fro}^2 = -2 U^T (B_iA_i V \u2013 UV \\Sigma_i)$.\nThese expressions suggest efficient r \u00d7 r linear systems to solve for U, V:\n$U = (\\sum B_iA_iV \\Sigma_i^T)(\\sum V\\Sigma_i V^T)^{-1}$\n$V = (\\sum A_i^T B_i^T U \\Sigma_i)(\\sum U \\Sigma_i U^T)^{-1}$\nFor $\\Sigma_i$, we extract the diagonal from our gradient above:\ndiag$(U^TB_iA_i V)_{jj} = (U^TB_iA_i V)_{jj}$\n$= \\sum_{m} (U^TB_i)_{jm} diag(\\Sigma_i)_{mm} (V^T A_i)_ {mj}$\n$= (U^T B_iA_i V \\odot  V^T diag(\\Sigma_i))$\ndiag$(U^T B_iA_i V)_{jj} = \\sum_{m} (U^T B_i)_{jm} (A_i V)_{mj}$\n$= \\sum_{m} (U^T B_i)_{jm} (A_i V)_{jm}$\n$= (U^T B_i \\odot  A_i V)1$\ndiag($\\Sigma_i$) = $(U^T U \\odot  V^TV)^{-1} (U^T B_i \\odot  A_i V)1$\nHere $\\odot$ denotes the Hadamard product.\nCombining these expressions, we use a simple coordinate descent algorithm cycling between the following three steps:\n1.  Solve for U\n2.  Solve for V\n3.  Solve for the $\\Sigma_i$'s\n4.  Optionally, normalize so $\\sum_i || \\Sigma_i ||_{Fro} = 1$"}, {"title": "Additional Eigenvalue Iteration Algorithm", "content": "For the first case in \u00a7B.1, we introduce an alternative algorithm that eschews the use of SVD. This alternative is optimized for GPU execution, enabling tractable runs to convergence.\nTo derive this algorithm, we employ Lagrange multipliers to formulate the derived objective from (7):\n$U_{opt}, V_{opt} = \\underset{U^TU=I}{\\arg \\underset{VV^T=I}{max}}\\sum_i ||U^T B_iA_iV ||_{Fro}^2$ \nyielding the expression\n$\\Lambda = \\frac{1}{2}||U^T B_iA_iV ||_{Fro}^2 - \\frac{1}{2}tr(\\Xi^T (I-UU^T)) - \\frac{1}{2}tr(\\Upsilon^T (I \u2013 VTV)).$"}, {"title": "Taking the derivatives gives", "content": "$\\nabla_U \\Lambda = - \\sum B_i(A_iV)(A_iV)^T (B_i^TU) + UX$\n$\\nabla_V \\Lambda = - \\sum A_i^T(B_i^TU)(U^T B_i)(A_iV) + VY$\nSetting these derivatives to zero shows\n$\\sum B_i(A_iV)(A_iV)^T (B_i^TU) = UX$\n$\\sum A_i^T(B_i^TU)(U^T B_i)(A_iV) = VY$.\nHere, one can show that the Lagrange multiplier matrices X and Y are diagonal and nonnegative, since the problem reduces to an eigenvalue problem when either U or V is fixed; this is essentially the argument behind the alternating algorithm in Appendix B. Hence, taking inspiration from classical eigenvalue iteration, we use the following updates to improve our estimates of U and V:\n$U^{(k+1)} \\leftarrow \\sum B_i(A_iV^{(k)})((V^{(k)})^TA)^T (B_i^TU^{(k)})$\n$V^{(k+1)} \\leftarrow \\sum A_i^T(B_i^TU^{(k)})((U^{(k)})^TB_i)(A_iV^{(k)})$\n$U^{(k+1)} \\leftarrow orthogonalize(U^{(k+1)})$\n$V^{(k+1)} \\leftarrow orthogonalize(V^{(k+1)})$"}, {"title": "Here, the function orthogonalize orthogonalizes the columns of a matrix, e.g. by using the Q part of the reduced-size QR factorization.", "content": "Although we lack a formal convergence proof, in practice we find that this method reliably reaches a local optimum of our problem.\nBy executing matrix operations in the specified sequence, these computations can be rapidly performed on GPUs. Note the expressions above are parenthesized to avoid constructing a large matrix product as an intermediate computation."}, {"title": "Proof of Theorem 1", "content": "For the lower bound, note that by Jensen's inequality,\n$\\sum_i ||U^T B_iA_iV ||_{Fro}^2  \\geq  |\\sum_i U^T B_iA_iV|_{Fro}^2$,\nfor any U, V. Hence,\n$\\underset{U,V\\in S_t(k,d)}{sup} \\sum_i ||U^T B_iA_iV ||_{Fro}^2  \\geq  \\underset{U,V\\in S_t(k,d)}{sup} |\\sum_i U^T B_iA_iV|_{Fro}^2$.\nBy the definition of singular value decomposition, the right hand side of (18) is maximized with U, V being the top r singular vectors of $\\sum_{i=1}^n B_i A_i$, yielding $||U^T \\sum_{i=1}^n B_iA_iV ||_{Fro} = \\sum_{j=1}^r \\sigma_j^2$.\nRecalling that $\\Sigma_i = U^T B_iA_iV yields the lower bound.\nFor the upper bound, recall that $\\Sigma_i = U^T B_iA_iV$. Rearranging,\nvec($\\Sigma_i$) = $(V^T \\otimes  U^T)$vec$(B_i A_i)$.\n$\\Sigma := [vec(\\Sigma_1), ..., vec(\\Sigma_n)]$.\nBy our previous simplification,\n$\\Sigma = (V^T \\otimes  U^T)L$"}, {"title": "Now", "content": "$\\sum_{i} ||\\Sigma_{i}||_{Fro}^{2} = ||\\Sigma||_{Fro}^{2} = tr (((V \\otimes U)(V \\otimes U)^T)(LL^T))$\nSince U, V are orthogonal and size d \u00d7 r, the top $r^2$ eigenvalues of the symmetric matrix $(V \\otimes  U)(V \\otimes  U)^T$ will be equal to 1, and the rest will equal 0. The eigenvalues of the symmetric matrix $LL^T$ will be equal to the squared singular values of L. We can then apply the Von Neumann trace inequality to obtain the upper bound.\nThe last statement follows from the Pythagorean theorem and the fact that the $\\Sigma_i$ is a projection of $B_i A_i$ to the U, V subspace.\nNote that we have only used the fact that the matrix $(V \\otimes  U)(V \\otimes  U)^T$ has singular values equal to 1; we have not used the fact that it has Kronecker product structure. On the other hand, each vector vec$(B_i A_i)$ is a sum of $r_i$ Kronecker products and cannot be expressed as a Kronecker product. As a result, while the upper bound in the Von Neumann trace inequality is achieved if the eigenvectors of the two matrices align, the Kronecker product structure is a severe constraint and the upper bound we have provided is generous."}, {"title": "Training LoRAs", "content": "We trained LORA adapters on 500 natural instruction tasks [27] using Mistral-7B-Instruct-v0.2 [10] as the base model. All LoRA adapters were configured with a rank of 16, i.e., $\\forall i, r_i = 16$. We selected 10 diverse tasks manually for consistent evaluation across experiments and randomly sampled an additional 490 tasks, resulting in a total of 500 tasks. These tasks were exclusively in English (both input and output), ensuring higher quality and thorough review [27]. Each task dataset was divided into training, validation, and test sets (80-10-10). Hyperparameters, such as early stopping, were tuned using the validation sets; that is, we train for five epochs and take the best-performing epoch-checkpoint per validation loss. Evaluation on the test sets demonstrated that LoRA consistently outperformed the base model in terms of both Rouge scores and loss metrics (see Table 1)."}, {"title": "Avoiding Batched Matrix Multiplication (BMM)", "content": "In the envisioned deployment scenario, a service provider hosts a large collection of LoRAs. Upon receiving a request, each user specifies both the input data and the desired LoRA identifier. The provider then processes the base model augmented with the specified LoRA for each user's data. As a provider is batching a collection of requests for GPU parallelization, they can expect to frequently have more than one unique LoRA identifier per batch.\nTraditionally, a specific LoRA is integrated into the base model by transforming $W_o \\rightarrow W_o + B_iA_i$. Serving multiple LoRAs conventionally would necessitate maintaining and executing a separate copy of the base model for each LoRA, bringing substantial computational overhead. Alternatively, the computation for $W_o x$ and $B_iA_i x$ can be performed independently and subsequently merged. This strategy necessitates only a single instance of $W_o x$ computation and storage of LoRA specific parameters rather than the entire base model.\nConsider the batch processing of $B_i A_i x$, where boldface indicates that $B_i$, $A_i$ are stacked into tensors of dimensions $(b \\times m \\times r)$ and $(b \\times r \\times n)$ respectively, with batched data x shaped $(b \\times 1 \\times n)$:\n$A x \\leftrightarrow  (b \\times r \\times n) \\times (b \\times 1 \\times n) \\rightarrow (b \\times 1 \\times r)$ bmm\n$B(Ax) \\leftrightarrow  (b \\times m \\times r) \\times (b \\times 1 \\times r) \\rightarrow (b \\times 1 \\times m)$ bmm.\nHere, \u201cbmm\u201d denotes batched matrix multiplication, a known bottleneck in both throughput and latency. Consider the corresponding operations for our joint compression scheme, $U \\Sigma V^T x$:\n$V^T x \\leftrightarrow  (\\tilde{r} \\times n) \\times (b \\times 1 \\times n) \\rightarrow (b \\times 1 \\times \\tilde{r})$ broadcasted\n$\\Sigma (V^T x) \\leftrightarrow (b \\times \\tilde{r}) \\times (b \\times 1 \\times \\tilde{r}) \\rightarrow (b \\times 1 \\times \\tilde{r})$ broadcasted\n$U (\\Sigma V^T x) \\leftrightarrow (m \\times \\tilde{r}) \\times (b \\times 1 \\times \\tilde{r}) \\rightarrow (b \\times 1 \\times m)$ broadcasted\nIn our optimized setup, batched matrix multiplications can be completely circumvented if the $D_i$ matrices are diagonal. If not, given that $\\tilde{r} < m, n$, any required batched matrix multiplication remains computationally inexpensive."}]}