{"title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead", "authors": ["Rickard Br\u00fcel Gabrielsson", "Jiacheng Zhu", "Onkar Bhardwaj", "Leshem Choshen", "Kristjan Greenewald", "Mikhail Yurochkin", "Justin Solomon"], "abstract": "Fine-tuning large language models (LLMs) with low-rank adapters (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of compression when serving LoRA adapters. We consider compressing adapters individually via SVD and propose a method for joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. Our experiments with up to 500 LoRAS demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 75% of the throughput of serving a single LoRA.", "sections": [{"title": "Introduction", "content": "The myriad uses for foundation models (FMs) has led to a proliferation of specialized models, each fine-tuned to perform downstream tasks. The growing number of parameters of these models, however, incurs significant costs both for the fine-tuning and for serving these models.\nTo avoid fine-tuning giant foundation models, parameter-efficient fine-tuning (PEFT) algorithms update a smaller set of values that define an edit to the original model. An especially successful PEFT method is low-rank adaptation (LoRA) [8], which achieves parameter-efficient fine-tuning by learning low-rank additive changes to neural network matrices. Because of the low-rank parameterization, these matrices (called adapter weights) contain orders-of-magnitude fewer parameters than the base model. LoRA can achieve performance on par with full fine-tuning [8]. In addition, by merging the adapter weights into the base model, the fine-tuned model does not suffer from increased latency.\nLoRA's popularity has triggered a growing need to serve large collections of LoRA adapters at the scale where serving multiple copies of the base model becomes infeasible [25]. In the extreme, one may wish to serve hundreds to thousands of LoRAs, where each incoming request requires a different LoRA adapter, e.g., for a personalized chat assistant. To this end, S-LoRA [25] separates the base model computation from individual LoRA computations and optimizes the system's inner workings via custom CUDA kernels and memory management to increase the throughput when serving multiple LoRAs. Despite the optimized system design, serving LoRAs still has a fundamental limitation: when the number of adapters is large, they need to be constantly loaded and offloaded from GPU memory to accommodate incoming requests, degrading throughput.\nIn this work, we consider the problem of compressing a collection of LoRAs. We have two key objectives: (1) preserving the performance of the original LoRAs and (2) improving the throughput of serving many LoRAs. We formulate LoRA compression as a reconstruction problem, where the goal is to approximate the original adapters via collections of matrices of a smaller total size. We investigate two approaches: compressing each LoRA individually by lowering its rank via SVD and compressing LoRAs jointly by finding a shared basis and LoRA-specific scaling matrices. The former is inspired by [11], who demonstrated that lowering LoRA ranks is beneficial for multi-task learning and model merging. For the latter, we propose a joint compression algorithm inspired by models for joint diagonalization.\nOur individual and joint compression methods present interesting trade-offs. For example, joint compression can achieve extreme parameter count reduction when representing any specific LoRA by using a shared basis that can be pre-loaded onto the GPU. The shared basis, however, will typically increase the rank of LoRAs. On the other hand, individual compression can reduce the rank, but is limited in terms of its compression efficiency. We investigate how these trade-offs affect throughput when serving 1000s of LoRAs using vLLM [12], a state-of-the-art LLM serving system.\nWe summarize our main contributions below:\n\u2022 We formulate the problem of compressing a collection of LoRAs and propose two solutions: individual and joint compression.\n\u2022 We establish theoretical guarantees for the reconstruction error central to our compression formulation and verify the relation between reconstruction loss and performance empirically.\n\u2022 We train a collection of 500 high-quality LoRAs for Mistral-7B-Instruct-v0.2 [10] on 500 natural instruction tasks [27] and demonstrate that our compression techniques can preserve the performance of the original LoRAs. We will release the 500 LoRAs to facilitate future work on LORA compression as well as the code to compress new models.\n\u2022 We incorporate LoRA compression into a state-of-the-art LLM serving system and demonstrate that it is possible to serve over 1000 LoRAs across thousands of asynchronous requests with throughput comparable to serving a single LoRA.\n\u2022 We analyze the trade-offs between individual and joint compression, presenting opportunities for further improvement of LLM serving systems."}, {"title": "Related Work", "content": "Parameter-efficient fine-tuning (PEFT) has become a prevalent method for updating foundation models thanks to the need for efficiency in training and communication [14]. Many PEFT methods have been proposed, e.g. [7, 16], but the most used in practice is LoRA [8], partially due to the ease of switching between LoRAs in inference time.\nSeveral works propose improvements to LoRA [17, 26], sometimes with algebraic methods like SVD [19, 34, 11] or by leveraging its statistical properties [35, 33]. Relatively few methods, however, accelerate inference times. S-LoRA [25] provides an efficient means of switching between Lo-RAs. F-LoRA [28] adapts training to reduce batch multiplications and thus to accelerate inference. Our method achieves a similar outcome (see Appendix E) without changing the LoRA formulation or requiring that LoRAs be trained in a dedicated way; from this perspective, an advantage of our work is that future generic improvements to LoRA will also benefit (e.g., [19]).\nThere are many efforts to compress models [1, 5, 24, 13]\u2014including some that consider LoRAs specifically to accelerate inference. Predominantly, pruning and sparsification methods delete some of the weights [31], and quantization methods reduce the precision of the weights [4]. Some"}, {"title": "Rank-Based LoRA Compression", "content": "LORA updates are parameterized by pairs of matrices $A, B$, whose product $BA$ updates the fixed weight matrices $W_0 \\in \\mathbb{R}^{d_B \\times d_A}$ of a neural network foundation model. Given an input $x$ to a layer, the output of the LoRA-updated model at this layer is $(W_0 + BA)x$.\nIn formulating our compression algorithms, we consider a collection of given LoRA adapters $\\{(A_i, B_i)\\}_{i=1}^n$ that we would like to serve. We let $r_i$ refer to the rank of the LoRA adapter-pair $(A_i, B_i)$, i.e., $B_i \\in \\mathbb{R}^{d_b \\times r_i}, A_i \\in \\mathbb{R}^{r_i \\times d_a}$. After laying out the basic considerations in designing a compression scheme in \u00a73.1, we consider a simple baseline parameter reduction approach \u00a73.2 as well as a more aggressive joint diagonalization method in \u00a73.3."}, {"title": "Desiderata", "content": "While our compression technique has access only to a collection of $\\{(A_i, B_i)\\}_{i=1}^n$ pairs, in our experiments we will assess the efficacy of compression by comparing how the compressed matrices perform relative to the uncompressed LoRAs on typical data. For this reason, although in this section we optimize a Frobenius norm reconstruction error relative to the product $B_i A_i$, in reality this is a proxy for the nonlinear and complex way that compression errors in the adapters impact transformer performance. Our experimental evaluation will thus focus on the performance of the compressed LoRAs against the uncompressed versions on real data in \u00a76.\nOur compression methods significantly reduce the overall number of parameters. Reducing parameters through compression theoretically accelerates storage and serving processes for a collection of LoRAs. This reduction, however, alters the computational dynamics during inference, so parameter reduction alone does not immediately imply faster throughput. In light of the complexities of GPU optimization, we experimentally assess the throughput under realistic conditions in \u00a76.3."}, {"title": "Independent Compression via SVD", "content": "The simplest approach is to replace each rank-$r_i$ LoRA adapter $B_i A_i$ with a reduced rank-$r$ approximation, where typically $r < r_i$:\n$SVD_r(B_iA_i) = U_i \\Sigma_i V_i^T, \\forall i = 1,...,n \\quad \\text{(r \u2013 SVD)}$\nThanks to the Eckart-Young Theorem, this strategy finds the rank-$r$ approximation that best fits $B_i A_i$ in terms of Frobenius norm. As $\\sum_i U_i \\Sigma_i V_i^T$ can be saved as a single matrix, this approach reduces the number of parameters used from $\\sum_i (d_a + d_B)r_i$ to $rn(d_a + d_B)$. While this approximation is effective, it is limited in its compression abilities as $n$ increases, since it does not share information across LoRAs. This shortcoming motivates our next approach."}, {"title": "Joint Diagonalization", "content": "Next, we suggest a Joint Diagonalization (JD) method, which optimizes a shared basis onto which we can project the set of $n$ LoRAs. This will allow structure to be shared, implicitly clustering the collection of LoRAs.\nIn this model, each LoRA-product $B_i A_i$ is factorized into the form $U \\Sigma_i V^T$, where $U$ and $V$ are shared across all LoRAs and $\\Sigma_i$ is specific to each LoRA. In this formulation, every $\\Sigma_i$ shares the same rank $r$. This allows $U$ and $V$ to be pre-loaded onto the GPU, with $\\Sigma_i$ loaded when necessary"}, {"title": "Theoretical Analysis", "content": "While SVD-based approaches are relatively well-understood, in this section, we seek to better understand the role of the joint diagonalization method presented in \u00a73.3. We will focus on the full-$\\Sigma_i$ case with orthogonal $U, V$ matrices. Note that, for the same $r$, the $r$-JD-Diag has at least as large reconstruction error as $r$-JD- Full since it imposes an additional constraint on the $\\Sigma_i$.\nFirstly, note that perfect reconstruction can be achieved if and only if $r$ is large enough:\nProposition 1. Suppose that for all $i$, $\\text{rank}(B_i A_i) = r_i$, and let\n$ \\tilde{r} = \\max \\{ \\text{rank}([A_1,..., A_n]), \\text{rank}([B_1 . . . , B_n]) \\}$.\nNote $\\text{max}_i r_i \\leq r < \\sum_{i=1}^n r_i$. Then JD \u2013 Full (3) with $r = \\tilde{r}$ achieves lossless compression (perfect reconstruction), and using $r < \\tilde{r}$ will give nonzero reconstruction error.\nProof. There exist $U, V$ such that all the $B_i, A_i$ are in the spans of $U, V$ resp. if and only if $r \\geq \\tilde{r}$.\nDue to training noise, $\\tilde{r}$ will equal $\\sum_{i=1}^n r_i$ almost always. This implies that in most realistic settings, the joint diagonalization approach is a lossy reconstruction.\nThis reconstruction loss can be significant, as the following theorem shows (proved in Appendix C):"}, {"title": "Training LoRAs & Evaluating Task Performance", "content": "We trained LORA adapters on 500 natural instruction tasks [27] using Mistral-7B-Instruct-v0.2 [10] as the base model. All LoRA adapters were configured with a rank of 16, i.e., $\\forall i, r_i = 16$.\nWe selected 10 diverse tasks manually for consistent evaluation across experiments and randomly sampled an additional 490 tasks, resulting in a total of 500 tasks. These tasks were exclusively in English (both input and output), ensuring higher quality and thorough review [27]. Each task dataset was divided into training, validation, and test sets.\nHyperparameters, such as early stopping, were tuned using the validation sets. Evaluation on the test sets demonstrated that LoRA consistently outperformed the base model in terms of both Rouge scores and loss metrics\nWe evaluated multiple metrics for the natural instruction tasks, including cross-entropy loss, Rouge-1, Rouge-L [15], exact match, and agreement between uncompressed and compressed LoRA. Here, agreement measures the exact match in task-generations between the uncompressed LoRA model and the compressed LoRA model, rather than comparing to ground truth data. While detailed results and discussions for all metrics are provided in Appendix F, our primary focus in the main text is on Rouge-L. We find that all metrics correlate, but Rouge-L correlates most strongly with downstream utility. This finding aligns with prior work [27], which demonstrates that Rouge-L correlates well with classification accuracy.\nWhile cross-entropy is used for optimization during training, identical generation outputs across models can yield different cross-entropy losses. Exact match is too rigid and does not account for the variability in task responses. Similarly, agreement does not capture the inexactness associated with most of our tasks, nor does it account for the performance gains or losses of the compressed LoRAs. Arguably, practitioners are primarily concerned with task performance in the settings for which the LORA was designed, rather than exact generational agreement between models.\nJoint diagonalization optimizes reconstruction error measured by the Frobenius norm, and our theoretical analysis in \u00a74 bounds this reconstruction error. Empirically, reconstruction error and downstream Rouge-L performance correlate.\nInstead of listing the absolute performance of different methods, we compute the performance difference between the base model and the LoRA model for each task. We present the ratio\nPerformance relative to LoRA := $\\frac{\\text{method-performance}}{\\text{LORA-performance}}$\nfor the specific method in question, highlighting relative improvement with respect to the uncompressed LoRA and the base model."}, {"title": "Experiments", "content": "For each method, we vary the number of $n$ LoRAs that are compressed and the compression rank $r$. We run each experiment three times with different random seeds and report the mean and standard deviation.\nWe normalize each LoRA adapter to have a Frobenius norm of one prior to running joint diagonalization. This normalization enhances performance and reduces the variance in reconstruction error. We restore the original norms of the LoRA adapters before reconstruction and testing.\nFigure 2a plots the mean performance improvement relative to uncompressed LoRAs (y-axis) against the GPU workload parameter saved ratio, i.e., the reduction in per-LoRA parameters (x-axis). Similarly, Figure 2b relates mean performance improvement to the total parameter saved ratio. Both the independent compression via SVD and joint diagonalization methods significantly compress the LoRAs while preserving\u2014and occasionally enhancing\u2014performance. Notably, the JD methods are unique in their ability to approach the compression efficacy of a single LoRA, although this aggressive reduction in size may decrease performance in larger LoRA collections. As shown in Figure 4, a JD variant achieves the highest throughput. These results provide practitioners with a spectrum of methods and configurations, enabling them to optimize the balance between throughput, compression, and performance according to their specific requirements.\nFor efficiency, we limited the JD methods to ten iterations instead of pursuing full convergence. While the alternating algorithm quickly reaches an approximation of the minimizer, squeezing out the last few digits of precision takes many more iterations with limited to no performance gain."}, {"title": "Performance and Reconstruction Error", "content": "Figure 3 relates reconstruction error and performance. The y-axis measures mean performance improvement of Rouge-L relative to uncompressed LoRA, and the x-axis quantifies the mean reconstruction error between the compressed reconstruction of the product $BA$ and the original uncompressed product $BA$. Although the relationship between performance and reconstruction error is nonlinear, it demonstrates a generally decreasing, somewhat exponential trend. Notably, the minimal reconstruction error does not correlate with optimal performance, indicating that a degree of lossy reconstruction may be advantageous for enhancing generalization."}, {"title": "Throughput of Serving Compressed LoRAS", "content": "Figure 4 studies the efficacy of compression in a real-world serving scenario. We consider a varying number of rank-16 LoRAs, using a dataset of Shakespeare sonnets as inputs arriving asynchronously. We measure throughput, i.e., the number of requests served per second when generating ten tokens per request.\nIn these experiments, the base model is Mistral 7B Instruct as in the previous experiments; we simulate random LoRAs and assign inputs to LoRAs at random. Experiments were conducted on A100 80GB GPU capped at 30% memory consumption. This is done to reflect cost concerns in practical situations where a service provider might want to serve many LoRAs from cheaper hardware with lower memory than higher-end GPUs. This setting also takes into account the scenario where the LLM is large compared to the size of GPU and yet a provider may want to serve many LoRAs efficiently using the same device.\nAlthough LoRAs are meant as parameter-efficient adapters, when serving over 200 LORAs, the throughput starts to degrade rapidly, illustrating the challenge of serving many LoRAs. When serving over 500 LoRAs, our compression methods provide over 2\u00d7 improvement in throughput, and at over 1000 LoRAs, JD-Full-64 serves at 75% of the throughput of the base LLM (equivalent to serving a single LoRA after merging it with the LLM).\nNext, we analyze trade-offs between the compression methods. Each compression method except JD-Full-64 and JD-Diag-128 starts to degrade faster at a certain point as the number of LoRAS increases; this point is the threshold at which compressed LoRAs no longer fit into the GPU memory and the throughput degrades due to scheduling. The JD LoRAs are the most efficient in terms of memory consumption, and we can simultaneously load all 1024 LoRAs. Due to their increased rank (e.g., each LoRA is of rank 128 for JD-Diag-128), however, the throughput starts lower, but degrades at a lower rate. We conclude that for the current vLLM design, when serving a moderate number of LoRAS, SVD compression provides favorable throughput as long as it can fit all LoRAS into memory. When serving a large number of LoRAs that cannot fit into memory even with SVD compression, JD is the better option, as it can fit all LoRAs onto the GPU at the cost of a slight throughput reduction due to its larger rank.\nOur analysis above suggests that scheduling LoRAs is an important bottleneck. Compressed LoRAs, especially with JD, significantly reduce the amount of parameters that need to be loaded onto a GPU to process a request requiring an offloaded LoRA. Designing a custom scheduler that can leverage"}, {"title": "Discussion", "content": "This study introduces approaches to LoRA compression, addressing significant challenges facing foundation models and large language models. Our contributions include theoretical formulations, empirical validation, and practical implementations that enhance understanding and application of LLMs in scalable environments.\nThe implications of our findings are manifold. Our theoretical guarantees for reconstruction error not only increase confidence in the use of compressed models but also lay a groundwork for future explorations in this area. Demonstrating that our compression techniques can preserve up to 100% of the original LoRAs' performance highlights the effectiveness of our methods. Furthermore, integrating LoRA compression into state-of-the-art LLM serving systems demonstrates the potential for resource optimization, with throughput for thousands of LoRAs nearing that of a single LoRA.\nA primary limitation involves integration of new LoRAs. Our findings indicate that while compressing a new LoRA within the original space improves upon the baseline, it can degrades performance compared to uncompressed LoRAs (see Appendix F.11). Thus, the optimal strategy varies with the frequency of new additions. Infrequent additions justify recompressing all LoRAs given the minimal impact relative to inference costs. Conversely, in scenarios where a proportion of new LoRAs are often introduced but seldom served, one should compress only frequently-served LoRAs and otherwise use the original LoRAS.\nThe promising results of our study suggest several future research directions. Extending our compression techniques to a wider array of models and tasks could further substantiate and expand the generalizability of our findings. This includes out-of-distribution evaluations, which can highlight generalization improvements associated with compression. Moreover, more sophisticated optimization algorithms could improve the balance between compression and performance.\nIn conclusion, our research significantly advances the deployment of LLMs by providing robust, scalable, and efficient compression solutions. The ability of compressed LoRAs to maintain high performance while facilitating substantial resource savings opens new avenues for the broader application and adoption of LLMs across various industries. We encourage the community to build upon our findings and the shared LoRAs to further explore and enhance the utility of these technologies."}, {"title": "Limitations", "content": "Compression techniques, while reducing time or memory constraints, invariably lose information, which can increase with the number of models compressed. Consequently, while our approach seeks to minimize this loss, it is axiomatic that compression will diminish information to some degree.\nOur methodology compresses a collection of known models. Introducing new models may necessitate either recompression, projecting into a suboptimal basis, or opting to use some models in an uncompressed state.\nEmpirically, our methods achieve reasonable balances between the number of models compressed, the amount of information retained, and throughput. Nonetheless, these benefits can be accompanied by increased latency as more models are compressed."}, {"title": "Joint Diagonalization Algorithms", "content": "Our goal is to derive algorithms that optimize (2). Common to both methods, we expand the objective functional:\n$\\sum_i||B_iA_i \u2013 UV\\Sigma_iV^T||_{Fro}^2 = \\sum_i tr((B_iA_i - UV\\Sigma_iV^T)(B_iA_i \u2013 UV\\Sigma_iV^T)^T)$ by definition\n= $\\sum_i [tr(B_iA_iA_i^TB_i^T) \u2013 2tr(B_iA_iV\\Sigma_i^TU^T) + tr(UV\\Sigma_iV^TV\\Sigma_i^TU^T)]$\n= const. \u2013 $2 \\sum_i tr(B_iA_iV\\Sigma_i^TU^T) + \\sum_i ||UV\\Sigma_iV^T||_{Fro}^2$.\nUsing this expansion, we now consider the two settings discussed in \u00a73.3.\nCase 1: Non-diagonal $\\Sigma_i$, orthogonal $U, V$. Setting the derivative of (5) with respect to $\\Sigma_i$ to zero, we find\n$\\Sigma_i = \\Sigma(U, V) = U^TB_iA_iV$.\nWe simplify our objective function after plugging in this expression:\n$\\sum_i||B_iA_i \u2013 UV\\Sigma_iV^T||_{Fro}^2 + const. = \\sum_i[||B_iA_i||_{Fro}^2 \u2013 2tr(B_iA_iV\\Sigma_i^TU^T)]  \\text{ from (5)}$\n=$\\sum_i [tr(U^T B_iA_iVV^TA_i^TB_i^T U) \u2013 2tr(B_iA_iV\\Sigma_i^T A_i^T B_i^T U)]  \\text{ from (6)}$\n= $- \\sum_i tr(U^T B_iA_iVV^T A_i^T B_i^T U^TU)$.\nSubstituting (6), we find\n$U_{opt}, V_{opt} = \\text{arg }\\underset{U^TU=I, V^TV=I}{\\text{max}} \\sum_i ||U^TB_iA_iV||_{Fro}^2 =  \\underset{U^TU=I, V^TV=I}{\\text{arg }\\text{max}} \\sum_i ||\\Sigma(U, V)||_{Fro}^2$ \nNote that\n$\\sum_i ||U^TB_iA_iV||_{Fro}^2 =  tr((\\sum B_iA_iVV^T A_i^T B_i^T) UUT)$\n= $tr((\\sum A_i^T B_i^T UUT B_iA_i) VVT)$\nby the identity $|| A||_{Fro}^2 = tr(A^TA)$. Hence, we optimize (7) by alternating between $U$ and $V$:\n\u2022 U iteration: Define $M := \\sum_i B_iA_iVVT A_i^TB_i^T$. Parenthesizing this expression properly requires only $O((m + n)r)$ storage/computation time. With this definition, we maximize $tr(MUUT)$ over U satisfying $UTU = I$. Since M is positive semidefinite, the optimum is to take U to be the r eigenvectors of M with largest eigenvalue, equivalent to an SVD problem."}, {"title": "Alternating Methods", "content": "Our goal is to derive algorithms that optimize (2). Common to both methods, we expand the objective functional:\n$\\sum_i||B_iA_i \u2013 UV\\Sigma_iV^T||_{Fro}^2 = \\sum_i tr((B_iA_i - UV\\Sigma_iV^T)(B_iA_i \u2013 UV\\Sigma_iV^T)^T)$ by definition\n= $\\sum_i [tr(B_iA_iA_i^TB_i^T) \u2013 2tr(B_iA_iV\\Sigma_i^TU^T) + tr(UV\\Sigma_iV^TV\\Sigma_i^TU^T)]$\n= const. \u2013 $2 \\sum_i tr(B_iA_iV\\Sigma_i^TU^T) + \\sum_i ||UV\\Sigma_iV^T||_{Fro}^2$.\nUsing this expansion, we now consider the two settings discussed in \u00a73.3.\nCase 1: Non-diagonal $\\Sigma_i$, orthogonal $U, V$. Setting the derivative of (5) with respect to $\\Sigma_i$ to zero, we find\n$\\Sigma_i = \\Sigma(U, V) = U^TB_iA_iV$.\nWe simplify our objective function after plugging in this expression:\n$\\sum_i||B_iA_i \u2013 UV\\Sigma_iV^T||_{Fro}^2 + const. = \\sum_i[||B_iA_i||_{Fro}^2 \u2013 2tr(B_iA_iV\\Sigma_i^TU^T)]  \\text{ from (5)}$\n=$\\sum_i [tr(U^T B_iA_iVV^TA_i^TB_i^T U) \u2013 2tr(B_iA_iV\\Sigma_i^T A_i^T B_i^T U)]  \\text{ from (6)}$\n= $- \\sum_i tr(U^T B_iA_iVV^T A_i^T B_i^T U^TU)$.\nSubstituting (6), we find\n$\\underset{U^TU=I, V^TV=I}{\\text{max}} \\sum_i ||U^TB_iA_iV||_{Fro}^2 =  \\underset{U^TU=I, V^TV=I}{\\text{arg }\\text{max}} \\sum_i ||\\Sigma(U, V)||_{Fro}^2$ \nNote that\n$\\sum_i ||U^TB_iA_iV||_{Fro}^2 =  tr((\\sum B_iA_iVV^T A_i^T B_i^T) UUT)$\n= $tr((\\sum A_i^T B_i^T UUT B_iA_i) VVT)$\nby the identity $|| A||_{Fro}^2 = tr(A^TA)$. Hence, we optimize (7) by alternating between $U$ and $V$:\n\u2022 U iteration: Define $M := \\sum_i B_iA_iVVT A_i^TB_i^T$. Parenthesizing this expression properly requires only $O((m + n)r)$ storage/computation time. With this definition, we maximize $tr(MUUT)$ over U satisfying $U^TU = I$. Since M is positive semidefinite, the optimum is to take U to be the r eigenvectors of M with largest eigenvalue, equivalent to an SVD problem."}, {"title": "Additional Eigenvalue Iteration Algorithm", "content": "For the first case in \u00a7B.1, we introduce an alternative algorithm that eschews the use of SVD. This alternative is optimized for GPU execution, enabling tractable runs to convergence.\nTo derive this algorithm, we employ Lagrange multipliers to formulate the derived objective from (7):\n$U_{opt}, V_{opt} = arg \\underset{UTU=I \\\\ VVT=I}{\\text{max}} \\sum_i ||UTB_iA_iV||_{Fro}^2$\nyielding the expression\n$\\Lambda = -\\frac{1}{2} ||UT B_iA_iV||_{Fro}^2 -  \\frac{1}{2}tr(\\Gamma^T (I - UUT)) -  \\frac{1}{2}tr(\\Upsilon^T (I \u2013 VVT)).$"}, {"title": "Proof of Theorem 1", "content": "Proof. For the lower bound, note that by Jensen's inequality,\n$\\sum^n_{i=1} ||U^TB_iA_iV||_{Fro}^2 \\geq |\\sum_{i=1}^n  U^TB_iA_iV||_{Fro}^2$.\nFor any $U, V$. Hence,\n$\\underset{U,V \\in St(k,d)}{\\text{sup}} \\sum_{i=1}^n ||U^TB_iA_iV||_{Fro}^2  \\geq  \\underset{U,V \\in St(k,d)}{\\text{sup}} |\\sum_{i=1}^n  U^TB_iA_iV||^2_{Fro}$.\nBy the definition of singular value decomposition, the right hand side of (18) is maximized with $U, V$ being the top r singular vectors of $\\sum_{i=1}^n B_i A_i$, yielding $||U^T \\sum_{i=1}^n B_iA_iV||_{Fro}^2 = \\sum_{j=1}^r \\sigma_j^2$.\nRecalling that $\\Sigma_i = U^T B_iA_iV$ yields the lower bound.\nFor the upper bound, recall that $\\Sigma_i = U^T B_iA_iV$. Rearranging,\nvec($\\Sigma_i$) = $(V^T \\otimes U^T)$vec($B_iA_i$).\nDefine\n$\\Sigma :=  [vec(\\Sigma_1), ..., vec(\\Sigma_n)]$.\nBy our previous simplification,\n$\\Sigma = (V^T \\otimes U^T)L$."}, {"title": "Training LoRAs", "content": "We trained LORA adapters on 500 natural instruction tasks [27] using Mistral-7B-Instruct-v0.2 [10] as the base model. All LoRA adapters were configured with a rank of 16, i.e., $\\forall i, r_i = 16$. We selected 10 diverse tasks manually for consistent evaluation across experiments and randomly sampled an additional 490 tasks, resulting in a total of 500 tasks. These tasks were exclusively in English (both input and output), ensuring higher quality and thorough review [27]. Each task dataset was divided into training, validation, and test sets (80-10-10). Hyperparameters, such as early stopping, were tuned using the validation sets; that is, we train for five epochs and take the best-performing epoch-checkpoint per validation loss. Evaluation on the test sets demonstrated that LoRA consistently outperformed the base model in terms of both Rouge scores and loss metrics (see Table 1).\nWe use Huggingface [29] in our implementation. For the base model, we use quantization with configuration:\nBitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_use_double_quant=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.bfloat16,\n)\nand LORA configuration:\nLoraConfig(\nr=16,\nlora_alpha=32,"}, {"title": "Avoiding Batched Matrix Multiplication (BMM)", "content": "In the envisioned deployment scenario", "n)$": "n$Ax \\leftrightarrow (b \\times r \\times n) \\times (b \\times 1 \\times n) \\rightarrow (b \\times 1 \\times r) \\text{ bmm"}, "n$B(Ax) \\leftrightarrow (b \\times m \\times r) \\times (b \\times 1 \\times r) \\rightarrow (b \\times 1 \\times m) \\text{ bmm}.$\nHere, \"bmm\" denotes batched matrix multiplication, a known bottleneck in both throughput and latency. Consider the corresponding operations for our joint compression scheme, $U\\Sigma V^Tx$:\n$V^"]}