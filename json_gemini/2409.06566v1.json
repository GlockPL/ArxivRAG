{"title": "Indirect Dynamic Negotiation in the Nash Demand Game", "authors": ["Tatiana V. Guy", "Jitka Homolov\u00e1", "Aleksej Gaj"], "abstract": "The paper addresses a problem of sequential bilateral bargaining with incomplete information. We proposed a decision model that helps agents to successfully bargain by performing indirect negotiation and learning the opponent's model. Methodologically the paper casts heuristically-motivated bargaining of a self-interested independent player into a framework of Bayesian learning and Markov decision processes. The special form of the reward implicitly motivates the players to negotiate indirectly, via closed-loop interaction. We illustrate the approach by applying our model to the Nash demand game, which is an abstract model of bargaining. The results indicate that the established negotiation: i) leads to coordinating players' actions; ii) results in maximising success rate of the game and iii) brings more individual profit to the players.", "sections": [{"title": "I. INTRODUCTION", "content": "POLITICS and business are considered traditional spheres of human negotiation. The internet and modern means of communication have extended human negotiation to new domains such as social networks, deliberative democracy, e-commerce, cloud-based applications, [1], [2]. Besides, automatic bargaining and negotiation, being inevitable in modern cyber-physical-social systems [3], have been established in variety of applications, like network negotiation, energy trading [4] and traffic management [5], multi-robot systems [6], manufacturing service allocation [7] and newly in ransomware negotiation [8]. While solving negotiation task, agents must take into account incomplete information and strategically interact with other, human or artificial, agents. Majority of the existing research however assumes negotiation with non-human agents.\nHere we consider the simplest bilateral bargaining scenario with incomplete information often found in e-commerce [9]. A typical example is two self-interested agents (say, a buyer and a seller) bargaining on some goods or service. As soon as their price preferences differ, agents begin negotiations to achieve a mutually acceptable price. Either agent strives to satisfy own preferences as much as possible, but also has to take into account the opponent's preferences. Otherwise it is unlikely that an agreement can be reached\u00b9. Additional aspects of real-life bilateral bargaining to be considered are: i) multi-attribute negotiation when agents need to agree on goods/service characterised by several, possibly interrelated, attributes (say price of a product and terms of its delivery); ii) limited negotiation time as no agent can deliberate infinitely; iii) absence of moderator to coordinate the negotiation, so the agents must reach agreement themselves [11].\nThe negotiation has been widely addressed in diverse fields ranging from economy and sociology to computer science. An amount of works is much too large to survey them here. One can distinguish several main frameworks: game theoretic approach, negotiation protocols approach, evolutionary approach. Existing works however have different limitations preventing them from wide use. Game theoretic approach [12], [11], assumes that agents are perfectly rational and have common knowledge. Negotiation protocols approach, [13], needs the clear rules for negotiation, [14], and the results largely depend on the information available to the agents about each other. Evolutionary approach, [15], being inspired by biological evolution, finds optimal negotiation via trial-error and agents should have access to policy of their opponents and their profits. Some approaches are based on an agent-coordinator responsible for assigning goods or\n\u00b9 for details on modelling bargaining, see for instance [10]."}, {"title": "II. PRELIMINARIES", "content": "This section introduces and recalls necessary notions."}, {"title": "A. GENERAL CONVENTIONS", "content": "$\\, \\,\\,$set of natural numbers, set of real numbers\n$X_t\u2208 X\\,$value x from finite set X at discrete time t\n$p(x)\\,$probability mass function of discrete random variable x\n$p(x|y)\\,$probability mass function of x conditioned on y\n$E[x|y]\\,$the expectation of x conditioned on y\nNote that no notational distinction is made between a random variable and its realisation."}, {"title": "B. MARKOV DECISION PROCESS", "content": "We model player's decision making in the NDG via Markov Decision Process (MDP) framework [47]. MDPs were first introduced and developed in the operations research and economics [48]. Since that MDP framework has been widely used to describe and solve decision-theoretic problems. MDP allows to capture the underlying stochastics omnipresent in application domain and also allows to respect multiple DM criteria. Typical examples of using MDP framework include medical applications [49], predictive maintenance [50], power systems [51], more examples see [52].\nThe overall scenario is as follows. An player interacts with the environment by taking actions to achieve its\u00b2 DM goal. The player is motivated by a reward it receives after each action taken. A finite state and action MDP is considered.\nDefinition 1 (MDP): The fully observable MDP is characterised by {T, S, A, p, R}, where $T = \\{1, 2, ..., N\\}, N \u2208 N$, is a set of decision epochs; S is a finite set of all possible environment states and A denotes a finite set of all actions available to the player. Function $p : S \u00d7 S \u00d7 A \u2192 [0, 1]$ is the transition model $p(s_{t+1}|s_t, a_t)$ that moves the environment from state $s_t \u2208 S$ to state $s_{t+1} \u2208 S$ after the agent took action $a_t \u2208 A$; $R: S\u00d7S\u00d7A \u2192 R$ is a real-valued function representing the player's reward $R(s_{t+1}, s_t, a_t)$ after taking action $a_t \u2208 A$ in state $s_t \u2208 S$.\nThe transition model captures environment dynamics and is represented by a family of probability distributions $p(s_{t+1} |s_t, a_t)$, each denotes the probability that at time t + 1 the environment will move from $s_t$ to $s_{t+1}$ when action $a_t$ is executed. The state transitions obey Markov property: the distribution over states at time t + 1 is independent of any previous state $s_{t\u2212j}$ and action $a_{t\u2212j}, j \u2264 1$ for fixed $s_t$ and $a_t$.\nThe player's preferences are described by a reward function, R. The aim of the player is to choose a sequence of actions in order to maximise the total expected sum of rewards as described in the following section.\n\u00b2\"It\" is used as the generic pronoun. A device or an algorithm can be considered as the agent."}, {"title": "C. OPTIMAL DECISION POLICY", "content": "The player chooses action $a_t \u2208 A$ based on the randomised DM rule $p(a_t|s_t) : S \u2192 A$ in each decision epoch $t \u2208 T$.\nA sequence of DM rules forms DM policy \u03c0t,h at time t over decision horizon $h \u2208 N, s_t \u2208 S, \u03b1_t \u2208 A$:\n$\u03c0_{t,h} = \\underset{\u03b1_t \u2208 A}{\\text{arg max}} \\underset{\\{\u03b1_\u03c4\\}_{\u03c4=t}^{t+h-1}}{\\Sigma} \\underset{\\{s_\u03c4\\}_{\u03c4=t}^{t+h-1}}{\\Sigma} \\underset{\u03c4=t}{\\Pi}^{t+h-1} p(s_{\u03c4+1}|\u03b1_\u03c4) \\, \u2200s_t \u2208 S$.    (1)\nMDP with finite horizon h evaluates the quality of DM policy by expected total reward defined as follows:\n$E \\left[\\sum_{\\tau=t}^{t+h-1} R(s_{\\tau+1}, s_{\\tau}, a_{\\tau}) | s_t \\right] = \\sum_{\\tau=t}^{t+h-1} \\sum_{s_{\\tau+1} \\in S} \\sum_{s_{\\tau} \\in S} \\sum_{a_{\\tau} \\in A} \\left( R(s_{\\tau+1}, s_{\\tau}, a_{\\tau}) \\, p(s_{\\tau+1}, s_{\\tau}, a_{\\tau} | s_t) \\right)$,     (2)\nwhere $p(s_{\\tau+1}, s_{\\tau}, a_{\\tau} | s_t) = p(s_{\\tau+1} | s_{\\tau}, a_{\\tau}) p(a_{\\tau} | s_{\\tau}) p(s_{\\tau} | s_t)$.\nThe solution to MDP [47] is a sequence of DM rules, $\\{\\rho^{opt}(a_t | s_t) \\}_{\u03c4=t}^{t+h-1}$, that maximises the expected reward (2) and forms the optimal decision policy:\n$\\left\\{\\rho^{opt}_{t, h}\\right\\} =  arg \\max_{\\{\\pi_{\\tau, h}\\}} E \\left[ \\sum_{\\tau=t}^{t+h-1} R(s_{\\tau+1}, s_{\\tau}, a_{\\tau}) | s_t \\right\\}$,    (3)\nwhere \u03c0 is a set of all possible DM policies, see (1). The optimal policy (3) is computed by dynamic programming algorithm [48], [53], which requires knowledge of transition model $p(s_{\u03c4+1} | s_\u03c4, a_\u03c4)$."}, {"title": "D. LEARNING TRANSITION MODEL", "content": "In bilateral bargaining, the transition model is a model of the opponent, that is, it predicts the opponent's reaction to the player's action. Generally it describes the dynamics of the opponent's decision making. In real-life tasks, opponent model $P(s_{t+1} | s_t, a_t)$ is usually unknown to the player\u00b3. It reflects the player's knowledge about the behaviour of the opponent. Without lost of generality the model can be assumed time-invariant, i.e. $p(s_{t+1} | s_t, a_t) = p(s_t | s_{t-1}, a_{t-1})$ and can be learned from the observed data.\nTo simplify the presentation, let us drop out the time index and introduce the following temporary notations: $s' = s_{t+1}, s = s_t$ and $a = a_t$. The transition model then can be written $p(s'|s, a)^4$.\nWe consider a parametrised form of the opponent's model with time-invariant parameter $\u03b8 \u2208 \u0398$\n$p(s'|s, a, \u03b8) = \u03b8_{s'sa}, \u03b8_{s'sa} \u2208 \u0398$,    (4)\nwhere \u0398 is a set of all possible \u03b8's and $0 \u2264 \u03b8_{s'sa} < 1$, $\\sum_{s' \u2208 S} \u03b8_{s'sa} = 1, \u2200(s, a) | s \u2208 S and a \u2208 A$. Thus, parameter \u03b8 in (4) is an array defining transition probabilities $\u03b8_{s'sa}$ that opponent's state in the next time will equal s' whenever the\n\u00b3it can be partially known or incorrectly specified.\n\u2074The new notation is valid within Section II-D only.\nprevious state is s and the player takes action a. Our aim is to learn parameter \u03b8, (4).\nLet the player have belief b(\u03b8) about the opponent's dynamics expressed via the probability density function of the parameter \u03b8. While interacting with the opponent, the player updates belief about the parameter, b(\u03b8), to a new value, b'(\u03b8), given observed transition (s', s, a) as follows, see [54]:\n$b'(\u03b8) \u221d b(\u03b8)p(s'|s, a, \u03b8) = b(\u03b8)\u03b8_{s'sa}$.   (5)\nChoosing belief b(\u03b8) in conjugate form of Dirichlet distribution implies that the posterior (5) induced by Bayes' rule [54] is\n$Dir(\u03bd, \u03b8) \u221d \\prod_{s' \u2208 S} \u03b8_{s' sa}^{\u03bd_{s'sa}-1}$.    (6)\nIn (6) concentration parameter \u03bd > 0 is an array containing occurrences $\u03bd_{s'sa} > 0$ of triples (s', s, a). Each observation of a triplet (s', s, a) increases the corresponding entry, $\u03bd_{s' sa}$, by one.\nTherefore, after n \u2208 N observations $\\{(s', s, a)\\}_{n \u2208 N}$, update visa contains the actual occurrences of (s', s, a). Recalling (4), the expectation of (6) can be interpreted as Bayesian estimate of unknown parameter @ based on the observed data (i.e. transitions occurred):\n$E[p(s'|s, a, \u03b8) | \u03bd'] = E[\u03b8_{s' sa}|\u03bd'] = \\frac{\u03bd'_{s' sa}}{\\sum_{s''sa} \u03bd'_{s'' sa}}$.   (7)\nRecursive implementation of the prior statistics update is described in [55].\nA real-life dynamic decision making requires an efficient and feasible learning that can be performed online. Markov models belong to the exponential family for which exact estimation is feasible. The estimation and prediction within this family is very simple, especially with the conjugate prior in the form of Dirichlet distribution. The needed update of functions (probability density functions, see (5)) is given by the algebraic recursive update of the finite dimensional sufficient statistics. This clarifies applicability of this learning in combination with decision making."}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. MDP FORMALISATION OF NASH DEMAND GAME", "content": "The considered repetitive scenario of the game is as follows. Two structurally identical players A and B are bargaining on splitting an amount of money $q \u2208 N$. The roles of both players are the same. In each round, two stages are present: an action stage and a reward stage. During action stage, each player decides how much to claim from the total available amount. The players do not communicate and their interests can be competitive. At reward stage, the players announce their demanded shares, observe the demands of their opponents and reward is allocated. Note that in action stage each player has no information about their opponent's demand or preferences. The game runs for a fixed and known number of periods.\nLet $q \u2208 N$ is a total amount to split. At the beginning of round $t \u2208 T$, each player $k \u2208 \\{A,B\\}$ chooses action $a_t^k \u2208$"}, {"title": "B. HEURISTIC MODEL OF OPPONENT", "content": "The proposed approach formalised and solved bilateral dynamic bargaining of learning self-interested player within MDP framework (Section II-C). To verify the approach we propose a probabilistic bargaining model for non-learning and non-optimising opponent. The model is based on the reported experimental evidence obtained with human-players, see [56], [28]. For simplicity here we consider player B is serving as an opponent to A.\nHeuristic behaviour of B reflects the dependence of its future demand on the results of the previous round. Once the previous round demands are incompatible, that is $a_{t-1}^A + a_{t-1}^B > q$, player B tends to decrease next demand. If there are unclaimed money left in the previous round, B, on the contrary, increases the next demand. The proportion (speed) of demands' increase/decrease may depend on personal traits (i.e. reflect the personality of B).\nThe remainder of this section introduces model that reflects the behaviour of an opposing player, B.\n1) B had Low Demand in the Previous Round\nConsider the previous demand of player B is low, i.e. less than the fair split would have been, $a_{t-1}^B \u2264 \\frac{q}{2}$. The next de-"}, {"title": "C. PRIOR MODELS USED IN LEARNING", "content": "Our approach considers decision making of the player in question, A, who models behaviour of the opponent, B, and optimises own demand in order to maximise the accumulated profit. The ability to accurately predict the opponent's behaviour significantly affects the success of A's decision making, (12). To learn a model of the opponent, A follows the approach described in Section II-D. It exploits knowledge available in the form of a parameter prior that quantifies A's belief about dynamics of the opponent, B. Following Bayesian paradigm this prior will be gradually updated with new data accumulated, see Section II-D and [55]. The choice of prior model is important, especially when a number of"}, {"title": "IV. SIMULATED EXPERIMENTS", "content": "The proposed approach is illustrated with the Nash demand game, described in Section III-A, using simulated examples5.\nWe selected the most representative experiments from a much wider set of the experiments differing in the number of rounds and horizons. The selected experiments are long enough to perform learning (because very short runs will not be sufficient to learn the models used), while longer runs will add no significant information about the results.\nGoal of the experiments\nThe goal was to analyse the impact of the proposed distributed solution and indirect negotiation and to verify that player employing the proposed DM policy is capable of achieving better results than heuristic player playing the same role. The main objectives of the performed experiments are:\nCommon settings of the experiments\nEach game has 60 rounds and optimisation horizon h\u2208 N equals 10 game rounds. The amount of money that players can split (if they reach an agreement) is q = 10 CZK per round. The reward (10) is evaluated for the optimal policy (3) resulted from the dynamic programming [48]. The initial state of each player $s_1 = (a_t^A, a_t^B)$ is preset to $a_t^A = a_t^B = 3$.\n5The examples were implemented in MATLAB, The MathWorks, Inc."}, {"title": "V. DISCUSSION", "content": "Section IV describes simulation results obtained on the NDG. It can be seen that our DM model can help the players to effectively bargain and counteract the incomplete knowledge.\nThe main advantages of the proposed DM model are as follows:\n\u2022\nThe proposed reward function respects individual eco-"}, {"title": "VI. CONCLUDING REMARKS", "content": "The paper addresses a problem of sequential bilateral bargaining with incomplete information. We proposed DM model that helps agents to successfully bargain by performing indirect negotiation and learning the opponent's model. Methodologically the paper casts heuristically-motivated bargaining of a self-interested independent agent into a framework of Bayesian learning and Markov decision processes. The proof of the main results is based on the standard methodology. However, the problem formulation and the gained solution are novel and practically important. The special form of the reward implicitly motivates the players to negotiate indirectly, via closed-loop interaction. At the"}]}