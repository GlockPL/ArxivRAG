{"title": "LLM-Powered Benchmark Factory: Reliable, Generic, and Efficient", "authors": ["Peiwen Yuan", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Yueqi Zhang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "abstract": "The rapid advancement of large language models (LLMs) has led to a surge in both model supply and application demands. To facilitate effective matching between them, reliable, generic and efficient benchmark generators are widely needed. However, human annotators are constrained by inefficiency, and current LLM benchmark generators not only lack generalizability but also struggle with limited reliability, as they lack a comprehensive evaluation framework for validation and optimization. To fill this gap, we first propose an automated and unbiased evaluation framework, structured around four dimensions and ten criteria. Under this framework, we carefully analyze the advantages and weaknesses of directly prompting LLMs as generic benchmark generators. To enhance the reliability, we introduce a series of methods to address the identified weaknesses and integrate them as BENCHMAKER. Experiments across multiple LLMs and tasks confirm that BENCHMAKER achieves superior or comparable performance to human-annotated benchmarks on all metrics, highlighting its generalizability and reliability. More importantly, it delivers highly consistent evaluation results across 12 LLMs (0.967 Pearson correlation against MMLU-Pro), while taking only $0.005 and 0.38 minutes per sample.", "sections": [{"title": "1 Introduction", "content": "With the ongoing scaling up of large language models (LLMs) in multiple dimensions over the past few years, two key trends have emerged (Figure 1): (1) The LLM release process has accelerated and now exceeds 30k per season; (2) The growth in LLM capabilities has spurred application demand, reflected in over 50M downloads of open-source models per season. Serving as a bridge between massive LLM supply and various application needs, the demand for customized benchmarks is rapidly growing, helping downstream tasks identify the most suitable LLM.\nHowever, current benchmark construction processes largely rely on human-provided signals (Chang et al., 2024; Wang et al., 2024b), leading to long cycles and high costs. To this end, efficient LLM-driven methods have recently been explored. Unfortunately, they generally rely on the existence of seed benchmarks for data augmentation (Zhu et al., 2024b; Wu et al., 2024; Li et al., 2024a; Maheshwari et al., 2024) and task specific designs (Zhu et al., 2024a; Lei et al., 2023), lacking generalization across tasks and domains. Meanwhile, the current absence of a comprehensive evaluation framework hinders the assessment and optimization of benchmark generators, weakening our confidence in their reliability for real applications. Hence, an automatic and comprehensive evaluation framework and a generic and reliable benchmark generator that can handle any assessment demands and efficiently generate high-quality samples are urgently needed.\nTo this end, we first construct an automatic evaluation framework with ten criteria for benchmark"}, {"title": "2 Backgrounds", "content": "In this section, we first review the latest developments in data synthesis \u00a72.1 and then discuss the potential values of developing a generic benchmark generator \u00a72.2."}, {"title": "2.1 Synthetic Data Generation", "content": "The growth of language model abilities has led to widespread research on LLM-driven data synthesis, which demonstrates much better quality and controllability over traditional approaches (Wang et al., 2024a; Long et al., 2024). Centering around the construction of data flywheel (LLM-driven evolution) (Luo et al., 2024a; Tao et al., 2024), training data synthesis has garnered much attention in fields like mathematics (Yu et al., 2024), science (Li et al., 2024b), and code (Luo et al., 2024b), continuously pushing LLMs' capability boundaries. Unlike the training data synthesis aimed at optimizing model performance, the goal of benchmark synthesis is to accurately evaluate models on specific task, presenting greater challenges in both measurement and implementation (Chang et al., 2024). In terms of measurement, recent studies (Zhu et al., 2024a; Maheshwari et al., 2024; Li et al., 2024a) generally focus on specific criteria, without establishing a comprehensive evaluation system for benchmark generators. In terms of implementation, current benchmark generators (Perez et al., 2023; Wu et al., 2024; Zhu et al., 2024b; Lei et al., 2023) are constrained by their dependence on existing benchmarks and task specific designs, preventing them from being generic. We construct a comprehensive evaluation framework and develop generic and reliable BENCHMAKER method to fill this gap."}, {"title": "2.2 Potential Applicable Scenarios of BENCHMAKER", "content": "Given arbitrary assessment demands X as the sole input, a generic benchmark generator (BENCHMAKER) G is expected to generate a well-aligned high-quality benchmark D. On this basis, we summarize its applicable scenarios as follows: (1) Complementing existing benchmarks for tailored assessment demands; (2) Acting as a dynamic benchmark generator to alleviate data contamination issues (Balloccu et al., 2024); (3) Serving as a difficulty controllable benchmark generator to mitigate the benchmark saturation problem (Glazer et al., 2024); (4) Functioning as a versatile training data generator. Therefore, building BENCHMAKER holds significant importance for both scientific research and practical applications within the NLP community."}, {"title": "3 Benchmarking Benchmark Generator", "content": "While training data synthesis focuses on faithfulness, diversity and the final performance of the trained models (Yu et al., 2023; Long et al., 2024), the evaluation of synthetic benchmark should be more comprehensive to ensure the reliability of its benchmarking results. Thus, we carefully establish an evaluation framework for benchmark generator with ten criteria, as illustrated in Table 1."}, {"title": "3.1 Credibility", "content": "Two key criteria for ensuring the credibility of a benchmark are faithfulness and alignment. Faithfulness indicates that the generated sample be free of ambiguity with a correct answer. Alignment requires the generated samples to strictly adhere to the specified assessment demands X, especially in abilities to be assessed. For these criteria, previous approaches rely on human evaluation (Wu et al., 2024; Zhu et al., 2024b) or LLM-as-a-judge (Zheng et al., 2023). However, the former lacks automation, and the latter is susceptible to biases (Thakur et al., 2024).\nTo this end, we seek to detect and mitigate any biases of LLM-as-a-judge that may exist within the framework. We choose Qwen-Plus (Yang et al., 2024) as the judge with scoring range as [0, 1] (See prompt in Appendix I). Experiments are conducted on the high-quality MATH benchmark (Hendrycks et al., 2021b), for which we assign score 1 to both faithfulness and alignment for every sample. Ideally, the scores assigned by the judge should not exhibit any consistency with specific factors. However, as shown in Figure 2-(a), both faithfulness and alignment are significantly correlated (p-value < 0.05) with sample difficulty, sample length, and the length of the judge's rationale. For each factor, we highlight its weightiest path in red, revealing a clear causal chain: harder questions lead to longer samples, requiring judges to conduct lengthier analyses. For faithfulness, longer analyses increases the likelihood of judge errors, resulting in lower faithfulness ratings. While for alignment, longer analyses increases the probability of task-relevant words appearing and results in higher alignment rat-"}, {"title": "3.2 Diversity", "content": "With credibility ensured, the diversity of the benchmark determines the extent to which evaluation results can reflect the true model capability across the assessed domain. Apart from the widely tested lexical and semantic diversity, our framework also examines the knowledge diversity to make the evaluation more comprehensive.\nLexical Diversity reflects vocabulary richness in benchmarks. Traditional metrics like vocabulary size and self-BLEU (Zhu et al., 2018) used in Wu et al. (2024) and (Yu et al., 2023) are biased by sample length (Guo and Vosoughi, 2023). We use unbiased word frequency entropy (Montahaei et al., 2019) as the metric to evaluate lexical diversity.\nSemantic Diversity quantifies a benchmark's semantic comprehensiveness. We calculate the average Euclidean distance between semantic embeddings of samples as the metric. Specifically, we use powerful text-embedding-ada-002 (OpenAI) as the embedding model.\nKnowledge Diversity evaluates whether the samples evaluate different sub-abilities within the assessment demands. When samples test the same sub-ability, the model is likely to exhibit similar"}, {"title": "3.3 Difficulty", "content": "When diversity meets requirements, we should further consider the difficulty attribute, which is particularly significant in an era of increasingly divergent model capabilities.\nDifficulty Controllability refers to assigning differentiated difficulty labels to the samples (e.g., MATH (Hendrycks et al., 2021b)). These labels enable the benchmark to be divided into subsets for more targeted evaluation of models with varying capabilities. For each sample, we use the average error rate of M1:|M| as the ground truth for difficulty label. Based on this, we compute the Spearman correlation between the difficulty labels provided by the benchmark and the ground truth as the metric.\nDifficulty Boundary denotes the difficulty of the hardest subset of a benchmark. With the growing strength of LLMs, their performance on simpler benchmarks has reached saturation (Hendrycks et al., 2021a), making it difficult to differentiate their capabilities. Consequently, more challenging benchmarks (Wang et al., 2024b) are continuously introduced to evaluate the latest LLMs. Thus, we propose assessing the average error rate of M1:|M| on the hardest subset of benchmark to measure its difficulty boundary."}, {"title": "3.4 Benchmark-Level", "content": "Lastly, we introduce high-level metrics for assessing benchmark generators.\nEffectiveness. While the earlier criteria assess benchmark quality from various aspects, a unified metric is required to measure benchmark effectiveness. Taking high-quality human-annotated benchmark as the ground truth, we examine whether generated benchmark under identical assessment demands can deliver equivalent evaluation results. To this end, we calculate the accuracy of M1:|M| on both generated and human benchmarks and use the"}, {"title": "4 Development of BenchMaker", "content": "In this section, we first discuss the primary sample format we studied in \u00a74.1. Afterwards, since previous studies have yet to realize generic benchmark generators (with assessment demands X as the sole input), we analyze the pros and cons of directly prompting the LLM as such generator in \u00a74.2. Building on the experimental results, we refine its weaknesses in the following sections, leading to the development of BENCHMAKER."}, {"title": "4.1 Sample Format Selection", "content": "Following previous studies (Li et al., 2024a; Zhu et al., 2024b), we have chosen multiple-choice questions (MCQs) as the primary sample format for benchmark generation based on the following reasons: (1) Versatility: MCQ serves as a universal format for evaluating most capabilities; (2) Accuracy: Misjudgment can be effectively prevented caused by variations in output formats (Tam et al., 2024); (3) Efficiency: MCQs do not depend on external modules such as LLM-as-a-Judge, ensuring a streamlined evaluation process; (4) Transformability: Each generated sample includes a rationale, enabling easy conversion into other formats, such as the text generation format presented in the Appendix G."}, {"title": "4.2 Pros and Cons of Directly Prompting", "content": "We choose MATH (Hendrycks et al., 2021b), MMLU-Pro (Wang et al., 2024b) and HellaSwag (Zellers et al., 2019) as high-quality benchmarks Dhuman for comparison. We adopt the prompt in Appendix I to guide M: GPT-40 mini in generating credible and diverse samples 81:|Dhuman|:\n$\\begin{equation}S_i = {q_i, r_i, O_i, A_i} = M(prompt_{base}, l, X) (2)\\end{equation}$\nwhere qi, ri, Oi, a\u017c denote question, rationale, options, and label, respectively. We proportionally adjust the difficulty level l from 1 to 10 in the prompt (see descriptions in Appendix D), and select samples with top 20% difficulty level to form the hardest subset. The assessment demands X are shown in Appendix K. As shown in Table 2, compared to Dhuman, directly prompting LLM as generic benchmark generator demonstrates poorer faithfulness, lower lexical and semantic diversity, weaker difficulty controllability, and less challenging subset. Meanwhile, we also observe its advantages in better alignment\u00b9, greater knowledge diversity, and improved efficiency."}, {"title": "4.3 Faithfulness Optimization", "content": "To enhance faithfulness, previous studies have explored methods such as self-correction (Wang et al., 2023b; Ji et al., 2023) and the use of external tools (Li et al., 2024c; Lewis et al., 2020). As self-correction offers greater versatility, we propose the following two BenchMaker-compatible techniques to optimize faithfulness.\nStepwise Self-correction. Since errors might occur at any step during the generation of {qi, ri, Oi, ai}, we instruct the model to validate the content at each step. If an error is detected, the model will return to the beginning. Compared to full-sample self-checking, step-wise critique boosts error detection with less decoding cost (See Appendix B).\nConflict Guided Contrastive Discrimination. Huang et al. (2024) finds that LLMs struggle to correctly judge their prior answers on challenging questions. Therefore, we extend Stepwise Self-correction by having the LLM not only act as a judge but also as a test-taker to identify potential errors. Let the LLM answers qi T times to attain \u0101\u00a1T,"}, {"title": "4.4 Difficulty Optimization", "content": "Difficulty Controllability. From \u00a74.2, we know that the LLM's ability to control the difficulty of generated samples is limited. In particular, for the language understanding task (MMLU-Pro), the Spearman correlation between the actual and expected difficulty of the samples is only 0.021. To further explore this, we examine LLM's difficulty perception by asking it to score the difficulty label of the generated samples. However, the correlation only increases to 0.089, suggesting that while LLM has some capacity to perceive difficulty, it is still weak. We then switch the role of LLM and assess the difficulty from the perspective of test-taker:\n$\\begin{equation}\\beta_i = \\frac{1}{T} \\sum_{j=1}^T |\\bar{a}_j - a_i| (3)\\end{equation}$\nBy taking the inconsistency between \u0101T and ai as difficulty label, the correlation increases to 0.415, suggesting that \u03b2 is a reliable metric for difficulty controllability.\nDifficulty Diffusion Mechanism. Given that the LLM has a certain level of difficulty perception, we iteratively select the more challenging samples according to \u03b2 from the generated ones as difficulty references, and instruct the LLM to generate a more difficult sample. This allows the sample difficulty to rise continuously through diffusion. The detailed algorithm is described in Appendix F.\nDifficulty Strategy Guidance. We further consider providing the LLM with task-specific"}, {"title": "4.5 Diversity Optimization", "content": "The optimization of synthetic data diversity has been widely studied (Wang et al., 2024a). We conduct extensive tests and select the most generic and effective AttrPrompt (Yu et al., 2023) technique for BENCHMAKER. AttrPrompt explicitly enhances the lexical and semantic diversity of benchmarks by randomly assigning pre-generated (attribute, value) pairs as part of the input for each sample. Furthermore, we notice that the introduction of treating the generated samples as difficulty references might cause sample homogeneity. To mitigate this, we propose an In-batch Diversity Boosting method, where LLM generates L (We set L as 5 for our default setting) candidate samples and selects the one with the greatest word frequency entropy difference from the input reference samples."}, {"title": "5 Experiments and Analyses", "content": "We conduct comprehensive experiments to validate BENCHMAKER under the proposed framework in this section.\nSettings. We select the widely used human-annotated MATH\u00b2 (Hendrycks et al., 2021b) (mathematical reasoning), MMLU-Pro (multi-task language understanding) (Wang et al., 2024b) and Hel-"}, {"title": "5.1 Comparison with Human-annotated Benchmark", "content": "As shown in Table 2, overall, BENCHMAKER achieves comparable performance to human-annotated benchmarks in terms of faithfulness and lexical&semantic diversity. Meanwhile, BENCHMAKER outperforms them in all other metrics, es-"}, {"title": "5.2 Ablation Studies", "content": "We validate the effectiveness of different techniques by sequentially integrating them to the Direct Prompt baseline on the MATH benchmark, as shown in Table 2.\nDiversity. Compared to Direct Prompt, both Attr-Prompt and In-batch Diversity Boosting effectively enhance lexical and semantic diversity. Notice-ably, we observe that knowledge diversity remains unchanged, indicating that surface-level diversification does not necessarily equate to a broader assessment of knowledge and skills. Meanwhile, the diversity improvement leads to a slight drop in faithfulness, possibly because of the attributes constraints.\nFaithfulness. After applying Stepwise Self-correction and Conflict Guided Contrastive Discrimination, we observe a sustained and significant improvement in faithfulness. At the same time, we notice a reduction in the difficulty of the hardest subset, with the error rate decreasing from 0.659 to 0.557. We hypothesize that this may be due to the high error rate in labels when faithfulness is not ensured, which leads to an underestimation of model performance. Consequently, once the labels are corrected, the accuracy can better reflect the actual difficulty of the benchmark.\nDifficulty Controllability. By treating the generator as the test-taker and using its error rate as the difficulty label, we achieve more precise control over sample difficulty (Spearman correlation of 0.403). Considering the previously observed weak difficulty perception of LLMs, we hypothesize that this improvement stems from the role shift, which requires the model to engage in explicit reasoning, along with the adoption of prediction-label inconsistency as an objective metric."}, {"title": "Difficulty Boundary", "content": "With our proposed Difficulty Diffusion Mechanism and Difficulty Strategy Guidance, the difficulty boundary is significantly extended, as evidenced by an increase in error rate from 0.515 to 0.768, validating their effectiveness. Additionally, we analyze how the actual difficulty and difficulty labels evolve with the order of generated samples. As illustrated in Figure 4, both the difficulty label and actual difficulty exhibit a continuous upward trend. This not only confirms that Difficulty Diffusion Mechanism operates as intended but also visually demonstrates the strong consistency between difficulty label and actual difficulty."}, {"title": "5.3 A Closer Look at the Generated Benchmark", "content": "After metric analysis, we perform a more thorough examination of BENCHMAKER. Some of the gen-"}, {"title": "5.4 Reliability Estimation", "content": "Since faithfulness of the generated benchmark cannot be totally ensured, we are curious about the effects of incorrect samples: Let a and b be the observed accuracies of two models A and B on the generated benchmark of size N, where a fraction K of the samples are incorrect (which can be estimated by the LLM-as-a-judge). Suppose that \u0101 > b, we aim to estimate the probability that the observed ability rank is correct (the true accuracies satisfy E[a] > E[b]). See detailed derivation in Appendix A. Suppose A and B have the same accuracy p on incorrect samples, we get:\n$\\begin{equation}E[a] = \\frac{\\bar{a} - K \\cdot p}{1 - K} (4)\\end{equation}$\nand\n$\\begin{equation}E[b] = \\frac{\\bar{b} - K \\cdot p}{1 - K} (5)\\end{equation}$\n$\\begin{equation}E[a] - E[b] = \\frac{\\bar{a} - \\bar{b}}{1 - K}\\end{equation}$\nNext, we perform hypothesis testing to assess the probability of E[a] > E[b]. We assume that \u0101 b follows a normal distribution. The z-score for the difference is:\n$\\begin{equation}z = \\frac{(\\bar{a} - \\bar{b}) / (1 - K)}{\\sqrt{(\\bar{a} (1 - \\bar{a}) + \\bar{b} (1 - \\bar{b})) / (N (1 - K)^2)}} = \\frac{(\\bar{a} - \\bar{b}) \\sqrt{N}}{\\sqrt{\\bar{a}(1 - \\bar{a}) + \\bar{b}(1 - \\bar{b})}} (6)\\end{equation}$\nwhere \u03a6(z) is the cumulative distribution function of the standard normal distribution. The probability P(E[a] > E[b]) is given by the right-tail probability of the normal distribution:\n$\\begin{equation}P(E[a] > E[b]) = 1 - \\Phi(z) (7)\\end{equation}$\nwhere \u03a6(z) is the cumulative distribution function of the standard normal distribution. We can assess the reliability of BENCHMAKER evaluation results using (7). Also, we notice that K has the same scaling effect on both the numerator and denominator of the test statistic, thus does not alter the z-score. Consequently, as long as there is no bias, a certain proportion of noise in the benchmark will not affect the statistical significance of ability ranking."}, {"title": "Conclusions", "content": "The rapid advancement of large language models has driven an urgent demand for a generic benchmark generator. To this end, we first propose a comprehensive, automated, and unbiased evaluation framework to validate and optimize the reliability of benchmark generators. Based on this, we develop the BENCHMAKER method for reliable, generic, and efficient benchmark generation. Comprehensive experiments across multiple tasks and LLMs demonstrate that BENCHMAKER achieves human-aligned benchmark quality, with superior efficiency and generalization."}, {"title": "Impact Statement", "content": "This paper presents BENCHMAKER, an LLM-driven reliable, generic and efficient benchmark generator. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "Ethics Statement", "content": "All of the datasets used in this study were publicly available. Multiple authors jointly conducted the manual check for the Actual Error Rate section, and no extra annotators were employed for our data collection. We confirm that the datasets we used did not contain any harmful"}, {"title": "B Unsuccessful Attempts for Optimizing Benchmark Generator", "content": "B.1 Faithfulness\nWe explored the widely studied self-correction strategy to improve the faithfulness of benchmarks. Specifically, for each generated sample, the model first acts as a judge and then refines samples it deems insufficiently faithful. However, our preliminary results indicate that while this approach yields minor improvements in mathematical tasks, it provides little benefit for tasks such as MMLU-Pro and instead introduces additional computational overhead.\nB.2 Difficulty Controllability\nAs previously mentioned, we attempted to have the model generate samples with specified difficulty levels, but the resulting samples exhibited low difficulty differentiation. To address this, we further explored having the model assess the difficulty of its generated samples. However, this strategy yielded promising results only on the MATH task.\nB.3 Difficulty Diffusion Mechanism\nPrevious studies (Wang et al., 2024b) have attempted to increase question difficulty by expanding the number of answer choices. However, our experiments show that scaling up the number of candidates quickly reaches a saturation point. We hypothesize that this is due to the model's difficulty in generating a large number of sufficiently deceptive distractors.\nB.4 Diversity\nTo enhance sample diversity, in addition to AttrPrompt, we experimented with assigning different personas (Chan et al., 2024) to the model and instructing it to generate characteristic samples based on its assigned persona. However, we found that this approach was not particularly effective for the MATH task, especially in semantic diversity."}, {"title": "C Data from Huggingface", "content": "We obtained information on open-source model releases and download counts from the Hugging Face API (from huggingface_hub import HfApi). Since the number of open-source model releases far exceeds that of closed-source models, we use the former to represent the \"Number of Language Model Releases.\" Additionally, as Hugging Face does not provide monthly download counts for each model, we use the historical total downloads of models released within a given statistical period as the total downloads for that period. The corresponding code is shown below."}, {"title": "D Difficulty Levels", "content": "\u2022 Level 1: The simplest, equivalent to lower-grade elementary school\n\u2022 Level 2: Relatively simple, equivalent to upper-grade elementary school\n\u2022 Level 3: Simple, equivalent to middle school\n\u2022 Level 4: Average, equivalent to high school\n\u2022 Level 5: Slightly difficult, equivalent to university student\n\u2022 Level 6: Difficult, equivalent to Master's\n\u2022 Level 7: Quite difficult, equivalent to PhD student\n\u2022 Level 8: Very difficult, equivalent to professor\n\u2022 Level 9: Extremely difficult, equivalent to field expert\n\u2022 Level 10: Most difficult, equivalent to top human level or beyond human level"}, {"title": "E Benchmarking Model List", "content": "\u2022 phoenix-inst-chat-7b: https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b\n\u2022 vicuna-7b-v1.3: https://huggingface.co/lmsys/vicuna-7b-v1.3\n\u2022 Qwen2.5-3B:https://huggingface.co/Qwen/Qwen2.5-3B\n\u2022 phi-2: https://huggingface.co/microsoft/phi-2\n\u2022 Phi-3.5-mini-instruct: https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n\u2022 Yi-1.5-6B-Chat: https://huggingface.co/01-ai/Yi-1.5-6B-Chat\n\u2022 Qwen2.5-7B:https://huggingface.co/Qwen/Qwen2.5-7B\n\u2022 vicuna-7b-v1.5: https://huggingface.co/lmsys/vicuna-7b-v1.5\n\u2022 Qwen2-1.5B-Instruct: https://huggingface.co/Qwen/Qwen2-1.5B-Instruct\n\u2022 phoenix-inst-chat-7b-v1.1: https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b-v1.1\n\u2022 Qwen-Plus: https://huggingface.co/Qwen\n\u2022 GPT-3.5 turbo: https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-update:"}, {"title": "F Details of Difficulty Diffusion Mechanism", "content": "Given that the LLM has a certain level of difficulty perception, we iteratively select the more challenging samples according to \u03b2 from the generated ones as difficulty references, and instruct the LLM to generate a more difficult sample. Specifically, To prevent reference samples from becoming overly fixed, which may lead to homogenization in generated samples, we adopt the following strategy:\n1. We track the number of times each sample xi has been used as a reference sample, denoted as ti, and compute a calibrated difficulty label:\n$\\begin{equation}\\text{Calibrate\\_Difficulty} = \\text{Difficulty\\_Label} \\times 0.9^{t_i/\\text{Reference\\_Number}} (10)\\end{equation}$\nThe samples are then sorted based on this adjusted difficulty.\n2. Each time, we select 2 \u00d7 Reference_Number samples with the highest Calibrate_Difficulty as candidates. From this pool, we randomly sample Reference_Number as reference samples and shuffle their order.\nOur preliminary experiments indicate a positive correlation between problem difficulty and Reference_Number. In our experiments, we set Reference_Number to 8. This allows the sample difficulty to rise continuously through diffusion."}, {"title": "G Converting Benchmark Sample Format", "content": "MCQ to OTG Format. By removing the options from the samples and using the solution and answer corresponding to the correct option as the ground truth, we can easily transform the MCQ-style benchmark generated based on MATH assessment demands into an open-ended text generation (OTG) benchmark. Comparing these two benchmark formats, we find that the OTG format makes the questions more challenging (error rate: 0.865 > 0.768) and results in lower knowledge diversity (hamming distance: 0.365 < 0.403). We attribute this to the model's inability to rely on option cues to answer certain questions, which leads to a large portion of the knowledge vector being zero, thereby reducing knowledge diversity. Additionally, we observe a decline in the benchmark's effectiveness (pearson: 0.915 < 0.935), which we hypothesize is indirectly caused by the drop in knowledge diversity."}]}