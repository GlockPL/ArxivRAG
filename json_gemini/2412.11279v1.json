{"title": "VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping", "authors": ["Hao Shao", "Shulun Wang", "Yang Zhou", "Guanglu Song", "Dailan He", "Shuo Qin", "Zhuofan Zong", "Bingqi Ma", "Yu Liu", "Hongsheng Li"], "abstract": "Video face swapping is becoming increasingly popular across various applications, yet existing methods primarily focus on static images and struggle with video face swapping because of temporal consistency and complex scenarios. In this paper, we present the first diffusion-based framework specifically designed for video face swapping. Our approach introduces a novel image-video hybrid training framework that leverages both abundant static image data and temporal video sequences, addressing the inherent limitations of video-only training. The framework incorporates a specially designed diffusion model coupled with a Vid-FaceVAE that effectively processes both types of data to better maintain temporal coherence of the generated videos. To further disentangle identity and pose features, we construct the Attribute-Identity Disentanglement Triplet (AIDT) Dataset, where each triplet has three face images, with two images sharing the same pose and two sharing the same identity. Enhanced with a comprehensive occlusion augmentation, this dataset also improves robustness against occlusions. Additionally, we integrate 3D reconstruction techniques as input conditioning to our network for handling large pose variations. Extensive experiments demonstrate that our framework achieves superior performance in identity preservation, temporal consistency, and visual quality compared to existing methods, while requiring fewer inference steps. Our approach effectively mitigates key challenges in video face swapping, including temporal flickering, identity preservation, and robustness to occlusions and pose variations. For more information, please refer to the project page.", "sections": [{"title": "1. Introduction", "content": "In recent years, face swapping has emerged as a crucial technology across various domains, from content cre-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Face Swapping", "content": "The frameworks of face swapping are generally categorized into three types: 3D-based, GAN-based, and diffusion-based methods. 3D-based frameworks typically employ the parameterized 3DMM model to reconstruct the swapped face. Face2Face transferred expressions from source to target face by fitting a 3DMM face model to both faces. The authors in show that face swapping with robust"}, {"title": "2.2. Diffusion Models", "content": "Diffusion models have recently emerged as a powerful generative framework, achieving state-of-the-art performance in various domains, including image synthesis, editing, super-resolution, and video generation. Unlike GANs, which often suffer training instability, diffusion models offer a more stable training process by gradually denoising data from random noise, resulting in high-fidelity outputs. Notable advancements include Stable Diffusion, which enhances efficiency by operating in the latent space, and SVD, which incorporates temporal modules to scale diffusion models for video tasks. Conditioning mechanisms,"}, {"title": "3. Method", "content": "In this section, we will introduce our method VividFace, the first diffusion model based video face swapping framework in detail. An overview of our framework is shown in Fig. 2."}, {"title": "3.1. Preliminaries", "content": "Our method employs Stable Diffusion (SD) as the backbone network. Stable Diffusion is a text-to-image model built on the Latent Diffusion Model (LDM), which enables efficient image generation by operating within a compressed latent space. SD uses a variational autoencoder (VAE) to map the original image xo unto a latent representation z0. reducing computational cost while preserving visual quality. The image is encoded as z0 = E(xo) and decoded back as xo = D(z0). SD follows the Denoising Diffusion Probabilistic Model (DDPM) framework, introducing Gaussian noise \u03f5 to the latent z0 across timesteps t, generating a noisy latent zt over a series of steps. During inference, the model denoises z\u0142 back to z0, guided by condition features. The denoising backbone \u03f5\u03b8, based on a U-Net, is trained to predict the noise and remove it progressively, using the objective:\nL = Et,c,zt,\u03f5 [||\u03f5 \u2013 \u03f5\u03b8(zt, t, c) ||2],\nwhere c represents text features derived from a CLIP encoder. SD uses a U-Net with cross-attention mechanisms, to fuse text embeddings with latent features, enabling fine control over generated images based on text prompts. This allows SD to generate detailed, high-fidelity images while responding effectively to user input."}, {"title": "3.2. Hybrid Face Swapping Framework", "content": "Video Face Swapping Task. The goal of video face swapping is to transfer the identity of a source face onto a target video while preserving the target's pose, expression, lighting, and background. Following the approach of Diff-Swap, we model the video face swapping task as conditional inpainting. This involves masking the face region in the target frame and injecting conditioning vectors that represent the identity of the source face and the attributes of the target face to guide the generation process. Recent diffusion-based methods have primarily focused on static image face swapping. However, directly applying these methods to video sequences by decomposing the video into multiple static images introduces new problems, such as temporal distortions, flickering, occlusions, and issues with large pose variations."}, {"title": "3.3. Designs of Condition Vectors", "content": "In our framework, several carefully designed condition vectors are used to guide the generation process, ensuring accurate and consistent visual outputs for both static images and video sequences. We formulate video face swapping as a conditional inpainting task, where masked videos with cropped face regions provide the background and lighting conditions. The corresponding face regions guide the diffusion model on where are generated the faces.\nIn many in-the-wild videos, faces often exhibit significant pose variations, which can lead diffusion models to produce suboptimal results, such as facial distortions and inaccurate pose estimations. To address this issue, we propose using a 3D reconstruction technique to reconstruct the face and use its output as local guidance for pose and expression details. Specifically, we employ 3DMM to extract BFM coefficients, setting the texture component to zero to reduce information leakage. Replacing the reconstructed face with the original face would introduce even more pronounced information leakage. Since the ground truth face is identical to the input target face, excessive information leakage would degrade the model's generalization capabilities. To ensure that the generated face maintains the same identity as the source face while preserving attributes (such as pose, expression, etc.,), we inject cross-attention features C extracted by our face encoder as global context to the diffusion model.\nFace Encoder. The face encoder module in our framework plays a critical role in extracting and integrating features from the target and source faces to guide the face-swapping process effectively. As illustrated in the right part of Figure 2, the face encoder is composed of three primary networks, each responsible for capturing distinct aspects of facial information: (1) identity net: this network focuses on extracting the core identity features from the target face; (2) texture net: this network is designed to capture detailed tex-"}, {"title": "3.4. Training Strategies", "content": "Our training process involves three stages to progressively enhance model performance for video face swapping. The first stage focuses on training the VidFaceVAE, where we apply reconstruction, perceptual, and KL divergence losses to ensure high-quality reconstruction and a well-structured latent space. The training data primarily consists of facial images and videos. Given the specifically designed architecture, the spatial modules are initialized using the original 2D VAE. In subsequent stages, the VAE is frozen and no longer updated. In the second stage, we pretrain the model using image data, while the ReferenceNet and temporal modules of the backbone network remain inactive. The backbone is initialized from the original SD weights. The final stage introduces image-video hybrid training, which incorporates temporal coherence by activating the temporal modules and utilizing video data. The temporal modules are initialized from AnimateDiff, enabling smooth frame transitions and reducing flicker artifacts."}, {"title": "4. AIDT Dataset", "content": "In this section, we describe the construction of triplet pairs for our AIDT (Attribute-Identity Disentanglement Triplet) dataset, as illustrated in Figure 5. For image data, we first cluster the facial images based on identity similarity. From each cluster, we randomly select two images to form a target-source pair that shares the same identity but has different attributes. To generate the decoupling image, which has a different identity but the same attribute, we use the InsightFace Swapper to create synthetic images with a distinct identity, while preserving the gender of the original face. We have observed that when the original and swapped faces belong to different genders, the results tend to degrade. Additionally, we exclude triplets with significant facial expression discrepancies by comparing the face landmarks. For video data, the process is similar, except that both the source and target images come from the same video clip, but not from the same frames as the target or motion images, which reduces the pose variation. Since video data is less abundant than image data, clustering does not yield enough pairs to form a sufficient number of triplets.\nThe AIDT dataset enables the face encoder to disentangle and fuse distinct facial components-ID features, texture features from the source face, and attribute features from the decoupling face. This enhances generalization, especially when the source and target faces belong to different individuals during inference."}, {"title": "5. Experiment", "content": ""}, {"title": "5.1. Implementation Details", "content": "We collected approximately 300 hours of facial videos from the internet to train our models, and the facial images are partially sourced from VGGFace2-HQ. In our experiments, we use a latent space of size 1/3 \u00d7 64 \u00d7 64 and a U-Net architecture for the \u03f5\u03b8 denoising network. Images and video clips sampled from the dataset are resized and cropped to 512 \u00d7 512. The number of motion frames, M, is set to 4, and the generated video length, T, is set to 8 frames. For the face encoder, the identity network is based on ArcFace, while the texture and attribute networks are based on DINO. We use SCRFD for facial bounding box detection. The AdamW optimizer is used for training. In the first stage of the VAE training, the learning rate is set to 5e-6 with a batch size of 32. The weights of reconstruction, perceptual, and KL divergence loss are 1.0, 0.1, 1e-6 respectively. For the second and third stages, the learning rate is increased to 1e-5, with the batch size remaining at 32. During inference, we generate video clips using the DDIM sampling algorithm for 32 steps."}, {"title": "5.2. Evaluation Protocol", "content": "Considering that most previous baselines, such as CelebA and FFHQ, are primarily focused on image face swapping, we propose a new benchmark for video face swapping. Our benchmark includes 200 source images and 200 high-resolution target videos, with each video containing 128 frames and a single trackable face. These videos and images feature unseen identities and backgrounds, ensuring a diverse and challenging dataset. To evaluate performance, we generate 200 swapped videos using our framework. For comparison, since other methods are based on image-level face swapping, we perform face swapping frame by frame for those methods. For facial data reconstruction, we use SSIM, PSNR and LPIPS to evaluate the quality of reconstructed images and videos. For video face swapping, we use FVD to assess the overall quality of the generated videos. The attribute transfer error is measured by pose and expression errors. We use HopeNet and Deep3DFaceRecon to detect these attributes, and the L2 distance to the ground truth is used as the evaluation metric. For ID retrieval, we extract identity features from the source images using ArcFace, and for"}, {"title": "5.3. Comparisons with Existing Methods", "content": "Qualitative Results. Since videos cannot be displayed in the PDF and due to submission policy restrictions on showing generated videos, we provide several comparison videos in the supplementary materials and strongly encourage the reader to view them. We perform quantitative comparison at 512 \u00d7 512 resolution. As shown in Figure 6 (a) and (d), our method generates high-fidelity swapped faces, with attributes that closely match the target faces. In Figure 6 (b), our method successfully transfers both face shape and expression under large pose variations, benefiting from the 3D reconstruction mask, while other methods exhibit generation artifacts. In Figure 6 (c), where a toy and hand occlude the girl's face, most other methods fail to handle the occlusion properly, with the toy and hand either displaced or fused together. Additionally, many methods result in noticeable facial deformations. In contrast, our method successfully recovers the occluded areas and maintains accurate face swapping, thanks to our augmentation strategy.\nQuantitative Results. In Table 1, we compare five open-source methods (two GAN-based and three diffusion-based). The results show that our method outperforms others in ID retrieval and FVD, generating high-fidelity swapped face videos while preserving the source identity. It also achieves comparable performance in pose and expression, maintaining target attributes effectively."}, {"title": "5.4. Analysis", "content": "VAE architecture. Table 2 shows the reconstruction performance of different VAE architectures. The first model is a pure 2D VAE (SD-VAE), the second uses a (2+1)D decoder with a 2D encoder, and the third is our proposed Vid-FaceVAE, which uses a full (2+1)D encoder-decoder. The VidFaceVAE outperforms the others in all metrics, achieving the highest SSIM (0.983), PSNR (41.11), and the lowest LPIPS (0.027), indicating superior reconstruction quality for facial videos. This shows that incorporating both spatial and temporal processing leads to better results compared to 2D-only approaches.\nFace feature Mixing. In our experiment, the identity, texture, and attribute weights in the face encoder are set to 1.0, 0.6, and 0.6, respectively. In Figure 7, we demonstrate the effects of varying texture and attribute weights. As the texture weight increases, we observe an improvement in identity similarity. However, if the texture weight continues to"}, {"title": "6. Conclusion", "content": "In this paper, we introduced a novel diffusion-based framework for video face swapping, addressing key challenges such as temporal consistency, identity preservation, and handling large pose variations. Our image-video hybrid training strategy leverages both static images and video data, improving model diversity and robustness. The VidFaceVAE, coupled with a custom Attribute-Identity Disentanglement Triplet (AIDT) dataset and 3D Morphable Model integration, enables accurate face swapping while mitigating issues like flickering and occlusions. Experimental results demonstrate that our framework outperforms existing methods in terms of FVD, temporal consistency, and identity preservation, while requiring fewer inference steps. Overall, our approach provides a more efficient and effective solution for high-quality video face swapping and sets the stage for future advancements in the field."}]}