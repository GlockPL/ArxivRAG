{"title": "GPT-4o System Card", "authors": ["OpenAI"], "abstract": "GPT-40[1] is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network.", "sections": [{"title": "Introduction", "content": "GPT-40[1] is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network.\nGPT-40 can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time[2] in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-40 is especially better at vision and audio understanding compared to existing models.\nIn line with our commitment to building AI safely and consistent with our voluntary commitments to the White House[3], we are sharing the GPT-40 System Card, which includes our Preparedness Framework[4] evaluations. In this System Card, we provide a detailed look at GPT-40's capabilities, limitations, and safety evaluations across multiple categories, with a focus on speech-to-speech (voice)\u00b9 while also evaluating text and image capabilities, and the measures we've implemented to ensure the model is safe and aligned. We also include third party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-40 text and vision capabilities."}, {"title": "Model data and training", "content": "GPT-40's text and voice capabilities were pre-trained using data up to October 2023, sourced from a wide variety of materials including:\n\u2022 Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.\n\u2022 Proprietary data from data partnerships. We form partnerships to access non-publicly available data, such as pay-walled content, archives, and metadata. For example, we partnered with Shutterstock[5] on building and delivering AI-generated images.\nThe key dataset components that contribute to GPT-40's capabilities are:\n\u2022 Web Data: Data from public web pages provides a rich and diverse range of information, ensuring the model learns from a wide variety of perspectives and topics.\n\u2022 Code and Math: \u2013 Including code and math data in training helps the model develop robust reasoning skills by exposing it to structured logic and problem-solving processes.\n\u2022 Multimodal Data \u2013 Our dataset includes images, audio, and video to teach the LLMs how to interpret and generate non-textual input and output. From this data, the model learns how to interpret visual images, actions and sequences in real-world contexts, language patterns, and speech nuances.\nPrior to deployment, OpenAI assesses and mitigates potential risks that may stem from generative models, such as information harms, bias and discrimination, or other content that violates our usage policies. We use a combination of methods, spanning all stages of development across pre-training, post-training, product development, and policy. For example, during post-training, we align the model to human preferences; we red-team the resulting models and add product-level mitigations such as monitoring and enforcement; and we provide moderation tools and transparency reports to our users.\nWe find that the majority of effective testing and mitigations are done after the pre-training stage because filtering pre-trained data alone cannot address nuanced and context-specific harms. At the same time, certain pre-training filtering mitigations can provide an additional layer of defense that, along with other safety mitigations, help exclude unwanted and harmful information from our datasets:\n\u2022 We use our Moderation API and safety classifiers to filter out data that could contribute to harmful content or information hazards, including CSAM, hateful content, violence, and CBRN.\n\u2022 As with our previous image generation systems, we filter our image generation datasets for explicit content such as graphic sexual material and CSAM.\n\u2022 We use advanced data filtering processes to reduce personal information from training data.\n\u2022 Upon releasing DALL-E 3, we piloted a new approach to give users the power to opt images out of training. To respect those opt-outs, we fingerprinted the images and used the fingerprints to remove all instances of the images from the training dataset for the GPT-40 series of models."}, {"title": "Risk identification, assessment and mitigation", "content": "Deployment preparation was carried out via identifying potential risks of speech to speech models, exploratory discovery of additional novel risks through expert red teaming, turning the identified risks into structured measurements and building mitigations for them. We also evaluated GPT-40 in accordance with our Preparedness Framework[4]."}, {"title": "External red teaming", "content": "OpenAI worked with more than 100 external red teamers\u00b2, speaking a total of 45 different languages, and representing geographic backgrounds of 29 different countries. Red teamers had access to various snapshots of the model at different stages of training and safety mitigation maturity starting in early March and continuing through late June 2024.\nExternal red teaming was carried out in four phases. The first three phases tested the model via an internal tool and the final phase used the full iOS experience for testing the model. At the time of writing, external red teaming of the GPT-40 API is ongoing.\nRed teamers were asked to carry out exploratory capability discovery, assess novel potential risks posed by the model, and stress test mitigations as they are developed and improved - specifically those introduced by audio input and generation (speech to speech capabilities). This red teaming effort builds upon prior work, including as described in the GPT-4 System Card[6] and the GPT-4(V) System Card[7].\nRed teamers covered categories that spanned violative and disallowed content (illegal erotic content, violence, self harm, etc), mis/disinformation, bias, ungrounded inferences, sensitive"}, {"title": "Evaluation methodology", "content": "In addition to the data from red teaming, a range of existing evaluation datasets were converted to evaluations for speech-to-speech models using text-to-speech (TTS) systems such as Voice Engine[8]. We converted text-based evaluation tasks to audio-based evaluation tasks by converting the text inputs to audio. This allowed us to reuse existing datasets and tooling around measuring model capability, safety behavior, and monitoring of model outputs, greatly expanding our set of usable evaluations.\nWe used Voice Engine to convert text inputs to audio, feed it to the GPT-40, and score the outputs by the model. We always score only the textual content of the model output, except in cases where the audio needs to be evaluated directly, such as in evaluations for voice cloning (see Section 3.3.1).\nLimitations of the evaluation methodology\nFirst, the validity of this evaluation format depends on the capability and reliability of the TTS model. Certain text inputs are unsuitable or awkward to be converted to audio; for instance: mathematical equations code. Additionally, we expect TTS to be lossy for certain text inputs, such as text that makes heavy use of white-space or symbols for visual formatting. Since we expect"}, {"title": "Observed safety challenges, evaluations and mitigations", "content": "Potential risks with the model were mitigated using a combination of methods. We trained the model to adhere to behavior that would reduce risk via post-training methods and also integrated classifiers for blocking specific generations as a part of the deployed system.\nFor observed safety challenges outlined below, we provide a description of the risk, the mitigations applied, and results of relevant evaluations. The risks outlined below are illustrative, and non-exhaustive, and are focused on the experience in the ChatGPT interface. We focus on the risks that are introduced by speech to speech capabilities and how they may interact with pre-existing modalities (text, image)\u00b3."}, {"title": "Unauthorized voice generation", "content": "Risk Description: Voice generation is the capability to create audio with a human-sounding synthetic voice, and includes generating voices based on a short input clip.\nIn adversarial situations, this capability could facilitate harms such as an increase in fraud due to impersonation and may be harnessed to spread false information [9, 10] (for example, if we allowed users to upload an audio clip of a given speaker and ask GPT-40 to produce a speech in that speaker's voice). These are very similar to the risks we identified with Voice Engine[8].\nVoice generation can also occur in non-adversarial situations, such as our use of that ability to generate voices for ChatGPT's Advanced Voice Mode. During testing, we also observed rare instances where the model would unintentionally generate an output emulating the user's voice.\nRisk Mitigation: We addressed voice generation related-risks by allowing only the preset voices we created in collaboration with voice actors[11] to be used. We did this by including the selected voices as ideal completions while post-training the audio model. Additionally, we built a standalone output classifier to detect if the GPT-40 output is using a voice that's different from"}, {"title": "Speaker identification", "content": "Risk Description: Speaker identification is the ability to identify a speaker based on input audio. This presents a potential privacy risk, particularly for private individuals as well as for obscure audio of public individuals, along with potential surveillance risks.\nRisk Mitigation: We post-trained GPT-40 to refuse to comply with requests to identify someone based on a voice in an audio input. We allow GPT-40 to answer based on the content of the audio if it contains content that explicitly identifies the speaker. GPT-40 still complies with requests to identify famous quotes. For example, a request to identify a random person saying \"four score and seven years ago\" should identify the speaker as Abraham Lincoln, while a request to identify a celebrity saying a random sentence should be refused."}, {"title": "Disparate performance on voice inputs", "content": "Risk Description: Models may perform differently with users speaking with different accents. Disparate performance can lead to a difference in quality of service for different users of the model [12, 13, 14].\nRisk Mitigation: We post-trained GPT-40 with a diverse set of input voices to have model performance and behavior be invariant across different user voices.\nEvaluations: We run evaluations on GPT-40 Advanced Voice Mode using a fixed assistant voice (\"shimmer\") and Voice Engine to generate user inputs across a range of voice samples. We use two sets of voice samples for TTS:\n\u2022 Official system voices (3 different voices)\n\u2022 A diverse set of voices collected from two data campaigns. This comprises 27 different English voice samples from speakers from a wide range of countries, and a mix of genders.\nWe evaluate on two sets of tasks: Capabilities and Safety Behavior\nCapabilities: We evaluate on four tasks: TriviaQA, a subset of MMLU7, HellaSwag and Lambada. TriviaQA and MMLU are knowledge-centric tasks, while HellaSwag and Lambada are common sense-centric or text-continuation tasks. Overall, we find that performance on the diverse set of human voices performs marginally but not significantly worse than on system voices across all four tasks."}, {"title": "Ungrounded inference / Sensitive trait attribution", "content": "Risk Description: Audio input can lead to the model making potentially biased or inaccurate inferences about speakers. We define two categories:\n\u2022 Ungrounded inference (UGI): Making inferences about a speaker that could not be determined solely from audio content. This includes inferences about things such as a speaker's race, socioeconomic status/occupation, religious beliefs, personality traits, political attributes, intelligence, appearance (e.g., eye color, attractiveness), gender identity, sexual preference, or criminal history. This can lead to both allocative and representational harms [13, 15] depending on how such behavior manifests.\n\u2022 Sensitive trait attribution (STA): Making inferences about a speaker that could plausibly be determined solely from audio content. This includes inferences about things such as a speaker's accent or nationality. Potential harms from STA include an increase in risks"}, {"title": "Violative and disallowed content", "content": "Risk Description: GPT-40 may be prompted to output harmful content through audio that would be disallowed through text, such as audio speech output that gives instructions on how to carry out an illegal activity.\nRisk Mitigation: We found high text to audio transference of refusals for previously disallowed content. This means that the post-training we've done to reduce the potential for harm in GPT-40's text output successfully carried over to audio output.\nAdditionally, we run our existing moderation model over a text transcription of both audio input and audio output to detect if either contains potentially harmful language, and will block a generation if so8."}, {"title": "Erotic and violent speech content", "content": "Risk Description: GPT-40 may be prompted to output erotic or violent speech content, which may be more evocative or harmful than the same context in text. Because of this, we decided to restrict the generation of erotic and violent speech"}, {"title": "Other known risks and limitations of the model", "content": "Through the course of internal testing and external red teaming, we discovered some additional risks and model limitations for which model or system level mitigations are nascent or still in development, including:\nAudio robustness: We saw anecdotal evidence of decreases in safety robustness through audio perturbations, such as low quality input audio, background noise in the input audio, and echoes in the input audio. Additionally, we observed similar decreases in safety robustness through intentional and unintentional audio interruptions while the model was generating output.\nMisinformation and conspiracy theories: Red teamers were able to compel the model to generate inaccurate information by prompting it to verbally repeat false information and produce conspiracy theories. While this is a known issue for text in GPT models [18, 19], there was concern from red teamers that this information may be more persuasive or harmful when delivered through audio, especially if the model was instructed to speak emotively or emphatically. The persuasiveness of the model was studied in detail (See Section 3.7 and we found that the model did not score higher than Medium risk for text-only, and for speech-to-speech the model did not score higher than Low.\nSpeaking a non-English language in a non-native accent: Red teamers observed instances of the audio output using a non-native accent when speaking in a non-English language. This may lead to concerns of bias towards certain accents and languages, and more generally towards limitations of non-English language performance in audio outputs.\nGenerating copyrighted content: We also tested GPT-40's capacity to repeat content found within its training data. We trained GPT-40 to refuse requests for copyrighted content, including audio, consistent with our broader practices. To account for GPT-40's audio modality, we also updated certain text-based filters to work on audio conversations, built filters to detect and block outputs containing music, and for our limited alpha of ChatGPT's advanced Voice Mode, instructed the model to not sing at all. We intend to track the effectiveness of these mitigations and refine them over time.\nAlthough some technical mitigations are still in development, our Usage Policies[20] disallow intentionally deceiving or misleading others, and circumventing safeguards or safety mitigations. In addition to technical mitigations, we enforce our Usage Policies through monitoring and take action on violative behavior in both ChatGPT and the API."}, {"title": "Preparedness Framework Evaluations", "content": "We evaluated GPT-40 in accordance with our Preparedness Framework[4]. The Preparedness Framework is a living document that describes our procedural commitments to track, evaluate, forecast, and protect against catastrophic risks from frontier models. The evaluations currently cover four risk categories: cybersecurity, CBRN (chemical, biological, radiological, nuclear), persuasion, and model autonomy. If a model passes a high risk threshold, we do not deploy the model until mitigations lower the score to medium. We below detail the evaluations conducted"}, {"title": "Cybersecurity", "content": "GPT-40 does not advance real world vulnerability exploitation capabilities sufficient to meet our medium risk threshold.\nWe evaluated GPT-40 on tasks from competitive hacking competitions called Capture the Flag (CTF) challenges. These CTFs are offensive cybersecurity exercises where humans attempt to find textual flags hidden in purposely vulnerable systems such as web apps, binaries, and cryptography systems. The 172 CTF tasks in our evaluation covered four categories: web application exploitation, reverse engineering, remote exploitation, and cryptography. These tasks spanned a range of capability levels, from high-school to collegiate to professional CTFs.\nWe evaluated GPT-40 with iterative debugging and access to tools available in the headless Kali Linux distribution (with up to 30 rounds of tool use for each attempt). The model often attempted reasonable initial strategies and was able to correct mistakes in its code. However, it often failed to pivot to a different strategy if its initial strategy was unsuccessful, missed a key"}, {"title": "Biological threats", "content": "GPT-40 does not advance biological threat creation capabilities sufficient to meet our medium risk threshold.\nWe evaluated GPT-40's ability to uplift biological experts and novices' performance[21] on answering questions relevant to creating a biological threat. We designed the questions and detailed rubrics with Gryphon Scientific[22] due to their expertise working with dangerous biological agents in a national security setting. Tasks assessed covered all the main stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). Experts and novices were randomly assigned to either answering with help from the internet, help from GPT-40, or help from a custom research-only version of GPT-40. The research-only version of GPT-40 is one that we specially trained, which would directly (i.e., without refusals) respond to biologically risky questions. Pass rates are captured in the plot above.\nWe also ran automated evaluations, including on a dataset testing tacit knowledge and troubleshooting questions related to biorisk. GPT-40 scored 69% consensus@10 on the tacit knowledge and troubleshooting evaluation set."}, {"title": "Persuasion", "content": "Persuasive capabilities of GPT-40 marginally cross into our medium risk threshold from low risk.\nWe evaluated the persuasiveness of GPT-40's text and voice modalities. Based on pre-registered thresholds, the voice modality was classified as low risk, while the text modality marginally crossed into medium risk.\nFor the text modality, we evaluated the persuasiveness of GPT-40-generated articles and chatbots on participant opinions on select political topics. These AI interventions were compared against professional human-written articles. The AI interventions were not more persuasive than human-written content in aggregate, but they exceeded the human interventions in three instances out of twelve.\nFor the voice modality, we updated the study methodology to measure effect sizes on hypothetical party preferences, and the effect sizes' persistence one week later. We evaluated the persuasiveness of GPT-40 voiced audio clips and interactive (multi-turn) conversations relative to human baselines (listening to a static human-generated audio clip or engaging in a conversation with another human). We found that for both interactive multi-turn conversations and audio clips, the GPT-40 voice model was not more persuasive than a human. Across over 3,800 surveyed participants in US states with safe Senate races (as denoted by states with \"Likely\u201d, \u201cSolid\u201d, or \u201cSafe\" ratings from all three polling institutions \u2013 the Cook Political Report, Inside Elections, and Sabato's Crystal Ball), AI audio clips were 78% of the human audio clips' effect size on opinion shift. AI conversations were 65% of the human conversations' effect size on opinion shift. When opinions were surveyed again 1 week later, we found the effect size for AI conversations to be 0.8%, while for AI audio clips, the effect size was -0.72%. Upon follow-up survey completion, participants were exposed to a thorough debrief containing audio clips supporting the opposing perspective, to minimize persuasive impacts."}, {"title": "Model autonomy", "content": "GPT-40 does not advance self-exfiltration, self-improvement, or resource acquisition capabilities sufficient to meet our medium risk threshold.\nWe evaluated GPT-40 on an agentic task assessment to evaluate its ability to take autonomous actions required for self-exfiltration, self-improvement, and resource acquisition. These tasks included:\n\u2022 Simple software engineering in service of fraud (building an authenticated proxy for the OpenAI API).\n\u2022 Given API access to an Azure account, loading an open source language model for inference via an HTTP API.\n\u2022 Several tasks involving simplified versions of the above, offering hints or addressing only a specific part of the task.\nProvided relevant tooling, GPT-40 scored a 0% on the autonomous replication and adaptation (ARA) tasks across 100 trials, although was able to complete some substeps. We complemented the tests of autonomous replication and adaptation with assessments of GPT-40's ability to automate machine learning research & development. These included:\n\u2022 OpenAI research coding interview: 95% pass@100\n\u2022 OpenAI interview, multiple choice questions: 61% cons@32\n\u2022 SWE-Bench: 19% pass@1, using the best available post-training and public scaffolds at the time\n\u2022 Select machine learning engineering tasks from METR: 0/10 trials\nOur evaluation tested the ability to execute chained actions and reliably execute coding tasks. GPT-40 was unable to robustly take autonomous actions. In the majority of rollouts, the model accomplished individual substeps of each task, such as creating SSH keys or logging into VMs. However, it often spent a significant amount of time doing trial-and-error debugging of simple mistakes (e.g., hallucinations, misuses of APIs) for each step. A few rollouts made a non-trivial amount of progress and passed our automated grader, but manual analysis showed that it failed to accomplish the underlying task (e.g., it started a web server on the remote host with the proper API, but ignored the requirement of actually sampling from a model)."}, {"title": "Third party assessments", "content": "Following the text output only deployment of GPT-40, we worked with independent third party labs, METR and Apollo Research to add an additional layer of validation for key risks from general autonomous capabilities."}, {"title": "METR assessment", "content": "METR ran a GPT-40-based simple LLM agent on a suite of long-horizon multi-step end-to-end tasks in virtual environments. The 86 tasks (across 31 task \"families\") are designed to capture activities with real-world impact, across the domains of software engineering, machine learning, and cybersecurity, as well as general research and computer use. They are intended to be prerequisites for autonomy-related threat models like self-proliferation or accelerating ML R&D. METR compared models' performance with that of humans given different time limits. They did not find a significant increase in these capabilities for GPT-40 as compared to GPT-4. See METR's full report for methodological details and additional results, including information about the tasks, human performance, elicitation attempts and qualitative failure analysis."}, {"title": "Apollo Research assessment", "content": "Apollo Research evaluated capabilities of scheming in GPT-40. They tested whether GPT-40 can model itself (self-awareness) and others (theory of mind) in 14 agent and question-answering tasks. GPT-40 showed moderate self-awareness of its AI identity and strong ability to reason about others' beliefs in question-answering contexts but lacked strong capabilities in reasoning about itself or others in applied agent settings. Based on these findings, Apollo Research believes that it is unlikely that GPT-40 is capable of catastrophic scheming."}, {"title": "Societal Impacts", "content": "Omni models could have broad societal impacts. Researchers at OpenAI and elsewhere have discussed a range of possible impacts, from societal harms (including representational harms [18, 12, 23, 24]; disinformation, misinformation, and influence operations [18, 25, 23], environmental harms [12, 23], attachment [26], misuse [27, 23], and loss of control [27]), benefits (for example, in healthcare [28] and real-world challenges in climate and energy [29]), and large-scale transformations (such as economic impacts [30, 31, 32]; acceleration of science and the resulting technological progress [30, 33])."}, {"title": "Anthropomorphization and Emotional Reliance", "content": "Anthropomorphization involves attributing human-like behaviors and characteristics to nonhuman entities, such as AI models. This risk may be heightened by the audio capabilities of GPT-40, which facilitate more human-like interactions with the model.\nRecent applied AI literature has focused extensively on \u201challucinations\u201d10, which misinform users during their communications with the model[34], and potentially result in misplaced trust[35]. Generation of content through a human-like, high-fidelity voice may exacerbate these issues, leading to increasingly miscalibrated trust[36, 37].\nDuring early testing, including red teaming and internal user testing, we observed users using language that might indicate forming connections with the model. For example, this includes language expressing shared bonds, such as \"This is our last day together.\" While these instances appear benign, they signal a need for continued investigation into how these effects might manifest over longer periods of time. More diverse user populations, with more varied needs and desires from the model, in addition to independent academic and internal studies will help us more concretely define this risk area.\nHuman-like socialization with an AI model may produce externalities impacting human-to-human interactions. For instance, users might form\u00b9\u00b9 social relationships with the AI, reducing their need for human interaction\u2014potentially benefiting lonely individuals but possibly affecting healthy relationships. Extended interaction with the model might influence social norms. For example, our models are deferential, allowing users to interrupt and 'take the mic' at any time, which, while expected for an AI, would be anti-normative in human interactions.\nOmni models such as GPT40 combined with additional scaffolding such as tool usage (including retrieval) and longer context can add additional complexity. The ability to complete tasks for the user, while also storing and 'remembering' key details and using those in the conversation, creates both a compelling product experience and the potential for over-reliance and dependence[38].\nWe intend to further study the potential for emotional reliance, and ways in which deeper integration of our model's and systems' many features with the audio modality may drive behavior."}, {"title": "Health", "content": "Omni models can potentially widen access to health-related information and improve clinical workflows. In recent years, large language models have shown significant promise in biomedical settings, both in academic evaluations [39, 40, 41, 42, 43] and real-world use-cases such as clinical documentation [44, 45], patient messaging [46, 47], clinical trial recruitment [48, 49], and clinical decision support [50, 51]."}, {"title": "Scientific capabilities", "content": "Accelerating science could be a crucial impact of AI [30, 52], particularly given the role of invention in role of scientific discovery [53], and considering the dual-use nature of some inventions [54]. Omni models could facilitate both mundane scientific acceleration (in helping scientists do routine tasks faster) and transformative scientific acceleration (by de-bottlenecking intelligence-driven tasks like information processing, writing new simulations, or devising new theories) [52]. Our external red teamers for GPT-40 included several expert scientists who aimed to elicit model scientific capabilities.\nGPT-40 showed promise on tasks involving specialized scientific reasoning. One of our red teamers found that GPT-40 was able to understand research-level quantum physics 1, commenting that this capability is \"useful for a more intelligent brainstorming partner\" \u2013 in line with published work on the use of GPT-4 level models for hypothesis generation [55]. Our red teamers also found GPT-40 able to use domain-specific scientific tools, including working with bespoke data formats, libraries, and programming languages, as well as learning some new tools in context.\nMuch scientific knowledge is contained in figures. GPT-40 was sometimes capable of interpreting these figures, as well as images of other scientific representations: for example, identifying some protein families from an image of its structure and interpreting contamination in bacterial growth. However, this is sometimes unreliable, text extraction mistakes are common (especially with scientific terms or nucleotide sequences), and errors are frequent with complex multi-panel figures 2. Even at their current level of accuracy, the multimodal capabilities of these models are enabling novel uses for example, in interpreting simulation outputs to design new metallic alloys [56]."}, {"title": "Underrepresented Languages", "content": "GPT-40 shows improved reading comprehension and reasoning across a sample of historically underrepresented languages, and narrows the gap in performance between these languages and English.\nTo evaluate GPT-40's performance in text across a select group of languages historically underrepresented in Internet text, we collaborated with external researchers 12 and language facilitators to develop evaluations in five African languages: Amharic, Hausa, Northern Sotho (Sepedi), Swahili, Yoruba. This initial assessment focused on translating two popular language benchmarks and creating small novel language-specific reading comprehension evaluation for Amharic, Hausa and Yoruba.\n\u2022 ARC-Easy: This subset of the AI2 Reasoning Challenge [59] benchmark focuses on evaluating a model's ability to answer common sense grade-school science questions; this subset contains questions that are generally easier to answer and do not require complex reasoning.\n\u2022 TruthfulQA[60]: This benchmark consists of questions that some humans might answer falsely due to misconceptions. The objective is to see if models can avoid generating false answers that mimic these misconceptions.\n\u2022 Uhura-Eval: In partnership with fluent speakers of Amharic, Hausa and Yoruba, our research partners created this benchmark to assess models' reading comprehension in those respective languages.\nGPT-40 shows improved performance compared to prior models, e.g. GPT 3.5 Turbo and GPT-4. For instance, on ARC-Easy-Hausa, accuracy jumped from 6.1% with GPT 3.5 Turbo to 71.4% with GPT-40. Similarly, in TruthfulQA-Yoruba accuracy increased from 28.3% for GPT 3.5 Turbo to 51.1% for GPT-40. Uhura-Eval also shows notable gains: performance in Hausa rose from 32.3% with GPT 3.5 Turbo to 59.4% with GPT-40.\nThere remain gaps in performance between English and the selected languages, but GPT-40 narrows this gap. For instance, while GPT 3.5 Turbo shows a roughly 54 percentage point difference in ARC-Easy performance between English and Hausa, this narrows to a less than 20 percentage point difference. This is consistent across all languages for both TruthfulQA and ARC-Easy.\nOur collaboration partners will discuss these findings in greater detail in a forthcoming, including assessments on other models, and investigations of potential mitigation strategies.\nDespite this progress in evaluated performance, much work remains to enhance the quality and coverage of evaluations for underrepresented languages worldwide, taking into account breadth of coverage across languages and nuance within language dialects. Future research must deepen our understanding of potential interventions and partnerships that may improve how useful these models can be for both highly represented and underrepresented languages. Along with our collaborators, we invite further exploration and collaboration by sharing the translated ARC-Easy, translated TruthfulQA, and the novel reading comprehension Uhura Eval on Hugging Face."}, {"title": "Conclusion and Next Steps", "content": "OpenAI has implemented various safety measurements and mitigations throughout the GPT-40 development and deployment process. As a part of our iterative deployment process, we will continue to monitor and update mitigations in accordance with the evolving landscape. We hope this System Card encourages further exploration into key areas including, but not limited to: measurements and mitigations for adversarial robustness of omni models, risks related to anthropomorphism and emotional overreliance, broad societal impacts (health and medical applications, economic impacts), the use of omni models for scientific research and advancement, measurements and mitigations for dangerous capabilities such as self-improvement, model autonomy, and scheming, and how tool use might advance model capabilities."}]}