{"title": "The Lessons of Developing Process Reward Models\nin Mathematical Reasoning", "authors": ["Zhenru Zhang", "Chujie Zheng", "Yangzhen Wu", "Beichen Zhang", "Runji Lin", "Bowen Yu", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin"], "abstract": "Process Reward Models (PRMs) emerge as a promising approach for process supervision\nin mathematical reasoning of Large Language Models (LLMs), which aim to identify\nand mitigate intermediate errors in the reasoning processes. However, the development\nof effective PRMs faces significant challenges, particularly in data annotation and evalu-\nation methodologies. In this paper, through extensive experiments, we demonstrate that\ncommonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically\nyields inferior performance and generalization compared to LLM-as-a-judge and human\nannotation methods. MC estimation relies on completion models to evaluate current-\nstep correctness, which can generate correct answers from incorrect steps or incorrect\nanswers from correct steps, leading to inaccurate step verification. Furthermore, we\nidentify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs:\n(1) The unreliable policy models generate responses with correct answers but flawed\nprocesses, leading to a misalignment between the evaluation criteria of BoN and the\nPRM objectives of process verification. (2) The tolerance of PRMs of such responses\nleads to inflated BoN scores. (3) Existing PRMs have a significant proportion of min-\nimum scores concentrated on the final answer steps, revealing the shift from process\nto outcome-based assessment in BoN Optimized PRMs. To address these challenges,\nwe develop a consensus filtering mechanism that effectively integrates MC estimation\nwith LLM-as-a-judge and advocates a more comprehensive evaluation framework that\ncombines response-level and step-level metrics. Based on the mechanisms, we signifi-\ncantly improve both model performance and data efficiency in the BoN evaluation and\nthe step-wise error identification task. Finally, we release a new state-of-the-art PRM\nthat outperforms existing open-source alternatives and provides practical guidelines for\nfuture research in building process supervision models.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical\nreasoning (OpenAI, 2023; Dubey et al., 2024; Shao et al., 2024; Zhu et al., 2024; Yang et al., 2024a;c;b),\nyet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions.\nMoreover, even when achieving correct final answers, these powerful models can still regularly make up\nplausible reasoning steps, where the final answers build upon flawed calculations or derivations, which\nundermine the reliability and trustworthiness of LLMs' reasoning processes. To address these challenges,\nProcess Reward Models (PRMs; Lightman et al. 2023; Wang et al. 2024b), as a representative and recently\nfocal approach, are proposed to identify and mitigate process errors, thereby enabling finer-grained\nsupervision on the reasoning process.\nOne critical challenge of developing PRMs lies in the data annotation for the correctness of reasoning\nprocesses, which is typically expensive and time-consuming. While Lightman et al. (2023) recruited\nhuman annotators with detailed instructions and elaborate procedures to achieve satisfactory annotation\nquality, the prohibitive cost pushes researchers to explore automated annotation methods. Among them,\none commonly used approach is to assess process correctness by estimating the empirical probability\nof leading to the correct final answers through Monte Carlo (MC) methods, which has attracted great\nresearch interests and has also been commonly employed in practice (Xiong et al., 2024; Wang et al., 2024b;\nLuo et al., 2024). Another challenge lies in evaluating PRM performance, as previous studies (Lightman\net al., 2023; Wang et al., 2024b; Luo et al., 2024) have predominantly relied on the Best-of-N (BoN)\nevaluation, which selects the highest-scored response from N candidates according to a PRM. Recently,\nPROCESSBENCH (Zheng et al., 2024) have emerged to evaluate the capability of PRMs in identifying\nstep-wise correctness.\nNevertheless, during the training of our own PRM following conventional principles to construct data\nusing MC estimation and evaluate on BoN, we gain several crucial lessons. In terms of MC estimation, (1)\nwe observe that the PRM trained via MC estimation demonstrated significantly inferior performance and\ngeneralization capabilities compared to LLM-as-a-judge (Zheng et al., 2023) and human annotation. (2)\nWe attribute the suboptimal performance of MC estimation to its fundamental limitation, which attempts\nto evaluate deterministic current-step correctness based on potential future outcomes. It significantly\nrelies on the performance of the completion model, which may generate correct answers based on\nincorrect steps, or incorrect answers based on correct steps, introducing substantial noise and inaccuracy\nverification into step-wise correctness estimation. Regarding the BoN evaluation, (1) the unreliable policy\nmodels generate responses with correct answers but flawed processes, leading to a misalignment between\nthe evaluation criteria of BoN and the PRM objectives of process verification. (2) the limited process\nverification capability makes PRMs demonstrate tolerance for these cases, resulting in inflated BoN\nperformance. (3) We find that in the step scores distribution of existing PRMs, a significant proportion of\nminimum scores are concentrated on the final answer steps, indicating PRMs have shifted from process\nto outcome-based assessment in BoN.\nTo address these challenges, we develop a consensus filtering mechanism that combines MC estimation\nwith LLM-as-a-judge. The instances are only retained when both LLM-as-a-judge and MC estimation\nshow consensus on the error reasoning step locations in the solution. Our approach demonstrates more\nefficient data utilization and surpass existing open-source PRMs in the conventional BoN evaluation.\nFurthermore, we advocate for complementing response-level BoN with step-wise evaluation methods.\nWe employ the step-wise benchmark PROCESSBENCH (Zheng et al., 2024) to measure the ability to\nidentify process errors in mathematical reasoning. Our trained PRMs exhibit impressively stronger error\nidentification performance than other open-source models, from PRMs to general language models,\nconfirming that our training approach genuinely teaches PRMs to assess the correctness of intermediate\nreasoning steps.\nOur key contributions can be summarized as follows:\n\u2022 We identify critical limitations in current data construction approaches for PRMs, demonstrating\nthat MC estimation-based data construction yields inferior performance compared to LLM-as-a-\njudge and human annotation.\n\u2022 We reveal the potential bias in using response-level BoN evaluation alone for PRMs and advocate\nfor comprehensive evaluation strategies combining both response-level and step-level metrics.\n\u2022 We propose a simple yet efficient consensus filtering mechanism that integrates MC estimation\nwith LLM-as-a-judge, significantly improving both model performance and data efficiency in\nPRM training.\n\u2022 We substantiate our findings through extensive empirical studies and also open source our\ntrained PRMs, which can establish practical guidelines and best practices for future research and\ndevelopment for reasoning process supervision."}, {"title": "2 Preliminary Trials", "content": "In this section, we describe our preliminary attempts to train PRMs via MC estimation-based reasoning\nstep annotation. Despite our efforts in scaling up training data and careful tuning of training objectives,\nwe found that the MC estimation-based PRMs do not possess noticeable advantages over the one\ntrained on human-annotated data (Lightman et al., 2023), and even lag significantly behind the latter in\nidentifying specific erroneous reasoning steps."}, {"title": "2.1 Training Setup", "content": "Training Data Synthesis We followed the commonly used MC estimation approach, Math-Shepherd\n(Wang et al., 2024b), to construct the PRM training data. Specifically, we collected a large-scale dataset of\napproximately 500,000 queries with golden answers. For each query, we generate 6-8 diverse responses\nby mixing outputs from the Qwen2-Math-Instruct and Qwen2.5-Math-Instruct series models (Yang et al.,\n2024c), spanning the model sizes of 7B and 72B parameters. These responses are systematically split\ninto individual steps using the delimiter \u201c\\n\\n\u201d. To assess the correctness of each step, we conduct\neight independent completions starting from this step using Qwen2.5-Math-Instruct series with the\ncorresponding model size, estimating the step labels based on the empirical probabilities of each step\nyielding the correct final answer.\nTraining Details Our trained PRMs were initialized from the supervised fine-tuned Qwen2.5-Math-\n7B/72B-Instruct models (Yang et al., 2024c), where we replace the original language modeling head\n(used for next token prediction) with a scalar-value head, consisting of two linear layers. We trained\nPRMs with either hard labels or soft labels. For hard labels, we treat a step as correct if any one of the\neight completions yields the correct final answer, and negative otherwise. For soft labels, we determined\nthe value (between 0 and 1) as the proportion of completions leading to the correct final answers. We\ncalculated the cross-entropy (CE) loss and mean squared error (MSE) loss on the last tokens of each\nstep for the binary classification task using hard labels and for the regression task using soft labels,\nrespectively. Note that we eliminated all steps subsequent to those labeled as incorrect (label 0), as their\nvalidity becomes irrelevant after an error occurs. This removal was implemented to prevent potential\nmodel confusion during training."}, {"title": "2.2 Evaluation Setup", "content": "We evaluate our trained PRMs from two aspects: their utilities in straightforwardly improving down-\nstream task performance and their abilities to identify specific erroneous steps in reasoning processes.\nBest-of-N Consistent with previous work (Lightman et al., 2023; Wang et al., 2024b; Luo et al., 2024;\nCobbe et al., 2021; Yang et al., 2024c), we employed the Best-of-N (BoN) sampling strategy for evalua-\ntion, which selects the highest-scored response from N candidates according to a PRM. We denote the\nevaluation metric as \u201cprm@N\u201d. Following Yang et al. (2024c), we sampled eight responses (i.e., N = 8)\nfrom Qwen2.5-Math-7B-Instruct across multiple mathematical benchmarks, including GSM8K (Cobbe\net al., 2021), MATH (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022), GaoKao 2023 En\n(Liao et al., 2024), OlympiadBench (He et al., 2024), College Math (Tang et al., 2024), and MMLU STEM\n(Hendrycks et al., 2021a). Each candidate response is scored using the product of all the individual scores\nof each step within the response, as computed in Lightman et al. (2023). We also report the result of\nmajority voting among eight samplings (maj@8) as the baseline, and pass@8 (i.e., the proportion of test\nsamples where any of the eight samplings lead to the correct final answers) as the upper bound.\nPROCESSBENCH We also evaluated on PROCESSBENCH as a complement. PROCESSBENCH (Zheng\net al., 2024) measures the capability of models to identify erroneous steps in mathematical reasoning.\nModels are required to identify the first step that contains an error or conclude that all steps are correct.\nFollowing the evaluation methods for PRMs in PROCESSBENCH, we locate the first erroneous step from\npredict scores yielded by PRMs."}, {"title": "2.3 Evaluation Results", "content": "As shown in Table 1 and Table 2, we denote the models trained on our MC estimated dataset as Qwen2.5-\nMath-7B-PRM-MC-hard (trained with hard labels) and Qwen2.5-Math-7B-PRM-MC-hard (trained with\nsoft labels), respectively, and compare them with a baseline model trained exclusively on the PRM800K\n(Lightman et al., 2023) dataset named Qwen2.5-Math-7B-PRM-PRM800K. The experimental results\ndemonstrate that on the Best-of-8 evaluation, none of the PRMs achieved prm@8 scores superior to"}, {"title": "3 Discussion and Analysis", "content": "In this section, we present the critical lessons gained during the PRM training. Our discussion comprises\nthree main aspects: (1) the limitations of commonly adopted MC estimation approaches in PRMs training,\nand (2) the bias in using BoN as the sole evaluation metric for optimizing PRMs."}, {"title": "3.1 Limitations of MC Estimation for PRMs Training", "content": ""}, {"title": "3.1.1 Distinguishing PRMs from Value Models", "content": "Reward models in mathematical reasoning serve as correctness verifiers and PRMs provide fine-grained\nsupervision by evaluating the correctness of intermediate reasoning steps. In contrast, value models\nestimate the potential of reaching the correct final answer from the current step in the future. The key\ndifference between PRM and value model lies in that PRMS function as deterministic evaluators of\ncurrent step correctness, while value models operate as predictive estimators of future solution potential.\nMC estimation attempts to estimate the potential of reaching the correct final answer in the future from\nthe current step. When we follow this approach to construct data and train the PRMs, the value model\nprinciples are incorporated into PRMs training essentially. This methodology potentially introduces\nperformance and generalization limitations which we will discuss in subsequent sections."}, {"title": "3.1.2 MC Estimation vs. LLM-as-a-judge vs. Human Annotation", "content": "We found that MC estimation methods limit PRM's capability to identify erroneous steps as demonstrated\nin the experiments of Section 2.3. For further investigation, we compare the performance using 3\ndistinct data construct approaches: MC estimation, LLM-as-a-judge, and human annotation. For the\nMC estimation approach, we respectively train the PRM on 445k open-source datasets Math-shepherd\n(Wang et al., 2024b) and our 860k similarly constructed dataset. For our constructed dataset, the MC\nestimation employs responses from Qwen2-Math-Instruct and completes subsequent reasoning processes\nby Qwen2.5-Math-Instruct. For the LLM-as-a-judge approach, we use the same 860k query and response\nand employ Qwen2.5-72B-Instruct to verify the correctness of each step in the responses. We show the\nprompt template we implement for verification in Appendix C. For the human annotation approach,\nwe use the open-source dataset PRM800K (Lightman et al., 2023) which consists of approximately 265k\nsamples after deduplication against the test set."}, {"title": "3.1.3 Stringent Data Filtering Mechanisms Required in MC Estimation", "content": "We attribute the inferior performance of MC estimation compared to LLM-as-a-judge and human annota-\ntion to its high noise in reasoning step correctness estimation and inaccurate error position identification\ndue to its heavy dependence on the policy model. For instance, the policy model may generate correct\nfinal answers but incorrect reasoning steps, which will be investigated thoroughly in Section 3.2.1.\nMotivated by LLM-as-a-judge's encouraging results in Section 3.1.2, we naturally propose a simple yet\nefficient consensus Filtering mechanism that integrates LLM-as-a-judge with MC estimation. Based\non the aforementioned 860K samples, the instances are only retained when both LLM-as-a-judge and\nMC estimation show consensus on the error reasoning step locations in the solution. As demonstrated\nin Figure 2, it can be found that only approximately 40% of the data are preserved after consensus\nfiltering. For evaluation on PROCESSBENCH, the results reveal that the reduced dataset after consensus\nfiltering significantly outperforms MC estimation, and notably, achieves comparable performance to\nLLM-as-a-judge while using only 40% of the data. Regarding the BoN evaluation, the performance\nvariations among these three models are marginal. The limitations of BoN evaluation in PRMs will be\nelaborated on in Section 3.2 later."}, {"title": "3.1.4 Hard Label vs. Soft Label in MC Estimation", "content": "Although we have previously demonstrated that MC estimation is not as effective as LLM-as-a-judge\nand human annotation, there remains a noteworthy point of MC estimation to be discussed, i.e., whether\nto train with soft label or hard label. We construct 3 million training data using MC estimation, where for\neach reasoning step we perform 8 completions. Subsequently, we apply the consensus filtering strategy\ndiscussed in Section 3.1.3 to filter the 3 million samples, which reduces the dataset to 1.5 million samples.\nWe respectively train PRMs using both soft labels and hard labels on 3 million and 1.5 million data.\nThe performance of trained PRMs on Best-of-8 and PROCESSBENCH are illustrated in Figure 3 and 4\nseparately. Before data filtering, the performance difference between soft and hard labels is not significant,\nwhich we attribute to the high noise level masking their distinctions. However, this difference becomes\nmuch more pronounced after data filtering, with hard labels substantially outperforming soft labels"}, {"title": "3.1.5 Summary", "content": "Through extensive experimentation, we have demonstrated that MC estimation yields inferior per-\nformance and generalization compared to both LLM-as-a-judge and human annotation. However,\nincorporating MC estimation with LLM-as-a-judge via a consensus filtering strategy leads to enhanced\nperformance and improved data efficiency. Furthermore, optimal results are achieved when treating MC\nestimation values of 0 as negative labels and training with hard labels."}, {"title": "3.2 Bias in BoN Sampling for PRM Performance Evaluation", "content": "Although BoN evaluations are commonly used in PRM optimization, their effectiveness as a sole opti-\nmization criterion is worth careful consideration due to potential limitations in performance assessment."}, {"title": "3.2.1 Unreliable Policy Models Cause BoN-PRMs Misalignment", "content": "In an ideal scenario, the responses generated by the policy model would exhibit both correct answers and\naccurate solution steps or conversely, flawed processes would correspond to incorrect answers. However,\nexisting policy models are prone to generating responses with correct answers but flawed processes, while\nBoN inherently only focuses on answers, leading to a misalignment between the evaluation criteria of\nBoN and the PRM objectives of process verification. To provide empirical evidence for this phenomenon,\nwe sample 8 responses from GSM8K, MATH, OlympiadBench, and Omni-MATH using the policy model\nQwen2.5-Math-7B-Instruct. Then we randomly choose correct-answer responses from them and conduct\nthorough manual annotations. As detailed in Figure 6, a substantial percentage of responses contain\nprocess errors while maintaining correct answers. Notably, compared with easy task GSM8K and hard\ntask Omni-MATH, this phenomenon becomes more pronounced as the problem's complexity increases.\nThis implies that an effective PRM might assign low scores to responses with correct answers but flawed\nprocesses, resulting in overall lower performance on the BoN evaluation."}, {"title": "3.2.2 Limited Process Verification Capability in PRMs Lead to BoN Scores Inflation", "content": "When the PRM cannot distinguish responses that have correct answers but flawed processes and assign\nthem high scores, this leads to overestimated performance in the BoN evaluation, thereby creating an\noverly optimistic and potentially misleading assessment of PRM capabilities. A typical example is the\ncomparative experiment in Section 3.1.2, as shown in Figure 8, where the PRMs trained on our MC\nestimated data, LLM-as-a-judge and PRM800K demonstrate opposite performance trends in BoN and"}, {"title": "3.2.3 Process-to-Outcome Shift in BoN Optimized PRMS", "content": "The majority of current PRMs are optimized towards BoN. However, the limitations of BoN result in\nPRMS process-to-outcome shift. During the BoN selection process based on PRM-predicted scores and\nfollow the scoring method for responses in (Lightman et al., 2023), it can be found that regardless of\nwhether we employ the minimum score or the product of scores to evaluate the full solution, the lowest\nstep score acts as the key limiting factor that affects the selection criteria of PRMs.\nAs shown in Figure 7, we analyze the distribution of minimum step scores assigned by multiple open-\nsourced PRMs, specifically focusing on cases where the lowest score occurred at the final step, which\ntypically contains the final answer. The results show that models EurusPRM-Stage1, EurusPRM-Stage2,\nMath-Shepherd-PRM-7B and Skywork-PRM-7B exhibit notably high proportions in this category, which\nexceed 40%. In contrast, our released PRMs Qwen2.5-Math-PRM-72B and Qwen2.5-Math-PRM-7B exhibit\na significantly lower proportion of minimum scores at the final step.\nThis analysis reveals that some PRMs' performance in BoN evaluation is predominantly determined\nby final answer scores rather than intermediate reasoning steps, indicating a model degradation from\nprocess-based to outcome-oriented assessment. In other words, optimizing solely for the BoN evaluation\nhas made current PRMs perform more like ORMs in practice. Hence, it is essential to supplement\nresponse-level evaluation BoN with step-level assessment methods to avoid the process-to-outcome shift.\nSpecifically, we can employ process error localization tasks such as PROCESSBENCH. Other commonly\nused step-wise BoN methodologies leverage the integration of PRMs or value models with search\nmechanisms, which provide a more granular assessment of process reliability. It worth noting that the\nlatter requires more computational costs."}, {"title": "3.2.4 Different PRMs, Different Optimal Scoring Strategies", "content": "In the BoN evaluation, the overall solution score is derived by combining individual step scores. When\neach step's score represents the probability of that specific step being correct, it's generally acceptable\nto combine these step-level scores (through methods like product or minimum) to calculate the overall\nsolution score. However, the situation becomes different when using MC estimation. In this case, each\nstep's score actually estimates the probability of reaching the correct final answer in the future from the\ncurrent position. Given this forward-looking nature of MC estimation, we should neither multiply the\nestimated probabilities across steps (as these estimates are dependent on each other), nor simply take the\nminimum estimated value from a particular step as the overall score. Instead, the estimated value from\nthe final step naturally integrates information from the entire solution process, making it more suitable as\nthe final score for the complete solution.\nTo validate that, we evaluate BoN in different scoring strategies for the PRMs trained on MC estimation,\nLLM-as-a-judge, and human annotation data, as shown in Figure 9. We found that in MC estimation,\nusing the last score shows significantly better performance than product and minimum approaches across\nmultiple PRMs. And the trend is the opposite for human annotation and LLM-as-a-judge. This suggests\nthat if the PRM has to be trained via MC estimation and evaluated in BoN, the last score strategy may be\nmore reasonable and effective. However, it's worth noting that this use of PRM in BoN has deviated from\nPRM's original intended purpose."}, {"title": "3.2.5 Summary", "content": "The above observations underscore critical limitations in BoN evaluation. Firstly, the unreliable policy\nmodels generate responses with correct answers but flawed processes, leading to a misalignment between\nthe evaluation criteria of BoN and the PRM objectives of process verification. Secondly, the limited process\nverification capability makes PRMs demonstrate tolerance for the responses with correct answers but\nflawed reasoning processes, resulting in inflated BoN performance. Thirdly, model optimization solely\nfocused on BoN evaluation leads PRMs to drift to prioritize final answers over reasoning processes."}, {"title": "4 Our Approach", "content": "This section presents our methodology for overcoming the previously discussed limitations and the details\nof our trained PRM achieving state-of-the-art performance. Additionally, we outline our experimental\nsettings, and baseline models for comparison and evaluation results."}, {"title": "4.1 Training Details", "content": "The data construction procedure comprises two primary phases: data expansion and data filtering. In the\nexpansion phase, we follow the MC estimation to construct data described in Section 2.1. We employ hard\nlabels, where a response is classified as negative only if none of the 8 completions achieves the correct\nfinal answer. In the subsequent filtering phase, we employ the LLM instantiated by Qwen2.5-Instruct-72B\n(Yang et al., 2024b) to serve as a critic to verify the reasoning process for all responses step by step, i.e.,\nLLM-as-a-judge. We implement a simple yet efficient consensus filtering mechanism by filtering out\ninstances where there is a discrepancy between the LLM-annotated and MC-estimated process labels.\nThis ensures the retained data maintains high quality and consistency in the reasoning process annotation.\nFor the training task, we employ cross-entropy loss on the tokens at the end of each step to train the\nbinary classification task based on hard labels. We trained both 7B and 72B-parameter PRMs, initialized\nwith Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct respectively."}, {"title": "4.2 Experimental Setup", "content": "To validate the effectiveness of our trained PRM Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B,\nwe respectively conduct the response-level BoN evaluation and the step-level process errors identification\ntask PROCESSBENCH (Zheng et al., 2024).\nBest-of-N We follow the experimental setting in Section 2.2. In rm@8, we evaluate Outcome Reward\nModels (ORMs) and Process Reward Models (PRMs). For ORMs, we introduce Qwen2.5-Math-RM-72B\n(Yang et al., 2024c), which assigns a single score to each complete response. For PRMs, we compute the\nproduct of each step score as the final response score.\nWe compare with the following PRMs:\n\u2022 Math-Shepherd-PRM-7B (Wang et al., 2024b): determining process labels for each step by\nestimating the empirical probability of reaching the correct final answer.\n\u2022 RLHFlow-PRM-Mistral-8B & RLHFlow-PRM-Deepseek-8B (Xiong et al., 2024): two LLaMA-\n3.1-based PRMs that adopt Math-Shepherd's training methodology while implementing different\nsolution generation models and optimization objectives.\n\u2022 Skywork-PRM-1.5B & Skywork-PRM-7B (Skywork, 2024): two recently released Qwen2.5-\nMath-based PRMs by Skywork.\n\u2022 EurusPRM-Stage1 & EurusPRM-Stage2 (Cui et al., 2025): two PRMs trained using Implicit PRM\napproach (Yuan et al., 2024) with 7B parameters, which obtains process rewards replying on the\nORM trained on the response-level labels.\n\u2022 Qwen2.5-Math-7B-Math-Shepherd & Qwen2.5-Math-7B-PRM800K: two additional PRMs our\ndeveloped by fine-tuning Qwen2.5-Math-7B-Instruct separately on the PRM800K (Lightman\net al., 2023) and Math-Shepherd (Wang et al., 2024b) opensource datasets.\nPROCESSBENCH The compared PRMs are consistent with the previously mentioned PRMs. For the\nLLM prompted as Critic Models, i.e., LLM-as-a-judge, we compare with proprietary language models\nGPT-40-0806 (Hurst et al., 2024) and o1-mini (OpenAI, 2024), open-source language models Llama-3.3-\n70B-Instruct (Dubey et al., 2024), Qwen2.5-Math-72B-Instruct (Yang et al., 2024c), Qwen2.5-72B-Instruct\n(Yang et al., 2024b) and QwQ-32B-Preview (Qwen, 2024). We also decompose the N-step response\ntrajectory into N separate instances to enable individual scoring by the ORM Qwen2.5-Math-RM-72B."}, {"title": "4.3 Experimental Results", "content": "Best-of-N The evaluation on policy model Qwen2.5-Math-7b-Instruct is shown in Table 6. Qwen2.5-\nMath-PRM-7B demonstrates superior performance compared to other PRMs of equivalent model scale.\nNotably, it outperforms maj@8 across all 7 tasks, achieving an average improvement of 1.4%. Furthermore,\nthe Qwen2.5-Math-PRM-72B exhibits slightly better overall performance than Qwen2.5-Math-RM-72B,\nwith particularly significant improvements observed in the Minerva Math and MMLU STEM tasks.\nFuthermore, detailed experimental results, including BoN performance on Policy model Qwen2.5-Math-\n72b-Instruct, alternative scoring strategies, and evaluations on Chinese benchmarks, are comprehensively\ndocumented in the Appendix A.\nPROCESSBENCH The evaluation results on PROCESSBENCH are presented in Table 7. When compared\nwith LLM-as-judge, Qwen2.5-Math-PRM-7B in smaller model size demonstrates superior performance\nover all open-source models. For proprietary language models, Qwen2.5-Math-PRM-7B outperforms GPT-"}, {"title": "5 Related Work", "content": "Reward Model in Mathematical Reasoning To further improve mathematical reasoning accuracy, the\nreward model plays a crucial role in selecting the best answers. Two main types of reward models have\nemerged: (1) Outcome Reward Model (ORM) which provides an evaluation score for the entire solution,\nespecially for the final answer. (2) Process Reward Model (PRM) (Uesato et al., 2022; Lightman et al.,\n2023) which evaluates each step in the reasoning process. Previous work (Lightman et al., 2023; Wang\net al., 2024b) has demonstrated that PRM outperforms ORM which exhibits greater potential, though it\nrequires more high-quality training data.\nMathematical Reasoning Step Verification There are two primary approaches to evaluating the cor-\nrectness of reasoning steps. The first approach relies on human annotation (Lightman et al., 2023), which\nproduces high-quality data but suffers from substantial costs. The second approach, which has attracted\nconsiderable research attention, focuses on automated evaluation of reasoning step correctness. Current\nautomated methods can be categorized into two main types: (1) backward-propagation based methods\nthat infer step correctness from solution outcomes, including MC estimation (Wang et al., 2024b; Luo\net al., 2024; Chen et al., 2024), progressive ORM labeling (Xi et al., 2024), and credit assignment (Wang\net al., 2024a; Cui et al., 2025; Yuan et al., 2024) techniques; (2) prompting-based methods that leverage\nLLMs serve as critic, i.e., LLM-as-a-judge (Zhang et al., 2024; Gao et al., 2024; Xia et al., 2024) to assess step\ncorrectness directly. In this work, we integrate the two approaches MC estimation and LLM-as-a-judge."}, {"title": "6 Conclusion", "content": "In this paper, we investigate the Process Reward Model (PRM) and release an effective PRM that demon-\nstrates superior performance. Firstly, we discuss the undesirable trials on MC estimation. Then we\ndemonstrate that data construction via MC estimation yields inferior performance and generalization\ncompared to both LLM-as-a-judge and human annotation through extensive experiments. Besides, we\ninvestigate the limitations of vanilla BoN evaluation for PRMs which leads to inaccurate assessment of the\nPRM's ability and causes an optimization bias that shifts focus from process-oriented to outcome-oriented\nverification. Finally, we propose a simple yet effective consensus filtering strategy combining MC estima-\ntion and LLM-as-a-judge to overcome the limitation of MC estimation. In terms of evaluation, we conduct\nthe response-level BoN evaluation and the step-level process errors identification task PROCESSBENCH to\navoid the bias of relying solely on BoN. The experiments demonstrate our strategy significantly improves\nboth data efficiency and model performance. In the future, there remains substantial potential in data\nconstruction and evaluation for PRMs, driving the development of more robust and reliable PRMs.\nLimitation Several limitations remain in our current work. Firstly, there exists a considerable perfor-\nmance gap between our PRM and the BoN upper bound (pass@8), suggesting substantial optimization\npotential. Finally, although our approach combines LLM-as-a-judge with MC estimation for consensus\nfiltering, the efficient utilization of existing high-quality human annotation data is still largely under-\nexplored. For instance, gradually expanding high-quality datasets through weakly supervised methods\ncan be investigated as a promising direction for future exploration."}]}