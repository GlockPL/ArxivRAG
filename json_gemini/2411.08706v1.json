{"title": "Searching Latent Program Spaces", "authors": ["Cl\u00e9ment Bonnet", "Matthew V Macfarlane"], "abstract": "Program synthesis methods aim to automatically generate programs restricted to\na language that can explain a given specification of input-output pairs. While\npurely symbolic approaches suffer from a combinatorial search space, recent\nmethods leverage neural networks to learn distributions over program structures to\nnarrow this search space significantly, enabling more efficient search. However,\nfor challenging problems, it remains difficult to train models to perform program\nsynthesis in one shot, making test-time search essential. Most neural methods\nlack structured search mechanisms during inference, relying instead on stochastic\nsampling or gradient updates, which can be inefficient. In this work, we propose\nthe Latent Program Network (LPN), a general algorithm for program induction\nthat learns a distribution over latent programs in a continuous space, enabling\nefficient search and test-time adaptation. We explore how to train these networks\nto optimize for test-time computation and demonstrate the use of gradient-based\nsearch both during training and at test time. We evaluate LPN on ARC-AGI, a\nprogram synthesis benchmark that evaluates performance by generalizing programs\nto new inputs rather than explaining the underlying specification. We show that\nLPN can generalize beyond its training distribution and adapt to unseen tasks\nby utilizing test-time computation, outperforming algorithms without test-time\nadaptation mechanisms.", "sections": [{"title": "Introduction", "content": "Program synthesis aims to automatically generate programs that satisfy a given specification, typically\nas input-output examples [Summers, 1977, Biermann, 1978]. Although symbolic approaches have\nshown success in limited domains [Gulwani, 2011, Albarghouthi et al., 2013, Osera and Zdancewic,\n2015, Feser et al., 2015, Frankle et al., 2016], they fail to scale to modern challenges involving large\nsearch spaces and complex patterns like in ARC-AGI [Chollet, 2019, Chollet et al., 2024]. To handle\nthe complexity and exponential search space of such difficult problems, neural approaches have\nemerged that aim to learn algorithms and programs from data [Graves, 2014, Kurach et al., 2015, Reed\nand De Freitas, 2015, Zaremba et al., 2016, Gaunt et al., 2016, Bunel et al., 2016, Bo\u0161njak et al., 2017].\nSuch methods are required as they can significantly narrow down the programmatic search space by\nleveraging biases learned through training, enabling more efficient search. Large language models\n(LLMs) have emerged as a particularly strong architecture for program synthesis tasks, with language\npre-training helping to narrow the search space before any further problem-specific fine-tuning is\nperformed [Austin et al., 2021]. This can be particularly useful if generating problem-specific data is\ntoo expensive or limited. However, models trained on a specific program distribution will likely only\ngeneralize to problems close to this distribution and perform poorly on problems such as ARC-AGI,\nwhich are specifically designed to be out of distribution for LLMs [Gendron et al., 2023]. Such\ngenerative neural methods lack mechanisms for systematic search at test time, with models usually\nresorting to stochastic sampling [Chen et al., 2021] or heuristic search [Zhang et al., 2023]. Hottung\net al. [2021b], Li et al. [2024a] and the ARC Prize 2024 leading team MindsAI explore fine-tuning a\nmodel on a test task. However, such fine-tuning is computationally expensive and highly prone to\noverfitting. This requires creating synthetic datasets on the fly, making fine-tuning a less scalable\napproach.\nTo address these limitations, we introduce a general and scalable algorithm, Latent Program Network\n(LPN), to perform program induction learning [Sun et al., 2018]. LPN builds a mechanism for\ntest-time adaptation directly into the neural architecture, without the need for parameter updates,\nand utilizes a training objective suitable to test-time search. To perform adaptation, LPN leverages\na continuous latent space to model a wide range of potential programs, which a neural decoder\ncan then use to execute that program on a specific input. Note that our decoder directly generates\noutputs pixel by pixel instead of writing Python code that would execute the task. Contrary to\nmost existing works, we train the neural architecture from scratch, as opposed to building on top\nof large-scale models such as LLMs. Instead, our goal is to generalize purely through a test-time\nadaptation mechanism, with as few priors as possible. Our key innovation is the ability to search\nthrough a structured latent space during both training and inference, enabling efficient adaptation\nat test time. We leverage gradient-based optimization in this latent space to find the latent program\nthat best explains the given specification. This latent program can then be executed on new inputs\nto generate their corresponding outputs. Since this space is a highly compressed representation\nof input-output pairs, we can perform gradient ascent in this space without encountering potential\noverfitting that parameter-based fine-tuning methods face and therefore we do not require synthetic\nexpansion. Our training objective ensures that we learn a structured latent space that is smooth and\nperforms well with gradient-based optimization, allowing for more efficient program discovery. First,\nto assess the benefits of our latent-search architecture, we evaluate it on a simple subset of ARC-type\nprograms. Second, we benchmark on ARC-AGI, a difficult program synthesis problem with a public\ntrain and evaluation set and hidden test set. In this work, we specifically choose to not enhance our\nresults by using additional synthetic datasets, human or LLM generated, as we believe it is in the\nspirit of the ARC-AGI competition to only use priors from the training set. Specifically, we only\nuse re-arc [Hodel, 2024] to generate input-output pairs that follow the programs of the training set.\nBy training only on train set problems, we ensure no possibility of data leakage from the evaluation\ndataset, which likely occurs in methods leveraging pre-trained LLMs or LLM-generated programs [Li\net al., 2024a].\nOur works' main contributions are:\n1. We introduce Latent Program Network (LPN) which directly builds in test time adaption\ninto the architecture by learning a latent space representing the space of possible programs,\nenabling test time adaption of an output decoder by moving in this latent space. We train this\nwith a novel training objective for learning the latent space that prevents encoding the output\ndirectly into the latent space. Instead, we encode an input-output pair to the latent space but\ntrain this representation to decode the output of a different input-output pair, which prevents\nthe latent space from representing the output space instead of the program space.\n2. We demonstrate that gradient-based search on the given specification in the latent space\nsignificantly improves the performance of LPN at test time compared to performance without\nlatent search.\n3. We show that adding gradient-based latent search during training enables the latent space\nitself to be trained such that gradient optimization in the space works well, resulting in\nsignificant improvement in sample efficiency.\n4. We do not make use of any pre-trained models or LLM / human-generated synthetic data\nwhen evaluating our work in the ARC domain, aside from generating input-output pairs\nusing re-arc [Hodel, 2024] based on the training set. This makes our method highly\nscalable and could be quickly applied to a different domain without requiring a domain-\nspecific language, synthetic data, or pre-trained model. LPN can be applied to any problem\nfor which a large enough number of input-output pairs from a given set of programs is\navailable. In our current setting, we do not even apply enough compute during training to\nobserve convergence indicating that improved results on ARC-AGI could be found simply\nby scaling training compute resources/parameter counts.\n5. Our work directly refutes recent claims [Li et al., 2024b] that vision transformer architec-\ntures [Dosovitskiy, 2020] struggle to solve individual arc problems. We show this is not a"}, {"title": "Related Work", "content": "Early approaches to program synthesis focused on fully symbolic methods for LISP program con-\nstruction, such as Summers [1977], Biermann [1978], which aimed to infer LISP programs from\nexamples using symbolic reasoning. Such approaches were extended to domains such as string\nprogramming [Gulwani, 2011] with applications to Microsoft Excel spreadsheets. However, these\nmethods were limited in their scalability and adaptability to more complex domains, often relying on\nenumerative search with domain-specific heuristics to reduce the search space [Albarghouthi et al.,\n2013, Osera and Zdancewic, 2015]. With the advent and successes of Deep Learning [LeCun et al.,\n2015], there has been a trend to either train fully end-to-end neural networks for program synthesis\nor combine neural networks with symbolic methods. Neural programmers-interpreters [Reed and\nDe Freitas, 2015] proposed a full end-to-end recurrent neural network approach. They found that\nsuch architecture could learn 21 different programs using the same core inference module, inferring\nprograms to execute at test time. Our model extends such approaches by learning a latent program\nrepresentation instead of learning only a fixed discrete set of programs. We also do not require\nprogram IDs during training, which are inferred by our encoder during training. A popular neuro-\nsymbolic approach called DreamCoder [Ellis et al., 2021] tackles the limitations of a domain-specific\nlanguage (DSL) by building up a library of useful programs that can be reused progressively making\nsearch more efficient by being able to search over higher-level programs. LPN works by predicting\noutputs from specifications via the latent space, and is therefore distinct from methods leveraging\nDSLs to synthesize programs that map inputs to outputs. By directly predicting outputs, LPN does\nnot restrict the types of programs we can execute, which is a consequence of using a DSL.\nRecently, much of the work in neural program synthesis has shifted toward leveraging pre-trained\nlarge language models (LLM). These approaches are significantly different from ours in terms of\nwhat priors come into the model since these LLMs are pre-trained on data that likely have non-zero\nmutual information with program synthesis problems, allowing them to tighten any generalization\ngap. This branch of approaches aims to improve generalization by reducing the required gap between\ntrain and test, by expanding the dataset directly or via a pre-trained model. CodeIt [Butt et al.,\n2024] iteratively samples programs from a pre-trained CodeT5 [Wang et al., 2023] model, performs\nhindsight relabeling, and then fine-tunes the model, helping tackle the sparsity of rewards in program\nsynthesis. Kim et al. [2024] create ARCLE an environment consisting of fixed actions according to\ncore knowledge priors in which Reinforcement Learning can be performed. This is not as scalable and\nrelies on human-extracted primitives for training and restricts the space of possible programs that can\nbe executed to solve ARC tasks. Gendron et al. [2023] investigate the zero-shot performance of LLMs\non various reasoning tasks, including ARC-AGI, showing limited but non-zero performance without\nfine-tuning, which suggests some degree of generalization. Mitchell et al. [2023] report comparable\nfindings on Concept-ARC [Moskvichev et al., 2023]. Li et al. [2024a] investigate distinct induction\nand transduction methods using LLM-based synthetic datasets for training. However, following a\ntrend with ARC-based research, they evaluate on the public evaluation dataset instead of the hidden\ntest dataset. Since they leverage synthetic LLM-based programs and human-crafted prompts, there is\na strong possibility for data leakage. LPN is classified as an induction-based approach since it tries to\nfind a latent program that explains each of the input-output pairs in the specification to then predict\nthe test input.\nA training approach that is most similar to LPN is Kolev et al. [2020], one of the few attempts to learn\nARC program synthesis end-to-end using a neural approach without a DSL or pre-trained language\nmodels. They use an autoencoder to learn grid embeddings, embedding the entire instruction set in\none step to predict the output grid directly using a transformer [Vaswani et al., 2017] architecture, also\nmaking use of spectral norm regularization [Yoshida and Miyato, 2017]. There are still significant\ndifferences in architecture as they do not learn a latent space of programs that can then be used for\nsearch, instead opting to directly predict the output from the specification. Such approaches do not\nleverage test-time adaptation, likely struggle to generalize across varying specification sizes, and\nrequire careful architectures to be permutation invariant neither of which are problems present in the\nLPN architecture."}, {"title": "Background", "content": "Program Synthesis aims to generate deterministic programs in a target language, such that outputs\ngenerated from inputs are consistent with the given specification. Typically, the problem space Y\nconsists of programs formulated within a domain-specific language (DSL). Each task is defined by a\nspecification set X, where each specification, $X_m \\in X$, is described by a set of input/output (I/O)\nexamples:\n$X_m = \\{(x_1, y_1),..., (x_n, y_n)\\}$\n(1)\nA program f \u2208 Y is considered to solve the task associated with $X_m$ if it satisfies:\n$\\forall j\\in [1,n], f(x_j) = y_j$\n(2)\nThis definition requires that the program exactly replicates the output for each input in its specification.\nWe denote $F_m$ to represent the true function that generates the input-output pairs.\nProgram Synthesis Generalization. In this work, we consider the problem where we are given a\nspecification set of input-output examples generated by a program $F_m$ (not necessarily constrained to\na DSL), along with an additional input $x_{m+1}$:\n$P_m = \\{(x_1, y_1),..., (x_n, y_n), x_{m+1}\\}.$\n(3)\nThe goal is not to be able to explain the specification, but to apply the program shown in the specifi-\ncation to a new input example $x_{m+1}$, demonstrating generalization. This can be done via induction"}, {"title": "Latent Program Network", "content": "We propose the Latent Program Network (LPN), an algorithm that trains a neural network end to\nend to take a specification of input-output pairs and generate the output of a newly given input. With\na focus on abstraction, search, and synthesis, LPN is designed with the ability to perform test-time\nadaptation explicitly built into the architecture. LPN is composed of three components (see figure 1):\n(1) a neural network encoder that takes in a specification $X_m$ and outputs an abstracted latent program,\n(2) an optimization process that refines the latent to best explain the data, (3) a decoder neural network\nthat executes the latent program to generate an output.\nPrior work has focused on directly training models to maximize the likelihood of decoding the correct\noutput given a specification [Kolev et al., 2020]. However, we diverge from such transduction-based\nmethods, which condition on all input-output pairs in the specification to predict an output, as these\napproaches face challenges when scaling to specifications of different sizes and generalizing to unseen\ntasks. They also offer no inherent ability to adapt at test time. Instead, by explicitly factorizing our\nsystem into abstraction generation, latent program synthesis, and output prediction, we aim to build\nan induction machine that can utilize test-time computation to adapt to unseen instances."}, {"title": "Latent Program Inference", "content": "The encoder and decoder play similar roles as in a VAE, while the latent optimization process attempts\nto solve a dynamic optimization problem. Unlike VAEs, we can utilize our encoder at test time as a\nstarting point for latent program search, which we find crucial for performance.\nEncoder: The probabilistic encoder is trained to approximate the Bayesian posterior over programs.\nSpecifically, it maps an input-output pair (x, y) to a distribution in the latent space $q_{\\phi}(z|x,y)$,\nrepresenting possible programs that could explain the given input-output mapping. Using a variational\napproach is important because, for any given input-output pair, there exists a broad range of possible\nprograms that map the input to the output, even when restricting to e.g. programs of low Kolmogorov\ncomplexity [Solomonoff, 1964, Kolmogorov, 1965]. Intuitively, the encoder is trained to learn an\nabstract representation of programs in a continuous latent space, by implicitly encoding input-output\npair examples. In practice, we use a multivariate normal distribution whose mean \u00b5 and diagonal\ncovariance \u2211 parameters are inferred by the encoder. To take advantage of hardware parallelization,\nthe encoder can process all the I/O pairs in a given specification in parallel. It should be noted that\nLPN is permutation invariant to the specification order by encoding each pair independently, contrary\nto a naive sequence model over the concatenation of I/O pairs.\nDecoder: The probabilistic decoder is responsible\nfor mapping a latent program and an input to the\nspace of outputs, directly predicting the output pixel\nby pixel instead of via a DSL. It models the distri-\nbution of possible outputs y given an input x and a\nlatent z. Note that even if the underlying I/O map-\npings are deterministic, we still use a probabilistic\ndecoding framework $p_{\\theta} (y|x,z)$ to be compatible with\nmaximum likelihood learning. Figure 2 shows the\ndecoder generating different outputs by keeping the\ninput fixed but varying the latent program, which in\nthis figure represents a specific grid pattern to repro-\nduce. In a real task, the aim of this encoder-decoder\nsystem is to learn a compressed representation of the\nspace of possible programs we care about (e.g. in\nthe case of ARC-AGI, this would correspond to pro-\ngrams that use the Core Knowledge priors [Chollet,\n2019]).\nLatent Optimization: The encoder is trained to\napproximate the posterior over programs and may not encode the right abstraction given an I/O pair.\nEspecially if the task is very novel, the encoder may fail at producing the right latent program, which,\nfed to the decoder, would generate the wrong output. Therefore, we include a middle stage of latent\noptimization where, starting from the encoder's prediction z, we search for a better latent program\nz', one that would better explain the observed data according to the decoder $p_{\\theta}$. The search process\nis generally denoted $z' = f(p_{\\theta}, z, x, y)$ and can be implemented in several ways (c.f. section 4.2).\nAnalogous to system 1/system 2 thinking [Kahneman, 2011], we can think of the encoder generating\nan intuitive first guess as to what the observed program may be (system 1), and the latent optimization\nprocess executing a search for hypotheses that would better explain the observations (system 2)."}, {"title": "Search Methods for Latent Optimization", "content": "Given n input-output pairs $\\{(x_i, y_i)\\}_{i=1...n}$, the search process $z' = f (p_{\\theta}, z, x, y)$ attempts to find a\nz' that satisfies:\n$z' \\in arg\\max_z \\sum_{i=1}^n log\\ p_{\\theta} (y_i|x_i, z)$\n(6)\nThis means we search for the latent that would most likely make the decoder generate the right outputs\ngiven the corresponding inputs. By finding a latent that can explain all the input-output pairs, the\nlatent solution to the optimization problem is more likely to generalize to a new input-output pair. We\ndescribe here two instantiations of the search process, namely a random search and a gradient ascent\nalgorithm, both acting in the latent space of programs. We leave for future work the exploration of\nother search methods like evolutionary strategies [Hansen and Ostermeier, 2001, Chalumeau et al.,\n2023] that could better trade-off exploration and exploitation of the latent space.\nRandom Search An initial version of the latent search process is to sample some latents from\neither the prior distribution p(z) or around the approximate Bayesian posterior $q_{\\phi}(z|x_i, y_i)$ and select\nthe latent that gives the highest log likelihood of the input-output pairs according to the decoder.\n$\\forall k \\in [1, K], z_k ~ p(z)$\n$z' \\in arg\\max_{z_k} \\sum_{i=1}^n log\\ p_{\\theta} (y_i|x_i, z_k)$\n(7)\nRandom search asymptotically converges to the true maximum likelihood latent (equation 6) and can\nprove useful when the function to optimize (here, the decoder log-likelihood) is not differentiable or\nsmooth. However, the efficiency of random search decreases exponentially with the dimension of the\nlatent space, which makes it impractical for most applications.\nGradient Ascent Since the decoder is a differentiable neural network, its log-likelihood\n$log\\ p_{\\theta} (y|x,z)$ is also differentiable with respect to z and one can use first-order methods like gradient-\nascent to efficiently search through the latent space for a solution to the latent optimization problem\n(equation 6). Figure 3 shows the gradient field and the landscape of the decoder log-likelihood of a\n2D latent space and displays how gradient-ascent can be performed to find a local optimum in the\nlatent space. This visualization highlights that only a small portion of the latent space can explain\nall the input-output pairs, corresponding to high decoding likelihood. Notably, poor initialization\ncan lead the search to converge to different local minima, highlighting the importance of amortized\ninference from the encoder.\n$\\forall k \\in [1, K], z_k = \\frac{1}{n}\\sum_{i=1}^n z_i$\n$z^k = z^{k-1} + \\alpha\\cdot\\nabla_z\\sum_i log\\ p_{\\theta} (y_i|x_i, z)|_{z = z^{k-1}}$\n(8)\nThe series $(z_k)_{k\\in[1,K]}$ should exhibit increasing decoding likelihood if the step size \u03b1 is small enough.\nIn practice, we generate the output with the best latent found during the gradient ascent algorithm,\nwhich may not always be the one that is obtained after taking the last gradient step ($z_K$)."}, {"title": "Training", "content": "To train the LPN system end-to-end, we assume to have a dataset of tasks, where a task is defined\nas n input-output pairs (xi, Yi) generated by the same program. To simulate the test conditions of\npredicting a new input from a given specification, we design the training procedure to reconstruct"}, {"title": "ARC-AGI Experiments", "content": "In this work, we consider the ARC-AGI 2024 challenge [Chollet et al., 2024] as our testing domain for\nthe proposed method. It is a challenging program synthesis dataset that encompasses a wide variety\nof unique tasks. Because very little training data is available (400 training tasks), the benchmark is\nresistant to memorization and rather tests for adaptation and out-of-distribution generalization. As\njustified in Chollet [2019], it is a benchmark that tests developer-aware generalization, which means\none possesses limited information regarding the test tasks. Indeed, developers attempting to solve\nARC-AGI cannot see the private test set but are given a set of Core Knowledge priors upon which\nall tasks are built. Such priors include objectness, goal-directedness (agency), simple arithmetic\n(numbers and counting), and basic geometry and topology."}, {"title": "LPN Architecture", "content": "In all our experiments, programs are defined in the input-output space of ARC-AGI, i.e. 2D grids\nwhose cells can take 10 different values and shapes are (n,m) with n, m \u2208 [1,30]. To train a\nhigh-performing latent space on such input-output pairs, it is critical to design an encoder that can\nprocess both input and output grids to infer a distribution over possible programs and a decoder that\ncan execute a large number of programs on input grids.\nWe implement both the encoder and decoder as small transformers [Vaswani et al., 2017] specifically\ndesigned for this benchmark, in contrast to the more general large language models (LLMs) typically\nused [Wang et al., 2023]. By foregoing the transfer learning benefits of large pre-trained models,\nwe aim to investigate the effectiveness of learning programs in this narrow domain and evaluate the\npotential of test-time latent search. The code used in this research is open source and available at\nhttps://github.com/clement-bonnet/lpn. Our codebase is implemented in JAX [Bradbury\net al., 2018] and uses neural network building blocks from the Flax library [Heek et al., 2024].\nWe model the input and output images as 2D grids that we pad and flatten in a raster-scan fashion\nto form sequences of pixel values each of size 30 * 30 = 900 (see figure 4). We prefix each grid\nsequence with shape information, namely 2 extra values for the number of rows and columns, leading\nto sequences of 902 values."}, {"title": "Validating the Decoder", "content": "Training deep networks from scratch to solve ARC-like tasks has been challenging [Li et al., 2024b].\nIf it is true that such networks struggle even to learn to execute single programs, then this would\nrepresent a significant bottleneck to models training from scratch on a broad distribution of programs.\nTherefore, before training LPN end-to-end, we conclusively show that our decoder architecture does\nnot suffer from such a bottleneck, and is able to learn individual programs.\nWe take 5 of the 400 tasks from the ARC-AGI training set, and for each of these tasks, we train a small\nLPN architecture of 800k parameters (except for the last task which required a bigger model with\n8.7M parameters) on the corresponding task generator from re-arc [Hodel, 2024]. Specifically, we\nselect the first five tasks from the arc-agi_training_challenges json file (007bbfb7, 00d62c1b,\n017c7c7b, 025d127b, 045e512c)"}, {"title": "Pattern Task", "content": "The ARC-AGI challenge contains many diverse programs leveraging different knowledge priors.\nInjecting these priors into LPN by training the model to master ARC-like tasks requires significant\ncompute resources when training from scratch, without an LLM-based initialization. Therefore,\nto investigate the training dynamics and properties of LPN before such a large-scale training, we\ndevelop a simpler task called Pattern task (see figure 7) within the same domain, but using a narrow\ndistribution of pattern-like programs. This specific task always generates fully-black 10x10 inputs\nwith a single blue pixel at a random location that defines where the output pastes a 4x4 pattern sampled\nfrom a uniform distribution. The pattern is program-specific, which means it is the same across\ndifferent pairs but it varies from one specification to another. This task enables us to demonstrate how\ndeep learning methods may still make errors on such tasks without test-time computation. We later\nextend this task to study an out-of-distribution setting in section 5.5."}, {"title": "Analyzing the Latent Space", "content": "To validate that the encoder is learning programs in its latent space, we design an even simpler\ntask with small 4x4 grids that have 2x2 patterns. We train an LPN model until convergence with\na latent space of dimension 2 to easily visualize it in figure 9. Due to the simplicity of the task\nwe train the model with mean training, i.e. no latent optimization. Because we are using a 2D\nGaussian prior for the latent space, we can convert $R^2$ to the unit square using the normal cumulative\ndistribution function (CDF), and then plot on the unit square at coordinates (x, y) the decoder's output\nwhen conditioned by the latent z = (CDF(x), CDF(y)). These results demonstrate the diversity\nand smoothness of the latent space, showing structure in terms of color patterns, which motivates\nperforming gradient ascent latent optimization in more complex tasks. This shows that the latent\nspace can encode a wide range of diversity in its latent space which is especially important for\nadapting to unseen patterns."}, {"title": "Adapting Out-Of-Distribution", "content": "We study the out-of-distribution (OOD) behavior of different training methods in table 2. We use\nthe same Pattern task as in section 5.3 but with different levels of color density for the patterns\nto reproduce. Specifically, we train on a density of 50% (which means half the patterns are black\ncells), and evaluate on 50%, 75% (weakly OOD), and 100% (strongly OOD). We observe that LPN\nequipped with gradient ascent at inference time can recover strong performance in the OOD setting\n(88% accuracy) whereas the prediction from the mean latent essentially gets an accuracy of 0%.\nMoreover, all methods suffer from a performance drop when increasing the OOD setting, but training\nwith gradient ascent gets the lowest drop in accuracy, demonstrating a latent space better behaved for\nsearch at inference time. This motivates training LPN on some distribution of programs and then\nusing the model equipped with gradient ascent at inference time given a novel task."}, {"title": "ARC-AGI 2024", "content": "Data Generation To test LPN on the ARC-AGI benchmark, we design a training phase where we\naim at injecting the Core Knowledge priors into our LPN system. In this work, we use the re-arc"}, {"title": "Limitations and Future Work", "content": "Despite its strengths, LPN faces limitations. An initial limitation\nof our work is that, despite access to TPUs for training, our main ARC-AGI training run has not\nbeen trained to convergence yet. Training networks from scratch has the downside of long training\ntimes, so future work would need to scale compute to understand the convergence properties of\nLPN when training on RE-ARC, both in training time and parameter counts. Secondly, while we\nshow that gradient ascent can be used to boost test time performance to a significant extent, gradient\nascent as an optimization method may encounter local optima, which could restrict LPN's capacity to\nfind the best latent solution. Future work could investigate different optimization procedures in the\nlatent space to overcome this challenge, such as alternatives to initializing search and procedures for\nupdating latents according to the gradient. Hybrid approaches combining evolution strategies (e.g.,\nCOMPASS [Chalumeau et al., 2023]) with gradient-based methods might improve search efficacy\nin future iterations. Another limitation is the challenge of representing complex, discrete programs\nwithin a continuous latent space, which may restrict expressivity for certain tasks or compositional\ngeneralization Shi et al. [2023]. Future work could explore discrete program representations, though\nthis would require addressing the complexities of discrete space search.\nIn summary, LPN represents a step forward in adaptive program synthesis, demonstrating effective\ntest-time adaptation, scalability, and potential for generalization. This work underscores the value of\nstructured search and adaptive latent representations in advancing program synthesis capabilities."}]}