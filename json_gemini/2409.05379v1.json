{"title": "PersonaTalk: Bring Attention to Your Persona in Visual Dubbing", "authors": ["LONGHAO ZHANG", "SHUANG LIANG", "ZHIPENG GE", "TIANSHU HU"], "abstract": "For audio-driven visual dubbing, it remains a considerable challenge to uphold and highlight speaker's persona while synthesizing accurate lip synchronization. Existing methods fall short of capturing speaker's unique speaking style or preserving facial details. In this paper, we present PersonaTalk, an attention-based two-stage framework, including geometry construction and face rendering, for high-fidelity and personalized visual dubbing. In the first stage, we propose a style-aware audio encoding module that injects speaking style into audio features through a cross-attention layer. The stylized audio features are then used to drive speaker's template geometry to obtain lip-synced geometries. In the second stage, a dual-attention face renderer is introduced to render textures for the target geometries. It consists of two parallel cross-attention layers, namely Lip-Attention and Face-Attention, which respectively sample textures from different reference frames to render the entire face. With our innovative design, intricate facial details can be well preserved. Comprehensive experiments and user studies demonstrate our advantages over other state-of-the-art methods in terms of visual quality, lip-sync accuracy and persona preservation. Furthermore, as a person-generic framework, PersonaTalk can achieve competitive performance as state-of-the-art person-specific methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Audio-driven visual dubbing broadly extends its application in real-world scenarios, such as creating digital human oral broadcast, translating human videos to various languages or altering recorded video contents. Existing methods are mainly implemented in two different ways. Some works can generate high-fidelity results through person-specific training or fine-tuning, yet require plenty of target speaker's videos for personalized modeling. In addition, the customized training process also limits their application and popularization. Other works explore to train a universal model. Despite recent advances in synthesizing lip synchronization, these person-generic methods fail to preserve speaker's persona, like speaking style and facial details. In particular, a number of them achieve audio-lip sync through the AdaIN layers, which directly interpose audio features to the renderer and implicitly edit lips. We argue that this leads to imprecise and homogenized lip movements. While IP_LAP obtains the 2D landmarks through audio features, and explicitly warps the face with these lip-synced landmarks, yet it ignores the speaking style. Besides, averaging the warped features or images causes blurriness and loss of facial details.\nIn this paper, we bring attention to speaker's persona, proposing an attention-based two-stage framework PersonaTalk, which is capable of not only producing lip movements that are well synchronized with the audio, but also upholding speaker's unique speaking style and facial details. Specifically, in the first stage, we start by extracting video speaker's 3D facial geometric information using a carefully designed hybrid geometry estimation approach. We hypothesize that introducing 3D geometry as the intermediate representation is crucial, because a) learning \"audio-to-geometry\" is much easier than learning \"audio-to-image\" since the former does not involved head pose or texture, which produces more precise lip movements; b) speaking style can be learned from the geometric statistical characteristics, which is inspired by and described in detail in Section 3.1. Then we inject the style into the audio features by a single cross-attention layer, and employ it to drive the template geometry to obtain the target lip-synced geometries through multiple cross-attention and self-attention layers. Taking the lip-synced geometries as priors, in the second stage, we generate the target talking face with a novel dual-attention face renderer. It consists of two parallel cross-attention layers: Lip-Attention and Face-Attention. The former samples lip-related textures from lip-reference frames, while the latter samples other facial textures from face-reference frames. Furthermore, to synthesize realistic faces while preserving speaker's facial details (e.g., Teeth, face outline, skin tone, makeup, lighting, etc.) as much as possible, we design an elaborate strategy to select different reference frames for them. Detailed description is in Section 3.2.\nOur main contributions can be summarized as:\n1) We propose an attention-based two-stage framework for high-fidelity and persona-preserved visual dubbing.\n2) We introduce 3D facial geometry as the intermediate representation and extract speaker's speaking style from it. The style embedding is then injected into audio features to achieve stylized driving.\n3) We devise a novel dual-attention face renderer, which consists of Lip-Attention and Face-Attention. It can generate photo-realistic images with intricate facial details.\n4) Extensive experiments and user studies demonstrate our advantages over other state-of-the-art methods. Moreover, as a person-generic framework, our approach can achieve competitive performance as person-specific methods."}, {"title": "2 RELATED WORK", "content": "Person-Generic Visual Dubbing. Person-generic methods intend to build a universal model that can be generalized to any speaker without retraining or fine-tuning. LipGAN introduces a GAN-based network to in-paint the masked lower face. Wav2Lip further proposes a SyncNet as the lip-sync discriminator to achieve better audio-visual alignment. Inspired by 3D pose prior in talking faces, PC-AVS modularizes the representations of talking faces into the spaces of speech content, head pose, and identity respectively through a compact pose code. DINet modularizes this task into deformation and inpainting stages to facilitate facial dubbing in high-resolution video content. Further advancements have been made by VideoReTalking, which develops an expression editing network to eliminate expression, a lip-sync network to generate target lip movements, and an identity-aware enhancement network to improve photo-realism. TalkLip escalates lip-speech synchronization via contrastive learning coupled with a transformer-based audio encoder. StyleSync innovates a StyleGAN-based generator with the mask-based spatial information encoding to obtain high-fidelity results on a variety of scenes. Recently, diffusion models have risen to prominence for their effectiveness in various generative tasks. DiffusedHead produces top-tier results adaptable to a range of speaking styles. However, it relies on videos with static backgrounds for training, which presents challenges when applied to extensive in-the-wild datasets. Diff Talk attempts to extend model generalization, but struggles with significant temporal flickering and poor lip accuracy in cross-identity results. It is worth noting that some works like  also explore how to extract the speaking style. However, our work is different from theirs. We learn style from geometric statistical characteristics and inject them into audio features through cross-attention layers.\nPerson-Specific Visual Dubbing. Person-specific visual dubbing is much easier than the generic one, since they are limited to the certain person in the known environment. SynthesizingObama collects 17 hours of training data to achieve high-quality dubbing. With the development of Neural Radiance Fields (NeRF) , numerous works adapt NeRF in the field of lip sync. Typically, RAD-NeRF"}, {"title": "3 METHOD", "content": "We propose an attention-based two-stage framework consisting of geometry construction and face rendering. The former estimates the speaker's facial geometry and speaking style from the reference video  and constructs the target geometries  which are lip-synced with the target audio  while retaining the personalized speaking style. The latter renders the target talking faces  with the geometries generated in the first stage and textures sampled from the reference video. Owing to our novel dual-attention face renderer, facial details can be effectively retained during rendering. For ease of writing, we assume that the reference video and target audio have the same length T."}, {"title": "3.1 Style-Aware Geometry Construction", "content": "In this stage, we first employ a hybrid geometry estimation approach with our carefully designed loss functions to obtain speaker's facial geometric coefficients  from . Then we extract and encode the audio features from . Note that we learn personalized speaking style S from the geometric statistical characteristics and inject it to the audio features, making this step style-aware. Finally, we generate  from  and .\nFacial Geometry Estimation. We adopt the hybrid geometry estimation paradigm, including learning-based initialization and optimization-based tuning, to extract video speaker's 3DMM coefficients. Specifically, given the reference video frames , we first use an off-the-shelf network to predict the initial coefficients , where ,  and  denote shape, expression and pose, respectively. Subsequently, we iteratively optimize  following . On the basis of it, we introduce a temporal-smooth loss  to ensure temporal stability and accuracy, and a regularization loss  to prevent entanglement. The former is written as\n$\\mathcal{L}_{tempo} = \\sum_{k=1}^{K} \\sum_{t=1}^{T-1} ||(kp_{k,t+1} - kp_{k,t}) - (kp_{k,t+1} - kp_{k,t})||_2 + \\lambda (||p_t||_2 + ||\\nabla \\beta_t||_2),$\nwhere  are some selected mesh vertices projected onto the image plane, and  are their corresponding ground-truth landmarks .\u2207 is the Laplace operator, and A is set to 0.2. The latter loss is a regularization term,\n$\\mathcal{L}_{reg} = \\sum_{t=1}^{T} (||a_t - \\overline{a}||_2 + ||\\beta_t - \\overline{\\beta}_{init}||_2 + ||p_t - p_{init}||_2),$\nwhere  is the initial mean shape coefficient over T.\nStyle-Aware Audio Encoding. HuBERT  has been proven to be able to transform the raw audio waveform to rich contextualized speech representations. We follow the design of HuBERT to build the audio encoder , which consists of several temporal convolution networks (TCN) and a transformer encoder. In practical, we utilize the pretrained HuBERT weights as the initialization of our , followed by an additional randomly initialized linear projection layer to interpolate the audio features to the target frame rate. The encoded audio features are denoted as , where D is the feature dimension.\nSpeaking style is learned from the geometric statistical characteristics of the reference video. In particular, we first use to construct the expression-only mesh vertices , where L is the total number of vertices. Then, a fully connected layer-based vertex encoder  is introduced to project Vtx to a lower dimension trexpD. Afterwards, the statistical mean u and standard deviation \u03c3 are calculated along the temporal dimension, followed by a fully connected layer to interpolate the concatenation of u and \u03c3 to the style embedding . We consider it as \"style\" since it is the overall statistical distribution of facial dynamics. It should be noted that, our model learns a universal style representation, and once the model is trained, it can be generalized to any speaker without person-specific fine-tuning as many other approaches. More specific visual proof can be found in Section 4.5.\nTo inject the style into the audio features, we apply a single cross-attention layer popularized by , written as\n$\\text{Attention}(Q, K, V) = \\text{Softmax}(\\frac{QXK^T}{\\sqrt{D}}) \\times V,$\nwhere Q, K and V refer to the queries, keys and values. Here, we take the style embedding S as Q, and take the audio features  as K and V.\nLip-Synced Geometry Generation. Firstly, we use shape-only coefficient  to construct the speaker's template mesh vertices, which are then encoded to  by . They are then driven to exhibit synchronized lip movements with the stylized audio features through a cross-attention layer. In particular, the stylized audio features outputted from the first cross-attention layer are set to Q,  are designated as K and V. Several self-attention layers are followed behind, whose Q, K and V are the same values, which are the output feature of the previous layer. Subsequently, a fully connected layer-based vertex decoder  is introduced to project the feature to the target mesh vertices . Note that we only generate speaker's lower-face vertices which are highly related to the audio, while retain the ground-truth upper-face vertices that provide audio-unrelated expressions such as blinking and frowning. That is, , where idxlow and idxup are vertex indexes of lower face and upper face,  is constructed by . Finally, we add head pose to and project them to the image plane in the form of PNCC , formed as ."}, {"title": "3.2 Dual-Attention Face Rendering", "content": "In the second stage, we propose a dual-attention face renderer with carefully-designed reference selection strategies to synthesize the target talking faces  , supported with the personalized lip-synced geometries , the reference video geometries , as well as the reference video frames .\nGeometry & Texture Encoding. Considering computational resources and efficiency, we apply attention in the latent space, rather than the pixel space. To this end, we employ a geometry encoder  to encode  and  into  and , and a texture encoder Etex to encode  into . Note that these encoders only contain two downsampling convolution layers, which are lightweight yet effective.\nDual-Attention Texture Sampling. For the target talking face  in the latent space, we render its texture using a cross-attention layer described in Equation 3. Specifically, we take  as Q, and select N reference frames including geometries and textures  as K and V. Note that Q is flattened and reshaped from size (C,H,W) to ( \u00d7, C), K and V are rearranged from size (N, C, \u3142, ) to (N\u00d7\u00d7, C). where the softmaxed value of the product Q \u00d7 KT can be seen as the geometry correspondences between  and . In this paper, we also add learnable positional embeddings to Q and K. The textures of  are sampled from reference textures by multiplying geometry correspondences with . For better preservation of facial details, we separate texture sampling for lip and other facial regions. In particular, we propose a dual-attention structure module consisting of two parallel attention layers, namely Lip-Attention and Face-Attention. The former only renders lip with lip-releated geometries and textures. While the latter is responsible for rendering face. These two segments are fused together though a lip mask Ml. Therefore, the full attention operation becomes\n$\\text{Dual-Attention} = \\text{Lip-Attention}(Q^l, K^l, V^l) \\times M^l + \\text{Face-Attention}(Q^f, K^f, V^f) \\times (1 - M^l).$\nReference Frames Selection. As described above, textures of lip and face are sampled separately, hence we select different references for them. During training, for frame i, we uniformly select Nf face-reference frames from an i-centered frame window with a small window size, and randomly select Nl lip-reference frames from the entire video. We argue that selecting the face-reference from adjacent frames, with slight changes in head pose and a static background, could significantly reduce the challenge of texture sampling and force the network to learn subtle facial muscle dynamics, thereby improving the facial stability and fidelity. During inference, for every i-th frame, we directly take i-centered Nf frames as the face-reference, since head poses and lighting in these frames are most similar to the current frame, allowing for better preservation. As for lip-reference, we utilize the canonical geometries Gcan, constructed by Gcan = , to perform a global sorting of mouth opening-size, and uniformly select the reference from these sorted frames. It should be noted that, attention mechanism naturally extends to any length of K and V, thus N selected for inference can be much greater than that for training, e.g., 5x or more. It ensures both diversity and global consistency of"}, {"title": "4 EXPRIMENTS", "content": "4.1 Experimental Settings\nDatasets. We utilize a mixture of datasets, including VoxCeleb2 [Chung et al. 2018], CelebV-HQ , and VFHQ to train our PersonaTalk. VoxCeleb2 is a large-scale audio-visual dataset that contains over 1 million utterances for 6,112 celebrities, extracted from videos uploaded to YouTube. CelebV-HQ is a large-scale, high-quality, and diverse video dataset consists of 35,666 video clips involving 15,653 identities with rich facial attribute annotations. VFHQ is a high-quality video face dataset for super-resolution, which contains over 16,000 high-fidelity clips of diverse interview scenarios. We apply the state-of-the-art Image Quality Assessment (IQA) method to filter out videos, which are of low quality.\nFor a fair comparison, we choose an out-of-domain dataset HDTF , which comprises about 362 different videos for 15.8 hours with original resolution 720P or 1080P, as our evaluation dataset. 30 videos in HDTF are randomly selected for quantitative comparison.\nImplementation Details. In our experiments, videos are first converted to 25 fps and then cropped to the size of 256 \u00d7 256, according to face landmarks extracted by the MediaPipe . Audios are resampled to 16kHz.\n4.2 Evaluation Metrics\nThe experimental results are evaluated in three aspects: visual quality, lip-sync accuracy and persona preservation.\nVisual Quality. Following previous studies , we use Structured Similarity (SSIM), Peak Signal-to-Noise Ratio (PSNR) and Frechet Inception Distance (FID) to evaluate the visual quality.\nLip-Sync Accuracy. In terms of lip-sync accuracy, we utilize normalized landmark distance around the lip (LMD) and the confidence score of SyncNet (SyncScore) .\nPersona Preservation. We compute the cosine similarity (CSIM) of identity vectors extracted by the face recognition model ArcFace to evaluate the identity preservation. LPIPS is employed to measure the feature-level similarity between generated and ground-truth faces.\nCurrently, there is no suitable evaluation metric for speaking style preservation in visual dubbing. In this paper, we introduce the Speaking Style Similarity (StyleSim). Given two videos V\u2081 and V2, we first extract their expression-only vertices  and  as described in Section 3.1, following by mapping them to style representations S1 and S2 with a pretrained speaking style encoder \u03a6. Note that we refer to  for the design of its model structure. Ultimately, we consider the cosine similarity of S\u2081 and S2 as the StyleSim.\n4.3 Comparisons\nComparison Methods. We compare our method against SOTA person-generic methods, including Wav2Lip , VideoRetalking , DINet and IP_LAP . Wav2Lip trains the model with a pretrained lip-expert, to improve the audio-visual synchronization. VideoRetalking introduces delicate designs of pre-/post-processing on generating photo-realistic talking faces. DINet performs spatial deformation on feature maps to preserve high frequency details, yielding high-fidelity results. IP_LAP leverages landmark and appearance prior information to preserve identity. In addition, we also compare with the person-specific approach SyncTalk , a NeRF based method which demonstrates state-of-the-art performance.\nQuantitative Comparisons. As shown in Table 1, our method achieves the best visual qualities according to SSIM, PSNR and FID. As for the lip-sync accuracy, our method still gets much better and"}, {"title": "4.4 Ablation Study", "content": "We ablate two major modules of our framework in Table 4. Specifically, we remove the style encoding in the first stage. As we expected, the generated results cannot preserve the speaker's speaking style, reflected by the decrease of the StyleSim. Besides, we change the dual-attention structure to a single cross-attention layer. We observe that there is degradation in the quality of lip and teeth generation as well as the stability of the face outline. Correspondingly, the LPIPS increases while the CSIM decreases. Finally, we forgo the frame selection strategy and randomly select reference frames for face and lip. Results indicate that the generation ability of the model has severely degraded.\n4.5 Visualization of Style Embeddings.\nWe first randomly select 10 speakers in the test set and generate 12 dubbing videos for each speaker using our method. Then we extract the style embeddings from these videos as well as the original videos with our style encoding module. We visualize these embeddings in Figure 5, which are dimension-reduced by t-SNE . As can be seen, style embeddings of different speakers are well-clustered. Additionally, style embeddings extracted from the generated dubbing videos are closely grouped with the embeddings extracted from the original video, demonstrating our model's ability to preserve speaker's speaking style in visual dubbing."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose an attention-based two-stage framework to achieve high-fidelity and persona-preserved visual dubbing. As demonstrated, our method could capture video speaker's unique speaking style, and uphold speaker's intricate facial details. Extensive experiments and user studies demonstrate our advantages over other state-of-the-art methods. Moreover, as a person-generic"}]}