{"title": "ParLS-PBO: A Parallel Local Search Solver for Pseudo Boolean Optimization", "authors": ["Zhihan Chen", "Peng Lin", "Hao Hu", "Shaowei Cai"], "abstract": "As a broadly applied technique in numerous optimization problems, recently, local search has been employed to solve Pseudo-Boolean Optimization (PBO) problem. A representative local search solver for PBO is LS-PBO. In this paper, firstly, we improve LS-PBO by a dynamic scoring mechanism, which dynamically strikes a balance between score on hard constraints and score on the objective function.\nMoreover, on top of this improved LS-PBO, we develop the first parallel local search PBO solver. The main idea is to share good solutions among different threads to guide the search, by maintaining a pool of feasible solutions. For evaluating solutions when updating the pool, we propose a function that considers both the solution quality and the diversity of the pool. Furthermore, we calculate the polarity density in the pool to enhance the scoring function of local search. Our empirical experiments show clear benefits of the proposed parallel approach, making it competitive with the parallel version of the famous commercial solver Gurobi.", "sections": [{"title": "Introduction", "content": "With the recent impressive progress in high-performance Boolean Satisfiability (SAT) and Maximum Boolean Satisfiability (MaxSAT) solvers, increasing real-world problems are solved with the Con-junctive Normal Form (CNF) encoding. However, in practice, CNF is ineffective in dealing with cardinality constraints, resulting in its size growing dramatically [3]. As a rich subject in various fields, Pseudo-Boolean Optimization (PBO) provides a better formalization than CNF in expressive power, with the use of Linear Pseudo-Boolean (LPB) constraints. Meanwhile, LPB constraints stay close to CNF and can benefit from advances in SAT solving [34]. The PBO problem is to find an assignment satisfying all LPB constraints that maximizes the objective function given.\nBriefly, there are three categories of complete algorithms to solve the PBO problem. The first one is the linear search, which extends the PB solver by adding a constraint enforcing to find a better solution (in terms of the objective function value) when finding a solution satisfying all constraints [2]. Several well-known PBO solvers are based on this idea, including Sat4j [23], RoundingSAT [14], and HYBRID [12]. The second one is Branch-and-Bound, which focuses on the techniques to estimate the lower bounds of the objective value, as the search can be pruned whenever the lower bound is greater than or equal to the upper bound. The symbolic techniques to determine the lower bounds include Maximum Hitting Set [11] and Linear Programming Relaxation [27]. The third one is to call the SAT solvers after encoding PB constraints into the CNF formula, such as MINISAT+ [13] and OpenWBO [32]. Moreover, mixed-integer programming (MIP) solvers can be directly applied to solve the PBO problem, as PB constraints can be treated as 0-1 linear constraints, representative solvers include SCIP [5] and Gurobi [18].\nComplete algorithms often suffer from the scalability issue, which motivates the development of incomplete algorithms. A typical incomplete approach is local search, which has been successfully used in many problems, including SAT [26, 1, 6], MaxSAT [24, 9], etc. Nevertheless, the literature on local search algorithms to solve PBO problem is quite limited. The first local search-based PBO solver was proposed in [25], called LS-PBO. This local search solver introduced a constraint weighting scheme and a scoring function considering both hard and soft constraints to select Boolean variables to flip. Later, LS-PBO was improved by using a unit propagation-based method to produce better initial assignments [20], resulting in the DeciLS-PBO solver. Very recently, on top of LS-PBO, Chu et. al. developed NuPBO [10], which established the latest state-of-the-art local search based PBO solving. Additionally, Iser et. al. proposed an oracle-based local search approach in the context of PBO [19], which outperforms on various benchmark domains clearly the recent pure stochastic local search approach.\nRecently, with the evolution of multi-core processors, parallel solving received growing interest. The SAT competition2 set up a parallel track from 2009, while Satisfiability Modulo Theories (SMT) competition3 introduces parallel tracks in 2021. In short, parallel algorithms contain two major directions. The first one is based on the concept of divide-and-conquer, which divides the problem into several sub-problems, and each thread solves sub-problems. For example, Treengeling [16] is a representative SAT solver of this kind. Meanwhile, commercial solvers, such as CPLEX4 and Gurobi [18] also implement their parallel versions via this approach. The other parallel approach is to integrate different solvers, including a solver with different configurations, and each thread runs a solver. This approach is commonly known as portfolio, which is simple but effective. The portfolio-based parallel SAT solvers, such as PRS [8], P-mcomsps [17], and Pakis [37], dominate the parallel track of SAT competitions in recent years. The parallel MaxSAT solvers [30, 31] based on the portfolio method also demonstrate a strong ability to efficiently solve a large number of problem instances due to the use of complementary search strategies and sharing learned clauses between threads.\nIn this paper, at first, we improve the typical LS-PBO solver by introducing a new dynamic scoring"}, {"title": "Preliminaries", "content": ""}, {"title": "Pseudo-Boolean Optimization", "content": "A Boolean variable $x_i$ can take only two values false and true, or equivalently {0, 1}. A literal $l_i$ is either a variable $x_i$ or its negation $\\neg x_i$. Given a set of n Boolean variables {$x_1,..., x_n$}, a linear pseudo-Boolean constraint (LPB constraint) is formed as follows:\n$\\sum_{i=1}^{n} a_i l_i \\triangleright b, a_i, b\\in Z, \\triangleright \\in { =, <, \\leq, >, \\geq}$\nwhere $a_i$ is the coefficient for literal $l_i$, b is called as the degree of the constraint, and $\\triangleright$ is one of the classical relational operators. With a given assignment or partial assignment, the constraint is satisfied if its left and right terms satisfy the relational operator. Otherwise, it is unsatisfied.\nMoreover, replacing all literals $x_i$ (respectively $\\neg x_i$) with negative coefficients with 1 \u2013 $x_i$ (respectively 1 \u2013 $x_i$), a LPB constraint can be normalized into the following form [34]:\n$\\sum_{i=1}^{n} a_i l_i \\geq b, a_i, b\\in N^+$"}, {"title": "A Review of LS-PBO Solver", "content": "LS-PBO is a representative local search solver for PBO, and serves as the basis of other local search PBO solvers. Briefly, it contains two main ideas: a Constraint Weighting Scheme and Scoring Functions for guiding the search process.\nTo solve a standard PBO instance, LS-PBO proposed a soft objective constraint: $\\sum_{i=1}^{n} C_i l_i < obj^*$, where $obj^*$ indicates the objective value of the best solution in the current run, and other constraints are set as hard. LS-PBO uses a weighting technique to increase the weights of falsified constraints, so that the search process is biased toward satisfying them. Specifically, it used dynamic weights (denoted as $w(\u00b7)$) to help the search avoid stuck in the local optimum, while increasing the weights of hard constraints to find feasible solutions, and the weight of objective constraint to find better solutions.\nBesides, scoring functions are essential in local search algorithms to guide the search process, which typically measures the benefits of flipping a Boolean variable. In LS-PBO, the score of flipping a variable x (denoted as score(x)) was defined as follow:\n$score(x) = hscore(x) + oscore(x)$ (1)\nwhere $hscore(x)$ indicates the decrease of the total penalty of falsified hard constraints caused by flipping $x$, and $oscore(x)$ indicates the decrease of the penalty of the objective constraint caused by flipping $x$. In detail, the penalty of falsifying a hard constraint $h_c$ was defined as $w(h_c) \\cdot max (0,b \u2013 \\sum_{i=1}^{n} a_i l_i )$, and the penalty for the objective constraint $o_c$ was defined as $w(o_c) \\cdot \\sum_{i=1}^{n} C_i l_i$"}, {"title": "Improving LS-PBO Solver with Dynamic Scoring Mechanism", "content": "As introduced in the preliminary, the score of a filliping variable x ($score(x)$) in LS-PBO is presented as Equation 1. The algorithm selects the variable with the highest positive score, indicating the biggest decrease in the penalty of hard constraints and objective constraint. A drawback of LS-PBO is the lack of dynamic adjustments to the ratio of the soft and hard constraints. If a feasible solution cannot be found within a certain period of time, the search mechanism should adaptively prioritize finding feasible solutions, thereby increasing the ratio attributed to the hard constraints. Conversely, if feasible solutions have been frequently found recently, then it would be beneficial to increase the ratio of the soft constraints to guide the search towards discovering better solutions."}, {"title": "Parallel Local Search Solver for Pseudo-Boolean Optimization", "content": "In this section, we propose a parallel local search solver ParLS-PBO. The architecture of ParLS-PBO is shown in Figure 1, which consists of two major contributions: Solution Pool and Polarity Density Weight. We first describe the global framework of ParLS-PBO, then we present the contributions in detail separately."}, {"title": "Framework of ParLS-PBO Solver", "content": "As a portfolio-based local search PBO solver, ParLS-PBO contains a master thread and multiple worker threads. The master thread reads the input PBO instance, and then produces different initial partial assignments via literal assume technique for worker threads; finally, when the time limit is reached, it outputs the best solution returned from all worker threads. In detail, supposing there are T"}, {"title": "Maintaining the Solution Pool", "content": "The solution pool aims to collect good feasible solutions, preferring those with more differences. To this end, we consider a mixed quality rating function $r_{mix}(\u00b7)$ by measuring two terms together: the quality in objective value, and the diversity. For a feasible solution S, the rating function $r_{mix}(S)$ w.r.t. a solution pool is defined as follows:\n$r_{mix}(S) = rank_{obj}(S) \\cdot p^* + rank_{div}(S) \\cdot (1 \u2212 p^*)$ (3)\nwhere $rank_{obj}(\u00b7)$ and $rank_{div}(\u00b7)$ represent the ranking of S in the solution pool in objective value and diversity value. Specifically, for the objective value, the solution with the minimal objective value is considered as the best solution, hence its $rank_{obj}(\u00b7)$ value is 1. While for the diversity value, the solution with the maximum diversity value is considered the best, thus its $rank_{div}(\u00b7)$ value is 1. $p^*$ is a penalty parameter within [0, 1] to adjust the significance of the objective value term and diversity term.\nThe difference between two solutions is measured as the sum of the number of different polarities, and the diversity value of a solution S w.r.t. The solution pool is measured as the sum of differences between S and all other solutions in the solution pool. Formally,\n$div(S) = \\sum_{S'\\in P} Hamming(S, S')$ \nWhen a worker thread finds a new feasible solution S, if the solution pool is not full, then just add it. Otherwise, S replaces the worst one (the solution with the biggest $r_{mix}(\u00b7)$ value).\nWe note that the $r_{mix}(S)$ function in this work resembles a previous population management strategy [7]. We focus on the ranking rather than the value, which can be seen as a normalization."}, {"title": "Using the Solution Pool to Guide the Search", "content": "In this subsection, we discussed how the solution pool guides each worker, including replacing solutions with better solutions from the solution pool when a worker being trapped, and utilizing the variable polarity preference in the solution pool to influence the selection of variables to flip during the search process."}, {"title": "Solution Sharing Strategy", "content": "When a worker thread fails to find a better feasible solution for a while, which means that it may be trapped in a local optimum, it selects a feasible solution with a smaller objective value from the solution pool and replaces the current one.\nIn practice, each worker thread preserves the current best feasible solution (denoted as $S^*$) as well as the corresponding objective value $obj^*$. When the search process fails to find a better solution after R steps, it picks a solution from the solution pool as a new starting point.\nTo prevent excessive overlap of search spaces among various threads, we employ a probability-based method to select solutions in the pool, rather than directly choosing the best solution in the pool. Specifically, let {$S_1, ..., S_k$} denotes the set of feasible solutions in the solution pool with objective values not bigger than $obj^*$ (the set will not be empty, as it at least contains $S^*$), and $\\Delta_i$ denotes the difference between the objective value of $S_i$ and $obj^*$. Then the probability of selecting $S_i$ is $\\Delta_i / \\sum_{j=1}^{k} \\Delta_j$."}, {"title": "Polarity Density Weight", "content": "Besides using the solutions in the pool to guide the search process directly when it gets stuck, we propose a deeper guiding method, which utilizes a piece of valuable hidden information in the solution pool the occurrence of polarities (0 or 1) of variables. To measure the effect of this kind of information, we propose the concept of polarity density weight for a variable x, denoted as $w_{pd}(x)$, which reflects the preference of certain polarity of x appearing in high-quality solutions.\nIn detail, for a variable x, $w_{pd}(x)$ is initialized as 1. Once a high-quality feasible solution S is added into the solution pool, $w_{pd}(x)$ will add (respectively, minus) a step value \u03b2 when positive (respectively, negative) polarity of x appears in S. In fact, via this updating mechanism, the higher (respectively, lower) value in $w_{pd}(x)$ indicates the higher preference of positive (respectively, negative) polarity for x in high-quality solutions. To limit the influence of $w_{pd}(x)$ and avoid possible calculation problems in negative values, we restrict $w_{pd}(x)$ into an interval of [1 \u2013 \u20ac, 1 + \u20ac], where e scales the bound. Therefore, the $w_{pd}(x)$ is updated as follows:\n$w_{pd}(x) = \\begin{cases} max(w_{pd}(x) \u2013 \u03b2,1 - \u20ac), &\\text{if } x = 0 \\text{ in S} \\\\ min(w_{pd}(x) + \u03b2, 1 + \u20ac), &\\text{if } x = 1 \\text{ in S} \\end{cases}$ (4)\nThe polarity density weight is used to enhance the scoring function of picking a variable to flip during the search process. The resulting enhanced scoring function, denoted as $score^{**}(x)$, is defined as follows:\n$score^{**}(x) = \\begin{cases} score^{*}(x) \\cdot w_{pd}(x), &\\text{if } x = 0 \\text{ in } S_{cur} \\\\ score^{*}(x) / w_{pd}(x), &\\text{if } x = 1 \\text{ in } S_{cur} \\end{cases}$ (5)\nwhere $S_{cur}$ is the current assignment maintained by the local search process.\nThe multiplication of polarity density weight influences the flip of a variable x from 0 to 1, as it increases the combined score if the preference of positive polarity exists ($w_{pd}(x) > 1$) to guide the search process to realise the flip. Respectively, in reverse, the division of polarity density weight influences the flip from 1 to 0."}, {"title": "Experiments", "content": "The experiments are organized as three parts. At first, we focus on comparing DLS-PBO, ParLS-PBO with state-of-the-art solvers including commercial solvers. Secondly, we analyze the effectiveness of the strategies to guide the search via the solution pool in ParLS-PBO. Finally, we present the tendency in performance of ParLS-PBO with the increase of the number of threads. Source code and detailed results are made publicly available on GitHub."}, {"title": "Benchmark", "content": "Real-World: Three real-world application problems, which are presented in the literature [25], including the Minimum-Width Confidence Band Problem [4]6 (24 instances), the Seating Arrange-ments Problem [33] (21 instances), the Wireless Sensor Network Optimization Problem [21, 22] (18 instances).\nMIPLIB: All satisfiable 0-1 integer programs from the MIPLIB 2017 library and earlier MIPLIB releases7, which contains 252 instances, provided in the literature [36].\nPB16: The OPT-SMALL-INT benchmark from the most recent Pseudo-Boolean Competition 20168. We filter out the duplicated instances that appear in both MIPLIB and PB16, resulting in 1524 instances in the final. PB16 contains different problem categories. We select those representatives (containing more than 30 instances) categories for finer-grained experimental analysis."}, {"title": "Candidate Methods to Compare", "content": "In the sequential track, we compare DLS-PBO with 7 state-or-the-art sequential PBO solvers, including 3 local search-based solvers: LS-PBO, DeciLS-PBO and NuPBO, 3 complete non-commercial solvers: HYBRID, PBO-IHS, and SCIP and the commercial solver Gurobi (both complete and heuristic versions).\nIn the parallel track, we compare ParLS-PBO with the academic solver FiberSCIP, and the parallel version of the commercial solver Gurobi.\nLS-PBO [25]: the state-of-the-art SLS algorithm for solving PBO9.\nDeciLS-PBO [20]: a recent SLS algorithm based on LS-PBO10.\nNuPBO [10]: a recent SLS algorithm based on LS-PBO, which established the latest state-of-the-art local search based PBO solving11.\nHYBRID [12]: a recent core-guided PBO solver building upon RoundingSAT [15]12.\nPBO-IHS [36]: a recent IHS PBO solver building upon RoundingSAT13.\nGurobi [18]: one of the most powerful commercial MIP solvers. We use both its complete and heuristic versions14.\nSCIP [5]: one of the fastest non-commercial solvers for MIP (the latest version 8.0.1)15,\nFiberSCIP [35]: a parallel non-commercial MIP solvers based on SCIP (the latest version 1.0.0)16.\nWe download the latest version of all candidate methods to compare from their published links. In all experiments, we always use their default parameter settings."}, {"title": "Experimental Settings", "content": "DLS-PBO and ParLS-PBO are implemented in C++, and compiled with g++ (version 9.2.0) using the option '-O2'. All experiments are carried out on a cluster with two AMD EPYC 7763 CPUs @ 2.45Ghz of 128 physical cores and 1TB memory running the operating system Ubuntu 20.04 LTS (64bit).\nAs with the previous research on PBO solvers [25, 20], we set the time limit for each run as 300 and 3600 seconds. For each sequential randomized solver, we run 10 times for each instance with different seeds from {0,1,..., 9}, and select the median of the 10 runs as the final result. Without"}, {"title": "Performance Evaluations", "content": ""}, {"title": "The Sequential Track", "content": "We first compare DLS-PBO with LS-PBO, and the results are shown in Table 2. DLS-PBO significantly improves LS-PBO in terms of both #win and avgsc* on all the benchmarks.\nFurther, we evaluate DLS-PBO with other PBO solvers, as well as integer programming solvers. The results (Table 3) indicate that NuPBO performs best for the Real-world benchmark, while Gurobi is the best on MIPLIB and PB16 benchmarks. DLS-PBO cannot rival these two solvers, yet it is better than other PBO solvers. We note that the emphasis of this work is to develop an effective parallel method for PBO solvers. We choose LS-PBO as the baseline as it is the typical local search PBO solver (NuPBO is also developed on top of it). We simply remedy its drawback to obtain DLS-PBO,"}, {"title": "The Parallel Track", "content": "The comparative results of our parallel solver ParLS-PBO with other parallel solvers are shown in Table 4 (We only show #win due to the space limit). ParLS-PBO gives the best performance on all categories of the Real-World benchmark, and 3 categories of the PB16 benchmark, including Kexu, Logic Synthesis, and Prime.\nIn terms of the Total instances, ParLS-PBO outperforms the non-commercial solver FiberSCIP, and is competitive with the commercial solver Gurobi. Comparing Table 3 and Table 4, it can be found that the gap between ParLS-PBO and Gurobi (32 threads) is decreasing compared with the gap between DLS-PBO and Gurobi (1 thread), which indicates the effectiveness of our solver in parallel solving.\nWe also observe that ParLS-PBO outperforms the best sequential PBO solver NuPBO on all benchmarks (44 vs. 32, 171 vs. 156, and 1238 vs. 1002). Although this comparison is unfair (and thus we do not report it in the table), it indicates that by parallelization, the performance of PBO solvers can be significantly improved."}, {"title": "Effectiveness Analysis", "content": "This subsection evaluates the effectiveness of the key strategies of ParLS-PBO. In Table 5, we compare ParLS-PBO with its 2 variants:\nV\u2081: to analyze the effectiveness of the solution-pool-based sharing, we modify ParLS-PBO by disabling the sharing mechanism and making each thread solve separately.\nV2: to analyze the effectiveness of the global score mechanism, we modify ParLS-PBO by disabling the global score mechanism and using score* (x) directly in the local search.\nAs shown in Table 5, ParLS-PBO outperforms other variations, confirming the effectiveness of the strategies."}, {"title": "Scalability Analysis", "content": "In order to analyze the scalability of ParLS-PBO, we choose Gurobi (complete version) with 32 threads as the comparison baseline to test the performance gap between different threads of ParLS-PBO. We report #win for threads set to {4, 8, 16, 32} compared to baseline. As is shown in Figure 2, in each benchmark, #win is gradually increasing, which verifies the scalability of ParLS-PBO."}, {"title": "Conclusions", "content": "We proposed two local search solvers for the PBO problem: DLS-PBO and ParLS-PBO. DLS-PBO is an enhanced version of the LS-PBO solver, incorporating a dynamic scoring mechanism. ParLS-PBO is a parallel solver with a solution pool collecting good solutions from multiple threads. The solution pool guides the local search process by providing better starting points and utilizing polarity information from high-quality solutions to improve the scoring function. Experimental results show that our parallel solver has significantly better performance than all sequential solvers and exhibits strong competitiveness against the parallel versions of Gurobi.\nThe ideas of this work can be applied to other problems, particularly including SAT and MaxSAT. It is also interesting to implement a distributed version of ParLS-PBO for cloud computation."}]}