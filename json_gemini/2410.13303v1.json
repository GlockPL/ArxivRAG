{"title": "Hiformer: Hybrid Frequency Feature Enhancement Inverted Transformer for\nLong-Term Wind Power Prediction", "authors": ["Chongyang Wan", "Shunbo Lei", "Yuan Luo"], "abstract": "The increasing severity of climate change necessitates an urgent transition to renewable energy sources, making the large-scale adoption of wind energy crucial for mitigating environmental impact. However, the inherent uncertainty of wind power poses challenges for grid stability, underscoring the need for accurate wind energy prediction models to enable effective power system planning and operation. While many existing studies on wind power prediction focus on short-term forecasting, they often overlook the importance of long-term predictions. Long-term wind power forecasting is essential for effective power grid dispatch and market transactions, as it requires careful consideration of weather features such as wind speed and direction, which directly influence power output. Consequently, methods designed for short-term predictions may lead to inaccurate results and high computational costs in long-term settings. To adress these limitations, we propose a novel approach called Hybrid Frequency Feature Enhancement Inverted Transformer (Hiformer). Hiformer introduces a unique structure that integrates signal decomposition technology with weather feature extraction technique to enhance the modeling of correlations between meteorological conditions and wind power generation. Additionally, Hiformer employs an encoder-only architecture, which reduces the computational complexity associated with long-term wind power forecasting. Compared to the state-of-the-art methods, Hiformer: (i) can improve the prediction accuracy by up to 52.5%; and (ii) can reduce computational time by up to 68.5%.", "sections": [{"title": "Introduction", "content": "Over recent decades, the rapid consumption of fossil fuels has exacerbated climate warming, making the transition to renewable energy increasingly urgent. As a clean, safe and renewable energy source, wind power serves as an effective alternative to fossil fuels, significantly reducing carbon emissions (Zhao et al. 2024). Hence, we have witnessed a large-scale wind power grid connections in reality (Tawn and Browell 2022). However, unlike traditional power station, the output of wind power plants is highly dependent on weather parameters such as wind speed, wind direction and temperature. This dependence introduces significant volatility and intermittency in wind power production, making grid systems highly fragile and challenging to manage (Wang et al. 2021; Liu et al. 2024).\nTo reduce potential supply and load imbalances in the power grid caused by the uncertainty of wind power, accurate wind power prediction is crucial. This enables grid operators to flexibly adjust the output of each generator and energy storage unit in advance (Konstantinou and Hatziargyriou 2024). Existing studies primarily focus on designing deep learning models to explore the spatial-temporal correlations of wind power data (see Related Work Section for details). However, these methods often fail to produce accurate long-term predictions (i.e., one day in advance) due to the highly non-stationary nature of wind power. As shown in Figure 1, unlike traffic or solar data, which exhibit more predictable patterns, wind power patterns change unpredictably over time. Factors such as seasonal shifts, climate variations and other meteorological influences can cause significant changes in wind patterns, making it difficult for models that only consider spatial-temporal data to accurately capture and predict these long-term variations (Wang and Luo 2023).\nThe issues of incorporating such weather impact into wind power prediction have recently been studied in (He et al. 2022; Liu et al. 2023). However, these work neglect the computational complexity problem. As a result, their methods require substantial computational time, rendering them impractical for real-world wind power systems (Du et al. 2020; Wang et al. 2021).\nDesigning a model for long-term wind power prediction with low computational complexity is non-trivial and has not been done before. The complexity arises from the"}, {"title": "Related Work", "content": "non-stationary nature of wind power, which requires additional techniques to handle the unpredictable wind patterns influenced by varying weather conditions. Additionally, widely used techniques, such as transformer-based architectures, which are effective at capturing feature correlations in data, often suffer from performance degradation and computational inefficiency when applied to explore the spatial-temporal dependencies between wind power generation and weather features. Hence, we cannot simply combine existing methods to achieve accurate predictions within a reasonable computational time.\nAgainst this background, we propose a Hybrid Frequency Feature Enhancement Inverted Transformer (Hiformer) for long-term wind power prediction. By carefully incorporating the Variational Mode Decomposition (VMD) technique within an encoder-only architecture, Hiformer effectively addresses the non-stationary nature of wind power caused by varying weather conditions. Furthermore, Hiformer adopts inverted multi-attention mechanisms (Liu et al. 2023) to prevent computational explosion during the prediction process.\nThis paper advances the state-of-the-art in the following ways. First, Hiformer is the first wind power prediction model to integrate VMD with weather feature extraction methods. This integration enhances Hiformer\u2019s ability to predict wind power generation that are highly influenced by unpredictable weather conditions. Second, by carefully designing an encoder-only structure with an inverted attention mechanism, Hiformer achieves computational efficiency, making it feasible for practical wind power systems. Finally, our empirical studies show that by considering the non-stationary nature of wind power and applying the VMD technique together with weather feature extraction technique to mitigate the impact of weather conditions, Hiformer improves prediction accuracy by up to 52.5%. Additionally, compared to state-of-the-art models, Hiformer\u2019s encoder-only structure with an inverted attention mechanism reduces computational time by up to 68.5%.\nTo capture the non-linear and fluctuating temporal correlations in wind power data, researchers have adopted deep learning techniques (Giebel and Kariniotakis 2017). For example, Kisvari, Lin, and Liu (2021) employed Recurrent Neural Networks (RNNs) to model complex temporal relationships in wind power data by maintaining a memory of past states. Liu et al. (2024) developed a Long Short-Term Memory (LSTM) model that selectively retains and forgets information over long sequences, thereby enabling more accurate wind power predictions. Given the highly variable and unpredictable nature of wind power data, researchers have also applied signal decomposition techniques in deep learning models to further enhance prediction accuracy (Jiang et al. 2024). For instance, Abedinia et al. (2020) applied Empirical Mode Decomposition (EMD) method to decompose wind power data, followed by the use of a back propagation neural network for prediction. Sun, Zhao, and Zhang (2019) employed the Variational Mode Decomposition (VMD) method to achieve short-term wind power prediction. However, these methods often overlook the influence of spatial information on prediction accuracy. Note that wind turbines possess spatial characteristics within a wind farm, which can significantly impact prediction accuracy (Zhao et al. 2024). For example, wind turbines located downwind in a wind farm are significantly affected by the wake of the adjacent upwind wind turbines, resulting in reduced power generation (Mittelmeier, Blodau, and K\u00fchn 2017; Wang et al. 2021).\nTo incorporate turbine spatial information in wind power prediction, a commonly used method is integrating graph structures of wind turbines into the prediction model (Qiu et al. 2024). For example, Wang et al. (2022) proposed an ultra-short-term wind farm cluster power forecasting method that effectively improves prediction accuracy. Yu et al. (2020) developed a superposition graph neural network for extracting spatial features. However, a common limitation of these methods is that they only consider the correlations between wind turbines, neglecting the impact of weather conditions on wind power generation. Given that wind power generation is highly dependent on weather features such as wind speed and direction, these methods fall short in providing accurate long-term predictions.\nTo address this, many studies incorporate weather features into spatial-temporal prediction models using attention mechanisms to capture multivariate relationships (Zheng et al. 2020). For example, He et al. (2022) achieved long-term wind power prediction under varying weather conditions. Liu et al. (2023) modified the attention module in the traditional transformer model (Vaswani et al. 2017) to focus on the correlations between weather features and wind power, enabling long-term predictions across multiple turbines. Although these studies have made some progress, they require substantial computing time to calculate the complex relationships between weather features and wind power, limiting their practicality for real-world applications."}, {"title": "Hybrid Frequency Feature Enhancement\nInverted Transformer", "content": "In this section, we first present the mathematical definition of the wind power prediction problem. Following this, we offer a detailed description of our proposed Hiformer."}, {"title": "Problem Definition", "content": "Our goal is to predict the wind power generation $\\mathbf{X} = (X_{t_{P}+1}, X_{t_{P}+2},..., X_{t_{P}+Q}) \\in \\mathbb{R}^{Q \\times N}$ for next Q time steps, given N turbines' wind power generation $\\mathbf{X} = (X_{t_1}, X_{t_2}, ..., X_{t_{P}}) \\in \\mathbb{R}^{P \\times N}$ and weather data $\\mathbf{W} = (W_{t_1}, W_{t_2},..., W_{t_{P}}) \\in \\mathbb{R}^{P \\times N \\times C}$ for P historical time steps. Here, C is the number of weather features (e.g., wind speed, wind direction and ambient temperature).\nIn order to characterize the spatial correlations among power outputs of N wind turbines, we represent a weighted undirected graph $G = (V, E, A)$, where V is the set of locations of N wind turbines, E is the set of edges between turbines and $A \\in \\mathbb{R}^{N \\times N}$ is a weighted adjacency matrix with $A_{v_i, v_j} \\in [0,1], v_i, v_j \\in V$ denoting the proximity between vertices $v_i$ and $v_j$."}, {"title": "Model Overview", "content": "As shown in Figure 2(a), Hiformer employs an encoder-only structure (Vaswani et al. 2017). In more details, the model begins with an Embedding block to extract temporal, spatial, and weather information from the raw data. This is followed by the Hybrid Frequency Feature Enhancement (HFF) block, which enhances Hiformer's ability to capture the spatial-temporal correlations between wind power generation and weather features. To prevent overfitting, we employ an Add&Norm layer, and a Feed-Forward network is used to strengthen the temporal representations of wind power. Finally, a Projection block decodes the encoder's output, producing the final prediction results. Detailed explanations of these components are provided below."}, {"title": "Embedding", "content": "As shown in Figure 2(b), we use three embedding blocks to process wind power data X, spatial data A and weather features data W. We denote the output of the VMD Module, Spatial Embedding and Weather Embedding as $\\mathbf{e_T}$, $\\mathbf{e_S}$ and $\\mathbf{e_W}$, respectively. We also denote the output of Spatial-Temporal Embedding and Spatial-Weather Embedding as $\\mathbf{e_{ST}}$ and $\\mathbf{e_{SW}}$, respectively. The followings are the detailed explanations."}, {"title": "Variational Mode Decomposition Module", "content": "We first employ Variational Mode Decomposition (VMD) Module to process the wind power data. VMD is a signal decomposition technique that decomposes highly volatile wind power data into M amplitude-modulated-frequency-modulated (AM-FM) signals, known as the Intrinsic Mode Function (IMF) (Dragomiretskiy and Zosso 2013). As shown in Figure 3, the IMF exhibits stronger periodicity compared to the original wind power data, enabling us to better capture their temporal dependencies. We denote each IMF as $u_i \\in \\{U_1, U_2,..., U_M\\}$ and define it as:\n$u_i(t) = A_i(t) \\cos(\\phi_i(t)),$ (1)\nwhere $A_i(t)$ is the non-negative amplitude, $\\phi_i(t)$ is the phase of the signal. Note that IMF $u(t)$ can be approximated as a pure harmonic signal with amplitude $A_i(t)$ and central frequency $\\omega_i(t) = \\dot{\\phi}(t)$.\nTo obtain M IMFs in each turbine, we solve the following constrained variational problem:\n$\\min_{\\mathcal{U},\\{u_{v,i}\\}, \\{\\omega_{v,i}\\}} \\left\\{ \\sum_{v} \\sum_{i=1}^{M} \\left\\| \\partial_t \\left[ (\\delta(t) + \\frac{j}{\\pi t}) * u_{v,i}(t) \\right] e^{-j \\omega_{v,i} t} \\right\\|_2^2 \\right\\},$  \ns.t. $\\sum_{i=1}^{M} u_{v,i} = f_v.$\n(2)\nwhere $u_{v,i}$ is the i-th IMF given the v-th turbine\u2019s wind power data, $\\omega_{v,i}$ is i-th IMF\u2019s central frequency at v-th turbine, $\\parallel \\cdot \\parallel$ is the squared euclidean norm, $\\partial_t$ is the partial derivative of the function for time $t \\in \\{1, 2, . . ., P\\}$, $\\delta(t)$ is the unit pulse function, j is the imaginary unit, * indicates the convolution operation and $f_v$ is the original wind power data at vertex v.\nAs is common in the literature (Dragomiretskiy and Zosso 2013; Wang et al. 2019), we apply Hilbert transform to solve this problem, and obtain the M IMFs vector $\\{\\mathbf{U}_v,i\\} = \\{\\mathbf{U}_v,1, \\mathbf{U}_v,2,..., \\mathbf{U}_v,M\\}$ of vertex v. Then we combine all the IMFs together to obtain the matrix $\\mathbf{e_{IMF}} = \\{\\{\\mathbf{U}_{1,i}\\}, \\{\\mathbf{U}_{2,i}\\}, ...\\{\\mathbf{U}_{N,i}\\}\\}\\in \\mathbb{R}^{P \\times N \\times M}$ for N turbines in P times steps. Similarly to Liu et al. (2023), we apply multi-layer perceptrons (MLPs) to transform IMFs vector $\\mathbf{e_{IMF}}$ into $\\mathbf{e_T} \\in \\mathbb{R}^{D \\times N \\times M}$, where D represents the number of correlation hidden states for P time steps. This transformation can enhance the model\u2019s ability to capture the tempo-"}, {"title": "Spatial Embedding", "content": "ral relationships between wind power and IMFs (as we will show in the Experiments Section)\nSpatial Embedding: We employ node2vec (Grover and Leskovec 2016), which is commonly used in the literature (Hou, Deng, and Cui 2021; Wang and Luo 2023), to capture spatial features among wind turbines in graph G. The input of node2vec is the weighted adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$, and the output is $\\mathbf{e_{node}} \\in \\mathbb{R}^{D \\times N}$. We then apply MLPs to process $\\mathbf{e_{node}}$, ensuring that its matrix dimensions are consistent with the VMD module\u2019s output, and obtain the Spatial Embedding output $\\mathbf{e_S} \\in \\mathbb{R}^{D \\times N}$"}, {"title": "Weather Embedding", "content": "Weather Embedding: We apply MLPs to process weather feature data W, enabling us to characterize the correlations between weather features and wind power generation, and obtain the transformed output $\\mathbf{e_W} \\in \\mathbb{R}^{D \\times N \\times C\u2019}$, where D is the correlation hidden state, N is the number of turbines and C is the number of weather features."}, {"title": "Spatial-Temporal Embedding", "content": "Spatial-Temporal Embedding: As shown in Figure 2(b), we design Spatial-Temporal Embedding (STE) to capture the temporal relationship between IMFs and wind power generation within a spatial context. We superimpose the spatial embedding and VMD module outputs to obtain the STE output $\\mathbf{e_{ST}} = (\\mathbf{e_S} + \\mathbf{e_T}) \\in \\mathbb{R}^{D \\times N \\times M}$, which represents the spatial-temporal correlations of M IMFs across N wind turbines in D dimensions."}, {"title": "Spatial-Weather Embedding", "content": "Spatial-Weather Embedding: As shown in Figure 2(b), we design Spatial-Weather Embedding (SWE) to capture the impact of weather features on wind power generation within a spatial context. We superimpose the spatial embedding and weather embedding outputs to obtain the SWE output $\\mathbf{e_{SW}} = (\\mathbf{e_S} + \\mathbf{e_W}) \\in \\mathbb{R}^{D \\times N \\times C\u2019}$, which represents the spatial-temporal correlations of C weather features across N turbines in D dimensions."}, {"title": "Hybrid Frequency Feature Enhancement Block", "content": "As shown in Figure 2(c), HFF Enhancement block has two attention modules, where the Frequency Attention module captures the correlations between IMFs and wind power generation within a spatial context, and Feature Attention module captures the correlations between weather features and wind power generation in a spatial context. Additionally, we design a Correlation Determining Gate (CD Gate) to adaptively integrate the correlations from these attention mechanisms. We provide the details of these modules below.\nAs shown in Figure 2(a), we define the input of encoder as $\\mathbf{H}^{(0)}$, the number of encoder layers as L and the input of l-th HFF Enhancement Block as $\\mathbf{H}^{(l)}$. The hidden state for vertex v at correlation dimension d is represented as $\\mathbf{h}_{v,d}^{(l)}$ and $\\mathbf{H}^{(l)} = \\{\\{\\mathbf{h}_{1,d}^{(l)}\\}, \\{\\mathbf{h}_{2,d}^{(l)}\\},..., \\{\\mathbf{h}_{N,d}^{(l)}\\}\\} \\in \\mathbb{R}^{D \\times N}$, where $\\mathbf{h}^{(l)} = \\{\\mathbf{h}_{v,d}^{(l)}\\}$ is the hidden state set, $v \\in V$ is the vertex, N is the number of turbines and D is the number of correlation dimensions. The outputs of Frequency Attention and Feature Attention mechanisms in the l-th block are denoted as $\\mathbf{H}_{ST}^{(l)} \\in \\mathbb{R}^{D \\times N}$ and $\\mathbf{H}_{SW}^{(l)} \\in \\mathbb{R}^{D \\times N}$, respectively, We also denote hidden states of $\\mathbf{H}_{ST}^{(l)}$ and $\\mathbf{H}_{SW}^{(l)}$ for vertex v at correlation dimension d as $h_{ST,v,d}^{(l)}$ and $h_{SW,v,d}^{(l)}$, respectively. After CD Gate, we obtain the output of HFF Enhancement Block $\\mathbf{H}^{(l)} \\in \\mathbb{R}^{D \\times N}$."}, {"title": "Frequency Attention", "content": "Frequency Attention: Due to the uncertainty of wind power, we designed a Frequency Attention mechanism to capture the dynamic spatial-temporal correlations between wind power and IMFs. The hidden state $h_{ST,v,d}^{(l)}$ for vertices $v \\in V$ at correlation dimension $d \\in \\{1,2,..., D\\}$ is denoted as:\n$h_{ST,v,d}^{(l)} = \\sum_{v_i \\in V} a_{ST, v_i, v} h_{v,d\u2019}^{(l-1)},$ (3)\nwhere $a_{ST,v_i,v}$ is the attention score indicating the relevance of vertex $v_i$ to v and the summation of attention scores is 1. Next, we use the no-masked multi-head attention to calculate the attention scores of K attention heads in parallel. Unlike masked multi-head attention, this approach can globally capture the correlations between IMFs and wind power (Liu et al. 2023). Specifically, as shown in the following equations, we use the dot-product method to compute the relationship between vertices $v_i$ and v, and then normalize these scores using the softmax method.\n$Z_{ST,v_i,v}^{(k)} = \\frac{f_t^{(k)} (\\mathbf{h}_{ST,v,d}^{(l-1)} \\|\\| e_{ST}), f_t^{(k)} (\\mathbf{h}_{ST,v_i,d}^{(l-1)} \\|\\| e_{ST}))}{\\sqrt{\\zeta}},$ (4)\n$a_{ST,v_i,v}^{(k)} = \\frac{\\exp(Z_{ST,v_i,v}^{(k)})}{\\sum_{v_i \\in V} \\exp(Z_{ST,v_i,v}^{(k)})},$ (5)\nwhere $\\| \\cdot \\|$ represents the concatenation operation and $\\langle \\cdot, \\cdot \\rangle$ denotes the inner product operator, $Z_{ST,v_i,v}^{(k)}$ is the spatial correlation between wind power modes in kth attention head, $\\sqrt{\\zeta}$ is the scaling factor and $\\zeta = D/K$, where D is the number of correlation states and K is the number of attention heads. We then update the hidden state of each turbine at each dimension as:\n$h_{ST,v,d}^{(l)} = \\sum_{k=1}^K \\sum_{v_i \\in V} a_{ST,v_i,v}^{(k)} f_t^{(k)} (\\mathbf{h}_{ST,v,d}^{(l-1)}),$ (6)\nwhere $\\parallel_{k=1}^K$ indicates the concatenation for each k from 1 to K, $f_t^{(k)}(\\cdot)$, $f_t^{(2)}(\\cdot)$ and $f_t^{(3)}(\\cdot)$ represent three different nonlinear projections as:\n$f(x) = \\text{Gelu}(Bx + b).$ (7)\nNote that $B \\in \\mathbb{R}^{D \\times 1}$ and $b \\in \\mathbb{R}^{D \\times 1}$ are learnable parameters and Gelu is the activation function which performs best in Transformer models and avoids the vanishing gradient problem (Hendrycks and Gimpel 2016). Based on the result of Equation (6), the output of Frequency Attention module at the lth encoder layers is $\\mathbf{H}_{ST}^{(l)} = \\{\\{h_{ST,1,d}^{(l)}\\}, \\{h_{ST,2,d}^{(l)}\\},..., \\{h_{ST,N,d}^{(l)}\\}\\} \\in \\mathbb{R}^{D \\times N}$."}, {"title": "Feature Attention", "content": "Feature Attention: Note that the weather features collected by different turbines are influenced by their respective locations. Therefore, we designed the Feature Attention module to capture the spatial-temporal correlations between weather features and wind power generation. Specifically, For vertices $v \\in V$ at correlation state dimension $d \\in \\{1,2,..., D\\}$, we denote the hidden state $h_{SW,v,d}^{(l)}$ as:\n$h_{SW,v,d}^{(l)} = \\sum_{v_i \\in V} a_{SW, v_i, v} h_{v,d\u2019}^{(l-1)},$ (8)\nwhere $a_{SW,v_i,v}$ is the attention score indicating the relevance of vertex $v_i$ to v and the summation of attention scores is 1. Similar to Equations (4) and (5), we define the no-masked multi-head attention score $a_{SW,v_i,v}^{(k)}$, which characterizes the relationship between vertex $v_i$ and v for the kth attention head, as follows:\n$Z_{SW,v_i,v}^{(k)} = \\frac{f_t^{(k)} (\\mathbf{h}_{SW,v,d}^{(l-1)} \\|\\| e_{SW}), f_t^{(k)} (\\mathbf{h}_{SW,v_i,d}^{(l-1)} \\|\\| e_{SW}))}{\\sqrt{\\zeta}},$ (9)\n$a_{SW,v_i,v}^{(k)} = \\frac{\\exp(Z_{SW,v_i,v}^{(k)})}{\\sum_{v_i \\in V} \\exp(Z_{SW,v_i,v}^{(k)})},$ (10)\nwhere $\\| \\cdot \\|$ represents the concatenation operation and $\\langle \\cdot, \\cdot \\rangle$ denotes the inner product operator, $Z_{SW,v_i,v}^{(k)}$ is the spatial correlation between weather features in kth attention head, $\\sqrt{\\zeta}$ is the scaling factor and $\\zeta = D/K$. We then update the hidden state of each vertex at each dimension as:\n$h_{SW,v,d}^{(l)} = \\sum_{k=1}^K \\sum_{v_i \\in V} a_{SW,v_i,v}^{(k)} f_t^{(k)} (\\mathbf{h}_{SW,v,d}^{(l-1)}),$ (11)\nwhere $f_t^{(1)}(\\cdot)$, $f_t^{(2)}(\\cdot)$ and $f_t^{(3)}(\\cdot)$ represent three different nonlinear projections, similar to Equation (8). Finally, the output of Feature Attention module at the lth encoder layer is $\\mathbf{H}_{SW}^{(l)} = \\{\\{h_{SW,1,d}^{(l)}\\}, \\{h_{SW,2,d}^{(l)}\\},..., \\{h_{SW,N,d}^{(l)}\\}\\} \\in \\mathbb{R}^{D \\times N}$."}, {"title": "Correlation Determining Gate (CD Gate)", "content": "Correlation Determining Gate (CD Gate): The IMFs obtained by VMD contain only part of the temporal information in wind power (Dragomiretskiy and Zosso 2013). Simple fusion of $\\mathbf{H}_{ST}^{(l)}$ and $\\mathbf{H}_{SW}^{(l)}$ will produce suboptimal results (Wang and Luo 2023). Therefore, we propose a gated fusion technology to adaptively fuse the spatial-temporal correlations of wind power and weather features across different IMFs. The output of CD Gate module in the l-th encoder layer is:\n$\\rho = \\sigma(B_{\\rho,1} \\mathbf{H}_{ST}^{(l)} + B_{\\rho,2} \\mathbf{H}_{SW}^{(l)} + b),$ (12)\n$\\mathbf{H}^{(l)} = \\rho \\mathbf{H}_{ST}^{(l)} + (1 - \\rho) \\mathbf{H}_{SW}^{(l)}$ (13)\nwhere $B_{\\rho,1} \\in \\mathbb{R}^{D \\times D}$, $B_{\\rho,2} \\in \\mathbb{R}^{D \\times D}$ and $b \\in \\mathbb{R}^{D \\times N}$ are learnable parameters, $\\odot$ represents the element-wise product and $\\sigma(\\cdot)$ is the sigmoid activation function."}, {"title": "Add&Norm layer", "content": "Add&Norm layer\nTo increase the convergence and training stability of deep networks (Ba, Kiros, and Hinton 2016), we apply normalization to the output of HFF Enhancement block and Feed-Forward layer."}, {"title": "Feed-Forward", "content": "Feed-Forward\nWe use Feed-Forward network (FFN) to enhance Hiformer\u2019s performance of temporal correlation analysis, as commonly done in literature (Liu et al. 2023; Zeng et al. 2023). The FFN consists of fully-connected layers (FCs) for mapping (Wang and Luo 2023) and a dropout layer to prevent overfitting (Srivastava et al. 2014). The details are shown as below:\n$\\mathbf{H}_{F,out}^{(l)} = \\text{dropout} \\{FCs(\\mathbf{H}_{in}^{(l)}\\}\\}.$ (14)\nwhere $\\mathbf{H}_{F,out}^{(l)}$ and $\\mathbf{H}_{in}^{(l)}$ is the output and input of FFN at the l-th encoder layer, respectively."}, {"title": "Projection", "content": "Projection\nIn order to enhance the calculation speed and convergence rate, Hiformer employs Projection module (Liu et al. 2023) to decode the encoder\u2019s output $\\mathbf{H}^{(L)} \\in \\mathbb{R}^{D \\times N}$ and obtain the prediction results $\\mathbf{X} \\in \\mathbb{R}^{Q \\times N}$. The Projection module primarily consists of MLPs and is represented as follows:\n$\\mathbf{X} = \\text{MLPs}(\\mathbf{H}^{(L)}).$ (15)"}, {"title": "Experiments", "content": "In this section, we conduct empirical studies on Hiformer by applying it to two real-world wind power datasets, demonstrating its prediction accuracy and computational efficiency. Our experiments validated the effectiveness of integrating the signal decomposition technique with weather feature extraction technique."}, {"title": "Benchmarks", "content": "Benchmarks\nWe compare Hiformer with the following six Benchmark methods:\n\u2022 ITransformer (Liu et al. 2023): ITransformer adopts inverted attention mechanism and the feed-forward module to achieve long-term prediction.\n\u2022 WeaGAN (Wang and Luo 2023): WeaGAN uses Weather-Aware Graph to capture spatial-temporal correlations.\n\u2022 RLinear (Li et al. 2023): RLinear employs Linear Mapping to improve long-term forecasting.\n\u2022 D2STGNN (Shao et al. 2022): D2STGNN decouples the time series and proposes a dynamic graph learning module to capture spatial-temporal correlations.\n\u2022 Autoformer (Wu et al. 2021): Autoformer decomposes long time series using auto-correlation to enhance the performance in long-term prediction.\n\u2022 Informer (Zhou et al. 2021): Informer uses Generative Style Decoder to get all predictions in one step, which improves the transformer\u2019s long-term prediction accuracy and computational efficiency."}, {"title": "Datasets", "content": "We evaluate the performance of Hiformer on two real-world public wind power datasets: (1) The SDWPF dataset captures various parameters from 134 wind turbines over a span of 430 days, with recordings taken every 10 minutes. The recorded parameters include active power (Patv), reactive power (Prtv), blade pitch angle (Pab), nacelle direction (Ndir), temperature inside the turbine nacelle (Itmp), environment temperature (Etmp), the angle between the wind direction and the nacelle position (Wdir), and wind speed (Wspd) measured by the anemometer (Zhou et al. 2022); (2) The GEFcom dataset records wind power and wind speed information for 10 zones every hour from 2012 to 2013. The wind speed data is measured at two heights: 10 meters and 100 meters above the ground level (Hong et al. 2016).\nAs it is common in the literature (Bubalo et al. 2023; Wang and Luo 2023), we use the Z-Score method to normalise the wind power and weather data and split the dataset into training, validation and test sets in a ratio of 7:1:2.\n$y_i = \\frac{x_i - \\overline{X}}{S}$ (16)\nwhere $y_i$ represents the normalised data, $x_i$ is the original data, $\\overline{X}$ is the sample mean and S is the sample standard deviation."}, {"title": "Experimental Settings", "content": "Experimental Settings\nWe have made the code publicly accessible on GitHub anonymously and trained the models using an Intel(R) Xeon(R) Gold 6246R CPU @ 3.40GHz and GeForce RTX 3090 GPUs with PyTorch 1.13. The system is equipped with 128 GB of RAM.\nFor benchmarks, we use the default settings and best model hyperparameters as specified in their original proposals. We use the 2 days of historical data (P = 288 time steps for SDWPF and P = 48 time steps for GEFcom) to predict the wind power for the next $Q \\in \\{6,36, 72, 144, 288\\}$ time steps in SDWPF and $Q \\in \\{1, 6, 12, 24, 48\\}$ time steps in GEFcom, following common practices in the literature (Ma, Huang, and Shi 2023; Qiu et al. 2024). We train our model using the Adam optimizer (Kingma and Ba 2014) with an initial learning rate of 0.001. The batch size is set to 64, and the training runs for 100 epochs. The encoder comprises"}, {"title": "Experimental Results", "content": "L = 5 layers", "models": "Mean Absolute Error (MAE) and Mean Squared Error (MSE) (Wang and Luo 2023). The equations are as follows:\n$M A E=\\frac{1"}, {"Comparison": "Table 1 shows the prediction results with the best in bold and the second underlined. As we can see, Hiformer achieves the best prediction results across all time periods, especially for the 2-day prediction length (288 steps in SDWPF and 48 steps in GEFcom). Compared to the second-best method, Hiformer shows improvements of 38.6% and 52.5% in MAE and MSE, respectively, over the 2-day horizon in the SDWPF dataset, and 27.3% and 28.6% in MAE and MSE, respectively, over the same horizon in the GEFcom dataset.\nTo better demonstrate the performance of Hiformer, we visualized the prediction results from two datasets spanning August 7 to August 9. As shown in Figure 4(a), most methods capture the general trend of wind power but struggle with its random fluctuations. This difficulty arises from their inability to account for the strong uncertainty in wind power cased by weather conditions. In contrast, Hiformer effectively captures these fluctuations through the HFF Enhancement block.\nFigure"}]}