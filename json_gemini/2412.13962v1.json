{"title": "Threshold UCT: Cost-Constrained Monte Carlo Tree Search with Pareto Curves", "authors": ["Martin Kure\u010dka", "V\u00e1clav Nevyho\u0161t\u011bn\u00fd", "Petr Novotn\u00fd", "V\u00edt Un\u010dovsk\u00fd"], "abstract": "Constrained Markov decision processes (CMDPs), in which the agent optimizes expected payoffs while keeping the expected cost below a given threshold, are the leading framework for safe sequential decision making under stochastic uncertainty. Among algorithms for planning and learning in CMDPs, methods based on Monte Carlo tree search (MCTS) have particular importance due to their efficiency and extendibility to more complex frameworks (such as partially observable settings and games). However, current MCTS-based methods for CMDPs either struggle with finding safe (i.e., constraint-satisfying) policies, or are too conservative and do not find valuable policies. We introduce Threshold UCT (T-UCT), an online MCTS-based algorithm for CMDP planning. Unlike previous MCTS-based CMDP planners, T-UCT explicitly estimates Pareto curves of cost-utility trade-offs throughout the search tree, using these together with a novel action selection and threshold update rules to seek safe and valuable policies. Our experiments demonstrate that our approach significantly outperforms state-of-the-art methods from the literature.", "sections": [{"title": "1 Introduction", "content": "Safe Decision Making and MCTS Monte-Carlo tree search (MCTS) has emerged as the de-facto method for solving large sequential decision making problems under uncertainty (Browne et al. 2012). It combines the scalability of sampling-based methods with the robustness of heuristic tree search, the latter feature making it easily extendable to settings with partial observability (Silver and Veness 2010), multiple agents (Silver et al. 2018), or settings with history-dependent optimal decisions (Chatterjee et al. 2018). While MCTS-based methods demonstrated remarkable efficiency in optimizing the agent's performance across diverse domains, the deployment of autonomous agents in real-world domains necessitates balancing the agent performance with the safety of their behavior. In AI planning and reinforcement learning, the standard way of modeling safety issues is via the constrained decision-making framework. Here, apart from the usual reward signals, the agents are also collecting penalties (or costs), and the objective is to maximize the expected accumulated reward under the constraint that the expected accumulated cost is below a given threshold \u0394. Compared with ad-hoc reward shaping, the constrained approach provides explicit and domain-independent way of controlling agent safety. Hence, safe and efficient probabilistic planning (and indeed, also model-free reinforcement learning, where algorithms such as MuZero (Schrittwieser et al. 2020) are built on top of efficient MCTS planners) necessitates the development of stable and sample-efficient cost-constrained MCTS algorithms.\nKey Components of Constrained MCTS An efficient constrained MCTS-based algorithm must be able to identify safe and valuable policies.\nFinding safe policies (i.e., those that do not exceed the cost threshold) requires identifying \"dangerous\" (in terms of future cost) decisions and keeping track of cost risk accumulated in stochastic decisions: a 50/50 gamble which incurs cost C if lost contributes at least C/2 towards the expected cost of the policy irrespective of the gamble's outcome.\nAn agent which never moves might be safe but never does anything useful. To identify reward-valuable policies among the safe ones, the algorithm must not be constrained beyond the requirements given by the threshold A and hence it must be able to reason about the trade-off between rewards and costs during both tree search and actual action selection.\nLimitations of previous approaches Two prominent examples of constrained MCTS-based algorithms are CC-POMCP (Lee et al. 2018) and RAMCP (Chatterjee et al. 2018). While these algorithms represented significant steps towards practical constrained decision making, they exhibit fundamental limitations in identifying both safe and valuable policies.\nSafety limitations: A usual way of tracking the cost risk is updating the current threshold \u25b3 appropriately after each decision. As we discuss in Section 3, both CC-POMCP and RAMCP perform this update in an unsound manner and might thus produce policies violating the cost constraint even if a safe policy exists within the explored part of the tree.\nValue limitations: Both CC-POMCP and RAMCP compute randomized policies, which are necessary for optimality in constrained decision-making scenarios (Altman 1999). However, their reasoning about the reward-cost payoff is incomplete. RAMCP does not use the cost information during"}, {"title": "2 Preliminaries", "content": "We denote by D(X) the set of all probability distributions over a finite support X. We formalize the constrained decision making problem via the standard notion of constrained Markov decision processes (CMDPs).\nDefinition 1. A constrained Markov decision process (CMDP) is a tuple C = (S, A, \u03b4, r, c, so) where:\n\u2022 S is a finite set of states,\n\u2022 A is a finite set of actions,\n\u2022 \u03b4: S \u00d7 A \u2192 D(S) is a probabilistic transition function; we abbreviate d(s, a)(t) to d(t | s, a),\n\u2022 r: S \u00d7 A \u00d7 S \u2192 R is a reward function,\n\u2022 c: S \u00d7 A \u00d7 S \u2192 R is a cost function, and\n\u2022 S0 \u2208 S is the initial state\nCMDP dynamics CMDPs evolve identically to standard MDPs. A history is an element of (SA)*S, i.e., a finite alternating sequence of states and actions starting and ending in a state. A policy is a function assigning to each history a distribution over actions.\nUnder a given policy \u03c0, a CMDP evolves as follows: we start in the initial state; i.e., the initial history is ho = s0. Then, for every timestep i \u2208 {0, 1, 2, . . . }, given the current history hi = s0A0S1A1...Si-1Ai-1Si, the next action a\u017c is sampled from \u03c0: \u03b1\u00a1 ~ \u03c0(hi). The next state si+1 is sampled according to the transition function, i.e, si+1 ~ \u03b4(Si, ai). Then, the agent obtains the reward r(si, Ai, Si+1) and incurs the cost c(Si, Ai, Si+1). The current history is updated to hi+1 = 80A0S1A1...Si\u22121Ai\u22121SiAiSi+1 and the process continues in the same fashion ad infinitum.\nWe denote by PT (E) the probability of an event E under policy \u03c0, and by E\u2122 [X] the expected value of a random"}, {"title": "3 Threshold UCT", "content": "We propose a new algorithm for CMDP planning, Threshold UCT (T-UCT). Like many other MCTS-based algorithms, T-UCT only requires access to a generative simulator of the underlying CMDP, i.e., an algorithm allowing for an efficient sampling from d(-|s, a), given (s, a); and providing r(s, a, s') and c(s, a, s') for given (s, a').\nHistory-action values, feasibility We consider payoffs achievable by a policy after witnessing a history h and possibly also playing action a:\n$Payoff_\\pi(h) = \\mathbb{E}^\\pi \\Big[\\sum_{i=|h|}^{T-1} \\gamma_r^{i-|h|} r(s_i, a_i, s_{i+1}) \\Big| h\\Big],$\n$Payoff_\\pi(h, a) = \\mathbb{E}^\\pi \\Big[\\sum_{i=|h|}^{T-1} \\gamma_r^{i-|h|} r(s_i, a_i, s_{i+1}) \\Big| h, a\\Big],$\nwhere (h) is a condition of producing history h in the first |h| steps and (h, a) is a condition that a is played immediately after witnessing h. The quantities Cost(h) and Cost(h, a) are defined analogously. We say that \u03c0 is \u0394-feasible from h if Cost(h) < \u0394.\nAchievable vectors A vector (c,r) \u2208 R \u00d7 R is achievable from history h if there exists a policy \u03c0 such that Cost(h) < c and Payoff (h) \u2265 r. Similarly, we say that (c, r) is achievable from (h, a) if Cost(h, a) \u2264 c and Payoff (h, a) > r for some \u03c0.\nWe write (c', r') \u2264 (c,r) if c > candr' < r. A <-closure of a set X CRX R is the set of all vectors (c', r') \u2208 R \u00d7 R s.t. (c', r') \u2264 (c, r) for some (c, r) \u2208 X.\nPareto sets A Pareto set of history h is the set of all vectors achievable from h, while the Pareto set of (h, a) is the set of all vectors achievable from (h, a).\nIt is known (Chatterjee, Forejt, and Wojtczak 2013; Barrett and Narayanan 2008) that the Pareto sets are (i) convex (since we allow randomized policies), and (ii) <-closed (i.e., if (c, r) belongs to the set and (c', r') \u2264 (c, r), then (c', r') also belongs to the set). From (ii) it follows that a Pareto set is wholly determined by its Pareto curve, i.e. the set of all points maximal w.r.t. the <-ordering. Furthermore, in finite MDPs, the Pareto curve is piecewise linear, with finitely many pieces. The whole Pareto set can then be represented by a finite set of vertices, i.e., points in which the piecewise-linear curve changes its slope; indeed, the Pareto set is the <-closure of the convex hull of the set of vertices. In what follows, we denote by P(h) and P(h, a) these finite representations of the Pareto sets of h and (h, a), respectively.\nBellman equations for Pareto sets Pareto sets in CMDPs obey local optimality equations akin to classical unconstrained MDPs. To formalize these, we need additional notation. The sum X + Y is the standard Minkowski sum of the sets of vectors. For a vector (a, b) we define X \u00b7 (a, b) = {(xa, y \u00b7 b) | (x, y) \u2208 X}, with X \u00b7 a as a shorthand for X \u00b7 (a, a). It is known (Barrett and Narayanan 2008; Chen et al. 2013) that for the finite-vertex representation of Pareto sets it holds:\n$P(h) = \\text{prune} \\Big(\\bigcup_{a \\in A} P(h, a) \\Big)$\n$P(h, a) = \\text{prune} \\Big(\\sum_{t \\in S} \\delta(t|h, a) (P(hat) \\cdot (\\gamma_c, \\gamma_r) + \\{(c(h, a, t), r(h, a,t))\\})\\Big),$\nwhere the prune operator removes all points that are <-dominated by a convex combination of some other points in the respective set.\nT-UCT: Overall structure T-UCT is presented in Algorithm 1. It follows the standard Monte Carlo tree search (MCTS) framework, with blue lines highlighting parts that conceptually differ from the setting with unconstrained payoff optimization (the whole procedure GetActionDist is constraint-specific, and hence we omit its coloring). The algorithm iteratively builds a search tree whose nodes represent histories of the CMDP, with child nodes of h representing one-step extensions of h. Each node stores additional information, in particular the estimates of P(h) and P(h, a), denoted by P(h) and P(h, a) in the pseudocode. The tree structure is global to the whole algorithm and not explicitly pictured in the pseudocode.\nThe algorithm uses transition probabilities \u03b4 of the CMDP. If these are not available (e.g., when using a generative model of the CMDP), we replace \u03b4 with a sample"}, {"title": "4 Experiments", "content": "Baseline Algorithms We compare T-UCT to two state-of-the-art methods for solving CMDPs: CC-POMCP (Lee et al. 2018) and RAMCP (Chatterjee et al. 2018).\nCC-POMCP is a dual method based on solving the Lagrangian relaxation of the CMDP objective:\n$\\min_{\\lambda \\geq 0} \\max_{\\pi} Payoff_\\pi + \\lambda \\cdot (\\Delta - Cost_\\pi).$\nCC-POMCP performs stochastic gradient descent on \u03bb while continuously evaluating (9) via the standard UCT search. For a fixed \u5165, the maximization of (9) yields a point on the cost-payoff Pareto frontier, where a larger value of \u5165 induces more cost-averse behavior. A caveat of the method is its sample inefficiency when A converges slowly.\nRAMCP is a primal method combining MCTS with linear programming. The search phase of RAMCP greedily optimizes the payoff in a standard single-valued UCT fashion, completely ignoring the constraint. Consequently, the cost is considered only in the second part of the action selection phase, where the algorithm solves a linear program to find a valid probability flow through the sampled tree (which serves as a local approximation of the original CMDP) such that the expected cost under the probability flow satisfies the constraint and the payoff is maximized.\nBenchmarks We evaluated the algorithms on three tasks: two of them are variants of the established Gridworld benchmark (Br\u00e1zdil et al. 2020; Undurti and How 2010); the third is based on the Manhattan environment (Blahoudek et al. 2020), where the agent navigates through mid-town Manhattan, avoiding traffic jams captured by a stochastic model.\nTask: Avoid The setup (see Figure 1a) involves navigating the agent (robot) through a grid-based maze with four permissible actions: moving left, right, down, or up. The agent's movements are subject to stochastic perturbations: it can be displaced in a direction perpendicular to its intended movement with probability Pslide. The agent's objective is to visit special \"gold\" tiles to receive a reward while avoiding \"trap\" tiles, which, with probability Ptrap, incur a cost of 1 and terminate the environment (the agent suffers fatal damage).\nThe task is evaluated on a set of 128 small maps (6 \u00d7 6 grid, 5 gold, approx. 103 states) and 64 large maps (25 \u00d7 25"}, {"title": "5 Conclusion and Future Work", "content": "We introduced Threshold UTC, a MCTS-based planner for constrained MDPs utilizing online estimates of reward-cost trade-offs in search for safe and valuable policies. We presented an experimental evaluation of our approach, showing that it outperforms competing cost-constrained tree-search algorithms. Our aim for future work is to augment T-UCT with function approximators of Pareto sets, thus obtaining a general MCTS-based reinforcement learning algorithm for cost-constrained optimization."}, {"title": "A Appendix", "content": "A.1 Necessity to Consider the Action Outcome\nCMDP A in Figure 4 with \u2206 = 0.5 and yc = 1 witnesses the unsoundness of CC-POMCP. The update rule of CC-POMCP does not take the final state into account as it essentially updates the threshold to the value \u2206act in T-UCT algorithm plus adjusts for the immediate cost and the discount factor yc. Especially, if CC-POMCP deterministically selects an action a, the threshold update only considers the immediate cost and the discount factor \u03b3c, i.e.,\n$\\Delta \\leftarrow \\frac{\\Delta - c(h, a)}{\\gamma_c}.$\nIn the case of CMPD A, the agent keeps the threshold at 0.5 regardless of whether the final state of action a\u2081 is s2 or s3. From state s2 onwards, the agent incurs an expected cost and reward of 0.5. However, from state s3, it always incurs a cost of 1. Therefore, the total expected cost of CC-POMCP from so is 0.75, which violates the constraint.\nA.2 Proofs\nIn what follows, we fix a CMDP C with dynamics d, a length of the horizon T we optimize over, and the initial threshold A. We further use \u03c3\u2070, . . ., \u03c3T\u22121 to denote the random sequence of single-step policies computed by T-UCT on line 9 of Algorithm 1, and \u03c0T\u2212UCT to denote the policy which is the concatenation of the single-step policies. The sequence \u2206\u2070, ..., \u2206T-1 denotes the random thresholds computed by T-UCT with \u2206\u2070 = \u2206, and \u00ce\u00ba, . . ., \u015cT-1 denotes the random sequence of estimates of d before individual steps. Additionally, H denotes the set of all histories of length at most T reachable from the initial state with a positive probability. Finally, we use Es, to denote the expected value under particular dynamics \u03b4'.\nWe define a threshold \u2206' to be \u00ce\u00ba-feasible from a history h if it is feasible according to the Pareto set estimates of T-UCT at time t. Since the Bellman equations (1) and (2) compute precise Pareto sets if the environment follows the dynamics \u1ed5i, the threshold \u2206' is \u00ce\u00ba-feasible iff there exists a policy that yields Cost at most \u2206' under the dynamics \u1ed5i.\nWe begin by proving the property (7):\nLemma 1. Let h \u2208 H be a history of length i. The following holds:\n$\\Delta^{i} \\geq \\mathbb{E}_{\\hat{\\delta}^{i}} [c(h, a, s) + \\gamma_c \\cdot \\Delta^{i+1} | \\sigma^{i}].$\nProof. For brevity, we use \u03c3 := \u03c3\u2070, and \u2642 := \u00ce\u00ba. We first show that E[\u2206act | \u03c3] = \u2206i. If o is a deterministic policy, then T-UCT sets Aact = A by the definition of Aact, and the claim follows. Otherwise, \u2206act is equal to c\u0105 or ch with probability \u03c3\u03b9 or \u03c3\u03b7, respectively, where the probabilities are computed precisely to satisfy \u0394i = \u03c3\u0131c\u0131 + \u03c3\u03b7ch. It follows that E[\u2206act | \u03c3] = \u0394i. Therefore, it is enough to show that for every a in the support of o\u00b2, we have\n$\\Delta_{act} \\geq \\mathbb{E}_{\\hat{\\delta}} [c(h, a, s) + \\gamma_c \\cdot \\Delta^{i+1} | a].$\nIn the \u201cmixing\u201d case, the new threshold \u2206i+1 is equal to one of cs from equation (3), which are chosen so as to satisfy (10). In the \"surplus\u201d case, o is the deterministic distribution corresponding to the choice of action a, \u2206act is equal to Ai, and Ai+1 is set to\n$c_s = \\frac{B - c(h, a)}{E_S(h, a) + \\gamma_c B} c_{max}$"}]}