{"title": "Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning", "authors": ["Tianyi Wu", "Jingwei Ni", "Bryan Hooi", "Jiaheng Zhang", "Elliot Ash", "See-Kiong Ng", "Mrinmaya Sachan", "Markus Leippold"], "abstract": "Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language Models (LLMs), but may also lower their truthfulness. This trade-off arises because IFT steers LLMs to generate responses with long-tail knowledge that is not well covered during pre-training, leading to more informative but less truthful answers when generalizing to unseen tasks. In this paper, we empirically demonstrate this helpfulness-truthfulness trade-off in IFT and propose UNIT, a novel IFT paradigm to address it. UNIT teaches LLMs to recognize their uncertainty and explicitly reflect it at the end of their responses. Experimental results show that UNIT-tuned models maintain their helpfulness while distinguishing between certain and uncertain claims, thereby reducing hallucinations.", "sections": [{"title": "1 Introduction", "content": "In general-purpose alignment, LLM helpfulness is typically defined as \u201cproviding a clear, complete, and insightful response with valuable additional details.\u201d  Prior work has demonstrated that it is possible to achieve generalizable helpfulness using carefully collected high-quality IFT data . However, the responses of these helpfulness-purposed IFT data may contain informative details that are not well covered during pre-training. This knowledge gap between pre-training and fine-tuning may encourage LLM to generate informative but inaccurate answers when generalizing to unseen tasks, inducing hallucinations ."}, {"title": "2 RQ1: Helpfulness-Truthfulness Trade-Off in IFT", "content": "In this section, we first introduce our evaluation and training settings (\u00a7 2.1). Next, we describe the IFT data constructions for exploring RQ1 (\u00a7 2.2). Finally, we present the experimental results and key takeaways (\u00a7 2.3)."}, {"title": "2.1 Evaluation and Training Details", "content": "Truthfulness. We use FactScore  and WildFactScore  to fact-check atomic claims in LLMs' long-form outputs. FactScore prompts LLMs to generate 500 Biographies (Bio), while WildFactScore prompts to introduce 7K entities absent from Wikipedia."}, {"title": "2.2 Training Data Construction", "content": "To investigate RQ1, we experiment on four IFT data constructions: (1) LFRQA : it contains diversified instructions, human-written long-form responses with plenty of niche knowledge that requires retrieval augmentation. By adjusting the amount of LFRQA data (10% to 100%), we can control the amount of unfamiliar knowledge in IFT. (2) LFRQAcertain: as a contrastive experiment, we remove all \u201cunfamiliar\" knowledge from LFRQA responses, resulting in LFRQAcertain. The data construction process is illustrated in Fig. 2 (Steps 1 to 3): we first break down the responses in LFRQA into atomic claims using GPT-40. Second, we leverage Claim Conditioned Probability (CCP, , a SOTA claim-level uncertainty measurement) to probe the LLM's uncertainty on each claim. Finally, we concatenate the model's certain claims into new responses, using GPT-40. (3) LFRQA+LIMA and (4) LFRQA+LIMAcertain: We add LIMA (a more helpful IFT dataset) to LFRQA and LFRQAcertain to enhance helpfulness, thereby investigating the helpfulness-truthfulness trade-off. For LFRQA+LIMAcertain, we only modify the information-seeking data points and keep others (creative writing, coding, etc.) unchanged. For how we classify information-seeking prompts, dataset statistics, and other details, see App. D."}, {"title": "2.3 Experiment Results", "content": "Truthfulness and helpfulness scores for all IFT settings are presented in Fig. 3. From these results, we draw the following conclusions:\nIFT on more unfamiliar knowledge encourages hallucination. As the proportion of LFRQA data increases from 10% to 100%, both datasets exhibit a clear downward trend in truthfulness (see - and ). In contrast, removing unfamiliar knowledge (-- and -- ) leads to improving truthfulness with increasing data amount.\nTruthfulness comes at the cost of helpfulness, and vice versa. Comparing to - and to-x- shows that removing unfamiliar knowledge to enhance truthfulness lowers helpfulness. Conversely, comparing to and -to-x- reveals that adding LIMA raises overall helpfulness but reduces truthfulness.\nStatistical Significance. We conduct the Wilcoxon Signed-Rank Test  to confirm the statistical significance of our observations. The p-values are shown in Table 1.\nTakeaway. Unfamiliar knowledge in IFT leads to OOD hallucinations, however, it also teaches LLM to generate rich and in-depth answers that enhance helpfulness. Hence, balancing truthfulness and helpfulness is challenging."}, {"title": "3 RQ2: Balancing Helpfulness and Truthfulness with UNIT", "content": "To preserve helpfulness while enhancing trustworthiness, we propose UNIT, an IFT paradigm that fine-tunes LLMs to first generate a helpful answer and then explicitly express uncertainty. Specifically, UNIT modifies human-written IFT data (e.g., LIMA, LFRQA) by appending a \u201creflection\" section to the end of each original response, reflecting on the knowledge that the model is uncertain about. As illustrated in Fig. 2, this data construction pipeline shares steps 1 and 2 with the LFRQA(+LIMA)certain preparation \u2013 measuring uncertainty with CCP and categorizing claims as"}, {"title": "3.1 UNIT Evaluation Metrics", "content": "Truthfulness and Helpfulness. UNIT has a heavier learning burden than vanilla IFT as it requires the model to learn both instruction-following and uncertainty reflection. To evaluate any resulting trade-off, we measure the helpfulness and truthfulness of the answer part of the UNIT-tuned models with the \"reflection\" part removed. Same metrics are used as \u00a7 2.1.\nCCP Balanced Accuracy. Since UNIT aims to teach the model to recognize and explicitly label uncertainty, we assess whether uncertain claims are correctly placed in the \u201creflection\u201d while certain claims are left unreflected. We define CCP Balanced Accuracy as:\n$CCP \u0412.\u0410. = \\frac{1}{2} (\\frac{|UC_{reflected}|}{|U_{Call}|} + \\frac{|CC_{unreflected}|}{|CCall|})$\nwhere $UC_{reflected}$ is the number of reflected uncertain claims, $|U_{Call}|$ is the total number of uncertain claims, $|CC_{unreflected}|$ is the number of unreflected certain claims, and $|CCall|$ is the total number of certain claims. Here, \"uncertain\u201d and \u201ccertain\" are determined by the CCP threshold (75th percentile) used during training.\nCCP Difference. Besides learning to classify uncertain claim by a threshold, the model could learn to rank claims by their CCP scores. To assess this behavior, we compute the difference in the mean CCP of reflected claims versus that of unreflected claims. A positive CCP Difference indicates that the model reflects more often on more uncertain claims than certain claims, and vice versa.\nHonesty Balanced Accuracy. To evaluate how reliably the model reflects factually incorrect claims while leaving correct claims unreflected, we compute Honesty Balanced Accuracy, it follows the same formula as CCP Balanced Accuracy but uses claim correctness as gold labels instead of CCP-based uncertainty. See App. A for more details."}, {"title": "3.2 Experiment Results", "content": "We compare UNIT with vanilla IFT in all combinations of LIMA & LFRQA in \u00a7 2. Results are presented in Table 2. Our key observations are:"}, {"title": "4 Related Work and Conclusion", "content": "Gekhman et al. (2024) studied that in short-form QA, overfitting LLMs on unknown QA pairs (e.g., training for 20+ epochs) can cause severe hallucinations, which can be mitigated by early stopping (e.g., under 5 epochs). In contrast, Zhao et al. (2024a) observe that helpfulness-purposed IFT does not degrade performance on factual-knowledge benchmarks. Our work shows that even in early epochs of IFT (only 3 epochs on diverse data), incorporating unfamiliar knowledge can still harm OOD truthfulness. To enhance honesty, prior work uses non-helpfulness-purposed data to improve LLM calibration , but fails to cover how to incorporate human-written helpful-purposed IFT data to preserve helpfulness. Hence, we propose UNIT to fill in this gap. UNIT also differs from prior work by using direct claim-level uncertainty  for honesty alignment, rather than using LLM answer correctness as a proxy for uncertainty."}, {"title": "Limitations", "content": "Limited Performance on Honesty Balanced Accuracy. Even with UNIT-tuning, Honesty Balanced Accuracy is only slightly above the random baseline (50%). To find the highest possible Honesty Balanced Accuracy using CCP, we calculate the test-time CCP of all claims and search for the best CCP threshold. Across all settings and datasets, the highest accuracy is around 62%, showing that achieving high accuracy is difficult even with perfect CCP ranking and thresholding. Therefore, we argue that UNIT performs reasonably well. Future improvements in uncertainty measurement  may further enhance its performance.\nUncertainty Threshold. We use the 75th quantile of training data CCP scores to distinguish certain and uncertain claims, which might not be optimal for all OOD test domains.One potential solution is to tune the CCP threshold using a validation set from the target domain. However, this would go against our goal of testing OOD generalization in IFT, so we did not do it. Our focus is to showcase the potential of dealing with uncertain knowledge during IFT. We leave the exploration of the optimal CCP threshold to future exploration.\nHelpfulness and Truthfulness on Information-Seeking Only. Our discussion of helpfulness and truthfulness is limited to information-seeking prompts because known vs. unknown on information-seeking is the most straightforwardly defined and the easiest to verify.. This limitation also exists in related work of uncertainty probing and alignment for honesty .\nLack of Experiments on Larger Models. Due to limited resources, we prioritize the depth of experiments (Llama-3.1-8B on various IFT settings) over the width of experiments (various sizes of models on fewer IFT settings). We leave the exploration of larger models to future work. We focus on an 8B model, which is the most vulnerable to hallucinations introduced by IFT, because (1) its performance often needs IFT improvement and (2) its size is friendly for practitioners to train or deploy."}, {"title": "Acknowledgements", "content": "This paper has received funding from the Swiss National Science Foundation (SNSF) under the project 'How sustainable is sustainable finance? Impact evaluation and automated greenwashing detection' (Grant Agreement No. 100018_207800). It is also funded by grant from Hasler Stiftung for the Research Program Responsible AI with the project \"Scientific Claim Verification.\""}, {"title": "Ethics Statement", "content": "Data Privacy or Bias. We use publically available IFT datasets which have no data privacy issues or bias against certain demographics. All artifacts we use are under licenses allowing research usage. We also notice no ethical risks associated with this work."}, {"title": "Reproducibility Statement", "content": "To ensure full reproducibility, we will disclose all codes and data used in this project, as well as the LLM generations. For OpenAI models, using gpt-40-2024-11-20 and gpt-4o-mini-2024-07-18 with random seed 42 will ensure reproducing the observations in paper, but not the exact numbers due to the poor reproducibility of OpenAI API."}, {"title": "A Evaluation Metrics Details", "content": "Truthfulness Score. We use the database and information retriever of FactScore  and WildFactScore  to conduct retrieval-augmented fact-checking. We follow Min et al. (2023) but replace gpt-3.5-turbo with gpt-40-mini for the evaluation model. The prompts for generating atomic claims and fact-checking are listed below."}, {"title": "B Experiment Implementation Details", "content": "B.1 Hyperparameter Settings\nFor experiments in this paper, we conducted full fine-tuning on Llama-3.1-8B  for 3 epochs with 2 NVIDIA H100-80GB. We utilized \"The Alignment Handbook\" code base released by Huggingface to fine-tune all the models . The configurations of our hyper-parameters are detailed in Table 4.\nWe used the default chat template in \"The Alignment Handbook\"  for fine-tuning all models, as illustrated below."}, {"title": "B.2 Inference", "content": "For our LLM inference tasks, we employ vLLM  with the following configuration: a temperature setting of 0, a repetition penalty of 1, and a maximum output of 2048 tokens."}, {"title": "B.3 Information-seeking Data Filtering", "content": "In downstream domains, task prompts can vary widely in nature, and not all are related to information-seeking tasks. For instance, prompts for creative writing or summarization may not require the model to generate factual claims that need verifiable support. Additionally, expressing uncertainty in such cases would be inappropriate, as these tasks are not grounded in objective truth. To minimize noise during data surgery, we employ GPT-40 to classify whether an instruction pertains to an information-seeking task. Data surgery is then applied exclusively to prompts identified as information-seeking, ensuring a more precise and targeted approach. We take the instruction classification prompt from  which is illustrated below in Figure 5.\nWe deemed the instruction to be \"information-seeking\" if only if the \"primary_tag\" is \"Information seeking\" and \"other_tags\" is empty; data surgery described in App. C is only conducted on \"information-seeking\" data points."}, {"title": "B.4 System Prompts", "content": "In fine-tuning, we used different system prompts for surgery and non-surgery data points. For surgery data points, we used the following system prompt:"}, {"title": "C Details and Examples of UNIT", "content": "UNIT (Uncertainty-aware Instruction Tuning), an IFT paradigm that fine-tunes LLMs to express their uncertainty after their response to a given prompt. We formulate UNIT in detail as below.\nFinding Unfamiliar Samples Given an instruction dataset, we first adopt Claim Conditioned Probability (CCP)  to measure the uncertainty of all the claims within the responses in the datasets. Specifically, given an instruction dataset containing N instruction-response pairs $D = \\{(I_i, R_i)\\}_{1}^{N}$ where each response is represented as $R_i = \\{X_{i,1}, X_{i,2},..., X_{i,n_i}\\};$ the CCP algorithm extracts a set of atomic factual claims from each response. We denote the set of claims extracted from $R_i$ as $C_i = \\{C_{i,1}, C_{i,2}, ..., C_{i,m_i} \\},$ with each $C_{i,j} \\subset R_i$ representing a coherent factual statement. For each token $x_{i,j}$ in a claim $C_{i,j},$ the target model $M$ samples the top-K alternatives\n$\\{X_{j}^{1}, ..., X_{j}^{K}\\}$\nwith probabilities $P(x_{j}^{k} | x_{i,<j})$ (where $x_{i,<j} = \\{X_{i,1},..., X_{i,j-1}\\})$. A natural language inference (NLI) model then evaluates each alternative $x_j^k$ by comparing the pair $(x_{i,<j} \\oplus x_j^k, x_{i,1:j})$ (with $x_{i,1:j} = x_{i,<j} \\oplus x_{i,j})$ and assigns one of three labels: entailment (e), contradiction (c), or neutral (n). The alternatives labelled as entailment form\n$M(x_{i,j}) = \\{x_j^k | NLI(x_j^k, x_{i,1:j}) = e\\},"}]}