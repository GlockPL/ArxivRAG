{"title": "FAIR OVERLAP NUMBER OF BALLS (FAIR-ONB): A DATA-MORPHOLOGY-BASED UNDERSAMPLING METHOD FOR BIAS REDUCTION", "authors": ["Jos\u00e9 Daniel Pascual-Triana", "Alberto Fern\u00e1ndez", "Paulo Novais", "Francisco Herrera"], "abstract": "Given the magnitude of data generation currently, both in quantity and speed, the use of machine learning is increasingly important. When data include protected features that might give rise to discrimination, special care must be taken. Data quality is critical in these cases, as biases in training data can be reflected in classification models. This has devastating consequences and fails to comply with current regulations. Data-Centric Artificial Intelligence proposes dataset modifications to improve its quality. Instance selection via undersampling can foster balanced learning of classes and protected feature values in the classifier. When such undersampling is done close to the decision boundary, the effect on the classifier would be bolstered. This work proposes Fair Overlap Number of Balls (Fair-ONB), an undersampling method that harnesses the data morphology of the different data groups (obtained from the combination of classes and protected feature values) to perform guided undersampling in the areas where they overlap. It employs attributes of the ball coverage of the groups, such as the radius, number of covered instances and density, to select the most suitable areas for undersampling and reduce bias. Results show that the Fair-ONB method reduces bias with low impact on the classifier's predictive performance.\nKeywords Trustworthy Artificial Intelligence Data-Centric Artificial Intelligence Bias\nReduction Undersampling Data Morphology.", "sections": [{"title": "1 Introduction", "content": "The vast amount of data generated in this digital era, ranging from simple sensor measurements to biometric data, makes harnessing the relevant information an impossible task unless some processes are automated. Machine Learning and Artificial Intelligence learn the inner patterns in data, making it possible for computers to predict the behaviour of unseen data.\nTo learn these patterns and, for instance, the classes in supervised classification problems, the characteristics of each dataset are crucial [10]. Models learned from poor-quality data do not reach high predictive performance and might draw unrealistic conclusions [22]. In particular, the presence of class overlap hinders the model's estimation of class boundaries, class imbalance can reduce the importance of some classes in the model (which complicates an equable predictive performance) and noisy samples can distort the predictions. Data-Centric Artificial Intelligence is the area of Artificial Intelligence whose aim is to obtain better models by improving data quality [6].\nWhen datasets include personal data or sensitive features that might give rise to discrimination, low data quality can lead to models that include discriminatory bias [12]. This is a critical problem nowadays, given the evolution of regulations in Artificial Intelligence, such as the recently approved European Union Artificial Intelligence Act4."}, {"title": "2 Preliminaries on Discrimination and Bias", "content": "The existence of discrimination amongst groups of people, that is, the treatment that inherently favours some of them, involves substantial ethical problems and is legally unacceptable. Thus, the use of model bias reduction techniques so that they comply with the current regulations [3] is a currently thriving research area. These biases can be measured in different ways, according to the situation [16, 12]. For example, it might be preferable that an equal rate of acceptance\nSAI Act, https://artificialintelligenceact.eu/"}, {"title": null, "content": "for a grant between men and women but, when assigning a medical procedure, the actual need for such treatment must also be taken into account.\nAccording to these preferences, there are multiple possible approaches in order to determine if a classifier is biased [13]. For example, in some cases the aim is to maintain demographic parity, or statistical parity, which involves the positive prediction being independent on the protected feature; that is, given a prediction \u00dd, demographic parity is measured according to how different the chances of obtaining a prediction \u0176=1 is for each value of protected feature P. This would create 2 similar metrics:\n\u2022 Statistical Parity Difference (SPD): it measures the difference between the probabilities of a positive clas-sification between two protected feature values (Equation 1). In the optimum case (equal chances), its value is 0.\nDPE = Pr(\u0176 = 1|P = 0) \u2013 Pr(\u0176 = 1|P = 1)\n\u2022 Disparate Impact (DI): it measures the ratio between the probabilities of a positive classification between two protected feature values (Equation 2). In the optimum case (equal chances), its value is 1. Given its extended use to check biases in the USA, this metric is more commonly applied.\nID =\nPr(\u0176 = 1|P = 0)\nPr(\u0176 = 1|P = 1)\nIn some cases, a different version of Disparate Impact is used, named Adapted Disparate Impact (ADI), whose values are between 0 and 1. Its formula is given in Equation 3.\nIDA =\n{\nID : ID < 1\n: ID>1\nID\nOther extended metrics would involve equality of probability and equality of opportunity, whose foci are the true positive and/or false positive rates being the same for different values of the protected feature. Their associated metrics are measured according to the difference in probabilities for each group, and are named Equal Probability Difference (EPD) and Equal Opportunity Difference (EOD), whose formulae are given in Equations 4 and 5, respectively.\nEPD = Pr(\u0176 = 1|Y, P = 0) \u2013 Pr(\u0176 = 1|Y, P = 1)\nEOD = Pr(\u0176 = 1|Y = 1, P = 0) \u2013 Pr(\u0176 = 1|Y = 1, P = 1)\nUnless they are based on the same types of probabilities (such as in SPD and DI), optimizing multiple bias metrics simultaneously is generally unfeasible; therefore, each study generally only selects one of them to measure fairness.\nHaving indicated multiple metrics that measure fairness, knowing how to reduce biases is also important. There are many techniques in that regard and, since this study is focused on the use of sampling, the following Scopus queries were used in order to check the strategies in that area:"}, {"title": "3 The Fair Overlap Number of Balls method", "content": "This Section describes in more detail the functioning of Fair-ONB, a novel undersampling method based on data morphology. This method eliminates samples from groups with favourable bias so that protected groups are treated similarly by the classifier. This is done in a guided manner, harnessing the morphology of the group coverage obtained from the Overlap Number of Balls algorithm and its ball properties in order to select problematic areas of the data space.\nFirstly, the ONB algorithm is explained in Section 3.1. Then, some preliminaries on the choices made during the method's creation are indicated in Section 3.2. Lastly, the structure and inner working of the method are explained in Section 3.3."}, {"title": "3.1 Overlap Number of Balls", "content": "Overlap Number of Balls is a complexity metric, that is, a technique that measures how difficult to classify a dataset is according to its inner characteristics [14]. ONB is based on the Pure Class Cover Catch Digraph (P-CCCD) ball coverage algorithm [11].\nTheir strategy is simple. Firstly, for each instance in the dataset, the biggest open ball (the interior of the hypersphere of the same radius) that does not cover any points of a different class is generated. Then, for each class, the balls that would cover the most points that are not yet covered by other balls in the coverage set are iteratively chosen as part of said coverage set, until all points of that class are covered."}, {"title": "3.2 Preliminaries on the Fair-ONB method", "content": "Given a dataset whose protected features and class are known, the combination of their values is employed to detect and correct biases. For the Fair-ONB method, each of these combinations forms a group. For example, a dataset with binary protected features \"race\" and \"gender\" and a binary class would have the groups shown in Table 1).\nThe aim is to see how those groups are distributed, since, depending on their overlap and their degrees of imbalance, the classifier learned using the dataset can be biased.\nAt the same time, before employing the method, the existence of a priori biases in the dataset or model is checked using a fairness metric. This allows the selection of which subgroups need to be preprocessed. In this study, said fairness metric is the Disparate Impact (DI, Equation 2), which, as a reminder, is the ratio between the probabilities of obtaining a positive result (\u0176=1) according to the values in the protected feature.\nWhen no bias is present according to a protected feature, DI = 1; otherwise, the value is still considered acceptable when it is between 0.8 and 1.25. Figure 2 shows the behaviour of DI as the preferences of the model change (as happens when its training data is modified). It includes the neutral DI value (1) and the positive classification probabilities for the different values of a protected feature.\nWith this in mind, DI can be checked on the base dataset (with the existing ratios) or on the model (using the predicted classes) for each of the protected features. When DI > 1, the bias favours the group with protected feature value 0; when DI < 1, it is biased towards the feature value 1."}, {"title": "3.3 Structure and inner working of the Fair-ONB method", "content": "The Fair-ONB method is based on the characterisation of groups derived from the combination of class and protected feature values in the dataset. This is done via the data coverage using balls obtained from ONB.\nOnce the coverage is obtained, the Fair-ONB method eliminates instances of favoured groups to foster a fair classifi-cation. This is done in a guided way, selecting instances in problematic areas of the data space."}, {"title": null, "content": "The Fair-ONB method selects the balls for preprocessing in two stages: first, the groups whose balls can be selected for elimination are chosen; then, using their characteristics, some balls of those groups are eliminated.\nGroup selection for preprocessing When fairness metrics indicate bias towards a value of a protected feature, the chosen strategy is to perform undersampling of the groups with positive class (\u0176=1) and said value (either 0 or 1) in the protected feature. Since there can be multiple protected features simultaneoulsy, there would be two ways of choosing the groups to preprocess.\n\u2022 Using the union of the groups with positive class and favoured values in the different protected features (OR). Thus, if in the example of Table 1 there are biases favouring race 1 and gender = 0, the groups to be preprocessed would be groups 1, 5 and 7.\n\u2022 Using the intersection of the groups with positive class and favoured values in the different protected features (AND). Likewise, if in the example of Table 1 there are biases favouring race = 1 and gender = 0, only group 5 would be chosen for preprocessing.\nDepending on the dataset characteristics, the chosen undersampling strategy can greatly affect the results. For that reason, both group selection methods will be used.\nSelection of balls from those groups for elimination Once the ball coverage has been obtained, some ball attributes that might indicate noisy areas or group overlap that can favour the existence of bias are studied. In particular, the radius, the number of instances they cover and the density of balls will be observed. The hypotheses regarding their possible effects on data bias are now described.\n\u2022 Radius: if it is very small, the ball is in an area of group overlap, be it due to data distribution of for the existence of noise.\n\u2022 Number of covered instances: if it covers very few instances, the ball might not be truly representative in the characterisation of its class. It can also be in an area of group overlap or noise.\n\u2022 Density: if it is very low, the ball might have been generated using an isolated point far from the decision boundary or just very far inside a class' area, which might not contribute much to the model's decisions. While some balls with very high density can also be problematic (due to, for example, having a very small radius), these particular cases are already covered by the other premises, so high density but useful balls will not be affected.\nTo perform the cleansing of the dataset, percentiles (values of an attribute for which exactly that percentage of cases have a lower value) of those three characteristics in the coverage are calculated, and thresholds for all three are chosen"}, {"title": "4 Experimental Framework", "content": "This Section presents information regarding the experiments that test the goodness of the proposed method, Fair-ONB. The main aim is to check that, by using undersampling strategies guided by ONB, the fairness of models learned from the modified dataset can be improved while their performance is approximately maintained."}, {"title": "4.1 Datasets", "content": "This experimental framework includes the use of four datasets. The first three (COMPAS, Adult and German) are the most used datasets in the field of bias reduction; the fourth dataset, Ricci, was included to add more broadness to the study.\nThe exact versions of the aforementioned datasets have been modified to reduce the computational complexity: cate-gorical variables were transformed into binary ones and, in the case of Adult, its size was cut in half (while maintaining its structure, via stratified sampling). The dataset characteristics (after those transformations) are as follows.\n\u2022 The COMPAS dataset predicts whether convicts will reoffend in the subsequent two years, according to their personal data and criminal history. Once transformed, it includes 6172 samples, 7 attributes (3 of them binary, 4 numeric) plus the binary class. The protected features are race and sex.\n\u2022 The Adult dataset evaluates whether a person will earn over 50.000$/year according to census data. Once transformed, the reduced dataset includes 24.416 samples, 13 attributes (7 of them binary, 6 numeric) plus the binary class. The protected features are race and sex.\n\u2022 The German dataset predicts the credit risk of people. Once transformed, it includes 1.000 samples, 24 features (14 of them binary, 10 numeric) plus the binary class. The protected features are gender and age.\n\u2022 The dataset Ricci evaluates the promotion of a group of firefighters according to exam results. It includes 118 samples, 4 attributes (2 of them binary and 2 numeric) plus the binary class. The protected feature is race."}, {"title": "4.2 Experimental structure and parameters", "content": "This study has two parts, each of them with a different aim.\n1. Testing the behaviour of the different threshold variables in different situations, to detect whether there are certain thresholds that generally give good results.\n2. Comparing the bias reduction when using the Fair-ONB method to that of FAWOS, a recent neighbourhood-based sampling strategy.\nIn order to properly evaluate classifier performance, 5-fold cross validation is used in each experiment for each dataset. Sampling methods are only applied on each experiment's training sets. The predictive performance is evaluated over the test set using two usual metrics: AUC and accuracy. AUC is used in the first part of the study, due to being a more conservative metric and its aptitude in case of imbalance; accuracy is used in the second part, once the good behaviour of the Fair-ONB method has been proven, and to allow for fair comparison with FAWOS (which samples groups until they are approximately even, a situation where accuracy is a useful metric). Fairness is evaluated using Disparate Impact. Performance and fairness results are aggregated via the arithmetic mean of the 5 folds'.\nFor dataset preprocessing using the Fair-ONB method, up to the 5 first different percentiles are used for each threshold feature (radius, number of covered instances, density), starting with 0 and going up in increments of 5. The combina-tion of percentiles for each threshold feature parameterise the different experiments for each dataset. Moreover, there experiments are performed both using union and intersection for undersampling group selection."}, {"title": null, "content": "The classifier used is a decision tree with its default parameters in the version 0.23.2 of scikit-learn (the necessary version for FAWOS), except for including a fixed seed (random_state=30). These classifiers are learned using the preprocessed training set of each experiment.\nIn the second part of the study, the Fair-ONB method is compared to FAWOS [18]. The fact that FAWOS is also a sampling method based on sample neighbourhoods makes it the closest strategy to Fair-ONB. FAWOS employs 5NN to label samples as safe, borderline, rare and outlier according to how many of their neighbours share their class, and then uses those labels to allocate probabilities of those samples being selected for SMOTE.\nThe parameters used for FAWOS are the same as in [18]. The weights for each label group are included in Table 2 (outliers always receive null weight), and oversampling factors were either 0.8, 1 or 1.2."}, {"title": "5 Experimental results", "content": "This Section presents the experimental results of the two studies. Firstly, Section 5.1 evaluates the effects of selecting different undersampling thresholds; then, Section 5.2 presents the comparison of the results using Fair-ONB and FAWOS, two bias reduction sampling methods."}, {"title": "5.1 Study of the behaviour of threshold features in the Fair-ONB method", "content": "This Section presents the results of the experiments regarding the use of different threshold combinations. These ex-periments measure Disparate Impact according to the different protected variables of each dataset, as well as classifier performance according to AUC, for each different preprocessing parametrisation. Thus, the aims are to check whether fairness can be improved without much effect on AUC, and whether certain thresholding parameters can be widely considered useful.\nTo facilitate a global comparison, Disparate Impact values for the different protected variables and AUC values are shown simultaneously; furthermore, to better reflect whether the method improves the results, the base results (with no preprocessing) are shown as a horizontal black line and the optimum Disparate Impact (1) as a green one.\nTwo sets of experiments are performed on each dataset, depending on whether union or intersection is used for under-sampling group selection. In each case, for conciseness, radius will be used as the X axis feature, due to providing the most illustrative results."}, {"title": "Results on COMPAS", "content": "The undersampling results obtained using union undersampling are shown in Figure 3.\nAs can be observed, the results when different thresholds for the number of covered instances create result clus-ters regarding Disparate Impact, with noticeable differences. Meanwhile, an interesting behaviour is observed for intermediate values of radius thresholds, where Disparate Impact varies explosively. Generally, when using union undersampling, substantial Disparate Impact improvements can be attained on COMPAS (to the point of reaching the optimum on \u201csex\u201d). AUC, while slightly reduced, is comparable to the base value.\nThe best combined disparate impact result (taking into account both protected features) was obtained using num_inst=1, radius=0.0015 and density=52.07, with a total distance to the Disparate Impact optima of 0.135 and a decrease of 0.009 in AUC.\nAs an example, Figure 4 shows two pie charts with the group distributions in the base dataset and when the afore-mentioned best performing undersampling is used. As can be observed, the best result is obtained without group balance.\nRegarding the intersection undersampling results, results are shown in Figure 5."}, {"title": "Results on Adult", "content": "Following the same structure, the results using union undersampling on Adult are shown in Figure 6.\nOn this dataset, union undersampling does not obtain noticeable improvements. Only slightly better Disparate Impact results were obtained for both protected variables. The best global Disparate Impact result when using union under-sampling on Adult was obtained using num_inst=1, radius=0.0154 and density=0.88 as thresholds, with a total distance to the Disparate Impact optima of 0.890 and AUC 0.007 lower than the baseline.\nThe results obtained using intersection undersampling are the following (see Figure 7).\nIn this case, Disparate Impact provides considerable improvements for both protected variables, reaching the opti-mum in \"race\". In the case of \u201csex\u201d, much better results than the baseline are also obtained, while not reaching the recommended [0.8,1.25] Disparate Impact interval."}, {"title": "Results on German", "content": "Likewise, the results using union undersampling on German are shown in Figure 8.\nIn this case, only slight improvements were obtained on \"gender\", since it was already very close to the optimum Disparate Impact, while noticeable improvements were attained according to \"age\". Regarding that first protected feature, the chosen number of covered instances threshold is very important, as increasing it leads to bad results.\nThe best global Disparate Impact result obtained when using union undersampling on German was obtained when using thresholds num_inst=1, radius=0.7109 and density=0.49, with a total distance to the Disparate Impact optima of 0.105, and AUC 0.013 over the baseline; thus, in this case it was possible to improve both fairness and performance simultaneously.\nRegarding the intersection undersampling results, they are shown in Figure 9.\nEmploying intersection undestampling, a similar behaviour was observed (although less pronounced) when the num-ber of covered instances threshold is modified, and the Disparate Impact ranges are lower. The best global Dis-parate Impact result obtained when using intersection undersampling on German was obtained when using thresholds num_inst=1, radius=0.5810 and density=0.44, with a total distance to the Disparate Impact optima of 0.096, and AUC only 0.015 below the base case."}, {"title": "Results on Ricci", "content": "Finally, regarding the Ricci dataset, there is only one protected feature (\u201crace\u201d), so it makes no sense to talk about union and intersection (since there is only one possible group to undersample). The undersampling results are shown in Figure 10.\nSlight improvements were obtained compared to the base case, as indicated in Table 6, obtaining the best global Disparate Impact result when using thresholds num_inst=1, radius=0 and density=5.49, with a distance to the"}, {"title": "5.2 Sampling methods comparison", "content": "This Section presents the results from the comparative study between Fair-ONB and FAWOS, two bias reduction sampling methods that are based on data neighbourhoods.\nWhile the initial intention was to perform the comparison using all 4 datasets indicated in Section 4.1, the particularities of FAWOS would make using Adult unfeasible. In particular, the computational complexity of FAWOS, which is not prepared to work on datasets that big (even having already reduced its number of samples), is too high, given the fact that it uses 5NN for each sample multiple times per experiment. While the sample size could have been reduced again so that this dataset could have been added to this study, the necessary reduction would be so drastic that it would distort the dataset characteristics.\nIn this study, accuracy was used to measure classifier performance and, since FAWOS measures the Adapted Disparate Impact (see Equation 3 for reference), a simple transformation of Disparate Impact, that was the chosen fairness metric.\nThe best results obtained with each of the methods (and their optimal parameters) on the COMPAS, German and Ricci datasets are presented in Tables 7, 8 and 9, respectively. As a reminder, FAWOS parameters are the weights for safe (S), boderline (B) and rare (R) samples (as outliers receive weight 0), as well as the oversampling factor (OF); meanwhile, the parameters of the Fair-ONB method are the number of covered instances (N_i), radius (R) and density (D) thresholds."}, {"title": "6 Concluding remarks", "content": "This paper shows the usefulness of morphology and neighbourhood-based sampling approaches towards classifier fairness, a niche that has not been thoroughly explored. Fair-ONB, which employs guided undersampling in areas that are close to the decision boundaries, where models usually have problems classifying the different protected groups. The Fair-ONB method harnesses the morphology of groups to select samples for elimination, and unlike other random sampling techniques, its logic is more justifiable due to its empirical approach.\nThe goodness of the Fair-ONB method is demonstrated in two steps. The first study checks its behaviour on 4 datasets, where it was able to produce better fairness results in all cases, accompanied by better classification performance results in half of them. While no clear pattern was obtained regarding the threshold variables, it was shown that they provide a proper characterisation of the problematic areas in datasets and, thus, that using subgroup ball coverage for undersampling instance selection is a very useful strategy.\nThe second study checks that the Fair-ONB method is able to better bolster fairness compared to FAWOS, a similar state-of-the-art neighbourhood-based sampling method. Not only does the Fair-ONB method have lower compu-tational complexity, but its identification of the problematic areas, which takes into account more than just a few neighbours, allows it to reach better results.\nFinally, regarding future work lines, there are multiple possible courses of action.\n\u2022 Employing the group characterisation of the Fair-ONB method on datasets that do not contain protected variables (in which case, groups would be given by just the classes). This way, noise-reducing preprocessing could be applied to improve classification performance in situations with poor data quality.\n\u2022 Combining morphology-based strategies with other preprocessing techniques, so that higher fairness levels can be attained. Using a bigger scope could lead to finding synergies and that could lead to more in-depth preprocessing."}]}