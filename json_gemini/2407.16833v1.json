{"title": "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach", "authors": ["Zhuowan Li", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Michael Bendersky"], "abstract": "Retrieval Augmented Generation (RAG) has been a powerful tool for Large Language Models (LLMs) to efficiently process overly lengthy contexts. However, recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to understand long contexts directly. We conduct a comprehensive comparison between RAG and long-context (LC) LLMs, aiming to leverage the strengths of both. We benchmark RAG and LC across various public datasets using three latest LLMs. Results reveal that when resourced sufficiently, LC consistently outperforms RAG in terms of average performance. However, RAG's significantly lower cost remains a distinct advantage. Based on this observation, we propose SELF-ROUTE, a simple yet effective method that routes queries to RAG or LC based on model self-reflection. SELF-ROUTE significantly reduces the computation cost while maintaining a comparable performance to LC. Our findings provide a guideline for long-context applications of LLMs using RAG and LC.", "sections": [{"title": "1 Introduction", "content": "Retrieval augmented generation (RAG) has been shown to be a both effective and efficient approach for large language models (LLMs) to leverage external knowledge. RAG retrieves relevant information based on the query and then prompts an LLM to generate a response in the context of the retrieved information. This approach significantly expands LLM's access to vast amounts of information at a minimal cost.\nHowever, recent LLMs like Gemini and GPT-4 have demonstrated exceptional capabilities in understanding long contexts directly. For example, Gemini 1.5 can process up to 1 million tokens (Reid et al., 2024). This prompts the need for a systematic comparison between long-context (LC) LLMs and RAG: on one hand, RAG conceptually acts as a prior, regularizing the attention of LLMs onto retrieved segments, thus avoiding the distraction of the irrelevant information and saving unnecessary attention computations; on the other hand, large-scale pretraining may enable LLMs to develop even stronger long-context capabilities. Therefore, we are motivated to compare RAG and LC, evaluating both their performance and efficiency.\nIn this work, we systematically benchmark RAG and LC on various public datasets, gaining a comprehensive understanding of their pros and cons, and ultimately combining them to get the best of both worlds. Different from findings in previous work (Xu et al., 2023), we find that LC consistently outperforms RAG in almost all settings (when resourced sufficiently). This demonstrates the superior progress of recent LLMs in long-context understanding.\nDespite the suboptimal performance, RAG remains relevant due to its significantly lower computational cost. In contrast to LC, RAG significantly decreases the input length to LLMs, leading to reduced costs, as LLM API pricing is typically based on the number of input tokens. (Google, 2024; Ope-"}, {"title": "2 Related Work", "content": "Long-context LLMs. There has long been efforts for enabling LLMs to handle long contexts (Guo et al., 2022; Beltagy et al., 2020; Chen et al., 2023b). While recent LLMs like Gemini-1.5 (Reid et al., 2024), GPT-4 (Achiam et al., 2023), Claude-3 (Anthropic, 2024) achieve significantly larger context window size, long-context prompting is still expensive due to the quadratic computation cost of transformers regarding to the input token numbers. Recent work proposes methods to reduce cost by prompt compression (Jiang et al., 2023), model distillation (Hsieh et al., 2023), or LLM cascading (Chen et al., 2023a).\nRetrieval-augmented generation. Augmenting LLMs with relevant information retrieved from various sources (Lewis et al., 2020), i.e., RAG, has been successful in complementing LLMs with external knowledge. RAG achieves good performance on various of tasks like language modeling (Khandelwal et al., 2019; Shi et al., 2023) and QA (Guu et al., 2020; Izacard and Grave, 2020), with a significantly lower computation cost (Borgeaud et al., 2022). Related to but different from our work, recently works augment RAG with correction (Yan et al., 2024), critique (Asai et al., 2023), or verifi-\nWhile retrieval may introduce extra cost, retrieval system is much easier to set up and can be hosted on customer side.\ncation (Li et al., 2023) to improve retrieval quality on knowledge-intensive tasks.\nLong-context evaluation. Evaluating long-context models is challenging due to the difficulty in collecting and analyzing long texts. Recent researchers propose both synthetic tests like needle-in-a-haystack (Greg Kamradt, 2023), Ruler (Hsieh et al., 2024), or Counting Stars (Song et al., 2024), and real datasets including LongBench (Bai et al., 2023), \u221eBench (Zhang et al., 2024), L-Eval (An et al., 2023), and others (Shaham et al., 2022; Yuan et al., 2024; Maharana et al., 2024). Evaluating on these datasets, recent works study the performance degradation over various context lengths (Levy et al., 2024; Hsieh et al., 2024), the lost-in-the-middle phenomenon (Liu et al., 2024), and explore solutions (Kuratov et al., 2024). Related to our work, Xu et al. (2023) compare RAG and long-context prompting and find that long-context models still lags behind RAG. This is different from our findings, possibly due to consideration of stronger LLMs and longer contexts in our work."}, {"title": "3 Benchmarking RAG versus LC", "content": "We evaluate on a subset of datasets from Long-Bench (Bai et al., 2023) and \u221eBench (Zhang et al., 2024), which are recent benchmarks containing a collection of new and existing datasets for LLM evaluation, covering both synthetic and real texts in multiple languages. LongBench contains a collection of 21 datasets, with an average context length of 7k words. \u221eBench consists of even longer contexts with an average length of 100k tokens.\nAmong the datasets, we mainly focus on tasks that are (a) in English, (b) real, and (c) query-based (e.g. summarization tasks do not contain queries for retrieving relevant information). This results in 7 datasets from LongBench including NarrativeQA (Ko\u010disk\u1ef3 et al., 2018), Qasper (Dasigi et al., 2021), MultiFieldQA (Bai et al., 2023), HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), QMSum (Zhong et al., 2021); and 2 datasets from \u221eBench including En.QA and EN.MC. Please refer to Appendix A for more details. Additionally, in Sec. 5.4, we will provide an ablation a synthetic datasets PassKey from \u221eBench.\nFor evaluation metrics, we report F1 scores for the open-ended QA tasks, accuracy for the multi-choice QA tasks, and ROUGE score for the sum-"}, {"title": "3.1 Datasets and metrics", "content": "marization tasks."}, {"title": "3.2 Models and Retrievers", "content": "Three latest LLMs are evaluated, including Gemini-1.5-Pro (Reid et al., 2024), GPT-4O (OpenAI, 2024a), and GPT-3.5-Turbo (OpenAI, 2023) \u00b2. Gemini-1.5-Pro is a recent long-context LLM from Google, supporting up to 1 million tokens. GPT-4O, the newest lightweight yet strong LLM from OpenAI, supports 128k tokens. GPT-3.5-Turbo supports 16k tokens.\nTwo retrievers are used in our study: Contriever (Izacard et al., 2021), which is a contrastively trained dense retriever outperforming BM25 on BEIR datasets, and Dragon (Lin et al., 2023), which is a recent generalizable dense retriever achieving high performance in both supervised and zero-shot settings without complex late interaction. Following (Xu et al., 2023), we divide long contexts into chunks of 300 words, and select the top k chunks (default k = 5) based on the cosine similarity of the query embedding and the chunk embeddings. The chunks are ordered by the similarity scores, with the chunk index prepended at the beginning.\nSince black-box LLMs are pretrained on unknown datasets, the leakage of evaluation datasets may occur. Especially, some of the evaluation datasets are based on Wikipedia, which has likely been seen by LLMs during during. In some cases, we find that model may predict the correct answer using exactly the same words as the groundtruth (e.g. \u201cmeticulously\u201d), even when they do not appear in the provided context. In our experiment, we try mitigating this issue by prompting the model to answer \u201cbased only on the provided passage\u201d for both RAG and LC. It remains an open question how to address the data leakage issue in LLM evaluation."}, {"title": "3.3 Benchmarking results", "content": "We benchmark the performance of LC and RAG across the nine datasets, using three recent LLMs: Gemini-1.5-Pro, GPT-4O and GPT-3.5-Turbo. Tab. 1 presents the results using the Contriever retriever, where rows *-1 and rows *-2 present the benchmarking results for LC and RAG respectively. Results using the Dragon retriever will be discussed in Sec. 5.3 and Tab. 2.\nAs shown in Tab. 1, LC consistently outperforms RAG for all the three models, with a significant\n\u00b2gpt-3.5-turbo-0125, gpt-4o-2024-05-13\nmargin. On average, LC surpasses RAG by 7.6% for Gemini-1.5-Pro, 13.1% for GPT-40, and 3.6% for GPT-3.5-Turbo. Noticeably, the performance gap is more significant for the more recent models (GPT-4O and Gemini-1.5-Pro) compared to GPT-3.5-Turbo, highlighting the exceptional long-context understanding capacity of the latest LLMs.\nHowever, there is an exception observed on the two longer datasets from \u221eBench (i.e., En.QA and En.MC), where RAG achieves higher performance than LC for GPT-3.5-Turbo. This result deviates from the overall trend, likely due to the significantly longer context in these datasets (147k words on average) compared with the limited context window (16k) of GPT-3.5-Turbo. This finding highlights the effectiveness of RAG when the input text considerably exceeds the model's context window size, emphasizing a specific use case of RAG."}, {"title": "4 Self-Route", "content": "As demonstrated in Sec. 3, RAG lags behind long-context LLMs in terms of performance. However, despite this performance gap, we surprisingly find a high degree of overlap in their predictions, as illustrated in Fig. 2."}, {"title": "4.1 Motivation", "content": "Fig. 2 displays the distribution of the differences between RAG prediction scores $S_{RAG}$ and LC prediction scores $S_{LC}$, specifically $S_{RAG} \u2013 S_{LC}$ (the scores are multiplied by 100 to be scaled to 1-100). These scores S represent the evaluation of model predictions against the groundtruth. Notably, for most queries, RAG scores and LC scores are highly similar. In fact, for 63% queries, the model predictions are exactly identical; and for 70% queries, the score difference is less than 10 (absolute value)."}, {"title": "4.2 Self-Route", "content": "Based on the above motivation, we propose SELF-ROUTE, a simple yet effective method combining RAG and LC to reduce cost while maintaining a performance comparable to LC. SELF-ROUTE utilizes LLM itself to route queries based on self-reflection, under the assumption that LLMs are well-calibrated in predicting whether a query is answerable given provided context.\nConcretely, our method consists of two steps: a RAG-and-Route step and a long-context prediction step. In the first step, we provide the query and the retrieved chunks to the LLM, and prompt it to predict whether the query is answerable and, if so, generate the answer. This is similar to standard RAG, with one key difference: the LLM is given the option to decline answering with the prompt \u201cWrite unanswerable if the query can not be answered based on the provided text\u201d. For the queries deemed answerable, we accept the RAG prediction as the final answer. For the queries deemed unanswerable, we proceed to the second step, providing the full context to the long-context"}, {"title": "4.3 Results", "content": "Rows *-3 to *-5 in Tab. 1 present the results of our method, utilizing the three LLMs. Rows *-3 report the performance. Rows *-4 show the percentage of answerable queries, as predicted in the RAG-and-Route step. Rows *-5 display the percentage of tokens used by our method, compared to that of LC. In terms of performance (rows *-3), SELF-ROUTE significantly outperforms RAG, achieving results comparable to LC. Across all three models, SELF-ROUTE surpasses RAG (rows *-2) by over 5%. Compared to LC (rows *-1), there is a slight performance drop for GPT-40 (-0.2%) and Gemini-1.5-Pro (-2.2%), but an improvement for GPT-3.5-Turbo (+1.7%).\nAll three LLMs consistently route more than half of queries towards RAG, as shown in rows *-4. For Gemini-1.5-Pro, the answerable percentage even reaches 81.74% (row 1-4). This indicates that RAG may answer most queries without the need for LC, confirming our initial motivation.\nDue to the high answerable rate, the number of tokens required is significantly reduced (rows *-\n5). For example, GPT-40 uses only 61% tokens"}, {"title": "5 Analysis", "content": "Both RAG and SELF-ROUTE relies on the top-k retrieved text chunks. The larger k is, the longer context are fed into LLMs for RAG prediction as well as routing, resulting in different costs versus performances. To study the influence of k, in Fig. 3, we plot the performance and cost (i.e. input token percentage) curves when different ks are used."}, {"title": "5.1 Ablations of k", "content": "while achieving comparable performance (46.83) with LC (47.04), Gemini-1.5-Pro uses 38.6% of the tokens. Since the computation cost of the transformer-based LLMs is quadratic to token count, and most LLM APIs charge based on token count (OpenAI, 2024b; Google, 2024), this lower token count translates to substantial cost savings.\nOn longer datasets, the advantage of our method is more pronounced for OpenAI models, but less significant for Gemini. For instance, for GPT-4O, SELF-ROUTE outperforms LC by 2.3% and 7.4% respectively on EN.QA and EN.MC, which contain longer contexts. For GPT-3.5-Turbo, the advantage margins are even larger. However, for Gemini-1.5-Pro, the performance is lower than LC. These different behaviors are possibly due to the difference in LLM alignments, i.e., OpenAI models are more likely to reject answering using RAG, leading to a lower answerable percentage but higher accuracy, which results in a different performance-cost trade-off compared with Gemini-1.5-Pro."}, {"title": "5.2 Why does RAG fail?", "content": "To gain a better understanding of why RAG lags behind LC, we analyze the failure reasons for the examples that cannot be answered by RAG. We first manually check some examples for which our RAG-and-Route step predicts \u201cunanswerable\u201d and summarize four typical failure reasons, then prompt LLM to classify all the examples.\nThe four reasons include: (A) The query requires multi-step reasoning so the results of previous steps are needed to retrieve information for later steps, e.g. \u201cWhat nationality is the performer of song XXX\u201d. (B) The query is general, e.g. \u201cWhat does the group think about XXX\u201d, which is challenging for the retriever to formulate a good query. (C) The query is long and complex, which is challenging for the retriever to understand. However, answering this kind of questions is arguably, an advantage of LLMs. (D) The query is implicit, demanding a thorough understanding of the entire context. For instance, in a lengthy conversational narrative about a space voyage, a question like \u201cWhat caused the shadow behind the spaceship?\u201d requires readers to connect the dots and deduce the answer, as there is no explicit mention of the shadow when the cause is revealed.\nUsing these reasons, we prompt Gemini-1.5-Pro with few-shot in-context examples that we manually annotated, to classify all the unanswerable"}, {"title": "5.3 Different retrievers", "content": "The results using a retriever, Dragon, is shown in Tab. 2 based on Gemini-1.5-Pro. As can be seen, the results are consistent with Contriever, for all of LC, RAG, and SELF-ROUTE, showing that our findings are generalizable across retrievers."}, {"title": "5.4 Results on synthetic data", "content": "In this study, we mainly focus on real datasets, with a consideration that results on synthetic data, which are artificially created by researchers, may subject to dataset artifacts. We notice some methods that researchers adopted to create synthetic long context datasets may unconsciously, but largely, influence the performance comparison between RAG and LC. For example, here we describe the results on the \u201cPassKey\u201d dataset in \u221eBench and its variations.\nThis \u201cPassKey\u201d dataset presents a needle-in-a-haystack test, where a sentence with a passkey (e.g. \u201cthe passkey is 123456\u201d) is hidden within chunks of irrelevant text, and the model is asked to answer the question \u201cWhat is the passkey\u201d. The task requires strong retrieval capability. On this dataset, RAG achieves 80.34% accuracy, outperforming LC, which gets 65.25% using Gemini-1.5-Pro. However, if the query is slightly modified as \u201cWhat is the special token hidden inside the texts\u201d, RAG accuracy sharply drops to only 4.58%, while LC keeps roughly the same (69.32%). Another example: if the chunks contain two passkeys and the query is \u201cWhich passkey is larger? First or second?\u201d, then RAG (47.63%) under-performs LC (64.24%) as well.\nThese examples demonstrate that the evaluation results highly subjects to artifacts in dataset construction, showing limitation of synthetic testing."}, {"title": "6 conclusion", "content": "This paper presents a comprehensive comparison of RAG and LC, highlighting the trade-offs between performance and computational cost. While LC demonstrate superior performance in long-context understanding, RAG remains a viable option due to its lower cost and advantages when the input considerably exceeds the model's context window size. Our proposed method, which dynamically routes queries based on model self-reflection, effectively combines the strengths of both RAG and LC, achieving comparable performance to LC at a significantly reduced cost. We believe our findings contribute valuable insights for the practical application of long-context LLMs and pave the way for future research in optimizing RAG techniques."}, {"title": "A Dataset details", "content": "We evaluate on 7 datasets from LongBench (Bai et al., 2023). NarrativeQA (Ko\u010disk\u1ef3 et al., 2018) is a question answering dataset, where the context is a long story like a novel or a movie script. Qasper (Dasigi et al., 2021) focuses on question answering over academic NLP papers and is annotated by NLP practitioners. MultiFieldQA, originally proposed in LongBench, contains human-annotated QA over documents and articles from multiple sources, including legal documents, government reports, encyclopedias, academic papers, etc. HotpotQA (Yang et al., 2018) contains two-hop questions written by native English speakers that requires reasoning over two related Wikipedia paragraphs in the long context. 2WikiMultihopQA (Ho et al., 2020) contains up to 5-hop questions that are synthesized through manually designed templates, ensuring that they cannot be solved through shortcuts. The questions in MuSiQue (Trivedi et al., 2022) are up to 4-hop, first constructed from single-hop question compositions, and then paraphrased by annotators for linguistic diversity. QMSum (Zhong et al., 2021) is a query-based summarization dataset over meeting scripts from multiple domains.\nWe evaluate on 2 datasets from \u221eBench (Zhang et al., 2024). En.QA contains human-annotated question-answer pairs for long novels, with key entity names manually replaced in order to avoid knowledge leakage due to model pretraining. EN.MC is annotated similarly to En.QA, but differs in that the model is presented with four challenging answer choices written by the annotators.\nTab. 3 shows the details of the datasets, including the number of queries in each evaluation dataset and the average context length (i.e. number of words)."}, {"title": "B Ablations of k", "content": "Tab. 4 shows the performance and token ratio for different k, which corresponds to Fig. 3. The performance of LC, which serves as an upper bound, is 45.53. The token ratio is computed the token counts for RAG or SELF-ROUTE divided the number of tokens required by LC."}, {"title": "C Prompts", "content": "Tab. 5 shows the prompts for each dataset in our study. The prompts are modified from the released prompts as in LongBench (Bai et al., 2023) and \u221eBench (Zhang et al., 2024). Tab. 6 shows the prompts used in the failure case study as in Sec. 5.2."}]}