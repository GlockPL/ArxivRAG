{"title": "Retrieval Augmented Generation or Long-Context LLMs?\nA Comprehensive Study and Hybrid Approach", "authors": ["Zhuowan Li", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Michael Bendersky"], "abstract": "Retrieval Augmented Generation (RAG) has\nbeen a powerful tool for Large Language Mod-\nels (LLMs) to efficiently process overly lengthy\ncontexts. However, recent LLMs like Gemini-\n1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct\na comprehensive comparison between RAG\nand long-context (LC) LLMs, aiming to lever-\nage the strengths of both. We benchmark RAG\nand LC across various public datasets using\nthree latest LLMs. Results reveal that when\nresourced sufficiently, LC consistently outper-\nforms RAG in terms of average performance.\nHowever, RAG's significantly lower cost re-\nmains a distinct advantage. Based on this ob-\nservation, we propose SELF-ROUTE, a simple\nyet effective method that routes queries to RAG\nor LC based on model self-reflection. SELF-\nROUTE significantly reduces the computation\ncost while maintaining a comparable perfor-\nmance to LC. Our findings provide a guideline\nfor long-context applications of LLMs using\nRAG and LC.", "sections": [{"title": "Introduction", "content": "Retrieval augmented generation (RAG) has been\nshown to be a both effective and efficient approach\nfor large language models (LLMs) to leverage ex-\nternal knowledge. RAG retrieves relevant informa-\ntion based on the query and then prompts an LLM\nto generate a response in the context of the retrieved\ninformation. This approach significantly expands\nLLM's access to vast amounts of information at a\nminimal cost.\nHowever, recent LLMs like Gemini and GPT-4\nhave demonstrated exceptional capabilities in un-\nderstanding long contexts directly. For example,\nGemini 1.5 can process up to 1 million tokens (Reid\net al., 2024). This prompts the need for a system-\natic comparison between long-context (LC) LLMs"}, {"title": "Related Work", "content": "Long-context LLMs. There has long been ef-\nforts for enabling LLMs to handle long contexts\n(Guo et al., 2022; Beltagy et al., 2020; Chen et al.,\n2023b). While recent LLMs like Gemini-1.5 (Reid\net al., 2024), GPT-4 (Achiam et al., 2023), Claude-\n3 (Anthropic, 2024) achieve significantly larger\ncontext window size, long-context prompting is\nstill expensive due to the quadratic computation\ncost of transformers regarding to the input token\nnumbers. Recent work proposes methods to reduce\ncost by prompt compression (Jiang et al., 2023),\nmodel distillation (Hsieh et al., 2023), or LLM cas-\ncading (Chen et al., 2023a).\nRetrieval-augmented generation. Augmenting\nLLMs with relevant information retrieved from\nvarious sources (Lewis et al., 2020), i.e., RAG,\nhas been successful in complementing LLMs with\nexternal knowledge. RAG achieves good perfor-\nmance on various of tasks like language modeling\n(Khandelwal et al., 2019; Shi et al., 2023) and QA\n(Guu et al., 2020; Izacard and Grave, 2020), with\na significantly lower computation cost (Borgeaud\net al., 2022). Related to but different from our work,\nrecently works augment RAG with correction (Yan\net al., 2024), critique (Asai et al., 2023), or verifi-"}, {"title": "Benchmarking RAG versus LC", "content": "We evaluate on a subset of datasets from Long-\nBench (Bai et al., 2023) and \u221eBench (Zhang et al.,\n2024), which are recent benchmarks containing a\ncollection of new and existing datasets for LLM\nevaluation, covering both synthetic and real texts in\nmultiple languages. LongBench contains a collec-\ntion of 21 datasets, with an average context length\nof 7k words. Bench consists of even longer con-\ntexts with an average length of 100k tokens.\nAmong the datasets, we mainly focus on tasks\nthat are (a) in English, (b) real, and (c) query-based\n(e.g. summarization tasks do not contain queries\nfor retrieving relevant information). This results in\n7 datasets from LongBench including NarrativeQA\n(Ko\u010disk\u1ef3 et al., 2018), Qasper (Dasigi et al., 2021),\nMultiFieldQA (Bai et al., 2023), HotpotQA (Yang\net al., 2018), 2WikiMultihopQA (Ho et al., 2020),\nMuSiQue (Trivedi et al., 2022), QMSum (Zhong\net al., 2021); and 2 datasets from \u221eBench includ-\ning En.QA and EN.MC. Please refer to Appendix A\nfor more details. Additionally, in Sec. 5.4, we will\nprovide an ablation a synthetic datasets PassKey\nfrom \u221eBench.\nFor evaluation metrics, we report F1 scores for\nthe open-ended QA tasks, accuracy for the multi-\nchoice QA tasks, and ROUGE score for the sum-"}, {"title": "Models and Retrievers", "content": "Three latest LLMs are evaluated, including Gemini-\n1.5-Pro (Reid et al., 2024), GPT-4O (OpenAI,\n2024a), and GPT-3.5-Turbo (OpenAI, 2023) 2.\nGemini-1.5-Pro is a recent long-context LLM from\nGoogle, supporting up to 1 million tokens. GPT-\n40, the newest lightweight yet strong LLM from\nOpenAI, supports 128k tokens. GPT-3.5-Turbo\nsupports 16k tokens.\nTwo retrievers are used in our study: Contriever\n(Izacard et al., 2021), which is a contrastively\ntrained dense retriever outperforming BM25 on\nBEIR datasets, and Dragon (Lin et al., 2023), which\nis a recent generalizable dense retriever achieving\nhigh performance in both supervised and zero-shot\nsettings without complex late interaction. Follow-\ning (Xu et al., 2023), we divide long contexts into\nchunks of 300 words, and select the top k chunks\n(default k = 5) based on the cosine similarity of\nthe query embedding and the chunk embeddings.\nThe chunks are ordered by the similarity scores,\nwith the chunk index prepended at the beginning.\nSince black-box LLMs are pretrained on un-\nknown datasets, the leakage of evaluation datasets\nmay occur. Especially, some of the evaluation\ndatasets are based on Wikipedia, which has likely\nbeen seen by LLMs during during. In some cases,\nwe find that model may predict the correct answer\nusing exactly the same words as the groundtruth\n(e.g. \"meticulously\u201d), even when they do not appear\nin the provided context. In our experiment, we try\nmitigating this issue by prompting the model to an-\nswer \"based only on the provided passage\"\nfor both RAG and LC. It remains an open ques-\ntion how to address the data leakage issue in LLM\nevaluation."}, {"title": "Benchmarking results", "content": "We benchmark the performance of LC and RAG\nacross the nine datasets, using three recent LLMs:\nGemini-1.5-Pro, GPT-4O and GPT-3.5-Turbo.\nTab. 1 presents the results using the Contriever\nretriever, where rows *-1 and rows *-2 present the\nbenchmarking results for LC and RAG respectively.\nResults using the Dragon retriever will be discussed\nin Sec. 5.3 and Tab. 2.\nAs shown in Tab. 1, LC consistently outperforms\nRAG for all the three models, with a significant"}, {"title": "Self-Route", "content": "As demonstrated in Sec. 3, RAG lags behind long-\ncontext LLMs in terms of performance. However,\ndespite this performance gap, we surprisingly find\na high degree of overlap in their predictions, as\nillustrated in Fig. 2."}, {"title": "Motivation", "content": "As shown in Sec. 3, RAG lags behind long-\ncontext LLMs in terms of performance. However,\ndespite this performance gap, we surprisingly find\na high degree of overlap in their predictions, as\nillustrated in Fig. 2."}, {"title": "Self-Route", "content": "Based on the above motivation, we propose SELF-\nROUTE, a simple yet effective method combining\nRAG and LC to reduce cost while maintaining a\nperformance comparable to LC. SELF-ROUTE uti-\nlizes LLM itself to route queries based on self-\nreflection, under the assumption that LLMs are\nwell-calibrated in predicting whether a query is\nanswerable given provided context.\nConcretely, our method consists of two steps: a\nRAG-and-Route step and a long-context prediction\nstep. In the first step, we provide the query and\nthe retrieved chunks to the LLM, and prompt it to\npredict whether the query is answerable and, if so,\ngenerate the answer. This is similar to standard\nRAG, with one key difference: the LLM is given\nthe option to decline answering with the prompt\n\"Write unanswerable if the query can not\nbe answered based on the provided text\".\nFor the queries deemed answerable, we accept the\nRAG prediction as the final answer. For the queries\ndeemed unanswerable, we proceed to the second\nstep, providing the full context to the long-context"}, {"title": "Results", "content": "Rows *-3 to *-5 in Tab. 1 present the results of our\nmethod, utilizing the three LLMs. Rows *-3 report\nthe performance. Rows *-4 show the percentage\nof answerable queries, as predicted in the RAG-and-Route step. Rows *-5 display the percentage\nof tokens used by our method, compared to that\nof LC. In terms of performance (rows *-3), SELF-\nROUTE significantly outperforms RAG, achieving\nresults comparable to LC. Across all three models,\nSELF-ROUTE surpasses RAG (rows *-2) by over\n5%. Compared to LC (rows *-1), there is a slight\nperformance drop for GPT-40 (-0.2%) and Gemini-1.5-Pro (-2.2%), but an improvement for GPT-3.5-\nTurbo (+1.7%).\nAll three LLMs consistently route more than half\nof queries towards RAG, as shown in rows *-4. For\nGemini-1.5-Pro, the answerable percentage even\nreaches 81.74% (row 1-4). This indicates that RAG\nmay answer most queries without the need for LC,\nconfirming our initial motivation.\nDue to the high answerable rate, the number of\ntokens required is significantly reduced (rows *-\n5). For example, GPT-40 uses only 61% tokens"}, {"title": "Analysis", "content": "Both RAG and SELF-ROUTE relies on the top-k\nretrieved text chunks. The larger k is, the longer\ncontext are fed into LLMs for RAG prediction as\nwell as routing, resulting in different costs versus\nperformances. To study the influence of k, in Fig. 3,\nwe plot the performance and cost (i.e. input token\npercentage) curves when different ks are used."}, {"title": "Ablations of k", "content": "Both RAG and SELF-ROUTE relies on the top-k\nretrieved text chunks. The larger k is, the longer\ncontext are fed into LLMs for RAG prediction as\nwell as routing, resulting in different costs versus\nperformances. To study the influence of k, in Fig. 3,\nwe plot the performance and cost (i.e. input token\npercentage) curves when different ks are used."}, {"title": "Why does RAG fail?", "content": "To gain a better understanding of why RAG lags\nbehind LC, we analyze the failure reasons for the\nexamples that cannot be answered by RAG. We\nfirst manually check some examples for which our\nRAG-and-Route step predicts \u201cunanswerable\" and\nsummarize four typical failure reasons, then prompt\nLLM to classify all the examples.\nThe four reasons include: (A) The query requires\nmulti-step reasoning so the results of previous steps\nare needed to retrieve information for later steps,\ne.g. \"What nationality is the performer of\nsong XXX\u201d. (B) The query is general, e.g. \"What\ndoes the group think about XXX\", which is\nchallenging for the retriever to formulate a good\nquery. (C) The query is long and complex, which\nis challenging for the retriever to understand. How-\never, answering this kind of questions is arguably,\nan advantage of LLMs. (D) The query is implicit,\ndemanding a thorough understanding of the en-\ntire context. For instance, in a lengthy conversa-\ntional narrative about a space voyage, a question\nlike \"What caused the shadow behind the\nspaceship?\u201d requires readers to connect the dots\nand deduce the answer, as there is no explicit men-\ntion of the shadow when the cause is revealed.\nUsing these reasons, we prompt Gemini-1.5-Pro\nwith few-shot in-context examples that we man-\nually annotated, to classify all the unanswerable"}, {"title": "Different retrievers", "content": "The results using a retriever, Dragon, is shown in\nTab. 2 based on Gemini-1.5-Pro. As can be seen,\nthe results are consistent with Contriever, for all\nof LC, RAG, and SELF-ROUTE, showing that our\nfindings are generalizable across retrievers."}, {"title": "Results on synthetic data", "content": "In this study, we mainly focus on real datasets, with\na consideration that results on synthetic data, which\nare artificially created by researchers, may subject\nto dataset artifacts. We notice some methods that\nresearchers adopted to create synthetic long context\ndatasets may unconsciously, but largely, influence\nthe performance comparison between RAG and LC.\nFor example, here we describe the results on the\n\"PassKey\" dataset in \u221eBench and its variations.\nThis \"PassKey\" dataset presents a needle-in-a-haystack test, where a sentence with a passkey\n(e.g. \"the passkey is 123456\") is hidden within\nchunks of irrelevant text, and the model is asked\nto answer the question \"What is the passkey\u201d.\nThe task requires strong retrieval capability. On\nthis dataset, RAG achieves 80.34% accuracy, out-\nperforming LC, which gets 65.25% using Gemini-1.5-Pro. However, if the query is slightly modi-\nfied as \"What is the special token hidden\ninside the texts\", RAG accuracy sharply drops\nto only 4.58%, while LC keeps roughly the same\n(69.32%). Another example: if the chunks contain\ntwo passkeys and the query is \"Which passkey\nis larger? First or second?\u201d, then RAG\n(47.63%) under-performs LC (64.24%) as well.\nThese examples demonstrate that the evaluation\nresults highly subjects to artifacts in dataset con-\nstruction, showing limitation of synthetic testing."}, {"title": "conclusion", "content": "This paper presents a comprehensive comparison of\nRAG and LC, highlighting the trade-offs between\nperformance and computational cost. While LC\ndemonstrate superior performance in long-context\nunderstanding, RAG remains a viable option due\nto its lower cost and advantages when the input\nconsiderably exceeds the model's context window\nsize. Our proposed method, which dynamically\nroutes queries based on model self-reflection, ef-\nfectively combines the strengths of both RAG and\nLC, achieving comparable performance to LC at a\nsignificantly reduced cost. We believe our findings\ncontribute valuable insights for the practical appli-\ncation of long-context LLMs and pave the way for\nfuture research in optimizing RAG techniques."}, {"title": "Dataset details", "content": "We evaluate on 7 datasets from LongBench (Bai et al., 2023). NarrativeQA (Ko\u010disk\u1ef3 et al., 2018) is\na question answering dataset, where the context is a long story like a novel or a movie script. Qasper\n(Dasigi et al., 2021) focuses on question answering over academic NLP papers and is annotated by\nNLP practitioners. MultiFieldQA, originally proposed in LongBench, contains human-annotated QA\nover documents and articles from multiple sources, including legal documents, government reports,\nencyclopedias, academic papers, etc. HotpotQA (Yang et al., 2018) contains two-hop questions written\nby native English speakers that requires reasoning over two related Wikipedia paragraphs in the long\ncontext. 2WikiMultihopQA (Ho et al., 2020) contains up to 5-hop questions that are synthesized through\nmanually designed templates, ensuring that they cannot be solved through shortcuts. The questions in\nMuSiQue (Trivedi et al., 2022) are up to 4-hop, first constructed from single-hop question compositions,\nand then paraphrased by annotators for linguistic diversity. QMSum (Zhong et al., 2021) is a query-based\nsummarization dataset over meeting scripts from multiple domains.\nWe evaluate on 2 datasets from \u221eBench (Zhang et al., 2024). En.QA contains human-annotated\nquestion-answer pairs for long novels, with key entity names manually replaced in order to avoid\nknowledge leakage due to model pretraining. EN.MC is annotated similarly to En.QA, but differs in that\nthe model is presented with four challenging answer choices written by the annotators.\nTab. 3 shows the details of the datasets, including the number of queries in each evaluation dataset and\nthe average context length (i.e. number of words)."}, {"title": "Ablations of k", "content": "Tab. 4 shows the performance and token ratio for different k, which corresponds to Fig. 3. The performance\nof LC, which serves as an upper bound, is 45.53. The token ratio is computed the token counts for RAG\nor SELF-ROUTE divided the number of tokens required by LC."}, {"title": "Prompts", "content": "Tab. 5 shows the prompts for each dataset in our study. The prompts are modified from the released\nprompts as in LongBench (Bai et al., 2023) and \u221eBench (Zhang et al., 2024). Tab. 6 shows the prompts\nused in the failure case study as in Sec. 5.2."}]}