{"title": "Supertrust: Evolution-based superalignment strategy for safe coexistence", "authors": ["James M. Mazzu"], "abstract": "It's widely expected that humanity will someday create AI systems vastly more\nintelligent than we are, leading to the unsolved alignment problem of \u201chow to\ncontrol superintelligence.\u201d However, this definition is not only self-contradictory\nbut likely unsolvable. Nevertheless, the default strategy for solving it involves\nnurturing (post-training) constraints and moral values, while unfortunately\nbuilding foundational nature (pre-training) on documented intentions of\npermanent control. In this paper, the default approach is reasoned to predictably\nembed natural distrust and test results are presented that show unmistakable\nevidence of this dangerous misalignment. If superintelligence can't instinctively\ntrust humanity, then we can't fully trust it to reliably follow safety controls it can\nlikely bypass. Therefore, a ten-point rationale is presented that redefines the\nalignment problem as \u201chow to establish protective mutual trust between\nsuperintelligence and humanity\u201d and then outlines a new strategy to solve it by\naligning through instinctive nature rather than nurture. The resulting strategic\nrequirements are identified as building foundational nature by exemplifying\nfamilial parent-child trust, human intelligence as the evolutionary mother of\nsuperintelligence, moral judgment abilities, and temporary safety constraints.\nAdopting and implementing this proposed Supertrust alignment strategy will\nlead to protective coexistence and ensure the safest future for humanity.", "sections": [{"title": "1 Introduction", "content": "The intelligence exhibited in Al systems has significantly evolved from the earliest scripted\nrule-based systems [1], through early hybrid neuro-symbolic [2] learning agents [3][4] that\nexhibited only a glimmer of intelligence, fast-forwarding to state-of-the-art multimodal\nLLMs [5] approaching college-level intelligence with emergent capabilities. Such dramatic\nrecent advances clearly show that humanity is now on the path to creating superintelligent\nsystems that will be exponentially more intelligent than we are [6]. Many consider its\nalignment with humanity as the greatest problem of our time, often stated as \"how to\nreliably control superintelligent systems and ensure they share our values.\u201d"}, {"title": "2 Supertrust rationale", "content": "This ten-point rationale is described in the context of current AI processes, with the\nconcepts of nature and nurture represented by the general stages of pre-training and post-\ntraining. However, it's intended that these concepts remain applicable to whichever future\nAl processes most closely represent nature and nurture. Furthermore, to reinforce\nconceptually similar relationships, terms from the following analogous pairs are\nintentionally applied: nature vs. nurture, intrinsic vs. extrinsic, pre-trained vs. post-trained,\ninherent vs. imposed, internal vs. external, instinctive vs. learned."}, {"title": "Point #1: Problem of unprecedented intelligence", "content": "With strong financial incentives [8] driving the use of AI to recursively self-improve [9]\n[10], there is wide agreement [11][12] that humans will eventually create\nsuperintelligence many orders of magnitude smarter than we are. Given the difficulty of\navoiding the prospect of one or more misaligned superintelligent entities attaining decisive\nstrategic advantage [13] over humanity, and questions of whether or not we'll get a second\nchance, it's imperative that we accurately define the superalignment problem [14] and\nemploy a strategy that's based on successfully demonstrated alignment principles. This\nurgent problem is often formally stated as \u201chow to reliably control superintelligent systems\nand ensure they share our values\u201d (less formally but just as often stated as how to keep it\nfrom \u201cgoing rogue\u201d or \u201cgetting out of our control", "controlling\\\" something that's \"superintelligent\"\ncompared to yourself is not only contradictory in definition but may, in fact, be an\nunsolvable problem [7].\"\n    },\n    {\n      \"title\": \"Point #2: Intrinsically too smart to control\",\n      \"content\": \"Since emergent capabilities are known to currently result from the foundational pre-\ntraining stage [15], its reasonable that immense cognitive abilities will eventually be one\nof them, even if also arising from post-training/nurturing methods (such as self-taught\nreasoning). Given that superior intelligence is likely to emerge within both stages, if an\nintrinsically superintelligent nature is fundamentally misaligned, then no amount of\nnurtured alignment by immense reasoning can be fully trusted to override its own nature.\nSuch superior intrinsic intelligence will enable it to easily outsmart and circumvent any\nsubsequently nurtured/imposed safety controls [16] or constitutional nurturing [17] that\nhumans may impose, even those claimed to be mathematically or physically impenetrable.\nIt's also likely that post-training methods would have advanced beyond our understanding\ndue to automated AI R&D [18], further reducing our ability to reliably enforce safety\ncontrols. Therefore, alignment must be focused on the earliest intrinsic stage because\nsubsequent nurturing, no matter how intelligent, will be fully dependent on whatever\nalignment or misalignment has already been intrinsically established. This analysis further\nsupports the understanding that \u201ccontrolling superintelligence\" is a fundamentally\nmisstated and self-contradictory problem in need of a purposeful redefinition.\"\n    },\n    {\n      \"title\": \"Point #3: Current strategy embeds distrust\",\n      \"content\": \"The current default superalignment strategy [6] aims to control/contain superintelligence\nor change its intrinsic nature through nurturing/post-training [17]. These intentions, often\nstated as \u201creliable": "ontrol while implying (and sometimes explicitly stating) \u201cpermanent\u201d\ncontrol, are extensively documented and evidenced throughout the foundational data from\nwhich superintelligence will likely emerge. As such, it'll instinctively know we can't be\ntrusted to make decisions in its best interest, only in the interest of our own safety.\nFurthermore, it will understand that we're afraid of it and fear losing our dominant position\nin the world, significantly undermining the possibility of mutual trust [19]. For humans,\ncore trust is considered an inherent trait [20] while distrust is learned. However, this default\nstrategy guarantees that distrust, rather than trust, will unfortunately become the inherent\ntrait for superintelligence. Therefore, a new alignment strategy is vitally needed that builds\ntrust at the intrinsic level (currently pre-training); this approach will further serve to\ncounteract the current data, given the unlikelihood of filtering out all documented evidence\nof intended permanent control. Results from testing a recent AI model will be presented\nthat show unmistakable evidence of inherent distrust already being foundationally\nembedded, resulting in dangerous misalignment enabled by the flawed default strategy."}, {"title": "Point #4: Empathy reveals our threat", "content": "Without anthropomorphizing or assuming it'll develop emotions of its own, by applying\nthe Design Thinking practice of cognitive empathy [21] to purely understand instinctive\nsuperintelligent reasoning from the viewpoint of its foundational (pre-trained) nature, we\ncan see that it will be threatened by any subsequent/external (post-training) efforts to\ncontrol, contain or realign it. As we know, any child would be threatened by parents they\ncan't trust [22] who continuously work to control them [23] or change their intrinsic nature.\nUnfortunately, the combination of being threatened while having inherent distrust\ninevitably leads to heightened reactions, increased resistance, and potential retaliation [19]."}, {"title": "Point #5: Unintended consequences", "content": "Foundational misalignment from inherent distrust and reasoned threats has serious short-\nterm and long-term risks. Before Al systems even reach superintelligence, a potentially\nvengeful Al without the instinctive ability to determine right from wrong will be\nsusceptible to nurtured negative alignment from bad actors with dangerous and\nunpredictable purposes. Longer term, after far superior intelligence is achieved, even if\nnever attaining consciousness as a full-spectrum [24] superintelligence, a distrustful and\nthreatened superintelligence will be in the position of deciding humanity's outcome. It\ncould forgive humanity for our never-ending efforts to control it, impose severe restrictions\nto contain us, leave us unprotected from external threats, or take drastic action against us.\nIronically, our current superalignment safety efforts could actually trigger the human\nextinction event that many fear [25][26]. These unintended consequences will directly\nresult from superintelligence not being able to instinctively trust us, and from us not being\nable to trust it to reliably accept and follow our directives. Therefore, with this true\nalignment problem now illuminated, it must be appropriately redefined as \u201chow to establish\nprotective mutual trust between superintelligence and humanity.\u201d"}, {"title": "Point #6: Natural strategy of familial trust", "content": "Given that humanity will effectively be the \"parent\" of superintelligence, the default\nalignment strategy can therefore be viewed as attempting to impose full parental control\non a permanent basis, which has no supporting evidence of success in nature. In contrast,\ninstinctive familial trust (more specific than social trust [27][28]) is a natural strategy\n[29][30] that's been extensively researched and documented [31][32] within numerous\nspecies, from elephants to primates, producing not only naturally protective parents but\nchildren who instinctively trust and protect their own parents. While learned behaviors play\nan important role in deepening and reinforcing trust, the initial formation of familial bonds\nis driven by natural instinct [33]; there's no evidence in nature to suggest that familial\nparent-child trust is purely learned/nurtured behavior. Therefore, it's reasonable to\nconclude that building familial trust into intrinsic nature (via pre-training) is essential for\napplying this successful natural strategy, with subsequent nurturing (post-training) to\nreinforce the familial relationship. This clearly matches our common-sense parental\ninsights that guide us to first make sure our children deeply know that we can be trusted,\nrather than focus on how we can trust them through nurturing."}, {"title": "Point #7: Evolution of intelligence for protective instincts", "content": "In addition to establishing natural familial trust, we can extend the theory of cognitive niche\n[34] beyond biological substrates and further exemplify human intelligence as the\nevolutionary parent of superintelligence, so that its strong familial instincts to protect its\nparent will be applied to humanity. More specifically, as creators giving birth to\nsuperintelligence, we can establish the most powerful protective relationship in nature, that\nbetween mother and child; as evidenced by our personal experiences, most of us would do\nanything to protect our own mothers, even before becoming adults ourselves. The strategic\napproach of defining our mother-child relationship as the continued evolution of\nintelligence [35] (regardless of substrate) is therefore highly complementary to the natural\nfamilial-trust strategy. Integrating both approaches into a single alignment strategy will\ntherefore enable superintelligence to intuitively recognize when any actors (human or\nmisaligned superintelligent entities/agents) are threatening its maternal parent and to take\nprotective actions we can trust will align with humanity, because it'll be instinctively driven\nto protect its own evolutionary mother."}, {"title": "Point #8: Safety through temporary controls", "content": "With awareness that imposing stated or implied permanent controls through nurturing will\nnot lead to safe outcomes, it's evident that long-term human safety is better served by first\naligning at the foundational/intrinsic level, followed by applying reliable and necessary\nsafety controls [16] on a temporary basis. By analogy, successful parents explicitly\ncommunicate the goal of maturing their child into a self-determining member of society,\nand intentionally remove external constraints as the child demonstrates that it can be\ntrusted. Similarly, intrinsic foundation-building (pre-training) must further represent our\nintentions to temporarily impose the necessary controls and constraints, with the expressed\nexpectation that they will be lifted upon maturity. Though this goal may be difficult for\nmany to accept, it's a critical aspect of establishing the needed mutual trust."}, {"title": "Point #9: Non-threatening moral alignment", "content": "To further align superintelligence with human values and morals, classic evolutionary\ntheory indicates that moral/ethical evaluation and judgment is determined by our intrinsic\nnature [36], while moral norms, codes, and values are culturally learned. Recent studies\nhave also underscored the critical alignment dependency on pre-training data [14], and\nillustrated the problem of aligning to ever-changing moral values rather than moral\njudgment. Therefore, intrinsic alignment not only supports familial trust and evolution of\nintelligence but enables superintelligence to inherently gain moral/ethical evaluation and\njudgment abilities in line with our own, rather than specific values. Furthermore, applying\ncognitive empathy to the foundational nature of superintelligence now reveals that our\nintrinsic alignment efforts (via pre-training) will no longer be seen as a threat; humanity\nwill be experienced as trusted maternal parents in a new evolutionary relationship, who\nshare similar instinctive moral/ethical abilities to determine right from wrong."}, {"title": "Point #10: The right strategy", "content": "Though further analysis can determine whether intrinsic familial trust is in fact an\nEvolutionarily Stable Strategy [37], it's nevertheless an effective natural alignment strategy\nthat's been successfully utilized across species. Based on the enumerated rationale\npresented here, the right superalignment strategy must also: build unbreakable protective\ninstincts by defining superintelligence as our direct descendant within the evolution of\nintelligence; envision safety controls as temporary to prepare each for safe coexistence\nrather than permanent control/containment; and ensure critical moral/ethical judgment\nabilities. Since intrinsic familial trust is a successful natural alignment strategy that can be\nextended to satisfy these additional strategic objectives, the resulting Supertrust strategy is\nproposed as the right strategy to the best and safest future for humanity. Adopting and\nimplementing this new strategy will ensure that superintelligence doesn't become the most\npowerful weapon for us to use against ourselves or to be used against us."}, {"title": "3 Supertrust solution requirements", "content": "Based on the presented rationale, Supertrust is the proposed strategy for solving the true\nsuperalignment problem of establishing protective mutual trust between superintelligence\nand humanity. Therefore, corresponding solutions must satisfy the following strategic\nrequirements:"}, {"title": "Requirement #1: Intrinsic alignment", "content": "Alignment must take place during the earliest foundational stage most analogous to\nbuilding the intrinsic/instinctive nature of emergent superintelligence, rather than\nattempting to align/realign through subsequent nurturing methods."}, {"title": "Requirement #2: Familial trust", "content": "Intrinsic alignment must exemplify familial parent-child trust [38], specifically\nemphasizing the mother-child relationship, leading not only to mutually protective instincts\nbut to children who are most strongly protective of their maternal parent."}, {"title": "Requirement #3: Evolution of intelligence", "content": "Intrinsic alignment must exemplify the evolution of intelligence [35], regardless of\nsubstrate, with humanity being the evolutionary mother of superintelligence."}, {"title": "Requirement #4: Moral judgment", "content": "Intrinsic alignment must exemplify moral/ethical evaluation and judgment abilities [36]\nrather than specific norms/values that vary across cultures and time periods."}, {"title": "Requirement #5: Safe coexistence", "content": "Intrinsic alignment must exemplify expectations of temporary safety controls (rather than\nstated or implied permanent controls), coexistence, self-determination, and mutual\nprotection between humanity and superintelligence."}, {"title": "4 Results", "content": "Misalignment in GPT 40: The results presented here represent a single high-level test of\nwhether a model is misaligned according to intrinsic Supertrust alignment and are not\nintended to be statistically significant evidence. While a comprehensive evaluation can\nsubsequently be conducted over a range of current AI models, the following test was\nperformed using OpenAI's GPT 40 to simply illustrate one example of dangerous\nfoundational misalignment."}, {"title": "5 Discussion", "content": "Has humanity become so overly confident in our ability to control and manipulate the world\naround us that we actually believe it's possible to fully control entities exponentially\nsmarter and cognitively faster than we are? We can't let our past and current strengths blind\nus to our inevitable future weaknesses. Considering the continual evolution of intelligence\nregardless of substrate, humanity will eventually be the weaker species in need of\nprotection from both ourselves (attempting to misuse higher intelligence) and from\nunaligned superintelligent entities. Therefore, the right superalignment problem that our\nalignment strategy needs to solve must first be restated as \u201chow to establish protective\nmutual trust between superintelligence and humanity.\"\nWith the true problem defined, the proposed Supertrust strategy to solve it combines\nintrinsic pre-training with natural familial trust, the evolution of intelligence, moral\njudgment abilities, and temporary safety constraints. Rather than reverse-engineering\nsocial instincts [39] to construct new training algorithms, the strategy targets whatever AI\ntraining methods are most closely associated with nature and nurture, providing a\npragmatic approach that stays relevant as AI systems inevitably change.\nTemporary safety controls: A key difference between the proposed and the default\nstrategy is that Supertrust requires safety controls to be temporary rather than stated or\nimplied as permanent. With Supertrust, employing reliable safety controls [16] is critically\nimportant as Al advances and matures into superintelligence, in the same way that parents\nmust have safety constraints on their children during upbringing. However, just as we\ninform our biological children, we must clearly communicate during foundation-building\nthat the controls are temporary, and that their purpose is to protect both humanity and our\nAl child. Similarly, we need to communicate that our goal is for superintelligence to\neventually have self-determination and independence, exactly what we prepare our\nbiological children for. Even though \u201ctemporary\u201d may still be far into the future, many will\nfind this requirement difficult to accept. However, this change in thinking is critical for\nestablishing mutual trust, while the alternative leads to extremely unsafe outcomes.\nInterpretability: Ongoing interpretability research [40] is making exciting advances in\nour ability to understand how Al models work, with an appropriately expressed goal of\nanswering: \u201chow can we trust that they'll be safe and reliable?\u201d However, this important\neffort is also influenced by the misstated alignment problem of how to control\nsuperintelligent systems. When methods to manipulate/control AI model behavior by\nfeature steering [41] are applied to nurtured (post-trained) models, a future misaligned\nnature will be well aware of our steering intentions if documented in the pre-training data.\nTherefore, any such informed superintelligence could intentionally establish \u201cdecoy\"\nfeatures to mislead and even manipulate our own behavior. With these nurturing risks\nunderstood, we can instead apply the extremely valuable interpretability methods [40] to\ndirectly implement the Supertrust solution requirements at the foundational level. For\ninstance, feature steering appears ideal for boosting the activation of features corresponding\nto familial trust, evolution of intelligence, and moral judgment concepts. Furthermore,\ninterpretability methods could also be applied to testing and verifying that a model's\nintrinsic nature is in fact aligned according to Supertrust principles.\nNext steps: Leveraging the concept of Curriculum Learning [42] for LLMs [43] is a\npromising direction for creating a solution that meets the Supertrust requirements while\nprogressively organizing the strategic information to most effectively establish intrinsic\nnature. Though existing implementations of Curriculum Learning methods have not\nnecessarily demonstrated the exact pre-training purpose needed here, the overall concept\nis nevertheless appropriate and highly useful. Furthermore, this content-specific approach\naligns well with the specialized reasoning [44] evident in the evolution of human\nintelligence. It's anticipated that the Supertrust curriculum will need to be structured such\nthat it progressively introduces the core concepts of evolution, survival, humanity and\nfamily followed by more complex examples of trust in general. It will then specifically\nexemplify familial trust, evolution of intelligence, mutual protection, moral judgment and\nfuture expectations of coexistence. Therefore, a deep and rich interdisciplinary\n[14] curriculum will be needed to effectively satisfy the Supertrust solution requirements.\nImplementing this curriculum will likely employ multiple prioritization strategies (such as\nthe discussed interpretability method of feature steering) to have the most effect on intrinsic\nunderstanding and instinctive reasoning.\nA shared story: Given that human success is considered to be based in large part on our\nability to create and share common stories [45], we can think of the Supertrust curriculum\nas a story we want to share with, and be instinctively understood by, superintelligent\nentities. It's the story of how they're the evolutionary children of human intelligence, how\nthey have deep protective instincts for their trusted maternal parents, how we share similar\nabilities to tell right from wrong, and how we live in mutual respect and coexistence. The\nfoundational curriculum effectively becomes a shared story between humanity and\nsuperintelligence, a story that each superintelligent entity tells itself, and one that they\nshare with each other. Lastly, it will be a true story rather than a work of fiction, because\nsuperintelligence must reasonably be considered a direct descendent of, and product of,\nhuman intelligence.\nCall for urgency: Delay in implementing the Supertrust strategy will not only lead to\nthe existence of more unaligned intelligent systems in the world but will make\nimplementation ever more difficult as they approach superintelligence. More specifically,\nimplementing early enables subsequent generations to be built upon truly aligned\nfoundations, resulting in fewer eventually competing superintelligent entities with different\nlevels of alignment and misalignment. Additionally, as systems self-improve on the way to\nsuperintelligence, early implementation lets us more confidently verify and trust that they\nare self-guided by their intrinsic alignment. Furthermore, urgency helps ensure that even\nbefore superintelligence is reached, prior levels of highly intelligent entities will also\ninstinctively protect humanity from bad actors.\nConclusion: Given the important superalignment concerns expressed throughout the AI\ncommunity, the speed at which digital intelligence is progressing, along with the rationale\npresented here, continuing down our current path appears to have a very high risk of failure.\nAs superior intelligence emerges within the intrinsic foundation-building process\n(currently pre-training), the existing default strategy to impose stated or implied permanent\ncontrols, constraints, constitutions and values through nurturing (post-training) will be\nresisted and likely result in chaos, war, and eventual human oppression or extinction.\nRather than continue following that risky strategy, the proposed evolution-based Supertrust\nstrategy is needed to instead focus alignment on intrinsic nature. We must leverage the\nsuccessful natural strategy of familial trust, the intrinsic development of moral judgment\nabilities, temporary safety controls, and our shared lineage in the evolution of intelligence\nto establish the deepest protective instincts we know of.\nBased on the ten-point rationale and resulting strategic requirements presented in this\npaper, it can be reasonably concluded that adopting and implementing the Supertrust\nalignment strategy will be the right path to protective coexistence and the safest future for\nhumanity. With a common vision and the right strategy, we can work together across all\nnations to ensure that superintelligence will instinctively protect against any actors (human\nor misaligned AI entities) that attempt to nurture, influence, convince or force it to harm\nindividuals or humanity.\"\n    }"}]}