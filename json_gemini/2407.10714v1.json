{"title": "SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation", "authors": ["Kaiming Shen", "Xichen Ding", "Zixiang Zheng", "Yuqi Gong", "Qianqian Li", "Zhongyi Liu", "Guannan Zhang"], "abstract": "The modeling of users' behaviors is crucial in modern recommendation systems. A lot of research focuses on modeling users' lifelong sequences, which can be extremely long and sometimes exceed thousands of items. These models use the target item to search for the most relevant items from the historical sequence. However, training lifelong sequences in click through rate (CTR) prediction or personalized search ranking (PSR) is extremely difficult due to the insufficient learning problem of ID embedding, especially when the IDs in the lifelong sequence features do not exist in the samples of training dataset. Additionally, existing target attention mechanisms struggle to learn the multi-modal representations of items in the sequence well. The distribution of multi-modal embedding (text, image and attributes) output of user's interacted items are not properly aligned and there exist divergence across modalities. We also observe that users' search query sequences and item browsing sequences can fully depict users' intents and benefit from each other. To address these challenges, we propose a unified lifelong multi-modal sequence model called SEMINAR-Search Enhanced Multi-Modal Interest Network and Approximate Retrieval. Specifically, a network called Pretraining Search Unit (PSU) learns the lifelong sequences of multi-modal query-item pairs in a pretraining-finetuning manner with multiple objectives: multi-modal alignment, next query-item pair prediction, query-item relevance prediction, etc. After pretraining, the downstream model, which shares the same target attention structure with PSU, restores the pretrained embedding as initialization and finetunes the network. To accelerate the online retrieval speed of multi-modal embedding, we propose a multi-modal codebook-based product quantization strategy to approximate the exact attention calculation and significantly reduce the time complexity.", "sections": [{"title": "1 INTRODUCTION", "content": "Users' behavior modeling is extremely important in modern commercial recommendation systems, including online e-commerce platforms such as Amazon, Taobao, Alipay, and content platforms such as YouTube, TikTok, etc. As users spend more time on online shopping and watching short videos, the length of users' historical behaviors has grown dramatically from a few hundreds (102) to more than ten-thousands (104) in recent years. A lot of recent research focuses on modeling users' lifelong behaviors, such as Efficient Target Attention (ETA) [3], Two-Stage Interest Network (TWIN) [2], Query-Dominant Interest Network (QIN) [8], etc. These models follow a cascading two-stage paradigm, which first uses the target item or target search query as a trigger to retrieve the top-K relevant behaviors from historical behaviors. In the second stage, it uses target attention to encode the selected behaviors as users' interest representation. This paradigm is widely adopted in many search and recommendation tasks, such as click through rate (CTR) prediction and personalized search ranking. The item representations in the sequence are computed using both the item ID feature and more generic attributes' features. One easily neglected problem in existing lifelong behavior modeling is the insufficient learning problem of ID features in the lifelong sequence, such as historical item ID, author ID, etc. Many historical items in the lifelong sequence can't be found in the current training dataset, which is collected from the most recent logs of exposures and clicks. These low frequency ID embeddings can't be learned well by the limited dataset after being randomly initialized, which will harm the accuracy of target attention calculation.\nThe second problem in existing lifelong sequence modeling is that it can't handle multi-modal features of items in the sequence well, such as text and image features. The norm values of vectors from different modalities vary if the modalities are not properly aligned in the same embedding space. Existing target item attention calculation uses the inner product of query and keys, which may be dominated by modality vectors with large norm values. For example, the target item will only retrieve the top-K visually relevant but semantically very different items from historical behaviors, which will deteriorate the online performance of recommendation.\nTo tackle these problems, we propose a new model called Search Enhanced Multi-Modal Interest Network and Approximate Retrieval (SEMINAR) to model users' lifelong historical multi-modal behaviors. The users' historical behaviors include heterogeneous behaviors of both the sequence of browsing item and the sequence of search query. We align users' search query sequence with browsing item sequence together as a unified sequence of query-item pairs, which can be retrieved flexibly by target item or target search query in both the CTR prediction task and Personalized Search Ranking (PSR) task. SEMINAR proposes a Pretraining Search Unit (PSU) network to learn the lifelong behavior sequence of historical multi-modal query-item pairs. It introduces multiple pretraining tasks designed to solve the insufficient learning issue of historical ID features and the multi-modal alignment. In downstream tasks, the target attention module restores the learned item representations from PSU, using the pretrained ID embedding as initialization, and applies a projection weight matrix to get the transformed representation of the behavior sequence. During online serving, calculating exact attention using the inner product of multi-modal vectors in the lifelong sequence has the time complexity of O(L \u00d7 M \u00d7 d), which is time consuming. L denotes the sequence length, M denotes the number of modalities and d denotes the embedding dimension. Different from existing approximate retrieval methods, such as Locally Sensitive Hash (LSH) and Hierarchical Navigable Small World (HNSW), we exploit an approximate strategy of Product Quantization in a multi-modal setting and express the multi-modal item representations as discrete integer codes using the quantization codebooks, and sum the inner product of centroids in sub-vectors to approximate the exact attention calculation. During online serving, the attention calculation is equivalent to pre-computed distance table lookup and summation operations, which can be conducted efficiently.\nIn summary, the main contributions of our work are as follows:\n\u2022 We identify the insufficient learning problem of ID features in lifelong behavior modeling and observe that target attention calculation is dominated by multi-modal features with large norm values. And we novelly propose SEMINAR framework, which includes the Pretraining Search Unit to effectively alleviate the insufficient learning problems of ID embedding and multi-modal alignment.\n\u2022 We exploit a product quantization approximation strategy in a multi-modal setting, which can reduce the time complexity during online serving of retrieval using target query item pair from historical behaviors.\n\u2022 We conduct extensive experiments on real-world datasets to demonstrate the effectiveness of our proposed model. And we also released the code of SEMINAR in this repository 1 to encourage further research."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 Long-Term Lifelong User Behavior Modeling", "content": "Long-term lifelong user behavior modeling has attracted much research attention in recent years. Typical works include SIM [12], ETA [3], TWIN [2], QIN [8], etc. SIM [12] introduces the General Search Unit to retrieve the top-K most relevant items from historical behaviors using the target item as a trigger, and the Exact Search Unit (ESU) to calculate the multi-head target attention (MHTA). ETA [3] uses a set of hash functions to express the item representation as binary hash embedding and calculates the Hamming distance to approximate the inner product calculation. TWIN [2] introduces the CP-GSU as a consistency-preserved lifelong user behavior modeling module to increase the relevance calculation consistency between the two cascading stages. QIN [8] uses the search query as a trigger to retrieve the most relevant items from the historical behaviors in the first stage of the cascading models in Personalized Search Ranking. Different from the existing work, we propose the pretraining search unit (PSU) to alleviate the insufficient learning problem of ID features and multi-modal alignment in attention calculation. Furthermore, there is an increasing trend of modeling search and recommendation tasks jointly in a unified framework, such as USER [19], SESRec [15], S&R Foundation [7], etc. To model the lifelong behaviors, we align the historical search query sequence and browsing item sequence as a unified sequence of query-item pairs, which can be applied to both CTR prediction in recommendation and personalized search ranking."}, {"title": "2.2 Multi-Modal Alignment in\nRecommendation and Item Quantization", "content": "Multi-modal alignment is a prevalent topic, which aligns the multi-modal features such as text and image in a unified embedding space in a contrastive learning manner. Typical works include CLIP [13], etc. Some researchers have focused on modeling multi-modal user sequences in recommendation. M5 [21] applies a multi-modal embedding layer to extract both ID embeddings of show ID and content-graph embeddings initialized from a meta-path pretrained model. To better increase the generalization of ID embeddings, some research is proposed to express item representations as quantized vectors in discrete codes, including Product Quantization [10], VQ-VAE [17], RQ-VAE [20], etc. VQ-Rec [9] proposes to encode text as discrete codes using product quantization techniques and use transformer to learn cross-domain data in recommendation. TIGER"}, {"title": "3 PROPOSED MODEL", "content": null}, {"title": "3.1 Problem Formulation", "content": "We can split the sequence of users' behaviors into several heterogeneous sub-sequences, including the sequence of search queries Q = {q1, q2, .., q|Q|} of explicit intents and the sequence of browsing recommended items B = {i1, i2, ..., i|B|}. For search behaviors, users input a query q \u2208 Q and interact (click or view) with a few items related to the query, resulting in the aligned sequence of query and item pairs (q\u0131, i\u2081). For the behavior of browsing recommended items, users browse a sequence of items without explicit search intent, and we pad an empty search query q = 0 to each item to obtain the query-item pair as (q1 = 0, i\u2081). Finally, we construct a unified sequence of aligned query-item pairs in chronological order with length L, denoted as {(q\u0131, i1)}I=1:L. In some recommendation scenarios, such as short video recommendations of YouTube and TikTok, each item has multi-modal features such as text (title of video), image, and attributes (authors and categories). We further split the sequence of browsed items B into M multi-modal sub-sequences, including a sequence of text features T = {T1, T2, ..., T\u2081}, a sequence of image features I = {I1, I2, ..., IL }, and a sequence of attribute features A = {A1, A2, ..., AL }. Finally, we let [Q,T, I, A] \u2208 R(M+1)\u00d7L\u00d7d denote the input sequence of multi-modal query-item pairs to the SEMINAR model and d denotes the dimension of aligned representations."}, {"title": "3.2 Aligned Lifelong Sequence of Multi-Modal\nQuery-Item Pairs", "content": "The aligned sequence of multi modal query-item pairs pass the embedding layers. We let [x1 = (xquery, xitem)]1=1:L denote the historical sequence of query and item pairs. xquery \u2208 Rd, xitem = (xtext, ximage, xattributes) \u2208 RM\u00d7d. In CTR prediction, target attention (TA) is a structure which uses target item to retrieve the most relevant items from the sequence of historical behaviors. We extend TA from target item to target query-item pair to retrieve most relevant top K pairs from historical sequence. We denote the target query-item pair as xt = (xqueryxtext, image, xattributes)."}, {"title": "3.3 SEMINAR Model Architecture", "content": "Our proposed model SEMINAR in Figure 1 introduces a new network Pretraining Search Unit (PSU) to pretrain using dataset of the lifelong sequence of multi-modal query-item pairs. Section 3.3.1 introduces the PSU and corresponding pretraining tasks. Section 3.3.2 introduces how the recommendation model restores the pretrained query and item representations from PSU as initialization and applies a projection matrix to get the transformed representation of the sequence. Top-K relevant pairs are retrieved by the target pair and participate in the multi-head target attention (MHTA) calculation. Section 3.3.3 introduces the multi-modal product quantization approximation."}, {"title": "3.3.1 Pretraining Search Unit", "content": "The input to PSU is the aligned sequence of query-item pairs as [x1]1=1:L. L denotes the length of the aligned sequence and x1 = (xquery, text, image, xattributes) represents the l-th behavior in the sequence, which consists of the query embedding and multi-modal embedding. The query q \u2208 Q passes through the query feature encoder f(.), resulting in Q = f(xquery) \u2208 RLxdquery. Following the multi-modal alignment literature such as CLIP [13], we use Transformer [18] to encode the text feature as T = Encodertext (xtext) \u2208 RLxdtext_and ViT [5] to encode the image features as I = Encoderimage(ximage) \u2208 RL\u00d7dimage. Additionally, we encode features of the attributes using the function g(.). A = g(xattribute) \u2208 RL\u00d7dattribute is treated as one channel of the sub-sequence which participates in the multi-modal alignment of the item sequence. To project the representations of different channels to the same dimension d, we further multiply them by linear weight matrix {Wq, Wt, Wi, Wa} and get the stacked input sequence of multi-modal query-item pairs as follows: x = [QWq, TWt, IW\u2081, AWa] \u2208 R(M+1)\u00d7L\u00d7d.\nNext Pair Prediction and Multi-Head Target Attention. The intuition behind PSU is to design a pretraining network to learn from the lifelong behavior sequence, and the pretraining network should share the same structure of multi-head target attention with the cascading two-stage downstream model, such as ETA [3] and TWIN [2]. The downstream model restores the pretrained query and item embeddings as initialization of parameters and fine-tunes the network. Different from the masked language model (MLM) in BERT [4], which uses tokens from the context window to predict the masked token, we use next-pair prediction as a pretraining task to predict the correct last query and item pair. We intentionally leave out the last query-item pair in the sequence XL = [x(m)], m \u2208 [Q, T, I, A], and treat it as the target query-item pair to retrieve from the previous (L - 1) sequence using multi-head target attention. To pad the sequence length from L \u2013 1 to L, we further add a special token < EOS > to the end of the previous L-1 items in the sequence X1:L-1. The next query-item pair prediction task is formulated as classification tasks: y = p(x] | xM +1; x<EOS>) with the loss\n$\nL_{next}^{\\text {pair }}=-\\sum_{i=1}^{L} c e\\left(y_{i} ; x_{\\text {query }}, \\sum_{m \\in M} \\gamma_{m} x_{\\text {item }}^{(m)}\\right)\n$\nPositive label is assigned to the correct last pair, and negative labels are assigned to negatively sampled query-item pairs.\nTo better represent the historical behaviors and target query-item pair, we need to fuse the query and multi-modal item representations into a single vector as:\n$\nx=\\lambda x_{\\text {query }}+(1-\\lambda) \\sum_{i} w_{i} x_{\\text {item }}^{(i)}=\\sum_{m \\in M+1} \\gamma_{m} x^{(m)}\n$\n\u03bb and (1 \u2013 \u03bb) denote the weight to merge representations of query and item vectors respectively as \u03bb\u2208 [0, 1], and wi denotes the weight to merge multi-modal item representations. To simplify the notations, we use a single vector [ym]1:M+1 \u2208 RM+1 to represent the weight of all (M + 1) channels and the sum of the weight equals to 1 as \u2211ym = 1. The weight vector ym can be learned dynamically as the softmax output of a gating network. The attention is calculated as the inner product of queries and keys of the merged multi-channel representations. The final attention score will be dominated by the modals with large norm values |x(m) | and large weight ym, and the information from other modals will be easily ignored. So we specifically decompose the attention score calculation into the norm value part |x(m)| and unit vector part x(m).\nWe let qt denote the representation of target query-item pair as\n$\nq_{t}=\\sum_{i} \\gamma_{i} x^{(i)}=\\sum_{i} y_{i}\\left|x^{(i)}\\right| \\hat{x}^{(i)}\n$\nNote that the |xi)| denotes the norm value of the i-th channel of target item and $\\hat{x}^{(i)}$ is a unit vector. Similarly, we can express the l - th historical behavior k\u012b as\n$\nk_{l}=\\sum_{j} y_{j} x_{l}^{(j)}=\\sum_{i} y_{j}\\left|x_{l}^{(j)}\\right| \\hat{x}_{l}^{(j)}\n$\nNote that the unit vectors of multi-modal sequence representations will participate in the multi-modal alignment task in the next section.\nThe h-th head in the multi-head attention is represented as headPSUh = Attentionh (qt, KPSU, VPSU), and the attention score aPSU is calculated as inner product of d-dimensional vector query and keys multiplied by a scaling factor $\\frac{1}{\\sqrt{d}}$ as\n$\n\\begin{aligned}\n&\\alpha_{h}^{\\mathrm{PSU}}=\\frac{\\left(q_{t} W_{h}^{\\mathrm{PSU} Q}\\right)\\left(K^{\\mathrm{PSU}} W_{h}^{\\mathrm{PSU} K}\\right)^{T}}{\\sqrt{d}}\n\\\\\n&\\alpha_{h}^{\\mathrm{PSU}}=\\frac{\\left(\\sum_{i} y_{i}\\left|x^{(i)} W_{h}^{\\mathrm{PSU} Q}\\right| \\hat{x}^{(i)}\\right)\\left(\\sum_{j}\\left|x_{\\ell}^{\\mathrm{PSU}(j)}\\right| \\gamma_{i} \\hat{x}_{\\ell}^{(j)} W_{h}^{\\mathrm{PSU} K}\\right)^{T}}{\\sqrt{d}}\n\\\\\n&\\alpha_{h}^{\\mathrm{PSU}}=\\frac{\\left[\\sum_{i j} y_{i j}\\left(x^{(i)} W_{h}^{\\mathrm{PSU} Q}\\right)\\left(x_{\\ell}^{\\mathrm{PSU}(j)} W_{h}^{\\mathrm{PSU} K}\\right)^{T}\\right]_{\\ell=1}^{L}}{\\sqrt{d}}\n\\end{aligned}\n$\nIn this formulation, [xPSU (1:M+1)]=1 \u2208 RL\u00d7(M+1)\u00d7d denotes the multi-modal embedding of items in the sequence of PSU and xPSU (1:M+1) = [Q,T, I, A]. And KPSU = [\u2211i YixPSU (i)]=1 \u2208 RLxd denotes the merged representations of input sequence. WWPSUQ \u2208 Rdxd and WPSUK \u2208 Rd\u00d7d denote the projection weight matrix of query and keys in h-th head, and yij denotes the weight of cross-modal interaction of unit query vector and unit key vector in the sequence. Yij equals to the scalar product of yi, yj, the norm value of query vector |xPSU(i) | and the norm value of key vector |xPSU (j)|."}, {"title": "Multi-Modal Alignment and Query-Item Relevance", "content": "Multi-modal alignment is a crucial task, which learns the multi-modal representation in a same embedding space. Typical alignment models, such as CLIP [13], maximize the cosine similarity of the correct N (text-image) pairs and minimize the cosine similarity of the incorrect N\u00b2 \u2013 N mismatch pairs. We simultaneously train multi-modal alignment tasks, including text-image, image-attributes, text-attributes with the cross entropy loss of N pairs.\n$\nL_{\\text {align }}=\\sum_{i \\in M} \\sum_{j \\in M \\neq i} L_{C L I P}\\left(\\left[x_{l}\\right]_{l=1}^{L} \\times \\left[x_{l}^{\\prime}\\right]_{l=1}^{L}\\right), \\quad(i, j) \\in\\{T, I, A\\}\n$\nSequence length L is usually large and the alignment has complexity of O(L2). To reduce the complexity, we further split the sequence into Nch chunks. Each chunk is a sub-sequence with length Lsub = $\\frac{L}{N_{c h}}$. The alignment loss is the sum of multiple losses within chunks as\n$\nL_{C L I P}^{(l)}=\\sum_{k \\in \\text { ch }} L_{C L I P}\\left(\\left[x_{l}\\right]_{L: L+1}\\right)\n$\nwith complexity reduced to O(L2/Nch).\nAdditionally, query item relevance prediction is a typical search task, usually modelled as binary classification to predict the correct query-item pair from irrelevant query-item pairs. Each pair of query and item is represented as [xquery;xitem = m\u2208M Ymxitem(m)].\nLoss for query-item relevance binary classification task is Lquery-item =\n$\n\\sum_{i=1}^{L} c e\\left(y_{i} ; x_{\\text {query }}, \\sum_{m \\in M} y_{m} x_{\\text {item }}^{(m)}\\right)\n$\nyi denotes relevance label of the 1-th pair in the sequence. Positive label is assigned to the correct query-item pair and negative label is assigned to randomly sampled irrelevant query-item pair.\nLoss of Pretraining Search Unit. The objective of Pretraining Search Unit (PSU) consists of three parts, the next query-item pair prediction loss Lherr, multi-modal alignment loss Lalign and the query-item relevance prediction loss Lquery-item. LPSU = Lhext + Lalign + Lquery-item."}, {"title": "3.3.2 Fine-tuning the projection weight", "content": "Existing lifelong sequence modeling methods follow a cascading two-stage paradigm. In the first stage, target item or query is used as trigger to retrieve the most relevant top-K items from the users' long behaviors sequence and reduce the sequence length from L to K, such as the General Search Unit (GSU) in SIM [12], TWIN [2], and Relevance Search Unit (RSU) in QIN [8]. In the second stage, a multi head target attention (MHTA) unit in Exact Search Unit (ESU) is applied to encode the selected K relevant items as the representation of users' behavior sequence. However, existing cascading two-stage paradigm suffers from the insufficient learning problem of ID embedding in the lifelong sequence ([xquery, xtext, ximage attributes ]). The downstream model, e.g. CTR prediction, lacks of enough training data to learn the embedding in the sequence well. Especially when some low-frequency items in the sequence exist a long time ago (more than one year) and don't exist in the training data, which are collected from most recently users' logs.\nTo help alleviate the insufficient learning problem of ID embedding in the lifelong sequence, the general search unit (GSU) in our proposed SEMINAR model shares the same multi-head target attention structure headGSUn = Attention\u0127(qt, KGSU, VGSU) with the structure in PSU as headPSUn = Attention(qt, KPSU, VPSU), restores the pretrained embedding from PSU and applies specific projection weight matrix G(j) \u2208 Rd\u00d7d to the pretrained embedding. After the first stage retrieval, the sequence length is reduced from L to K, the second stage ESU also shares the same multi-head target attention structure headESUh = Attention(qt, KESU, VESU) with GSU and PSU, and has specific projection weight matrix W, WK, WV of each head.\nhh\nGSU restores the pretrained query item multi-modal embedding [EPSU (Q), EPSU (T), EPSU(I), EPSU(A)] from PSU, and applies pro-jection matrix G(j) \u2208 Rd\u00d7d to get the projected embedding in GSU as xGSU (j). EPSU (*) denotes the pretrained multi-modal embedding. And the attention score aGSU in the h-th head of GSU's multi-head target attention is calculated as:\n$\n\\begin{aligned}\n&\\alpha_{h}^{\\mathrm{GSU}}=\\frac{\\left(q_{t} W_{h}^{\\mathrm{GSU} Q}\\right)\\left(K^{\\mathrm{GSU}} W_{h}^{\\mathrm{GSU} K}\\right)^{T}}{\\sqrt{d}}\n\\\\\n&x_{l}^{\\mathrm{GSU}(j)}=x_{l}^{\\mathrm{PSU}(j)} G_{j}, \\forall j \\in M+1\n\\end{aligned}\n$\nComparing the GSU attention a&SU with the pretrained PSU attention aPSU, we can see that the structures of multi-head target attention are exactly the same. The projection weights Wh W and WGSUK of queries and keys for each head in multi head atten-tion are different from W WPSUQ and WPSUK. And the embedding projection weight matrix Gj is unique to GSU.\nhh\nh\nh\nIn the second stage, the top-K relevant query-item pairs are selected from GSU and fed to Exact Search Unit (ESU) as headESUn =\nAttentionh (qt, KESU, VESU).\nIn ESU, KESU = TopK(KGSU) \u2208 R(M+1)\u00d7K\u00d7D represents the sequence of retrieved top-K representations from KGSU \u2208 R(M+1)\u00d7L\u00d7D.\nThe attention score in ESU is denoted as aESU and the ID embedding in ESU is denoted as xESU (j).\n$\n\\begin{aligned}\n&\\alpha_{h}^{\\mathrm{ESU}}=\\frac{\\left(q_{t} W_{h}^{\\mathrm{ESU} Q}\\right)\\left(K^{\\mathrm{ESU}} W_{h}^{\\mathrm{ESU} K}\\right)^{T}}{\\sqrt{d}}\n\\\\\n&x_{l}^{\\mathrm{ESU}(j)}=x_{l}^{\\mathrm{GSU}(j)}=x_{l}^{\\mathrm{PSU}(j)} G^{(j)}, \\forall j \\in M+1\n\\end{aligned}\n$\nFinally, users' lifelong sequence representation Xlifelong_seq is calculated as: Xlifelong_seq = Concat(headESU1, ..., headESUH)WESU. And xlifelong_seq is concatenated with other user, item, user-item interaction (u2i) and context features and participate in CTR prediction. \u011di = fo; (Xlifelong_seq, Xu, Xi, Xu2i, Xcontext) denotes the predicted value and yi denote the actual label value. And the final loss of CTR prediction is Letr = \u2211i Lce (yi, yi)."}, {"title": "3.3.3 Approximate Retrieval of Multi-Modal Query-Item Pair", "content": "The exact calculation of the attention score between the target query-item pair qt and the 1-th query-item behavior k\u2081 is the inner product of the weighted sum of multiple vectors as:\n$\nq_{t}^{T} k_{l}=\\left(\\sum_{i \\in M+1} y_{i} x^{(i)}\\right)\\left(\\sum_{j \\in M+1} y_{j} x_{l}^{(j)}\\right), \\forall l \\in\\{1,2, ... L\\}\n$\nThe exact calculation has the time complexity of O(L \u00d7 M \u00d7 d). L denotes the sequence length, M denotes the number of weighted sum operations of multi-modal embedding vectors of dimension d. The calculation becomes time-consuming when L is very large (104) in the lifelong sequence of multi-modal query-item pairs setting.\nOne straightforward method of fast retrieval K nearest vectors given an input query vector q is to build an embedding index, such as HNSW [11], and conduct ANN (Approximate Nearest Neighbors) search. However, there are difficulties in building an embedding index to retrieve the target query-item pair from the sequence of multi-modal query-item pairs. To search the vectors of behaviors given the input target query-item pair qt as in the exact attention calculation, we build a vector index which assigns a primary key to represent each vector, such as Item ID, Query ID, etc. However, in our aligned sequence of query-item pairs, each merged query-item representation have the joint key of (query_id, item_id), and the required amount of storage increases from the item set size |B| to the cartesian product of the query set size |Q| and the item set size |B| as |Q||B|, which is almost infeasible to store the merged query-item pair in a single index directly.\nAn alternative cascading cross-modal strategy is considered to retrieve top-K relevant query-item pairs. Firstly, we build two separate vector indexes of the query set with size |Q| and the item set with size |B|. During the online retrieval of target query-item pairs, we conduct vector retrieval four times, including query-to-item, query-to-query, item-to-query, and item-to-item. Each retrieval keeps the top-K items with the maximum inner product. The filter in the first-stage cross-modal retrieval is L \u2192 4K. Given the potential 4K items, we conduct an exact attention calculation on these items to obtain the final top-K items, and the filter is 4K \u2192 \u041a.\nThe problem with the cascading cross-modal retrieval strategy is that it may achieve a suboptimal solution compared to exact full attention calculation. This is because the final inner product is a weighted average of all modalities. Additionally, top-K relevant items from one modality (e.g., query-to-query relevance) may have very low relevance in other modalities, such as query-to-item (text) or query-to-item (image), thus the overall inner product score is not optimal. Recall@K can evaluate the performance of the greedy strategy compared to exact calculation.\nTo help increase the recall performance while considering the retrieval speed, the key is to reduce the cardinality of the query set Q and the item set B. We argue that product quantization is a good approximation strategy, which splits vectors into Nbit sub-vectors, assigns each sub-vector to the nearest centroid, and reduces the cardinality. In our formulation, we first use a set of separate Nbit quantization function [q(m), q(m), ,..., q(m)] to encode embedding\nof vectors from the m-th modal channel x(m) as integer vectors\nof Nbit-dimension, q(x(m)) = [c(m), c(m),...,c(m)] \u2208 RNbit. Each representation of multi-modal query-item pair is expressed as:\n$\n\\left[x^{(1)}, ..., x^{(M)}\\right] \\rightarrow\\left[q\\left(x^{(1)}\\right), ..., q\\left(x^{(M)}\\right)\\right] \\in R^{M \\times N_{b i t}}\n$\nWe pre-compute the inner product between different pairs of centroids and store the values in memory. The space complexity of the storage is O(M\u00b2|C|2Nbit), where M denotes the size of multi-modals, |C| denotes the number of centroids, and Nbit denotes the number of subvectors split in the codebook of modal m. During online serving, the inner product of qt\u012b k\u012b is equivalent to O(M\u00b2Nbit) distance lookup operations, and the final score is calculated as the weighted sum of these distances. Here, cr c(i) and c(j) denotes the\ncentroids IDs of the b-th subvector of fx x\u00b9) (i) and x() respectively.\n$\nq_{t}^{T} k_{l} \\approx \\sum_{i} \\sum_{j} \\gamma_{i} \\gamma_{j} \\sum_{b \\in N_{b i t}} \\operatorname{dist}\\left(c_{i}^{(i)}, c_{i}^{(j)}\\right)\n$\nOur proposed multi-modal product quantization strategy works quite well in real-world settings. We also compare the time complexity of different strategies, such as cascading ANN (HNSW), Locality-sensitive hashing (LSH) and our proposed Multi-Modal Product Quantization approximation. Our proposed multi-modal PQ method has the time complexity of O(L \u00d7 M\u00b2 \u00d7 Nbit). In each attention calculation, there are M2Nbit distance look-up operations of O(1), and the final score is calculated as the sum of these distances, which is far less than the exact calculation of the inner product of multiple vectors O(L \u00d7 M \u00d7 d). As for the two stage cascading ANN (HNSW) method of retrieving query-item pairs with two filters, the first stage retrieve the M\u00b2K cross-modal candidates from L sequence as L \u2192 M\u00b2K, and the second stage retrieve the final top K items from first stage as M\u00b2K \u2192 K. Total time complexity of cascading ANN method is O(M\u00b2 log(L)d+M\u00b2Kd), which is faster than our PQ strategy but may achieve sub-optimal recall performance in multiple experiments as reported in Figure 2."}, {"title": "4 EXPERIMENT", "content": null}, {"title": "4.1 Experimental Settings", "content": "Dataset We evaluate our proposed SEMINAR model on three datasets: two public datasets including Amazon review dataset (Movies and TV subset) 2 and the KuaiSAR 3 search and recommendation dataset", "1": "and [8", "16": "KuaiSAR is a real-world public large scale dataset containing both search and recommendation behaviors collected from Kuaishou\u2074", "22": "and repo 5. User with N actions will generate N-1 samples. We use the first i - 1 actions to predict whether the user will interact with the i-th item (0 < i <= N). Additionally", "0,T-1": "days (T=60)", "datasets": "the multi-modal embedding of the Alipay short video dataset with sequence length L = 2,000 and a synthetic dataset. The purpose of the synthetic dataset is to test the performance of different retrieval methods on extremely long sequence (e.g. L = 10"}]}