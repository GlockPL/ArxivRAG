{"title": "Edge-DIRECT: A Deep Reinforcement Learning-based Method for Solving Heterogeneous Electric Vehicle Routing Problem with Time Window Constraints", "authors": ["Arash Mozhdehit", "Mahdi Mohammadizadeht", "Xin Wang"], "abstract": "In response to carbon-neutral policies in developed countries, electric vehicles route\noptimization has gained importance for logistics companies. With the increasing focus\non customer expectations and the shift towards more customer-oriented business mod-\nels, the integration of delivery time-windows has become essential in logistics operations.\nRecognizing the critical nature of these developments, this article studies the hetero-\ngeneous electric vehicle routing problem with time-window constraints (HEVRPTW).\nTo solve this variant of vehicle routing problem (VRP), we propose a DRL-based ap-\nproach, named Edge-enhanced Dual attentIon encoderR and feature-EnhanCed dual\naTtention decoder (Edge-DIRECT). Edge-DIRECT features an extra graph representa-\ntion, the node connectivity of which is based on the overlap of customer time-windows.\nEdge-DIRECT's self-attention encoding mechanism is enhanced by exploiting the en-\nergy consumption and travel time between the locations. To effectively account for the\nheterogeneity of the EVs' fleet, a dual attention decoder has been introduced. Experi-\nmental results based on two real-world datasets reveal that Edge-DIRECT outperforms\na state-of-the-art DRL-based method and a well-established heuristic approach in solu-\ntion quality and execution time. Furthermore, it exhibits competitive performance when\ncompared to another leading heuristic method.", "sections": [{"title": "1. Introduction", "content": "Recently, electric vehicles (EVs) have surged in popularity, driven by global commitments\nto reduce carbon emissions and increased environmental awareness among corporations [1\u2013\n3]. This trend is particularly evident in the logistics sector, where companies are actively\nintegrating EVs into their transportation fleets. At the heart of this transition is the electric\nvehicle routing problem (EVRP), an optimization problem central to the operations of these\nlogistics companies, focusing on dealing with the complexities of deploying EVs instead of\ninternal combustion engine vehicles. This article addresses a practical routing problem for\nEVs, named heterogeneous electric vehicle routing problem with time-window constraints\n(HEVRPTW). It considers both vehicle attributes, such as varying cargo and battery capa\u0430\u0441-\nities [4] and customer preferences regarding delivery times [5]. These factors create a more\\orealistic and applicable model for contemporary logistics challenges. HEVRPTW, recog-\nnized as an NP-hard optimization problem, seeks to determine a set of routes with minimal\ncost, total traveling time, or total traveling distance, for a fleet of Heterogeneous EVs to\nserve each geographically dispersed customer's demands within a specified time-window.\nTraditional methods, including exact and heuristics solvers, are conventionally employed\nto solve various vehicle routing problem (VRP) variants. Due to the NP-Hard nature of\nHEVRPTW, and VRPs in general, exact methods, such as branch-and-price [6] and branch-\nand-price-and-cut [7], consume prohibitively long time for solving practical-size problems [8].\nHeuristics solvers, on the other hand, are significantly faster than exact methods, trading"}, {"title": "2. Related Works", "content": "In the operations research literature, only a few studies have focused on HEVRPTW. We\nbriefly review these and then explore DRL-based approaches proposed for solving different\nVRPs.\nThe study by Hiermann et. al. [10] introduced an adaptive large neighborhood search-\nbased heuristic method to tackle the HEVRPTW. In another study, Sassi et. al. [11]\npresented a mixed integer programming model formulation for HEVRPTW and proposed a\nconstructive-based heuristic method combined with a local search heuristic using an inject-\neject technique for solving this NP-hard optimization problem. While numerous studies\nhave employed heuristic solvers for VRPs, a significant drawback of these methods is their\nreliance on complex, manually-engineered search rules. Secondly, these methods suffer from\na lack of generalizability and adaptability. Even a minor modification to the problem may\nnecessitate rerunning the algorithm to find a solution [12].\nMotivated by promising results utilizing DRL-based methods for solving combinatorial\noptimizations, Nazari et al. [13] proposed a DRL algorithm for solving VRP with an RNN-\nbased decoder for its policy model and utilized the aggregated Euclidean travel distance\nas feedback for training the model. In subsequent research, Kool et al. [14] enhanced\nthe Deep Reinforcement Learning (DRL)-based method by utilizing a Transformer-based\npolicy network. They also employed a self-critical REINFORCE algorithm for training the\npolicy model, which resulted in performance surpassing competitive baselines, including\nGoogle OR-Tools. Duan et al. [8] argued that previously proposed methods fall short\nin effectively solving the VRP when actual travel distances replace aggregated Euclidean\ndistances. In such scenarios, these methods fail to outperform OR-Tools. To address this,\nthey proposed using a graph convolutional network that leverages the actual travel distances\nbetween locations, successfully surpassing the performance of OR-Tools in terms of total\ntravel distance. However, none of these studies investigated the EVRP were vehicles due\nto their limited battery capacity need to be recharged multiple times within a planning\nhorizon. The study by Lin et. al. [15] tried to improve the previous DRL-based methods\nfor solving the EVERP with time-window constraints. To this end, Structure2Vec is used\nto capture the EVs features as well as the location and demands of the customers. A\ndecoder comprised of an attention-based model and an LSTM-based was used for route\nconstruction. However, this method utilizes a complete graph to represent and embed the\nproblem using its encoder. Hence, it fails to consider the reachability of the node based\non their time-window constants and captures the underlying structure for effective routing\nwhen time-windows are considered. Besides, the RNN-based model used in the decoder leads\nto insufficient parallel processing and reduced computational efficiency compared to decoder\nmodels used by previous studies such as Kool et al. [14]. Thirdly, this proposed method\ndoes not consider the heterogeneity of the fleets in real-world logistics operations. Finally,\nthis approach does not consider utilizing the energy consumption between the locations and\nonly relies on the travel distance for decision-making despite its importance for effective\npolicy search considering the limited capacity of EV batteries."}, {"title": "3. Problem Definitions and Formulation", "content": "In this section, we define the HEVRPTW and express this routing problem through\nMarkov decision process (MDP) formulation."}, {"title": "4. Methodology", "content": "In this section, we introduce our deep reinforcement learning (DRL)-based method,\ntermed Edge-enhanced Dual attentIon encoderR and feature-EnhanCed dual aTtention\ndecoder (Edge-DIRECT), designed for tackling the HEVRPTW. We first delve into the"}, {"title": "4.1. Policy Network Architecture of Edge-DIRECT", "content": "Edge-DIRECT, as illustrated in Figure 1, employs a Transformer-style policy network to\nsolve the HEVRPTW instance constructively, which involves sequentially adding a node to\nthe solution at each decoding step. The policy network of Edge-DIRECT comprises two\nprimary elements: an edge-enhanced dual attention encoder and a feature-enhanced dual\ndecoding module. The functionalities and characteristics of each of these components are\ndetailed in the subsequent sections."}, {"title": "4.1.1. Edge-Enhanced Dual Attention Encoder", "content": "The edge-enhanced dual attention Encoder projects the raw features of an instance into\na higher-dimensional space, facilitating the extraction of features pertinent to the problem\ninstance. This encoder consists of two key encoding modules: a time-window graph attention\nmodule and a feature-enhance self-attention encoding module.\nIn the time-window graph attention module, given the time-window graph G = (N, E),\na L-layer graph attention (GAT) operation, as detailed in [16], is applied. This method is\nspecifically tailored to capture the correlations between neighbor nodes that are able to be\nreached from each other based on their time-window constraints. The introduction of a time-\nwindow graph, coupled with this technique for node correlation extraction, is to enhance\nthe policy network's decision-making process, in the presence of time-window constraints,\nfor better routing performance.\nIn each layer 1, for every element aij in the time-window graph's adjacency matrix, an\nattention coefficient $a_{ij}^{(l)}$ is calculated at the l-th GAT layer. This coefficient, determined by\na learnable weight vector $a^{(l)}$ and a learnable weight matrix $W^{(l)}$, quantifies the influence\nof neighboring nodes, as follows:\n$\\begin{equation}a_{i j}^{(l)}=\\frac{\\exp \\left(\\operatorname{ReLU}\\left(a^{(l) T}\\left[W^{(l)} h_{i}^{(l-1)} \\| W^{(l)} h_{j}^{(l-1)}\\right]\\right)\\right)}{\\sum_{k \\in N(i)} \\exp \\left(\\operatorname{ReLU}\\left(a^{(l) T}\\left[W^{(l)} h_{i}^{(l-1)} \\| W^{(l)} h_{k}^{(l-1)}\\right]\\right)\\right)}\\end{equation}$ (4.1)\nwhere $||$ denotes the concatenation operation, ReLU represents the ReLU non-linear activa-\ntion function, and $h_{i}^{(l-1)}$ is the feature vector of the node $n_i$ at layer (l \u2013 1). For layer 0,\nthis feature vector is equivalent to a linear projection of the attributes associated with node\n$n_i$, as described in Definition 1."}, {"title": "4.1.2. Feature-Enhanced Dual Decoder", "content": "Given node embeddings $h_i$, where i ranges from 0 to $|N|$, derived from the encoder,\nand the aggregate graph embeddingh, the decoder, at each step T, produces two distinct\nprobability distributions: $p_F^T$ for selecting a vehicle from a heterogeneous fleet, and $p_r^T$ for\ndetermining the node to be visited by the chosen vehicle. This setup includes two dedicated\nmodules: the vehicle decoder module and the node decoder module.\nEach decoding step starts by selecting a vehicle from a heterogeneous fleet using the\nvehicle decoder module. This module allows the policy model to make informed routing\ndecisions for a fleet of heterogeneous EVs, by capturing the diverse characteristics of these\nvehicles. For this vehicle decoding module, we propose employing a deep learning model\nwith cross-attention mechanism.\nGiven the vehicle fleet features vector F and fleet's state $s_F^T$, for each vehicle j, a high-\ndimensional representation $h_j^F$ is computed by: $h_j^F =$ FF $\\left(f_j\\right) ||$ FF $\\left(s_j^T\\right)$, where FF represents a two-layer, fully connected network with ReLU activation func-\ntions. It is important to note that for efficiency, the operation of the fully connected network\non $f_j$ is performed only at the start of each episode.\nThen, given the vehicle embeddings vector HF = {$\\{h_1^F,...,h_v^F\\}$ and node embeddings\n$h_i, i \\in N$, a context embedding $h_r^c$ is computed through the following equation:\n$\\begin{equation}h_r^c=\\operatorname{Readout}\\left(\\mathrm{HF}\\right) \\| \\operatorname{Readout}\\left(\\cup_{i \\in N_{\\text {visited }}} \\{h_i\\}\\right) \\| \\operatorname{Readout}\\left(\\cup_{i \\in N_{\\text {to-visit }}} \\{h_i\\}\\right)\\end{equation}$ (4.4)\nwhere Readout is denotes the standard Readout operation [20].\nAfterward, given the vehicle embeddings HF and the context embedding $h_r^c$, using a\nMulti-Head Attention (MHA) operation, as described in Kool et al. [14], an embedding\n$h_q^G = MHA(W_Q^G h_r^c, W_K^G HF, W_V^G HF)$ is calculated. $W_Q^G, W_K^G$ and $W_V^G$ are respectively the learnable weights for the query, key, value. Using a Single-Head At-\ntention [21], and given the embedding $h_q^G$ and the vector HF, with learnable parameters\n$W_q^G, W_k^G$, and $W_v^G W_v^G$ for query, key, and value respectively, the compatibility $h_i^G$ is\ncalculated as $h_i^G= SHA(W_q^G h_q^G, W_k^G HF, W_v^G HF)$. Finally, a probability distri-\nbution $p_F^T$ over the heterogeneous fleet is computed through a Softmax operation on $h_i^G$. At the beginning of node decoding process, given the selected vehicle j, problem instance's\ngraph embedding h, node representations H = {$\\{h_0,...,h_{|N|-1}\\}$}, and vehicle state $s_F^T$, a\ncontext embedding $h_a^c$ is computed through: $h_a^c = [n_j^T || \\operatorname{rc}j^T || hr^T] \\oplus h$Here, $n_j^T, \\operatorname{rc}j^T, and hr^T represent the current location of the vehicle j, its remaining\nenergy, and its remaining capacity, respectively.\nThis novel context embedding equips the policy network with essential state information\nneeded to handle the constraints of HEVRPTW, facilitating informed and effective decision-\nmaking in solving the routing problem.\nNext, given the context embedding $h_a^c$ and encoded node features vector H at the\ndecoding step T, using a MHA layer, glimpse $h^G = MHA(W_Q^H h_a^c, W_K^H H, W_V^H H)$ is\ncomputed.\n$W_Q^H \\in R^{d_h \\times d_Q}, W_K^H, W_V^H \\in R^{d_n \\times d_k}, and W_V^H \\in R^{d_h \\times d_v}$ are the trainable parameters\ncorresponding to the query, kay, and value, respectively. For every node $n_i$, given the\nnode embedding $h_i$ and the glimpse $h^G$, a compatibility score is determined to indicate"}, {"title": "5. Experiments", "content": "This section presents thorough experimental analyses using two real-world datasets, de-\nrived from traffic data of two major Canadian cities, Edmonton and Calgary, to address the\nsubsequent research questions:"}, {"title": "5.1. Performance Analysis (RQ1)", "content": "In evaluating the performance of Edge-DIRECT for solving the HEVRPTW, we bench-\nmarked its performance against heuristic methods and a state-of-the-art Deep Reinforcement\nLearning (DRL)-based approach. Specifically, we adapted and enhanced the Ant Colony Op-\ntimization (ACO) method originally proposed by Mavrovouniotis et al. [25] for the EVRP\nby integrating techniques from Han et al. [26] to tackle time-window constraints and fleet\nheterogeneity. Furthermore, we utilized and enhanced the Variable Neighborhood Search /\nTabu Search (VNS/TS) heuristic, based on the work by Schneider et al. [27], for EVRP\nwithin the context of heterogeneous fleets and time-window considerations. Alongside these\nheuristics, Edge-DIRECT's performance was compared with a DRL-based method, named\nEVRPTW-RL, as described by Lin et al. [15], which employs an encoder-decoder structure\nspecifically designed for EVRP challenges. To ensure a fair comparison, as EVRPTW-RL\ncannot accommodate vehicle selection, vehicles are randomly selected at the end of each\ntrip. HEVRPTW-X denotes problem instances with X customers.\nFor DRL-based methods, there are two decoding strategies: 1. The Greedy approach,\nwhere at each decoding phase, the decoder selects the vehicle and the node being visits by\nthat vehicle with the highest probability as indicated by the Softmax layers. 2. The Sampling\nstrategy, where 1280 routes are sampled from the distributions, outputted by vehicle decoder\nand node decoder modules, and the route with the highest reward is selected.\nAs perceivable through Table 1, Edge-DIRECT with sampling-based decoding outper-\nforms the state-of-the-art DRL-based method proposed for EVs' routing, i.e. EVRPTW-RL,\nwith with decoding strategies in terms of cost. As the size of the problem increases the per-\nformance gap widens favoring Edge-DIRECT over EVRPTW-RL despite requiring much\nless computational times, due to EVRPTW-RL's step-wise encoding. Edge-DIRECT (Sam-\npling) significantly surpasses the performance of ACO, a well-established heuristic methods\non both datasets. This superiority is not only evident in terms of solution quality but also\nin computational efficiency and the gap increases as the problem size grows. Edge-DIRECT\nwith greedy decoding strategy mandates least amount of running time compared to all\nbaselines and is able to exceeds the performance of ACO for problem of size 100 on both\ndatasets. While the sampling strategy of Edge-DIRECT demands more computational\ntime compared to its greedy-based counterpart, it remains competitive in terms of total\ntravel time when juxtaposed with heuristic methods such as VNS/TS. Moreover, it benefits\nfrom significantly reduced computational time compared to both heuristic approaches and\nEVRPTW-RL (Sampling)."}, {"title": "5.2. Ablation Study (RQ2)", "content": "Edge-DIRECT is featured with 3 major enhancements to effectively solve the HEVRPTW.\nThe contribution of each of these features has been investigated on the Calgary Dataset with\ngreedy decoding strategy and results are detailed in Table 2. \"Edge-DIRECT w/o EE\"\nrepresents Edge-DIRECT without the edge-enhanced encoder, wherein the encoder only\nexploits the node features. \"Edge-DIRECT w/o TWE\" indicates the variant lacking the\ntime-window graph encoding module, where time-windows are instead assigned as raw fea-\ntures to each node. Finally, \"Edge-DIRECT w/o HD\" denotes the model variant excluding\nthe specialized decoder designed to manage fleet heterogeneity.\nResults demonstrate that the original Edge-DIRECT has better performance compared\nto the other variants. While removing the time-window encoding module notably reduces\nrunning time more than the other two variants, this module also yields a more substantial\nimprovement in cost compared to the 2 enhancements to Edge-DIRECT. After the time\nwindow encoder, the addition of the heterogeneous vehicle decoder notably boosts cost\nefficiency. In terms of computational time, its impact is comparable to that of the time\nwindow encoder, yet marginally less. The edge-enhanced encoder contributes the least\nto cost reduction compared to the other modules and incurs minimal computational time\namong them."}, {"title": "5.3. Impact of Charging Station-to-Customer Ratio (RQ3)", "content": "Experiments were conducted to explore the effect of varying the ratio of charging sta-\ntions to customers on the performance of DRL-based methods, on the Calgary dataset for a\nproblem instance comprising 100 customers. The findings are detailed in Table 3. Notably,\nincrease in charging stations leads to a reduction in overall cost but a rise in computa-\ntional time, attributable to the enhanced complexity from incorporating more nodes into"}, {"title": "6. Conclusion", "content": "In this study, we address the HEVRPTW by introducing a DRL-based method, Edge-\nDIRECT. This approach features a Transformer-style policy network with an edge-enhanced\nencoder that leverages energy and travel costs between nodes for efficient EV routing. To\naccommodate time-window constraints, Edge-DIRECT employs a GAT layer to discern the\ngraph's structure from the nodes' connectivity based on time-window constraints. Addi-\ntionally, a dual-attention decoder is utilized to account for fleet heterogeneity, focusing on\nbattery and cargo capacity. Through comprehensive testing on datasets from Edmonton and\nCalgary, Edge-DIRECT's efficacy is assessed against leading heuristics and state-of-the-art\nDRL methods. The results demonstrate that Edge-DIRECT outperforms EVRPTW-RL,\na competitive deep reinforcement learning (DRL)-based approach designed for addressing\nElectric Vehicle Routing Problems with Time-Windows (EVRPTW), in terms of solution\nquality with significantly reduced computational time. Notably, it competes well with the\nVNS/TS and surpasses ACO in route efficiency, with substantially less running time.\nThere are several avenues for future research endeavors. First, in this article we assumed\nthat the energy recharging and consumption is linear. However, we aim to delve into more\ncomplex, non-linear models of consumption and recharging in subsequent research, aligning\nour approach more closely with real-world dynamics. Second, we plan to explore routing\nvariations that accommodate the potential for order cancellations, further enhancing the\npractical applicability of our findings."}]}