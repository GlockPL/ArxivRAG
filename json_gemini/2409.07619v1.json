{"title": "Ensemble Methods for Sequence Classification with Hidden Markov Models", "authors": ["Maxime Kawawa-Beaudan", "Srijan Sood", "Soham Palande", "Ganapathy Mani", "Tucker Balch", "Manuela Veloso"], "abstract": "We present a lightweight approach to sequence classification using Ensemble Methods for Hidden Markov Models (HMMs). HMMs offer significant advantages in scenarios with imbalanced or smaller datasets due to their simplicity, interpretability, and efficiency. These models are particularly effective in domains such as finance and biology, where traditional methods struggle with high feature dimensionality and varied sequence lengths. Our ensemble-based scoring method enables the comparison of sequences of any length and improves performance on imbalanced datasets.\nThis study focuses on the binary classification problem, particularly in scenarios with data imbalance, where the negative class is the majority (e.g., normal data) and the positive class is the minority (e.g., anomalous data), often with extreme distribution skews. We propose a novel training approach for HMM Ensembles that generalizes to multi-class problems and supports classification and anomaly detection. Our method fits class-specific groups of diverse models using random data subsets, and compares likelihoods across classes to produce composite scores, achieving high average precisions and AUCs.\nIn addition, we compare our approach with neural network-based methods such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory networks (LSTMs), highlighting the efficiency and robustness of HMMs in data-scarce environments. Motivated by real-world use cases, our method demonstrates robust performance across various benchmarks, offering a flexible framework for diverse applications.", "sections": [{"title": "1 Introduction", "content": "Sequence modeling is a fundamental task in machine learning, with wide-ranging applications across multiple domains. Sequences span many data types, from categorical tokens (e.g., natural language, biological markers), continuous values (e.g., temperature, sensor readings), and mixed multivariate data (e.g., trading activity, user behavior, system logs). Sequence modeling problems are relevant in healthcare (diagnosis from patient and test history, genome and protein classification), finance (anomaly detection in sequences of financial activity like credit card fraud, money laundering, and anomalous trades), technology (monitoring system logs and sensor data), and operations (identifying system and pipeline failures).\nDespite considerable progress in sequence classification learning methods, challenges persist, particularly with high-dimensional data, varying sequence lengths, the demand for real-time processing, and, most importantly, data imbalances present in real-world datasets. Existing approaches broadly fall into two main categories: traditional methods and modern deep learning-based models. Traditional methods offer interpretability but often fail to capture the intricate temporal dependencies in sequential data. Conversely, modern deep learning methods like RNNs, LSTMs, and Transformers excel at modeling complex sequences but face issues like high computational costs, overfitting, and reduced interpretability. Both sets of approaches are significantly limited by severe data imbalances often seen in real-world data streams.\nTypically, one class (usually the positive class) is under-represented, while the other (negative class) dominates the dataset. For example, fraudulent transactions are far less common than legitimate ones, normal operations vastly outnumber anomalous events in systems, and bot activity is a fraction of normal user activity. Class imbalance leads to biased models that perform well on the negative majority class, but poorly on the positive class (failing to detect the rare but crucial positive instances).\nHidden Markov Models (HMMs) are an established technique for sequence modeling, known for their ability to capture temporal dependencies and handle variable sequence length. They are particularly well suited in domains that require interoperability, and can be integrated with modern deep learning techniques for enhanced performance. However, traditional HMMs also struggle with class imbalance.\nWe propose a novel approach that leverages ensembles of HMMs for the task of sequence classification, particularly in scenarios with extreme class imbalance (such as anomaly detection), while remaining extendable to multi-class classification. While our real-world deployment scales to millions of sequences, we demonstrate our approach on established genome classification benchmark tasks.\nOur contributions are as follows:\n1. We propose a novel framework leveraging HMM ensembles (HMM-e) for sequence classification, and establish training and inference procedures.\n2. We propose a novel method for constructing composite scores from base learners in an ensemble, allowing for the comparison of sequences of extremely different lengths. This method is model-agnostic, and can be used with ensembles of other model classes.\n3. We demonstrate the robustness of HMM-e to class imbalances.\n4. We demonstrate the utility of HMM-e as feature extractors for downstream classifiers, and show that the combination of HMM ensembles with downsteam models (SVMs and neural networks) outperforms established deep-learning baselines."}, {"title": "2 Background & Related Work", "content": ""}, {"title": "2.1 Sequence Classification", "content": "Sequence classification, the categorization of sequential data into semantically meaningful classes, is an extensively researched problem in machine learning, essential for tasks across various domains.\nData Imbalance and Anomaly Detection Many real-world problems involve the detection of rare events like system intrusions, credit card fraud, or money laundering. These anomaly detection problems are made difficult by class imbalance, or the scarcity of examples for the behavior of interest.\nOne-class anomaly detection approaches avoid modeling the anomalous class, as its examples are so sparse, and focus instead on robustly modeling the nominal behavior class. They use these models to measure the distance between observed and nominal behaviors. In other applications, such as those motivating our use case, both the normal and anomalous classes are modelled. This more targeted approach allows us to detect only particular kinds of behavioral anomalies, not just any behavior deviating from nominal.\nSynthetic data approaches can also be used in conjunction with other methods to augment the data of the underrepresented class. While synthetic data is not the focus of our study, our approach also inherently provides a mechanism to generate synthetic samples."}, {"title": "2.2 Hidden Markov Models (HMMs)", "content": "Hidden Markov Models (HMMs) are statistical models for sequential data, which have a long history of use in natural language processing, finance, and bioinformatics. While they are simpler than many deep-learning based models, they can still robustly model sequential data by making the assumption that an underlying observation sequence $O = {o_1, o_2, ..., o_T}$ is a series of unobserved transitions between hidden states ${a_1, a_2, ..., a_T}$. HMMs make two fundamental assumptions:\n$p(A_{t+1} = a_{t+1} | a_t, ..., a_1) = p(A_{t+1} = a_{t+1} | a_t)$ (1)\n$p(O_t = o_t | a_t, O_{t-1}, A_{t-1}, ..., O_1, A_1) = p(O_t = o_t | a_t)$ (2)\nEq. 1 describes the Markov property of the hidden states: the next state is independent of the rest of the state history given the curent state. Eq. 2 describes the conditional independence of the current observation from the rest of the state and observation history given the current state.\nAn HMM with $n$ states and a vocabulary of $m$ possible emissions consists of three parameter sets: the initial state distribution $\\pi$, the $n \\times n$ transition matrix $A$, and the $n \\times m$ emission distribution $B$. These parameters are collectively denoted by $\\lambda = (A, B, \\pi)$. There are three fundamental HMM problems:\n1. Estimating the likelihood $p(O | \\lambda)$ of an observation sequence $O$ given an HMM with parameters $\\lambda$\n2. Estimating the most likely state sequence $(a_1, ..., a_T)$ given an observation sequence $(o_1, ..., o_N)$.\n3. Estimating the parameters $\\lambda$ given an observation sequence $O$ or a set of observation sequences $O_N$ to maximize $p(O | \\lambda)$ or $p(O | \\lambda)$\nThere exist long-standing solutions for each of these problems: problem 1 is solved via the forward-backward algorithm, problem 2 via the Viterbi Algorithm, and problem 3 via the Baum-Welch algorithm. While many state-of-the-art approaches for sequential data problems have adopted deep-learning based models like transformers, recent works have shown the applicability of HMMs to time series analysis, financial applications like stock selection and trading, and behavioral anomaly detection in robotics. Although HMMs have also been leveraged in ensemble settings, we're not aware of any concrete frameworks that establish training and inference routines for sequence classification."}, {"title": "3 Approach", "content": "Singleton HMMs We first investigate the use of single HMMs in (binary) sequence classification. In this setup, the training data is separated by class, and each class' training sequences are used to train individual HMMs, yielding one positive class HMM $\\lambda^+$ and one negative class HMM $\\lambda^-$.\nFor a new unseen sequence, the predicted label is the class of the HMM that assigns the sequence the higher likelihood:\n$c(O) = 1\\{p(O | \\lambda^+) > p(O | \\lambda^-)\\}$\n(3)\nVariable Sequence Length HMMs have been applied to sequence analysis with tremendous success. While obtaining a sequence's likelihood under a trained HMM is straightforward, comparing the likelihoods of sequences of varying lengths poses a significant challenge (despite length-based normalization approaches). Sequence length heavily influences the likelihood due to sequential multiplication of probabilities at each timestep. Some deep-learning based approaches are often similarly limited by the need to size parameter matrices to fixed lengths a priori, resulting in a constrained maximum sequence length.\nWe leverage model-driven normalization by computing a set of likelihood scores for a given sequence across multiple models and using these scores to determine an overall rank or score, rather than directly comparing sequence likelihoods."}, {"title": "3.1 HMM Ensembles", "content": "HMMs, while relatively simple and lightweight, can struggle to capture the complexity of behaviors represented in training data, especially when trained as a single model per class. Ensemble frameworks involve training multiple models over subsets of the training data. This allows each learner to specialize in capturing distinct patterns or behaviors, while still modeling the entire data distribution collectively, leading to a more robust and generalize approach. Ensemble-based approaches also do well in scenarios with data imbalance, whereas monolithic models skew towards modeling the majority class (or underfitting in case of class-specific models).\nWe propose HMM-e, an ensemble framework for computing composite scores from the individual learner scores in our ensemble. This framework is model-agnostic, and while we find that HMMs work well for our use case, the base learners could just as well be deep-learning based models, SVMs, decision trees, or any other model class.\nFormalization and Algorithmic Framework First, we train $N$ models {$\\lambda_1^+, ..., \\lambda_N^+$} on the positive class and $M$ models {$\\lambda_1^-, ..., \\lambda_M^-$} on the negative class, taking care to ensure diversity among the models by training each on a randomly selected subset of samples from the training data. Each model sees $s\\%$ of the training data in its relevant class. While we set $N = M$ for all settings, these parameters $(M, N, s)$ can be established using typical hyperparameter optimization approaches. For any given sequence in the training data, the probability of not being selected for any model's random subset is $(1 - s)^N$. The expected number of unsampled sequences is the same, so it is important to select $s$ and $N$ to keep this proportion of the training data small.\nFor a previously unseen observation sequence $O$, we compute the likelihood scores of the sequence under all models: {$p(O | \\lambda_1^+), ..., p(O | \\lambda_N^+)$\\} and {$p(O | \\lambda_1^-), ..., p(O | \\lambda_M^-)$\\}. We then compute the composite score:\n$s(O) = \\sum_{i=1}^{N} \\sum_{j=1}^{M} 1\\{p(O | \\lambda_i^+) > p(O | \\lambda_j^-)\\}$\n(4)\nThe composite score represents the number of unique pairwise matchups of positive class models and negative class models in which the positive class model assigns the sequence a higher likelihood than the negative class model. $s(O)$ therefore takes on values in the range $[0, N \\times M]$, where $s(O) = 0$ indicates that negative class models outscored positive class models in all pairwise matchups, and $s(O) = N \\times M$ the opposite. The lower $s(O)$ is, the less likely a sequence is to be in the positive class, and vice versa. A sample distribution of composite scores over a corpus of held-out sequences, colored by class, is shown in Fig. 2. We see that the classes are well-separated, and indeed a decent classifier can be produced simply by thresholding the distribution at around $s(O) = 50,000$.\nGiven the formulation in Eq. 4, likelihoods for sequences of different lengths are never compared. Rather, given a corpus of sequences ${O_1, ..., O_S}$ and the distribution of scores {$s(O_1), ..., s(O_S)$\\}, sequences can be compared and classified on the basis of $s(O)$. Eq. 4 can be thought of as an approach to normalizing for sequence lengths. $N$ and $M$ should be chosen such that the range $[0, N \\times M]$ is sufficiently wide to differentiate the classes."}, {"title": "Model Diversity in Ensembles", "content": "It is important to ensure the diversity of the trained models in any ensembling approach. Without sufficient diversity amongst training data subsets or model parameters, ensembles may learn redundant models. This reduces the efficiency of the ensemble due to redundancies. We demonstrate this in Fig. 3 by comparing sub-model similarities in two settings: (a) a well trained ensemble (right), and (b) a poorly initialized ensemble (left).\nFor each unique pair of HMMs in the ensemble we plot the HMM similarity $1 - D(i, j)$, where $D(i, j)$ is the HMM distance proposed in (Azzalini et al. 2020). This distance is a sum of Hellinger distances between emission distributions of states matched using linear sum assignment, and weighted by the stationary distribution among states.\nFig. 3(a) demonstrates the scenario where HMM-e is properly trained. As expected, the dark diagonal demonstrates that $D(\\lambda_i, \\lambda_i) \\approx 0 \\forall i$, while the noisy off-diagonals demonstrate non-zero and uncorrelated distances for $D(i, j) \\forall i, j, i \\neq j$. We see four large blocks showing that intra-class HMM similarities are higher on average the inter-class similarities, as desired.\nFig. 3(b) demonstrates the effect of reduced ensemble sub-model diversity. This is done by artificially inducing the learning of redundant models on demo_human_or_worm. HMMs are sensitive to their initialization, and we seed subsets of the ensemble with the same initialization. We also train for only 5 iterations (vs 25) to increase the sensitivity to initialization. The blocks of similarly-colored HMM pairs show that many models are redundant. Each of these blocks can be thought of as one highly inefficient meta-model, effectively reducing the ensemble size to the number of blocks.\nEnsemble size $N$ and subset factor $s$ are important hyperparameters for ensuring diversity \u2013 if $N$ is too small, our coverage of behaviors in the training data is poor. If $s$ is too large, we learn redundant models. This is discussed in Section 5. Individual learner parameters can also be modulated over the ensemble to encourage diversity. For HMMs, the number of states $n$ is the most impactful hyperparameter, and we observe some improvement (~3% AUC-ROC) by varying the number of states $\\in [3, 4, 5]$ across models rather than using $n = 5$ for all models. In general, more diverse ensembles are more efficient as they can achieve equivalent performance to less diverse ensembles at smaller ensemble sizes."}, {"title": "3.2 Downstream Modeling using HMM-e Scores", "content": "Given a corpus of sequences ${O_1, ..., O_S}$ and the distribution of scores {$s(O_1), ..., s(O_S)$\\}, we can set a threshold $S_{thresh}$ and classify sequences directly as:\n$c(O) = 1\\{s(O_i) \\geq S_{thresh}\\}$\n(5)\nWe can also use the likelihoods of the base learners as features for consumption by downstream classifiers. For each sequence $O_i$, we construct a feature vector comprised of the HMM-e scores,\n$f_i = [p(O_i|\\lambda_1^+), ..., p(O_i|\\lambda_N^+), p(\\lambda_1^-), ..., p(\\lambda_M^-)]$\n(6)\nGiven that the magnitude of $f_i$ is highly sensitive to the sequence length, as discussed in Section 3, we normalize $f_i$ by $||f_i||_2$ before training classifiers on ${f_1, ..., f_S}$.\nThis technique utilizes HMMs as feature extractors, where each feature $p(O_i|\\lambda_j)$ represents the similarity between the sequence $O_i$, and the random subset of training data underlying $\\lambda_j$. Fig. 4 shows the Uniform Manifold Approximation and Projection (UMAP) projections of these features $f_i$ for the dataset demo_human_or_worm, with the positive class in blue and the negative class in green. This unsupervised dimensionality reduction highlights the class structure, suggesting that the full high-dimensional feature vectors may be linearly separable. We train two types of classifiers in this framework: support vector machines (HMM-e + SVM) and simple Neural Networks (HMM-e + NN)."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Data", "content": "Our work is applicable to many real-world tasks in domains spanning healthcare, finance, operations, and technology. To demonstrate our approach's effectiveness, we leverage an open-source genomics benchmark that consists of eight datasets for different classification problems. The benchmark represents our real-world problem setting well: each dataset contains sequences of tokens from a small vocabulary representing the four nucleotide bases (A, T, C, and G) and an unknown base token (N). Labels are provided at the sequence level. Sequences are varied in length and contain genetic information from a diverse set of model organisms including humans, mice, and roundworms. Labels correspond to the function and behavior of each genomic sequence \u2013 whether the sequence is a promoter gene, enhancer gene, or an open chromatin region. We provide results on four datasets included in the benchmark.\nAll datasets are split by the benchmark authors into train and test sets. For our experiments we artificially create imbalance in the datasets by randomly subsampling the positive class. These imbalance subsets are fixed across all HMM approaches, and fixed separately across all deep-learning based approaches."}, {"title": "4.2 HMM-based Methods", "content": "HMMs and HMM-e For our experiments we adopt an ensemble size of 250 models per side, a sampling rate of 1%, and 5 states in our HMMs.\nHMM-e + SVM We train the downstream SVM using a radial basis function kernel, exploring an effective and simple model class in non-deep learning contexts.\nHMM-e + NN The architecture is similar to the baseline models in terms of number of layers and hidden dimensions; the classifier consists of four fully connected layers, starting with a hidden dimension of 512. Each layer incorporates batch normalization, ReLU activation, and dropout for regularization. The model is trained using binary cross-entropy loss, with the Adam optimizer and a learning rate scheduler. To address data imbalance, weighted sampling is applied during training."}, {"title": "4.3 Baselines", "content": "Neural network approaches often face significant challenges when dealing with imbalanced classes in classification tasks. These models, driven by gradient-based optimization, tend to favor the majority class, leading to biased predictions and reduced performance on minority classes. The imbalance skews the learning process, causing the model to underfit the minority class, which is typically underrepresented in the loss function. Consequently, the network exhibits poor generalization, particularly in real-world applications where the minority class is critical. To mitigate the impact of class imbalances, several techniques have been proposed, including weighted loss functions, data augmentation, advanced sampling strategies, and hyperparameter tuning. However, despite the application of these ad-hoc methods, in practice, leveraging extremely imbalanced datasets using neural networks remains a significant challenge.\nWe compare our approaches against baseline Convolutional and LSTM based neural networks.\nCNN We evaluate our approach against the baseline CNN in. The model comprises of an embedding layer that maps the input tokens into dense vectors. These embeddings are processed through a sequence of 1-D convolutional layers with ReLU activations, batch normalization, and max-pooling. The model is optimized with the Adam optimizer and trained using the binary cross-entropy loss function for binary classification. The results of the CNN baseline are shown in Table 1.\nLSTM We implement a LSTM model for binary sequence classification. The architecture includes an embedding layer that maps input tokens to dense vectors, followed by a single LSTM layer with a hidden state dimension of 64. The LSTM output is fed into a dense layer with ReLU activation, and a sigmoid activation is applied to the final output. The model is trained using binary cross-entropy loss. The results of the CNN baseline are shown in Table 1.\nTo address data imbalance during training, we employ weighted sampling during training for both the CNN and LSTM. Each sample in the training set is assigned a weight inversely proportional to its class frequency, giving higher weights to underrepresented classes. For balanced datasets, uniform weights are applied across all samples."}, {"title": "4.4 Evaluation", "content": "The choice of classification threshold (decision boundary) is critical as it impacts the trade-off between precision and recall, and directly influences metrics like classifier accuracy. In risk-sensitive domains, or in scenarios with ethical and regulatory considerations, this choice is further influenced by downstream factors that weigh the cost of false positives versus false negatives. For e.g., in fraud detection, a lower threshold will flag more fraud at the expense of increased false positives (user friction), but in healthcare, a lower threshold ensures patient safety, even at the expense of false alarms. At a fixed boundary, Accuracy can be a misleading metric, particularly in scenarios with data imbalance, for e.g., if anomalies constitute only 0.5% of a dataset, a classifier can attain 99.5% accuracy by labeling every sample identically (as normal).\nWe use two popular techniques of evaluating classifiers across different decision boundaries: ROC and PR curves.\nAUC-ROC The Receiver Operating Characteristic (ROC) curve is used to understand the trade-off between the True Positive Rate (also known as Recall) and False Positive Rate at different classification boundaries/thresholds. The area under the curve (AUC) enables us to compare multiple ROC curves, with higher values suggesting better classifiers.\nAverage Precision (AP) Precision-Recall (PR) curves are often used in settings with data imbalance, particularly when the positive class is rare. AP provides a way to condense the Precision-Recall (PR) curve into a singular metric; although it approximates the area under the PR curve (AUC-PR), it is not as optimistic in its computation as it does not interpolate, and instead computes the weighted average of precisions at the each threshold (weighted by change in recall)."}, {"title": "5 Results & Discussion", "content": "Table 1 presents a detailed performance evaluation of the various sequence classification approaches across 4 datasets in balanced (1:1) and imbalanced (50:1) class settings, reporting two metrics: AUC-ROC and AP.\nHMMs: Singleton vs Ensemble We find that our HMM-e ensemble approach consistently outperforms the singleton HMM approach, yielding a significant increase in both AUC and AP. For example, on the demo_human_or_worm dataset, moving from a singleton HMM to an ensemble improves AUC from 55.7 to 83.9 and AP from 55.6 to 84.6. This trend continues under class imbalance: in the 50:1 class ratio setting, the singleton HMM achieves 61.4 AUC and 14.0 AP, while HMM-e achieves 87.2 and 27.5 AUC and AP respectively.\nOur Approach vs Baselines While the baseline CNN and LSTM classifiers perform adequately on the balanced datasets, their efficacy diminishes with high class imbalance despite implementing weighted sampling approached to remedy this. The HMM-e + SVM and HMM-e + NN models consistently outperform other methods, particularly in the imbalanced data scenario (50:1). The combination of HMM ensembles with downstream classifiers offers significant advantages in capturing complex sequential relationships while mitigating the effects of class imbalance.\nIn the few instances where the baseline CNN and LSTM methods outperform HMM-e + SVM or HMM-e + NN, the difference in performance is small. For example, for the human_nontata_promoters dataset, in the balanced data setting the CNN model has an AUC-ROC score of 89.5 and AP of 91.0, versus HMM-e +SVM's 89.5 AUC and 84.0 AP.\nUnderstanding Ensemble Parameters While we present results with a fixed ensemble size in Table 1, the ensemble size parameters (M, N) and data subsample size (s%) can be varied as discussed in Section 3.1. We examine how ensemble size and class imbalance impact HMM performance and summarize our findings in Figure 5. We conduct a detailed experiment on the demo_human_or_worm dataset, training and evaluating models spanning three class imbalance settings (1:1, 25:1:, 50:1), five ensemble sizes (1, 10, 50, 100, 250), as well as varying data sample sizes (0.1%, 1%, 10%).\nFigure 5 illustrates the impact of ensemble size and class imbalance on classifier performance. The ideal classifier would achieve AUC-ROC and AP of 100%, and be positioned on the top-right of the plot. As expected, classifier performance decays with increased class imbalance. In general, larger ensembles outperform smaller ones, albeit only to a certain extent. Multiple values for the same (ensemble size, class ratio) configuration represent ensembles trained with different fractions of the dataset (s). We find that lower values of s are essential at smaller ensemble sizes for creating diverse ensembles, whereas at larger ensemble sizes allow for larger values of s."}, {"title": "6 Conclusion", "content": "We introduce the HMM-e framework, an ensemble approach for sequence classification that leverages HMMs. Our method demonstrates robust performance in scenarios with severe data imbalance, outperforming learned methods on Genomics Benchmark datasets. The HMM-e framework is generalizable across problem definitions and has applications in domains spanning healthcare, finance, operations, and technology. Its compatibility with downstream modeling approaches (including both deep learning and traditional methods) makes it particularly useful in real-world settings, as it can be deployed in pipelines which may leverage additional data and domain knowledge.\nKey contributions include:\n\u2022 HMM-e's robustness to data imbalance\n\u2022 Adaptability to multi-class settings\n\u2022 Compatibility with downstream modeling approaches\n\u2022 Inherent generative capabilities, lending it useful to synthetic data applications\nFuture work includes exploring approaches for identifying underperforming models in the ensemble and pruning or fine-tuning them. It also includes using unsupervised clustering on ensemble score features to identify classes of behavior, and leveraging the generative properties of HMMs for synthetic data generation."}, {"title": "Supplementary Materials: Ensemble Methods for Sequence Classification with Hidden Markov Models", "content": ""}, {"title": "A Quantifying Uncertainty", "content": "In Table 2, we present results from all methods explored in our study, with uncertainty measures, on the dataset demo_coding_vs_intergenomic_seqs. These findings are computed both under balanced (1:1) and imbalanced (50:1) conditions, across 3 training runs.\nWe find, as expected, that the standard deviation of both AUC-ROC and average precision rises across all methods in the imbalanced setting compared to the balanced setting. HMM-e, HMM-e +SVM, and HMM-e +NN have less variance in their performance than both the CNN and LSTM learned baselines, across data imbalance settings."}, {"title": "B Datasets", "content": "To demonstrate our approach's effectiveness, we leverage an open-source genomics benchmark that consists of eight datasets for different classification problems. We provide results on four datasets included in the benchmark.\nAll datasets are split by the benchmark authors into train and test sets. For our experiments we artificially create imbalance in the datasets by randomly subsampling the positive class.\nhuman_nontata_promoters Originally drawn from, this dataset consists of 36,131 sequences with a median length of 251. These sequences are drawn from the human genome and fall into one of two well-known classes of promoter genes \u2013 TATA or non-TATA.\nhuman_enhancers_ensembl This dataset is documented in and consists of 154,842 sequences with median length 269. Positive sequences are human enhancers and negative sequences are randomly selected, non-overlapping segments from human genome GRCh38.\ndemo_human_or_worm, demo_coding_vs_intergenomic_seqs These two datasets are computationally generated by the authors of by sampling randomly from human and worm genomes. Both consist of 100,000 sequences with median length 200. In demo_human_or_worm the classes correspond to the organism from which the transcript is sampled, and in demo_coding_vs_intergenomic_seqs the classes correspond to whether the transcript codes for proteins or does nothing."}, {"title": "C Hyperparameter Setting", "content": "CNN We adopt the same CNN hyperparameters as in the baseline included in. The CNN model comprises of three convolutional layers with 16, 8, and 4 filters, respectively, each with a kernel size of 8 and a stride of 1. Batch normalization and ReLU activation follow each convolution, with 2x max-pooling applied after each block. The model includes two dense layers, with the first layer containing 512 units. The default learning rate of 0.001 set by the Adam optimizer is used. The output layer utilizes a sigmoid activation. We use a binary cross-entropy loss.\nLSTM The LSTM baseline model consists of a single LSTM layer with a hidden dimension of 64, followed by two dense layers (256 units and an output layer). The network includes an embedding layer with a configurable embedding dimension (set to 100). The Adam optimizer is used with a learning rate of 0.001. For binary classification, a sigmoid activation and binary cross-entropy loss are used.\nHMM We perform a search for the number of states underlying our HMMs, ranging over [2, 7]. From an AUC and AP perspective we find that the number of states makes little difference to performance, so we choose 5 as it sits in the middle of the range.\nHMM-e We perform a search over three hyperparameters: ensemble size, subset factor, and number of states per HMM. For ensemble size we try values in [10, 50, 100, 250, 500]. For subset factor we try values in [0.1%, 1%, 2%, 3%, 5%, 10%]. For number of states we try values in the range [2, 7]. We find ensemble size, then subset factor, then number of states per HMM to be the most important hyperparameters for performance. We select our final setting \u2013 ensemble size of 250, subset factor of 1, and number of states of 5 - to maximize AUC and AP.\nHMM-e + SVM We use almost all default hyperparameters for our SVM and perform no hyperparameter search. The only non-standard hyperparameter we use is to set probability=True which allows us to compute log-likelihoods of sequences under the model after training.\nHMM-e + NN The MLP classifier architecture follows that of the baseline CNN but with fully connected layers in place of the Convolutional layers. It features four fully connected layers with hidden dimensions of 512, 256, and 128, employing batch normalization and dropout (0.25) after each ReLU activation. The model is optimized using the Adam optimizer with a learning rate of 0.001 and a batch size of 64. The output dimension is set to 1 for binary classification"}, {"title": "D Hardware and Software Stack", "content": "Our experiments are performed on an AWS r5.24xlarge EC2 instance featuring 96 virtual CPUs and 768 GB of memory. Due to the lightweight nature of the models trained, we do not have to leverage GPU acceleration. The environment is configured with Ubuntu 20.04 LTS as the operating system, and we use Python version 3.8.10. Aside from standard machine-learning libraries like Pandas, NumPy, PyTorch, Scikit-Learn, and Tensorflow, we also use HMMLearn to train our HMMs, as well as the Github repository linked in to load our datasets."}, {"title": "E Pseudocode", "content": "We include detailed pseudocode for our method which should provide sufficient guidance for anyone looking to replicate our results. This pseudocode covers all aspects of our approach, including:\n1. Algorithm 2: Details the basic framework for training our HMM-e approach. A fuller description can be found in the paper in Algorithm 1.\n2. Algorithm 3: Evaluation code for computing metrics on trained models.\n3. Algorithm 4: Details the approach for constructing feature vectors for training downstream classifiers, given trained models.\n4. Algorithm 5: Code for training SVMs on HMM-e feature vectors.\n5. Algorithm 6: Code for training neural networks on HMM-e feature vectors."}]}