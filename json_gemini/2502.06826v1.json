{"title": "TRANSFERRING GRAPH NEURAL NETWORKS FOR SOFT SENSOR MODELING USING PROCESS TOPOLOGIES", "authors": ["Maximilian F. Theisen", "Gabrie M. H. Meesters", "Artur M. Schweidtmann"], "abstract": "Data-driven soft sensors help in process operations by providing real-time estimates of otherwise hard- to-measure process quantities, e.g., viscosities or product concentrations. Currently, soft sensors need to be developed individually per plant. Using transfer learning, machine learning-based soft sensors could be reused and fine-tuned across plants and applications. However, transferring data-driven soft sensor models is in practice often not possible, because the fixed input structure of standard soft sensor models prohibits transfer if, e.g., the sensor information is not identical in all plants. We propose a topology-aware graph neural network approach for transfer learning of soft sensor models across multiple plants. In our method, plants are modeled as graphs: Unit operations are nodes, streams are edges, and sensors are embedded as attributes. Our approach brings two advantages for transfer learning: First, we not only include sensor data but also crucial information on the plant topology. Second, the graph neural network algorithm is flexible with respect to its sensor inputs. This allows us to model data from different plants with different sensor networks. We test the transfer learning capabilities of our modeling approach on ammonia synthesis loops with different process topologies [1]. We build a soft sensor predicting the ammonia concentration in the product. After training on data from one process, we successfully transfer our soft sensor model to a previously unseen process with a different topology. Our approach promises to extend the data-driven soft sensors to cases to leverage data from multiple plants.", "sections": [{"title": "1 Introduction", "content": "Data-driven soft sensors promise to increase operating efficiency in chemical plants by providing real-time estimates of hard-to-measure quantities. Previous works have shown the merit of soft sensors in many applications, e.g., estimating viscosity or product concentrations [2, 3]. However, data scarcity remains a major hindrance in developing machine learning-based solutions such as soft sensors in industry [4]."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Spatio-temporal modeling of soft sensors with graphs", "content": "To enable transfer learning between processes, we utilize a graph-based representation of the process topology and the soft sensor data to enable topology-awareness together with a model that accounts for both spatial and temporal dependencies in the data. We represent the underlying process as a directed graph, representing the topology of the process, see Figure 1. We model unit operations as nodes. To model different types of units, we one-hot encode the unit type into the node attribute vector. We further represent streams as directed edges. The direction of the edge follows the direction of the material. Finally, we encode sensor measurements according to their location in the process into the attribute vectors of nodes and edges.\nA complete overview of the modeling framework can be found in Figure 2. We model the spatial relationships between the graph-encoded data and the soft sensor target using GNNs for each time step. We deploy a message-passing GNN [9], allowing us to utilize both node and edge information. The output is an individual flowsheet embedding for"}, {"title": "2.2 Transfer learning", "content": "The transfer learning process in this work is conducted in two stages. First, the model undergoes initial training on the source domain. Subsequently, a partial retraining is performed on the target domain. We also test the case of no retraining on the target domain, which is referred to as zero-shot transfer learning.\nTo further stabilize the training, we scale our input data by applying a log-scale. Since each process has a different topology and set of embedded sensor measurements due to different sensor information, different streams, etc., we cannot uniformly apply per-feature normalization across them. We, however, found that with sufficiently small learning rates, the training is still stable. We do normalize our soft sensor target to zero mean and unit standard deviation.\nWe evaluate the model prediction \u0177 against the ground truth y on our test set $N_{test}$ for all time steps t using the root mean squared error (RMSE) on the normalized data:\n$$\\begin{equation}\n\\text { RMSE }=\\sqrt{\\frac{1}{N_{\\text {test }}} \\sum_{t=1}^{N_{\\text {test }}}\\left(y_{t}-\\hat{y}_{t}\\right)^{2}}\n\\end{equation}$$"}, {"title": "3 Case study: Ammonia synthesis loops", "content": "We consider two ammonia synthesis loops as illustrative case studies with the same equipment sizing and feed flows but different topologies. In both processes, ammonia is produced from N2 and H2 under high pressure. The soft sensor target is the concentration of ammonia in the respective product streams. Both processes consist of four types of processing steps: (1) Compression, (2) Separation via flashing, (3) Heating/Cooling, and (4) Reaction with three reactor beds. The combination of these major units, however, differs between the two processes, reflecting different topologies found in industry [10].\nIn Process A (see Figure 3, left), the feed is first compressed in compressor K-101 and mixed with the recycle. The mixture is then further compressed in compressor K-102 and heated before entering the reactor R-101. The effluent leaving the reactor is flashed in the flash vessel V-101. The liquid phase leaving V-101 is the product stream, while the vapor phase of V-101 is purged and mixed with the inlet gas.\nIn Process B (see Figure 3, right), the feed is first compressed in compressor K-101 and combined with the reactor outlet. The mixture is then cooled and flashed in V-101, removing the ammonia as a liquid. The liquid ammonia then leaves the process as its product. The vapor phase leaving the flash V-101 is further compressed in compressor K-102 and heated, after which it is sent into the reactor R-101. The reactor outlet is purged before being mixed with the inlet stream.\nBoth processes are controlled using PID control. The control scheme follows the \"Mode I\" as detailed by [1]. Both the feed and the purge stream are controlled with a flow controller (FC-1 and FC-2, respectively). The level in the flash"}, {"title": "4 Results", "content": "We carry out the transfer learning with our topology-aware GNN in a two-step process: (1) We first train our model on the dataset of Process A until we reach convergence on our validation set. (2) We then test the transfer capabilities of the GNN model on Process B.\nFor this, we fine-tune the pretrained model on fractions of Process B, specifically on up to 51 points. This low number of training points reflects the often limited number of soft sensor target measurements available in industrial settings. For comparison, we also train a model on the data from Process B from scratch. We additionally test the zero-shot capability of the model pretrained on Process A. We train each model nine times using different seeds to mitigate the effects of training noise, and plot both the mean and standard deviation of the nine models combined. For training, we used a NVIDIA GEFORCE RTX 3090. The training times varied between 30 minutes for the full training dataset and 5 minutes for a fraction of the dataset.\nThe results are shown in Figure 4. We plot the fraction of the data from Process B used for training vs. RMSE on the test set of Process B. We also illustrate the zero-shot capabilities of the pretrained model by adding a data point for 0 datapoints used for training.\nBy analyzing the results shown in Figure 4, we observe three notable trends: (1) The zero-shot performance of the pretrained model is significant, achieving an RMSE of 0.9753 on Process B without any training data from Process B. In contrast, the model trained from scratch requires 46 datapoints to reach a similar performance level. We attribute this to the pretrained model's ability to transfer learned dynamics from Process A to the previously unseen Process B. (2) Both models improve their RMSE scores on Process B when trained on more data. The pretrained model achieves an RMSE of 0.7633 when trained on 51 datapoints, while the model trained from scratch also shows improvement, reaching an RMSE of 0.9776. However, the pretrained model consistently outperforms the model trained from scratch across different training data sizes. (3) The performance of the pretrained model deteriorates when fine-tuned on a single datapoint from Process B, performing worse than its zero-shot performance. We hypothesize that this might be due to the lack of sufficient information in a single datapoint, which could lead to overfitting or misrepresentation of the underlying process dynamics."}, {"title": "5 Conclusion", "content": "We propose a spatio-temporal soft sensor modeling framework using GNNs to enable transfer learning across topo- logically different processes. We utilize two similar but topologically different ammonia synthesis loops as our case study. We show that the topology-aware GNN is transferred from one process to the other. We further demonstrate how transfer learning reduces data requirement through good performance without any data on the target domain and even better performance on very little additional data. We show that using the pretrained model, we can reduce the RMSE by up to 24.15% when trained on 46 datapoints compared to the model trained from scratch.\nThis work presents a flexible framework that can be extended to various chemical process systems. Future work could focus on adapting this transfer learning framework to multi-process environments, generalizing beyond one-process pretraining. Further, the approach is yet to be verified on industrial data, which often poses additional challenges."}]}