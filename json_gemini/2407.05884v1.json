{"title": "One system for learning and remembering episodes and rules", "authors": ["Joshua T. S. Hewson", "Sabina J. Sloman", "Marina Dubova"], "abstract": "Humans can learn individual episodes and generalizable rules and also successfully retain both kinds of acquired knowledge over time. In the cognitive science literature, (1) learning individual episodes and rules and (2) learning and remembering are often both conceptualized as competing processes that necessitate separate, complementary learning systems. Inspired by recent research in statistical learning, we challenge these trade-offs, hypothesizing that they arise from capacity limitations rather than from the inherent incompatibility of the underlying cognitive processes. Using an associative learning task, we show that one system with excess representational capacity can learn and remember both episodes and rules.", "sections": [{"title": "Introduction", "content": "In the study of learning, two trade-offs have historically been observed in the behavior of computational models: (1) between the abilities to simultaneously learn individual episodes and generalizable rules and (2) between the abilities to learn and to remember. For example, connectionist models often show that (1) memorizing individual episodes leads to a reduced ability to learn the rules required to generalize to new episodes (\"overfitting\") (McClelland, McNaughton, & O'Reilly, 1995) and (2) learning in a new task leads to catastrophic forgetting of what has been learned in previous tasks (McCloskey & Cohen, 1989). These observations motivated the creation of dual-system theories, such as the complementary learning systems model (McClelland et al., 1995), which posit separate learning systems for learning and remembering episodes and rules.\nRecent research has shown that the trade-off between learning episodes and rules is not inherent to learning in computational systems. The computational models in which these trade-offs were historically observed had limited capacity: They could memorize only a small number of their observations. Computational systems with excess capacity \u2013 which can recover far more relationships between the features of observations - have the ability to both memorize and generalize, i.e., to learn both episodes and rules (Dubova & Sloman, 2023; Belkin, Hsu, Ma, & Mandal, 2019; Nakkiran et al., 2019; Davies, Langosco, & Krueger, 2023). In this study, we demonstrate that excess capacity systems can also overcome the apparent trade-off between learning and remembering, i.e., they can simultaneously successfully learn new episodes and rules and remember previously-learned episodes and rules."}, {"title": "Methods", "content": "Catastrophic forgetting. Human participants in the behavioral test referenced by McClelland et al. (1995) were tasked with memorizing batches of random word pairings in a blocked regime (Barnes & Underwood, 1959). During the first block, participants were presented with a list of words (list A) and tasked with memorizing arbitrary associations between the words on list A and the words on another list B (A - B pairings). During the second block, they were presented with a new word list C and tasked with memorizing arbitrary associations between the words on list A and on list C (A - C pairings). Over the course of training on the A - C pairings, participants were tested on the A - B pairings they learned during the first block. Participants showed memory interference, but were still able to retain most of the previously learned associations. McCloskey and Cohen (1989) modeled behavior in this task with a simple connectionist model. This model forgot nearly all information about the A - B pairings after being trained on the A - C pairings, a phenomenon they referred to as catastrophic forgetting.\nModel. Inspired by McCloskey and Cohen (1989), we used a simple multi-layer perceptron architecture with two hidden layers of equal width.\nTask. We expand on McCloskey and Cohen (1989)'s procedure by changing the data to vary on a continuum from rules to episodes, so that the dynamics of learning and forgetting of arbitrary associations between episodes and generalizable rules can be studied together.\nTwo sample datasets of 10 5-dimensional samples, $A_{train}$ and $A_{test}$, were created by sampling from a Gaussian probability distribution. These datasets were then passed through a transformation $f$. Two target datasets, $B$ and $C$, were each formed by taking a weighted sum between the transformed sample data and a set of random perturbations. A third target dataset $D$ was created by removing the random perturbations from $C$:\n$A_{train} \\sim N(0,1)$\n$A_{test} \\sim N(0,1)$\n$B = (1 \u2013 noise) \\cdot f(A_{train}) + noise \\cdot E_{B}$\n$C = (1 - noise) \\cdot f(A_{test}) + noise \\cdot E_{C}$\n$D = (1 \u2013 noise) \\cdot f(A_{test})$\nwhere $0 \\leq noise < 1$, $\u0454_{B} \\sim N(0,1)$ and $\u00a3_{C} \\sim N(0,1)$. $f$ represents the generalizable rule that characterizes the relationship between the sample and corresponding target data (i.e., that characterizes each of the $A_{train}$ - $B$, $A_{test}$ - $C$ and $A_{test}$-$D$ pairings). The noise parameter controls the amount of structure in the data: When $noise = 0$, the task amounts entirely to learning of the generalizable rule; when $noise = 1$, the task amounts entirely to learning arbitrary associations between the sample data and episodes $\u0454_{B}$ (in the $A_{train}$ - $B$ pairings) and $\u025b_{C}$ (in the $A_{test}$ - $C$ pairings).\nCapacity. Our key manipulation was the capacity of each model we tested. The capacity of a model is defined as the minimum number of hidden nodes needed to fully memorize a given dataset. Constrained capacity models have fewer hidden nodes than required to memorize the data they are"}, {"title": "Training", "content": "Training. During Block 1, the models were trained to associate $A_{train}$ with $B$, which involves learning both the rule $f$ and the arbitrary component of the episodes, $E_{B}$. During Block 1, we also tested the models' abilities to generalize from $A_{test}$ to $C$. During Block 2, the models were trained to associate $A_{train}$ with $C$. During Block 2, we also tested the models' abilities to recall the $A_{train}$ - $B$ pairings and to predict the $A_{test}$ - $D$ pairings, which capture the models' abilities to remember episodes and rules, respectively (we test learning of the rule on the basis of performance on the $A_{test}$ - $D$ pairings in order to isolate error from failure to learn $f$ from error caused by the noise added to $C$).\nThe models were optimized with Stochastic Gradient Descent using a mean squared error loss function (learning rate = 0.01). All models were trained until convergence, defined as a rate of decrease in loss going below 1 \u00d7 10-5 per 5,000 epochs. We ran all simulations 100 times."}, {"title": "Results", "content": "Consistent with prior literature (e.g., Belkin et al. (2019); Nakkiran et al. (2019)), in all cases, the models with excess capacity were better able than models with constrained and sufficient capacity to learn both episodes and rules (Fig. 2). The difference between the mean classification accuracy of all models was statistically significant (p < 0.001) for both learning episodes, defined by the t-test of the means between each pair of models, at the end of training. In other words, there was not a consistent trade-off between learning episodes and learning rules in the excess capacity regime.\nConsistent with prior work on catastrophic forgetting, models with constrained and sufficient capacity exhibited only a limited ability to retain prior knowledge when having to learn a new set of interfering associations. The models with excess capacity were better able to retain knowledge of both episodes and rules (Figs. 1 and 2). All pairwise comparisons between the means of performance of excess vs. constrained/sufficient capacity models were significant at the p < .001 level."}, {"title": "Conclusion", "content": "Our results demonstrate the in-principle ability of one computational learning system to both learn and remember episodes and rules. By challenging the traditional view of learning and remembering episodes and rules as inherently competing processes, this work opens new avenues for understanding the flexibility and nuance of cognitive function by exploring the properties of learning in different capacity regimes. Our findings also have important implications for the study of continual learning, transfer learning, and the development of more advanced cognitive architectures"}]}