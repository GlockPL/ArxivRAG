{"title": "ElastiFormer: Learned Redundancy Reduction in Transformer via Self-Distillation", "authors": ["Junzhang Liu", "Tingkai Liu", "Yueyuan Sui", "Stephen Xia"], "abstract": "We introduce ElastiFormer, a post-training technique that adapts pretrained Transformer models into an elastic counterpart with variable inference time compute. ElastiFormer introduces small routing modules (as low as .00006% additional trainable parameters) to dynamically selects subsets of network parameters and input tokens to be processed by each layer of the pretrained network in an input-dependent manner. The routing modules are trained using self-distillation losses to minimize the differences between the output of the pretrained-model and their elastic counterparts. As ElastiFormer makes no assumption regarding the modality of the pretrained Transformer model, it can be readily applied to all modalities covering causal language modeling, image modeling as well as visual-language modeling tasks. We show that 20% to 50% compute saving could be achieved for different components of the transformer layer, which could be further reduced by adding very low rank LoRA weights (rank 1) trained via the same distillation objective. Finally, by comparing routing trained on different subsets of ImageNet, we show that ElastiFormer is robust against the training domain.", "sections": [{"title": "1 Introduction", "content": "The success of Transformer [70] models across various domains has led to increasingly large and computationally intensive architectures. While these models achieve impressive performance, recent works have demonstrated significant redundancies in both the parameters and computations in pretrained large transformer architectures - which can either manifest as unnecessary processing of tokens [7, 56], or activation [16] of model parameters that contribute minimally to the final output.\nSeveral recent studies have attempted to locate these redundancies [16, 12, 53] and to leverage such redundancies via modified/optimized model architectures (e.g. Mixture-of-Depth which skips tokens around transformer layers [57]). However, such approaches often require architectural modifications, extensive retraining or even training from scratch (such as commonly done for sparse Mixture-of-Expert models [22, 34]).\nWe address these challenges by introducing ElastiFormer, a post-training technique that transforms pre-trained Transformer models into elastic alternatives with variable inference time compute. ElastiFormer introduces lightweight routing modules (as little as 0.0006% additional parameters) that dynamically select subsets of network parameters and input tokens to be processed in an input-dependent manner. These routing modules are trained using self-distillation loss to minimize differences between the output of the pretrained model and their elastic counterparts. Crucially, ElastiFormer makes no assumptions about the modality of the pretrained Transformer model, allowing it to be effectively applied across language, image, and visual-language modalities.\nThrough extensive experimentation, we demonstrate that 38% (Mutli-Head Attention, MHA) and 56% (Multi-Layer Perceptron, MLP) active parameters are required for ElastiFormer to match the performance of the base pretrained model in the language domain. We also show that ~ 20% tokens can be dropped from MLP processing (routed to output via residual without going through MLP), while almost all tokens need to be processed by MHA. However, by adding Low-Rank Adaptation (LORA) weights to MHA with as low as rank 1 learnable parameters, we can drop 20% tokens from MHA processing as well. We observe similar redundancy in parameters and token processing in the visual domain and show that applying ElastiFormer to even layers of the pretrained ViT backbone significantly improves performance at equivalent level of compute to applying ElastiFormer to all layers. Applying ElastiFormer to visual-language models, we show that 40% of image tokens can be dropped before being decoded by the language decoder without significantly impacting performance. Finally, by comparing routing modules in Elasti-ViT trained using different subsets of ImageNet, we show that the learned routing is robust to changing data distributions.\nIn summary, our main contributions are:"}, {"title": null, "content": "1. We introduce ElastiFormer, a post-training technique that converts pretrained transformer models across language, visual, and multi-modal modalities into flexible alternatives with variable inference time compute. ElastiFormer is compatible with other efficient post-training techniques."}, {"title": null, "content": "2. We demonstrate that with less than 0.3% (as low as 0.00006%) additional learnable parameters, ElastiFormer can reduce the total active parameters or active tokens by 20% to 50% for all modalities without noticeable impact on performance. We also provide extensive analysis of the degree of redundancy in different components of the transformer architecture across modalities."}, {"title": null, "content": "3. We show that the learned routing is robust to different distributions of training data, thereby providing a reliable representation for interpreting learned representations in pretrained models."}, {"title": "2 Related Works", "content": "Efficient Transformer Processing There is a large body of work aimed at improving transformer efficiency that falls into several categories: mixture of experts (MoE), mixture of depths (MoD), pruning, and quantization [78, 60, 18, 44, 41, 43]. Our work is compatible with existing post-training techniques, such as QLoRA [18], but is most similar in spirit to MoE, MoD, and pruning. Since the introduction of MoE in transformer architectures [61], there have been a flurry of developments, including more efficient MoE architectures [31, 15], sparsification of dense MLPs [85], incorporating gating functions for routing [82, 34], and realizing LLMs at scales larger than ever [22].\nPruning is another class of techniques used to statically remove redundant weights (unstructured) [11, 30] or smaller blocks (structured) in the MHA or FFN layers [47, 21, 37, 2, 76, 77, 9, 80]. Other works focus on pruning or merging context in the KV cache [3, 79] or individual patches in vision transformers [67, 33].\nMost similar to our work is the recent development of MoD [57], which proposes learnable routers that \"skip\" less relevant transformer layers in an input-dependent and compute-adjustable manner. In contrast to \"early exit\u201d methods [59, 45], which skip any remaining transformer layers, MoD may skip middle layers and still process later layers. Other works have shown varying levels of spatial and temporal redundancies in transformer architectures [19, 12, 16], and introduce methods for efficiently identifying and removing them during inference (e.g., skipping attention heads [28]). In contrast, our work provides a general learning framework for discovering and bypassing redundancies to enable more efficient processing pathways through pretrained transformer models.\nKnowledge Distillation (KD) and Domain Generalization. KD is a class of techniques that transfer knowledge from one model to another (often smaller), where the structure of the models are generally fixed [29, 65, 36, 73, 58, 69, 26, 66, 75, 40, 81]. Self-distillation (SD) is a sub-class of KD, where the student and teacher models are the same architecture. In many cases, the SD aims to distill knowledge learned in deeper layers to shallow layers in the same model, which has shown to reduce overfitting, improve domain generalization, and embed semantic information that is not commonly seen in non-SD models [64, 54, 8, 63, 35, 46, 32, 38, 74]. Most of these works focus on pretraining to improve task performance, while we incorporate SD as a mechanism to self learn router weights to reduce computation in an input-dependent and non-sequential manner.\nMechanistic Interpretability (MI). MI has recently emerged as a promising class of techniques for interpreting deep neural networks and transformer architectures [51, 55] by decomposing models into smaller components and identifying human interpretable \u201cfeatures\u201d and their pathways that form subnetworks or \u201ccircuits\u201d. A key question in MI is to explore the notion of \u201cuniversality\u201d or how similar features, activations, or circuits arise between different models and tasks. While many works have explored and identified how each component (e.g., MHA and FFN sublayers) functions within transformer architectures [20, 48, 49, 62, 23], there are mixed results when it comes to discovering universality. Several works have identified similar components, such as induction and duplication heads, that develop across different models and tasks [52, 24, 72, 50], while others observe that different weight initializations for the same task result in different circuits and representations [84, 13]. We believe our work could be used to study the interpretability of transformer architectures by analyzing the learned routers across different model architectures and domains."}, {"title": "3 Redundancy in Transformer Architecture", "content": "We first demonstrate that pretrained transformer models have data-dependent redundancies in both their multi-head attention (MHA) and multi-layer perceptron (MLP) modules. We progressively dropped attention heads and MLP layers in a pretrained Gemma-2-2b-it [68] model, and quantified the performance degredation on both mathematical reasoning (GSM8K[14]) and code generation (HumanEval[10]) tasks. Note that in these experiments, no additional learnable parameters were introduced."}, {"title": "4 ElastiFormer - Learned Routing of Transformer Models via Self-Distillation", "content": "Given the redundancy in pre-trained Transformer models, we seek a lightweight post-training technique to minimize the number of active parameters necessary without significantly deteriorating overall performance. Consequently, we propose ElastiFormer which introduces lightweight learned, input-dependent routing modules that route input tokens (regardless of modality) through a subset of the pre-trained Transformer network.\nDepending on the modality, ElastiFormer introduces 4-5 routing modules that control routing around and within all modules of the transformer architecture (i.e. Multi-Head Attention, MLP). The routing schemes can be roughly divided into two categories (see Figure. 3):"}, {"title": null, "content": "\u2022 Input Subset Selection: For this type of subset selection, given a sequence of T tokens, we select k to be processed by a given module (e.g. MHA, MLP). This routing scheme saves computational cost by reducing the total number of input tokens processed. Examples of routing in ElastiFormer that employ this scheme are routing around MHA, around MLP (Figure 1(Left)), and visual token selection in VLM (Figure 1(Mid-Bottom))."}, {"title": null, "content": "\u2022 Parameter Subset Selection: For this type of subset selection, the total number of inputs to a given module remains unchanged. Instead, we save computational cost by reducing the number of active parameters within a given module that is used to process the given input. Examples of routing in ElastiFormer that employ this scheme are routing within MHA (attention head selection) and within MLP (expert selection)."}, {"title": "4.1 Parameter Subset Selection", "content": "We consider the routing problem where we are given input token $x_t \\in \\mathbb{R}^D$, and M subsets of sub-networks $f_1, ..., f_M$ ($M \\ll D$). A simple router (linear projector) is trained to select top k, 1 < $k \\leq M$ sub-networks to process the given input following the procedure outlined in Algorithm 1."}, {"title": "Algorithm 1 Parameter Subset Selection w/ Linear Router", "content": "Require: Input token $x_t \\in \\mathbb{R}^D$\nRequire: Set of sub-networks ${f_m}_{m=1}^M$\nRequire: Number of routes to select k, where 1 $\\leq$ k $\\leq$ M\nRequire: Router parameters $W_r \\in \\mathbb{R}^{M \\times D}$\n1: $w_t \\leftarrow M \\cdot softmax(W_rx_t)$ $\\triangleright$ Routing Weights\n2: ${ (m_i, W_{t,m_i}) }_{i=1}^k \\leftarrow top-k(w_t, k)$ $\\triangleright$ Subset Selection\n3: for i = 1 to k do\n4:  $Y_{t,m_i} \\leftarrow W_{t,m_i} \\cdot f_{m_i}(x_t)$ $\\triangleright$ Sub-network Forward\n5: end for\nreturn $y_t$ = Aggregate($\\{y_{t,m_i}\\}$)"}, {"title": null, "content": "At the high level, the parameter subset selection works essentially the same as the Mixture-of-Expert routing with a straight-through estimator for gradient propagation, with the noticeable difference that the weight vector is normalized to sum to M. This additional normalization ensures that when the router selects all sub-networks (number of routes k = M) with uniform routing weights $W_{t,i} = 1 \\forall i = 1,..., k$, the overall input/output of the routed network is exactly the same as the pre-trained model without routing.\nThe sub-network ${f_m}_{m}$ could either refer to processing via individual attention heads or via an expert in the Mixture-of-Expert MLP module. As many pre-trained transformer models have dense MLP layers (no experts), ElastiFormer converts a dense MLP module to a MoE counterpart losslessly by breaking parameters into block matrices. Consider a simple MLP with 1 hidden layer, we can rewrite the input/output relationship of the dense MLP as the equivalent MoE MLP with 2-experts:\n$Y_t = W_2 \\sigma(W_1x_t) = [W_{2,1} W_{2,2}] \\sigma \\left(\\begin{array}{c}W_{1,1} \\\\W_{1,2}\\end{array} x_t\\right)$\nwhere $\\sigma(\\cdot)$ is some element-wise nonlinearity, and $W_{1,i}, W_{2,i}$ are block matrices obtained by splitting the weight matrices row-wise and column-wise respectively. While we found decomposing the pretrained weights to block matrices sufficient, we acknowledge that alternative initialization of MoE MLP from dense MLP exists and have been explored by contemporary works [85, 82, 86]. We leave further explorations of MoE initialization from dense MLP in the context of ElastiFormer for further work."}, {"title": "Input Subset Selection", "content": "We consider the routing problem where we are given input token $1_{1:T} \\in \\mathbb{R}^{T \\times D}$ and a given module f : $\\mathbb{R}^D \\rightarrow \\mathbb{R}^D$. A router is trained to select top k, 1 $\\leq$ k $\\leq$ T tokens to process by the module following the Algorithm 2.\nAs opposed to parameter subset selection where the router outputs a M-dimensional logits for M sub-networks, the routers in input subset selection output scalar-valued logits for each input token."}, {"title": "Algorithm 2 Input Subset Selection w/ Linear Router", "content": "Require: Input tokens $1_{1:T} \\in \\mathbb{R}^{T \\times D}$\nRequire: Module f\nRequire: Number of tokens to select k, where 1 $\\leq$ k $\\leq$ T\nRequire: Router parameters $W_r \\in \\mathbb{R}^D$\n1: $w \\leftarrow softmax(W_rx_{1:T})$ $\\triangleright$ Routing weights $w \\in \\mathbb{R}^T$\n2: $\\mathcal{I} \\leftarrow top-k(w, k)$ $\\triangleright$ Index set of selected input\n3: $Y_{1:T}\\leftarrow 0\\in \\mathbb{R}^{T \\times D}$ $\\triangleright$ Zero-initialized output\n4: $Y_{t \\in \\mathcal{I}} \\leftarrow W_{t \\in \\mathcal{I}}\\cdot f(x_{t \\in \\mathcal{I}})$\nreturn y"}, {"title": null, "content": "The top-k input tokens are then processed by the subsequent module (MHA or MLP), and the output is added to a zero-initialized output tensor with the same shape as the un-selected input $1_{1:T}$.\nNote that, as discussed in [57], for causal language models that generate tokens auto-regressively, the top-k token selection only applies to the training phase and not the inference phase - where causality dictates whether a given token will be in the top-k cannot be determined without first completing the generation of the entire sequence. Authors of [57] proposed two approaches to predict whether a given token will be in the top-k during inference time based on 1) the output logits of the same router that computes the routing weights, or 2) output logits of an additional dedicated classification MLP that similarly maps input embedding $x_t$ to a scalar logit value. Within the context of ElastiFormer, we found the two methods of top-k prediction during inference to give similar performance and opted to use the former (same router for top-k prediction) to minimize additional learnable parameters."}, {"title": "4.2 Objective Function", "content": "The primary training objective of ElastiFormer is distillation loss $\\mathcal{L}_{distill}$. For causal language modeling and visual-language modeling where the output modality is language, we compute KL-divergence between the student model's output probability $P_{student}$ and the teacher model's output probability $P_{teacher}$. We compared different modifications of the KL-divergence objective for distillation losses on a toy problem, where the teacher model is a pretrained GPT-Neo-125M [6] model, and the student model is initialized from teacher model with Gaussian parameter noised and a rank-32 LoRA adapter for q_proj,v_proj components in the MHA module. The student model is trained on the GSM8K training dataset using combinations of three different types of modifications to the KL-divergence objective (see also Figure 4):"}, {"title": null, "content": "\u2022 Forward $D_{KL} (P_{student} || P_{teacher})$ versus Reverse KL $D_{KL}(P_{teacher}||P_{student})$ [71, 25]"}, {"title": null, "content": "\u2022 Top-K KL [4]: convert the teacher model's output probability over vocabulary to a k +1 dimensional vector, where the first k values correspond to the probability of the top-k tokens in the vocabulary with the highest probability, and the last value is the residual probability to ensure the k + 1 probability vector sums to 1. The student model's output is arranged using the top-k token indices of the teacher model."}, {"title": null, "content": "\u2022 Temperature scaling [83]: divide the logits of student and teacher model outputs by a positive scalar temperature before converting to probability distribution via softmax."}, {"title": null, "content": "As shown in Figure 4, the language modeling loss of the student model on the evaluation dataset shows that forward KL loss of top-50 tokens leads to the best performance and fastest convergence rate, which is the distillation loss we adopted in the current work for both language modeling and visual-language modeling tasks. For image modeling (applying ElastiFormer to ViT-MAE [27]), we chose the cosine distance between the student image encoder's output token embedding and that of the teacher model for simplicity. Note that as the output token embedding of the ViT-encoder is normalized via LayerNorm, minimizing cosine distance is equivalent to maximizing the inner product between student and teacher token embedding.\nIn addition to the distillation loss, ElastiFormer training involves additional auxiliary losses for the two types of routing modules described above.\nFor routing modules that perform parameter subset selection, the routers are additionally trained using load-balancing loss $\\mathcal{L}_{load}$ which minimizes the weighted sum of input tokens processed by each sub-network. While such load-balancing loss is crucial for MoE models where both the experts and the routers are trainable to prevent mode collapse and ensure balanced sub-network utilization, its effect is much less pronounced for ElastiFormer where the sub-networks are frozen. Nevertheless, we found this auxiliary loss to be beneficial for stabilizing training of router modules especially for very low k values (very few sub-networks activated).\nFor routing modules that perform input subset selection, we only include auxiliary loss in the case of causal language modeling. As mentioned in the previous section, during inference, scalar logits of a token selection module are used to determine if a given token will likely be in the top-k of the final generated sequence or not. Consequently, during the training phase, binary cross-entropy loss $\\mathcal{L}_{top-k}$ between the scalar logits and one-hot encoding of whether the corresponding token is indeed in top-k is introduced as an auxiliary loss to the input subset selection for causal LMs [57].\nAs such the overall objective for ElastiFormer is:\n$\\mathcal{L} = \\mathcal{L}_{distill} + \\lambda_{load}\\mathcal{L}_{load} + \\lambda_{top-k}\\mathcal{L}_{top-k}$\nfor Causal LM(1)\nIn practice, we found $\\lambda_{load} = \\lambda_{top-k} = 1$ to be sufficient for convergence of all evaluated model architectures."}, {"title": "5 Experiments", "content": "In this section, we describe in detail the experimental setup and findings of the current work. For reference, we included the total number of additional parameters introduced for each ElastiFormer experiment in Table 1 which highlights that ElastiFormer introduces minimal (0.25% ~ 0.00006%) additional learnable parameters while achieving significant redundancy reduction across modalities. Unless otherwise specified, all experiments were performed on 1 NVIDIA H100 NVL 96GB GPU with AdamW optimization of learning rate 1e-4 and cosine learning rate scheduler with 3% warmup."}, {"title": "5.1 Elasti-LLM - ElastiFormer for Causal Language Model", "content": "Experimental Setup We applied ElastiFormer to both Phi-3.5-mini-instruct [1] and Gemma-2-2b-it [68] pretrained language models, where the models were trained via self-distillation on the GSM8K [14] training set (7.5K question/answer pairs) for 3 epochs and batch-size of 32.\nScaling of Performance vs. Capacity We begin by performing an extensive ablation study of the 4 types of routing in Elasti-LLM shown in Figure 5."}, {"title": "5.2 Elasti-ViT - ElastiFormer for Vision Transformer", "content": "Experimental Setup We applied ElastiFormer to vit-mae-large [27] pretrained ViT models and trained the encoder part of the ViT-MAE model via self-distillation on a 10% subset of the ImageNet-1K [17] training set (~1M images) for 3 epochs with batch-size of 900. The training objective minimizes the cosine distance between Elasti-ViT's output token embedding and that of the pretrained ViT-MAE model, while the evaluation is done by comparing the cosine similarities between the decoder output between student and teacher models (reconstructed images).\nScaling of Performance vs. Capacity Similar to scaling analysis for causal language models, we performed scaling analysis of Elasti-ViT for different level of subset selection of the 4 different types of routing modules. We evaluate the performance of each experiment by cosine similarity between the MAE decoder (pretrained and frozen) outputs when given encoder output of Elasti-ViT encoder and the based encoder. Example reconstructions are shown in the Supplementary Materials.\nContrary to the experiments in language modeling, results in Figure 7 show that only input selection for MLP module is able to achieve > 0.95 cosine similarity with teacher model's decoder output with input capacity > 0.8 (dropping 20% input tokens to MLP modules). This is potentially caused by the much smaller size of ViT-MAE-L models, which has 330M parameters which less than 10% of the size of Phi-3.5-mini-instruct.\nWe explore an alternative technique to improve the performance of Elasti-ViT, instead of adding learnable LORA modules, we applied ElastiFormer to only even layers of the ViT-MAE model. Applying ElastiFormer to even layers reduces computational savings by half while simultaneously reducing the learnable routing parameters by half. As shown in Figure 7, when Elasti-ViT with even layer routing significantly improves the model performance, where 50% MHA Attention-Heads, 31% MLP active parameters, 70% active tokens for MHA and 10% active tokens for MLP are sufficient to achieve > 0.95 cosine similarity to the teacher model's decoder output. More importantly, by preserving computing in half of the layers, we are able to achieve higher saturing performance of Elasti-ViT. Note that this approach to applying ElastiFormer to even layers is also compatible with adding LoRA weights, which could further improve computational savings as in the case for Elasti-LLM.\nAdditionally, we examined the robustness of learned routing of Elasti-ViT against different distributions of training data. To this end, we trained 10 instances of Elasti-ViT using 10 subsets of ImageNet"}, {"title": "5.3 Elasti-VLM - ElastiFormer for Visual Language Model", "content": "Training and Evaluation Setup We applied ElastiFormer to llava-v1.5-7b [42] pretrained visual-language models, which the was trained via self-distillation on a 10K subset of the 665K LLAVA-Instruct dataset for 1 epoch with batch-size of 32.\nScaling of Performance vs. Capacity Since experiments for causal LMs and ViTs have established results on redundancy in image encoder and language decoder respectively, we focus our experiments on Elasti-VLM to redundancy in image tokens (output of the visual encoder) processed by the language decoder. In particular, we added an input subset selection routing module that selects the top-k image tokens to be processed by the language decoder as shown in Figure 1(Mid-Bottom).\nWe evaluated Elasti-VLM's performance on both OpenChair [5] hallucination benchmark and LLava-Bench [42] open domain QA benchmark, and compared against performance of the base pretrained VLM model (1lava-v1.5-7b). We additionally experimented with more complex routing modules with an MLP router with 1 hidden layer and GELU activation function. As shown in Figure 9, the LLava-Bench results show that Elasti-VLM is able to achive the same performance as the base model with 60% 70% of input tokens, and is able to even outperform the base model for complex and conversational tasks. However, for tasks that required detailed visual information such as LLava-Bench (detail) and OpenChair which evaluates VLM's ability to accurately describe all key objects in an image, Elasti-VLM offers no clear advantage over the base VLM model."}, {"title": "6 Summary & Discussion", "content": "ElastiFormer demonstrates that significant computational savings can be obtained through learned routing across modalities, and can be readily incorporated into existing post-training pipelines with techniques such as LoRA. Our method far out-performs static pruning of model parameters with minimal additional parameters and robust learned routing, which provides practical tools for not only model efficiency but also interpreting learned representations in various components of the transformer architecture.\nAn interesting finding of the current work, and perhaps a key limitation, is that the computational savings of skipping tokens from processing by attention modules do not perform as well as other sources of computational savings (skipping attention heads, skipping MLP blocks, reducing MLP active parameters). While this performance degradation can be rescued by very low-rank LoRA adapters for the attention module, it raises the question of the type of redundancy that exists in attention computation. We hypothesize that the comparatively poor performance of input subset selection for MHA is due to the design of the routing module, which performs routing based on individual token embedded in a way that is agnostic to context. Indeed, choosing the tokens that have the least impact on MHA module output requires knowing a priori the degree to which this token will be attended to by other tokens. One potential improvement of the current formulation of ElastiFormer is to compute the routing of tokens for the MHA module based on not only the token embedding but also the attention weights of previous layers. We leave explorations of alternative methods for routing tokens for MHA layer for future works."}, {"title": "A Pruning Experiments for Analyzing Redundancy in Pretrained LLMs", "content": "Here we describe the procedure in which the redundancy in pretrained LLMs is studied as reported in Section 3."}, {"title": "A.1 Model and Setup", "content": "We utilized the Gemma-2-2b-it model as the base pretrained LLM for our experiments. The redundancy was analyzed by systematically removing components of the model and evaluating the resulting impact on its performance. Specifically, we focused on two key architectural components:"}, {"title": null, "content": "\u2022 Entire Transformer Layers: Skipping entire layers in the Transformer architecture."}, {"title": null, "content": "\u2022 Multi-Head Attention (MHA): Removing individual attention heads within the Transformer layers."}, {"title": "A.2 Component Removal Procedure", "content": "Random Selection:"}, {"title": null, "content": "\u2022 We progressively skipped a random subset of full Transformer layers or removed a random subset of attention heads within the MHA module. Random selection ensured that the skipping process did not introduce systematic bias."}, {"title": null, "content": "\u2022 For each target number of components to remove (e.g., 3 Transformer layers or 3 attention heads), we randomly selected 5 distinct groups of components and performed removal sepa-rately. The final results for each configuration were obtained by averaging the performance metrics across these 5 groups."}, {"title": null, "content": "\u2022 The number of skipped Transformer layers or removed attention heads was increased incrementally in each experimental configuration to evaluate performance degradation at different levels of model pruning."}, {"title": "No Additional Training:", "content": "\u2022 After skipping Transformer layers or removing attention heads, no additional learnable parameters were introduced. The remaining model weights were frozen, with no fine-tuning or retraining performed."}, {"title": "A.3 Evaluation Metrics", "content": "The impact of skipping Transformer layers or removing attention heads was quantified using the following metrics:"}, {"title": null, "content": "\u2022 Language Modeling Loss Difference ($\\Delta$ LM Loss): The difference in LM loss between the base model and the modified (pruned) model was computed as:\n$\\Delta$LM Loss = $LOSS_{pruned}$ - $LosS_{original}$\nThis metric captures the overall degradation in predictive performance due to skipping layers or components."}, {"title": null, "content": "\u2022 Top-1 Token Prediction Agreement (Top-1 Match): The percentage of tokens for which the pruned model and the base model predicted the same vocabulary index. This is calculated as:\nTop-1 Match = $\\frac{Count_{matched}}{Total_{tokens}}$\nwhere $Count_{matched}$ is the number of tokens with identical predictions, and $Total_{tokens}$ is the total number of tokens in the evaluation dataset."}, {"title": "A.4 Experimental Tasks and Datasets", "content": "The experiments were conducted on two datasets with distinct characteristics:"}, {"title": null, "content": "\u2022 GSM8K: A benchmark for mathematical reasoning tasks."}, {"title": null, "content": "\u2022 HumanEval: A benchmark for code generation tasks."}, {"title": "A.5 Results Analysis", "content": "The results, visualized in Figure 2 of the main text, reveal the following:"}, {"title": null, "content": "\u2022 Skipping a small number of Transformer layers or attention heads caused minimal perfor-mance degradation across both datasets."}, {"title": null, "content": "\u2022 Skipping Transformer Layers: Performance degradation was more severe as the number of skipped layers increased, with a sharp rise in LM loss and a significant reduction in Top-1 Match."}, {"title": null, "content": "\u2022 Removing Attention Heads: The model exhibited greater tolerance to the removal of individual attention heads, with slower performance degradation compared to skipping entire Transformer layers."}, {"title": null, "content": "\u2022 The differing rates of performance degradation between GSM8K and HumanEval suggest that redundancy is task-dependent."}, {"title": "B Implementation Details of ElastiFormer", "content": "ElastiFormer lowers model redundancy through two key mechanisms: input selection and parameter selection. Input selection determines which tokens (inputs) are processed by the transformer layer and which are skipped. Once a token passes through the input selection stage and enters the layer, parameter selection identifies the specific parameters to be utilized during computation. In this section, we will discuss the implementation of each mechanism in detail."}, {"title": "B.1 Input Subset Selection", "content": "Routing Module Given a set of tokens $X$ = {$x_1", "B$": "R^D$ $\\rightarrow$ $R^D$", "R$": "R^D$ $\\rightarrow$ [0", "1": "produces a set of routing scores $R(T)$ = {$s_1", "by": "n$B_R(X)$ = {$B(x_t) \\cdot 1[R(x_t) > \\theta"}, {"1": "R $\\rightarrow$ {$0", "models": "due to the auto-regressive nature of causal language models, the distribution of the routing weights can change during the generation process, and thus the top k tokens as well, therefore we use a decision threshold of 0.5 during inference instead.\nThe selection threshold $\\theta$ is determined by a hyperparameter called the capacity factor, c. During training, tokens are selected based on the top k routing scores, where k = c.T. However, this approach is impractical during inference. Due to the auto-regressive nature of language models, the distribution of routing weights can vary throughout the generation process. Consequently, the set of top k tokens may also change dynamically, making static selection infeasible. Therefore we, use a decision threshold of 0.5 during inference instead.\nAuxiliary Loss During inference, the router does not enforce the capacity constraint used in training, as it relies on top k selection. This discrepancy can lead to the router selecting significantly more tokens than intended, as the gradient produced by (B.2) may encourage such"}]}