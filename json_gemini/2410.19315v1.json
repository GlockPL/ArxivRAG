{"title": "A PRESCRIPTIVE THEORY FOR BRAIN-LIKE INFERENCE", "authors": ["Hadi Vafaii", "Dekel Galor", "Jacob L. Yates"], "abstract": "The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models, such as Variational Autoencoders (VAEs). In the neuroscience literature, an identical objective is known as the variational free energy, hinting at a potential unified framework for brain function and machine learning. Despite its utility in interpreting generative models, including diffusion models, ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson assumptions for general sequence data leads to a spiking neural network that performs Bayesian posterior inference through its membrane potential dynamics. The resulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to biological neurons than previous brain-inspired predictive coding models based on Gaussian assumptions. Compared to amortized and iterative VAES, iP-VAE learns sparser representations and exhibits superior generalization to out-of-distribution samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides a solid foundation for developing prescriptive theories in NeuroAI.", "sections": [{"title": "1 Introduction", "content": "Optimizing the Evidence Lower Bound (ELBO) serves as a unifying objective for training deep generative models (Hinton et al., 1995; Dayan et al., 1995; Kingma & Welling, 2014; Rezende et al., 2014; Luo, 2022). Even when models don't explicitly reference ELBO, they're often optimizing objectives closely related to it (Luo, 2022; Kingma & Gao, 2023). This is directly paralleled by the Free Energy Principle (FEP) in neuroscience, which absorbs previous theoretical frameworks like Predictive Coding, Bayesian Brain, and Active Learning (Friston, 2005, 2009, 2010). FEP states that a single objective, the minimization of variational free energy, is all that is needed. Because this is equivalent to maximizing ELBO, it suggests a powerful unifying theoretical framework for neuroscience and machine learning (Friston, 2010).\nHowever, in many ways, Free Energy (and by proxy, ELBO) is too general to be useful as a theory (Gershman, 2019; Andrews, 2021). In practice, the specific implementations of FEP predictive coding have been difficult to map directly onto neural circuits (Millidge et al., 2021a, 2022), struggling with negative rates and prediction signals that have not been observed empirically (Walsh et al., 2020; Millidge et al., 2022). Similarly, in machine learning, it is often discovered after the fact that a new objective is actually ELBO maximization (or KL minimization; Hobson (1969)) masquerading as something else (Kingma & Gao, 2023)\u2014and not the other way around. If ELBO is \u201call you need,\u201d then why is ELBO not prescriptive?\nOne possibility, at least in neuroscience, is that ELBO's lack of prescriptive theory results from incorrect approximating distributions. In fact, most of the difficulty mapping predictive coding onto neural circuits has to do with terms that result from the Gaussian assumption (Millidge et al., 2022). In contrast, biological neurons are largely modeled as conditionally Poisson (Goris et al., 2014).\nRecent work provides a potential prescriptive route: replacing Gaussians with Poisson distributions. To this end, Vafaii et al. (2024) introduced a reparameterization algorithm for training Poisson Variational Autoencoders (P-VAE). They observed that replacing Gaussians in ELBO reduces to an amortized version of sparse coding, an influential model inspired by the brain that captures many features of the selectivity in early visual cortex (Olshausen & Field, 1996, 2004). P-VAE learns sparse representations, avoids posterior collapse, and performs better on downstream classification tasks. However, the authors identified a large amortization gap in P-VAE (Vafaii et al., 2024), adding to a growing body of work that highlights limitations of amortized inference (Cremer et al., 2018; Kim & Pavlovic, 2021). A potential"}, {"title": "2 Background and related work", "content": "Generative models and ELBO. Generative models learn to represent the data distribution, $p_{\\theta}(x)$, typically by invoking latent variables z, such that $p_{\\theta}(x) = \\int p_{\\theta}(x|z)p_{\\theta}(z)dz$ (Bishop & Nasrabadi, 2006). The key challenge is computing, $p_{\\theta}(z|x)$, the posterior distribution of these latent variables given the data, which is typically intractable except for simple cases.\nVariational inference (Blei et al., 2017), offers a practical solution by introducing an approximate posterior $q_{\\phi}(z|x)$ parameterized by $\\phi$. The goal is to make this approximation as close as possible to the optimal posterior $p_{\\theta}(z|x)$. Ideally, one would minimize the KL divergence between $q_{\\phi}(z|x)$ and $p_{\\theta}(z|x)$, but since we cannot compute $p_{\\theta}(z|x)$ exactly, direct minimization is not feasible.\nThe Evidence Lower Bound (ELBO) provides a tractable objective that indirectly minimizes the KL divergence between the approximate and optimal posteriors. Specifically, the relationship is:\n$\\log p_{\\theta}(x) = E_{q_{\\phi}(z|x)} [\\log \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z|x)}] + D_{KL} (q_{\\phi}(z|x) || p_{\\theta} (z|x))$\n$\\text{ELBO}$\n(1)\nSince $\\log p_{\\theta}(x)$ does not depend on $\\phi$, and the KL divergence is non-negative, maximizing the ELBO effectively mini- mizes the intractable KL divergence (Hinton et al., 1995; Kingma & Welling, 2014; Rezende et al., 2014). Interestingly, even when generative models seem to optimize a different loss function, like diffusion models (Chan, 2024; Ho et al., 2020), they are often still performing KL minimization through the ELBO (Kingma & Gao, 2023; Luo, 2022).\nELBO in Neuroscience. The Evidence Lower Bound (ELBO) has an identical formulation in neuroscience, where it is referred to as the \"variational free energy\" (Friston, 2005, 2009, 2010). The Free Energy Principle (FEP) extends the framework of perception as inference (Alhazen, 1011\u20131021 AD; Von Helmholtz, 1867; Lee & Mumford, 2003), drawing concepts from predictive coding (PC; Srinivasan et al. (1982); Rao & Ballard (1999)). Extensive research has explored how PC might be implemented by neurons (Boerlin et al., 2013; Millidge et al., 2021a), and PC has been applied in machine learning for predictive models (Lotter et al., 2017; Wen et al., 2018; Millidge et al., 2024).\nDespite their neural inspiration, FEP is difficult to map directly onto neuronal circuits (Kogo & Trengove, 2015; Aitchison & Lengyel, 2017). This challenge stems from assuming Gaussian distributions for the approximate posterior and prior (Millidge et al., 2022). The Gaussian assumption results in models with explicit predictions or prediction errors, which have not been observed empirically (Mikulasch et al., 2023). Solutions also struggle with avoiding negative firing rates due to subtraction operations (Bastos et al., 2012; Keller & Mrsic-Flogel, 2018). While leaky integrate-and-fire circuits can be engineered to perform predictive coding (Boerlin et al., 2013), this approach goes in"}, {"title": "3 Iterative Poisson VAE (IP-VAE)", "content": "In this section, we derive the ELBO for sequences with Poisson distributions. We show the resulting architecture (iP-VAE; Fig. 1b) implements iterative Bayesian posterior inference with dynamics on the log rates. We relate this directly to membrane potential dynamics in a spiking neural network and show that it solves many of the implementation limitations of classic predictive coding.\nGeneral setup. We conceptualize iterative inference by starting with the more general framework of inference over a sequence (Chung et al., 2015). From there, we can treat iterative inference for images as a sequence of the same image repeated at all time points. This approach is appealing because dynamics emerge necessarily, and it builds a foundation for future work on dynamic sequences.\nConsider a sequence of T + 1 observed data points, $X = {x_t}_{t=0}^T$ where $x_t \\in \\mathbb{R}^M$, and corresponding latent variables, $Z = {z_t}_{t=0}^T$ where each $z_t$ is K-variate. We denote the full probabilistic generative model as the joint distribution, $p_{\\theta}(X, Z)$. A reasonable starting assumption for modeling the physical world is Markovian dependence between consecutive data points (Van Kampen, 1992), resulting in the marginal distribution:\n$p_{\\theta}(X) = \\int p_{\\theta}(X,Z) dZ = p_{\\theta}(x_0) \\prod_{t=1}^T p_{\\theta}(x_t|x_{t-1}),$\n(2)\nwhere $p_{\\theta}(x_0) = \\int p_{\\theta}(x_0|z_0)p_{\\theta}(z_0)dz_0$, and $p_{\\theta}(x_t|x_{t-1}) = \\int p_{\\theta}(x_t|z_t)p_{\\theta}(z_t|x_{t-1})dz_t$. For an intuitive derivation, see appendix B.1. The ELBO for our sequence data can be written as follows:\n$\\log p_{\\theta}(X) \\geq E_{q_{\\phi}(Z|X)} [\\log \\frac{p_{\\theta}(X, Z)}{q_{\\phi}(Z|X)}]$\n$= E_{q_{\\phi}(Z|X)} [\\log p_{\\theta} (X|Z)] - D_{KL} (q_{\\phi} (Z|X) || p_{\\theta}(Z))$\n$= L_{ELBO} (X; \\theta, \\phi),$\n(3)\nwhere $p_{\\theta}(X|Z)$ is a prior (either learned or fixed) over latents and $p_{\\theta}(X|Z)$ is the conditional likelihood distribution, which is computed via a decoder network. The model parameters ($\\phi$, $\\theta$)\u2014corresponding to the encoder and decoder networks of a VAE\u2014are jointly optimized. For readability, we will omit the explicit dependence on ($\\phi$, $\\theta$) moving forward. We will next express the ELBO for sequences within the Poisson VAE (P-VAE) framework.\nIterative Poisson VAE. To extend the P-VAE to sequences, iP-VAE needs to make explicit how the prior and posterior distributions update with each sample. The simplest starting point is assuming stationarity, implying that the posterior over the previous stimulus should act as a prior for the current one (Fig. 7; although future extensions could extend to nonstationary signals such as videos with a more sophisticated update rule). Because of the Markovian assumption, the prior, p(Z), then factorizes into the initial prior, p(zo) and a product over all future time steps:\n$p(Z) = p(z_0) \\prod_{t=1}^T p(z_t|x_{t-1})$\n(4)\nThe initial prior, $p(z_0) = Pois(z_0; r_0)$, is Poisson with learned prior rates, $r_0 \\in \\mathbb{R}_+^K$. Subsequent time steps have prior rates that depend on the stimulus from the previous time step, $p(z_t|x_{t-1}) = Pois(z_t; r_t(x_{t-1}))$, with $r_t \\in \\mathbb{R}_+^K$ for all t. The approximate posterior factorizes as well:\n$q(Z|X) = q(z_0|x_0) \\prod_{t=1}^T q(z_t|x_t, x_{t-1}),$\n(5)\nwith initial posterior, $q(z_0|x_0) = Pois(z_0; r_0 \\odot \\delta r(x_0))$, and time-dependent posterior, $q(z_t|x_t, x_{t-1}) = Pois(z_t; r_t(x_{t-1}) \\odot \\delta r(x_t))$, both parameterized as Poisson distributions. We follow the formulation in Vafaii et al. (2024), and define the posterior rates via an element-wise multiplicative interaction between r and some gain modulator, $\\delta r \\in \\mathbb{R}_+^K$. This is a natural choice because rates must be positive. Without loss of generality, the relationship between two positive variables can be expressed as a base rate with a multiplicative gain applied to it."}, {"title": "4 Experiments", "content": "We performed empirical analyses of iP-VAE and alternative iterative VAE models. In section 4.1, we test the general performance and stability of inference dynamics, including generalization to longer sequence lengths. Section 4.2 shows iP-VAE closes the gap with sparse coding. Section 4.3 demonstrates robustness to out-of-distribution (OOD) samples by evaluating models trained on MNIST (LeCun et al., 2010) with perturbed samples (e.g., rotated MNIST). We then evaluate OOD generalization from MNIST to other character-based datasets in section 4.3. Finally, in section 4.4, we visualize the learned weights of iP-VAE, revealing their compositional nature, which is consistent with iP-VAE's strong generalization capabilities. We push the limits of MNIST-trained models by testing their performance on natural images.\nArchitecture notation. We experimented with both convolutional and multi-layer perceptron (MLP) architectures. We highlight the encoder and decoder networks using red and blue, respectively. We use the (enc|dec) convention to clearly specify which type was used. For example, (mlp|mlp) means both encoder and decoder networks were mlp. We use the notation (jacob|mlp) to denote our fully iterative (non-amortized) iP-VAE, with a Jacobian-based encoder (Fig. 1b). We chose symmetrical architectures, such that (mlp|mlp) has exactly twice as many parameters as (jacob|mlp).\nDatasets. For the generalization results, we use MNIST, extended MNIST (EMNIST; Cohen et al. (2017)), Omniglot (Lake et al., 2015) and Imagenet32x32 (Chrabaszcz et al., 2017). We resize Omniglot and ImageNet32 down to 28 \u00d7 28 for more straightforward comparisons. We also replicated the sparsity analysis in Fig. 3 of Vafaii et al. (2024) in our Table 1, using the van Hateren natural images dataset with whitened, contrast normalized 16 \u00d7 16 patches.\nAlternative models. We compare our iterative P-VAE (iP-VAE) to P-VAE. The main difference between their two architectures is that the latter independently parameterizes an encoder, whereas the former constructs its encoder adaptively by inverting the decoder. We also compare to state-of-the-art methods that combine iterative with amortized inference. These include iterative amortized VAE (ia-VAE; Marino et al. (2018)), and semi-amortized VAE (sa-VAE; Kim et al. (2018)). Since ia-VAE comes with both hierarchical (h) and single-level (s) variants, we compare to each of these.\nNumber of iterations. For iP-VAE, we experimented with different numbers of training iterations, $T_{train}$. During training, we differentiate through the entire sequence of iterations, which can lead to qualitatively different dynamics. We report results for $T_{train} = 4, 16, 32, 64$. For generalization results, we use a model with $T_{train} = 64$. At test time, we report results using $T_{test} = 1,000$ iterations, unless stated otherwise. For semi-amortized models, we use their default number of train and test iterations found in their code, unless stated otherwise (sa-VAE: $T_{train} = T_{test} = 20$; ia-VAE: $T_{train} = T_{test} = 5$)."}, {"title": "4.1 Stability beyond the training regime and convergence.", "content": "An algorithm with strong generalization potential should learn how to perform inference that extends beyond the training regime. We evaluated this by training models on MNIST under different numbers of training iterations, $T_{train} = 4, 16, 32$, and 64. We used both (jacob|mlp) and (jacob|conv) architectures and then tested each model on its ability to keep improving beyond the training number of iterations. In Fig. 2a, we show that iP-VAE converges. Even with as few as 4 iterations, iP-VAE learns to keep improving. We also observe that increasing the number of training iterations has an interesting effect: iP-VAE trained with a larger number of iterations starts from worse performance, but converges to better solutions (Fig. 2a). This suggests iP-VAE learns dynamics that depend on the training sequence length, but generalizes beyond the training set in all cases.\nIn contrast, the two hybrid models (sa-VAE and ia-VAE) start with strong amortized initial guesses, but plateau rapidly (Fig. 2a; right), and converge to a much higher MSE than iP-VAE models, which have a fraction of the parameters. We also see that ia-VAE (single-level) starts to diverge outside its training regime.\nOverall, iP-VAE achieves the best reconstruction performance and continues to improve outside the training regime, unlike other models. This shows the first sign of OOD generalization in iP-VAE: temporal generalization. In later sections, we test whether iP-VAE can generalize OOD in vision tasks, but first, we evaluate the performance and sparsity on natural images, as in Vafaii et al. (2024)."}, {"title": "4.2 IP-VAE closes the gap with Sparse Coding", "content": "One of the limitations of previous work with P-VAE, was that the authors identified a large performance gap between P-VAE and LCA sparse coding (Vafaii et al., 2024). Here, we evaluated iP-VAE and compared models on their ability to reconstruct whitened natural image patches (table 1). Unlike P-VAE, iP-VAE performs as well as LCA with"}, {"title": "4.3 Out-of-distribution generalization.", "content": "In this section, we evaluate whether MNIST-trained models generalize to OOD perturbations and datasets. First, we tested whether MNIST-trained models generalize to Omniglot (see Fig. 2b). We found that iP-VAE improves over iterations and outperforms alternative models in terms of reconstruction quality. In this section, we evaluate two levels of generalization tasks: (1) within-dataset perturbations; and, (2) across similar datasets (i.e., digits to characters).\nOOD generalization to within-dataset perturbation. We tested whether models trained on standard MNIST generalized to rotated MNIST digits. We rotate MNIST between 0 and 180 degrees, with incremental steps of 15 degrees. We then test (a) whether models are capable of reconstructing the rotated digits, and (b) whether the representations of rotated digits can be used to classify them (Fig. 3). iP-VAE and sa-VAE demonstrated consistent performance across angles, both in terms of reconstruction loss and classification accuracy. Amortized P-VAE shows worse reconstruction performance than all iterative models, but its classification accuracy is remarkably consistent across angles, beating or matching all models except for iP-VAE. ia-VAE variants were greatly affected by the rotation, with"}, {"title": "4.4 A compositional code that generalizes across domains .", "content": "Using the (jacob|mlp) variant of iP-VAE, we visualized the 512 learned features of the last layer of the mlp decoder. In Fig. 5, we show the features learned by iP-VAE trained on MNIST and contrast them to features learned by P-VAE, also trained on MNIST. We see a stark contrast. iP-VAE features are Gabor-like, while P-VAE features look like digits or strokes of the digits. While previous work highlighted strokes as the compositional subcomponents of digits (Lee et al., 2007), iP-VAE learns an even more general code that generalized to cropped, grey scaled natural images (Fig. 6).\nSince both iP-VAE and P-VAE are spiking models, this result suggests that the difference lies in the inference algorithm: iP-VAE is iterative and adaptive; whereas, P-VAE is one-shot amortized."}, {"title": "5 Discussion and Conclusions", "content": "In this work, we introduced the iP-VAE, a spiking neural network that maximizes ELBO and performs Bayesian posterior updates via membrane potential dynamics. Empirically, iP-VAE demonstrates strong adaptability, robustness to OOD samples, and the ability to dynamically trade off compute and performance. iP-VAE outperforms amortized versions and recent hybrid iterative/amortized inference VAEs on every task we tested while using substantially fewer parameters.\niP-VAE results directly from the choice of Poisson in the ELBO and it avoids many of the problems with predictive coding. First, there is no population-wide prediction signal, only a feedforward receptive field and recurrent terms. Second, neurons only communicate through spikes and all dynamics are private on the membrane potential. Finally, additive terms in the membrane potential appear as gains in the spike rate, which avoids negative rates and is more consistent with real neurons (Gilbert & Li, 2013).\nThe solid theoretical foundation of iP-VAE, along with the promising empirical results, position it as a strong candidate for neuromorphic implementation. With the rise of neuromorphic hardware offering performance improvements, new algorithms are needed to leverage this architecture (Schuman et al., 2022). We found that iP-VAE with a linear decoder reduces to a spiking LCA, bridging the performance gap noted by Vafaii et al. (2024). Both algorithms share key features: sparsity, recurrence, and parameter efficiency. Since LCA has been implemented as an SNN (Zylberberg et al., 2011) and on neuromorphic hardware (Du et al., 2024), we expect the same for iP-VAE.\nIn summary, the choice of Poisson in the ELBO results in a spiking neural network, iP-VAE, that performs iterative Bayesian inference. This lays the groundwork for a prescriptive theoretical framework for building brain-like generative models that can leverage neuromorphic hardware.\nLimitations and future work. In our experiments, we tested the simplest version of iP-VAE, showing the practical benefits of the derived theory. There are a few avenues that we did not test, and we think are exciting for future work. The design of a hierarchical model is a natural extension for brain-like algorithms, especially given evidence that hierarchical VAEs are more aligned to the brain (Vafaii et al., 2023). In addition, training and evaluating on nonstationary sequences like videos would be a straightforward extension, as we derived the theory with this in mind. When attempting to use such sequences, it may also be beneficial to explore more sophisticated forward-predictive models (Fiquet & Simoncelli, 2023) that \u201cevolve\" current posteriors to future priors.\""}, {"title": "A Are real neurons truly Poisson?", "content": "In this section, we discuss empirical and theoretical observations from neuroscience that support our Poisson assumption.\n\"Poisson-like\" noise in neuroscience has a long history. It begins with observations that neurons do not fire the same sequence of spikes to repeated presentations of the same input and that the variance is proportional to the mean (Tolhurst et al., 1983; Dean, 1981) and was followed by the observation that for short counting windows, that proportion is one (Teich, 1989; Shadlen & Newsome, 1998; Averbeck et al., 2006; Rieke et al., 1999; Dayan & Abbott, 2005). Larger windows and higher visual areas are notably super-Poisson, but that can be attributed to a modulation of the rate of an inhomogeneous Poisson process (Goris et al., 2014). In other words, neurons are conditionally Poisson, not marginally Poisson (Truccolo et al., 2005).\nSpike-generation, it is argued, is not noisy (Mainen & Sejnowski, 1995; Calvin & Stevens, 1968), but synaptic noise (Allen & Stevens, 1994), or noise on the membrane potential, can create a Poisson-like distribution of spikes (Carandini, 2004). An important caveat is that the well-known example of precision in spike generation by Mainen & Sejnowski (1995) is effectively captured by a Poisson-process Generalized Linear Model (GLM; Weber & Pillow (2017)), though this precision relies on a Bernoulli approximation to a Poisson process, where only 0 or 1 spikes are possible. There is a widely-held misconception that precise timing cannot be produced by spike-rate models, but inhomogeneous rate models can operate at high time resolution and produce precise spiking patterns (Butts et al., 2016).\nIn summary, neurons are not literally Poisson, but it is a good choice. To set up the ELBO, one has to choose an approximate posterior and prior. Because spike counts are integer and cannot be negative, Poisson is a more natural choice than Gaussian without knowing anything about neural firing statistics. Here, we found that the Poisson assumption produced a prescriptive theory for neural coding. Future work might interpret this assumption at higher time resolution using inhomogeneous Poisson processes in the limit of binary spiking."}, {"title": "B Extended derivations", "content": "Recall that the input consists of a sequence, $X = {x_t}_{t=0}^T$, with a Markovian dependence between consecutive time points: $p(X) = p(x_0)p(x_1|x_0)...p(x_T|x_{T-1})$.\nIn this section, we will assume T = 1 for illustration purposes. We will derive results for this simplified case to gain intuition, as they can be easily generalized for a generic T.\nFor the case of T = 1, we introduce a pair of latent variable groups, $Z = {z_0, z_1}$, and assume the observed data, $X = {x_0,x_1}$, are sampled through the following generative process:\n$p(x_0, x_1)$\n$= \\int p(x_0, x_1, z_0, z_1) dz_0dz_1,$\n$= \\int p(x_0, z_0)p(x_1, z_1 | x_0, z_0) dz_0dz_1,$\n$= \\int p(x_0|z_0)p(z_0)p(x_1, z_1|x_0) dz_0dz_1,$\n$= \\int p(x_0|z_0)p(z_0)p(x_1|z_1, x_0)p(z_1|x_0) dz_0dz_1,$\n$= \\int p(x_0|z_0)p(z_0)p(x_1|z_1)p(z_1|x_0) dz_0dz_1,$\n$= \\int p(x_0|z_0)p(z_0) dz_0 \\int p(x_1|z_1)p(z_1|x_0) dz_1,$\n$= p(x_0) p(x_1|x_0)$.\n(9)\nWe made specific choices that led us to drop certain dependencies in two of the distributions in eq. (9). Let's make them explicit:\n1. Knowledge of stimulus at time t is sufficient to determine the future joint distribution at time t + 1:\n$p(x_t, z_t | x_{t-1}, z_{t-1}) \\rightarrow p(x_t, z_t | x_{t-1})$\n2. Current latent state, $z_t$, contains all the necessary predictive information about the incoming stimulus $x_t$:\n$p(x_t | z_t, x_{t-1}) \\leftarrow p(x_t | z_t)$"}, {"title": "B.2 Poisson KL divergence", "content": "For completeness, here we provide a closed-form derivation of the KL divergence between two Poisson distributions. Recall that the Poisson distribution for a single variable z, conditioned on rate $\\lambda \\in \\mathbb{R}_{>0}$, is given by:\n$Pois(z; \\lambda) = \\frac{\\lambda^z e^{-\\lambda}}{z!}$\n(10)\nSuppose $p = Pois(z; r)$ is the prior distribution, and $q = Pois(z; r \\odot \\delta r)$ is the approximate posterior, and $\\odot$ is the element-wise (Hadamard) product. Both prior rates and the posterior rate modulator are positive real-valued vectors of length K, where K is the latent dimensionality. That is, $r \\in \\mathbb{R}^K_+$ and $\\delta r \\in \\mathbb{R}^K_{>0}$.\nFor K neurons, the prior and approximate posterior distributions can be written as a product of K independent Poisson distributions of the form given in eq. (10):\n$p = Pois(z; r) = \\prod_{k=1}^K \\frac{r_k^{z_k}e^{-r_k}}{z_k!},$\n$q = Pois(z; r \\odot \\delta r) = \\prod_{k=1}^K \\frac{(\\delta r_k)^z_k e^{-\\lambda_k}}{z_k!},$\n(11)\nwhere we have defined $\\lambda := r \\odot \\delta r$ for convenience.\nPlug these expressions into the KL divergence definition to get:"}, {"title": "B.4 Dynamics", "content": "In this section, we will go through the derivation of the dynamics of iP-VAE (eq. (7) in the main paper). Our goal is to define membrane potential updates in a way that the resulting dynamics will minimize the ELBO loss.\nWe begin with the general definition of the ELBO, $E_{q_{\\phi}(z|x)} [\\log \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z|x)}]$, and consider its Monte Carlo estimate using a single sample, z, drawn from the approximate posterior $q_{\\phi}(z|x)$:\n$l(x, z) := \\log \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z|x)}$\n$= \\log \\frac{p_{\\theta}(x|z)p_{\\theta}(z)}{q_{\\phi}(z|x)}$\n$= \\log p_{\\theta} (x|z) + \\log \\frac{p_{\\theta}(z)}{q_{\\phi}(z|x)}$\n$= -MSE(x, z) + r \\odot (exp(\\delta u) \u2013 1) \u2013 z \\odot \\delta u.$\n(19)\nIn the last line of eq. (19), we inserted our specific choice of Gaussian conditional density, resulting in $\\log p_{\\theta} (x|z) = -MSE(x, z) = - ||x - f_{\\theta}(z) ||^2$. We also expressed the log ratio between the prior and approximate posterior distributions, both modeled as Poisson, as in the case in iP-VAE.\nNext, we take the partial derivative of $l(x, z)$ w.r.t. the samples z and keep only the first-order terms. This results in:\n$\\frac{\\partial}{\\partial z} -l(x, z) \\approx - \\frac{\\partial}{\\partial z} MSE(x, z) \u2013 \\delta u.$\n(20)\nWe define our posterior updates, $\\delta u$, to be proportional to the gradient of $l(x, z)$ w.r.t. the state variable, u. Since $l(x, z)$ does not explicitly depend on u, we compute the gradient through the chain rule:"}]}