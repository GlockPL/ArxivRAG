{"title": "EXPLAIN, AGREE, LEARN: Scaling Learning for Neural Probabilistic Logic", "authors": ["Victor Verreet", "Lennert De Smet", "Luc De Raedt", "Emanuele Sansone"], "abstract": "Neural probabilistic logic systems follow the neuro-symbolic (NeSy) paradigm by combining the perceptive and learning capabilities of neural networks with the robustness of probabilistic logic. Learning corresponds to likelihood optimization of the neural networks. However, to obtain the likelihood exactly, expensive probabilistic logic inference is required. To scale learning to more complex systems, we therefore propose to instead optimize a sampling based objective. We prove that the objective has a bounded error with respect to the likelihood, which vanishes when increasing the sample count. Furthermore, the error vanishes faster by exploiting a new concept of sample diversity. We then develop the EXPLAIN, AGREE, LEARN (EXAL) method that uses this objective. EXPLAIN samples explanations for the data. AGREE reweighs each explanation in concordance with the neural component. LEARN uses the reweighed explanations as a signal for learning. In contrast to previous NeSy methods, EXAL can scale to larger problem sizes while retaining theoretical guarantees on the error. Experimentally, our theoretical claims are verified and EXAL outperforms recent NeSy methods when scaling up the MNIST addition and Warcraft pathfinding problems.", "sections": [{"title": "1 Introduction", "content": "The field of neuro-symbolic (NeSy) artificial intelligence (AI) aims to combine the perceptive capabilities of neural networks with the reasoning capabilities of symbolic systems. Many prominent NeSy systems achieve such a combination by attaching a probabilistic logic component to the neural output [26, 46]. As a result, they tend to generalize better, can deal with uncertainty and require less training data compared to pure neural networks. The main challenge that prevents the widespread adoption of NeSy is the difficulty to learn, because the learning signal for the neural network has to be propagated through the probabilistic logic component.\nExisting work has tackled propagating the learning signal in a plethora of ways. One line of work has tackled this challenge by relying on exact propagation using knowledge compilation. This scales poorly for more complex systems because inference in probabilistic logic is #P-complete [12]. Other lines of work propose different approximation schemes for the propagation. A couple of approaches only consider the k-best solutions that satisfy the probabilistic logic component [27, 21], but they introduce biases that can reinforce the (possibly incorrect) beliefs of the neural network. More recently, A-NeSI [40] proposed to use neural networks to provide a differentiable approximation of the logic component. A-NeSI can provide some semantic guarantees, but it has to rely on extensive optimization and hyperparameter tuning to reduce the bias of its approximation. While it still outperforms other methods in terms of scalability, the long training times of A-NeSI still limit its application to more complex problems. None of the above methods provide bounds on the error of approximation.\nTo address these issues, this research focuses on scaling learning of the neural component in NeSy systems with a probabilistic logic component [26, 43, 40] while providing strong statistical guarantees. Concretely, we propose a surrogate objective to approximate the data likelihood, and prove it has some desirable properties that k-best or A-NeSI lacks. First, it is an unbiased approximation of the likelihood. Second, its approximation error is theoretically bounded. Third, we show how this error can be decreased using a newly introduced concept of diversity. Learning the neural component with this approximate objective sidesteps the requirement of differentiability for the probabilistic logic component that other methods focus on.\nWe also introduce the EXPLAIN, AGREE, LEARN (EXAL) method for constructing the surrogate objective. First, EXPLAIN samples explanations for the data at the level of the neural output, propagating the learning signal through the probabilistic logic component. We can control the resource scaling by choosing the number of samples. Then AGREE assigns an importance to each sampled explanation based on the predictions of the neural component. Together, EXPLAIN and AGREE construct the surrogate objective. LEARN then uses the surrogate objective to perform a classical learning iteration for the neural component with direct supervision on the neural output.\nExperimentally, we validate our theoretical claims on synthetic data and show that all three steps in EXAL are necessary to achieve good performance. Moreover, we apply EXAL to two prominent NeSy problems, MNIST addition and Warcraft pathfinding and show that EXAL outperforms other NeSy methods such as A-NeSI [40] in terms of execution time and accuracy for some instances when scaling to larger problem sizes."}, {"title": "2 Background", "content": "NeSy systems consist of both a neural and a symbolic component. In the case of neural probabilistic logic, the neural component handles perception of the world by mapping raw input $x \\in \\mathbb{R}^d$ to a probability distribution over $n$ binary variables $f \\in \\{0, 1\\}^n$. The symbolic component then performs logical reasoning over these binary variables to verify whether a formula $\\phi$ is satisfied. More specifically,"}, {"title": "3 The Objective", "content": "We now consider optimizing a NeSy-WMC model, consisting of a neural and logic component. We leave the neural component unspecified and only require it to be learnable via gradient descent. The data set contains $I$ tuples $D = \\{(x_i, \\phi_i) | i = 1, ..., I\\}$, where $x_i$ is a raw input and $\\phi_i$ is a logical formula that is assumed to hold for\nthat data point and can be seen as its label. For example, if a self-driving car does not brake given image $x_i$ (see Ex. 1), then the label is $\\phi_i = \\neg f_1 \\wedge \\neg f_2 \\wedge (f_3 \\vee \\neg f_4)$, indicating there is no pedestrian, no red light and either there is no crosswalk or the car is driving slow enough. We will often drop the index $i$ of the data point.\nOur setup is modelled by the probabilistic graphical model in Fig. 2. It contains the input $x \\in \\mathbb{R}^d$, the $n$ binary variables $f \\in \\{0, 1\\}^n$ and whether $\\phi$ is satisfied. The logic component is deterministic $P(\\phi | f) = 1$ if $f$ satisfies $\\phi$ and zero otherwise. The neural component outputs a distribution $P(f | x)$, which are the weights of the NeSy-WMC problem. The answer $W$ of the NeSy-WMC problem is the probability $P(\\phi | x)$. The neural component is usually learned via likelihood maximization on the data set $D$. However, calculating the likelihood requires expensive inference, which is intractable for complex systems. We therefore propose the following surrogate objective $L$ that bounds the true likelihood.\nFor a data point $(x_i, \\phi_i)$ let $Q_i$ be an auxiliary distribution over the explanations $\\phi_i$ of the label $i$ and let $P_i$ be the distribution over the explanations that the neural component outputs given $x_i$. We then define the surrogate objective $L = \\sum_i KL(Q_i | P_i)$.\nIntuitively, the auxiliary distributions $Q_i$ propose explanations for the label $\\phi_i$, from which the neural component $P_i$ can learn (Fig. 2). We now show the bound on the negative log-likelihood loss. The full derivation of this bound can be found in the appendix.\n$-\\log P(D) \\leq -\\sum_i \\sum_{f \\in \\phi_i} Q_i(f) \\log \\frac{P(f | x_i)}{Q_i(f)}$\n$=\\sum_i KL(Q | P) = L$    (1)\nThe tightest bound in Eq. 1 is obtained by minimizing the Kullback-Leibler (KL) divergence with respect to both $P_i$ and $Q_i$. The learning"}, {"title": "4 EXPLAIN, AGREE, LEARN", "content": "The EXPLAIN, AGREE, LEARN method aims to minimize the objective $L$ from Def. 4. This objective contains a KL divergence between the distributions $P_i$ and $Q_i$, raising a number of issues. The first issue is that each $Q_i$ is supported on all explanations of the formula $\\phi_i$, which are unknown a priori and infeasible to obtain for complex problems. Hence, we introduce the EXPLAIN algorithm that samples explanations of $\\phi_i$ used to approximate the support of $Q_i$. After sampling explanations, the samples are reweighted by the neural component $P_i$ in the AGREE step such that samples deemed more likely by $P_i$ have a higher weight. Alternatively, the AGREE step can be interpreted as a minimization of $L$ with respect to all $Q$. The EXPLAIN and AGREE steps both serve to construct a suitable proposal distribution $Q_i$ for the objective $L$. $L$ is then used to optimize each $P_i$ in the LEARN step, which performs a traditional update step using backpropagation."}, {"title": "4.1 EXPLAIN: Sampling Explanations", "content": "The first step to minimizing $L$ is to construct the support of the proposal distributions $Q$. As the support of each $Q$ consists of explanations of the logical formula $\\phi$, we propose the EXPLAIN algorithm to sample such explanations. EXPLAIN is a stochastic variant of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, for which we first introduce some necessary concepts.\nDefinition 5. A sampling strategy $\\Sigma(\\mu)$ for an assignment $\\mu$ is a probability distribution over the set of all assignments obtained from $\\mu$ by assigning one more variable a value.\nEXPLAIN is shown in Alg. 1 and differs from traditional DPLL on lines 10 and 11. Specifically, instead of iterating over all possible assignments, a new assignment $\\mu'$ is sampled according to $\\Sigma(\\mu)$. EXPLAIN starts with the formula $\\phi$, assumed to be in conjunctive normal form, the empty assignment $\\mu_?$, defined as $\\forall k : \\mu_?(k) = ?$, and a uniform sampling strategy $\\Sigma(\\mu)$. First, trivial variable assignments, e.g. unit clauses, are propagated to $\\mu$. The algorithm then terminates if $\\phi$ is satisfied or restarts if $\\phi$ is unsatisfiable. Otherwise, a value for one more variable is sampled using $\\Sigma(\\mu)$ and EXPLAIN is recursively called. Notice how the distribution of returned explanations is entirely determined by the sampling strategy $\\Sigma(\\mu)$.\nDuring the execution of EXPLAIN it is possible to encounter conflicting variable assignments. Specifically, a conflict occurs when the query $\\phi$ is unsatisfiable by $\\mu$. The execution of EXPLAIN is stochastic because of the sampling strategy $\\Sigma$, so it is possible that some sampled assignments lead to a conflict whereas others do not. There are several policies for dealing with conflicts. The easiest policy is to simply stop execution and restart EXPLAIN from the top. Another policy is to backtrack and sample a different new variable assignment $\\mu'$. Furthermore, a limit on the amount of backtracking can be"}, {"title": "4.2 AGREE: Updating the Weights", "content": "We can choose any auxiliary distribution $Q$ over explanations of $\\phi$ in Def. 4 of $L$ and will choose $Q$ to minimize $L$. The explanations sampled by the EXPLAIN algorithm define the support of $Q$ and varying $Q$ amounts to reweighing every sample. It turns out that the objective $L$ is minimized with respect to these weights $Q$ when they are proportional to the output of the neural component $P(f | x)$. Intuitively, explanations that the neural component deems more likely, should be assigned a higher weight to minimize $L$. We now formalize this result and provide a bound on the error of the approximation.\nLet $\\Psi$ be the explanations sampled by EXPLAIN and define $Q^*(f) = \\frac{P(f | x)}{\\sum_{f' \\in \\Psi} P(f' | x)}$, then:\n1. Optimality. $Q^*$ globally minimizes $L$ with respect to $Q$.\n2. Optimum. The minimum of $L$ is given by $\u2212 \\sum_i \\log P^*(\\phi_i | x_i)$, with $P^*(\\phi_i | x_i) = \\sum_{f \\in \\Psi_i} P(f | x_i)$.\n3. Bounds. For any $x$ and $\\phi$, the true probability is bounded by $P^*(\\phi | x) < P(\\phi | x) < 1 - P^*(\\neg\\phi | x)$.\nProof. Optimality. Let us write out one term of $L$, dropping the data point index $i$, yielding\n$KL(Q | P) = - \\sum_{f \\in \\Psi} Q(f) \\log \\frac{P(f | x)}{Q(f)}$  (2)\nNext, we use the method of Lagrange multipliers to impose the constraint $\\sum_{f \\in \\Psi} Q(f) = 1$ while minimizing this KL divergence with respect to all the $Q(f)$. This method leads to the equations\n$\\frac{\\partial}{\\partial Q(f)} (- \\log \\frac{P(f | x)}{Q(f)} + \\lambda (\\sum_{f' \\in \\Psi} Q(f') - 1)) + 1 = 0$,    (3)\nand thus $Q(f) = \\exp(\\lambda+1) P(f | x)$. From the constraint we solve $\\lambda = -1 - \\log \\sum_{f \\in \\Psi} P(f | x)$ and filling this into the expression for $Q(f)$ gives the optimum at $Q^*(f) = \\frac{P(f | x)}{\\sum_{f' \\in \\Psi} P(f' | x)}$.\nOptimum. Filling in the optimal value $Q^*(f)$ in Eq. (2) gives\n$KL(Q | P) = - \\log \\sum_{f \\in \\Psi} P(f | x) = - \\log P^*(\\phi | x)$    (4)"}, {"title": "4.3 LEARN: Updating the Neural Component", "content": "The LEARN step minimizes $L$ with respect to the parameters of the neural component $P$ using standard gradient descent techniques. When $|\\Psi| = 1$, we observe that the loss $L$ is equivalent to the traditional cross-entropy loss. We can interpret the LEARN step as a standard supervised learning step, where the labels are provided by the EXPLAIN and AGREE steps. The full EXAL algorithm is shown in Alg. 2."}, {"title": "5 Optimizations using Diversity", "content": "The following section provides an optimization for EXAL using the concept of diversity. The formula $\\phi$ of a data point can have many explanations and the true underlying explanation is not known. Consequently, sampling a diverse set of explanations with EXPLAIN can increase the chance of seeing the true explanation. For example, a self-driving car will more often brake because for a pedestrian than an animal on the road. An animal should however not be dismissed as a possible explanation for braking and could still be the true explanation for a certain data point. We formalise this intuition through\nthe notion of diversity of explanations. Additionally, we provide two practical methods to increase the diversity of the samples obtained from the EXPLAIN algorithm."}, {"title": "5.1 Diversity of Explanations", "content": "We formally define diversity and prove in Prop. 1 that sampling a more diverse set of explanations leads to a tighter bound $L$ on the negative log-likelihood (see Eq. 1).\nDefinition 6. The diversity of a set $\\Psi$ of assignments with respect to $\\phi$ is $\\delta(\\Psi) = |\\Psi \\cap \\Phi|$, where $\\Phi$ are the explanations of $\\phi$.\nIf $\\Psi_1 \\subset \\Psi_2$ then $\\delta(\\Psi_1) < \\delta(\\Psi_2)$ and $L_2 \\leq L_1$ where $L_1$ and $L_2$ are the objectives constructed from Equation (2) using $\\Psi_1$ and $\\Psi_2$, respectively (see optimum in Thm. 1).\nProof. If $\\Psi_1 \\subset \\Psi_2$ then $\\sum_{f \\in \\Psi_1} P(f | x) \\leq \\sum_{f \\in \\Psi_2} P(f | x)$. Taking the negative logarithm of both sides gives the result\n$L_2 = - \\log \\sum_{f \\in \\Psi_2} P(f | x) \\leq - \\log \\sum_{f \\in \\Psi_1} P(f | x) = L_1$  (6)"}, {"title": "5.2 Reformulation as a Markov Decision Process", "content": "A recent result [5] of generative flow networks (GFlowNets) for Markov decision processes (MDPs) also allows us to increase the diversity of sampled explanations through optimisation of $\\theta$. Their results include a method to maximize the expected reward when executing an MDP with certain properties. If we set the reward in the MDP equal to the diversity, their method can be used to increase the diversity. To apply the result, running EXPLAIN $T$ times for a query $\\phi$ to obtain a set of $T$ samples first has to be framed as an MDP. Recall that $A(n) = \\{0, 1, ?\\}^n$ is the set of all assignments to $n$ binary variables. Therefore, the MDP has states $(\\mu, t, \\Psi) \\in A(n) \\times \\mathbb{N} \\times 2^{A(n)}$, where $\\mu$ is the current assignment in the $t$th run of EXPLAIN and $\\Psi$ contains the assignments returned by previous runs of EXPLAIN. The initial state is $(\\mu_?, 0, \\{\\})$. For an intermediate state $(\\mu, t, \\Psi)$ with partial $\\mu$, the possible actions are to assign a value to an unassigned variable of $\\mu$, leading to a new state $(\\mu', t, \\Psi)$. If $\\mu$ is complete, the only action is to go to the next run of EXPLAIN, transitioning to the state $(\\mu_?, t+1, \\Psi \\cup \\{\\mu\\})$. The terminal states are $(\\mu_?, T, \\Psi)$ which have a reward $R(\\mu_?, T, \\Psi) = \\delta(\\Psi)$. All other states have no reward.\nProposition 2. The transition graph of the above MDP is acyclic and the MDP will always terminate."}, {"title": "6 Experiments", "content": "Experiments are provided to support our theoretical claims. We show that we can learn to generate diverse samples using the EXPLAIN algorithm. Then the importance of diversity and the AGREE step are illustrated by looking at the convergence of the learning objective. Lastly, we apply the EXAL method to the MNIST addition and Warcraft pathfinding tasks. Our implementation can be found here (https://anonymous.4open.science/r/exal-526C) and more details about the experiments are in the appendix."}, {"title": "6.1 Diversity (EXPLAIN)", "content": "For the first experiment, we want to investigate how different sampling strategies compare in terms of diversity. To this aim, we run EXPLAIN on formulas generated in 3 different classes and with a varied number of variables, ranging between 15 and 60. Generation details for each class branch, split and bottomup are in the appendix. For every generated formula, we evaluate the diversity of different sampling strategies so they can be directly compared. This experiment is concerned only with logical explanations and does not use neural networks. We count how many unique explanations are sampled during subsequent calls to EXPLAIN, i.e. the diversity. The counts at each time step are averaged out over 200 runs on the same formula. This is done for fixed values of $\\theta$, as well as for parameters $\\theta$ that are learned using the diversity algorithm in Sec. 5.2. As baselines, a uniform sampler and the theoretical maximally diverse sampler are considered.\nFig. 3 shows the evolution of the diversity for each strategy. The diversity has been normalized to the unit interval for easy comparison. Although the EXPLAIN algorithm is not guaranteed to sample uniformly, in many instances it is close to uniform. This can be seen by the overlapping of the uniform line and the line with $\\theta = 0$. For the bottomup formulas however, EXPLAIN is not uniform and performs worse for $\\theta = 0$. The diversity of EXPLAIN is typically increased by choosing a larger $\\theta$ and can get close to the optimal diversity. In some instances, such as in the split formulas, having $\\theta > 0$ increases diversity, but there is little difference in diversity for the different values of $\\theta$. We also see that learning to optimize the diversity performs better than sampling uniformly. It is recommended to choose a higher $\\theta$ for improving the diversity, but stability issues can occur for too high values of $\\theta$."}, {"title": "6.2 Convergence of Bounds (AGREE)", "content": "We now experimentally answer what the importance of diversity and of the AGREE step are. For this purpose we compare the surrogate objective with the true likelihood for different number of samples. More concretely, explanations are sampled for the formula $\\phi$ and for its negation, using both the diverse and non-diverse versions of EXPLAIN. Then the objectives with and without the AGREE step are calculated. The AGREE step makes use of the probabilities given by the neural network. We perform this synthetic experiment without neural networks by generating the probabilities uniformly at random on the unit interval. The results for all four combinations are shown in Figure 4. The samples for the formula give a lower bound, whereas samples for its negation give an upper bound. The AGREE step is necessary for the objectives to converge to the true likelihood. Furthermore, convergence happens faster when the samples are more diverse, confirming our theoretical results that diversity improves the error bounds."}, {"title": "6.3 MNIST Addition (EXAL)", "content": "We evaluate our approach on the standard NeSy experiment of learning to sum MNIST digits [26, 27]. The input to this task is two sequences of $N$ MNIST images, where each sequence represents a decimal number of length $N$. The desired output is the sum of these two numbers, which is also the only supervision. The sum is encoded in a logical formula. For example, if two single-digit numbers $a$ and $b$ add up to 3, the formula is $\\phi = (a = 0 \\wedge b = 3) \\vee (a = 1 \\wedge b = 2) \\vee (a = 2 \\wedge b = 1) \\vee (a = 3 \\wedge b = 0)$. The size of the possible"}, {"title": "6.4 Warcraft Pathfinding (EXAL)", "content": "The task of Warcraft pathfinding, as described in [41], is to find the shortest path between two corner points of a two-dimensional grid given an image representing the grid. The exact traversal costs of each node are not known and have to be predicted from the image by the neural component. The only supervision for training the neural component is the true shortest path in the grid.\nIn this context, the symbolic component is a shortest path finding algorithm, e.g. Dijkstra's algorithm, and an explanation is a cost assignment to every node in the grid such that the shortest path given by those costs coincides with the true shortest path. Sampling explanations is done as follows. First the grid is initialized with all nodes set to the highest cost, except for nodes on the true shortest path, which are set to the lowest cost. This guarantees that the given shortest path coincides with the true shortest path. Then a Gibbs sampling procedure is executed that resamples node costs with the restriction that the shortest path is unchanged. After a burn-in period of 100 resampling steps, the resampled node costs are used as a training signal for the neural component.\nOnce the neural component has been trained to predict node costs, the NeSy system can predict the shortest path given an image of a grid. A prediction is only considered correct if the predicted shortest path overlaps entirely with the true shortest path.\nTwo main conclusions can be drawn from the reported test accuracies in Table 2. First, the same trend from the MNIST experiment continues, namely that EXAL provides orders of magnitude faster learning times compared to the state-of-the-art. Second, this quick convergence time does not impact the acquired accuracies. Even more, for the most challenging 30 \u00d7 30 grid, EXAL significantly outperforms A-NeSI. Together, they provide strong evidence that using explanations improves scaling of NeSy learning."}, {"title": "7 Related Work", "content": "We review recent works on neurosymbolic (NeSy) learning and scalable logical inference. For a more detailed discussion on NeSy, we refer the reader to recent surveys in [7, 19].\nSeveral NeSy systems address the scalability of inference with continuous relaxations based on fuzzy logic semantics [3, 18, 11, 25, 17, 34, 14]. However, these relaxations may introduce approximations that can yield different outputs for equivalent logic formulas [39, 20]. In contrast, our work performs inference and learning without resorting to such relaxations while retaining a probabilistic interpretation. Similar to us, the work in [42] provides a NeSy solution based on SMT solvers, which does not require backpropagation through the symbolic component. The solution approximates the gradient by discretizing the neural representation and solving a combinatorial optimization problem. In contrast, our work does not require to approximate the gradient as training proceeds in a supervised fashion thanks to the sampled explanations. Additionally, we avoid discretizing the neural representation as we leverage neural predicates, essential for neatly integrating probability and logic. Lastly, our aim here is different as the paradigm is designed to scale probabilistic NeSy and to quantify the quality-speed tradeoff.\nNeural inference strategies. The work from [10] proposes a neurosymbolic pipeline consisting of a symbolic engine and three neural modules. Specifically, a perception network mapping the images to their symbolic representations, a neural solver attempting to correct the symbolic representation, and a mask predictor to identify possible mistakes done by the neural solver. The symbolic solver then corrects the mistakes on the neural predictions. The neural modules are pre-trained under supervision and fine-tuned using reinforcement learning. In contrast, our simplified framework is based on a scalable logic engine and single neural component. Learning is performed by directly supervising the neural component using the sampled explanations using variational inference strategies. The work in [40] introduces two neural modules for neural-symbolic learning: a perception component mapping input data to the probabilities of facts, and a neural reasoner component mapping probabilities to the query. Learning involves training the neural reasoner to mimic synthetic input/output pairs obtained through logical inference, and then training the perception component in a supervised manner using the frozen neural reasoner. In contrast, our work only requires a perception component and utilizes a sampling algorithm that guarantees logically consistent solutions while ensuring diversity. The Neural Theorem Prover [29, 30] introduces a continuous and differentiable relaxation of the backward-chaining logic reasoning algorithm. In contrast, our approach does not rely on relaxations or differentiability through the logic program. Other neural sampling strategies, such as GFlowNets [6, 5, 47], treat sampling as a sequential decision-making"}, {"title": "8 Discussion and Conclusion", "content": "The EXAL method is designed as a scalable solution to learning for neural probabilistic logic. The key idea is to use explanation sampling to propagate the learning signal through the symbolic component. This allows the neural component to be trained fast compared to exact approaches. We provide theoretical guarantees on the approximation error and provide practical methods to reduce this error, in particular by encouraging diversity. Experimentally, EXAL is shown to be competitive with state-of-the-art NeSy methods on larger problems in terms of accuracy while significantly outperforming them in terms of speed.\nDespite these benefits, sampling explanations is NP-hard and can be slow for problems with a large explanation space. Exploring the entire space can be expensive, but is often unnecessary in practice. A good training signal can be obtained from a sampled subset of explanations. For example, in the Warcraft pathfinding problem it is unlikely to sample the true grid, but sampling similar grids suffices to properly train the network. Whether or not it is necessary to explore the entire explanation space is of course problem dependent. In the worst case where the explanation space is large and only one explanation can provide a good training signal, EXAL will also perform poorly. Furthermore, if satisfiability is difficult to check, sampling explanations for a formula is also difficult, as the existence of an explanation implies satisfiability. NeSy methods, including EXAL, can also suffer from reasoning shortcuts, an issue identified experimentally in [26]. It will be interesting to address this general NeSy issue in future work. Furthermore, this work focuses on scaling learning with a fixed logical formula, but scalability for structure learning is also an open problem. Lastly, EXAL supports continuous inputs x, but does not support continuous variables in the symbolic representation f. Extending EXAL with continuous f is another line of research that would lead into the field of satisfiability modulo theories and weighted model integration [31].\nAlthough this work focused on NeSy-WMC for the symbolic component, EXAL can be used with any symbolic component for which a suitable EXPLAIN algorithm can be devised. This makes the method flexible to be applied to other use cases in the future."}, {"title": "Appendix: Proof of Loss Bounds", "content": "We provide the full proof of Eq. 1 here:\n$\\begin{aligned} - \\log P(D) &= - \\log \\left( \\prod_i P(\\phi_i | X_i) \\right) \\\\ &= - \\sum_i \\log P(\\phi_i | X_i) \\\\ &= - \\sum_i \\log \\sum_f P(\\phi_i | f) P(f | x_i) \\\\ &= - \\sum_i \\log \\sum_{f \\in \\phi_i} P(f | x_i) \\\\ &= - \\sum_i \\log \\sum_{f \\in \\phi_i} Q_i(f) \\frac{P(f | x_i)}{Q_i(f)} \\\\ &\\leq - \\sum_i \\sum_{f \\in \\phi_i} Q_i(f) \\log \\frac{P(f | x_i)}{Q_i(f)} \\\\ &= \\sum_i KL(Q | P) = L \\end{aligned}$     (9)\nRecall that $P(\\phi_i | f)$ is an indicator that returns 1 if and only if $f$ satisfies $\\phi_i$, limiting the sum to only explanations $f \\in \\Phi_i$. The bound is obtained using Jensen's inequality, which states that $\\log(E_Q[X]) \\geq E_Q[\\log(X)]$. In this case $X = P(f | x_i)/Q_i(f)$."}, {"title": "Appendix: Experimental Details", "content": "Diversity\nThe diversity experiment has been performed on 9 formulas, of which 4 are shown in Figure 3. These are 3 branch, 3 bottom-up and 3 split formulas. Branch formulas contain implications of the form $f_0 \\leftarrow f_1 \\vee ... \\vee f_b$ with b the branching factor. It also contains one clause with b variables in disjunction. The branching factors are 3, 3 and 10 and the formulas have depth of 3, 5 and 3 respectively. The bottom-up formulas have as parameters the fraction of starting variables, which is always set to 0.5, the number of inferred variables, respectively 20, 60 and 60, and the in-degree, respectively 3, 3 and 4. These formulas recursively define new variables in terms of previously existing variables, e.g. $f_4 \\leftarrow f_1 \\wedge (f_2 \\vee f_3)$. The split formulas alternate between layers of conjunctions and disjunctions. All 3 programs have a depth of 4 and a conjunction size of 3 whereas the disjunction size varies from 2 to 4 inclusive.\nDataset. Generating the data for the MNIST addition experiment [26] on two sequences of $N$ digits is a straightforward process. It involves randomly selecting 2N images from the MNIST dataset and concatenating them to create two distinct sequences, each with a length of N. To supervise these sequences, we easily obtain the desired values by multiplying the labels of the selected MNIST images by the appropriate power of 10. We then sum the resulting sequence of values for each number and further sum the two resulting numbers. It is important to note that each MNIST image is only allowed to appear once in the sequences. Hence, the dataset consists of [60000/2N] sequences available for learning. The test set follows a similar procedure, using the test set partition of the MNIST dataset.\nModelling. In this experiment, a traditional LeNet [24] neural network is utilized. The network architecture consists of two convolutional layers with 6 and 16 filters of size 5, employing ReLU activations. These layers are followed by a flattening operation. Subsequently, three dense layers with sizes of 120, 84, and 10 are employed. The first two dense layers also utilize ReLU activations, while the final layer applies a softmax activation. The network outputs the probabilities indicating the likelihood that each image in the two sequences corresponds to a specific digit.\nFor this experiment, we adopted the standard Adam optimizer with a learning rate of $\\eta = 10^{-3}$, known for its reliable performance. Other critical hyperparameters include the number of samples drawn by EXAL and the number of epochs for training. In all cases, 600 samples were used, the same number as A-NeSI. The maximum number of epochs available for training was set to 10, 20 and 100 for $N = 2$, $N = 4$ and $N = 15$, respectively. Reproduction of the A-NeSI results for MNIST was done using the optimised hyperparameters as reported by van Krieken et al. [40]."}]}