{"title": "EXPLAIN, AGREE, LEARN:\nScaling Learning for Neural Probabilistic Logic", "authors": ["Victor Verreeta", "Lennert De Smet", "Luc De Raedt", "Emanuele Sansone"], "abstract": "Neural probabilistic logic systems follow the neuro-symbolic (NeSy) paradigm by combining the perceptive and learning capabilities of neural networks with the robustness of probabilistic logic. Learning corresponds to likelihood optimization of the neural networks. However, to obtain the likelihood exactly, expensive probabilistic logic inference is required. To scale learning to more complex systems, we therefore propose to instead optimize a sampling based objective. We prove that the objective has a bounded error with respect to the likelihood, which vanishes when increasing the sample count. Furthermore, the error vanishes faster by exploiting a new concept of sample diversity. We then develop the EXPLAIN, AGREE, LEARN (EXAL) method that uses this objective. EXPLAIN samples explanations for the data. AGREE reweighs each explanation in accordance with the neural component. LEARN uses the reweighed explanations as a signal for learning. In contrast to previous NeSy methods, EXAL can scale to larger problem sizes while retaining theoretical guarantees on the error. Experimentally, our theoretical claims are verified and EXAL outperforms recent NeSy methods when scaling up the MNIST addition and Warcraft pathfinding problems.", "sections": [{"title": "1 Introduction", "content": "The field of neuro-symbolic (NeSy) artificial intelligence (AI) aims to combine the perceptive capabilities of neural networks with the reasoning capabilities of symbolic systems. Many prominent NeSy systems achieve such a combination by attaching a probabilistic logic component to the neural output [26, 46]. As a result, they tend to generalize better, can deal with uncertainty and require less training data compared to pure neural networks. The main challenge that prevents the widespread adoption of NeSy is the difficulty to learn, because the learning signal for the neural network has to be propagated through the probabilistic logic component.\nExisting work has tackled propagating the learning signal in a plethora of ways. One line of work has tackled this challenge by relying on exact propagation using knowledge compilation. This scales poorly for more complex systems because inference in probabilistic logic is #P-complete [12]. Other lines of work propose different approximation schemes for the propagation. A couple of approaches only consider the k-best solutions that satisfy the probabilistic logic component [27, 21], but they introduce biases that can reinforce the (possibly incorrect) beliefs of the neural network. More recently, A-NeSI [40] proposed to use neural networks to provide a differentiable approximation of the logic component. A-NeSI can provide some semantic guarantees, but it has to rely on extensive optimization and hyperparameter tuning to reduce the bias of its approximation. While it still outperforms other methods in terms of scalability, the long training times of A-NeSI still limit its application to more complex problems. None of the above methods provide bounds on the error of approximation.\nTo address these issues, this research focuses on scaling learning of the neural component in NeSy systems with a probabilistic logic component [26, 43, 40] while providing strong statistical guarantees. Concretely, we propose a surrogate objective to approximate the data likelihood, and prove it has some desirable properties that k-best or A-NeSI lacks. First, it is an unbiased approximation of the likelihood. Second, its approximation error is theoretically bounded. Third, we show how this error can be decreased using a newly introduced concept of diversity. Learning the neural component with this approximate objective sidesteps the requirement of differentiability for the probabilistic logic component that other methods focus on.\nWe also introduce the EXPLAIN, AGREE, LEARN (EXAL) method for constructing the surrogate objective. First, EXPLAIN samples explanations for the data at the level of the neural output, propagating the learning signal through the probabilistic logic component. We can control the resource scaling by choosing the number of samples. Then AGREE assigns an importance to each sampled explanation based on the predictions of the neural component. Together, EXPLAIN and AGREE construct the surrogate objective. LEARN then uses the surrogate objective to perform a classical learning iteration for the neural component with direct supervision on the neural output.\nExperimentally, we validate our theoretical claims on synthetic data and show that all three steps in EXAL are necessary to achieve good performance. Moreover, we apply EXAL to two prominent NeSy problems, MNIST addition and Warcraft pathfinding and show that EXAL outperforms other NeSy methods such as A-NeSI [40] in terms of execution time and accuracy for some instances when scaling to larger problem sizes."}, {"title": "2 Background", "content": "NeSy systems consist of both a neural and a symbolic component. In the case of neural probabilistic logic, the neural component handles perception of the world by mapping raw input x \u2208 R^d to a probability distribution over n binary variables f \u2208 {0, 1}^n. The symbolic component then performs logical reasoning over these binary variables to verify whether a formula \u03a6 is satisfied. More specifically, we choose weighted model counting (WMC) for the symbolic component, because probabilistic inference can be reduced to WMC [8].\nDefinition 1. Let A(n) = {0, 1, ?}^n. We call \u03bc\u2208 A(n) an assignment of n binary variables. If \u03bc(k) = ? for some k then variable k is not assigned a value and we call \u03bc partial. Otherwise \u03bc is complete.\nDefinition 2. An explanation of a formula \u03a6 is a complete assignment \u03bc that satisfies \u03a6. Like [33], we use the term explanation, but it is also called a model. The set \u03a8 contains all explanations of \u03a6.\nDefinition 3. A WMC problem over n binary variables f \u2208 {0,1}^n is defined by a propositional logical formula \u03a6, together with weight assignments w_k : {0,1} \u2192 R^+ for each variable. The answer W to the WMC problem is W = \u03a3_{f\u2208\u03a6} \u03a0_{k} w_k(f_k). A NeSy-WMC problem is a WMC problem where the weights are computed by a neural network.\nExample 1. The camera of a self-driving car provides an image with d pixels as raw input. From this image, the neural component outputs n = 4 probabilities for detecting f_1 (pedestrian), f_2 (red light), f_3 (driving slow) and f_4 (crosswalk). The logical component decides the car should brake if there is a pedestrian or a red light or if it is driving too fast near a crosswalk. This is encoded in \u03a6 = f_1 \u2228 f_2 \u2228 (f_3 \u2227 f_4). The weights of this NeSy-WMC problem are given by the neural component w_k(f_k) = \u03b7_k(x)^{f_k} (1 \u2013 \u03b7_k(x))^{1-f_k}, where for example \u03b7_1(x) is the probability that there is a pedestrian in the input image x according to the neural network. The answer W to the NeSy-WMC problem is the probability that the car should brake given the input image. This is illustrated in Fig. 1. We will use this as a running example throughout this paper."}, {"title": "3 The Objective", "content": "We now consider optimizing a NeSy-WMC model, consisting of a neural and logic component. We leave the neural component unspecified and only require it to be learnable via gradient descent. The data set contains I tuples D = {(x_i, \u03a6_i) | i = 1, ..., I}, where x_i is a raw input and \u03a6_i is a logical formula that is assumed to hold for that data point and can be seen as its label. For example, if a self-driving car does not brake given image x_i (see Ex. 1), then the label is \u03a6_i = \u00acf_1 \u2227 \u00acf_2 \u2227 (\u00acf_3 \u2228 \u00acf_4), indicating there is no pedestrian, no red light and either there is no crosswalk or the car is driving slow enough. We will often drop the index i of the data point.\nOur setup is modelled by the probabilistic graphical model in Fig. 2. It contains the input x \u2208 R^d, the n binary variables f\u2208 {0,1}^n and whether \u03a6 is satisfied. The logic component is deterministic P(\u03a6|f) = 1 if f satisfies \u03a6 and zero otherwise. The neural component outputs a distribution P(f | x), which are the weights of the NeSy-WMC problem. The answer W of the NeSy-WMC problem is the probability P(\u03a6 | x). The neural component is usually learned via likelihood maximization on the data set D. However, calculating the likelihood requires expensive inference, which is intractable for complex systems. We therefore propose the following surrogate objective L that bounds the true likelihood.\nDefinition 4. For a data point (x_i, \u03a6_i) let Q_i be an auxiliary distribution over the explanations \u03a8_i of the label \u03a6_i and let P_i be the distribution over the explanations that the neural component outputs given x_i. We then define the surrogate objective L = \u03a3_i KL(Q_i | P_i).\nIntuitively, the auxiliary distributions Q_i propose explanations for the label \u03a6_i, from which the neural component P_i can learn (Fig. 2). We now show the bound on the negative log-likelihood loss. The full derivation of this bound can be found in the appendix.\n- log P(D) \u2264 - \u03a3_i \u03a3_{f\u2208\u03a8_i} Q_i(f) log (P(f|x_i) / Q_i(f)) (1)\n= \u03a3_i KL(Q | P) = L\nThe tightest bound in Eq. 1 is obtained by minimizing the Kullback-Leibler (KL) divergence with respect to both P_i and Q_i. The learning signal for the neural component P_i is thus provided by Q_i, sidestepping the requirement of backpropagating gradients through the logic component [26, 44, 1]. Moreover, we also avoid expensive exact inference based on knowledge compilation [32] when learning with L."}, {"title": "4 EXPLAIN, AGREE, LEARN", "content": "The EXPLAIN, AGREE, LEARN method aims to minimize the objective L from Def. 4. This objective contains a KL divergence between the distributions P_i and Q_i, raising a number of issues. The first issue is that each Q_i is supported on all explanations of the formula \u03a6_i, which are unknown a priori and infeasible to obtain for complex problems. Hence, we introduce the EXPLAIN algorithm that samples explanations of \u03a6_i used to approximate the support of Q_i. After sampling explanations, the samples are reweighted by the neural component P_i in the AGREE step such that samples deemed more likely by P_i have a higher weight. Alternatively, the AGREE step can be interpreted as a minimization of L with respect to all Q. The EXPLAIN and AGREE steps both serve to construct a suitable proposal distribution Q_i for the objective L. L is then used to optimize each P_i in the LEARN step, which performs a traditional update step using backpropagation."}, {"title": "4.1 EXPLAIN: Sampling Explanations", "content": "The first step to minimizing L is to construct the support of the proposal distributions Q. As the support of each Q consists of explanations of the logical formula \u03a6, we propose the EXPLAIN algorithm to sample such explanations. EXPLAIN is a stochastic variant of the Davis-Putnam-Logemann-Loveland (DPLL) algorithm, for which we first introduce some necessary concepts.\nDefinition 5. A sampling strategy \u03a3(\u03bc) for an assignment \u03bc is a probability distribution over the set of all assignments obtained from \u03bc by assigning one more variable a value.\nEXPLAIN is shown in Alg. 1 and differs from traditional DPLL on lines 10 and 11. Specifically, instead of iterating over all possible assignments, a new assignment \u03bc' is sampled according to \u03a3(\u03bc). EXPLAIN starts with the formula \u03a6, assumed to be in conjunctive normal form, the empty assignment \u03bc^?, defined as \u2200k : \u03bc^?(k) = ?, and a uniform sampling strategy \u03a3(\u03bc). First, trivial variable assignments, e.g. unit clauses, are propagated to \u03bc. The algorithm then terminates if \u03a6 is satisfied or restarts if \u03a6 is unsatisfiable. Otherwise, a value for one more variable is sampled using \u03a3(\u03bc) and EXPLAIN is recursively called. Notice how the distribution of returned explanations is entirely determined by the sampling strategy \u03a3(\u03bc).\nExample 2. Recall Ex. 1 and Fig. 1. If the self-driving car brakes at some point, the corresponding query is \u03a6(f) = f_1 \u2228 f_2 \u2228 (\u00acf_3 \u2227 f_4). EXPLAIN can sample multiple explanations for this query, e.g. \u03bc_1 = (0, 1, 1, 0) (the car brakes due to a red light) or \u03bc_2 = (1, 0, 0, 1) (the car brakes due to a pedestrian on a crosswalk).\nDuring the execution of EXPLAIN it is possible to encounter conflicting variable assignments. Specifically, a conflict occurs when the query \u03a6 is unsatisfiable by \u03bc. The execution of EXPLAIN is stochastic because of the sampling strategy \u03a3, so it is possible that some sampled assignments lead to a conflict whereas others do not. There are several policies for dealing with conflicts. The easiest policy is to simply stop execution and restart EXPLAIN from the top. Another policy is to backtrack and sample a different new variable assignment \u03bc'. Furthermore, a limit on the amount of backtracking can be imposed before deciding to restart. Although Alg. 1 restarts execution upon encountering a conflict, our implementation employs backtracking to the latest sampled assignment. Note that, if backtracking and a deterministic sampling strategy are used, EXPLAIN reduces to DPLL."}, {"title": "4.2 AGREE: Updating the Weights", "content": "We can choose any auxiliary distribution Q over explanations \u03a8 of \u03a6 in Def. 4 of L and will choose Q to minimize L. The explanations sampled by the EXPLAIN algorithm define the support of Q and varying Q amounts to reweighing every sample. It turns out that the objective L is minimized with respect to these weights Q when they are proportional to the output of the neural component P(f | x). Intuitively, explanations that the neural component deems more likely, should be assigned a higher weight to minimize L. We now formalize this result and provide a bound on the error of the approximation.\nTheorem 1. Let \u03a8 be the explanations sampled by EXPLAIN and define Q^\u2217(f) = P(f | x)/\u03a3_{f'\u2208\u03a8} P(f' | x), then:\n1. Optimality. Q^\u2217 globally minimizes L with respect to Q.\n2. Optimum. The minimum of L is given by \u2013 \u03a3_i log P^\u2217(\u03a6_i | x_i), with P^\u2217(\u03a6_i | x_i) = \u03a3_{f\u2208\u03a8_i} P(f | x_i).\n3. Bounds. For any x and \u03a6, the true probability is bounded by P^\u2217 (\u03a6 | x) < P(\u03a6 | x) < 1 \u2013 P^\u2217 (\u00ac\u03a6 | x).\nProof. Optimality. Let us write out one term of L, dropping the data point index i, yielding\nKL(QP) = - \u03a3_{f\u2208\u03a8} Q(f) log (P(f | x) / Q(f)) (2)\nNext, we use the method of Lagrange multipliers to impose the constraint \u03a3_{f \u2208 \u03a8} Q(f) = 1 while minimizing this KL divergence with respect to all the Q(f). This method leads to the equations\n\u2202 / \u2202Q(f) KL(QP) = - log (P(fx) / Q(f)) + \u03a3_{f'\u2208\u03a8} Q(f') (P(f') / Q(f')) + \u03bb + 1 = 0, (3)\nand thus Q(f) = exp(\u03bb+1)P(f | x). From the constraint we solve \u03bb=\u22121\u2212log\u03a3_{f\u2208\u03a8} P(f | x) and filling this into the expression for Q(f) gives the optimum at Q^\u2217(f) = P(f | x)/\u03a3_{f'\u2208\u03a8}P(f' | x).\nOptimum. Filling in the optimal value Q^\u2217 (f) in Eq. (2) gives\nKL(Q | P) = \u2212 log \u03a3_{f\u2208\u03a8} P(f | x) = - log P^\u2217(\u03a6 | x) (4)\nand summing over the data index i gives the result.\nBounds. Since \u03a8 \u2286 \u03a6, the inequality\nP^\u2217 (\u03a6 | x) = \u03a3_{f\u2208\u03a8} P(f | x) \u2264 \u03a3_{f\u2208\u03a6} P(f | x) = P(\u03a6 | x) (5)\nholds. The upper bound on P(\u03a6 | x) is the same inequality applied to \u00ac\u03a6 and using P(\u03a6 | x) = 1 \u2013 P(\u00ac\u03a6 | x)."}, {"title": "4.3 LEARN: Updating the Neural Component", "content": "The LEARN step minimizes L with respect to the parameters of the neural component P using standard gradient descent techniques. When |\u03a8| = 1, we observe that the loss L is equivalent to the traditional cross-entropy loss. We can interpret the LEARN step as a standard supervised learning step, where the labels are provided by the EXPLAIN and AGREE steps. The full EXAL algorithm is shown in Alg. 2."}, {"title": "5 Optimizations using Diversity", "content": "The following section provides an optimization for EXAL using the concept of diversity. The formula \u03a6 of a data point can have many explanations and the true underlying explanation is not known. Consequently, sampling a diverse set of explanations with EXPLAIN can increase the chance of seeing the true explanation. For example, a self-driving car will more often brake because for a pedestrian than an animal on the road. An animal should however not be dismissed as a possible explanation for braking and could still be the true explanation for a certain data point. We formalise this intuition through the notion of diversity of explanations. Additionally, we provide two practical methods to increase the diversity of the samples obtained from the EXPLAIN algorithm."}, {"title": "5.1 Diversity of Explanations", "content": "We formally define diversity and prove in Prop. 1 that sampling a more diverse set of explanations leads to a tighter bound L on the negative log-likelihood (see Eq. 1).\nDefinition 6. The diversity of a set \u03a8 of assignments with respect to \u03a6 is \u03b4(\u03a8) = |\u03a8 \u2229 \u03a6|, where \u03a6 are the explanations of \u03a6.\nProposition 1. If \u03a8_1 \u2286 \u03a8_2 then \u03b4(\u03a8_1) \u2264 \u03b4(\u03a8_2) and L_2 \u2264 L_1 where L_1 and L_2 are the objectives constructed from Equation (2) using \u03a8_1 and \u03a8_2, respectively (see optimum in Thm. 1).\nProof. If \u03a8_1 \u2286 \u03a8_2 then \u03a3_{f\u2208\u03a8_1} P(f | x) \u2264 \u03a3_{f\u2208\u03a8_2} P(f | x). Taking the negative logarithm of both sides gives the result\nL_2 = -log \u03a3_{f\u2208\u03a8_2} P(f | x) \u2264 \u2212 log \u03a3_{f\u2208\u03a8_1} P(f | x) = L_1 (6)\nThe following paragraph explains how to increase diversity with a suitable sampling strategy. Recall that the execution of EXPLAIN depends entirely on the sampling strategy \u03a3(\u03bc). We parametrize \u03a3_\u03b8(\u03bc) with parameters \u03b8 \u2208 R^+ and change \u03b8 to increase the diversity. Concretely, we keep track of how often \u03a3_\u03b8(\u03bc) samples a (partial) assignment \u03bc' during the execution of EXPLAIN. Call this count N (\u03bc'). Every time \u03bc' is sampled, the probability to sample \u03bc' will be reduced by a factor of exp(-\u03b8), encouraging the exploration of different assignments. In other words, the probability that \u03a3_\u03b8(\u03bc) samples \u03bc' is chosen proportional to exp(-N(\u03bc')\u03b8). Uniform sampling without diversity optimizations corresponds to \u03b8 = 0."}, {"title": "5.2 Reformulation as a Markov Decision Process", "content": "A recent result [5] of generative flow networks (GFlowNets) for Markov decision processes (MDPs) also allows us to increase the diversity of sampled explanations through optimisation of \u03b8. Their results include a method to maximize the expected reward when executing an MDP with certain properties. If we set the reward in the MDP equal to the diversity, their method can be used to increase the diversity. To apply the result, running EXPLAIN T times for a query \u03a6 to obtain a set of T samples first has to be framed as an MDP. Recall that A(n) = {0,1,?}^n is the set of all assignments to n binary variables. Therefore, the MDP has states (\u03bc, t, \u03a8) \u2208 A(n) \u00d7 N \u00d7 2^{A(n)}, where \u03bc is the current assignment in the tth run of EXPLAIN and \u03a8 contains the assignments returned by previous runs of EXPLAIN. The initial state is (\u03bc^?, 0, {}). For an intermediate state (\u03bc, t, \u03a8) with partial \u03bc, the possible actions are to assign a value to an unassigned variable of \u03bc, leading to a new state (\u03bc', t, \u03a8). If \u03bc is complete, the only action is to go to the next run of EXPLAIN, transitioning to the state (\u03bc^?, t + 1, \u03a8 \u222a {\u03bc}). The terminal states are (\u03bc^?, T, \u03a8) which have a reward R(\u03bc^?, T, \u03a8) = \u03b4(\u03a8). All other states have no reward.\nProposition 2. The transition graph of the above MDP is acyclic and the MDP will always terminate.\nProof. For a state (\u03bc, t, \u03a8) we define a quantity, which we call the progress, as (n + 1)t + |{k | \u03bc(k) \u2260 ?}| with n the number of variables. We show that the progress increases by a value of one for every transition in the MDP. There are two cases. If \u03bc is partial, the only action is to assign a value to a variable. This leaves the first term of the progress unchanged, but increases the second term by one. If \u03bc is complete, the MDP transitions to (\u03bc^?, t + 1, \u03a8). The second term changes by -n, but the increment of t causes the first term to increase by n + 1, again resulting in a net gain of one. Since the progress always increases when transitioning from state to state, no previous state can ever be visited. Hence the MDP is acyclic. Furthermore all states with progress (n + 1)T are terminal states, so the MDP is guaranteed to terminate.\nThe execution of EXPLAIN, governed by the sampling strategy \u03a3(\u03bc), translates to a probabilistic policy for the MDP as follows. In a state (\u03bc, t, \u03a8) with partial \u03bc, the probability to take the action transitioning to (\u03bc', t, \u03a8) is chosen equal to the probability of \u03bc' as given by \u03a3(\u03bc). For complete \u03bc, the only possible action is to rerun EXPLAIN. There is thus a mapping from sampling strategies \u03a3(\u03bc) to MDP policies. This mapping allows the maximization of the diversity of parametrized sampling strategies \u03a3_\u03b8(\u03bc) via GFlowNets.\nTheorem 2. Consider the flow function F : (A(n) \u00d7 N\u00d72^{A(n)})^2 \u2192 R^+ that solves the flow equation\n\u03a3_{s'->s} F(s', s) = R(s) + \u03a3_{s->s'} F(s, s') (7)\nwhere the sums run over all states s' = (\u03bc', \u03c4', \u03a8') incoming into s = (\u03bc, t, \u03a8) or outgoing from s respectively. Following the policy that transitions from s to s' with probabilities proportional to F(s, s'), the probability to terminate in s is proportional to R(s) = R(\u03bc, \u03c4, \u03a8) = \u03b4(\u03a8), i.e. the diversity of \u03a8.\nProof. Because the MDP is acyclic and guaranteed to terminate (Prop. 2), this result follows by applying Prop. 2 in [5] to the MDP.\nThe above theorem provides a way to optimize the expected diversity when running EXPLAIN, namely by choosing \u03b8 such that the policy given by \u03a3_\u03b8(\u03bc) is as close as possible to the policy given by F. The solution F to the flow equation can be difficult to find, but Bengio et al. [5] provide one method. The general idea is to minimize the logarithmic difference \u0394 between the left- and right-hand side of the flow equation (Eq. 7)\n\u0394(F) = \u2211_i log ^2 (\u03a3_{s'->s_i} F(s', s_i) / R(s_i) + \u03a3_{s_i->s'} F(s_i, s')) (8)\nwith the sum running over states s_i in trajectories sampled from the MDP. We experimentally compare this optimization approach to the simpler approach at the end of Sec. 5.1. The results are discussed in Sec. 6.1."}, {"title": "6 Experiments", "content": "Experiments are provided to support our theoretical claims. We show that we can learn to generate diverse samples using the EXPLAIN algorithm. Then the importance of diversity and the AGREE step are illustrated by looking at the convergence of the learning objective. Lastly, we apply the EXAL method to the MNIST addition and Warcraft pathfinding tasks. Our implementation can be found here (https://anonymous.4open.science/r/exal-526C) and more details about the experiments are in the appendix."}, {"title": "6.1 Diversity (EXPLAIN)", "content": "For the first experiment, we want to investigate how different sampling strategies compare in terms of diversity. To this aim, we run EXPLAIN on formulas generated in 3 different classes and with a varied number of variables, ranging between 15 and 60. Generation details for each class branch, split and bottomup are in the appendix. For every generated formula, we evaluate the diversity of different sampling strategies so they can be directly compared. This experiment is concerned only with logical explanations and does not use neural networks. We count how many unique explanations are sampled during subsequent calls to EXPLAIN, i.e. the diversity. The counts at each time step are averaged out over 200 runs on the same formula. This is done for fixed values of \u03b8, as well as for parameters \u03b8 that are learned using the diversity algorithm in Sec. 5.2. As baselines, a uniform sampler and the theoretical maximally diverse sampler are considered.\nFig. 3 shows the evolution of the diversity for each strategy. The diversity has been normalized to the unit interval for easy comparison. Although the EXPLAIN algorithm is not guaranteed to sample uniformly, in many instances it is close to uniform. This can be seen by the overlapping of the uniform line and the line with \u03b8 = 0. For the bottomup formulas however, EXPLAIN is not uniform and performs worse for \u03b8 = 0. The diversity of EXPLAIN is typically increased by choosing a larger \u03b8 and can get close to the optimal diversity. In some instances, such as in the split formulas, having \u03b8 > 0 increases diversity, but there is little difference in diversity for the different values of \u03b8. We also see that learning to optimize the diversity performs better than sampling uniformly. It is recommended to choose a higher \u03b8 for improving the diversity, but stability issues can occur for too high values of \u03b8."}, {"title": "6.2 Convergence of Bounds (AGREE)", "content": "We now experimentally answer what the importance of diversity and of the AGREE step are. For this purpose we compare the surrogate objective with the true likelihood for different number of samples. More concretely, explanations are sampled for the formula \u03a6 and for its negation, using both the diverse and non-diverse versions of EXPLAIN. Then the objectives with and without the AGREE step are calculated. The AGREE step makes use of the probabilities given by the neural network. We perform this synthetic experiment without neural networks by generating the probabilities uniformly at random on the unit interval. The results for all four combinations are shown in Figure 4. The samples for the formula give a lower bound, whereas samples for its negation give an upper bound. The AGREE step is necessary for the objectives to converge to the true likelihood. Furthermore, convergence happens faster when the samples are more diverse, confirming our theoretical results that diversity improves the error bounds."}, {"title": "6.3 MNIST Addition (EXAL)", "content": "We evaluate our approach on the standard NeSy experiment of learning to sum MNIST digits [26, 27]. The input to this task is two sequences of N MNIST images, where each sequence represents a decimal number of length N. The desired output is the sum of these two numbers, which is also the only supervision. The sum is encoded in a logical formula. For example, if two single-digit numbers a and b add up to 3, the formula is \u03a6 = (a = 0 \u2227 b = 3) \u2228 (a = 1 \u2227 b = 2) \u2228 (a = 2\u2227 b = 1) \u2228 (a = 3\u2227 b = 0). The size of the possible assignments of values to each digit in each sequence (10^2N) in combination with the distant supervision is what makes learning to sum MNIST digits a challenging task. Although our general implementation of EXPLAIN is not parallelized, we have used a parallelized implementation of EXPLAIN that can sample digits for this experiment. More details on the experimental setup and hyperparameters is given in the appendix."}, {"title": "6.4 Warcraft Pathfinding (EXAL)", "content": "The task of Warcraft pathfinding, as described in [41], is to find the shortest path between two corner points of a two-dimensional grid given an image representing the grid. The exact traversal costs of each node are not known and have to be predicted from the image by the neural component. The only supervision for training the neural component is the true shortest path in the grid.\nIn this context, the symbolic component is a shortest path finding algorithm, e.g. Dijkstra's algorithm, and an explanation is a cost assignment to every node in the grid such that the shortest path given by those costs coincides with the true shortest path. Sampling explanations is done as follows. First the grid is initialized with all nodes set to the highest cost, except for nodes on the true shortest path, which are set to the lowest cost. This guarantees that the given shortest path coincides with the true shortest path. Then a Gibbs sampling procedure is executed that resamples node costs with the restriction that the shortest path is unchanged. After a burn-in period of 100 resampling steps, the resampled node costs are used as a training signal for the neural component.\nOnce the neural component has been trained to predict node costs, the NeSy system can predict the shortest path given an image of a grid. A prediction is only considered correct if the predicted shortest path overlaps entirely with the true shortest path.\nTwo main conclusions can be drawn from the reported test accuracies in Table 2. First, the same trend from the MNIST experiment continues, namely that EXAL provides orders of magnitude faster learning times compared to the state-of-the-art. Second, this quick convergence time does not impact the acquired accuracies. Even more, for the most challenging 30 \u00d7 30 grid, EXAL significantly outperforms A-NeSI. Together, they provide strong evidence that using explanations improves scaling of NeSy learning."}, {"title": "7 Related Work", "content": "We review recent works on neurosymbolic (NeSy) learning and scalable logical inference. For a more detailed discussion on NeSy, we refer the reader to recent surveys in [7, 19].\nNeSy relaxations. Several NeSY systems address the scalability of inference with continuous relaxations based on fuzzy logic semantics [3, 18, 11, 25, 17, 34, 14]. However, these relaxations may introduce approximations that can yield different outputs for equivalent logic formulas [39, 20]. In contrast, our work performs inference and learning without resorting to such relaxations while retaining a probabilistic interpretation. Similar to us, the work in [42] provides a NeSy solution based on SMT solvers, which does not require backpropagation through the symbolic component. The solution approximates the gradient by discretizing the neural representation and solving a combinatorial optimization problem. In contrast, our work does not require to approximate the gradient as training proceeds in a supervised fashion thanks to the sampled explanations. Additionally, we avoid discretizing the neural representation as we leverage neural predicates, essential for neatly integrating probability and logic. Lastly, our aim here is different as the paradigm is designed to scale probabilistic NeSy and to quantify the quality-speed tradeoff.\nNeural inference strategies. The work from [10] proposes a neurosymbolic pipeline consisting of a symbolic engine and three neural modules. Specifically, a perception network mapping the images to their symbolic representations, a neural solver attempting to correct the symbolic representation, and a mask predictor to identify possible mistakes done by the neural solver. The symbolic solver then corrects the mistakes on the neural predictions. The neural modules are pre-trained under supervision and fine-tuned using reinforcement learning. In contrast, our simplified framework is based on a scalable logic engine and single neural component. Learning is performed by directly supervising the neural component using the sampled explanations using variational inference strategies. The work in [40] introduces two neural modules for neural-symbolic learning: a perception component mapping input data to the probabilities of facts, and a neural reasoner component mapping probabilities to the query. Learning involves training the neural reasoner to mimic synthetic input/output pairs obtained through logical inference, and then training the perception component in a supervised manner using the frozen neural reasoner. In contrast, our work only requires a perception component and utilizes a sampling algorithm that guarantees logically consistent solutions while ensuring diversity. The Neural Theorem Prover [29, 30] introduces a continuous and differentiable relaxation of the backward-chaining logic reasoning algorithm. In contrast, our approach does not rely on relaxations or differentiability through the logic program. Other neural sampling strategies, such as GFlowNets [6, 5, 47], treat sampling as a sequential decision-making process and learn a policy based on a reward function. However, sampling with hard constraints and exploring solution modes from logical programs remains a challenging problem [16, 35].\nLogical inference/sampling strategies. Probabilistic logical inference can be performed exactly by transforming the logical program into a probabilistic circuit (PC) through knowledge compilation [12]. This allows for efficient evaluation in polynomial time [9, 44, 1, 2]. Alternatively, approximate strategies can be used to avoid the computational burden of knowledge compilation, but they may result in biased learning [27, 22, 36] and in lacking guarantees on the uniformity/quality of the sampled solutions [23, 4]. The assumption of uniform distribution of worlds is often made when sampling solutions from a CNF formula, and various uniform samplers have been proposed with theoretical guarantees on query complexity and uniformity [28]. State-of-the-art samplers based on hashing-based methods and SAT solvers achieve approximate uniformity by partitioning the solution space into smaller regions [15, 37, 13, 38, 37, 45]. In contrast, our algorithm focuses on a stronger criterion than uniformity, namely diversity."}, {"title": "8 Discussion and Conclusion", "content": "The EXAL method is designed as a scalable solution to learning for neural probabilistic logic. The key idea is to use explanation sampling to propagate the learning signal through the symbolic component. This allows the neural component to be trained fast compared to exact approaches. We provide theoretical guarantees on the approximation error and provide practical methods to reduce this error, in particular by encouraging diversity. Experimentally, EXAL is shown to be competitive with state-of-the-art NeSy methods on larger problems in terms of accuracy while significantly outperforming them in terms of speed.\nDespite these benefits, sampling explanations is NP-hard and can be slow for problems with a large explanation space. Exploring the entire space can be expensive, but is often unnecessary in practice. A good training signal can be obtained from a sampled subset of explanations. For example, in the Warcraft pathfinding problem it is unlikely to sample the true grid, but sampling similar grids suffices to properly train the network. Whether or not it is necessary to explore the entire explanation space is of course problem dependent. In the worst case where the explanation space is large and only one explanation can provide a good training signal, EXAL will also perform poorly. Furthermore, if satisfiability is difficult to check, sampling explanations for a formula is also difficult, as the existence of an explanation implies satisfiability. NeSy methods, including EX"}]}