{"title": "ALPHAZIP: NEURAL NETWORK-ENHANCED LOSSLESS TEXT COMPRESSION", "authors": ["Swathi Shree Narashiman", "Nitin Chandrachoodan"], "abstract": "Data compression continues to evolve, with traditional information theory methods being widely used for compressing text, images, and videos. Recently, there has been growing interest in leveraging Generative AI for predictive compression techniques. This paper \u00b9 introduces a lossless text compression approach using a Large Language Model (LLM). The method involves two key steps: first, prediction using a dense neural network architecture, such as a transformer block; second, compressing the predicted ranks [1] with standard compression algorithms like Adaptive Huffman, LZ77, or Gzip. Extensive analysis and benchmarking against conventional information-theoretic baselines demonstrate that neural compression offers improved performance.", "sections": [{"title": "1 Introduction", "content": "Data compression, especially in the realm of text, is the art of reducing the size of information without sacrificing its integrity. In an era where vast amounts of data are constantly exchanged, efficient text compression has become more critical than ever, enabling faster communication, reduced storage costs, and enhanced performance in bandwidth-limited environments. Text in digital form is stored using different character sets (ASCII, Unicode, etc.) that are encoded into binary using encoding schemes like ASCII encoding, UTF-8, UTF-16 etc. Text files can be stored in plain text format devoid of any formatting metadata or could be stored in Rich Text, HTML or XML formats where structuring and formatting add an overhead to the storage space. Compressing text files involves identifying the redundancies in these representations and encoding them to capture recurring pattern information. This is usually achieved through lossless compression algorithms. Though there have been many purely information theoretic frameworks for compressing text in a lossless manner, Neural Network based architectures outperform most of these techniques, as they can encode additional information about the relationships between different components of text[2].\nGenerative pre-trained transformers (GPTs) are the state-of-the-art in text generation and prediction [3]. In this study, we leverage the power of Large Language models in our generative compression mechanism. Our model applies a compression algorithm over the outputs from the transformer block and compresses the text to a binary bit stream that can be transmitted [4]. Although the use of GPTs is computationally heavy, we show that by utilising accelerated linear algebra (XLA) compilation combined with an optimal neural model size the process can be performed in reasonable time for realistic data.\nWe also present a behavioral analysis of various models on compression performance and process latency. We show that by constraining the model using fine-tuning techniques, we can achieve domain based compression that performs better than agnostic techniques that do not use this information.\nInformation theoretic compression is well studied, and we do not expect any significant changes in the compression ratios achieved through standard compression algorithms. Instead, the main focus of this paper is to see if the prediction"}, {"title": "2 Background", "content": "Prior art in the area of text compression primarily points to two major text compression methods, namely entropy encoding and dictionary encoding. A purely probabilistic model based on greedy prediction, followed by compression analysis is proposed in Ref. [5]. Their work gives an understanding of how rank prediction can be exploited to capture the redundancies and compress text effectively. Another study[6], tries to avoid the latency in two passes by proposing a one-pass adaptive method. This method models the chances of a character occurring in the same pre-context and accommodates the probability as the input is parsed in. DeepZip [7] utilises the power of Recurrent Neural Networks for compression and seems to outperform traditional compressors like GZIP [8] and BSC [9]. Deepzip combines RNNS with arithmetic coder for compression making it effective compared to other finite context models.\nThe Transformer architecture [10] has proposed the concept of attention mechanisms as a useful way of predicting text. The query, key, value based prediction in transformers surpassed all the state-of-the-art sequence to sequence models. This was followed by many attempts including the usage of LSTMs [2] and transformers, that have shown interesting results. A recent work [11] emphasises the potential of Large Language Models (LLMs) in prediction, and advocates viewing the prediction problem through the lens of compression. It demonstrates how a compressor can be transformed into a predictor by developing probability distributions using Shannon's entropy principle.\nTokenization can be viewed as a pre-compression step allowing models to increase the information content in their context. This drives our motivation to analyse the power of LLMs in lossless text compression in a rank based compression mechanism.\nFine-tuning a pre-trained model can improve the performance in task specific applications. To reduce the compute intensity PEFT2 [12] techniques like LoRA\u00b3 [13] are found to be helpful. Fine-tuned models are domain specific, reducing the uncertainty in prediction, hence making the model a better compressor (although they cannot be generalised). This intuition has been studied in our experiments to compare the performance of domain specific LLMs in text compression.\nIn contrast to standard information-theoretic compressors (which model the frequency of symbols for encoding) neural network-based predictors also capture the semantic similarity within a continuous stream of tokens. This approach effectively reduces the amount of information to be transmitted, thereby enhancing compression efficiency. Unlike many previous approaches that use probabilistic models to process the input text before compression, our experiments are conducted on unseen text (not present in the training data for fine tuning). As a result, the effectiveness of compression depends heavily on the accuracy of the neural network predictor."}, {"title": "Contributions", "content": "In this study, we conducted an experimental analysis on the compressive capabilities of large language models. It is important to note that we have taken the approach of using the LLM for predicting tokens, and leaving the actual compression to a known algorithm \u2013 this allows us to focus on the effectiveness of the LLM in predicting text.\n\u2022 We examine how the predictive performance of the neural network can improve compression ratio.\n\u2022 We achieved a compression ratio of up to 57% through the proposed two step predictive compression mechanism. Constraining the domain knowledge of the LLM proves to enhance the compression performance."}, {"title": "2.1 Information Theoretic Baselines", "content": "Lossless compression algorithms reduce the redundancies in the data to ensure that there is no data loss when de-compressing data back to the original form[16]. Certain types of data such as images can survive a certain amount of loss in accuracy, while text compression comes under the lossless type[17]. To quantify the performance of neural compression, we compare the results with few of the baselines discussed below."}, {"title": "2.1.1 Arithmetic Encoding", "content": "Arithmetic encoding is an entropy encoding that requires two inputs: the next symbol and its frequency. The frequencies are utilised to calculate the probabilities of each symbol. The result of arithmetic encoding is a single number ranging between 0 to 1, a variable length representation. This is a compact representation and requires further processing to convert it into a binary representation."}, {"title": "2.1.2 Huffman and Adaptive Huffman Encoding", "content": "Huffman encoding assigns variable length codes to symbols based on their frequency of occurrence. The probability distribution is used to construct a Huffman Tree using a bottom-to-top approach. The tree is traversed from the root to the leaf node to encode a given symbol."}, {"title": "2.1.3 Lempel Ziv 77 (LZ77)", "content": "To capture the higher order relationships in words and phrases, Jacob Ziv and Abraham Lempel developed a sliding window concept. The standard implementation of the LZW (Lempel Ziv Welch) algorithm utilises a table (dictionary) of 4096 entries of symbols and their binary representation. The first 255 entries correspond to ASCII. As the input is parsed, the longest match is obtained from the dictionary (if found) else new entries are filled in the dictionary.\nIn contrast, the LZ77 algorithm does not store a dictionary: instead it uses a sliding window buffer to identify and encode repeated sub-strings by specifying their distance and length from the current position in the buffer."}, {"title": "2.1.4 Gzip compression algorithm", "content": "Gzip 6 uses the DEFLATE compression algorithm[19], which combines LZ77 (Lempel-Ziv 1977) and Huffman coding techniques. LZ77 is used to find repeated sequences in the data, while Huffman coding is employed to assign variable-length codes to these sequences based on their frequencies.\n1. Repeated sequences are identified and given shorter codes (LZ77).\n2. The codes with high frequencies are assigned fewer bits whereas the ones appearing rarely are assigned more number of bits (Huffman)."}, {"title": "2.1.5 Brotli compression algorithm", "content": "Brotli 7, a dictionary based compression algorithm developed by Google [20] has been found to surpass most of the state-of-the-art compression techniques including gzip. This is attributed to Brotli's context modeling method to adapt to the input data pattern.\nBrotli compression occurs in two phases: in the first phase, parsing based on the previous occurrences is used to create the static LZ77 dictionary, while in the second phase a Huffman encoding of the parsed data is performed by choosing the optimal encoding from the canonical Huffman forms. Thus by using a combination of static dictionary encoding and building dynamic Huffman trees, Brotli achieves better and faster compression than most of the LZ family counterparts."}, {"title": "3 Neural Compression Model Architecture", "content": "As discussed earlier, compression involves identifying redundancy in the input data and then efficiently encoding that redundancy. In this work, we focus on mechanisms that can enhance the former part: namely using the predictive powers of LLMs to identify possible redundancy in the textual data, thereby providing added redundancy in the input that is provided to the standard compression algorithms. In particular, we focus on the use of transformer based architectures to predict text in such a way as to enhance the capability of compression algorithms."}, {"title": "3.1 Tokenization", "content": "Tokenization is the process of breaking down an input text into smaller chunks that can be processed by Large Language models. GPT2[14] uses Byte pair encoding[21]. In this method, initially, each character is treated as a symbol. The algorithm then iteratively merges frequently occurring pairs of symbols into new symbols and adds them to the vocabulary. The final vocabulary converges to a point where the indices of each symbol represent its token ID. GPT2 also performs word-level encoding and text pre-processing to improve the model performance.\nThe input text is parsed through the tokenizer to obtain a tokenized vector. This vector is utilised to find the right token ID following a given context, and to predict the rank of the right token."}, {"title": "3.2 Rank Prediction", "content": "The rank of the right Token ID following a context is the heart of AlphaZip's architecture. When a given context is fed into the Pre-Trained Transformer, the output logits 8 are used to calculate the probability distribution across the entire vocabulary of tokens in the tokenizer. By comparing it with the tokenized input, we calculate the rank of the right token ID based on the sorted probability distribution. This means that the most probable token being the right token corresponds to rank 0, the second most probable token being the right token corresponds to rank 1 and so on.\nIt is observed that the distribution of these ranks decreases with the order of the rank with 0 being the most recurring rank. The variation of the distribution of ranks vs the rank plotted below clearly shows a curve that has an exponential decay behavior. The better the predictor, ranks close to 0 occur more frequently, thus improving the compression ratio. The sequence of ranks encode the redundancy in the input string while the actual representation of these ranks has little effect on the compression. Hence, we feed the ranks separated by '.'s as ASCII into the standard compressor. It is quite possible that better compression algorithms could be identified to further compress this data \u2013 however, our goal here is to identify redundancy, and hence we fall back to traditional lossless compression algorithms to remove the redundancy in the data."}, {"title": "3.3 Individual Inference vs Batch Inference", "content": "Inferencing is the process of applying a given model at the individual token level to infer the predicted values corresponding to a given input. This involves computing probabilities for every token in the tokenized input given the right previous context. This increases the latency and time of computing. Batch inferencing on the other hand computes the output probabilities for the next k tokens given a context size.\nWhile individual inferencing follows a greedy prediction algorithm, batch inferencing introduces some stochasticity in the model prediction and hence achieving ranks closer to 0 becomes less likely. Optimising the value of k to get better results can help set the trade off between latency and compression performance. In all the experiments that follow, we have opted to utilise Individual inferencing."}, {"title": "3.4 Accelerating inferencing using TensorFlow XLA", "content": "TensorFlow's XLA (Accelerated Linear Algebra) [22] is a domain specific compiler that optimizes the execution of computation graphs especially for matrix multiplications and vector operations and hence increases performance. Studies show that XLA can show a 50% speed up in computation in GPUs over TensorFlow models without XLA[22].\nDuring XLA compilation, the initial computation graph is optimized through several passes such as Dead Code Elimination and Common Sub-expression Elimination [23]. The fusion techniques applied include horizontal fusion, instruction fusion, multi-output fusion etc. In our implementation procedures, we use Just In Time (JIT) Compilation,"}, {"title": "4 Results and Analysis", "content": "We conduct a set of experiments to compress works of various authors. We quantify compression using bpc and compression ratio metrics. To maintain uniformity and ease out comparative analysis we compress only the first 10000 characters in every book. We use TensorFlow XLA during compression throughout the experiments. The obtained ranks from the transformer architecture are compressed using the gzip algorithm to get the output bit stream. We analyse the style transfer capability of transformer based architectures in domain based compression. Through results, we state that domain specific transformers perform better compression than vanilla transformer models."}, {"title": "Datasets and Training Arguments", "content": "We utilise the open source Gutenberg corpus [24] for all our tests and analysis. The corpus has books stored in .txt format and does not require any pre-processing. To compress multi-lingual text we utilised resources from the OPUS dataset[25]. For a few experiments, we would also be using the Tiny Shakespeare dataset [26]."}, {"title": "Hyperparameters", "content": "To show our compression results and performance, we try to compress nearly the first 10KB (10000 characters) of data from each book in the corpus with the context window being of size 100. Through our experiments we observed that, bigger the context window size is longer the process latency is, thus choosing an optimal window size can set the trade off between accuracy of prediction and latency. For better convergence, we fine-tuned or knowledge distilled vanilla GPT2 models with a learning rate of 5e-7."}, {"title": "4.1 Compression Performance Quantification", "content": "Compression Ratio is defined as the ratio of the uncompressed file size to the compressed file size. Generally, a larger value of compression ratio is desirable.\nCompression ratio =  \\frac{Uncompressed file size}{Compressed file size}\nBits Per Character (bpc) is the ratio of the size of the compressed file in bits to the number of ASCII characters in the content of the original file. This is a popular metric for text, for other types of data bits per byte (bpb) is used.\nbpc = \\frac{Compressed file size in bits}{Number of characters in original file}\nEntropy (Shannon Entropy) of a file is a measure of the average information content [27] in the data source. Lower entropy indicates more predictable data, which can be compressed more effectively.\nH = - \\sum_{i=1}^{n}p_{i} log_{2}(p_{i})"}, {"title": "4.2 Results on Vanilla GPT2", "content": "Results of compression of the first 10000 characters of 8 works per author for different authors are presented below and compared against the GZIP baseline. The context length is 100 characters. The compression ratio for the Gzip baseline is approximately 2, whereas that of the neural predictive compression model based on gpt2 (124 M) is close to 3.5 which is roughly 1.57 times that of GZIP."}, {"title": "4.3 Traditional fine-tuned GPT2 on a domain corpora", "content": "Fine-tuning the model entails adjusting its weights and biases to optimize performance for a specific task using a targeted dataset. In this study, we fine-tune a raw GPT-2 model on a corpus consisting of 15 books by a single author, using a learning rate of 5e-7. We then evaluate the model's compression performance on texts by both the same author and different authors to assess its generalization capability.\nFine-tuned GPT2 performs better than raw GPT2 and improves compression by nearly 8%. While compression also improves by at least 3% for works of other authors. This suggests that some degree of style adaptation may occur during fine-tuning. Given that GPT-2 was originally trained on a diverse dataset of 8 million web pages spanning various genres, fine-tuning it on storybooks appears to have enhanced its ability to capture patterns specific to narrative writing. As a result, this may have contributed to improved compression performance in out-of-distribution (OOD) inference, though further analysis is needed to fully understand the impact."}, {"title": "4.4 Knowledge Distilled GPT2 on a domain corpora", "content": "Knowledge distillation is a process of training a smaller machine learning model to replicate the behavior of a large complex model. There have been SOTA transformer models whose distilled version is made open source, examples include BERT and distilBERT. The goal is to transfer the knowledge from a large and complex teacher model to a small and compact student model to reduce the computing requirements.\nWe obtain the results obtained from distilled gpt2 in which the teacher model is gpt2-xl(1.5 B) and student model is gpt2(124 M). We perform knowledge distillation on books written by a specific author and perform compression on the works of the same author and other authors and compare it with gpt2 baselines. We train the model on a learning rate of 5e-7 and obtain the best checkpoint for model evaluation.\nResults validate that the distilled gpt2 performs better in compression against standard gpt2 baselines on the works of the same author hinting at the efficiency of fine tuning. It is also observed that distilled gpt2 performs better with works of other authors but not to the same level as that of the original author validating that there is some degree of style transfer during knowledge distillation.\nWhile traditional style transfer has been shown to improve compression in some cases, we encountered an instance where no significant improvement was observed when attempting to transfer an author's style. We distilled GPT-2 on the TinyShakespeare dataset and tested compression on the works of Zane Grey and Rudyard Kipling. The results indicate that the distinct differences in writing style between Shakespeare and these later-period authors likely contributed to the minimal improvement or deterioration (in the second case) in compression performance. This highlights the complexity of style transfer in relation to compression across diverse literary styles."}, {"title": "4.5 Compression across different LLMs", "content": "We perform two lines of comparison here. Firstly, we measure how compression varies with size of the model. We compare the compression performance of gpt2 (124 M), gpt2-medium (335), gpt2-large (774 M), gpt2-xl(1.5 billion) on 8 books of 4 different authors against gzip baseline. From the results, it's very clear that the larger the model, the better the compression results which validates that LLMs with a huge number of parameters are better predictors. The biggest GPT2 variant improves gzip compression by 72%."}, {"title": "4.6 Compression Performance with Brotli", "content": "With Brotli [15] compression algorithm the compression ratio improved significantly, this is because Brotli utilises a dynamic dictionary based algorithm and works adaptively with the input. It outperforms Gzip's compression ratio.\nTo evaluate the performance of Brotli we compressed first 1 lakh characters of Alice in Wonderland from the Gutenberg corpus. In the next experiment we distilled the gpt2-xl (1.5 billion) model and transferred its knowledge to GPT2 (124 million) trained on a part of the Alice in Wonderland book for 50 epochs. We tested compression performance on gpt2 (raw), gpt2-xl(teacher model) and distilled gpt2(student model)."}, {"title": "4.7 Compression on multi-lingual text", "content": "In this experiment, we compressed French and Hindi text from the OPUS Corpora 10. The input size was 100,000 characters with a context length of 100 characters. For French text, raw GPT-2 showed minimal improvement over the gzip baseline. However, with Hindi text, predictive compression using GPT-2 failed, resulting in expansion rather than compression. This performance is likely due to two factors: the tokenized length of Hindi characters is longer than French or English, and GPT-2's training data was predominantly English. \nWe further tested the compression capabilities of fine-tuned GPT-2 models on French 11 and Hindi 12 text. The results showed improved compression performance, supporting the claim that domain-specific compression outperforms the vanilla GPT-2. The improvement was more significant for Hindi text, likely due to the greater syntactic differences between Hindi and English compared to the closer similarities between French and English."}, {"title": "5 Inferences", "content": "1. Compression ratio improves by at least 57% in neural networks based predictive compression method when compared against the GZIP standards.\n2. On fine-tuning and Knowledge distillation compression performance improves by at least 10% upto 10000 runs. With a higher amount of training data and a larger number of epochs improvement close to 50% is expected as can be seen in the Alice in Wonderland case.\n3. When multi-lingual text are analysed, vanilla GPT2 performs better compression in English text when compared to French and Hindi text. Fine-tuning improves compression performance on French and Hindi text.\n4. Compression performance depends on the size of the input text, on average neural compression compresses the original file size by 3.6x using the biggest model GPT2-xl.\n5. Brotli compression mechanism is more efficient than GZIP mechanism and improves the neural compression by nearly 20%.\n6. Time complexity of neural compression process is roughly proportional to the model size. Compression ratio increases monotonically with model size.\n7. Compression ratio of neural compression method does not solely depends on the size of the file. It is an interplay of entropy, tokenized length and the content of the file."}, {"title": "6 Conclusion and Future Work", "content": "In conclusion, our experiments demonstrate that rank-based predictive compression offers a more effective approach compared to traditional information-theoretic methods, particularly when leveraging context through fine-tuned or knowledge-distilled models. These techniques show promise in achieving better compression ratios, highlighting the potential of predictive compression in diverse applications. While our study focused on a subset of open-source large language models, future work could expand this scope by incorporating models (such as LLaMa and Zephyr), and further comparing them with the GPT-2 family. Future research can also include developing suitable information theoretic compressors that complement the neurally predicted ranks in reducing redundancies. As advancements in compression algorithms like Brotli and Burrows-Wheeler align with ongoing developments in neural networks, predictive compression stands poised for substantial growth. The continuous evolution of machine learning techniques presents opportunities for refining and optimizing compression strategies, marking this as a dynamic area for future research."}]}