{"title": "Federated Smoothing Proximal Gradient for Quantile Regression with Non-Convex Penalties", "authors": ["Reza Mirzaeifard", "Diyako Ghaderyan", "Stefan Werner"], "abstract": "Abstract-Distributed sensors in the internet-of-things (IoT) generate vast amounts of sparse data. Analyzing this high-dimensional data and identifying relevant predictors pose substantial challenges, especially when data is preferred to remain on the device where it was collected for reasons such as data integrity, communication bandwidth, and privacy. This paper introduces a federated quantile regression algorithm to address these challenges. Quantile regression provides a more comprehensive view of the relationship between variables than mean regression models. However, traditional approaches face difficulties when dealing with nonconvex sparse penalties and the inherent non-smoothness of the loss function. For this purpose, we propose a federated smoothing proximal gradient (FSPG) algorithm that integrates a smoothing mechanism with the proximal gradient framework, thereby enhancing both precision and computational speed. This integration adeptly handles optimization over a network of devices, each holding local data samples, making it particularly effective in federated learning scenarios. The FSPG algorithm ensures steady progress and reliable convergence in each iteration by maintaining or reducing the value of the objective function. By leveraging nonconvex penalties, such as the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD), the proposed method can identify and preserve key predictors within sparse models. Comprehensive simulations validate the robust theoretical foundations of the proposed algorithm and demonstrate improved estimation precision and reliable convergence.", "sections": [{"title": "I. INTRODUCTION", "content": "The internet-of-things (IoT) landscape has been transformed by the adoption of cyber-physical systems characterized by numerous distributed devices and sensors that revolutionize data collection and decision-making processes. In this decentralized environment, traditional methods relying on centralized data aggregation are impractical due to substantial computational, energy, and bandwidth demands, as well as significant privacy concerns [1,2]. Federated learning (FL) offers a solution by enabling collaborative model training across edge devices, reducing privacy risks and the need for centralized data storage [3,4].\nDespite these advancements, FL struggles with outlier data, particularly from heavy-tailed distributions, which can distort learning outcomes and undermine model reliability [5,6]. This highlights the need for robust approaches like quantile regression [7], which analyzes predictor-response relation-ships across different data quantiles. Quantile regression is particularly beneficial in applications such as wind power prediction [8], uncertainty estimation in smart meter data [9], and load forecasting in smart grids [10], where it manages data variability and intermittency, enhancing stability and efficiency.\nSparse regression techniques are crucial for managing com-plex IoT data, as they efficiently process vast, heteroge-neous data from distributed sensors, providing robust solutions across various fields [11,12]. For instance, in genetics, they help elucidate quantitative traits [13]; in bioinformatics, they refine gene selection in microarray studies [14]; in finance, they enhance risk management models [15]; and in ecology, they clarify relationships between environmental factors and species distribution [16]. To further improve adaptability and efficacy within federated learning, sophisticated penalization methods like the minimax concave penalty (MCP) [17] and smoothly clipped absolute deviation (SCAD) [18] could be integrated. Despite their non-convex and non-smooth nature, MCP and SCAD have some advantages that make them interesting alternatives to the traditional l1-penalty, e.g., they selectively shrink coefficients, effectively reducing the bias and adapting more flexibly across the sparsity spectrum of models [19]\u2013[21].\nHowever, despite numerous available optimization tech-niques for l1-penalized quantile regression, including sub-gradient methods, primal-dual approaches, and the alternating direction method of multipliers (ADMM) which have established a solid framework for addressing sparsity in centralized settings [22]\u2013[25], the optimization techniques for SCAD or MCP penalized models remains limited. Initially, techniques such as majorization-minimization (MM) and local linear approximation (LLA) have been employed to construct and solve surrogate convex functions that approximate the original non-convex problems [26,27]. Although these methods facilitate the optimization process, they often involve secondary convergence iterations within each loop, which can result in slower overall convergence and potential precision losses. Building on these concepts, the recently proposed sub-gradient algorithm specifically addresses weakly convex functions and offers a more streamlined approach to handling non-convex penalties in quantile regression [28]. A recent innovation in this field is the Single-loop Iterative ADMM (SIAD) algo-"}, {"title": "II. PRELIMINARIES", "content": "This section defines concepts and notations necessary for deriving the federated smoothing proximal gradient (FSPG) algorithm in Section III. In particular, we briefly review sparse quantile regression utilizing non-convex penalties for sparsity and outlier handling, and smoothing approximations aiding gradient-based optimization for non-smooth objectives.\nConsider a scalar variable Y and a predictor vector x of dimension P. The conditional cumulative distribution func-tion is defined as Fy(y|x) = P(Y \u2264 y|x). For a given \u03c4 \u2208 (0,1), the \u03c4th conditional quantile, QY(\u03c4|x), is given by Qy(\u03c4|x) = inf{y : Fy(y|x) > \u03c4}. The linear model for quantile regression relates QY(\u03c4|x) to x \u2208 RP as follows [36]:\n$Q_{Y}(\\tau|x) = x^T \\beta_{\\tau} + q$"}, {"title": "A. Sparse Quantile Regression Framework", "content": "where \u03b2\u03c4 \u2208 RP represents the coefficients of the regression model, and q\u2208 R is the \u03c4th quantile of the error term, both being unknown and require estimation.\nWith a dataset consisting of n pairs {xi, Yi}i=1 and a chosen \u03c4, we can estimate the model parameters by solving the following optimization problem [36]:\n$\\hat{w} = arg min_{w} \\frac{1}{n}\\sum_{i=1}^{n} \\rho_{\\tau}(Y_i-x^T w)$", "latex": ["\\hat{w} = arg min_{w} \\frac{1}{n}\\sum_{i=1}^{n} \\rho_{\\tau}(Y_i-x^T w)"]}, {"title": "B. Smoothing Approximation", "content": "Handling non-smooth functions in optimization presents significant challenges, primarily as we cannot exploit the ben-eficial properties of smooth functions, such as gradient-based convergence guarantees. A common strategy to overcome this limitation is to employ smoothing techniques, which involve replacing non-smooth functions with smooth approximations. These smoothed functions are easier to optimize, particularly within the proximal gradient framework we propose in this paper.\nWe begin by introducing a smoothing function, which approximates a non-smooth function g with a smooth sur-rogate g\u0303, thereby facilitating a more streamlined optimization trajectory.\nDefinition 1 [38]: Let g\u0303: \u03a9 \u2286 Rm \u00d7 (0, +\u221e) \u2192 R serve as a smoothing approximation of g, with g : \u03a9 \u2286 Rm \u2192 R exhibits local Lipschitz continuity. The smoothing function g\u0303 satisfies the following properties:\n1) Differentiability: g\u0303(\u00b7,\u00b5) is continuously differentiable over Rm for any \u00b5 > 0. Additionally, for every x \u2208 \u03a9, g\u0303(x,\u00b7) is differentiable over (0, +\u221e].\n2) Convergence Criterion: For every x \u2208 \u03a9, lim\u00b5\u21920+ g\u0303(x, \u00b5) = g(x).\n3) Bound on Gradient: There exists a positive constant Kg\u0303 such that |\u2207\u00b5g\u0303(x, \u00b5)| \u2264 Kg\u0303 for all \u00b5\u2208 (0,+\u221e) and x\u2208\u03a9.\n4) Gradient Convergence: The condition limz\u2192x,\u00b5\u21920 \u2207zg\u0303(z, \u00b5) \u2286 \u2202g(x) is satisfied. Moreover, for each x \u2208 Rm, the smoothing function g\u0303 upholds:"}, {"title": "III. FEDERATED SMOOTHING PROXIMAL GRADIENT FOR PENALIZED QUANTILE REGRESSION", "content": "In order to address the challenges arising from the lack of Lipschitz differentiability in our objective function, we here introduce the federated smoothing proximal gradient (FSPG) algorithm. This algorithm incorporates a dynamically increas-ing penalty parameter to enhance convergence properties. The core of FSPG involves approximating the non-smooth term ||y(l) \u2212 (X(l))Tw||1 with a sum of smooth functions, follow-ing the smoothing approach described in [38]. Specifically, we employ a smoothing approximation function for each term yi \u2212 (X(l))Tw, defined as:\n$\\tilde{g}_{l} (Y_i - (X_i^l)^T w, \\mu) = \\frac{(Y_i - (X_i^l)^T w)^2}{2\\mu}, Y_i - (X_i^l)^T w \\leq \\mu$\n$\\tilde{g}_{l} (Y_i - (X_i^l)^T w, \\mu) = \\frac{(Y_i - (X_i^l)^T w)^2}{2\\mu}, Y_i - (X_i^l)^T w \\leq \\mu$", "latex": ["\\tilde{g}_{l} (Y_i - (X_i^l)^T w, \\mu) = \\frac{(Y_i - (X_i^l)^T w)^2}{2\\mu}, Y_i - (X_i^l)^T w \\leq \\mu", "\\tilde{g}_{l} (Y_i - (X_i^l)^T w, \\mu) = \\frac{(Y_i - (X_i^l)^T w)^2}{2\\mu}, Y_i - (X_i^l)^T w \\leq \\mu"]}, {"title": "IV. CONVERGENCE PROOF", "content": "Establishing the convergence of the proposed proximal gradient algorithm requires validating four essential condi-tions: boundedness, sufficient descent, subgradient bound, and global convergence, as highlighted in [41, Theorem 2.9]. To this end, we start by demonstrating the boundedness of the augmented Lagrangian, formalized in Lemma 1 below.\nLemma 1. The function \u03a6\u03c3\u03b9 (w, w', \u00b5) = \u2211l=1 \u011f\u0131(w', \u00b5) + \u03b7P\u03bb,\u03b3(w) + \u03c3 \u2225w \u2212 w\u2032\u222522 is lower bounded."}, {"title": "V. SIMULATION RESULTS", "content": "In this section, we present an extensive simulation study to evaluate the efficacy of our federated smoothing proximal gradient (FSPG) algorithm in the context of sparse quantile regression. We compare the FSPG with leading contemporary approaches, including the proximal gradient descent with increasing penalty (PGD) [35] and the sub-gradient method (SUB) [33] tailored for federated settings, and the smoothing time-increasing penalty ADMM (SIAD) [34] as a benchmark for centralized scenarios. Our evaluation criteria include con-vergence rate, efficiency in minimizing the mean square error (MSE) across both synthetic and real-world datasets, as well as accuracy in recognizing active and non-active coefficients.\nAdditionally, we compare the performance and convergence dynamics of the FSPG algorithm against the Federated Hu-ber Loss Proximal Gradient (FHPG) a variant of FSPG with static \u00b5and \u03c3parameters focusing on the MSE performance. This examination aims to highlight the nuanced advantages and potential trade-offs inherent in using the FSPG approach to quantile regression."}, {"title": "A. Simulation Setup", "content": "We evaluate the performance of the FSPG algorithm across six distinct simulation scenarios. In these evaluations, the penalty parameters are set to \u03b3SCAD = 3.1, \u03b3MCP = 2.4, \u03b2 = 4, and c ="}, {"title": "B. Results", "content": "In the first scenario, we assessed the algorithms based on their convergence speed and MSE. The learning curves, which plot MSE against the number of iterations for different values of \u03c4 = {0.55,0.7}, are shown in Figure 1. We see that the FSPG algorithm consistently outperforms existing methods in reducing MSE, regardless of the chosen penalty function and the specific value of \u03c4. Moreover, the FSPG algorithm demonstrates a faster convergence rate compared to the FPG and SUB algorithms and achieves convergence speeds comparable to those of the SIAD algorithm."}]}