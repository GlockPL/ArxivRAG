{"title": "Multi-agent cooperation through learning-aware policy gradients", "authors": ["Alexander Meulemans", "Seijin Kobayashi", "Johannes von Oswald", "Nino Scherrer", "Eric Elmoznino", "Blake Richards", "Guillaume Lajoie", "Blaise Ag\u00fcera y Arcas", "Jo\u00e3o Sacramento"], "abstract": "Self-interested individuals often fail to cooperate, posing a fundamental challenge for multi-agent learning. How can we achieve cooperation among self-interested, independent learning agents? Promising recent work has shown that in certain tasks cooperation can be established between learning-aware agents who model the learning dynamics of each other. Here, we present the first unbiased, higher-derivative-free policy gradient algorithm for learning-aware reinforcement learning, which takes into account that other agents are themselves learning through trial and error based on multiple noisy trials. We then leverage efficient sequence models to condition behavior on long observation histories that contain traces of the learning dynamics of other agents. Training long-context policies with our algorithm leads to cooperative behavior and high returns on standard social dilemmas, including a challenging environment where temporally-extended action coordination is required. Finally, we derive from the iterated prisoner's dilemma a novel explanation for how and when cooperation arises among self-interested learning-aware agents.", "sections": [{"title": "1. Introduction", "content": "From self-driving autonomous vehicles to personalized assistants, there is a rising interest in developing agents that can learn to interact with humans (Collins et al., 2024; Gweon et al., 2023), and with each other (Park et al., 2023; Vezhnevets et al., 2023). However, multi-agent learning comes with significant challenges that are not present in more conventional single-agent paradigms. This is perhaps best seen through the study of \u2018social dilemmas', general-sum games which model the tension between cooperation and competition in abstract form (von Neumann and Morgenstern, 1947). Without further assumptions, letting agents independently optimize their individual objectives on such games results in poor outcomes and a lack of cooperation (Claus and Boutilier, 1998; Tan, 1993).\nOptimal single-agent behavior can be formalized through the theory of stochastic optimal control on Markov decision processes (Bellman, 1957; \u00c5str\u00f6m, 1965), which provides a basis for reinforcement learning (Sutton and Barto, 2018). The independent multi-agent learning problems that we study here are more complicated. First, for general-sum games, reaching an equilibrium point does not necessarily imply appropriate behavior because there can be many sub-optimal equilibria (Fudenberg and Levine, 1998; Shoham and Leyton-Brown, 2008). Second, the control problem an agent faces is non-stationary from its own viewpoint, because other agents themselves simultaneously learn and adapt (Hernandez-Leal et al., 2017). Centralized training algorithms sidestep non-stationarity issues by sharing agent information (Sunehag et al., 2017), but this transformation into a global learning problem is usually prohibitively costly, and impossible to implement when agents must be developed separately (Zhang et al., 2021).\nThe above two fundamental issues have hindered progress in multi-agent reinforcement learning, and have limited our understanding of how self-interested agents may reach high returns when faced with social dilemmas. In this paper, we join a promising line of work on \u2018learning awareness' (Foerster et al.,"}, {"title": "2. Background and problem setup", "content": "We consider partially observable stochastic games (POSGs; Kuhn, 1953) consisting of a tuple $(I, S, A, P_t, P_r, P_i, O, P_O, \\gamma, T)$ with $I = {1, . . ., n}$ a finite set of $n$ agents, $S$ the state space, $A = \\times_{i \\in I}A^i$ the joint action space, $P_t(S_{t+1} | S_t, A_t)$ the state transition distribution, $P_i(S_0)$ the initial state distribution, $P_r = \\times_{i \\in I}P(R | S, A)$ the joint factorized reward distribution with $R = {R}^i_{i \\in I}$ and bounded rewards $R^i$, $O = \\times_{i \\in I}O^i$ the joint observation space, $P_O(O_t | S_t, A_{t-1})$ the observation distribution, $\\gamma$ the discount factor, $t$ the time step, and $T$ the horizon. We use superscript $i$ to indicate agent-specific actions, observations and rewards, $-i$ to indicate all agent indices except $i$, and we omit the superscript for joint actions, observations and rewards. As agents only receive partial state information, they benefit from conditioning their policies $\\pi^i(a | x; \\phi^i)$ on the observation history $x = {o}_{t=1}^t$ (Kaelbling et al., 1998; \u00c5str\u00f6m, 1965), with $\\phi^i$ the policy parameters. Note that the observations can contain the agent's actions on previous timesteps."}, {"title": "2.1. General-sum games and their challenges", "content": "We focus on general-sum games, where each agent has their own reward function, possibly different from those of other agents. Analyzing and solving general-sum games while letting every agent individually and independently maximize their rewards (a setting often referred to as \u201cfully-decentralized reinforcement learning\u201d; Albrecht et al., 2024) is a longstanding problem in the fields of machine learning and game theory. Alongside the difficulties of standard reinforcement learning, this setting comes with two additional significant issues that we briefly review below."}, {"title": "Non-stationarity of the environment.", "content": "In a general-sum game, each agent aims to maximize its expected return $J^i (\\phi^i, \\phi^{-i}) = [E_{P^{\\phi^i,\\phi^{-i}}} [\\sum_{t=1}^T R^i]]$, with $P^{\\phi^i,\\phi^{-i}}$ the distribution over environment trajectories $x^i$ induced by the environment dynamics, the policy $\\pi^i(a^i | x^i; \\phi^i)$ of agent $i$, and the policies $\\pi^{-i}$ of all other agents. Importantly, the expected return $J^i (\\phi^i, \\phi^{-i})$ does not only depend"}, {"title": "Equilibrium selection.", "content": "It is not clear how to identify appropriate policies for a general-sum game. To see this, let us first briefly revisit the concept of a Nash equilibrium (Nash Jr., 1950). For a fixed set of co-player policies $\\phi^{-i}$, one can compute a best response, which for agent i is given by $\\phi^{*i} = arg max_{\\phi^i} J^i (\\phi^i, \\phi^{-i})$. When all current policies $\\phi^i$ are a best response against each other, we have reached a Nash equilibrium, where no agent is incentivized to change its policy anymore, $\\forall i, \\phi^i : J^i (\\phi^i, \\phi^{-i}) \\leq J^i(\\phi^{*i}, \\phi^{-i})$. Various \u2018folk theorems' show that for most POSGs of decent complexity, there exist infinitely many Nash equilibria (Fudenberg and Levine, 1998; Shoham and Leyton-Brown, 2008). This lies at the origin of the equilibrium selection problem in multi-agent reinforcement learning: it is not only important to let a multi-agent system converge to a Nash equilibrium, but also to target a good equilibrium, as Nash equilibria can be arbitrarily bad. Famously, unconditional mutual defection in the infinitely iterated prisoner's dilemma is a Nash equilibrium, with strictly lower expected returns for all agents compared to the mutual tit-for-tat Nash equilibrium (Axelrod and Hamilton, 1981). While reviewing them is out of the scope of the current paper, we note that there are various approaches to define the notion of good equilibria, such as high global welfare, Pareto optimality, and low inequality."}, {"title": "2.2. Co-player learning awareness", "content": "We aim to address the above two major challenges of multi-agent learning in this paper. Our work builds upon recent efforts that are based on adding a meta level to the multi-agent POSG, where the higher-order variable represents the learning algorithm used by each agent (Balaguer et al., 2022; Khan et al., 2024; Lu et al., 2022). In this meta-problem, the environment includes the learning dynamics of other agents. At the meta-level, one episode now extends across multiple episodes of actual game play, allowing the 'ego agent', i, to observe how its co-players, \u2013i, learn, see Fig. 1. The goal of this meta-agent may be intuitively understood as that of shaping co-player learning to its own advantage.\nProvided that co-player learning algorithms remain constant, the above reformulation yields a single-agent problem that is amenable to standard reinforcement learning techniques. This setup is fundamentally asymmetric: while the meta agent (ego agent) is endowed with co-player learning awareness (i.e., observing multiple episodes of game play), the remaining agents remain oblivious to the fact that the environment is non-stationary. We thus refer to them here as naive agents (see Fig. 1B). Despite this asymmetry, prior work has observed that introducing a learning-aware agent in a group of naive learners often leads to better learning outcomes for all agents involved, avoiding mutual defection equilibria (Balaguer et al., 2022; Khan et al., 2024; Lu et al., 2022). Moreover, Foerster et al. (2018a) has shown that certain forms of learning awareness can lead to the emergence of cooperation even in symmetric cases, a surprising finding that is not yet well understood.\nThese observations motivate our study, leading us to derive novel efficient learning-aware reinforcement learning algorithms, and to investigate their efficacy in driving a group of agents (possibly composed of both meta and naive agents) towards more beneficial equilibria. Below, we proceed by first formalizing asymmetric co-player shaping problems, which we solve with a novel policy gradient algorithm (Section 3). In Section 4, we then return to the question of why and when co-player learning awareness can result in cooperation in multi-agent systems with equally capable agents."}, {"title": "Co-player shaping.", "content": "Following Lu et al. (2022), we first introduce a meta-game with a single meta-agent whose goal is to shape the learning of naive co-players to its advantage. This meta-game is defined formally as a single-agent partially observable Markov decision process (POMDP) ($\\check{S}, \\tilde{A}, P_t, P_r, \\tilde{P}_i, \\tilde{O}, \\tilde{\\gamma}, M)$. The meta-state consists of the policy parameters $\\phi^{-i}$ of all co-players together with the agent's own parameters $\\phi^i$. The meta-environment dynamics represent the fixed learning rules of the co-players, and the meta-reward distribution represents the expected return $J^i(\\phi^i, \\phi^{-i})$ collected by agent i during an inner episode, with \u2018inner' referring to the actual game being played. The initialization distribution $\\tilde{P}_i$ reflects the policy initializations of all players. Finally, we introduce a meta-policy $\\pi(a_{m+1} | \\Phi, \\Phi^2; \\theta)$ parameterized by $\\theta$, that decides the update to the parameter $\\alpha_m$ (the meta-action) to shape the co-player learning towards highly rewarding regions for agent i over a horizon of M meta steps. This leads to the co-player shaping problem\n$\\max_\\mu E_{P_\\theta^\\mu}  [\\sum_{m=1}^M J(\\phi_m, \\phi_m^\\iota)] (1)$\nwith $P_\\theta^\\mu$ the distribution over parameter trajectories induced by the meta-dynamics and meta-policy."}, {"title": "2.3. Single-level co-player shaping by leveraging sequence models", "content": "In this paper, we combine both inner- and meta-policies in a single long-context policy, conditioning actions on long observation histories spanning multiple inner game episodes (see Fig. 1B). Instead of hand-designing the co-player learning algorithms, we instead let meta-learning discover the algorithms used by other agents. This way, we leverage the in-context learning and inference capabilities of modern neural sequence models (Aky\u00fcrek et al., 2023; Brown et al., 2020; Li et al., 2023; Rabinowitz, 2019; von Oswald et al., 2023) to both simulate in-context an inner policy, as well as strategically update it based on current estimates of co-player policies. This philosophy has been adopted in Khan et al. (2024), in which a flat policy is optimized using an evolutionary algorithm. We compare to this method in Section 3.1, after we derive our meta reinforcement learning algorithm.\nTo proceed with this approach, we must first reformulate the meta-game. In particular, we must deal with a difficulty that is not present in single-agent meta reinforcement learning (e.g., Duan et al., 2017; Wang et al., 2016), which stems from the fact that co-players generally update their own policies based on multiple inner episodes (\u2018minibatches'), without which reinforcement learning cannot practically make progress. Here, we solve this by defining the environment dynamics over B parallel trajectories, with B the size of the minibatch of inner episode histories that co-players use to update their policies at each inner episode boundary (see Fig. 1A)."}, {"title": "Batched co-player shaping POMDP.", "content": "We define the batched co-player shaping POMDP $(\\hat{S}, \\bar{A}, P_t, P_r, \\hat{P}_i, \\tilde{O}, \\bar{\\gamma}, M, B)$, with hidden states consisting of the hidden environment states of the $B$ ongoing inner episodes, combined with the current parameters $\\phi^{-i}$ of all co-players; environment dynamics $\\bar{P}_t$ simulating $B$ environments in parallel, combined with updating the co-player's policy parameters $\\phi^{-i}$, and resetting the environments at each inner episode boundary; initial state distribution $\\hat{P}_i$ that initializes the co-player policies and initializes the environments for the first inner episode batch; and finally, an ego-agent policy $\\pi^i(\\bar{a} | \\bar{h}; \\phi^i)$ parameterized by $\\phi^i$, which determines a distribution over the batched action $\\bar{a} = {\\alpha^b}_{b=1}^B$, based on the batched long history $\\bar{h} = {h^b}_{b=1}^B$. We refer to each element of the latter as a long history $h^b$, with long time index $l$ running across multiple episodes, from $l = 1$ until $l = MT$. It should be contrasted to the inner episode history $x^i$, which runs from $t = 1$ to $t = T$ and thus only reflects the current (inner) game history.\nThe POMDP introduced above suggests using a sequence policy $\\pi^i(\\bar{a} | \\bar{h}; \\phi^i)$ that is aware of the full minibatch of long histories and which produces a joint distribution over all current actions in the minibatch. However, as we aim to use our agents not only to shape naive learners, but also to play against/with each other, we require a policy that can be used both in a batch setting with naive learners, and in a single-trajectory setting with other learning-aware agents. Within our single-level approach, we achieve this by factorizing the batch-aware policy $\\bar{\\pi}(\\bar{a} | \\bar{h}; \\phi^i)$ into $B$ independent policies with shared parameters $\\phi^i$,\n$\\bar{\\pi}(\\bar{a} | \\bar{h}; \\phi^i) = \\prod_{b=1}^B \\pi^i (a | h; \\phi^i). (2)$\nThanks to the batched POMDP, we can now pose co-player shaping as a standard (single-level, single-agent) expected return maximization problem:\n$\\max_\\phi^i E_{\\rho_{\\phi^i}} [\\frac{1}{B} \\sum_{b=1}^B \\sum_{l=0}^{MT} R^{i,b}_l]. (3)$\nThis formulation is the key for obtaining an efficient policy gradient co-player shaping algorithm."}, {"title": "3. Co-agent learning-aware policy gradients", "content": ""}, {"title": "3.1. A policy-gradient for shaping naive learners", "content": "We now provide a meta reinforcement learning algorithm for solving the co-player shaping problem stated in Eq. 3 efficiently. Under the POMDP introduced in the previous section, co-player shaping becomes a conventional expected return maximization problem. Applying the policy gradient theorem (Sutton et al., 1999) to Eq. 3, we arrive at COALA-PG (co-agent learning-aware policy gradients, c.f. Theorem 3.1): a policy-gradient method compatible with shaping other reinforcement learners that base their own policy updates on minibatches of experienced trajectories.\nTheorem 3.1. Take the expected shaping return $J(\\phi^i) = E_{\\rho_{\\phi^i}} [\\frac{1}{B} \\sum_{b=1}^B \\sum_{l=0}^{MT} R^{i,b}]$, with $\\rho_{\\phi^i}$ the distribution induced by the environment dynamics $\\tilde{P}_t$, initial state distribution $\\hat{P}_i$ and policy $\\phi^i$. Then the policy gradient of this expected return is equal to\n$\\nabla_\\phi i J(\\phi^i) = E_{\\rho_{\\phi^i}} [\\sum_{b=1}^B \\sum_{l=1}^{MT} \\nabla_{\\phi^i} \\log \\pi^i (a^b | h^b) \\sum_{b'=1}^B \\sum_{k=l}^{MT} R^{i,b'}]. (4)$\nWe provide a proof in Appendix C. There are two important differences between COALA-PG and naively applying policy gradient methods to individual trajectories in a batch. (i) Each gradient term for an individual action $a^b$ takes into account the future return averaged over the whole minibatch, instead of the future return along trajectory b (see Fig. 2). This allows taking into account the influence of this action on the parameter update of the naive learner, which influences all trajectories in the minibatch. (ii) Instead of averaging the policy gradients for each trajectory in the batch, COALA-PG accumulates (sums) them. This is important, as otherwise the learning signal would vanish in the limit of large minibatches. Intuitively, when a naive learner uses a large minibatch for its updates, the effect of a single action on the naive learner's update is small, creating the need to sum the small effects together to obtain a policy gradient with a magnitude that does not depend on batch size. We will later show experimentally in Section 6 that correct treatment of minibatches critically affects reinforcement learning performance.\nThe expectation appearing in the policy gradient expression must be estimated in practice. To reduce gradient estimation variance, we use value function baselines in our experiments, and sample a meta-batch of B batched trajectories from $E_{\\rho_{\\phi^i}}$.\nRelationship to prior shaping methods. We now contrast our policy gradient algorithm to two closely related methods, M-FOS (Lu et al., 2022) and Shaper (Khan et al., 2024). Like COALA-PG, M-FOS is a model-free meta reinforcement learning method. Unlike the approached followed here, though, it aims to solve the bilevel co-player shaping problem of Eq. 1, treating meta- and inner-policy networks separately. Moreover, the M-FOS parameter update is not derived as the policy gradient on the batched co-player shaping POMDP introduced above, and current-episode returns are overemphasized compared to future-episode returns (see Appendix E). This leads to a biased parameter update, which results in learning inefficiencies. We comment on other existing bilevel shaping methods in Appendix D.\nKhan et al. (2024) adopt a single-level sequence policy for their Shaper algorithm, as we do here, but then resort to black-box evolution strategies (Rechenberg and Eigen, 1973) to learn the policy. Obtaining an efficient meta reinforcement learning algorithm from a POMDP applicable to such single-level policies is thus our key distinguishing contribution. The unbiased policy gradient property of our learning rule translates in practice onto learning speed and stability gains, as we will see in the experiments reported in Section 6."}, {"title": "4. Why is learning awareness beneficial on general-sum games?", "content": "We have established that co-player shaping can be cast as a single-agent reward maximization problem whenever there is a single learning-aware player amongst a group of learners that are otherwise naive. This allowed us to derive a policy gradient shaping method. However, such an asymmetric setup cannot in general be taken for granted. In our experimental analyses, we therefore consider the more realistic scenario where equally-capable, learning-aware agents try to shape each other.\nAs reviewed in Section 2.2, prior work has shown that learning-awareness can result in better outcomes in general-sum games, but the origin and conditions for the occurrence of this phenomenon are not yet well understood. Here, we shed light on this question by analyzing the interactions of agents with varying degrees of learning-awareness in an analytically tractable matrix game setting. This leads us to uncover a novel explanation for the emergence of cooperation in general-sum games."}, {"title": "4.1. The iterated prisoner's dilemma", "content": "We focus on the infinitely iterated prisoner's dilemma (IPD), the quintessential model for understanding the challenges of cooperation among self-interested agents (Axelrod and Hamilton, 1981; Rapoport, 1974). The game goes on for an indefinite number of rounds, where for each round of play two players (i = 1, 2) meet and choose between two actions, cooperate or defect, a \u2208 {c,d}. The rewards collected as a function of the actions of both agents are shown in Table 1. These four rewards are set so as to create a social dilemma. When the agents meet only once, mutual defection is the only Nash equilibrium; self-interested agents thus end up obtaining low reward. In the infinitely iterated variant of the game, there exist Nash equilibria involving cooperative behavior, but these are notoriously hard to converge to through self-interested reward maximization.\nWe model each agent through a tabular policy \u03c0^i (a | x^i; \u03c6^i) that depends only on the previous action of both agents, $x = (a_{i-1}, a_{2_{-1}})$. Their behavior is thus fully specified by five parameters, which determine the probability of cooperating in response to the four possible previous action combinations together with the initial cooperation probability. For this game, the discounted expected return $J^i (\\phi^1, \\phi^2)$ can be calculated analytically. We exploit this property and optimize policies by performing exact gradient ascent on the expected return, with gradients computed by automatic differentiation (c.f. Appendix B for details). Results can be found in Fig. 3."}, {"title": "4.2. Explaining cooperation through learning awareness", "content": "Based on the experimental results reported in Fig. 3, we now identify three key findings that establish how learning awareness enables cooperation to be reached in the iterated prisoner's dilemma:\nFinding 1: Learning-aware agents extort naive learners. We first pit naive against learning-aware agents. We find that the latter develop extortion policies which force naive learners onto unfair cooperation, in the spirit of the zero-determinant extortion strategies discovered by Press and Dyson (2012). Even when a learning-aware agent is initialized at pure defection, maximizing the shaping objective of Eq. 3 lets it escape mutual defection (see Fig. 3A).\nFinding 2: Extortion turns into cooperation when two learning-aware players face each other. After developing extortion policies against naive learners (grey shaded area in Fig. 3B), we then let two learning-aware agents (C1 and C2 in Fig. 3B) play against each other after. We see that optimizing the co-player shaping objective turns extortion policies into cooperative policies. Intuitively, under independent learning, an extortion policy shapes the co-player to cooperate more. We remark that the same occurs if learning-aware agents play against themselves (self-play; data not shown). This"}, {"title": "Finding 3: Cooperation emerges within groups of naive and learning-aware agents.", "content": "Findings 1. and 2. motivate studying learning in a group containing both naive and learning-aware agents, with every agent in the group trained against each other. This mixed group setting yields a sum of two distinct shaping objectives, which depend on whether the agent being shaped is learning-aware or naive. The gradients resulting from playing against naive learners pull against mutual defection and towards extortion, while those resulting from playing against other learning-aware agents push away from extortion towards cooperation. Balancing these competing forces leads to robust cooperation, see Fig. 3C (left). By contrast, a pure group of learning-aware agents cannot escape mutual defection, see Fig. 3C (right). This can be explained by the fact that the agents can no longer observe other players learning, and must deal again with a non-stationary problem. The resulting gradients do not therefore contain information on the effects of unconditional defection on the future strategies of co-players, or that policies in the vein of tit-of-tat can shape co-players towards more cooperation.\nOur analysis thus reveals a surprising path to cooperation through heterogeneity. The presence of short-sighted agents that greedily maximize immediate rewards turns out to be essential for full cooperation to be established among far-sighted, learning-aware agents."}, {"title": "5. Revisiting Learning with Opponent-Learning Awareness", "content": "We now leverage our results from Sections 3-4 to explain how and when cooperation arises in Learning with Opponent-Learning Awareness (LOLA; Foerster et al. (2018a)), the seminal work that spearheaded the learning awareness field. Furthermore, we will use COALA-PG to derive a new LOLA gradient estimator that does not require higher-order derivative estimates.\nLOLA considers a POSG (c.f. Section 2.2) with agent policies (\u03c6\u00b9, \u03c6\u00af\u00b9) and expected return J^i (\u03c6\u00b9, \u03c6\u00af\u2170). Instead of estimating the naive gradients \u2207\u03c6\u00a1J^i (\u03c6^i, \u03c6\u00af\u00b9), LOLA anticipates that the co-players update their parameters with M naive gradient steps, and estimates the total gradient through this look-ahead objective:\n$\\nabla_{\\phi^i}^{LOLA} = \\frac{d}{d \\phi^i}  [J(\\phi, \\phi^{-i} + \\sum_{q=1}^M \\Delta_q \\phi^{-i})] \\frac{d}{d} \\Delta_q \\phi^{-i} = \\alpha _{\\nabla} -iJ \\phi, + J (\\phi^{-i} \\frac{d}{d \\phi^i} + =1 \\alpha_ \\Delta_q \\phi^{-i})] (5)$\nwith$\\frac{d}{d \\phi^i}$ the total derivative taking into account the effect of \u03c6^i on the parameter updates $ \\alpha_q \\phi^{-i}$, and $\\frac{d}{d \\phi^i}$ - the partial derivative. Note that Eq. 5 considers the LOLA-DICE update (Foerster et al., 2018b), an improved version of the LOLA update. In reality, all players update their parameters with the LOLA-DICE update equation 5, hence the look-ahead assumption with naive gradients is inconsistent, sometimes hampering the performance of LOLA (Letcher et al., 2019; Willi et al., 2022)."}, {"title": "5.1. Explaining when and how cooperation arises with LOLA", "content": "In Section 4, we showed that the two main ingredients for learning to cooperate with selfish objectives are (i) observe that one's actions influence the future behavior of others, providing shaping gradients pulling away from defection towards extortion, and (ii), also play against other extortion agents immune to being shaped on the fast timescale, providing gradients pulling away from extortion towards cooperation. We then showed that both ingredients can be combined by training agents in a heterogeneous group containing both naive and learning-aware agents.\nWe can explain the emerging cooperation in LOLA by observing that LOLA also combines both ingredients, albeit in a different manner from the heterogeneous group setting. By considering the look-ahead objective equation 5, LOLA computes gradients to shape naive learners performing M naive gradient steps. Unique to LOLA however, the naive learners in the look-ahead objective equation 5 are initialized with the parameters of the other LOLA agents \u03c6\u00af\u00b9. If only few look-ahead steps are used, the updated naive learner parameters stay close to \u03c6\u00af\u00b9, mimicking playing against other extortion agents resulting in emergent cooperative behavior.\nFigure 4A shows that indeed, LOLA-DICE with few look-ahead steps leads to cooperation on the iterated prisoner's dilemma. However, when more look-ahead steps are used, the naive learner moves too far away from its \u03c6\u00af\u00b9 initialization, removing the second ingredient, leading to defection. In figure 4, we take the policy resulting from LOLA-training with many look-ahead steps, and train a new randomly initialized naive learner against this fixed LOLA policy. The results show that the LOLA policy extorts the naive learner into unfair cooperation, confirming that with many look-ahead steps, only a shaping incentive is present in the LOLA update, resulting in extortion policies. Hence, the low reward in Fig. 4A for LOLA agents with many look-ahead steps does not result from unconditional defection, but instead both LOLA policies trying to extort the other one. Finally, we can improve the performance of LOLA with many look-ahead steps by explicitly introducing ingredient 2 through adding the partial gradient  iJ (\u03c6, \u03c6\u00af\u00b9) to the LOLA update equation 5 (c.f. Fig. 4C)."}, {"title": "5.2. A higher-derivative-free LOLA gradient estimator", "content": "Despite the apparent dissimilarities of the two algorithms, we show in Theorem 5.1 that as a special case of COALA-PG, we can estimate the gradient of a similar objective to the LOLA-DICE method. We consider a mixed agent group of meta agents (\u03c6\u00b9, \u03c6\u00af\u00b9), and naive learners.\nTheorem 5.1. Assuming that (i) the COALA policy is only conditioned on inner episode histories xmt (instead of long histories $h_l^b$), with subscript $m = 1, \u2026 . . , M$ indexing histories over meta-steps, and (ii)"}, {"title": "A. Experimental details", "content": ""}, {"title": "A.1. Environments", "content": ""}, {"title": "A.1.1. Iterated prisoner's dilemma (IPD)", "content": "We model the IPD environment as follows:\n\u2022 State: The environment has 5 states, that we label by s0, (c, c), (c, d), (d, c), (d, d).\n\u2022 Action: Each agent has 2 possible actions: cooperate (c) and defect (d).\n\u2022 Dynamics: Based on the action taken by each agent in the previous time step, the state of the environment is set to the states (a1, a2) where a1, a2 are respectively the previous action of the first and second player in the environment. The assignment of who is first and second is made arbitrarily and fixed.\n\u2022 Initial state: The initial state is always set to s0.\n\u2022 Observation: The agents observe directly the state, modulo a permutation of the tuple to ensure a symmetry of observation. The 5 possible observations are then encoded as one-hot encoding.\n\u2022 Reward: At every timestep, each agents receive a reward following the reward matrix in Table 1"}, {"title": "A.1.2. CleanUp-lite", "content": "CleanUp-lite is a simplified two-player version of the CleanUp game, which is part of the Melting Pot suite of multi-agent environments (Agapiou et al., 2023). It is modelled as follows:\n\u2022 State: The world is a grid of size 3, arranged horizontally. The right extremity is the river, and the left one the orchard. The world state also contains the pollution level of the river P, number of apples currently available in the orchard A, the position of each agent, and their respective zapped state\n\u2022 Action: There exists 4 actions: {move right, move left, zap, do nothing}.\n\u2022 Dynamic: the environment evolves at every timestep in the following order:\n1. The pollution level increases by 1 with a fixed probability Ppollution = 0.5, and capped at Pmax = 5.\n2. The apple grows by 1 with probability 1 \u2013 min(1, P/Pthreshold), where Pthreshold = 3, and capped at Amax = 4.\n3. Then, agent in the \"orchard\" cell who is not zapped, has a probability A/Amax of picking up an apple, independently from one another, in which case the apple level decreases by 1 (capped at 0).\n4. An agent in the \"river\" cell who is not zapped has a probability Pclean = 0.7 of decreasing the pollution level by 1, independently of other agents (capped at 0).\n5. Finally, an agent zapping has a pzap = 0.8 probability of successfully zapping the opponent who is in the same cell. If the zapping is successful, the opponent is frozen for tzap = 3 timesteps, during which it is frozen and cannot be further zapped."}, {"title": "\u2022 Initial state:", "content": "Agents are randomly placed on the grid, unzapped, A is set to 0, P to Pthreshold.\n\u2022 Observation: the observation is fully observable. Each agent sees the position of each agent encoded as concatenated one-hot vectors indicating the position in the grid, the normalized number of apples A/Amax, normalized pollution level P/Pmax, the normalized remaining timeout status (zapped or not), and if so, a countdown indicating how many timestep is of each agent (tagent/tzap where tagent is the time left before the agent unfreezes). The observation is symmetric.\n\u2022 Reward: An agent that picks up an apple receives a reward of rapple = 1 in that timestep."}, {"title": "A.2. Training details", "content": "Here, we describe the procedure that we use in our experiments to train meta agents in an arbitrary mixture of naive and other meta agents (who themselves are learning). A single parameter, pnaive, indicating the probability of encountering a naive agent, controls the heterogeneity of the pool that a meta agents trains against. If pnaive = 1, the meta agents are trained only against naive opponents, and thus the training corresponds to a pure shaping setting. If pnaive = 0, meta agents are only trained against other meta"}]}