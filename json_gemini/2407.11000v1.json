{"title": "Autonomous Prompt Engineering in Large Language Models", "authors": ["Daan Kepel", "Konstantina Valogianni"], "abstract": "Prompt engineering is a crucial yet challenging task for optimizing the performance of large language models (LLMs) on customized tasks. This pioneering research introduces the Automatic Prompt Engineering Toolbox (APET), which enables GPT-4\u00b9 to autonomously apply prompt engineering techniques. By leveraging sophisticated strategies such as Expert Prompting, Chain of Thought, and Tree of Thoughts, APET empowers GPT-4 to dynamically optimize prompts, resulting in substantial improvements in tasks like Word Sorting (4.4% increase) and Geometric Shapes (6.8% increase). Despite encountering challenges in complex tasks such as Checkmate in One (-14.8%), these findings demonstrate the transformative potential of APET in automating complex prompt optimization processes without the use of external data. Overall, this research represents a significant leap in Al development, presenting a robust framework for future innovations in autonomous Al systems and highlighting the ability of GPT-4 to bring prompt engineering theory to practice. It establishes a foundation for enhancing performance in complex task performance and broadening the practical applications of these techniques in real-world scenarios.", "sections": [{"title": "Introduction", "content": "The landscape of artificial intelligence has undergone a remarkable transformation in recent years. In the past, leveraging AI for specific tasks required a dedicated team of data scientists to build and train specialized models. This process was not only resource-intensive but also limited in its accessibility to organizations with the requisite expertise and financial capacity. However, the advent of Large Language Models (LLMs) like GPT-4 has radically changed this scenario.\nLLMs are advanced artificial intelligence systems designed to process, understand, and generate human language by learning from extensive datasets. Imagine a tool that can read and understand vast amounts\nof text everything from books and articles to websites and social media posts. LLMs use this knowledge to perform a wide range of tasks involving language. They function as versatile tools capable of performing a broad range of linguistic tasks, from translation and content creation to answering complex questions, without requiring task-specific programming.\nThrough their ability to generalize across different domains, generalist foundation LLMs like GPT-4 represent a significant leap in AI. These models are outperforming specialized, state-of-the-art (SOTA) models right out of the box, without any need for task-specific training (OpenAI, 2023). This shift, together with the rise of ChatGPT as a product available to the public, which rose to 100 million users in just two months (Milmo, 2023), represents a significant democratization of AI, making powerful tools accessible to a wider audience.\nThe evolution of Large Language Models in recent years has been nothing short of revolutionary. This progress can be quantified in terms of the scale of model architecture and training data. The journey began with smaller-scale models like the original Transformer, introduced by Vaswani et al. (2017), which laid the groundwork for modern LLMs and enabled the creation of models like GPT-1, which were trained on datasets comprising of 117 million parameters (Radford et al., 2018), a figure that was groundbreaking at the time. These parameters refer to the internal settings of the model that are learned from the training data. These parameters help the model make predictions and generate text. The more parameters a model has, the more complex and nuanced its understanding and generation of text can be.\nThe following years saw an exponential growth in the size and complexity of these models. BERT, short for Bidirectional Encoder Representations from Transformers, is a groundbreaking model in the field of natural language processing (NLP) introduced by Google in 2018. It revolutionized how machines understand human language by focusing on the context of words in a sentence, rather than just the words themselves (Devlin et al., 2018). Bidirectional training allows the model to understand the context of a word based on all of its surroundings (both left and right of the word), unlike previous models which processed text in one direction at a time. BERT quickly became a benchmark in NLP tasks, including being applied to Google Search (Google, 2019). After this, the development of more advanced LLMs accelerated. It was the release of GPT-3 in 2020, a model with 175 billion parameters, that set a new standard for LLMs. GPT-3's ability to understand context and generate coherent text on a wide range of topics was unprecedented (Brown et al., 2020).\nThe most recent milestone in this journey is OpenAI's GPT-4. This model, estimated at a staggering one trillion parameters, is five times the size of GPT-3 and approximately 3,000 times the size of BERT when it first came out. The sheer scale of GPT-4 represents a significant advancement in the field, with capabilities far surpassing its predecessors (OpenAI, 2023). The GPT-4 model can be described as a state-of-the-art foundation model, which is a term coined by researchers at Stanford University (Bommasani et al., 2021). Foundation Models are characterized by their scale, the extent of their training data, and their ability to be adapted to a wide range of tasks without task-specific training.\nThe most recent iterations of these models (i.e. GPT-4, and to some extent Google's PaLM) demonstrate emerging capabilities, including reasoning (the ability to make sense of complex information and come to logical conclusions), planning (the ability to sequence actions towards achieving a goal), decision-making (choosing between different options based on given criteria), in-context learning (adapting to new tasks based on the context provided without additional training), and responding in zero-shot scenarios (handling tasks they have never seen before without any prior examples).\nThese skills are attributed to their vast scale and the complexity of their training, despite the fact that the pretrained LLMs are not explicitly programmed to exhibit these attributes (Wei et al., 2022). It's important to note that all this is happening not because LLMs can actually think, but simply because they can generate text and have been trained on massive amounts of text. LLMs use deep neural networks, which are complex mathematical models inspired by the way the human brain works. These networks consist of layers of nodes (neurons) that process and transmit information. Through extensive training on large datasets, these networks learn to recognize patterns and relationships in the data, enabling LLMs to generate text that appears thoughtful and contextually appropriate. In these varied tasks, the performance of GPT-4 is remarkably close to that of a human expert, showcasing a near-human level of competence and adaptability (Bubeck et al., 2023). This near-human performance is an emergent property of the complex interactions within the neural network, the extensive training on diverse data, and the ability of the model to generalize from this data. Thus, while LLMs do not think in the human sense, their sophisticated architecture and training enable them to mimic many aspects of human-like reasoning and language use.\nDespite these advancements, a critical challenge persists: the efficacy of LLMs is heavily dependent upon the quality of input prompts they receive (Wei et al., 2023). While a carefully crafted prompt can harness the full potential of these AI systems, an inadequately formulated prompt can yield results that fall short of their potential. This happens because LLMs generate responses based on the context provided by the prompts. A well-crafted prompt provides clear, specific, and relevant context, guiding the model to produce accurate and coherent responses. In contrast, a poorly designed prompt may lack clarity, specificity, or necessary context, leading the model to generate responses that are vague, irrelevant, or incorrect.\nThe reliance of the models on prompt design establishes a significant barrier, particularly for users who do not possess the expertise or experience in crafting effective prompts (Zamfirescu-Pereira et al., 2023). For instance, a prompt that ambiguously asks \"Tell me about it\" can lead to a variety of responses depending on what \"it\" refers to, whereas a more specific prompt like \"Explain the process of photosynthesis in plants\" is likely to yield a focused and accurate explanation. Additionally, the use of structured prompts that guide the model through a step-by-step process or include specific instructions can significantly enhance the quality of the output.\nConsequently, the democratization of AI, with all its potential, faces limitations in its depth of accessibility and utility. Without the ability to formulate effective prompts, many users may find it"}, {"title": null, "content": "challenging to fully leverage the capabilities of LLMs. This underscores the importance of developing tools and methodologies to assist users in creating high-quality prompts, thereby making powerful Al technologies more accessible and effective for a broader audience.\nRecent literature has explored various methods to improve the performance of LLMs through prompt optimization. Studies have introduced techniques such as Chain of Thought (CoT) prompting, Tree of Thoughts (ToT) frameworks, and self-consistency methods to enhance the reasoning and decision-making abilities of LLMs (Wei et al., 2022; Yao et al., 2023). Additionally, research has focused on methods like \"Ask Me Anything\" (AMA) and universal prompt retrieval systems (UPRISE) to improve zero-shot performance and reduce hallucinations (Arora et al., 2022; Cheng et al., 2023). These advancements have significantly improved the accuracy and reliability of LLMs, yet they still largely depend on human-crafted prompts and external interventions. To address these challenges and enhance the capabilities of the LLM, this research aims to explore the autonomous capabilities of GPT-4, focusing on its potential to self-optimize prompts. Self-optimization refers to the ability of a system, in this case, GPT-4, to autonomously refine and improve the prompts it receives to generate more accurate and relevant responses. This involves the model analysing the initial prompt, identifying potential improvements, and adjusting the prompt to better suit the task at hand. Moreover, we explore the increasing ability of generalist foundation models to walk the fine line between specialized expertise and broad applicability of these AI models.\nThe theoretical contributions of this research are significant, enhancing the body of knowledge on the autonomous capabilities of Large Language Models like GPT-4. This study aims to advance understanding of how such models can independently optimize prompts, challenging the current reliance on human intervention for improving AI performance. It suggests a move towards LLMs that can self-improve, broadening the research into Al's potential for self-directed learning and adaptation.\nFrom a practical standpoint, the implications of this research extend into the wider adoption and application of AI technologies across diverse sectors. By demonstrating GPT-4's ability to autonomously optimize prompts, this study highlights the potential for LLMs to lower the barriers to effective AI use, making sophisticated AI tools more accessible to non-experts. This democratization of AI could revolutionize how businesses, educational institutions, and individuals approach problem-solving, creativity, and decision-making, fostering innovation and efficiency. Furthermore, the insights derived from this research could inform the development of more intuitive and self-sufficient AI systems, paving the way for broader societal adoption of AI technologies. In doing so, this study not only contributes to the academic discourse but also offers practical strategies for harnessing the full potential of LLMs in real-world applications."}, {"title": "Literature Review", "content": "This literature review critically examines the evolution and current state of Large Language Models (LLMs) and their role in the democratization of artificial intelligence. It will trace the development of\nthese models from their early iterations to the advanced, multifaceted GPT-4, highlighting key technological advancements and their implications. A particular focus will be on the challenges and strategies related to prompt design, its impact on the effectiveness of LLMs and the consequences that this has for every day users. The review will also explore recent studies on prompt optimization within these models. By synthesizing existing research, this review aims to contextualize the challenges and potentials of LLMs, setting the stage for the research questions addressed in this thesis."}, {"title": "The Evolution and Inner Workings of Large Language Models (LLMs)", "content": "The evolution of LLMs is an important aspect of the modern artificial intelligence landscape, characterized by a series of revolutionary advancements in model architecture, training techniques, and an increasingly sophisticated understanding of language.\nAt the heart of this evolution is the Transformer model, introduced by Vaswani et al. in 2017. This model marked a significant departure from previous approaches in natural language processing (NLP) through its unique use of the 'attention mechanism', which was first introduced by Bahdanau et al. (2015). Unlike earlier models that processed input sequences in a linear or sequential manner, the Transformer could focus on different parts of the input sequence, determining which parts were most relevant for a given task. This attention mechanism is akin to how a spotlight highlights specific actors on a stage, allowing the audience to focus on key performances while maintaining awareness of the entire scene.\nFollowing the Transformer, OpenAI developed the Generative Pre-training Transformer (GPT) series, starting with GPT-1 (Radford et al., 2018). This model leveraged the Transformer architecture to generate coherent text, demonstrating the potential of scaling up models for improved performance. These models operate using tokens, which are essentially pieces of text converted into a format understandable by the model. The tokens are processed through layers of neural networks a complex arrangement of nodes and connections inspired by the human brain's architecture. Each layer of the network captures different aspects of language, from basic syntax to complex semantic relationships.\nBERT, introduced by Google, added another dimension to this landscape with its bidirectional training approach (Devlin et al., 2019). Unlike the unidirectional approach of GPT models, where the context is understood based on preceding text, BERT analyzes text in both directions forwards and backwards. This bidirectionality allows for a more nuanced understanding of context, as the meaning of a word can be influenced by words that come both before and after it.\nThe release of GPT-3 by OpenAI took these advancements further, scaling up the model to unprecedented levels (Brown et al., 2020). With an increased number of parameters and more extensive training data, GPT-3 was capable of generating even more nuanced and contextually aware text. Its successor, GPT-4, continued this trend, pushing the boundaries of model size and complexity, resulting in enhanced linguistic proficiency and a broader range of capabilities (OpenAI, 2023).\nLLMs operate on complex concepts such as word embeddings, transformer architecture, and self-attention mechanisms. Word embeddings translate words into high-dimensional vectors, capturing"}, {"title": null, "content": "semantic relationships. Transformers process these embeddings, using self-attention to weigh the importance of different words in a sentence for understanding context. The attention mechanism involves assigning weights to different parts of the input data, which are calculated using SoftMax functions a mathematical approach that helps the model decide which parts of the input to focus on. The training of these models involves adjusting model parameters to minimize the difference between predicted and actual word sequences, refining the model's ability to generate coherent and contextually relevant text. (Lee & Trott, 2023)."}, {"title": "Performance and Evaluation of Foundation LLMs", "content": "Foundation models are transformative Al systems trained on extensive datasets to grasp a broad spectrum of knowledge, enabling them to be adapted for diverse tasks without domain-specific tuning (Bommasani et al., 2022). These models, including GPT-4, BERT, and others, through self-supervised learning from extensive data, demonstrate adaptability to numerous tasks (OpenAI, 2023). This chapter delves into the comparative analysis of the advancements in foundation models across various domains. By examining their capabilities, limitations, and potential for innovation, we aim to map the current AI landscape and lay the groundwork for this research's further development.\nAs the current landscape of AI development has been evolving with incredible speed, we will compare the latest, state-of-the-art models with each other, evaluating the LLMs on specific tasks. Two broader categories of tasks have been defined to evaluate the performance of Language Models (Naveed et al., 2023):\n1. Natural Language Understanding (NLU): This task evaluates the language comprehension\nabilities of Language Models. It covers a range of tasks such as sentiment analysis, text classification, natural language inference, question answering, commonsense reasoning, and reading comprehension, among others.\n2. Natural Language Generation (NLG): This task covers the language production proficiency\nof Large Language Models based on the given input context. It involves tasks like summarization, sentence completion, machine translation, and dialogue generation, among others.\nNext to these LLM capabilities, it is evident that the scale of the latest generation LLMs, notably, GPT-4 (OpenAI, 2023), PaLM (Anil et al., 2023), and LLaMa (Touvron et al., 2023), has uncovered emerging capabilities. These are tasks that a smaller size LLM is not able to perform, but only emerges once the size of the model (e.g., training compute, model parameters, etc.) becomes large enough (Wei et al.,"}, {"title": null, "content": "2022). These emergent abilities include performing arithmetic, playing chess, summarizing passages, and more, which LLMs learn simply by observing natural language. Moreover, it becomes increasingly apparent that the increasing scale of these models not only reveals new capabilities but also significantly enhances their proficiency in the described tasks (Srivastava et al., 2023). This enhancement in performance underscores the critical role that model scale plays in advancing what is achievable with AI, making these state-of-the-art LLMs more versatile across a broader spectrum of complex tasks.\nMeasuring the performance of these Large Language Models is currently done by a vast list of benchmark datasets. The benchmark datasets measure the natural language processing, general knowledge, reasoning, and problem-solving capabilities of these models. Benchmarks such as GLUE (Wang et al., 2019) and SuperGLUE (Wang et al., 2020) challenge models on a range of natural language understanding tasks, including sentiment analysis and paraphrase detection, while specialized datasets like ARC (Moskvichev et al., 2023) and MMLU (Hendrycks et al., 2020) go deeper into models' reasoning capabilities and general knowledge across various disciplines. More advanced tasks presented by benchmarks like AGIEval (Zhong et al., 2023) and BIG-Bench (Srivastava et al., 2022) test the limits of LLMs' problem-solving abilities, including their capacity for multi-step reasoning and understanding complex, real-world scenarios. As LLMs continue to evolve, these benchmarks serve as critical tools for assessing their progress, highlighting both their strengths and limitations. However, measuring the performance of these models is getting increasingly difficult, as traditional benchmarks may no longer fully capture the breadth and depth of capabilities exhibited by state-of-the-art LLMs like GPT-4 (Bubeck et al., 2023).\nThe challenge lies in the models' ability to generalize across a vast array of tasks, some of which have not been explicitly encountered during training. This generality suggests a form of intelligence that transcends simple pattern recognition or memorization, venturing into areas of creativity, problem-solving, and even intuition. Given this context, the evaluation of such models demands innovative approaches that go beyond conventional metrics. Collins et al. (2022) and Bubeck et al. (2023) propose methods which focus more on linguistic reasoning, measuring the ability to perform human-like tasks (e.g. the ability to plan, the ability to explain why something is happening). Next to these methods, other benchmarking datasets have also been developed which focus more on the emerging capabilities of these LLMs. For instance, The Game of 24 (Yao et al., 2023), challenges LLMs to creatively form arithmetic expressions to reach the number 24 from four given numbers, testing their numerical reasoning in a unique manner. Similarly, the BIG-Bench Hard (BBH) tasks introduced by Suzgun et al. (2023), including Geometric Shapes, Multi-Step Arithmetic Two, and Word Sorting, along with a reasoning task, Checkmate-in-One. Python Programming Puzzles (P3) by Schuster et al. (2021) presents a series of intricate programming challenges, assessing LLMs' coding prowess across various difficulty levels. Additionally, the Multilingual Grade School Math (MGSM) dataset by Shi et al. (2023) expands the scope of evaluation to include linguistic diversity, translating arithmetic problems into ten different languages, thereby testing not just mathematical logic but also cross-linguistic understanding. While"}, {"title": null, "content": "more resource intensive, these methods cover the ability of these models to perform tasks beyond Natural Language Understanding and Generation."}, {"title": "The Role of Prompt Design", "content": "As the performance of these models increases as the size keeps growing, the efficacy of LLMs is not solely a manner of their architectural design or the size of their training data. An equally critical factor is the manner in which these models are interacted with, particularly through the formulation of prompts. This chapter explores the important role of prompting techniques in unlocking the full potential of LLMs, highlighting how it has become a cornerstone in the practical application of these advanced AI tools.\nThe significance of prompt engineering stems from its direct impact on the quality, relevance, and accuracy of the responses generated by LLMs. A well-optimized prompt can lead to outputs that closely align with user expectations, effectively leveraging the model's capabilities. In contrast, poorly crafted prompts may yield irrelevant or inaccurate responses (Zhou et al., 2022).\nPrompt engineering refers to the process of crafting inputs for LLMs in a way that effectively guides the models to generate the desired outputs (Chen et al., 2023). Given the generalist nature of LLMs, which are not fine-tuned for specific tasks, the use of prompt engineering emerges as a crucial skill for users and developers alike. It enables a more intuitive interaction with AI, transforming these models from mere repositories of information into dynamic tools capable of engaging in creative problem-solving, generating insightful analyses, and even participating in complex decision-making processes (Bubeck et al., 2023). Moreover, using the correct prompting techniques even enables generalist LLMs like GPT-4 to outperform fine-tuned models, specifically trained for a task (Nori et al., 2023). This means that, using the correct formulation of a question, GPT-4 can outperform fine-tuned models, further contributing to the democratization of AI.\nMoreover, prompt engineering is not a static field; it is continuously evolving in response to advances in model architectures, changes in user needs, and the development of new application areas. Researchers and practitioners are exploring various strategies to refine the process, including the use of prompt engineering, few-shot learning (examples of correct answers), and the incorporation of meta-information into prompts, which refers to additional context or data about the primary information within a prompt that can help guide the model's response. These efforts are aimed at developing more systematic and efficient ways to interact with LLMs, making AI more accessible and effective for a broader audience.\nTo further investigate the different prompting techniques, we first make a distinction between zero-shot prompting and few-shot prompting. Zero-shot and few-shot prompting are techniques used in machine learning, particularly with language models, to handle tasks without or with minimal task-specific training data. Zero-shot prompting requires a model to perform a task without any prior examples, relying on its pre-existing knowledge and the instructions provided within the prompt. It assesses the"}, {"title": null, "content": "model's ability to generalize from its training to new tasks without explicit examples (Wang et al., 2019). This capability is essential for tasks where labeled data is scarce or not available. In contrast, few-shot prompting involves providing the model with a small number of examples (the \"shots\") within the prompt that illustrate what is expected. These examples serve as a direct guide, helping the model understand the context and the specific task requirements. Few-shot prompting effectively leverages the model's learned patterns from training and applies them to the task at hand with the help of these examples, enhancing its ability to generate more accurate and relevant responses based on the limited examples provided (Xia et al., 2020). Understanding the distinction between these two approaches helps us tailor the model to specific applications, where data might not be freely available but reasoning capabilities are still needed.\nPrompt optimization techniques for enhancing the performance of models vary widely in complexity. Simple strategies include the use of delimiters to separate different sections of the input clearly (OpenAI, n.d.), which can help in structuring the information more effectively for the model. Even seemingly straightforward interventions, such as prefacing prompts with phrases like \"Let's think step by step,\" have been shown to significantly boost the model's performance in a zero-shot environment (Kojima et al., 2023). On the more complex end of the spectrum, there are multi-step approaches that necessitate multiple interactions with the model to refine the response.\nOne method that further underscores this advancement is the Chain of Thought (CoT) approach. Chain of Thought prompting has emerged as a compelling method for enhancing the complex reasoning capabilities of LLMs. This technique involves providing models with prompts that include a series of intermediate reasoning steps, which guide the model towards generating the final answer. Studies have shown that when models are prompted within a few-shot environment demonstrating this chain of thought, their performance improves significantly on various arithmetic, commonsense, and symbolic reasoning tasks. For instance, the use of CoT prompting with just eight examples has enabled a PaLM 540B model to achieve state-of-the-art accuracy on the GSM8K benchmark, a collection of math word problems, outperforming even fine-tuned GPT-3 models equipped with a verifier (Wei et al., 2022). Whether it's solving mathematical puzzles or making logical deductions, the CoT approach not only elevates the accuracy of the outcomes but also renders the model's thought process transparent and understandable to users.\nThe \"Tree of Thoughts\" (ToT) framework introduced by Yao et al. (2023) expands on the CoT method by enabling LLMs to explore multiple reasoning paths and evaluate different solutions to solve complex problems. ToT allows for strategic decision-making, looking ahead, and backtracking when necessary, significantly enhancing LLMs' problem-solving abilities across tasks like the Game of 24, Creative Writing, and Mini Crosswords. For example, ToT achieved a 74% success rate in the Game of 24, a substantial improvement over CoT's 4% success rate with GPT-4. This framework represents a novel approach to leveraging LLMs for extended problem solving and reasoning."}, {"title": null, "content": "A method that is more focused on the reliability of the output, is \u201cSelf-Consistency\u201d. This method generates multiple reasoning paths and selects the most consistent answer across them, leveraging the intuition that correct reasoning often follows multiple paths to the same conclusion. Self-consistency significantly boosts performance across various arithmetic and commonsense reasoning benchmarks. This approach simplifies existing methods by working off-the-shelf with pre-trained models, requiring no additional training or human annotations, acting as a self-ensemble to enhance reasoning accuracy (Wang et al., 2022).\nRecent research in prompt engineering has introduced sophisticated techniques aiming for more precise and contextually relevant outputs. A prominent innovation is the \"Expert Prompting\" method developed by Xu et al. (2023), which improves responses by first creating an \"expert identity\" aligned with the query's context, and then integrating this identity into the prompt. This method exists in two forms: a static version, which uses a consistent expert profile, and a dynamic version, creating a unique expert identity for each query to produce adaptive and finely tuned responses. Additionally, Du et al. (2023) managed to increase LLM performance through \"Multi-persona Prompting,\" also referred to as solo-performance prompting (SPP). This approach directs the LLM to construct various \"personas\" tailored to a specific task or question. These personas participate in a simulated group discussion, offering solutions, critiquing each other, and refining their suggestions collaboratively. The final step synthesizes these interactions into a unified, comprehensive answer.\nIn conclusion, this chapter has highlighted the significance of prompt design in enhancing the performance of LLMs. The discussion underscored that beyond the model's architecture and training data size, the art of crafting prompts plays an important role in leveraging the full capabilities of LLMs. Through methodologies like Chain of Thought, Tree of Thoughts, and Expert Prompting, we have seen the potential for nuanced interaction between humans and AI to produce more accurate, relevant, and sophisticated outputs.\nAs we proceed to the next chapter, the focus shifts to the field of automated prompt optimization. This area represents an important research direction, aiming to reduce the reliance on manual prompt engineering by developing algorithms capable of refining and generating prompts autonomously. This advancement holds the promise of making LLMs more accessible and effective, by systematically improving how these models interpret and respond to user queries."}, {"title": "Emerging Trends in Prompt Optimization", "content": "As we have observed, the capacity of LLMs to interpret and respond to human queries with high degrees of accuracy and relevance is significantly influenced by the quality of the prompts they are given. This has led to an increased focus on developing methods that not only enhance the effectiveness of prompts but also enable models to autonomously refine their responses. These methods showcase a range of approaches, from enhancing model responsiveness with diverse prompts to enabling models to critique"}, {"title": null, "content": "and improve their reasoning through debate, thereby marking significant progress in making LLMs more adaptable and reliable.\nArora et al. (2022) introduced a method called \u201cAsk me Anything\" (AMA) to improve the performance of language models on a variety of tasks without additional training. AMA involves using multiple, imperfect prompts and aggregating their outputs to produce a final prediction. The approach is based on the observation that question-answering (QA) prompts, which encourage open-ended responses, tend to be more effective than those that limit responses to specific formats. The authors develop a scalable method to transform task inputs into effective QA formats using the language model itself and then aggregate these using a process called weak supervision. This process combines noisy predictions to produce a final output. AMA was evaluated across different model families and sizes, demonstrating significant performance improvements over baseline models. The method enabled a smaller, open-source model to match or exceed the performance of larger, few-shot models on several benchmarks.\nCheng et al. (2023) present an approach to enhance the zero-shot performance of LLMs through a universal prompt retrieval system. This system, named UPRISE, employs a lightweight and versatile retriever that automatically selects the most effective prompts for any given task input in a zero-shot environment. The innovation of UPRISE lies in its ability to generalize across different tasks and models without the need for task-specific fine-tuning or manual prompt engineering. The retriever is trained on a diverse range of tasks but is capable of working on unseen tasks and with various LLMs, demonstrating its universal applicability. The methodology involves tuning the retriever on a smaller model (GPT-Neo-2.7B) and evaluating its effectiveness on larger models such as BLOOM-7.1B, OPT-66B, and GPT3-175B. Remarkably, UPRISE also shows potential in mitigating the hallucination issue prevalent in models like ChatGPT, thereby enhancing the factual accuracy of their outputs. This approach significantly improves upon baseline zero-shot performance across multiple LLMs and tasks, underscoring its potential to make LLMs more versatile and effective in real-world applications without extensive retraining.\nDu et al. (2023) explore an approach to enhance the reasoning and factuality LLMs. The authors propose a method where multiple instances of LLMs engage in debates over their generated responses to a given query. This debate process involves iterative rounds where each LLM critiques and revises its responses based on the feedback from other models, aiming for a consensus. This method significantly improves the LLMs' ability to reason and generate factually accurate content across various tasks, including arithmetic, strategic reasoning (e.g., chess move prediction), and factual information extraction (e.g., generating biographies). The study demonstrates that this multiagent debate approach not only reduces the occurrence of false facts but also enhances the models' mathematical and strategic reasoning capabilities. The approach requires only black-box access to the LLMs, and shows that a \"society of minds\" can effectively advance LLMs' performance without the need for additional training or fine-tuning on specific tasks."}, {"title": null, "content": "Another method, called AutoHint, combines the strengths of zero-shot and few-shot learning. It optimizes prompts by generating and incorporating hints from input-output demonstrations, significantly improving task accuracy. The method starts with an initial prompt, identifies incorrect predictions, and uses these to generate hints that refine the prompt. Evaluated on the BIG-Bench Instruction Induction dataset, AutoHint showed notable accuracy improvements across various tasks, demonstrating its effectiveness in prompt optimization (Sun et al., 2023).\nPromptAgent is an optimization method that autonomously develops prompts of a level comparable to those designed by experts. PromptAgent employs a Monte Carlo tree search algorithm to explore and optimize the prompt space efficiently, leveraging error feedbacks to iteratively refine prompts towards expert-level quality through a process of selection, expansion, simulation, and back-propagation. This approach enables PromptAgent to generate highly effective, domain-specific prompts, demonstrating superior performance over strong Chain-Of-Thought methods across diverse tasks, including BIG-Bench Hard and various NLP challenges.\nAnother method, exploring the possibility of chaining LLM prompts to enhance the effectiveness of AI in complex tasks, allows the output of one LLM operation to serve as the input for the next, improving task outcomes and user experience in terms of transparency, controllability, and collaboration. Through a 20-person study, the authors demonstrate how chaining can lead to better quality results and offer users new ways to interact with LLMs, such as through calibration of model expectations and debugging of model outputs (Wu et al., 2022).\nCumulative Reasoning is a method that improves LLMs' ability to tackle complex problems through cumulative and iterative processing, emulating human thought processes. By breaking down tasks into smaller, manageable components, Cumulative Reasoning significantly enhances problem-solving effectiveness. The method employs three types of LLMs-proposer, verifier, and reporter-to progressively refine solutions, demonstrating superior performance on logical inference tasks and establishing new benchmarks on datasets like FOLIO wiki and MATH. This approach addresses the limitations of LLMs in handling complex tasks by facilitating a more structured and effective problem-solving process.\nAutomatic Prompt Engineering is a methodology that leverages LLMs for generating and selecting effective prompts automatically. This approach significantly enhances the performance of LLMs across a variety of NLP tasks by optimizing instructions to achieve better or comparable results to those generated by human experts. APE demonstrates its effectiveness by outperforming the baseline LLM performance and matching or exceeding human-level prompt engineering in most tasks, highlighting the potential of LLMs in reducing the manual effort involved in prompt design (Zhou et al., 2023).\nIn a groundbreaking study, Medprompt, employing dynamic few-shot selection, self-generated chain of thought, and choice shuffle ensembling, was introduced to significantly enhance GPT-4's performance on medical benchmarks. Without specialized training, these techniques combined to surpass existing benchmarks, demonstrating a remarkable 27% reduction in error rate on the MedQA dataset (Nori et al.,"}, {"title": null, "content": "2023). This approach not only set new standards for accuracy but also showcased its broad applicability beyond medical domains, signaling a major advancement in the use of generalist models for specialized tasks.\nCombining previous methods, Suzgun and Kalai (2024) introduce a technique called \u201cmeta-prompting\". Meta-prompting transforms a singular LLM into a conductor, who can orchestrate multiple independent LLMs to collaboratively address complex tasks. This process involves the LLM breaking down tasks into smaller, manageable subtasks, which are then delegated to specialized \"expert\" instances of the same LLM, each provided with tailored instructions for execution. The central LLM, acting as the conductor, ensures seamless integration and communication among these expert models, applying critical thinking and robust verification processes to refine and authenticate the final outcome. Remarkably, meta-prompting outperformed standard prompting methods by significant margins, demonstrating an average improvement of 17.1% over standard prompting, 17.3% over dynamic expert prompting, and 15.2% over multi-persona prompting.\nYe et al. (2024) propose a novel approach called \"Prompt Engineering a Prompt Engineer\", which involves creating a meta-learning framework where the LLM is trained to optimize its own prompts. This technique allows the model to generate and refine its prompts iteratively, enhancing its performance across various tasks. It leverages historical task data to inform the prompt optimization process, ensuring that the generated prompts are contextually relevant and tailored to the task requirements. The method was tested on diverse benchmarks, including natural language understanding and generation tasks, showing substantial gains in performance and adaptability.\nPryzant et al. (2023) introduce \"Gradient Descent for Prompt Optimization\" (GDPO), a technique that applies gradient descent algorithms to refine prompts using existing task data. This approach treats prompt tokens as parameters that can be optimized to minimize the loss on specific tasks. By iteratively adjusting these tokens based on past performance data, GDPO fine-tunes the prompts to enhance the model's accuracy and efficiency. The authors evaluated GDPO across multiple benchmarks, including sentiment analysis and question answering, demonstrating notable improvements in task performance"}]}