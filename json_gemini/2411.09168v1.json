{"title": "Theory of Mind Enhances Collective Intelligence", "authors": ["Michael S. Harr\u00e9", "Catherine Drysdale", "Jaime Ruiz-Serra"], "abstract": "Collective Intelligence plays a central role in a large variety of fields, from economics and evolutionary theory to neural networks and eusocial insects, and it is also core to much of the work on emergence and self-organisation in complex systems theory. However, in human collective intelligence there is still much more to be understood in the relationship between specific psychological processes at the individual level and the emergence of self-organised structures at the social level. Previously psychological factors have played a relatively minor role in the study of collective intelligence as the principles are often quite general and applicable to humans just as readily as insects or other agents without sophisticated psychologies. In this article we emphasise, with examples from other complex adaptive systems, the broad applicability of collective intelligence principles while the mechanisms and time-scales differ significantly between examples. We contend that flexible collective intelligence in human social settings is improved by our use of a specific cognitive tool: our Theory of Mind. We identify several key characteristics of psychologically mediated collective intelligence and show that the development of a Theory of Mind is a crucial factor distinguishing social collective intelligence from general collective intelligence. We then place these capabilities in the context of the next steps in artificial intelligence embedded in a future that includes an effective human-AI hybrid social ecology.", "sections": [{"title": "1 Introduction", "content": "All intelligence is collective intelligence. [70]\n\nCollectives are capable of achieving things that individuals alone cannot. Notwithstanding the simplicity or complexity of the individuals, their aggregate behaviour can often be understood as a complex processing of information that individuals store, modify, and transfer between each other producing 'useful' collective behaviour at the scale of the whole collective. In most instances of Collective Intelligence (CI), where the agents might be ants in an ant colony, bees in a beehive, or neurons in a neural network, the individual is not aware of the drivers of their behaviour or the behaviour of other agents. For example, a single neuron is neither aware of its own internal processes nor that of a neuron it is connected to, nor is it aware of the end goal to which its activity contributes. Despite both this lack of awareness and the lack of a centralised controller, evolutionary and learning processes have produced an intricate, precise, and highly adaptive system that is capable of functional behaviour that would be impossible for any single neuron to achieve. In other instances of CI, such as teams of humans, or businesses interacting in economic markets, the agents themselves may be highly complex and exhibit varying degrees of purposefulness and awareness. Within this context, we draw attention to the role of psychological factors in improving the CI of human social collectives and quantifying the intelligence of social collectives, both natural and artificial.\n\nIn order to understand how collectives process information, we first consider the variety of ways in which agents interact. The topology of the network describing agent-to-agent interactions is well known to be important for the proper functioning of social groups [83, 79]. In particular it has been shown that mammalian social groups exhibit patterns of fractal-like topologies [40, 51] that are a result of a cognitive ability to form discrete social connections between conspicifics [49]. These links are often both spatially and temporally transient; people meet for a while, go their separate ways, and come back together later. Despite this transience, individual connections are often the basis of long term social relationships between specific individuals as in pair-bonding and friendships. Consequently an important distinction can be made regarding connections between agents in complex adaptive systems: they can be more fluid-like or more solid-like [101]. For example the links between neurons in the brain are relatively fixed in nature when compared to the brief communicative interactions between ants, either instantaneous interactions between individual ants or via transient pheromone trails that coordinate the behaviour of large numbers of ants. Sol\u00e9 and colleagues [101, 88] identify a distinction between solid brains, in which interactions between agents fixed in place are highly persistent in time (e.g. neural networks, spin glasses) and liquid brains, in which interactions between highly mobile agents are much more short-lived (e.g. ants, immune cells). As Sol\u00e9 et al. note regarding liquid brains [101]: \"Here there are no neural-like elements and yet in many ways these systems solve complex problems, exhibit learning and memory, and make decisions in response to environmental conditions.\"\n\nAll biological agents are composed of sub-units such as organs, cells, and molecular networks [67, 69, 71]. Cells in particular are the simplest living organisms with individual intelligence, or competencies [67, 33], within their native contexts."}, {"title": "2 Cognitive morphospaces: The network topology of agents' interactions", "content": "The emergence of cognitive networks marked a pivotal moment in our evolutionary history [120]. Earlier, microorganisms had developed collective structures capable of responding to the physical environment, particularly conditions threatening individual cells [5] and survival became dependent on information exchanges within these groups. Habituation and the ability to minimise the energy response to danger stimuli is considered one of the simplest forms of learning and has been extensively studied in simple collective intelligences such as slime moulds [13, 106]. Similarly bacteria use quorum sensing to coordinate their behaviour based on the density of the population of their community to coordinate responses, leading to a change in gene expression and function regulation e.g. bioluminescence, release of toxins, and biofilm formation [72]. Information processing and problem solving capabilities developed using a variety of interaction types and network topologies long before the appearance of central nervous systems with fixed neuronal structures [5]. But this leads to an interesting question regarding the typologies of agent-to-agent interactions and the intelligence these structures might enable, a question that can be approached by looking at the morphospace of collective intelligence.\n\nA morphospace is a theoretical framework used to simplify and organize the complex shapes and forms of organisms, typically focusing on external anatomical features, into a more manageable space representing their potential variations. For instance, in [3], a three-dimensional morphospace was constructed for organisms with shells, where the diversity of shell shapes is described by three key parameters: a deviation angle, a translation factor, and a growth factor. This reduces a high-dimensional structural space to a lower-dimensional one, where a small number of parameterised properties captures key variations between forms. Morphospaces have found uses in many different fields of research, including the body shapes of fish [20], network topologies [3], and the structural forms of language [96].\n\nThese structures, which represent recurring patterns of trait variation, are of great interest to evolutionary biologists because they may indicate shared evolutionary processes and their constraints. They are also of great interest to researchers investigating the underlying structures of collective intelligence [68], as intelligence in its different forms may also be subject to shared evolutionary mechanisms and constraints. With this in mind, work has been done in studying a variety of morphospaces related to collective intelligence and in the next section we review some examples."}, {"title": "2.1 Network topology and the \u201cSolid Brain, Liquid Brain\u201d framework", "content": "In the most general of terms, a cognitive network has multiple information processing components that exchange information with each other and interact with their environmental context. Sol\u00e9 et al. [101] identified two key dimensions for categorising different types of cognitive networks: the system's physical state either more liquid or more solid in nature, differentiated by how freely individual agents (components) can move in space and the presence or absence of neurons. The collective dynamics of a large population of agents is influenced by the individuals' mobility which dictates how they respond to signals both internal and external to the collective. To help conceptualise this diversity, Sol\u00e9 et al. [101] developed a morphospace and taxonomy in order to compare and contrast the physical states of different types of CI. In this way they were able to consider the physical properties that form constraints on the computations achievable by a system. This then poses an interesting question: Is the entire space of possible intelligences being exploited by either synthetic biology or abiotic computation?\n\nLiquid brains exhibit cognitive behaviours without neurons. For example, models by Watson and colleagues [112, 111] illustrated how systems of self-interested agents, driven by a simple mechanism of strengthening beneficial connections, can lead to robust group-level adaptation and problem-solving. This self-organisation, akin to Hebb's rule in neural networks, enables the system to recall and consequently leverage past configurations that were successful. This also allows the system to generalise from experience and then predict beneficial states it has not encountered before, highlighting how decentralised actions can produce a form of CI that guides the system towards greater global utility. The agents in these models are very simple, but this need not be the case each agent within a collective may itself have a solid brain as in human social networks where fluid social interactions allow each solid brain to connect and communicate with other solid brains for the benefit of the collective [14]. This is extended to a hybrid model by Kao et al. [55], wherein the modular organisation of mobile animal populations (as an example of a liquid brain) suggests that certain communication pathways exhibit localised and persistent characteristics, akin to those in solid brains. These pathways enhance collective decision-making in complex environments and in turn raise important questions regarding the relative strengths of liquid and solid brains in different contexts. In particular, the conditions under which liquid brains outperform solid brains, especially in terms of adaptability and scalability, remain an open area of investigation. Which computational problems are more effectively solved by liquid brains vis-\u00e0-vis solid brains remains to be better understood.\n\nWhole classes of models that describe which biological or artificial structures are capable of some form of computation, either in potentia or in practice, can often be usefully represented using morphospaces [16]. Computational morphospaces [101] have proven effective in studying key properties of complex adaptive systems, for example the statistical mechanics of information processing and structural variations [2, 4]. This sheds light on how energy constraints influence the evolution and adaptability of neural networks across a variety of biological systems. Arsiwalla et al. [2] have examined how liquid brains and solid brains fit within such a framework, by comparing the flexibility and adaptability of different neural architectures [2]. The dimensions of their framework are three different types of complexity that a system may display: autonomic, computational, and social complexity. We suggest that other axes for consideration are a system's solid-liquid dimension as Sol\u00e9 et al. [101] and Oll\u00e9-Vila et al. [85] have done, as well as the system's degree of TOM (see Section 2.2) and the systems information processing capacity (see Section 3).\n\nWe posit that the collective intelligence that emerges from liquid brains (human social networks) is enhanced by our individual capacity for a ToM, where individuals are aware of the goals of others as well as that of the collective, which can then be achieved by adaptation at the local level. Rather than collective intelligence arising as an epiphenomenon or byproduct of agents interacting, our ToM allows agents to causally affect the outcome of the system they are a part of."}, {"title": "2.2 Models of another agent's internal states", "content": "Frith and Frith defined Theory of Mind (ToM) as how we explain other people's behaviour on the basis of their internal cognitive states, i.e. their knowledge, beliefs, and desires [34]. There is now a vast literature on this topic in psychology, sociology, and more recently artificial intelligence, but for the purposes of this article we restrict ToM to apply to the subset of the beliefs, preferences, and constraints of other agents in the sense of incentivised decisions. This borrows from the BPC model [35] put forward as an approach to understanding the socio-cognitive aspects of human decision-making [37].\n\nOne way to interpret the BPC model is that it imposes structural constraints on the process by which decisions are made, and then optimal decisions are discovered within these constraints by parametric variation. Recent work by Peterson et al. [87] compared more than 20 structurally constrained models of individual decision-making using human data. That study was extended to human data during strategic interactions by Harr\u00e9 and El-Tarifi [48] in order to test agents' constrained representations of other agents. This extends Yoshida et al.'s [123] notion of a Game Theory of Mind to a larger variety of models in which an agent's strategic reasoning about other agents modulates their behaviour.\n\nIn the field of artificial intelligence, Jara-Ettinger [53] proposed the use of Inverse Reinforcement Learning (IRL) as a model of agential ToM, whereby agents modelling other agents' mental states is equivalent to inferring an unobserved world model the other agent uses in their decision-making, as well as their reward function. Jara-Ettinger discusses some key limitations of IRL, such as the difficulty in recovering an agent's beliefs and desires even while assuming that all"}, {"title": "2.3 Configurations of our social networks inform individual reasoning", "content": "Frith and Frith have previously considered the benefits that accrue to humans via our ToM [34], but what is functionally happening when we use our ToM in social groups? It has been shown that in early hunter-gatherer societies that some emergent phenomena at the social level, e.g. Dunbar's Number [26, 27], are a consequence of the layered, fractal topology of social networks [28, 40, 51], and that these are in turn the product of very specific, discrete, cognitive constraints at the individual level that shaped the structures of early human societies [49]. At the individual level, a recent review by Momennejad [83] collected the evidence for different social network topologies and how they integrate interpersonal knowledge differently, showing that topology allows social networks to serve a rich variety of collective goals. Momennejad also reviews the neuro-imaging evidence showing that humans neurologically encode these topologies and these encodings are shared across the members of a social group. This was also demonstrated in the work of Lau et al. [65] showing that people are able to integrate information about how agents relate to one another in addition to how they relate to oneself in order to infer social group structures. Lastly, there is evidence for improved collective intelligence when individuals with higher competencies in ToM are present in the group, as shown in the study by Woolley et al. [121]. It was found that, just as there is for an individual person a measure of general cognitive ability, usually denoted g, there is an equivalent measure of collective intelligence, denoted c, for a group of people. They noted a key explanatory factor of a group's task performance, as measured by c, was the proportion of group members who ranked highly on the Reading the Mind in the Eyes cognitive test introduced by Baron-Cohen and colleagues [6, 7], a test used to measure an individual's capacity for TOM.\n\nA key takeaway from the work of Woolley and colleagues [121] is that the c factor is not strongly correlated with either the average or maximum intelligence of the individuals in a group. However, it does correlate well with the average social sensitivity of its members, the evenness of the distribution of contributions to group discussions, as well as the proportion of people in the group who rate highly on a ToM test. So there is considerable evidence for the role our TOM plays in group performance, how this shapes interpersonal interactions between people, and, as a consequence, the emergent topological properties of our social groups. This provides support for the argument that ToM is (one of) the individual, bottom-up mechanism(s) through which agents form higher-order social structures with measurable collective intelligence. This may be how we \"learn to read, interpret and re-write our interpersonal information content\" as we asked following the Watson-Levin quote in Section 1. Next, we illustrate these ideas in a dyadic and then a triadic example of agents interacting with each other and exhibiting non-trivial measure of CI."}, {"title": "3 Topological and cognitive structures in CI: illustrative examples", "content": null}, {"title": "3.1 A dyadic example of individual learning at short time scales", "content": "To illustrate how we will use information theory as a proxy for Woolley et al.'s c intelligence, we use a dataset that was previously studied [44] to measure the information flow in an iterated economic game experiment between monkeys and computers, based on data from an earlier study by Lee et al. [66]. In that study, the experiment had a monkey playing the matching pennies game against a computer algorithm for a reward, the (Nash) optimal reward for the monkey was received if it plays 50:50 across its two choices. A simplified description of the three algorithms used by the computer follows, see Lee et al. [66] for the exact descriptions:\n\nAlgorithm 0: Play uniformly and independently of the monkey's choices,\n\nAlgorithm 1: The computer stores the history of the choices made by the monkey, then to predict what the monkey would do in each trial the computer calculates the conditional probability of the monkey's choice given the monkey's"}, {"title": "3.2 A triadic example of evolutionary learning at long time scales", "content": "Before we introduce ToM for social interactions, we consider a second example where evolution has found an agent-to-agent interaction that is similar to the worked example used next in Section 4. Our evolutionary example is a three-agent system: the larval stage of the fly Liriomyza huidobrensis, the pea plant family Fabaceae that fly larvae predate on, and the parasitic wasp Opius dissitus that predates on L. huidobrensis. These species interact in the following way [116]: The larvae of L. huidobrensis infest a pea plant, the pea plant gives off volatiles, called infochemicals, that attract the wasps to the plant, which in turn feed on the larvae, and this larva-wasp conflict indirectly benefits the pea plant.\n\nIn this triadic relationship, the pea plant does not directly respond to the threat from the larvae for example it has not evolved a chemical agent that repels the larvae-laying flies. Instead it signals a third party, the wasp, to bring the wasp into contact with the larvae, and the wasp then eats the larvae. The wasps and the flies are in a dyadic evolutionary competition that could be modelled using two-agent evolutionary game theory, but the pea plant, having facilitated an instance of this conflict, benefits indirectly from the wasps success, in a sense plants can employ other species as a kind of 'body guard' [58]. We note that, like the example in Section 4, the plant only signals the wasps to a dyadic interaction when the plant detects the larvae, so that the plant only signals wasps when the plant perceives an intermittent information carrying cue from its environment that wasps are needed.\n\nThis type of second order interaction occurs in other ecological examples as well [38] where Trait-Mediated Indirect Interactions (TMIIs) induce hyper-graphs of interactions between species. More complicated interactions have also been observed in which a plant that is being attacked emits infochemicals that lead to unaffected plants emitting volatiles to attract predators [58], to reduce the risk of the unaffected plant being attacked while also aiding the plant being attacked. Kobayashi and Yamamura [58] specifically call this evolutionary development a form of altruism but of course there is no cognitive aspect to this altruism. These examples illustrate that evolution produces forms of sophisticated, inter-species, mixed competitive-altruistic interaction networks but without any need for a ToM, individual awareness, or strategic understanding of the interactions, and so individuals are not knowingly strategic or altruistic as we might interpret a person to be. But these evolved strategies are limited, by definition, to be fixed within the lifetime of a single agent and they can only adapt on evolutionary time-scales. This is in contrast to a TOM which allows us to adapt to multiple strategic and social contexts that may require novel solutions within the lifespan of a single person."}, {"title": "4 Social network modification through ToM: a minimal model", "content": "In this model we introduce a simple example of interacting agents that form a hypergraph [38]. As in the previous examples, this model is general enough to allow the agents to have any form of biological or artificial psychology, to have zeroth order ToM or fourth order ToM, and it applies to artificial collectives just as readily as it can to biological and ecological collectives.\n\nGame theory provides a formal approach to the analysis of incentivised social interactions in which agents attempt to"}, {"title": "4.1 Model scenario", "content": "We begin with three agents in proximity to one another situated in a noisy environment in which it is possible for them to collectively do something useful but they are initially in the unfortunate situation in which the behaviour of the three agents produces nothing that is of value. The possible actions (xi) of the agents (A\u2081) are binary: xi \u2208 {\u22121,1}, i \u2208 {1,2,3} and so in this example the xi are not probabilities. A1 is randomly and uniformly changing its states due to a (useful, information carrying) signal it receives from the environment at time t: st \u2208 {\u22121,1} such that P(st = 1) = P(st = \u22121) = 0.5. A2 and A3 are initially engaged in the prisoner's dilemma (PD) game where, as a consequence of selfishly (na\u00efvely) optimising their choice of xi, they are in the defect-defect Nash equilibrium (NE), and so their joint action is constant (with zero value). They are capable of a second output when in the cooperate-cooperate configuration, and this output has a positive value. However, being stuck in the PD NE, they are not initially able to produce it.\n\nA2 and A3 can produce something of value when they cooperate, but it only has value if the external signal st = +1. The output value of A2 and A3 interacting is initially a sequence of 0s: they are not cooperating with each other, and nothing of value is being produced. We assign each agent a state at time t, x \u2208 {\u22121,1}, such that their dynamic is an ordered sequence of binary states [x1,x2,...,xT] for t \u2208 {1,...,T}. The output from A2 and A3 interacting at time t is a result of having matched their states: V(ot) = 1 if xt2 = xt3 = 1 and V(ot) = 0 if xt2 = xt3 = \u22121, the xt2 \u2260 xt3 cases are never achieved as they are not NE and A2 and A3 will always choose according to the NE of their interactions. The collective behavioural vector is: x = {X1,X2,X3} or time indexed: x = {x1t,x2t,x3t}.\n\nThe signal st that A\u2081 receives indicates whether or not A2 and A3 should cooperate at time t, but initially no information is passing from A\u2081 to {A2, A3} and A\u2081 cannot produce anything by itself. So U\u2081(x; a) = 0, and agents A2 and A3 only produce output of zero value: V(ot) = 0. The agent utilities in this case are:\n\n$U_{1} (x; a) = 0,$\n\n$U_{2}(x; a) = a_{2}^{0} + a_{2}^{1} x_{3} + a_{2}^{2} x_{2},$\n\n$U_{3} (x; a) = a_{3}^{0} + a_{3}^{2} x_{2} + a_{3}^{3} x_{3},$"}, {"title": "4.2 Model interpretation", "content": "We make three observations regarding these two scenarios. First, we note that the interactions in the second scenario are a hypergraph in the sense that if any one of the nodes were removed in Figure 2, b. no remaining agent receives any payoff, all agents are necessary contributors to any single agent receiving a utility for their actions [38]. Second,"}, {"title": "4.3 Our perspective in context", "content": "We have reviewed some of the recent advances in how people, as complex agents, with a vast space of specialised, context-dependent behaviours, are able to coordinate their activities. In particular, finding ways to recombine our individual competencies to produce, in a short period of time, the appropriate collective competencies is a combinatorially complex task. The central theme of this article is that having a causal (generative) model of another person's behaviour that reflects the current environmental and social context helps us manage this complexity. This is one way in which our solid brains produce the necessary liquid social structures that quickly build collective solutions with novel collective competencies. In this section we bring these ideas together and summarise our view.\n\nWe first draw attention to how, in Section 4, A\u2081 has constructed a niche for itself in the context of the preexisting dyadic network between A2 and A3. In ecological networks a new agent joins a pre-existing network if it can find a niche within which it can fit. This occurs in one of three distinct ways: niche choice, niche conformance, and niche construction [19, 108]. Niche choice occurs when an individual selects environmental conditions that align with its phenotype, while niche conformance involves adjusting its phenotype to suit the environment. Niche construction is the modification of the environment to meet individual needs, which may also impact other species. This suggests an analogy between the formation of ecological networks, where new agents joining may enhance or disrupt the current configuration, and social networks where membership can be explicitly or implicitly gated according to a prospective agent's 'fit' within the group, and even if the group can adjust to accommodate a newcomer, just as the newcomer can adjust in order to be accepted. In human social groups people can be recruited or excluded depending on their contribution to the better functioning of the collective, which in turn corresponds to changes in individual neural activity related to the social network structure [95].\n\nIn our analogy with ecological networks, we suggest that any signalling agents in a network need to encode messages over an information-carrying medium that receivers are receptive to and that can then be decoded by a specific receiver. We have argued that humans, as both signallers and receivers, take advantage of our ToM in order to understand what signals will be correctly interpreted by another person and to adapt their signalling to the cognitive state of the receiver. Specifically, our psychology allows us to learn how to read, interpret, and re-write our interpersonal communication over a very short time frame, allowing us rapid, targeted control over our collective behaviour.\n\nIn some sense the adage there is nothing new under the sun holds here as evolution and biology have recycled fundamental, pre-existing principles in the service of human sociality. But the adaptive speed of our social networks, the psychological mechanisms involved, the variety of purposes they serve, and the complexity of the communications appear most highly developed in humans. In ecological networks, for example, some agents can act as an encoder-sender of semantic information via infochemicals that signal another agent. A second agent then acts as a receiver-decoder of this signal that in turn changes the behaviour of the receptive agent. The combination of the receiver's anatomical configuration and behavioural phenotype elicits an appropriate stimulus-response that benefits the signalling agent, and these interactions can form vast, complex hypergraphs of competition and cooperation between agents of many different species. But neither agent needs higher cognitive faculties their ToM is of the zeroth order. By analogy, humans can target specific individuals or groups of individuals in order to have them adjust their behaviours to best suit the goals of the signaller. The competencies that a ToM affords the sender allows them to know that a receiver is capable of both decoding the signal and acting appropriately in response because they know of the receiver's receptive and causal states, both cognitive (hidden) and behavioural (overt). That is to say they take advantage of their ToM to understand how, when, what, and to whom they need to signal in order to achieve a beneficial outcome, whereas blind evolution would take much longer to achieve the same results.\n\nThe separation of time-scales also plays an important distinction between learning processes that use the same theoretical foundation. The mathematical description of the processes that underpin the evolution of species has been shown"}, {"title": "5 Discussion: Theory of Mind in the Context of Social \u0391\u0399", "content": "Each step in our cultural development has changed the ways in which we combine individual skills to achieve better, more sophisticated collective outcomes. There is evidence that hunter gatherers participated in the division of labour, complex social networks, multilevel, and fractal-like social structures long before we settled into villages [22, 29, 49, 40]. This was in part due to our ability to construct our own niches [61] and this practice has persisted from hunter-gathers to farming [92] through to civilisation building. As Arroyo-Kalin et al. [1] quote in their opening to the special issue Civilisation and Human Niche Construction:\n\nIt is impossible to avoid the conclusion that organisms construct every aspect of their environment themselves. They are not the passive objects of external forces, but the creators and modulators of these forces. The metaphor of adaptation must therefore be replaced by one of construction, a metaphor that has implications for the form of evolutionary theory (Levins and Lewontin 1985: 104).\n\nIt has also been argued that through the manipulation of our environmental niche we have brought about the Anthropocene [57, 100, 30, 118].\n\nOn the other hand, social niche construction [94] extends this idea to the process whereby agents modify their social context so as to influence their own social evolution. A social niche is the context in which social behaviour occurs, and so social niche construction is where agents actively choose to change their social environments, for example, by choosing who to associate with and how to behave [61]. Ryan et al. [94] describe this in game theoretical terms as the effective game agents are playing after all relevant factors are accounted for, such as the underlying game itself (the payoff matrices in conventional games for example) and any social niche modifiers, amongst others. A social niche modifier is a trait that alters the effective game being played, causing it to differ from the immediate payoff matrices that are usually the complete description of the incentivised interactions, for example population structure, relatedness, punishment etc. In the example in Section 4, A\u2081 constructs a social niche for itself by manipulating the structure of the game that A2 and A3 are playing, for the benefit of A\u2081 and incidentally for the benefit of the other agents as well. However, social niche construction theory pertains to evolution more broadly and is not specific to human social networks as the formal description of the model in Section 4 could be an evolved network of plants and animals or a more rapidly changing social network.\n\nIn moving towards AI that is situated within a hybrid human-AI ecology, the complexities of effective network construction, communication, and cognitive tools will need to be worked through. Here, we discuss just two of the issues: the difficulties of building psychologically complex AIs and the social environment AI will need to adapt to. The capacity of current AI theories to be sufficient to encompass, in principle at least, human levels of cognition is a rich area of research, from attention mechanisms [109], to reinforcement learning subsuming reasoning [99] and chain of thought reasoning in language models [73] there are arguments being made for emergent phenomena in AI [115] as well as TOM [103] and even artificial general intelligence [15]. What is often missed in these examples is that the individual AI's ability to digest information and update parameter weights is only a small fraction of what is needed to be effective in a specifically social context. An important tool in our psychological toolbox is our ability to maintain shared hidden variables that are the causal basis of our coordinated joint actions in physical and social environments. Even progress on single agents having effective causal models of the physical environment has been much slower than in other areas of AI [122]. As we have shown in this article, there is very good evidence that causal models, both physical and social, are necessary for people to be able to communicate with each other, and communication is inextricably tied to our ToM. This triad of language, shared causal models of hidden variables, and ToM appears to be a minimum for human social coordination.\n\nEven with this triad established within an AI, the next challenge is where, when, and how an AI should fit into any given collaborative social context. Simple machine intelligence in collaboration with people, such as auto-correct, recommender systems, or GPS navigation, are effective because a human decided there was a need for these tools, they placed them in the appropriate context and then the users adjust their behaviour to any shortcomings the tool might have. But the more autonomous machine intelligence becomes, the more complex the physical and social environment is, and the more trust that needs to be placed in the causal models that drive the behaviour of an AI (i.e. their analogue of a person's hidden cognitive variables) the more an AI needs to know how to interact with us in a way that resembles how we interact with each other. They will ultimately need to be able to do this via niche construction, niche adaptation, and niche choice, all of which, for people, is a negotiated relationship between each other that needs to be satisficing for those involved. This brings human-AI joint adaptation closer to what Laland et al. have in mind when considering a rethink of evolutionary theory [62]:"}]}