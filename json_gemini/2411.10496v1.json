{"title": "Guided Learning: Lubricating End-to-End Modeling for Multi-stage Decision-making", "authors": ["Jian Guo", "Saizhuo Wang", "Yiyan Qi"], "abstract": "Multi-stage decision-making is crucial in various real-world artificial intelligence applications, including recommendation systems, autonomous driving, and quantitative investment systems. In quantitative investment, for example, the process typically involves several sequential stages such as factor mining, alpha prediction, portfolio optimization, and sometimes order execution. While state-of-the-art end-to-end modeling aims to unify these stages into a single global framework, it faces significant challenges: (1) training such a unified neural network consisting of multiple stages between initial inputs and final outputs often leads to suboptimal solutions, or even collapse, and (2) many decision-making scenarios are not easily reducible to standard prediction problems. To overcome these challenges, we propose Guided Learning, a novel methodological framework designed to enhance end-to-end learning in multi-stage decision-making. We introduce the concept of a \"guide\", a function that induces the training of intermediate neural network layers towards some phased goals, directing gradients away from suboptimal collapse. For decision scenarios lacking explicit supervisory labels, we incorporate a utility function that quantifies the \"reward\" of the throughout decision. Additionally, we explore the connections between Guided Learning and classic machine learning paradigms such as supervised, unsupervised, semi-supervised, multi-task, and reinforcement learning. Experiments on quantitative investment strategy building demonstrate that guided learning significantly outperforms both traditional stage-wise approaches and existing end-to-end methods.", "sections": [{"title": "Introduction", "content": "Many real-world AI systems depend on multi-stage decision-making, where a sequence of predictions/decisions is made in order to output the final decision. Examples include search engine (Almukhtar, Mahmoodd, and Kareem 2021), recommendation systems (Ko et al. 2022), dialog system (McTear 2022), robotics (Zhao, Queralta, and Westerlund 2020), autonomous driving (Chen et al. 2024), drones (Kangunde, Jamisola Jr, and Theophilus 2021), AIGC systems (Foo, Rahmani, and Liu 2023), and quantitative trading (Wei, Dai, and Lin 2023).\n\u2022 Example 1: Traditional autonomous driving systems (Figure 1) consist of three key sequential stages: perception, prediction, and planning. The perception stage detects traffic lights and other critical elements, followed by the prediction stage, which forecasts the potential movements of surrounding entities. Finally, the planning module ensures that the vehicle's path prioritizes both safety and efficiency.\n\u2022 Example 2: In quantitative finance, the research pipeline typically includes the following sequential stages: factor mining (extracting informative features or trading signals), alpha prediction (building machine learning models to predict alphas), and portfolio optimization (finding the optimal asset positions to balance portfolio return and risk).\nAlthough traditional stage-wise approaches for multi-stage decision-making offer advantages such as transparency and flexibility, they suffer from several limitations. For instance, the optimization goals across these stages often lack consistency, leading to sub-optimal final solutions. Moreover, errors in one stage can accumulate and propagate through subsequent stages.\nAs an alternative to stage-wise approaches, end-to-end deep learning has demonstrated its efficiency and effectiveness in modeling by processing initial inputs directly to produce final outputs. However, end-to-end modeling faces significant challenges as well. Firstly, neural networks designed for end-to-end learning in multi-stage decision-making problems are typically deep, making model training and parameter tuning difficult. Secondly, many scenarios cannot be easily formulated as standard supervised learning problems. For example, when applying end-to-end modeling to a quantitative investment pipeline, it is challenging to explicitly formulate it as a supervised learning problem because ground-truth optimal positions are not observable and positions depend on the alphas predicted in previous layers of the end-to-end neural network.\nTo address these issues, we propose Guided Learning (GL), a new methodological framework for smoothing the end-to-end learning for multi-stage decision-making.\nOur main contributions are summarized as:\n\u2022 We propose a general machine learning framework that enhances the training effectiveness of end-to-end learning"}, {"title": "Guided Learning", "content": "In this section, we first formally define the learning paradigms of stage-wise learning and end-to-end learning. Next, we provide detailed training procedures for our guided learning approach. Finally, we discuss the relationship between our guided learning framework and other existing learning frameworks."}, {"title": "Preliminary", "content": "Many complex systems in real-world applications can be modeled as multi-stage decision problems. The stage-wise learning paradigm handles each stage by a separate model. These models are trained independently with unique objectives, forming a pipeline where the output of one model serves as the input for the next. This process can be formalized as:\n$x^{(i)} = q_i(x^{(i-1)}; \\theta_i), i = 1, ..., L$\nHere, the pipeline is divided into $L$ stages. The terms $x^{(i-1)}$ and $x^{(i)}$ represent the input and output of the $i$-th stage's model, respectively. The overall dataset is denoted as $D = {D_1, D_2, ..., D_L}$, where $D_i$ is the train dataset of the $i$-th stage model. Each stage's model, parameterized by $\\theta_i$, is trained separately as follows:\n$\\theta_1 = \\arg \\min_{\\theta_1} L_1(\\theta_1, D_1)$\n$\\theta_2 = \\arg \\min_{\\theta_2} L_2(\\theta_2, D_2)$\n...\nThe overall goal of the entire pipeline is then formulated through the following utility function:\n$\\theta_L = \\arg \\max_{\\theta_L} U(D_L;\\theta_L)$,\n(1)\nwhere $\\theta_L$ is the optimal parameters of the $L$-th stage's model, $U$ is the utility function, often a prediction error or cost to be minimized. For traditional supervised learning, $U$ is the expected utility over all samples. Although this pipelined approach can produce satisfactory outcomes, it has an inherent limitation. The training objectives of the intermediate models ($L_i$) might not align perfectly with the final optimization objective $U(\\cdot; \\cdot)$. This mismatch is referred to as the prediction-optimization gap (Yan et al. 2021).\nEnd-to-end learning has been proposed to align the training objective with the final goal. In an end-to-end learning paradigm, the whole problem is handled by a single model, parameterized by $\\theta$. The training objective can be formulated as:\n$\\theta^* = \\arg \\max_{\\theta} U (D, \\theta)$.\n(2)\nHowever, directly optimizing a unified model can be unstable due to the complexity of integrating multiple stages from the original input to the final outputs."}, {"title": "Formulation of Guided Learning", "content": "Considering the two approaches discussed above, our guided learning separates the end-to-end model into $L$ conceptual stages. Each stage is considered as handled by part of the machine learning model's parameter. In this way, the whole model can be separated into $L$ phases. Let $h_i \\in \\mathbb{R}^{N \\times d_i}$ denote the output of the $i$-th stage, where $N$ is the sample size and $d_i$ is the feature dimension of the $i$-th stage. Specifically, $h_0 = x$ is the raw input, and $h_L = \\hat{y}$ is the final output of the model. By definition, we have:\n$h_i = f_{\\theta_i}(h_{i-1}), i = 1, ..., L$\n(3)\nThe training objective is to maximize the final goal $U(h_L)$. Guided learning uses intermediate 'guidance' at the middle stages to prevent intermediate representations from collapsing while maintaining the flexibility of end-to-end learning.\nGuided Objective The guided objective at an intermediate stage $i$ consists of the following components: an optional guided head $g_\\phi$, the phased output $c_i$, the phased goal $y_i$, and the guided loss function $\\mathcal{L}(\\cdot, \\cdot)$ on $c_i$ and $y_i$. In the following, we will describe each component in detail."}, {"title": "Discussion", "content": "In this section, we first discuss the impact of our proposed phased goal on the final objective. Then, we compare our guided learning with other learning paradigms with similar architecture.\nInfluence of Guide In end-to-end learning, particularly in complex tasks such as multi-step investment strategy building, the loss landscape can be irregular. Adding phased goals aims to smooth this landscape, potentially guiding optimization to better parameter spaces and enhancing performance. However, a poor guide might lead to suboptimal areas, reduce performance, or have no significant impact if it adds no new information. The outcome depends on the specific problem, model architecture, and guidance choices, necessitating empirical studies to predict and manage these effects. In our experiments, we investigate multiple guidance choices in the domain of portfolio optimization and then give some insight into how to select a better guide.\nCompared with Other Learning Paradigm In this section, we compare our guided learning approach with existing learning paradigms that have similar architectures.\n\u2022 Multi-task learning. Multi-task learning (MTL) (Zhang and Yang 2021) and guided Learning both aim to enhance model performance by leveraging multiple objectives. However, several distinctions set them apart: 1) Sub-objective placement: MTL typically confines all auxiliary tasks to the final layer, using them to restrict the search space for a shared feature encoder. Guided Learning, on the other hand, distributes its objectives throughout the network according to the stages. 2) Parameter Sharing: MTL often involves extensive parameter sharing across tasks, which can result in a significant seesaw phenomenon (Yang, Lu, and Liu), where performance improvements in one task lead to declines in another. Guided Learning, on the other hand, adopts a more flexible approach that allows for stage-specific guidance, thereby diminishing such conflicts. Our experiments demonstrate that Guided Learning can more efficiently enhance the main task's performance compared to MTL.\n\u2022 Deep supervision. Deep Supervision (Wang et al. 2015) and guided learning both utilize intermediate loss to regularize the learning process. However, as illustrated in Figure 2, deep supervision regularizes each sample independently. In contrast, our guided learning approach employs diverse, stage-specific guidance objectives. These guidance objectives can target subsets of input samples, allowing for more tailored and effective regulation at different stages of the network.\n\u2022 Reinforcement learning. Guided learning and reinforcement learning (RL) differ in their approach to multi-step decision problems. While RL assigns credit to individual steps and optimizes them separately, guided learning maintains a holistic view of the trajectory, optimizing all steps simultaneously while using guides to enhance model training. This distinction has several implications: guided learning may better preserve temporal coherence in long-term strategies, potentially achieve higher sample efficiency (especially when the final outcome is more informative than intermediate rewards), and offer more stable learning in problems with sparse or delayed rewards. Additionally, the use of guides provides flexibility in incorporating domain knowledge, which can be challenging to integrate into RL via reward shaping."}, {"title": "Experiment", "content": "In this section, we present an empirical study of guided learning applied to a real-world quantitative investment problem. We will first introduce the background of the problem, followed by an analysis of the experimental results to evaluate the effectiveness of guided learning."}, {"title": "Problem Background", "content": "We select for the experiment the problem of cross-sectional investment strategy building. The objective is to make sequential (usually in hundreds of steps) investment decisions from a universe of financial instruments, with the aim of maximizing portfolio performance as measured by risk-adjusted metrics.\nAs illustrated in Figure 3, the traditional approach to this problem involves multiple stages: factor mining to extract relevant features from raw financial data, alpha prediction to forecast returns, and portfolio construction to determine optimal asset weights. While widely used in practice, this multi-stage approach typically requires substantial human expertise to optimize each component independently. Furthermore, the separate objectives for each stage may lead to misalignment in the overall process.\nIn response to these limitations, end-to-end learning approaches that directly map raw data to portfolio positions have also been studied. This method reduces the need for manual feature engineering and aligns the entire process with the final objective. However, training such end-to-end models presents new challenges due to the extended pipeline it covers and the instability in directly optimizing risk-adjusted portfolio metrics, which involve the standard deviation of portfolio returns over multiple holding periods.\nFormulation Consider features of a cross-section as a sliding window of stock data, $x \\in \\mathbb{R}^{N \\times W \\times F}$, where $N$ is the number of instruments in a cross-section, $W$ is the size of the lookback window, and $F$ is the number of features (e.g. open price, volume, etc.). In multi-stage methods, a predictive model $f_\\theta$ parameterized by $\\theta$ takes as input $x$ and predicts the cross-sectional expected return $\\hat{y} \\in \\mathbb{R}^{N}$. This prediction is then used in a portfolio optimizer to generate the final positions $w \\in [0, 1]^N$ by solving a convex quadratic optimization problem (Markowitz 1952). The model is trained by minimizing the prediction error, formulated as\n$\\theta^* = \\arg \\min_{\\theta} E_{x \\sim D}[d(f_{\\theta}(x), y)]$\n(5)\nwhere $D$ is the training set consisting of historical cross-sections, $\\delta(\\cdot,\\cdot)$ is certain distance metric such as mean-squared error or negative Pearson correlation coefficient (also known as information coefficient, IC), and $y$ is the label that is usually set as the actual future return. However, this training objective does not correspond to the final goal, risk-adjusted performance, such as the Sharpe ratio (Sharpe 1966). Therefore, in end-to-end modeling, the model $g_\\phi$ parametrized by $\\phi$ is expected to take as input the whole history used for training $X \\in \\mathbb{R}^{T \\times N \\times W \\times F}$ and directly generates consecutive positions $W \\in [0, 1]^{T \\times N}$, and is trained by maximizing the performance across whole history\n$\\phi^* = \\arg \\max_{\\Phi} U (W, R)$\n(6)\nwhere $U$ is a utility function such as Sharpe ratio, and $R \\in \\mathbb{R}^{T \\times N}$ is the actual return. In practice, due to computation complexity and memory limit, the objective in Eq. (6) cannot be directly optimized. It is then approximated by subsampling on history to get $x \\in \\mathbb{R}^{T' \\times N \\times W \\times D}$, where $T'$ is smaller than the length of the whole training set but also long enough for carrying out a reasonable estimate of the risk-adjusted return for stochastic optimization.\nTo implement guided learning in our end-to-end model, we conceptually divide the model into multiple interconnected components, as illustrated in Figure 3(c). This modular approach allows us to apply guides to intermediate representations at various stages of the model. For example, we can apply guides to the temporal embedding, enhancing its ability to predict absolute trends of individual financial instruments. Similarly, guides can be applied to the cross-sectional embedding, improving its capacity to maximize returns for each individual cross-section. By introducing these guides, we aim to leverage domain-specific knowledge and improve the model's performance at different stages of the investment process while maintaining the end-to-end nature."}, {"title": "Experimental Setup", "content": "Dataset We conducted our experiments using historical data from the Chinese A-share stock market, spanning from 2013-01-01 to 2022-01-04. The dataset was divided as follows: training set (2013-01-01 to 2020-06-30), validation set (2020-07-01 to 2020-12-31), and test set (the entire year of 2021). We utilized 500 proprietary meta features as initial inputs, including basic volume-price data and fundamental indicators. Prior to model input, we performed necessary data preprocessing and filtering.\nModel Architecture The end-to-end model architecture follows the design illustrated in Figure 3(c). For comparison, we also implemented a multi-stage predictive model consisting of an embedding layer, a temporal module, and a prediction head that directly maps temporal embeddings to predictions. Our default configuration includes:\n\u2022 Embedding layer: An MLP layer mapping meta features to 1024-dimensional latent embeddings, followed by a temporal encoding layer similar to that in (Zhou et al. 2021).\n\u2022 Temporal encoder: An MLP-Mixer (Tolstikhin et al. 2021) alternately applying to the last two dimensions.\n\u2022 Cross-sectional encoder: A multi-head self-attention (Vaswani et al. 2017) layer with increased dropout probability to promote sparsity of connections among stocks.\n\u2022 Position sizer: An LSTM operating on the first dimension to encode sequential information across multiple holding periods.\nEvaluation We employed several portfolio metrics to assess performance:"}, {"title": "Results", "content": "Overall Effectiveness We evaluated the performance of various portfolio construction approaches, comparing traditional multi-stage methods with guided learning techniques. For multi-stage approaches, we considered two settings: \"Optimization,\" which applies a portfolio optimizer to model predictions, and \"NN,\" which appends a neural network to trained predictive models. In the realm of end-to-end modeling approaches, we explored four variants. The \"Guide-free\" setting serves as our baseline, employing no additional guidance. The \"Multi-task\u201d setting incorporates the Information Coefficient (IC) as a guide added to the cross-sectional embedding. The \"IC-Guide\" approach adds the IC guide to the temporal embedding. Lastly, the \"IC-Guide + Return Guide\u201d setting combines the IC guide with an additional cross-sectional return guide on the cross-sectional embedding.\nResults in Table 1 demonstrate that guide-free end-to-end learning outperforms traditional stage-wise methods, achieving higher annualized returns (23.19% vs 17.97% for optimization-based and 12.41% for NN-based) and better risk-adjusted metrics (Sharpe ratio of 0.94 vs 0.73 and 0.52 respectively). The introduction of IC-Guide further enhances performance, particularly in controlling maximum drawdown (-7.70% vs -8.90% for guide-free) and improving the Calmar ratio (3.27 vs 2.58). Notably, the multi-task approach and the addition of a Return Guide to IC-Guide did not yield further improvements, suggesting that the benefits of guided learning are sensitive to the specific implementation. The stage-wise NN setting showed decreased performance compared to the optimization approach, possibly due to conflicting training objectives.\nGeneralization We examined the generalizability of guided learning across different model architectures. Our experiment involved three representative architectures as temporal encoders: LSTM for RNN, TCN for CNN, and PatchTST for Transformers. We applied the same IC-Guide to the temporal embedding of each model. Table 2 presents the results, demonstrating the effectiveness of the IC-Guide approach across various architectures. LSTM models showed improvements in all metrics, with a notable 12.30% increase in Calmar ratio. TCN models exhibited enhanced performance, particularly in Sharpe ratio (6.59% improvement) and Calmar ratio (11.84% improvement). PatchTST models showed modest gains in annualized return and Sharpe ratio, potentially requiring more tuning of the model configuration. The consistent performance enhancement across diverse models indicates that guided learning generalizes well across different model architectures.\nInfluence of Guide Placement Table 3 illustrates the effect of applying the IC-Guide at different stages of the model. Temporal embedding guidance yields the best overall performance, with the highest Sharpe ratio (0.88) and Calmar ratio (2.73). This suggests that guiding the model at the temporal level allows it to better capture time-dependent"}, {"title": "Guide Type Comparison", "content": "Table 4 compares different types of guides. Mean Squared Error (MSE) guidance yields the best overall performance, with the highest annualized return (18.94%) and Calmar ratio (2.06). Classification (CLF) guidance shows comparable performance to MSE, with slightly lower metrics across the board. Ranking-based guidance significantly underperforms, suggesting that this approach may not align well with the portfolio optimization objective. These results highlight the importance of choosing appropriate guidance objectives that align with the end goal.\nParameter Sensitivity We also studied the parameter sensitivity of guided learning to verify its effectiveness in lubricating end-to-end modeling. See more experimental results in the appendix."}, {"title": "Related Work", "content": "End-to-end Modeling With the growth in data and computational resources, end-to-end modeling has become increasingly prevalent in many domains, superseding multi-stage approaches that follow the predict-then-optimize paradigm (Elmachtoub and Grigas 2022). For instance, in autonomous driving, (Hu et al. 2023) proposes a Transformer-based end-to-end model that generates planned trajectories directly from perceptual input. This model is trained using both final planning utility and intermediate vision-based losses, including occlusion prediction and object detection. In quantitative investment, (Wei, Dai, and Lin 2023) introduces an end-to-end deep learning framework that generates positions from raw stock data, employing a training approach that combines final portfolio metrics with intermediate losses incorporating feature selection and inter-stock relation modeling. Other works (Liu, Roberts, and Zohren 2023; Nagy et al. 2023) also explore end-to-end approaches, demonstrating the growing trend towards unified modeling strategies.\nManipulating Intermediate Representations The concept of incorporating intermediate supervision in neural network training has a rich history. Deep supervision (Lee et al. 2015), extensively studied in computer vision (Shen et al. 2019; Ren et al. 2023; Zhang et al. 2022), aims to stabilize neural network training by applying auxiliary (self-)supervised losses to intermediate embeddings. Recently, similar principles of interpreting and controlling intermediate representations have been explored in large language models (Gao et al. 2024; Templeton et al. 2024). These studies utilize sparse autoencoders to map intermediate representations of large-scale Transformer models to discrete \"concept\" vectors. Notably, (Templeton et al. 2024) demonstrated that manipulating these intermediate representations at the concept level can lead to controlled outputs, offering potential benefits in scenarios such as AI safety."}, {"title": "Conclusions and Future Works", "content": "To conclude, guided learning has the potential to enhance training stability, performance, and interpretability across various complex real-world AI applications. Meanwhile, we currently envision several key aspects for further investigation:\n1. Designing guides: Current approaches rely on ad-hoc manual design with domain expertise. Future research may focus on developing principled methods for creating effective guides, including automated techniques and investigating their transferability across different applications.\n2. Analyzing guide effectiveness: A deeper understanding of why guided learning works is crucial. This includes theoretical analyses of how guides influence the optimization landscape and empirical studies on their impact on model convergence and performance. Such investigations could potentially draw from optimization theory and information geometry to provide a solid theoretical foundation for guided learning. Exploring the interplay between guides and the primary optimization goal could lead to more nuanced strategies, potentially uncovering synergies that enhance overall system performance. Furthermore, comparative studies across various problem domains could help identify the characteristics of effective guides in different contexts.\n3. Integration with more scenarios: Potential applicability to other complex, end-to-end scenarios such as autonomous driving, and robotics control. These domains share characteristics that make them suitable for guided learning, which could offer a middle ground between traditional pipelines and end-to-end paradigms."}, {"title": "Additional Experimental Details", "content": "Dataset\nWe used historical dataset of Chinese A-share market, including over 4200 stocks from 2013-01-01 to 2022-01-04. Features in this dataset contains volume-price data, fundamental data, and dummy variables such as industry and country. We set the lookback window to 10 days and predict horizon as 1 day, meaning that at each day after market close, we take the 10-day historical features of each stock and predict their 1-day forward close-to-close return tomorrow.\nData preprocessing All features are first wonorized with the upper and lower limit set to 0.1 times the cross-sectional sample median, in order to remove outliers. Then the data is transformed into cross-sectional z-scores with a clip bound of 3. Finally, the NaNs in input features are filled with Os.\nSampling We filtered out samples that are non-tradable in each trading day. For multi-stage decision models, we applied the cross-sectional sub-sampling technique. At each iteration, a trading day is randomly sampled. Then we randomly pick 80% of all the stocks on that trading day as a cross-section sample. Compared with full cross-sectional sampling, such sampling greatly enriches the sample size. For guided learning models, we picked 22 consecutive trading days to compute the Sharpe ratio throughout the whole period, and the sub-sampling ratio is adjusted to 10% due to GPU memory limit.\nModel Details\nWe trained the model with learning rate of le-4 and a maximum epoch number of 200, with early-stopping patience of 10. Models are validated each 0.5 training epochs. The temporal encoder has 3 layers. The cross-sectional has only 1 layer, and the LSTM model also has 1 layer. All model parameter counts total up to 8 million.\nEvaluation Details\nLet $r_t$ denote the excess return of a portfolio over the benchmark at day $t \\in \\{1,...,T\\}$. Then the annualized return is computed as\nAnnualized Return =\n$\\frac{238}{T}\\sum_{t=1}^T{r_t}$\nwhere 238 indicates the number of trading days in a year. The maximum drawdown is computed as\nMaximum Drawdown\n=$\\min_{t \\in \\{1,...,T\\}} (\\frac{max_{j \\in \\{1,...,t\\}} \\sum_{i=1}^{j}r_i - \\sum_{i=1}^{t}r_i}{max_{j \\in \\{1,...,t\\}} \\sum_{i=1}^{j}r_i})$\nThe mean and standard deviation of daily excess returns is computed as\n$\\mu = \\frac{1}{T}\\sum_{t=1}^T{r_t}$\n$\\sigma = \\sqrt{\\frac{1}{T} \\sum_{t=1}^T{(r_t - \\mu)^2}}$\nwhere $\\mu$ is the average daily excess return. The Sharpe ratio is then computed as\nSharpe Ratio =$\\sqrt{238} \\frac{\\mu}{\\sigma}$\nFinally, the Calmar ratio is computed as\nCalmar Ratio = $\\frac{Annualized Return}{Maximum Drawdown}$\nThe Calmar ratio measures the risk-adjusted return of a portfolio by comparing the annualized return to the maximum drawdown. A higher Calmar ratio indicates a better performance considering the drawdown risk."}]}