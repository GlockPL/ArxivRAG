{"title": "Substrate Scope Contrastive Learning:\nRepurposing Human Bias to Learn Atomic\nRepresentations", "authors": ["Wenhao Gao", "Priyanka Raghavan", "Ron Shprints", "Connor W. Coley"], "abstract": "Learning molecular representation is a critical step in molecular machine learning\nthat significantly influences modeling success, particularly in data-scarce situations.\nThe concept of broadly pre-training neural networks has advanced fields such as com-\nputer vision, natural language processing, and protein engineering. However, similar\napproaches for small organic molecules have not achieved comparable success. In this\nwork, we introduce a novel pre-training strategy, substrate scope contrastive learning,\nwhich learns atomic representations tailored to chemical reactivity. This method con-\nsiders the grouping of substrates and their yields in published substrate scope tables as\na measure of their similarity or dissimilarity in terms of chemical reactivity. We focus\non 20,798 aryl halides in the CAS Content CollectionTM spanning thousands of publi-\ncations to learn a representation of aryl halide reactivity. We validate our pre-training\napproach through both intuitive visualizations and comparisons to traditional reac-\ntivity descriptors and physical organic chemistry principles. The versatility of these\nembeddings is further evidenced in their application to yield prediction, regioselectivity", "sections": [{"title": "Introduction", "content": "Encoding molecular structures into appropriate numerical representations that are computer-\nreadable is key to accurate prediction of molecules' reaction behavior, which is a pivotal\nchallenge in chemical science and engineering. Traditionally, explaining reactivity based on\nchemical structure has relied on physical modeling and mechanistic analysis, along with the\ncomputation of key physical descriptors to build statistical models. 1\u20134 This \u201cfeature engi-\nneering\" approach relies on a prior understanding of relevant (computable) properties, which\nvary across different classes of molecules and reactions. In contrast, the advent of machine\nlearning (ML)5 and, notably, deep learning (DL),6 has facilitated end-to-end learning as an\nalternative to manual crafting of molecular features. In principle, DL models can extract\ntask-specific information from chemical structures, forming numerical features known as rep-\nresentations or embeddings. While already applied in modeling reaction yields, 7-9 most DL\nmodels remain remarkably ineffective or inaccurate in low data regimes.\nThe paradigm of pre-training deep neural networks on tasks with abundant data and\nadapting them to other tasks10-14 offers an attractive approach to enhance efficiency and\ngeneralizability. Ideally, such strategies enable few-shot learning, where models can learn\nnew concepts from just a handful of examples. 15,16 Successes in this approach are evident\nacross computer vision, 17-20 natural language processing, 21-23 and protein engineering. 24,25\nInspired by these successes, numerous network pre-training strategies for molecular structures\nhave been developed. 26\u201330 However, unlike in other domains, these methods have not yet led\nto substantial improvements in modeling. 31\nWe believe that there is a fundamental misalignment between existing pre-training tasks"}, {"title": "Substrate Scope Contrastive Learning", "content": "The goal of our pre-training approach is to derive a multidimensional vector representation\nfor the reactive atom in an aryl halide substrate that captures essential aspects of its reactiv-\nity. Ideally, this could be accomplished through supervised learning using a vast and varied\ndataset of molecules and their respective yields across numerous reaction conditions. How-\never, the reality of data acquisition presents us with substrate scope tables that represent\na narrow selection of molecules, influenced by the selective reporting tendencies and norms\nin scientific research and publication (see Figure S2 for statistics of the dataset). The core\nidea of substrate scope contrastive learning (ContraScope) is to use molecules not listed in a\nscope table as negative samples, leveraging the artificial bias in the substrate scope selection.\nThe model is trained on triplet of molecules: for each molecule as an anchor, we randomly\nselect another molecule from the same substrate scope to serve as a positive sample, ensuring"}, {"title": "Results", "content": "Learned aryl halide embeddings exhibit alignment with qualitative\nand quantitative measures of reactivity\nTo evaluate the consistency of the learned aryl halide representations with human under-\nstanding of reactivity, we perform a combination of qualitative and quantitative analyses.\nWe first visualize the learned embeddings to elucidate how the model has learned to organize\nthe chemical space of aryl halides. To this end, we encoded a set of molecules comprising\nthe 500 most frequently used aryl halides, augmented by a random sample of 2922 aryl\nhalides from the substrate scope dataset. Upon encoding, we used t-distributed stochas-\ntic neighbor embedding (t-SNE) 50 projection to visualize the 64-dimensional embedding\nlearned by the model in Figure 2A (see Figure S8 for results of other projection methods\nand a comparison with embeddings from an untrained network). A detailed examination of\nthe position of specific structures in the embedding space (referred to as call-outs in Figure\n2A) reveals that molecules with qualitatively similar reactivities\u2014characterized by either\nelectron-withdrawing groups (e.g., nitro, aldehyde, carbonyl) or electron-donating groups\n(e.g., hydroxyl, ether, amine, alkyl)-tend to cluster together. This pattern is consistent\nacross various halide classes, underscoring the broad applicability of the embeddings we\nhave developed.\nIn order to validate whether quantitative distances in the learned embedding space ac-\ncurately reflect known functional similarity in chemical reactivity, we next encoded a set of\n12 hand-selected para-substituted bromobenzenes, then calculated and visualized their pair-"}, {"title": "Pre-trained embeddings help enable various downstream applica-\ntions", "content": "The primary motivation for pursuing better molecular representations through pre-training\nis to enable downstream applications, particularly in low data regimes. We therefore select\nthree example use cases with which to illustrate the promise of substrate scope contrastive\nlearning.\nYield prediction. As a first case study, we examine the prediction of reaction yields of\nvarious aryl bromides under identical reaction conditions. We select a Ni/photoredox cat-\nalyzed cross-coupling reaction, with reported yields for a diverse set of substrates published\nafter 2015,3 and therefore not contained in our pre-training set. Kariofillis et al. report\nachieving a validation r\u00b2 of 0.57 with a univariate model based on a computed DFT descrip-\ntor, specifically electronegativity. Without being trained on any physical organic chemistry\nconcepts or features (like electronegativity) and without requiring the cost of electronic struc-\nture calculations, our learned ContraScope embedding is able to achieve a comparable r2 of\n0.51 (Figure 4A). Other common fixed or pre-trained representations based on structure are\nfar less successful.\nRegioselectivity prediction. As a second case study, we applied our embeddings to\ncompare likelihood of multiple reactive sites within a molecule. We focused on a palladium-\ncatalyzed Suzuki-Miyaura coupling reaction using polyfluoronitrobenzenes. 60 The model was\ntrained with yield data from penta- and trifluoronitrobenzenes, and used to predict the reac-"}, {"title": "Discussion", "content": "We have demonstrated that substrate scope tables from journal articles, known for being\nsmall and biased towards high-yielding examples, are in fact valuable sources of information\nfor molecular representation learning. This is the first approach to utilize these groupings"}, {"title": "Methods", "content": "Substrate data for training In this study, we focus on aryl halides. We partitioned\nreactions recorded in the CAS Content Collection between 2010 and 2015 into substrate scope\ntables comprised of reactions from the same publication source, wherein the only variable is a\nsingle aryl halide substrate. All other substrates and recorded conditions remained constant\nwithin each scope. Importantly, all reactions take place at an aryl C-X bond. Scopes with\nfewer than 5 reactions and reactions without recorded yields were excluded. This led to\nour final training dataset of 20,798 distinct aryl halides, covering 64,192 reactions and 6,919\nsubstrate scopes.\nCalculation of reactivity descriptors All reactivity descriptors are calculated by density-\nfunctional theory (DFT) with Gaussian 1656 via an automated descriptor generation pipeline\nbuilt on top of AutoQChem.57 For each unique aryl halide, up to 20 conformers were gen-\nerated using RDKit's ETKDG algorithm63 and optimized using the MMFF94 force field. 64\nTo reduce computational overhead when using DFT, the lowest-energy conformer for each\nmolecule was then selected via GFN2-xTB.65 Any conformer for which any energy calcu-\nlation did not converge was discarded. The lowest-energy conformer for each aryl halide\nthen underwent geometry optimization and frequency calculations in Gaussian 16 using the\nB3LYP functional66,67 with the 6-31G*68 basis set. Atoms with atomic number > 35 use the\nLANL2DZ69 basis set instead. This led to the generation of 25 molecule-level descriptors\nper molecule (energies, energy corrections, dipole moment, HOMO/LUMO energies, elec-\ntronegativity, etc.) and 19 atom-level descriptors per atom per molecule (buried volume,\npartial charges, NMR shielding constants, etc.), and the relevant atom-level descriptors for", "equations": []}, {"title": "Featurization and network architecture", "content": "We adopt a graph representation of molecules\nand the graph isomorphism network (GIN) 48 for modeling them. GIN is one kind of graph\nneural network that updates the representation of each atom over multiple iterations as\nfollows:", "equations": ["h_v^{(k)} = {\\text{MLP}}^{(k)} \\left((1 + \\epsilon^{(k)}) h_v^{(k-1)} + \\sum_{u \\in N(v)} h_u^{(k-1)} \\right) \\qquad (2)", "d(m_i, m_j) = d_{\\text{SNR}}(f_i, f_j) = \\frac{\\text{var}(f_i)}{\\text{var}(f_j - f_i)} \\qquad (3)"]}, {"title": "Substrate scope contrastive learning", "content": "The training of substrate scope contrastive learn-\ning involves triplet sampling, computing loss function depicted in Eq. 1, and back-propagation.\nWithin an epoch, each aryl halide serves as the anchor molecule once and 16 triplets are\nsampled randomly for each anchor. We exclude all cases with identical molecules sampled in\none triplet. Based on empirical performance, we adopt a distance metric called signal-noise\nratio distance (SNR), 70 defined as follows:", "equations": ["\\mathcal{L}(m_a, m_p, m_n) = \\frac{(\\text{d}(m_a, m_p) - \\gamma |y_a - y_p|)^2}{\\text{Distance proportional to yield difference}} + \\beta \\text{log}[1 + \\text{exp}(M - \\text{d}(m_a, m_n))] \\qquad (1)"]}, {"title": "Additional Results", "content": "Intuitive investigation of the learned embeddings\nTo offer an intuitive examination of the bit-level details of the embeddings, we illustrate\nthe bit value of the corresponding position in the embeddings with a heatmap and their\nhierarchical clustering in Figure S7. We analyzed the same set of aryl bromides as Figure\n2B-C.\nVisualization of learned aryl halide chemical spaces\nTo offer insights into the evolution of the embeddings during training, we present the com-\nparative analysis of our embeddings using PCA, t-SNE, and UMAP projections, both pre-\nand post-training, in Figure S8. This analysis utilizes the identical set of molecules featured\nin Figure 2A."}]}