{"title": "RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question Answering", "authors": ["Yang Bai", "Christan Grant", "Daisy Zhe Wang"], "abstract": "Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text and images, has gained significant attention in information retrieval (IR) and natural language processing (NLP). Traditional ranking methods rely on small encoder-based language models, which are incompatible with modern decoder-based generative large language models (LLMs) that have advanced various NLP tasks. To bridge this gap, we propose RAMQA, a unified framework combining learning-to-rank methods with generative permutation-enhanced ranking techniques. We first train a pointwise multi-modal ranker using LLaVA as the backbone. Then, we apply instruction tuning to train a LLaMA model for re-ranking the top-k documents using an innovative autoregressive multi-task learning approach. Our generative ranking model generates re-ranked document IDs and specific answers from document candidates in various permutations. Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant improvements over strong baselines, highlighting the effectiveness of our approach. Code and data are available at: https://github.com/TonyBY/RAMQA", "sections": [{"title": "1 Introduction", "content": "Multi-modal retrieval-augmented question answering (MRAQA) involves searching and integrating information from diverse modalities such as text and images (Talmor et al., 2021; Chang et al., 2021). This capability is crucial for applications requiring comprehensive understanding and reasoning. While powerful generative language models have revolutionized NLP, achieving state-of-the-art results across various tasks (Wu et al., 2024; Touvron et al., 2023; Liu et al., 2023), leveraging these advanced LLMs for information retrieval tasks like MRAQA remains challenging.\nExisting MRAQA methods rely on small encoder-based ranking models (Hu et al., 2022b; Yang et al., 2023a,b), which are not fully compatible with modern large generative language models. Although recent generative LLMs trained on massive datasets have dominated NLP tasks, they are typically decoder-only, making it challenging to encode documents into dense representations as encoder-based models do.\nGenerative retrieval paradigms (Metzler et al., 2021; Tay et al., 2022; Wang et al., 2022b) differ from traditional retrieval methods by directly generating relevant document identifiers for a query. However, applying these methods to multi-modal information retrieval faces challenges: (1) multi-modal documents have aspects not effectively represented by static identifiers; (2) existing multi-modal LLMs are not structured or pretrained to infer across multiple multi-modal documents; (3) LLMs' limited input sequence length hinders ranking many documents in a single run."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Multi-Modal Retrieval-Augmented Question Answering", "content": "Multi-modal retrieval-augmented question answering (MRAQA) integrates information from various modalities, such as text, images, and tables, to answer complex questions. Benchmark datasets like MultimodalQA (Talmor et al., 2021) and WebQA (Chang et al., 2021) have been developed to address these challenges.\nRecent frameworks like MuRAG (Hu et al., 2022b), SKURG (Yang et al., 2023a), and PERQA (Yang et al., 2023b) have made significant strides in MRAQA by integrating text and image data using retrieval and generation techniques. However, these methods primarily rely on encoder-based models and structured knowledge, limiting their ability to fully leverage the capabilities of state-of-the-art multi-modal generative LLMs. Our work addresses this gap by introducing a novel framework that combines traditional ranking with multi-modal generative LLMs, offering a more robust solution for MRAQA."}, {"title": "2.2 Learning-to-Rank", "content": "Learning-to-Rank (LTR) techniques optimize item ranking in information retrieval systems based on relevance. These models include pointwise (Cossock and Zhang, 2006; Liu, 2009; Li, 2011; Nogueira and Cho, 2019; Nogueira et al., 2019), pairwise (Freund et al., 2003; Clark et al., 2020; Li et al., 2023a), and listwise (Cao et al., 2007; Ai et al., 2019; Zhang et al., 2018) approaches. The advent of Transformer encoders like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) has significantly enhanced LTR by enabling more accurate relevance scoring.\nRecent advancements have explored using large language models (LLMs) in LTR. For example, RankLLaMA (Ma et al., 2024) fine-tuned the LLaMA model, demonstrating that decoder-based LLMs can surpass traditional encoder-based models in ranking tasks. Building on this, we fine-tuned LLaVA (Liu et al., 2023), a multi-modal LLM that combines LLaMA with the CLIP visual encoder ViT-L/14 (Dosovitskiy et al., 2021), creating RankLLaVA, a multi-modal pointwise ranker that enhances ranking performance by leveraging both language and visual data."}, {"title": "2.3 Generative Retrieval", "content": "Generative retrieval techniques (Tay et al., 2022; Tang et al., 2023; Bevilacqua et al., 2022; Zhang et al., 2024; Li et al., 2023b) represent a shift from traditional retrieval methods by directly generating document identifiers (DocIDs) for a query using generative models. Advances like Differentiable Search Index (DSI) (Tay et al., 2022) and SEAL (Bevilacqua et al., 2022) have introduced more efficient and effective retrieval processes. However, these methods primarily focus on unimodal data and often struggle with integrating multi-modal information.\nOur work addresses this limitation by introducing a unified framework that combines multi-modal pointwise learning-to-rank with generative ranking in a two-stage retrieval process, effectively bridging the gap in multi-modal retrieval."}, {"title": "3 Methodology", "content": "In this section, we provide a comprehensive description of our proposed framework designed to address multi-modal learning-to-rank and generative retrieval tasks. We start by defining these tasks and then explore the structure and training methodologies of our unified framework, as outlined in Figure 2."}, {"title": "3.1 Preliminaries", "content": ""}, {"title": "3.1.1 Task Definition", "content": "Given a question Q and a set of input documents D = {d1,d2,...,dn}, where n represents the number of documents and each document may be a text with a title or an image with a caption, MRAQA aims to retrieve evidence from D and generate an answer A based on the retrieved evidence. Although the MRAQA task can encompass other document modalities like tables, audio, and video,"}, {"title": "3.1.2 LLaMA", "content": "LLAMA (Touvron et al., 2023) is a large language model based on the Transformer architecture, operating in an auto-regressive, decoder-only manner. With billions of parameters, it is pre-trained on a massive dataset of web content. As a uni-directional model, its attention mechanism only considers the preceding elements in the input sequence to make predictions. Specifically, for a given input sequence s = [t1,t2,..., tn\u22121], the model predicts the next token tn based solely on the prior tokens. This prediction process is mathematically expressed as P(tn|t1, t2, ..., tn\u22121), where P denotes the probability of the next token tn in the sequence."}, {"title": "3.1.3 LLaVA", "content": "LLaVA (Liu et al., 2023) extends the LLaMA model to handle multi-modal inputs, specifically text and images, by incorporating a vision encoder alongside its Transformer-based architecture. LLaVA retains the auto-regressive, decoder-only structure for text generation, while its vision encoder, often based on a pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2021), processes"}, {"title": "3.2 RankLLaVA for Multi-modal Pointwise Ranking", "content": "Our first-stage ranking model, named RankLLaVA, is trained as a pointwise ranker. This method involves feeding both the query and a candidate document into the model, which then generates a relevance score indicating how well the document matches the query (Nogueira and Cho, 2019). The backbone model is initialized with LLaVA.\nTraditionally, pointwise ranking models use bi-directional encoder-only models like BERT, where the [CLS] token is added at the beginning of the input sequence, and its hidden representation is used to represent the entire sequence. In contrast, since LLaVA is unidirectional, we append an end-of-sequence token (</s>) to the input query or document, and the hidden representation of this </s> token is used to represent the input sequence in LLaVA.\nRankLLaVA is trained on query-document pairs as detailed in Algorithm 1. To compute the query-document similarity score, we utilize the LLaVA model's image encoder, tokenizer, and decoder as described in (Liu et al., 2023). We process the input through these components to obtain the hidden representations of the tokens. Specifically, we extract the hidden representation of the last token in the sequence from the decoder's last layer. This representation is then passed through a linear layer, and a sigmoid activation function is applied to produce"}, {"title": "3.3 Multi-task Generation", "content": "We now introduce the second stage of our framework, which functions as a multi-task generator for second-stage ranking and question answering. This stage is designed to accurately identify the correct documents from the top-k candidates predicted by the first-stage ranker that can assist in answering the question. Simultaneously, it generates the answer based on the identified documents. We experimentally show that this additional objective makes the model's ranking performance more robust."}, {"title": "3.3.1 Data Unification", "content": "We begin by unifying data from different modalities by converting images to text using a pre-trained LLaVA model with a customized prompt, following the format used during LLaVA's training. Fig-"}, {"title": "3.3.2 RAMLLAMA", "content": "Our second-stage ranking and question-answering model, RAMLLaMA (Retrieval-Augmented Multi-task LLaMA), is trained autoregressively using instruction tuning (Ouyang et al., 2022). Given a prompt comprising a question and the top-k unified candidate documents from the first-stage ranking, along with their IDs, the model generates the relevant document IDs and the answer.\nTo prevent the model from overfitting to the sequence of input documents, we permute the candidate documents five times for each question during"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "We conduct experiments on two widely used MRAQA datasets: WebQA (Chang et al., 2021) and MultimodalQA (Talmor et al., 2021). The dataset statistics are presented in Table 1."}, {"title": "4.1.1 WebQA", "content": "WebQA (Chang et al., 2021) contains multi-hop, multi-modal question-answer pairs, where each query, typically requiring 1-2 images or text documents, is paired with around 40 multi-modal distractors (hard negatives). Although the input sources are multi-modal, the questions are entirely text-based. Answers are free-form sentences. Evaluation metrics include source retrieval F1 and a QA score, which combines BARTScore-based (Yuan et al., 2021) fluency and relevance (QA-FL) with keyword accuracy (QA-Acc). The overall QA score, a product of QA-FL and QA-Acc, is the key metric for WebQA."}, {"title": "4.1.2 MultimodalQA", "content": "MultimodalQA (Talmor et al., 2021) contains multi-modal QA pairs across tables, texts, and images, with 16 question types, 13 of which require cross-modal retrieval and reasoning. As tables are outside the scope of our paper, following (Hu et al., 2022b) we focus on the subset of queries involving only text and image information, specifically selecting questions labeled as 'TextQ' or \u2018ImageQ'. Each query typically requires 1 image and/or 1 text snippet to answer and is paired with around 20 visual and text distractors. Since test set labels are unavailable, we report RAMQA results on the validation set. The answers are spans or short phrases, and the evaluation metrics are Exact Match (EM) and average F1 as described in (Dua et al., 2019)."}, {"title": "4.2 Baselines", "content": "We compare RAMQA against SOTA models\u00b2 on WebQA and MultimodalQA in an distractor setting, i.e., the input documents are positives and hard negatives provided by the datasets, rather than the entire document corpus."}, {"title": "4.2.1 AutoRouting", "content": "AutoRouting (Talmor et al., 2021) converts multi-modal QA into unimodal QA by using a question-type classifier to identify the modality likely to contain the final answer. It directs the question and input sources to the appropriate QA module (textQ, tableQ, or imageQ) and extracts answer spans using specialized sub-models. AutoRouting employs RoBERTa-large (Liu et al., 2019) for question-type classification as well as textQ and tableQ, while VILBERT-MT (Lu et al., 2019) handles imageQ with image features extracted by Faster R-CNN (Ren et al., 2015)."}, {"title": "4.2.2 VLP and VLP + VinVL", "content": "Leveraging VinVL (Zhang et al., 2021) for image feature extraction, these transformer-based encoder-decoder models begin by concatenating each document with the question and employing a classifier"}, {"title": "4.2.3 MuRAG", "content": "MuRAG (Hu et al., 2022b) is pre-trained on a combination of large-scale image-text and text-only corpora. It retrieves the Top-K nearest neighbors from a memory of image-text pairs using a query Q from any modality. The retrieved results are combined with Q and fed into an encoder-decoder for answer generation. During fine-tuning, the question is used as the query Q along with the Top-4 retrieved sources, and a beam search with size 2 is applied. MuRAG uses ViT-large (Dosovitskiy et al., 2021) for image encoding and T5-base (Raffel et al., 2019) for text encoding and answer generation. MuRAG is evaluated only on the text and image subsets of MultimodalQA, excluding the table modality."}, {"title": "4.2.4 SKURG", "content": "SKURG (Yang et al., 2023a) integrates evidence features using entity relations and feeds them into a transformer to generate key evidence and answers. It employs OFA-base (Wang et al., 2022a) as the image encoder and BART-base (Lewis et al., 2019) for text and knowledge graph encoding. The BART decoder is then used to generate the relevant document IDs and answers from the encoded documents. The BART-base model is pre-trained on SQUAD2.0 (Rajpurkar et al., 2018). For entity and relation extraction, SKURG uses ELMo-based (Peters et al., 2018) NER (Peters et al., 2017) and OpenNRE (Han et al., 2019), respectively."}, {"title": "4.2.5 PERQA", "content": "PERQA (Yang et al., 2023b) is a framework for evidence retrieval and question answering. After preprocessing all images by generating descriptions using image captioning with OFA (Wang et al., 2022a) and object detection with Fast RCNN (Girshick, 2015), it performs iterative pairwise ranking using BERT (Devlin et al., 2019), followed by an extra \"evidence refinement\" using pointwise reranking with Deberta-large (He et al., 2021). Once the top candidate documents are retrieved, PERQA integrates them into a dialogue format and fine-tunes a multi-modal LLM mPLUG-Owl (Ye et al., 2023), to generate answers based on the retrieved documents and the question."}, {"title": "4.3 Implementation Details", "content": "The backbone of RankLLaVA is based on the LLaVA-1.5-7B model\u00b3 (Liu et al., 2024). We added a linear layer to project the final layer's end-of-sequence token representation into a scalar, as detailed in section 3.2. Parameter-efficient fine-tuning (PEFT) techniques, including Quantization (Jacob et al., 2017) and low-rank adaptation (LoRA) (Hu et al., 2022a), were used to fine-tune the model on a single NVIDIA A100 80GB GPU with a maximum input sequence length of 2048, a batch size of 8, and gradient accumulation steps of 4. With LoRA, only the linear layer parameters of the LLM were updated, while all other layers, including the visual encoder, were kept frozen.\nThe backbone of RAMLLAVA is based on the LLaMA-3-70B model4 (Dubey et al., 2024). Following the instruction tuning approach outlined in Section 3.3.2, we fine-tuned the model using similar PEFT methods. This enabled fine-tuning on a single NVIDIA A100 80GB GPU with a maximum input sequence length of 8192 tokens. We employed a batch size of 2 with 16 gradient accumulation steps. The input data comprised the top 15 ranked documents."}, {"title": "4.4 Main Results", "content": "We compare RAMQA against the most relevant methods, including SOTA models.\nTable 2 presents the results on WebQA. For the QA score, which is the most critical metric in the WebQA benchmark (described in Section 4.1.1), RAMQA outperforms all baselines, exceeding the"}, {"title": "4.5 Ablation Studies", "content": ""}, {"title": "4.5.1 Effectiveness of Permutation-Based Generative Retrieval and Multi-Task Objective Generation.", "content": "In this section, we investigate the impact of Permutation-based Generative Retrieval and the multi-task objective generation on the final MRAQA results over the WebQA test set. As shown in Table 4, without the generative retrieval objective, our second-stage generation model achieves an overall QA score of only 40.4. The retrieval F1 score here reflects the ranking performance of our first-stage model, RankLLaVA. Documents are selected if their binary classification confidence exceeds a specified threshold, determined through tuning on the WebQA development set.\nWhen we introduce the retrieval generation objective during the fine-tuning of our second-stage generative model, both the QA score and retrieval F1 score see significant improvements. Specifically, the QA score increases by 14.4%, and the retrieval F1 score rises by 14.3%. This demonstrates the effectiveness of the multi-task objective generation in enhancing the model's generative capabilities.\nFurthermore, by introducing the permutation of candidate documents in the training data, the retrieval F1 score is boosted by an additional 2.6%, and the QA score improves by 4.1%. This indicates that permutation-based generative retrieval not only enhances the model's retrieval performance but also contributes to a better understanding of context, thereby improving overall QA performance."}, {"title": "4.5.2 Impact of Document Count on Ranking Effectiveness.", "content": "We investigated the impact of the number of input documents on the performance of our second-stage generation model, RAMLLaMA. Figure 4 illustrates how varying the number of documents in RAMLLaMA's input affects its final performance. We found that increasing the input document count from 1 to 15 improved retrieval performance, suggesting that the model benefited from the higher recall provided by the larger document set. However, increasing the input to 20 documents resulted in a performance decline. This drop is likely due to the lack of additional recall from the top 20 retrieved documents compared to the top 15, combined with the inclusion of less relevant documents, which made it more challenging for RAMLLaMA to process the input effectively, potentially leading to overfitting on irrelevant details."}, {"title": "5 Conclusion", "content": "In this paper, we introduced RAMQA, a unified framework for Retrieval-Augmented Multi-modal Question Answering that combines traditional learning-to-rank methods with generative permutation-enhanced ranking techniques to address the challenges of multi-modal retrieval-augmented question answering. By leveraging state-of-the-art generative LLMs like LLaVA and LLaMA, RAMQA significantly improves both retrieval accuracy and question-answering performance across diverse data sources, including text and images.\nExperiments on two MRAQA benchmarks, WebQA and MultiModalQA, demonstrate significant improvements compared to strong baselines, highlighting the effectiveness of our approach in enhancing multi-modal retrieval-augmented QA systems. The introduction of permutation-based generative retrieval and multi-task learning objectives played a key role in these advancements, contributing to a better understanding of context and more accurate information retrieval.\nIn conclusion, RAMQA sets a new benchmark in multi-modal question answering, demonstrating the effectiveness of combining traditional and generative approaches. We anticipate that RAMQA and similar models will continue to advance the capabilities of multi-modal information retrieval and generation."}, {"title": "Limitations", "content": "While RAMQA demonstrates strong performance and introduces several innovative techniques in multi-modal question answering, it is important to acknowledge its limitations: (1) Dependency on High-Quality Multi-Modal Data: RAMQA's performance is closely tied to the quality and diversity of the multi-modal data available during training. In scenarios where such data is scarce or noisy, the model's ability to accurately retrieve and generate relevant answers may degrade. This limitation is particularly evident in domains where multi-modal datasets are limited or not well-structured. (2) Generalization to Novel Domains: While RAMQA has demonstrated strong results on the WebQA and MultimodalQA datasets, its ability to generalize to entirely new domains or query types remains uncertain. The model may struggle with domain-specific terminology or data formats that were not encountered during training, limiting its applicability in specialized fields. (3) Bias and Ethical Concerns: Despite its sophisticated design, RAMQA is not immune to biases present in the training data. These biases can be reflected in the retrieval and generation processes, leading to outputs that may reinforce existing stereotypes or omit crucial perspectives. Addressing these ethical concerns requires further research and careful consideration.\nBy recognizing these limitations, we hope to guide future research efforts aimed at overcoming these challenges and improving the robustness, scalability, and ethical integrity of multi-modal question answering systems like RAMQA."}, {"title": "C Scientific Artifacts", "content": "The licenses for the resources used in this paper are as follows: MultiModalQA (MIT License), WebQA (CC0-1.0 License), LLaVA (Llama 2 Community License), LLaMA (Llama 3 Community License Agreement), and Huggingface Transformers (Apache License 2.0). We have adhered to the intended use of all referenced artifacts in this paper."}]}