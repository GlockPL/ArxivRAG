{"title": "ODGR: Online Dynamic Goal Recognition", "authors": ["Matan Shamir", "Matthew E. Taylor", "Osher Elhadad", "Reuth Mirsky"], "abstract": "Traditionally, Reinforcement Learning (RL) problems are aimed at optimization of\nthe behavior of an agent. This paper proposes a novel take on RL, which is used\nto learn the policy of another agent, to allow real-time recognition of that agent's\ngoals. Goal Recognition (GR) has traditionally been framed as a planning problem\nwhere one must recognize an agent's objectives based on its observed actions. Re-\ncent approaches have shown how reinforcement learning can be used as part of the\nGR pipeline, but are limited to recognizing predefined goals and lack scalability in\ndomains with a large goal space. This paper formulates a novel problem, \"Online\nDynamic Goal Recognition\" (ODGR), as a first step to address these limitations.\nContributions include introducing the concept of dynamic goals into the standard\nGR problem definition, revisiting common approaches by reformulating them using\nODGR, and demonstrating the feasibility of solving ODGR in a navigation do-\nmain using transfer learning. These novel formulations open the door for future\nextensions of existing transfer learning-based GR methods, which will be robust to\nchanging and expansive real-time environments.", "sections": [{"title": "Introduction", "content": "Goal Recognition plays a crucial role in a variety of domains in Human-Robot Interaction (Massardi\net al., 2020; Jiang et al., 2021; Shvo et al., 2022; Geib, 2002; Inam et al., 2018) and Multi-Agent\nSystems (Avrahami-Zilberbrand & Kaminka, 2005; Freedman & Zilberstein, 2017; Su et al., 2023).\nReasoning about the goal of another agent presents an important challenge that complements an\nego agent's policy learning and can inform and leverage RL techniques by facilitating significant\nbreakthroughs in how machines perceive and interpret other agents, especially human intentions.\nExisting GR approaches commonly assume that a fixed set of goals is provided as part of the problem\nformulation (Mirsky et al., 2021; Meneguzzi & Pereira, 2021). In recent GR approaches that leverage\nReinforcement Learning (RL), which are also called GR as RL (Amado et al., 2022), a policy is\nlearned for each goal during a dedicated learning phase, often occurring offline before the GR task.\nThen, during the inference phase, the algorithm is given a sequence of observed actions and uses\ninference methods to perform GR. Unlike planning-based GR (Ram\u00edrez & Geffner, 2010; Meneguzzi\n& Pereira, 2021), these approaches have two fundamental limitations: first, all possible goals must\nbe specified before the inference phase. This requirement may not hold in dynamic environments\nwhere goals can emerge or change over time. Second, an issue of scalability arises when dealing with"}, {"title": "Preliminaries", "content": "The basic model used to describe a domain is a Markov Decision Process (MDP) M, which\nis a 4-tuple (S, A, P, R) such that S represents a set of states in the environment, A is the set of\nactions the agent can execute, P is a transition function, and R is a reward function. A transition\nfunction P(s'|s, a) returns the probability of transitioning from state s to state s' after taking action\na, and a reward function R(s, a, s') returns the reward obtained when an agent transitions from s to\ns' using action a. The utility function Q (or Q-function) is a function Q : S \u00d7 A \u2192 R that defines\nthe expected utility of executing an action a \u2208 A in state s\u2208 S. AQ-learning agent estimates the\nvalues of Q(s, a) from its experiences. A Policy \u03c0 : S \u00d7 A \u2192 [0, 1] represents the probability that\nan agent would choose to perform an action a \u2208 A in state s \u2208 S.\nThe Goal Recognition (GR) problem consists of two agents: an actor and an observer. The GR prob-\nlem is defined from the observer's perspective, who needs to recognize the actor's goal. Recognizing\nsomeone's goal can be challenging, particularly when these goals undergo sudden and unanticipated\ntransformations. This work relies on a line of work in which a GR algorithm produces a set of\npolicies or behaviors, one per goal (Polyvyanyy et al., 2020; Ko et al., 2023; Chiari et al., 2023). We\nfollow the utility-based GR problem formulation from Amado et al. (2022):"}, {"title": "Online Dynamic Goal Recognition", "content": "Certain methodologies for GR rely on the notion that inputs to the problem arrive at distinct time\nsteps, along with varying assumptions regarding their nature, such as whether they constitute goals\nor new observations. For example, in Plan Recognition as Planning (Ram\u00edrez & Geffner, 2010), it is\nexpected that the description of the environment will be decoupled from a specific problem instance,\nand different GR problems can be defined for the same environment. We now present a formulation\nthat aims to identify the different components that can change within each problem, potentially\nfacilitating the selection of a more appropriate framework based on the expected scenarios the user\nmay encounter, and the requirements from the expected solution.\na\nDefinition 2 An Online Dynamic Goals Recognition (ODGR) problem is a tuple\n(T, (G\u00b2, {O}')i\u22081..n), where T = \u3008S, A) is a domain theory such that S is the set of all possi-\nble states and A is the set of actions, Gi is a set of goals such that di \u2208 {1..n},G\u00b2 \u2286 S, and\n{O} represents a set of observations sequences (which might contain gaps), where each Oj \u2208 {0}\nis a single observation sequence Oj = (0,03...) = {\u3008s}, a}), (sz, az),...). Aligning with other\nonline problems, each input is given at an increasing time-step, lexicographically ordered, while it is\ninherent that because of the input dependency, T,Grand O; \u2208 {O}' arrive at increasing time-steps\nVi,j. For each two sequences of observations o, om \u2208 Ot of, of \u2208 O; where t < j, l < m, and z < k,\nit holds that the observations are given chronologically, such that o\u00a6 < or < o <o\n=.\nAn algorithm for the ODGR problem is expected to return a goal g\u2208 Gr that best ex-\nplains O; \u2208 {0} upon its arrival for all i,j. The final output is a set of goals G*\n{{91, 92, ...}, {91, 92, ...}, ..., {91, 92, ...}} where g is the goal returned by the algorithm upon re-\nceiving Oj \u2208 {0}\u00b2. Using this formulation, one can now define a general GR domain according to\nthe domain theory T. In this work, the domain consists of an MDP, but the formulation allows for\ndifferent representations as well. For each domain, several instances can be constructed at different\ntimes according to the set of potential goals Gr the actor may be pursuing. For each instance, a"}, {"title": "Model-Based GR (MBGR)", "content": "In MBGR, the recognition process entails utilizing a pre-defined model that encompasses the char-\nacteristics of an environment along with the actions that can be executed within that environ-\nment (Mirsky et al., 2021; Meneguzzi & Pereira, 2021; Masters & Vered, 2021). Traditional MBGR\noften exploits planning and parsing techniques (Avrahami-Zilberbrand & Kaminka, 2005; Geib &\nGoldman, 2009; Geib, 2009; Ramirez & Geffner, 2009; Mirsky & Gal, 2016; Son et al., 2016). A\nmajor limitation of such approaches is their inflexibility in dynamic domains and that a complete\nmodel is crucial to solving the GR problem. Such algorithms require complex computation processes\nto calculate the likelihood of the goal given the observations. These computations are required for\nevery given observation sequence, making these solutions less suitable for real-time performance.\nBaker et al. (2009) and Ramirez & Geffner (2011) propose approaches to perform GR over MDPs and\nPartially Observable MDPs (POMDPs) using planners, which generally incur lower computational\ncosts than training RL agents. However, it is worth noting that MDP and POMDP planners may\nnot scale as effectively as classical planners. The use of MDPs enables a framework to incorporate\nuncertainty modeling and, despite being model-based, opens a door for employing RL to solve GR.\nThese approaches require significant computation for every observation sequence at Inference Time."}, {"title": "Model-Free GR (MFGR)", "content": "A Model-Free GR problem (MFGR) (Geffner, 2018) is one in which the recognizer does not have\naccess to the underlying model that describes the properties and dynamics of the environment. While\nsome approaches learn the model dynamics and then employ MBGR methods (Asai & Fukunaga,\n2018; Amado et al., 2018), other approaches perform GR directly, without learning the model of the\nworld. Among these approaches, the main techniques include leveraging RL (Amado et al., 2022)\nor employing deep neural networks to perform GR using a classification network (Min et al., 2014;\nBorrajo et al., 2020; Chiari et al., 2023). These methods typically have no Domain Learning Time,\nas the learning process depends on the set of goals, a fairly long Goals Adaptation Time, and a short\nInference Time when given an observation sequence.\nDifferent approaches have been proposed for learning an MFGR classification model using languages\nlike PDDL (Geib & Kantharaju, 2018; Maynard et al., 2019; Borrajo et al., 2020; Min et al., 2014;\n2016; Fang et al., 2023). These approaches aim for generality by employing a simple domain file that\nspecifies actions using labels and defines a state space based on a finite set of fluents (facts). Chiari\net al. (2023) proposed a framework (termed GRnet) that generalizes to any set of goals without\nadditional learning, presenting a model-free solution suitable for environments with changing goals.\nCompared to the other approaches, GRnet excels in ODGR, demonstrating immediate adaptation to\nnew goal sets and fast inference for new observations through a forward pass of the trained network.\nWhile this approach is promising, its reliance on fluent enumeration is critical for its success.\nA separate line of work uses MDPs instead of planning languages to model the environment. Amado\net al. (2022) presented an MDP-based method termed GR as RL, which trains RL agents during\nDomain Adaptation Time and computes the likelihood of such agents to produce the sequence of\nobservations given at Inference Time. In this method, the sharing of information across various\nGR problems constructed by different observation traces with identical domain theories and goals\nis inherent. This sharing facilitates short Inference Time. This approach integrates the domain\ntheory with the problem instance by incorporating the policies of agents for each goal within their\ndomain theory. The fact that learning typically occurs only after receiving the domain theory along\nwith a set of goals limits the framework to recognizing goals only from this set. This framework\ndemonstrates high effectiveness in an OSGR setting, However, as these approaches are not designed\nto handle dynamic environments if employed in an ODGR context, every new set of goals necessitates\nexpensive re-training of agents for each distinct goal."}, {"title": "Dynamic Goal Recognition using Transfer Learning", "content": "To tackle ODGR problems, we introduce a general algorithm based on transfer learning between\nsource and target RL tasks (Taylor & Stone, 2009). It facilitates learning in new tasks by transferring\nrelevant parts from the policies of the source tasks. This algorithm selects specific goals, Gb, after\nreceiving the domain theory T, and trains agents with policies IG for each of them as part of the\nDomain Learning time. Upon receiving a set of goals Ga, the algorithm creates a set of policies\nIG for each new goal by leveraging transfer learning from the base goals' trained agents, as part\nof the Goals Adaptation time. When given an observation trace of an actor, the algorithm returns\nthe most likely goal by using distance metrics, as part of the Inference time. For convenience, we\nrefer to the original set of goals, Gb, and the policies of their agents, IG, as base goals and base\npolicies. Similarly, the new goals, Ga, in the adaptation phase are referred to as dynamic goals, and\nthe policies generated for them, Iga, are called dynamic policies."}, {"title": "Implementation in a Navigational Domain", "content": "We implement GATLing using a simple navigational domain without obstacles to show feasibility.\nWe use the Gym MiniGrid navigational domain (Chevalier-Boisvert et al., 2023) as a case study, in\nwhich we conduct a comparative analysis of the different techniques' respective performances.\nWe implement Algorithm 1 using Q-Learning, as suggested in the GRAQL framework of Amado\net al. (2022). As part of Algorithm 2, for policy transfer, we assign utilities to states and actions by\napplying weights to the expected utilities of actions from states in the Q-functions of the base goals.\nThe weights assigned to these Q-functions are determined by the similarity of the path from the\nstate to the base goal and the path to the dynamic goal, as perceived at each state individually. The\nnotion of similarity can also be interpreted as distance in navigational domains. In the navigational\ndomain, we used the distance metric as a similarity metric (where a lower distance means greater\nsimilarity). We employed two different methods to calculate this distance:\nStatic weights are assigned between each pair of base on dynamic goal, according to their Euclidean\ndistance. The weight assignment is considered static as the weight for each action utility remains\nunchanged regardless of the states' spatial proximity to the dynamic goal. However, this approach"}, {"title": "", "content": "introduces challenges, notably because it applies equal weights to all Q-values for every state in\nrelation to a dynamic goal. As a result, in certain states, there may be a tendency to move in\ndirections contrary to those leading to the dynamic goal, deviating from the optimal policy. This\nissue often arises from the proximity of the dynamic goal to another foundational goal in the opposite\ndirection within these states.\nDynamic assigns variable weights to each state's utility. This weight assignment is based on the\ncosine similarity between the trajectory from the current state at which the Q-function was being\ncalculated to the dynamic goal and the trajectory from this state to each base goal. The equation\nthat describes the dynamic approach based on cosine similarity is denoted as follows:\nCosine_Similarity(s, gb, ga) = \\frac{traj(s, g_b) \\cdot traj(s, g_a)}{||traj(s, g_b) || \\cdot ||traj(s, g_a) ||}\nwhere s is the current state, go is a base goal, ga is a dynamic goal, and traj(s, gt) represents a 2D\nvector originating in s and ending at gr. The cosine similarity between two trajectories ranges from\n-1 (perfectly opposite directions) to 1 (perfectly aligned directions), with 0 indicating orthogonality.\nTo combine the policies of the base goals into a single policy for the dynamic goal, we employed\nthree different aggregation techniques of the Q-values:\nWeighted Average (normalize) is a fair and accurate approach, but slow: \\sum_{i=1}^n \\frac{n w_i Q_i(s,a)}{\\sum_{j=1}^i w_j}\nSoftmax is an accurate approach, but slow, and highly variable: \\sum_{i=1}^n \\frac{e^{w_i \\cdot Q_i(s,a)}}{\\sum_{j=1}^n Q_j(s,a)}\nMax (Hard Weighted Average) is a greedy approach, fast but less accurate: max_{i=1}^n Q_i(s, a)"}, {"title": "Empirical Evaluation", "content": "In this section, we present the results of our experiments, which evaluate GATLing in different\nscenarios. Our evaluation focuses on four key evaluation metrics: accuracy, precision, recall, and\nF-score. As the output of GATLing is a distribution over a set of goals rather than a label, we\nexplain the implementation of each metric in Appendix \u0410.\nEvaluating policy transfer can be complex, as it varies greatly given the domain's dynamics and the\nproperties of the source policies. Instead of running batches of runs on an arbitrary subspace of poli-\ncies, we focus our experiments on two clear and controlled use cases: the first presents a completely\nsmooth environment (without sudden changes in Q-values) and the second breaks this smoothness\nproperty. Both experiments employed Q-learning for learning, and during the inference stage, we\nleveraged KL-divergence to compare each policy with a pseudo-policy based on the observations\nreported by Amado et al. (2022). The hyperparameters were set according to that work, and were\nconsistent across both experiments: \u03b1 = 1 \u00d7 10\u22123, \u03f5 = 1 \u00d7 10\u22128, \u03b3 = 0.99, and Episodes = 107.\nThe base goals used for the first experiment were (1,6), (6,6), and (6,1), and for the second one\n(1,7), (7,7), and (7,1). The results are reported for partial traces of 0.1, 0.3, and 0.5, as traces with\na completeness score of 0.7 or higher achieved a 100% success rate across all four metrics in all\nour experiments, for both GRAQL and GATLing. (fuller traces showed absolute success for both\nGRAQL and GATLing). We also implemented a scaling factor that prioritizes and emphasizes each\nstate's highest-ranked action (or actions) while reducing the Q-values of other actions in the same\nstate. Appendix B shows an example of the Q-table generated with and without this scaling."}, {"title": "Results", "content": "Tables 1 and 2 show the results from Experiment 1 and Experiment 2, respectively. The dynamic\napproach significantly outperformed the static approach, highlighting the importance of adaptability\nin ODGR. When using the dynamic approach, the softmax and normalized techniques were always\nable to effectively combine policies from base goals' agents into coherent policies for dynamic goals.\nIn contrast, the max aggregation could not rank the true goal as the most likely one. We, therefore,"}, {"title": "Conclusion", "content": "This paper presented the novel problem of ODGR, which emphasizes the time in which different\nparts of the input are received, and the potential gain from policy transer. It formally defines\nODGR problems and how they compare to most existing GR problems. Then, the paper provides\nan overview of existing GR algorithms and their computational effort distribution according to\nODGR's components. Lastly, the paper introduces GATLing, a general algorithm for performing\nODGR in 2D navigational domains, with an implementation using two distance metrics and three\naggregation techniques for the policy ensemble. Our experiments demonstrate that in navigational\ndomains without obstacles, GATLing offers effective ODGR capabilities, with accurate recognition\nperformance and significantly reduced run-time compared to existing RL recognition approaches.\nWhile GATLing provides a foundation for future algorithms, there are several clear areas for potential\nimprovements. First, the proposed aggregation techniques are expected to fail in domains that\nconsist of rapid changes in Q values. Moreover, it is not straightforward how this approach can\nbe extended to continuous state and action spaces. Nevertheless, by avoiding the lengthy policy\nlearning phases. GATLing ensures that the recognition process remains efficient and scalable, even\nin environments where goals may emerge or evolve. As the field of data-driven GR develops, we\nanticipate that the need for faster goal learning will increase. This research is expected to contribute\nto developing adaptable and efficient GR systems in dynamic, changing environments."}, {"title": "Evaluation Metrics Definition", "content": "As the output of our GATLing is a distribution over a set of goals rather than a label, we explain\nhow we implemented each measure:\n\u2022 Accuracy measures the proportion of correctly recognized dynamic goals. A goal is con-\nsidered correctly recognized if it has the highest likelihood of being the actor's goal in\ncomparison to other goals.\n\u2022 Precision indicates how many of the most likely goals were the actor's goal.\n\u2022 Recall indicated how many of the actor's true goals were correctly identified.\n\u2022 F-score is the harmonic mean of Precision and Recall."}, {"title": "Scaling Factor for Q-function Aggregation", "content": "Sometimes, the desired action may not have the highest Q-value after aggregation. This phenomenon\ncan sometimes be attributed to how two base goals' policies bias the dynamic policy in a slightly\nwrong direction. To mitigate this issue, we implemented a scaling factor that prioritizes and empha-\nsizes each state's highest-ranked action (or actions) while reducing the Q-values of other actions in\nthe same state. Figures 2 (left) and 2 (right) show the crafting of a dynamic goal (4,4) Q-table for\na 8 \u00d7 8 Minigrid environment without obstacles. The experimental setup is the same as experiment\n1. These scaling methods fine-tune the Q-values for particular actions for every state to ensure\nconsistent and optimal behavior of the agent."}]}