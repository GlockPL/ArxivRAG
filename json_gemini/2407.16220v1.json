{"title": "ODGR: Online Dynamic Goal Recognition", "authors": ["Matan Shamir", "Matthew E. Taylor", "Osher Elhadad", "Reuth Mirsky"], "abstract": "Traditionally, Reinforcement Learning (RL) problems are aimed at optimization of the behavior of an agent. This paper proposes a novel take on RL, which is used to learn the policy of another agent, to allow real-time recognition of that agent's goals. Goal Recognition (GR) has traditionally been framed as a planning problem where one must recognize an agent's objectives based on its observed actions. Recent approaches have shown how reinforcement learning can be used as part of the GR pipeline, but are limited to recognizing predefined goals and lack scalability in domains with a large goal space. This paper formulates a novel problem, \"Online Dynamic Goal Recognition\" (ODGR), as a first step to address these limitations. Contributions include introducing the concept of dynamic goals into the standard GR problem definition, revisiting common approaches by reformulating them using ODGR, and demonstrating the feasibility of solving ODGR in a navigation domain using transfer learning. These novel formulations open the door for future extensions of existing transfer learning-based GR methods, which will be robust to changing and expansive real-time environments.", "sections": [{"title": "1 Introduction", "content": "Goal Recognition plays a crucial role in a variety of domains in Human-Robot Interaction (Massardi et al., 2020; Jiang et al., 2021; Shvo et al., 2022; Geib, 2002; Inam et al., 2018) and Multi-Agent Systems (Avrahami-Zilberbrand & Kaminka, 2005; Freedman & Zilberstein, 2017; Su et al., 2023). Reasoning about the goal of another agent presents an important challenge that complements an ego agent's policy learning and can inform and leverage RL techniques by facilitating significant breakthroughs in how machines perceive and interpret other agents, especially human intentions.\nExisting GR approaches commonly assume that a fixed set of goals is provided as part of the problem formulation (Mirsky et al., 2021; Meneguzzi & Pereira, 2021). In recent GR approaches that leverage Reinforcement Learning (RL), which are also called GR as RL (Amado et al., 2022), a policy is learned for each goal during a dedicated learning phase, often occurring offline before the GR task. Then, during the inference phase, the algorithm is given a sequence of observed actions and uses inference methods to perform GR. Unlike planning-based GR (Ram\u00edrez & Geffner, 2010; Meneguzzi & Pereira, 2021), these approaches have two fundamental limitations: first, all possible goals must be specified before the inference phase. This requirement may not hold in dynamic environments where goals can emerge or change over time. Second, an issue of scalability arises when dealing with domains that encompass many goals. GR requires significant computational effort and resources for considering each individual goal, posing a challenge for large-scale applications. This problem is further exacerbated in learning-based GR methods, where training an agent for each goal becomes prohibitively costly and infeasible due to the computational burden and time required for training.\nConsidering these two limitations, the first contribution of this paper is conceptual: it formulates a problem called Online Dynamic Goal Recognition (ODGR) with a temporal setting, where the time steps in which inputs are given are specified. By generalizing the definition of GR, this paper can identify the strengths and limitations of various GR frameworks across different settings and assumptions. For example, consider the potential locations that a researcher attending the RLC 2024 conference will choose to visit during their day off. First, a recognition task should focus on navigation in Massachusetts's towns. Once inside Amherst, one might recognize the locations people are moving towards to identify which attraction they are interested in. Lastly, if the researcher is visiting the Beneski Museum of Natural History, the recognition task changes to recognize the specific exhibition that the researcher is interested in. One alternative is representing each task as a different recognition problem, yet this solution means that a new set of goals needs to be learned for each new task. This paper suggests a formulation where these tasks can all be captured using a single recognition problem with a single set of states and actions and changing goals.\nThe second contribution of this paper is algorithmic: we offer a general algorithm that leverages Transfer Learning between RL tasks and demonstrates the feasibility of a future framework that would implement this solution by presenting a proof-of-concept for simple navigational environments. The suggested algorithm expands the capacity of traditional learning-based GR methods by adapting to \"dynamic goals\" - goals not set before the inference phase. For this simple navigational domain, we propose a heuristic method that aggregates existing Q-functions into new Q-functions for the dynamic goals, which can adequately facilitate recognition. In the proposed solution, existing Q-functions are learned before any observations are given, and they are specifically designed to represent the expected policies for a predefined set of base goals. The success of our solution is measured by the ability of the recognizer to use the generated policies when performing the recognition process and succeed in inferring the correct goal.\nBy introducing the concept of dynamic goals and presenting an approach for policy transfer in navigational domains, we show that GR methods can be extended to recognize and adapt to goals that emerge or change over time, thereby broadening the range of problems and real-time domains to which these methods can be applied. This research provides a significant first step towards a more robust and adaptable GR system capable of handling dynamic environments."}, {"title": "2 Preliminaries", "content": "The basic model used to describe a domain is a Markov Decision Process (MDP) M, which is a 4-tuple (S, A, P, R) such that S represents a set of states in the environment, A is the set of actions the agent can execute, P is a transition function, and R is a reward function. A transition function P(s's, a) returns the probability of transitioning from state s to state s' after taking action a, and a reward function R(s, a, s') returns the reward obtained when an agent transitions from s to s' using action a. The utility function Q (or Q-function) is a function $Q : S \u00d7 A \u2192 R$ that defines the expected utility of executing an action a \u2208 A in state s\u2208 S. AQ-learning agent estimates the values of Q(s, a) from its experiences. A Policy $\u03c0 : S \u00d7 A \u2192 [0, 1]$ represents the probability that an agent would choose to perform an action a \u2208 A in state s \u2208 S.\nThe Goal Recognition (GR) problem consists of two agents: an actor and an observer. The GR problem is defined from the observer's perspective, who needs to recognize the actor's goal. Recognizing someone's goal can be challenging, particularly when these goals undergo sudden and unanticipated transformations. This work relies on a line of work in which a GR algorithm produces a set of policies or behaviors, one per goal (Polyvyanyy et al., 2020; Ko et al., 2023; Chiari et al., 2023). We follow the utility-based GR problem formulation from Amado et al. (2022):"}, {"title": "3 Online Dynamic Goal Recognition", "content": "Certain methodologies for GR rely on the notion that inputs to the problem arrive at distinct time steps, along with varying assumptions regarding their nature, such as whether they constitute goals or new observations. For example, in Plan Recognition as Planning (Ram\u00edrez & Geffner, 2010), it is expected that the description of the environment will be decoupled from a specific problem instance, and different GR problems can be defined for the same environment. We now present a formulation that aims to identify the different components that can change within each problem, potentially facilitating the selection of a more appropriate framework based on the expected scenarios the user may encounter, and the requirements from the expected solution.\nDefinition 2 An Online Dynamic Goals Recognition (ODGR) problem is a tuple $(T, (G^i, \\{O^j\\})_{i\\in 1..n})$, where $T = \\langle S, A\\rangle$ is a domain theory such that S is the set of all possible states and A is the set of actions, $G^i$ is a set of goals such that $i \\in \\{1..n\\}$,$G^i \\subseteq S$, and $\\{O^j\\}_i$ represents a set of observations sequences (which might contain gaps), where each $O^j \\in \\{O\\}$ is a single observation sequence $O^j = (o_1, o_2, ...) = (\\{(s_1, a_1)\\}, \\{(s_2, a_2)\\}, ...)$. Aligning with other online problems, each input is given at an increasing time-step, lexicographically ordered, while it is inherent that because of the input dependency, T,$G^i$and $O_j \\in \\{O\\}^i$ arrive at increasing time-steps $Vi,j$. For each two sequences of observations $o_t, o_l \\in O_i^t, o_j^z \\in O_j^z$ where t < j, l < m, and z < k, it holds that the observations are given chronologically, such that $o_t^i < o_l^j < o_z^k < o_m^o$.\nAn algorithm for the ODGR problem is expected to return a goal $g \\in G^i$ that best explains $O^j \\in \\{O\\}^i$ upon its arrival for all i,j. The final output is a set of goals $G^* = \\{\\{g_1, g_2, ...\\}, \\{g_1, g_2, ...\\}, ..., \\{g_1, g_2, ...\\}\\}$ where g is the goal returned by the algorithm upon receiving $O^j \\in \\{O\\}^i$. Using this formulation, one can now define a general GR domain according to the domain theory T. In this work, the domain consists of an MDP, but the formulation allows for different representations as well. For each domain, several instances can be constructed at different times according to the set of potential goals $G^i$ the actor may be pursuing. For each instance, a"}, {"title": "3.1 Model-Based GR (MBGR)", "content": "In MBGR, the recognition process entails utilizing a pre-defined model that encompasses the char- acteristics of an environment along with the actions that can be executed within that environ- ment (Mirsky et al., 2021; Meneguzzi & Pereira, 2021; Masters & Vered, 2021). Traditional MBGR often exploits planning and parsing techniques (Avrahami-Zilberbrand & Kaminka, 2005; Geib & Goldman, 2009; Geib, 2009; Ramirez & Geffner, 2009; Mirsky & Gal, 2016; Son et al., 2016). A major limitation of such approaches is their inflexibility in dynamic domains and that a complete model is crucial to solving the GR problem. Such algorithms require complex computation processes to calculate the likelihood of the goal given the observations. These computations are required for every given observation sequence, making these solutions less suitable for real-time performance.\nBaker et al. (2009) and Ramirez & Geffner (2011) propose approaches to perform GR over MDPs and Partially Observable MDPs (POMDPs) using planners, which generally incur lower computational costs than training RL agents. However, it is worth noting that MDP and POMDP planners may not scale as effectively as classical planners. The use of MDPs enables a framework to incorporate uncertainty modeling and, despite being model-based, opens a door for employing RL to solve GR. These approaches require significant computation for every observation sequence at Inference Time."}, {"title": "3.2 Model-Free GR (MFGR)", "content": "A Model-Free GR problem (MFGR) (Geffner, 2018) is one in which the recognizer does not have access to the underlying model that describes the properties and dynamics of the environment. While some approaches learn the model dynamics and then employ MBGR methods (Asai & Fukunaga, 2018; Amado et al., 2018), other approaches perform GR directly, without learning the model of the world. Among these approaches, the main techniques include leveraging RL (Amado et al., 2022) or employing deep neural networks to perform GR using a classification network (Min et al., 2014; Borrajo et al., 2020; Chiari et al., 2023). These methods typically have no Domain Learning Time, as the learning process depends on the set of goals, a fairly long Goals Adaptation Time, and a short Inference Time when given an observation sequence.\nDifferent approaches have been proposed for learning an MFGR classification model using languages like PDDL (Geib & Kantharaju, 2018; Maynard et al., 2019; Borrajo et al., 2020; Min et al., 2014; 2016; Fang et al., 2023). These approaches aim for generality by employing a simple domain file that specifies actions using labels and defines a state space based on a finite set of fluents (facts). Chiari et al. (2023) proposed a framework (termed GRnet) that generalizes to any set of goals without additional learning, presenting a model-free solution suitable for environments with changing goals. Compared to the other approaches, GRnet excels in ODGR, demonstrating immediate adaptation to new goal sets and fast inference for new observations through a forward pass of the trained network. While this approach is promising, its reliance on fluent enumeration is critical for its success.\nA separate line of work uses MDPs instead of planning languages to model the environment. Amado et al. (2022) presented an MDP-based method termed GR as RL, which trains RL agents during Domain Adaptation Time and computes the likelihood of such agents to produce the sequence of observations given at Inference Time. In this method, the sharing of information across various GR problems constructed by different observation traces with identical domain theories and goals is inherent. This sharing facilitates short Inference Time. This approach integrates the domain theory with the problem instance by incorporating the policies of agents for each goal within their domain theory. The fact that learning typically occurs only after receiving the domain theory along with a set of goals limits the framework to recognizing goals only from this set. This framework demonstrates high effectiveness in an OSGR setting, However, as these approaches are not designed to handle dynamic environments if employed in an ODGR context, every new set of goals necessitates expensive re-training of agents for each distinct goal."}, {"title": "4 Dynamic Goal Recognition using Transfer Learning", "content": "To tackle ODGR problems, we introduce a general algorithm based on transfer learning between source and target RL tasks (Taylor & Stone, 2009). It facilitates learning in new tasks by transferring relevant parts from the policies of the source tasks. This algorithm selects specific goals, $G_b$, after receiving the domain theory T, and trains agents with policies $I_G$ for each of them as part of the Domain Learning time. Upon receiving a set of goals $G_a$, the algorithm creates a set of policies $I_{Ga}$ for each new goal by leveraging transfer learning from the base goals' trained agents, as part of the Goals Adaptation time. When given an observation trace of an actor, the algorithm returns the most likely goal by using distance metrics, as part of the Inference time. For convenience, we refer to the original set of goals, $G_b$, and the policies of their agents, $I_G$, as base goals and base policies. Similarly, the new goals, $G_a$, in the adaptation phase are referred to as dynamic goals, and the policies generated for them, $I_{Ga}$, are called dynamic policies."}, {"title": "4.1 Implementation in a Navigational Domain", "content": "We implement GATLing using a simple navigational domain without obstacles to show feasibility. We use the Gym MiniGrid navigational domain (Chevalier-Boisvert et al., 2023) as a case study, in which we conduct a comparative analysis of the different techniques' respective performances.\nWe implement Algorithm 1 using Q-Learning, as suggested in the GRAQL framework of Amado et al. (2022). As part of Algorithm 2, for policy transfer, we assign utilities to states and actions by applying weights to the expected utilities of actions from states in the Q-functions of the base goals. The weights assigned to these Q-functions are determined by the similarity of the path from the state to the base goal and the path to the dynamic goal, as perceived at each state individually. The notion of similarity can also be interpreted as distance in navigational domains. In the navigational domain, we used the distance metric as a similarity metric (where a lower distance means greater similarity). We employed two different methods to calculate this distance:\nStatic weights are assigned between each pair of base on dynamic goal, according to their Euclidean distance. The weight assignment is considered static as the weight for each action utility remains unchanged regardless of the states' spatial proximity to the dynamic goal. However, this approach"}, {"title": "4.2 Empirical Evaluation", "content": "In this section, we present the results of our experiments, which evaluate GATLing in different scenarios. Our evaluation focuses on four key evaluation metrics: accuracy, precision, recall, and F-score. As the output of GATLing is a distribution over a set of goals rather than a label, we explain the implementation of each metric in Appendix \u0410.\nEvaluating policy transfer can be complex, as it varies greatly given the domain's dynamics and the properties of the source policies. Instead of running batches of runs on an arbitrary subspace of poli- cies, we focus our experiments on two clear and controlled use cases: the first presents a completely smooth environment (without sudden changes in Q-values) and the second breaks this smoothness property. Both experiments employed Q-learning for learning, and during the inference stage, we leveraged KL-divergence to compare each policy with a pseudo-policy based on the observations reported by Amado et al. (2022). The hyperparameters were set according to that work, and were consistent across both experiments: \u03b1 = 1 \u00d7 10\u22123, \u0454 = 1 \u00d7 10\u22128, \u03b3 = 0.99, and Episodes = 107. The base goals used for the first experiment were (1,6), (6,6), and (6,1), and for the second one (1,7), (7,7), and (7,1). The results are reported for partial traces of 0.1, 0.3, and 0.5, as traces with a completeness score of 0.7 or higher achieved a 100% success rate across all four metrics in all our experiments, for both GRAQL and GATLing. (fuller traces showed absolute success for both GRAQL and GATLing). We also implemented a scaling factor that prioritizes and emphasizes each state's highest-ranked action (or actions) while reducing the Q-values of other actions in the same state. Appendix B shows an example of the Q-table generated with and without this scaling."}, {"title": "4.2.1 Results", "content": "Tables 1 and 2 show the results from Experiment 1 and Experiment 2, respectively. The dynamic approach significantly outperformed the static approach, highlighting the importance of adaptability in ODGR. When using the dynamic approach, the softmax and normalized techniques were always able to effectively combine policies from base goals' agents into coherent policies for dynamic goals. In contrast, the max aggregation could not rank the true goal as the most likely one. We, therefore,"}, {"title": "5 Conclusion", "content": "This paper presented the novel problem of ODGR, which emphasizes the time in which different parts of the input are received, and the potential gain from policy transer. It formally defines ODGR problems and how they compare to most existing GR problems. Then, the paper provides an overview of existing GR algorithms and their computational effort distribution according to ODGR's components. Lastly, the paper introduces GATLing, a general algorithm for performing ODGR in 2D navigational domains, with an implementation using two distance metrics and three aggregation techniques for the policy ensemble. Our experiments demonstrate that in navigational domains without obstacles, GATLing offers effective ODGR capabilities, with accurate recognition performance and significantly reduced run-time compared to existing RL recognition approaches.\nWhile GATLing provides a foundation for future algorithms, there are several clear areas for potential improvements. First, the proposed aggregation techniques are expected to fail in domains that consist of rapid changes in Q values. Moreover, it is not straightforward how this approach can be extended to continuous state and action spaces. Nevertheless, by avoiding the lengthy policy learning phases. GATLing ensures that the recognition process remains efficient and scalable, even in environments where goals may emerge or evolve. As the field of data-driven GR develops, we anticipate that the need for faster goal learning will increase. This research is expected to contribute to developing adaptable and efficient GR systems in dynamic, changing environments."}], "equations": ["\u03c0_g(a|s) = \\frac{e^{Q_g(s,a)}}{\\Sigma_{a'\\in A}e^{Q_g(s,a')}}", "Cosine_Similarity(s, g_b, g_a) = \\frac{traj(s, g_b) \\cdot traj(s, g_a)}{||traj(s, g_b) || \\cdot ||traj(s, g_a) ||}", "\\frac{\\Sigma_{i=1}^n w_i Q_i(s,a)}{\\Sigma_{j=1}^j w_j}", "\\frac{e^{w_i \\cdot Q_i(s,a)}}{\\Sigma_{j=1}^{n} e^{w_j \\cdot Q_j(s,a)}}", "max_{i=1}^{n} Q_i(s, a)"]}