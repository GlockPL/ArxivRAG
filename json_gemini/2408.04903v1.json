{"title": "Axiomatic Characterisations of Sample-based Explainers", "authors": ["Leila Amgouda", "Martin Cooper", "Salim Debbaouib"], "abstract": "Explaining decisions of black-box classifiers is both important and computationally challenging. In this paper, we scrutinize explainers that generate feature-based explanations from samples or datasets. We start by presenting a set of desirable properties that explainers would ideally satisfy, delve into their relationships, and highlight incompatibilities of some of them. We identify the entire family of explainers that satisfy two key properties which are compatible with all the others. Its instances provide sufficient reasons, called weak abductive explanations. We then unravel its various subfamilies that satisfy subsets of compatible properties. Indeed, we fully characterize all the explainers that satisfy any subset of compatible properties. In particular, we introduce the first (broad family of) explainers that guarantee the existence of explanations and their global consistency. We discuss some of its tractable instances including the irrefutable explainer and the surrogate explainer whose explanations can be found in polynomial time.", "sections": [{"title": "1 Introduction", "content": "In recent years, AI systems have demonstrated remarkable capabilities but this success has often come at the expense of a corresponding lack of explainability. Indeed, deep neural networks behave effectively like black-box functions, meaning that explaining their decisions is inherently intractable. On the other hand, legislators have recognised the importance of providing explanations to end-users affected by decisions taken by AI systems [20, 37]. Thus, the question is how to reconcile the user's right to an explanation with the intractability of the corresponding computational problem.\nIn this paper, we concentrate on decisions taken by classifiers learnt by Machine-Learning (ML). There have been some important successes in identifying ML models which allow either polynomial-time explainability [5, 12, 16, 27, 30] or effectively-tractable explainability via efficient solvers [8, 9, 23, 26]. However, success of such formally-correct explaining on neural networks has been limited to relatively small networks [25]. One solution to the right-to-an-explanation/intractability-of-explaining dilemma is the use of surrogate models: instead of explaining the decision of classifier \u043a on an instance x, the back-box function is approximated by a simpler explainable function in the neighbourhood of x [33, 34].\nAnother solution is to produce explanations which are valid on a subset of feature space [1, 3, 14]. The argument behind this endeavour is that since ML models are obtained from a dataset rather than a complete search of feature space, a dataset-based approach to explanation has the advantage of being a coherent echo of the techniques used in ML. In the literature, the dataset may be, for example, the one used to train the ML model or any other sample of instances which are deemed general enough to represent the whole feature space. The most studied type of explanations is sufficient reasons, called also abductive explanations. An abductive explanation for assigning a class to a given instance is a subset of the instance, viewed as a set of (feature, value) pairs, which is sufficient to guarantee the class.\nThe current literature has revealed that the sample-based approach is quite challenging as it faces various issues. Indeed, it has been shown in [1, 14] that dataset-based abductive explanations can be globally inconsistent in the following technical sense: two explanations for two instances corresponding to distinct classes may be compatible, meaning that a third instance exists satisfying the two sufficient reasons for contradictory classes. According to [3], it is also not possible to define an explanation function that generates subset-minimal sufficient reasons and guarantees existence of explanations (success) and their global consistency. Consequently, in the same paper the authors introduced various functions that guarantee consistency at the cost of success while the functions defined in [14] ensure success and violate consistency. However, there is no explanation function in the literature which generates dataset-based abductive explanations while guaranteeing both properties.\nFollowing an axiomatic approach, we provide in this paper a complete investigation of functions that generate feature-based explanations from datasets. We start by proposing formal properties of explainers, then identify all the families of explainers that may be defined, shed light on the pros/cons of each family and on the links between the families. The findings unravel the origins of the above issues and uncover the full landscape of sample-based explainers. More precisely, the contributions of the paper are sixfold:\n\u2022 We propose a set of axioms - desirable properties - that explainers may satisfy. Three axioms are borrowed from the literature while others are new.\n\u2022 We delve into the relationships between the axioms and shed light on problematic incompatibilities of some of them. In particular, we show that the above-mentioned consistency property is incompatible with subset-minimality and success.\n\u2022 We fully characterize the entire family of explainers that generate weak abductive explanations.\n\u2022 We also characterize all its sub-families that satisfy subsets of compatible properties.\n\u2022 We introduce the first (broad family of) explainers that guarantee explanations and their global consistency.\n\u2022 We discuss some of its tractable instances including the irrefutable explainer and the surrogate explainer which is based on surrogate models.\nThe paper is organized as follows: Section 2 introduces sample-"}, {"title": "2 Sample-based Explainers", "content": "Throughout the paper, we consider a classification theory as a tuple T = (F, dom, C) comprising a finite set F of features, a function dom which returns the domain of every feature, where dom(.) is finite and |dom(.) > 1, and a finite set C of classes with |C| \u2265 2. A literal in T is a pair (f, v) where f \u2208 F and v \u2208 dom (f). A set L of literals is consistent iff, for any two elements (f, v) and (f', v') of L, if f = f', then v = v'. A partial assignment is any set of literals with each feature in F occurring at most once; it is called an instance when every feature appears once. Notice that partial assignments and instances are consistent. We denote by E(T) the set of all possible partial assignments and by F(T) the feature space, i.e., the set of all instances, of theory T. We consider a classifier on a theory T as a function K: F(T) \u2192 C, i.e., mapping every instance in F[T] to a class in the set C of classes.\nIn this paper, we are interested in explaining decisions taken by a classifier K for instances of a theory T. Explanations are generated using a subset of the feature space. For the sake of generality, its origin is left unspecified and it may be any sample of instances.\nDefinition 1. A question is a tuple Q = (\u0422, \u043a, D, x) such that T is a classification theory, \u043a is a classifier on T, DC F(T), and x \u2208 D.\nAn explainer is a function that takes as input a question and outputs a set of explanations. In the paper, we focus on feature-based explanations which describe the input features that contribute to a classifier's output for a given instance. Such explanations are thus partial assignments of the theory.\nDefinition 2. An explainer is a function L mapping every question Q = (\u0422, \u043a, D, x) into a subset of E(T). Every E \u2208 L(Q) is an explanation of \u043a(x) under the dataset D.\nTo theoretically scrutinize such explainers, we propose axioms (or formal properties) that they may satisfy. Axioms are important not only for a better understanding of the explanation process in general, but also for clarifying the basic assumptions underlying explainers, highlighting pros and cons of an explainer, comparing different (families of) explainers, and for also identifying families of explainers that have not been explored yet. We provide in Table 1 a list of ten, of which the three first ones (Success, Strong Irreducibility, Coherence) have counterparts in [14] while the others are new. Success guarantees at least one explanation to every question. (Strong) Irreducibility ensures that explanations do not contain unnecessary information to the explained decision. Irreducibility is tailored to sample-based explainers while its strong version concerns explainers that use the feature space. Coherence is crucial for sample-based explainers as it ensures global consistency of all generated explanations. Technically, two explanations for two instances corresponding to distinct classes should not be compatible, otherwise there would exist a third instance satisfying the two sufficient reasons for contradictory classes. Let us now introduce the new axioms. As we are interested in feature-based explanations, Feasibility states that an explanation should be part of the instance being explained. Validity ensures that generated explanations are locally consistent within the dataset. (Strong) Completeness ensures that no valid explanations are omitted. Monotonicity states that the process of constructing explanations is monotonic, that is, an explanation remains valid if the dataset is enlarged. Counter-monotonicity (CM) states that enlarging a dataset can only lead to discarding explanations.\nWe show that some axioms follow from others. Despite these dependencies, we keep all the axioms because some explainers may satisfy a property but violate some of those from which it follows. Hence, they are useful for discriminating explainers.\nProposition 1. The following implications hold.\n\u2022 Completeness \u21d2 Strong Completeness,\nStrong Completeness \u2192 Success.\n\u2022 Irreducibility \u2192 Strong Irreducibility.\n\u2022 Success, Feasibility and Coherence \u2192 Validity.\n\u2022 Feasibility, Validity and Completeness \u2192 Counter-Monotonicity.\nDespite the importance of all the axioms, some of them are incompatible, i.e., they cannot be satisfied all together by an explainer.\nTheorem 1. The axioms of every set $I_{i=1,5}$ below are incompatible.\n(I1) Feasibility, Success, Coherence and Irreducibility.\n(I2) Feasibility, Coherence and Completeness.\n(I3) Strong Irreducibility and Strong Completeness.\n(I4) Feasibility, Validity, Success, Irreducibility and Monotonicity.\n(I5) Feasibility, Validity, Success, Irreducibility and CM.\nNote that the important property of Irreducibility is incompatible with most of key axioms like Success and Coherence. This shows that subset-minimality of explanations has a cost in the sample-based explaining context. Coherence (or global consistency of explanations) cannot be ensured together with some other properties. Below is a list of five maximal sets of compatible axioms.\nTheorem 2. The axioms of every set $C_{i=1,5}$ below are compatible.\n(C1) Feasibility, Validity, Success, (Strong) Completeness, CM.\n(C2) Feasibility, Validity, Success, (Strong) Irreducibility.\n(C3) Feasibility, Validity, Success, Coherence, Monotonicity, CM, Strong Completeness.\n(C4) Feasibility, Validity, Success, Coherence, Monotonicity, CM, Strong Irreducibility.\n(C5) Feasibility, Validity, Coherence, (Strong) Irreducibility, Monotonicity, CM.\nFrom the above results, we have the following characterisation.\nTheorem 3. Considering all sets of axioms among those listed in Table 1 which include Feasibility and Validity, the only minimal incompatible sets are $I_1, ..., I_5$ and the only maximal compatible sets are $C_1,..., C_5$.\nRecall that Feasibility defines the (feature-based) type of explanations and Validity guarantees their local consistency within the datatset. They are thus mandatory in our investigation of feature-based explanations. In what follows, we provide a full characterization of the entire family of explainers that satisfy them. We show that instances of this family generate the so-called weak abductive explanations [14, 30]. Such explanations are sufficient reasons for predicting the classes of instances of a classification theory. They are"}, {"title": "3 Sub-Families of Explainers", "content": "In this section, we scrutinize the types of explainers that can be defined from the set of axioms. More precisely, we characterize all subfamilies of explainers which select in different ways subsets of weak abductive explanations. Each sub-family satisfies one of the first four subsets ($C_i$) of compatible axioms identified in Theorem 2. The last set $C_5$ is satisfied by the trivial explainer which returns the empty set for each question. Furthermore, existing explainers from [3] satisfy Coherence and Irreducibility at the cost of Success.\n3.1 Weak Abductive Explainers\nThe first sub-family contains two explainers, namely the function $L_{dw}$ which returns all weak abductive explanations and $L_w$ which explores all the feature space. We show that $L_{dw}$ privileges Completeness in the two conflicts (I2) and (I3). Furthermore, it is the only explainer that satisfies Feasibility, Validity and Completeness.\nTheorem 5. An explainer L satisfies Feasibility, Validity and Completeness iff $L = L_{dw}$.\nWe summarize below the complete list of axioms satisfied and violated respectively by the function $L_{dw}$. Notice that $L_{dw}$ satisfies the first largest set (C1) of compatible axioms from Theorem 2.\nTheorem 6. The explainer $L_{dw}$ satisfies Feasibility, Validity, Success, (Strong) Completeness and CM. It violates the remaining axioms.\nThe explainer $L_{dw}$ satisfies thus all the axioms that are compatible with Completeness except Monotonicity. The latter is violated due to incompleteness of information of a dataset. If the latter is enlarged, it is possible that a dwAXp will no longer be valid as the new dataset would contain a new instance which invalidates the explanation (violation of the second condition of Def. 3).\nThe second function $L_w$ which generates weak abductive explanation from the feature space is the only explainer that satisfies the four axioms Feasibility, Validity, Monotonicity and Strong Completeness.\nTheorem 7. An explainer L satisfies Feasibility, Validity, Monotonicity and Strong Completeness iff $L = L_w$.\nUnlike $L_{dw}$, the explainer $L_w$ satisfies Coherence and all the other axioms of the set C3.\nTheorem 8. The explainer $L_w$ satisfies Feasibility, Validity, Success, Coherence, Monotonicity, CM and Strong Completeness. It violates (Strong) Irreducibility and Completeness."}, {"title": "3.2 Concise Abductive Explainers", "content": "Our next characterization concerns explainers that satisfy Irreducibility in the four conflicts (I1, I3, I4, I5). We show that they generate subset-minimal weak abductive explanations. Note that such explainers have been studied in [1, 14].\nDefinition 4. Let Q = (\u0422, \u043a, D, x) be a question and E \u2208 E(T). \u0415 is a concise abductive explanation (cAXp) of \u043a(x) iff:\n\u2022 E\u2208 $L_{dw}(Q)$,\n\u2022 \u2204 E'\u2208 $L_{dw}(Q)$ such that E' \u2282 E.\nWe denote by $L_{dc}$ the explainer generating cAXp.\nWe show next that explainers that provide cAXp's are the only ones satisfying Feasibility, Validity, and Irreducibility.\nTheorem 9. An explainer L satisfies Feasibility, Validity, and Irreducibility iff for any question Q = (T, \u03ba, D, x), $L(Q) \u2286 L_{dc}(Q)$.\nAn instance of this family of explainers is $L_{de}$ which generates all the subset-minimal weak abductive explanations. We show that it satisfies the second set (C2) of compatible axioms, and thus violates several properties due to their incompatibility with Irreducibility.\nTheorem 10. The explainer $L_{dc}$ satisfies Feasibility, Validity, Success and (Strong) Irreducibility. It violates the remaining axioms.\nAnother explainer which generates concise abductive explanations is $L_c$ which explores the feature space. Such explanations, known as prime implicants, have been largely studied in the literature (e.g., [5, 6, 8, 15, 17, 18, 24, 25, 36]).\nDefinition 5. A prime implicant explainer is a function $L_c$ such that for any question Q = (\u0422, \u043a, D, x), $L_c(Q) = L_{dc}(Q')$, where Q' = (\u0422, \u03ba, F(T), x).\nThe function $L_c$ satisfies all the axioms of the set (C4), hence all axioms except (Strong) Completeness, and also Irreducibility despite the fact it generates subset-minimal explanations. This violation is merely due to the definitions of the axiom which is sample-based.\nTheorem 11. The function $L_c$ satisfies all the axioms except Irreducibility and (Strong) Completeness.\nRemark: Note that the inclusion $L_c(Q) \u2286 L_{dc}(Q)$ may not hold."}, {"title": "3.3 Coherent Sample-based Abductive Explainers", "content": "The two previous sub-families of explainers, in particular their sample-based instances ($L_{dw}, L_{dc}$) privilege Completeness and Irreducibility at the cost of Coherence, hence their explanations may be globally inconsistent. Indeed, we have seen that Coherence is incompatible with completeness, and with the pair (Success, Irreducibility). In what follows, we propose the first (family of) explainers that satisfy Success and Coherence. We start by showing that its instances generate weak abductive explanations, meaning that they select a collection of globally consistent dwAXp's.\nTheorem 12. If an explainer L satisfies Feasibility, Success and Coherence, then for any question Q, $L(Q) \u2286 L_{dw}(Q)$.\nBefore introducing the novel family, let us first provide some useful notions, including a coherent set of partial assignments. The latter is nothing more than a set which satisfies the consistency condition of the Coherence axiom.\nDefinition 6. Let T be a theory, X \u2286 E(T), D \u2286 F(T) and \u043a \u0430 classifier. X is coherent under (D, \u043a) iff \u2200 E, E' \u2208 X such that:\n\u2022 E\u222aE' is consistent, and\n\u2022 \u2203y, z \u2208 D such that E \u2286 y, E' \u2286 z, and \u03ba(y) \u2260 \u03ba(z).\nOtherwise, X is said to be incoherent.\nSince no two distinct instances of a feature space are consistent with each other, every dataset is coherent.\nProperty 4. For any D \u2286 F(T) and any classifier k, D is coherent under (D, \u043a).\nLet us introduce the notion of envelope, which is a coherent set of weak abductive explanations covering every instance of a dataset.\nDefinition 7. Let T be a theory, D \u2286 F(T), \u043a \u0430 classifier. An envelope under (D, k) is any X \u2286 E(T) such that the following hold:\n\u2022 X is coherent under (D, \u043a),\n\u2022 \u2200E \u2208 X, \u2203x \u2208 D such that E\u2208 $L_{dw}$ ((\u0422, \u043a, D, x)),\n\u2022 \u2200x \u2208 D, \u2203E\u2208 X such that E\u2286x.\nLet Coh(D, k) be the set of all envelopes under (D, \u043a).\nIt is easy to see that the set Coh(D, K) is not empty as it contains at least the dataset D itself. Furthermore, each X \u2208 Coh(D, K) contains a subset of the dwAXp's of every instance in D."}, {"title": "4 Tractable Coherent Explainers", "content": "The family of coherent explainers is large and covers a variety of functions because from a given dataset D, one may generate several envelopes, i.e., |Coh(D, \u043a)| \u2265 1. In this section, we discuss three functions whose explanations can be generated in polynomial time.\n4.1 Trivial Explanation Function\nThe first function, called trivial explainer, is the one whose envelope is the dataset itself. We have seen in Proposition 2 that any dataset is an envelope. This function assigns thus a single explanation to every question, which is nothing more than the instance being explained.\nDefinition 9. A trivial explainer is a function $L_{tr}$ such that for any question Q = (\u0422, \u043a, D, x), $L_{tr}(Q) = {x}$.\nThe function $L_{tr}$ is clearly a coherent explainer.\nProperty 5. The function $L_{tr}$ is a coherent explainer.\nTheorem 15. The function $L_{tr}$ satisfies all the axioms except (Strong) Irreducibility and (Strong) Completeness.\nObviously, an explainer which returns an instance as explanation of its outcome is not informative and definitely not useful for users."}, {"title": "4.2 Irrefutable Explanation Functions", "content": "We now introduce the novel irrefutable explainer. It uses the so-called irrefutable envelope that contains all non-conflicting weak abductive explanations.\nDefinition 10. Let T be a theory, D \u2286 F(T), \u043a \u0430 classifier, and $W = \\cup_{x \\in D} L_{dw} ((T, \\kappa, D, x))$. An irrefutable envelope under (D, \u043a) is X \u2286 E(T) such that the following hold:\n\u2022 X \u2286 W,\n\u2022 \u2200E \u2208 X, \u2204 E\u02b9 \u2208 W such that {E, E'} is incoherent,\n\u2022 \u2204X' \u2283 X that satisfies the above conditions.\nThe next result shows that the irrefutable envelope is unique.\nProperty 6. Let T be a theory, D \u2286 F(T), and \u043a a classifier. For all X, X' \u2286 E(T), if X and X' are irrefutable envelopes under (D, \u043a), then X = X'.\nNotation: Throughout the paper, Irr(D, \u043a) denotes the irrefutable envelope under (D, K) in theory T.\nAn irrefutable envelope contains the whole dataset under which it is defined. Furthermore, it is the intersection of all subset-maximal envelopes.\nProposition 3. Let T be a theory, D \u2286 F(T), and \u043a a classifier.\n\u2022 D\u2286Irr(D, \u043a).\n\u2022 Irr(D, \u03ba) \u2208 Coh(D, \u043a).\n\u2022 Irr(D,\u043a) = \u22c2X\u1d62, where S is the set of all subset-maximal\nXiES\nenvelopes of Coh(D, \u043a).\nWe are now ready to present the instance of the family of coherent explainers that is based on irrefutable envelopes.\nDefinition 11. An irrefutable explainer is a function $L_{ir}$ such that for any question Q = (\u0422, \u043a, D, x),\n$L_{ir}(Q) = {E \u2208 Irr(D,\u03ba) | E \u2286 x}$.\nUnlike the trivial explainer, $L_{ir}$ violates Monotonicity and Counter-Monotonicity."}, {"title": "4.3 Surrogate Classifier", "content": "In this section we describe another type of coherent explainers. Starting from a result in [2] stating that Coherence is ensured by explainers that provide sufficient reasons from the whole feature space, the idea is, given a question Q = (\u03a4, \u03ba, D, x), to find a surrogate classifier \u03c3 which is equal to k on D but which allows tractable explaining on the whole feature space. Explanations of Q are then explanations of Q' = (\u03a4, \u03c3, F(T), x). Tractability follows if we choose \u03c3 from a family of classifiers that allows tractable explaining on the whole feature space [4, 12, 22, 27].\nThe approach relies heavily on o, thus the question of its existence naturally arises. The answer is fortunately positive. Indeed, it is always possible to find a decision tree o for any model with 100% accuracy on any dataset D (by over-fitting, if necessary). Each instance x \u2208 D corresponds to a unique path from the root to a leaf of the decision tree and will be consulted, during construction of the decision tree, at each node of the path (and at no other node). In the case when all features are boolean, the length of such a path is at most n, the number of features, under the reasonable assumption that the same boolean feature is not redundantly tested two times on the same path. Standard heuristic algorithms [10, 39] which choose which feature to branch on at each node according to a score, such as entropy, will thus have a complexity of O(n\u00b2m), where m is the number of instances in D.\nProposition 4. Let T be a theory and D \u2286 T. For any classifier \u043a, there exists a decision-tree classifier \u03c3 on T such that \u2200y \u2208 D, \u03c3(y) = \u03ba(y).\nWe now introduce the novel coherent explainers, called surrogate explainers, which generate weak abductive explanations from the entire feature space but using the predictions of a surrogate classifier.\nDefinition 12. Let T be a theory, D\u2286 T, \u043a and \u03c3 two classifiers on T such that \u2200y \u2208 D, \u03c3(y) = \u03ba(\u0443). A surrogate explainer of k is a function $L_{su}$ such that \u2200x \u2208 D,\n$L_{su}((T, \u03ba, D, x)) = L_{dw}((T, \u03c3, F(T), x)$.\n$L_{DT}$ denotes the surrogate explainer where o is a decision tree.\nRemark: Notice that the same surrogate classifier o is used to explain the predictions of K for all instances of a given dataset. However, since o and \u03ba may disagree on instances outside a dataset, $L_{su}$ may use different surrogate classifiers for different datasets."}, {"title": "5 Worked Example", "content": "As an example, we study the well-known zoo dataset [38] which consists of 101 instances corresponding to animals from a zoo. Each animal is described by 16 features and belongs to one of 7 classes: Mammal, Bird, Reptile, Fish, Amphibian, Bug or Invertebrate. For example, antelope is a mammal and crow is a bird. We assume that the classifier k correctly classifies all 101 instances in the dataset.\nIn the case of a black-box classifier k over a large feature space, we cannot perform an exhaustive search over all instances. First consider concise abductive explanations based on this dataset (cAXp's). A cAXp of antelope is {(milk,1)}, since all other animals in the dataset that give milk are also mammals, and a cAXp of crow is {(feathers,1)}, since all other animals in the dataset with feathers are also birds. Although no animal in the dataset milks its young and has feathers, these explanations are consistent and hence $L_{de}$ (same for $L_{dw}$) does not satisfy coherence.\nIf we now look for coherent explanations, we find a size-14 irrefutable explanation for classifying antelope as a mammal, namely"}, {"title": "6 Discussion", "content": "This paper presented a comprehensive study of explainers that generate sufficient reasons from samples, thereby advancing our understanding of their families, strengths and weaknesses. It proposed some basic axioms, or formal properties, and showed that some subsets uniquely define various families of explainers that all generate weak abductive explanations. In other words, it identified the unique (family of) explainers that satisfy a given subset of axioms.\nThe reason of the diversity of families is that certain axioms are incompatible, making it tricky to define efficient sample-based functions that satisfy all the axioms. Indeed, it is not possible to define a function that returns concise (irreducible) explanations while guaranteeing at least one explanation for every instance and global coherence of explanations of all instances. Thus, one of the three properties should be abandoned. The right-to-an-explanation imposed by legislators [20, 37] impels to keep Success and the need of non-erroneous explanations suggests keeping Coherence. Thus, the compromise would be to sacrifice Irreducibility and accept explanations that contain unnecessary information. For the same reasons, it is reasonable to sacrifice Completeness in the incompatible set I2. These choices have been made by our novel family of Coherent explainers.\nOn the positive side, some coherent explainers ($L_{ir}$ and $L_{su}$) allow tractable explaining. The worked example showed that irrefutable explanations by $L_{ir}$ may contain a large number of features compared to those provided by $L_{su}$. However, $L_{ir}$ coincides with $L_w$ when it uses the whole feature space, while this is not the case of $L_{su}$ which, even if it is based on a dataset, generates weak abductive explanations from the whole feature space. Indeed, the equality $L_{su}(Q) = L_w (Q)$ may not hold since the surrogate model o may differ from the original classifier \u043a outside the dataset. Finally, as shown in Table 2, both functions violate the Fidelity property which states that an explainer would recover all the \"real\" weak-abductive explanations, i.e., those generated from the feature space. Note that the two functions recover some but not necessarily all real explanations. This is not surprising since a sample-based approach reasons under incomplete information."}, {"title": "7 Related Work and Conclusion", "content": "There are several works in the AI literature on explaining classification models. Some of them explain the inherent reasoning of models (e.g., [6, 21, 23, 36]) while others consider classifiers as black-boxes and look for possible correlations between instances and the classes assigned to them. In this second category, explanations are generally feature-based, like sufficient reasons (eg. [8, 17, 19, 24, 33, 34]), contrastive/counterfactual explanations (eg. [19, 29, 41]), or semifactuals (eg. [11, 28]). Our paper fits within the second category with a focus on sufficient reasons. However, unlike most works in the literature which explore the whole feature space to generate explanations, we follow a sample-based approach limiting search on a subset of feature space. The closest works to our are those from [1, 14, 33, 34].\nThe authors in [33, 34] proposed the well-known LIME and Anchors models which generate explanations using a sample of instances that are in the neighborhood of the instance being explained. It has been shown in [35, 40] that they may return incorrect explanations. The origin of this deficiency has been uncovered in [1] where the author proved that it is not possible to define an explainer that provides subset-minimal abductive explanations and guarantees Success and Coherence. As a consequence of this negative result, various sample-based explainers have been proposed, some of which satisfy Coherence at the cost of Success [3] while others promote Success [14].\nOur paper generalized the above negative result, introduced novel axioms and identified several other incompatible properties. It provided full characterizations of all families of explainers that can be defined, including one ($L_{co}$) that ensures Success and Coherence. $L_{co}$ is the first family of sample-based explainers that that satisfy both axioms. We proved that some of its instances are tractable.\nThis papers lends itself to several developments. A challenging open problem is the definition of sample-based coherent explainers that recover as many \"real\" explanations as possible while being tractable. A possible solution would be the use of constraints on input data in order to solve conflicts between dwAXp's. Another perspective is an axiomatic study of sample-based contrastive explanations."}, {"title": "A Proofs of Properties", "content": "Property 1. [14] Let T = (F, dom, C) be a theory with n = |F|, DC F(T) with m = |D| and E \u2208 E(T). Testing whether E is a dwAXp can be achieved in O(mn) time.\nProof. Proved in [14].\nProperty 2. [14] Let T = (F, dom, C) be a theory with n = |F|, DCF(T) with m = |D| and E \u2208 E(T). Finding a cAXp can be achieved in O(mn\u00b2) time.\nProof. Proved in [14].\nProperty 3. For any DC F(T) and any classifier k, D is coherent under (D, \u043a).\nProof. Follows from the fact that instances are pairwise inconsistent by definition.\nProperty 4. The function Ltr is a coherent explainer.\nProof. Straightforward from the definition.\nProperty 5. Let T be a theory, DC F(T), and \u043a a classifier. For all X, X' \u2286 E(T), if X and X' are irrefutable envelopes under (D, \u043a), then X = X'.\nProof. Let T be a theory, DC F(T), and \u043aa classifier.\n\u25ba Assume that X and X' are irrefutable envelopes under (D, \u043a). Suppose that E \u2208 X \\ X'. From the definition of irrefutable envelope, \u2203E' \u2208 W such that {E, E'} is incoherent. Hence, X' \u222a {E} satisfies the second condition of the same definition. This contradicts the maximality of X'. So, X \u2282 X'. In the same way, we can show that X' \u2282 X and hence X = X'.\nProperty 6. Lir is a coherent explainer, and for any question Q = (\u0422, \u043a, D, x), x \u2208 $L_{ir}(Q)$.\nProof. Lir is a coherent explainer since from Proposition 3, Irr(D,K) \u2208 Coh(D, K) and for any question Q where Q = (\u0422, \u043a, D, x), $L_{ir}(Q) = {E \u2208 Irr(D, \u043a) | E \u2286 x}$.\nThe second property follows from the first item of Proposition 3, namely the fact that D\u2286Irr(D, \u043a).\nProperty 7. Let Q = (\u0422, \u043a, D, x) be a question.\n\u2022 x \u2208 $L_{dw}(Q)$.\n\u2022 $L_{dc}(Q) \u2286 L_{dw}(Q)$.\nProof. The properties follow straightforwardly from the definitions.\nProperty 8. Let L be an explainer that satisfies Completeness. For any question Q = (T, \u03ba, D, x), x \u2208 L(Q).\nProof. Let L be an explainer that satisfies Completeness. Consider the question Q = (\u0422, \u043a, D, x), x \u2208 L(Q) and assume that x \u2209 L(Q). Completeness of L implies that \u2203y \u2208 D such that x\u2286 y and \u043a(x) \u2260 \u043a(y). From definition of instances, x = y which is a contradiction."}, {"title": "B Proofs of Propositions", "content": "Proposition 1. The following implications hold.\n\u2022 Completeness \u2192 Strong Completeness,\nStrong Completeness \u2192 Success.\n\u2022 Irreducibility \u2192 Strong Irreducibility.\n\u2022 Success, Feasibility and Coherence \u2192 Validity.\n\u2022 Feasibility, Validity and Completeness \u2192 Counter-Monotonicity.\nProof. Let L be an explainer.\n\u25ba Assume that L satisfies Completeness. Let EC x be such that E \u2209 L(Q). From Completeness, \u2203y \u2208 D s.t. E \u2286 y and \u043a(y) \u2260 \u043a(x). Thus, \u2203\u0443 \u2208 F(T) s.t. E\u2286 y and \u043a(y) \u2260 \u043a(x). So, L satisfies Strong Completeness.\n\u25ba Assume that L satisfies Strong Completeness and that x \u2209 L(Q) such that Q = (\u0422, \u043a, D, x). From Strong Completeness, \u2203y \u2208 F(T) such that x \u2282 y and \u043a(x) \u2260 \u043a(\u0443). By definition of instances, x = \u0443. Since \u043a assigns a single value to every instance, it follows that \u043a(\u0445) = \u043a(y). Thus x \u2208 L(Q) and L(Q) \u2260 0.\n\u25ba Strong Irreducibility follows straightforwardly from Irreducibility since DC F(T).\n\u25ba Assume that L satisfies"}]}