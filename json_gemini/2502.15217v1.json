{"title": "FormalSpecCpp: A Dataset of C++ Formal Specifications created using LLMs", "authors": ["Madhurima Chakraborty", "Peter Pirkelbauer", "Qing Yi"], "abstract": "FormalSpecCpp is a dataset designed to fill the gap in standardized benchmarks for verifying formal specifications in C++ programs. To the best of our knowledge, this is the first comprehensive collection of C++ programs with well-defined preconditions and postconditions. It provides a structured benchmark for evaluating specification inference tools and testing the accuracy of generated specifications. Researchers and developers can use this dataset to benchmark specification inference tools, fine-tune Large Language Models (LLMs) for automated specification generation, and analyze the role of formal specifications in improving program verification and automated testing. By making this dataset publicly available, we aim to advance research in program verification, specification inference, and AI-assisted software development. The dataset and the code are available at https://github.com/MadhuNimmo/FormalSpecCpp.", "sections": [{"title": "I. INTRODUCTION", "content": "Preconditions and postconditions serve as formal specifications that define the expected state of a program before and after code execution. This ensures software correctness, formal verification, and code reliability [1], [2]. Despite their significance, standardized benchmarks for C++ programs with formal specifications have been notably absent due to the lack of standardization, the complexity of C++, the evolving language standards, and the limited adoption of formal contracts. There is a growing need for formal specification support within programming languages to improve software reliability. Historically, C++ lacked support for formal specifications. Recognizing their importance, the C++ Standards Committee committed to introducing formal specifications in C++26 [3]. This will standardize the ability to specify function contracts directly in the language. For example, refer to the code below:\nint f(const int x)\n{\n}\npre (x != 1) // a precondition assertion\npost(r: r != 2) // a postcondition assertion\ncontract_assert (x != 3); // an assertion statement\nreturn x;\nThis feature represents a better alternative to the current practice of using macros or comments for specifying contracts, allowing developers to express expectations more clearly and formally. It also enhances the reliability of the code, facilitates better documentation of function behavior, and supports run-time behaviors for contract verification. Although this is promising for newer codebases, manually adapting it to the scale of legacy C++ is prohibitively expensive.\nUsing LLMs to generate formal specifications is a viable solution to this bottleneck. Given that such specifications can be verified against existing test suites, they enhance code reliability without requiring extensive manual effort. This work highlights the potential of LLMs in supporting automated specification generation, addressing the need for tools that can infer formal specifications from code. Our dataset enables an empirical evaluation of the specification inference tools. By comparing inferred specifications against ground truth, researchers can assess precision, recall, and correctness in automated inference methods.\nThe FormalSpecCpp dataset is derived from the Dafny-synthesis benchmark [4], which provides formally verified Dafny programs with preconditions and postconditions. We use a prompt-driven approach with OpenAI's GPT-4-turbo model to systematically translate these Dafny programs into C++, ensuring that the generated code preserves its formal specifications.\nThis dataset is the ground truth benchmark for evaluating formal specification inference tools by providing verified C++ programs with preconditions and postconditions. It also enables the testing of formal verification tools to assess how well they enforce these specifications. Additionally, it supports static and dynamic analysis research, helping researchers examine the impact of different specification techniques on software correctness, tool performance, and contract validation in real-world C++ programs.\nOur prompt engineering methodology systematically translates Dafny specifications into C++ by explicitly handling type mapping, assertion transformation, and safety constraints. This structured approach reduces hallucinations, improves syntactic correctness, and ensures that the translated contracts accurately represent program intent."}, {"title": "II. BACKGROUND", "content": "This section briefly overviews the background details: formal specifications in software engineering, LLMs and prompt engineering."}, {"title": "A. Formal Specifications", "content": "Formal specifications define the expected behavior of software using precise mathematical statements, typically through preconditions (requirements before execution) and postconditions (guarantees after execution). They play a critical role in software correctness, modular reasoning, and formal verification [1], [2].\nDespite their benefits, manual specification writing is complex and time-consuming, especially for large or legacy code-bases. This has led to increasing research in automating specification inference, reducing the cost of manual annotations while improving software reliability."}, {"title": "B. Large Language Models (LLMs)", "content": "LLMs are transformer-based neural networks trained on vast text corpora, enabling them to generate human-like text and code. These models, such as GPT-4, excel in code synthesis, reasoning, and text generation, making them valuable tools for automating formal specification inference.\nLLMs can be broadly categorized into:\n\u2022 General-purpose models (e.g., GPT-4, Claude) trained on diverse text and code.\n\u2022 Code-specific models (e.g., Code Llama) optimized for programming tasks.\nFor our dataset development, we selected GPT-4-turbo, a variant of OpenAI's GPT models, known for its strong performance in generating semantically correct code and a large 128k-token context window [5]. Prompt engineering is used to refine the model inputs to systematically guide the synthesis of formal specifications."}, {"title": "C. Prompt Engineering", "content": "Prompt engineering involved designing structured inputs to guide LLMs toward accurate and relevant outputs. Effective prompts improve clarity, control structure, and mitigate issues like hallucinations or incorrect type inferences.\nIn our work, prompt engineering plays three critical roles:\n\u2022 Specification Preservation: Ensuring formal properties are accurately translated between languages.\n\u2022 Type Safety: Mapping Dafny types to appropriate C++ equivalents.\n\u2022 Assertion Mapping: Converting Dafny verification constructs into valid C++ assertions and macros.\nBy carefully designing prompts, we systematically transformed Dafny programs with verified specifications into C++ implementations while preserving formal correctness."}, {"title": "III. THE FORMALSPECCPP DATASET", "content": "This section describes the steps to generate the FormalSpecCpp dataset. Figure 1 presents the workflow."}, {"title": "A. Conversion of Dafny Programs to C++", "content": "The Dafny-synthesis benchmark [4], which contains verified Dafny programs with formal specifications, is the starting point for generating C++ programs. Their study evaluated how well LLMs can synthesize Dafny code, including formal specifications and validation conditions that pass the Dafny verifier.\nWhile their approach relies on few-shot prompting with the LLM to generate formal specifications [4], we opted for a different strategy. Instead of inferring formal specifications directly for C++ using LLMs, we used LLMs for a structured translation process from Dafny to C++. This decision was based on the following key considerations:\n\u2022 Formal correctness preservation: Since Dafny programs in the benchmark are already verified, translating their specifications directly to C++ ensures that formal and logical correctness is maintained rather than relying on speculative inference by the LLM.\n\u2022 Avoiding prompt dependence and unnecessary constraints: Prior work [4] found that effective specification inference with LLMs requires carefully engineered prompts, yet the results are often redundant or overly restrictive. By translating verified Dafny specifications,"}, {"title": "B. Conversion of Associated Tests", "content": "The Dafny-synthesis benchmark includes test cases that were manually translated from Python tests in the MBPP dataset [6]. In contrast, we used LLM to directly convert the original MBPP test cases into C++, translating test input, expected output, and assertions while maintaining functional equivalence."}, {"title": "C. Automated Testing of Translated Programs", "content": "After generating the C++ programs and their corresponding tests, we performed automated test execution to validate their correctness. This step revealed several types of errors, including: (a) inconsistencies in preconditions and postconditions, (b) incomplete or ill-formed test cases that failed to capture intended functionality, and (c) mismatches between expected and actual outputs, indicating discrepancies in program behavior.\nThe automated tests provided valuable feedback, identifying functional issues in the translated programs. In our initial run, 62 programs passed all tests, while 43 failed due to assertion errors, linker errors, and compilation errors. Further analysis showed that many assertion failures stemmed from incorrect test specifications, such as misordered arguments, incorrect expected values, or type mismatches (unsigned int vs. double). Compilation errors, on the other hand, often reflected the limitations of LLM-based translation of the tests."}, {"title": "D. Manual Verification of Translations", "content": "To address these shortcomings, one of the authors, an expert in formal specifications and C++ programming, manually reviewed and refined a subset of translated programs and test cases. This process focused on:\n\u2022 Correcting errors detected by automated tests, such as misaligned assertions, type mismatches, and faulty preconditions.\n\u2022 Inspecting a subset of successful programs to verify semantic correctness and adherence to best practices.\n\u2022 Refining specifications and tests to better capture intended function behavior.\nManual verification uncovered errors that automated methods missed, particularly in cases where specifications were syntactically valid but logically incorrect. These included logical inconsistencies such as misaligned assertions and faulty precondition constraints. This step improved the dataset's reliability by ensuring that specifications accurately reflected program behavior.\nThe FormalSpecCpp dataset is available at https://github.com/MadhuNimmo/FormalSpecCpp. It serves as a standardized dataset for evaluating and improving precondition and postcondition inference in C++. It will be useful in advancing research in specification verification and automated program analysis."}, {"title": "IV. COST ANALYSIS AND EFFICIENCY", "content": "The conversion process from Dafny to C++ programs and tests was highly efficient, as summarized in Table I. A total of 105 files with preconditions and postconditions were processed, with all converted programs successfully compiling on the first attempt, achieving a 100% success rate. The entire process was completed in approximately 27 minutes, demonstrating the scalability and speed of the approach.\nThe total cost for converting these files was $2.07, averaging $0.02 per file. Similarly, generating the associated test cases cost an additional $1.31 and required around 15 minutes. These results underscore the minimal resource usage and cost-efficiency of the pipeline. Dafny files averaged 17.3 lines of code, while the converted C++ code showed a 91.2% increase in size, with an average of 33.1 lines per file.\nThis workflow's smooth transition and minimal resource overhead demonstrate its potential for practical use in large-scale datasets and real-world applications."}, {"title": "V. RELATED WORKS", "content": "Hoare's axiomatic basis for computer programming [2] established the foundation for reasoning about program correctness through preconditions and postconditions, while Dijkstra's weakest precondition calculus [7] provided systematic approaches for deriving specifications. These theoretical frameworks influenced the development of specification languages like JML [8] and modern verification-oriented programming languages such as Dafny [9]. Meyer's design by contract [1] further popularized the use of preconditions and postconditions in software development. However, the manual effort required to write formal specifications has historically limited their widespread adoption in industrial practice [10]."}, {"title": "A. Formal Methods and Specifications", "content": "Hoare's axiomatic basis for computer programming [2] established the foundation for reasoning about program correctness through preconditions and postconditions, while Dijkstra's weakest precondition calculus [7] provided systematic approaches for deriving specifications. These theoretical frameworks influenced the development of specification languages like JML [8] and modern verification-oriented programming languages such as Dafny [9]. Meyer's design by contract [1] further popularized the use of preconditions and postconditions in software development. However, the manual effort required to write formal specifications has historically limited their widespread adoption in industrial practice [10]."}, {"title": "B. Automated Specification Inference", "content": "The challenge of manual specification writing has motivated extensive research in automated specification inference. Early approaches like Daikon pioneered dynamic analysis techniques to infer likely program invariants [11]. Static analysis methods, including abstract interpretation [12] and symbolic execution [13], provided more comprehensive approaches but faced scalability challenges with complex codebases [14].\nRecent research has demonstrated the potential of LLMs in generating specifications and code for formal verification languages like Dafny [4] and Java [15]. This trend aligns with broader efforts to leverage AI in software engineering tasks, particularly in areas requiring formal reasoning."}, {"title": "VI. CONCLUDING REMARKS", "content": "This paper introduces the FormalSpecCpp dataset with formal specifications for C++, encouraging the community to utilize it for diverse research applications. Although the benchmark is a significant step forward, there is room for improvement. We highlight some of the challenges and lessons learned along the way. This is meant to serve as a guidance for researchers interested in extending the approach."}, {"title": "A. Code Translation", "content": "The prompt to convert Dafny programs into valid and semantically correct C++ code required non-trivial iterative refinement, especially given the differences in their type systems. For instance, Dafny supports an unbounded integer type, which can represent any integer value without overflow, whereas C++ offers multiple fixed-size integer types, including int, unsigned int, short, and long. C++ has a well-defined overflow behavior for signed integers and wraps around for unsigned integers. One should be mindful of these changes when translating from one programming language to another."}, {"title": "B. Prompt Engineering Refinements", "content": "Crafting effective prompts for Dafny to C++ translation involved refining the language model's handling of preconditions, postconditions, and correct C++ constructs. Early iterations resulted in misaligned assertions and syntax errors due to differences in language semantics. The iterative process focused on explicitly specifying rules around types, preconditions, invariants, and postconditions, ensuring consistency and correctness across the translations."}, {"title": "C. Test Case Translation", "content": "Adapting test cases from Dafny to C++ required a structured approach to facilitate compatibility with C++ testing frameworks. We designed prompts to guide the LLM in generating accurate C++ test cases in a predefined format, reducing manual effort and ensuring smooth integration into the C++ testing environment.\nRefining these prompts provided valuable insights into the strengths and limitations of using LLMs for test case translation. While improved prompts enhanced the LLM's handling of complex cases, challenges remained, such as occasional misinterpretation of logical relationships or the generation of syntactically correct but semantically flawed code."}, {"title": "D. Future work", "content": "Some natural extensions to this work are (a) including additional programs and programming languages, (b) refining the prompt engineering process, (c) exploring reinforcement learning and hybrid approaches to further enhance the specification generation and validation, and (d) extending the dataset to include larger programs with more complex specifications to test the scalability and robustness of the inference and validation processes."}]}