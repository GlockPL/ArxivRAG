{"title": "Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling", "authors": ["Yuanchao Li", "Zixing Zhang", "Jing Han", "Peter Bell", "Catherine Lai"], "abstract": "The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Fr\u00e9chet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech classification tasks, especially those related to cognitive states, are crucial for various applications, including human-computer interaction, health monitoring, and clinical diagnosis. However, human annotation for these tasks is often expensive, time-consuming, and requires extensive subjective assessment. This data scarcity issue hinders the progress of these classifications in real-world applications. To address this challenge, Semi-Supervised Learning (SSL) offers a promising approach by leveraging both limited labeled data and a larger amount of unlabeled data for various classification tasks, such as hate speech detection, emotion recognition, sound event detection, sleepiness detection, and gender classification [1]-[3].\nGenerally, SSL methods can be categorized into two main types: generating reliable pseudo-labels and building reliable models using limited data. For example, pseudo-labeling involves creating labels for unlabeled data based on predictions from an iteratively trained model [4]. Consistency regularization ensures that the model produces consistent predictions for augmented versions of the same input (e.g., with added noise) [5]. These two types are often complementary and are commonly used together in existing SSL frameworks [6]. Moreover, the advancements in self-supervised learning have further improved SSL by utilizing large amounts of unlabeled data to pre-train SSL models [7], [8].\nAmong the literature, the most relevant studies to our work focus on pseudo-label generation. D'Sa et al. [1] used label propagation, transducing labels from labeled data to unlabeled data with probabilistic transitions for hate speech classification. Zhang et al. [2] added unlabeled data with high confidence levels to the training set and resampled the originally labeled data for sound event classification. They also proposed dividing acoustic features into two views, selecting high-confidence instances in each view, and aggregating them with their predictions into the initial training sets per iteration [3]. Zhu et al. [4] applied noisy student training [9] to emotion recognition, using a teacher model trained on labeled data to infer soft labels for unlabeled data. Feng et al. [6] proposed incorporating federated learning, utilizing both labeled and unlabeled data at each local client in a multi-view pseudo-labeling approach.\nDespite these advances, selecting high-confidence pseudo-labeled data remains challenging. In this work:\n\u2022 We propose a novel SSL framework, integrating multi-view pseudo-labeling that leverages both acoustic and linguistic characteristics to select the most confident data for model training.\n\u2022 We employ Fr\u00e9chet Audio Distance (FAD) as a reference-free method to cluster unlabeled data based on acoustic similarity.\n\u2022 We use task-specific prompts to predict labels from ASR transcriptions, learning insights from acoustics, linguistics, and psychology.\n\u2022 We examine multiple fusion methods in the context of SSL to build the bimodal classifier.\nOur proposed SSL framework is evaluated on emotion recognition and dementia detection for classifying short-term and long-term cognitive states, demonstrating competitive performance using only 30% of the labeled data compared to fully supervised learning, and showing greater effectiveness than the selected baselines."}, {"title": "II. METHODOLOGY", "content": "As illustrated in Fig. 1, the proposed SSL framework with multi-view pseudo-labeling consists of two paths: acoustic and linguistic. The acoustic path utilizes the similarity between labeled and unlabeled data based on diverse audio embeddings, while the linguistic path employs LLMs to predict class labels from ASR transcriptions using task-specific knowledge. If the generated pseudo-labels from both paths align, we consider them as high-confidence data for training a bimodal classifier. Otherwise, the data are treated as low-confidence and will be further predicted using the trained bimodal classifier. The semi-supervised training of the bimodal classifier will iterate until a predefined criterion is met.\nA. Multi-View Pseudo-Labeling\n1) Acoustic Path: We extract acoustic features from multiple audio encoders trained with different objectives to reduce the bias of relying on a single one, inspired by a previous study in the music field [10]. We use the following four audio encoders, resulting in four sets of embeddings for calculating the respective FAD scores:\n\u2022 VGGish: convolutional embeddings [11]\n\u2022 EnCodec: low-rate audio codecs [12]\n\u2022 Wav2vec 2.0: self-supervised acoustic embeddings [13]\n\u2022 CLAP: contrastive audio-text embeddings [14]\nGiven the embeddings of labeled data $X^l$ and unlabeled data $X^u$, the FAD score is calculated using multivariate Gaussians from two embedding sets $X^l(\\mu_l, \\Sigma_l)$ and $X^u(\\mu_u, \\Sigma_u)$ as follows:\n$F(X^l, X^u) = ||\\mu_l - \\mu_u||^2 + tr(\\Sigma_l + \\Sigma_u - 2\\sqrt{\\Sigma_l\\Sigma_u})$ (1)"}, {"title": "2) Linguistic Path:", "content": "Previous speech classification tasks that used textual information typically relied on ground-truth text. However, in real-world applications, ASR is the only text source, and its transcriptions are usually noisy and contain errors, which can lead to incorrect classifications or pseudo-labels. We argue that it is more challenging to prompt LLMs for classification tasks based on ASR transcriptions compared to human transcriptions due to the presence of word errors.\nTo address this, we use the REVISE-REASON-RECOGNIZE (R3) prompting pipeline to perform speech classification with ASR Error Correction (AEC) and reasoning on ASR transcriptions [17]. The R3 pipeline involves three steps: REVISE, where ASR errors are corrected based on N-best hypotheses; REASON, where the LLMs self-explain based on the corrected transcriptions and task-specific knowledge; and RECOGNIZE, where the label is identified.\nFor the ASR systems, we adopt the following ten models, the same as [18], to generate diverse transcriptions and form 10-best ASR hypotheses:\n\u2022 Wav2Vec2-base-{100h,960h}\n\u2022 Wav2Vec2-large-960h\n\u2022 Wav2Vec2-large-960h-lv60-self\n\u2022 HuBERT-large-ls960-ft\n\u2022 WavLM-libri-clean-100h-base-plus\n\u2022 Whisper-{tiny, base, small, large-v2}.en\nTo perform AEC, we follow an AEC-specific Alpaca template [19], which uses the \"You are an ASR error corrector\" instruction, guiding the LLMs to perform error correction. As LLMs have demonstrated their ability in both AEC and emotion recognition [20], we expect that this capability can be extended to dementia detection from ASR transcriptions as well. The revised ASR transcriptions will be used for subsequent text feature extraction to train the bimodal classifier. For reasoning, we design task-specific knowledge that incorporates acoustics, linguistics, and psychology as in Fig. 1.\nWe first apply Parameter-Efficient Fine-Tune (PEFT) on the following three LLMs with the LoRA adapter [21] using the labeled data with the R3 prompt:\n\u2022 Llama2-7b-chat-hf\n\u2022 Llama2-13b-chat-hf\n\u2022 Falcon-7b-instruct\nThe learning rate, weight decay, and number of epochs are set to 1e-4, 1e-5, and 5, respectively, with AdamW optimizer used. The three fine-tuned LLMs are then prompted with the R3 prompt to predict class labels from ASR transcriptions of the unlabeled data."}, {"title": "3) Data Selection:", "content": "The acoustic and linguistic pseudo-labels gen-erated from the two paths are combined to select the most confident data for semi-supervised training. Data with matching pseudo-labels from both paths are selected as high-confidence, while data with differing pseudo-labels are considered low-confidence. Together with the labeled data, the high-confidence data will be used to train the bimodal classifier in the first iteration, ensuring robust initial training.\nB. Semi-Supervised Training\n1) Bimodal Classifier: The bimodal classifier consists of pre-trained feature encoders-HuBERT [22] and RoBERTa [23]-that extract audio and text features, respectively, and a classification model that uses these features to generate a prediction label. For PEFT the encoders and training the classification model, the learning rate, weight decay, number of epochs, and batch size are set as le-4, le-5, 30, and 64, respectively. The AdamW optimizer is used. For the classification model, we examine the following four fusion methods:\n\u2022 Early fusion: text and audio features are concatenated at the embedding level\n\u2022 Cross-attention fusion: text and audio features are attended to each other via attention and then concatenated [24]\n\u2022 Tensor fusion: unimodal information and bimodal interactions are learned explicitly and then aggregated [25]\n\u2022 Modality-gated fusion: primary modality is dynamically adjusted in each training step [26]\n2) Iteration: After training the bimodal classifier, low-confidence data are predicted and labeled using the trained classifier. In most previous SSL studies, model-labeled data are fully trusted and incorporated into the training set in the next iteration. However, as training progresses, mislabeled data (noise) may accumulate, leading to a cycle of erroneous learning [27]. To address this issue, we choose not to fully trust the model-labeled data. Instead, the pseudo-label generated by the bimodal classifier is compared with pseudo-labels from multi-view pseudo-labeling. If the model pseudo-label matches either the acoustic or linguistic pseudo-label, the data are added to the training set for the next iteration. Otherwise, they remain low-confidence and will be predicted in the next iteration.\nIn each iteration, we update the training set by adding model-labeled data and randomly removing 20% of the initial high-confidence data to avoid over-reliance on multi-view pseudo-labeling. The model is reinitialized in every iteration to reduce overfitting and bias. The maximum number of iterations is set to 40. However, if there is no performance improvement on the validation set for two consecutive iterations, the iteration will be terminated. The process is summarized in Algorithm 1."}, {"title": "Algorithm 1: Iteration Process", "content": "Input: $H$: Bimodal classifier; $S$: Validation set; $D_c$: Confident (high-confidence) data; $D_u$: Unconfident (low-confidence) data; $L_a$: Acoustic pseudo-labels; $L^l$: Linguistic pseudo-labels; $L_h$: Model pseudo-labels; $I$: Maximum number of iterations\n1 for i = 1,..., I do\n2   Train classifier $H^i \\leftarrow f(D_c)$;\n3   Evaluate $L_e \\leftarrow H^i(S_v)$;\n4   if performance on $D_c$ does not improve then\n5     break;\n6   Generate pseudo-label $L_h \\leftarrow h^i(D_u)$;\n7   if $L_h$ equals $L_a$ or $L^l$ then\n8     $D_c \\leftarrow D_c \\cup D_u$;\n9   Update $D_c$ and $D_u$;\n10  Reinitialize classifier $H^i$;\n11 end"}, {"title": "III. EXPERIMENTS", "content": "We use IEMOCAP [28] for emotion recognition and ADRESSO [29] for dementia detection. For IEMOCAP, we focus on the Big Four emotion classes and exclude utterances with blank transcriptions, resulting in 5,500 utterances (1,103 angry, 1,615 happy+excited, 1,704 neutral, 1,078 sad). For ADRESSo, since there are no human labels in the test set to verify our approach, we use only the training set and focus on binary classes: Alzheimer's Dementia (AD) and Cognitively Normal (CN). Due to the long duration of each audio file and the presence of interviewer's speech, we segment all files using the official segmentation information, extracting participants' speech, which results in 2,268 utterances (1,200 AD, 1,068 CN). Punctuation and extra whitespace are removed, and all text is converted to lowercase. The revised ASR transcriptions by R3 prompt yield word error rates of 11.48% and 30.25% for IEMOCAP and ADRESSo, respectively.\nFor both tasks, we use an 80/10/10 split for training, validation, and testing, applying the ratio equally to each class to ensure a balanced distribution. Additionally, the ground-truth labeling rates for the training data are compared at 20%, 25%, and 30%, with the remaining data labeled using our method. All results are measured using Unweighted Accuracy (UA). Random seeds are kept consistent across all experiments. Other settings have been detailed in the previous section (Code available).\nB. Results and Discussions\nFour baselines are used for comparison:\n\u2022 Supervised_full: the classification model is trained on the entire training data (i.e., the 80% split)\n\u2022 Supervised_limited: the classification model is trained on the limited labeled data without pseudo-labeling the unlabeled data\n\u2022 Decision merging: two classification models are trained using audio and text, respectively, and their probability distribution are merged to select high-confidence data for the next iteration\n\u2022 Co-training: two classification models are trained using audio and text, respectively. High-confidence data selected by each model are added to the training set for the other model in the next iteration [30], [31]\nThe comparison of our method with supervised_full and super-vised_limited are shown in Table II. It can be observed that 1) With only 30% labeled data, our method achieves performance com-petitive with the supervised_full baseline. 2) Our proposed method outperforms the supervised_limited baseline, which lacks multi-view pseudo-labeling and semi-supervised training to augment the labeled data, particularly in dementia detection. 3) The less ground-truth labeled data available, the more effective our method is in dementia detection, likely because binary classes are easier for pseudo-labeling. 4) When ground-truth labels are limited, classification performance of the supervised_limited baseline for both tasks drops significantly compared to the supervised_full training. Our proposed method, how-ever, mitigates this drop by more than 2% in emotion recognition and 3%-7% in dementia detection. 5) Modality-gated fusion performs best among the four fusion methods. This is reasonable as it dynamically selects the primary modality contributing most to the classification tasks, thereby reducing the impact of ASR errors in the text modality.\nAs bimodal fusion does not apply to the decision merging and co-training baseline settings, we select our best-performing results for comparison. Note that the principle of decision merging is the same as that of a fusion method-late fusion (or decision-level fusion) [32]. The difference is that we further use the merged probability to determine the high-confidence data for iteration, whereas late fusion directly outputs results based on the highest probability. Here, we refer to it as decision merging to avoid confusion with late fusion, as we have used fusion techniques in the previous experiment. For both decision merging and co-training, we set the threshold at 0.5 for emotion recognition and 0.7 for dementia detection. For example, the fourth emotion class will be selected as the pseudo-label for an unlabeled data with a probability distribution of {0.1, 0.1, 0.3, 0.5}, and the unlabeled data will be added to the training set as high-confidence data.\nFrom Table III, we can observe that: 1) Our method significantly outperforms the two baselines, likely for two reasons: first, it gener-ates more high-confidence unlabeled data with pseudo-labels, which iteratively trains the classifier for performance improvement; second, our bimodal classifier takes both modalities as input during training. In contrast, although the two baselines also consider two modalities, their classifiers are separate, each relying on a single modality and ignoring the interrelatedness between them. 2) Although both decision merging and co-training consist of two classifiers that output respective labels, the latter performs better than the former, especially in emotion recognition. This result is plausible since decision merging could potentially weaken the better prediction if the other probability is significantly incorrect. On the contrary, by incorporating high-confidence data judged by the other view into training set, co-training enables the two models to learn from each other indirectly. Its relatively lower effectiveness in dementia detection is likely due to the low-quality audio, which makes one of the models less powerful."}, {"title": "C. Effect of Multi-View Pseudo-Labeling Components", "content": "We explore the contributions of each audio encoder and LLM in multi-view pseudo-labeling by keeping either the acoustic or linguistic path unchanged (i.e., full path) while adding an individual encoder from the other path. For brevity, Table IV presents the results of early fusion with 30% labeled data omitting the other conditions. The results show that 1) The acoustic path contributes more than the linguistic path. 2) CLAP and Falcon perform best among the acoustic and linguistic encoders, respectively."}, {"title": "IV. CONCLUSION", "content": "In this work, we propose a novel semi-supervised learning frame-work that introduces a multi-view pseudo-labeling method leveraging both acoustic and linguistic characteristics. This method utilizes Fr\u00e9chet audio distance and large language models to select the most reliable unlabeled data for augmenting the training set. Multiple fu-sion techniques have been compared to utilize multi-view knowledge for further enhancement of the framework. We evaluate our method on emotion recognition and dementia detection tasks, demonstrating that it outperforms fully-supervised, limited-supervised, and two SSL baselines. Our method achieves competitive performance compared to fully supervised learning while using less than 30% of human-labeled data. In future work, we plan to explore the effects of additional audio encoders and large language models on multi-view pseudo-labeling and investigate more efficient fusion methods for the bimodal classifier."}]}