{"title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes", "authors": ["Bryan R. Christ", "Zack Gottesman", "Jonathan Kropko", "Thomas Hartvigsen"], "abstract": "Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within a model. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a method for isolating math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by removing those important for general language tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning ability without destroying its general language ability. Scaling these parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.", "sections": [{"title": "1 Introduction", "content": "Mathematical reasoning, or solving math problems with logic, is an active area of LLM research because it represents artificial intelligence (e.g., Ahn et al. 2024; Li et al. 2024b). Yet few works have explored how LLMs encode math reasoning abilities in their parametric knowledge. Identifying math-specific parameters could be beneficial for many reasons, including a) targeting the right parameters to intervene on to improve a model's math reasoning ability as others have done for language controllability (e.g., Tang et al. 2024) or toxicity mitigation (e.g., Suau et al. 2024), b) doing so without altering behavior on other tasks like these existing works have done, and c) fostering knowledge of how LLMs encode math reasoning. While some works explore how different math concepts or terms are stored or processed in model layers or neurons (Hanna et al., 2023; Rai and Yao, 2024; Stolfo et al., 2023), none have developed a method for isolating parameters for math reasoning.\nOutside of math reasoning, several works have explored how to identify neurons or parameters associated with particular knowledge or skills in LLMs (Chang et al., 2024; Dai et al., 2022; Panigrahi et al., 2023; Tang et al., 2024; Wang et al., 2022). While some methods are computationally expensive because they use gradient information and, therefore, may not be feasible for large models (e.g., Panigrahi et al. 2023), others are easier to compute because they rely on information obtained through forward passes, particularly as captured by activations (e.g., Tang et al. 2024). However, it is unknown if these domain-specific methods for single skill identification can effectively isolate a broad concept like math reasoning, which may be entangled with many other abilities within a LLM (e.g., reading comprehension, general knowledge).\nWe conduct the first study of parameter importance in LLMs for math reasoning. We apply two state-of-the-art (SOTA) forward-only parameter importance methods (Sun et al., 2023; Tang et al., 2024) to math reasoning. We find one of these methods, LAPE, consistently fails to identify math-specific neurons across models, while the other, Wanda, identifies parameters important for math, but is unable to isolate math-specific parameters because the parameters it identifies overlap significantly with those important for other tasks. To address these limitations of existing methods, we develop a new method to isolate math-specific parameters we call Math Neurosurgery (MathNeuro)."}, {"title": "2 Related Work", "content": "Skill and Knowledge Localization in LLMs\nSeveral works have explored skill and knowledge localization in language models, although none focus on math specifically (Bau et al., 2018; Chang et al., 2024; Dalvi et al., 2018, 2020; Dai et al., 2022; Gurnee et al., 2023; Kojima et al., 2024; Leng and Xiong, 2024; Panigrahi et al., 2023; Radford et al., 2017; Suau et al., 2024; Sun et al., 2023; Tang et al., 2024; Wang et al., 2022; Zhao et al., 2024; Xin et al., 2019). Many of these methods are computationally expensive because they require gradient information to calculate parameter importance, which may be infeasible for large models (Dai et al., 2022; Leng and Xiong, 2024; Panigrahi et al., 2023; Wang et al., 2022). However, others are more lightweight and calculate parameter importance using only forward passes, predominately through using information obtained through activation values (Kojima et al., 2024; Suau et al., 2024; Sun et al., 2023; Tang et al., 2024; Zhao et al., 2024). While these methods are known to find parameters important for the domains they study, it is unclear whether they would be effective in identifying parameters most important for math reasoning, which could potentially be distributed throughout a model or interwoven with other important natural language abilities given the task's complexity.\nMath Skill Localization in LLMs Some studies have explored how math knowledge is encoded within LLMs (Hanna et al., 2023; Rai and Yao, 2024; Stolfo et al., 2023). These works focus on how and where particular math concepts and key phrases such as addition and subtraction are processed by LLMs. While these findings are insightful, they ultimately do not identify parameters critical for a model's math performance but rather ones relating to processing different math concepts."}, {"title": "3 Methods", "content": "We propose MathNeuro, a parameter identification method that calculates importance based solely on forward passes. First, our method separately identifies LLM parameters important for a math and non-math, general language task using input data for each task. Next, MathNeuro isolates math-specific parameters by taking the subset that are important for the math task but not for the non-math task. We describe the method in more detail below."}, {"title": "3.1 Identifying Top Parameters", "content": "MathNeuro adapts Wanda (Sun et al., 2023), a SOTA LLM pruning method that prunes, or removes, parameters unimportant for a model's output. Wanda identifies parameters to prune using the absolute value of weights times activations for an input, providing a context-aware representation of parameter importance. Wanda produces a score Sij for individual parameters:\nSij = |Wij|\u00b7 ||Xj||2 (1)\nwhere Wij represents the weight, |\u00b7 | is absolute value, and ||Xj||2 is the l2 norm of the j-th feature aggregated across input tokens to normalize the input X, or activation, values. Wanda then prunes the parameters with the smallest scores. Wanda considers both weights and activations as critical elements of parameter importance because both small but highly activated weights and large but lightly activated weights can significantly influence model outputs. MathNeuro flips Wanda by identifying the parameters with the largest weights times activations as being most important to a given task."}, {"title": "3.2 Isolating Math-specific Parameters", "content": "While naively identifying the parameters with the highest absolute value of weights times activations may find parameters important for a given task, it may not isolate the parameters important for that task only, as previous work has found a high level of overlap between parameters important for different tasks (Tang et al., 2024). As a result, we calculate parameter importance for other unrelated tasks and use the disjoint set between these sets of parameters as the ones that are math specific, which is the critical innovation of MathNeuro. To do this, we separately accumulate the absolute value of weights times activations for each parameter in attention heads and MLP layers across N samples from a math dataset and a dataset for an unrelated natural language task. We focus on MLP layers and attention heads because recent work has found that knowledge and skills are often distributed in these two model components (Wei et al., 2024; Yin et al., 2024). We compute scores for each parameter over math and non-math inputs:\nSmath = \\sum_{k=1}^{N} |Wij| \\cdot ||jk||2 \\text{ for } X \\in D_{\\text{math}} (2)\nSnon-math = \\sum_{k=1}^{N} \\sum |Wij |||jk||2 \\text{ for } X \\in D_{\\text{non-math}} (3)\nThen, we separately identify the top K% of parameters with the highest score for each task in each layer. Lastly, we take the subset of parameters most important for the math task that are not in the set of parameters most important for the unrelated task, or Tmath = TopKmath \\setminus TopKnon-math."}, {"title": "4 Experiments", "content": "We next validate if MathNeuro successfully identifies math-specific parameters. We compare against SOTA alternatives and a simple baseline in two settings: 1) pruning parameters identified as important for math and 2) scaling these parameters. Pruning or scaling task-specific parameters is equivalent to the approach recent work has taken to deactivate or more highly activate neurons identified as language or knowledge specific (Kojima et al., 2024; Suau et al., 2024; Tang et al., 2024; Zhao et al., 2024), respectively, but intervenes on the weight rather than activation level. We show the impact of each intervention on both math and non-math performance across five popular LLMs varying in size from 1-8B parameters. We perform experiments for parameter identification using both a larger number of samples (500) and a single sample."}, {"title": "4.1 Experimental Setup", "content": "Models We evaluate five LLMs of varying sizes: Phi 1.5 (1B) (Li et al., 2023), Llama 3.2 1B Instruction Tuned (IT) (MetaAI, 2024b), Gemma 2 2B IT (Team et al., 2024), Llama 3.2 3B IT (MetaAI, 2024b), and Llama 3.1 8B IT (MetaAI, 2024a). We display results for Llama 3.2 1B IT in the sections below and report results for the other models in Appendices A, B, C, and D, which follow similar trends to those discussed below. We focus on instruction tuned models because we want to evaluate whether MathNeuro can successfully identify math-specific parameters in models that a) perform well at math given their size and b) are trained for a range of tasks, which means it may be more difficult to identify math-specific parameters. Phi 1.5, however, serves as a baseline for whether MathNeuro works for a pretrained, non-IT model.\nDatasets For identifying math-specific parameters, we use the GSM8K dataset due to its popularity and high quality (Cobbe et al., 2021). We calculate parameter importance based on the GSM8K training split and evaluate the impact of each method on the GSM8K test split for each model. Following prior work (Agarwal et al., 2024; Brown et al., 2024; Lee et al., 2024; Li et al., 2024a), we subset the GSM8K test split to the same 200 random samples for every model for experimental efficiency. For identifying parameters important for non-math tasks and measuring performance drops after finding and eliminating math-specific parameters, we follow recent work that assesses catastrophic forgetting in LLMs (Luo et al., 2024) by using RACE (Lai et al., 2017) for measuring reading comprehension and MMLU (Hendrycks et al., 2021) for measuring general knowledge. These datasets are general natural language understanding tasks that are different from math reasoning. While MMLU does contain some math-related questions, it assesses a wide variety of knowledge which, in aggregate, is mostly not math specific. We conduct all evaluations using the Eleuther AI LM Evaluation Harness (Gao et al., 2024) and prompt models to respond to GSM8K questions in a 8-shot chain-of-thought (CoT) format, as is standard.\nBaselines We compare MathNeuro to three identification methods computed using forward passes: (a) Wanda (Sun et al., 2023): We calculate parameter importance for math inputs and choose the top K% of parameters without removing those important for other unrelated tasks.\n(b) Language Activation Probability Entropy (LAPE) (Tang et al., 2024): LAPE finds language-specific neurons by thresholding activation probabilities as calculated by samples for each language under consideration. We use GSM8K, MMLU, and RACE for calculating task-specific neurons using this method. Using LAPE allows us to determine if existing activation-only parameter identification methods can isolate math-specific parameters. We use LAPE's default implementation for pruning and more highly activating neurons it identifies.\n(c) Random Parameter Identification: As a sanity check, we randomly select the same number of parameters as those identified by MathNeuro."}, {"title": "4.2 Pruning Top Math Parameters", "content": "To test if the four parameter identification methods (MathNeuro and the three baselines) identify parameters important for math reasoning, we identify important parameters using each method for each model and prune them (set them to 0). We then compare each model's GSM8K, RACE, and MMLU accuracy to their own unedited performance. We do this five times for each model with different random subsets of 500 samples from each dataset to identify the average performance of each method. We identify the top .01, .1, .5, 1, 2.5, 5, 10 and 15% of parameters for each comparison and report the parameter proportion with the best performance. Appendix A explores how GSM8K performance is impacted by parameter proportion.\nFigures 2, 12, 13, 14 and 15 show results from this experiment. An ideal method would fall in the top left of these plots, meaning math performance (GSM8K) is deleted while non-math performance (RACE and MMLU) is maintained. As shown in these plots, only MathNeuro and Wanda eliminate math performance across models, while LAPE is unable to identify parameters important for math. However, Wanda also destroys each model's ability to perform non-math tasks, as shown in large drops in MMLU and RACE performance. Unlike Wanda, MathNeuro effectively isolates math-specific parameters across models, as reflected in the method resulting in significantly smaller decreases in non-math performance that are similar to the effect of randomly pruning parameters in most cases."}, {"title": "4.3 Scaling Top Math Parameters", "content": "We next evaluate model performance when more highly activating math-specific parameters by scaling the weights by a universal factor. For smaller models, we find the scalar 1.1 performs best, while for larger models (Llama 3.1 8B IT), a smaller factor (1.01) works better. While we leave a rigorous study of this hyperparameter to future work due to its computational expense, see Appendix F for our experimentation with scale factors. As in Section 4.2, we scale the top parameters identified by each method based on a random subset of 500 samples from each relevant dataset and repeat the process five times to identify average performance, reporting the parameter proportion that performs best.\nFigures 3, 17, 18, 19, and 20 display results from this experiment. An ideal method would fall in the top right of these plots, meaning GSM8K accuracy increases while non-math performance is maintained. As shown in these figures, scaling parameters identified by MathNeuro results in a GSM8K performance increase of 4-17% across models, while scaling Wanda-identified parameters tends to either harm or slightly improve performance. As with pruning, LAPE has no effect for most models except for increasing GSM8K performance for Gemma 2 2B IT. Scaling random parameters can help for some models, although the effect is not consistent across models. Each parameter identification method does not harm performance on RACE or MMLU, suggesting scaling's impact tends to be localized to math performance. The GSM8K performance gains from scaling parameters identified by MathNeuro and the lack of catastrophic forgetting are consistent across models and highlight the potential for future work to utilize these parameters for interventions."}, {"title": "4.4 MathNeuro with a Single Sample", "content": "If MathNeuro could identify math-specific parameters using a single sample, then it could inform math reasoning interventions for settings in which data is limited such as for assessing a specific mathematical operation or topic. To evaluate if this is the case, we conduct experiments to see if MathNeuro's performance holds when identifying parameters based on a single math and non-math input. We conduct these experiments by pruning or scaling parameters identified by each method and run each experiment five times using different random samples from each dataset. As shown in Figures 4, 21, 22, 23 and 24, MathNeuro still performs best at isolating math-specific parameters when pruning using a single sample, as shown in lower drops in RACE and MMLU performance. However, these performance drops, particularly for MMLU, are larger than when using more samples, suggesting additional samples help the method more effectively isolate math-specific parameters.\nAs shown in Figures 5, 25, 26, 27, and 28, we see similar or smaller, but still meaningful, boosts in GSM8K accuracy when scaling parameters MathNeuro identifies using one math and non-math sample. While random scaling sometimes helps as observed in Section 4.3, the effect is again not consistent across models. In some cases, LAPE and Wanda increase GSM8K accuracy, though the effects are not consistent across models. For all methods, there is no meaningful performance drop in MMLU or RACE accuracy, suggesting scaling's impact on non-math performance is still minor."}, {"title": "4.5 MathNeuro Parameter Consistency, Number, Location and Qualitative Impact", "content": "In the sections below, we conduct additional experiments to explore whether MathNeuro identifies the same set of parameters as math-specific across different random subsets of math and non-math data and the number and location of these parameters. We also conduct a qualitative evaluation of sample model outputs after pruning parameters identified by MathNeuro to further explore how math and non-math outputs are affected by pruning. All experiments are conducted using Llama 3.2 1B IT.\nConsistency of Math-specific Parameters We next explore whether MathNeuro consistently identifies the same parameters as math-specific across different random subsets from a math and non-math dataset. This allows us to identify if math reasoning is in fact reliably concentrated in a subset of model parameters like the experiments above suggest. We first identify Llama 3.2 1B IT's math-specific parameters using MathNeuro on two different random subsets from a math and non-math dataset. Next, we calculate the percentage overlap between the parameters identified in both subsets. We do this five times for different sample sizes (1, 10, 100, 500, and 1,000) and for calculating different proportions of top parameters from each dataset. This allows us to construct confidence intervals and see how parameter identification consistency varies when calculating based on different sample sizes and top parameter proportions. As shown in Figures 6 and 7, by 100 samples, roughly 95% or more of the parameters MathNeuro identifies overlap between two random subsets regardless of the proportion of top parameters calculated, which shows that the method is able to consistently identify the most important parameters for math performance and that these parameters are largely invariant with regard to the subset of data used to calculate them.\nNumber and Location of Math-specific Parameters We next examine the proportion of Llama 3.2 1B IT parameters MathNeuro identifies as math-specific. We first identify math-specific parameters using MathNeuro on random subsets from each dataset. Next, we calculate the percentage of the top K% of parameters that are identified as math-specific using those subsets. We do this five times for different numbers of samples (1, 10, 100, 500, and 1,000) and for calculating different proportions of top parameters from each dataset. This allows us to construct confidence intervals and see how the proportion of math-specific parameters identified varies when calculating based on different sample sizes and top parameter proportions.\nAs shown in Figures 8 and 9, while the largest number of parameters are identified as math-specific when calculating importance based on one sample due to randomness, in general, the amount of math-specific parameters identified by MathNeuro increases with the number of samples considered regardless of the proportion of top parameters calculated. The relatively high amount of overlap in top parameters between tasks displayed in these figures is likely why we see that MathNeuro performs better than existing parameter identification methods that do not remove parameters important for other tasks. The percentage of math-specific parameters in the top K% of parameters declines as the proportion of top parameters calculated increases due to the fact that as this proportion increases, more of the model's top parameters are being considered, which are likely more invariant based on the task than when considering a smaller percentage of the model's top parameters. Indeed, the top 15% of model parameters are likely similar across tasks while the top 1% will vary more.\nTo explore where math-specific parameters are located within a model, we sum the number of parameters MathNeuro identifies in each decoder block for Llama 3.2 1B IT. To do this, we calculate the top 15% of parameters, which is the parameter proportion for which MathNeuro performs best for this model. As shown in Figures 10 and 11, the number of math-specific parameters MathNeuro identifies is relatively consistent across decoder blocks when using either MMLU or RACE for parameter identification. This suggests that math reasoning is distributed throughout the model rather than being concentrated in a few layers. The parameters identified in these figures correspond to just 1.51% and 1.84% of the model's overall parameters when calculating parameter importance based on MMLU and RACE, respectively, despite being responsible for nearly all of its math performance.\nQualitative Analysis To verify model outputs are still coherent after pruning, we conduct a small qualitative analysis of Llama 3.2 1B outputs before and after pruning parameters identified by MathNeuro. As shown in Tables 1, 2, and 3, only the output for answering a GSM8K question becomes incoherent after pruning math-specific parameters. Before pruning, the model correctly solves the GSM8K problem; after pruning, it fails to generate an effective CoT. The model effectively responds to RACE questions, although it gets the same question wrong both before and after pruning. The model generates coherent output to MMLU questions both before and after pruning, though it gets the answer right when calculating parameters to prune based on RACE and wrong when calculating parameters to prune based on MMLU. These findings largely confirm those discussed above where the model is still able to perform non-math tasks after pruning math-specific parameters, although it does experience a drop in performance similar to that obtained from randomly pruning parameters."}, {"title": "5 Conclusion", "content": "Despite math reasoning being an active area of LLM research, few works have explored how it is encoded within LLM parameters and if it is a skill that can be isolated within a model. We introduce MathNeuro, a forward-only identification method for isolating math-specific parameters in LLMs. Through comprehensive experiments, we demonstrate MathNeuro's effectiveness by showing pruning or scaling the parameters it identifies can delete or reinforce a LLM's math reasoning ability, respectively, despite its simplicity and ease of calculation. Future work should build on this method by developing interventions for math-specific parameters that improve a model's performance on mathematical reasoning without catastrophic forgetting."}, {"title": "Limitations", "content": "While we comprehensively evaluate MathNeuro using three key task-specific datasets used in other works (RACE for general language understanding, MMLU for general knowledge, and GSM8K for math reasoning), there are many other natural language and mathematical reasoning tasks models could be evaluated on. Future work should consider extending MathNeuro to these additional tasks. While we used five recent models for our experiments, future work should also include additional models, especially those of larger sizes (>8B). Additionally, due to computational expense, we were unable to conduct a full hyperparameter sweep for an optimal universal scaling factor for parameters identified by MathNeuro, though the rough grid search we report in Appendix F highlights that larger scale factors tend to work better for smaller models and smaller scale factors tend to work better for larger models."}, {"title": "Ethics Statement", "content": "All data used in this paper come from open-access datasets and, therefore, should not contain any private sensitive information."}]}