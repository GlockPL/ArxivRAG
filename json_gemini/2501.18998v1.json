{"title": "Adversarial Attacks on AI-Generated Text Detection Models: A Token Probability-Based Approach Using Embeddings", "authors": ["Ahmed K. Kadhim", "Lei Jiao", "Rishad Shafik", "Ole-Christoffer Granmo"], "abstract": "In recent years, text generation tools utilizing Artificial Intelligence (AI) have occasionally been misused across various domains, such as generating student reports or creative writings. This issue prompts plagiarism detection services to enhance their capabilities in identifying AI-generated content. Adversarial attacks are often used to test the robustness of AI-text generated detectors. This work proposes a novel textual adversarial attack on the detection models such as Fast-DetectGPT. The method employs embedding models for data perturbation, aiming at reconstructing the AI generated texts to reduce the likelihood of detection of the true origin of the texts. Specifically, we employ different embedding techniques, including the Tsetlin Machine (TM), an interpretable approach in machine learning for this purpose. By combining synonyms and embedding similarity vectors, we demonstrates the state-of-the-art reduction in detection scores against Fast-DetectGPT. Particularly, in the XSum dataset, the detection score decreased from 0.4431 to 0.2744 AUROC, and in the SQUAD dataset, it dropped from 0.5068 to 0.3532 AUROC.", "sections": [{"title": "1 Introduction", "content": "The responsibility of integrating into the scientific research and higher education community entails adhering to numerous behaviors and principles essential to safeguarding the integrity of educational and scientific progress. Consequently, the utilization of tools such as text editors or language enhancement applications must align with sound practices and uphold the ethical standards of scientific research and education [Lund et al., 2023; Foltynek et al., 2023]. Despite the substantial advancements in AI models, particularly within Natural Language Processing (NLP) applications, there remains ongoing debate about the appropriate use of these tools for text generation [Leidner and Plachouras, 2017; \u0160uster et al., 2017]. This issue could significantly impact the integrity of the academic domain [Tauginien\u0117 et al., 2018]. Large Language Model (LLM) such as Generative Pre-trained Transformer (GPT) [Brown et al., 2020; OpenAI, 2022; OpenAI, 2023], BERT [Devlin et al., 2018], and XL-Net [Yang et al., 2020] have gained widespread acceptance among users, demonstrating such high efficiency that distinguishing between human-generated and machine-generated content has become increasingly challenging [Shahid et al., 2022; Ippolito et al., 2020].\nIn response to this surge in the use of Al techniques for text generation, various detection methods have been developed to ascertain the origin of text [Solaiman et al., 2019; Fagni et al., 2021; Mitrovi\u0107 et al., 2023; Gehrmann et al., 2019; Mitchell et al., 2023; Su et al., 2023]. These methods are categorized as supervised and unsupervised. Unsupervised methods are notable for their versatility in text detection, as they are capable of handling diverse text domains due to the nature of their pretraining [Gehrmann et al., 2019; Mitchell et al., 2023]. These tools calculate probabilities and distribute them throughout the text. Under the zero-shot framework, it is presumed that AI-generated text exhibits a higher degree of probability variation compared to human-authored text, which can demonstrate both stability and variation. To assess such changes, additional text generation is required, significantly increasing execution time. The latest approach, FastDetect [Bao et al., 2024], hypothesizes that AI-generated words are produced based on decisions made by the generative model, adhering to specific generation probabilities distinct from human word choices. For humans, word selection involves multiple influencing factors, rendering the output more personal and less statistically general.\nThe majority of generated texts are fundamentally based on embedding principles in the initial stages of any Large Language Models (LLMs), where a dense vector in space is extracted for each word to encapsulate the contextual information present in the training dataset [Goldberg and Levy, 2014; Pennington et al., 2014]. The embedding maps words or tokens into a high-dimensional continuous space, which the model employs to facilitate various stages of text generation. This embedding, coupled with additional layers and algorithms used in LLMs [Vaswani et al., 2023], informs the model's predictions for subsequent words. Understanding this generative pattern enables the identification of AI-suggested content versus natural, human-like predictions, which are not strictly bound by patterns.\nBuilding on this foundation, in this paper, we propose an adversarial attack approach in which the embeddings are reverse engineered to exploit detection models by assigning low probability rates to predicted subsequent words, thereby lowering the overall text score as assessed by the detection systems.\nAmong the various embedding schemes considered in this work, we place particular emphasis on the TM-based approach. The TM is an emerging machine learning technique that has demonstrated notable success in various applications [Granmo, 2018; Yadav et al., 2021; Berge et al., 2019; Abeyrathna et al., 2021; Maheshwari et al., 2023], including NLP and computer vision. One architecture employed within the TM framework is the Tsetlin Machine Auto-Encoder (TM-AE), which is used in NLP tasks to generate word embeddings [Bhattarai et al., 2024]. The TM is distinguished by its interpretability and the transparency of its output [Granmo, 2018; Sharma et al., 2023; Abeyrathna et al., 2023; Yadav et al., 2021]. TM-AE can produce word embeddings that encapsulate the contextual information conveyed by words derived from the training dataset [Kadhim et al., 2024]. Consequently, TM can be leveraged to modify texts under examination, using its contextual insights to influence AI-generated text evaluation tools, thereby creating an interpretable adversarial attack that compromises their detection capabilities. This work makes two primary innovations:\n1. Proposing a novel adversarial attack on AI-origin detection systems, reducing detection accuracy from 0.4431 to 0.2744 AUROC on the XSum dataset and from 0.5068 to 0.3532 AUROC on the SQUAD dataset, through data perturbation with embedding models leveraging similar word probability vectors.\n2. Employing the interpretable TM model to gain deeper insights into the adversarial attack mechanism and its effects on text origin detection systems."}, {"title": "2 Related Work", "content": "In related studies focusing on constructing adversarial attacks on AI-generated text detection systems, two primary approaches are identified in [Huang et al., 2024] and in [Sadasivan et al., 2024] respectively. The approach [Huang et al., 2024] involves replacing specific tokens in the text with words generated randomly by LLMs. Multiple variants of GPT models are employed to generate a probability distribution for replacement, thereby influencing the source of the scoring. However, that method significantly increases the complexity of the attack due to the inherent opacity of LLMs, which operate as black-box models, making it challenging to rationalize token selection. Furthermore, the reliance on large-scale LLMs results in extensive execution times. Additionally, [Huang et al., 2024] employs the same models utilized for scoring or generating perturbations, thereby restricting the exposure of detection models to alternative types of probability distributions. This limitation arises because these detection models are inherently familiar with the probability distributions of the tokens they analyze. Conversely, approach [Sadasivan et al., 2024] proposes relying on a single LLM as the source for token replacement, thereby creating a paraphrase-based attack. This approach is more constrained than the former, as it significantly limits the diversity of probability distributions considered."}, {"title": "3 AI-Text Detection Exploration", "content": "To understand the process of creating an adversarial attack on AI-text detection tools, it is essential first to comprehend the underlying mechanisms for detection. This section will explore advanced models like DetectGPT [Mitchell et al., 2023] and Fast-DetectGPT [Bao et al., 2024].\nIn Detect-GPT, the scoring process relies on perturbations introduced by making minor modifications, such as replacing or masking specific words. These perturbations are typically generated using GPT-based models, such as T5. That method engages the model's execution logic through a sequence of operations, generally involving 100 perturbations per input, with the model processing each input independently. As a result, that approach increases the execution time for detection compared to, e.g., Fast-DetectGPT. Additionally, the scoring procedure processes each input and its corresponding perturbations sequentially, further contributing to the computational burden. Summary of Detect-GPT Features:\n\u2022 Perturbation-Based: Generates perturbed versions of the input text by making slight alterations using a masked language model (e.g., T5).\n\u2022 Probability Curvature: Compares the log-probabilities of the original text with those of its perturbations.\nThe Fast-DetectGPT method, in contrast, is designed to expedite the identification of text origins by utilizing conditional probabilities. This approach mitigates the additional complexities introduced by Detect-GPT for handling original texts. Instead of generating perturbations, the model creates alternatives-typically around 10,000-for each input token. The conditional likelihood function for each token is then evaluated, thereby eliminating the need for perturbations. This method obviates the sequential invocation of generation models, thereby achieving faster performance. Furthermore, the generation of 10,000 samples is streamlined, requiring only a single pass through the scorer. Discrepancy is determined by comparing the log-likelihood of the original token with the mean expected likelihood of the sampled alternatives. Discrepancy Formula:\nDiscrepancy = $\\frac{\\text{Log-likelihood (original)} - \\text{Mean (alternatives)}}{\\sqrt{\\text{Variance (alternatives)}}}$\nSummary of Fast-DetectGPT Features:\n\u2022 Sampling-Based: Generates alternative tokens for each position in the input text based on the model's probability distribution.\n\u2022 Single Forward Pass: Samples from a categorical distribution derived from the model's logits to streamline execution.\nFrom the above summary of the AI-text detectors, it can be inferred that determining the origin of a text fundamentally relies on assessing the discrepancy between model-suggested probabilities and the natural probabilities characteristic of human decision-making. This principle is utilized in industrial tools, such as Turnitin, to evaluate text originality. According to their website [Turnitin, 2024], Turnitin employs an AI model grounded in the text-generation methodology of LLMs. The text is segmented into groups of five to ten sentences, with overlapping segments to analyze contextual inclusivity. Each segment is then processed by an AI checking model, which assigns a score between 0 and 1, indicating whether the text was human-written (0) or AI-generated (1). The overall average of these scores represents the final percentage of AI-generated content in the text.\nThe Turnitin model relies on the GPT-3 framework for scoring. GPT-3 is trained on extensive Internet content, allowing it to generate text by predicting the most likely next words based on its training data. This prediction process is primarily governed by the transformer architecture [Vaswani et al., 2023], specifically its decoder, which determines the subsequent symbol during text generation. The key components of this mechanism are:\n1. Embedding Layer: Comprising two types-input embedding, which calculates contextual information for each symbol, and position embedding, which identifies each symbol's placement within a sentence.\n2. Transformer Layer: Includes the self-attention mechanism, which evaluates the importance of sentence components based on individual symbols.\n3. Output Layer (Softmax): Processes the preceding results across the vocabulary to select the most likely next word.\nBased on the above observations, the sentence score can be lowered by replacing words with alternatives subject to the absolute probabilities within the embedding vector. This reduces the likelihood of selecting the next word, thereby decreasing the sentence's overall score. As a result, the probability of detecting the text as AI-generated is reduced."}, {"title": "4 Proposed Adversarial Strategy", "content": "In this section, we first introduce the framework for the adversarial attack using the embedding models and then we discuss the TM-AE architecture and elaborate on its implementation within this study."}, {"title": "4.1 Adversarial Framework", "content": "We aim to develop an adversarial attack scheme on AI-generated text detection tools to deceive these models into classifying Al-generated texts as human-written. Embedding models are employed to guide the probability distribution during the selection of alternative words for replacement. By replacing targeted words, the overall text score decreases compared to scores typically associated with AI-generated or human-written texts. Embedding models, which compute dense vectors for each token in the vocabulary, are integral to establishing relationships between tokens within a high-dimensional space. These vectors have been widely adopted in Pre-Trained Language Model (PLM) model architectures, particularly in Transformer-based models.\nThis work incorporates three methods for constructing probability distributions for tokens:\nEmbedding Vector of Similarity\nThe first method uses the original vector produced by the embedding model to identify alternatives based on their similarity to the target token. Cosine similarity is used as the similarity metric, and two primary parameters are introduced:\n\u2022 Similarity Vector Length: Defines the length of the similarity vector derived from the embedding model's original vector. In most cases the length is 400.\n\u2022 Similarity Threshold: Specifies the minimum similarity score for selecting alternatives to the target token.\nSynonym Similarity Vector\nThe second method utilizes grammatically correct synonyms sourced from external lexical databases, such as WordNet, which organizes English words into sets of cognitive synonyms crafted by human linguists and experts. The similarity between the target token and its synonyms is computed using the embedding vector and cosine similarity. Alternatives are ranked based on their similarity scores, with a predefined similarity degree used to select the final alternatives.\nHybrid Scheme with Synonym and Embedding Vectors\nThe third method is a hybrid approach of the above two, conducted in two stages. In the first stage, a synonym was randomly selected from WordNet. In the second stage, this synonym was replaced with a low-probability word derived from the knowledge vector generated by an embedding model. The TM-AE embedding model was chosen for this scheme due to its transparent and interpretable structure."}, {"title": "4.2 Implementation of Tsetlin Machine", "content": "The options among the deep-learning based approaches lack of interpretability due to their black-box nature. To address this, the TM architecture was employed to enhance synonym substitution and assess the impact of these substitutions on text origin detection tools. The TM provides interpretability by allowing detailed insights into decision-making processes, making it a suitable choice for understanding the implications of adversarial attacks. Below we explain the operational concept of TM and how it is adapted to this work.\nTM [Granmo, 2018] effectively addresses complex pattern recognition tasks by leveraging propositional conjunctives, with each literal being managed by an individual automaton. Literals represent the entire vocabulary of the input, including their negations. For instance, in a sparse dataset where rows correspond to documents and columns correspond to literals (twice the size of the vocabulary), encoding assigns a value of 1 to a column if the corresponding word exists in a document, while its negation receives the opposite value. If the word does not appear in the document, the value is 0, and its negation is set to 1.\nTM characterizes words through clauses, which are propositional expressions formed by included literals. For example, to describe the word car, its clause could include the literals for words door, wheels, and no wing, where door and wheels are literals in the original form, and no wing arises from the literal representing the negation of the word wing. Although wing does not directly contribute to the clause, its negation plays a role in forming the description.\nClauses are iteratively trained to represent a word. In more detail, the final form of a clause (C) is represented as a conjunctive propositional expression consisting of literals that surpass the threshold state N. This clause, among others with a size of n clauses with their selective literals, collectively define the output class (y). See Eq. (1).\ny = u(\\sum_{j=1}^{n} C_j(X)). (1)\nHere u is a unit step thresholding, u(v) = 1 if v \u2265 0 else 0, and X is the input vector. Clearly, as seen from Eq. (1), one word can have multiple corresponding clauses. For instance, the word heart can have one clause with love and woman, and another clause with old and hospital.\nThe TM-AE architecture [Bhattarai et al., 2024] employed in this design is based on an enhanced TM structure [Glimsdal and Granmo, 2021], incorporating a coalesced weight matrix W to facilitate the simultaneous training of multiple outputs and enabling nested elections among clauses. For example, for the word heart, the weight assigned to the clause containing love and woman may differ from the weight assigned to the clause containing old and hospital, depending on the training context. Particularly, in this work, the weights from TM-AE are utilized to compute similarity probabilities for target words."}, {"title": "5 Experimental Settings", "content": "A variety of datasets, Pre-Trained Language Models (PLMs), AI-text detection models, and embedding models were utilized in this study. Except for the embedding models, the remaining datasets and models align with those used in [Bao et al., 2024], aiming to evaluate the proposed approach comprehensively. This diversity ensures a broad evaluation scope, encompassing various scenarios across downstream applications. The following subsections provide a concise overview of the datasets, PLMs, detection models, and embedding models employed."}, {"title": "5.1 Datasets", "content": "Three English datasets from diverse domains were used in the experiments. The XSum dataset, introduced by [Narayan et al., 2018], is designed for abstractive summarization and provides 500 concise summaries of news articles. The SQUAD dataset [Rajpurkar et al., 2016], based on Wikipedia contexts, includes 300 samples for training models to answer questions. The Writing Prompts dataset [Fan et al., 2018] contains 500 samples aimed at generating creative and coherent stories based on prompts. These datasets address distinct yet overlapping text generation tasks, enhancing various natural language processing capabilities."}, {"title": "5.2 Pre-trained Language Models (PLMs)", "content": "Several pre-trained language models (PLMs) with transformer-based architectures were employed during different stages of preparation and scoring, both during the pre-detection sample collection phase and the text perturbation process for detection. The PLMs utilized include GPT-2 XL with approximately 1.5 billion parameters, OPT-2.7B and GPT-Neo-2.7B with 2.7 billion parameters each, T5-3B with 3 billion parameters, GPT-J-6B with 6 billion parameters, and GPT-NeoX-20B with 20 billion parameters."}, {"title": "5.3 AI-Text Detection Models", "content": "The study utilized a set of classifiers designed for zero-shot evaluation of adversarial attacks, including: Fast-Detect and Detect-GPT: As detailed in Section 3. Normalized Perturbation Rank (NPR) and Log Probability and Log Rank (LRR): Both leverage rank- and probability-based features to enhance accuracy while balancing computational efficiency [Su et al., 2023]. LogRank, Likelihood, and Rank: Metrics based on token probabilities and their ranks to evaluate the likelihood of text being AI-generated [Gehrmann et al., 2019; Solaiman et al., 2019]. Entropy and Divergent N-Gram Analysis (DNA): Techniques focusing on distributional irregularities and n-gram variations to detect machine-generated text [Ippolito et al., 2020; Yang et al., 2023]."}, {"title": "5.4 Embedding Models", "content": "A diverse set of embedding models was employed to calculate token probability distributions, ensuring variety in the representation of text. The embedding models used include: GloVe: Pre-trained embeddings capturing word relationships based on co-occurrence statistics. FastText: Embeddings incorporating character-level information, effectively handling out-of-vocabulary words. Word2Vec: Static embeddings generated using CBOW or Skip-Gram methods for context prediction. TM-AE: As detailed in Section 4.2, this model uses logical expressions for word embeddings. ELMo: Contextual embeddings derived from bidirectional language models (BiLSTMs). BERT: Contextual embeddings utilizing Transformers, capturing bidirectional context. These models were trained on the One Billion Word dataset [Chelba et al., 2013], with a vocabulary size limited to 40,000 tokens."}, {"title": "6 Results", "content": "In the experiments, the Area Under the Receiver Operating Characteristic (AUROC) score was utilized to evaluate the performance of AI-generated text detection models. An AUROC score of 1.0 signifies perfect detection, indicating the model's certainty that the text was AI-generated. Conversely, an AUROC score of 0.5 represents a random detection performance. Perturbations were generated for each dataset sample using PLM source models, as outlined in [Bao et al., 2024], which served as the baseline source for text samples in this study. Subsequently, an additional perturbation was applied based on the proposed approach, as detailed below."}, {"title": "6.1 Experiments with Embedding Vector of Similarity", "content": "The datasets were sampled first using five PLM source models (GPT-2 XL, OPT-2.7, GPT-Neo-2.7, GPT-J-6, and GPT-NeoX-20) to generate AI-text samples, and then perturbed using six embedding models (GloVe, FastText, Word2Vec, TM-AE, ELMo, and BERT). The maximum permissible word change ratio was set to 5% of each sample, with an average text length of approximately 150 words across all datasets (XSum, SQUAD, and Writing Prompts). Consequently, the number of altered words did not exceed eight, and in most cases, fewer words were changed. The process involved filtering each text to exclude non-informative tokens that do not represent valid English words. The remaining tokens were then checked for their presence in the vocabulary of the embedding model used. As the embedding models have limited vocabularies, not all words could be replaced."}, {"title": "6.2 Experiments Utilizing Synonym Similarity Vectors", "content": "Previous experiments may not guarantee grammatically and semantically accurate substitutions to represent adversarial attacks, as they rely solely on embedding models and the dense vectors these models provide to generate replacements. In practical scenarios, constructing such adversarial representations would benefit from ensuring substitutions do not disrupt the context or alter the original meaning of the text. To achieve this, the following experiments employed human-curated synonym sets from WordNet to determine replacements for target words. For instance, the word car has synonyms such as motorcar, railcar, auto, cablecar, machine, elevatorcar, automobile, railroadcar, railwaycar, gondola. Replacing the target word with an appropriate synonym does not affect the overall sentence meaning. In these experiments, an embedding model was utilized to rank the likelihood of these synonyms, and the effect of varying both the number of substituted words and the nature of the substitution-whether the synonym was the most similar, intermediate, or least similar in the probability vector-was analyzed."}, {"title": "6.3 Hybrid Experiments Using Synonym and Embedding Similarity Vectors", "content": "In the hybrid model, the interpretability of the TM-AE embedding is particularly advantageous. Since the first stage is human-understandable (craft by human), the transparency of the second stage, which uses TM-AE, ensures that the entire two-stage process remains fully traceable. This transparency allows us to track completely the inference process of word replacement. For instance, for the target word car, which is eventually replaced by engine, we can observe how the synonym machine was selected in the first stage and then how machine is replaced by engine in the second stage. This human-understandable nature is critical for further analysis and debugging.\nIn TM-AE, the knowledge associated with a word is derived from the documents within the training database. For example, the target word car might be represented by a clause containing engine and not wing after training. This representation can be based on a set of training documents where car frequently appears alongside engine but not with wing. Notably, during the preparation of training data, word frequency is disregarded. The model considers only the presence or absence of a word (a Boolean value) when updating the corresponding column in the vocabulary of the input sparse vector. Essentially, the TM operates like an electoral system, where clauses vote on the target word using weights that are iteratively updated during training, ultimately forming a detailed and interpretable description of the word. Further details on the interpretability of the TM model can be found in works such as [Bhattarai et al., 2024; Yadav et al., 2021; Yadav et al., 2022]."}, {"title": "7 Conclusion", "content": "This study demonstrates the efficacy of adversarial attacks leveraging embedding-based substitutions to challenge AI-text detection methods. Results reveal that embedding models such as TM-AE and Word2Vec, as well as hybrid substitution methods, significantly reduce detection accuracy, with hybrid approaches achieving detection the lowest scores, particularly in black-box settings and with complex PLMs. Conversely, BERT showed to be the least effective for adversarial attacks due to its probability distributions aligning closely with those of PLM generation models, making it easier for detection systems to identify AI-generated content. These findings underscore the vulnerabilities of detection systems and highlight the potential of interpretable embedding approaches for crafting sophisticated adversarial attacks."}, {"title": "Ethical Statement", "content": "This work inherently involves ethical considerations, as it explores methods that could potentially bypass systems designed to detect the origin of text. Such systems often play a critical role in evaluating academic, professional, or creative works, and misuse of these methods could lead to ethical challenges by undermining trust and accountability. The primary aim of this research is to advance cybersecurity by examining the vulnerabilities of text detection systems and identifying potential adversarial strategies that could compromise their reliability. Specifically, this study highlights the role of embedding models in crafting adversarial attacks and emphasizes the need for detection systems to incorporate robust measures against such vulnerabilities. To address this, we propose the development of hybrid detection systems that integrate embedding model-based probability distributions with those from large language models (LLMs), as suggested in [Huang et al., 2024], to enhance their resilience against adversarial attacks."}, {"title": "Appendix", "content": "This appendix presents detailed tables corresponding to the heatmap plot (Figure 3), summarizing the performance of each detection method and embedding model utilized in the experiments. The results include AUROC scores for various detection methods, including Fast-DetectGPT, Detect-GPT, NPR, LRR, DNA, Likelihood, Rank, LogRank, and Entropy, each provided in a separate table. The experiments were conducted across all datasets (XSum, SQuAD, and Writing Prompts), with text samples generated using different PLM models (GPT-2 XL, OPT-2.7B, GPT-Neo-2.7B, GPT-J-6B, and GPT-NeoX-20B) and perturbed using diverse embedding models (GloVe, FastText, Word2Vec, TM-AE, ELMo, and BERT)."}]}