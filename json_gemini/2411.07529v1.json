{"title": "Evaluating ChatGPT-3.5 Efficiency in Solving Coding Problems of Different Complexity Levels: An Empirical Analysis", "authors": ["Minda Li", "Bhaskar Krishnamachari"], "abstract": "ChatGPT and other large language models (LLMs) promise to revolutionize software development by automatically generating code from program specifications. We assess the performance of ChatGPT's GPT-3.5-turbo model on LeetCode, a popular platform with algorithmic coding challenges for technical interview practice, across three difficulty levels: easy, medium, and hard. We test three main hypotheses: First, that ChatGPT solves fewer problems as difficulty rises (Hypothesis 1). Second, that prompt engineering improves ChatGPT's performance, with greater gains on easier problems and diminishing returns on harder ones (Hypothesis 2). Third, that ChatGPT performs better in popular languages like Python, Java, and C++ than in less common ones like Elixir, Erlang, and Racket (Hypothesis 3). To investigate these hypotheses, we conduct automated experiments using Python scripts to generate prompts that instruct ChatGPT to create Python solutions. These solutions are stored and manually submitted on LeetCode to check their correctness. For Hypothesis 1, results show the GPT-3.5-turbo model successfully solves 92% of easy, 79% of medium, and 51% of hard problems. For Hypothesis 2, prompt engineering yields improvements: 14-29% for Chain of Thought Prompting, 38-60% by providing failed test cases in a second feedback prompt, and 33-58% by switching to GPT-4. From a random subset of problems ChatGPT solved in Python, it also solved 70% in Java, 50% in C++, and none in Elixir, Erlang, or Racket. These findings generally validate all three hypotheses.", "sections": [{"title": "1 Introduction", "content": "Large language models have catalyzed significant advancements in various domains, including natural language processing [24], text generation [9], and now code generation [11]. LLMs have demonstrated skills in understanding, generating, and manipulating code, transforming the landscape of programming assistance. Large language models are trained on vast datasets comprising both natural language text and source code, which enables them to offer support in software development tasks. One of the more prominent LLMs is ChatGPT, a conversational AI model developed by OpenAI. The potential of these models to assist with programming has led to increased interest in their ability to improve programmer productivity, code correctness, and automated code generation from problem specifications.\nIn this paper, we focus on exploring ChatGPT's ability (primarily working with the low-cost GPT-3.5-turbo model) to solve coding problems in the popular programming language Python, using the platform Leetcode [8]. Leetcode is an online platform with more than 2000 coding challenges widely used for technical interview preparation and programming practice. The platform categorizes problems into three difficulty levels easy, medium, and hard covering a broad range of topics such as algorithms, databases, shell scripting, and concurrency. Each problem provides a detailed problem statement and typically includes 3-4 example input-output pairs. LeetCode's integrated compiler and test cases make it an ideal tool for evaluating the correctness of solutions generated by GPT models, allowing for a structured and objective comparison. It is important to note that Leetcode, while useful as a benchmarking tool, does not fully capture the complexities of real-world coding environments."}, {"title": "2 Related Works", "content": "We briefly identify below some of the research literature most closely related to our study.\nLLMs used to code: Before large language models, developers often turned to platforms like Stack Overflow and GitHub for assistance with coding issues. However with the rise of LLMs, the use of large language models to solve coding issues has become increasingly popular. LLMs have demonstrated the ability of code understanding [14], code generation [12], test automation [22], and program repair [7]. In our work, we focus on evaluating the code generation capabilities of LLMs, with a focus on GPT-3.5.turbo but also include some comparative experiments using the GPT-4, Claude 3 Sonnet, and Gemini 1.0 Pro models.\nPrompt engineering: As the use of LLMs has increased, the need to improve the responses of LLMs has been investigated through prompt engineering. Prompt engineering is the practice of designing inputs to produce optimal results from an LLM. A key factor in this process is the sensitivity of LLMs to the exact phrasing of prompts [21]. One particular approach, chain-of-thought prompting, enables LLMs to decompose multi-step problems into a sequence of intermediate steps, improving their performance on complex tasks [23]. In this work, we attempt to optimize ChatGPT's ability to solve coding problems through chain-of-thought prompting [21] and selecting better demonstration examples [5].\nStruggles of LLMs: Despite their advancements, large language models (LLMs) still face significant challenges. First, the scope of knowledge available to an LLM is limited by its training data. For instance, ChatGPT's GPT-4 model, released in March 2023, builds on prior iterations like GPT-3.5-turbo, and has knowledge up until January 2022. Consequently, LLMs are susceptible to reflecting biases from the datasets they were trained on [19]. Additionally, ChatGPT can struggle to correct errors based on provided feedback, particularly in coding contexts, due to its training limitations [18]. Another challenge is inefficiency in algorithmic code generation [17]. One of ChatGPT's most significant shortcomings is hallucination, where the model generates unverifiable or incorrect statements [2]. This issue can lead to faulty code output, especially when ChatGPT lacks a complete understanding of the task or access to external resources. Furthermore, when presented with ambiguous or complex queries, ChatGPT often makes assumptions rather than seeking clarification, which can further exacerbate its limitations. Given these challenges, our research seeks to quantify ChatGPT's effectiveness in solving coding problems, especially in light of its limitations in understanding and generating code. We designed our evaluation process with these issues in mind, particularly focusing on how the model handles feedback, algorithmic complexity, and ambiguity in coding tasks. By recognizing these potential pitfalls, we aim to assess both the model's strengths and its areas for improvement, providing insights into how it might be optimized for better performance in real-world coding applications.\nAlthough several studies have examined LLMs' performance in solving LeetCode problems [3], [15], and [16], they have all explored less than 250 problems, unlike our more comprehensive dataset of 1475 problems; moreover we provide results of evaluations on more languages as well as some comparisons with other models. Other coding platforms, such as HumanEval [1], [5] and CodeContests [10], have also been tested with LLMs, highlighting their potential across a wider range of challenges. The table below compares the scope and methodology of three closely related studies with our own work. The left column outlines the areas our study investigates, while a checkmark indicates that a study fully addresses a specific topic, and an \"X\" signifies that it does not."}, {"title": "3 Methodology", "content": "We present here the methodology for our empirical analysis. We have made the corresponding source code publicly available at https://github.com/ANRGUSC/coding_gpt_testing"}, {"title": "3.1 General Methodology:", "content": "To begin, we first needed to determine which large language model (LLM) to use for testing coding problems. We compared the performance of ChatGPT's GPT-3.5-turbo and Google's Bard by evaluating both models on the first ten easy and medium Leetcode problems. GPT-3.5-turbo significantly outperformed Bard, leading us to focus our study primarily on the GPT-3.5-turbo model.\nA total of 1,475 Leetcode problems were selected for testing. These problems, available on LeetCode, were taken up to problem number 1825, excluding all premium-level questions. To speed up the testing process, we automated the creation of the 1,475 queries that would be sent to ChatGPT. We used a LeetCode CSV file to streamline the query writing process. All queries were named query{problem number}.txt. The queries followed a consistent structure, beginning with: \"Write a Python program to do the following, and place this program in a JSON object as the value corresponding to the key 'code'.\" The problem description was then inserted, including an explanation of the problem and 2-3 test cases that the program should be able to solve. This was followed by the instruction: \"Write code that fits the following template: The program should continue below the following lines.\"\nLastly, the two initial lines of code provided by LeetCode needed to be inserted into the end of each query. To streamline this process, a curl program was written to download the source code for all 1,475 problems. Then another program was created to execute the curl command for each problem listed in the Leetcode CSV file. A parser program was also implemented to read each curl file, and another script ran the parser program to extract the two lines of code from each problem. These two lines were subsequently added to the queries.\nOnce all queries were written, we developed a Python script to interact with the OpenAI GPT-3.5-turbo API. This script submitted each query (at a temperature of 0.7) and saved ChatGPT's response to a file named code_query{problem number}.py.\nNext, we tested all generated code on Leetcode. For a solution to be accepted, it had to pass all of Leetcode's test cases, which ranged from 10 to 100 per problem. If a solution was not accepted, the number of test cases passed served as a metric to assess how close ChatGPT was to correctly solving the problem."}, {"title": "3.2 Main Hypotheses", "content": "As we continued to test the problems from Leetcode, new questions and hypotheses emerged in regard to each of our research questions, guiding further exploration and testing. Each hypothesis is in respect to the three research questions we have stated earlier.\nHypothesis 1: As the difficulty of problems increases, ChatGPT will solve fewer test cases.\nHypothesis 2: ChatGPT's performance will improve with the use of prompt engineering techniques. We hypothesized that prompt engineering would yield the most significant improvements for easy problems, with diminishing returns for harder problems.\nHypothesis 3: ChatGPT's performance will vary based on the programming language used. We hypothesized that ChatGPT would perform similarly in well-established languages like Java and C++ compared to Python, but results would differ for less common languages like Elixir, Erlang, and Racket."}, {"title": "3.3 Hypothesis 1:", "content": "To test Hypothesis 1, we examined if a problem passed all the test cases in LeetCode. We noted if a problem was correctly solved or not and then grouped the results based on the difficulty level easy, medium, or hard. First, problems were pre-categorized into these three difficulty levels based on their complexity and structure. Then, GPT-3.5-turbo was prompted with each problem in the dataset, and the number of test cases it solved successfully was recorded. After collecting the results, we grouped the number of correct solutions by difficulty level to enable comparison. We measured performance by calculating the percentage of test cases solved within each category, which allowed us to evaluate how the model's effectiveness varied with problem difficulty. By comparing ChatGPT's performance across these levels, we aimed to determine whether the hypothesis holds true that the model solves fewer test cases as the difficulty of the problems increases."}, {"title": "3.4 Hypothesis 2:", "content": "Next, for testing Hypothesis 2, we sought to evaluate if it is possible to improve ChatGPT's performance by refining the prompts. We evaluated the following three possible improvements:\nImprovement 1: The first improvement method applied was chain-of-thought (CoT) prompt engineer-ing. We first began to go back and prompt engineer the initial queries written. The first method completed was chain of thought prompt engineering. We selected a subset of 20 unsolved problems and 10 solved problems from each difficulty level (easy, medium, hard). The 20 unsolved problems were chosen based on the lowest acceptance rates, while the solved problems were randomly selected. As difficulty increased, a greater number of problems had an acceptance rate of 0%, which means that no test cases were passed. In order to execute CoT prompt enginering on each query, we asked ChatGPT to first generate pseudocode\nImprovement 2: When testing on Leetcode, the system indicated which test cases were not passed. In the second improvement method, we incorporated these failed test cases into the prompts. The same set of problems used in the Chain of Thought Prompt Engineering was tested. Our hypothesis was that explicitly providing the failed test cases might help ChatGPT refine its solutions. For each query, we included the failed test case as follows: \"Here is a test case the previous program failed to solve. Input:\ngrid = [[0,0,0,0,0,1],[1,1,0,0,1,0],[0,0,0,0,1,1],[0,0,1,0,1,0],[0,1,1,0,0,0],[0,1,1,0,0,0]]. Output: -1. Expected: 11.\nPlease consider this when writing the next program.\"\nImprovement 3: During our study, a more advanced version of ChatGPT, namely, GPT-4 was released. To evaluate its performance relative to GPT-3.5-turbo, we modified our testing code to call GPT-4 instead. The same set of problems used in Chain of Thought Prompt Engineering was tested. This allowed us to compare the two versions of ChatGPT on identical problem sets."}, {"title": "3.5 Hypothesis 3:", "content": "Lastly, to test Hypothesis 3, we further evaluated Leetcode problems in five additional languages: Java, C++, Elixir, Erlang, and Racket. Specifically, we selected 10 problems that were successfully solved in Python and 10 that were not. This allowed us to use Python as a baseline for comparison with the other five languages. The decision to include these particular languages was motivated by their diversity in terms of syntax, paradigms, and popularity, allowing us to assess the adaptability of ChatGPT to both mainstream and less common programming languages."}, {"title": "4 Results", "content": "In this section, we present the outcomes of our experiments, aligning them with the hypotheses outlined in the methodology. The results are organized according to the complexity of the problem, code characteristics, prompt engineering methods, programming languages, and problem types, each validated or challenged by the findings."}, {"title": "4.1 Hypothesis 1 Difficulty Levels:", "content": "We tested our first hypothesis that as the difficulty of the problem increases, ChatGPT (using GPT-3.5-turbo) would solve fewer test cases. The results supported this hypothesis: GPT-3.5-turbo successfully solved 92% of easy problems, 79% of medium problems, and 51% of hard problems."}, {"title": "4.2 Hypothesis 2 \u2013 Prompt Engineering:", "content": "We tested three methods to improve GPT-3.5-turbo problem solving performance: chain-of-thought prompt engineering, incorporating failed test cases, and switching to GPT-4. Table 2 summarizes the improvements observed across different difficulty levels (Easy, Medium, Hard).\nChain-of-thought (CoT) prompt engineering proved especially effective for easy problems, where it resulted in a 29% improvement\u2014more than any other method in this category. This aligns well with our hypothesis that simpler problems benefit the most from better-structured prompts. By guiding the model to first generate pseudocode, CoT helps break down easier tasks into manageable steps, allowing GPT-3.5-turbo to catch and correct logical errors early on. However, as problem difficulty increased, the effectiveness of CoT diminished. For medium and hard problems, the improvement rates were lower at 19% and 14%, respectively. This suggests that while CoT provides a logical framework for tackling straightforward problems, it struggles to handle the complex problem-solving and deeper reasoning required in higher-difficulty cases.\nIncorporating failed test cases into the querys showed a notable performance boost for easy problems, with an improvement of 38%. This increase, while less pronounced than in other categories, still highlights the model's ability to learn from its mistakes even in simpler scenarios. The effect was even more pronounced in medium-difficulty problems, where this method led to an impressive improvement 60%. By directly addressing incorrect solutions, the model was able to refine its problem solving approach effectively. The balanced complexity of medium difficulty may facilitate learning from errors while significantly benefiting from the additional information provided. For hard problems, the incorporation of failed test cases resulted in a solid improvement 45%, reinforcing the value of explicitly correcting errors. Interestingly, many of these failed cases were already embedded in the initial prompts, but reintroducing them as key focal points prompted more sophisticated problem-solving pathways. Overall, the best results were observed in medium-difficulty problems, showcasing the most significant gains in performance through the integration of failed test cases. Interestingly, even though many of these failed cases were already in the initial prompts, reintroducing them as key focal points appeared to achieve better results.\nUsing GPT-4 instead of GPT-3.5-turbo provided significant improvements across all difficulty levels, particularly for harder problems. For hard problems, GPT-4 showed a 52% improvement, demonstrating its superior ability to handle more complex scenarios. The model upgrades in GPT-4 seem to have directly enhanced problem-solving capacity across the board, with a balanced 33% improvement in easy problems and a substantial 58% gain in medium problems.\nUnlike CoT prompt engineering or failed test cases, which are highly dependent on prompt structure and feedback, the improvements brought by switching to GPT-4 highlight the impact of model architecture. While CoT and incorporating failed test cases addressed specific reasoning flaws, GPT-4 appears to offer a more robust, general-purpose upgrade, especially beneficial in the hardest problem category where logical depth and model capacity matter most. In conclusion, we found that prompt engineering with failed test cases yielded results comparable to those of GPT-4. However, the key advantage of GPT-4 is that it does not require additional refining in the queries.\nAfter observing the initial improvements from prompt engineering with GPT-3.5-turbo and from GPT-4, we expanded the scope of our study to include other large language models: Claude 3 Sonnet and Gemini 1.0 Pro. Using the same subset of 20 unsolved and 10 solved problems, we tested these models with the same queries.\nClaude 3 Sonnet showed strong performance, particularly in the Easy and Medium categories, with improvements of 36% and 48%, respectively. However, its performance dropped off in the Hard category, where it improved only by 26%. This suggests that Claude 3 Sonnet is well-suited to problems of moderate complexity but may struggle with more challenging problems compared to GPT-4, which showed better performance across the board.\nGemini 1.0 Pro, on the other hand, displayed limited improvement, particularly in harder problems. While it showed respectable gains in the Easy category (34%), its performance in Medium and Hard problems was significantly lower, with only a 17% and 6% improvement, respectively. This highlights Gemini 1.0 Pro's limitations in handling more complex problem-solving tasks.\nOverall, the results indicate that while alternative models like Claude 3 Sonnet and Gemini 1.0 Pro can enhance performance, GPT-4 remains the most consistent across all problem difficulty levels. The gap between models becomes more apparent as the problem difficulty increases, with GPT-4 demonstrating superior handling of the more complex tasks."}, {"title": "4.3 Hypothesis 3 Other Languages:", "content": "Lastly, we tested GPT-3.5-turbo across multiple programming languages, specifically C++, Java, Elixir, Er-lang, and Racket. For this analysis, we selected a subset of 20 problems: 10 that GPT-3.5-turbo successfully solved in Python and 10 that it could not solve. The results revealed distinct patterns in performance across these languages.\nIn Elixir, Erlang, and Racket, none of the 20 problems were solved by GPT-3.5-turbo. This highlights potential limitations in the model's ability to handle these languages effectively. It suggests that the model may struggle with the unique syntax and paradigms of functional programming or may lack sufficient training data in these languages to generate valid solutions.\nConversely, C++ and Java produced more meaningful results. We initially assumed that these two languages would exhibit similar performance levels, given the success of Python solutions. However, as figure 7 shows C++ solved only 5 out of the 10 problems that ChatGPT successfully solved in Python, while Java solved 7 out of 10 (Note that we omit Elixir, Erlang and Racket from the figure since they could not solve any of the problems). This discrepancy suggests that language-specific challenges play a role in the model's effectiveness.\nInterestingly, in the 10 problems that ChatGPT (GPT-3.5-turbo) could not solve in Python, both C++ and Java managed to solve a few. In C++, the model solved 4 of these 10 problems, with all solutions related to array-based problems. This may indicate that the model's understanding of arrays is better represented in C++ than other topics. However, the more complex syntax and manual memory management of C++ could have limited the model's success, making it less effective overall compared to Python. In Java, GPT-3.5-turbo successfully solved 3 of the 10 previously unsolved problems. These included both arrays and dynamic programming topics. Java's stronger performance may be due to its explicit handling of data structures and clear object-oriented principles, which align well with the model's training. Furthermore, Java's verbose syntax likely provides clearer contextual cues, allowing the model to generate more accurate solutions.\nThe differences in performance between C++ and Java highlight the varying challenges that each language presents. C++, with its focus on pointers and memory management, can hinder the model's ability to translate logical reasoning into code. In contrast, Java's structured environment and well-defined syntax might make it easier for the model to solve more complex problems, such as those involving dynamic pro-gramming. Overall, these results suggest that the effectiveness of large language models like GPT-3.5-turbo varies significantly across programming languages. This variability emphasizes the importance of further research into language-specific training and refining model performance to better accommodate the nuances of each language."}, {"title": "4.4 Further Results 1 Types of Problems:", "content": "After analyzing our initial three hypotheses, we decided to conduct some more experiments. We first decided to look at the types of problems GPT-3.5-turbo did best in. We categorized the problems by type to assess how GPT-3.5-turbo's performance varied across algorithms. As shown in the table below, GPT-3.5-turbo performed best on hash table, search, and divide-and-conquer problems, with only 15-20% of problems unsolved. These categories typically involve straightforward, modular patterns, making them easier for the model to recognize and generate correct solutions.\nHowever, GPT-3.5-turbo struggled more with database, dynamic programming (DP), and greedy al-gorithms, leaving 27-31% of these problems unsolved. These problem types present additional challenges. Database problems require correct SQL syntax and logic, where small errors can invalidate solutions; dy-namic programming problems demand state management and optimal substructure reasoning, which can overwhelm the model; and Greedy algorithms often rely on problem-specific heuristics, which the model may struggle to infer correctly.\nGPT-3.5-turbo performs well on problems with clear, reusable structures, like search and divide-and-conquer. However, it struggles with complex reasoning and syntax-heavy tasks such as SQL queries and dynamic programming. This variability highlights opportunities for future improvements, particularly in areas that require advanced logic and precision."}, {"title": "4.5 Further Results 2 \u2013 Lines of Code:", "content": "After seeing the results for hypothesis 1, we further hypothesized that the number of lines of code of in a program correlated to the likelihood the program would be correct. The less lines of code there are, the more likely a program is correct and fulfills the task of the problem.\nOur findings support this hypothesis shown in figure 8, although with some nuanced observations. In easy problems, the average lines of code for both solved and unsolved cases were under 10. However, the small difference between the two groups can largely be attributed to MySQL-related tasks, which were shorter in code but frequently unsolvable by GPT-3.5-turbo. These results suggest that for simpler tasks, efficiency in code length was not the primary factor driving correctness."}, {"title": "5 Conclusions", "content": "This research investigated ChatGPT's (GPT-3.5-turbo) performance in solving coding problems of varying complexity, across different programming languages, and with the application of prompt engineering tech-niques. Our findings provide valuable insights into the capabilities and limitations of ChatGPT in automated code generation.\nWe observed a clear correlation between the difficulty of the problem and ChatGPT's success rate. As the complexity of the problems increased, the model's ability to solve them decreased significantly. This outcome was consistent with our hypothesis that ChatGPT's performance would diminish on medium and hard problems. Additionally, our analysis of the number of lines of code revealed that shorter, more efficient solutions were more likely to be correct, though brute-force solutions emerged in some cases, particularly for medium and hard problems.\nFurthermore, our exploration of prompt engineering techniques demonstrated the potential to improve performance, with putting failed test cases into the query yielded the most substantial gains. Although GPT-4 offered enhanced capabilities, particularly in the medium and hard problem categories, it was clear that thoughtful prompting, combined with the introduction of diverse test cases, contributed meaningfully to problem solving improvements. These findings highlight the importance of input structuring when working with large language models in coding applications.\nOur comparative analysis of ChatGPT performance in different programming languages revealed interesting disparities. While Python remained the strongest language, models faced greater challenges in languages like C++ and Java. Moreover, programming languages such as Elixir, Erlang, and Racket posed considerable obstacles, with no solutions generated, emphasizing the need for further exploration of language-specific optimizations for these models. Lastly, analysis of the types of problems identified the strengths of ChatGPT in solving hash table, search, and divide-and-conquer problems, while dynamic programming and database-related problems proved to be more challenging.\nIn conclusion, while ChatGPT demonstrates impressive problem-solving capabilities, particularly for simpler problems, there are significant limitations in handling complex algorithms, certain languages, and problem types. Our study emphasizes the role of tailored prompt engineering in enhancing the performance of these models. As large language models continue to evolve, future research should focus on overcoming these challenges and refining the behavior of the model to improve the reliability of the coding in more advanced scenarios."}]}