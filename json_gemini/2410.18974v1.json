{"title": "3D-ADAPTER: GEOMETRY-CONSISTENT\nMULTI-VIEW DIFFUSION FOR HIGH-QUALITY\n3D GENERATION", "authors": ["Hansheng Chen", "Bokui Shen", "Yulin Liu", "Ruoxi Shi", "Linqi Zhou", "Connor Z. Lin", "Jiayuan Gu", "Hao Su", "Gordon Wetzstein", "Leonidas Guibas"], "abstract": "Multi-view image diffusion models have significantly advanced open-domain 3D\nobject generation. However, most existing models rely on 2D network architec-\ntures that lack inherent 3D biases, resulting in compromised geometric consis-\ntency. To address this challenge, we introduce 3D-Adapter, a plug-in module de-\nsigned to infuse 3D geometry awareness into pretrained image diffusion models.\nCentral to our approach is the idea of 3D feedback augmentation: for each denois-\ning step in the sampling loop, 3D-Adapter decodes intermediate multi-view fea-\ntures into a coherent 3D representation, then re-encodes the rendered RGBD views\nto augment the pretrained base model through feature addition. We study two vari-\nants of 3D-Adapter: a fast feed-forward version based on Gaussian splatting and\na versatile training-free version utilizing neural fields and meshes. Our extensive\nexperiments demonstrate that 3D-Adapter not only greatly enhances the geome-\ntry quality of text-to-multi-view models such as Instant3D and Zero123++, but\nalso enables high-quality 3D generation using the plain text-to-image Stable Dif-\nfusion. Furthermore, we showcase the broad application potential of 3D-Adapter\nby presenting high quality results in text-to-3D, image-to-3D, text-to-texture, and\ntext-to-avatar tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models (Ho et al., 2020; Song et al., 2021) have recently made significant strides in visual\nsynthesis, achieving production-quality results in image generation (Rombach et al., 2022). How-\never, the success of 2D diffusion does not easily translate to the 3D domain due to the scarcity\nof large-scale datasets and the lack of a unified, neural-network-friendly representation (Po et al.,\n2024). To bridge the gap between 2D and 3D generation, novel-view or multi-view diffusion mod-\nels (Liu et al., 2023b; Long et al., 2024; Shi et al., 2023; Li et al., 2024; Chen et al., 2024; Voleti\net al., 2024) have been finetuned from pretrained image or video models, facilitating 3D generation\nvia a 2-stage paradigm involving multi-view generation followed by 3D reconstruction (Liu et al.,\n2023a; 2024a; Li et al., 2024; Wang et al., 2024; Xu et al., 2024a). While these models generally\nexhibit good global semantic consistency across different view angles, a pivotal challenge lies in\nachieving local geometry consistency. This entails ensuring precise 2D-3D alignment of local fea-\ntures and maintaining geometric plausibility. Consequently, these two-stage methods often suffer\nfrom floating artifacts or produce blurry, less detailed 3D outputs (Fig. 1 (c)).\nTo enhance local geometry consistency, previous works have explored inserting 3D representations\nand rendering operations into the denoising sampling loop, synchronizing either the denoised out-\nputs (Gu et al., 2023; Xu et al., 2024b; Zuo et al., 2024) or the noisy inputs (Liu et al., 2023c;\nGao et al., 2024) of the network, a process we refer to as I/O sync. However, we observe that\nI/O sync generally leads to less detailed, overly smoothed textures and geometry (Fig. 1 (a)). This\nphenomenon can be attributed to two factors:"}, {"title": "2 RELATED WORK", "content": "3D-native diffusion models. We define 3D-native diffusion models as injecting noise directly into\nthe 3D representations (or their latents) during the diffusion process. Early works (Bautista et al.,\n2022; Dupont et al., 2022) have explored training diffusion models on low-dimensional latent vec-\ntors of 3D representations, but are highly limited in model capacity. A more expressive approach\nis training diffusion models on triplane representations (Chan et al., 2022), which works reason-\nably well on closed-domain data (Chen et al., 2023b; Shue et al., 2023; Gupta et al., 2023; Wang\net al., 2023). Directly working on 3D grid representations is more challenging due to the cubic\ncomputation cost (M\u00fcller et al., 2023), so an improved multi-stage sparse volume diffusion model is\nproposed in Zheng et al. (2023). In general, 3D-native diffusion models face the challenge of limited\ndata, and sometimes the extra cost of preprocessing the training data into 3D representations (e.g.,\nNeRF), which limit their scalability.\nNovel-/multi-view diffusion models. Trained on multi-view images of 3D scenes, view diffusion\nmodels inject noise into the images (or their latents) and thus benefit from existing 2D diffusion\nresearch. Watson et al. (2023) have demonstrated the feasibility of training a conditioned novel\nview generative model using purely 2D architectures. Subsequent works (Shi et al., 2024; 2023; Liu\net al., 2023b; Long et al., 2024) achieve open-domain novel-/multi-view generation by fine-tuning\nthe pre-trained 2D Stable Diffusion model (Rombach et al., 2022). However, 3D consistency in\nthese models is generally limited to global semantic consistency because it is learned solely from\ndata, without any inherent architectural bias to support detailed local alignment.\nTwo-stage 3D generation. Two-stage methods (Fig. 2 (a)) link view diffusion with multi-view\n3D reconstruction models, offering a significant speed advantage over score distillation sampling\n(SDS) (Poole et al., 2023). Liu et al. (2023a) initially combine Zero-1-to-3 (Liu et al., 2023b) with\nSparseNeuS (Long et al., 2022), and subsequent works (Liu et al., 2024a; Xu et al., 2024a; Long\net al., 2024; Tang et al., 2024a) have further explored more effective multi-view diffusion models\nand enhanced reconstruction methods. A common issue with two-stage approaches is that existing\nreconstruction methods, often designed for or trained under conditions of perfect consistency, lack\nrobustness to local geometric inconsistencies. This may result in floaters and blurry textures.\n3D-aware view diffusion and I/O sync. To introduce 3D-awareness in single-image diffusion\nmodels, Anciukevicius et al. (2023); Tewari et al. (2023) elevate image features into 3D NeRFs to\nrender denoised views. Extending this concept to multi-view diffusion, DMV3D (Xu et al., 2024b)\nrepresents an end-to-end feed-forward output sync approach. However, these methods often produce\nblurry outputs due to the information loss in the absence of residual connections. Liu et al. (2024b)\nattempt to preserve these connections through input sync and attention-based feature fusion, yet it\nlacks a robust architecture, leading to subpar quality as noted in Liu et al. (2023a). Alternatively,\nemploying feed-forward Gaussian reconstruction models (Xu et al., 2024a; Tang et al., 2024a) for\noutput sync preserves more information but often struggles to coherently fuse inconsistent views,\nresulting in only marginal improvements in 3D consistency as seen in Zuo et al. (2024). On the other\nhand, optimization-based I/O sync methods in Gu et al. (2023); Liu et al. (2023c); Gao et al. (2024)\neither require strong local conditioning or suffer from the pitfalls of score averaging, resulting in\noverly smoothed textures."}, {"title": "3 PRELIMINARIES", "content": "Let $p(x|c)$ denote the real data distribution, where $c$ is the condition (e.g., text prompts) and\n$x \\in R^{V\\times 3 \\times H \\times W}$ denotes the $V$-view images of a 3D object. A Gaussian diffusion model de-\nfines a diffusion process that progressively perturb the data point by adding an increasing amount of\nGaussian noise $\\epsilon \\sim N(0, I)$, yielding the noisy data point $x_t := \\alpha_t x + \\sigma_t \\epsilon$ at diffusion timestep $t$,\nwith pre-defined noise schedule scalars $\\alpha_t, \\sigma_t$. A denoising network $\\mathcal{D}$ is then tasked with removing\nthe noise from $x_t$ to predict the denoised data point. The network is typically trained with an L2\ndenoising loss:\n$\\mathcal{L}_{diff} = \\mathbb{E}_{t, c, x, \\epsilon} [\\omega_{diff} || \\mathcal{D}(x_t, c, t) - x ||_2^2]$\nwhere $t \\sim \\mathcal{U}(0, T)$, and $\\omega_{diff}$ is an empirical time-dependent weighting function (e.g., SNR weight-\ning $\\omega_{diff} = (\\alpha_t / \\sigma_t)^2$). At inference time, one can sample from the model using efficient ODE/SDE\nsolvers (Lu et al., 2022) that recursively denoise $x_t$, starting from an initial noisy state $x_{t_{init}}$, until\nreaching the denoised state $x_0$. Note that in latent diffusion models (LDM) (Rombach et al., 2022),\nboth diffusion and denoising occur in the latent space. For brevity, we do not differentiate between\nlatents and images in the equations and figures.\nI/O sync baseline. We broadly define I/O sync as inserting a 3D representation and a render-\ning/projecting operation at the input or output end of the denoising network to synchronize multiple\nviews. Input sync is primarily used for texture generation, and it is essentially equivalent to output\nsync, assuming linearity and synchronized initialization (detailed in the Appendix A). Therefore, for\nsimplicity, this paper considers only output sync as the baseline. As depicted in Fig. 2 (b), a typical\noutput sync model can be implemented by reconstructing a 3D representation from the denoised\noutputs $\\hat{x}_t$, and then re-rendering the views from 3D to replace the original outputs."}, {"title": "4 3D-ADAPTER", "content": "To overcome the limitations of I/O sync, our key idea is the 3D feedback augmentation architec-\nture, which involves reconstructing a 3D representation midway through the denoising network and\nfeeding the rendered views back into the network using ControlNet-like feature addition. This ar-\nchitecture preserves the original flow of the base model while effectively leveraging its inherent\npriors.\nBased on this idea, we propose the 3D-Adapter, as illustrated in Fig. 2 (c). For each denoising step,\nafter passing the input noisy views $x_t$ through the base U-Net encoder, we use a copy of the base"}, {"title": "4.1 3D-ADAPTER USING FEED-FORWARD GRM", "content": "GRM (Xu et al., 2024a) is a feed-forward sparse-view 3D reconstruction model based on 3DGS.\nIn this section, we describe the method to train GRM-based 3D-Adapters for the text-to-multi-view\nmodel Instant3D (Li et al., 2024) and image-to-multi-view model Zero123++ (Shi et al., 2023).\nTraining phase 1: finetuning GRM. GRM is originally trained on consistent ground truth input\nviews, and is not robust to low-quality intermediate views, which are often highly inconsistent and\nblurry. To overcome this challenge, we first finetune GRM using the intermediate images$\\hat{x}$ as\ninputs, where the time $t$ is randomly sampled just like in the diffusion loss. In this training phase,\nwe freeze the base encoder and decoder of the U-Net, and initialize GRM with the official checkpoint\nfor finetuning. As shown in Fig. 2 (c), a rendering loss $\\mathcal{L}_{rend}$ is employed to supervise GRM with\nground truth novel views. Specifically, both the appearance and geometry are supervised using the\ncombination of an L1 loss $\\mathcal{L}_{RGBAD}$ on RGB/alpha/depth maps, and an LPIPS loss $\\mathcal{L}_{LPIPS}^{RGB}$ (Zhang\net al., 2018) on RGB only. The loss is computed on 16 rendered views $X_t \\in R^{16\\times 5 \\times 512 \\times 512}$ and\nthe corresponding ground truth views $X_{gt}$, given by:\n$\\mathcal{L}_{rend} = \\mathbb{E}_{t, c, x, \\epsilon} [\\omega_{rend} (\\mathcal{L}_{RGBAD}(X_t, X_{gt}) + \\mathcal{L}_{LPIPS}^{RGB}(X_t, X_{gt}))]$,\nwhere $\\omega_{rend}$ is a time-dependent weighting function. We use $\\omega_{rend} = \\alpha_t/\\sqrt{\\alpha_t^2 + \\sigma_t^2}$. The L1\nRGBAD loss also employs channel-wise weights, which are detailed in our code.\nTraining phase 2: finetuning feedback ControlNet. In this training phase, we freeze all mod-\nules except the feedback ControlNet encoder, which is initialized with the base U-Net weights for\nfinetuning. Following standard ControlNet training method, we employ the diffusion loss in Eq. (1)\nto finetune the RGBD feedback ControlNet. To accelerate convergence, we feed rendered RGBD\nviews of a less noisy timestep $x_{0.1t}$ to the ControlNet during training.\nInference: guided 3D feedback augmentation. One potential issue is that the ControlNet en-\ncoder may overfit the finetuning dataset, resulting in an undesirable bias that persists even if the\nrendered RGBD $\\hat{x}_t$ is replaced with a zero tensor. To mitigate this issue, inspired by classifier-free\nguidance (CFG) (Ho & Salimans, 2021), we replace $\\hat{x}_t$ with the guided denoised views $\\bar{x}_t$ during\ninference to cancel out the ControlNet bias:\n$\\bar{x}_t = \\mathcal{D}_{aug}(\\mathcal{D}_{aug}(x_t, c, t, x_t) - \\mathcal{D}_{aug}(x_t, c, t, 0)) + d_c\\mathcal{D}(x_t, c, t) + (1 - d_c)\\mathcal{D}(x_t, 0, t)$,\nwhere $d_c$ is the regular condition CFG scale, and $d_{aug}$ is our feedback augmentation guidance\nscale. During training, we feed zero tensors to the ControlNet with a 20% probability, so that\n$\\mathcal{D}_{aug}(x_t, c, t, 0)$ learns a meaningful dataset bias.\nTraining details. We adopt various training techniques to reduce the memory footprint, including\nmixed precision training, 8-bit AdamW (Dettmers et al., 2022; Loshchilov & Hutter, 2019), gradient\ncheckpointing, and deferred back-propagation (Xu et al., 2024a; Zhang et al., 2022). The adapter is\ntrained with a total batch size of 16 objects on 4 A6000 GPUs. In phase 1, GRM is finetuned with\na small learning rate of 5 \u00d7 10-6 for 2k iterations (for Instant3D) or 4k iterations (for Zero123++).\nIn phase 2, ControlNet is finetuned with a learning rate of 1 \u00d7 10-5 for 5k iterations. 47k (for\nInstant3D) or 80k (for Zero123++) objects from a high-quality subset of Objaverse (Deitke et al.,\n2023) are rendered as the training data."}, {"title": "4.2 3D-ADAPTER USING 3D OPTIMIZATION/TEXTURE BACKPROJECTION", "content": "Feed-forward 3D reconstruction methods, like GRM, are typically constrained by specific camera\nlayouts. In contrast, more flexible reconstruction approaches, such as optimizing an Instant-NGP\nNeRF (M\u00fcller et al., 2022; Mildenhall et al., 2020) and DMTet mesh (Shen et al., 2021), can ac-\ncommodate diverse camera configurations and achieve higher-quality results, although they require\nlonger optimization times.\nIn this subsection, we design an optimization-based 3D adapter, using Stable Diffusion v1.5 (Rom-\nbach et al., 2022) as the base model, where each view is denoised independently, with the adapter\nbeing the only component that shares information across views.\nDuring the sampling process, the adapter performs NeRF optimization for the first 60% of the de-\nnoising steps. It then converts the color and density fields into a texture field and DMTet mesh,\nrespectively, to complete the remaining 40% denoising steps. All optimizations are incremental,\nmeaning the 3D state from the previous denoising step is retained to initialize the next. As a result,\nonly 96 optimization steps are needed per denoising step. Alternatively, for texture generation only,\nmulti-view aggregation can be achieved by backprojecting the views into UV space and blending\nthe results according to visibility. For feedback augmentation, the off-the-shelf \"tile\u201d ControlNet,\noriginally trained for superresolution, performs effectively for RGB feedback due to its robustness\nto blurry inputs, while the off-the-shelf depth ControlNet can naturally handle depth feedback.\nIt should be noted that, when using single-image diffusion as the base model, 3D-Adapter alone\ncannot provide the necessary global semantic consistency for 3D generation. Therefore, it should\nbe complemented with other sources of consistency, such initialization with partial noise like\nSDEdit (Meng et al., 2022) or extra conditioning from ControlNets. For text-to-avatar generation,\nwe use rendered views of a human template for SDEdit initialization with the initial timestep $t_{init}$\nset to 0.88T, and employ an extra pose ControlNet for conditioning. For text-to-texture generation,\nglobal consistency is usually good due to ground truth depth conditioning.\nOptimization loss functions. For NeRF/mesh optimization, we employ L1 and LPIPS losses\non RGB and alpha maps, and total variation (TV) loss on normal maps. Additionally, we en-\nforce stronger geometry regularization using ray entropy loss for NeRF, and Laplacian smoothing\nloss (Sorkine et al., 2004) plus normal consistency loss for mesh, making the optimization more\nrobust to imperfect intermediate views. More details can be found in Appendix C."}, {"title": "4.3 TEXTURE POST-PROCESSING", "content": "To further enhance the visual quality of objects generated from text, we implement an optional\ntexture refinement pipeline as a post-processing step. First, when using the GRM-based 3D-Adapter,\nwe convert the generated 3DGS into a textured mesh via TSDF integration. With the initial mesh,\nwe render six surrounding views and apply per-view SDEdit refinement ($t_{init}$ = 0.5T) using Stable\nDiffusion v1.5 with \"tile\u201d ControlNet. Finally, the refined views are aggregated into the UV space\nusing texture backprojection. For fair comparisons in the experiments, this refinement step is not\nused by default unless specified otherwise."}, {"title": "5 EXPERIMENTS", "content": null}, {"title": "5.1 EVALUATION METRICS", "content": "To evaluate the results generated by 3D-Adapter and compare them to various baselines and com-\npetitors, we compute the following metrics based on the rendered images of the generated 3D rep-\nresentations:\n\u2022 CLIP score (Radford et al., 2021; Jain et al., 2022): Evaluates image-text alignment in text-to-\n3D, text-to-texture, and text-to-avatar tasks. We use CLIP-ViT-L-14 for all CLIP-related metrics.\n\u2022 Aesthetic score (Schuhmann et al., 2022): Assesses texture details. The user study in Wu et al.\n(2024) revealed that this metric highly correlates with human preference in texture details.\n\u2022 FID (Heusel et al., 2017): Measures the visual quality when reference test set images are avail-\nable, applicable to text-to-3D models trained on common dataset and all image-to-3D models."}, {"title": "5.2 TEXT-TO-3D GENERATION", "content": "For text-to-3D generation, we adopt the GRM-based 3D-Adapter with Instant3D U-Net as the base\nmodel. All results are generated using EDM Euler ancestral solver (Karras et al., 2022) with 30\ndenoising steps and mean latent initialization (Appendix B.2). The inference time is around 0.7\nsec per step, and detailed inference time analysis is presented in Appendix B.3. For evaluation, we\nfirst compare 3D-Adapter with the baselines and conduct ablation studies on a validation set of 379\nBLIP-captioned objects sampled from a high-quality subset of Objaverse (Li et al., 2022; Deitke\net al., 2023). The results are shown in Table 1, with the rendered images from the dataset used\nas real samples when computing the FID metric. Subsequently, we benchmark 3D-Adapter on the\nsame test set as GRM (Xu et al., 2024a), consisting of 200 prompts, to make fair comparisons to the\nprevious SOTAs in Table 2. Qualitative results are shown in Fig. 3.\nBaselines. The two-stage GRM (A0) exhibits good visual quality, but the MDD metric is magni-\ntudes higher than that of our 3D-Adapter (B0-B3) due to the highly ambiguous geometry caused"}, {"title": "5.3 IMAGE-TO-3D GENERATION", "content": "For image-to-3D generation, we adopt the same approach used for text-to-3D generation, except for\nemploying Zero123++ U-Net as the base model. We follow the same evaluation protocol as in Xu\net al. (2024a), using 248 GSO objects (Downs et al., 2022) as the test set. As shown in Table 3,\n3D-Adapter ($\\lambda_{aug}$=1) outperforms the two-stage GRM and other competitors (Liu et al., 2023a; Zou\net al., 2024; Jun & Nichol, 2023; Tang et al., 2024a;b; Long et al., 2024; Liu et al., 2024a; Xu"}, {"title": "5.4 TEXT-TO-TEXTURE GENERATION", "content": "For text-to-texture evaluation, 3D-Adapter employs fast texture backprojection to blend multiple\nviews for intermediate timesteps, and switches to high-quality texture field optimization (similar to\nNeRF) for the final timestep. A community Stable Diffusion v1.5 variant, DreamShaper 8, is adopted\nas the base model. During the sampling process, 32 surrounding views are used initially, and this\nnumber is gradually reduced to 7 views during the denoising process to reduce computation in later\nstages. We adopt the EDM Euler ancestral solver with 24 denoising steps. The total inference time\nis around 1.5 minutes per object on a single RTX A6000 GPU (including UV unwrapping), which\nis faster than the competitors (SyncMVD (Liu et al., 2023c): ~1.9 min, TEXTure (Richardson\net al., 2023): ~2.0 min, Text2Tex (Chen et al., 2023a): ~11.2 min). 92 BLIP-captioned objects are\nsampled from a high-quality subset of Objaverse as our test set.\nComparison with baselines. As shown in Table 5 and Fig. 5, the two-stage baseline has good tex-\nture details but notably worse CLIP score due to poor consistency. The I/O sync baseline has much\nbetter consistency, but it sacrifices details, resulting in the worst aesthetic score. In comparison,\n3D-Adapter excels in both metrics, producing detailed and consistent textures.\nComparison with other competitors. Quantitatively, Table 6 demonstrates that 3D-Adapter sig-\nnificantly outperforms previous SOTAs on both metrics. Interestingly, even our two-stage baseline\nin Table 5 surpasses the competitors, which can be attributed to our use of texture field optimization"}, {"title": "5.5 TEXT-TO-AVATAR GENERATION", "content": "For text-to-avatar generation, the optimization-based 3D-Adapter is adopted with a custom pose\nControlNet for Stable Diffusion v1.5, which provides extra conditioning given a human pose tem-\nplate. 32 full-body views and 32 upper-body views are selected for denoising, capturing both the\noverall figure and face details. These are later reduced to 12 views during the denoising process.\nWe use the EDM Euler ancestral solver with 32 denoising steps, with an inference time of approx-\nimately 7 minutes per object on a single RTX A6000 GPU (including 0.5 minutes on CPU-based\nUV unwrapping). Texture editing (using text-to-texture pipeline and SDEdit with $t_{init}$ = 0.3T) and\nrefinement can be optionally applied to further improve texture details, which costs 1.4 minutes. For\nevaluation, we compare 3D-Adapter with baselines using 21 character prompts on the same pose\ntemplate. As shown in Table 4, 3D-Adapter achieves the highest scores across all three metrics,\nindicating superior appearance and geometry. Fig. 6 reveals that I/O sync produces overly smoothed\ntexture and geometry due to mode collapse, while the two-stage baseline results in noisy, less coher-\nent texture and geometry. These observations also align with the quantitative results in Table 4."}, {"title": "6 CONCLUSION", "content": "In this work, we have introduced 3D-Adapter, a plug-in module that effectively enhances the 3D ge-\nometry consistency of existing multi-view diffusion models, bridging the gap between high-quality\n2D and 3D content creation. We have demonstrated two variants of 3D-Adapter: the fast 3D-Adapter\nusing feed-forward Gaussian reconstruction, and the flexible training-free 3D-Adapter using 3D op-\ntimization and pretrained ControlNets. Experiments on text-to-3D, image-to-3D, text-to-texture,\nand text-to-avatar tasks have substantiated its all-round competence, suggesting great generality and\npotential in future extension.\nLimitations. 3D-Adapter introduces substantial computation overhead, primarily due to the VAE\ndecoding process before 3D reconstruction. In addition, we observe that our finetuned ControlNet\nfor 3D feedback augmentation strongly overfits the finetuning data, which may limit its generaliza-\ntion despite the proposed guidance method. Future work may focus on developing more efficient,\neasy-to-finetune networks for 3D-Adapter."}, {"title": "A THEORETICAL ANALYSIS OF THE I/O SYNC TECHNIQUE", "content": "When performing diffusion ODE sampling using the common Euler solver, a linear input sync op-\neration is equivalent to syncing the output $\\hat{x}_t$ as well as the initialization $x_{t_{init}}$. This is because the\ninput $x_t$ can be expressed as a linear combination of all previous outputs $\\{x_{t-\\Delta t}, x_{t-2\\Delta t},... \\}$ and\nthe initialization $x_{t_{init}}$ by expanding the recursive Euler steps.\nFurthermore, linear I/O sync is also equivalent to linear score sync, since the learned score function\n$s_t(x_t)$ can also be expressed as a linear combination of the input $x_t$ and output $\\hat{x}_t$:\n$s_t(x_t) = \\frac{\\sigma_t}{\\alpha_t}\\hat{x}_t - \\frac{\\alpha_t}{\\sigma_t}x_t$\nHowever, synchronizing the score function, a.k.a. score averaging, is theoretically problematic.\nLet $p(x|c_1), p(x|c_2)$ be two independent probability density functions of a corresponding pixel $x$\nviewed from cameras $c_1$ and $c_2$, respectively. A diffusion model is trained to predict the score\nfunction $s_t(x_t|c_v)$ of the noisy distribution at timestep $t$, defined as:\n$s_t(x_t|c_v) = \\nabla_{x_t} \\log \\int p(x_t|x)p(x|c_v)dx$,\nwhere $p(x_t|x) = \\mathcal{N}(x_t; \\alpha_t x, \\sigma_t I)$ is a Gaussian perturbation kernel. Ideally, assuming $c_1$ and\n$c_2$ are independent, combining the two conditional PDFs $p(x|c_1)$ and $p(x|c_2)$ yields the product\n$p(x|c_1, c_2) = \\frac{p(x|c_1)p(x|c_2)}{Z}$, where $Z$ is a normalization factor. The corresponding score func-\ntion should then become $s_t(x_t|c_1, c_2) = \\nabla_{x_t} \\log \\int p(x_t|x)p(x|c_1, c_2)dx$. However, the average\nof $s_t(x_t|c_1)$ and $s_t(x_t|c_2)$ is generally not proportional to $s_t(x_t|c_1, c_2)$, i.e.:\n$\\frac{1}{2}s_t(x_t|c_1) + \\frac{1}{2}s_t(x_t|c_2) = \\nabla_{x_t} \\log(\\int p(x_t|x)p(x|c_1)dx)(\\int p(x_t|x)p(x|c_2)dx) \\neq \\nabla_{x_t} \\log \\int p(x_t|x) \\frac{p(x|c_1)p(x|c_2)}{Z}dx = s_t(x_t|c_1, c_2)$.\nIn Fig. 7, we illustrate a simple 1D simulation, showing that score averaging leads to mode col-\nlapse, when compared to the real product distribution. This explains the blurry, mean-shaped results\nproduced by the I/O sync baselines."}, {"title": "B DETAILS ON GRM-BASED 3D-ADAPTER", "content": null}, {"title": "B.1 CONTROLNET", "content": "The GRM-based 3D-Adapter trains a ControlNet (Zhang et al., 2023) for feedback augmentation,\nwhich has very large model capacity and can easily overfit our relatively small finetuning dataset\n(e.g., 47k objects for Instant3D). Therefore, using the CFG-like bias subtraction technique (Eq. (4))\nis extremely important to the generalization performance, which is already validated in our ablation\nstudies. Additionally, we disconnect the text prompt input from the ControlNet to further alleviate\noverfitting."}, {"title": "B.2 MEAN LATENT INITIALIZATION", "content": "Instant3D's 4-view UNet is sensitive to the initialization method, as noted in the original paper (Li\net al., 2024), which develops an empirical Gaussian blob initialization method to stabilize the back-\nground color. In contrast, this paper adopts a more principled mean latent initialization method by"}, {"title": "B.3 INFERENCE TIME", "content": "Detailed module-level inference times per denoising step is shown in Table 7 (with classifier-free\nguidance and guided 3D feedback augmentation enabled). Apparently, the SDXL VAE decoder is\nthe most expensive module within 3D-Adapter, which may be replaced by a more efficient decoder\nin future work."}, {"title": "C DETAILS ON OPTIMIZATION-BASED 3D-ADAPTER", "content": "The optimization-based 3D-Adapter faces the challenge of potentially inconsistent multi-view in-\nputs, especially at the early denoising stage. Existing surface optimization approaches, such as\nNeuS (Wang et al., 2021a), are not designed to address the inconsistency. Therefore, we have devel-\noped various techniques for the robust optimization of InstantNGP NeRF (M\u00fcller et al., 2022) and\nDMTet mesh (Shen et al., 2021), using enhanced regularization and progressive resolution.\nRendering. For each NeRF optimization iteration, we randomly sample a 128\u00d7128"}]}