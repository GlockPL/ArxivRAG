{"title": "Kolmogorov-Arnold Network Autoencoders", "authors": ["Mohammadamin Moradi", "Shirin Panahi", "Ying-Cheng Lai", "Erik Bollt"], "abstract": "Deep learning models have revolutionized various domains, with Multi-Layer\nPerceptrons (MLPs) being a cornerstone for tasks like data regression and image\nclassification. However, a recent study has introduced Kolmogorov-Arnold Net-\nworks (KANs) as promising alternatives to MLPs, leveraging activation functions\nplaced on edges rather than nodes. This structural shift aligns KANs closely with\nthe Kolmogorov-Arnold representation theorem, potentially enhancing both model\naccuracy and interpretability. In this study, we explore the efficacy of KANs in the\ncontext of data representation via autoencoders, comparing their performance with\ntraditional Convolutional Neural Networks (CNNs) on the MNIST, SVHN, and\nCIFAR-10 datasets. Our results demonstrate that KAN-based autoencoders achieve\ncompetitive performance in terms of reconstruction accuracy, thereby suggesting\ntheir viability as effective tools in data analysis tasks.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep learning has seen unprecedented advancements, catalyzing breakthroughs in\nvarious fields such as image recognition, natural language processing, medical diagnostics, and\ncybersecurity [1, 2, 3, 4, 5, 6, 7, 8]. Central to these advancements are neural network architectures\nlike MLPs, which have proven effective in tasks ranging from simple regression to complex image\nclassification. MLPs are well-known for their ability to approximate a wide range of functions, a\nproperty substantiated by the universal approximation theorem [9, 10, 11]. However, deep learning\narchitectures are constantly evolving, driven by the constant thirst for improved performance, in-\nterpretability, and efficiency. One of the emerging paradigms challenging the dominance of MLPs\nis the recently introduced KANs [12, 13]. KANs draw inspiration from the Kolmogorov-Arnold\nrepresentation theorem, which asserts that any continuous multivariate function can be represented\nas a composition of functions of a single variable. This theorem underpins the structure of KANs,\nwhere activation functions are trained and applied not at neuron nodes but directly on the edges of\nthe network graph. This alters the dynamics of information flow within the network and presents\nintriguing implications: KANs potentially offer enhanced model capacity, better handling of complex\ndependencies in data, and improved interpretability of learned representations compared to traditional\nMLPs. By decentralizing activation functions to the edges, KANs facilitate a more modular ap-\nproach to feature extraction and transformation, potentially yielding more structured and interpretable\nrepresentations of input data.\nKAN has received a great deal of attention recently. MonoKAN builds on the KAN architecture\nand is a version that enforces certified partial monotonicity. This modification ensures that model"}, {"title": "2 Methods", "content": "The primary objective of an autoencoder is to map input data into a lower-dimensional latent space\n(encoding) and then reconstruct the original input from this compressed representation (decoding).\nThis process involves two main components: the encoder and the decoder. The encoding is achieved\nthrough a series of neural network layers that reduce the dimensionality of the input while retaining\nessential features. Mathematically, if x represents the input, the encoder function $f_e$ transforms x\ninto a latent representation z (a.k.a. the \u201cbottleneck\u201d), where $z = f_e(x)$. On the other hand, the\ndecoder reconstructs the input data from the latent representation. The decoder function $g_d$ aims to\nreverse the encoding process, producing a reconstruction $\\hat{x}$ from z, such that $\\hat{x} = g_d(z)$."}, {"title": "2.1 Autoencoders and Features Reduction", "content": "The primary objective of an autoencoder is to map input data into a lower-dimensional latent space\n(encoding) and then reconstruct the original input from this compressed representation (decoding).\nThis process involves two main components: the encoder and the decoder. The encoding is achieved\nthrough a series of neural network layers that reduce the dimensionality of the input while retaining\nessential features. Mathematically, if x represents the input, the encoder function $f_e$ transforms x\ninto a latent representation z (a.k.a. the \u201cbottleneck\u201d), where $z = f_e(x)$. On the other hand, the\ndecoder reconstructs the input data from the latent representation. The decoder function $g_d$ aims to\nreverse the encoding process, producing a reconstruction $\\hat{x}$ from z, such that $\\hat{x} = g_d(z)$."}, {"title": "2.2 Kolmogorov-Arnold Networks (KAN)", "content": "KANs represent an innovative approach to neural network architecture, inspired by the Kolmogorov-\nArnold representation theorem. This theorem asserts that any continuous multivariate function can\nbe expressed as a composition of continuous functions of a single variable and addition operations.\nKANs leverage this theorem by placing activation functions on the edges of the network graph rather\nthan at the nodes, as is typical in MLPs and CNNs. This means that each connection between neurons\nincludes an activation function, which transforms the information flow between nodes. Also, by\ndecentralizing the activation functions to the edges, KANs promote a more modular approach to\nfeature extraction and transformation. This modularity potentially enhances the interpretability and\nflexibility of the network, allowing for more nuanced representations of the input data.\nAs mentioned above, the Kolmogorov-Arnold representation theorem states that any continuous\nmultivariate function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ can be represented as a superposition of continuous univariate\nfunctions. The general form of the theorem is:\n$$\nf(x_1, x_2,..., x_n) = \\sum_{i=1}^{2n-1} \\phi_i \\bigg(\\sum_{j=1}^{n} \\psi_{i,j}(x_j)\\bigg)\n$$\nwhere $\\phi_i$ and $\\psi_{i,j}$ are continuous univariate functions. Each of these functions is parameterized using\na linear combination of the residual and spline functions. Spline functions are piecewise polynomials\nthat can adapt to the data more flexibly than traditional linear models and are parametrized as a linear\ncombination of B-splines such that:\n$$\nS(x) = \\sum_{i=0}^{n} c_i B_{i,d}(x).\n$$\nwhere $c_i$ are the trainable coefficients and $B_{i,d}(x)$ are the B-spline basis functions of degree d [28].\nKANs scale more efficiently than traditional MLPs, which typically scale as $O(n^2)$ with the number\nof neurons and layers. The spline-based structure of KANs allows them to achieve high accuracy\nwith fewer parameters. The empirical results suggest that KANs demonstrate faster scaling behavior\ncompared to MLPs, leading to improved accuracy and efficiency in tasks like data fitting and solving\npartial differential equations (PDEs) [12, 13]."}, {"title": "2.3 \u039a\u0391\u039d Autoencoders", "content": "The encoder-decoder structure of KAN-based autoencoders mirrors that of traditional autoencoders\nbut with the key difference of edge-based activations. This architectural shift aims to capture\nmore complex dependencies within the data, potentially leading to better performance and more\ninterpretable latent representations. In this study, we integrate KANs into the autoencoder framework\nto evaluate their efficacy in image representation tasks. We compare the performance of KAN-\nbased autoencoders with that of traditional convolutional autoencoders on the MNIST, SVHN, and\nCIFAR-10 datasets, assessing both reconstruction accuracy and the quality of the learned features."}, {"title": "3 Results", "content": "In this section, we present the results of our experiments on three datasets: MNIST, CIFAR-10, and\nSVHN. We compare the performance of autoencoders built with CNNs and KANS."}, {"title": "3.1 Datasets", "content": "MNIST: The MNIST dataset consists of 70,000 grayscale images of handwritten digits, with each\nimage having a resolution of 28x28 pixels. The dataset is divided into 60,000 training images and\n10,000 test images. Each image belongs to one of ten classes, representing the digits 0 through 9.\nThe simplicity and standardized format of MNIST make it a popular benchmark for evaluating image\nprocessing algorithms.\nSVHN: The Street View House Numbers (SVHN) dataset contains 600,000 color images of house\nnumbers extracted from Google Street View images. Each image is 32x32 pixels and includes\nmultiple digits, but the task typically involves classifying the digit at the center of the image. The\ndataset is split into 73,257 training images, 26,032 test images, and 531,131 additional training\nimages. SVHN is more complex than MNIST due to its real-world origins and multi-digit context.\nCIFAR-10: The CIFAR-10 dataset comprises 60,000 color images of size 32x32 pixels, divided into\n10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. Each class contains\n6,000 images, with 50,000 images used for training and 10,000 for testing. The dataset poses a more\nchallenging problem compared to MNIST due to its higher variability and color information."}, {"title": "3.2 Settings and Strudcture", "content": "Both types of autoencoders were evaluated on the MNIST, CIFAR-10, and SVHN datasets. The\nautoencoders were trained for 10 epochs using the AdamW optimizer with a learning rate of le-3 and\na weight decay of le-4. We used the MSE loss function to measure reconstruction accuracy. The\nperformance of the autoencoders was assessed based on the reconstruction loss on the test sets. Our\nAE-KAN also includes two extra dense layers compared to AE-CNN (see Fig. 1). Additionally, we\ntrained a KNN classifier on the latent representations learned by the autoencoders to evaluate the\nquality of these representations for downstream classification tasks. The accuracy and F1-score of the\nKNN classifier were used as additional metrics.\nThe KAN model used in our codes uses the efficient KAN implemantion [31]. The performance issue\nin the original implementation arises from the need to expand all intermediate variables to perform ac-\ntivation functions, requiring the input tensor to have the shape (batch_size, out_features, in_features).\nSince all activation functions are linear combinations of a fixed set of basis functions, specifically\nB-splines, the computation can be reformulated by activating the input with the basis functions\nand then linearly combining them. This reformulation reduces memory cost and simplifies the\ncomputation to a matrix multiplication, and it works naturally for both the forward and backward\npass. The original KAN proposed an L1 regularization defined on input samples, which requires\nnon-linear operations on the (batch_size, out_features, in_features) tensor and is incompatible with"}, {"title": "3.3 Results and Discussion", "content": "The results of our experiments are summarized in Table 1.\nFor the MNIST dataset (see the reconstruction sample with bottleneck_size = 8 in Figs. 4a and 5a),\nthe performance of AE-KAN and AE-CNN varies across different bottleneck configurations. At the\nsmallest bottleneck configuration (2x2 for AE-CNN and 4 for AE-KAN), AE-KAN demonstrates\nsuperior reconstruction loss (0.2170) compared to AE-CNN (0.2667). This suggests that AE-KAN\nis more effective at reconstructing the original data from a compressed representation, potentially\ndue to the unique structure of KANs where activation functions on edges might provide richer\ntransformations. Additionally, AE-KAN shows slightly higher accuracy and F1-score, indicating\nbetter classification performance. However, this comes at the cost of significantly more parameters\n(62,796 for AE-KAN versus 617 for AE-CNN), suggesting that AE-KAN's improved performance\nmay partly stem from its higher complexity and capacity. As the bottleneck size increases to 3x3 for\nAE-CNN and 9 for AE-KAN, AE-KAN continues to outperform AE-CNN in terms of reconstruction\nloss (0.2024 vs. 0.2288) and achieves marginally better accuracy and F1-scores. Interestingly,\nAE-KAN completes this task slightly faster (43.14 seconds) than AE-CNN (44.21 seconds), despite\nhaving a higher parameter count (62,881 vs. 818). However, with the largest bottleneck configuration\n(6x3 for AE-CNN and 18 for AE-KAN), AE-CNN outperforms AE-KAN. AE-CNN achieves a\nreconstruction loss of 0.1722 and accuracy and F1-scores of 0.90, compared to AE-KAN's 0.2238\nreconstruction loss and 0.78 accuracy and F1-scores. This reversal indicates that AE-CNN can\nleverage its structure more effectively for larger bottlenecks.\nOn the SVHN dataset (see the reconstruction sample with bottleneck_size = 64 in Figs. 4b and\n5b) as well as the CIFAR-10 dataset (see the reconstruction sample with bottleneck_size = 64 in"}, {"title": "4 Discussion", "content": "In this study, we investigated the application of KANs in the context of autoencoder architectures for\nimage representation tasks. Our experiments focused on comparing KAN-based autoencoders with\ntraditional CNN-based autoencoders across three benchmark datasets: MNIST, CIFAR-10, and SVHN.\nThe primary objectives were to evaluate reconstruction accuracy and assess the quality of learned\nrepresentations using classification tasks. To ensure a fair comparison between the CNN and KAN\nautoencoders, we maintained identical training conditions for both models. This included using the\nsame optimizer, learning rate, weight decay, number of epochs, and batch size. Our findings indicate\nthat KAN-based autoencoders performed competitively compared with CNN-based autoencoders in\nterms of reconstruction accuracy on all three datasets. Specifically, in some cases KANs achieved"}]}