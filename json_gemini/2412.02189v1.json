{"title": "Comparative Performance of Machine Learning Algorithms for Early Genetic Disorder and Subclass Classification", "authors": ["Abu Bakar Siddik", "Faisal R. Badal", "Afroza Islam"], "abstract": "A great deal of effort has been devoted to discovering a particular genetic disorder, but its classification across a broad spectrum of disorder classes and types remains elusive. Early diagnosis of genetic disorders enables timely interventions and improves outcomes. This study implements machine learning models using basic clinical indicators measurable at birth or infancy to enable diagnosis in preliminary life stages. Supervised learning algorithms were implemented on a dataset of 22083 instances with 42 features like family history, newborn metrics, and basic lab tests. Extensive hyperparameter tuning, feature engineering, and selection were undertaken. Two multi-class classifiers were developed: one for predicting disorder classes (mitochondrial, multifactorial, and single-gene) and one for subtypes (9 disorders). Performance was evaluated using accuracy, precision, recall, and the F1-score. The CatBoost classifier achieved the highest accuracy of 77% for predicting genetic disorder classes. For subtypes, SVM attained a maximum accuracy of 80%. The study demonstrates the feasibility of using basic clinical data in machine learning models for early categorization and diagnosis across various genetic disorders. Applying ML with basic clinical indicators can enable timely interventions once validated on larger datasets. It is necessary to conduct further studies to improve model performance on this dataset.", "sections": [{"title": "1. Introduction", "content": "A genetic disorder is an illness that results from a change or mutation in the DNA (Deoxyribonucleic Acid) sequence that hinders a person from developing normally and healthily [1]. It can be caused by a mutation in one or more genes or by a chromosomal aberration [2]. Early and accurate identification of genetic disorders remains an ongoing challenge in healthcare. While significant advances have been made in diagnosing specific conditions, the categorization and prediction of disorders across the spectrum of genetic inheritance types have proven elusive. The ability to systematically discern genetic abnormalities in the preliminary stages of life carries profound clinical implications. Timely diagnosis enables prompt intervention and improves prognosis and quality of life for affected individuals [3]. Hence, there is an urgent need for sophisticated yet accessible techniques to delineate the broad classes of genetic disorders and pinpoint particular subtypes.\nMachine learning refers to the procedure of acquiring knowledge and skills to develop a statistical model that has the ability to make predictions about future events or classify future observations [4]. In recent years, machine learning (ML) has garnered tremendous interest in advancing genetic research [5, 6], owing to its aptitude for discerning multidimensional interactions devoid of assumptions [7]. The supervised learning paradigm has proven especially invaluable for performing robust classification from complex inputs. Supervised learning is a specialized area of machine learning that involves the use of an algorithm to learn from input data that has been explicitly labeled with the aim of producing a desired output. During the training phase, the system is fed sets of data that have been labeled to show the system what values of input correspond to what values of output. Predictions can then be made using the trained model.\nA few pioneering studies have attempted to leverage ML for elucidating the genetic underpinnings of diseases like cancer [8], Diabetes [9], Alzheimer's and [10]. However, these works centered on predicting specific illnesses, seldom exploring the full taxonomy. Besides, the predictive models relied predominantly on clinical or imaging data that manifest much later in the disease timeline. To address these limitations, this paper implements an epitomized ML approach for categorizing genetic disorders from baseline indicators observable early in life. We develop two multi-class classifiers using five supervised algorithms - support vector machine (SVM), Random Forest [11], CatBoost [12], Gradient Boosting [13], and LightGBM [14]. The K-nearest neighbour (KNN) and Logistic Regression [15] algorithms have been excluded from subsequent evaluation due to their suboptimal performance in our initial experimentation. These models have been used to identify the underlying genetic condition (multifactorial, mitochondrial, and single-gene) as well as the specific subtype (Leigh syndrome, Mitochondrial myopathy, Cystic fibrosis, Tay-Sachs, Diabetes, Hemochromatosis, Leber's hereditary optic neuropathy, Alzheimer's disease, and cancer). The input features are derived from readily obtainable parameters like family history, newborn metrics, and basic lab tests. This confers the additional advantage of easy adaptability. Extensive tuning of hyperparameters, feature engineering, and selection are undertaken to optimize model performance.\nThe key contributions of this work are:\n1.  Demonstrating the viability of ML techniques for early delineation across the scope of genetic disorders rather than isolated conditions;\n2.  Designing predictive models based solely on elementary clinical variables that can be measured at birth or infancy.\n\nThe proposed methodology can permit timely interventions and equip families to confront challenges ahead. We envision this pioneering effort to spur more research into data-driven approaches for expediting genetic disorder diagnosis. In the subsequent sections, we describe the dataset, ML architectures, training procedures, evaluation metrics, and results obtained from our experiments."}, {"title": "2. Related Works", "content": "In recent years, machine learning (ML) techniques have shown immense potential for advancing genetic disorder diagnosis and prognosis. ML models are well-suited for discerning complex multivariate relationships from high-dimensional data. For instance, Ghazal et al. [7] presents a machine learning approach using SVM and KNN classifiers to predict three diseases - dementia, cancer, and diabetes - from genetic and clinical data. The SVM model achieved a higher accuracy of 92.8% on training data and 92.5% on testing data, compared to KNN which got 92.8% and 91.2%. Various statistical measures like sensitivity, specificity, F1-score, etc. were also analyzed. The key contributions are using genetic and clinical data for multiclass disease prediction, testing two standard machine learning models, and achieving state-of-the-art accuracy. In another study Nasir et al. [16] proposed a machine-learning approach for the prediction of single gene inheritance disorders (SGID) and mitochondrial gene inheritance disorders (MGID) using patient medical history data. The motivation for both of these papers is that early prediction of these genetic diseases can help improve prognosis and health outcomes and demonstrate the utility of computational intelligence for the early detection of fatal hereditary disorders. Limitations for both of these papers are the lack of model optimization and testing on more genetic markers.\nResearchers examined machine learning techniques for predicting psychiatric diseases based on genetic information in [5]. Based on an analysis of multiple studies, the authors have arrived at the conclusion that the effectiveness of diverse machine learning algorithms is subject to variability and that their ultimate performance remains uncertain. Furthermore, it has been found that support vector machines and neural networks are the most commonly utilized machine learning algorithms in such investigations. This study concentrated on the capacity of machine learning techniques to accurately forecast psychiatric disorders solely based on genetic data. Furthermore, they did not emphasize early predictions.\nThe co-inheritance of DNA variants at two distinct genetic loci has been studied in the context of certain uncommon genetic disorders, such as retinitis pigmentosa and Alport syndrome in [17]. The authors provide an overview of statistical and machine learning methods for digenic inheritance. Digenic inheritance goes beyond standard Mendelian inheritance where a single genetic variant determines disease status. This study highlights the promise of machine learning to uncover digenic inheritance and gene-gene interactions underlying human disease. However, work is still needed to maximize analytical power while minimizing false discoveries. Mukherjee et al. [18] employed a supervised machine-learning technique and a random forest classifier to identify gene pairings that have the potential to cause digenic disorders. The study compared the functional network and evolutionary features of known digenic gene pairs with real sets of non-digenic gene pairs, including variant pairs from healthy individuals. The aim was to identify gene pairs that could lead to the development of digenic diseases. The findings of the study suggest that the identified gene pairings have the potential to contribute to the development of digenic disorders.\nIn a study Rahman et al. [19] proposed a machine-learning approach to identify newborns at risk for autism spectrum disorder (ASD) using electronic medical records (EMRs). The authors developed and validated a predictive model based on demographic, clinical, and laboratory features extracted from EMRs of over 200,000 newborns. The model achieved an AUC of 0.81 in the validation cohort and identified several risk factors for ASD, such as male sex, low birth weight, and maternal infections.\nHepatocellular carcinoma (HCC) is the sixth most common cancer in the world. Early diagnosis of HCC is crucial for improving treatment outcomes and reducing the mortality rate. Plawiak et al. [20] propose a novel machine learning approach for early detection of HCC patients based on gene expression data. The authors aim to overcome limitations such as high dimensionality, overfitting, noise, and heterogeneity by using a hybrid machine learning approach that combines feature selection, dimensionality reduction, and classification techniques. The authors use gene expression data from 139 HCC patients and 50 healthy controls obtained from the Gene Expression Omnibus (GEO) database. The authors first apply a filter-based feature selection method to select the most relevant genes for HCC prediction. Then, they use a linear discriminant analysis (LDA) method to reduce the dimensionality of the gene expression data and extract the most discriminative features. Finally, they use an SVM method to classify the samples into HCC or non-HCC groups. The authors also use a genetic algorithm to optimize the parameters of the SVM classifier. One of the key limitations of this approach is the use of only gene expression data which may not capture all the biological variations and interactions involved in HCC development and progression.\nIqbal et al. [8] provides a review of the clinical applications and future potential of artificial intelligence (AI) and machine learning in cancer diagnosis and treatment. The authors discuss how AI can be used to analyze large datasets to identify patterns and biomarkers to enable early cancer detection, precision diagnosis, and personalized treatment.\nWhile prior studies have made promising advances, some key limitations remain. Most works have focused on predicting specific diseases like cancer, diabetes, and Alzheimer's in isolation rather than categorizing genetic disorders more broadly. The predictive models also tend to rely on clinical, imaging, or molecular data that manifest in later disease stages rather than at birth or infancy. Furthermore, robust validation on large datasets and model optimization is often lacking. Finally, the application of machine learning for expediting early diagnosis across the spectrum of genetic disorders remains relatively unexplored. To address these gaps, this study implements a range of supervised learning models using basic clinical indicators measurable at birth or infancy to categorize genetic disorders at preliminary life stages."}, {"title": "3. Dataset", "content": "To effectively train and evaluate machine learning models, it is essential to use a dataset that captures meaningful patterns related to genetic disorders. The following subsections provide detailed information on the dataset used in this study, along with the feature engineering and selection techniques applied to optimize model performance."}, {"title": "3.1. Dataset Description", "content": "The dataset employed in this study was obtained from Kaggle. The source dataset was a comma-separated file with the majority of columns being categorical and initially consisted of 22083 rows, 42 dependent features, and 2 independent features (genetic disorder and disorder subclass). Table 2 displays the principal independent features. \"Inherited from father\" indicates a gene flaw in the patient's father, while \"Genes on the mother's side\" indicates a gene deficit in the patient's mother. The \"Maternal Gene\" refers to a genetic defect that originates from the patient's mother, whereas the \"Paternal Gene\" refers to a genetic fault that originates from the patient's father. The patient's respiration rate is recorded in the \"Respiratory Rate (breaths/min)\" column, while the heart rate is recorded in the \"Heart Rate (rates/min)\" column. The \"H/O radiation exposure\" feature indicates whether or not the patient's parents have a history of radiation exposure, while the \"H/O substance abuse\" feature indicates whether or not the patient's parents have a history of drug addiction. There are more characteristics such as \"History of abnormalities in previous pregnancies\" \"Number of prior abortions\", \"Count of White Blood Cells,\" etc. \"Mitochondrial genetic inheritance disorders,\" \"Multifactorial genetic inheritance disorders,\" and \"Single-gene inheritance diseases\" are the three categories in the \"Genetic Disorder\" column, which is one of the two dependent features. \"Leigh syndrome,\" \"Mitochondrial myopathy,\" \"Cystic fibrosis,\" \"Tay-Sachs,\" \"Diabetes,\" \"Hemochromatosis,\" \"Leber's hereditary optic neuropathy,\" \"Alzheimer's,\" and \"Cancer\" are the nine classes featured in the \"Genetic Subclass\" column."}, {"title": "3.2. Data Processing", "content": "Effective data processing is crucial in preparing the dataset for machine learning models. This process involves cleaning, transforming, and structuring the raw data to enhance the quality and relevance of the features used for classification. By carefully processing the data, we ensure that the models can make accurate and reliable predictions. The following subsections outline the feature engineering and selection techniques applied to improve the model's performance."}, {"title": "3.2.1. Feature Engineering", "content": "Various new features were derived from the original dataset to better capture relevant information. The motivation behind constructing these engineered variables was to amplify pertinent signals related to genetic disorders and handle data sparsity issues. In total, five new engineered features were constructed using techniques like binning, arithmetic operations, and logical rules. All features were motivated by domain insights and intended to better expose predictive signals. The utility of these constructed variables was analyzed in the feature selection stage.\nMaternal Age Above 40: A study [21] suggests that a mother's age may increase the risk of autism in her newborn. This binary variable was derived using the thresholding approach.\nLet $X_{ma}$ and $X'_{ma}$ be variables for \"Maternal Age\" and \"Maternal Age Above 40\". $X'_{ma} = \\begin{cases}\n1 & \\text{if age } > 40\\\\\n0 & \\text{if age } < 40\n\\end{cases}$\nNumber of Symptoms: This feature was generated by summing the presence of the multiple symptom variables. It aims to quantify the overall symptomatic level of the patient.\nAny Inherited Gene: This binary variable checks if any of the two gene inheritance indicators are positive using an OR logical operation. It combines the maternal and paternal inheritance patterns.\nHigh WBC Count: Thresholding was utilized to create this feature to flag abnormally high white blood cell counts based on standard clinical ranges.\nHeart or Respiratory Issues: This variable merges two key physiological parameters - heart rate and respiratory rate - using an OR operation to identify any cardiac or respiratory irregularities."}, {"title": "3.2.2. Feature Selection", "content": "Feature selection refers to the procedure of selecting a subset of features from an original set of features, guided by specific criteria for feature selection. This identifies the essential characteristics of a dataset. It aids in reducing the amount of data that must be processed by eliminating unnecessary features. Good feature selection results can increase the accuracy of learning, reduce the time required to learn, and make learning results easier to comprehend [22]. We incorporated the chi2 feature selection method. It determines the level of similarity of variances between two distributions. The test assumes that the given distributions are independent in its null hypothesis. The mathematical equation for the Chi-Square test is given by:\n$\\chi^2 = \\sum_{i=1}^{m} \\sum_{j=1}^{k} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}$\nHere, m is the number of attribute values for the feature in question. k is the number of class labels for the output. $O_{ij}$ is the observed frequency and $E_{ij}$ is the expected frequency. For each feature, a contingency table is created with m rows and k columns. Each cell (i, j) denotes the number of rows having attribute feature as i and class label as k. Thus each cell in this table denotes the observed frequency. The expected frequency for each cell is calculated by first determining the proportion of the feature value in the total dataset and then multiplying it by the total number of the current class label. The higher the value of $\\chi^2$, the more dependent the output label is on the feature and the higher the importance the feature has on determining the output."}, {"title": "4. Algorithms", "content": "Various machine learning algorithms have been explored to address the complex nature of genetic disorder classification. Each algorithm brings its unique strengths and challenges when applied to medical datasets. In the following subsections, we discuss the supervised learning algorithms implemented in this study and how they contribute to the classification tasks."}, {"title": "4.1. Logistic Regression", "content": "Logistic regression is a commonly employed classification method in the field of machine learning. Logistic regression is characterized by a binary outcome variable, whereas linear regression is characterized by a continuous outcome variable. This is the major distinction between the two types of regression. Because of its tremendous flexibility and understandable interpretation, logistic regression was preferred over alternative distribution functions [23]. A logistic regression model uses different characteristics to figure out how likely an outcome is [24]. The logit function is a fundamental mathematical concept that serves as the basis for logistic regression analysis [25]. The form of the simple logistic model is\n$logit(Y) = ln(\\frac{\\pi}{1-\\pi}) = \\alpha + \\beta X$\nThe probability of the outcome of interest can be predicted by substituting the antilog of Equation 1 as follows:\n$\\pi(x) = Probability(Y = \\text{outcome of interest } | X = x, \\text{ a specific value of } X) = \\frac{e^{\\alpha + \\beta x}}{1 + e^{\\alpha + \\beta x}}$\nwhere $\\beta$ represents the regression coefficient, $\\pi$ represents the probability of the result of interest, and X represents the predictor. This simple logistic regression is extended to multiple predictors (2 predictors) as follows:\n$logit(Y) = ln(\\frac{\\pi}{1-\\pi}) = \\alpha + \\beta_1 X_1 + \\beta_2 X_2$\nTherefore,\n$\\pi(x) = Probability(Y = \\text{outcome of interest } | X_1 = x_1, X_2 = x_2) = \\frac{e^{\\alpha + \\beta_1 x_1 + \\beta_2 x_2}}{1 + e^{\\alpha + \\beta_1 x_1 + \\beta_2 x_2}}$\nhere, the regression coefficients are denoted by $\\beta_s$, the probability of the result of interest by $\\pi$, the Y intercept by $\\alpha$ and $X_s$ represents the predictors."}, {"title": "4.2. SVM", "content": "The Support Vector Machine (SVM) is a widely used machine learning algorithm that can be applied for both classification and regression purposes. The approach is founded upon the concept of Structural Risk Minimization (SRM), thereby endowing it with greater generality. SRM is accomplished by performing an optimization that reduces the maximum value of the generalization error [26]. If the training data are linearly separable, we can choose the two margin hyperplanes so that there are no points in between them, and then maximize their distance. Using geometry, we calculate the distance between these two hyperplanes as 2/||w||. Given a set of training data D, n points of the form\n$D = \\{(x_j, y_j) | x_j \\in \\mathbb{R}^m, y_j \\in \\{-1,1\\}\\}_{j=1}^{n}$\nwhere $x_j$ is an m-dimensional real vector, $y_j$ is either -1 or 1 indicate the class to which point $x_j$ belongs. The minimization of error can be expressed as the quadratic optimization problem that is represented as,\n$\\text{Minimize: } P(w, b, \\xi) = \\frac{1}{2} ||w||^2 + C \\sum_{j=1}^{m} \\xi_j$\n$\\text{Subject to }: y_j(\\langle w, \\phi(x) \\rangle + b) \\geq 1 - \\xi_j$\nwhere $\\xi_j \\geq 0$ for all integers j between 1 and m, $\\xi_j$ are slack variables, and C (cost of slack) is a constant. C is a trade-off parameter that determines the optimal margin and training error. The decision function of SVMs can be expressed as $f(x) = w \\phi(x) + b$, where the parameters w and b are obtained by solving the optimization problem P as stated in the preceding expression. The optimization problem R can be expressed using Lagrange multipliers as\n$\\text{Minimize : } F(\\beta) = \\frac{1}{2} \\beta^T Q \\beta - \\beta^T 1$\n$\\text{Subject to: } 0 \\leq \\beta \\leq C$\n$y^T \\beta = 0$\nThe notation $[Q]_{ij} = y_i y_j \\phi(x_i) \\phi(x_j)$ represents the Lagrangian multiplier factor. While familiarity with $\\phi$ is not mandatory, proficiency in calculating the modified inner product, denoted as the kernel function $K(x_i, x_j) = \\phi (x_i) \\phi(x_j)$, is imperative. As a result, it follows that the expression for $[Q]_{ij}$ is given by $y_i y_j K(x_i, x_j)$. According to Mercers' theorem, the optimization problem denoted as P can be classified as a convex quadratic programming (QP) problem that features linear constraints. If the kernel K is positive definite, then the problem can be solved within polynomial time."}, {"title": "4.3. Random Forest", "content": "The random forest algorithm is a machine learning technique that comprises a set of predictors. Each tree in the forest predicts its own behavior by considering the values of a random vector that is independently obtained but has the same distribution across all trees in the forest [11]. This technique of using a series of predictors to perform one task is known as an ensemble predictor. The ensemble technique used by Random Forest allows it to make more accurate predictions as well as better generalizations [27]. It is comprised of a collection of tree-structured classifiers denoted by $\\{h(U, \\Theta_k), k = 1,2,3,4, ...\\}$where $\\{\\Theta_k\\}$ represents independent identically distributed random vectors. Each tree contributes a single vote towards the most commonly occurring class for the given input u. The present study considers a set of classification methods denoted as $h_1(u), h_2(u), ..., h_k(u)$, whereby the training set is randomly sampled from the distribution of the random vector V, U. The margin function, denoted as $mg(U, V)$, is defined as Equation 7.\n$mg(U, V) = avkI(h_k(U) = V) - \\max_{j \\neq v} avkI(h_k(U) = j)$\nAs the number of trees grows, $PE^*$ gets closer based on the Equations 8 and 9.\n$P(U, V)(PO(h(U, \\Theta) = V) - \\max_{j \\neq v} P_{\\Theta}(h(U, \\Theta) = j) < 0$\n$PE^* = P(U, V)(mg(U, V) < 0)$\n$mr(U, V) = P(h(U, \\Theta) = V) - \\max_{j \\neq v} P_{\\Theta}(h(U, \\Theta) = j)$\nEquation 10 represents the margin function of the random forest. Equation 11 represents the strength of the set of classifiers h(U, $\\Theta$):\n$s = E_{U,V}mr(U, V)$\nA more enlightening expression for the variance of mr can be represented as,\n$j(U, V) = \\text{argmax}_{j \\neq v} P_{\\Theta}(h(U, \\Theta) = j)$\nThus, Equation 13 illustrates the random forest margin function as:\n$mr(U, V) = P(h(U, \\Theta) = V) - P_{\\Theta}(h(U, \\Theta) = \\hat{j}(U, V)) = E_{\\Theta}[I(h(U, \\Theta) = V) - I(h(U, \\Theta) = \\hat{j}(U, V))]$\nand the raw margin can be expressed as:\n$rmg(\\Theta,U, V) = I(h(U, \\Theta) = V) - I(h(U, \\Theta) = \\hat{j}(U, V))$\nThe maximum value for the generalization error of the random forest algorithm can be calculated as:\n$PE^* < \\frac{\\sqrt{p(1 - s^2)}}{s^2}$\nIt has been demonstrated to be highly useful as a method of classification and regression for a variety of applications [28]. In feature importance measurement, radio frequency (RF) is a widely used methodology. The random forest model provides significant advantages in terms of feature selection owing to its efficient training time, superior accuracy, and lack of requirement for intricate parameter tuning [29]. The random forest has several advantages over typical decision tree methods, including the fact that mature trees are not removed [30]."}, {"title": "4.4. Catboost", "content": "The CatBoost algorithm is an algorithm for machine learning that employs a decision tree model based on the boosting gradient methodology. Gradient boosting refers to the technique of creating a predictor ensemble in multiple dimensions through the use of gradient descent [31]. A series of decision trees is built one after the other during training. Each subsequent tree is constructed with less loss than its predecessor. Consider a dataset consisting of instances denoted by $Z = \\{(x_t, y_t)\\}_{t=1...n}$ where $x_t = x_1...x_n$, where $x_i$ is a random vector of m features and $y_t \\in \\mathbb{R}$ is a target variable that can take on binary or numeric values. The objective of learning tasks is to obtain a function $f : \\mathbb{R}^m \\rightarrow \\mathbb{R}$ that minimizes the anticipated loss. The function $L(f)$ is defined as the expected value of the loss function $L(y, f (x))$. The smooth loss function is denoted by $L(..)$, and the test example (x, y) is sampled from P, excluding the Z training set. The gradient boosting technique constructs a series of successive approximations $f^r : \\mathbb{R}^m \\rightarrow \\mathbb{R}, r = 0,1,2, ...$ in a greedy manner through iterative processes. The current estimate $f^r$ is obtained through an additive derivation process from the previous estimate $f^{r-1}$, as expressed by the equation $f^r = f^{r-1} + \\alpha h^r$. Here, $\\alpha$ denotes the step size, and the function $h^r : \\mathbb{R}^m \\rightarrow \\mathbb{R}$, which serves as a base predictor, is selected from a set of functions. The objective is to minimize the expected loss of the variable H.\n$h^r = \\text{arg min}_{h \\in \\mathbb{H}} L(f^{r-1} + h) = \\text{arg min}_{h \\in \\mathbb{H}} E L(y, f^{r-1}(x) + h(x))$\nThe Newton method is commonly employed for the purpose of resolving the minimization problem. This involves utilizing a second-order approximation of $L(f^{r-1} + h^r)$ at $f^{r-1}$ or a negative gradient step. Both of the mentioned methods are different versions of operational gradient descent. The selection of the gradient step $h^r$ is based on the proximity between $h'(x)$ and $g'(x, y)$, where $g'(x, y) := \\frac{\\partial L(y, f^{r-1}(x))}{\\partial s}|_{s=f^{r-1}(x)}$. A frequent approximation technique is the least-squares method:\n$h' = \\text{arg min}_{h \\in \\mathbb{H}} E(-g'(x, y) - h(x))^2$\nThis algorithm is capable of handling categorical features effectively. When choosing the tree structure, it employs a new method for computing leaf values and it reduces overfitting[32]."}, {"title": "4.5. Gradient Boosting", "content": "Gradient boosting is a machine learning algorithm that has numerous applications, including multiclass classification. It is one of the best ways to build predictive models.\nThe aim of gradient boosting is to derive an estimate, denoted as $f(x)$, of the function $f^*(x)$ that maps instances x to their corresponding target value y, based on a training dataset $Z = \\{x_i, y_i\\}_{i=1}^{N}$. This is achieved by minimizing the expected value of a specified loss function, $L(y, f(x))$. The gradient boosting algorithm generates a summation of $f^*(x)$ estimation, which is then multiplied by a weighted combination of functions\n$f_n(x) = f_{n-1}(x) + \\rho_n h_n(x)$\nwhere $\\rho_n$ is the weight of the nth function, $h_n(x)$. These functions represent the ensemble's models (e.g. decision trees). The approximation is constructed in a recursive manner. Initially, a constant estimate of $f^*(x)$ is acquired as\n$f_0(x) = \\text{arg min}_{\\alpha} \\sum_{i=1}^{N} L(y_i, \\alpha)$\nNext models are anticipated to minimize\n$(\\rho_n, h(x)) = \\text{arg min}_{\\rho, h} \\sum_{i=1}^{N} L(y_i, f_{n-1}(x) + \\rho h(x))$\nInstead of directly dealing with the optimization problem, it is possible to view each $h_n$ as a greedy iteration within a gradient descent optimization for $f^*$. In this approach, every model $h_n$ undergoes training on a new dataset $Z = \\{x_i, r_{ni}\\}_{i=1}^{N}$, where the pseudo-residuals, $r_{ni}$, are computed by\n$r_{ni} = - [\\frac{\\partial L(y_i, F(x))}{\\partial f(x)}]_{f(x)=f_{n-1}(x)}$\nThe determination of the value of $\\rho_n$ is achieved through the resolution of an optimization problem involving line search. If the iterative procedure is not sufficiently regularized, there is a risk of overfitting with this approach [33]. Gradient boost has the disadvantage of being substantially more time-consuming and inefficient when the data dimension is quite large. This is because they have to look at every piece of data to figure out how much information can be gained from each possible split point."}, {"title": "4.6. LightGBM", "content": "Every day, the world is becoming more and more data-driven. As the dimension of data grows larger, the gradient boost techniques become more time-consuming. LightGBM was formulated to overcome this issue. LightGBM is a decision tree algorithm that integrates Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) with Gradient Boosting Decision Tree (GBDT). Given the supervised training set $X = \\{(x_i, y_i)\\}_{i=1}^{n}$ consists of n samples, where each sample x is associated with a class label y. The estimated function is denoted by F(x), and the aim of GBDT optimization is to minimize the loss function L(y, F(x)):\n$\\hat{f} = \\text{arg min}_{f} E_{x,y} L(y, F(x))$\nThen, the determination of the iterative criterion of the Gradient Boosting Decision Tree (GBDT) can be achieved through a line search approach aimed at minimizing the loss function as,\n$F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$\nwhere $\\gamma_m = \\text{arg min}_{\\gamma} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))$, m is the number of iteration, $h_m(x)$ denotes the base decision tree. To separate each node in GBDT, the information gain is commonly used. GOSS is used by LightGBM to calculate variance gain and estimate the split point. Initially, the magnitudes of the gradients pertaining to the training examples are arranged in a descending order. Subsequently, the uppermost a \u00d7 100% data samples, which are denoted as A, are selected based on their gradient values. Subsequently, a stochastic subset denoted as B with cardinality b \u00d7 |A| is chosen at random from the residual samples $A^c$. The instances are then subdivided based on the estimated variance $V_j(d)$ on $A \\cup B$ as,\n$V_j(d) = \\frac{1}{n} (\\sum_{x_i \\in A_l} g_i + \\frac{1}{b} \\sum_{x_i \\in B_l} g_i)^2 \\frac{1}{n^l(d)} + (\\sum_{x_i \\in A_r} g_i + \\frac{1}{b} \\sum_{x_i \\in B_r} g_i)^2 \\frac{1}{n^r(d)}$\nwhere $A_l = \\{x \\in A : x_{ij} \\leq d\\}, A_r = \\{x \\in A : x_{ij} > d\\}, B_l = \\{x \\in B : x_{ij} \\leq d\\}, B_r = \\{x \\in B : x_{ij} > d\\}, g_i$ denotes the loss function's negative gradient and $\\frac{g_i}{b}$ is used to standardize the summation of gradients. The LightGBM algorithm has the potential to expedite the training process by a factor of 20, while maintaining a comparable level of precision [14]."}, {"title": "5. Evaluation", "content": "Accuracy, precision, recall, f1-score, and confusion matrix have been utilized to evaluate the performance of the model. Before diving into our evaluation system, it's important to understand four different terms.\n1.  True positives (TP): The outcome entails the model's precise prediction of the positive class.\n2.  True negatives (TN): The outcome pertains to a scenario where the model has successfully made precise predictions for the negative class.\n3.  False positives (FP): It is an outcome where a condition exists when it actually doesn't. It is also known as type-I error.\n4.  False negatives (FN): It is a scenario where the model predicts that something is false when in reality it is true. It is the most catastrophic sort of error, commonly known as a type-II error.\nEquations 25, 26, 27, and 28 present the mathematical expressions for Accuracy, Precision, Recall, and F-1 score, respectively.\n$Accuracy = \\frac{TP+TN}{TP+TN+FP + FN}$\n$Precision = \\frac{TP}{TP+FP}$\n$Recall = \\frac{TP}{TP+FN}$\n$F1 score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$"}, {"title": "6. Result and Discussion", "content": "The performance of the machine learning models is evaluated using accuracy, precision, recall, and F1-score. The results reveal significant insights into the capabilities and limitations of each model. Table 3 summarizes the overall accuracy of the five implemented classifiers on the genetic disorder and disorder subclass prediction tasks. For categorizing samples into one of the three genetic disorder classes, the CatBoost model achieved the highest accuracy of 77% on the test set. Support Vector Machine (SVM) also performed well, attaining an accuracy of 76%. The remaining models had accuracy scores in the 72-75% range. In the subclass prediction task, SVM emerged as the top performer with 80% accuracy, slightly outperforming CatBoost at 79%. The other classifiers had accuracy between 71-73% for discriminating the 9 different subclasses. The results indicate SVM and CatBoost are the overall best-performing models across both tasks.\nThe per-"}]}