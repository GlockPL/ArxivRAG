{"title": "Beyond Scalars: Concept-Based Alignment Analysis in Vision Transformers", "authors": ["Johanna Vielhaben", "Dilyara Bareeva", "Jim Berend", "Wojciech Samek", "Nils Strodthoff"], "abstract": "Vision transformers (ViTs) [10] can be trained using various learning paradigms, from fully supervised to self-supervised. Diverse training protocols often result in significantly different feature spaces, which are usually compared through alignment analysis. However, current alignment measures quantify this relationship in terms of a single scalar value, obscuring the distinctions between common and unique features in pairs of representations that share the same scalar alignment. We address this limitation by combining alignment analysis with concept discovery, which enables a breakdown of alignment into single concepts encoded in feature space. This fine-grained comparison reveals both universal and unique concepts across different representations, as well as the internal structure of concepts within each of them. Our methodological contributions address two key prerequisites for concept-based alignment: 1) For a description of the representation in terms of concepts that faithfully capture the geometry of the feature space, we define concepts as the most general structure they can possibly form - arbitrary manifolds, allowing hidden features to be described by their proximity to these manifolds. 2) To measure distances between concept proximity scores of two representations, we use a generalized Rand index and partition it for alignment between pairs of concepts. We confirm the superiority of our novel concept definition for alignment analysis over existing linear baselines in a sanity check. The concept-based alignment analysis of representations from four different ViTs reveals that increased supervision correlates with a reduction in the semantic structure of learned representations.", "sections": [{"title": "1. Introduction", "content": "Vision Transformers are gaining increased popularity as backbones for various computer vision tasks. There is a large zoo of pre-trained models trained with various learning paradigms and a range of supervision strengths. To guide practitioners, previous work has evaluated performance on various common downstream tasks [18]. A complimentary view of comparisons within and between models beyond quantitative accuracy is achieved by analyzing patterns in hidden activations and measuring representational alignment between them [34, 40].\nWhen choosing a model for a downstream task, we want to understand how the model solves its pre-training task. Where does the model representation change the most and how? Which concepts, i.e. dominant structures in representation space, are encoded in lower layers vs. upper layers? Where does the model representation change the most and how? How structured are the representations? Does the model encode semantically similar concepts in spatial proximity to each other? How is the representation of model A different from that of model B across layers? Answering these questions can aid the selection of pre-trained models and the design of fine-tuning strategies through detailed insights into robustness and generalization capabilities. Previous work on alignment, however, only provides a single scalar value to measure alignment between representations at two different layers [38], leaving the questions above largely unanswered. In this paper, we propose a more fine-grained alignment analysis based on concepts that structure the latent representation. To this end, we represent the original activations by concept membership scores that quantify proximity to the discovered concepts. Then, we measure alignment between concept proximity scores of representations and can therefore partition it into the concepts. This gives insights into universal and specific concepts between representations of different layers or models, as well as how a single representation is structured.\nTo achieve concept-based alignment we need solutions for 1) concept discovery, and 2) measuring the alignment between concept proximity scores.\nPrevious work on concept discovery ranges from merely identifying neurons or other pre-existing units as concepts [1] to linear directions in feature space [13]. The most general definition so far relies on concepts as multi-dimensional linear subspaces [39]. The common strong assumption among these is the linearity of concept structures, which is challenging to verify and controversial [2, 7]. For concepts that faithfully represent the underlying geometry of the representation, we avoid the linearity assumption and consider concepts as the most general structure they can form, namely as nonlinear manifolds. So far, alignment between representations has been measured as the similarity of similarities, e.g. through linear or kernel-based Centred Kernel Alignment (CKA) [25], which results in a single scalar value. Our fine-grained concept-based alignment measure requires a distance measure between concept proximity scores. Here, we choose a generalized Rand index between soft clusterings with pseudo metric properties [23] that we partition into pairwise concept distances.\nTo summarize, our key idea is the following:\n\u2022 We combine concept discovery with alignment analysis to provide insights into which concepts are universal or specific between two representations, and how structured a single representation is.\nWe make the following methodological contributions to realize concept-based alignment:\n\u2022 We propose a novel concept definition of concepts as non-linear manifolds to faithfully capture the geometry of the feature space with concept proximity scores.\n\u2022 We leverage a generalized Rand index with pseudo-metric properties to measure the alignment between concept proximity scores of two representations and partition it for fine-grained concept alignment."}, {"title": "2. Concept Discovery for Representational Alignment", "content": "This section is partitioned into three parts: First, we introduce our novel concept definition based on the manifold hypothesis. Then, we describe our methodology for discovering these concepts in latent activations, shown in Fig. 1. Finally, we describe how our concept-based description of hidden representations can be used to measure alignment between representations, identify commonalities and uniqueness between models, and investigate information flow within one model."}, {"title": "2.1. Concept definition", "content": "Motivation According to the manifold hypothesis, which is widely accepted in machine learning, many datasets, including image data that nominally lie in high dimensional space, can be described in terms of a few underlying latent factors and are thus concentrated on a (potentially disconnected) low-dimensional manifold embedded in high-dimensional space [19]. [31] shows how a neural network trained on a toy classification problem solves the task by transforming the topology of the input data, and layerwise reducing the Betti numbers of the class-wise components. We hypothesize that state-of-the-art vision models behave similarly and try to recover the connected components in the hidden representations, which we call concepts.\nDefinition We analyze the hidden representation at an intermediate feature layer of a neural network. To this end, we split the model \\(f\\) into two parts, \\(f = g \\circ h\\), where \\(h_l\\) is the mapping to a hidden feature layer \\(l\\). Our definition then relies on hidden representations \\(h_l(x_i) \\in \\mathbb{R}^{N'\\times F}\\) of input samples \\(x_i\\) from a set \\(S\\). \\(N'\\) is the number of spatially separable elements in the representation, i.e. the number of tokens in a transformer model or the number of superpixels in a convolutional feature map. We spatially decompose the feature maps \\(h(x_i)\\) into a set of \\(N = N'\\cdot |S|\\) feature vectors \\(\\Phi \\in \\mathbb{R}^F\\). Previously, concepts have been mostly defined as linear structures [13, 42]. The most general linear structure would be affine subspaces, which would already represent an extension compared to the recently considered definition as linear subspaces [39]. In this work, we generalize this idea even one step further and define concepts as manifolds in the \\(F\\)-dimensional feature space.\nDefinition 1 We define a concept \\(C_\\alpha\\), as a manifold in the \\(F\\)-dimensional feature space, represented by a point cloud \\(\\{\\phi_i\\}\\) consisting of the feature vectors \\(\\phi_i\\) that lie on the concept manifold with index \\(\\alpha\\).\nBenefits of concept manifold definition In the following, we want to compute concept proximity scores by which we measure alignment. Incorrect assumptions about the structure of the concept manifold, e.g., assuming it has no curvature (affine subspaces) or it is spherical and the distance to the manifold can be estimated by the distance to the centroid, directly lead to distorted concept proximity scores and hence to distorted alignment. Later, in a sanity check our definition performs best for measuring representational alignment."}, {"title": "2.2. Concept discovery", "content": "Clustering Having established our definition of concepts as manifolds in feature space, we now turn to the challenge of discovering these concepts through clustering. As stated above, we assume that feature vectors \\(\\{\\phi_i\\}\\) from a hidden representation are sampled from a set of low-dimensional concept manifolds \\(\\{C_\\alpha\\}\\). Recovering these concept manifolds in high-dimensional space (\\(F = 768\\) in our experiments) is a challenging clustering problem. Therefore, we revert to density-based clustering on a low-dimensional embedding of the data [17, 21]. For this embedding, we utilize UMAP (Uniform Manifold Approximation and Projection) [27], a dimensionality reduction technique that preserves local and some global structure. Given that we have no a priori knowledge about the number of clusters, we employ HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise), which can handle clusters of varying densities [4]. HDBSCAN builds a hierarchy of clusters based on density, represented by a condensed tree, and allows for robust handling of noise, making it suitable for the possibly intricate structure of feature representation spaces. While UMAP does not fully preserve density, its ability to maintain the overall structure of the data makes it a valuable preprocessing step before applying HDBSCAN. We use the HDBSCAN implementation from [28].\nConcept proximity scores We leverage soft clustering with HDBSCAN based on the condensed tree which is roughly a density function over the data points to compute fuzzy cluster membership as described in [28], which we formalize in the appendix for the reader's convenience. It is based on the distance to concept anchor points a cluster and an outlier score, both derived from the condensed tree. We now have a fuzzy clustering \\(P\\{\\phi_i\\} = \\{P(\\phi_1),\\dots,P(\\phi_N)\\}\\), where \\(P_{\\alpha}(\\phi) \\in [0, 1]^n\\) holds the concept proximity scores of each concept \\(C_\\alpha\\). We interpret the concept proximity scores \\(P_{\\alpha}(\\phi_i)\\) as the probability that a feature vector \\(\\phi_i\\) belongs to a concept \\(P_{\\alpha}\\) in clustering \\(P\\). This approach contrasts with previous concept assignment paradigms [13, 39], which often rely on hard clustering, where each feature vector is assigned to a single concept, or linear methods that project onto specific concept directions, limiting the representation to a more rigid framework. In contrast, our soft clustering method allows for nuanced membership scores that reflect the degree of belonging to multiple concepts. In the following, we refer to our concept discovery method as NLMCD (non-linear multi-dimensional concept discovery)."}, {"title": "2.3. Concept-based Representational Alignment", "content": "We now address the question of measuring representational alignment based on the concept proximity scores derived from fuzzy clustering.\nPseudo-metric between fuzzy clusterings The concepts are at this point characterized by a probabilistic clustering \\(P\\{\\phi_i\\} = \\{P(\\phi_1),\\dots,P(\\phi_n)\\}\\), where \\(P(\\phi_i) = \\{P_1(\\phi_i),\\dots,P_r(\\phi_i)\\}\\). We want to measure the similarity between two probabilistic clusterings \\(P, Q\\) from two different representations to evaluate how aligned their concepts are. For this purpose, we leverage an extension of the pair-based Rand index generalized to fuzzy clusterings proposed in [23]. The original Rand index counts the number of concordant pairs (either two points are paired or not paired both clusterings) and disconcordant pairs (two points are paired in one clustering but not in the other). The distance between probabilistic clustering \\(P, Q\\) is based on a generalized degree of concordance that is based on the distance between two membership vectors \\(d_{ms}(P(\\phi_i), P(\\phi_j))\\):\n\n\\[ d_{cross}(P, Q) = \\frac{2}{N(N - 1)} \\sum_{i,j} | d_{ms}(P(\\phi_i), P(\\phi_j)) - d_{ms}(Q(\\phi_i), Q(\\phi_j)) | \\] \n\nA commonly used choice for the distance \\(d_{ms}\\) is \\(d_{ms}(P(\\phi_i), P(\\phi_j)) = 1 - ||P(\\phi_i) - P(\\phi_j)||_1\\) [8]. Finally, we refer to the similarity between two clusterings, derived from the uncovered concepts, as Concept-Based Alignment (CBA):\n\n\\[ CBA = 1 - d_{cross}(P, Q) \\]\n\nWe choose this measure because \\(d_{cross}(P, Q)\\) is a pseudo-metric satisfying desirable properties that ease interpretation Also, when P, Q are crisp partitions, CBA reduces to the original Rand index."}, {"title": "3. Related work", "content": "Concept discovery Most existing methods model concepts as linear directions [13, 14, 16, 42]. Generalizing this definition, [39] suggest that concepts can be represented more faithfully as multidimensional linear subspaces, which they discover through sparse subspace clustering. While above methods operate unsupervised without concept labels, [6] employ kernel classifier for supervised, nonlinear concept discovery, showing improvement over linear concepts. In the field of mechanistic interpretability, many studies aim to enumerate all features encoded in the representations of neural networks [3]. This line of work focuses mainly on language models, often identifying linear features using sparse autoencoders [15, 22, 26]. However, [11] find evidence for the existence of multi-dimensional non-linear features. Unlike these approaches, our main goal in concept discovery is representation summarization for alignment measurement, rather than interpretability or feature enumeration. For this reason, we employ the most general, non-linear concept definition.\nAlignment Representational alignment measures are categorized, with a particular emphasis on Centered Kernel Alignment (CKA) in [25]. CKA evaluates the similarity of similarities, either linearly or under a non-linear kernel. Similarly, [9] measure alignment through the similarities of binary k-nearest neighbor adjacency matrices, which resembles CKA with a narrow Gaussian kernel. Our method relates to CKA in that it condenses these similarities into clusters and subsequently measures the similarity between these clusterings."}, {"title": "4. Results", "content": "We evaluate concept discovery in Sec. 4.1, check the superiority of our new concept definition over linear baselines for concept alignment analysis in Sec. 4.2, and perform a concept-alignment analysis between four ViTs in Sec. 4.3."}, {"title": "4.1. Concept discovery", "content": "First, we outline the concept discovery procedure as described in Sec. 2.2 and evaluate the quality of the UMAP embeddings used for HDBSCAN clustering and the clustering itself. For concept discovery and later analysis of representational alignment, we use a random subset of 25% of the ImageNet train set, stratified samples across all 1000 classes. We study four different ViTs [10] with the same architecture (base, patch size 16, input size 224) but different training objectives and training datasets described in Tab. 1.\nWe perform concept discovery separately for the sequence (SEQ) and the CLS token. We extract activations at the last MLP layer of each of the twelve transformer blocks. For the sequence tokens, we average-pool 4\u00d74 token and select one of the pooled tokens from the sequence with more weight on the center of the image. For SEQ tokens, we discard the last block as for the considered models only the CLS token in the final layer enters the loss. We evaluate how well the embedding on which we perform the clustering preserves the distances by measuring the mean squared error between the distance matrices in the original representation and its embedding (RMSE). To evaluate the clustering, we compute a density-based validity index (DBCV) [30], which measures intra- vs inter-cluster density. Further, we report the rate of points classified as noise by HDBSCAN. To treat the noise rate and validity index separately, we do not weight the average for the DBCV across clusters by the cluster size as proposed in [30]. Lastly, we evaluate how robust our approach is by measuring the alignment between two runs with different initializations by CBA from Eq. 3. Before discussing the results on embedding and clustering quality (see Fig. 2), we detail the hyperparameter tuning process for UMAP and HDBSCAN. For UMAP, we tune the minimal distance parameter to enhance local cluster density, acknowledging that a lower minimal distance can increase noise. The number of neighbors parameter controls the local structure captured by the embedding; smaller values capture finer local neighborhoods but may distort the global structure, which is important for subsequent concept alignment analysis. We also experiment with the embedding dimensionality, constrained by practical considerations-the curse of dimensionality renders density clustering in the original high-dimensional representation infeasible, where \\(F' = 50\\) is the practical limit for the embedding dimensionality. For HDBSCAN, the minimum cluster size parameter is tuned to balance between identifying noise and merging distinct clusters; a too-small value may recognize noise as clusters, while a too-large value could merge distinct clusters. We set the min samples parameter, which controls the algorithm's conservativeness regarding noise, relatively low due to sampling limitations - some concept manifolds may not be sampled densely enough. We tune all these hyperparameters to maximize the DBCV across models and layers. During hyperparameter tuning, we weight the average DBCV across clusters by their respective sizes to indirectly account for the noise rate. The final hyperparameters used in all subsequent experiments are reported in the appendix. Turning to the results presented in Fig. 2, we observe that RMSE increases slightly across layer for most models. Only for FS there is a strong increase from layer eight onwards, indicating these representations are more difficult to embed and we can trust the clustering on the embedding less which has a high DBCV but low robustness. The density-based validity is medium, but similar across models and SEQ tokens vs. CLS tokens. Given how challenging the clustering task is, we view this result as decent and refer to the convincing qualitative impression of the clusters in Fig. 3 and Fig. 4. Noise rates are rather high but decrease across layers. The high noise may be due to insufficiently dense sampling, i.e. thorough sampling of noisy regions could result in concept clusters. However, the number of input samples is restricted computationally by UMAP and HDBSCAN. Robustness decreases for all models across layers but stagnates at around 0.84 for most models in the late layers. This links back to the trend in RMSE which shows that higher layers are harder to embed. For the qualitative evaluation of our concept discovery method, we construct concept formation graphs (CFGs) that depict the flow of token assignments to concepts from one layer to the next as an unweighted, directed graph. Fig. 3 displays the formation of the \"apples\" concept throughout the layers of the FS model. Note that these graphs may be incomplete, as some nodes might not be detected by the clustering method, illustrating the under-sampling problem described above. Additional examples for other models and the detailed algorithm for CFG construction are provided in the appendix."}, {"title": "4.2. Sanity checking concept structure for alignment", "content": "We use a sanity check to demonstrate how concept-based alignment analysis benefits from concepts defined as non-linear manifolds by comparing against concept alignment based on other definitions and discovery methods. The sanity check is based on the assumption that neighboring representations should be most aligned. We measure the ratio of layers for which a neighboring layer is most aligned under CBA from Eq. (3) We compare NLMCD concepts against one-dimensional linear subspaces discovered by [13], multi-dimensional linear subspaces discovered by MCD [39], and spherical concepts discovered by KMeans clustering [13]. To obtain soft concept membership scores for the linear subspaces, we project the feature vector onto the concept subspace and clip to negative values to 0, as we argue that a feature vector pointing into the opposite direction of a concept signifies the concept not being active. For KMeans concepts, we measure concept proximity by the euclidean distance to the cluster centroid. We also normalize concept membership scores \\(P'^{\\alpha} = P^{\\alpha}/\\Sigma_{\\alpha} P^{\\alpha}\\) as their sum is required to be less bounded by one \\(\\Sigma_{\\alpha} P < 1\\) in Eq. 1. There is no direct way to estimate the number of concepts for PCA, MCD and KMeans, so we use all \\(F = 768\\) components for PCA for a conservative baseline, and the number of concepts discovered by NLMCD for MCD and KMeans discovery. We present the scores in Tab. 2 for SEQ and CLS token concept alignment. We find that our approach performs best across all models except DINO where PCA achieves the highest score. All other concept frameworks reach NLMCD scores only for single models. For the CLS token, the gap between NLCMD and the other methods is larger than for SEQ token alignment."}, {"title": "4.3. Concept Alignment Analysis", "content": "We now investigate concept-based alignment described in Sec. 2.3 between representations across layers and models. We structure the analysis into intra-model and inter-model."}, {"title": "5. Conclusion", "content": "We propose a novel approach that combines concept discovery with representational alignment analysis in ViTs. With concept-based alignment analysis, we answer the questions raised in the introduction and examine the structuredness in feature spaces of different ViTS, as well as fine details between the concepts of two different models. These insights are not available through traditional scalar alignment measures. Understanding the structured nature of latent spaces can guide practitioners in choosing models that not only perform well on benchmark datasets but also exhibit robust feature representations for downstream tasks. For instance, the nucleation process in FS emphasizes the importance of model structure over mere classification accuracy when selecting a pre-trained model.\nLimitations The computational scalability of HDBSCAN limits the sampling of feature vectors which makes under-sampled concept regions appear as noise. The limited variability of ImageNet-1k might obfuscate the meaning of a concept, e.g. when a concept represents a color but there are only dog patches of that color."}, {"title": "A. HDBSCAN", "content": "After concept discovery with HDBSCAN, we compute concept proximity scores \\(P\\{\\phi_i\\} = \\{P(\\phi_1),\\dots,P(\\phi_N)\\}\\), where \\(P_{\\alpha}(\\phi) \\in [0, 1]^n\\) holds the concept proximity scores \\(P_{\\alpha}\\) of each concept \\(C_\\alpha\\). These rely on the implementation of soft clustering with HDBSCAN from [28], which we formalize here for the reader's convenience.\nClustering HDBSCAN first transforms the feature space using a density-informed metric called mutual reachability distance\n\n\\[ MRD(\\phi_i, \\phi_j) = \\max(coreDistance_k(\\phi_i), coreDistance_k(\\phi_j), d(\\phi_i, \\phi_j)) \\]\n\nwhere \\(coreDistance_k(\\phi)\\) is the distance between a point \\(\\phi\\) and its \\(k\\)-nearest neighbor. Based on the mutual reachability distance between all pairs, a minimum spanning tree is constructed that connects all points and minimizes the sum of the edges weighted by MRD. From this, a hierarchical tree is constructed via robust single linkage clustering. The hierarchical tree is condensed by eliminating insignificant clusters and simplifying the hierarchy. This is achieved by selecting a range of persistence values \\(\\lambda\\), which are the inverses of the mutual reachability distances (\\(\\Lambda = 1/MRD\\)). Clusters that persist over significant ranges of \\(\\lambda\\), i.e. they are stable across multiple density levels, are retained, while clusters that exist only over narrow ranges of \\(\\lambda\\) are considered noise and pruned from the tree. The result is a condensed tree that focuses on the most significant clusters. Finally clusters are extracted from the condensed tree either based on their stability across different density levels or simply the leaf nodes are identified as clusters.\nSoft clustering with HDBSCAN The soft cluster membership scores combine a distance-based membership with and an outlier score.\nFor the distance-based membership to cluster \\(C_\\alpha\\), first \\(k\\) exemplar points \\(\\{\\phi^\\alpha_i\\}_\\{i \\in [1, k]\\}\\), are extracted. A single centroid is not enough to characterize a cluster as its shape can be arbitrary. The exemplar points are the points within the leaf nodes beneath cluster \\(C^\\alpha\\) with maximum persistence \\(\\lambda\\) in the condensed tree, i.e. the densest points where the cluster persists.\nThen, the distance membership score between a point \\(\\phi\\) and a cluster \\(C_\\alpha\\) is the inverse minimum distance across the exemplar points \\(\\{\\phi^\\alpha_i\\}\\),"}, {"content": "M^\\alpha (\\phi)_{dist} = \\frac{\\left[\\min_i (d(\\phi, \\phi^\\alpha_i))\\right]^{-1}}{\\sum_{\\beta} \\left[\\min_j (d(\\phi, \\phi^\\beta_j))\\right]^{-1}}\nnormalized across all clusters.\nThe outlier-based membership compares a point's membership persistence to the total persistence of a cluster:"}]}