{"title": "Revisiting Rogers' Paradox in the Context of Human-AI Interaction", "authors": ["Katherine M. Collins", "Umang Bhatt", "Ilia Sucholutsky"], "abstract": "Humans learn about the world, and how to act in the world, in many ways: from individually conducting experiments to observing and reproducing others' behavior. Different learning strategies come with different costs and likelihoods of successfully learning more about the world. The choice that any one individual makes of how to learn can have an impact on the collective understanding of a whole population if people learn from each other. Alan Rogers developed simulations of a population of agents to study these network phenomena where agents could individually or socially learn amidst a dynamic, uncertain world \u2013 and uncovered a confusing result: the availability of cheap social learning yielded no benefit to population fitness over individual learning. This paradox (Rogers' Paradox) spawned decades of work trying to understand this equilibrium and uncover factors that foster the relative benefit of social learning that centuries of human behavior suggest exists. But what happens in such network models now that humans can socially learn from AI systems that are themselves socially learning from us? We revisit Rogers' Paradox in the context of human-AI interaction and extend the simulations introduced in Rogers' Paradox to examine a simplified network of humans and AI systems learning together about an uncertain world. We propose and examine the impact of several learning strategies on the quality of the equilibrium of a society's \u201ccollective world model\u201d. We consider strategies that can be undertaken by various stakeholders involved in a single human-AI interaction: the human, the AI model builder, and the society or regulators around the interaction. We then extend the environment model to consider possible negative feedback loops that may arise from humans learning socially from AI: that learning from the AI may impact our own ability to learn about the world. We close with several open directions into studying net-works of human and AI systems that can be explored in enriched versions of our simulation framework.", "sections": [{"title": "Introduction", "content": "For centuries, humans have learned about the world in different ways: from each other, and from individually conducting experiments and exploring the world around us. From young children [Gopnik, 1996] to pioneers like Isaac Newton, Marie Curie and Archimedes, humans have long engaged with the world and each other in many ways like scientists \u2013 tinkering with our models of the world [Bramley et al., 2023, Rule et al., 2020] and sharing this knowledge to impact society's collective understanding of the world.\nBut conducting experiments yourself \u2013 whether exploring out in the world, or thinking really hard to process what you have already observed \u2013 often comes with costs. It takes time and en-ergy to think and to explore, and we have fundamental constraints on such resources [Griffiths, 2020, Lieder and Griffiths, 2020]. Sometimes, it is easier to just build on the behavior or insights from another person; to update your understanding of the world based on what someone"}, {"title": "Integrating Human-AI Interaction in Rogers' Paradox", "content": "We first introduce Rogers' Paradox and the simulation environment introduced in [Rogers,\n1988], see Figure 2 for a visual summary. Rogers' Paradox considers the case where agents in\na changing environment try to adapt their individual (simplified) \u201cworld models\u201d by either\nlearning about the environment individually or by learning socially from a number of other\nagents (e.g., their cultural parents). The underlying \u201ctrue\u201d environment, or world, is continually\nchanging; a behavior that may be adaptive in one moment in time may no longer be advan-tageous if this world changes. The population has some average level of fitness which can be\nthought of as the quality of a collective \u201cunderstanding\u201d of the world."}, {"title": "Background", "content": "We next provide a concrete walkthrough of such a network. Consider the following thought\nexperiment (setup adapted from Deffner and McElreath [2022] and notation adapted from\nEnquist et al. [2007]). Suppose we have agents (N = 1000) in a world that is slowly changing\nover time, with a fixed probability (u = 0.01) that the optimal behavior for succeeding in\nthis world changes at each time step (e.g., due to some weather event or other change to the\naffordances in the world). Agents in this environment who discover the optimal behavior\n(consistent with Enquist et al. [2007] we call this the \u201cOK\u201d behavior) for a given timestep are\n\"adapted\" and have an increased probability of surviving ($sOK := P(survival|OK) = 0.93$)\ncompared to non-adapted individuals ($s\u00acOK := P(survival|not OK) = 0.85$). To discover the\ncurrent optimal behavior, agents can attempt to learn about the current state of the world at a cost\n($c\u2081 = 0.05$) and with some risk of failure (success probability $zi := P(OK|individual learning =\n0.66, pK := (1 \u2212 c\u2081)zi = 0.95 * 0.66 = 0.627$. At the end of each timestep, agents survive\naccording to their survival probability, and the environment is replenished back to its original"}, {"title": "Introducing AI into the Network", "content": "In this work, we extend the simulations to introduce an abstract \u201cAI model\" into the network.\nWith the propagation of AI systems like GPT, we increasingly see people using these systems\nfor knowledge retrieval, decision-making, and problem-solving - a clear form of social learning.\nHowever, these AI systems themselves are trained on virtually all the text ever produced by\nhumans. Books, text, and the Internet can be thought of as cultural artifacts that reflect our"}, {"title": "Exploring Strategies for AI Rogers' Paradox", "content": "How can we design for positive learning outcomes when we introduce an AI agent into the\nnetwork? There are multiple parties that can help make human-AI interaction \u201cgo right\":\nthe human, the developer of the AI model, and the developer of the infrastructure around\nthe model (e.g., the interface builder, even \u201csociety\u201d that may change affordances that guide\nuse of an AI system). We consider several strategies that are motivated by the literature on\nhuman-AI interaction. For each strategy, we explore how one possible instantiation in our\nsimulation framework may modify the collective \u201cworld model\u201d acquisition and quality. We\nemphasize that our simulations act as a guide for imagining the impact that these strategies\nmay have on population dynamics (not just the individual interacting with the AI system), as they\nhave in evolutionary biology, anthropology, and other disciplines, extending to thinking about\nhuman-Al interaction."}, {"title": "Human- and Interaction-Centric Strategies", "content": "We first consider strategies that can be undertaken by the individual human or addressed in the\ninfrastructure around learning, e.g., by organizations, developers, or regulators."}, {"title": "When Should You Learn from an AI System?", "content": "Deciding who to learn from and when is not always an easy task with humans [Kendal et al.,\n2018, Schwartz, 2015]. Deciding when (and when not) to learn from an AI system is an ever\nmore important question as these tools grow more powerful and accessible. In the context\nof human-AI interaction, there is a burgeoning literature for approaches that encourage the\nnon-use of AI assistance in favor of human judgment [Bhatt and Sargeant, 2024, Mozannar and\nSontag, 2020, Swaroop et al., 2024]. Within the network models around Rogers' Paradox, this\nequates to humans choosing to engage with individual learning over social learning from the AI\nsystem. We as authors certainly advocate for critically appraising whether to engage with an AI\nsystem. We consider this idea in the context of our simulations by assuming that each agent can\nassess the relative expected utility of a given computation: accounting for their likely world\nknowledge versus that of the model, and the cost of individually boosting their knowledge\nversus engaging with the AI system. This appraisal could be done by each individual, but also\nmay come with a cost (e.g., monitoring the quality of the AI system at test-time and assessing\none's own abilities).\nRecent work has highlighted that humans may not have a well-calibrated understanding of\nAl system capabilities [Vafa et al., 2024b] and may think a model's output is correct even when\nits not, perhaps arising from a lack of confidence in one's own ability [Collins et al., 2024b]\nor other \"illusions of understanding\u201d [Messeri and Crockett, 2024]. The infrastructure around\nthe human-AI interaction (e.g., the interface or other affordances that scaffold access to \u0391\u0399\nassistance [Collins et al., 2024c]) can therefore be used to help an individual decide when they\nshould engage with an AI system in social learning [Collins et al., 2024a, Li et al., 2024]. One\ncould imagine then that humans are told a priori about the AI system's ability, quality, or cost\nand can use this information to decide where they should perform individual learning or social\nlearning, which would correspond to engaging with the AI system.\nHowever, when we implement such a strategy \u2013 by making the AI system unavailable\nwhen the expected adaptation value of learning from it is lower than the expected adaptation"}, {"title": "When Should You Override the Output of an AI System?", "content": "After you have decided to engage with an AI system for learning, humans still have agency\nto decide whether or not to uptake the system's output into their thinking. While the decision\naround whether to engage with an AI system does not necessarily impact the equilibria of\npopulation world understanding in our current simulations, we next consider the impact of the\ndecision to update one's knowledge of the world upon accessing the model. Understanding\nwhen humans rely appropriately or inappropriately on the output of AI systems has been\nstudied in the literature of behavioral economics [Dietvorst et al., 2015, Logg et al., 2019],\nmachine learning [Guo et al., 2024, Mozannar and Sontag, 2020], and cognitive science [Steyvers\net al., 2022]. The preferences on when humans integrate the output of an AI system into\ntheir decision making will differ between individuals [Bhatt et al., 2023, Steyvers and Kumar,\n2024, Swaroop et al., 2024] and how humans are permitted to express an intervention to an\nAI system may depend on factors like the intervener's uncertainty [Collins et al., 2023]. In\nthe same vein, policymakers are deciding when and how humans are given the ability to\noverride a system [S and S, 2021]. For instance, Article 14 of the EU Act explicitly spells\nout requirements for human oversight of AI systems: specifically, users must be granted\nthe ability to understand the capabilities and limitations of a system, as it pertains to our\ntransparency discussion above, and must be able to \u201cdisregard, override, or reverse\u201d the\noutcome of an AI assistant. Similar requirements will emerge in the wake of Biden's Executive\nOrder [Biden, 2023] and Australia's 10 voluntary guardrails for AI safety that includes \u201chuman\ncontrol or intervention in an AI system to achieve meaningful human oversight\u201d [Australia's\nDepartment of Industry and Resources, 2024]. Any human who uses an AI system for learning\nwill be affronted with a decision on whether they integrate a system's output into their beliefs."}, {"title": "Model-Centric Strategies", "content": "We next explore some strategies that can be applied at the level of the model. a series of\nquestions that warrant consideration to help guide the possible futures we laid out above in a\npositive direction."}, {"title": "How Often Should an AI System Update Its Understanding of the World?", "content": "Our world is constantly changing. While humans can dynamically update their understanding\nof the world on-the-fly, the process by which an AI system can efficiently update its model\nremains in question [Wong et al., 2023]. In practice, however, there are very real costs to AI\nsystems updating its understanding of the world [Ibrahim et al., 2024, Khetarpal et al., 2022,\nRillig et al., 2023]. This raises a question of strategy: in an ever-changing, interconnected world\nof learners, what is the impact of variable update schedules on collective world understanding?\nThus far, we have assumed that the AI system snaps to the population mean on each iteration\n(t). We next consider the impact of the AI systems' \u201cupdate schedule\u201d (expected rate of AI\nengaging in social learning at each step; if AI currently has adaptation $x$, then $p_{AI}^{OK,(t+1)} :=\n1- (1-P_{q^{K}}^{OK}q^{OK}) (1 - P_{PAI}^{K} (1-u))$, and $p_{AI}^{7} := cx_{AI}$ where $cx_{AI}$ is the cost of updating\nthe AI socially to the mean of the agents in\nthe network, so $PAI = (1 \u2212 ci)zAi P_{q^{K}}^{OK}$ We find in Figure 5\nthat there is a saturation point at which the frequency of updates has minimal impact on the\nequilibrium of the population's collective world model understanding (thereby, costs could\nbe saved by a model builder from less frequent updates) that depends on the base rate of"}, {"title": "How Should an AI System Update Its Understanding of the World: Socially or Indi-vidually?", "content": "While many popular AI systems today (e.g., several large language models) may be viewed\nas having socially learned from us, the quality of such an AI system's \u201cunderstanding\u201d of the\nworld is then based on the knowledge of the individual humans in the system: the advantage\nto the population comes from the AI system's ability to consolidate and provide access to this\nknowledge at a cheap cost to other humans, not \u201cnew\u201d knowledge per say. To go beyond what\nhumans know to bring our collective \u201cworld model\u201d closer to the actual world, we may also\nconsider an Al system which updates based on its individual interactions with the world. Many\nresearchers in AI have, and are, pursuing alternate training schemes, wherein AI systems learn\nfrom non-human generated data (e.g., distilling information from from other AI models [Hinton\net al., 2015], engaging with explicit simulators of the world [Allen et al., 2020, Hermann et al.,\n2017], or relying on self-play [Silver et al., 2017]). Recent approaches have begun examining\ntest-time compute scaling as an additional way to improve AI performance (albeit at a fairly\nsteep cost) by letting the AI \u2018think' for extended periods of time before responding [Snell et al.,\n2024]. Increasing discussion around \u201cagentic\u201d AI also opens up the possibilities that AI systems\nindividually learn about the world by taking action in that world, for instance, iteratively\nchecking code against a compiler, searching the web, or taking physical action.\nWhether an Al system has learned socially (from us) or individually (from self-play or other\nactions), we may learn from such an AI system through the same mechanisms. We explore this\nidea by considering an AI agent in our population network which can learn individually, with\nsome cost relative to social learning ($p7^{OK} := (1 \u2212 cx\u2081)zAi$ where $cx_{AI}$ is the cost of updating\nthe Al individually, and $ZAI$ is the success rate for the AI's individual learning). But in practice,\njust because an agent can individually learn does not mean that such learning will be successful.\nWe take first steps to explore the impact of varied individual learning success rates and varied\nindividual learning costs on population dynamics in Figure 6. We see that when the AI system\nhas a low cost to individually learning and a high success rate when pursuing individual\nlearning \u2013 the population's net collective understanding of the world can improve substantially,\nregardless of whether the population (or infrastructure around users' interactions with the\nAI) involves critical evaluation of use. However, we see that when the AI is often unsuccessful\nwith individual learning (i.e., it comes to a bad conclusion about the world) and particularly\nwhen costs to individual learning are low such that the model is frequently individual learning,\nthen population world understanding is hampered, unless the human agents have not already"}, {"title": "When Interactions Change Learning Efficacy", "content": "Now, imagine if you could at the snap of a finger engage instantly with an expert to learn about\nthe world? What if you never needed to pour over a textbook for hours on your own to learn a\nnew concept \u2013 to get stuck and need to start again? While to (hopefully) many, this would not\nbe a particularly fun or fulfilling future: if you can always socially learn for cheap and therefore\n\u201cavoid\u201d individual learning, your future ability to re-engage with individual learning may be\nsubstantially weakened. Thus far, we have focused on the impact of various learning strategies\nin a modified version of the original Rogers Paradox setting wherein a human attempting to\nimprove their understanding of the world can choose to learn individually or socially from\nanother agent (human or AI). However, this choice has no impact on the human other than\nimproving (or failing to improve) that person's understanding of the world. Whichever way\nyou choose to learn, you will always have the same expected learning success. Yet, the ways that\nyou choose to learn can impact how successful your future learning may be. This is especially\na concern with AI tools or possible \u201ccognitive extenders\u201d [Hern\u00e1ndez-Orallo and Vold, 2019].\nSeveral works have raised concerns about the impact of AI tools on our cognition and relative\nself-appraisal: these include algorithm appreciation [Logg et al., 2019], loafing [Inuwa-Dutse\net al., 2023, Saluja et al., 2024], algorithm aversion [Dietvorst et al., 2015, 2018], algorithmic"}, {"title": "Looking Ahead", "content": "We close by noting several open directions that excite us about studying human-AI interaction\nin the context of collaborative learning in these kind of network models, which we believe are\nripe for richer representations of our uncertain, dynamic world. To support further exploration\nof and around AI Rogers' Paradox, we make all simulation code open-source at the following\nrepository: https://github.com/collinskatie/ai-rogers-paradox."}, {"title": "Handling Different \u201cClasses\u201d of AI Systems", "content": "Our analyses thus far have focused on a single aggregate, abstract \u201ckind\u201d of Al system. However,\nat the time of writing, there is a burgeoning offshoot of more traditional large language models"}, {"title": "Transparency and Information Communication", "content": "Humans frequently employ explanations to justify our recommendations and choices when\nlearning from each other. Our analyses here do not account for (1) what potential information\nhumans can access about AI systems, and (2) how access to that information affects their decision\nto engage in social learning. More broadly, AI systems may communicate their understanding of\nthe world to humans in an attempt to modulate when and how humans integrate an AI system\ninto their learning process. There is a large literature on the transparency of AI systems, which\ncould potentially be repurposed for world model communication [Ehsan et al., 2021, Gunning\nand Aha, 2019]. Much of this work refers to obtaining and designing explanations of the\nbehavior of an AI system in humans [Doshi-Velez and Kim, 2017]. The goal of such transparency\nis often to modulate how and when humans elect to learn from an AI system [Zerilli et al.,\n2022]. This transparency information could span from the communication of uncertainty\ninformation [Bhatt et al., 2021] to natural language explanations of behavior [Zaidan et al., 2007]\nto representations of the systems' world model via explicit probabilistic programs [Collins et al.,\n2024c, Dalrymple et al., 2024, Vidal, 2022, Wong et al., 2023], or other indications of each agents'\nrepresentation of the world as it relates to how that agent may communicate with another\nagent [Sucholutsky et al., 2023b, 2024]. Thus far, our network simulations have focused on a\nsingle determiner of success: whether you can \u201cplay\u201d the right strategy or not, based on your\nunderstanding of the world. Future work could separate out these components of deciding\nwhat to do and what is known about the world, as they relate to what you know about another\nagent. Transparency into an AI system's understanding of the world may also be used to deter\nfrom social learning [Dietvorst et al., 2015, Zerilli et al., 2022]. In the settings we have considered\nhere, the \u201cuse\u201d of an AI system is resigned in favor of human judgment and individual learning\nbased entirely on the relative success probabilities of \u201cplaying\u201d a certain learned strategy.\nIn our current instantiation of a form of selective use, we have assumed humans are only\nprovided with the option to see and disregard the AI system output; transparency information\ninstead permits the communication of why an output was shown or not shown (e.g., the AI\nsystem's understanding of the world is knowingly flawed, e.g., due to infrequent updates or\nthe uncertainty is too high). Although our simulations find that such interventions do not affect\npopulation equilibrium, future work can account for alternate veils of transparency and known\nconfounding effects for how humans update their decisions to learn upon receiving information\nfrom an AI system during learning [Bhatt et al., 2023, Bu\u00e7inca et al., 2021]. We may also consider\nvarying degrees of information richness passed between agents about the world, which could\ninclude, for example, soft labels that capture a human's uncertainty on the task at hand [Collins\net al., 2022, Sucholutsky et al., 2023a]."}, {"title": "Collaborative Learning with Human-AI Thought Partners", "content": "The bulk of analyses related to Rogers' Paradox focus on agents as \u201clone explorers\u201d engaging\nwith the world, or agents learning hierarchically from another agent (akin to a child learning\nfrom a parent or student learning from a teacher). However, humans also learn about the"}, {"title": "Lowering Barriers to Human-Human Social Learning", "content": "Our simulations also spotlight the question around who you choose to partner with and when: if\nyour goal is to develop a faithful understanding of the world", "access": "in science, it can be hard, though a gift, to find good collaborators. And\nthe human partners that we find may generally be \u201clike\u201d us in many ways, likely a byproduct\nof how we found the partner in the first place (e.g., same lab or university). The introduction\nof AI systems into networks of human learners may not only impact collective world model\nbuilding by increasing the ease of access of good AI-based thought partners to human learners to"}]}