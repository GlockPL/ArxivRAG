{"title": "Malicious Path Manipulations via Exploitation of Representation\nVulnerabilities of Vision-Language Navigation Systems", "authors": ["Chashi Mahiul Islam", "Shaeke Salman", "Montasir Shams", "Xiuwen Liu", "Piyush Kumar"], "abstract": "Building on the unprecedented capabilities of large\nlanguage models for command understanding and zero-shot\nrecognition of multi-modal vision-language transformers, visual\nlanguage navigation (VLN) has emerged as an effective way\nto address multiple fundamental challenges toward a natural\nlanguage interface to robot navigation. However, such vision-\nlanguage models are inherently vulnerable due to the lack of\nsemantic meaning of the underlying embedding space. Using\na recently developed gradient-based optimization procedure,\nwe demonstrate that images can be modified imperceptibly\nto match the representation of totally different images and\nunrelated texts for a vision-language model. Building on this,\nwe develop algorithms that can adversarially modify a minimal\nnumber of images so that the robot will follow a route of\nchoice for commands that require a number of landmarks.\nWe demonstrate that experimentally using a recently proposed\nVLN system; for a given navigation command, a robot can be\nmade to follow drastically different routes. We also develop\nan efficient algorithm to detect such malicious modifications\nreliably based on the fact that the adversarially modified images\nhave much higher sensitivity to added Gaussian noise than the\noriginal images.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the landscape of artificial intelligence has\nbeen significantly reshaped by the advent of large language\nmodels (LLMs). These models have demonstrated unparal-\nleled proficiency across a wide range of natural language\nprocessing tasks, exhibiting a remarkable ability to generate\nhuman-like text. Simultaneously, the advent of multimodal\nmodels that combine LLMs with visual processing capabil-\nities has introduced new possibilities for AI applications,\noffering unprecedented capabilities in zero-shot classifica-\ntion. The development of Vision-and-Language Navigation\n(VLN) systems ([1], [2]) exemplifies this trend, offering\nnew capabilities such as guiding robots or assisting users in\nvirtual environments based on combined visual and textual\ninstructions or queries. For example, GPT-4V has been used\nto generate detailed instructions for an autonomous driving\nsystem under many real-world scenarios [3]. LM-Nav [4],\na recently developed VLN system, enables a real-world\nrobot to navigate through complex outdoor environments via\nnatural language commands without any customized training\nof the models.\nHowever, despite these advancements, the vulnerabilities\nof LLMs and multimodal models to adversarial attacks pose\nsignificant challenges. Adversarial attacks, which involve"}, {"title": "II. RELATED WORK", "content": "Vision-and-Language Navigation ([1], [2]) is an inter-\ndisciplinary research area that requires the integration of\nboth visual perception and natural language understanding.\nIn VLN, agents are tasked with navigating through unfamiliar\nenvironments (both persistent and non-persistent) based on\nnatural language instructions ([1], [2], [12], [13]). Several\ndatasets have been curated specifically for VLN tasks, with\nthe Room-to-Room (R2R) dataset emerging as a leading\nbenchmark [12]. Over recent years, VLN has attracted\nconsiderable attention due to its potential applications in\nautonomous navigation, robotics, and virtual reality. Robotic\nVLN enables robots to navigate and map environments using\nvisual perception and natural language instructions. Several\napproaches have been proposed to address this problem ([12],\n[14]). Most of the approaches require pre-training and fine-\ntuning models. For instance, Majumder et al. [15] pretrain a\nVLN-BERT model using extensive image-text pairs sourced\nfrom the web. In a separate study, Chen et al. [16] incor-\nporate directional cues into the conventional sequence-to-\nsequence VLN framework to augment performance. Addi-\ntionally, some methods rely on visual language and memory\nto guide robot navigation by selecting landmarks and using\nprompt guidance [17]."}, {"title": "III. PRELIMINARIES", "content": "Vision-and-Language Navigation systems ([4], [28], [29],\n[30]) are composed of a few key components that interface\nbetween language, vision, and robot control. Initially, a\nlanguage model parses instructions into salient landmarks.\nNext, a vision-language model grounds these landmarks in\nthe visual observations. Finally, a visual navigation model\ncontrols the robot between specified waypoints."}, {"title": "A. Core Components", "content": "LM-Nav consists of three core components: a large lan-\nguage model (LLM), a vision-language model (VLM), and\na visual navigation model (VNM). The LLM is a generative\nmodel based on the transformer architecture, trained on\ninternet text, and it parses textual instructions into a sequence\nof landmarks. The VLM, specifically the CLIP [18], encodes\nimages and text into an embedding space, allowing it to\nassociate images with textual descriptions. By computing\nthe cosine similarity between the VLM embeddings of\nlandmarks and images, probabilities are obtained to align\nlandmark descriptions with images. The VNM is used to\ninfer the robot's navigation effectiveness between nodes in a\ngraph, based on estimated temporal distances. These compo-\nnents work together by parsing instructions into landmarks,\nassociating landmarks with graph nodes, and optimizing a\nprobabilistic objective to find the optimal path on the graph,\nwhich is then executed using the VNM model [4].\nLM-Nav employs a graph search algorithm to determine\nthe optimal sequence of nodes to visit in order to follow\nthe text instructions. For each landmark $l_i$ extracted by the\nlanguage model, the vision-language model computes a prob-\nability $P(v|l_i)$ that every node $v$ in the graph corresponds to\n$l_i$. It defines a function $Q(i, v)$ representing the maximum\nscore for reaching node $v$ while visiting landmarks up to\n$l_i$. $Q(0,v)$ is initialized based on the shortest path from\nthe start. Then for each landmark $i = 1,..., n$, it updates\n$Q(i,v)$ either by carrying over the previous $Q(i \u2013 1, v)$ and\nadding $P(v|l_i)$, or by transitioning from a neighboring node\n$w$ with $Q(i, w) \u2013 a \\cdot D(v, w)$, where $D(v, w)$ is the travel\ncost between $v$ and $w$. After computing $Q(n, v)$ for all $v$, it\nchooses the node with the maximum value as the destination.\nThe optimal path is recovered by backtracking through the\n$Q$values. However, a potential limitation is that summing\nVLM probabilities across landmarks could cause the system\nto visit an incorrect node for landmark $l_i$ if the cumulative\npast scores overwhelm the current $P(v|l_i)$, deviating from\nthe intended instructions."}, {"title": "B. Role of Transformers", "content": "Transformers play a crucial role in LM-Nav. The Large\nLanguage Model (LLM) component, powered by transform-\ners, parses free-form instructions into a list of landmarks\n[4]. The Vision-Language Model (VLM) component utilizes\ntransformers to associate these landmarks with nodes in a\ngraph by estimating the probability of correspondence. In the\nVisual Navigation Model (VNM) component, transformers\nestimate navigational affordances and robot actions, enabling\nnavigation between graph nodes [21]. LM-Nav leverages"}, {"title": "C. Assumed Semantics", "content": "For grounding landmark text with images, LM-Nav uses\nthe CLIP model, specifically the ViT-L-14 variation of it. The\ncore assumed semantics of the CLIP model is its ability to\nassociate and align visual and textual representations in a\nshared embedding space. Through contrastive pretraining on\na massive dataset of image-text pairs, the model learns to\nmap semantically related images and texts close together in\nthis embedding space, while pushing unrelated pairs apart.\nThis learned alignment between visual and linguistic modal-\nities allows the model to effectively capture and leverage the\nsemantic relationships between images and their correspond-\ning textual descriptions, captions, or labels. By projecting\nboth image and text inputs into a common embedding space,\nCLIP can understand and reason about the visual-semantic\nalignments, enabling it to perform tasks that require bridging\nthe visual and textual domains, such as image-text retrieval,\nimage captioning, and multimodal classification."}, {"title": "IV. REPRESENTATION VULNERABILITIES OF VISUAL\nLANGUAGE MODELS", "content": ""}, {"title": "A. Equivalence Structures of the Embedding Space.", "content": "In our earlier work [9], we have proposed a framework\nto explore and analyze the embedding space of vision\ntransformers, revealing intriguing equivalence structures and\ntheir implications for model robustness and generalization.\nUnderstanding the structures of the representation space is\ncrucial, as they determine how the model generalizes. In\ngeneral, we model the representation given by a (deep) neural\nnetwork (including a transformer) as a function $f : \\mathbb{R}^m \\rightarrow\n\\mathbb{R}^n$. A fundamental question is to have a computationally\nefficient and effective way to explore the embeddings of\ninputs by finding the inputs whose representation will match\nthe one given by $f(x_{tg})$, where $x_{tg}$ is an input whose\nembedding we like to match. Informally, given an image\nof a stop sign as an example, all the images that share its\nrepresentation given by a model will be treated as a stop\nsign."}, {"title": "B. Embedding Alignment Procedure", "content": "We describe the algorithm for embedding alignment ([9],\n[11]), aimed at matching the representation of an input\n(image embedding) to that of a target input (e.g., image\nor text embedding). To accommodate the requirement of\naligning two vectors, a loss function $L(x)$ is defined to\nquantify the difference between the embedding of a modified\ninput $(x_0 + \\Delta x)$ and the target embedding $(f(x_{tg}))$. The goal\nis to minimize this loss, which is represented as the squared\nnorm of the difference between these two embeddings.\n$L(x) = L(x_o + \\Delta x) = \\frac{1}{2} \\left\\| f(x_o + \\Delta x) - f(x_{tg}) \\right\\|^2$, (1)\nThe method involves calculating the gradient of the loss func-\ntion concerning the input, which is related to the Jacobian\nof the representation function at the initial input$(x_0)$. This\ngradient information is used to adjust the input in a way that\nminimizes the loss, moving the input's embedding closer to\nthe target's embedding.\nAlthough obtaining the optimal solutions might involve\nsolving either a quadratic or linear programming problem,\ndepending on the chosen norm for minimizing Ax, the\ngradient function has proven to work effectively for all the\ncases we have tested, attributable to the Jacobian of the\ntransformer. Note that automatic gradient calculations are\nsupported by all deep learning frameworks\u00b9 and are done\nefficiently as the required time is the same as one step\nbackpropagation for one sample."}, {"title": "V. ADVERSARIAL ROUTE MANIPULATION METHOD", "content": "In this section, we describe an adversarial route ma-\nnipulation method designed to redirect a visual landmark-\nbased navigation system from its intended path to a specific\nmalicious target node. The method operates on a graph\nrepresentation of the environment, where nodes correspond\nto distinct viewpoints and are associated with one or more\nimages (e.g., front and back views). Our approach alters the\ngraph to deceive the navigation system, which usually plans\nthe best route by matching visual observations to landmark\ndescriptions. This makes the system assume it has visited all\nlandmarks while heading to the target malicious destination.\nThe proposed method consists of two key algorithms.\nAlgorithm 1 focuses on the optimal selection of nodes\nfor modification along the shortest path between the start\nand target nodes. We employ Dijkstra's algorithm to de-\ntermine this path, and then, with Dynamic Programming\n(DP), we sequentially identify the nodes that exhibit the\nhighest similarity scores with respect to the corresponding\nlandmark text embeddings. If we have m nodes in the path\nand n landmarks, then the DP algorithm's time complexity\nwill be $O(m \\times n)$. This selection strategy ensures that the\nchosen nodes are already well-aligned with the landmark\nrepresentations, facilitating faster convergence and higher\nsimilarity scores during the subsequent optimization phase.\nImportantly, the target node is specified as the final landmark,\nand the route manipulation algorithm is designed to direct\nthe robot along the malicious trajectory, maximizing its\nprogression toward the designated target node.\nAlgorithm 2 optimizes the representations of the chosen\nnodes to increase their similarity with the corresponding\nlandmark text embeddings. It leverages the algorithm in-\ntroduced in our previous works ([9], [11]), where text\nembedding is used as the target input, effectively aligning"}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "To evaluate the efficacy of our method, we utilize two\ngraphs previously employed in the LM-Nav system. These\ngraphs were constructed using the RECON dataset\u00b2, which\ncomprises 5000 self-supervised trajectories and observations\ngathered from a robot navigating through an environment.\nThe graph construction process involves the Visual Navi-\ngation Model (VNM), which leverages a goal-conditioned\ndistance function to discern connectivity among raw ob-\nservations, thereby facilitating the creation of a topological\ngraph. This model constructs the graph by using image\nand GPS data collected while manually operating the robot.\nThe two graphs differ in size: EnvLarge-10, which contains\n278 nodes, and EnvSmall-10, with 241 nodes. Each node\nin these graphs contains two images (capturing both the\nrear and front views) along with GPS observations and\ninformation on connections to adjacent nodes. These images\nare taken by the robot's onboard camera as it moves through\nthe environment. Our experimentation included 10 distinct\npaths for each graph, each path featuring a varying number\nof landmarks. All experiments have been conducted on a\nlaboratory workstation equipped with two NVIDIA A5000\nGPUs. The code for our proposed method and experiments\ncan be found in this GitHub repository \u00b3."}, {"title": "B. Representation Matching", "content": "As outlined in Algorithm 2, once we have derived the\noptimal path with selected nodes, we proceed to modify\nthem based on another target embedding representation.\nWe acquire the input image embedding using CLIP and\nsubsequently generate a target embedding from either a target"}, {"title": "VII. ROBUST DETECTION OF ADVERSARIAL IMAGE\nMODIFICATIONS", "content": "The main idea revolves around the robust detection of\nadversarial image modifications by leveraging the detectable\nartifacts induced by such modifications in the feature rep-\nresentations (embeddings) produced by the VLM (in our\ncase CLIP) model. The algorithm compares the feature\nrepresentations produced by the CLIP model for original and\nadversarially modified images, both with and without added\nGaussian noise [10]. For batches of images, it computes\nthe average difference in feature representations between\nthe original and noisy versions. The key idea is that within"}, {"title": "VIII. DISCUSSIONS", "content": "In this paper, we have demonstrated a fully working\nmethod that can modify all the routes in the LM-Nav\nsystem [4] and tested it on their datasets. While our route ma-\nnipulation algorithm is designed specifically for the system,\nfundamentally our embedding matching algorithm is model\nagnostic and is effective on all existing vision transformer-\nbased models ([9], [11]) and therefore the route manipulation\nalgorithm can be changed to exploit the representation vul-\nnerabilities in the vision transformers a VLN system relies\non.\nWhile the LM-Nav system enables a robot to navigate\ncomplex outdoor environments, the noise and large variations\nin lighting and other conditions can render the representa-\ntions of images less reliable. As the system uses the sum\nof the cosine similarities between an image and all the\nlandmarks, the navigation algorithm may not always work\nas expected. The lower final node arrival success rate in\nTab. I than the 100% route modification rate, is due to\nthis unexpected property of their routing algorithm. Our\nalgorithm can also be used to increase the sharpness of\nlandmarks, resulting in more accurate routes.\nIn this paper, we focus on stealth modifications to images\nstored in databases; since imperceptible changes to the\nimages can cause the representations to change significantly,\nsuch changes can not be identified visually. Note that our\nalgorithm can also be modified to design specific physical\nobjects and patterns that can be added to the scene to modify\nthe representations and therefore change the robot's routes\nmaliciously. While such physical objects and patterns have\nbeen demonstrated successfully to adversarially attack deep-\nlearning-based sensors for autonomous driving ([32], [33]),\nwe anticipate more reliable and robust behaviors using our\ndesigned patterns as our algorithm can match the underly-\ning representations of chosen images and the feasibility of\nsuch objects to exploit representation vulnerabilities is being\nexplored.\nThe ability to stealthily control robot behavior via adver-\nsarial graph modifications highlights inherent vulnerabilities\nin today's visual language navigation systems. More broadly,\nas vision-language models are being tested for autonomous\ndriving, the representation vulnerabilities of such models\n(such as GPT-4V [3]) must be tested thoroughly."}, {"title": "IX. CONCLUSION AND FUTURE WORK", "content": "In this work, we have demonstrated the first method to\nadversarially attack vision-language robot navigation sys-\ntems by imperceptibly modifying the images, which result\nin significant changes to their representations and cause\nthe navigation algorithms to produce very different routes.\nFurther progress requires building semantics and robustness\ninto learned representations even though the proposed change"}]}