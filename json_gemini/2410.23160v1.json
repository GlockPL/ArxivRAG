{"title": "FLEXTSF: A UNIVERSAL FORECASTING MODEL FOR TIME SERIES WITH VARIABLE REGULARITIES", "authors": ["Jingge Xiao", "Yile Chen", "Gao Cong", "Wolfgang Nejdl", "Simon Gottschalk"], "abstract": "Developing a foundation model for time series forecasting across diverse domains has attracted significant attention in recent years. Existing works typically assume regularly sampled, well-structured data, limiting their applicability to more generalized scenarios where time series often contain missing values, unequal sequence lengths, and irregular time intervals between measurements. To cover diverse domains and handle variable regularities, we propose FlexTSF, a universal time series forecasting model that possesses better generalization and natively support both regular and irregular time series. FlexTSF produces forecasts in an autoregressive manner and incorporates three novel designs: VT-Norm, a normalization strategy to ablate data domain barriers, IVP Patcher, a patching module to learn representations from flexibly structured time series, and LED attention, an attention mechanism to seamlessly integrate these two and propagate forecasts with awareness of domain and time information. Experiments on 12 datasets show that FlexTSF outperforms state-of-the-art forecasting models respectively designed for regular and irregular time series. Furthermore, after self-supervised pre-training, FlexTSF shows exceptional performance in both zero-shot and few-show settings for time series forecasting.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series forecasting, the task of predicting future values based on historical observations, plays an indispensable role across numerous domains, including finance, manufacturing, retail, healthcare, and meteorology. Recently, advancements in large language models, which exhibit remarkable generalization ability to a range of language tasks, have inspired a new trend of research focused on developing foundation models for time series forecasting. These models aim to deliver robust and effective performance across diverse time series datasets. Developing a foundation model for time series forecasting presents significant challenges due to the diverse characteristics of time series. First, time series contain a wide range of measurement types, such as systolic blood pressure, exchange rate, and electricity consumption, each demonstrating different scales (e.g., 1-10, 50-150) and temporal granularities (e.g., minutes, hours, days, months). Such domain diversity leads to various temporal patterns that are difficult to be captured within a single model. Second, time series exhibit structural diversity, with missing values, varying sequence lengths, and irregular sampling time intervals. For instance, in Figure 1, the blood pressure observations depicted in (a) are sparse at the beginning but become denser over time due to the patient's deteriorating condition. In (b), some data is missing due to factors such as holidays. The time series in (d) shows a clear pattern, while the pattern in (c) is less obvious."}, {"title": "2 RELATED WORK", "content": "There is an emerging trend of research training forecasting foundation models from scratch using time series. In Table 1, we provide a technical comparison of five open-source foundation models with FlexTSF. All these models are trained on large collections of time series data and support zero-shot prediction on new datasets. Specifically, ForecastPFN and DAM use pointwise time-value pairs as input tokens within encoder-only models, but differ in their forecasting approaches: the former relies on time queries while the latter utilizes basis function composition. Lag-Llama and TimesFM employ decoder-only models, adopting autoregressive prediction, which is similar to next token prediction in LLMs, for training on time series data. MOIRAI utilizes time series patches with multiple variables and leverages an encoder-only model trained via masked patch prediction. Existing studies on time series forecasting foundation models typically achieve the generalization capability by pre-training models on multiple time series datasets. However, these solutions for domain diversity are still immature, as no explicit methods have yet been proposed. Some researchers alleviate this problem indirectly by using data from as many domains as possible for pre-training. The more types of data a model sees during pre-training, the fewer unfamiliar data types it will encounter in real applications. However, this paradigm is inefficient, and the time series data currently available for pre-training is not as extensive as the vast text corpora used to train large language models. Moreover, current research largely ignores the problem of structural diversity. Most of these models are designed for well-structured and regularly sampled time series, leaving them ill-equipped to handle the heterogeneous data structure issues often encountered in real-world scenarios. In contrast, as shown in Table 1, FlexTSF stands out by effectively handling various time series characteristics, such as missing values, varying lengths, and irregular intervals, making it more adaptable to diverse applications. Additional related works on Transformers for time series forecasting, adapting LLMs for time series forecast- ing, and irregular time series modeling are provided in Appendix A.1."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 PROBLEM FORMULATION", "content": "We consider a time series \\(S\\) as a sequence of \\(M\\) elements \\(S = \\{(x_i, t_i)\\}_{i=1}^M\\), where each element \\((x_i, t_i)\\) consists of an observation \\(x_i \\in \\mathbb{R}^C\\) with \\(C\\) variables collected at a specific timestamp \\(t_i\\). This formulation is flexible to accommodate a diverse range of time series characterized by various features, such as multiple variables \\((C > 1)\\), different sequence lengths (varying \\(M\\) across samples), irregular time intervals \\((t_{i+1}-t_i \\neq t_i - t_{i-1})\\), and incomplete observations (some variables in \\(x_i\\) are missing), etc. Our target is to develop a foundation model for time series forecasting that can take any time series \\(S\\) with different data characteristics as input and predicting its future values \\(\\{x_i\\}_{i=M+1}^{M+H}\\) over a subsequent time window \\(\\{t_i\\}_{i=M+1}^{M+H}\\), where \\(H\\) denotes the forecast horizon. A notation summary is available in Appendix A.2."}, {"title": "3.2 PROPOSED MODEL: FLEXTSF", "content": "A dataflow overview of FlexTSF is illustrated in Figure 2. The process begins with value and time normal- ization via VT-Norm and then segmenting the time series into patches and appending a dummy patch at the end. For each patch, an IVP Patcher generates its vector representation by evolving the latent states, derived from all data points within the patch, backward in time using neural IVP solvers. These representations are subsequently processed by several attention layers, with a Leader node encompassing statistical domain features extracted by VT-Norm. After a forward pass, the representation of the final node, corresponding to the dummy patch, is passed to IVP Patcher, which evolves the latent states forward in time (as opposed to backward on the input side) to generate future time series values at specified time points. These predicted values replace the previous dummy patch, allowing the autoregressive process to continue until all values within the forecasting horizon are produced. Next, we describe the details of each component."}, {"title": "3.2.1 VT-NORM", "content": "To address the challenges faced by foundation models due to diverse time series data characteristics, such as varying measurement types, scales, and sampling intervals, we propose VT-Norm to standardize both Values and Timestamps. Given the time series values \\(X = \\{x_i\\}_{i=1}^M\\) from dataset \\(D\\), we apply a two-step normalization scheme: global normalization and instance normalization. First, we calculate the mean and standard deviation of all variables in \\(D\\) and use them to normalize \\(X\\) by subtracting the mean and dividing by the standard deviation. We refer to this process as global normalization and relevant statistics as global mean \\(\\mu_g\\) and global standard deviation \\(\\sigma_g\\). Next, before passing each time series into the model, we perform instance normalization on the sequence itself, obtaining the instance mean \\(\\mu_i\\) and instance standard deviation \\(\\sigma_i\\). Let \\(G_g\\) and \\(G_i\\) denote the normalization operations using global and instance statistics, respectively. The whole normalization process for \\(X\\) can be described as: \\(X' = G_i(G_g(X))\\), where \\(X'\\) represents the normalized time series values. For the corresponding timestamps \\(T = \\{t_i\\}_{i=1}^M\\) of \\(X\\) in dataset \\(D\\), we define the global time \\(w_g\\) as the reciprocal of \\(D\\)'s frequency. For each sequence, we calculate the intervals between successive timestamps and obtain \\(\\{\\Delta t_i = t_{i+1}-t_i\\}_{i=1}^{M-1}\\). We then take the minimum of \\(\\Delta t_i\\) as the instance time unit \\(w_i\\), and scale the time intervals as \\(\\Delta t_i = \\frac{\\Delta t_i}{w_i}\\). The new timestamps \\(T' = \\{t'_i\\}_{i=1}^M\\) are calculated by summing the scaled time intervals: \\(t'_j = t_1 + \\sum_{l=1}^{j-1} \\Delta t_l\\). This normalization strategy effectively decouples static domain information from dynamic patterns. To summarize, for a time series sample \\(S\\), we derive normalized values \\(X'\\) and normalized time indicators \\(T'\\), along with six statistical features conveying domain information related to each dataset. We concatenate these features as: \\(L = [\\mu_g, \\sigma_g, \\mu_i, \\sigma_i, w_g, w_i]\\). The vector \\(L\\) is provided to the Leader node, while \\(X'\\) and \\(T'\\) are fed into IVP Patcher for subsequent processing."}, {"title": "3.2.2 IVP PATCHER", "content": "Unlike previous methods that are only applicable to regular time series and rely on splitting data into fixed temporal windows as patches, we propose IVP Patcher, which models the continuous evolution process of time series by solving initial value problems (IVP). This approach allows for us to derive temporal representations with arbitrary time intervals and allows for the handling of patches with varying lengths. The key idea behind IVP Patcher is that time series values \\(x_i\\) within a patch are discrete, indirect observations of an unknown continuous process. To parameterize these processes with available observations, we use ordinary differential equations of the form \\(\\frac{dF(t)}{dt} = f(t, z_i)\\), where \\(z_i\\) is the latent state of \\(x_i\\). Given an initial condition \\((t_0, z_0)\\), the latent state \\(z_i\\) at \\(t_i\\) can be computed using numerical methods, i.e., IVP solvers. In IVP Patcher, the time series input \\(\\{(x_i, t_i)\\}_{i=1}^{P_s+p-1}\\) is divided into non-overlapping patches (sequence segments). Each patch is represented as \\(V = \\{(x_i, t_i)\\}_{i=P_s}^{P_s+p-1}\\), where \\(p\\) is the patch length (i.e., the number of observed data points within a patch), and \\(P_s\\) is the starting index of the patch. For simplicity, we illustrate the process by setting \\(P_s = 1\\), resulting in a patch \\(V = \\{(x_i, t_i)\\}_{i=1}^{p}\\) as the input. The output patch after LED Attention has \\(P_s\\) and \\(P_e = P_s + p - 1\\) denoting the start and end indices, respectively. Algorithm 1 describes the process of generat- ing patch representations for the input patches and making forecasts on the dummy patches. For the input patches (Part 1 of Algorithm 1), a linear layer maps each data point \\(x_i\\) to a latent state \\(z_i\\), which serves as the initial condition \\((z_i, t_i)\\) for the neural IVP solver. The neural IVP solver starts at \\(t_i\\) and evolves the state towards timestamp \\(t_1\\), where observations begin:  \\(z_{1,i} = IVPSolve(z_i, \\Delta t_i),\\)  where \\(\\Delta t_i = t_1 - t_i\\)."}, {"title": "3.2.3 LED ATTENTION", "content": "IVP Patcher primarily focuses on modeling the internal patterns within each patch. However, inter-patch correlations must also be effectively captured for accurate predictions. While the Transformer's self-attention mechanism is usually adopted for this purpose, this mechanism has been found to be less effective in capturing temporal order for time series due to the limitations of their positional encoding functions. To better accommodate time series with variable regularities, we adapt the Transformer self-attention layer into LED Attention, incorporating the following novel design elements. First, we integrate the temporal encoding technique in LED Attention to model complex inter-patch dependen- cies with irregular time intervals. Inspired by rotary position embedding (RoPE) which offers strong theoretical properties and inductive bias for continuous values aligned with temporal information, we adopt this method and calculate Query and Key of the self-attention module by  \\(f_{Q/K}(r, t) = (Wr) e^{i\\tau\\theta},\\)  where \\(W\\) denotes a learnable transformation matrix, \\(i\\) is the imaginary unit \\(\\sqrt{-1}\\), and \\(\\tau\\) denotes the patch time indicator. Since IVP Patcher is designed to model the evolution of patches with respect to their initial timestamps, the first timestamp within each patch is designated as the time indicator \\(\\tau\\). It is noteworthy that the patch time indicators \\(\\{\\tau\\}_{k=0}^K\\) may exhibit variable intervals, reflecting the irregular and continuous nature of the time series, making them well-suited to the RoPE technique. In addition, we utilize the static domain information \\(L\\), extracted from VT-Norm, by transforming it through the Leader node and appending it to the beginning of the sequence. This allows patches from any positions in the sequence to attend to domain-specific information during the autoregressive process. The Leader node is designed to inject sequence-level features into the attention module. To support versatility across different datasets, we use the six features extracted from VT-Norm, as they can be obtained from any dataset. Furthermore, considering LED Attention operates on the patch-level representations, we append a dummy patch at the end of the sequence, associated with the patch time indicator of the forecast horizon. As causal self-attention operates in a manner that restricts each node to attend only to its predecessors, the dummy patch is strategically positioned at the end of the sequence to aggregate the latent information from every preceding patch. This accumulation of information enables the dummy patch to serve as a comprehensive summary of the sequence, which can then be used for generating forecasts. Input representations to subsequent attention layers, denoted by \\(A_0\\), are constructed as \\(A_i = [A_L, A_F, A_D]\\), where the concatenation is performed along the time dimension, and \\(A_L\\) is the transformed static domain"}, {"title": "3.2.4 TRAINING", "content": "Since FlexTSF is essentially an autoregressive generative model, we train it by modeling the joint distribution of all data points in a sequence. Given the input sequence \\(X = \\{x_i\\}_{i=1}^M\\), the future sequence \\(X^+ = \\{x_i\\}_{i=M+1}^{M+H}\\), and the full sequence \\(X^* = \\{x_i\\}_{i=1}^{M+H}\\), the objective is to maximize \\(log p(X^*)\\). By applying the Evidence Lower Bound (ELBO), it becomes:  \\(log p(X^*) = E_{z_0 \\sim q_\\phi(Z_0 | X^*)} [log p_\\theta (X^* | Z_0)] - D_{KL}(q_\\phi(Z_0 | X^*) || p(Z_0)) \\\\ \\geq E_{z_0 \\sim q_\\phi(Z_0 | X^*)} [log p_\\theta (X^+ | Z_0)] - D_{KL}(q_\\phi(Z_0 | X^*) || p(Z_0)) = -\\mathcal{L}\\)  where \\(\\phi\\) and \\(\\theta\\) are learnable parameters, and \\(Z_0 = \\{z_i\\}_{i=1}^M\\) represents the latent variables obtained from all patches. The first term corresponds to the log-likelihood of all available observations within the forecast horizon, while the second term is the KL-divergence between the learned posterior distribution of all patches and the prior distribution. Note that in \\(\\mathcal{L}\\), the likelihood is calculated based on the newly generated values \\(X^+\\) rather than \\(X^*\\), which is used in the calculation of the KL-divergence. The derivation and explanation of the loss can be found in Appendix A.3. Using this framework, FlexTSF can be trained in both supervised and self-supervised settings (see Appendix A.4 for illustration). Moreover, after pre-training on a large collection of datasets, FlexTSF is found to be capable of achieving effective performance by fine-tuning on a target domain with only a small subset of parameters (parameters for transforming input, output values of time series, and static domain features). This attribute can significantly reduce the time and computational resources required to fine-tune the model."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "We evaluate FlexTSF's forecasting performance for both regular and irregular time series across different domains and training settings. We conduct three stages of experiments using two non-overlapping groups of datasets: the pre-training datasets \\(D_p\\) and the held-out datasets \\(D_h\\). In the first stage, we perform classic training-validation-testing experiments on \\(D_h\\) to demonstrate the effectiveness of FlexTSF. Next, we pre-train FlexTSF on \\(D_p\\), yielding a pre-trained model with 61 million parameters. This model is initially used to perform zero-shot forecasts on the test partition of \\(D_h\\), evaluating its potential as a foundation model, and then fine-tuned on \\(D_h\\) for time series forecasting, assessing its adaptability to new domains in few-shot scenarios. Each dataset in \\(D_h\\) is split into training, validation, and testing sets, following their original splits if known or a split ratio of 8:1:1, otherwise. As in prior works, we repeat each experiment three times with different random seeds for dataset splitting and model parameter initialization. To ensure fair comparisons of model performance across datasets, we uniformly use the first 80% of each time series as input and the remaining 20% as the prediction target. The model performance is assessed using mean squared error (MSE). All models are tested in the same computing environment with NVIDIA Tesla V100 GPUs."}, {"title": "4.1.1 DATASETS", "content": "Our pre-training dataset group \\(D_p\\) consists of datasets from the Monash Time Series Forecasting Archive and the UCR & UEA Time Series Classification Archive"}, {"title": "4.1.2 BASELINES", "content": "Previous studies have typically been conducted either on regular or irregular time series. Therefore, we incorporate baselines from both regular and irregular time series domains. To evaluate the zero-shot prediction capabilities of our pre-trained FlexTSF model, we include several pre-trained models. The baselines used in our research are listed in Table 3. Darlow et al. considered irregularity when building a foundation model, but the model was not applied to irregular time series forecasting, and neither pre-trained checkpoints nor replication source code has been released. Therefore, we do not use it in our experiments. These three zero-shot baseline models originally do not support irregular data. To enable them to run on irregular datasets, we adapted the datasets by imputing missing values and aligning timestamps."}, {"title": "4.2 RESULTS", "content": ""}, {"title": "4.2.1 REGULAR & IRREGULAR TIME SERIES FORECASTING", "content": "Table 4 presents the mean squared error (MSE) results for our experiments in the classic training-validation- testing setting on Dh. Across all datasets, FlexTSF consistently ranks among the best two models. On regular time series, FlexTSF performs comparably to state-of-the-art models, demonstrating its effectiveness in capturing temporal dynamics. For irregular datasets, FlexTSF excels, achieving the lowest MSE on METR-LA, CharTraj, and HAR-IMU, demonstrating its superiority when dealing with irregularity issues. Among the baseline models, those designed for regular time series outperform other models on such time series, and their advanced designs enable some of them to also perform well on irregular datasets. The baselines specifically tailored to irregular time series perform particularly well on irregular time series, specifically on eICU and PhysioNet12, which have higher rates of missing data and greater sparsity."}, {"title": "4.2.2 ZERO-SHOT FORECASTING", "content": "Table 5 shows results of our zero-shot experiments where we pre-train FlexTSF on Dp and then directly apply it to the testing data sets of Dh. FlexTSF outperforms other pre-trained time series forecasting foundation models for all except one dataset, and performs exceptionally well on irregular datasets. Two of the baselines fail to produce meaningful results on eICU and PhysioNet12 within reasonable computing time and resources. FlexTSF's consistent performance across both regular and irregular datasets in zero-shot scenarios highlights its potential as a universal foundation model due to its native support for various data structures."}, {"title": "4.2.3 FEW-SHOT FORECASTING", "content": "Figure 3 shows results of our few-shot forecasting experiments: after pre-training on Dp, we fine-tune FlexTSF using varying numbers of samples drawn from the training sets of Dh and evaluate it on the corresponding test sets. FlexTSF consistently outperforms all baselines across both regular and irregular datasets, with even greater advantages when fewer training samples are available. This underscores FlexTSF's superior sample efficiency and adaptability, making it highly effective in few-shot learning scenarios where data is limited."}, {"title": "4.2.4 ABLATION STUDY", "content": "To assess the contributions of our components VT-Norm, IVP Patcher, and LED Attention, we conduct an ablation study by independently removing each component at a time, resulting in three different model variants. Each model is pre-trained on the same datasets (Dp) using the same computational resources. After pre-training, all models are evaluated through zero-shot forecasting on the same test datasets. Table 6 presents the MSE changes compared to the original FlexTSF model in zero-shot scenarios. Removing VT-Norm results in clear performance degradation, particularly on ExRate and HAR-IMU, highlighting its importance in handling domain-specific variations by decoupling static and dynamic patterns. The absence of IVP Patcher causes severe degradation on irregular datasets such as CharTraj and ArabDigit,"}, {"title": "5 CONCLUSION", "content": "We introduced FlexTSF, a universal time series forecasting model designed to address the challenges posed by the domain and structural diversity of time series. FlexTSF comes with three novel designs, namely VT-Norm, IVP Patcher, and LED Attention, which together enable the model to generalize across diverse domains and handle irregularities in time series. Our experiments across 12 datasets demonstrate that FlexTSF consistently outperforms state-of-the-art models specifically designed for either regular or irregular time series. FlexTSF's ability to excel in both few-shot and zero-shot settings highlights its versatility as a foundation model. In future work, the impact of pre-training on datasets with larger scale could be further explored to push the boundaries of foundation models in time series forecasting."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 MORE RELATED WORKS", "content": ""}, {"title": "A.1.1 TRANSFORMERS FOR TIME SERIES FORECASTING", "content": "Time series forecasting has been a longstanding research area, evolving from traditional statistical approaches such as ARIMA and GARCH to modern deep learning models based on RNNs, CNNs, and Transformers. In recent years, the powerful sequential modeling capabilities of Transformers have led to the development of numerous Transformer-based models, with an emphasis on reducing the computational complexity of the original attention mechanism to enhance the feasibility of long-term time series forecasting. Notably, PatchTST represents a significant advancement by employing patch-level representations instead of processing individual records at each timestamp. This strategy has become a cornerstone, which has proven effective in capturing complex temporal patterns and improving forecasting accuracy."}, {"title": "A.1.2 FoOUNDATION MODELS FOR TIME SERIES FORECASTING", "content": "Building on the remarkable successes of large language models (LLMs) across various domains, several studies have attempted to adapt LLMs to the domain of time series forecasting. PromptCast transforms numerical input into prompts, and frames the forecasting task in a conversational manner. GPT4TS and Chronos finetune GPT-2 and T5 by directly training them on time series data for respective dataset or applying time series tokenization. Time-LLM is a reprogramming method that aligns input time series with text prototypes, and transforms them into prefix prompts to augment LLM's ability to reason with time series. TEMPO decomposes time series into trend, seasonal, and residual components and designs prompts for distribution adaptation. TEST employs contrastive learning to align time series representations with the language space of LLM. More related to our model is ISTS-PLM, which adapts LLM to handle irregularly sampled time series."}, {"title": "A.1.3 IRREGULAR TIME SERIES MODELING", "content": "In addition to regular time series, which have uniformly sampled data points, irregular time series are becoming increasingly prevalent due to widespread adoption of various sensing devices and recording practices. In recent years, substantial progress has been made in developing models to handle irregular time series, as demonstrated by works such as GRU-D, Latent-ODE, mTAN, and IVP-VAE. However, research on regular and irregular time series has largely progressed in parallel. A recent attempt to bridge this gap proposes a model capable of handling both regular and irregular time series, but it suffers from high memory consumption and inefficient performance. Moreover, there remains a scarcity of foundational models that can be seamlessly applied across both data types. Irregularity is considered in when building a foundation model, but the model was not applied to irregular time series forecasting."}, {"title": "\u0391.2 \u039d\u039f\u03a4ATIONS", "content": "An overview of notations used in this paper is given in Table 7."}, {"title": "A.3 DERIVATION OF THE LOSS FUNCTION", "content": "Given \\(X = \\{x_i\\}_{i=1}^M\\), \\(X^+ = \\{x_i\\}_{i=M+1}^{M+H}\\) and \\(X^* = \\{x_i\\}_{i=1}^{M+H}\\), following ELBO in VAE , we can obtain the standard formation of the objective function to maximize during training as:  \\(log p(X^*) = E_{z_0 \\sim q_\\phi(Z_0 | X^*)} [log p_\\theta (X^* | Z_0)] - D_{KL}(q_\\phi(Z_0 | X^*) || p(Z_0))\\)  where for the likelihood, we have:  \\(log p_\\theta (X^* | Z_0) = log p_\\theta (X X^+ | Z_0) = log p_\\theta (X | X^+, Z_0) + log p_\\theta (X^+ | Z_0)\\)  We further have:  \\(log p_\\theta (X^* | Z_0) \\geq log p_\\theta (X^+ | Z_0)\\)  By substituting this term into ELBO, we obtain:  \\(log p(X^*) \\geq E_{z_0 \\sim q_\\phi(Z_0 | X^*)} [log p_\\theta (X^+ | Z_0)] - D_{KL}(q_\\phi(Z_0 | X^*) || p(Z_0))\\)  Then the question follows, can we maximize \\(log p_\\theta (X^* | Z_0)\\) by maximizing \\(log p_\\theta (X^+ | Z_0)\\)? The answer is Yes, with some presumption. The fundamental assumption of all time series forecasting models is that future values \\(X^+\\) can be predicted based on past values \\(X\\). To achieve this, researchers often further assume that \\(X\\) and \\(X^+\\) follow the same distribution. This distribution can be explicit, such as a Gaussian distribution, which leads to the concept of stationarity, where statistical properties\u2014such as mean, variance, and autocovariance\u2014remain constant over time. Alternatively, the distribution can be implicit or unknown, in which case it can be learned by neural networks. In this work, we adopt this assumption. From this perspective, if a neural network is good at modeling \\(X^+\\), then it is good at modeling \\(X^*\\). Modeling either can be used to train a neural network. We chose to model \\(X^+\\) because it is more efficient. If we are about to model \\(X^*\\), based on the chain rule, we have:  \\(P(X_1 X_2 ... X_{M+N}) = P(X_{M+N} | X_1 ... X_{M+N-1}) ... P(X_3 | X_1 X_2)P(X_2 | X_1)P(X_1) \\\\ = \\prod_{i=1}^{M+N} P(x_i | X_1 ... X_{i-1}).\\)  However, if we about to model \\(X^+\\), based on the chain rule, we have:  \\(P(x_{M+1} ... x_{M+N} | X_{1:M}) = P(x_{M+N} | X_1 ... x_{M+N-1}) ... P(x_{M+1} | X_1 ... x_{M+N-1}) \\\\ = \\prod_{i=M+1}^{M+N} P(x_i | X_1 ... X_{i-1}).\\)  The latter is more efficient. Since attention modules are often computationally intensive, this efficiency advantage benefits the model during both training and inference."}, {"title": "A.4 ILLUSTRATION OF THE TRAINING PROCESS", "content": "Figure 4 shows a simplified illustration of the training process. Let's assume we have one variable in the time series, the patch length is 1, and observations are collected at irregular timestamps. In the supervised training or inference phase, each forward pass generates the next patch, which is then used as input to predict subsequent values until the entire forecast horizon is populated. During unsupervised pre-training, instead of using the full sequence, a sub-sequence is randomly selected at each iteration. First, a starting position is randomly determined, followed by the random selection of the sub-sequence length. The goal of pre-training is to predict the observations that follow the sub-sequence."}]}