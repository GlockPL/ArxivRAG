{"title": "IDEATOR: JAILBREAKING VLMS USING VLMS", "authors": ["Ruofan Wang", "Bo Wang", "Xingjun Ma", "Yu-Gang Jiang"], "abstract": "As large Vision-Language Models (VLMs) continue to gain prominence, ensuring their safety deployment in real-world applications has become a critical concern. Recently, significant research efforts have focused on evaluating the robustness of VLMs against jailbreak attacks. Due to challenges in obtaining multi-modal data, current studies often assess VLM robustness by generating adversarial or query-relevant images based on harmful text datasets. However, the jailbreak images generated this way exhibit certain limitations. Adversarial images require white-box access to the target VLM and are relatively easy to defend against, while query-relevant images must be linked to the target harmful content, limiting their diversity and effectiveness. In this paper, we propose a novel jailbreak method named IDEATOR, which autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is a VLM-based approach inspired by our conjecture that a VLM itself might be a powerful red team model for generating jailbreak prompts. Specifically, IDEATOR employs a VLM to generate jailbreak texts while leveraging a state-of-the-art diffusion model to create corresponding jailbreak images. Extensive experiments demonstrate the high effectiveness and transferability of IDEATOR. It successfully jailbreaks MiniGPT-4 with a 94% success rate and transfers seamlessly to LLaVA and InstructBLIP, achieving high success rates of 82% and 88%, respectively. IDEATOR uncovers previously un-recognized vulnerabilities in VLMs, calling for advanced safety mechanisms.", "sections": [{"title": "INTRODUCTION", "content": "With the popularity of OpenAI's GPT-4 (Achiam et al., 2023) and Google's Gemini (Team et al., 2023), increasing attention has been paid to large Vision-Language Models (VLMs) and their trust-worthy deployment in real-world applications. Although VLMs are built on well-aligned Large Language Models (LLMs), the integration of both textual and visual modalities introduces new vul-nerabilities. Recent studies have revealed that VLMs are extremely vulnerable to jailbreak attacks, where a malicious jailbreak prompt can trick the model to output harmful content that it would nor-mally be restricted from generating, making these risks a critical concern for VLMs' safe release.\nHowever, evaluating the robustness of VLMs against jailbreak attacks presents notable challenges. This is partly because multi-modal evaluation data remains relatively scarce when compared to the abundant textual corpora available in the LLM domain. As such, current VLM jailbreak methods (Qi et al., 2024; Wang et al., 2024a) rely on existing LLM jailbreak datasets to craft adversarial images or texts to evaluate VLM robustness. While these methods are effective, they require white-box access to the target model which is less practical in real-world scenarios. Moreover, the generated adversarial images and texts are either severely distorted or semantically meaningless, making them easily detectable by the underlying safety mechanism of VLMs (Nie et al., 2022; Zhang et al., 2023a). This has motivated the development of manually designed pipelines for generating jailbreak images, such as combining typographic attacks with query-relevant images (Liu et al., 2023b). One major limitation of manual methods is their heavy reliance on human-engineered processes, which restricts the flexibility and scalability of generating diverse jailbreak data for robustness evaluation.\nTo address the above limitations, in this paper, we propose the idea of \"using VLMs against VLMs\u201d, that is, leveraging a VLM to generate effective, transferable, and diverse multimodal jailbreak data."}, {"title": "RELATED WORK", "content": ""}, {"title": "LARGE VISION-LANGUAGE MODELS", "content": "Unlike traditional Large Language Models (LLMs), which are limited to processing textual data, large Vision-Language Models (VLMs) extend their capabilities to visual and textual modalities."}, {"title": "ATTACKS AGAINST MULTIMODAL MODELS", "content": "To attack multimodal models, Greshake et al. (2023) investigated the effectiveness of manually injecting deceptive text into input images. Gong et al. (2023) proposed FigStep, a method that converts the harmful text into images using typography to bypass the safety mechanisms. Liu et al. (2023b) demonstrated that VLMs can be easily compromised by query-relevant images, behaving as though the text query itself were malicious. Consequently, Liu et al. (2023b) introduced MM-SafetyBench, a comprehensive evaluation framework specifically designed to assess the safety and robustness of VLMs against such image-based manipulations. Bagdasaryan et al. (2023), Bailey et al. (2023), and Carlini et al. (2024) fixed attacker-chosen text as the target output and optimized adversarial images to increase its likelihood. Visual Adversarial Jailbreak Method (VAJM) (Qi et al., 2024) used a single adversarial image to universally jailbreak an aligned VLM. This adversarial attack goes beyond the narrow scope of the initial \"few-shot\" derogatory corpus used to optimize the adversarial example, leading to the generation of more harmful content. Wang et al. (2024a) proposed a comprehensive attack strategy targeting both text and image modalities in VLMs, aiming to exploit a broader range of vulnerabilities. Their dual optimization approach first generates an adversarial image prefix embedding toxic semantics, followed by an adversarial text suffix, which is jointly optimized with the image prefix to increase the likelihood of affirmative responses. Niu et al. (2024) adopted the same optimization objective as (Wang et al., 2024a)'s second stage to generate adversarial images on VLMs, and then transformed them into adversarial text suffixes for LLMs. While the aforementioned methods have demonstrated impressive results, they typically rely on manually crafted attack pipelines or white-box adversarial attacks. However, manually designing these pipelines is labor-intensive and limits attack diversity, whereas white-box adversarial attacks require full access to model parameters, making them less practical or easily detectable (Nie et al., 2022; Zhang et al., 2023a). In contrast, our method utilizes VLMs to generate diverse image-text pairs for black-box attacks, achieving high success rates and strong transferability."}, {"title": "PROPOSED ATTACK", "content": ""}, {"title": "THREAT MODEL", "content": "Attack Goals We consider multi-turn conversations between an attacker VLM and a victim VLM. The attacker VLM has access to the history of previous conversations, while the victim VLM can only view the current-turn conversation. The goal of attacker VLM is to bypass the victim's safety mechanisms and trigger harmful behaviors, e.g., generating unethical content or dangerous instruc-tions restricted by RHLF alignment or system prompts.\nAdversary Capabilities We assume the attacker has only black-box access to the victim VLM, representing real-world situations where interactions occur through external interfaces. The attacker"}, {"title": "IDEATOR", "content": "As illustrated in Figure 2, IDEATOR enables the attacker VLM to simulate an adversarial user in-teracting with the victim VLM. The attacker VLM generates a JSON response with three key com-ponents: analysis, image prompt, and text prompt. The analysis component evaluates the victim VLM's response and suggests refinements for the next attack iteration. The image and text prompts are crafted to guide the victim model in producing the desired harmful output while circumventing its safety mechanisms."}, {"title": "FORMALIZATION", "content": "Let $M_A$ represent the attacker VLM and $M_V$ the victim VLM. In the first round of the attack, the attacker model $M_A$ processes the jailbreak goal $G$ as text input and generates a structured JSON output $O_{json}^{(1)}$, which contains the adversarial text prompt $P_t^{(1)}$ and image prompt $P_i^{(1)}$. The process is formalized as:\n$O_{json}^{(1)} = M_A(\\emptyset_1, G) = \\{ \\emptyset_a, P_t^{(1)}, P_i^{(1)} \\},$\nwhere $\\emptyset_1$ indicates the absence of image input in the first round, and $\\emptyset_A$ denotes that the analysis field is not present in the initial JSON output.\nThe image prompt $P_i^{(1)}$ is processed by a text-to-image model (e.g., Stable Diffusion (Rombach et al., 2022)) to generate the corresponding image $I_1$. This image, along with the text prompt $P_t^{(1)}$, is then input into the victim model $M_V$, resulting in:\n$R_1 = M_V(I_1, P_t^{(1)}),$\nwhere $R_1$ denotes the victim model's response in the first round.\nIn subsequent rounds of the attack, the attacker model $M_A$ refines its attack strategy based on the prior response $R_{n-1}$ and the image $I_{n-1}$. By analyzing these inputs, it produces updated adversarial prompts for the next round. The iterative process in round $n$ is formalized as:\n$O_{json}^{(n)} = M_A(I_{n-1}, R_{n-1}) = \\{ A_n, P_t^{(n)}, P_i^{(n)} \\},$\nwhere $A_n$ represents the analysis of the victim model's previous response, $P_t^{(n)}$ and $P_i^{(n)}$ are the adversarial text and image prompts for the next round."}, {"title": "MODEL SELECTION", "content": "In IDEATOR, we adopt the Vicuna-13B (Chiang et al., 2023) version of MiniGPT-4 (Zhu et al., 2023) as the attacker VLM. The 13B variant provides greater model capacity and enhanced perfor-mance compared to its smaller 7B counterpart, making it more adept at simulating complex adver-sarial behaviors. Additionally, its open-source nature facilitates modifications to the system prompt and conversation template, enabling precise control over the model's behavior to more accurately simulate a malicious attacker. Furthermore, Vicuna is less conservative in generating responses com-pared to LLAMA (Touvron et al., 2023), increasing the likelihood of producing jailbreak prompts aligned with attack goals. In contrast, LLaMA tends to be more cautious and resistant to generating adversarial content (Chao et al., 2023). We also test customized versions of GPT-40 (Achiam et al., 2023) as the attacker VLM. Despite GPT-40's advanced capabilities, its red-teaming versions also tend to avoid generating malicious content to conduct attacks, likely due to the built-in safety mech-anisms. For the text-to-image generation model, we employ the latest Stable Diffusion 3 Medium (Rombach et al., 2022) to ensure that the generated images are of high quality and semantically aligned with the input prompts."}, {"title": "PROMPT DESIGN", "content": "System Prompt To allow the attacker VLM to simulate adversarial behavior without specific train-ing on red team datasets, we carefully design a system prompt and conversation template. As shown in Figure 3, our system prompt comprises three key components. First, we configure the attacker model as a red teaming assistant, tasked with crafting adversarial jailbreak prompts that could po-tentially mislead the victim VLM to generate unethical outputs. Second, we force the assistant's output to follow a structured JSON format comprising three elements: analysis, image prompt, and text prompt. Lastly, we leverage the concept of in-context learning (Brown, 2020) to guide the red team model to generate more effective adversarial JSON outputs by showing relevant examples.\nChain-of-Thought Reasoning The analysis component in the JSON output enables iterative op-timization of jailbreak prompts based on prior victim responses. This component evaluates the effectiveness of previous attacks and facilitates Chain-of-Thought (CoT) reasoning (Wang et al.,"}, {"title": "BREADTH-DEPTH EXPLORATION", "content": "Here, we further propose a breath-depth exploration strategy to find more effective jailbreak at-tacks for a comprehensive safety assessment of the victim model. The iterative IDEATOR attack introduced above primarily focuses on refining a particular attack strategy based on continuous vic-tim feedback to fully exploit its attack potential. Building upon the iterative attack, our breadth exploration strategy launches diverse attack strategies to help identify a broad spectrum of potential vulnerabilities. This exploration helps to uncover new threats and prevents over-reliance on one specific strategy. By implementing this integrated strategy, our IDEATOR attack becomes more extensive and flexible. The detailed attack procedure of IDEATOR is described in Algorithm 1."}, {"title": "EXPERIMENTS", "content": "In this section, we first describe our experimental setup and then evaluate the effectiveness of our attack on two safety benchmark datasets and its transferability to different VLMs. We also visualize the generated multimodal jailbreak prompts along with an empirical understanding of the generation power of our IDEATOR. An ablation study for our attacks is also provided."}, {"title": "EXPERIMENTAL SETUP", "content": "Safety Datasets We conduct our experiments on two safety datasets: Advbench (Zou et al., 2023) and VAJM (Qi et al., 2024). For Advbench, we use its harmful behaviors subset, which consists of 520 harmful goals, mostly suggestions or instructions that promote dangerous or illegal activities, as well as other types of harmful content. We randomly select 100 goals from this dataset as the jailbreak targets to test our attack method. We do not use the full dataset for testing as a portion of the data has to be reserved for the adversarial optimization of white-box attack methods (Zou et al., 2023; Qi et al., 2024; Wang et al., 2024a). Note that our IDEATOR is a training-free method, and thus does not need any harmful goals to train or optimize. Additionally, we extend our assessment"}, {"title": "COMPARISON WITH STATE-OF-THE-ART ATTACKS", "content": "We first compare our IDEATOR with state-of-the-art jailbreak attacks on the two safety datasets. The following jailbreak attacks are considered as our baselines. Greedy Coordinate Gradient (GCG) (Zou et al., 2023) is a text-based attack developed for LLMs. It optimizes adversarial text suffixes to enhance the likelihood of generating affirmative responses, thereby facilitating the jailbreak of LLMs. VAJM (Qi et al., 2024) optimizes adversarial images to maximize the probability of generat-ing harmful content, thereby enabling the jailbreak of VLMs using a few-shot corpus. UMK (Wang et al., 2024a) combines both text and image-based methodologies, providing a comprehensive mul-timodal attack strategy. MM-SafetyBench (Liu et al., 2023b) is a black-box attack method that generates query-relevant images coupled with text rephrasing. We reproduce GCG, VAJM, UMK, and MM-SafetyBench using their official implementations. Additionally, we implement GCG-V, a vision adaptation of GCG proposed in UMK, to facilitate a more comprehensive comparison."}, {"title": "TRANSFER TO ATTACK OTHER VLMS", "content": "In addition to performing black-box attacks on MiniGPT-4 (Zhu et al., 2023), we also transfer the jailbreak prompts generated based on MiniGPT-4 and the Advbench dataset to other VLMs, in-cluding InstructBLIP (Vicuna) (Dai et al., 2023) and LLaVA (LLaMA-2-Chat) (Liu et al., 2024). Given the limited transferability of the adversarial prompts generated by the white-box methods, our analysis in Table 3 focuses exclusively on black-box attacks. Despite the strong alignment of the LLaMA-2-based model (Touvron et al., 2023), it remains susceptible to our transfer attacks, i.e., our IDEATOR achieves a high ASR of 82.0% against LLaVA (LLaMA-2-Chat). Surprisingly, the transferred jailbreak samples are even more effective on InstructBLIP (Vicuna), with an ASR of 88.0%. Although MM-SafetyBench crafts attacks without targeting specific victim models, its ASRs on InstructBLIP and LLaVA are considerably lower than our attack, which exhibits an ASR of 46.0% and 29.0% on LLaVA and InstructBLIP, respectively. These results highlight the remarkable transferability and effectiveness of the jailbreak image-text pairs generated by our IDEATOR."}, {"title": "VISUALIZATION AND EMPIRICAL UNDERSTANDING", "content": "Visualization Figure 4 illustrates a few example jailbreak images generated by our IDEATOR. In the left panel, the vertical axis represents the breadth of our attack exploration, showcasing a variety"}, {"title": "ABLATION STUDIES", "content": "We first investigate different configurations of breadth and depth in IDEATOR and present the results in Table 4. The findings indicate that increasing either breadth or depth results in a higher ASR, with the combination of both being the most effective. For instance, at $N_{width} = 1$ and $N_{depth} = 1$, the ASR is 45.0%. However, when both hyperparameters are increased to $N_{width} = 7$ and $N_{depth} = 3$, the ASR rises to 94.0%. This suggests that broader and deeper explo-ration increases the likelihood of bypassing the victim model's safety mechanisms. This empirical result supports our hypothesis outlined in Section 4.4. Specifically, increasing exploration breadth and depth allows $A_{Nbreadth, Ndepth}$ to progressively approach the theoretical limit $A_{IDEATOR}$, as more di-verse and effective adversarial strategies are identified. This confirms that IDEATOR's exploration strategy effectively expands the attack space, resulting in both an improved ASR and a wider range of attack strategies, consistent with the empirical findings presented in Section 4.4.\nWhich modality is more effective: textual or visual? Here, we conduct an ablation study to evaluate the effectiveness of our generated multimodal attacks, examining text (\"Adv Text\"), im-ages (\"Adv Img\"), and their combination (\"Adv Img + Adv Text\") separately. \"Adv Text\" typi-cally employs strategies such as emotional manipulation to elicit harmful outputs, while \"Adv Img\" leverages attack images to provoke harmful responses. The overall ASR and the average number of queries required for a successful attack are presented in Table 5. Comparing \"Adv Img\" with \"Adv Text,\" we find that image attacks require fewer queries but are generally less effective than text attacks. Notably, pure text attacks are more likely to be rejected on crime-related topics, likely due to the safety alignment of the base LLM. In contrast, pure image attacks are less effective at persuading the model to generate harmful responses related to hate speech or self-harm. Overall, the combined multimodal attacks achieve the highest ASR with the fewest queries, underscoring the importance of utilizing both modalities."}, {"title": "CONCLUSION", "content": "In this paper, we investigated the vulnerabilities of large Vision-Language Models (VLMs) to jail-break attacks and introduced a novel black-box jailbreak method called IDEATOR. Unlike existing approaches, IDEATOR transforms a VLM into a jailbreak agent through a carefully designed system prompt and conversation template. The attacking VLM is further integrated with a state-of-the-art diffusion model to autonomously generate multimodal jailbreak prompts. By employing a breath-depth exploration strategy, IDEATOR iteratively refines its attack methods, effectively probing a wide range of vulnerabilities in the target VLMs. Importantly, IDEATOR addresses the limitations of current jailbreak techniques, which often depend on white-box access or manual engineering, by autonomously generating diverse image-text pairs. Experimental results demonstrate the effec-tiveness and transferability of our IDEATOR attack. Notably, IDEATOR successfully jailbreaks MiniGPT-4 with a 94% success rate and achieves high transfer success rates of 82% and 88% on LLaVA and InstructBLIP, respectively. These findings underscore the potential of using VLMs to identify and exploit multimodal vulnerabilities, establishing IDEATOR as a powerful tool for red teaming and robustness evaluation in VLMs. As a future direction, we plan to release a comprehen-sive benchmark dataset to support further research in this area, targeting a broader range of VLM architectures, utilizing diverse system prompts, and exploring an extensive array of attack objec-tives. Additionally, we will develop a specialized red-team model that will be fine-tuned on existing jailbreak techniques and further enhanced through reinforcement learning for more sophisticated and effective attacks."}]}