{"title": "MCU-MixQ: A HW/SW Co-optimized Mixed-precision Neural Network Design Framework for MCUs", "authors": ["Junfeng Gong", "Cheng Liu", "Long Cheng", "Huawei Li", "Xiaowei Li"], "abstract": "Mixed-precision neural network (MPNN) that utilizes just enough data width for the neural network processing is an effective approach to meet the stringent resources constraints including memory and computing of MCUs. Nevertheless, there is still a lack of sub-byte and mixed-precision SIMD operations in MCU-class ISA and the limited computing capability of MCUs remains underutilized, which further aggravates the computing bound encountered in neural network processing. As a result, the benefits of MPNNs cannot be fully unleashed. In this work, we propose to pack multiple low-bitwidth arithmetic operations within a single instruction multiple data (SIMD) instructions in typical MCUs, and then develop an efficient convolution operator by exploring both the data parallelism and computing parallelism in convolution along with the proposed SIMD packing. Finally, we further leverage Neural Architecture Search (NAS) to build a HW/SW co-designed MPNN design framework, namely MCU-MixQ. This framework can optimize both the MPNN quantization and MPNN implementation efficiency, striking an optimized balance between neural network performance and accuracy. According to our experiment results, MCU-MixQ achieves 2.1\u00d7 and 1.4x speedup over CMix-NN and MCUNet respectively under the same resource constraints.", "sections": [{"title": "I. INTRODUCTION", "content": "The application of Artificial intelligence (AI) has become prevalent in typical Internet of Things (IoT) scenarios such as health monitoring, mechanical equipment fault diagnosis, and industrial automation. These applications commonly rely on microcontrollers (MCUs) known for their ultra-low power consumption and cost as the central processing units. Yet, AI especially deep learning models demands significant computational and memory resources, posing great challenges to its deployment on resource-limited MCUs. While embedding deep learning accelerators within MCUs is a conceivable strategy, it substantially escalates chip costs and energy consumption, thereby constraining its use in IoTs. Consequently, there is considerable interest in deploying lightweight deep learning models on MCUs to attain efficient inference and empower the intelligence of things [1]\u2013[3]."}, {"title": "", "content": "The two major challenges of deploying deep learning on MCUs are insufficient computational resources and limited memory capacity. For the memory issue, initiatives such as MCUNet [4], [5] optimizes the memory scheduling during the inference from the perspective of the overall model topology. MCUNetV2 [6] further decomposes deep learning computations into smaller block computations to reduce the memory requirements. For the computation issue, neural network processing is known to be computing bound for MCUs and many efforts have been devoted to alleviate this challenge. Mixed-precision neural network (MPNN) quantization that can reduce both the computational and memory requirements of the models is a straightforward yet effective approach [7]. However, MCUs generally lack support for sub-byte instructions [12] and using primitive instructions for neural network processing directly would waste the limited computational resources in MCUs and aggravate the computing bound problem.\nPrior works [12], [14] investigated the use of SIMD instructions to accelerate low-bitwidth convolution operations, but they generally spread the low-bitwidth operations across the different SIMD lanes and the number of low-bitwidth operations packed in the SIMD is limited to the number of SIMD lanes. Essentially, they fail to make full use of the SIMD computing fabric because each SIMD lane is actually underutilized. In fact, the approach of packing low-bitwidth operations into high-bitwidth operations has also been explored for integer instructions of CPUs [8] and primitive DSPs of FPGAs [8], [36]\u2013[38], but these approaches cannot be applied directly on SIMD instructions of MCUs. For instance, the shifting required by packing is almost free on FPGAs, but it is non-trivial for MCUs and inappropriate packing can even result in considerable performance penalty.\nTo achieve efficient packing on MCUs, we propose to pack low-bitwidth operations with the granularity of the SIMD lanes such that each SIMD lane can be fully utilized. In addition, SIMD fabric usually enables different lane configurations, so we can also configure the SIMD lane sizes to suit the different bitwidth requirements of the convolution and achieve higher SIMD utilization. Furthermore, we rearrange the packing order of the low-bit operands to reduce the auxiliary instructions of packing and lower the packing overhead accordingly."}, {"title": "", "content": "As the bitwidth of the neural network models affects both the model accuracy and model implementation efficiency on MCUs, optimizing the model quantization and model implementation independently will lead to sub optimal results. In this work, we leverage NAS to take the model quantization and implementation efficiency of low-bitwidth operators into consideration at the same time and co-optimize the model accuracy and model performance. As mentioned, packing on MCUs requires additional shifting operations and the implementation efficiency of different bitwidth is not proportional to the bitwidth. To this end, we construct a performance model for neural network operators of low bitwidth such that the influence of different quantization on MCU implementation efficiency can be predicted immediately for the co-optimization. Finally, we have the SIMD-based convolution operator supporting various sub-byte operations added to TinyEngine [4] such that MPNNs can be deployed on top of TinyEngine efficiently and benefit the memory optimizations in TinyEngine.\nThe main contributions of this work can be summarized as follows.\n\n\u2022 To enable efficient MPNN processing on MCUs, we propose an efficient low-bitwidth operation packing algorithm to make full into the SIMD fabric of MCUs. Specifically, we propose to have multiple low-bitwidth operations packed in the granularity of SIMD lanes and adapt the SIMD lane sizes to fit the convolution bitwidth requirements at the same time, achieving significantly higher packing efficiency compared to prior SIMD packing strategies.\n\n\u2022 Considering that the bitwidth of neural network models impacts both model accuracy and implementation efficiency, we leverage NAS to perform a packing-aware quantization for MCUs, thus co-optimizing model accuracy and performance concurrently. This framework is open sourced on GitHub.\n\n\u2022 With both the low-bitwidth packing and packing-aware NAS, we establish a HW/SW co-optimization framework, MCU-MixQ, for efficient neural network implementation on MCUs. According to our experiments on a set of neural network models, MCU-MixQ achieves 2.1\u00d7 and 1.5x performance speedup over CMix-NN and MCUNet on average respectively under the same resource and accuracy constraints."}, {"title": "II. RELATED WORK", "content": "To achieve efficient deep learning on MCUs with limited computing resources, prior work have proposed various optimization strategies from distinct angles. Early work mainly investigated mixed-precision quantization that explores smaller bitwidth for each different neural network layer to reduce both the compute and memory requirements. Some approaches concentrate on optimizing fundamental neural network operators by employing low-bitwidth operation packing and SIMD optimization, thereby increasing the implementation efficiency of the major neural network operators. Some of the research focused on HW/SW co-optimization, simultaneously considering operator implementation efficiency and model quantization. Additionally, there are also strategies [4] [6] targeting at memory optimization via scheduling and patching to accommodate larger deep learning models on MCUs with minimal performance loss. While the memory optimizations are generally orthogonal to the computing optimization, we mainly illustrate the computing optimization approaches in the rest of this section."}, {"title": "A. Mixed-Precision Nerual Network Quantization", "content": "Quantization is an established method for model compression, effectively reducing both computing and memory overhead. Currently, unified-precision quantization method has achieved remarkable success [26], even achieving lossless precision at 8-bit precision after fine-tuning [32] or other novel techniques [28]. Nevertheless, since model layers differ in their sensitivity to compression, conventional uniform precision quantization methods cannot realize optimal results. Several studies [18], [19], [22] have introduced various metrics to assess the sensitivity of different model layers, guiding the configuration of quantization schemes to strike a balance between accuracy and quantization bitwidth. It is a generic computing optimization approach for various computing engines including CPUs [8], FPGAs [8], [27], and GPUs [30], [31], and can be particularly beneficial to MCUs with rather limited computing resources and energy budgets [29]."}, {"title": "B. Network Operator Optimizations", "content": "MPNNs that can reduce both the computing and memory requirements without compromising the model accuracy fit well with MCUs with limited hardware resources. Some recent works proposed to develop customized computing fabrics such as dot-product units and vector processing units to support convolution with lower bitwidth [13] [24] [25] in MPNNs, but there is still a lack of native low-bitwidth operator, particularly under 8 bit, in mainstream commodity MCUs. Implementing low-bitwidth deep learning models with primitive MCU arithmetic instructions directly leads to underutilization of the limited computing resources. To address this, CMSIS-NN [14] leverages SIMD instructions to optimize typical fixed point neural network operations like int8. CMix-NN [12] investigates the use of SIMD instructions for 2 bit, 4 bit, and 8 bit convolution kernels for efficient MPNN inference. Hikconv [8] presents a more general approach to pack arbitrary low-bitwidth convolution kernels on primitive integer arithmetic instructions in MCUs. The authors in [21] explored the computing redundancy from inputs to improve the convolution performance."}, {"title": "C. HW/SW Co-Optimization", "content": "Despite of the improved computing and memory efficiency of MPNNs, the performance deployed on the target computing engines can vary substantially because of the implementation efficiency variations of the low-bitwidth neural network operations. To address the problem, hardware-aware quantization approaches have been explored to suit the different computing fabrics. While NAS [20] [9] provides a unified design framework to search through the model design space for multi-objective optimization, it has been widely adopted to co-optimize the accuracy and performance of MPNNs in prior works [10]. TinyEngine [4] explored the model architecture along with the memory limitation to ensure effective neural network processing on MUCs.\nIn summary, recent studies have shown considerable promise for leveraging neural network redundancies through mixed-precision quantization and developing low-bitwidth convolution on current MCUs. However, dissociating quantization from operator optimization-targeting accuracy and performance separately-may result in suboptimal results. While several HW/SW co-optimization methods exist, there is still a lack of study on SIMD optimization for MPNNS, and prior efforts still fail to unleash the computing potential of MCUs. Innovative approaches that can both fully exploit MCU computing resources and concurrently compress model redundancies through quantization are highly demanded to perform neural network processing efficiently on MCUs."}, {"title": "III. OVERALL DESIGN FRAMEWORK", "content": "In this work, we present an MPNN design framework for MCUs, namely MCU-MixQ, as depicted in Fig. 1. This framework comprises SIMD-based low-bitwidth neural network operators and a hardware-aware quantization explorer based on NAS. The low-bitwidth neural network operator has multiple low-bitwidth operations packed into SIMD fabric, which makes full use of the computing resources in MCUS to mitigate the computing bottleneck. The hardware-aware quantization explorer is employed to reduce the data width of neural network models as much as possible, allowing for reduced computing and memory resource usage while retaining inference accuracy. Given that the implementation efficiency of low-bitwidth neural network operators also varies with the bitwidth configurations and significantly impacts network performance, the quantization explorer must be aware of the operator's implementation efficiency. This is achieved by incorporating a performance loss component alongside the standard accuracy loss component, as illustrated in Fig. 1."}, {"title": "A. Low-bitwidth Network Operators", "content": "To fully harness the computational resources, particularly SIMD, in MCUs for the acceleration of neural network processing, we seek to pack multiple low-bitwidth operations in a single SIMD fabric. Unlike prior SIMD packing [8], [33], [34] that fits each low-bitwidth operation to an independent SIMD lane, we propose to conduct the packing in each SIMD lane such that the packing is not limited to the minimum lane size i.e. 8-bit. Notably, we can adjust the SIMD lane sizes to the bitwidth requirements of the convolution and make best use of the SIMD fabric in MCUS. In addition, the packing typically requires shifting operations which also takes non-trivial overhead and affects the resulting performance of the network operators. To alleviate the packing overhead, we further propose a data reordering mechanism to reduce the number of auxiliary instructions required by the packing. The overall SIMD-based low-bitwidth convolution packing algorithm, namely SLBC, will produce an optimized sub-byte convolution operator, which will be utilized to sustain the execution of MPNNs. SLBC will be detailed in Section IV."}, {"title": "B. Hardware-Aware MPNN Quantization", "content": "Motivated by prior NAS-based quantization works [10], [15]\u2013[17], [23], we leverage a differentiable NAS to achieve hardware-aware quantization and co-optimize the model accuracy and performance. It starts with a pre-trained floating point model and sets the possible quantization data width as the initial design space of NAS. Then, it creates a quantization super-net to cover all the possible quantization configurations. Each layer of the target model to be quantized will be replaced with a mixed kernel composed of multiple weighted branches and each branch represents a specific quantization option. Given the quantization search space $Q = \\{q_1, q_2, ..., q_n\\}$, the quantization super-net can be denoted as $f(Q)$, while a sub-net sampled from Q is $f(q_i)$. The optimization goal is to search for a quantization sub-net $q^*$ to maximize the accuracy and minimize the latency while fulfilling the design constraints such as model sizes.\nWith the super-net architecture, we can start the super-net training and have two loss components included to take both the model accuracy and model performance of different quantization setups into consideration in training as shown in Eq. 2 and Eq. 1. Particularly, the performance loss component mainly characterizes the network performance when deployed on MCUs with the proposed SLBC packing approach. Since it is expensive to deploy the network with various quantization configurations on MCUs and extract the performance with realistic deployment, we have a simplified yet precise performance model for the NAS. The model is closely coupled with the SLBC packing and it will be illustrated in Section IV as well. After the quantization optimization, MCU-MixQ performs quantization aware training (QAT) on the selected mixed-precision model and the model will be deployed on MCUs, completing the entire workflow."}, {"title": "", "content": "$Loss(\\alpha_w, \\alpha_\\alpha) = \\sum_{l=1}^{L} C^l$\\n(1)\n$Loss(\\alpha_w, \\alpha_\\alpha) = Loss_{acc}(\\alpha_w, \\alpha_\\alpha) + Loss_{comp}(\\alpha_\\omega, \\alpha_\\alpha)$ (2)\nFinally, we deploy the obtained MPNN on MCUs with TinyEngine [4] which is an memory-efficient inference framework designed for MCUs. It provides all the major functionalities required to deploy a high-level model on MCUs. Particularly, it optimizes the memory usage of the model during the process of code generation and manages the memory scheduling to ensure on-demand parameter loading. These techniques prevent the out of memory issues during inference while minimizing its influence on the performance. While TinyEngine does not support sub-byte convolution operators, we have SLBC integrated to enable the deployment of MPNNS on TinyEngine. The revised TinyEngine in combination with the proposed quantization explorer constitutes the comprehensive HW/SW co-designed MPNN framework for MCUS known as MCU-MixQ."}, {"title": "IV. LOW-BITWIDTH NETWORK OPERATOR OPTIMIZATIONS", "content": "As a compute-intensive operator, convolution is heavily bounded by the computing capability of MCUs due to the lack of massively parallel computing fabrics. Therefore, optimizing the convolution, which is the major kernel of neural networks, is critical to the neural network performance on MCUs."}, {"title": "A. SIMD-based Low-bitwidth Convolution", "content": "Considering the mathematical equivalence of polynomial multiplication and convolution operation, for an st-bit sequence s and a k-bit convolution kernel k, we can pack multiple low-bitwidth elements of s and k into one wider hardware unit $R_1$ and $R_2$ which can be represented with the following polynomial forms."}, {"title": "", "content": "$R_1 = \\sum_{i=0}^{N_s-1} s[i] \\cdot 2^{iG_b}$ (3)\n$R_2 = \\sum_{j=0}^{N_k-1} k[j] \\cdot 2^{jS_b}$ (4)\nWith the packing, the product P of a high-precision multiplier can be simplified to Equation 5 according to the rule of polynomial multiplication."}, {"title": "", "content": "$P = R_1 \\times R_2$\n$= (\\sum_{i=0}^{N_s-1} s[i] \\cdot 2^{iG_b}) \\cdot (\\sum_{j=0}^{N_k-1} k[j] \\cdot 2^{jS_b})$\n$= \\sum_{k=0}^{N_s+N_k-2} (\\sum_{i+j=k} s[i] \\cdot k[j] \\cdot 2^{kS_b})$ (5)\nAccording to the definition of convolution, the application of the $N_k$-kernel k to a $N_s$-element sequence s also yields $N_s + N_k-1$ elements and the nth element of convolution sequence y can be represented as:\n$y[n] = \\sum_{m=0}^{N_k-1} s[n - m] \\cdot k[m]$\n$= \\sum_{i+j=n} s[i][j]$ (6)\nAccording to Eq. 5 and Eq. 6, it is evident that the multiplication product P is composed of convolution sequence y, each of which has been left-shifted by the corresponding number of bits with Eq. 7. As a result, each element of convolution sequence can be segmented from P through bit operations. By utilizing a single multiplication instruction along with multiple bit-wise instructions for packing and segmentation, the overhead of the convolution can be significantly reduced compared to naive implementation."}, {"title": "", "content": "$P = \\sum_{k=0}^{N_s+N_k-2} y[k] \\cdot 2^k$ (7)\nThe above analysis assumes that the computation unit is long enough to accommodate all elements from the sequence and kernel. In practice, most commodity MCUs like Cortex-M incorporate SIMD instructions to enhance parallel computing capabilities. Thus, we investigate the use of the SIMD instructions for efficient low-bitwidth convolution and propose SLBC, a SIMD low-bitwidth convolution optimized for MCUs. The detailed execution flow of SLBC is presented in Algorithm 1. With SLBC, multiple multiply and add operations in a convolution operator can be substituted with a single SIMD multiplication instruction and bit-wise operations. SLBC mainly consists of three processing stages including packing, SIMD multiplication, and SIMD segmentation as illustrated in Fig. 2."}, {"title": "", "content": "$v_s = \\sum_{l=0}^{N_l-1} (2^{lG_b} \\cdot (\\sum_{i=0}^{N-1} s[lN_s + i] \\cdot 2^{iS_b}))$ (8)\n$v_k = \\sum_{l=0}^{N_l-1} (2^{lG_b} \\cdot (\\sum_{i=0}^{N_k-1} k[i] \\cdot 2^{iS_b}))$ (9)\nIn SIMD multiplication stage, the packed data vs and vk multiply with an SIMD instruction and the product is presented in Eq. 10. Note that $\\cdot$ denotes the SIMD multiplication and $N_l$ denotes the total number of SIMD lanes. After the SIMD multiplication, the convolution sequence is already stored in the output vector, which means that we can replace more ADD and MUL i.e. single instruction single data (SISD) instructions with one SIMD instruction.\n$v_p = v_s \\cdot v_k$\n$= \\sum_{l=0}^{N_l-1} (2^{lG_b} \\cdot (\\sum_{i=0}^{N-1} s[lN_s + i] \\cdot 2^{iS_b}))$\n$= \\sum_{l=0}^{N_l-1} (2^{lG_b} \\cdot (\\sum_{i=0}^{N_k-1} k[i] \\cdot 2^{iS_b}))$\n$= \\sum_{l=0}^{N_l-1} (2^{G_b} \\cdot \\sum_{k=0}^{N_s+N_k-2} (\\sum_{i+j=k} (v_s[lN_s + i] \\cdot v_k[j] \\cdot 2^{kG_b})))$ (10)\nIn segmentation stage, we notice that SLBC can be viewed as multiple parallel packing tasks. As shown in Eq. 11, the last element in a lane will be combined with the first element of the next lane to form an element of the convolution sequence, while the other data in each SIMD lane also become elements of the convolution sequence. Note that $v_p$ represents a 2-D array while the first dimension represents the lane index and the second index represents the element position in each lane. Finally, SIMD bit-wise operations is utilized to extract the convolution sequence from the output vector.\n\n\n$y[i] = \\begin{cases}v_p[l][N_s + N_k - 2] + v_p[l + 1][0], i \\neq 0, i \\neq N - 1,k = 0\\\\v_p[l][k], others\\end{cases}$ (11)"}, {"title": "B. Enhance Locality Through Reordering", "content": "Despite the packing efficiency, SLBC requires extra bit operations such as LSR to extract convolution elements from the output vector and the overhead of these bit operations is non-trivial. Inspired by ULPPACK [11] that utilizes local accumulation to combine multiple bit operations together and reduces segmentation overhead substantially, we propose a new reordering algorithm for SLBC to improve the register reuse during packing, as shown in Theorem IV.1."}, {"title": "Theorem IV.1.", "content": "For SIMD registers with L lanes, each lane can accommodate N low-bitwidth elements, a group of N\u00d7 L\u00b2 elements will be reordered and packed within L SIMD registers. For the yth lane of the xth SIMD register, it will be packed into the yth position of the xth one.\nTo illustrate the reordering algorithm, we have two simplified packing examples presented in Fig. 3 and Fig. 4. Suppose each SIMD register has 2 lanes and each lane can pack 2 elements. Assume that the kernel size is also 2 so that the entire kernel can be fully packed into one SIMD lane. Fig. 3 shows the processing of the naive packing proposed in Algorithm 2. Since the entire kernel can be packed into one lane and convolution sequence needs to be segmented, two different segmentations of sequence will be packed into one SIMD register and meanwhile the entire kernel will be packed into each lane of the same SIMD register to perform an optimized convolution through SIMD multiplication. But according to the details of SLBC, the overlapping part are distributed in adjacent lanes within the same SIMD register. According to the principle of SIMD, the overlapping part can not be utilized through shift operation. As a result, the overlapping part needs to be segmented separately from adjacent lanes, thus leading to unnecessary overhead in bit-wise operation instructions.\nIn order to fully utilize the overlapping portions and merge multiple segmentation operations together, the arrangement order of elements has been modified so that the overlapping portions appear in adjacent SIMD registers rather than between adjacent lanes within the same one. The specific method of reordered packing SLBC is shown in Fig. 4. It can be observed that since the order of data rearrangement has been changed, there exists overlap between the result of adjacent iterations. For SIMD registers with L lanes that store the product results, which represent the convolution sequence after packing, the element contained in the boundary lane cannot form one complete element of the convolution independently. Instead, it needs to be added to the element held in the first lane in the adjacent SIMD register to become an real element. In other words, the elements located in boundary position require an additional segmentation operation. Fig. 4 illustrates the packing positions of these two elements in SIMD registers."}, {"title": "", "content": "However, After rearranging the packing order of elements, the boundary elements to jointly form one complete convolution element are located in corresponding lanes of adjacent SIMD registers. Therefore, these two SIMD registers can be accumulated after performing parallel shifting operations, which eliminates the need for additional splitting overhead. For the configuration discussed above, L segmentation operations will be eliminated for every N \u00d7 L \u00d7 L elements, thus reducing segmentation overhead to $\\frac{1}{L}$ of the original count.\nThe complete algorithm for reordered SLBC is illustrated in Algorithm.2. During packing stage, N \u00d7 L \u00d7 L elements will be considered as a group to be packed into N SIMD registers. After the completion of the multiplication operation, the convolution elements squeezed in the register will not be segmented immediately. Instead, it will be right-shifted and added to the local accumulation after each round of multiplication. After the group of multiplications is completed, the actual elements will be segmented from the local accumulation. In the subsequent experimental phase, we conducted an ablation study on reordered SLBC and SLBC within an end-to-end framework to validate the effectiveness of the improved method in reducing segmentation overhead."}, {"title": "C. Adaptive SIMD Packing", "content": "SLBC has low-bitwidth operations packed into each SIMD lane independently, but the packing efficiency depends on both the SIMD lane size and the operation bitwidth to be packed. Since SIMD usually allows different lane sizes, we can adjust the SIMD lane size to fit the convolution bitwidth for higher SIMD utilization. For each convolution of an MPNN, we adaptively decide the optimized packing and SIMD lane sizes at compilation time to ensures optimized MPNN performance."}, {"title": "D. Packing Performance Prediction", "content": "The proposed HW/SW co-design framework MUC-MixQ requires a large number of performance evaluation of MPNNs with different quantization setups which can be too expensive for evaluation with realistic deployment, so we further build a performance model for this purpose. As mentioned, the low-bitwidth convolution implemented with SLBC includes both SIMD MUL instruction and bitwise operations. Considering the varied execution time of the different types of instructions, we use SISD instructions as the calibration metric and align SIMD MUL instruction and SISD bit operations with it. Specifically, as shown in Eq. 12, the complexity of SISD instructions $C_{SISD}$ is roughly proportional to the number of SISD accumulation and multiplication operations where \u03b1 and \u03b2 refers to the proportion coefficients and they can be obtained with experiments."}, {"title": "", "content": "$C = C_{SISD} + \\alpha C_{SIMD} + \\beta C_{bit}$ (12)"}, {"title": "V. EXPERIMENT", "content": "To showcase the outstanding performance of MCU-MixQ on MCUs, we conducted experiments on two datasets: Visual Wake Word (VWW) and CIFAR-10. VWW is a vision-oriented dataset specifically designed to determine the presence or absence of a person in an image. CIFAR-10 is a widely adopted benchmark for image classification tasks. For the hardware platform, we selected ARM Cortex-M7 microcontroller STM32F746, which is equipped with 320kB of SRAM and 1MB of Flash memory. All the latency measurement is obtained at a clock frequency of 216MHz."}, {"title": "A. End-to-End Performance Evaluation", "content": "We have the neural network benchmark implemented on the target hardware platform with CMix-NN [12], WPC&DDD [35], TinyEngine [4] and the proposed MCU-MixQ respectively. Note that CMix-NN and WPC&DDD only supports three different bitwidth setups i.e. 2bit, 4bit, and 8bit, TinyEngine only supports 8bit while MCU-MixQ supports all the bitwidth between 2bit and 8bit. Given the same model accuracy constraint, we compared the end-to-end performance of the resulting neural network models and the comparison is summarized in Table I. It can be observed that MCU-MixQ achieves the best performance and outperforms all the other solutions. This can be attributed to multi-folded reasons including the more efficient low-bitwidth convolution optimization and more flexible quantization, as well as the HW/SW co-optimization. They will be analyzed in detail in the rest of the experiments. Moreover, MCU-MixQ takes advantage of the memory optimization provided by TinyEngine, so the peak memory usage is also reduced. On the other hand, we notice that CMix-NN and WPC&DDD with more flexible quantization setups show even lower performance than TinyEngine with fixed int8 quantization. This is mainly attributed to other optimization techniques introduced by TinyEngine, model-adaptive memory scheduling and computation kernel specialization for example."}, {"title": "B. SLBC Efficiency Evaluation", "content": "First of all, we compare SLBC with other convolution kernels. In order to showcase its efficacy on low-bit convolutions, we compare SLBC with naive convolution, SIMD convolution and CMix-NN. SIMD convolution uses SIMD instructions to accelerate convolution without other optimization method. Due to the lack of support for sub-byte in naive and SIMD convolution, the latency of the convolution operator does not change when executing with different bitwidths under 8 bits. Fig 5 illustrates the speedups of SLBC under different bitwidths over the two methods. According to the experimental results, SLBC achieves an average speedup of 4\u00d7 and 2\u00d7 over naive and SIMD convolution seperately.\nCMix-NN is a flexible mixed-precision inference library, which supports any combination of 2, 4, 8 bitwidth. It compresses low-bitwidth data for storage and simultaneously constructs vector instructions using masks in convolution. In order to demonstrate the superiority of SLBC over CMix-NN in terms of hardware resource utilization, we compared the theoretical throughput of the two methods. More specifically, Fig 6 presents the acceleration ratios for different bitwidth combinations, which represents the equivalent ratio of operations performed by the one SIMD instruction. According to Fig 6, SLBC can achieve up to 1.5\u00d7 speedup over CMix-NN in most quantization combination."}, {"title": "C. HW/SW Co-optimization Evaluation", "content": "In order to evaluate our hardware-aware quantization explorer, we choose EdMIPs as baseline, and utilize them to perform a search for optimal model quantization configurations. EdMIPs estimates complexity by using MACs as a proxy approximately. In contrast, according to Eq.12, our hardware-aware quantization explorer categorizes various operations within the operators, and adapt them with adjusting parameters. Quantization configurations searched by EdMIPs and our quantization explorer are illustrated in Fig. 8. Compared to the quantization configuration searched by EdMIPs, our approach allows for quantizing to lower average bitwidths for both weights and activations under the same model architecture. Under the respective given quantization biwidth configurations, our model can reach up to 78.3% Top-1 accuracy, which is +2.3% up to EdMIPs, reflecting the effectiveness of our performance prediction model which can accurately directs NAS to perform hardware-aware quantization."}, {"title": "VI. CONCLUSION", "content": "In this work, we present MCU-MixQ, a HW/SW co-optimized MPNN framework designed for MCU, which improves inference speed while meeting stringent hardware resources. We enhance the parallelism of the low-bitwidth convolution operator through packing and SIMD instructions, and meanwhile implement a low-bitwidth convolution library designed for MCU. As for model quantization, we employ differentiable NAS to automatically configure the optimal combination of quantization bit-widths for the model, while simultaneously considering the runtime efficiency of the model running on MCU. After quantization search stage, MPNN will undergo quantization-aware training and be ultimately mapped onto the optimized MCU kernels. Our experimental results demonstrate that MCU-MixQ achieves better performance compared to sota TinyEngine framework."}]}