{"title": "UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-View Urban Scenarios", "authors": ["Baichuan Zhou", "Haote Yang", "Dairong Chen", "Junyan Ye", "Tianyi Bai", "Jinhua Yu", "Songyang Zhang", "Dahua Lin", "Conghui He", "Weijia Li"], "abstract": "Recent evaluations of Large Multimodal Models (LMMs) have explored their capabilities in various domains, with only few benchmarks specifically focusing on urban environments. Moreover, existing urban benchmarks have been limited to evaluating LMMs with basic region-level urban tasks under singular views, leading to incomplete evaluations of LMMs' abilities in urban environments. To address these issues, we present UrBench, a comprehensive benchmark designed for evaluating LMMs in complex multi-view urban scenarios. UrBench contains 11.6K meticulously curated questions at both region-level and role-level that cover 4 task dimensions: Geo-Localization, Scene Reasoning, Scene Understanding, and Object Understanding, totaling 14 task types. In constructing UrBench, we utilize data from existing datasets and additionally collect data from 11 cities, creating new annotations using a cross-view detection-matching method. With these images and annotations, we then integrate LMM-based, rule-based, and human-based methods to construct large-scale high-quality questions. Our evaluations on 21 LMMs show that current LMMs struggle in the urban environments in several aspects. Even the best performing GPT-40 lags behind humans in most tasks, ranging from simple tasks such as counting to complex tasks such as orientation, localization and object attribute recognition, with an average performance gap of 17.4%. Our benchmark also reveals that LMMs exhibit inconsistent behaviors with different urban views, especially with respect to understanding cross-view relations. UrBench datasets and benchmark results will be publicly available at https://opendatalab.github.io/UrBench/.", "sections": [{"title": "Introduction", "content": "Recently, the research community has witnessed an emergent interest in developing Large Multimodal Models (LMMs) (Achiam et al. 2023; Liu et al. 2024b; Chen et al. 2024) that have exhibited impressive abilities in a variety of benchmarks (Yue et al. 2024; Liu et al. 2023b). The central purpose behind these explorations is to build human-centric AI models that can serve as helpful assistants for everyday life. Given that over 57% of the global population resides in urban areas (World Bank 2024), it is crucial that these AI models should be capable of performing a variety of urban tasks, such as assisting government officials to manage urban development and facilitating citizens to make decisions in daily life (Zhou et al. 2024b; Feng et al. 2024a). On the other hand, urban areas are often captured from various perspectives, including the vertical view from satellite or aerial imagery and the horizontal view from street-view imagery. To be truly effective in assisting the large urban population, Al models should also be capable of comprehensively understanding these environments from multiple perspectives.\nTo better evaluate and develop human-centric AI models, several works have explored the prospect of LMMs for urban environments. For example, various works evaluate the capabilities of LMMs on region-level visual recognition (Hao et al. 2024; Yan et al. 2024) and urban planning (Zhou et al. 2024b). Besides, researchers also examine the performance of LMMs with remote sensing images (Li, Ding, and El-hoseiny 2024; Kuckreja et al. 2024). However, as shown in Fig.1, while these studies primarily focus on urban understanding at a region level, they neglect human-centric tasks within the urban scenarios. A more comprehensive approach should address urban tasks across multiple levels, from region-level recognition tasks to role-level tasks such as geo-localization and scene understanding.\nAnother important aspect of the urban environments is that they are usually captured by multiple different perspectives. As each perspective offers unique information, it is vital for LMMs to comprehend different perspectives to complete certain tasks. For instance, geo-localization tasks require satellite-view imagery for spatial orientation and street-view imagery for detailed contexts. LMMs must utilize both perspectives to successfully perform geo-localization. Given the importance of understanding urban environments from multiple perspectives, current benchmarks that only evaluate LMMs on single-view data (Wang et al. 2024b; Feng et al. 2024b), as shown in Fig.1, are incomprehensive. Therefore, it is crucial to develop a multi-view benchmark to accurately evaluate current models under complex urban settings. However, one of the key challenges of curating such datasets is to construct annotations for the cross-view scenarios (Zhu, Yang, and Chen 2021; Ye et al. 2024a). While it is easy to acquire paired-up street and satellite view images, creating questions about their cross-view correspondences remains challenging due to lack of annotations (Li et al. 2023b).\nTo address these challenges, we propose UrBench, a multi-task, multi-view benchmark designed for comprehensively evaluating LMMs in urban environments. UrBench comprises over 11.6K questions across 14 tasks spanning four dimensions: Geo-Localization, Scene Understanding, Scene Reasoning, and Object Understanding. UrBench includes both region-level tasks from previous benchmarks as well as role-level tasks aimed at assisting humans in daily life. Additionally, considering the multi-view characteristics of urban environments, UrBench incorporates multiple urban perspectives to evaluate models' capabilities in understanding complex multi-view relations. In constructing UrBench, we introduce a novel cross-view detection and matching method to create multi-view annotations. We then utilized these images and annotations to construct our high-quality and diverse set of questions with various methods. As shown in Fig.2, our evaluation results indicate that current models lag behind human experts in most tasks, highlighting their limitations towards human-centric assistants in complex urban environments. Our main contributions are summarized as follows:\n\u2022 We propose UrBench, a multi-view benchmark designed to evaluate LMMs' performances in urban environments. Our benchmark includes 14 urban tasks that we categorize into various dimensions. These tasks encompass both region-level evaluations that assess LMMs' capabilities in urban planning, as well as role-level evaluations that examine LMMs' responses to daily issues.\n\u2022 We introduce a novel benchmark curation pipeline that involves a cross-view detection-matching algorithm for object-level annotation generation and a question generation approach that integrates LMM-based, rule-based, and human-based methods. This pipeline ensures the creation of a large-scale and high-quality corpus of questions, significantly enhancing the diversity and depth of evaluation across multiple urban tasks.\n\u2022 We evaluate 21 popular LMMs on UrBench. Our evaluation results show that current models lag behind human experts in most tasks and reveal LMMs' inconsistent behaviors with different urban views, which demonstrates the limitations of current LMMs in urban environments."}, {"title": "Related Work", "content": "Building on the strengths of Large Language Models (LLMs) (Brown et al. 2020; Chowdhery et al. 2023; Touvron et al. 2023) in complex language reasoning and understanding, Large Multimodal Models (LMMs) can process inputs from multiple modalities and accomplish sophisticated visual reasoning and understanding tasks. The rapid growth of LMMs has given rise to both closed-source models like GPT-40 (Achiam et al. 2023) and Gemini (Reid et al. 2024), as well as open-source models such as the InternVL series (Chen et al. 2024), LLaVA series (Liu et al. 2024b), and VILA series (Lin et al. 2024), all of which have demonstrated significant potential for various application tasks (Cui et al. 2024; Xiao et al. 2024).\nFor urban-related tasks, recent developments show growing interest in utilizing LLMs and CLIP (Radford et al. 2021), covering aspects such as urban planning and vision-language navigation (Zhou et al. 2024b; Schumann et al. 2024). For instance, UrbanCLIP (Yan et al. 2024) leverages LLMs (Touvron et al. 2023) and CLIP (Radford et al. 2021) for urban region profiling using remote sensing images, while Velma (Schumann et al. 2024) combines LLMs with CLIP (Radford et al. 2021) for street view navigation."}, {"title": "Benchmark Analysis", "content": "Overview. We introduce UrBench, a novel benchmark designed for evaluating LMMs in urban scenarios. As detailed in Fig.3(c), UrBench comprises 11.6K questions, which are divided into a validation set for hyperparameter selection and a test set for evaluation. The validation set and the test set contain approximately 1.1K and 10.5K questions, respectively. Please refer to the appendix for further statistical details. The UrBench is characterized by the following features: (1) UrBench integrates street-view, satellite-view, and street-satellite cross-view images, offering a more comprehensive understanding of urban scenarios (Fig.3(b)). (2) UrBench evaluates the capability of LMMs focusing on urban scenarios from comprehensive dimensions, including Geo-Localization, Scene Reasoning, Scene Understanding, and Object Understanding, with a total of 14 task types (Fig.3(a)). (3) The questions of UrBench are generated by an integrated approach, encompassing model-based, rule-based, and human-based methods, which ensures the generation of a substantial and high-quality corpus of questions.\nComparison with existing benchmarks. While general benchmarks such as MUIRBench (Wang et al. 2024a) and MMMU (Yue et al. 2024) evaluate the capacities of LMMs in general scenarios, typically from a single view, our benchmark is focused on urban scenarios from more different perspectives. On the other hand, unlike existing benchmarks in urban scenarios such as CityBench (Feng et al. 2024b) and EarthVQA (Wang et al. 2024b) that place significant emphasis on single-view images and a limited range of tasks, our benchmark incorporates questions that utilize multi-view images and cover more diverse task types.\nBenchmark Tasks\nUrBench comprehensively evaluates LMMs in urban scenarios from four evaluation dimensions. Fig.4 illustrates the specific task types under each evaluation dimension. Please refer to the appendix for additional task questions.\nGeo-Localization. This evaluation dimension contains 4 tasks associated with geographical coordinates and directions (Task 1-4 in Fig.4). The Image Retrieval (IR) task entails retrieving the corresponding satellite-view image of a street-view image or vice versa. The City Retrieval (CR) task is to predict the city name from a street-view image, a satellite-view image, or both. The Orientation (OR) task is to identify the north direction in a street-view image, referenced by the corresponding satellite-view image. The Camera Localization (CL) task entails locating the camera of the street-view image in the corresponding satellite-view image.\nScene Reasoning. This evaluation dimension encompasses 3 tasks that assess the reasoning capacity of LMMs in a specific urban scenario (Task 5-7 in Fig.4). The Visual Prompt Reasoning (VPR) task entails providing an answer to a question about a framed object in a street-view image. The Traffic Sign Reasoning (TSR) task is to answer a question about a traffic sign in a street-view image. The Role-Based Reasoning (RBR) task requires answering an urban scenario-related question posed from the perspective of a specific role, such as that of a shopper, a visitor, a city manager, etc.\nScene Understanding. This evaluation dimension is constituted by 4 tasks designed to assess the overall comprehension of an urban scenario (Task 8-11 in Fig.4). The Road Understanding (RU) task contains two sub-tasks. The first is to predict the type of road in a satellite-view image, such as a crossroad, or a T-junction. The second is to ascertain whether 4 street-view images were captured on a narrow stretch of road. The Scene Recognition (SR) task requires predicting the ground object type that appears in a satellite-view image, a street-view image, or both. The Counting (CO) task entails predicting the crosswalk number in a satellite-view image. The Scene Comparison (SC) task entails predicting which image exhibits the greatest or least value for a given attribute. This could be the image with the highest plant cover, or the largest number of vehicles.\nObject Understanding. This evaluation dimension contains 3 tasks associated with understanding object attributes (Task 12-14 in Fig.4). The Object Grounding (OG) task requires predicting the location of an object in a satellite-view or street-view image given its brief description. The Object Attribute Recognition (OAR) task encompasses two sub-tasks. The first is to predict the land use type of a building with its location framed in a satellite-view image, a street-view image, or both. The second is to predict a building floor number in a street-view image with its location framed in a satellite-view image. The Object Matching (OM) task entails locating an object in a satellite-view image with its location framed in the corresponding street-view image or vice versa."}, {"title": "Benchmark Curation", "content": "Data Collection. As outlined in the data collection stage in Fig.5, there are two data sources of UrBench, the in-house data collected by ourselves and data from open datasets. The in-house data contains 2,604 street-view images from Google Street View and 4,239 satellite-view images from Google Earth (Level 19). Among these images, 1,965 street-satellite image pairs are fit together according to their geological coordinates. In addition, each satellite-view image is equipped with some ground object annotation from OpenStreetMap\u00b9. Furthermore, part of the satellite-view images are equipped with building floor numbers and land use types, derived from PLUTO\u00b2. To support more urban tasks, we also collect images from existing open source datasets, including the val subset of Cityscapes (Cordts et al. 2016), the val subset of Mapillary Traffic Sign Dataset (Ertler et al. 2020), the VIGOR dataset (Zhu, Yang, and Chen 2021), and the test subset of IM2GPS (Hays and Efros 2008).\nData Pre-processing. For most cross-view questions, the object (mainly building) matching between paired images is required for annotation-sharing. Moreover, visual prompts are required to indicate the object in question in a complex scenario. To this end, we develop a cross-view detection-matching method that combines the Grounding DINO model (Liu et al. 2023a) and ray-tracing algorithm,\nfrom which we provide building matching and building bounding boxes. In addition, an annotation database is constructed on top of the open dataset annotations, OSM/-PLUTO, geographical coordinates, etc. to facilitate the extraction of answers to questions generated by rule-based methods. The data pre-processing stage in Fig.5 details these two processes.\nQuestion Generation. As detailed in the question generation stage in Fig.5, three methods are utilized in UrBench:\n(1) LMM-based. To guarantee question diversity, for all Scene Reasoning tasks, Q&A generation is carried out via LMMs. When generating RBR Q&As, a dilemma is encountered whereby, regardless of the LMM employed, it will act as both the question setter and the test taker. Our solution is to enable as many test takers as possible to act as question setters. 4 LMMs (ChatGPT-40 (Achiam et al. 2023), Claude-3.5-Sonnet (Anthropic 2024), Gemini-1.5-Falsh (Reid et al. 2024), InternVL2-26B (Chen et al. 2024)) out of 10 LMMs show the capacity to generate high-quality Q&As. For TSR and VPR, ChatGPT-40 is tasked with generating questions whose answers are tightly aligned with the ground truth. Consequently, ChatGPT-4o gains minimal benefit in the test and no more LMMs are needed for Q&A generation. (2) Rule-based. In the case of tasks such as Image Retrieval (IR), whose question answers can be directly extracted from the annotation database constructed in the data pre-processing stage, questions are generated by rule-based methods with high efficiency. (3) Human-based. In the absence of ground truth, human annotation is employed for a portion of the Q&As in the Scene Comparison (SC) task.\nQuality Control. In UrBench, questions generated by LMMs and questions whose answers are not always correct due to the discrepancy in acquisition time and the visible range of images from different perspectives are subjected to human examination. Each question is subjected to at least two rounds of verification by experts with a graduate-level qualification in remote sensing. A total of 12 experts were engaged in the question verification, collectively spending approximately 160 hours on this task. Please refer to the appendix for further question generation details."}, {"title": "Experiments", "content": "In this section, we evaluate various LMMs on our proposed UrBench. We consider closed-source models, open-source single-image models and open-source multi-image models, and perform evaluations under zero-shot settings. In the following sections, we first introduce our evaluated models evaluation protocols. Then we summarize our findings of model performance with respect to different model types, view settings and tasks. Finally, we provide a detailed analysis in terms of different tasks and views.\nEvaluated Models. We evaluate 3 closed-source and 18 open-source LMMs across different model types and sizes. For closed-source models, we consider GPT-40 (Achiam et al. 2023), Gemini-1.5-Flash (Reid et al. 2024), Claude-3.5-Sonnet (Anthropic 2024). For open-sourced models, we categorize them into single-image type and multi-image type according to their training data and strategies, including TinyLLaVA (Zhou et al. 2024a), LLaVA-NeXT series (Liu et al. 2024a), XComposer (Zhang et al. 2023), InstructBLIP (Li et al. 2023a) and idefics (Lauren\u00e7on et al. 2024) for single-image type, and Mantis series (Jiang et al. 2024), VILA series (Lin et al. 2024), InternVL series (Chen et al. 2024) and LLaVA-NeXT-Interleave(Li et al. 2024) for multi-image type.\nEvaluation Protocols. In UrBench, our questions have two response formats: multiple-choice and open-ended. We follow standard setups in MMMU (Yue et al. 2024) and Muir-Bench (Wang et al. 2024a) to process LMMs' responses. To ensure reproducibility, we set the temperature to 0 and perform greedy decoding. Additionally, for models that do not support multi-image inputs, we concatenate the images as one input. More details on setups and the human evaluation protocols are provided in the Appendix.\nMain Results\nOverall Challenge Presented in UrBench. As indicated by Table 1, UrBench poses significant challenges to current SOTA LMMs. We find that the best performing closed-source model GPT-40 and open-source model VILA-1.5-40B only achieve a 61.2% and a 53.1% accuracy, respectively. Interestingly, our findings indicate that the primary limitation of these models lies in their ability to comprehend UrBench questions, not in their capacity to process multiple images, as the performance between multi-image and their single-image counterparts shows little difference, such as LLaVA-NeXT-8B and LLaVA-NeXT-Interleave in Table 1. Overall, the challenging nature of our benchmark indicates that current LMMs' strong performance on the general benchmarks (Fu et al. 2023; Liu et al. 2023b) are not generalized to the multi-view urban scenarios.\nLMMs' performances across task dimensions. In Table 1, we show LMMs performances across different dimensions. sions. We observe that most LMMs exhibit impressive capabilities in Scene Reasoning tasks such as Visual Prompt Reasoning (VPR) and Role-based Reasoning (RBR), which are greatly aligned with their SFT objectives (Liu et al. 2024b). In Scene Understanding, models perform relatively well in region-level tasks such as Scene Recognition (SR) and Scene Comparison (SC), but are bad at Counting (CO). While LMMs achieve impressive results in City Retrieval (CR), however, in other Geo-localization tasks, e.g., Camera Localization (CL) and Orientation (OR), most LMMs only perform slightly better or worse than random guessing, yielding a 28.6% and a 26.3% average accuracy, respectively. Overall, LMMs' exception capbilities in reasoning tasks and world knowledge are well-examplified in our benchmark. However, our benchmark also demonstrates current models' limited abilities in handling other important urban tasks such as geo-localization.\nAre LMMs consistent with Different Views? Out of the three views in our proposed benchmark, we find that models struggle the most with cross-view tasks, averaging only 36.2% in all models, while averaging 54.6% and 42.3 % in street-view and satellite-view, respectively. Fig.6 presents the overall model performance across different views. Claude-3.5-Sonnet (Anthropic 2024) obtains the highest 60.1% score in satellite-view. GPT-40 (Achiam et al. 2023) achieves the highest average score in street-view tasks, with Gemini-1.5-Flash (Reid et al. 2024), InternVL2-26B (Chen et al. 2024) and VILA-1.5-40B (Lin et al. 2024) following close. Most models perform no better than 40% in cross-view tasks, except for GPT-40 (Achiam et al. 2023), Claude-3.5-Sonnet (Anthropic 2024) and VILA-1.5-40B (Lin et al. 2024). Our results show that current LMMs are best at street-view tasks, while handling satellite-view and cross-view tasks insufficiently.\nDoes GPT-40 Surpass Human Experts? Human experts outperform GPT-40 by an average of 17.4% and achieve better performance in twelve out of the fourteen tasks of UrBench. Humans only fall significantly behind GPT-40 in the City Retrieval task, which we attribute to the rich geographic world knowledge in the LMMs. For humans, identifying the correct geographic location from a photo is challenging. GPT-40 falls behind human experts by 54.1% on simple tasks such as counting, and 67.8% on Object Grounding (OG). We note these performance gaps highlight the significant room for improvement in current models' understanding of urban environments.\nDisparity between Closed-source and Open-source Models. Under our urban settings, we observe that the gap between closed-source and open-source models are closing in. Notably, leading open-source models like VILA-1.5-40B (Lin et al. 2024) demonstrates superior performance over closed-source models like Gemini-1.5-Flash. However, the leading closed-source model GPT-40 is still very much ahead of other open-source models, approximately 8% higher than the leading VILA-1.5-40B. However, VILA-1.5 has already surpassed closed-source model Gemini-1.5-Flash, while keeping close with Claude-3.5-Sonnet."}, {"title": "Detailed Analysis", "content": "LMMs struggle to understand cross-view relations. Several UrBench tasks involve understanding the internal relations between satellite and street view images. While prior works (Shi and Li 2022; Ye et al. 2024b) have demonstrated that specialist models can achieve impressive results in cross-view tasks such as Camera Localization (CL) and Orientation (OR), our results indicate that general LMMs only possess very limited cross-view understanding capability, where their performances in average are only 3% higher than random choice. Although LMMs are capable of understanding relations across multiple images (Jiang et al. 2024), our findings show that their capabilities are yet to generalize to images across different views.\nLMMs are inconsistent with different views of the same geolocation. Even though prompted with the same question at the same geolocation, LMMs behave differently with different views. In City Retrieval CR, we find that models perform better in street-view (62.9%) and cross-view (63.8%) compared to satellite-view (50.9%) in average. We conjecture it is because most models are not well-trained on satellite samples and the parametric knowledge of geolocation is more aligned with street-view images. However, we find that the results of Scene Recognition SR in satellite-view are much higher than the other views, as recognizing ground objects from a vertically upward view is easier than from a horizontal view. The experiments exhibit the imbalance and bias between views during the training of LMMs."}, {"title": "Conclusion", "content": "In this work, we present UrBench, a new benchmark that evaluates LMMs in the urban environments with diverse task types and view types. To create our multi-view annotations, we propose a new data collection pipeline that pairs up cross-view images at an instance level. In the end, we collect 11.6K questions that include 14 subtasks of four dimensions. We carefully evaluate 21 LMMs on our questions and show their limitations in the urban environments. We conduct extensive analysis on measuring the performance of LMMs across different view types and task types, and show that current LMMs still lag behind human experts significantly in the urban environments. We also highlight that current LMMs struggle to understand multi-view image relations and their performance under different view types are inconsistent, shedding light on the imbalance and bias between different views during LMMs training. We hope our work can provide guidance for future work in improving the capability of LMMs in urban scenarios."}]}