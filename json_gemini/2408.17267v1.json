{"title": "UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal Models\nin Multi-View Urban Scenarios", "authors": ["Baichuan Zhou", "Haote Yang", "Dairong Chen", "Junyan Ye", "Tianyi Bai", "Jinhua Yu", "Songyang Zhang", "Dahua Lin", "Conghui He", "Weijia Li"], "abstract": "Recent evaluations of Large Multimodal Models (LMMs)\nhave explored their capabilities in various domains, with only\nfew benchmarks specifically focusing on urban environments.\nMoreover, existing urban benchmarks have been limited to\nevaluating LMMs with basic region-level urban tasks under\nsingular views, leading to incomplete evaluations of LMMs'\nabilities in urban environments. To address these issues, we\npresent UrBench, a comprehensive benchmark designed for\nevaluating LMMs in complex multi-view urban scenarios.\nUrBench contains 11.6K meticulously curated questions at\nboth region-level and role-level that cover 4 task dimensions:\nGeo-Localization, Scene Reasoning, Scene Understanding,\nand Object Understanding, totaling 14 task types. In con-\nstructing UrBench, we utilize data from existing datasets and\nadditionally collect data from 11 cities, creating new anno-\ntations using a cross-view detection-matching method. With\nthese images and annotations, we then integrate LMM-based,\nrule-based, and human-based methods to construct large-\nscale high-quality questions. Our evaluations on 21 LMMs\nshow that current LMMs struggle in the urban environments\nin several aspects. Even the best performing GPT-40 lags be-\nhind humans in most tasks, ranging from simple tasks such\nas counting to complex tasks such as orientation, localization\nand object attribute recognition, with an average performance\ngap of 17.4%. Our benchmark also reveals that LMMs exhibit\ninconsistent behaviors with different urban views, especially\nwith respect to understanding cross-view relations. UrBench\ndatasets and benchmark results will be publicly available at\nhttps://opendatalab.github.io/UrBench/.", "sections": [{"title": "Introduction", "content": "Recently, the research community has witnessed an emer-\ngent interest in developing Large Multimodal Models\n(LMMs) (Achiam et al. 2023; Liu et al. 2024b; Chen et al.\n2024) that have exhibited impressive abilities in a variety of\nbenchmarks (Yue et al. 2024; Liu et al. 2023b). The central\npurpose behind these explorations is to build human-centric\nAI models that can serve as helpful assistants for everyday\nlife. Given that over 57% of the global population resides in\nurban areas (World Bank 2024), it is crucial that these AI\nmodels should be capable of performing a variety of urban\ntasks, such as assisting government officials to manage ur-\nban development and facilitating citizens to make decisions\nin daily life (Zhou et al. 2024b; Feng et al. 2024a). On the\nother hand, urban areas are often captured from various per-\nspectives, including the vertical view from satellite or aerial\nimagery and the horizontal view from street-view imagery.\nTo be truly effective in assisting the large urban population,\nAl models should also be capable of comprehensively un-\nderstanding these environments from multiple perspectives.\nTo better evaluate and develop human-centric AI models,\nseveral works have explored the prospect of LMMs for urban\nenvironments. For example, various works evaluate the ca-\npabilities of LMMs on region-level visual recognition (Hao\net al. 2024; Yan et al. 2024) and urban planning (Zhou et al.\n2024b). Besides, researchers also examine the performance\nof LMMs with remote sensing images (Li, Ding, and El-\nhoseiny 2024; Kuckreja et al. 2024). However, as shown\nin Fig.1, while these studies primarily focus on urban un-\nderstanding at a region level, they neglect human-centric\ntasks within the urban scenarios. A more comprehensive ap-\nproach should address urban tasks across multiple levels,\nfrom region-level recognition tasks to role-level tasks such\nas geo-localization and scene understanding.\nAnother important aspect of the urban environments is\nthat they are usually captured by multiple different per-\nspectives. As each perspective offers unique information,\nit is vital for LMMs to comprehend different perspectives\nto complete certain tasks. For instance, geo-localization\ntasks require satellite-view imagery for spatial orienta-\ntion and street-view imagery for detailed contexts. LMMs\nmust utilize both perspectives to successfully perform geo-\nlocalization. Given the importance of understanding urban\nenvironments from multiple perspectives, current bench-\nmarks that only evaluate LMMs on single-view data (Wang\net al. 2024b; Feng et al. 2024b), as shown in Fig.1, are in-\ncomprehensive. Therefore, it is crucial to develop a multi-\nview benchmark to accurately evaluate current models under\ncomplex urban settings. However, one of the key challenges\nof curating such datasets is to construct annotations for the\ncross-view scenarios (Zhu, Yang, and Chen 2021; Ye et al.\n2024a). While it is easy to acquire paired-up street and satel-\nlite view images, creating questions about their cross-view\ncorrespondences remains challenging due to lack of annota-\ntions (Li et al. 2023b).\nTo address these challenges, we propose UrBench, a\nmulti-task, multi-view benchmark designed for comprehen-\nsively evaluating LMMs in urban environments. UrBench\ncomprises over 11.6K questions across 14 tasks spanning\nfour dimensions: Geo-Localization, Scene Understanding,\nScene Reasoning, and Object Understanding. UrBench in-\ncludes both region-level tasks from previous benchmarks as\nwell as role-level tasks aimed at assisting humans in daily\nlife. Additionally, considering the multi-view characteris-\ntics of urban environments, UrBench incorporates multiple\nurban perspectives to evaluate models' capabilities in un-\nderstanding complex multi-view relations. In constructing\nUrBench, we introduce a novel cross-view detection and\nmatching method to create multi-view annotations. We then\nutilized these images and annotations to construct our high-\nquality and diverse set of questions with various methods.\nAs shown in Fig.2, our evaluation results indicate that cur-\nrent models lag behind human experts in most tasks, high-\nlighting their limitations towards human-centric assistants\nin complex urban environments. Our main contributions are\nsummarized as follows:\n\u2022 We propose UrBench, a multi-view benchmark designed\nto evaluate LMMs' performances in urban environments.\nOur benchmark includes 14 urban tasks that we cate-\ngorize into various dimensions. These tasks encompass\nboth region-level evaluations that assess LMMs' capabil-\nities in urban planning, as well as role-level evaluations\nthat examine LMMs' responses to daily issues."}, {"title": "Benchmark Analysis", "content": "UrBench\nOverview. We introduce UrBench, a novel benchmark de-\nsigned for evaluating LMMs in urban scenarios. As detailed\nin Fig.3(c), UrBench comprises 11.6K questions, which are\ndivided into a validation set for hyperparameter selection\nand a test set for evaluation. The validation set and the test\nset contain approximately 1.1K and 10.5K questions, re-\nspectively. Please refer to the appendix for further statisti-\ncal details. The UrBench is characterized by the following\nfeatures: (1) UrBench integrates street-view, satellite-view,\nand street-satellite cross-view images, offering a more com-\nprehensive understanding of urban scenarios (Fig.3(b)). (2)\nUrBench evaluates the capability of LMMs focusing on ur-\nban scenarios from comprehensive dimensions, including\nGeo-Localization, Scene Reasoning, Scene Understanding,\nand Object Understanding, with a total of 14 task types\n(Fig.3(a)). (3) The questions of UrBench are generated by\nan integrated approach, encompassing model-based, rule-\nbased, and human-based methods, which ensures the gener-\nation of a substantial and high-quality corpus of questions.\nComparison with existing benchmarks. While general\nbenchmarks such as MUIRBench (Wang et al. 2024a) and\nMMMU (Yue et al. 2024) evaluate the capacities of LMMs\nin general scenarios, typically from a single view, our bench-\nmark is focused on urban scenarios from more different per-\nspectives. On the other hand, unlike existing benchmarks in\nurban scenarios such as CityBench (Feng et al. 2024b) and\nEarthVQA (Wang et al. 2024b) that place significant empha-\nsis on single-view images and a limited range of tasks, our\nbenchmark incorporates questions that utilize multi-view\nimages and cover more diverse task types.\nBenchmark Tasks\nUrBench comprehensively evaluates LMMs in urban sce-\nnarios from four evaluation dimensions. Fig.4 illustrates the\nspecific task types under each evaluation dimension. Please\nrefer to the appendix for additional task questions.\nGeo-Localization. This evaluation dimension contains 4\ntasks associated with geographical coordinates and direc-\ntions (Task 1-4 in Fig.4). The Image Retrieval (IR) task en-\ntails retrieving the corresponding satellite-view image of a\nstreet-view image or vice versa. The City Retrieval (CR)\ntask is to predict the city name from a street-view image,\na satellite-view image, or both. The Orientation (OR) task is\nto identify the north direction in a street-view image, refer-\nenced by the corresponding satellite-view image. The Cam-\nera Localization (CL) task entails locating the camera of the\nstreet-view image in the corresponding satellite-view image.\nScene Reasoning. This evaluation dimension encompasses\n3 tasks that assess the reasoning capacity of LMMs in a spe-\ncific urban scenario (Task 5-7 in Fig.4). The Visual Prompt\nReasoning (VPR) task entails providing an answer to a ques-\ntion about a framed object in a street-view image. The Traf-\nfic Sign Reasoning (TSR) task is to answer a question about\na traffic sign in a street-view image. The Role-Based Rea-\nsoning (RBR) task requires answering an urban scenario-\nrelated question posed from the perspective of a specific\nrole, such as that of a shopper, a visitor, a city manager, etc.\nScene Understanding. This evaluation dimension is consti-\ntuted by 4 tasks designed to assess the overall comprehen-\nsion of an urban scenario (Task 8-11 in Fig.4). The Road\nUnderstanding (RU) task contains two sub-tasks. The first\nis to predict the type of road in a satellite-view image,\nsuch as a crossroad, or a T-junction. The second is to as-\ncertain whether 4 street-view images were captured on a\nnarrow stretch of road. The Scene Recognition (SR) task\nrequires predicting the ground object type that appears in\na satellite-view image, a street-view image, or both. The\nCounting (CO) task entails predicting the crosswalk num-\nber in a satellite-view image. The Scene Comparison (SC)\ntask entails predicting which image exhibits the greatest or\nleast value for a given attribute. This could be the image with\nthe highest plant cover, or the largest number of vehicles.\nObject Understanding. This evaluation dimension contains\n3 tasks associated with understanding object attributes (Task\n12-14 in Fig.4). The Object Grounding (OG) task requires\npredicting the location of an object in a satellite-view or\nstreet-view image given its brief description. The Object At-\ntribute Recognition (OAR) task encompasses two sub-tasks.\nThe first is to predict the land use type of a building with its\nlocation framed in a satellite-view image, a street-view im-\nage, or both. The second is to predict a building floor number\nin a street-view image with its location framed in a satellite-\nview image. The Object Matching (OM) task entails locating\nan object in a satellite-view image with its location framed"}, {"title": "Benchmark Curation", "content": "Data Collection. As outlined in the data collection stage in\nFig.5, there are two data sources of UrBench, the in-house\ndata collected by ourselves and data from open datasets.\nThe in-house data contains 2,604 street-view images from\nGoogle Street View and 4,239 satellite-view images from\nGoogle Earth (Level 19). Among these images, 1,965 street-\nsatellite image pairs are fit together according to their geo-\nlogical coordinates. In addition, each satellite-view image is\nequipped with some ground object annotation from Open-\nStreetMap\u00b9. Furthermore, part of the satellite-view images\nare equipped with building floor numbers and land use types,\nderived from PLUTO\u00b2. To support more urban tasks, we also\ncollect images from existing open source datasets, including\nthe val subset of Cityscapes (Cordts et al. 2016), the val sub-\nset of Mapillary Traffic Sign Dataset (Ertler et al. 2020), the\nVIGOR dataset (Zhu, Yang, and Chen 2021), and the test\nsubset of IM2GPS (Hays and Efros 2008).\nData Pre-processing. For most cross-view questions, the\nobject (mainly building) matching between paired images is\nrequired for annotation-sharing. Moreover, visual prompts\nare required to indicate the object in question in a complex\nscenario. To this end, we develop a cross-view detection-\nmatching method that combines the Grounding DINO model\n(Liu et al. 2023a) and ray-tracing algorithm,\nfrom which we provide building matching and build-\ning bounding boxes. In addition, an annotation database is\nconstructed on top of the open dataset annotations, OSM/-\nPLUTO, geographical coordinates, etc. to facilitate the ex-\ntraction of answers to questions generated by rule-based\nmethods. The data pre-processing stage in Fig.5 details these\ntwo processes.\nQuestion Generation. As detailed in the question genera-\ntion stage in Fig.5, three methods are utilized in UrBench:\n(1) LMM-based. To guarantee question diversity, for all\nScene Reasoning tasks, Q&A generation is carried out via\nLMMs. When generating RBR Q&As, a dilemma is encoun-\ntered whereby, regardless of the LMM employed, it will\nact as both the question setter and the test taker. Our so-\nlution is to enable as many test takers as possible to act\nas question setters. 4 LMMs (ChatGPT-40 (Achiam et al.\n2023), Claude-3.5-Sonnet (Anthropic 2024), Gemini-1.5-\nFalsh (Reid et al. 2024), InternVL2-26B (Chen et al. 2024))\nout of 10 LMMs show the capacity to generate high-quality\nQ&As. For TSR and VPR, ChatGPT-40 is tasked with gen-\nerating questions whose answers are tightly aligned with\nthe ground truth. Consequently, ChatGPT-4o gains minimal\nbenefit in the test and no more LMMs are needed for Q&A\ngeneration. (2) Rule-based. In the case of tasks such as Im-\nage Retrieval (IR), whose question answers can be directly\nextracted from the annotation database constructed in the\ndata pre-processing stage, questions are generated by rule-\nbased methods with high efficiency. (3) Human-based. In the\nabsence of ground truth, human annotation is employed for\na portion of the Q&As in the Scene Comparison (SC) task.\nQuality Control. In UrBench, questions generated by\nLMMs and questions whose answers are not always correct\ndue to the discrepancy in acquisition time and the visible\nrange of images from different perspectives are subjected to\nhuman examination. Each question is subjected to at least\ntwo rounds of verification by experts with a graduate-level\nqualification in remote sensing. A total of 12 experts were\nengaged in the question verification, collectively spending\napproximately 160 hours on this task. Please refer to the ap-\npendix for further question generation details."}, {"title": "Experiments", "content": "In this section, we evaluate various LMMs on our proposed\nUrBench. We consider closed-source models, open-source\nsingle-image models and open-source multi-image models,\nand perform evaluations under zero-shot settings. In the fol-\nlowing sections, we first introduce our evaluated models\nevaluation protocols. Then we summarize our findings of\nmodel performance with respect to different model types,\nview settings and tasks. Finally, we provide a detailed anal-\nsis in terms of different tasks and views.\nEvaluation Setups\nEvaluated Models. We evaluate 3 closed-source and 18\nopen-source LMMs across different model types and sizes.\nFor closed-source models, we consider GPT-40 (Achiam\net al. 2023), Gemini-1.5-Flash (Reid et al. 2024), Claude-\n3.5-Sonnet (Anthropic 2024). For open-sourced models, we\ncategorize them into single-image type and multi-image\ntype according to their training data and strategies, includ-\ning TinyLLaVA (Zhou et al. 2024a), LLaVA-NeXT se-\nries (Liu et al. 2024a), XComposer (Zhang et al. 2023), In-\nstructBLIP (Li et al. 2023a) and idefics (Lauren\u00e7on et al.\n2024) for single-image type, and Mantis series (Jiang et al.\n2024), VILA series (Lin et al. 2024), InternVL series (Chen\net al. 2024) and LLaVA-NeXT-Interleave(Li et al. 2024) for\nmulti-image type.\nEvaluation Protocols. In UrBench, our questions have two\nresponse formats: multiple-choice and open-ended. We fol-\nlow standard setups in MMMU (Yue et al. 2024) and Muir-\nBench (Wang et al. 2024a) to process LMMs' responses. To\nensure reproducibility, we set the temperature to 0 and per-\nform greedy decoding. Additionally, for models that do not\nsupport multi-image inputs, we concatenate the images as\none input. More details on setups and the human evaluation\nprotocols are provided in the Appendix.\nMain Results\nOverall Challenge Presented in UrBench. As indicated\nby Table 1, UrBench poses significant challenges to cur-\nrent SOTA LMMs. We find that the best performing closed-\nsource model GPT-40 and open-source model VILA-1.5-\n40B only achieve a 61.2% and a 53.1% accuracy, respec-\ntively. Interestingly, our findings indicate that the primary\nlimitation of these models lies in their ability to comprehend\nUrBench questions, not in their capacity to process multi-\nple images, as the performance between multi-image and\ntheir single-image counterparts shows little difference, such\nas LLaVA-NeXT-8B and LLaVA-NeXT-Interleave in Table\n1. Overall, the challenging nature of our benchmark indi-\ncates that current LMMs' strong performance on the general\nbenchmarks (Fu et al. 2023; Liu et al. 2023b) are not gener-\nalized to the multi-view urban scenarios.\nLMMs' performances across task dimensions. In Table\n1, we show LMMs performances across different dimen-\nsions. We observe that most LMMs exhibit impressive ca-\npabilities in Scene Reasoning tasks such as Visual Prompt\nReasoning (VPR) and Role-based Reasoning (RBR), which\nare greatly aligned with their SFT objectives (Liu et al.\n2024b). In Scene Understanding, models perform relatively\nwell in region-level tasks such as Scene Recognition (SR)\nand Scene Comparison (SC), but are bad at Counting (CO).\nWhile LMMs achieve impressive results in City Retrieval\n(CR), however, in other Geo-localization tasks, e.g., Cam-\nera Localization (CL) and Orientation (OR), most LMMs\nonly perform slightly better or worse than random guess-\ning, yielding a 28.6% and a 26.3% average accuracy, re-\nspectively. Overall, LMMs' exception capbilities in reason-\ning tasks and world knowledge are well-examplified in our\nbenchmark. However, our benchmark also demonstrates cur-\nrent models' limited abilities in handling other important ur-\nban tasks such as geo-localization.\nAre LMMs consistent with Different Views? Out of\nthe three views in our proposed benchmark, we find that\nmodels struggle the most with cross-view tasks, averag-\ning only 36.2% in all models, while averaging 54.6%\nand 42.3 % in street-view and satellite-view, respectively.\nFig.6 presents the overall model performance across dif-\nferent views. Claude-3.5-Sonnet (Anthropic 2024) obtains\nthe highest 60.1% score in satellite-view. GPT-40 (Achiam\net al. 2023) achieves the highest average score in street-view\ntasks, with Gemini-1.5-Flash (Reid et al. 2024), InternVL2-\n26B (Chen et al. 2024) and VILA-1.5-40B (Lin et al.\n2024) following close. Most models perform no better than\n40% in cross-view tasks, except for GPT-40 (Achiam et al.\n2023), Claude-3.5-Sonnet (Anthropic 2024) and VILA-1.5-\n40B (Lin et al. 2024). Our results show that current LMMs\nare best at street-view tasks, while handling satellite-view\nand cross-view tasks insufficiently.\nDoes GPT-40 Surpass Human Experts? Human experts\noutperform GPT-40 by an average of 17.4% and achieve\nbetter performance in twelve out of the fourteen tasks of\nUrBench. Humans only fall significantly behind GPT-40 in\nthe City Retrieval task, which we attribute to the rich geo-\ngraphic world knowledge in the LMMs. For humans, iden-\ntifying the correct geographic location from a photo is chal-\nlenging. GPT-40 falls behind human experts by 54.1%, on\nsimple tasks such as counting, and 67.8% on Object Ground-\ning (OG). We note these performance gaps highlight the\nsignificant room for improvement in current models' under-\nstanding of urban environments.\nDisparity between Closed-source and Open-source Mod-\nels. Under our urban settings, we observe that the gap be-\ntween closed-source and open-source models are closing\nin. Notably, leading open-source models like VILA-1.5-\n40B (Lin et al. 2024) demonstrates superior performance\nover closed-source models like Gemini-1.5-Flash. How-\never, the leading closed-source model GPT-40 is still very\nmuch ahead of other open-source models, approximately\n8% higher than the leading VILA-1.5-40B. However, VILA-\n1.5 has already surpassed closed-source model Gemini-1.5-\nFlash, while keeping close with Claude-3.5-Sonnet.\nDetailed Analysis\nLMMs struggle to understand cross-view relations. Sev-\neral UrBench tasks involve understanding the internal rela-\ntions between satellite and street view images. While prior\nworks (Shi and Li 2022; Ye et al. 2024b) have demon-\nstrated that specialist models can achieve impressive results\nin cross-view tasks such as Camera Localization (CL) and\nOrientation (OR), our results indicate that general LMMs\nonly possess very limited cross-view understanding capabil-\nity, where their performances in average are only 3% higher\nthan random choice. Although LMMs are capable of under-\nstanding relations across multiple images (Jiang et al. 2024), our findings show that their capabilities are yet to generalize\nto images across different views.\nLMMs are inconsistent with different views of the same\ngeolocation. Even though prompted with the same question\nat the same geolocation, LMMs behave differently with dif-\nferent views. In City Retrieval CR, we find that models per-\nform better in street-view (62.9%) and cross-view (63.8%)\ncompared to satellite-view (50.9%) in average. We conjec-\nture it is because most models are not well-trained on satel-\nlite samples and the parametric knowledge of geolocation\nis more aligned with street-view images. However, we find\nthat the results of Scene Recognition SR in satellite-view\nare much higher than the other views, as recognizing ground\nobjects from a vertically upward view is easier than from a\nhorizontal view. The experiments exhibit the imbalance and\nbias between views during the training of LMMs."}, {"title": "Conclusion", "content": "In this work, we present UrBench, a new benchmark that\nevaluates LMMs in the urban environments with diverse\ntask types and view types. To create our multi-view anno-\ntations, we propose a new data collection pipeline that pairs\nup cross-view images at an instance level. In the end, we\ncollect 11.6K questions that include 14 subtasks of four di-\nmensions. We carefully evaluate 21 LMMs on our questions\nand show their limitations in the urban environments. We\nconduct extensive analysis on measuring the performance\nof LMMs across different view types and task types, and\nshow that current LMMs still lag behind human experts sig-\nnificantly in the urban environments. We also highlight that\ncurrent LMMs struggle to understand multi-view image re-\nlations and their performance under different view types are\ninconsistent, shedding light on the imbalance and bias be-\ntween different views during LMMs training. We hope our\nwork can provide guidance for future work in improving the\ncapability of LMMs in urban scenarios."}]}