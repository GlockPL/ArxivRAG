{"title": "ADAPTING LANGUAGE-SPECIFIC LLMS TO A REASONING MODEL in One Day VIA MODEL MERGING - AN OPEN RECIPE", "authors": ["Kunat Pipatanakul", "Pittawat Taveekitworachai", "Potsawee Manakul", "Kasima Tharnpipitchai"], "abstract": "This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120^1$, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks. This work releases the data, merge configurations, and model weights to promote the advancement of language-specific LLM initiatives.", "sections": [{"title": "INTRODUCTION", "content": "Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly through innovations in scaling at test time and specialized training paradigms (DeepSeek-AI et al., 2025). Notable breakthroughs by models such as OpenAI o1 and DeepSeek R1 (DeepSeek-AI et al., 2025) have established new standards in tackling reasoning challenges that were previously difficult for LLMs. However, these achievements primarily focus on high-resource languages, particularly English and Chinese, creating a significant gap in capabilities for low-resource languages.\n\nThe underlying foundation models, such as Llama (Grattafiori et al., 2024) and Qwen (Qwen et al., 2025), predominantly rely on training data in English and Chinese, leading to limitations when applied to low-resource languages. While these models may achieve impressive scores on certain low-resource language benchmarks, they frequently exhibit issues such as incorrect character usage and code-switching in practical applications, see example in Appendix A.1. These problems become more pronounced during reasoning-focused fine-tuning and reinforcement learning (Team, 2024), where both the query language and solution paths are optimized in high-resource languages.\n\nSeveral local and regional LLM initiatives, including Typhoon (Pipatanakul et al., 2024), Sailor (Dou et al., 2024), EuroLLM (Martins et al., 2024), Aya (\u00dcst\u00fcn et al., 2024), Sea-lion (Singapore,"}, {"title": "METHODOLOGY", "content": "In this section, we first explain the motivation and process behind our approach. First, we select two specialized LLMs-one proficient in a target low-resource language (e.g., Thai) and the other specialized in long-thought reasoning (Section 2.1). After selecting the models, we employ a two-stage procedure:\n\n1.  Representation Alignment via Supervised Fine-Tuning (SFT), described in Section 2.2)\n2.  Ability-Aware Model Merging, described in Section 2.3)"}, {"title": "MODEL SELECTION", "content": "To integrate advanced reasoning with strong language capabilities in a low-resource setting, we begin by selecting two LLMs that share the same foundational architecture. This architectural compatibility facilitates effective alignment and merging of learned parameters. Specifically, we choose one language-specific model specialized in Thai and another long-thought reasoning model.\n\nFor our approach, we choose Typhoon2 70B Instruct (Pipatanakul et al., 2024), a Thai-specialized model derived from Llama 3.1 70B. This model has undergone continuous pretraining (CPT) and alignment using a Thai-focused dataset to enhance its performance in Thai. Additionally, we incorporate DeepSeek R1 70B Distill (DeepSeek-AI et al., 2025), a reasoning-focused model fine-tuned from Llama 3.3 70B and SFT from an 600K-instance distilled reasoning trace dataset generated by DeepSeek R1 + 200K general SFT dataset.\n\nBoth models belong to the Llama 3.1 70B family (Grattafiori et al., 2024), sharing a common pretrained backbone. This shared foundation ensures well-aligned internal representations, improving the effectiveness of subsequent merging steps."}, {"title": "REPRESENTATION ALIGNMENT VIA SUPERVISED FINE-TUNING (SFT)", "content": "To facilitate effective model merging, we first align the internal representations of the language-specific and reasoning-focused LLMs through supervised fine-tuning. The goal of this step is to ensure that the two models develop a similar representation space, enabling smoother integration in the subsequent merging phase.\n\nFor this purpose, we construct a training dataset that promotes alignment between language and reasoning capabilities. Our foundation is Bespoke-Stratos (Labs, 2025), a carefully curated reasoning distillation dataset comprising 17,000 examples generated by DeepSeek R1. These examples have demonstrated state-of-the-art (SOTA) performance in reasoning tasks when fine-tuned (SFT) directly on Qwen2.5-32B-Instruct (Qwen et al., 2025). However, since Bespoke-Stratos is primarily in English, we adapt it for our low-resource setting by translating the question and solution components into Thai while retaining the original English reasoning traces. Intuitively, this bilingual setup aims to teach model to align Thai question-solution pairs to the original English long-form reasoning traces. This approach is inspired by a work on multilingual CoT (Shi et al., 2022).\n\nIn addition to this translated dataset, we explore additional datasets to assess the impact of each data type on the final merged model's performance, including (i) Distillation of DeepSeek R1 long-thought reasoning examples on general Thai prompts; (ii) General Thai instruction-tuning dataset."}, {"title": "ABILITY-AWARE MODEL MERGING", "content": "After fine-tuning the language-specific model, we merge its parameters with those of the long-thought reasoning model using a merge operation, specifically, we adopt the method proposed by Yu et al. (2024). Additionally, we optimize the merge ratio based on the hypothesis that different layers contribute uniquely to distinct capabilities, as observed in early empirical experiments. Intuitively:\n\n\u2022  Early to middle layers are assigned a higher weight from the long-thought reasoning model, as these layers primarily handle comprehension and abstract thinking.\n\u2022  Later layers are more influenced by the language-specific model, as they play a crucial role in fluent generation in the output.\n\nThe objective is to create a merged model that excels in both reasoning and Thai capabilities."}, {"title": "EXPERIMENTAL SETUP", "content": "We select Typhoon2 70B Instruct and DeepSeek R1 70B Distill as the base models for all experiments. SFT is applied to Typhoon2 70B, and we merge DeepSeek R1 70B with Typhoon2+SFT.\n\nAll models are trained using LoRA with a rank of 32 and a of 16, employing a cosine learning rate schedule with a peak learning rate of 4e-4 over a single training epoch. To enhance computational efficiency, we utilize sequence packing with a maximum length of 16,384, along with Liger kernels (Hsu et al., 2024), FlashAttention-2 (Dao, 2023), and DeepSpeed ZeRO-3. (Rajbhandari et al., 2020) Each experiment is conducted on 4\u00d7H100 GPUs for a maximum of 15 hours. Training is performed using axolotl. Model merging is performed using Mergekit (Goddard et al., 2025)."}, {"title": "EVALUATION", "content": "Evaluation is conducted on two aspects: (i) reasoning capability and (ii) performance on language tasks. First, the reasoning capability is assessed using standard benchmarks commonly employed for evaluating reasoning models, such as AIME 2024 and MATH-500 for the mathematics domain, and LiveCodeBench for the coding domain based on evaluation in Sky T1 (Team, 2025). We translate the math and code evaluation query to Thai using GPT-40 to assess the performance in Thai on reasoning tasks. Second, for languages tasks, we focus on assessing general instruction-following performance and the usability of LLMs in Thai. Which consist of IFEval, MT-Bench and Language accuracy. We also add Think accuracy, which represents how well the model can correctly output its thoughts before responding. All evaluation datasets are as follows:\n\n\u2022  IFEval: We use IFEval (Zhou et al., 2023) to evaluate instruction-following capabilities based on verifiable instructions and predefined test cases, measuring accuracy as the adherence metric. Alongside the standard (English) IFEval, we employ the Thai translated version scb10x/ifeval-th.\n\u2022  MT-Bench: We adopt MT-Bench, an LLM-as-a-judge framework, to assess responses based on correctness, fluency, and adherence to instructions. For Thai, we use Thai MT-Bench, proposed by ThaiLLMLeaderboard (10X et al., 2024; VISTEC, 2024), while the English version follows LMSYS (Zheng et al., 2023).\n\u2022  Language Accuracy: We use code-switching metrics based on Pipatanakul et al. (2024), inspired by IFEval (Zhou et al., 2023). Lang-acc measures the model ability to respond in the query language while adhering to natural linguistic conventions. Specifically, it must comply with two criteria: (1) Valid responses should contain only Thai and English, excluding Chinese or Russian, as these represent the languages commonly used by Thai speakers in daily life. (2) English usage must follow native conventions, with the total number of English characters being fewer than Thai characters. Evaluations are conducted using 500 sampled instructions from airesearch/WangchanThaiInstruct (Vistec, 2024), tested at T = 0.7 to ensure consistency in a non-greedy setting. More details are provided in A.2.\n\u2022  Think Accuracy: Inspired by R1 think patterns, think accuracy quantifies instances where the LLM generates structured \u201cthinking trace\u201c responses across the evaluation dataset. We measure think accuracy by assessing all responses obtained from other evaluations in this setup and aggregating their tendency to think before answering by checking the presence of the \u2018</think>\u2018 token and analyzing the content between think tokens. More details are provided in Appendix A.3.\n\u2022  MATH500: We use MATH-500 (Lightman et al., 2023) subset of the MATH benchmark Hendrycks et al. (2021), a dataset consisting of 500 problems to evaluate math reasoning ability. Additionally, we include a translated set, bringing the total to 1,000 problems.\n\u2022  AIME 2024: Also to evaluate math reasoning ability, we use AIME 2024, which is derived from 2024 USA Math Olympiad (AIME). The evaluation consists of 30 challenging problems. Additionally, we translate the dataset into Thai, resulting in a total of 60 problems."}, {"title": "RESULTS AND DISCUSSION", "content": "First, we examine the performance of our two base models on language and reasoning tasks to understand the performance gap between them.\n\nAs shown in 1, while DeepSeek R1 70B Distill performs well in reasoning tasks such as AIME and MATH500, its performance in IFEval is slightly worse but still acceptable. However, in MT-Bench-TH and language accuracy tasks, it falls significantly behind language-specific models such as Typhoon2 70B Instruct. On the other hand, Typhoon2 70B Instruct struggles with most reasoning tasks, achieving only 10% accuracy in AIME and trailing DeepSeek R1 70B Distill by more than 20% in MATH500."}, {"title": "MERGE RATIO", "content": "Based on the performance gap shown in Section 4.1, our goal is to combine the strengths of both models. To achieve this, we investigate model merging. In this section, We design two experiments to explore the merge ratio based on the intuition described in Section 2.3. We employ a merging strategy based on the dare-linear method (Yu et al., 2024) and constrain the search space by optimizing only the mixing ratios of two models: Typhoon+SFT-v1 and DeepSeek R1 70B Distill.\n\nTo conduct this experiment, we fine-tuned the SFT-v1 model, which includes Bespoke-Stratos (English) and a 2K Thai translation of Bespoke-Stratos. The details of the dataset used for SFT-v1 are provided in Table 6.\n\nIn this case, we try to confirm our hypothesis in Section 2.3 with this two questions"}, {"title": "WHICH MODEL SHOULD BE ASSIGNED A HIGHER RATIO IN THE FINAL MERGE TO BEST PRESERVE ITS REASONING CAPABILITIES?", "content": "In this experiment, we maintained a fixed ratio of merged to single constraints across all model layers. Specifically, we applied a 25%-75% merge ratio to each model combination. This experiment is represented by configurations M1 (More Typhoon) and M2 (More DeepSeek).\n\nAs shown in Table 3, our findings indicate that a high ratio of DeepSeek R1 70B Distill in M2 improves performance across all evaluation metrics including reasoning tasks. Even within 3% of original DeepSeek R1 70B Distill. However, there is still a degradation in the Language Accuracy task. This outcome aligns with expectations, as DeepSeek R1 70B Distill struggles to generate Thai reliably."}, {"title": "HOW DOES INCREASING TYPHOON'S CONTRIBUTION IN THE LATER LAYERS ENHANCE LANGUAGE-SPECIFIC PERFORMANCE?", "content": "After finding that M2 performs better on reasoning tasks and many language tasks but has lower language response accuracy, which may reduce its usefulness for end users, we explore ways to improve language response accuracy while preserving reasoning capability. To achieve this, we allocate a higher ratio to Typhoon in the later layers. This experiment is represented by M2 and M3."}, {"title": "SUPERVISED FINE TUNING (SFT): DATA MIXTURE", "content": "After identifying a merge configuration that effectively combines the abilities of two models in Section 4.2, we focus on optimizing the data mixture for the SFT model to enhance alignment before merging, ultimately improving end-to-end performance.\n\nIn this section, we explore the impact of the SFT dataset on overall model performance by addressing the following key dataset considerations\n\n1.  Does increasing the data mixture of Thai to 30% improve performance compared to 10%? \u2013 We investigate the impact of Thai-English data proportions, we add an additional 4.5k Thai translation examples based on translation of Bespoke-Stratos as in Section 2.2, which increase the Thai language ratio from 10% to 30%.\n2.  Does adding distilled reasoning traces on general Thai queries improve performance? \u2013 We hypothesize that Bespoke-Stratos primarily covers math, code, and puzzle domains, lacking diversity in instruction-following tasks. Does adding general-domain distillation with long-form reasoning improve performance? To test this hypothesis, we sample 1,000 prompts from the Thai general instruction dataset Suraponn/thai_instruction_sft, distill responses using DeepSeek R1, and apply rejection sampling to exclude non-Thai solutions, retaining approximately 50% of the samples. The final dataset consists of 500 examples.\n3.  Does adding a general instruction dataset improve performance? \u2013 We hypothesize that adding a general instruction dataset might improve dataset diversity and help prevent catastrophic forgetting. To investigate this, we incorporate 10,000 general instruction examples. For English, we use Capybara, and for Thai, we use Suraponn/thai_instruction_sft, following its usage in Typhoon 2 (Pipatanakul et al., 2024). Each dataset is subsampled to 10,000 examples to maintain balance."}, {"title": "DOES DIRECTLY MERGING THE ORIGINAL MODEL WORK?", "content": "Based on both merge configuration and SFT dataset mixture in Section 4.2 and 4.3 we also validate that whether merging alone, without any SFT, is sufficient for the model to function properly. In this experiment, we compare our best model (Typhoon2+SFT-v3+M3) with a directly merged version (Typhoon2+M3), skipping SFT entirely. Our results in Table 8 suggest that direct merging may not be effective, as it results in lower performance across all benchmarks."}, {"title": "DOES SFT ONLY MODEL WORK?", "content": "After verifying that the merged-only model does not work in Section 4.4, we also evaluate whether SFT alone, is effective. We set up the experiment in the same way as in Section 4.4. Specifically, we compare the best model (Typhoon2+SFT-v3+M3) with a directly fine-tuned version (Typhoon2+SFT-v3) without merging.\n\nAlthough there are examples of SFT improving performance in high-resource languages (Team, 2025; Labs, 2025), our results in Table 8 suggest that direct SFT alone is not effective. It results in lower performance across all benchmarks, which may be due to the limitations of language-specific LLM capabilities, the use of LoRA in our setup, or other factors an aspect left for future work."}, {"title": "FINAL MODEL", "content": "Based on the combination of all experiments in this work, we found that our best model, which we call Typhoon2-R1-70B, demonstrates the feasibility of leveraging model merging to combine the reasoning ability of DeepSeek R1 70B Distill with the Thai language proficiency of Typhoon2 70B Instruct. The results, presented in Table 10, suggest that Typhoon2-R1-70B achieves performance within approximately 4% of Typhoon2 70B Instruct on language tasks and comparable on reasoning tasks. Additionally, it boosts average across all tasks performance by 41.6% over Typhoon2 70B Instruct and by 12.8% over DeepSeek R1 70B Distill.\nWe also show additional sample responses from the model in Appendix A.5."}, {"title": "ADDITIONAL MODEL & ADDITIONAL LANGUAGE", "content": "Based on our final model configuration (Section 4.6), we investigate whether our method can be transferred to another model. To validate this, we design an experiment applying our approach to South-east Asia (SEA) language-specific model that supports Thai: Sealion v3 70B Instruct (Singapore, 2024).\n\nWe apply our final recipe to Sealion 70B to ensure that the method is transferable between language-specific LLMs. As shown in 11, we find that our method successfully transfers the reasoning capability of DeepSeek R1 70B to the Sealion model despite differences in the CPT and SFT recipes, similar to its effect on Typhoon. Additionally, it preserves comparable language performance.\n\nIn theory, our approach relies solely on translating the English reasoning dataset and target-language prompts. As a result, it should be adaptable to any language for which a language-specific model of the same reasoning model size and pretraining architecture is available. However, verifying this across additional languages is left for future work."}, {"title": "CONCLUSION & LIMITATION", "content": "In this work, we propose a method to enhance reasoning in language-specific models by combining two specialize models: one language-specific and another with long-thought reasoning capability. We showed that SFT & merging can be a practical resources alternative for teaching model a reasoning capability, however due to combination of merging and SFT technique, it has certain limitations. Experimentally, we focus solely on merging DARE (Yu et al., 2024) with a simple two-model setup and evaluate it on only one model family. Additionally, we do not optimize the instruction tuning subset, despite the availability of high-quality open-source instruction datasets such as Tulu3 (Lambert et al., 2025).\n\nAt a higher level, several challenges remain in the realm of multilingual reasoning and model merging. These include the absence of culturally aware reasoning traces, performance disparities between low-resource and high-resource languages, and a limited understanding of the internal representations of reasoning in LLMs. Nonetheless, our goal is to advance LLMs in underrepresented languages, ensuring they remain competitive within the broader AI community."}, {"title": "APPENDIX", "content": ""}, {"title": "EXAMPLE OF CODE-SWITCHING & LANGUAGE ACCURACY PROBLEM", "content": "In Figure 2 and Figure 3, we demonstrate the problem more concretely. We show how code-switching manifests in real-world situations. First, there is code-switching, where the LLMs incorporate incorrect language words into the response. Second, the model ignores the given language order and responds in its familiar language."}, {"title": "LANGUAGE ACCURACY EVALUATION", "content": "In order to evaluate language accuracy, such as the example in Appendix A.1 we focus on creating a verifiable rule that has two sub-rules:\n\n1.  Valid responses should contain only Thai and English characters, excluding Chinese, Russian, or Vietnamese, as these represent languages commonly used by Thai speakers in daily life.\n2.  English usage must follow native conventions, with the total number of English characters being fewer than Thai characters.\n\nThe verifiable rule pseudo-code is shown in A.2.\n\nTo ensure the validation works, we use prompts based on the airesearch/WangchanThaiInstruct (Vistec, 2024) (Vistec, 2024) test set, due to its authenticity and the fact that it is the only Thai instruction dataset created by humans in Thai, making it representative of real prompts that a Thai person would write. To prevent unclear"}, {"title": "THINK ACCURACY EVALUATION", "content": "To evaluate think accuracy-the rate at which an LLM correctly utilizes the ability to think before generating a response-we define the problem as a verifiable rule for both the format and content of the thought process. Specifically, DeepSeek R1 uses the '\u00a1think\u00bf' and '\u00a1/think\u00bf' tokens to separate its reasoning from the final solution. Our evaluation focuses on verifying whether the LLM-generated response meets the following criteria:\n\n1.  Does it follow the format? Does the response correctly include the '' and ''tokens?\n2.  Does it actually think? \u2013 Our initial investigation revealed that DeepSeek R1 70B Distill, even when correctly formatting its response with \u2018'and \u2018', sometimes generates an empty thought, such as \u2018<think>\n\n</think>\u2018\n\nTo apply this evaluation across various use cases, we enforce these verifiable rules on all responses generated across multiple benchmark datasets, including MT-Bench, IFEval, language-accuracy, AIME, MATH500, and LiveCodeBench. We then compute the accuracy based on the model's tendency to both format its thoughts correctly and generate non-empty reasoning.\n\nThe pseudocode for the verifiable rule implementation for think accuracy is provided in A.3."}, {"title": "MERGE CONFIG", "content": "To enhance understanding and transparency of our recipe, we provide our merge configuration for Mergekit below (Goddard et al., 2025)."}, {"title": "AN EXAMPLE RESPONSE FROM OUR MODEL", "content": "The model successfully responds fully in Thai while reasoning through its thought process on general question."}, {"title": null, "content": "To ensure the validation works, we use prompts based on the airesearch/WangchanThaiInstruct (Vistec, 2024) (Vistec, 2024) test set, due to its authenticity and the fact that it is the only Thai instruction dataset created by humans in Thai, making it representative of real prompts that a Thai person would write. To prevent unclear"}, {"title": null, "content": "\u9996\u5148,\u6211\u4eec\u9700\u8981\u5c06\u76f4\u89d2\u5750\u6807\u7cfb\u4e2d\u7684\u70b9(0,3)\u8f6c\u6362\u4e3a\u6781\u5750\u6807\u7cfb(r, 0)\u3002\n\n1.\u8ba1\u7b97\u534a\u5f84r:\n\n$r=\\sqrt{x^{2}+y^{2}}=\\sqrt{0^{2}+3^{2}}=\\sqrt{9}=3$\n\n2. \u8ba1\u7b97\u89d2\u5ea6\u65e5:\n\n\u2022 \u5f53x=0\u4e14y>0\u65f6,\u70b9\u4f4d\u4e8e\u6b63y\u8f74\u4e0a,\u5bf9\u5e94\u7684\u89d2\u5ea6\u662f\u03c0/2\u3002\n\n\u2022 \u56e0\u6b64, \u03b8 = \u03c0/2.\n\n\u7efc\u4e0a\u6240\u8ff0,\u8f6c\u6362\u540e\u7684\u6781\u5750\u6807\u8868\u793a\u4e3a:\n\n$(3, \\frac{\\pi}{2})$"}, {"title": null, "content": "\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e41\u0e1b\u0e25\u0e07\u0e08\u0e38\u0e14\u0e08\u0e32\u0e01\u0e1e\u0e34\u0e01\u0e31\u0e14\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21 (\u0e04\u0e32\u0e23\u0e4c\u0e17\u0e35\u0e40\u0e0b\u0e35\u0e22\u0e19) \u0e40\u0e1b\u0e47\u0e19\u0e1e\u0e34\u0e01\u0e31\u0e14\u0e40\u0e0a\u0e34\u0e07\u0e02\u0e31\u0e49\u0e27 (\u0e42\u0e1e\u0e25\u0e32\u0e23\u0e4c) \u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\u0e2a\u0e39\u0e15\u0e23\u0e14\u0e31\u0e07\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e19\u0e35\u0e49:\n\n\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e08\u0e38\u0e14 (x, y) \u0e43\u0e19\u0e23\u0e30\u0e1a\u0e1a\u0e1e\u0e34\u0e01\u0e31\u0e14\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21 \u0e1e\u0e34\u0e01\u0e31\u0e14\u0e40\u0e0a\u0e34\u0e07\u0e02\u0e31\u0e49\u0e27\u0e08\u0e30\u0e04\u0e33\u0e19\u0e27\u0e13\u0e44\u0e14\u0e49\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49:\n\n\u0e42\u0e14\u0e22\u0e17\u0e35\u0e48:\n\n$r = \\sqrt{x2 + y2}$ \n$0 = arctan(\\frac{y}{x})$\n\n\u2022 r \u0e41\u0e17\u0e19\u0e23\u0e31\u0e28\u0e21\u0e35 (\u0e23\u0e30\u0e22\u0e30\u0e2b\u0e48\u0e32\u0e07\u0e08\u0e32\u0e01\u0e08\u0e38\u0e14\u0e01\u0e33\u0e40\u0e19\u0e34\u0e14)\n\n0 \u0e41\u0e17\u0e19\u0e21\u0e38\u0e21 (\u0e21\u0e38\u0e21\u0e17\u0e35\u0e48\u0e27\u0e31\u0e14\u0e08\u0e32\u0e01\u0e41\u0e01\u0e19 x \u0e1a\u0e27\u0e01\u0e44\u0e1b\u0e22\u0e31\u0e07\u0e15\u0e33\u0e41\u0e2b\u0e19\u0e48\u0e07\u0e02\u0e2d\u0e07\u0e08\u0e38\u0e14)\n\n\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07: \u0e08\u0e38\u0e14 (0,3)\n\n1. \u0e04\u0e33\u0e19\u0e27\u0e13 r:\n\n$r = \\sqrt{02 + 32} = 0 + 9 = 9 = 3$\n\n2. \u0e04\u0e33\u0e19\u0e27\u0e13 0:\n\n\u0e40\u0e21\u0e37\u0e48\u0e2d x = 0 \u0e40\u0e23\u0e32\u0e08\u0e30\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e43\u0e0a\u0e49\u0e2a\u0e39\u0e15\u0e23 $0 = arctan (\\frac{y}{x})$ \u0e44\u0e14\u0e49\u0e42\u0e14\u0e22\u0e15\u0e23\u0e07 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e23\u0e14\u0e49\u0e27\u0e22\u0e28\u0e39\u0e19\u0e22\u0e4c\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e17\u0e33\u0e44\u0e14\u0e49 \u0e43\u0e19\u0e01\u0e23\u0e13\u0e35\u0e19\u0e35\u0e49 \u0e40\u0e21\u0e37\u0e48\u0e2d x = 0 \u0e41\u0e25\u0e30 y > 0 \u0e21\u0e38\u0e21 9 \u0e08\u0e30\u0e2d\u0e22\u0e39\u0e48\u0e17\u0e35\u0e48 (90 \u0e2d\u0e07\u0e28\u0e32) \u0e0b\u0e36\u0e48\u0e07\u0e15\u0e23\u0e07\u0e01\u0e31\u0e1a\u0e41\u0e01\u0e19 y \u0e1a\u0e27\u0e01\n\n\u0e2a\u0e23\u0e38\u0e1b\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c:\n\n\u0e1e\u0e34\u0e01\u0e31\u0e14\u0e40\u0e0a\u0e34\u0e07\u0e02\u0e31\u0e49\u0e27\u0e02\u0e2d\u0e07\u0e08\u0e38\u0e14 (0, 3) \u0e04\u0e37\u0e2d (3,5)"}]}