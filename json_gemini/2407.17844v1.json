{"title": "Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease Classification: A Systematic Review", "authors": ["Lisanne van Gelderen", "Cristian Tejedor-Garc\u00eda"], "abstract": "Parkinson's disease (PD), the second most prevalent neurodegenerative disorder world- wide, frequently presents with early-stage speech impairments. Recent advancements in Artificial Intelligence (AI), particularly deep learning (DL), have significantly enhanced PD diagnosis through the analysis of speech data. Nevertheless, the progress of research is restricted by the limited avail- ability of publicly accessible speech-based PD datasets, primarily due to privacy and ethical concerns. This review covers the latest DL-based AI approaches for speech-based PD classification, focusing on performance, available resources and associated challenges of 33 scientific works published be- tween 2020 and March 2024. These DL approaches are categorized into end-to-end (E2E) learning, transfer learning (TL) and deep acoustic features (DAF) extraction. Among E2E approaches, Con- volutional Neural Networks (CNNs) are prevalent, though Transformers are increasingly popular. E2E approaches face challenges such as limited data and computational resources, especially with Transformers. TL addresses these issues by providing more robust PD diagnosis and better general- izability across languages. DAF extraction aims to improve the explainability and interpretability of results by examining the specific effects of deep features on both other DL approaches and more traditional machine learning (ML) methods. However, it often underperforms compared to E2E and TL approaches. This review also discusses unresolved issues related to bias, explainability and privacy, highlighting the need for future research.", "sections": [{"title": "1. Introduction", "content": "Parkinson's disease (PD) is a neurodegenerative disorder that affects more than 10 million people around the world, being the second most common neurodegenerative disease after Alzheimer's [1]. People are generally diagnosed around an age of 65 years old [2]. Some of the symptoms include tremors, rigidity and slowness of movement. These symptoms, tending to appear in later stages, make early diagnosis based solely on them difficult [3]. PD can affect various aspects of speech production, including articulation, phonation, prosody, and voice quality [4], causing speech impairments characterized by dysphonia [5] and dysarthria [6]. Dysphonia implies a decrease in the ability to produce vocal sounds, while dysarthria refers to the difficulty of producing words. These speech impairments can occur up to five years before other symptoms, suggesting that early PD diagnosis might be actually possible [7]. Impaired speech is not always easy to recognize for humans at an early stage of the disease, however, a considerable amount of literature has shown the potential of automated speech-based methods to diagnose neurological disorders, including PD, from the speech signal even at the most early stages [7-9]. Such methods can be deployed for several speech tasks, of which speech classification and speech processing are the main ones. Speech classification generally aims to distinguish between people with Parkinson (PWP) and healthy controls (HC) [10]. Using speech processing techniques, acoustic features (also known as acoustic biomarkers) can be extracted from speech signals to capture these changes. Classical features include fundamental frequency (F0), jitter, shimmer, Mel Frequency Cepstral Coefficients (MFCCs) and formant frequencies [1]. However, recent studies in the literature employ deep acoustic features (DAF) from end-to-end (E2E) systems, auto-encoders, or pre-trained models [11-13]. These features can be utilized not only in the final layers of a deep learning (DL) model but also in more traditional machine learning (ML) approaches, offering more interpretable results but potentially resulting in lower performance. Until 2016, traditional methods such as Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) were the standard ML approach for speech processing tasks [14]. GMMs and HMMs typically use an acoustic, language and pronunciation model. Training each of these models separately and aligning their training data can be labor intensive. Recently, DL has emerged as the leading computational method in AI and its subfield, ML, delivering outstanding outcomes on a variety of complex cognitive tasks, and sometimes even outperforming humans [14,15]. ML approaches require manual feature extraction, whereas DL ones automatically extract deep features [16]. Moreover, these features tend to be more abstract and robust [16], potentially leading to a more accurate diagnosis of PD. Studies that have been published in most recent years within the realm of speech-based DL approaches ford PD classification mainly focus on E2E learning [17-19] and transfer learning (TL) [20-22]. Furthermore, there has been a growing interest in privacy-related issues in the speech- based Al research [23,24]. Scientists increasingly question the reliability of speech-based Al systems, becoming more aware of concerns and risks over the trustworthiness of these systems since speech can be used as a biometric identifier to identify individuals and obtain information about their health status [23]. Datasets containing the speech of patients with PD are sensitive in nature, safety and fairness (e.g. without bias) need to be ensured [23]; therefore, the number of such datasets that are publicly available is limited (see a list of such datasets in Table A1). This review presents an overview of the most recent, innovative speech-based DL approaches for PD classification as of March 2024. Previously, other authors have reviewed different approaches to PD classification from various perspectives. Ngo et al. [1] summa- rized the literature from 2010 to 2021 related to voice and speech ML and DL approaches for PD classification and the assessment of its severity. Saravanan et al. [10] presented a review of papers until 2021 of ML and DL methods that are not only related to speech but also to physiological signals, such as electroencephalogram (EEG), and neuroimaging tech- niques, such as functional magnetic resonance imaging (fMRI) and single-photon emission computed tomography (SPECT). In this review, we focus on the most recent (from 2020 to March 2024) innovative DL-based approaches related to speech, which, to the best of our knowledge, have not yet been covered in previous literature reviews, guided by the overarching research question: What is the current landscape of speech-based DL approaches for PD classification?, which has been further subdivided into the following ones: * RQ1. Which recent speech-based DL approaches are considered for PD classification? * RQ2. To what extent can speech-based DL approaches classify PD? * RQ3. Which are the issues related to bias, explainability and privacy of speech-based DL approaches for PD classification? The contributions of this systematic literature review are as follows: 1. This review classifies and discusses the most recent speech-based DL approaches for PD classification until March 2024. It explicitly focuses on speech-based data material and DL approaches. 2. This review includes an overview of publicly available speech-based PD datasets until March 2024."}, {"title": "3. E2E Approach", "content": "In this section we describe several studies which propose an speech-based E2E ap- proach for PD classification. In this approach the raw speech signal can be directly mapped to the final output [26]. This contrasts with the classical speech recognition pipeline that uses GMMs and HMMs. In this traditional approach, handcrafted features such as MFCCs must be extracted to build an acoustic model. These features are then mapped to textual symbols using a language model before final classification can be performed [14]. We categorize the speech-based E2E approaches for DL classification into three subapproaches: CNNs and LSTMs, hybrid CNN-LSTM architectures and Transformers."}, {"title": "3.1. CNNs and LSTMs", "content": "CNNs are primarily designed for processing images and generally consist of con- volutional, pooling and fully-connected layers [27]. Spectrograms, which represent the frequency spectrum of audio signals over time as an image, can be given as input to a CNN to identify patterns that are indicative of PD [28]. An LSTM is a type of Recurrent Neural Network (RNN) designed to capture long-range dependencies. Traditional RNNs usually use only one hidden state maintaining an internal memory, while LSTMs typically consist of an input gate, an output gate and a forget gate [29]. This allows the LSTM to learn which information is relevant over longer sequences, which are also contained in the audio signals obtained from PD patients [30]. Khaskhoussy & Ayed [31] implement a SVM and LSTM classifier and applied them on five different datasets. The LSTM classifier consistently obtains a better F-score on all datasets (99.0%, 99.0%, 81.0%, 83.0% and 94.0%, respectively) than the SVM clasifier (98.0%, 97.0%, 68.0%, 66.0% and 69.0%, respectively). The authors also report a significant difference in performance between men and women PD classification in favor of the masculine gender. Quan et al. [17] introduce an E2E method consisting of two modules. The first module contains series of time-distributed 2D-CNN blocks to transform the input to time series dynamic features. In the second module, a 1D-CNN block is built to learn the dependencies between these features. The authors preferred this 1D-CNN over a RNN architecture (e.g. LSTM), despite that RNNs have been specifically developed for sequential input. For each single element in a given sequence, RNNs consider not only that specific element, but also all preceding elements in the sequence. However, time dynamic features are often locally correlated, resulting in local motifs that are more easily detected by 1D-CNNs than RNNs. Quan et al. [17] test their method against two databases: a Chinese database collected at the GYENNO SCIENCE Parkinson's Disease Research Center, and PC-GITA, which is a Spanish database originally presented in [32]. On both databases, the proposed method outperforms traditional ML methods such as Support Vector Machines (SVM), K-Nearest Neighbours (KNN) and Decision Tree (DT), and reaches accuracy scores in a range of 66.67% to 73.33%. The CNN is able to reach an accuracy of 81.6% and 75.3% for sustained vowels and short sentences in Chinese, respectively. On the Spanish database, consisting of vowels, words and sentences, an accuracy of even 92% is achieved. To increase the interpretability of their E2E approach, Quan et al. [17] apply a feature visualization method, Grad-class activation maps (Grad-CAM), and show that their CNN tends focus on the lower frequency regions rather than than high-frequency regions of the log Mel-spectrogram. They apply the CNN again using low-frequency (ranging from 0 to 32), high-frequency (ranging from 0 to 64), and full-frequency Mel-spectrograms, as input. The best Area Under the Curve (AUC), for both Chinese and Spanish, is obtained when the lower frequency regions are used as input indicating that lower frequencies are more influential than higher frequencies. The AUC indicates the area under the Receiver Operating Characteristic (ROC) curve of a classification model [33]. The ROC shows the relationship between the True Positive Rate and False Positive Rate of the model at several thresholds. Similar to Quan et al. [17], Akila et al. [19] indicate that not all features are necessary to achieve high performance. It introduces a MASS-PCNN, which combines a multi-agent salp swarm (MASS) algorithm with a novel PD classification neural network (PCNN). The MASS algorithm, an updated version of SSA, selects relevant features based on various patterns. These selected features are then input into the PCNN, which includes several convolutional and pooling layers, an inception module, and a squeeze-and-excitation (SE) module. The pooling layers summarize the most important features, the SE module enhances feature importance, and the inception module captures multi-scale data. The authors argue that these additions make their CNN architecture more discriminative, achieving an accuracy of 95.1%. In many other studies CNNs have been employed for PD classification [3,22,26,34-39]. In [37], Zhang et al. reach an accuracy of 91%. Narendra et al. [26] implement both a traditional and E2E pipeline approach leading to an accuracy of 67.93% for the former and 68.56% for the latter. Although, the E2E approach is able to outperform the more traditional approach, the performance is considerably lower as compared to other studies. In [34], an Acoustic Deep Neural Network (ADNN), Acoustic Deep Recurrent Neural Network (ADRNN), and Acoustic Deep Convolutional Neural Network (ADCNN) are distinguished. They demonstrate that the CNN architecture, ADCNN, slightly outperforms the more regular DNN and RNN (99.92% versus 98.96% and 99.88%, respectively). In [35], the authors also compare a CNN and LSTM classifier using MFCCs and Gammatone Cepstral Coefficients (GTCCs) on several subsets of the PC-GITA [32] and the Parkinson's Disease Classification [40] datasets. The experiments demonstrate that the CNN significantly outperforms the LSTM classifier, and the GTCCs provide greater accuracy compared to the MFCC, reaching an accuracy of 100%, which implies none of the PWP and HC are misclassified. In [22], the authors use both CNN and Transformer (see Section 3.3 for more details) architectures. As CNNs, they use MobileNetV2, DenseNet201, DenseNet169, DenseNet121, ResNet152, ResNet50, GoogleNet, VGG19 and VGG16. The lowest accuracy is reported for the MobileNetV2 and VGG19, which led to an accuracy of 95.61%, and the highest accuracy is reported for ResNet152 for which 98.08% accuracy is reported. In this study, the authors use the Synthetic vowels of speakers with PD and Parkinsonism dataset [41], on which more information can be found in Table A1. Finally, other studies on the performance of CNN-based auto-encoders have demon- strated varying levels of accuracy across different databases, privacy-aware methods and noisy environments. Sarlas et al. [38] report an accuracy of 61.49% on the MVDR-KCL database following a Federated Learning (FL) method; whereas Janbakhshi et al. [39] observe a higher accuracy of 75.4% on the PC-GITA dataset [32] using speaker identity invariant representations and adverserial training. In Farag\u00f3 et al. [36], it is pointed out that CNNs can still reach an accuracy ranging from 92% to 96% in noisy environments on a custom private small dataset of 27 participants. In Hire\u0161 et al [3], the authors demonstrate that adding noise can even improve the accurateness with which PD can be classified on the PC-GITA dataset."}, {"title": "3.2. Hybrid CNN-LSTM architectures", "content": "While CNNs typically outperform LSTMs in PD classification tasks, some studies indicate that hybrid CNN-LSTM architectures can also perform effectively. Er et al. [28] use Variational Mode Decomposition to denoise the speech signals and obtain the Mel- spectrograms. Next, pre-trained ResNets models (ResNet-18, ResNet-50 and ResNet-101) are used to extract deep features from the Mel-spectrograms, which are then given as input to the LSTM model. The best result is obtained when combining the ResNet-101 and LSTM, leading to an accuracy of 98.61%. Mallela et al. [42] use four different types of speech tasks stimuli: subjects describing images shown to them on a computer screen in their native language (IMAG), sustained phonemes (PHON), monosyllabic targets (DIDK), and spontaneous speech (SPON). The authors apply stimuli-specific and pooled models to each of these stimuli. The stimuli- specific models are trained and tested on one specific speech task stimulus (e.g. train on IMAG and test on IMAG), whereas the pooled models are trained on all types of speech tasks stimuli and tested on each stimulus individually. For both the stimuli-specific and pooled model the highest accuracy (96.20% and 98.27%) is achieved when using SPON, followed by IMAG, DIDK, and PHON. For PHON, the accuracy scores are considerably lower than for the other speech stimuli, ranging from 59.45% to 73.57% and 70.25% to 82.18% for the stimuli-specific and pooled model, respectively. It can be observed that the pooled model outperforms the stimuli-specific model in all cases. This suggests that the hybrid CNN-LSTM benefits from receiving various types of speech tasks as input during training. In another study performed by these authors [43], they again employ a hybrid CNN-LSTM architecture and are able to reach an accuracy of 90.98%. Initially, this model is trained to predict three classes, Amyotrophic Lateral Sclerosis (ALS), PD and HC, and then is fine-tuned to perform binary classification between PD and HC only."}, {"title": "3.3. Transformers", "content": "Transformers were originally developed for Natural Language Processing (NLP) appli- cations [44]. Unlike RNNs and CNNs, Transformers do not use recurrent and convolutional mechanisms. Instead, Transformers use the so-called attention mechanism which allows for parallelization. In several studies [45-47], the Transformer architecture has been adopted for PD diagnosis. Nijhawan et al. [48] implement a Vocal Tab Transformer consisting of a feature embedder, Transformer encoder and Multilayer Perceptron (MLP) head. They name their proposed method Vocal Tab Transformer, because they obtain dysphonia measures, i.e. tabular vocal features, from the subjects' voice recordings. A XgBoost model is trained to estimate the importance of these features, of which only the most important ones are given as input to the feature embedder block. However, one of the main advantages of DNN architectures such as Transformers is that they can just take in the voice recordings as raw input [16], instead of performing a feature extraction and feature selection process, first. Although [48] are able to reach an AUC of 0.917, other studies (e.g. [45] and [46]) show even better performances, without needing to implement any additional feature extraction and selection steps. In [45], Chronowski et al. use Wav2vec2.0 [49] as a backbone model and reach an accuracy of 97.92% for the classification of PD and healthy control subjects. The authors also attempt to predict the severity of the disease based on the Hoehn & Yahr (H&Y) scale. This attempt is less successful. Unfortunately, the authors do not mention the actual performance for this task, making it difficult to determine the extent of their lack of success. Malekroodi et al. [46] reach an accuracy of 98.5% using a Swin Transformer for the classification of PWP and HC. The margin with the CNN architecture they implemented, a VGG16, is not that large, however. With the VGG16, an accuracy of 98.1% was achieved, which is only 0.4 percentage point lower than the accuracy that was reached with the Swin Transformer. Nevertheless, it shows the potential of Transformer architectures for PD classification. Furthermore, the authors discriminate between HC, mild PD, and severe PD, based on the Unified Parkinson's Disease Rating Scale (UPDRS). They show that the Swin Transformer has strong capabilities to discriminate between HC and PD, showing a precision of 95%. However, discriminating between only two classes, mild and severe PD, appears to be more difficult, resulting in a precision of 85%. Although the recognition of different stages of PD is still not optimal, there already seems to be some improvement as compared to the unsuccessful attempt to discriminate between five different classes in [45]. In Section 3, it was mentioned that [22] use several CNN and Transformer models to classify PD. For each of these models, the authors use also the TL approach, which will be discussed in more detail in Section 4. The Transformer models are the ViT-L-32 and ViT-B-16, which are both Vision Transformers (ViT), and lead to an accuracy of 96.74% and 91.57%, respectively. For most of the CNN models, such as ResNet50 (see Section 3 for more details), a higher performance was reported than for these Transformers, although the margin is small. The authors conclude from this that the latest and more advanced models are not necessarily better in terms of performance than the more conventional ones."}, {"title": "4. TL Approach", "content": "TL allows to transfer knowledge from a source domain to a target domain [50]. In most cases, a DNN is pre-trained on a large-scale database for a certain task, which we refer to as the source domain, and then re-trained for another task usually referred to as the target domain [50]. Karaman et al. [20] use three different architectures (SqueezeNet1_1, ResNet50 and DenseNet161) that have already been trained on ImageNet [51], a database consisting of more than 14 million images. ResNet and DenseNet are dense architectures while SqueezeNet is a lightweight architecture with fewer parameters [20]. At first, the authors froze the convolutional layers of the three architectures, and only the fully connected layers are retrained to explore the learning rate from 1.0x10-6 to 1.0. Then, the convolutional layers are unfrozen and the learning rate dynamically updated. With a dynamically updated learning rate, the minimum loss can be determined more accurately. Of the three architectures, DenseNet161 obtained the highest accuracy (89.75%). Furthermore, the DenseNet-161 obtained a test time of 0.029\u00b10.028 and a training time of 0.003\u00b10.005, which is satisfactory and acceptable for clinical practice. Hires et al. [5,52] use the German Saarbruecken Voice Database [53] and the Colombian Spanish PC-GITA dataset as their main databases [32], in their cross-lingual studies. More information about both datasets can be found in Table A1. A cross-lingual architecture is pre-trained on one certain language, also called the base language, and is then applied to another language, which is often referred to as the target language. In [5] an ensemble of CNN networks is introduced using a Multiple Fine-Tuning (MFT) approach. The ensemble consists of three base CNN networks, of which each network is pre-trained on ImageNet but fine-tuned on a different dataset. The first CNN network is fine-tuned on PC-GITA whereas the second CNN network is fine-tuned on the Vowels dataset [54], and then further- fine-tuned on PC-GITA. The third CNN network is fine-tuned on the Saarbruecken Voice Database and PC-GITA datasets. The Vowels dataset is not included in Table A1 since it does not contain any information about pathological diseases [5]. The CNN architectures used in this study are the ResNet50 and Xception networks. In general, fine-tuning these two types of CNNs on multiple databases (MFT) seems to boost the performance. For Xception, using MFT is associated with a higher accuracy in three of the five vowel utterance tasks. For ResNet50, this is the case for even four of these tasks. Furthermore, the ensemble of multiple fine-tuned CNNs generally performs better than the multiple fine-tuned CNNs individually. This shows the potential of both the MFT and ensemble approach proposed by these authors. In [52], Hires et al. train and test a CNN network on a single dataset, unseen data and a mix of datasets. These datasets include the private Czech Parkinsonian dataset (CzechPD) [55], the PC-GITA dataset [32], the Italian Parkinson's voice and speech dataset [56] and the RMIT-PD dataset [57]. The authors of the experiment describe that the CNN performed well on each a single dataset, showing an accuracy of 90.82%, 90.52%, 97.81%, and 94.83% for the CzechPD, PC-GITA, Italian dataset and RMIT-PD, respectively. However, when the CNN is tested on unseen data or a mix of the different datasets the performance decreases significantly, leading to accuracy scores between 43.07% and 78.85%. As compared to Hires et al. [52], Vasquez-Correa et al. [21] are able to obtain slightly better results with their transfer learning approach. They obtained accuracy scores between 61.5% and 77.3% with either Czech, German or Spanish as a base language and one of the remaining two as a target language. When Spanish is used as a base language, this generally led to higher performance (77.3% and 72.6%) than when Czech or German (70.0%, 72.0%, 76.7%, 70.7%) is used as a base language. This is line with earlier work of these authors [58], in which they demonstrate that the accuracy increased from 69.3% to 77.3% for German and from 68.5% to 72.6% for Czech while using Spanish as the base language. Orozco-Arroyave et al. [59] also observe that Spanish is generally a better base language than Czech and German for PD classification. When Spanish is the base language, accuracy scores around 90% are reached. When Czech is the base language, accuracy scores within a considerable wider and lower (range of 60 to 80% of accuracy). Specifically, these researchers experiment with moving fractions of the target language to the training data, and excluding these fractions, then, from the test data. Only 30% has to be moved when German is the target language and Spanish the base language, to obtain an accuracy of 90% approximately. However, when Czech is the base language, the accuracy reaches from 60%, which is considerably low. Arasteh et al. [60] employed a FL approach on specifically PC-GITA (Spanish), a German dataset with 176 subjects (88 PD and 88 HC) and a Czech dataset with 100 subjects (50 PD and 50 HC). They utilized a model consisting of four fully connected layers (1024, 256, 64, and 2) with Wav2vec2.0 embeddings. The model achieved a high robustness across different languages and population with accuracies of 83.2%, 78.9%, and 77.8% on the Spanish, German, and Czech datasets, respectively. Finally, there are other works in the literature which deal with classification of several diseases using the same approach. In [21], the authors combine CNNs with TL among different diseases. A base model is trained on either Parkinson's or Huntington's disease (HD), and then the model is tested on the other disease. The CNNs are more accurate in distinguishing between HD and non-HD, than PD and non-PD. Not surprisingly, the models pre-trained on HD tend to be more accurate than models that are pre-trained on PD. This shows again that some base models transfer better than others. In other words, similar to what is observed for TL among languages [58], HD seems to transfer better than PD [21]. Another study that uses TL among diseases is [37], which has already been mentioned in Section 3.2. Although several studies take a cross-pathological approach, more studies in the existing literature are cross-lingual. These cross-lingual studies are not limited to pre-trained DNN architectures and TL applications. Also for studies focused on deep feature extraction, a cross-lingual approach seems to become more popular. In Section 5 it is discussed in greater detail."}, {"title": "5. DAF Approach", "content": "DAF refers to deep features that are automatically extracted from an audio signal using a DL model. These features are either learned in the final layers of the DL model or used as input for a ML model [11]. There are a few recent studies that employ DAF for PD classification using different DL and ML approaches. For instance, Karan et al. [61] propose a stacked auto-encoder DNN framework to classify PD and HC using speech material from the PC-GITA database. They achieve an accuracy of 87% using time-frequency deep features from spectrograms and a softmax deep classifier in the final layer, outperforming a classical ML-based SVM classifier (which achieves an accuracy of 83%). Ferrante et al. [12] aim to understand how well a classification model trained on DAF in one language works in a different target language. To investigate this, they use three different architectures (Wav2Vec2.0, VGGish, and SoundNet) to generate the DAF. They alternate between English and Telugu as source and target languages, with Telugu being an under-resourced language. This could affect the accuracy of PD classification in this language. However, the results in [12] show that an accuracy higher than 90% is achieved for both English and Telugu, despite Telugu being an under-resourced language. This accuracy is observed for both the traditional features (MFCCs, F0, jitter, shimmer) and the DAF. Another study that uses DAF in multi-lingual and cross-lingual settings is carried out by Favaro et al. [13]. They compare the performance of interpretable feature-based models (IFM) and non-interpretable feature-based models (NIFM). They also consider pitch, loudness and variation as interpretable features, similar to Ferrante et al. [12]. Non- interpretable features are the DNN embeddings extracted with Wav2Vec2.0, TRILLsson, and HuBERT. According to the authors, their work is the first to apply TRILLsson and HuBERT representations to PD diagnosis and Wav2Vec2.0 in multi-lingual and cross-lingual settings [13]. They demonstrate that in mono-lingual, multi-lingual and cross-lingual settings, NIFMs outperform IFMs. In the mono-lingual setting, IFMs obtained an accuracy that is 4% lower than for NIFMs. In the multi-lingual setting, classifiers are trained on all languages individually, and NIFMs outperform IFMs with a 7% margin. The results are better than in a mono-lingual setting suggesting that using more languages improves PD diagnosis. In the cross-lingual setting, the classifiers are trained on all languages but the target language and there the NIFMs again outperform the IFMs, this time by 5.8% in terms of accuracy. One of the datasets Favaro et al. [13] use is the NeuroVoz database [62], detailed in Table A1. Jeancolas et al. [9] also use DAF, which they call X-vectors, instead of traditional MFCCs. They demonstrate that PD detection performs better in men due to greater variability in female MFCC distributions, which complicates MFCC-based classification in women. Despite improvements with X-vectors of 7% to 15% and discriminant analyses, gender differences persist due to factors such as less pronounced brain atrophy and different speech neural circuits in women. Ma et al. [63] introduce deep dual-side learning, comprising both deep feature learning and deep sample learning. For deep feature learning, the authors use an embedded stack group sparse auto-encoder (EGSAE) is to acquire the deep features. Then, these deep features are fused with the original features using L1 regularization. This results in a hybrid feature set. For deep sample learning, hierarchical sample spaces are created using an iterative clustering algorithm (IMC). Classification models are applied to each of these sample spaces. A weighted fusion mechanism fuses the different models into an ensemble model, thereby combining deep feature and deep sample learning. With this method, accuracy scores of 98.4% and 99.6% are obtained on the LSVT Voice Rehabilitation Dataset [64] and the Parkinson's Disease Classification dataset [40], respectively. Whereas the Parkinson's Disease Classification dataset is publicly available (see Table A1), the raw audio files of the LSVT dataset are private. Finally, Laganas et al. [65] aim to develop a \u201cprivacy-aware\u201d method for classifying PD between PWP and HC using running speech signals from passively-captured voice call recordings. The study involves a multi-lingual cohort of 498 subjects, comprising 392 HC and 106 PWP. A key feature of this method is that the data processing is performed locally on the smartphone, ensuring privacy by not transmitting sensitive information. The AUC for this classification method is 0.84 for the English sub-cohort, 0.93 for the Greek sub-cohort, and 0.83 for the German sub-cohort, demonstrating the method's efficacy across different languages."}, {"title": "6. Discussion", "content": "This section summarizes the main lessons learned from each research question, estab- lishes connections with the existing literature, and elaborates on the implications for the research community."}, {"title": "6.1. Findings and Implications", "content": "Regarding RQ1. Which speech-based DL approaches are considered for PD classification?"}, {"title": "Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease Classification: A Systematic Review", "authors": ["Lisanne van Gelderen", "Cristian Tejedor-Garc\u00eda"], "abstract": "Parkinson's disease (PD), the second most prevalent neurodegenerative disorder world- wide, frequently presents with early-stage speech impairments. Recent advancements in Artificial Intelligence (AI), particularly deep learning (DL), have significantly enhanced PD diagnosis through the analysis of speech data. Nevertheless, the progress of research is restricted by the limited avail- ability of publicly accessible speech-based PD datasets, primarily due to privacy and ethical concerns. This review covers the latest DL-based AI approaches for speech-based PD classification, focusing on performance, available resources and associated challenges of 33 scientific works published be- tween 2020 and March 2024. These DL approaches are categorized into end-to-end (E2E) learning, transfer learning (TL) and deep acoustic features (DAF) extraction. Among E2E approaches, Con- volutional Neural Networks (CNNs) are prevalent, though Transformers are increasingly popular. E2E approaches face challenges such as limited data and computational resources, especially with Transformers. TL addresses these issues by providing more robust PD diagnosis and better general- izability across languages. DAF extraction aims to improve the explainability and interpretability of results by examining the specific effects of deep features on both other DL approaches and more traditional machine learning (ML) methods. However, it often underperforms compared to E2E and TL approaches. This review also discusses unresolved issues related to bias, explainability and privacy, highlighting the need for future research.", "sections": [{"title": "1. Introduction", "content": "Parkinson's disease (PD) is a neurodegenerative disorder that affects more than 10 million people around the world, being the second most common neurodegenerative disease after Alzheimer's [1]. People are generally diagnosed around an age of 65 years old [2]. Some of the symptoms include tremors, rigidity and slowness of movement. These symptoms, tending to appear in later stages, make early diagnosis based solely on them difficult [3]. PD can affect various aspects of speech production, including articulation, phonation, prosody, and voice quality [4], causing speech impairments characterized by dysphonia [5] and dysarthria [6]. Dysphonia implies a decrease in the ability to produce vocal sounds, while dysarthria refers to the difficulty of producing words. These speech impairments can occur up to five years before other symptoms, suggesting that early PD diagnosis might be actually possible [7]. Impaired speech is not always easy to recognize for humans at an early stage of the disease, however, a considerable amount of literature has shown the potential of automated speech-based methods to diagnose neurological disorders, including PD, from the speech signal even at the most early stages [7-9]. Such methods can be deployed for several speech tasks, of which speech classification and speech processing are the main ones. Speech classification generally aims to distinguish between people with Parkinson (PWP) and healthy controls (HC) [10]. Using speech processing techniques, acoustic features (also known as acoustic biomarkers) can be extracted from speech signals to capture these changes. Classical features include fundamental frequency (F0), jitter, shimmer, Mel Frequency Cepstral Coefficients (MFCCs) and formant frequencies [1]. However, recent studies in the literature employ deep acoustic features (DAF) from end-to-end (E2E) systems, auto-encoders, or pre-trained models [11-13]. These features can be utilized not only in the final layers of a deep learning (DL) model but also in more traditional machine learning (ML) approaches, offering more interpretable results but potentially resulting in lower performance. Until 2016, traditional methods such as Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) were the standard ML approach for speech processing tasks [14]. GMMs and HMMs typically use an acoustic, language and pronunciation model. Training each of these models separately and aligning their training data can be labor intensive. Recently, DL has emerged as the leading computational method in AI and its subfield, ML, delivering outstanding outcomes on a variety of complex cognitive tasks, and sometimes even outperforming humans [14,15]. ML approaches require manual feature extraction, whereas DL ones automatically extract deep features [16]. Moreover, these features tend to be more abstract and robust [16], potentially leading to a more accurate diagnosis of PD. Studies that have been published in most recent years within the realm of speech-based DL approaches ford PD classification mainly focus on E2E learning [17-19] and transfer learning (TL) [20-22]. Furthermore, there has been a growing interest in privacy-related issues in the speech- based Al research [23,24]. Scientists increasingly question the reliability of speech-based Al systems, becoming more aware of concerns and risks over the trustworthiness of these systems since speech can be used as a biometric identifier to identify individuals and obtain information about their health status [23]. Datasets containing the speech of patients with PD are sensitive in nature, safety and fairness (e.g. without bias) need to be ensured [23]; therefore, the number of such datasets that are publicly available is limited (see a list of such datasets in Table A1). This review presents an overview of the most recent, innovative speech-based DL approaches for PD classification as of March 2024. Previously, other authors have reviewed different approaches to PD classification from various perspectives. Ngo et al. [1] summa- rized the literature from 2010 to 2021 related to voice and speech ML and DL approaches for PD classification and the assessment of its severity. Saravanan et al. [10] presented a review of papers until 2021 of ML and DL methods that are not only related to speech but also to physiological signals, such as electroencephalogram (EEG), and neuroimaging tech- niques, such as functional magnetic resonance imaging (fMRI) and single-photon emission computed tomography (SPECT). In this review, we focus on the most recent (from 2020 to March 2024) innovative DL-based approaches related to speech, which, to the best of our knowledge, have not yet been covered in previous literature reviews, guided by the overarching research question: What is the current landscape of speech-based DL approaches for PD classification?, which has been further subdivided into the following ones: * RQ1. Which recent speech-based DL approaches are considered for PD classification? * RQ2. To what extent can speech-based DL approaches classify PD? * RQ3. Which are the issues related to bias, explainability and privacy of speech-based DL approaches for PD classification? The contributions of this systematic literature review are as follows: 1. This review classifies and discusses the most recent speech-based DL approaches for PD classification until March 2024. It explicitly focuses on speech-based data material and DL approaches. 2. This review includes an overview of publicly available speech-based PD datasets until March 2024."}, {"title": "3. E2E Approach", "content": "In this section we describe several studies which propose an speech-based E2E ap- proach for PD classification. In this approach the raw speech signal can be directly mapped to the final output [26]. This contrasts with the classical speech recognition pipeline that uses GMMs and HMMs. In this traditional approach, handcrafted features such as MFCCs must be extracted to build an acoustic model. These features are then mapped to textual symbols using a language model before final classification can be performed [14]. We categorize the speech-based E2E approaches for DL classification into three subapproaches: CNNs and LSTMs, hybrid CNN-LSTM architectures and Transformers."}, {"title": "3.1. CNNs and LSTMs", "content": "CNNs are primarily designed for processing images and generally consist of con- volutional, pooling and fully-connected layers [27]. Spectrograms, which represent the frequency spectrum of audio signals over time as an image, can be given as input to a CNN to identify patterns that are indicative of PD [28]. An LSTM is a type of Recurrent Neural Network (RNN) designed to capture long-range dependencies. Traditional RNNs usually use only one hidden state maintaining an internal memory, while LSTMs typically consist of an input gate, an output gate and a forget gate [29]. This allows the LSTM to learn which information is relevant over longer sequences, which are also contained in the audio signals obtained from PD patients [30]. Khaskhoussy & Ayed [31] implement a SVM and LSTM classifier and applied them on five different datasets. The LSTM classifier consistently obtains a better F-score on all datasets (99.0%, 99.0%, 81.0%, 83.0% and 94.0%, respectively) than the SVM clasifier (98.0%, 97.0%, 68.0%, 66.0% and 69.0%, respectively). The authors also report a significant difference in performance between men and women PD classification in favor of the masculine gender. Quan et al. [17] introduce an E2E method consisting of two modules. The first module contains series of time-distributed 2D-CNN blocks to transform the input to time series dynamic features. In the second module, a 1D-CNN block is built to learn the dependencies between these features. The authors preferred this 1D-CNN over a RNN architecture (e.g. LSTM), despite that RNNs have been specifically developed for sequential input. For each single element in a given sequence, RNNs consider not only that specific element, but also all preceding elements in the sequence. However, time dynamic features are often locally correlated, resulting in local motifs that are more easily detected by 1D-CNNs than RNNs. Quan et al. [17] test their method against two databases: a Chinese database collected at the GYENNO SCIENCE Parkinson's Disease Research Center, and PC-GITA, which is a Spanish database originally presented in [32]. On both databases, the proposed method outperforms traditional ML methods such as Support Vector Machines (SVM), K-Nearest Neighbours (KNN) and Decision Tree (DT), and reaches accuracy scores in a range of 66.67% to 73.33%. The CNN is able to reach an accuracy of 81.6% and 75.3% for sustained vowels and short sentences in Chinese, respectively. On the Spanish database, consisting of vowels, words and sentences, an accuracy of even 92% is achieved. To increase the interpretability of their E2E approach, Quan et al. [17] apply a feature visualization method, Grad-class activation maps (Grad-CAM), and show that their CNN tends focus on the lower frequency regions rather than than high-frequency regions of the log Mel-spectrogram. They apply the CNN again using low-frequency (ranging from 0 to 32), high-frequency (ranging from 0 to 64), and full-frequency Mel-spectrograms, as input. The best Area Under the Curve (AUC), for both Chinese and Spanish, is obtained when the lower frequency regions are used as input indicating that lower frequencies are more influential than higher frequencies. The AUC indicates the area under the Receiver Operating Characteristic (ROC) curve of a classification model [33]. The ROC shows the relationship between the True Positive Rate and False Positive Rate of the model at several thresholds. Similar to Quan et al. [17], Akila et al. [19] indicate that not all features are necessary to achieve high performance. It introduces a MASS-PCNN, which combines a multi-agent salp swarm (MASS) algorithm with a novel PD classification neural network (PCNN). The MASS algorithm, an updated version of SSA, selects relevant features based on various patterns. These selected features are then input into the PCNN, which includes several convolutional and pooling layers, an inception module, and a squeeze-and-excitation (SE) module. The pooling layers summarize the most important features, the SE module enhances feature importance, and the inception module captures multi-scale data. The authors argue that these additions make their CNN architecture more discriminative, achieving an accuracy of 95.1%. In many other studies CNNs have been employed for PD classification [3,22,26,34-39]. In [37], Zhang et al. reach an accuracy of 91%. Narendra et al. [26] implement both a traditional and E2E pipeline approach leading to an accuracy of 67.93% for the former and 68.56% for the latter. Although, the E2E approach is able to outperform the more traditional approach, the performance is considerably lower as compared to other studies. In [34], an Acoustic Deep Neural Network (ADNN), Acoustic Deep Recurrent Neural Network (ADRNN), and Acoustic Deep Convolutional Neural Network (ADCNN) are distinguished. They demonstrate that the CNN architecture, ADCNN, slightly outperforms the more regular DNN and RNN (99.92% versus 98.96% and 99.88%, respectively). In [35], the authors also compare a CNN and LSTM classifier using MFCCs and Gammatone Cepstral Coefficients (GTCCs) on several subsets of the PC-GITA [32] and the Parkinson's Disease Classification [40] datasets. The experiments demonstrate that the CNN significantly outperforms the LSTM classifier, and the GTCCs provide greater accuracy compared to the MFCC, reaching an accuracy of 100%, which implies none of the PWP and HC are misclassified. In [22], the authors use both CNN and Transformer (see Section 3.3 for more details) architectures. As CNNs, they use MobileNetV2, DenseNet201, DenseNet169, DenseNet121, ResNet152, ResNet50, GoogleNet, VGG19 and VGG16. The lowest accuracy is reported for the MobileNetV2 and VGG19, which led to an accuracy of 95.61%, and the highest accuracy is reported for ResNet152 for which 98.08% accuracy is reported. In this study, the authors use the Synthetic vowels of speakers with PD and Parkinsonism dataset [41], on which more information can be found in Table A1. Finally, other studies on the performance of CNN-based auto-encoders have demon- strated varying levels of accuracy across different databases, privacy-aware methods and noisy environments. Sarlas et al. [38] report an accuracy of 61.49% on the MVDR-KCL database following a Federated Learning (FL) method; whereas Janbakhshi et al. [39] observe a higher accuracy of 75.4% on the PC-GITA dataset [32] using speaker identity invariant representations and adverserial training. In Farag\u00f3 et al. [36], it is pointed out that CNNs can still reach an accuracy ranging from 92% to 96% in noisy environments on a custom private small dataset of 27 participants. In Hire\u0161 et al [3], the authors demonstrate that adding noise can even improve the accurateness with which PD can be classified on the PC-GITA dataset."}, {"title": "3.2. Hybrid CNN-LSTM architectures", "content": "While CNNs typically outperform LSTMs in PD classification tasks, some studies indicate that hybrid CNN-LSTM architectures can also perform effectively. Er et al. [28] use Variational Mode Decomposition to denoise the speech signals and obtain the Mel- spectrograms. Next, pre-trained ResNets models (ResNet-18, ResNet-50 and ResNet-101) are used to extract deep features from the Mel-spectrograms, which are then given as input to the LSTM model. The best result is obtained when combining the ResNet-101 and LSTM, leading to an accuracy of 98.61%. Mallela et al. [42] use four different types of speech tasks stimuli: subjects describing images shown to them on a computer screen in their native language (IMAG), sustained phonemes (PHON), monosyllabic targets (DIDK), and spontaneous speech (SPON). The authors apply stimuli-specific and pooled models to each of these stimuli. The stimuli- specific models are trained and tested on one specific speech task stimulus (e.g. train on IMAG and test on IMAG), whereas the pooled models are trained on all types of speech tasks stimuli and tested on each stimulus individually. For both the stimuli-specific and pooled model the highest accuracy (96.20% and 98.27%) is achieved when using SPON, followed by IMAG, DIDK, and PHON. For PHON, the accuracy scores are considerably lower than for the other speech stimuli, ranging from 59.45% to 73.57% and 70.25% to 82.18% for the stimuli-specific and pooled model, respectively. It can be observed that the pooled model outperforms the stimuli-specific model in all cases. This suggests that the hybrid CNN-LSTM benefits from receiving various types of speech tasks as input during training. In another study performed by these authors [43], they again employ a hybrid CNN-LSTM architecture and are able to reach an accuracy of 90.98%. Initially, this model is trained to predict three classes, Amyotrophic Lateral Sclerosis (ALS), PD and HC, and then is fine-tuned to perform binary classification between PD and HC only."}, {"title": "3.3. Transformers", "content": "Transformers were originally developed for Natural Language Processing (NLP) appli- cations [44]. Unlike RNNs and CNNs, Transformers do not use recurrent and convolutional mechanisms. Instead, Transformers use the so-called attention mechanism which allows for parallelization. In several studies [45-47], the Transformer architecture has been adopted for PD diagnosis. Nijhawan et al. [48] implement a Vocal Tab Transformer consisting of a feature embedder, Transformer encoder and Multilayer Perceptron (MLP) head. They name their proposed method Vocal Tab Transformer, because they obtain dysphonia measures, i.e. tabular vocal features, from the subjects' voice recordings. A XgBoost model is trained to estimate the importance of these features, of which only the most important ones are given as input to the feature embedder block. However, one of the main advantages of DNN architectures such as Transformers is that they can just take in the voice recordings as raw input [16], instead of performing a feature extraction and feature selection process, first. Although [48] are able to reach an AUC of 0.917, other studies (e.g. [45] and [46]) show even better performances, without needing to implement any additional feature extraction and selection steps. In [45], Chronowski et al. use Wav2vec2.0 [49] as a backbone model and reach an accuracy of 97.92% for the classification of PD and healthy control subjects. The authors also attempt to predict the severity of the disease based on the Hoehn & Yahr (H&Y) scale. This attempt is less successful. Unfortunately, the authors do not mention the actual performance for this task, making it difficult to determine the extent of their lack of success. Malekroodi et al. [46] reach an accuracy of 98.5% using a Swin Transformer for the classification of PWP and HC. The margin with the CNN architecture they implemented, a VGG16, is not that large, however. With the VGG16, an accuracy of 98.1% was achieved, which is only 0.4 percentage point lower than the accuracy that was reached with the Swin Transformer. Nevertheless, it shows the potential of Transformer architectures for PD classification. Furthermore, the authors discriminate between HC, mild PD, and severe PD, based on the Unified Parkinson's Disease Rating Scale (UPDRS). They show that the Swin Transformer has strong capabilities to discriminate between HC and PD, showing a precision of 95%. However, discriminating between only two classes, mild and severe PD, appears to be more difficult, resulting in a precision of 85%. Although the recognition of different stages of PD is still not optimal, there already seems to be some improvement as compared to the unsuccessful attempt to discriminate between five different classes in [45]. In Section 3, it was mentioned that [22] use several CNN and Transformer models to classify PD. For each of these models, the authors use also the TL approach, which will be discussed in more detail in Section 4. The Transformer models are the ViT-L-32 and ViT-B-16, which are both Vision Transformers (ViT), and lead to an accuracy of 96.74% and 91.57%, respectively. For most of the CNN models, such as ResNet50 (see Section 3 for more details), a higher performance was reported than for these Transformers, although the margin is small. The authors conclude from this that the latest and more advanced models are not necessarily better in terms of performance than the more conventional ones."}, {"title": "4. TL Approach", "content": "TL allows to transfer knowledge from a source domain to a target domain [50]. In most cases, a DNN is pre-trained on a large-scale database for a certain task, which we refer to as the source domain, and then re-trained for another task usually referred to as the target domain [50]. Karaman et al. [20] use three different architectures (SqueezeNet1_1, ResNet50 and DenseNet161) that have already been trained on ImageNet [51], a database consisting of more than 14 million images. ResNet and DenseNet are dense architectures while SqueezeNet is a lightweight architecture with fewer parameters [20]. At first, the authors froze the convolutional layers of the three architectures, and only the fully connected layers are retrained to explore the learning rate from 1.0x10-6 to 1.0. Then, the convolutional layers are unfrozen and the learning rate dynamically updated. With a dynamically updated learning rate, the minimum loss can be determined more accurately. Of the three architectures, DenseNet161 obtained the highest accuracy (89.75%). Furthermore, the DenseNet-161 obtained a test time of 0.029\u00b10.028 and a training time of 0.003\u00b10.005, which is satisfactory and acceptable for clinical practice. Hires et al. [5,52] use the German Saarbruecken Voice Database [53] and the Colombian Spanish PC-GITA dataset as their main databases [32], in their cross-lingual studies. More information about both datasets can be found in Table A1. A cross-lingual architecture is pre-trained on one certain language, also called the base language, and is then applied to another language, which is often referred to as the target language. In [5] an ensemble of CNN networks is introduced using a Multiple Fine-Tuning (MFT) approach. The ensemble consists of three base CNN networks, of which each network is pre-trained on ImageNet but fine-tuned on a different dataset. The first CNN network is fine-tuned on PC-GITA whereas the second CNN network is fine-tuned on the Vowels dataset [54], and then further- fine-tuned on PC-GITA. The third CNN network is fine-tuned on the Saarbruecken Voice Database and PC-GITA datasets. The Vowels dataset is not included in Table A1 since it does not contain any information about pathological diseases [5]. The CNN architectures used in this study are the ResNet50 and Xception networks. In general, fine-tuning these two types of CNNs on multiple databases (MFT) seems to boost the performance. For Xception, using MFT is associated with a higher accuracy in three of the five vowel utterance tasks. For ResNet50, this is the case for even four of these tasks. Furthermore, the ensemble of multiple fine-tuned CNNs generally performs better than the multiple fine-tuned CNNs individually. This shows the potential of both the MFT and ensemble approach proposed by these authors. In [52], Hires et al. train and test a CNN network on a single dataset, unseen data and a mix of datasets. These datasets include the private Czech Parkinsonian dataset (CzechPD) [55], the PC-GITA dataset [32], the Italian Parkinson's voice and speech dataset [56] and the RMIT-PD dataset [57]. The authors of the experiment describe that the CNN performed well on each a single dataset, showing an accuracy of 90.82%, 90.52%, 97.81%, and 94.83% for the CzechPD, PC-GITA, Italian dataset and RMIT-PD, respectively. However, when the CNN is tested on unseen data or a mix of the different datasets the performance decreases significantly, leading to accuracy scores between 43.07% and 78.85%. As compared to Hires et al. [52], Vasquez-Correa et al. [21] are able to obtain slightly better results with their transfer learning approach. They obtained accuracy scores between 61.5% and 77.3% with either Czech, German or Spanish as a base language and one of the remaining two as a target language. When Spanish is used as a base language, this generally led to higher performance (77.3% and 72.6%) than when Czech or German (70.0%, 72.0%, 76.7%, 70.7%) is used as a base language. This is line with earlier work of these authors [58], in which they demonstrate that the accuracy increased from 69.3% to 77.3% for German and from 68.5% to 72.6% for Czech while using Spanish as the base language. Orozco-Arroyave et al. [59] also observe that Spanish is generally a better base language than Czech and German for PD classification. When Spanish is the base language, accuracy scores around 90% are reached. When Czech is the base language, accuracy scores within a considerable wider and lower (range of 60 to 80% of accuracy). Specifically, these researchers experiment with moving fractions of the target language to the training data, and excluding these fractions, then, from the test data. Only 30% has to be moved when German is the target language and Spanish the base language, to obtain an accuracy of 90% approximately. However, when Czech is the base language, the accuracy reaches from 60%, which is considerably low. Arasteh et al. [60] employed a FL approach on specifically PC-GITA (Spanish), a German dataset with 176 subjects (88 PD and 88 HC) and a Czech dataset with 100 subjects (50 PD and 50 HC). They utilized a model consisting of four fully connected layers (1024, 256, 64, and 2) with Wav2vec2.0 embeddings. The model achieved a high robustness across different languages and population with accuracies of 83.2%, 78.9%, and 77.8% on the Spanish, German, and Czech datasets, respectively. Finally, there are other works in the literature which deal with classification of several diseases using the same approach. In [21], the authors combine CNNs with TL among different diseases. A base model is trained on either Parkinson's or Huntington's disease (HD), and then the model is tested on the other disease. The CNNs are more accurate in distinguishing between HD and non-HD, than PD and non-PD. Not surprisingly, the models pre-trained on HD tend to be more accurate than models that are pre-trained on PD. This shows again that some base models transfer better than others. In other words, similar to what is observed for TL among languages [58], HD seems to transfer better than PD [21]. Another study that uses TL among diseases is [37], which has already been mentioned in Section 3.2. Although several studies take a cross-pathological approach, more studies in the existing literature are cross-lingual. These cross-lingual studies are not limited to pre-trained DNN architectures and TL applications. Also for studies focused on deep feature extraction, a cross-lingual approach seems to become more popular. In Section 5 it is discussed in greater detail."}, {"title": "5. DAF Approach", "content": "DAF refers to deep features that are automatically extracted from an audio signal using a DL model. These features are either learned in the final layers of the DL model or used as input for a ML model [11]. There are a few recent studies that employ DAF for PD classification using different DL and ML approaches. For instance, Karan et al. [61] propose a stacked auto-encoder DNN framework to classify PD and HC using speech material from the PC-GITA database. They achieve an accuracy of 87% using time-frequency deep features from spectrograms and a softmax deep classifier in the final layer, outperforming a classical ML-based SVM classifier (which achieves an accuracy of 83%). Ferrante et al. [12] aim to understand how well a classification model trained on DAF in one language works in a different target language. To investigate this, they use three different architectures (Wav2Vec2.0, VGGish, and SoundNet) to generate the DAF. They alternate between English and Telugu as source and target languages, with Telugu being an under-resourced language. This could affect the accuracy of PD classification in this language. However, the results in [12] show that an accuracy higher than 90% is achieved for both English and Telugu, despite Telugu being an under-resourced language. This accuracy is observed for both the traditional features (MFCCs, F0, jitter, shimmer) and the DAF. Another study that uses DAF in multi-lingual and cross-lingual settings is carried out by Favaro et al. [13]. They compare the performance of interpretable feature-based models (IFM) and non-interpretable feature-based models (NIFM). They also consider pitch, loudness and variation as interpretable features, similar to Ferrante et al. [12]. Non- interpretable features are the DNN embeddings extracted with Wav2Vec2.0, TRILLsson, and HuBERT. According to the authors, their work is the first to apply TRILLsson and HuBERT representations to PD diagnosis and Wav2Vec2.0 in multi-lingual and cross-lingual settings [13]. They demonstrate that in mono-lingual, multi-lingual and cross-lingual settings, NIFMs outperform IFMs. In the mono-lingual setting, IFMs obtained an accuracy that is 4% lower than for NIFMs. In the multi-lingual setting, classifiers are trained on all languages individually, and NIFMs outperform IFMs with a 7% margin. The results are better than in a mono-lingual setting suggesting that using more languages improves PD diagnosis. In the cross-lingual setting, the classifiers are trained on all languages but the target language and there the NIFMs again outperform the IFMs, this time by 5.8% in terms of accuracy. One of the datasets Favaro et al. [13] use is the NeuroVoz database [62], detailed in Table A1. Jeancolas et al. [9] also use DAF, which they call X-vectors, instead of traditional MFCCs. They demonstrate that PD detection performs better in men due to greater variability in female MFCC distributions, which complicates MFCC-based classification in women. Despite improvements with X-vectors of 7% to 15% and discriminant analyses, gender differences persist due to factors such as less pronounced brain atrophy and different speech neural circuits in women. Ma et al. [63] introduce deep dual-side learning, comprising both deep feature learning and deep sample learning. For deep feature learning, the authors use an embedded stack group sparse auto-encoder (EGSAE) is to acquire the deep features. Then, these deep features are fused with the original features using L1 regularization. This results in a hybrid feature set. For deep sample learning, hierarchical sample spaces are created using an iterative clustering algorithm (IMC). Classification models are applied to each of these sample spaces. A weighted fusion mechanism fuses the different models into an ensemble model, thereby combining deep feature and deep sample learning. With this method, accuracy scores of 98.4% and 99.6% are obtained on the LSVT Voice Rehabilitation Dataset [64] and the Parkinson's Disease Classification dataset [40], respectively. Whereas the Parkinson's Disease Classification dataset is publicly available (see Table A1), the raw audio files of the LSVT dataset are private. Finally, Laganas et al. [65] aim to develop a \u201cprivacy-aware\u201d method for classifying PD between PWP and HC using running speech signals from passively-captured voice call recordings. The study involves a multi-lingual cohort of 498 subjects, comprising 392 HC and 106 PWP. A key feature of this method is that the data processing is performed locally on the smartphone, ensuring privacy by not transmitting sensitive information. The AUC for this classification method is 0.84 for the English sub-cohort, 0.93 for the Greek sub-cohort, and 0.83 for the German sub-cohort, demonstrating the method's efficacy across different languages."}, {"title": "6. Discussion", "content": "This section summarizes the main lessons learned from each research question, estab- lishes connections with the existing literature, and elaborates on the implications for the research community."}, {"title": "6.1. Findings and Implications", "content": "Regarding RQ1. Which speech-based DL approaches are considered for PD classification?, this systematic review has categorized and described the most recent scientific works in the literature about speech-based DL approaches for PD classification intro three main categories. In particular, CNN architectures under the E2E learning approach are the most popular speech-based approach for PD classification. Most of the E2E-based studies de- scribed in Section 3 show that CNNs can consistently outperform other architectures, such as LSTMs and hybrid CNN-LSTMs. However, in recent years the Transformer architecture has gained popularity and is able to compete with CNNs in terms of performance (as for our RQ2. To what extent can speech-based DL approaches classify PD?). Whereas CNNs tend to focus more on local areas in the spectrogram, Transformers show more widespread patterns [46", "47": "describe that, intuitively, the features related to PD are not local and that it can therefore be expected that Transformers will outperform CNNs and increasingly dominate the field of speech-based PD classification in the following years. However, thus far, Transformers have not been able to outperform CNNs by a signifi- cant margin. Additionally, Transformers typically require extensive training data, which presents a challenge because PD speech datasets are scarce and tend to be relatively small. This limitation could hinder the growing popularity of Transformer-based approaches. Chronowski et al. [45", "46": ".", "58": [59], "21": "."}]}]}