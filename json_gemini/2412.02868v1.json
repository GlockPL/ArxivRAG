{"title": "A NOVEL COMPACT LLM FRAMEWORK FOR LOCAL, HIGH-PRIVACY EHR DATA APPLICATIONS", "authors": ["Yixiang Qu", "Yifan Dai", "Shilin Yu", "Pradham Tanikella", "Travis Schrank", "Trevor Hackman", "Didong Li", "Di Wu"], "abstract": "Large Language Models (LLMs) have shown impressive capabilities in natural language processing, yet their use in sensitive domains like healthcare, particularly with Electronic Health Records (EHR), faces significant challenges due to privacy concerns and limited computational resources. This paper presents a compact LLM framework designed for local deployment in settings with strict privacy requirements and limited access to high-performance GPUs. We introduce a novel preprocessing technique that uses information extraction methods, e.g., regular expressions, to filter and emphasize critical information in clinical notes, enhancing the performance of smaller LLMs on EHR data. Our framework is evaluated using zero-shot and few-shot learning paradigms on both private and publicly available (MIMIC-IV) datasets, and we also compare its performance with fine-tuned LLMs on the MIMIC-IV dataset. The results demonstrate that our preprocessing approach significantly boosts the prediction accuracy of smaller LLMs, making them suitable for high-privacy, resource-constrained applications. This study offers valuable insights into optimizing LLM performance for sensitive, data-intensive tasks while addressing computational and privacy limitations.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as GPT [1] and BERT [2], have gained substantial popularity and usefulness in recent years due to their remarkable ability to understand and generate human-like text. These models leverage vast amounts of data and complex neural network architectures to deliver insights and automation across various fields. The effectiveness of LLMs in processing and producing text has made them invaluable tools in both academic research and industry applications, pushing the boundaries of what machines can achieve in natural language understanding and generation.\nElectronic Health Records (EHR) are crucial for modern healthcare as they contain comprehensive patient data [3], including both structured data (e.g., demographics, medications, lab results, genomics) and unstructured data (e.g., clinical notes). The combination enhances personalized clinical decisions, drug discovery, and informative policy-making. Despite the advance in EHR systems, the lack of precise disease phenotyping remains a significant barrier to their full application. While the Internation Classification of Diseases (ICD) codes are recorded in most EHRs, they often fail to capture the details of a patient's condition [4]. In contrast, clinical notes can more accurately reflect disease progression, but their unstructured nature makes it challenging to extract relevant information. Manual annotation for these notes is labor-intensive, prone to errors [5], and requires highly-skilled healthcare professionals, further complicating the problem. Additionally, the sensitive nature of medical records demands strict adherence to privacy regulations [6], limiting the use of cloud-based LLMs such as ChatGPT.\nTo tackle the complexities of highly private EHR data, LLMs present a compelling solution to automate annotation processes, reducing the labor and time required to extract disease phenotype. However, the deployment of large-scale LLMs, such as LLaMA-70b, typically requires cloud-based infrastructure, raising privacy concerns due to sensitive health data. In addition, these models demand significant computational power, particularly high-performance GPUs with large amounts of VRAM, which may not be accessible to many labs.\nGiven these constraints, we focus on smaller, more accessible, locally-deployed LLMs such as Gemma-7B and LLaMA-7B. Although these models reduce the computational burden, they introduce new challenges due to their smaller context window [7, 8]: Clinical notes are often lengthy and unstructured, making it difficult for these smaller models to capture relevant information effectively.\nTo address these challenges, we propose a novel framework that enhances the performance of smaller LLMs by incorporating a preprocessing step. This step filters and focuses on disease-relevant contexts, reducing the input length and allowing the LLMs to work within their context window limitations. By focusing on the most pertinent sections of the clinical notes, we aim to improve the accuracy of disease phenotyping, while maintaining computational feasibility and data privacy. We present a comparison between our framework and others in Figure 1."}, {"title": "2 Related work", "content": "Keyword search and statistical phenotyping Early works of computational phenotyping search keywords from EHRs. Specifically, they first extract disease, medication, or lab-related keywords by simple searching algorithms, and then derive the disease phenotype based on rules designed by domain experts [10]. Rule-based systems, such as Unified Medical Language System [11], were utilized for various phenotypes including diabetes mellitus [12] and adverse drug events [13]. However, the designed rules can fall short in inferring complicated clinical notes. To address this issue, previous studies utilized statistical learning methods to predict disease phenotype after feature engineering the searched keywords and structural data [14, 15, 16]. Although these methods improve the manually designed rules, they usually ignore the context of the searched keywords in the clinical notes and may misinterpret those keywords.\nFine-tuning pre-trained language models To effectively learn the disease phenotype in the clinical notes, a popular approach is to fine-tune pre-trained language models, such as BERT or clinical BERT [17], using annotated clinical documents. However, fine-tuned models were mainly developed for clinical text mining and named entity extraction [18, 19, 20] and less intended for disease phenotyping, potentially due to the lack of publicly available annotated data. [21] first fine-tuned BERT to extract comprehensive collections of breast cancer-related phenotypes using clinical notes from a Chinese hospital. [22] then adopted a similar approach to develop a cancer domain-specific BERT model (CancerBERT) for breast cancer-related phenotypes extraction from the University of Minnesota Clinical Data Repository (21291 patients). [23] fine-tuned BERT and BART models to automate the generation of discharge summaries for neurology patients, aiming to reduce physician burnout by enhancing the efficiency of clinical documentation. [24] proposed NYUTron, a LLM pre-trained and fine-tuned on inpatient clinical notes of 387,144 patients at New York University to provide an automatic and comprehensive profiling of patients. Notably, all above applications require highly skilled clinicians to annotate notes from a large-scale Private HNC Dataset, which can be labor-intensive and time-consuming.\nUnsupervised learning using LLMs LLMs can understand contextual language and generate human-like text, excelling across a wide range of NLP tasks [25, 26]. Several studies have directly employed LLMs for various clinical tasks [27]. For example, [28] applied GPT-3.5 to extract social determinants and family history of 1,000 deidentified notes from a University Hospital. [29] employed both GPT-3.5 and GPT-4 to create draft responses for patient messages in electronic inboxes, demonstrating statistically significant decreases in the 4-item physician task load score derivative. [30] employed GPT-4 on private datasets from UW Medical Center Montlake, UW Medical Center Northwest, and Harborview Medical Center to aid clinicians in classifying perioperative risks. [31] introduced a two-agent LLM system named EHR-CoAgent to predict disease from EHR. The system comprises two LLM agents: a predictor agent that makes predictions and articulates reasoning processes, and a critic agent that reviews incorrect predictions and provides feedback for improvements. Remarkably, most of the studies utilized cloud-based LLMs such as OpenAI API instead of locally inferred models, which could result in some privacy issues in more sensitive EHRs."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Proposed Framework", "content": "Our observations indicate that LLMs with fewer parameters often fail to capture essential information when processing extensive text inputs. This issue is compounded in settings where computational resources are constrained, such as laboratories equipped with GPUs with small Video Random Access Memory (VRAM), which limits the deployment of more robust models.\nIn response to these computational constraints and privacy concerns inherent in processing sensitive data such as EHR, we have developed a structured framework to enhance the analysis of clinical notes."}, {"title": "3.2 Zero-Shot and Few-Shot learning", "content": "To enhance the effectiveness of zero-shot learning, we craft the prompts to guide the large language model's understanding and analysis of the task at hand. This involves structuring the prompt in a way that directly asks the model to assess whether indications of metastasis are present based on the content of a single clinical note. By doing so, we ensure that the model can make accurate assessments without prior training on specific examples, relying solely on its pre-existing knowledge and the context provided within the prompt.\nIn our few-shot learning methodology, we adopt a hands-on training approach by carefully selecting a balanced set of examples from the dataset. This collection typically consists of n positive samples, indicating the presence of metastasis, n negative samples, where metastasis is absent, and n neutral samples that lack information relevant to the disease state. We term this selection as n-shot learning. These samples are thoughtfully integrated into the learning prompts. This tailored integration is designed to familiarize the large language model with the specific subtleties and requirements of the task at hand. By interacting with this curated set of examples, the model can recalibrate its approach based on the limited yet significant exposure to task-specific data. We list the prompts for both zero-shot and few-shot learning in the Appendix."}, {"title": "3.3 Zero-Shot and Few-Shot learning with preprocessing", "content": "The preprocessing of clinical notes starts with the segmentation of the text into individual sentences. This step is essential for the meticulous examination of each sentence on its own, allowing for the precise identification of specific contents within the notes. During this review phase, we rigorously scan each sentence for predefined keywords related to metastasis, including \u2018metastasis,' \u2018metastatic,' and so on, which are listed in the Appendix.\nUpon detecting any of these keywords in a sentence, we not only flag that sentence for extraction but also include the sentences immediately before and after it. This approach ensures that we capture a comprehensive context surrounding mentions of metastasis, encompassing any preliminary or subsequent information crucial for an in-depth analysis. Extracting adjacent sentences helps preserve the continuity and relevance of the data, thereby enhancing the accuracy of subsequent interpretations. In this process, we employ regular expressions to identify and extract relevant sentences efficiently. However, more sophisticated NLP techniques could also be utilized to refine this extraction process further.\nOnce the preprocessing of the text is completed, we proceed with the analysis using the LLMs. We apply the same prompt that was used in the zero-shot and few-shot learning experiment, which is carefully designed to instruct the model to determine the presence of metastasis based on the preprocessed clinical note."}, {"title": "3.4 LLM fine tuning", "content": "We utilize the pre-trained LLMs, Gemma-2b and Gemma-7b, to classify sequences of text based on its extensive training on diverse datasets. Given the absence of definitive ground truth in our dataset, we carefully select 50 samples that explicitly include terms related to metastasis; these are designated as our positive samples. Similarly, we choose 50 samples that suggest the absence of metastatic conditions, which we classify as negative samples.\nWe then split these 100 curated samples into two sets: 80% of the samples are used for training the model, while the remaining 20% are set aside for validation. This division allows us to train the model on a robust set of data while also holding back a subset to test the model's ability to generalize to new, unseen data."}, {"title": "4 Applications", "content": ""}, {"title": "4.1 Introduction of Two EHR Datasets", "content": "Private HNC dataset This dataset comprises 7,284 patients of HNC, each having at least one ICD-9 or ICD-10 code along with a corresponding diagnosis date. Among these patients, 2,637 have at least one code beginning with ICD-10 codes C78, C79, or C80, representing potential metastasis. For all of the patients with the relevant code, collectively there are 97,703 records, of which 85,870 (approximately 87.9%) include at least one clinical note containing metastasis-related keywords within a 30-day window (15 days before and after the diagnosis date).\nMIMIC-IV dataset The MIMIC-IV is an extensive, anonymized EHR database encompassing data from over 70,000 patients admitted to Beth Israel Deaconess Medical Center between 2008 and 2019 [9]. This period covers more than a decade of intensive care unit (ICU) admissions, making it a pivotal resource for medical research. With 26 tables of detailed patient data including demographics, lab measurements, procedures, medications, ICD codes, and clinical notes, MIMIC-IV offers comprehensive access to real-world clinical data. This database adheres strictly to the Health Insurance Portability and Accountability Act (HIPAA), ensuring patient privacy while supporting a wide range of medical studies. The breadth and depth of MIMIC-IV facilitate large-scale analyses critical for advancing clinical practice and shaping health policy.\nWe filter for subjects with at least one clinical note containing metastasis-related keywords, resulting in a dataset comprising notes from 13,923 unique subjects across 28,390 hospital admissions. After examining the ICD codes, 8,510 (approximately 61.1%) of these subjects have ICD-9 codes 197, 198, 199, or ICD-10 codes C78, C79, C80, representing metastasis of tumor. Additionally, 18,552 hospital admissions (approximately 65.3%) are associated with these ICD codes."}, {"title": "4.2 Evaluation metrics", "content": "\"Time-sensitive\" accuracy defined on the Private HNC dataset Creating a definitive gold standard for evaluating experimental outcomes in this study presents a complex challenge, primarily due to the incomplete reflection of diseases through ICD codes alone. To address this and ensure a robust comparative analysis, we employ a methodological approach for our Private HNC Dataset centered around the use of ICD-10 codes C78, C79, and C80, which are indicative of metastatic conditions. It is crucial to acknowledge, however, that the absence of these codes does not unequivocally rule out the presence of metastasis in a patient.\nFor each patient diagnosed with metastasis according to the aforementioned codes, we review their clinical notes over a 20-day period, encompassing 10 days before and after the diagnosis date. This time frame is selected to capture the most relevant clinical observations related to the diagnosis. During this period, we conduct a systematic search for predefined keywords associated with metastasis in the clinical notes, utilizing both zero-shot and few-shot learning, with or without the preprocessing step.\nThe classification accuracy of our model is assessed based on the relative counts of notes: if the number of notes indicating metastasis exceeds those indicating its absence, the classification is considered correct. Conversely, if non-metastatic notes predominate, the classification is considered incorrect. Cases with an equal count are classified as inconclusive due to insufficient information. We denote the total number of correct, incorrect, and inconclusive classifications as $N_c$, $N_i$, and $N_i$, respectively. Additionally, if a review period lacks clinical notes altogether, we interpret this as an absence of relevant information rather than a misclassification by the LLM, and these instances are consequently excluded from our analysis. We use $P_c = \\frac{N_c}{N_c+N_i+N_I}$ to denote the proportion of correct classifications out of all evaluated cases. Similarly, we use $P_i = \\frac{N_i}{N_c+N_i+N_I}$, and $P_I = \\frac{N_I}{N_c+N_i+N_I}$ to denote the proportions of incorrect and inconclusive classifications, respectively."}, {"title": "4.3 Results", "content": "Preprocessing improves classification accuracy for the private HNC dataset Table 1 represents the experiment results using our Private HNC Dataset. The results from our investigations substantiate the efficacy of the preprocessing steps integrated into our framework. The data clearly indicates that this preprocessing significantly enhances the classification accuracy across various modeling paradigms, encompassing both zero-shot and few-shot learning approaches. This enhancement is particularly crucial when employing compact, locally-operated LLMs for processing EHR datasets. Such models, due to their smaller size relative to their more extensive counterparts, often face computational and memory constraints that can limit their effectiveness.\nFurthermore, our analysis reveals a noteworthy trend concerning few-shot learning: as the number of examples included in the training increases, there is a discernible degradation in model performance. This phenomenon can be attributed to the input length sensitivity of smaller-scale LLMs. Specifically, as the input length increases with the addition of more examples, these models tend to lose focus on extracting and processing the key information critical for accurate classification. This issue underscores the challenges posed by the inherent limitations of smaller LLMs when tasked with processing complex and lengthy datasets such as EHRs.\nBy mitigating these challenges through our targeted preprocessing steps, we are able to considerably improve the utility of smaller LLMs in high-privacy, local computational environments, thereby enhancing their applicability and effectiveness in real-world clinical settings. This finding not only advances our understanding of model scalability and efficiency but also opens avenues for further research into optimizing LLM frameworks for specialized data-intensive tasks in constrained environments."}, {"title": "5 Discussion", "content": "In this study, we introduce a novel compact LLM framework tailored for local deployment on EHR data, addressing significant challenges such as privacy concerns and computational limitations in healthcare settings. Our approach, which integrates a preprocessing step such as regular expressions, significantly enhances the performance of compact LLMs. This preprocessing not only reduces the data's complexity before it reaches the LLM but also ensures that the models focus on the most relevant information, thereby improving accuracy and efficiency.\nUsing metastasis as an example, we demonstrate the importance of preprocessing using the Private HNC dataset and MIMIC-IV dataset. The results of our experiments underscore the potential of compact LLMs to perform robustly even under constraints, aligning with previous research that emphasizes the scalability and adaptability of LLMs in various domains. However, unlike previous studies that primarily focus on maximizing model performance without regard for computational efficiency, our work specifically targets the balance between performance and resource utilization, which is critical in many practical healthcare applications.\nOur study offers practical insights for the local deployment of LLMs, particularly when utilizing models of a smaller scale. We recommend prioritizing the preprocessing of datasets to enhance model performance rather than focusing primarily on fine-tuning. This approach facilitates more effective use of limited computational resources and maintains data privacy, proving especially advantageous in settings where extensive model training is impractical.\nWhile our findings are promising, the study is not without limitations. The framework's reliance on regex-based preprocessing may not capture all nuances of clinical language, potentially overlooking subtle but clinically relevant information not explicitly highlighted by the regex patterns. Additionally, the performance of our framework is still dependent on the quality and scope of the input data, which may vary across different healthcare settings.\nFuture research could explore the integration of more sophisticated NLP techniques, such as context-aware embeddings or deep learning-based entity recognition, to enhance the accuracy and adaptability of the preprocessing step. Further studies could also evaluate the framework's performance across a broader range of diseases and clinical conditions to better understand its generalizability."}]}