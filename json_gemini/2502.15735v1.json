{"title": "DistrEE: Distributed Early Exit of Deep Neural\nNetwork Inference on Edge Devices", "authors": ["Xian Peng", "Xin Wu", "Lianming Xu", "Li Wang", "Aiguo Fei"], "abstract": "Distributed DNN inference is becoming increasingly\nimportant as the demand for intelligent services at the network\nedge grows. By leveraging the power of distributed computing,\nedge devices can perform complicated and resource-hungry infer-\nence tasks previously only possible on powerful servers, enabling\nnew applications in areas such as autonomous vehicles, industrial\nautomation, and smart homes. However, it is challenging to\nachieve accurate and efficient distributed edge inference due\nto the fluctuating nature of the actual resources of the devices\nand the processing difficulty of the input data. In this work, we\npropose DistrEE, a distributed DNN inference framework that\ncan exit model inference early to meet specific quality of service\nrequirements. In particular, the framework firstly integrates\nmodel early exit and distributed inference for multi-node collabo-\nrative inferencing scenarios. Furthermore, it designs an early exit\npolicy to control when the model inference terminates. Extensive\nsimulation results demonstrate that DistrEE can efficiently realize\nefficient collaborative inference, achieving an effective trade-off\nbetween inference latency and accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Neural Networks (DNNs [1]) that are required for high\naccuracy often have deeper or wider architectures, leading to\nincreased memory occupation and computational costs. This\nimpedes the deployment of DNNs on resource-constrained\nmobile or Internet of Things (IoT) devices. Currently, there are\nmany methods used to reduce the computational overhead of\nDNNs to provide efficient edge intelligence services, including\nmodel compression and inference offloading [2]. However,\nthese methods can excessively compress the network, resulting\nin a decrease in inference accuracy, and they heavily rely on\nunreliable wide area network connections [3]. The distributed\nDNN inference paradigm can allocate the workload of DNNS\nacross multiple devices to help alleviate the computational\nburden on a single edge device and fully utilize the available\nresources on nearby devices. Moreover, keeping data local and\nperforming inference nearby not only protects privacy but also\navoids unpredictable delays caused by remote transmission.\nHowever, during the inference process, the actual available\nresources of the device and the processing difficulty of the\ninput data fluctuate, and not all input data require the same\namount of computation to produce inference results that meet\nthe quality of service requirements. Performing the same\nmodel computation on all input data is highly detrimental\nto improving inference performance [4], especially in edge\ncomputing scenarios with limited device resources. Applying\nthe same model to both simple and complex data leads to\nresource wastage and increased latency.\nBased on the fact that not all inputs require the same\namount of computation to produce a reliable prediction, pre-\nvious works explored a wide range of techniques to achieve\ndynamic inference. Dynamic inference techniques aim to adapt\nto different input data, service requirements, and resource\nfluctuations by dynamically adjusting the inference plan. Teer-\napittayanon et al. in [5] propose BranchyNet, which realizes\nefficient inference by introducing early exit branches at middle\nlayers of the DNN and selectively exiting the network early de-\npending on the input instance. AdaEE [6] is proposed to avoid\nthe limitation of manually set exit rules in BranchyNet, which\ndynamically adjusts inference confidence thresholds based on\ncontext. AdaEE estimates the accuracy and computational\noverhead of each branch based on the historical inference\nresults and the target accuracy requirements and adjusts the\nearly exit decision accordingly, which can strike a balance\nbetween computational latency and accuracy. Another class of\nmethods that dynamically omit local model computations in-\ncludes skipping certain convolutional layers or network blocks\n[7] to achieve adaptive inference computations. These methods\noften utilize trainable gating components within the network\narchitecture, leading to a complex training process and limiting\nflexibility in deployment across different hardware platforms.\nDynamic selection of model variants is a simpler approach\nto dynamic inference, EdgeAdaptor [8] trains a series of\nmodels with different latency and accuracy specifications and\ndeploys all of them on the target device. At inference time, the\nmost appropriate model for each input is selected to perform\ninference based on a specific rule.\nAll of the above methods are dynamic inference techniques\nproposed for single-node model inference, and some works"}, {"title": "II. BACKGROUND KNOWLEDGE", "content": "To achieve efficient edge inference, distributed DNN infer-\nence became an important and promising solution. Zeng et\nal. [10] proposed a co-inference approach CoEdge, where the\ninput of each layer is used as a unit for workload distribution.\nIn this case, an intermediate feature map that is located in\nthe boundary of partitioning should be transferred, resulting\nin considerable communication burdens. To get rid of such\nprohibitive communication overheads, Bhardwaj et al. [11]\nproposed a completely different distribution approach, called\nNetwork of Neural Network (NoNN), based on Knowledge\nDistillation (KD). In NoNN, the original model was partitioned\ninto multiple independent and smaller models. Specifically,\nNoNN focused on the final convolutional (fconv) layer of a\ngiven trained model, referred to as the teacher network, the\nfilters in fconv were grouped into multiple exclusive subsets\nto derive student models. Each filter in fconv contributed dif-\nferently to the inference result, and dividing the filters should\ntake into account the correlation between individual filters. To\ndetermine how the filters should be grouped, NoNN proposed\na network science-based solution. NoNN first constructed an\nundirected graph where each vertex represents a filter in fconv,\nand the correlation between a pair of filters was quantified and\nassigned as the weight of corresponding edge in the graph.\nFormally, the weight of the edge between i and j is determined\nas follows:\n$$w(i, j) = \\sum_{val} a_i a_j |a_i - a_j|,$$\nwhere $a_i$ is the average activation of filter i. In the NoNN ap-\nproach, filters that respond simultaneously were encouraged to\nbe evenly distributed across multiple student networks because\nthey want to distribute knowledge as evenly as possible to all\nstudents. After this, a student network was trained to learn\ncertain knowledge from each subset of the partitioned filters.\nThe training loss of NoNN is defined as follows:\n$$Loss(\\theta_s) = Loss_{kd}(\\theta_s) + \\beta \\sum_{P \\in fconv} Loss_{act}(\\theta_s, P).$$\nIn the inference stage, each student network was executed\nindependently on a single device and the results were collected\nonly once at the end to infer the final prediction. In this\nway, the heavy communication overheads caused by other\ndistributed inference approaches [10] can be mitigated."}, {"title": "B. Early Exit", "content": "Based on the observation that features learned in the early\nstages of a deep network tend to infer the majority of the\ndata correctly [12], early exits were proposed to allow some\nsamples to execute just a portion of the DNN. Kung et al.\n[5] proposed the concept of BranchyNet for exiting early\nin inference to save time and resources. The BranchyNets\nare DNNs with side branches inserted in the intermediate\nlayers so that simpler input samples can be early inferred\nand, hence, reduce the inference time. In BranchyNet, each\nside branch consists of several convolutional layers and a\nfully connected (FC) layer, where the FC layer produces a\nvector z. Afterwards, a sotmax operation is performed on z to\ngenerate a probability vector $\\hat{y}$, where each element represents\nthe probability of the sample in each class.\nIn the inference phase, the confidence level is estimated\nbased on the value of $\\hat{y}$ generated at each exit to decide\nwhether the inference exits early or not. BranchyNet used\nentropy as the measure to quantify the prediction confidence\nat each exit point, the entropy is defined as:\n$$entropy(\\hat{y}) = - \\sum_{y_c \\in y} y_c log y_c.$$\nHigh entropy indicates similar probabilities across classes\nand therefore a non-confident prediction, while low entropy"}, {"title": "III. DISTREE DESIGN", "content": "In this section, we present the detailed design of the DistrEE\nscheme, which aims to combine distributed inference with\nearly exit to achieve adaptive inference in dynamic scenarios.\nOur scheme is divided into two stages. The first stage focuses\non offline joint training of neural networks with multiple\nbranches, while the second stage investigates early exit strate-\ngies for online dynamic inference."}, {"title": "A. Architecture", "content": "To develop an inference plan that meets the task require-\nments for diverse input data and fluctuating device resources to\nachieve efficient dynamic collaborative inference, we propose\nDistrEE, a dynamic collaborative inference framework based\non multi-branch neural networks. According to the execution\nphase, the dynamic inference method based on multi-branch\nneural networks can be divided into two steps: joint training\nof multi-branch models and online decision-making for dy-\nnamic inference. Consider a scenario where an edge cluster\nconsisting of multiple edge devices collaborates to perform\nDNN inference, the system scenario is shown in Fig. 1. Given\na set of edge devices $D = {d_1, d_2, ..., d_v}$ and an original\nteacher model T, the student models obtained by distillation\nof the teacher model are denoted as a unified student model\n$S = {S_1, S_2, ..., S_N}$. In the student model $S_i$, there are\n$M_i$ different exit branches in the backbone DNN network,\nwhich divides the DNN into $M_i$ sequentially connected stages.\nThe network structure of the exit branches after each stage is\ndetermined based on the structure of the backbone DNN output\nlayer to generate intermediate feature maps of the same shape.\nFor a given input data, when the intermediate features obtained\nin a branch are sufficient for a reliable prediction, it can exit\nfrom that branch early to save the device's computational\nresources and accelerate the inference. Input that is not exited\nearly ends the inference at the last backbone exit of the model."}, {"title": "B. Joint Training", "content": "DistrEE assumes that all student models have the same\nbackbone DNN, which is reasonable since networks with\nmultiple branches are inherently adapted to scenarios where\ndevices are heterogeneous. Specifically, for a weaker device,\nit is sufficient to split only a portion of the DNN in front\nof a particular exit for deployment. Therefore, we establish\n$M_i = M, \\forall i \\in {1, ..., N}$, as depicted in Fig.2.\nThe objective of joint training multi-branch student models\nis to enable each branch of each student model to effectively\nlearn from the teacher model, allowing as many inputs as\npossible to exit inference at an early exit. For the branch j\nof the unified student model, when it undergoes training as an\nindependent entity, the same training loss function utilized in\nNoNN is applicable, as delineated in Eq. 4. In our framework,\nthe improved end-to-end training method for the multi-branch\nneural network model is used to train the unified student\nmodel. The end-to-end training loss function is given by Eq.\n5, which represents the weighted sum of the training losses\ncorresponding to each branch.\n$$Loss(\\theta_{s,j}) = \\frac{(1 - \\alpha)H(y, P_{s,j}) + \\alpha H(P_t, P_{s,j})}{KD\\ loss} + \\beta \\sum_{P \\in fconv} \\frac{||F_t^P(P) - F_{s,j}^P(P)||^2}{||F_t^P(P)||^2 ||F_{s,j}^P(P)||^2},$$\n$$Loss(\\theta_s) = \\sum_{j=1}^M w_j Loss(\\theta_{s,j}),$$\nwhere $\\theta_s$ denotes the parameters of the student network, H is\nthe standard cross-entropy loss. The first term represents the\nstandard knowledge distillation loss that combines the hard\nand soft-label cross-entropy losses. The second term corre-\nsponds to the AT loss, which measures the error between the\noutput feature maps of the fconv partition of the teacher and\nthe output feature maps of the branch j of the corresponding\nstudent. $w_j$ is used to control the loss weight of the exit\nbranch j of the unified student model. Based on the above\nloss function, the classical stochastic gradient descent method"}, {"title": "C. Dynamic Inference", "content": "Once training is complete, the student networks can be\ndeployed to perform fast inference by predicting samples\nin the early stages of the network according to a specific\nexit strategy. If the feature maps observed at an exit branch\ncontains sufficient information about the input sample, the\ninference exits and returns an intermediate feature early, and\ncomputation in subsequent branches of the network can be\nomitted. Therefore, how to measure the confidence level of\nintermediate features is crucial to achieve efficient early exit.\nBranchyNet [5] uses a threshold on the entropy of the softmax\npredictions as a measure of confidence. For each exit point,\nthe input sample is fed through the corresponding branch. If\nthe entropy is less than the given threshold, the class label with\nthe maximum score is returned. Other approaches directly use\nthe top-1 softmax value as a quantification of confidence and\ninference exits when the top-1 value is large enough.\nHowever, the outputs of individual student models in Dis-\ntrEE are only concerned with local feature information, and\nthe outputs of all student models need to be aggregated and\nthen computed by the final inference model related to the task\nto obtain the complete inference results. Therefore, traditional\nmethods based on confidence measures such as top-1 prob-\nability or entropy value are not applicable in this scenario,\ni.e., the output of a single student cannot be directly used to\nmeasure the softmax confidence. Recent works on the encoder-\ndecoder framework face a similar problem because the hidden\nstate representation of the encoder is not directly related to\nthe final prediction. Tang et al. [13] observe the hidden-\nstate representation of each layer in Transformer will reach\na saturation status in the language model, which indicates\nthat the hidden-state representation change decreases as going\nthrough the latter layers. Therefore, the latter layers can be\nskipped safely without a significant performance drop when\nsuch saturation status is reached. Based on this observation,\nthey leverage the Cosine Similarity between layers as a proxy\nfor saturation level to control early exit. Inspired by that work,\nwe investigate whether the saturation state exists in a multi-\nbranch convolutional neural network. The similarity of the\noutput features between two neighboring exits is shown in\nFig.3 (a), these results indicate that similar saturation states\ndo not exist in our models.\nOn the contrary, we propose another criterion based on\nthe degree of feature differentiation, which takes the output\nfeatures of the shallowest exit as the benchmark and considers\nthe first exit as incapable. In this way, the degree of difference\nin the output features relative to the output features of the first\nexit reflects the improvement in the learning ability of the\ncorresponding exit, i.e., the greater the difference, the better\nthe learning ability. Fig. 3 (b) shows the experimental results\nof exit feature differences, the degree of feature differences\nrelative to the first exit increases with the depth of the exit.\nBased on this result, we use relative feature difference to mea-\nsure the confidence level of each exit and propose a dynamic\ninference control algorithm based on feature difference, as\nshown in Algorithm 1. Where $F_j$ represents the output feature"}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In this section, we evaluate the performance of the pro-\nposed distributed inference scheme DistrEE via extensive\nsimulations. Specifically, we verify the effectiveness of the\ntraining loss function used to jointly train multi-branch neural\nnetworks. Then, we compare the performance of the proposed\ndynamic inference algorithm with several baselines in terms\nof inference overhead and accuracy.\nWe use WideResNet-16x4, a well-trained classification net-\nwork on the CIFAR-10 dataset, as the teacher model, which\nachieves a classification accuracy of 91.86%. Unified student\nmodel DistrEE-2 contains 2 student models, and the student\nmodel uses WideResNet-16\u00d71 as the backbone network. The\nWideResNet model is designed with a shortcut connection in\nthe basic block structure, so appending early exit branches\nshould be done in base block units. In DistrEE-2, we attach\nexit branches after the 1st, 4th, 6th, 9th, 11th, 14th, and 16th\nconvolutional layers in the WideResNet-16\u00d71 architecture. No-\ntably, the exit following the 16th convolutional layer serves as\nthe original backbone network's output exit. The architecture\nof each exit branch is designed as a cascade, including Relu\nand Avg-pool layers, consistent with the exit structure of the\nbackbone network. The computational demands of both the\nbackbone network and the exit structures for each branch in\nthe DistrEE-2 setup are comprehensively detailed in Table I.\nThe total computational overhead of a single student model\nin DistrEE-2 is 34.55 MFLOPs and the number of parameters\ntotals 189044, while the total computational overhead of the\nbackbone model WideResNet-16\u00d71 is 34.26 MFLOPs and the\nnumber of parameters totals 178540. Compared to the original\nmodel WideResNet-16\u00d71, the additional computational over-\nhead and the number of model parameters introduced by the\nadditional 6 early exit branches are only 0.88% and 5.88% of\nthe original model, and thus do not result in a burdensome\nadditional overhead.\nUsing the training loss function expressed in Eq. 5, we set\nthe training loss weight vectors of the seven exit branches\nof DistrEE-2 to w = [1, 1, 1.1, 1.4, 1.4, 1.3, 1.2], aiming\nat allowing the intermediate exits to learn efficiently and\nthus increase the chances of early exit. The model is trained\nbased on the CIFAR-10 dataset with 200 epochs of training\nrounds, and the model training results are shown in Fig.4. The\ntest accuracy of the seven branches of the student model on\nthe test set is [47.16%, 66.34%, 72.96%, 78.68%, 86.96%,\n89.46%, 90.57%]. The results show that the proposed joint\ntraining framework for multi-branch models can train the\nstudent model effectively, and all branches of the model can\nreach the convergence state quickly.\nTo validate the performance of the proposed dynamic in-\nference algorithm, images are randomly selected from the\nCIFAR-10 dataset to form a test dataset, which contains 10\nimages from each of the 10 categories, totaling 100 images.\nThe trained model DistrEE-2 is used to execute inference\nbased on different early exit policies, and the experiment is\nrepeated 10 times on the Lenovo XiaoXin Pro 14 to record\nthe average inference latency and accuracy for comparing the\nperformance of different approaches. For DistrEE, we set the"}, {"title": "V. CONCLUSION", "content": "In this work, we have presented an efficient distributed\ninference scheme called DistrEE, which adapts to the diversity\nof input data to improve inference performance by dynami-\ncally adjusting the inference plans of the devices. To achieve\ndynamic collaborative inference of deep learning models, we\nhave integrated model early exit and distributed inference\nin DistrEE, and designed offline joint training framework\nand online inference exit method for multi-branch models.\nSimulation results have shown that DistrEE can efficiently\nrealize dynamic collaborative inference, maintain an inference\naccuracy comparable to non-early-exit methods, and signifi-\ncantly reduce inference computational overhead to achieve an\neffective trade-off between inference latency and accuracy."}]}