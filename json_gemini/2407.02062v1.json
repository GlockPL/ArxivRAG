{"title": "Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?", "authors": ["Wataru Hashimoto", "Hidetaka Kamigaito", "Taro Watanabe"], "abstract": "This work investigates the impact of data augmentation on confidence calibration and uncertainty estimation in Named Entity Recognition (NER) tasks. For the future advance of NER in safety-critical fields like healthcare and finance, it is essential to achieve accurate predictions with calibrated confidence when applying Deep Neural Networks (DNNs), including Pretrained Language Models (PLMs), as a real-world application. However, DNNs are prone to miscalibration, which limits their applicability. Moreover, existing methods for calibration and uncertainty estimation are computational expensive. Our investigation in NER found that data augmentation improves calibration and uncertainty in cross-genre and cross-lingual setting, especially in-domain setting. Furthermore, we showed that the calibration for NER tends to be more effective when the perplexity of the sentences generated by data augmentation is lower, and that increasing the size of the augmentation further improves calibration and uncertainty.", "sections": [{"title": "Introduction", "content": "Named Entity Recognition (NER) is a one of the fundamental tasks in Natural Language Processing (NLP) to find mentions of named entities and classify them into predefined categories. The predicted information by NER is essential for downstream tasks like event detection (Vavliakis et al., 2013), information retrieval (Cowan et al., 2015), and masking of personal user information (Kodandaram et al., 2021). Due to the demand, NER is the underlying technology for information extraction from text and documents.\nBased on the recent advances in Deep Neural Networks (DNNs), NER's performance is also improved like other NLP fields. In recent years, Pre-trained Language Models (PLMs) based architectures, such as BERT (Devlin et al., 2019) and DeBERTa (He et al., 2021), have been strong baselines in many NLP tasks, including NER.\nIn general, however, DNNs are prone to miscalibration (Guo et al., 2017), including PLMs (Desai and Durrett, 2020); calibration means the predicted confidence of the model aligns with the accuracy. The problem causes DNNs to make incorrect predictions with high confidence, which limits the applicability of DNNs on the number of domains where the cost of errors is high, e.g., healthcare and finance. Therefore, DNNs need to provide high prediction performance with appropriately calibrated confidence at the same time.\nConfidence calibration and uncertainty estimation methods are ways to solve the miscalibration of DNNs, and have been applied in NLP tasks such as text classification (Xiao and Wang, 2019), structured prediction (Jiang et al., 2022; Reich et al., 2020), question answering (Si et al., 2022), and machine translation (Malinin and Gales, 2021). However, many methods for confidence calibration and uncertainty estimation, typically Monte-Carlo Dropout (MC Dropout) (Gal and Ghahramani, 2016), are computationally expensive due to multiple stochastic inferences, making them difficult for real-world application.\nData augmentation has also been applied for NER (Dai and Adel, 2020; Zhou et al., 2022), though, it was focusing on the generalization ability on low-resource data. In computer vision (CV) areas, data augmentation makes the model more robust to the input and leads to confidence calibrations (Wen et al., 2021; Liu et al., 2023), in which the same labels are trained on different representations of the input than the original data. Based on the findings of these previous studies, there is a possibility that data augmentation in NER can improve confidence calibration without increasing inference"}, {"title": "Related Work", "content": "Named Entity Recognition In the last decade, NER using DNNs has been widely successful; Lample et al. (2016) reported a sequence-labeling model combining bi-directional LSTM with CRF (BiLSTM-CRF). Akbik et al. (2018) proposed contextualized character-level word embeddings combined with BiLSTM-CRF. In recent years, NER models based on PLMs, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and DeBERTa (He et al., 2021), have achieved state-of-the-art performance.\nUncertainty Estimation In general, DNNs are prone to miscalibration and overconfidence (Guo et al., 2017) especially without pretraining (Desai and Durrett, 2020; Ulmer et al., 2022). One way to estimate uncertainty is to run multiple stochastic predictions. Deep Ensemble (Lakshminarayanan et al., 2017) trains multiple DNN models and integrates their multiple stochastic predictions to make a final prediction. MC Dropout (Gal and Ghahramani, 2016) applies Dropout (Srivastava et al., 2014) regularization at both training and inference time, and by taking multiple samples of the network outputs during inference. These are known to perform calibration well in many cases (Ovadia et al., 2019; Immer et al., 2021), but their practical use is hampered by the fact that they make multiple probabilistic predictions. A relatively lightweight calibration method is the post-hoc approach. For example, temperature scaling (Guo et al., 2017) performs calibration via dividing logits by a constant, which is a simple and lightweight baseline.\nData Augmentation Data augmentation methods are widely used in machine learning, CV, and NLP areas. More recent attention has focused on the provision of data augmentation methods to improve calibration and uncertainty. Test-time augmentation (TTA) (Ashukha et al., 2020) generates multiple samples during inference and integrates the predictions to estimate the prediction uncertainty. MixUp (Zhang et al., 2018) uses linear interpolation between two samples to augment a new sample with soft labels, which has been investigated for situations where it is effective for calibration (Zhang et al., 2022).\nIn NLP tasks, the impact of data augmentation on calibration in text classification has been investigated in recent study (Kim et al., 2023), but only for In-domain (ID) and not for NER. Furthermore, it has been found that predictive performance is driven by data augmentation in NER (Dai and Adel, 2020; Chen et al., 2020; Zhou et al., 2022; Chen et al., 2022; Hu et al., 2023), but these studies have focused only on the predictive performance of NER and have not evaluated for calibration and uncertainty. This is the first study to comprehensively investigate the impact of data augmentation on calibration and uncertainty in NER, both in ID and OOD (Out-of-domain) settings."}, {"title": "Methods", "content": "In this section, we describe the popular baseline methods for confidence calibration and data augmentation methods for NER. Details about existing calibration methods are described in Appendix B."}, {"title": "Existing Calibration Methods", "content": "Baseline uses the maximum probability from the softmax layer.\nTemperature Scaling (TS) TS (Guo et al., 2017) is a post-processing technique for calibrating the confidence scores outputted by a neural network. It involves scaling the logits (i.e., the outputs of the final layer before the softmax) by a temperature parameter T before applying the softmax function to obtain the calibrated probabilities.\nLabel Smoothing (LS) LS (Miller et al., 1996; Pereyra et al., 2017) is prevalent regularization technique in machine learning, introduces a controlled level of uncertainty into the training process by modifying the cross-entropy loss.\nMonte-Carlo Dropout (MC Dropout) MC Dropout is a regularization technique that can be used for uncertainty estimation in neural networks, which requires multiple stochastic inferences (Gal and Ghahramani, 2016). We perform 20 stochastic inferences and output their average."}, {"title": "Data Augmentation Methods for NER", "content": "We investigate data augmentation methods in NER (Dai and Adel, 2020; Zhou et al., 2022) for confidence calibration and uncertainty estimation.\nLabel-wise Token Replacement (LwTR) LwTR uses binomial distribution to determine whether a token is replaced. The chosen token is randomly replaced with another token with the same label based on label-wise token distribution on training data. Thus, LwTR keeps the original label sequence.\nMention Replacement (MR) Unlike LwTR, MR replaces an entity with another entity with the same label instead of a token. Other parts are the same as LwTR. Since entities can have multiple tokens, MR does not keep the original label sequence.\nSynonym Replacement (SR) SR is similar to LwTR except that SR replaces a token with its synonym in WordNet (Miller, 1995). Since the synonym can have multiple tokens, SR does not keep the original label sequence.\nMasked Entity Language Modeling (MELM) MELM (Zhou et al., 2022) performs data augmentation using a language model that predicts contextually appropriate entities for sentences in which entity parts are masked by entity markers."}, {"title": "Evaluation Metrics", "content": "We use Expected Calibration Error (ECE), Maximum Calibration Error (MCE), and Area Under Precision-Recall Curve (AUPRC) to evaluate confidence calibration and uncertainty estimation."}, {"title": "Expected Calibration Error (ECE)", "content": "ECE (Naeini et al., 2015) measures the difference between the accuracy and confidence of a model. Specifically, it calculates the difference between the average confidence and the actual accuracy of the model on different confidence levels. Formally, ECE is defined as:\n\\text{ECE} = \\frac{1}{n} \\sum_{b=1}^{B} |\\text{acc}(D_b) - \\text{conf}(D_b)|\nwhere B is the number of confidence interval bins, \\(D_b\\) is the set of examples whose predicted confidence scores fall in the b-th interval, n is the total number of examples, acc(\\(D_b\\)) is the accuracy of the model on the examples in \\(D_b\\), and conf(\\(D_b\\)) is the average confidence of the model on the examples in \\(D_b\\)."}, {"title": "Maximum Calibration Error (MCE)", "content": "MCE (Naeini et al., 2015) is the maximum difference between the accuracy and the confidence of the model on different confidence levels. Formally, MCE is defined as:\n\\text{MCE} = \\max_{b=1}^{B} |\\text{acc}(D_b) - \\text{conf}(D_b)|,\nMCE takes the maximum calibration error in each bin, not the expectation; a smaller MCE means that the model's predictions are less likely to be far off in a given confidence region."}, {"title": "Area Under the Precision-Recall Curve (AUPRC)", "content": "AUPRC is the summary statistic the relationship between precision and recall at different thresholds. The higher the value, the higher the overall precision at a given threshold."}, {"title": "Experimental Settings", "content": "We conducted experiments on two different NER datasets to evaluate the performance of confidence calibration methods in different settings. For the cross-genre evaluation, we used the OntoNotes 5.0 dataset (Pradhan et al., 2013), which consists of six different genres, broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), telephone conversation (tc), and web data (wb). This dataset is commonly used for NER evaluation in a cross-domain setting (Chen et al., 2021).\nFor the cross-lingual evaluation, we used the MultiCoNER dataset, which is a large multilingual NER dataset from Wikipedia sentences, questions, and search queries (Malmasi et al., 2022). We selected English as the source language and English, German, Spanish, Hindi, and Bangla as the target languages. The details of the dataset statistics are provided in Table 1."}, {"title": "Training Details", "content": "In all experiments, we train out models on a single NVIDIA A100 GPU with 40GB of memory. We used MIT-licensed mDeBERTaV3 (microsoft/mdeberta-v3-base) (He et al., 2023) whose model size is 278M, as a multilingual transformer encoder from Hugging Face transformers (Wolf et al., 2020) pre-trained model checkpoints, and extracted entities via sequence labeling. Cross-entropy loss is minimized by AdamW (Loshchilov and Hutter, 2019) with a linear scheduler (Goyal et al., 2017). The batch size is 32, and gradient clipping is applied with maximum norm of 1. The initial learning rate was set to le-5. To avoid overfitting, we also applied early stopping with patients = 5.\nFor the temperature parameter in TS, we used Optuna (Akiba et al., 2019) to optimize the temperature parameter based on dev set loss with a search range of [0.001, 0.002, ..., 5.000] in 100 trials. In addition, we optimized the binomial distribution parameter for data augmentation methods using the dev set by a grid search in the range of [0.1, 0.2, ..., 0.8]. In LS, we conducted a grid search in the range of [0.01, 0.05, 0.1, 0.2, 0.3] to optimize the smoothing parameter. In the case of MELM, mask rate \\(n\\) during fine tuning and mask parameter \\(\\mu\\) during generation are hyperparameters. We conducted a grid search for each hyperparameter in the range [0.3, 0.5, 0.7], as in Zhou et al. (2022).\nWe perform each experiment 10 times using different random seeds, collect evaluation metric values, and report their average and standard deviation. For convenience, the reported values are multiplied by 100."}, {"title": "Evaluation Details", "content": "The NER model calibration is evaluated based on the \"Event of Interests\" concept introduced in the previous study (Kuleshov and Liang, 2015; Jagannatha and Yu, 2020). Since the full label space \\(L\\) is large for structured prediction tasks such as NER, we focus instead on the event set L(x), which is the set containing the events of interest \\(E \\in L(x)\\) obtained by processing the model output.\nThere are two main strategies for constructing L(x): The first strategy is to construct L(x) only from the events obtained by the MAP label sequence prediction of the model; The second strategy is to construct L(x) from all possible label sequences; The first strategy is easy to obtain events, but the coverage of events is low depending on the model's prediction. The second strategy provides a high coverage of events, but is computationally expensive to obtain events. Jagannatha and Yu (2020) is based on the first strategy, where the entities extracted by the NER model are calibrated on the basis of forecasters (e.g., gradient boosting decision trees (Friedman, 2000)), which are binary classifiers separate from the NER model. Since the training dataset for forecasters consists of entities extracted by the NER model, more entities are needed to improve the uncertainty performance of the forecasters. Therefore, for example, the top-k Viterbi decoding of the CRF is used to increase the entity coverage and the size of the forecaster's training dataset.\nOn the other hand, Jiang et al. (2022) is based on the second strategy, where it introduces a method to find the probability that a span has a specific entity type for datasets with short sequences, such as WikiAnn (Pan et al., 2017), with restricted token sequences and span lengths. However, this method is computationally difficult for datasets with longer token sequences and more complex"}, {"title": "Results and Discussion", "content": "We present the performance of cross-genre and cross-lingual confidence calibration and uncertainty estimation as the main results. The cross-genre evaluations are quantified by learning on a training set in one genre and evaluating calibration and uncertainty on a test in another genre. Similarly, in the cross-lingual evaluations, we train the model in one language (in this research, we use English; EN) and evaluate the calibration and uncertainty on a test set in another language."}, {"title": "Cross-genre Evaluation", "content": "The results shown in Table 2 demonstrate ECE and MCE in OntoNotes 5.0 for NER in the ID setting, which the source domain and target domain are the same. The table results show that data augmentation methods consistently have better calibration performance than TS, LS, and MC Dropout, which have been considered to work for general classification problems, in the evaluation of calibration performance, in the ID setting. In particular, when the source genre is tc, MELM and other data augmentation methods show superior calibration performance, with up to 6.01 % improvement for ECE and 5.62 % improvement for MCE compared to Baseline. As shown in Table 1, the tc domain is not a data-poor setting, where there is sufficient training data and data augmentation is generally effective. MR and SR also show good calibration performance following MELM. Moreover, we can see that applying data augmentation methods do not increase inference time (See Appendix A Table 7).\nOn the other hand, as Table 3 shows, when the target domain is OOD, especially when the target (e.g. OntoNotes 5.0 wb) is far from the source domain, the degree of improvement in the uncertainty estimation performance of data augmentation is not large, and sometimes even decreases.\nWe presume that the augmented data is not far from the original training set, because data augmentation methods we targeted in this study are based on the replacement of tokens or entities. Considering a recent study that indicates models tend to be more overconfident in areas with less training data (Xiong et al., 2023), we can consider calibration performance in OOD sets, especially far from the source domain, will not improve by data augmentation for NER, while the performance in ID sets will be better than existing methods.\nTo illustrate this, we performed t-SNE (van der Maaten and Hinton, 2008) for the token embeddings with only entity token from trained Baseline model, shown in Figure 1. We can understand that the token embeddings from augmented data are near the train set or ID test set, while the OOD test sets have some poorly covered regions. Generating sentences that are distant from the training data set and semantically aligned entities from label description for uncertainty estimation is an interesting direction for future research.\nAUPRC scores are shown in Table 4. Among the existing methods, TS shows superior performance; in data augmentation methods, MELM is not as good as in the case of calibration metrics such as ECE and MCE, and MR tends to show superior uncertainty performance."}, {"title": "Cross-lingual Evaluation", "content": "The results of cross-lingual transfer in MultiCoNER are shown in Table 5 with English as the source language. MR performs better in uncertainty performance for the ID situation. In contrast to the calibration and uncertainty performance in the cross-genre setting, both MR and SR show better calibration and uncertainty in the OOD setting.\nIn Jiang et al. (2022), the result shows that the larger the linguistic distance (Chiswick and Miller, 2005), the more lenient the calibration and uncertainty estimation tends to be, and similar trends are obtained in this experiment. Unlike the discussion in Section 6.1, the uncertainty performance by data augmentation is also good for OOD in cross-lingual setting because the areas where only target set exist is limited in MultiCoNER (illustrated in Appendix F). On the other hand, MELM, which tends to show excellent calibration performance in cross-genre calibration, does not show good performance in cross-lingual settings.\nThe amount of data for each language in the CC100 (Conneau et al., 2020) dataset used to train the base model, mDeBERTaV3, was highest for English, followed by German, Spanish, Hindi, and Bangla which correlates with the trend of the calibration results. Moreover, as mentioned in Limisiewicz et al. (2023), languages that tend to have"}, {"title": "Detailed Analyzes", "content": "We investigate the effects of entity overlap rates and the perplexity of the generated sentences to gain a better understanding of the confidence calibration and uncertainty estimation performance of data augmentation methods for NER. We also investigate the impact of data augmentation size in several settings."}, {"title": "Impact of Augmentation Size", "content": "To investigate the impact of data augmentation size on calibration and uncertainty performance, we analyze the trend of evaluation metrics in tc \u2192 mz scenario of OntoNotes 5.0 and EN \u2192 ES scenario of MultiCoNER, respectively. Figure 2 and 3 illustrate the results in the ID and OOD settings, respectively. In many cases, MR improves the calibration and uncertainty performance by increasing data. SR consistently improves as the dataset size doubles, whereas LwTR demonstrates only marginal improvement or even worsens as the dataset size increases. Finally, MELM improves further for OntoNotes 5.0 tc, which shows excellent performance, and deteriorates further for MultiCoNER EN, which shows poor performance.\nThese results show that the calibration algorithm with the best performance for cross-domain transfers is likely to have better performance as the augmentation size is increased. On the other hand, increasing the augmentation size in MR improves the calibration and uncertainty performance compared to similar other data augmentation methods. Since data augmentation by MR and MELM is performed only on the entity region, the uncertainty estimation performance is relatively less adversely affected by increasing the data augmentation size. On the other hand, in SR and LwTR, data augmentation that replaces tokens may often inject tokens with inappropriate parts of speech for that sentence, so increasing the data augmentation size often leads to a degradation of uncertainty estimation performance."}, {"title": "Impact of Perplexities for Augmented Sentences", "content": "To investigate the influence of replacement units on data augmentation for NER as mentioned in Section 6.3.1, we measured the perplexity of the augmented sentences using GPT-2 (Radford et al., 2019). The average perplexities of the augmented sentences and the average perplexities of the original training set for each dataset are shown in Table 6. Lower perplexity from augmented sentences tends to improve calibration performance and uncertainty performance. Consistently, the average perplexity of the sentences generated by MR is the lowest. Since MR performs substitutions on an entity-by-entity basis and does not affect the structure of the sentence itself, it has the lowest perplexity among the data augmentation methods in NER. MELM has the second lowest perplexity after MR, and may be adversely affected by generated entities that are adapted to the context but not actually present."}, {"title": "Conclusion", "content": "In this paper, we investigated the impact of data augmentation on the confidence calibration and uncertainty estimation in NER in terms of genre and language, using several metrics. First, we find that MELM, MR, and SR lead to better calibration and uncertainty performance in the ID setting consistently. On the other hand, in the OOD setting, uncertainty estimation by data augmentation is less effective, especially when the target domain is far from the source domain. Second, our results suggest that the lower the perplexity of the augmented data, as in MR, the further better the calibration and uncertainty performance as the augmentation size is increased. Data augmentation methods for NER do not require changes to the model structure and only require more data to improve entity-level calibration and performance without the need to change the model structure. Our findings indicate the effectiveness of uncertainty estimation through data augmentation for NER, and will be expected to stimulate future research based on their limitations."}, {"title": "Limitations", "content": "While this experiment provided valuable insights into the impact of data augmentation on confidence calibration and uncertainty estimation in NER across different genres and languages, there are several limitations that should be acknowledged.\nFirst, due to resource limitations, the experiment was limited to evaluation with English as the source language. To effectively investigate the calibration and uncertainty of zero-shot cross-lingual transfer, it is important to expand the investigation to include a wider range of languages as the source language. Therefore, future research should prioritize the investigation of calibration and uncertainty performance using different languages as the source for zero-shot cross-lingual transfer.\nSecond, as mentioned in Section 5.3, regarding the calibration and uncertainty evaluation policy, we simply evaluated an entity span as a single data instance, but a rigorous evaluation method that performs evaluation while considering multiple span candidates has been proposed (Jiang et al., 2022). Establishing span-level NER calibration evaluation methods that can efficiently and comprehensively evaluate calibration and uncertainty for entity types for datasets with many entity types and long sequence lengths is a topic for future research.\nLastly, we broadly evaluated the calibration and uncertainty performance in both cross-genre and cross-lingual settings on data augmentation for NER, but only using sequence labeling-based methods. Recently, other paradigms in NER, such as span-based methods (Fu et al., 2021) and Seq2Seq (sequence-to-sequence)-based methods (Yan et al., 2021), have been proposed. In the future, the calibration or uncertainty performance of these methods could be evaluated."}, {"title": "Ethical Considerations", "content": "In this study, we used existing datasets that have cleared ethical issues. Furthermore, the data augmentation methods we used for uncertainty estimation are substitution-based methods except for MELM, and MELM generated entities from existing datasets that have no ethical issues. Therefore, it is unlikely that toxic sentences would be generated."}]}