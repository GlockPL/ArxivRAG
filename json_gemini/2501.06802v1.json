{"title": "Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity", "authors": ["Jun Wan"], "abstract": "In 2020, OpenAI proposed the first type of Scaling Laws, describing the relationships between model performance and parameters, data, and compute. In 2024, OpenAI proposed the second type of Scaling Laws, describing the relationship between model inference performance and inference computation. In this paper, we analyze LLM training and inference processes from the perspective of lossless compression using conditional Kolmogorov complexity, and unify these two types of Scaling Laws. We find that both types of Scaling Laws improve approximation of conditional Kolmogorov complexity by increasing execution steps t. The first type of Scaling Laws increases t by increasing model parameters y. The second type of Scaling Laws increases t by increasing the number of output tokens.", "sections": [{"title": "Introduction", "content": "In October 2022, OpenAI released ChatGPT 3.5 to the public, demonstrating the success of the first type of Scaling Laws proposed in 2020(Kaplan et al., 2020). In September 2024, OpenAI released o1-preview and proposed test-time compute Scaling Laws(OpenAI, 2024). The subsequent series of successful models released by OpenAI have demonstrated the powerful effectiveness of these two types of Scaling Laws.\n(Hutter, 2006) has long advocated that \"a model that compresses well generalizes well\". (Del\u00e9tang et al., 2023) views large language models as powerful lossless compressors. In this paper, we also analyze large language models from the perspective of lossless compression. However, we differ by adopting the approach from NNCP(Bellard, 2019), viewing the training process of large language models as a form of lossless compression, where the model is trained solely on the data stream to be compressed.\nIn the realm of theoretical research, the analysis of lossless compression based on Kolmogorov complexity is a prevalent technical approach(Cover, 1985). Employing conditional Kolmogorov complexity as a theoretical tool, we systematically investigates the training process and inference mechanism of large language models.\nThis Work and Contribution:\n1. We provide a detailed explanation of the relationship between lossless compression and LLM training, reviewing how to train models on and compress the data stream to be compressed. During the compression process, we do not need to transmit the model parameters.\n2. We model the above compression process using conditional Kolmogorov complexity and prove that training large language models is a computable (total recursive) approximation of the upper bound of joint Kolmogorov complexity. Optimizing joint Kolmogorov complexity naturally has a regularization effect.\n3. Although Kolmogorov complexity is not computable, we prove that theoretically there exist total recursive functions or transformer neural networks such that $\\lim_{t\\to\\infty} M_t^+(x, y) = C(x | y)$\n4. Through conditional Kolmogorov complexity, we analyze the theoretical limits of large language models in the inference process, showing that there exist infinite problems (strings) that cannot be \"solved\" by large language models.\n5. Theoretically, we unify two types of Scaling Laws. Both types of Scaling Laws improve approximation of conditional Kolmogorov complexity by increasing execution steps t."}, {"title": "Related Work", "content": "Lossless Compression. The brilliant programmer Fabrice Bellard open-sourced NNCP(Bellard, 2019) in 2019, a tool for lossless data compression using neural networks, and upgraded it in 2021 to use autoregressive models (transformers)(Bellard, 2021) for lossless compression. NNCP trains models on and compresses the data stream to be compressed. Its concept can be traced back to (Knoll and de Freitas, 2012). In this paper, the authors explained how to achieve lossless compression using machine learning model predictions and arithmetic coding. In fact, lossless compression is equivalent to prediction. (Del\u00e9tang et al., 2023) further advocates viewing large language models as powerful universal lossless compressors, analyzing the first type of scaling laws and the role of tokenization from a lossless compression perspective.\nKolmogorov Complexity. Kolmogorov complexity is closely related to many fundamental principles in machine learning. For example, J. Rissanen's MDL (Minimum Description Length)(Wax and Kailath, 1985) can be viewed as a computable approximation of Kolmogorov complexity, while the maximum likelihood principle (Rissanen, 1983) and maximum entropy principle(Rissanen, 1986) can be seen as special cases of the MDL principle. Additionally, this book(Li et al., 2008) further explains the relationships between learning theories, such as PAC learning theory and Kolmogorov complexity. (DeMoss et al., 2024) studied the connection between the Grokking phenomenon in neural networks and MDL through Kolmogorov complexity."}, {"title": "BackGround", "content": "3.1 Turing Machines, Neural Networks & LLMS\nThe Turing Machine, proposed by British mathematician Alan Turing in 1936, is an abstract computational model designed to explore the computability of problems. As a cornerstone of computer science theory, the Turing Machine has the capability to simulate the execution of any algorithm, providing an important framework for understanding the nature of computation.\nFrom a theoretical perspective, Turing machines are enumerable, meaning all possible Turing machines can be systematically listed:\n$T_1, T_2,...,$\nwhere each $T_i$ represents a unique Turing machine instance. Among these Turing machines, there exists a special class called Universal Turing Machines, denoted as $U$, which can simulate the behavior of any other Turing machine. It's worth noting that modern computers can be theoretically viewed as physical implementations of Universal Turing Machines, although there are significant differences: modern computers have finite memory, while Universal Turing Machines theoretically have infinite tape.\nNeural networks operating on modern electronic computers with finite precision are necessarily equivalent in computational power to some Turing machine $T_i$, more precisely, equivalent to a Turing machine that halts in finite steps, i.e., a total recursive function. The process of training neural networks through training data is essentially a process of finding and determining a specific Turing machine $T_i$.\nLet's briefly review large language models. Large language models are based on neural network architectures and are trained through \"next token prediction\" tasks. Given an input sequence (training data) $x_{1:t} = (x_1, x_2,\\cdots, x_t)$, the model's training objective is to predict the next token $x_{t+1}$ in the sequence(Bengio et al., 2000). This process can be formalized as the following optimization problem:\n$\\theta^* = arg \\min_{\\theta} - \\sum_{t=1}^{T-1} log P(x_{t+1} | x_{1:t}; \\theta)$ (1)\nwhere $P(x_{t+1} | x_{1:t}; \\theta)$ represents the conditional probability of predicting $x_{t+1}$ given the context sequence $x_{1:t}$ under the neural network model with parameters $\\theta$, and $T$ represents the sequence length.\nFrom a computational theory perspective, large language models can still be viewed as a special type of Turing machine, specifically one that seeks to minimize 1. The core function of this Turing machine is to calculate the conditional probability distribution of the next token in a predefined vocabulary $S$ based on the given context sequence through complex probability calculations.\n3.2 Dynamic Arithmetic Coding\nDynamic arithmetic coding(Rissanen, 1976)(Pasco, 1976) is an adaptive data compression algorithm that achieves efficient coding by updating symbol"}, {"title": "LLMs & Lossless Compression", "content": "probability distributions in real-time. Here is its mathematical definition:\nLet the symbol set be $S = \\{s_1, s_2,\\dots, s_m\\}$, where each symbol $s_i$ has probability $P_t(s_i)$ at time t, satisfying $\\sum_{i=1}^{m} P_t(s_i) = 1$.\nArithmetic Encoding Process:\n1. Initialize: interval $[L_0, H_0) = [0,1)$\n2. Update interval: For input symbol $s_i$ at time t, update interval to:\n$L_t = L_{t-1} + \\sum_{j=1}^{i-1} P_t(s_j)\\cdot (H_{t-1} - L_{t-1})$\n$H_t = L_{t-1} + \\sum_{j=1}^{i} P_t(s_j)\\cdot (H_{t-1} - L_{t-1})$\n3. Probability update: Update $P_{t+1}(s_i), i = 1,2,..., m$\n4. Repeat: Continue processing next symbol until all symbols are processed\n5. Output: Any number in the final interval as the encoding result. In binary, to achieve shortest encoding, select the decimal with shortest binary representation in the interval.\nArithmetic Decoding Process:\nThe decoder reconstructs the original symbol sequence through reverse operations, using the same probability distributions step by step.\nIt's important to emphasize that predicted probability distributions $Q_t(s_i)$ can be used for arithmetic coding of symbols. The closeness between $Q$ and the true probability distribution $P$ directly affects coding efficiency - the closer they are, the shorter the average coding length.\nLet's look at a specific example.Consider a symbol set $S = \\{a, b, c, d, e\\}$, and we need to encode the message \"bab\".\nLet $P_1 = (\\frac{2}{11}, \\frac{3}{11}, \\frac{2}{11}, \\frac{1}{11}, \\frac{3}{11})$, which divides the [0, 1] interval proportionally.\nIn this section, we'll explore how to achieve lossless data compression using large language models. It's particularly noteworthy that this method is designed for streaming data, with the core idea being to train the model in real-time using the data stream to be compressed while simultaneously compressing the training data.\nSuppose we have an original message denoted as x, where $x = [s_0, x_1, x_2, x_3,\\dots, x_n]$, with $s_0$ being a special character indicating the start of the original message. Each $x_i$ represents a token in the original message, coming from a finite symbol set S.\nGiven a large language model $f$, let $\\theta_t$ represent the model parameters at time $t$. Based on this, the model's probability distribution prediction for the (t+1)-th token can be formally expressed as:\n$P_{t+1} = f(x_{t+1} | s_0, x_{1:t}; \\theta_t)$ (2)\nwhere $p_{t+1}$ represents the model's probability distribution prediction for the $(t + 1)$-th token, which is a discrete distribution."}, {"title": "LLM Pre-training and Conditional Kolmogorov Complexity", "content": "The large language model $f$'s code (such as the Pytorch training code) and various random seeds (such as parameter initializations, etc.) are packaged into a single piece of information. We denote this information as $F$. Now we begin lossless compression of the data $x$. Assume there is an information receiver and an information sender.\nThe information sender initializes neural network parameters $\\theta_0$ according to $F$.\n1. At time 0, execute $f(x_1 | s_0; \\theta_0)$. Output the prediction probability $p_1$ for the first token.\n2. At time 0, we use arithmetic coding on character $x_1$ with $p_1$. Note that while $x_1$'s true probability distribution $p_1$ is unknown, this does not affect our ability to perform arithmetic coding on $x_1$ using the predicted probability distribution $p_1$. Note that at this point, arithmetic coding has just begun its first interval division, then selected an interval based on $x_1$. We denote this interval as $[l_1, r_1]$.\n3. At time 0, perform backpropagation based on the true $x_1$ and predicted $p_1$ to update $f$'s parameters, obtaining $\\theta_1$.\n4. At time 1, execute $f (x_2 | s_0, x_1; \\theta_1)$. Output the prediction probability $p_2$ for the second token.\n5. At time 1, we use arithmetic coding on character $x_2$ with $p_2$. Note that at this point, arithmetic coding performs a second interval division based on $[l_1, r_1]$, then selects an interval based on $x_2$. We denote this interval as $[l_2, r_2]$. It's not difficult to see that the entire arithmetic coding process continues, with intervals being continuously subdivided, but the probability $p_t$ used for each division changes - this is dynamic arithmetic coding.\n6. At time 1, perform backpropagation based on the true $x_2$ and predicted $p_2$ to update $f$'s parameters, obtaining $\\theta_2$.\nWe continuously repeat steps 4 through 6 until time $n - 1$. Finally, we will obtain an interval $[l_n, r_n]$ and parameters $\\theta_n$. We select the shortest binary decimal $z_n$ within $[l_n, r_n]$ as the arithmetic coding for the entire information $x$.\nThis completes the overall lossless compression process. The compressed information consists of three parts:\n*   Arithmetic coding $z_n$\n*   Code information $F$\n*   Required number of decoding iterations $d$\nThrough the above process, we'll discover that the compressed information does not include the model parameters. In fact, we don't need to transmit the model parameters to perform lossless decompression.\nThe information sender transmits the three compressed components to the information receiver via bitstream. The information receiver now begins decoding.\nThe information receiver executes the code according to $F$ and initializes neural network parameters $\\theta_0$. Note that since $F$ contains all random seed information, this $\\theta_0$ is identical to the $\\theta_0$ from the earlier steps.\n1. At time 0, execute $f(x_1 | s_0; \\theta_0)$. Output the prediction probability $p_1$ for the first token.\n2. At time 0, perform arithmetic decoding based on $p_1$ and the received $z_n$. The decoding process makes the first interval division based on $p_1$. The information receiver will find that $z_n$ lies within the interval $[l_1, r_1]$. Thus, the first token $x_1$ is decoded.\n3. At time 0, perform backpropagation based on the decoded $x_1$ and predicted $p_1$ to update $f$'s parameters, obtaining $\\theta_1$. Note that since all random seeds are identical between the information sender and receiver, this $\\theta_1$ is identical to the previous $\\theta_1$.\n4. At time 1, execute $f (x_2|s_0, x_1; \\theta_1)$. Output the prediction probability $p_2$ for the second token.\n5. At time 1, perform arithmetic decoding based on $p_2$ and the received $z_n$, decoding the second token $x_2$. The decoding process makes a second interval division of $[l_1, r_1]$ based on $p_2$. The information receiver will find that $z_n$ lies within the interval $[l_2, r_2]$. Thus, the second token $x_2$ is decoded.\n6. At time 1, perform backpropagation based on the decoded $x_2$ and predicted $p_2$ to update $f$'s parameters, obtaining $\\theta_2$.\nWe continuously repeat steps 4 through 6 until time $n$ (where $n$ - 1 is obtained from reading information $d$). Finally, we will have losslessly decoded $z_n$ back into the original information $x$. It's not difficult to see that the entire decoding process did not require prior transmission of the model parameters $\\theta_n."}, {"title": "Kolmogorov complexity", "content": "If we use probability distribution $q$ to perform arithmetic coding on information $x$, we can calculate that its average coding length satisfies the following relationship:\n$L \\le - \\sum_{s \\in S} p(x = s) log_2 q(x = s) + 2$\n$= H(p,q) + 2$ (3)\nwhere $L$ denotes the average coding length, $p$ is the true distribution of $x$, and $H (p, q)$ is the entropy between distributions $p$ and $q$. We will use this conclusion to calculate the lossless compression efficiency of large language models.\nThe challenge in calculating compression rates lies in estimating the coding length of $|z_n|$.\nFor dataset $x$, let $p_t$ represent its true distribution in autoregressive modeling:\n$p_t = p(x_t | S_0, x_{1:t-1})$\nThe average coding length of arithmetic coding $z_n$ satisfies the following relationship:\n$L_{zn} < \\frac{1}{n}(2n + \\sum_{t=1}^{n}H (p_t, p_t))$\n$= 2 + \\frac{1}{n}\\sum_{t=1}^{n}H (p_t, p_t)$\n$= 2 - \\frac{1}{n}\\sum_{t=1}^{n} log_2 p_t$ (4)\nNote that here $p_t$ is no longer a vector but a value.\nThe $F$ and $|d$ of the compressed information are basically fixed and negligible compared to $|z_n|$. To maximize compression, we need to minimize the average coding length of $z_n$. Since directly reducing $z_n$ is difficult, and given that we have an upper bound for $z_n$, we can instead try to reduce $z_n$'s upper bound, namely:\n$\\min - \\frac{1}{n}\\sum_{t=1}^{n} log_2 p_t$ (5)\nObviously, the optimization objective described in equation 5 exactly matches the training objective of large language models. This indicates that the training process of large language models can essentially be understood as a continuous compression process of the training data.\n3.4 Kolmogorov complexity\nKolmogorov complexity(Li et al., 2008) is a concept for measuring the information content of an object (such as strings, numbers, etc.). Specifically, it represents the length of the shortest program needed to generate that object on a given universal Turing machine U.\nGiven a universal Turing machine U, the Kolmogorov complexity $C_U(x)$ of string $x$ is defined as:\n$C_U(x) = \\min\\{l(p) : U(p) = x\\}$ (6)\nwhere:\n*   p is a program, and l(p) is the length of the program.\n*   $U(p) = x$ means program p outputs x when run on universal Turing machine U.\nKolmogorov complexity measures the length of the shortest description needed to generate x. If x has regularity, it can be described by a relatively short program, resulting in low Kolmogorov complexity; if x is random, it requires a longer program to describe, resulting in high Kolmogorov complexity. Kolmogorov complexity has two very important properties:\n1. Invariance Theorem: While Kolmogorov complexity depends on the choice of universal Turing machine U, according to the invariance theorem, the Kolmogorov complexity under different universal Turing machines only differs by a constant. Therefore, we often omit the subscript U in the definition of Kolmogorov complexity.\n2. Uncomputability: Kolmogorov complexity is uncomputable, meaning there is no algorithm that can precisely calculate the Kolmogorov complexity of any arbitrary string. Conditional Kolmogorov complexity is an extension of Kolmogorov complexity that measures the shortest description length needed to generate an object given additional information (conditions). Specifically, it represents the length of the shortest program needed to generate object x on universal Turing machine U, where the program can utilize additional information y.\nGiven a universal Turing machine U, the conditional Kolmogorov complexity $C_U(x | y)$ of string x given condition y is defined as:\n$C_U(x | y) = \\min\\{l(p) : U(p, y) = x\\}$ (7)\nwhere:"}, {"title": "Two Types of Scaling Laws", "content": "*   p is a program, and l(p) is the length of the program.\n*   U(p, y) = x means program p runs on universal Turing machine U with input y and outputs x.\nConditional Kolmogorov complexity measures the shortest description length needed to generate x given knowledge of y. If y provides useful information about x, then the program to generate x might be shorter, thus reducing the conditional Kolmogorov complexity.\nJoint Kolmogorov complexity is another extension of Kolmogorov complexity, used to measure the shortest length needed to describe two objects x and y together. It represents the length of the shortest program needed to generate both x and y on universal Turing machine U.\nGiven a universal Turing machine U, the joint Kolmogorov complexity $C_U(x, y)$ of strings x and y is defined as:\n$C_U(x,y) = \\min\\{l(p) : U(p) = (x,y)\\}$ (8)\nwhere:\n*   p is a program, and l(p) is the length of the program.\n*   $U(p) = (x, y)$ means program p outputs some encoding of x and y when run on universal Turing machine U (e.g concatenating x and y into a single string).\nJoint Kolmogorov complexity measures the shortest description length needed to generate both x and y simultaneously. If there exists some correlation or regularity between x and y, then generating their joint description might be shorter than generating their descriptions separately.\n3.5 Two Types of Scaling Laws\nWithin the framework of large language models, Scaling Laws can primarily be categorized into two types:\n*   Pre-training scaling laws\n*   Inference scaling laws\n(Kaplan et al., 2020) systematically studied the impact of model parameter scale on language model performance and proposed corresponding Scaling Laws. This type of scaling law primarily focuses on resource optimization during the pre-training phase, with its core being the significant"}, {"title": "Analysis of Large Language Model Pre-training and Inference from the Perspective of Conditional Kolmogorov Complexity", "content": "improvement of model performance through increasing key resources such as data volume, model parameter count, and computational power. The proposal of such Scaling Laws not only deepened our understanding of the relationship between model performance and resource investment but also laid an important foundation for the training and optimization of large-scale language models.\n(Snell et al., 2024) conducted an in-depth study on the feasibility of improving Large Language Model (LLM) performance by increasing computational resources during the inference phase. This research direction was empirically supported by OpenAI's ol model released in September 2024. Research shows that significantly increasing computational resources and time investment during the inference phase can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. This finding not only validates the importance of computational scaling during inference but also provides a new technical pathway for optimizing large language model performance. The actual performance of OpenAI's ol model(Jaech et al., 2024) further confirms that there is a significant positive correlation between inference phase resource investment and model performance, particularly demonstrating important practical value when handling high-complexity tasks.\n4 Analysis of Large Language Model Pre-training and Inference from the Perspective of Conditional Kolmogorov Complexity\nIn this section, we will analyze the pre-training and inference processes of Large Language Models (LLMs) using conditional Kolmogorov complexity, and arrive at the following conclusions:\n1. The pre-training process of Large Language Models (LLMs) is equivalent to approaching joint Kolmogorov complexity C(x, y) from above.\n2. Although Kolmogorov complexity is uncomputable, we prove that theoretically there exist total recursive functions or transformer neural networks such that $\\lim_{t \\to \\infty} M_t^+(x,y) = C(x | y)$.\n3. LLMs are constrained by the number of Turing machine execution steps t, which prevents them from solving certain problems."}, {"title": "The Relationship Between LLM Pre-training and Conditional Kolmogorov Complexity", "content": "4. Theoretically, we have unified the two types of Scaling laws. Both types of Scaling laws work by increasing execution steps t to better approximate conditional Kolmogorov complexity.\n4.1 The Relationship Between LLM Pre-training and Conditional Kolmogorov Complexity\nLet us denote training data as x and the model as y. Using data x to train a model y with good \"generalization\" ability can be viewed as searching for a y that minimizes their joint Kolmogorov complexity C(x, y).\n$y^* = \\underset{y}{\\arg \\min} C(x, y)$ (9)\nNext, we'll demonstrate the reasonableness of this formula. According to (Zvonkin and Levin, 1970), C(x, y) can be further decomposed into the following form:\n$C(x,y) = C(y) + C(x | y)$\n$+ O(\\log(C(x, y)))$ (10)\nC(x | y) represents the shortest description length of x given model y. C(y) represents the shortest description length of model y. In other words, finding a y that minimizes C(x, y) is equivalent to finding the simplest possible model y that minimizes C(x | y).\nIn fact, the right side of equation 10 can be viewed as the Minimum Description Length (MDL) in machine learning. Here, C(y) represents model complexity, while C(x | y) represents the encoding length of data under that model. C(y) can be understood as model regularization, while C(x | y) reflects how well the model fits the data.\nIt's important to note that we measure model complexity using Kolmogorov complexity rather than simply counting the number of model parameters. This approach is quite reasonable. For example, for a transformer network with 10,000 parameters, when all parameters are 1, the network's complexity is very low; when the parameters are completely random, the network's complexity increases significantly.\nIf we only optimize y to minimize C(x | y), it may lead to overfitting. For instance, if we directly set y = x, then model y becomes exactly equivalent to data x, which is essentially just memorization, and in this case C(y) would be very large."}, {"title": "Conclusion", "content": "A neural network with good generalization ability should start from completely random parameter initialization and gradually learn, during which process both C(y) and C(x | y) should decrease gradually.\nBelow, we will demonstrate through rigorous mathematical derivation that Large Language Model (LLM) pre-training is equivalent to directly approaching joint Kolmogorov complexity C(x, y) from above, and naturally considers model complexity during the training process.\nTheorem 4.1. Given a universal Turing machine U, the joint Kolmogorov complexity C(x, y) of strings x and y satisfies the following inequality:\n$C(x,y) < C(y) + C(x | y)$\n$+ l(C(y)) + O(1)$ (11)\nwhere l represents string length, and O(1) is a constant related to Turing machine U.\nProof. Let p be the shortest program describing y, and q be the shortest program describing x given y. We can construct the following Turing machine: first use p to describe y, then mark the end position of p in the Turing machine with the encoding length l(p), then use program q to describe x based on the previously given y. The total length of this constructed Turing machine is C(y) + C(x | y) + l(C(y)). Finally, according to the invariance theorem of Kolmogorov complexity, the above inequality holds.\nCorollary 4.2. The pre-training process of LLMs is actually a computable (total recursive) approximation of the right side of equation 11.\nWe use the content from 3.3 to perform lossless compression of x. Note that during the lossless compression process, we not only obtain the encoding of x based on y but also obtain the parameters of model y.\nThe compressed information consists of three parts:\n*   Arithmetic coding $z_n$\n*   Code information F\n*   Required number of decoding iterations d\n$z_n$ and F are computable approximations (from above) of $C(y) + C(x | y) + O(1)$. l(d) is a computable approximation (from above) of $l(C(y))$. The smaller the encoding length of $z_n$, the closer it gets to the upper bound of C(x, y). In other"}, {"title": "The Relationship Between LLM Inference and Conditional Kolmogorov Complexity", "content": "words, the pre-training process of LLMs is essentially searching for a model y of moderate complexity such that, given y, the encoding length of input data x is minimized. This objective reflects the core idea of the Minimum Description Length (MDL) principle, which seeks optimal balance between model complexity and data fitting.\n4.2 The Relationship Between LLM Inference and Conditional Kolmogorov Complexity\nWe appropriately rewrote equation 9 and applied it to infinitely many x, obtaining the following expression:\n$y^* = \\underset{y}{\\arg \\min} \\sum_{x}C(x, y)$ (12)\nIn the framework of large language models, x here represents a string concatenated from (k, c), where k is our input (prompt) and c is our expected output (response). We conjecture that C(x | y*) represents the ideal state of artificial general intelligence (AGI), as it can perform lossless compression on any x we care about, indicating it has predictive capability for any x. Note that if x cannot be losslessly compressed, it means x is \"random\" and lacks regularity, therefore such x are not our targets of interest.\nAssuming y* in equation 12 exists and is known, for simplicity of expression, we will denote y* as y in the following sections.\nNext, we focus on whether we can represent C(x | y) through neural networks. Obviously, the answer is negative because C(x | y) is uncomputable. However, fortunately, we can approximate C(x | y) through total recursive functions.\nTheorem 4.3. There exists a total recursive function $\\phi(t,x,y)$ that is monotonically decreasing with t, such that $\\lim_{t\\to\\infty} \\phi(t,x,y) = C(x | y)$.\nProof. Select a universal Turing machine U. For each x, the length of its shortest program is at most l(x) + c. We construct $\\phi(t, x, y)$ as follows. On U, execute each program p of length at most l(x) + c with input y for t steps. If we can find some p that halts and outputs x, then define $\\phi(t, x, y)$ as the length of the shortest such p; otherwise, let it equal l(x)+c. Clearly, $\\phi(t, x, y)$ is a total recursive function and monotonically decreases with t. Its limit exists because for each x, there exists a t such that executing input p and y on U for t steps will output x, at which point l(p) = C(x | y)."}, {"title": "Conclusion", "content": "Corollary 4.4. Theoretically, there exists a series of transformer neural networks $M_t^+(x, y)$, such that $\\lim_{t \\to \\infty} M_t^+(x, y) = C(x | y)$, where t represents the number of Turing machine execution steps.\nIn (Roberts, 2024), the authors proved that transformers are Turing complete under infinite precision rational numbers. Therefore, we can construct transformer $M_t(x,y)$ to simulate $\\phi(t,x,y)$. According to transformer properties, the relationship between t and input length l(x) is of polynomial complexity. Note that we currently cannot prove whether the large language model obtained using the content from 3.3 can infinitely approximate C(x|y).\nCorollary 4.5. Given x and t, we cannot determine whether there exists $M_t^+(x,y) = C(x | y)$.\nIf we could determine this, we would conclude that C(x | y) is computable.\nCorollary 4.6. For any $\\epsilon > 0$ and each $t'$, there exist infinitely many x such that there is always an $\\epsilon$ error between $M_{t'}^+ (x, y)$ and C(x | y).\n4.4 tells us that theoretically, we can construct a series of transformer networks to gradually approximate C(x | y) (i.e., AGI). However, 4.5 and 4.6 indicate that in practice, we cannot precisely determine how close we are to C(x | y). Particularly, 4.6 further shows that there will always be some x that have a certain error with C(x | y). Unfortunately, we currently cannot determine whether these x with errors are all truly x that we care about (since some x might be completely meaningless random strings). However, we will prove through contradiction that there indeed exist some x that we care about where, due to the time step limitation t, there will be significant errors between C(x | y) and $M_t^+(x, y)$.\nNote that in the following proof, we assume NP \u2260 P. In fact, even without this assumption, as long as we can construct a problem that exceeds polynomial time complexity, the conclusion still holds.\nSuppose string (k, c) = x is as follows:\nk: Given n sets {n sets}, for each set, does there exist a subset whose sum equals exactly 42?\nc: yes, no, no, yes, no......\nwhere {n sets} are n large integer sets, and c is a string of length n composed of \"yes\" or \"no\","}, {"title": "Conclusion", "content": "determined by the content of {n sets"}, ".", "Obviously, x represents the subset sum problem, which is an NP-complete problem. Since we assume NP \u2260 P, this means there exists no polynomial-time algorithm that can solve this problem.\nTherefore, clearly, $C(x | y) = C(k | y)+l(p) + O(1)$, where l(p) represents the length of program p that solves the subset sum problem. Since no program p of polynomial complexity exists, this means the number of steps t that the program describing C(x | y) executes on the Turing machine exceeds polynomial complexity. Therefore, $C(x | y) \u2260 M_t^+(x,y)$. In conclusion, we have found a meaningful string x where there exists a significant error between $C(x | y)$ and $M_t^+(x, y)$.\nTo better approximate C(x | y), we need to increase the value of t in $M_t^+(x, y)$. In transformer structures, there are two ways to increase t in $M_t^+(x, y)$:\n*   Increase the model's parameter count: As the model's parameter count increases, under the same l(x), t in $M_t^+(x, y)$ becomes larger.\n*   Increase the encoding process steps: For example, introduce intermediate reasoning path m in x = (k, c), so that the encoding of the first character of c is not generated directly from k, but from (k, m).\nIt is not difficult to observe that the first method corresponds to the pre-training scaling laws while the second method corresponds to the inference scaling laws. The second method has multiple implementations, such as: chain-of-thought (CoT)(Wei et al., 2022), designing complex agent systems on LLMs(Anthropic, 2024), and OpenAI's ol(Jaech et al., 2024), among others. These can all be considered implementations of the second method. They expand the value of t by increasing the number of output tokens. Whether it is the first or the second method, both essentially approximate C(x | y)"]}