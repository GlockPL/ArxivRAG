{"title": "To Predict or Not To Predict?\nProportionally Masked Autoencoders for Tabular Data Imputation", "authors": ["Jungkyu Kim", "Kibok Lee", "Taeyoung Park"], "abstract": "Masked autoencoders (MAEs) have recently demonstrated\neffectiveness in tabular data imputation. However, due to\nthe inherent heterogeneity of tabular data, the uniform ran-\ndom masking strategy commonly used in MAEs can disrupt\nthe distribution of missingness, leading to suboptimal perfor-\nmance. To address this, we propose a proportional masking\nstrategy for MAES. Specifically, we first compute the statis-\ntics of missingness based on the observed proportions in the\ndataset, and then generate masks that align with these statis-\ntics, ensuring that the distribution of missingness is preserved\nafter masking. Furthermore, we argue that simple MLP-based\ntoken mixing offers competitive or often superior perfor-\nmance compared to attention mechanisms while being more\ncomputationally efficient, especially in the tabular domain\nwith the inherent heterogeneity. Experimental results validate\nthe effectiveness of the proposed proportional masking strat-\negy across various missing data patterns in tabular datasets.\nCode is available at: https://github.com/normal-kim/PMAE.", "sections": [{"title": "Introduction", "content": "Tabular data often contain missing values in real-world sce-\nnarios, posing significant challenges for the deployment of\nmachine learning algorithms (Donders et al. 2006). Inspired\nby the recent success of masked autoencoders (MAEs) in\nrepresentation learning across domains such as computer vi-\nsion (CV) (He et al. 2022) and natural language processing\n(NLP) (Devlin et al. 2018), Du, Melis, and Wang (2024) pro-\nposed adapting MAEs for tabular data imputation. However,\nwe argue that naively applying the uniform random mask-\ning strategy from MAEs to tabular data results in subopti-\nmal performance due to the intrinsic heterogeneity of tabular\ndata. Unlike images or word tokens, which are relatively ho-\nmogeneous and semantically invariant to spatial shifts, tab-\nular data are inherently heterogeneous. That is, each column\ncontains distinct information, making spatial shifts mean-\ningless. Such heterogeneity also extends to the distribution\nof missing values, which can vary across columns. Thus,\nuniform random masking can unintentionally disrupt these\ndistributions by omitting critical variables that are essential\nfor predicting others (Wilms et al. 2021), thereby leading to"}, {"title": "Preliminaries", "content": "Incomplete Data Let $x_i = (x_{i1}, ..., x_{id})^T \\in \\mathbb{R}^d$ be the\ni-th tabular data with d columns, sampled from a data dis-\ntribution $f(x)$. Without loss of generality on the order of\ncolumns, let $x_i = (x_{obs}, x_{mis})_i$ be a decomposition, where\n$x_{obs}$ and $x_{mis}$ represent the observed and missing columns\nof the data, respectively. For each scalar entry $x_{ij}$, let $\\delta_{ij} :=\n\\mathbb{I}(x_{ij} \\text{ is observed})$ be its associated missing indicator. Then,\nan incomplete data and its corresponding observed mask are\ndefined as follows:\n1. The scalar entry of an incomplete data is expressed as\n$x_{ij} := x_{ij} \\cdot \\mathbb{I}(\\delta_{ij} = 1) + \\text{nan} \\cdot \\mathbb{I}(\\delta_{ij} = 0)$.  (1)\n2. The observed mask $m_i \\in \\mathbb{R}^d$ is the realization of a ran-\ndom missing indicator $\\delta_i$.\nMissing Mechanism Three types of missing mechanisms\ncommonly occur in the real world (Little and Rubin 2019):\n1. Missing Completely At Random (MCAR) occurs when\nthe missingness does not depend on the data, i.e., $\\forall x$,\n$P(\\delta|x) = P(\\delta)$.\n2. Missing At Random (MAR) occurs when the missing-\nness depends only on the observed data, i.e., $P(\\delta|x) =$\n$P(\\delta|x_{obs})$.\n3. Missing Not At Random (MNAR) occurs when the miss-\ningness does not depend only on the observed data, i.e.,\n$P(\\delta|x) \\neq P(\\delta|x_{obs})$.\nMissing Patterns We propose a categorization of missing\ndata patterns commonly encountered in practice, as illus-\ntrated in Figure 1. When a value $x_{ij}$ is missing for a par-\nticular variable j, the pattern can be classified as follows: (i)\nMonotone, when there is a rearrangement of columns such\nthat all subsequent variables $x_{ik}$ for $k > j$ are also miss-\ning for the same observation i (Molenberghs et al. 1998),\n(ii) Quasi-Monotone, which is similar to Monotone, but al-\nlowing a few exceptions instead of requiring strict equality\nin the sequence of missing data, and (iii) General, when no\nspecific structure exists.\nImputation Task Given an incomplete dataset $D:=\n\\{(x_i, m_i)\\}_{i=1,...,n}$, we aim to obtain plausible estimates for\ninputs $x_i$ by learning an imputation function $f(x_i, m_i; \\theta)$\nthat can best approximate the true value of missing data:\n$\\hat{x_{ij}} = f(x_i, m_i; \\theta) \\cdot \\mathbb{I}(\\delta_{ij} = 0) \\approx x_{ij} \\cdot \\mathbb{I}(\\delta_{ij} = 0)$."}, {"title": "Motivation", "content": "In this section, we formalize the application of MAEs in the\ntabular domain.\nMasking Function Suppose an additional missing mask\nis applied to the raw data. After this additional masking, the\nobserved mask $m_i$ can be expressed as\n$m_i = m_i^+ + \\bar{m_i^+},$ (2)\nwhere $m_i^+$ is an indicator vector representing the parts that\nremain observed after applying the additional mask, with en-\ntries set to 1 for observed parts, and $\\bar{m_i^+}$ is an indicator vec-\ntor with entries set to 1 for additionally masked parts (see\nFig 2). The process of additional masking is as follows:\n*   Draw a uniform random variable $u_{ij} \\sim U(0,1)$,\n*   Given that $\\delta_{ij} = 1$, mask according to the following:\n$\\text{mij} := \\begin{cases}\n1 & \\text{if $u_{ij} < M_j(\\cdot)$},\n0 & \\text{otherwise},\n\\end{cases}$  (3)\nwhere $M_j(\\cdot) \\in \\mathbb{R}$ denotes the masking function applied to\nthe column j, indicating the extent to which the additional\nparts of data is masked.\nMAE A masked autoencoder, $h = d \\circ g$, is an encoder-\ndecoder architecture designed to predict the entries with the\nadditional missing mask, $m$, applied to the raw data. It op-\nerates on partial input information, $x_i^* = x_i \\odot m_i^+$, which is\nprovided due to masking.\nTabular MAE Loss While typical MAE in CV/NLP focus\non prediction tasks, reconstruction is also important in the\ntabular domain as they are semantically complex and non-\nredundant (Du, Melis, and Wang 2024). We formally define\nthe general loss for the standard MAE of tabular data for a\nsample i in an arbitrary batch B of the j-th column as:\n$l_{ij} := \\frac{\\sum_{i \\in B} ((h(x_i \\odot m_i^+))_j - x_{ij}) \\cdot m_{ij})^2}{\\sum_{i \\in B} m_{ij}},$ (4)\nwhere the j-th column of x is masked or unmasked based\non the value of $m_{ij}$ in (3).\nSince randomness exists in the encoder input through $m_i^+$\nover all j, we begin our analysis by fixing all but column j.\nThen, along with m, we focus on the randomness in the\nj-th entry of the following expression:\n$\\bar{mij} := m_i - mej$,  (5)\nwhere ej is the one-hot vector. Let $l_j^P := \\frac{\\sum_{i \\in B} ((h(x_i \\odot m_i^+))_j - x_{ij})^2}{\\sum_{i \\in B} \\bar{mij}}$ be a\nprediction loss and $l_j^R := \\frac{\\sum_{i \\in B} ((h(x_i \\odot m_i^+))_j - x_{ij})^2}{\\sum_{i \\in B} m_{ij}}$ be the\nreconstruction counterpart. Then, (4) can be re-written as:\n$l_{ij} := \\bar{mij} \\cdot l_j^P + mij \\cdot l_j^R$.  (6)\nNote that the MAE will solve for prediction task if $\\bar{mij} = 1$\nand reconstruction task if $mij = 0$. Now, since the random-\nness only exists for $\\bar{mij}$ (or equivalently for $m_{ij} = 1 - \\bar{mij}$,\nwhere observed mask $mij = 1$, if $\\delta_{ij} = 1$), we can compute\nthe expectation of the loss for column j defined in (6):\n$E_m[l_{ij}] = E_m[\\bar{mij}] \\cdot l_j^P + E_m[m_{ij}] \\cdot l_j^R$\n$= M_j(\\cdot) \\cdot l_j^P + (1 - M_j(\\cdot)) \\cdot l_j^R$.  (7)"}, {"title": "The Impact of Uniform Random Masking", "content": "Applying\nuniform random masking at a constant ratio (i.e., $M_j(\\cdot) =$\n0.5) in (7) essentially assigns equal prediction importance to\nall columns. However, since tabular data are heterogeneous\nand exhibit complex relationships between columns, such\nequal weighting may not be ideal. Moreover, this approach\nmay mask fully observed columns just as likely as partially\nobserved ones. If the inadvertently masked entries are crit-\nical for predicting values in other columns, this could lead\nto omitted variable bias (Wilms et al. 2021), leading to sub-\noptimal model performance (Wu et al. 2024). For columns\nwithout any missing data, it may be more effective to avoid\nmasking entirely and, consequently, refrain from predicting\nvalues that are already fully observed.\nBalancing with Inverse Propensities Moreover, naively\ncomputing the loss over only the complete cases may lead\nto biased estimation. Let us focus on the randomness in the\nmissingness of $x_i$; recall that the random variable $\\delta_{ij}$, indi-\ncates whether $x_{ij}$ is observed. Consider the naive empirical\nloss for the j-th column, $\\hat{l_j}^{naive} := \\frac{1}{B} \\sum_{i \\in B} \\delta_{ij} \\cdot l_{ij}$. Then,\n$E_\\delta[\\hat{l_j}^{naive}] = E_\\delta \\Big[\\frac{1}{B} \\sum_{i \\in B} l_{ij} - (1 - \\delta_{ij}) \\cdot l_{ij} \\Big]$\n$= l_j - E_\\delta \\Big[\\frac{1}{B} \\sum_{i \\in B} (1 - \\delta_{ij}) \\cdot l_{ij} \\Big]$.  (8)\nSince $\\delta$ is a random variable that may depend on x, the sec-\nond term in the last equality generally depends on x unless\n$\\delta \\perp x$. Consequently, $\\hat{l_j}^{naive}$ may be a biased estimator of $l_j$\ndue to the second term in (8).\nHowever, if we have access to the probability of an en-\ntry being observed given the realized data, or the propen-\nsity score function, $\\pi_{ij}(x_i; \\phi) := P(\\delta_{ij} = 1|x_i; \\phi)$, we\ncan balance the loss, using only the complete cases. Let\n$\\hat{l_j}^{IPS} := \\frac{1}{B} \\sum_{i \\in B} \\frac{\\delta_{ij}}{\\pi_{ij}(x_i; \\phi)} \\cdot l_{ij}$ be the empirical loss weighted\nby the inverse propensity score. Then,\n$E_\\delta[\\hat{l_j}^{IPS}] = E_\\delta \\Big[\\frac{1}{B} \\sum_{i \\in B} \\frac{\\delta_{ij}}{\\pi_{ij}(x_i; \\phi)} \\cdot l_{ij} \\Big] = l_j.$ (9)\nIntuitively, this approach assigns higher importance to less\nfrequently observed samples to balance the overall loss (Sea-\nman and White 2013; Kim and Shao 2021). This ensures that\nthe model adequately accounts for underrepresented sam-\nples, which might otherwise have a minimal impact on the\noverall loss.\nHowever, the practical application of this approach re-\nlies on accurately estimating the propensity score function,\n$\\pi_{ij}(x_i; \\phi)$. Incorrect estimation could result in unbounded\nloss values, compromising model performance (Li et al\n2023). To address this challenge, we propose a more implicit\nmasking strategy guided by the following core principles: (i)\nthe design of the masking function determines which sam-\nples are assigned higher prediction loss, (ii) masks should"}, {"title": "Method", "content": "Proportional Masking\nWe propose to guide MAEs to emphasize prediction or\nreconstruction in specific columns using a well-designed\nmasking scheme by leveraging the observed proportions of\ncolumns as prior information, defined as:\n$P_{obs} := \\frac{1}{B} \\sum_{i \\in B} m_i \\in \\mathbb{R}^d$.  (10)\nSince $\\delta_{ij} \\sim Ber(\\pi_{ij})$, (10) serves as an MLE estimate for\nthe column-wise average of the unknown propensity score\nfunction, i.e., $P_{obs,j} \\approx E_x[\\pi_{\\cdot j}(x; \\phi)]$.\nOur proposed masking function takes the logit-\ntransformed value of (10), where we verify the effectiveness"}, {"title": "Architecture: MLP-Mixers vs. Transformers", "content": "Transformer-based architectures have been prevalent in\nmany MAE designs in the CV/NLP domains, where stacks"}, {"title": "Experiments", "content": "Experimental Setup\nSemi-synthetic Missing Pattern Generation Given a\ncomplete dataset, we specify missing patterns as follows:\n1. Monotone Missing\n*   Generate $M_m$ with $p_{col} \\in \\{0.3, 0.6\\}$.\n*   Generate missing entries with a fixed probability of 0.5\n(i.e., $P(\\delta_{ij} = 1) = 0.5$ for all j).\n2. Quasi-Monotone Missing\n*   Set $p_{col} = 0.6$ for $M_1$, and let $M_2 := (M_1)^c$.\n*   $\\forall j \\in M_1, p_j \\sim U(0.95, 0.99), P(\\delta_{ij} = 1) = p_j$.\n*   $\\forall j \\in M_2, p_j \\sim U(0.2, 0.8)$, and $P(\\delta_{ij} = 1) = p_j$.\n3. General Missing (or Non-Monotone Missing)\n*   Set $p_{col} = 1$ for $M_g$.\n*   $\\forall j \\in M, p_j \\sim U(0.2, 0.8)$, and $P(\\delta_{ij} = 1) = p_j$.\nNote that $M := \\{j|P(j \\in M) = p_{col}\\}$ denotes the col-\numn indices that will have missing data, with the missing\nproportion $p_{col}$. The propensity score function $\\pi_{ij}(x_i; \\phi) =$\n$P(\\delta_{ij} = 1|x_i)$ is specified with the missing mechanism and\nthe generated $p_j$. In all settings, we apply the most challeng-\ning MNAR. Details are provided in the Appendix.\nDatasets We evaluate PMAE along with the baselines us-\ning a semi-synthetic setup on nine real-world benchmark\ndatasets (Asuncion, Newman et al. 2007)."}, {"title": "Baselines and Evaluation", "content": "Baseline Methods We compare our model with the fol-\nlowing baselines: Naive (numerical:mean and categori-\ncal:mode), KNN (Troyanskaya et al. 2001) EM (Demp-\nster, Laird, and Rubin 1977), MissForest (Stekhoven and\nB\u00fchlmann 2012), MiceForest (Shah et al. 2014), MI-\nWAE (Mattei and Frellsen 2019), GAIN (Yoon, Jordon, and\nSchaar 2018), MIRACLE (Kyono et al. 2021), HyperIm-\npute (Jarrett et al. 2022), TDM (Zhao et al. 2023), and Re-\nMasker (Du, Melis, and Wang 2024).\nParameter Setting Our implementation mostly follows\nReMasker with the same optimization procedures, but in-\ntroduces (i) a new loss formulation in (4) and (ii) a novel\nmasking function $MPM(p_{obs,j}; 0.05, 0.5)$, which are applied\nto (iii) Transformer and MLP-Mixer architecture.\nImputation Accuracy Instead of the widely used RMSE,\nwe evaluate imputation performance using a metric we call\nImputation Accuracy, a weighted average of Accuracy (for\ncategorical columns) and R2 (for numerical columns):\n$Acc_j = \\frac{\\sum_{i \\in I^{mis}} \\mathbb{I}(X_{ij} = \\hat{X}_{ij})}{|I^{mis}|}$  (14)\n$R^2_j = 1 - \\frac{\\sum_{i \\in I^{mis}} (\\hat{X}_{ij} - X_{ij})^2}{\\sum_{i \\in I^{mis}} (\\bar{X_{ij}} - X_{ij})^2}$, (15)\n$\\text{Imp. Acc} := \\frac{1}{D^{mis}}\\sum_{j \\in D^{mis}} \\big(Acc_j \\cdot \\mathbb{I}(j \\in D^{cat}) + R^2_j \\cdot \\mathbb{I}(j \\notin D^{cat})\\big)$, (16)\nwhere $D^{mis}$ and $I^{mis}$ denote missing categorical column in-\ndices and missing row indices for column j respectively.\nSince the values are adapted to each data type and normal-\nized to 1, this evaluation serves as an intuitive and holistic\nmeasure of imputation performance for tabular data.\nOther Measures While not the primary focus of this\nstudy, we also evaluate the following:"}, {"title": "Conclusion", "content": "Tabular data is inherently heterogeneous, with each column\nhaving distinct characteristics and often exhibiting complex\npatterns of missing values. To address this challenge, we\npropose PMAE, a simple yet effective strategy that employs\na logit-based masking function. This method preserves the\ndistribution of missingness while prioritizing data inversely\nto their observed proportions. When tested across diverse\nset of missing patterns, PMAE demonstrated robust perfor-\nmance, consistently surpassing state-of-the-art methods\nLimitations and Future Work This study does not exam-\nine the relationship between a dataset's covariance structure\nand the proportional masking scheme, which could provide\ndeeper understanding and broader applicability. Capturing\ncovariance in the presence of missing data, however, remains\na challenge. Addressing these issues may enable application\nof PMAE to high-dimensional tasks like image inpainting."}, {"title": "A Implementation Details", "content": "A.1 MAE\nThe MAE implementation closely follows ReMasker (Du, Melis, and Wang 2024), referencing their code available at\nhttps://github.com/tydusky/remasker, with the following details:\n*   Regarding the optimization procedures (learning rate, learning rate scheduler, and training epochs), we follow ReMasker's\n    configuration. We use different batch sizes depending on the size of the data set for better optimization:\n    *   For n < 1,000: batch size = 128\n    *   For 1,000 < n < 2, 500: batch size = 256\n    *   For 2, 500 < n < 5,000: batch size = 512\n    *   For 5,000 < n < 10,000: batch size = 1,024\n    *   For 10,000 < n < 20,000: batch size = 2,048\n    *   For n \u2265 20,000: batch size = 4,096\n*   Regarding the loss function, we use (4) for PMAE, and the for ReMasker as discussed in the Ablation Study.\n*   Regarding the architecture, we fix the configurations across all datasets:\n    *   We fix the encoder width to 32.\n    *   We fix the encoder depth at 6, and the decoder depth at 4.\n    *   We fix the attention heads to 4.\n    *   For Transformers, we use the same architecture as in ReMasker (stacking multiple layers of (12)).\n    *   For MLP-Mixers, we replace SA blocks with timm.layers. Mlp blocks (as in (13)), each with dropout 0.1, GELU\n        activation and expander width ratio 4.\n*   Regarding the masking function, we used $MPM(P_{obs,j}; 0.05, 0.5)$ for PMAE; for ReMasker, we followed the original\n    implementation and set $M_j$ to 0.5 for all columns regardless of the varying levels of observed proportions."}, {"title": "B Detailed Evaluation Procedures", "content": "B.1 Pre-processing\nDataset We layout the procedures for processing discrete categorical variables and continuous numerical variables.\n*   Categorical Variables: To address the varying levels of cardinality and the long-tail distribution of categorical variables,\n    we regroup minor categories into a single common category as follows:\n    Regrouping criteria: If a category's frequency is less than the following, it is assigned to a common category:\n    i. if n/100 > 30, regroup if category's frequency < round(n/100),\n    ii. else, regroup if category's frequency < 30.\n    Apply LabelEncoder function from the Scikit-learn python library and organize categorical variables.\n*   Numerical Variables: Apply QuantileTransformer for numerical variables to make the distributions symmetric.\n    As a result of the above, we have: $X \\in \\mathbb{R}^{n \\times (d_n + \\sum_{j=1,...,c} K_j)}$ where $d_n$ denotes the number of numerical columns and $K_j$\ndenotes the cardinality of the categorical column $C_j$.\nB.2 Synthetic Generation of Missing Mechanisms\nWe refer to (Jarrett et al. 2022) for simulating different missing mechanisms. We fit a logistic regression model for the propensity\nscore function $\\pi_{ij}(x; \\phi_j) = Pr(\\delta_{ij} = 1|x; \\phi_j)$. Here we describe how the parameters $\\phi_j = \\{\\beta_j\\'\\}$ are set:\n*   Choose an observed proportion $p_j$ for the given missing column index ($j \\in M$).\n*    $\\beta_j \\sim N(0, 1)$ with $j\\' = 1, ..., d_n, d_{c1}^1, d_{c1}^2, ..., d_{c1}^{K_1}, ..., d_{ck}^1, d_{ck}^2, ..., d_{ck}^{K_k}$, which is the reordered indices of numerical and cate-\ngorical variables; $d_n$ denotes the number of numerical variables and $d_{ck}^1$ denotes the first category of the kth categorical\ncolumn.\n*   Choose a mechanism and apply the following before fitting the propensity score function:\n    *   MCAR: Set $\\beta_{j\\'} = 0$ for all $j\\'$.,\n    *   MAR: Set $\\beta_{j\\'} = 0$ for $j\\' \\notin M$,\n    *   MNAR: For Monotone missing, set $\\beta_{j\\'} = 0$ for $j\\' \\notin M$; otherwise, do not change anything;\n*   Solve for $\\sigma(\\beta_0 + X \\beta) = p_j$ to set the values for $\\beta_0$ where $\\sigma(\\cdot)$ is a sigmoid function,\n*   Generate missingness: $\\delta_{ij} \\sim Ber(\\pi_{ij}(x; \\Phi_j))$\nB.3 Synthetic Generation of Missing Patterns\nWe present the simulated results of missing data patterns, where each line is a KDE-plot for the distribution of the observed\nproportion of columns. As the pattern becomes more general, the observed proportions across samples vary more uniformly.\nB.4 Evaluating the Imputation Model\nGround Truth and Missing Values After generating the observed mask, we restore the dimension of the column to d. The\nground truth data (X*), incomplete data (X), and imputed data (\\hat{X}) are then defined accordingly:\n$X^* := X \\odot (\\mathbb{1}_{n \\times d} - M) \\in \\mathbb{R}^{n \\times d}$  (B.1)\n$X := X \\odot M + nan \\cdot (\\mathbb{1}_{n \\times d} - M) \\in \\mathbb{R}^{n \\times d}$ (B.2)\n$\\hat{X} := f(X, M; \\hat{\\Theta}) \\cdot (\\mathbb{1}_{n \\times d} - M) \\in \\mathbb{R}^{n \\times d}$ (B.3)\nEvaluation We feed in (X, M) to the trained imputation model $f(X, M; \\hat{\\Theta})$ to obtain $\\hat{X}$, and evaluate for (X, X^*). Note\nthat for evaluation of categorical columns, we round the imputed values to the nearest integer."}]}