{"title": "SimBench: A Rule-Based Multi-Turn Interaction Benchmark for Evaluating an LLM's Ability to Generate Digital Twins", "authors": ["Jingquan Wang", "Harry Zhang", "Huzaifa Mustafa Unjhawala", "Peter Negrut", "Shu Wang", "Khailanii Slaton", "Radu Serban", "Jin-Long Wu", "Dan Negrut"], "abstract": "We introduce SimBench, a benchmark designed to evaluate\nthe proficiency of student large language models (S-LLMs)\nin generating digital twins (DTs) that can be used in simu-\nlators for virtual testing. Given a collection of S-LLMs, this\nbenchmark enables the ranking of the S-LLMs based on their\nability to produce high-quality DTs. We demonstrate this by\ncomparing over 20 open- and closed-source S-LLMs.\nUsing multi-turn interactions, SimBench employs a rule-\nbased judge LLM (J-LLM) that leverages both predefined\nrules and human-in-the-loop guidance to assign scores for the\nDTs generated by the S-LLM, thus providing a consistent and\nexpert-inspired evaluation protocol. The J-LLM is specific to\na simulator, and herein the proposed benchmarking approach\nis demonstrated in conjunction with the Chrono multi-physics\nsimulator. Chrono provided the backdrop used to assess an S-\nLLM in relation to the latter's ability to create digital twins for\nmultibody dynamics, finite element analysis, vehicle dynam-\nics, robotic dynamics, and sensor simulations. The proposed\nbenchmarking principle is broadly applicable and enables the\nassessment of an S-LLM's ability to generate digital twins for\nother simulation packages. All code and data are available at\nhttps://github.com/uwsbel/SimBench.", "sections": [{"title": "Introduction", "content": "It is only a matter of time before a non-expert will be able to\ninteract with an S-LLM in order to produce a sophisticated\ndigital twin for a physical system of interest. The DT request\nmight read \"generate a model of a VIPER rover operating\non regolith and equipped with two monocular cameras, one\nstereo camera, and an autonomy stack that uses sensor feeds\nto ensure obstacle avoidance while operating in lunar condi-\ntions.\" Such a model can be generated for a target simulator\nshould an expert dedicate enough time to the task. However,\nthe interest here lies in the scenario where the DT generation\nfalls upon an LLM. Specifically, this contribution proposes\na benchmark that measures the acumen of such an S-LLM\nwhen it comes to generating DTs for a target simulator.\nAnalyzing complex multi-physics phenomena via simu-\nlation requires sophisticated DTs. S-LLMs can potentially\nhandle these through multiple rounds of interaction, in-\ncorporating natural language feedback from humans. To\nthe best of our knowledge, dedicated evaluation protocols\nfor the task of generating DTs are nonexistent, as generic\nsimilarity-based metrics like CodeBLEU (Ren et al. 2020)\nand ROUGE-L (Lin 2004) do not align well with real-world\nsimulation applications and are too crude to be useful. Fur-\nthermore, execution-based benchmarks like pass@k (Chen\net al. 2021) are too strict, often resulting in zero scores even\nthough the generated DT has only marginal flaws. Against\nthis backdrop, the purpose of this contribution is to outline a\nbenchmark that can indicate how good an S-LLM is at gen-\nerating a DT. Here, a DT encapsulates a model used by a\nsimulator to carry out a virtual experiment. For the VIPER\nrover example above, this model might be set up via hun-\ndreds of lines of code, e.g., Python, C++, or a proprietary\nmodel definition language. Additionally, if the model is in-\nstead generated using input files, e.g., in JSON format, the\nfiles might have hundreds to thousands of lines. Going back\nto the VIPER example, the Python model-file or the JSON\nfile would establish for each wheel its center of mass, mass\nmoments of inertia, initial position, initial velocity, collision\ndetection mesh, visualization mesh, friction coefficients; for\nthe chassis one would specify similar information; the cam-\nera models would require parameters associated with the bi-\ndirectional reflectance distribution function, aperture, expo-\nsure, focal length, parameters associated with Image Sig-\nnal Processing step, etc. The rover would operate on ter-\nrain characterized by bulk density, internal friction angle,\ncohesion coefficient, particle size distribution, etc. Next, one\nwould add an autonomy stack by setting up a ROS2 environ-\nment, choosing perception algorithms, a planning solution,\na control policy, etc. Ultimately, the goal is to have an S-\nLLM generate the Python code that sets up the digital twin;\nand when the DT-generation request is vague and values for\nsome model parameters are not provided, the S-LLM would\neither use default values or prompt the user for input.\nInsofar as the nomenclature used in the contribution is\nconcerned, an analysis task in the real world corresponds\nto a simulation scenario in the digital world. A physical sys-\ntem, e.g., the VIPER rover, has a digital twin. Several dig-\nital twins, e.g., those of the rover, terrain, sensor suite, are\ncombined in a simulation scenario towards running a vir-\ntual experiment. In the absence of a sim-to-real gap, the re-\nsults collected in simulation are representative of results that\nwould have been collected had the experiment taken place\nin the real world. Here, the expectation is that an S-LLM\nis used to generate the required digital twin[s], place them\nin a simulation scenario, and run a virtual test of interest"}, {"title": "Methodology", "content": "SimBench employs a diverse set of simulation scenar-\nios associated with application areas that draw on multi-\nbody dynamics (MBD) analysis, vehicle (VEH) and robotic\n(RBT) experiments, finite element analysis (FEA), and sen-\nsor (SEN) use. Each scenario tests specific aspects of the\nS-LLM's performance, including DT set up, reasoning, and\nability to carry out PyChrono model adjustments. Obtain-\ning S-LLMs falls outside the scope of this contribution; the\nfocus is on J-LLM and the benchmarking methodology.\nThe J-LLM associated with the Chrono SimBench han-\ndles multi-physics simulations, including but not limited to:\n\u2022 Collision, Contact and Friction Dynamics (MBD):\nScenarios involving multi-link arms, gear mechanisms,\nslider-crank system and other typical mechanisms.\n\u2022 Vibration, deformation, stress, and strain (FEA): Sce-\nnarios involving cable, beam, shells, plates that evaluate\nthe S-LLM's proficiency in structural analysis.\n\u2022 Vehicle Dynamics (VEH): City buses, off-road vehicles\n(e.g., HMMWV, M113), trucks (e.g., Kraz, MAN), and\nsedans are used to test the S-LLM's ability to simulate\ndriving scenarios. Driver, engine, transmission, and tire\nmodels, as well as high-level control policies integrated\nwith Sensors are included in the benchmark.\n\u2022 Sensor Integration (SEN): Scenarios involving GPS,\nIMU, LiDAR, and camera sensors are used to exercise\nthe S-LLM's capability to support perception tasks for\nautonomous vehicles and robotic systems.\n\u2022 Robotics Dynamics (RBT): The benchmark touches on\nrobotic systems like Turtlebot, Curiosity, and VIPER, as\nwell as granular dynamics and deformable terrain simu-\nlations, e.g., the Soil Contact Model (SCM) (Krenn and\nGibbesch 2011), that come into play in off-road opera-\ntions for both robots and vehicles.\nSimBench draws on 102 demonstration tasks associated\nwith 34 distinct physical systems of the categories MBD\nthrough RBT listed above. These tasks involve setting up\nand progressively modifying digital twins, with each task\nbroken down into three high-quality turns. These turns have\nbeen designed by simulation experts to gradually increase\nin complexity, thus enabling the J-LLM to provide a robust\nassessment of the S-LLM's capabilities. A list of example\nsimulation scenarios in SimBench is provided in Figure 2.\nTask Design and Structure: SimBench tasks assigned to\nan S-LLM fall into the categories of \"Vague Requests\"\nand \"Sharp Requests.\" The \"Vague Request\" tasks provide\nmore abstract and less precise instructions, requiring the S-\nLLM to interpret and complete the task with minimal guid-\nance, thereby allowing for significant latitude. Conversely,\na \"Sharp Request\" offers detailed and explicit instructions,\nensuring that the S-LLM has the necessary information to\nproduce very specific DTs. Table 1 presents sample prompts\nfrom both categories, illustrating the varying levels of task\nrequest specificity."}, {"title": "Benchmarking Experiments", "content": "S-LLMS Evaluated. SimBench is demonstrated in conjunc-\ntion with benchmarking a set of 20 S-LLMs, encompassing\nboth open- and closed-source models of various sizes, archi-\ntectures, and family. For closed-source models, we include\nprominent commercial S-LLMs, such as gpt-40-mini and\ngpt-40 from OpenAI, as well as Claude 3.5 Sonnet from\nAnthropic, and Gemini 1.5pro from Google. The open-\nsource models evaluated include the LLaMA-3.1 family\n(8B, 70B, 405B), which consists of both base and instruct\nversions. We included specialized models like codellama-\n70B, codestral-22B, CodeGemma-7B, and also the mamba\nclass mamba-codestral-7b model, fine-tuned for code gen-\neration. We also considered S-LLMs that have been trained\non a mix of general and domain-specific data, e.g., Gemma-\n2 (2B, 9B, 27B) and Mistral (12b, 8x7b, 8x22B). Addi-\ntionally, models such as Phi-3 (3.8B, 7B, 14B) and Mix-\ntral (8x7B, 8x22B, 122B) are included to explore the im-\npact of different scales and training regimes on their abil-\nity to perform in multi-turn setups. Because we are testing\nthe instruction following capability of the S-LLMs, mod-\nels like Starcoder-2 are not included. For the J-LLM, we\npicked gpt-40-mini because it gives the best performance in\nthe pass@1 metric, which is highly relevant in simulation.\nMetric. We employ the most common used code evalua-\ntion metric - pass@1, as well as the proposed J-LLM with\nthree different modalities. As a reference point, we also em-\nploy the most commonly used similarity metrics CodeBLEU\nand ROUGE-LSUM. The results averaged over all test cases\nare shown in Table 2. Although all the tested S-LLMs are\npre-trained on Project Chrono, the highest pass@1 is only\n12.8% achieved by gpt-40-mini. Notably, gpt-4o gets a bot-\ntom score. Table 3 reports the multi-turn performance of S-\nLLMs in the SimBench environment, benchmarked with the\nmetrics J-LLM_Ref_Doc.\nAnalysis of results. The pass@k metric is the most impor-\ntant metric, as it indicates the correctness of the DT[s] and\nsimulation scenario. However, the pass@k metric is hard to\nemploy since it is difficult to write unit tests for complex\nsimulation scenarios, some of which might run for days, e.g.,\nfluid-solid interaction scenarios. Nonetheless, for the set of\n34 physical systems considered herein, we evaluated the per-\nformance of pass@1, of the proposed rule based J-LLM\nmetrics, of the similarity metrics CodeBLEU and ROUGE-\nLSUM, and the compile@1 metric. There are 34 mechan-\nical systems considered, each with three turns, attacked by\n19 S-LLMs (three S-LLMs failed, see bottom of Table 2).\nThis translated into 1938 simulations, which were assessed\nby seven metrics see heading of Table 2). This yielded\n1938 \u00d7 7 = 13,566 data points that entered a Spearman\nrank correlation analysis to compare the different metrics."}, {"title": "Conclusion and Future Work", "content": "To the best of our knowledge, this contribution is the first\nto establish a high-quality dataset and associated bench-\nmark for simulating the ability of a S-LLM to generate DTs\nfor multi-physics simulation. Although most S-LLMs are\npre-trained on the Project Chrono code base, the best \"out\nof box\" S-LLM only give a pass@1 value of 13%. Sim-\nBench is positioned to guide future S-LLMs improvements\nfor Chrono DTs or those associated with other simulators.\nSecondly, The proposed rule-based J-LLM is a more ef-\nfective judge compared to execution-based benchmarks like\ncompile@1, as well as similarity-based metrics like Code-\nBLEU and ROUGE-LSUM, because it more closely aligns\nwith pass@1, which reflects the true and ultimate perfor-\nmance of S-LLMs. Lastly, we note that utilizing reference\ncode and API documentation enhances the performance of\nthe J-LLM. This finding also suggests a potential direction\nfor future work: fine-tuning the J-LLM based on ground\ntruth code and API documentation; or using RAG-based ap-\nproaches. We plan to also increase the number of simulation\nscenarios available in the benchmark, particularly in areas\nsuch as fluid-solid interaction and multi-phase flow. Finally,\nan upcoming contribution addresses the challenge of pro-\nducing high-quality S-LLMs, complementing the J-LLM-\nfocused discussion presented here."}]}