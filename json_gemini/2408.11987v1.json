{"title": "SimBench: A Rule-Based Multi-Turn Interaction Benchmark for Evaluating an LLM's Ability to Generate Digital Twins", "authors": ["Jingquan Wang", "Harry Zhang", "Huzaifa Mustafa Unjhawala", "Peter Negrut", "Shu Wang", "Khailanii Slaton", "Radu Serban", "Jin-Long Wu", "Dan Negrut"], "abstract": "We introduce SimBench, a benchmark designed to evaluate the proficiency of student large language models (S-LLMs) in generating digital twins (DTs) that can be used in simulators for virtual testing. Given a collection of S-LLMs, this benchmark enables the ranking of the S-LLMs based on their ability to produce high-quality DTs. We demonstrate this by comparing over 20 open- and closed-source S-LLMs.\nUsing multi-turn interactions, SimBench employs a rule-based judge LLM (J-LLM) that leverages both predefined rules and human-in-the-loop guidance to assign scores for the DTs generated by the S-LLM, thus providing a consistent and expert-inspired evaluation protocol. The J-LLM is specific to a simulator, and herein the proposed benchmarking approach is demonstrated in conjunction with the Chrono multi-physics simulator. Chrono provided the backdrop used to assess an S-LLM in relation to the latter's ability to create digital twins for multibody dynamics, finite element analysis, vehicle dynamics, robotic dynamics, and sensor simulations. The proposed benchmarking principle is broadly applicable and enables the assessment of an S-LLM's ability to generate digital twins for other simulation packages. All code and data are available at https://github.com/uwsbel/SimBench.", "sections": [{"title": "Introduction", "content": "It is only a matter of time before a non-expert will be able to interact with an S-LLM in order to produce a sophisticated digital twin for a physical system of interest. The DT request might read \"generate a model of a VIPER rover operating on regolith and equipped with two monocular cameras, one stereo camera, and an autonomy stack that uses sensor feeds to ensure obstacle avoidance while operating in lunar conditions.\" Such a model can be generated for a target simulator should an expert dedicate enough time to the task. However, the interest here lies in the scenario where the DT generation falls upon an LLM. Specifically, this contribution proposes a benchmark that measures the acumen of such an S-LLM when it comes to generating DTs for a target simulator.\nAnalyzing complex multi-physics phenomena via simulation requires sophisticated DTs. S-LLMs can potentially handle these through multiple rounds of interaction, incorporating natural language feedback from humans. To the best of our knowledge, dedicated evaluation protocols for the task of generating DTs are nonexistent, as generic similarity-based metrics like CodeBLEU (Ren et al. 2020)\nand ROUGE-L (Lin 2004) do not align well with real-world simulation applications and are too crude to be useful. Furthermore, execution-based benchmarks like pass@k (Chen et al. 2021) are too strict, often resulting in zero scores even though the generated DT has only marginal flaws. Against this backdrop, the purpose of this contribution is to outline a benchmark that can indicate how good an S-LLM is at generating a DT. Here, a DT encapsulates a model used by a simulator to carry out a virtual experiment. For the VIPER rover example above, this model might be set up via hundreds of lines of code, e.g., Python, C++, or a proprietary model definition language. Additionally, if the model is instead generated using input files, e.g., in JSON format, the files might have hundreds to thousands of lines. Going back to the VIPER example, the Python model-file or the JSON file would establish for each wheel its center of mass, mass moments of inertia, initial position, initial velocity, collision detection mesh, visualization mesh, friction coefficients; for the chassis one would specify similar information; the camera models would require parameters associated with the bidirectional reflectance distribution function, aperture, exposure, focal length, parameters associated with Image Signal Processing step, etc. The rover would operate on terrain characterized by bulk density, internal friction angle, cohesion coefficient, particle size distribution, etc. Next, one would add an autonomy stack by setting up a ROS2 environment, choosing perception algorithms, a planning solution, a control policy, etc. Ultimately, the goal is to have an S-LLM generate the Python code that sets up the digital twin; and when the DT-generation request is vague and values for some model parameters are not provided, the S-LLM would either use default values or prompt the user for input.\nInsofar as the nomenclature used in the contribution is concerned, an analysis task in the real world corresponds to a simulation scenario in the digital world. A physical system, e.g., the VIPER rover, has a digital twin. Several digital twins, e.g., those of the rover, terrain, sensor suite, are combined in a simulation scenario towards running a virtual experiment. In the absence of a sim-to-real gap, the results collected in simulation are representative of results that would have been collected had the experiment taken place in the real world. Here, the expectation is that an S-LLM is used to generate the required digital twin[s], place them in a simulation scenario, and run a virtual test of interest"}, {"title": "Methodology", "content": "SimBench employs a diverse set of simulation scenarios associated with application areas that draw on multibody dynamics (MBD) analysis, vehicle (VEH) and robotic (RBT) experiments, finite element analysis (FEA), and sensor (SEN) use. Each scenario tests specific aspects of the S-LLM's performance, including DT set up, reasoning, and ability to carry out PyChrono model adjustments. Obtaining S-LLMs falls outside the scope of this contribution; the focus is on J-LLM and the benchmarking methodology.\nThe J-LLM associated with the Chrono SimBench handles multi-physics simulations, including but not limited to:\n\u2022 Collision, Contact and Friction Dynamics (MBD): Scenarios involving multi-link arms, gear mechanisms, slider-crank system and other typical mechanisms.\n\u2022 Vibration, deformation, stress, and strain (FEA): Scenarios involving cable, beam, shells, plates that evaluate the S-LLM's proficiency in structural analysis.\n\u2022 Vehicle Dynamics (VEH): City buses, off-road vehicles (e.g., HMMWV, M113), trucks (e.g., Kraz, MAN), and sedans are used to test the S-LLM's ability to simulate driving scenarios. Driver, engine, transmission, and tire models, as well as high-level control policies integrated with Sensors are included in the benchmark.\n\u2022 Sensor Integration (SEN): Scenarios involving GPS, IMU, LiDAR, and camera sensors are used to exercise the S-LLM's capability to support perception tasks for autonomous vehicles and robotic systems.\n\u2022 Robotics Dynamics (RBT): The benchmark touches on robotic systems like Turtlebot, Curiosity, and VIPER, as well as granular dynamics and deformable terrain simulations, e.g., the Soil Contact Model (SCM) (Krenn and Gibbesch 2011), that come into play in off-road operations for both robots and vehicles.\nSimBench draws on 102 demonstration tasks associated with 34 distinct physical systems of the categories MBD through RBT listed above. These tasks involve setting up and progressively modifying digital twins, with each task broken down into three high-quality turns. These turns have been designed by simulation experts to gradually increase in complexity, thus enabling the J-LLM to provide a robust assessment of the S-LLM's capabilities. A list of example simulation scenarios in SimBench is provided in Figure 2.\nTask Design and Structure: SimBench tasks assigned to an S-LLM fall into the categories of \"Vague Requests\" and \"Sharp Requests.\" The \"Vague Request\" tasks provide more abstract and less precise instructions, requiring the S-LLM to interpret and complete the task with minimal guidance, thereby allowing for significant latitude. Conversely, a \"Sharp Request\" offers detailed and explicit instructions, ensuring that the S-LLM has the necessary information to produce very specific DTs. Table 1 presents sample prompts from both categories, illustrating the varying levels of task request specificity."}, {"title": "Evaluation framework and modalities", "content": "In the approach proposed, a rule-based J-LLM evaluates the performance of an S-LLM using the tasks contained in the SimBench environment. Unlike traditional metrics such as BLEU, ROUGE, and CodeBLEU, the J-LLM is designed to provide a comprehensive evaluation of the S-LLM's performance in multi-turn simulation tasks. The J-LLM is conditioned on a large dataset of expert demonstrations and is capable of providing detailed feedback on the S-LLM's code quality, accuracy, and efficiency.\nThree different modalities can be used to evaluate the S-"}, {"title": "Benchmarking Experiments", "content": "S-LLMS Evaluated. SimBench is demonstrated in conjunction with benchmarking a set of 20 S-LLMs, encompassing both open- and closed-source models of various sizes, architectures, and family. For closed-source models, we include prominent commercial S-LLMs, such as gpt-40-mini and gpt-40 from OpenAI, as well as Claude 3.5 Sonnet from Anthropic, and Gemini 1.5pro from Google. The open-source models evaluated include the LLaMA-3.1 family (8B, 70B, 405B), which consists of both base and instruct versions. We included specialized models like codellama-70B, codestral-22B, CodeGemma-7B, and also the mamba class mamba-codestral-7b model, fine-tuned for code generation. We also considered S-LLMs that have been trained on a mix of general and domain-specific data, e.g., Gemma-2 (2B, 9B, 27B) and Mistral (12b, 8x7b, 8x22B). Additionally, models such as Phi-3 (3.8B, 7B, 14B) and Mixtral (8x7B, 8x22B, 122B) are included to explore the impact of different scales and training regimes on their ability to perform in multi-turn setups. Because we are testing the instruction following capability of the S-LLMs, models like Starcoder-2 are not included. For the J-LLM, we picked gpt-40-mini because it gives the best performance in the pass@1 metric, which is highly relevant in simulation.\nMetric. We employ the most common used code evaluation metric - pass@1, as well as the proposed J-LLM with three different modalities. As a reference point, we also employ the most commonly used similarity metrics CodeBLEU and ROUGE-LSUM. The results averaged over all test cases are shown in Table 2. Although all the tested S-LLMs are pre-trained on Project Chrono, the highest pass@1 is only 12.8% achieved by gpt-40-mini. Notably, gpt-4o gets a bottom score. Table 3 reports the multi-turn performance of S-LLMs in the SimBench environment, benchmarked with the metrics J-LLM_Ref_Doc.\nAnalysis of results. The pass@k metric is the most important metric, as it indicates the correctness of the DT[s] and simulation scenario. However, the pass@k metric is hard to employ since it is difficult to write unit tests for complex simulation scenarios, some of which might run for days, e.g., fluid-solid interaction scenarios. Nonetheless, for the set of 34 physical systems considered herein, we evaluated the performance of pass@1, of the proposed rule based J-LLM metrics, of the similarity metrics CodeBLEU and ROUGE-LSUM, and the compile@1 metric. There are 34 mechanical systems considered, each with three turns, attacked by 19 S-LLMs (three S-LLMs failed, see bottom of Table 2). This translated into 1938 simulations, which were assessed by seven metrics see heading of Table 2). This yielded 1938 \u00d7 7 = 13,566 data points that entered a Spearman rank correlation analysis to compare the different metrics.\nThe results are summarized in Fig. 3. Considering the correlation with pass@1, the metric induced by J-LLM_Ref_Doc has the highest correlation, 0.69, which sug-"}, {"title": "Related Work", "content": "LLMs in Digital Twin and Simulation Generation: In (Xia et al. 2024), the authors investigate the application of LLMs in multi-agent DT simulations, focusing on a simple 2D grid world. Their evaluation is primarily based on the similarity of the generated code to the ground truth, which limits the complexity and generalizability of the simulation scenarios considered. (Zhang et al. 2024a) explore LLM-driven decision-making within multi-agent simulations, highlighting the potential of LLMs to automate complex decision processes in dynamic environments. Additionally, in (Mu et al. 2024) the authors delve into the use of rule-based rewards for training LLMs in controlled settings.\nCoding Benchmarks: In the realm of testing code generation abilities, AlphaCode assesses the generated code against hidden test cases (Li et al. 2022). Similarly, CodeT employs self-generated unit tests to score the correctness of function implementations, ensuring that models are evalu-"}, {"title": "Conclusion and Future Work", "content": "To the best of our knowledge, this contribution is the first to establish a high-quality dataset and associated benchmark for simulating the ability of a S-LLM to generate DTs for multi-physics simulation. Although most S-LLMs are pre-trained on the Project Chrono code base, the best \"out of box\" S-LLM only give a pass@1 value of 13%. SimBench is positioned to guide future S-LLMs improvements for Chrono DTs or those associated with other simulators.\nSecondly, The proposed rule-based J-LLM is a more effective judge compared to execution-based benchmarks like compile@1, as well as similarity-based metrics like Code-BLEU and ROUGE-LSUM, because it more closely aligns with pass@1, which reflects the true and ultimate performance of S-LLMs. Lastly, we note that utilizing reference code and API documentation enhances the performance of the J-LLM. This finding also suggests a potential direction for future work: fine-tuning the J-LLM based on ground truth code and API documentation; or using RAG-based approaches. We plan to also increase the number of simulation scenarios available in the benchmark, particularly in areas such as fluid-solid interaction and multi-phase flow. Finally, an upcoming contribution addresses the challenge of producing high-quality S-LLMs, complementing the J-LLM-focused discussion presented here."}]}