{"title": "Enhancing Crash Frequency Modeling Based on Augmented Multi-Type Data by Hybrid VAE-Diffusion-Based Generative Neural Networks", "authors": ["Junlan Chen", "Qijie He", "Pei Liu", "Wei Ma", "Ziyuan Pu"], "abstract": "Crash frequency modeling is used to analyze how explanatory factors, such as traffic volume, road geometry, and environmental conditions, influence the frequency of crashes. Inaccurate predictions can distort the understanding of how different explanatory factors contribute to crashes. As a result, they can lead to misinformed policy decisions and wasted resources, ultimately jeopardizing traffic safety. Nevertheless, crash frequency modeling is often constrained by excessive zero observations in crash datasets, a limitation rooted in traditional data collection methods that rely on historical crash records, such as police reports, insurance claims, and hospital records. The low probability occurrence of crashes, unreported minor crashes, and the high costs associated with the data collection process collectively lead to a high prevalence of zero crash count. This problem can reduce the accuracy of crash frequency modeling, leading to biased predictions that impede safety policy-making. Existing approaches to address excessive zero observations primarily include statistical methods, data aggregation and resampling techniques. While statistical methods often rely on restrictive distributional assumptions, data aggregation and sampling techniques can lead to significant information loss and introduce biases that distort the representation of crash data. To overcome these limitations and provide a more cost-effective alternative for crash data collection, we introduce deep generative models that can augment crash data by producing high-quality synthetic samples and reducing the excess zero observations. However, while existing deep generative models have made progress in synthetic tabular data generation, they often struggle with the complexity of multi-type tabular crash data, which includes count, ordinal, nominal, and real-valued variables. In addition, typical deep generative models rely on simplistic one-hot encoding techniques, leading to dimensional expansion, which increases computational complexity and hinders models' ability to capture correlations between features, ultimately limiting their ability to generate realistic synthetic crash data. Therefore, this study proposes a hybrid VAE-Diffusion neural network to address excessive zero observations in crash data and tackle the data generation challenges posed by the complexities of multi-type tabular crash data. We evaluate the quality of the synthetic data generated by the proposed model against baseline models using multiple metrics such as similarity, accuracy, diversity, and structural consistency. Furthermore, we compare the predictive performance of a machine learning model trained on augmented crash data with that of a traditional statistical model. Finally, we interpret the crash frequency prediction results and provide policy recommendations to enhance traffic safety. By effectively capturing the complex patterns in multi-type crash data, the proposed hybrid VAE-Diffusion model surpasses baseline models across all metrics in terms of synthetic data quality and predictive accuracy. The findings of this study highlight the potential of synthetic data to improve crash frequency modeling and provide actionable recommendations to enhance traffic safety.", "sections": [{"title": "1 INTRODUCTION", "content": "Crashes are complex and multidimensional events where many interrelated factors can contribute to their occurrence. To account for the complexity and randomness of crash data, crash frequency modeling is widely employed as a useful tool to analyze how various explanatory factors, e.g., traffic volume, road geometry, and environmental conditions, correlate to the aggregated number of crashes that occurred in a given time at different locations (Lord et al., 2021). Accurate crash frequency modeling is essential in assisting governmental agencies to develop effective safety policies and allocate resources to high-risk areas, ultimately improving traffic safety and reducing crashes (Lord and Mannering, 2010). However, the crash data are often characterized by excess zero observations, indicating that a significant portion of the data consists of zero crash counts (Ali et al., 2024; Dzinyela et al., 2024). This can be problematic in crash frequency modeling because excess zeros can bias parameter estimates, resulting in erroneous predictions and misleading inferences about explanatory factors (Gu et al., 2020; Raihan et al., 2019).\nThe root of having excess zero observations in crash data lies in the limitations of the traditional data collection method. Crash data are often collected from police reports, insurance claims, and hospital records after crashes occurred and the data collection process is typically costly (Lord et al., 2021). Given that crashes are low-probability events, post-crash data collection methods naturally lead to a small sample size of recorded crashes. Moreover, many minor crashes are not documented due to factors such as perceived insignificance or absence of police involvement, which further reduces the sample size of recorded crashes (Lord et al., 2005). These limitations collectively result in a crash count distribution that skews heavily toward zero. Although there are emerging technologies, such as data collection from on-road electronic equipment like video surveillance, radars, loop detectors (Maha Vishnu et al., 2017), and from onboard equipment installed on vehicles to gather crash data related to vehicle conditions, driver distractions, traffic information, etc (Josephinshermila et al., 2023). However, since crashes are rare, such collection methods still struggle with the problem of excess zero observations, as they cannot capture data where crashes are nonexistent.\nApproaches to address excessive zero observations in crash data include both traditional model-driven statistical models and data-centric techniques. Model-driven statistical models, such as zero-inflated Poisson and zero-inflated negative binomial (Lee and Mannering, 2002) are often constrained by assumptions of data distribution and may not fully capture the underlying patterns in crash datasets with multiple data types (i.e., count, ordinal, nominal, and real-valued) and complex interactions between various explanatory variables (Ding et al., 2022a). Alternatively, data-centric techniques provide greater flexibility to handle excessive zero observations. An approach is to aggregate crash data over extended time periods or across longer road segments, though this can result in information loss and reduce reliability in crash frequency prediction (Usman et al., 2011). Resampling techniques have also been used, including under-sampling and over-sampling (Pei et al., 2016). However, under-sampling can discard potentially valuable data, while over-sampling methods suffer from generating minority samples that are most likely to be majority, which can be deceptive and lead to severe failures in real-world applications (Tarawneh et al., 2022).\nDeep generative models are effective techniques in alleviating excessive zero observations; meanwhile, they can reduce the time-consuming and expensive crash data collection process (Missaoui et al., 2023). Deep generative models show strong performance on image data with continuous pixel values and local spatial correlations, yet face challenges with heterogeneous tabular data due to its multi-type features and lack of inherent spatial relationships (Borisov et al., 2024a; Kim et al., 2022). Several deep generative models show improvement in tabular data generation, such as Conditional Tabular Generative Adversarial Networks (CTGAN)(Xu et al., 2019), tabular variational autoencoder (TVAE) (Xu et al., 2019), and Generation of Realistic Tabular data (GReaT)(Stoian et al., 2024). Recently, a novel Diffusion model has demonstrated even greater performance in generating data with high quality and diversity (Kim et al., 2023). Despite its impressive capabilities, applying diffusion models to tabular data still presents challenges (Kotelnikov et al., 2022). One of the key challenges is that encoding techniques commonly used in deep generative models face difficulties in handling tabular crash data (Borisov et al., 2024b). Most existing models adopted simple encoding techniques such as one-hot encoding (Lee et al., 2023; Liu et al., 2023) and analog bit encoding (Zheng and Charoenphakdee, 2022) to convert discrete features into numerical format. However, the crash dataset is highly structured multi-type data with high-cardinality variables such as count and ordinal variables, which simple encoding methods struggle to handle. Simple encoding may aggravate the \u2018curse of dimensionality' problem, expanding tabular crash data into high-dimensional vectors. This dimensional expansion in the data increases computational complexity as the models struggle to capture meaningful correlations across features. Consequently, this undermines the model's ability to generate realistic synthetic data (Lee et al., 2023; Liu et al., 2023).\nTo this end, this study introduces a data-driven crash frequency modeling approach based on the advanced VAE-Diffusion deep generative model (Zhang et al., 2023). Following data generation, we compare the proposed model with the baseline generative models, in terms of the synthetic data quality and prediction accuracy. Lastly, we interpret the prediction result and provide policy recommendations for traffic safety and management.\nThe main contributions of the study are summarized below:\n(1) A novel deep generative neural network designed to process multi-type crash data by transforming diverse input features into a unified embedding space, where the model can simultaneously learn and extract various features. It generates synthetic data that mirrors real-world crash data, thus resolving excessive zero observations and offering a more efficient and cost-effective alternative for data collection.\n(2) The hybrid VAE-Diffusion architecture of the generative model, integrating a VAE with a Transformer encoder and decoder, maps all features to a lower-dimensional space, mitigating the 'curse of dimensionality.'\n(3) A comprehensive study was conducted to evaluate the quality of the synthetic data with complex multi-types features, using coverage similarity metrics like distribution plots and statistical metrics, along with structural similarity metrics like Pair-wise Correlation Difference (PCD)."}, {"title": "2 LITERATURE REVIEW", "content": "Crash frequency modeling serves as a vital tool for analyzing nonnegative and discrete crash events on roadways, enabling the assessment of safety performance across entities such as roadway segments, intersections, and corridors(Lord et al., 2021). Conventional statistical models for handling excessive zero observations often adapt count-data models to better accommodate the large number of zero observations. Raihan et al. (2019) and Lord et al. (2005) applied zero inflation Poisson model and the zero-inflation negative binomial model, which assume two processes: one where the roadway section is virtually safe with zero crashes, and another where the section is in a non-negative count state for crashes, following a Poisson or negative binomial distribution. Yet, according to Lord et al. (2007), these models have intrinsic logical shortcomings, as they fail to establish clear boundary conditions to differentiate fully safe states from unsafe states. Subsequently, the negative binomial Lindley model, a mixture of negative binomial and Lindley distributions (Ghitany et al., 2008; Lindley, 1958), was introduced to address excess zero observations (Behara et al., 2021; Rusli et al., 2018; Shinthia Azmeri et al., 2023). Building on this, Islam et al. (2022) employ a Finite Mixture Negative Binomial-Lindley (FMNB-L) model, combining the Negative Binomial-Lindley distribution with finite mixture modeling to account for heterogeneous subpopulations and excess zero observations in crash data. However, similar to other statistical models, a notable limitation of this approach is that the increased complexity of the model restricts its applicability to relatively small datasets. Owing to the limitations of statistical models, such as predefined assumptions and inability to handle large datasets, machine learning-based approaches have gained traction for addressing excessive zero observations. Compared to statistical models,machine learningg modelsoutperformm in predictionass they are flexible, require no prior assumptions about data distribution, and can learn complex patterns in high-dimensional data (Li et al., 2008; Zeng et al., 2016a). For example, Chang and Chen (2005) employed the CART (Classification and Regression Trees) method, a decision tree-based approach that recursively splits data into subsets to model crash frequency. They found that the model achieved better prediction accuracy compared to a negative binomial regression model, with accuracy rates of 52.6% and 52.3%, respectively. Das and Abdel-Aty (2011) model crash frequency using a Genetic Programming (GP) approach and conclude that GP is effective in capturing the relationship between crash frequency and explanatory factors such as average daily traffic, roadway conditions, and skid resistance, providing a comprehensive understanding of rear-end crashes on urban arterials. Zeng et al. (2016) employs an optimized Neural Network (NN) approach and conclude that the structure-optimized NN model not only outperforms traditional Negative Binomial (NB) models in terms of fitting and predictive performance but also effectively identifies and removes variables with insignificant effects on crash frequency, resulting in a more interpretable and simplified model. Additionally, the extracted rules confirm the existence of nonlinear relationships between crash frequency and explanatory factors. Although machine learning models excel at capturing complex nonlinear relationships in crash frequency modeling, their performance is still limited by data quality, including the completeness and representativeness of the available data (Mannering et al., 2020)."}, {"title": "2.2 Resampling Techniques for Multi-Type Crash Frequency Data", "content": "Previous studies focusing on data-centric approaches often utilize data aggregation or resampling techniques to solve excessive zero observations. Data aggregation involves adjusting space and time scales to reduce the number of zero observations while maintaining the homogeneity of the observations (e.g., lane width, shoulder width, etc.). For example, aggregate 200-meter road segments into 400-meter segments, combined with aggregation over different time scales (e.g., 2, 3, 4, and 6 years) to decrease the proportion of zero observations before developing regression models. However, this may lead to a loss of spatial-temporal information and compromise the accuracy of crash frequency prediction (Usman et al., 2011). Traditional resampling techniques are more sophisticated methods to address excessive zero observations. Abdel-Aty et al. (2004) and Yang et al. (2018) use matched case-control design to select a matched sub-set out of the full dataset, controlling for external factors such as time of day, season, year, geometric and roadway features, etc. Yet such under-sampling methods may lead to a loss of important information and potentially degrade model performance (Cai et al., 2020). On the other hand, Chen et al. (2022) applied the Synthetic Minority Over-sampling Technique for panel data (SMOTE-P), which increases minority class samples to balance the data, to deal with excessive zero observations of bus-involved crashes and improve model accuracy. Nevertheless, since SMOTE uses interpolation of existing samples, it can oversimplify relationships between variables, generating synthetic data that fails to capture the full complexity of crash patterns or reflect real-world variability, potentially leading to overfitting or biased predictions (Branco et al., 2016; Tarawneh et al., 2022). This limitation has motivated the adoption of deep generative models that can better represent and address the complexities inherent in crash frequency data (Chen et al., 2024).\nCrash frequency data often contain multi-type variables (i.e., count, ordinal, nominal, and real-valued variables), posing significant challenges for deep generative models in addressing excessive zero observations. Most existing studies use Generative Adversarial Networks (GANs) (Cai et al., 2020; Man et al., 2022) or variational autoencoder (VAE) (Ding et al., 2022b) to address class imbalance in crash data. As deep generative models evolve, various models built on different foundational architectures emerge (Zhang et al., 2023). Although these methods offer advantages for multi-type data synthesis, concerns remain about data quality due to their limitations in capturing the complex relationships between variables. Conditional Tabular Generative Adversarial Networks (CTGAN) and tabular variational autoencoder (TVAE) (Xu et al., 2019) both utilize the Mode-specific Normalization model and treat each category of discrete variables as a separate mode and normalize the continuous variables within each mode. However, this method can introduce model complexity that can obscure the features of the variables and may fail to capture potential relationships between different categories. Score-based Tabular Data Synthesis (StaSy) (Kim et al., 2023) is a diffusion-based model, which employs a technique that treats one-hot encoded categorical variables as continuous features and processes them together with numerical columns. Nevertheless, treating all variables as continuous may result in high-dimensional feature spaces and make it difficult for the model to learn data features and capture intrinsic features of the discrete variables. Generation of Realistic Tabular data (GReaT) (Borisov et al., 2022) employs a serialization technique that converts both discrete and continuous variables into natural language sentences. Then the \u2018sentences' can be processed by an auto-regressive GPT model. However, the natural language descriptions can lead to loss of detail or ambiguity, as similar values might be described in similar ways, affecting the model's ability to capture precise data features. Additionally, while shuffling rows ensures permutation invariance, it may not fully capture the relationships and structures between columns, particularly when column order is important for analysis."}, {"title": "3 METHODS", "content": null}, {"title": "3.1 Multi-Type Data Generative Neural Network", "content": "We propose a Hybrid VAE-Diffusion deep generative model to generate synthetic samples of the minority class (i.e., non-zero crash samples) to rebalance the data. The model is comprised of two main components: a Variational Autoencoder (VAE) and a Diffusion data generator, as shown in Fig.2. The VAE, with a Transformer encoder and decoder, turns the data into a machine-readable form, so that the diffusion model can better understand and learn the intra- and inter-column patterns of the dataset. With the proposed two-component structure, the model can better handle and learn from the four-multi-type, highly-structured tabular crash data, and generate synthetic data of outstanding quality."}, {"title": "3.2 Problem Definition", "content": "Each column in the crash data table is a variable and each row is a sample. In this study, the crash dataset contains four types of variables (or columns): ordinal, nominal, count, and real-valued. The first three are discrete variables: nominal variables represent categorical data without any inherent order (e.g., the presence of light: 0 = no, 1 = yes); ordinal variables have a natural order (e.g., the hour of the day: 0, 1, 2, 23); and count variables indicate the frequency of occurrences using non-negative integers (e.g., crash frequency: 0, 1, 2, 3, ...). Real-valued variables are continuous variables that can take any value within a range, such as the length of the road segments.\n$M_{dis}$ and $M_{con}$ denote the number of discrete and continuous columns (or variables), respectively. Each row in the crash data table, corresponding to the record of the crash frequency that occurred in a certain road segment in the specific hour of the year along with the road information, is represented as a vector x = [xls,xon], where x has a dimensionality of ($M_{dis}$ + $M_{con}$). The i-th discrete variable $x^{dis}_{i}$ has $C_i$ candidate values, where $x^{dis}_{i}$ \u2208 {1,..., $C_i$} We input the original training dataset T = {x} to the model to learn the parameterized generative model $P_\\theta(T)$, where synthetic data $x$ \u2208\u00ce can be generated."}, {"title": "3.3 Data Transformation with VAE-Transformer Model", "content": "Data transformation refers to the process of converting the tabular data into a model-learnable format-column embedding. Effective transformation enables the model to extract meaningful information signal from the data (Borisov et al., 2024a). The transformation process involves several steps: Feature Tokenization, Transformer Encoding and Decoding, and Detokenization.\nFeature Tokenization. Tokenization is the first step is to map all the columns into a d-dimensional vector\u2014the initial column embedding. We apply one-hot encoding to preprocess discrete variables, i.e., count, ordinal, and nominal variables, denote as $x^{one-hot} \\in R^{1\\times C_i}$; Then each row (or record) can be represented as $x = [x^{con}, x^{one-hot},...,x^{one-hot}  ] \\in R^{M_{con} + \\sum_{i=1}^{M_{dis}}C_i}$, where x is a ($M_{con} + \\sum_{i=1}^{M_{dis}}C_i$) dimension vector. Then create a d-dimensional vector for continuous and discrete columns, by applying linear transformation and create an embedding lookup table for the two types of columns, respectively.\n$e^{con} = x^{con}.w^{con} + b^{con}$,   $w^{con}, b^{con}, e^{con} \\in R^{1\\times d}$ (1)\n$e^{dis} = x^{one-hot}.w^{dis} + b^{dis}$,   $b^{dis}, e^{dis} \\in R^{1\\times d}$   $w^{dis} \\in R^{C_i \\times d}$,   (2)\nwhere $w^{con}, b^{con}, b^{dis}, w^{dis}$ are learnable parameters of the tokenizer. Stack all the rows into the embeddings of all column, denoted as E, which is the initial column embedding:\n$E  = [e^{con},...,e^{con}, e^{dis},...,e^{dis} ] \\in R^{M \\times d}$   (3)\nTransformer Encoding and Decoding. Simple tokenization might lead to the model's incomplete understanding of each variable's unique characteristics, especially in the crash dataset like ours that includes ordinal, nominal, count, and real-valued variables. We introduce a Transformer encoder-decoder to perform the multi-dimensional encoding, which effectively aids our model to understand the variables features and complexity of the data pattern.\nThe encoder and decoder are both two-layer transformers, each with a Self-Attention module and a Feed Forward Neural Network (FFNN) denoted as H0, which is a simple two-layer MLP. The loss function for constructing the VAE is in Equation (4).\n$H_1 = ReLU(FC(H_0)) \\in R^{M \\times D}$,\n$H_2 = FC(H_1) \\in R^{M \\times d}$,  (4)\n$L =l_{recon} (x,\\hat{x}) + \\beta l_{kl}$ (5)\nThe VAE encoders are designed to generate $\\mu^i$ and $\\sigma^i$ to construct the latent embedding Z, shown as Equation (6):\n$Z = \\mu+\\sigma\\cdot\\epsilon, \\epsilon\\epsilon N (0,1)$ (6)"}, {"title": "3.4 Diffusion Data Synthesis", "content": "The Diffusion model is designed to learn the underlying distribution of crash data and generate synthetic data. Inspired by the thermodynamic process where particles spread out until evenly distributed, the model adds and spreads out noises to the data. It then learns to reverse this process, removing the noise step-by-step to create clear, structured samples. Instead of directly learning the distribution of the data itself, the Diffusion model learns how to denoise the data. This strategy allows it to generate diverse data and reduces the likelihood of mode collapse.\nAfter the VAE-Transformer model is well-learned, latent embedding Z can be extracted from the encoder, flattened, and then transfer into a vector as $z = Flatten(Encoder(x)) \\in R^{1\\times d}$.\nThe distribution of the latent embeddings is $P(z)$, learned by the diffusion model via two processes a forward adding noise process and a reverse denoising process. In the forward process, Stochastic Differential Equation (SDE) and Variance Exploding (VE) (Song et al., 2020) are applied to acquire the perturbed data $z_t$, which is the diffused embedding at time t, and $\\sigma(t)$ is the noise level, shown as Equation (10). In the reverse process, $\\bigtriangledown z_{t}log p(z_{t})dt$ is the score function of $z_t$, $W_t$ is the standard Wiener process. The loss function is in Equation (11) (Karras et al., 2022), where $\\epsilon_{\\theta}(z_t, t)$ is the denoising function, which is used to estimate the Gaussian noise.\n$z_t = z_0 + \\sigma(\\tau)\\epsilon, \\epsilon\\sim N (0,1)$ (Forward Process) (9)\n$dz_t = -2\\delta(t)\\sigma(t)\\bigtriangledown z_t log p(z_t)dt + \\sqrt{2\\delta(t)\\sigma(t)} dW_t$ (Reverse Process) (10)\n$L = E_{z_0\\sim P(z_0)}E_{t\\sim p(t)}E_{\\epsilon\\sim N (0,1)} ||\\epsilon_{\\theta}(z_t, t) - \\epsilon ||$ where $z_t = z_0 + \\sigma(t)\\epsilon$ (Loss Function) (11)\nTo obtain the denoising function $\\epsilon_{\\theta}(z_t, t)$, a five-layer Multilayer Perceptron (MLP) is trained, where the hidden layer is fully connected(Kotelnikov et al., 2022), as shown in Equation (12). Lastly, the denoising function can be applied to the loss function for model training in Equation (11).\n$h_0 = FC_{in} (z_t) \\in R^{1\\times d_{hidden}}$\n$h_{in} = h_0\\oplus t_{emb}$\n$h_1 = SiLU(FC_1(h_{in}) \\in R^{1\\times 2*d_{hidden}} )$\n$h_2 = SiLU(FC_2(h_{1}) \\in R^{1\\times 2*d_{hidden}} )$\n$h_3 = SiLU(FC_3(h_2) \\in R^{1\\times d_{hidden}} )$\n$\\epsilon_{\\theta}(z_t, t) = h_{out} = FC_{out}(h_3) \\in R^{1\\times d}$ (12)"}, {"title": "4 Experimental Design", "content": null}, {"title": "4.1 Generation and Evaluation Framework", "content": "This study aims to address the excessive zero observations and enhance the overall performance of crash frequency modeling. The data generation and evaluation framework is shown in Fig. 1. We compare the quality of synthetic data generated by both the proposed multi-type data generation model, and the baseline models. The quality of synthetic data can be assessed based on its coverage and structural similarity to the real data. We adopt various metrics, including distribution plots, Detection Score (C2ST), \u03b1-Precision and \u1e9e-Recall. While the structural similarity is appraised by Pair-wise Correlation Difference (PCD).\nWe use a machine learning (ML) model, XGBoost, for prediction. To evaluate the modeling accuracy of each resampling method, we compared crash frequency prediction models that were fitted using data balanced by different generative models. The relevant metrics, calculated using the test set, include RMSE and MSE. We also compare the accuracy of the proposed ML-based model to classic statistical models\u2014Zero-inflated Poisson Regression. Since ML-based models can often be perceived as black boxes due to their complexity, we use SHAP (SHapley Additive exPlanations) to visualize and provide an intuitive explanation of the prediction results."}, {"title": "4.2 Data Description", "content": "This study focuses on crashes on the mainline of Interstate 5 (I-5) from milepost 139.69 to 210.10 in Washington State in 2019. The crash dataset, sourced from the Highway Safety Information System (HSIS) (Anusha patel Nujjetty, 2022), includes roadway and crash files. The roadway file provides information for each road segment, such as the number of lanes, presence of lighting, lane width, segment length, and other details. The crash file contains officer-reported crash incidents.\nWe select road segments longer than 0.05 miles (approximately 64 meters) () to reduce the potential data coding errors. To learn the impact of time factors on crash frequency, we count the crashes that happened in each segment for every hour of the day throughout the year. The number of crashes on each segment for each hour within a 24-hour period, along with the segment's geometric information, is treated as a record. As a result, there are a total of 3714 crashes across 372 road segments in both directions in 2019. The total number of records is 372 (road segments) * 2 (directions) * 24 (hours) * 1(year) = 17856, of which 15142 (84.8%) have zero observations. The prevalence of zero observations in our dataset indicates a significant excess zero problem. The summary statistics of variables included in this research are shown in Table 1."}, {"title": "4.3 Data Resampling", "content": "To address excessive zero observations in our dataset, we use the deep generative over-sampling technique (i.e., VAE-Difussion, STaSy, CTGAN, GReaT, and TVAE) to resample our imbalanced crash dataset. We take the training set as the input for the generative models. They then can learn the data distribution and generate synthetic non-zero crash records for rebalancing."}, {"title": "4.4 Model Performance Evaluation", "content": "To validate the effectiveness of the proposed approach, we structured the experiment in two parts. First, we evaluated the quality of the synthetic data generated by different generative models. Second, we compared the prediction accuracy of these data-driven modeling approaches with a statistical model."}, {"title": "4.4.1 Baseline Models for Comparison", "content": "We compare four baseline deep generative models against our proposed multi-type data generation model, including Conditional Tabular Generative Adversarial Networks (CTGAN), tabular variational autoencoder (TVAE), Generation of Realistic Tabular data (GReaT), and Score-based Tabular Data Synthesis (StaSy). These baseline models are chosen as they represent key generative approaches based on Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion models, making them relevant for comparison with our proposed model. Each of these models learns the underlying data distribution differently. CTGAN and VAE are based on two fundamental generative models, namely GANs and VAEs. GReaT, inspired by the transformer-based large language models (LLMs), treats each row of tabular data as a sentence and learns sentence-level distributions. And StaSy is a diffusion-based generative model. We evaluate the quality of the synthetic data they generate. The evaluation focuses on coverage and structural similarity. We then use the data generated by these models to predict crash frequency with XGBoost.\nWe also compared our data-driven approach with the traditional statistical model, the Zero-inflated Poisson (ZIP) model. The ZIP model is designed to handle count data with excess zeros. It assumes that two separate processes generate the data: A process that generates only zeros (zero-accident state); and another Poisson process that generates non-negative counts, including zeros(Lee and Mannering, 2002). The ZIP model can be expressed as:\n$P(n_{i,j}= 0) = p_i+ (1-p_i)e^{-\\lambda_{ij}}$ (13)\n$P(n_{i,j}= n) = (1 - p_i) \\frac{\\lambda_{ij}^n}{n!}, n > 0$ (14)\nwhere $n_{i,j}$ is the number of accidents on the roadway section i during period j; $p_i$ is the probability that segment i is in the zero-accident state; and $\\lambda_{ij}$ is the expected number of accidents for segment i in period j when it is not in the zero-accident state."}, {"title": "4.4.2 Synthetic Data Quality Metrics", "content": "The quality of synthetic data is evaluated by how similar its coverage and structure are to the real data. The coverage similarity was evaluated via distribution plots and statistical metrics like Detection Score (C2ST), a -Precision and B-Recall. While the structural similarity is appraised by Pair-wise Correlation Difference (PCD).\nDetection Score (C2ST). Classifier Two Sample Test (C2ST) is a metric that evaluates how difficult it is to tell apart the real data from the synthetic data. To calculate C2ST, first, create a table combining all rows of real and synthetic data, have all rows labeled, split it into training and validation sets, and then train a machine learning model-Logistic Regression\u2014to predict these labels. The score is derived from the model's performance, typically using based on the average ROC AUC score across all the cross-validation splits. The C2ST score ranges from 0 to 1, where values closer to 1 indicate greater similarity to the real data, given by:\n$score=1-(max(ROC AUC,0.5)\\times2-1)$ (15)\na-Precision and \u1e9e-Recall. a-Precision $P_{\\alpha}$ and\u1e9e-Recall $R_{\\beta}$ are two high-order metrics that measure the overall fidelity and diversity of synthetic data (Ahmed M. Alaa, 2022). $\\overline{X}_r$ and $\\overline{X}_s$, represents the synthetic and real sample, $S^{\\alpha}_r$ and $S^{\\beta}_s$ denotes the \u03b1- support of the real distribution and \u1e9e- support of the synthetic data distribution, respectively. Both metrics range from 0 to 1, where the closer the values are to 1, indicates higher fidelity, diversity and better coverage, calculated by:\n$P_{\\alpha}= P(\\overline{X}_s\\in S^{\\alpha}_r)$ (16)\n$R_{\\beta}= P(\\overline{X}_r\\in S^{\\beta}_s)$ (17)\nPair-wise Correlation Difference (PCD). $S_{A,B}$ and $R_{A,B}$ denote the correlation coefficient between column A and B on the real and synthetic data, R and S, respectively. Then average the absolute difference for normalization to make sure it falls in the range of [0,1], defined as Equation ."}, {"title": "4.4.3 Crash Frequency Modeling Accuracy Metrics", "content": "Root Mean Squared Error (RMSE) and Mean Squared Error (MSE) are two commonly used metrics to evaluate regression accuracy. MSE represents the average of the squared differences between predicted and observed values, calculated as Equation, where y, are the observed values, \u0177 are the predicted values, and n is the number of observations. RMSE measures the average magnitude of the errors between predicted and observed values. It's the square root of MSE, calculated as Equation.\n$MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$ (19)\n$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}$ (20)"}, {"title": "4.5 Prediction Interpretation", "content": "SHAP (SHapley Additive exPlanations) provides a unified approach to interpreting machine learning model predictions by assigning each feature an importance value based on its contribution to the prediction. This methodology is grounded in the concept of Shapley values from cooperative game theory, ensuring a fair distribution of the \"payout\" (model prediction) among the features. The SHAP value of feature i is denoted as \u03c6i, given by:\n$\\phi = \\sum_{S\\subseteq N\\{i\\}} \\frac{|S|! (|N|-|S|-1)!}{| N |!} [f(S\\cup\\{i\\})-f(S)]$ (21)\nwhere S \u2286 N, N is the total set of all features, and S is any subset of N that excluded the feature i. |N| and |S| are the number of features in both sets, respectively. f(S) is the model's prediction using the features in subset S and $f(S\\cup\\{i\\})$ is the prediction when feature i is added to subset S."}, {"title": "5 RESULT ANALYSIS"}]}